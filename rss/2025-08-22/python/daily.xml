<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Thu, 21 Aug 2025 01:37:11 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>laude-institute/terminal-bench</title>
      <link>https://github.com/laude-institute/terminal-bench</link>
      <description>&lt;p&gt;A benchmark for LLMs on complicated tasks in the terminal&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;terminal-bench&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;#####################################################################
#  _____                   _             _     ______________       #
# |_   _|__ _ __ _ __ ___ (_)_ __   __ _| |   ||            ||      #
#   | |/ _ \ '__| '_ ` _ \| | '_ \ / _` | |   || &amp;gt;          ||      #
#   | |  __/ |  | | | | | | | | | | (_| | |   ||            ||      #
#   |_|\___|_|  |_| |_| |_|_|_| |_|\__,_|_|   ||____________||      #
#   ____                  _                   |______________|      #
#  | __ )  ___ _ __   ___| |__                 \\############\\     #
#  |  _ \ / _ \ '_ \ / __| '_ \                 \\############\\    # 
#  | |_) |  __/ | | | (__| | | |                 \      ____    \   #
#  |____/ \___|_| |_|\___|_| |_|                  \_____\___\____\  #
#                                                                   #
#####################################################################
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/6xWPKhGDbA"&gt;&lt;img src="https://img.shields.io/badge/Join_our_discord-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://github.com/laude-institute/terminal-bench"&gt;&lt;img src="https://img.shields.io/badge/T--Bench-000000?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=000&amp;amp;logoColor=white" alt="Github" /&gt;&lt;/a&gt; &lt;a href="https://www.tbench.ai/docs"&gt;&lt;img src="https://img.shields.io/badge/Docs-000000?style=for-the-badge&amp;amp;logo=mdbook&amp;amp;color=105864" alt="Docs" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Terminal-Bench is the benchmark for testing AI agents in real terminal environments. From compiling code to training models and setting up servers, Terminal-Bench evaluates how well agents can handle real-world, end-to-end tasks - autonomously.&lt;/p&gt; 
&lt;p&gt;Whether you're building LLM agents, benchmarking frameworks, or stress-testing system-level reasoning, Terminal-Bench gives you a reproducible task suite and execution harness designed for practical, real-world evaluation.&lt;/p&gt; 
&lt;p&gt;Terminal-Bench consists of two parts: a &lt;strong&gt;dataset of tasks&lt;/strong&gt;, and an &lt;strong&gt;execution harness&lt;/strong&gt; that connects a language model to our terminal sandbox.&lt;/p&gt; 
&lt;p&gt;Terminal-Bench is currently in &lt;strong&gt;beta&lt;/strong&gt; with ~100 tasks. Over the coming months, we are going to expand Terminal-Bench into comprehensive testbed for AI agents in text-based environments. Any contributions are welcome, especially new and challenging tasks!&lt;/p&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;Our &lt;a href="https://www.tbench.ai/docs/installation"&gt;Quickstart Guide&lt;/a&gt; will walk you through installing the repo and contributing.&lt;/p&gt; 
&lt;p&gt;Terminal-Bench is distributed as a pip package and can be run using the Terminal-Bench CLI: &lt;code&gt;tb&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv tool install terminal-bench
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install terminal-bench
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Further Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.tbench.ai/tasks"&gt;Task Gallery&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.tbench.ai/docs/task-ideas"&gt;Task Ideas&lt;/a&gt; - Browse community-sourced task ideas&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.tbench.ai/docs/dashboard"&gt;Dashboard Documentation&lt;/a&gt; - Information about the Terminal-Bench dashboard&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Core Components&lt;/h2&gt; 
&lt;h3&gt;Dataset of Tasks&lt;/h3&gt; 
&lt;p&gt;Each task in Terminal-Bench includes&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;a instruction in English,&lt;/li&gt; 
 &lt;li&gt;a test script to verify if the language model / agent completed the task successfully,&lt;/li&gt; 
 &lt;li&gt;a reference ("oracle") solution that solves the task.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Tasks are located in the &lt;a href="https://raw.githubusercontent.com/laude-institute/terminal-bench/main/tasks"&gt;&lt;code&gt;tasks&lt;/code&gt;&lt;/a&gt; folder of the repository, and the aforementioned list of current tasks gives an overview that is easy to browse.&lt;/p&gt; 
&lt;h3&gt;Execution Harness&lt;/h3&gt; 
&lt;p&gt;The harness connects language models to a sandboxed terminal environment. After &lt;a href="https://www.tbench.ai/docs/installation"&gt;installing the terminal-bench package&lt;/a&gt; (along with the dependencies &lt;code&gt;uv&lt;/code&gt; and &lt;code&gt;Docker&lt;/code&gt;) you can view how to run the harness using:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tb run --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For detailed information about running the harness and its options, see the &lt;a href="https://www.tbench.ai/docs/first-steps"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Submit to Our Leaderboard&lt;/h3&gt; 
&lt;p&gt;Terminal-Bench-Core v0.1.1 is the set of tasks for Terminal-Bench's beta release and corresponds to the current leaderboard. To evaluate on it pass &lt;code&gt;--dataset-name terminal-bench-core&lt;/code&gt; and &lt;code&gt;--dataset-version 0.1.1&lt;/code&gt; to the harness. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tb run \
    --agent terminus \
    --model-name anthropic/claude-3-7-latest \
    --dataset-name terminal-bench-core
    --dataset-version 0.1.1
    --n-concurrent 8
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more detailed instructions on submitting to the leaderboard, view our &lt;a href="https://www.tbench.ai/docs/submitting-to-leaderboard"&gt;leaderboard submission guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For more information on Terminal-Bench datasets and versioning view our &lt;a href="https://www.tbench.ai/docs/registry"&gt;registry overview&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Creating New Tasks&lt;/h2&gt; 
&lt;p&gt;View our &lt;a href="https://www.tbench.ai/docs/task-quickstart"&gt;task contribution quickstart&lt;/a&gt; to create a new task.&lt;/p&gt; 
&lt;h2&gt;Citing Us&lt;/h2&gt; 
&lt;p&gt;If you found Terminal-Bench useful, please cite us as:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{tbench_2025,
      title={Terminal-Bench: A Benchmark for AI Agents in Terminal Environments}, 
      url={https://github.com/laude-institute/terminal-bench}, 
      author={The Terminal-Bench Team}, year={2025}, month={Apr}} 
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>prowler-cloud/prowler</title>
      <link>https://github.com/prowler-cloud/prowler</link>
      <description>&lt;p&gt;Prowler is the Open Cloud Security platform for AWS, Azure, GCP, Kubernetes, M365 and more. It helps for continuous monitoring, security assessments &amp; audits, incident response, compliance, hardening and forensics readiness. Includes CIS, NIST 800, NIST CSF, CISA, FedRAMP, PCI-DSS, GDPR, HIPAA, FFIEC, SOC2, ENS and more&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img align="center" src="https://github.com/prowler-cloud/prowler/raw/master/docs/img/prowler-logo-black.png#gh-light-mode-only" width="50%" height="50%" /&gt; &lt;img align="center" src="https://github.com/prowler-cloud/prowler/raw/master/docs/img/prowler-logo-white.png#gh-dark-mode-only" width="50%" height="50%" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;b&gt;&lt;i&gt;Prowler&lt;/i&gt;&lt;/b&gt;&lt;i&gt; is the Open Cloud Security platform trusted by thousands to automate security and compliance in any cloud environment. With hundreds of ready-to-use checks and compliance frameworks, Prowler delivers real-time, customizable monitoring and seamless integrations, making cloud security simple, scalable, and cost-effective for organizations of any size. &lt;/i&gt;&lt;/p&gt;
&lt;i&gt; &lt;/i&gt;
&lt;p align="center"&gt;&lt;i&gt; &lt;b&gt;Learn more at &lt;a href="https://prowler.com"&gt;prowler.com&lt;/a&gt;&lt;/b&gt;&lt;/i&gt;&lt;a href="https://prowler.com"&gt; &lt;/a&gt;&lt;/p&gt;
&lt;a href="https://prowler.com"&gt; &lt;/a&gt;
&lt;p align="center"&gt;&lt;a href="https://prowler.com"&gt; &lt;/a&gt;&lt;a href="https://goto.prowler.com/slack"&gt;&lt;img width="30" height="30" alt="Prowler community on Slack" src="https://github.com/prowler-cloud/prowler/assets/38561120/3c8b4ec5-6849-41a5-b5e1-52bbb94af73a" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://goto.prowler.com/slack"&gt;Join our Prowler community!&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; &lt;a href="https://goto.prowler.com/slack"&gt;&lt;img alt="Slack Shield" src="https://img.shields.io/badge/slack-prowler-brightgreen.svg?logo=slack" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/prowler/"&gt;&lt;img alt="Python Version" src="https://img.shields.io/pypi/v/prowler.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/prowler/"&gt;&lt;img alt="Python Version" src="https://img.shields.io/pypi/pyversions/prowler.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://pypistats.org/packages/prowler"&gt;&lt;img alt="PyPI Prowler Downloads" src="https://img.shields.io/pypi/dw/prowler.svg?label=prowler%20downloads" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/toniblyx/prowler"&gt;&lt;img alt="Docker Pulls" src="https://img.shields.io/docker/pulls/toniblyx/prowler" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/toniblyx/prowler"&gt;&lt;img alt="Docker" src="https://img.shields.io/docker/cloud/build/toniblyx/prowler" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/toniblyx/prowler"&gt;&lt;img alt="Docker" src="https://img.shields.io/docker/image-size/toniblyx/prowler" /&gt;&lt;/a&gt; &lt;a href="https://gallery.ecr.aws/prowler-cloud/prowler"&gt;&lt;img width="120" height="19&amp;quot;" alt="AWS ECR Gallery" src="https://user-images.githubusercontent.com/3985464/151531396-b6535a68-c907-44eb-95a1-a09508178616.png" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/prowler-cloud/prowler"&gt;&lt;img src="https://codecov.io/gh/prowler-cloud/prowler/graph/badge.svg?token=OflBGsdpDl" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/prowler-cloud/prowler"&gt;&lt;img alt="Repo size" src="https://img.shields.io/github/repo-size/prowler-cloud/prowler" /&gt;&lt;/a&gt; &lt;a href="https://github.com/prowler-cloud/prowler/issues"&gt;&lt;img alt="Issues" src="https://img.shields.io/github/issues/prowler-cloud/prowler" /&gt;&lt;/a&gt; &lt;a href="https://github.com/prowler-cloud/prowler/releases"&gt;&lt;img alt="Version" src="https://img.shields.io/github/v/release/prowler-cloud/prowler" /&gt;&lt;/a&gt; &lt;a href="https://github.com/prowler-cloud/prowler/releases"&gt;&lt;img alt="Version" src="https://img.shields.io/github/release-date/prowler-cloud/prowler" /&gt;&lt;/a&gt; &lt;a href="https://github.com/prowler-cloud/prowler"&gt;&lt;img alt="Contributors" src="https://img.shields.io/github/contributors-anon/prowler-cloud/prowler" /&gt;&lt;/a&gt; &lt;a href="https://github.com/prowler-cloud/prowler"&gt;&lt;img alt="License" src="https://img.shields.io/github/license/prowler-cloud/prowler" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/ToniBlyx"&gt;&lt;img alt="Twitter" src="https://img.shields.io/twitter/follow/toniblyx?style=social" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/prowlercloud"&gt;&lt;img alt="Twitter" src="https://img.shields.io/twitter/follow/prowlercloud?style=social" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; &lt;img align="center" src="https://raw.githubusercontent.com/prowler-cloud/prowler/master/docs/img/prowler-cli-quick.gif" width="100%" height="100%" /&gt; &lt;/p&gt; 
&lt;h1&gt;Description&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Prowler&lt;/strong&gt; is an open-source security tool designed to assess and enforce security best practices across AWS, Azure, Google Cloud, and Kubernetes. It supports tasks such as security audits, incident response, continuous monitoring, system hardening, forensic readiness, and remediation processes.&lt;/p&gt; 
&lt;p&gt;Prowler includes hundreds of built-in controls to ensure compliance with standards and frameworks, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Industry Standards:&lt;/strong&gt; CIS, NIST 800, NIST CSF, and CISA&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Regulatory Compliance and Governance:&lt;/strong&gt; RBI, FedRAMP, and PCI-DSS&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Frameworks for Sensitive Data and Privacy:&lt;/strong&gt; GDPR, HIPAA, and FFIEC&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Frameworks for Organizational Governance and Quality Control:&lt;/strong&gt; SOC2 and GXP&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AWS-Specific Frameworks:&lt;/strong&gt; AWS Foundational Technical Review (FTR) and AWS Well-Architected Framework (Security Pillar)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;National Security Standards:&lt;/strong&gt; ENS (Spanish National Security Scheme)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Custom Security Frameworks:&lt;/strong&gt; Tailored to your needs&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Prowler CLI and Prowler Cloud&lt;/h2&gt; 
&lt;p&gt;Prowler offers a Command Line Interface (CLI), known as Prowler Open Source, and an additional service built on top of it, called &lt;a href="https://prowler.com"&gt;Prowler Cloud&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Prowler App&lt;/h2&gt; 
&lt;p&gt;Prowler App is a web-based application that simplifies running Prowler across your cloud provider accounts. It provides a user-friendly interface to visualize the results and streamline your security assessments.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/prowler-cloud/prowler/master/docs/img/overview.png" alt="Prowler App" /&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For more details, refer to the &lt;a href="https://docs.prowler.com/projects/prowler-open-source/en/latest/#prowler-app-installation"&gt;Prowler App Documentation&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Prowler CLI&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;prowler &amp;lt;provider&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/prowler-cloud/prowler/master/docs/img/short-display.png" alt="Prowler CLI Execution" /&gt;&lt;/p&gt; 
&lt;h2&gt;Prowler Dashboard&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;prowler dashboard
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/prowler-cloud/prowler/master/docs/img/dashboard.png" alt="Prowler Dashboard" /&gt;&lt;/p&gt; 
&lt;h1&gt;Prowler at a Glance&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Provider&lt;/th&gt; 
   &lt;th&gt;Checks&lt;/th&gt; 
   &lt;th&gt;Services&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://docs.prowler.com/projects/prowler-open-source/en/latest/tutorials/compliance/"&gt;Compliance Frameworks&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://docs.prowler.com/projects/prowler-open-source/en/latest/tutorials/misc/#categories"&gt;Categories&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AWS&lt;/td&gt; 
   &lt;td&gt;571&lt;/td&gt; 
   &lt;td&gt;82&lt;/td&gt; 
   &lt;td&gt;36&lt;/td&gt; 
   &lt;td&gt;10&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GCP&lt;/td&gt; 
   &lt;td&gt;79&lt;/td&gt; 
   &lt;td&gt;13&lt;/td&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Azure&lt;/td&gt; 
   &lt;td&gt;162&lt;/td&gt; 
   &lt;td&gt;19&lt;/td&gt; 
   &lt;td&gt;11&lt;/td&gt; 
   &lt;td&gt;4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Kubernetes&lt;/td&gt; 
   &lt;td&gt;83&lt;/td&gt; 
   &lt;td&gt;7&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;7&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GitHub&lt;/td&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;M365&lt;/td&gt; 
   &lt;td&gt;70&lt;/td&gt; 
   &lt;td&gt;7&lt;/td&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;NHN (Unofficial)&lt;/td&gt; 
   &lt;td&gt;6&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;0&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] The numbers in the table are updated periodically.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Tip] For the most accurate and up-to-date information about checks, services, frameworks, and categories, visit &lt;a href="https://hub.prowler.com"&gt;&lt;strong&gt;Prowler Hub&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] Use the following commands to list Prowler's available checks, services, compliance frameworks, and categories: &lt;code&gt;prowler &amp;lt;provider&amp;gt; --list-checks&lt;/code&gt;, &lt;code&gt;prowler &amp;lt;provider&amp;gt; --list-services&lt;/code&gt;, &lt;code&gt;prowler &amp;lt;provider&amp;gt; --list-compliance&lt;/code&gt; and &lt;code&gt;prowler &amp;lt;provider&amp;gt; --list-categories&lt;/code&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;üíª Installation&lt;/h1&gt; 
&lt;h2&gt;Prowler App&lt;/h2&gt; 
&lt;p&gt;Prowler App offers flexible installation methods tailored to various environments:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For detailed instructions on using Prowler App, refer to the &lt;a href="https://docs.prowler.com/projects/prowler-open-source/en/latest/tutorials/prowler-app/"&gt;Prowler App Usage Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Docker Compose&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;Docker Compose&lt;/code&gt; installed: &lt;a href="https://docs.docker.com/compose/install/"&gt;https://docs.docker.com/compose/install/&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Commands&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;curl -LO https://raw.githubusercontent.com/prowler-cloud/prowler/refs/heads/master/docker-compose.yml
curl -LO https://raw.githubusercontent.com/prowler-cloud/prowler/refs/heads/master/.env
docker compose up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Containers are built for &lt;code&gt;linux/amd64&lt;/code&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Configuring Your Workstation for Prowler App&lt;/h3&gt; 
&lt;p&gt;If your workstation's architecture is incompatible, you can resolve this by:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Setting the environment variable&lt;/strong&gt;: &lt;code&gt;DOCKER_DEFAULT_PLATFORM=linux/amd64&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Using the following flag in your Docker command&lt;/strong&gt;: &lt;code&gt;--platform linux/amd64&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Once configured, access the Prowler App at &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt;. Sign up using your email and password to get started.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Common Issues with Docker Pull Installation&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] If you want to use AWS role assumption (e.g., with the "Connect assuming IAM Role" option), you may need to mount your local &lt;code&gt;.aws&lt;/code&gt; directory into the container as a volume (e.g., &lt;code&gt;- "${HOME}/.aws:/home/prowler/.aws:ro"&lt;/code&gt;). There are several ways to configure credentials for Docker containers. See the &lt;a href="https://raw.githubusercontent.com/prowler-cloud/prowler/master/docs/troubleshooting.md"&gt;Troubleshooting&lt;/a&gt; section for more details and examples.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;You can find more information in the &lt;a href="https://raw.githubusercontent.com/prowler-cloud/prowler/master/docs/troubleshooting.md"&gt;Troubleshooting&lt;/a&gt; section.&lt;/p&gt; 
&lt;h3&gt;From GitHub&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;git&lt;/code&gt; installed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;poetry&lt;/code&gt; v2 installed: &lt;a href="https://python-poetry.org/docs/#installation"&gt;poetry installation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;npm&lt;/code&gt; installed: &lt;a href="https://docs.npmjs.com/downloading-and-installing-node-js-and-npm"&gt;npm installation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Docker Compose&lt;/code&gt; installed: &lt;a href="https://docs.docker.com/compose/install/"&gt;https://docs.docker.com/compose/install/&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Commands to run the API&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;git clone https://github.com/prowler-cloud/prowler
cd prowler/api
poetry install
eval $(poetry env activate)
set -a
source .env
docker compose up postgres valkey -d
cd src/backend
python manage.py migrate --database admin
gunicorn -c config/guniconf.py config.wsgi:application
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] As of Poetry v2.0.0, the &lt;code&gt;poetry shell&lt;/code&gt; command has been deprecated. Use &lt;code&gt;poetry env activate&lt;/code&gt; instead for environment activation.&lt;/p&gt; 
 &lt;p&gt;If your Poetry version is below v2.0.0, continue using &lt;code&gt;poetry shell&lt;/code&gt; to activate your environment. For further guidance, refer to the Poetry Environment Activation Guide &lt;a href="https://python-poetry.org/docs/managing-environments/#activating-the-environment"&gt;https://python-poetry.org/docs/managing-environments/#activating-the-environment&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;After completing the setup, access the API documentation at &lt;a href="http://localhost:8080/api/v1/docs"&gt;http://localhost:8080/api/v1/docs&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Commands to run the API Worker&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;git clone https://github.com/prowler-cloud/prowler
cd prowler/api
poetry install
eval $(poetry env activate)
set -a
source .env
cd src/backend
python -m celery -A config.celery worker -l info -E
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Commands to run the API Scheduler&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;git clone https://github.com/prowler-cloud/prowler
cd prowler/api
poetry install
eval $(poetry env activate)
set -a
source .env
cd src/backend
python -m celery -A config.celery beat -l info --scheduler django_celery_beat.schedulers:DatabaseScheduler
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Commands to run the UI&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;git clone https://github.com/prowler-cloud/prowler
cd prowler/ui
npm install
npm run build
npm start
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Once configured, access the Prowler App at &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt;. Sign up using your email and password to get started.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Prowler CLI&lt;/h2&gt; 
&lt;h3&gt;Pip package&lt;/h3&gt; 
&lt;p&gt;Prowler CLI is available as a project in &lt;a href="https://pypi.org/project/prowler-cloud/"&gt;PyPI&lt;/a&gt;. Consequently, it can be installed using pip with Python &amp;gt;3.9.1, &amp;lt;3.13:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;pip install prowler
prowler -v
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For further guidance, refer to &lt;a href="https://docs.prowler.com/projects/prowler-open-source/en/latest/#prowler-cli-installation"&gt;https://docs.prowler.com&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Containers&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Available Versions of Prowler CLI&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The following versions of Prowler CLI are available, depending on your requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;latest&lt;/code&gt;: Synchronizes with the &lt;code&gt;master&lt;/code&gt; branch. Note that this version is not stable.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;v4-latest&lt;/code&gt;: Synchronizes with the &lt;code&gt;v4&lt;/code&gt; branch. Note that this version is not stable.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;v3-latest&lt;/code&gt;: Synchronizes with the &lt;code&gt;v3&lt;/code&gt; branch. Note that this version is not stable.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;x.y.z&amp;gt;&lt;/code&gt; (release): Stable releases corresponding to specific versions. You can find the complete list of releases &lt;a href="https://github.com/prowler-cloud/prowler/releases"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;stable&lt;/code&gt;: Always points to the latest release.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;v4-stable&lt;/code&gt;: Always points to the latest release for v4.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;v3-stable&lt;/code&gt;: Always points to the latest release for v3.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The container images are available here:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Prowler CLI: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://hub.docker.com/r/toniblyx/prowler/tags"&gt;DockerHub&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://gallery.ecr.aws/prowler-cloud/prowler"&gt;AWS Public ECR&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Prowler App: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://hub.docker.com/r/prowlercloud/prowler-ui/tags"&gt;DockerHub - Prowler UI&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://hub.docker.com/r/prowlercloud/prowler-api/tags"&gt;DockerHub - Prowler API&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;From GitHub&lt;/h3&gt; 
&lt;p&gt;Python &amp;gt;3.9.1, &amp;lt;3.13 is required with pip and Poetry:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;git clone https://github.com/prowler-cloud/prowler
cd prowler
eval $(poetry env activate)
poetry install
python prowler-cli.py -v
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] To clone Prowler on Windows, configure Git to support long file paths by running the following command: &lt;code&gt;git config core.longpaths true&lt;/code&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] As of Poetry v2.0.0, the &lt;code&gt;poetry shell&lt;/code&gt; command has been deprecated. Use &lt;code&gt;poetry env activate&lt;/code&gt; instead for environment activation.&lt;/p&gt; 
 &lt;p&gt;If your Poetry version is below v2.0.0, continue using &lt;code&gt;poetry shell&lt;/code&gt; to activate your environment. For further guidance, refer to the Poetry Environment Activation Guide &lt;a href="https://python-poetry.org/docs/managing-environments/#activating-the-environment"&gt;https://python-poetry.org/docs/managing-environments/#activating-the-environment&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;‚úèÔ∏è High level architecture&lt;/h1&gt; 
&lt;h2&gt;Prowler App&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Prowler App&lt;/strong&gt; is composed of three key components:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Prowler UI&lt;/strong&gt;: A web-based interface, built with Next.js, providing a user-friendly experience for executing Prowler scans and visualizing results.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Prowler API&lt;/strong&gt;: A backend service, developed with Django REST Framework, responsible for running Prowler scans and storing the generated results.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Prowler SDK&lt;/strong&gt;: A Python SDK designed to extend the functionality of the Prowler CLI for advanced capabilities.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/prowler-cloud/prowler/master/docs/img/prowler-app-architecture.png" alt="Prowler App Architecture" /&gt;&lt;/p&gt; 
&lt;h2&gt;Prowler CLI&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Running Prowler&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Prowler can be executed across various environments, offering flexibility to meet your needs. It can be run from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Your own workstation&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;A Kubernetes Job&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Google Compute Engine&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Azure Virtual Machines (VMs)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Amazon EC2 instances&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;AWS Fargate or other container platforms&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;CloudShell&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;And many more environments.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/prowler-cloud/prowler/master/docs/img/architecture.png" alt="Architecture" /&gt;&lt;/p&gt; 
&lt;h1&gt;Deprecations from v3&lt;/h1&gt; 
&lt;h2&gt;General&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;Allowlist&lt;/code&gt; now is called &lt;code&gt;Mutelist&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;The &lt;code&gt;--quiet&lt;/code&gt; option has been deprecated. Use the &lt;code&gt;--status&lt;/code&gt; flag to filter findings based on their status: PASS, FAIL, or MANUAL.&lt;/li&gt; 
 &lt;li&gt;All findings with an &lt;code&gt;INFO&lt;/code&gt; status have been reclassified as &lt;code&gt;MANUAL&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;The CSV output format is standardized across all providers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Deprecated Output Formats&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The following formats are now deprecated:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Native JSON has been replaced with JSON in [OCSF] v1.1.0 format, which is standardized across all providers (&lt;a href="https://schema.ocsf.io/"&gt;https://schema.ocsf.io/&lt;/a&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;AWS&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;AWS Flag Deprecation&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The flag --sts-endpoint-region has been deprecated due to the adoption of AWS STS regional tokens.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Sending FAIL Results to AWS Security Hub&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;To send only FAILS to AWS Security Hub, use one of the following options: &lt;code&gt;--send-sh-only-fails&lt;/code&gt; or &lt;code&gt;--security-hub --status FAIL&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;üìñ Documentation&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Documentation Resources&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;For installation instructions, usage details, tutorials, and the Developer Guide, visit &lt;a href="https://docs.prowler.com/"&gt;https://docs.prowler.com/&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;üìÉ License&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Prowler License Information&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Prowler is licensed under the Apache License 2.0, as indicated in each file within the repository. Obtaining a Copy of the License&lt;/p&gt; 
&lt;p&gt;A copy of the License is available at &lt;a href="http://www.apache.org/licenses/LICENSE-2.0"&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>HKUDS/AI-Researcher</title>
      <link>https://github.com/HKUDS/AI-Researcher</link>
      <description>&lt;p&gt;"AI-Researcher: Autonomous Scientific Innovation"&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/assets/ai-researcher.png" alt="Logo" width="400" /&gt; 
 &lt;h1 align="center"&gt;"AI-Researcher: Autonomous Scientific Innovation" &lt;/h1&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/14638" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14638" alt="HKUDS%2FAI-Researcher | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://autoresearcher.github.io"&gt;&lt;img src="https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&amp;amp;color=FFE165&amp;amp;logo=homepage&amp;amp;logoColor=white" alt="Project Page" /&gt;&lt;/a&gt; 
 &lt;a href="https://join.slack.com/t/ai-researchergroup/shared_invite/zt-30y5a070k-C0ajQt1zmVczFnfGkIicvA"&gt;&lt;img src="https://img.shields.io/badge/Slack-Join%20Us-red?logo=slack&amp;amp;logoColor=white&amp;amp;style=for-the-badge" alt="Join our Slack community" /&gt;&lt;/a&gt; 
 &lt;a href="https://discord.gg/zBNYTk5q2g"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20Us-purple?logo=discord&amp;amp;logoColor=white&amp;amp;style=for-the-badge" alt="Join our Discord community" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://autoresearcher.github.io/docs"&gt;&lt;img src="https://img.shields.io/badge/Documentation-000?logo=googledocs&amp;amp;logoColor=FFE165&amp;amp;style=for-the-badge" alt="Check out the documentation" /&gt;&lt;/a&gt; 
 &lt;a href="https://arxiv.org/abs/2505.18705"&gt;&lt;img src="https://img.shields.io/badge/Paper%20on%20Arxiv-000?logoColor=FFE165&amp;amp;logo=arxiv&amp;amp;style=for-the-badge" alt="Paper" /&gt;&lt;/a&gt; 
 &lt;a href="https://autoresearcher.github.io/leaderboard"&gt;&lt;img src="https://img.shields.io/badge/DATASETS-000?logoColor=FFE165&amp;amp;logo=huggingface&amp;amp;style=for-the-badge" alt="Benchmark" /&gt;&lt;/a&gt; 
 &lt;hr /&gt; 
&lt;/div&gt; 
&lt;p&gt;Welcome to &lt;strong&gt;AI-Researcher&lt;/strong&gt;ü§ó AI-Researcher introduces a revolutionary breakthrough in &lt;strong&gt;Automated Scientific Discovery&lt;/strong&gt;üî¨, presenting a new system that fundamentally &lt;strong&gt;Reshapes the Traditional Research Paradigm&lt;/strong&gt;. This state-of-the-art platform empowers researchers with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üéØ &lt;strong&gt;Full Autonomy&lt;/strong&gt;: Complete end-to-end research automation&lt;/li&gt; 
 &lt;li&gt;üîÑ &lt;strong&gt;Seamless Orchestration&lt;/strong&gt;: From concept to publication&lt;/li&gt; 
 &lt;li&gt;üß† &lt;strong&gt;Advanced AI Integration&lt;/strong&gt;: Powered by cutting-edge AI agents&lt;/li&gt; 
 &lt;li&gt;üöÄ &lt;strong&gt;Research Acceleration&lt;/strong&gt;: Streamlined scientific innovation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;‚ú® The AI-Researcher system accepts user input queries at two distinct levels ‚ú®&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Level 1: Detailed Idea Description&lt;/strong&gt; &lt;br /&gt; At this level, users provide comprehensive descriptions of their specific research ideas. The system processes these detailed inputs to develop implementation strategies based on the user's explicit requirements.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Level 2: Reference-Based Ideation&lt;/strong&gt; &lt;br /&gt; This simpler level involves users submitting reference papers without a specific idea in mind. The user query typically follows the format: "I have some reference papers, please come up with an innovative idea and implement it with these papers." The system then analyzes the provided references to generate and develop novel research concepts.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;üåü&lt;strong&gt;Core Capabilities &amp;amp; Integration&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;AI-Researcher&lt;/strong&gt; delivers a &lt;strong&gt;Comprehensive Research Ecosystem&lt;/strong&gt; through seamless integration of critical components:&lt;/p&gt; 
&lt;p&gt;üöÄ&lt;strong&gt;Primary Research Functions&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìö &lt;strong&gt;Literature Review&lt;/strong&gt;: Conducts comprehensive analysis and synthesis of existing research.&lt;/li&gt; 
 &lt;li&gt;üìä &lt;strong&gt;Idea Generation&lt;/strong&gt;: Systematically gathers, organizes, and formulates novel research directions.&lt;/li&gt; 
 &lt;li&gt;üß™ &lt;strong&gt;Algorithm Design and Implementation&lt;/strong&gt;: Develops methodologies and transforms ideas into functional implementations.&lt;/li&gt; 
 &lt;li&gt;üíª &lt;strong&gt;Algorithm Validation and Refinement&lt;/strong&gt;: Automates testing, performance evaluation, and iterative optimization.&lt;/li&gt; 
 &lt;li&gt;üìà &lt;strong&gt;Result Analysis&lt;/strong&gt;: Delivers advanced interpretation of experimental data and insights.&lt;/li&gt; 
 &lt;li&gt;‚úçÔ∏è &lt;strong&gt;Manuscript Creation&lt;/strong&gt;: Automatically generates polished, full-length academic papers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;!-- &lt;img src="./assets/AI-Researchernew-intro.pdf" alt="Logo" width="100%"&gt; --&gt; 
 &lt;figure&gt; 
  &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/assets/AI-Researcher-Framework.png" alt="Logo" style="max-width: 100%; height: auto;" /&gt; 
  &lt;br /&gt; 
  &lt;figcaption&gt;
   &lt;em&gt;Quick Overview of AI-Researcher.&lt;/em&gt;
  &lt;/figcaption&gt; 
 &lt;/figure&gt; 
&lt;/div&gt; 
&lt;span id="news"&gt;&lt;/span&gt; 
&lt;h2&gt;üî• News&lt;/h2&gt; 
&lt;div class="scrollable"&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;[2025, May 24]&lt;/strong&gt;: &amp;nbsp;üéâüéâ &lt;b&gt;Major Release! AI-Researcher Comprehensive Upgrade!&lt;/b&gt; üöÄ &lt;br /&gt;We are excited to announce a significant milestone for AI-Researcher: 
   &lt;ul&gt; 
    &lt;li&gt;üìÑ &lt;b&gt;&lt;a href="https://arxiv.org/abs/2505.18705"&gt;Academic Paper Release&lt;/a&gt;&lt;/b&gt;: Detailed exposition of our innovative methods and experimental results&lt;/li&gt; 
    &lt;li&gt;üìä &lt;b&gt;&lt;a href="https://autoresearcher.github.io/leaderboard"&gt;Benchmark Suite&lt;/a&gt;&lt;/b&gt;: Comprehensive evaluation framework and datasets&lt;/li&gt; 
    &lt;li&gt;üñ•Ô∏è &lt;b&gt;Web GUI Interface&lt;/b&gt;: User-friendly graphical interface making research more convenient&lt;/li&gt; 
   &lt;/ul&gt; &lt;b&gt;ü§ù Join Us!&lt;/b&gt; We welcome researchers, developers, and AI enthusiasts to contribute together and advance AI research development. Whether it's code contributions, bug reports, feature suggestions, or documentation improvements, every contribution is valuable! &lt;br /&gt;üí° &lt;i&gt;Let's build a smarter AI research assistant together!&lt;/i&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2025, Mar 04]&lt;/strong&gt;: &amp;nbsp;üéâüéâWe've launched &lt;b&gt;AI-Researcher!&lt;/b&gt;, The release includes the complete framework, datasets, benchmark construction pipeline, and much more. Stay tuned‚Äîthere's plenty more to come! üöÄ&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;span id="table-of-contents"&gt;&lt;/span&gt; 
&lt;h2&gt;üìë Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#news"&gt;üî• News&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#quick-start"&gt;‚ö° Quick Start&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#api-keys-setup"&gt;API Keys Setup&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#examples"&gt;‚¨áÔ∏è Examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#how-it-works"&gt;‚ú® How AI-Researcher works&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#how-to-use"&gt;üîç How to use AI-Researcher&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#documentation"&gt;üìñ Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#community"&gt;ü§ù Join the Community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#acknowledgements"&gt;üôè Acknowledgements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#cite"&gt;üåü Cite&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="quick-start"&gt;&lt;/span&gt; 
&lt;h2&gt;‚ö° Quick Start&lt;/h2&gt; 
&lt;span id="installation"&gt;&lt;/span&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;h4&gt;AI Installation&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Using &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;We recommend to use &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt; to manage packages in our project (Much more faster than conda)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# install uv
curl -LsSf https://astral.sh/uv/install.sh | sh
source ~/.bashrc

# clone the project
git clone https://github.com/HKUDS/AI-Researcher.git
cd AI-Researcher

# install and activate enviroment
uv venv --python 3.11
source ./.venv/bin/activate
uv pip install -e .
playwright install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Docker Installation&lt;/h4&gt; 
&lt;p&gt;To set up the agent-interactive environment, we use Docker for containerization. Please ensure you have &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt; installed on your system before proceeding. For running the research agent, we utilize the Docker image 'tjbtech1/airesearcher:v1t'. You can pull this image by executing the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker pull tjbtech1/airesearcher:v1
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or you can build the docker image from our provided &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/docker/Dockerfile"&gt;Dockerfile&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd ./docker &amp;amp;&amp;amp; docker build -t tjbtech1/airesearcher:v1 .
&lt;/code&gt;&lt;/pre&gt; 
&lt;span id="api-keys-setup"&gt;&lt;/span&gt; 
&lt;h3&gt;API Keys Setup&lt;/h3&gt; 
&lt;p&gt;Create an environment variable file based on the provided '.env.template' file. In this file, you should set the configuration including api key, instance id of the test case.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;
# ================ container configuration ================
# workplace of the research agent
DOCKER_WORKPLACE_NAME=workplace_paper
# base image of the research agent
BASE_IMAGES=tjbtech1/airesearcher:v1
# completion model name, configuration details see: https://docs.litellm.ai/docs/
COMPLETION_MODEL=openrouter/google/gemini-2.5-pro-preview-05-20
# cheep model name, configuration details see: https://docs.litellm.ai/docs/
CHEEP_MODEL=openrouter/google/gemini-2.5-pro-preview-05-20
# specific gpu of the research agent, can be: 
# '"device=0"' using the first gpu
# '"device=0,1"' using the first and second gpu
# '"all"' using all gpus
# None for no gpu
GPUS='"device=0"'
# name of the container
CONTAINER_NAME=paper_eval
# name of the workplace
WORKPLACE_NAME=workplace
# path of the cache
CACHE_PATH=cache
# port of the research agent
PORT=7020
# platform of the research agent
PLATFORM=linux/amd64

# ================ llm configuration ================
# github ai token of the research agent
GITHUB_AI_TOKEN=your_github_ai_token
# openrouter api key of the research agent
OPENROUTER_API_KEY=your_openrouter_api_key
# openrouter api base url of the research agent
OPENROUTER_API_BASE=https://openrouter.ai/api/v1

# ================ task configuration ================
# category of the research agent, based on: ./benchmark/final. Can be: 
# diffu_flow
# gnn
# reasoning
# recommendation
# vq
# example: ./benchmark/final/vq
CATEGORY=vq
# instance id of the research agent, example: ./benchmark/final/vq/one_layer_vq.json
INSTANCE_ID=one_layer_vq
# task level of the research agent, can be: 
# task1
# task2
TASK_LEVEL=task1
# maximum iteration times of the research agent
MAX_ITER_TIMES=0
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üî• Web GUI&lt;/h3&gt; 
&lt;p&gt;We add a webgui based on gradio. Just run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python web_ai_researcher.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/assets/webgui/image-20250606135137558.png" alt="image-20250606135137558" /&gt;&lt;/p&gt; 
&lt;p&gt;You can configure the environment variables in the following tab:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/assets/webgui/image-20250606135325373.png" alt="image-20250606135325373" /&gt;&lt;/p&gt; 
&lt;p&gt;Select the following example to run our AI-Researcher:&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/assets/webgui/image-20250606135507970.png" alt="image-20250606135507970" style="zoom:67%;" /&gt; 
&lt;span id="examples"&gt;&lt;/span&gt; 
&lt;h2&gt;‚¨áÔ∏è Examples&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;ALERT&lt;/strong&gt;: The GIFs below are large files and may &lt;strong&gt;take some time to load&lt;/strong&gt;. &lt;strong&gt;Please be patient while they render completely&lt;/strong&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Example 1 (Vector Quantized)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed model designed in this paper is designed to improve the performance of Vector Quantized Variational AutoEncoders (VQ-VAEs) by addressing issues with gradient propagation through the non-differentiable vector quantization layer.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;The core methodologies utilized include: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Rotation and Rescaling Transformation&lt;/strong&gt;: A linear transformation that alters the encoder output to align it with the nearest codebook vector without changing the forward pass output.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Gradient Propagation Method&lt;/strong&gt;: The proposed model ensures that gradients flow from the decoder to the encoder while preserving the angle between the gradient and codebook vector.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Codebook Management&lt;/strong&gt;: Utilizes the connection between the encoder output and the corresponding codebook vectors to mitigate codebook collapse and improve utilization.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;The primary functions of these components are: 
    &lt;ul&gt; 
     &lt;li&gt;The rotation and rescaling transformation modifies how the encoder output is quantized and how information is retained during backpropagation, enabling gradients to reflect the true positioning of the encoder output relative to the codebook vectors.&lt;/li&gt; 
     &lt;li&gt;The gradient propagation method redefines how gradients are transported back to the encoder, allowing for an enhanced and nuanced movement through the quantization layer, which leads to a better performance during training.&lt;/li&gt; 
     &lt;li&gt;Codebook management practices help in maintaining a diverse set of codebook vectors throughout training, avoiding scenarios where multiple vectors become redundant or unused.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Implementation details for each component: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Key Parameters&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Codebook size should be configured based on the complexity of the dataset (e.g., 1024 or 8192).&lt;/li&gt; 
       &lt;li&gt;Commitment loss coefficient (Œ≤) is typically set within [0.25, 2].&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Input/Output Specifications&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Input to the encoder is a continuous high-dimensional vector, while the output is a corresponding quantized vector from the codebook.&lt;/li&gt; 
       &lt;li&gt;The output for reconstruction is generated using the decoder applied to the transformed codebook vectors.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Important Constraints&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Ensure that the codebook is updated correctly with an exponential moving average procedure, and treat both rotation and rescaling during the forward pass as constants with respect to the gradient.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Step-by-Step Integration of Components: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: Input the data vector into the encoder to obtain the continuous representation.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: Identify the nearest codebook vector to the encoder output.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 3&lt;/strong&gt;: Compute the rotation matrix that aligns the encoder output to the codebook vector.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 4&lt;/strong&gt;: Apply the rotation and rescaling transformation to obtain the modified output for the decoder (i.e., `Àú q`).&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 5&lt;/strong&gt;: Feed `Àú q` into the decoder to produce the reconstructed output.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 6&lt;/strong&gt;: Compute the loss using the reconstruction and apply backpropagation.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 7&lt;/strong&gt;: During backpropagation, modify the gradient transfer process to maintain the angle using the proposed model, replacing traditional shortcuts in gradient computation.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Critical implementation details affecting performance: 
    &lt;ul&gt; 
     &lt;li&gt;The choice of rotation matrix calculation should ensure computational efficiency‚Äîusing Householder transformations to minimize resource demands.&lt;/li&gt; 
     &lt;li&gt;The deployment of the stop-gradient technique effectively turns off the back-propagation through the quantization layer, which is essential to reflect the intended change without inducing undesired noise in the gradient updates.&lt;/li&gt; 
     &lt;li&gt;Monitor the codebook usage regularly during training to detect any potential collapse early and adjust the training dynamics (e.g., learning rate) accordingly to maintain effective utilization throughout the training period.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Neural discrete representation learning&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;Neural discrete representation learning&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Straightening out the straight-through estimator: Overcoming optimization challenges in vector quantized networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Estimating or propagating gradients through stochastic neurons for conditional computation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;High-resolution image synthesis with latent diffusion models&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Finite scalar quantization: Vq-vae made simple&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Elements of information theory&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Vector-quantized image modeling with improved vqgan&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Uvim: A unified modeling approach for vision with learned guiding codes&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Auto-encoding variational bayes&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Categorical reparameterization with gumbel-softmax&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/rotation_vq/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/rotation_vq/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/rotation_vq/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/rotation_vq/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Example 2 (Category: Vector Quantized)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed model focuses on discrete representation learning for tasks such as image generation, depth estimation, colorization, and segmentation using the proposed approach integrated into architectures like autoregressive transformers.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;Core Techniques: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Simplified Quantization&lt;/strong&gt;: Use a simplified quantization approach utilizing scalar quantization instead of VQ.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Dimensionality Projection&lt;/strong&gt;: Define a function to project the encoder output to a manageable dimensionality (typically between 3 to 10).&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Gradient Propagation&lt;/strong&gt;: Implement the Straight-Through Estimator (STE) for gradient propagation through the quantization operation.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Technical Components: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Bounding Function&lt;/strong&gt;: This compresses data dimensionality and confines values to a desired range. Use a function like \(f(z) = \left\lfloor \frac{L}{2} \right\rfloor \tanh(z)\) to project the data, where \(L\) is the number of quantization levels.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Quantization process&lt;/strong&gt;: Round each bounded dimension to its nearest integer to yield the quantized output.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Loss function&lt;/strong&gt;: Operate under a reconstruction loss paradigm typical in VAEs to optimize the proposed model parameters.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Key Parameters&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Number of dimensions \(d\) and levels \(L\) per dimension should be defined based on the codebook size you aim to replicate (e.g., set \(L_i \geq 5\) for all \(i\)).&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Input/Output Specifications&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;The input to the bounding function will be the output from the final encoder layer; the output after quantization will be in the format \(\hat{z}\), with shape matching the original \(z\).&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Constraints&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Ensure all inputs are preprocessed adequately to be within the functioning range of the bounding function.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Step-by-Step Integration: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: Train a standard VAE model and obtain its encoder output \(z\).&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: Apply the bounding function \(f\) on \(z\) to limit the output dimensions to usable values.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 3&lt;/strong&gt;: Quantize the resultant bounded \(z\) using the rounding procedure to generate \( \hat{z} \).&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 4&lt;/strong&gt;: Use the original \(z\) and \(\hat{z}\) in conjunction with the reconstruction loss to backpropagate through the network using the STE for gradient calculation.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Critical Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;Ensure the rounding process is correctly differentiable; utilize the STE to maintain gradient flow during backpropagation.&lt;/li&gt; 
     &lt;li&gt;Maintain high codebook utilization by selecting optimal dimensions and levels based on empirical trials, and monitor performance to refine the parameters if needed.&lt;/li&gt; 
     &lt;li&gt;Adjust the proposed model configurations (number of epochs, batch size) based on the structures laid out in this paper, ensuring hyperparameters match those recommended for the proposed approach integration.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Neural discrete representation learning&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;Neural discrete representation learning&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Conditional probability models for deep image compression&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;High-fidelity generative image compression&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;End-to-end optimized image compression&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Taming transformers for high-resolution image generation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;An algorithm for vector quantizer design&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Joint autoregressive and hierarchical priors for learned image compression&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Assessing generative models via precision and recall&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Variational bayes on discrete representation with self-annealed stochastic quantization&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;High quality monocular depth estimation via transfer learning&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/fsq/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/fsq/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/fsq/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/fsq/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Example 3 (Category: Recommendation)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed model aims to improve user-item interaction predictions in recommendation systems by leveraging heterogeneous relational information.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;Core Techniques/Algorithms: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Heterogeneous Graph Neural Networks (GNNs)&lt;/strong&gt;: Used for embedding initialization and message propagation across different types of user-item and user-user/item-item graphs.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Contrastive Learning&lt;/strong&gt;: Specifically, a cross-view contrastive learning framework is utilized to enhance representation learning by aligning embeddings from auxiliary views with user-item interaction embeddings.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Meta Networks&lt;/strong&gt;: Employed to extract personalized knowledge and facilitate customized knowledge transfer between auxiliary views and the user-item interaction view.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Purpose and Function of Each Major Component: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Heterogeneous GNN&lt;/strong&gt;: Encodes user and item relationships into embeddings that capture the semantics of various interactions.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Contrastive Learning&lt;/strong&gt;: Provides self-supervision signals to enhance the robustness of learned representations, allowing the proposed model to distinguish between relevant and irrelevant interactions.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Meta Network&lt;/strong&gt;: Models personalized characteristics to facilitate adaptive knowledge transfer, ensuring that the influence of auxiliary information is tailored to individual users and items.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Heterogeneous GNN&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;&lt;strong&gt;Key Parameters&lt;/strong&gt;: Use Xavier initializer for embedding initialization; set the hidden dimensionality &lt;code&gt;d&lt;/code&gt;.&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Input/Output&lt;/strong&gt;: Take adjacency matrices for user-item, user-user, and item-item graphs as input; output relation-aware embeddings.&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Constraints&lt;/strong&gt;: Ensure that the GNN can handle varying types of nodes and relations.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Contrastive Learning&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;&lt;strong&gt;Key Parameters&lt;/strong&gt;: Use cosine similarity as the similarity function; define a temperature coefficient for handling negative samples.&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Input/Output&lt;/strong&gt;: Input embeddings from the meta network and user/item views; output contrastive loss values.&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Constraints&lt;/strong&gt;: Maintain diverse representations to avoid overfitting.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Meta Network&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;&lt;strong&gt;Key Parameters&lt;/strong&gt;: Set up fully connected layers with PReLU activation to generate personalized transformation matrices.&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Input/Output&lt;/strong&gt;: Input user and item embeddings; output transformed embeddings for personalized knowledge transfer.&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Constraints&lt;/strong&gt;: Ensure low-rank decomposition of transformation matrices to reduce parameter count.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Step-by-Step Interaction: 
    &lt;ul&gt; 
     &lt;li&gt;Initialize user and item embeddings using a heterogeneous GNN.&lt;/li&gt; 
     &lt;li&gt;Perform heterogeneous message propagation to refine embeddings iteratively across user-item, user-user, and item-item graphs.&lt;/li&gt; 
     &lt;li&gt;Aggregate the refined embeddings from various views using a mean pooling function to retain heterogeneous semantics.&lt;/li&gt; 
     &lt;li&gt;Extract meta knowledge from the learned embeddings to create personalized mapping functions using the meta network.&lt;/li&gt; 
     &lt;li&gt;Apply contrastive learning to align embeddings from auxiliary views with the user-item interaction embeddings, generating a contrastive loss.&lt;/li&gt; 
     &lt;li&gt;Combine the contrastive loss with a pairwise loss function (like Bayesian Personalized Ranking) to optimize the proposed model.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Critical Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;Choose appropriate hyperparameters such as embedding size, learning rate, and the number of GNN layers through systematic experimentation.&lt;/li&gt; 
     &lt;li&gt;Monitor the proposed model for signs of overfitting, especially when increasing the number of GNN layers or embedding dimensions.&lt;/li&gt; 
     &lt;li&gt;Ensure diverse user-item interaction patterns are captured through sufficient training data and effective augmentation techniques.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Revisiting Graph Based Collaborative Filtering: A Linear Residual Graph Convolutional Network Approach&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;Revisiting Graph Based Collaborative Filtering: A Linear Residual Graph Convolutional Network Approach&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Graph Neural Networks for Social Recommendation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Improving Graph Collaborative Filtering with Neighborhood-enriched Contrastive Learning&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Knowledge-aware Coupled Graph Neural Network for Social Recommendation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Heterogeneous Graph Transformer&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Sequential Recommendation with Graph Neural Networks&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/hgcl/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/hgcl/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/hgcl/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/hgcl/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Example 4 (Category: Recommendation)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed model focuses on collaborative filtering for recommendation systems by leveraging graph neural networks (GNNs) and contrastive learning to address the issue of sparse user-item interactions.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;Core Techniques: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Graph Neural Networks&lt;/strong&gt;: Utilize GNNs for message passing to learn user and item embeddings from the interaction graph.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Disentangled Representations&lt;/strong&gt;: Implement a mechanism to model multiple latent intent factors driving user-item interactions.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Contrastive Learning&lt;/strong&gt;: Use contrastive learning techniques to generate adaptive self-supervised signals from augmented views of user-item interactions.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Purpose of Components: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;GNN Layers&lt;/strong&gt;: Capture high-order interactions among users and items through iterative message passing.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Intent Encoding&lt;/strong&gt;: Differentiate latent intents to improve the representation of user preferences.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Adaptive Augmentation&lt;/strong&gt;: Generate contrastive views that account for both local and global dependencies to enhance robustness against noise.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Graph Construction&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;&lt;strong&gt;Input&lt;/strong&gt;: User-item interaction matrix \( A \) of size \( I \times J \) (where \( I \) is the number of users and \( J \) is the number of items).&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: Normalized adjacency matrix \( \bar{A} \).&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;GNN Configuration&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Number of layers \( L \): Choose based on your dataset, typically 2 or 3 layers.&lt;/li&gt; 
       &lt;li&gt;Dimensionality \( d \) of embeddings: Start with \( d = 32 \).&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Intent Prototypes&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Number of intents \( K \): Experiment with values from {32, 64, 128, 256}, starting with \( K = 128 \).&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Learning Rate&lt;/strong&gt;: Use Adam optimizer with a learning rate around \( 1e-3 \).&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Loss Functions&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Use Bayesian Personalized Ranking (BPR) loss for the recommendation task.&lt;/li&gt; 
       &lt;li&gt;Implement InfoNCE loss for contrastive learning, incorporating both local and global augmented views.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Step-by-Step Interaction: 
    &lt;ul&gt; 
     &lt;li&gt;Construct the interaction graph from the user-item matrix.&lt;/li&gt; 
     &lt;li&gt;For each GNN layer: 
      &lt;ul&gt; 
       &lt;li&gt;Compute the aggregated embeddings \( Z(u) \) and \( Z(v) \) using the normalized adjacency matrix.&lt;/li&gt; 
       &lt;li&gt;Update user and item embeddings using residual connections to prevent over-smoothing.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;Generate intent-aware representations by aggregating embeddings over the latent intents.&lt;/li&gt; 
     &lt;li&gt;Apply the learned parameterized masks for adaptive augmentation during message passing to create multiple contrastive views.&lt;/li&gt; 
     &lt;li&gt;Calculate contrastive learning signals using the generated augmented representations and optimize using the combined loss function.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Critical Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;Ensure that the augmentation matrices are learned adaptively based on the current user-item embeddings to differentiate the importance of interactions.&lt;/li&gt; 
     &lt;li&gt;Monitor the performance with different numbers of latent intents \( K \) to find an optimal balance between expressiveness and noise.&lt;/li&gt; 
     &lt;li&gt;Regularly assess the proposed model for over-smoothing by checking the Mean Average Distance (MAD) metric on the embeddings.&lt;/li&gt; 
     &lt;li&gt;Tune hyperparameters \( \lambda_1, \lambda_2, \lambda_3 \) for the multi-task loss to balance the contribution of the self-supervised learning signals.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Lightgcn: Simplifying and powering graph convolution network for recommendation&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;Lightgcn: Simplifying and powering graph convolution network for recommendation&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Neural collaborative filtering&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Disentangled contrastive learning on graphs&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Improving Graph Collaborative Filtering with Neighborhood-enriched Contrastive Learning&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Curriculum Disentangled Recommendation with Noisy Multi-feedback&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Disentangled heterogeneous graph attention network for recommendation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Learning intents behind interactions with knowledge graph for recommendation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;LightGCL: Simple Yet Effective Graph Contrastive Learning for Recommendation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Self-supervised graph learning for recommendation&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/dccf/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/dccf/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/dccf/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/dccf/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Example 5 (Category: Diffusion and Flow Matching)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed model presented in this paper focuses on the task of generative modeling through the framework of Continuous Normalizing Flows (CNFs) to define straight flows between noise and data samples.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;Architecture: 
    &lt;ul&gt; 
     &lt;li&gt;Implement a neural network to parameterize the velocity field \( v_{\theta}(t, x) \) that maps from noise to data distributions.&lt;/li&gt; 
     &lt;li&gt;Use architectures suitable for continuous functions, such as feedforward or convolutional networks.&lt;/li&gt; 
     &lt;li&gt;Each layer should have non-linear activation functions (e.g., ReLU, Tanh).&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Loss Functions: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Velocity Consistency Loss&lt;/strong&gt;: This should be structured as: \[ L_{\theta} = E_{t \sim U} E_{x_t, x_{t+\Delta t}} \| f_{\theta}(t, x_t) - f_{\theta}(t+\Delta t, x_{t+\Delta t}) \|^2_2 + \alpha \| v_{\theta}(t, x_t) - v_{\theta}(t+\Delta t, x_{t+\Delta t}) \|^2_2 \] where \( f_{\theta}(t, x_t) = x_t + (1 - t) v_{\theta}(t, x_t) \). Choose \( \alpha \) based on cross-validation performance. &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Training Procedure: 
    &lt;ul&gt; 
     &lt;li&gt;Sample \( x_0 \) from the noise distribution \( p_0 \).&lt;/li&gt; 
     &lt;li&gt;For multiple time segments, define intervals and compute velocity fields iteratively.&lt;/li&gt; 
     &lt;li&gt;Use the weights of the proposed approach in an exponential moving average to stabilize training.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Sampling Process: 
    &lt;ul&gt; 
     &lt;li&gt;For single-step or multi-step generation, heuristically sample from the noise distribution and use the learned velocity field as follows: \[ x_{i/k} = x_{(i-1)/k} + \frac{1}{k} v_{i\theta}((i-1)/k, x_{(i-1)/k}) \] &lt;/li&gt; 
     &lt;li&gt;Apply the Euler method for iterative updates: \[ x_{t + \Delta t} = x_t + \Delta t v_i(t, x_t) \] where \( t \in [i/k, (i + 1)/k - \Delta t] \). &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Key Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;Ensure the network is equipped with a suitable optimizer such as Adam with a learning rate around \( 2 \times 10^{-4} \).&lt;/li&gt; 
     &lt;li&gt;The batch size should be appropriately set (e.g., 512 for CIFAR-10).&lt;/li&gt; 
     &lt;li&gt;Employ an ODE solver, suggested as Euler's method, during the training and sampling processes.&lt;/li&gt; 
     &lt;li&gt;Maintain a uniform distribution for sampling time intervals \( U \).&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Performance Considerations: 
    &lt;ul&gt; 
     &lt;li&gt;Monitor convergence rates and empirically validate parameter configurations through experiments. Start with fewer segments and gradually increase to capture complex distributions better.&lt;/li&gt; 
     &lt;li&gt;Adjust the decay rate for the EMA based on the stability of convergence (commonly around 0.999).&lt;/li&gt; 
     &lt;li&gt;Analyze the trade-offs between sampling efficiency and sample quality, ensuring a balance during proposed model development.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Flow matching for generative modeling&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;Flow matching for generative modeling&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Consistency models&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Rectified Flow&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Denoising diffusion probabilistic models&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Optimal flow matching: Learning straight trajectories in just one step&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Maximum likelihood training of score-based diffusion models&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/con_flowmatching/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/con_flowmatching/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/con_flowmatching/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/con_flowmatching/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Example 6 (Category: Graph Neural Networks)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed model focuses on the task of node classification in large graphs, addressing challenges like scalability, heterophily, long-range dependencies, and the absence of edges.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;The core techniques used in this study include a kernelized Gumbel-Softmax operator for all-pair message passing, which reduces computational complexity to linear (O(N)), and a Transformer-style network architecture designed for layer-wise learning of latent graph structures.&lt;/li&gt; 
   &lt;li&gt;The purpose of the kernelized Gumbel-Softmax operator is to enable differentiable learning of discrete graph structures by approximating categorical distributions. The Transformer-style architecture facilitates information propagation between arbitrary pairs of nodes through learned latent graphs.&lt;/li&gt; 
   &lt;li&gt;Implementation details for each component: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Kernelized Gumbel-Softmax Operator&lt;/strong&gt;: Set the temperature parameter (œÑ) to a range typically between 0.25 and 0.4 for training. It operates on node feature representations (D-dimensional feature vectors). The output of this operator is a distribution over node connections, facilitating the selection of neighbors for message passing.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Node Feature Input&lt;/strong&gt;: Each node input should be represented as a feature vector (e.g., {x_u} ‚àà R^D), and the output is an updated representation of the node embedding after message passing.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Relational Bias (if applicable)&lt;/strong&gt;: Introduces activation (e.g., sigmoid) to adjust the message passing weights based on an observed adjacency matrix, which enhances weight assignment for connected nodes.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Edge Regularization Loss&lt;/strong&gt;: Combines categorical edge probabilities with a supervised classification loss, encouraging the network to maintain predicted edges consistent with observed edges.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;The step-by-step interaction of these components includes: 
    &lt;ul&gt; 
     &lt;li&gt;Begin with an input matrix of node embeddings (X) and, if available, an adjacency matrix (A).&lt;/li&gt; 
     &lt;li&gt;Apply the kernelized Gumbel-Softmax operator to the embedding matrix to generate a probability distribution over neighbor selection for each node.&lt;/li&gt; 
     &lt;li&gt;Use these probabilities to sample neighbors, allowing for message passing where each node aggregates information from its selected neighbors.&lt;/li&gt; 
     &lt;li&gt;Update the node embeddings using an attention mechanism, which can be enhanced by relational bias if edges are available.&lt;/li&gt; 
     &lt;li&gt;After K iterations of neighbor sampling, apply loss functions comprising a supervised classification loss and, if applicable, edge-level regularization loss to optimize the embedding representations.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Critical implementation details affecting performance involve: 
    &lt;ul&gt; 
     &lt;li&gt;Careful tuning of the temperature parameter (œÑ) in the Gumbel-Softmax operator, as it significantly influences the proposed approach's capacity to capture the discrete nature of graph structures.&lt;/li&gt; 
     &lt;li&gt;Utilizing appropriate batch sizes for large-scale graphs, ensuring enough memory is available while also maintaining computational efficiency.&lt;/li&gt; 
     &lt;li&gt;Choosing the correct dimensionality for random features in the kernel approximation, balancing model expressiveness and training stability.&lt;/li&gt; 
     &lt;li&gt;The use of dropout or other regularization techniques such as edge-level regularization can influence the proposed model's generalization capabilities on unseen data.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;On the bottleneck of graph neural networks and its practical implications&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;On the bottleneck of graph neural networks and its practical implications&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Semi-supervised classification with graph convolutional networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Categorical reparameterization with gumbel-softmax&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Learning discrete structures for graph neural networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Graph attention networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Geometric deep learning: going beyond euclidean data&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Graph structure learning for robust graph neural networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Geom-gcn: Geometric graph convolutional networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;New benchmarks for learning on non-homophilous graphs&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Latent patient network learning for automatic diagnosis&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Few-shot learning with graph neural networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;The graph neural network model&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Characteristic functions on graphs: Birds of a feather, from statistical descriptors to parametric models&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Beyond homophily in graph neural networks: Current limitations and effective designs&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_nodeformer/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_nodeformer/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_nodeformer/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_nodeformer/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Example 7 (Category: Graph Neural Networks)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed approach works on the task of uncovering data dependencies and learning instance representations from datasets that may not have complete or reliable relationships, particularly in semi-supervised contexts like node classification, image/text classification, and spatial-temporal dynamics prediction.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;The core techniques/algorithms used in this paper include an energy-constrained diffusion model represented as a partial differential equation (PDE), an explicit Euler scheme for numerical solutions, and a form of adaptive diffusivity function based on the energy function. The proposed architecture utilizes a diffusion-based Transformer framework that allows for all-pair feature propagation among instances.&lt;/li&gt; 
   &lt;li&gt;The major technical components serve the following purposes: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Diffusion Process:&lt;/strong&gt; Encodes instances into evolving states by modeling information flow, where instance representations evolve according to a PDE illuminating the relationships among the instances.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Energy Function:&lt;/strong&gt; Provides constraints to regularize the diffusion process and guide the proposed model towards desired low-energy embeddings, enhancing the quality of representations.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Diffusivity Function:&lt;/strong&gt; Specifies the strength of information flow between instances, adapting based on the instance states, and allows for flexible and efficient propagation strategies.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Implementation details for each component: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Diffusion Process Input:&lt;/strong&gt; Requires a batch of instances represented as a matrix of size \(N \times D\), where \(N\) is the number of instances and \(D\) is the input feature dimension.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Diffusion Process Output:&lt;/strong&gt; Produces the updated instance representations after \(K\) propagation steps. The step size \(\tau\) should be set within the range (0, 1).&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Energy Function:&lt;/strong&gt; Implemented as \(E(Z, k; \delta) = ||Z - Z^{(k)}||^2_F + \lambda \sum_{i,j} \delta(||z_i - z_j||^2_2)\), with \(\delta\) being a non-decreasing, concave function.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Key Parameters:&lt;/strong&gt; 
      &lt;ul&gt; 
       &lt;li&gt;Step size \(\tau\)&lt;/li&gt; 
       &lt;li&gt;Layer number \(K\) (number of diffusion propagation steps)&lt;/li&gt; 
       &lt;li&gt;Regularization weight \(\lambda\).&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Step-by-step description of interactions: 
    &lt;ul&gt; 
     &lt;li&gt;Start by initializing the instance representations.&lt;/li&gt; 
     &lt;li&gt;For each layer of diffusion, compute the diffusivity \(S(k)\) based on current embeddings through a function \(f\) which can be defined differently depending on the proposed model implementation.&lt;/li&gt; 
     &lt;li&gt;Update the instance representations using the defined diffusion equations, ensuring to conserve states and introduce propagation according to the computed diffusivity.&lt;/li&gt; 
     &lt;li&gt;After \(K\) layers of diffusion, apply a final output layer to produce logits for predictions.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Critical implementation details that affect performance: 
    &lt;ul&gt; 
     &lt;li&gt;The choice of diffusivity function \(f\) greatly impacts the proposed model's capacity to learn complex dependencies, where specific formulations (like linear or logistic) yield different abilities in capturing inter-instance relationships.&lt;/li&gt; 
     &lt;li&gt;Ensure that the values of \(\tau\) and \(\lambda\) are set appropriately to balance convergence speed and representation quality; using a smaller \(\tau\) may require deeper layers to learn effectively.&lt;/li&gt; 
     &lt;li&gt;Optimization parameters like learning rate and early stopping criteria are essential, particularly for large-scale datasets where convergence behavior can vary widely depending on architecture size and complexity.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Diffusion-convolutional neural networks&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;Diffusion-convolutional neural networks&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Semi-supervised classification with graph convolutional networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Manifold regularization: A geometric framework for learning from labeled and unlabeled examples&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Geometric deep learning: going beyond euclidean data&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Artificial neural networks for solving ordinary and partial differential equations&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Scaling graph neural networks with approximate pagerank&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Learning discrete structures for graph neural networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Semi-supervised learning using gaussian fields and harmonic functions&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Graph convolutional networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Deep learning via semi-supervised embedding&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;A generalization of transformer networks to graphs&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Graph Convolution and Quadratic Time Complexity&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Bayesian graph convolutional neural networks for semi-supervised classification&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Do transformers really perform bad for graph representation?&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Big bird: Transformers for longer sequences&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Adaptive graph diffusion networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Transformers are RNNs&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Collective classification in network data&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;NodeFormer: A scalable graph structure learning transformer for node classification&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_difformer/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_difformer/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_difformer/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_difformer/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;span id="how-it-works"&gt;&lt;/span&gt; 
&lt;h2&gt;‚ú®How AI-Researcher works&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üîÑ &lt;strong&gt;End-to-End Scientific Research Automation System&lt;/strong&gt; &lt;br /&gt;Our &lt;strong&gt;AI-Researcher&lt;/strong&gt; provides comprehensive automation for the complete scientific research lifecycle through an integrated pipeline. The system orchestrates research activities across three strategic phases: 
  &lt;ol&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Literature Review &amp;amp; Idea Generation&lt;/strong&gt; üìöüí°&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt; &lt;p&gt;üîç &lt;strong&gt;Resource Collector&lt;/strong&gt;: Systematically gathers comprehensive research materials across multiple scientific domains through automated collection from major academic databases (e.g., arXiv, IEEE Xplore, ACM Digital Library, and Google Scholar), code platforms (e.g., GitHub, Hugging Face), and open datasets across scientific domains.&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;üß† &lt;strong&gt;Resource Filter&lt;/strong&gt;: Evaluates and selects high-impact papers, well-maintained code implementations, and benchmark datasets through quality metrics (e.g., citation count, code maintenance, data completeness) and relevance assessment.&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;üí≠ &lt;strong&gt;Idea Generator&lt;/strong&gt;: Leveraging the identified research resources, including high-impact papers and code repositories, the Idea Generator systematically formulates novel research directions through comprehensive analysis. It automatedly evaluates current methodological limitations, map emerging technological trends, and explore uncharted research territories.&lt;/p&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;New Algorithm Design, Implementation &amp;amp; Validation&lt;/strong&gt; üß™üíª &lt;br /&gt;&lt;strong&gt;Design ‚Üí Implementation ‚Üí Validation ‚Üí Refinement&lt;/strong&gt;&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt; &lt;p&gt;üìù&lt;strong&gt;Design Phase&lt;/strong&gt;: The initial phase focuses on conceptual development, where novel algorithmic ideas are formulated and theoretical foundations are established. During this stage, we carefully plan the implementation strategy, ensuring the proposed solution advances beyond existing approaches while maintaining practical feasibility.&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;‚öôÔ∏è&lt;strong&gt;Implementation Phase&lt;/strong&gt;: proceed to transform abstract concepts into concrete code implementations. This phase involves developing functional modules, establishing a robust testing environment, and creating necessary infrastructure for experimental validation.&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;üî¨&lt;strong&gt;Validation Phase&lt;/strong&gt;: Systematic experimentation forms the core of our validation process. We execute comprehensive tests to evaluate algorithm performance, collect metrics, and document all findings. This phase ensures rigorous implementation verification with practical requirements.&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;üîß&lt;strong&gt;Refinement Phase&lt;/strong&gt; üî¨: Based on validation results, we enter an iterative refinement cycle. This phase involves identifying areas for improvement, optimizing code efficiency, and implementing necessary enhancements. We carefully analyze performance bottlenecks and plan strategic improvements for the next development iteration.&lt;/p&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Paper Writing&lt;/strong&gt; ‚úçÔ∏èüìù&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Writer Agent&lt;/strong&gt; üìÑ: Automatically generates full-length academic papers by integrating research ideas, motivations, newly designed algorithm frameworks, and algorithm validation performance. Leveraging a hierarchical writing approach, it creates polished manuscripts with precision and clarity.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üöÄ This fully automated system removes the need for manual intervention across the entire research lifecycle, enabling effortless and seamless scientific discovery‚Äîfrom initial concept to final publication. üöÄ It serves as an excellent research assistant, aiding researchers in achieving their goals efficiently and effectively.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;üî¨ &lt;strong&gt;Comprehensive Benchmark Suite&lt;/strong&gt; &lt;br /&gt;We have developed a comprehensive and standardized evaluation framework to objectively assess the academic capabilities of AI researchers and the quality of their scholarly work, integrating several key innovations to ensure thorough and reliable evaluation.&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt; &lt;p&gt;üë®‚Äçüî¨ &lt;strong&gt;Expert-Level Ground Truth&lt;/strong&gt;: TThe benchmark leverages human expert-written papers as ground truth references, establishing a high-quality standard for comparison and validation.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;üåà &lt;strong&gt;Multi-Domain Coverage&lt;/strong&gt;: Our benchmark is designed to comprehensively span 4 major research domains, ensuring broad applicability: Computer Vision (CV), Nature Language Processing (NLP), Data Mining (DM), and Information Retrieval (IR).&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;üåê &lt;strong&gt;Fully Open-Source Benchmark Construction&lt;/strong&gt;: We have fully open-sourced the methodology and process for building the benchmark, including complete access to processed datasets, data collection pipelines, and processing code. This ensures &lt;strong&gt;Transparency in Evaluation&lt;/strong&gt; while empowering the community to customize and construct benchmarks tailored to their specific domains for testing AI researchers.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;üìä &lt;strong&gt;Comprehensive Evaluation Metrics&lt;/strong&gt;: Our evaluation framework adopts a hierarchical and systematic approach, where tasks are organized into two levels based on the extent of idea provision. Leveraging specialized &lt;strong&gt;Evaluator Agents&lt;/strong&gt;, the framework conducts thorough assessments across multiple dimensions, ensuring a robust and comprehensive evaluation. Key evaluation metrics include: 1) &lt;strong&gt;Novelty&lt;/strong&gt;: Assessing the innovation and uniqueness of the research work. 2) &lt;strong&gt;Experimental Comprehensiveness&lt;/strong&gt;: Evaluating the design, execution, and rigor of the experiments. 3) &lt;strong&gt;Theoretical Foundation&lt;/strong&gt;: Measuring the strength of the theoretical background and foundations. 4) &lt;strong&gt;Result Analysis&lt;/strong&gt;: Analyzing the depth and accuracy of result interpretation. 5) &lt;strong&gt;Writing Quality&lt;/strong&gt;: Reviewing the clarity, coherence, and structure of the written report.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üöÄ &lt;strong&gt;Advancing Research Automation&lt;/strong&gt;. This benchmark suite provides an objective framework for assessing research automation capabilities. It is designed to evolve continuously, incorporating new advancements and expanding its scope to meet the growing demands of the research community.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;üåü &lt;strong&gt;Easy-to-Use AI Research Assistant&lt;/strong&gt; &lt;br /&gt;&lt;strong&gt;AI-Researcher&lt;/strong&gt;E delivers a truly seamless and accessible experience for research automation, empowering users to focus on innovation without technical barriers. Key features include:&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt; &lt;p&gt;üåê &lt;strong&gt;Multi-LLM Provider Support&lt;/strong&gt;: Effortlessly integrates with leading language model providers such as Claude, OpenAI, Deepseek, and more. Researchers can select the most suitable AI capabilities for their specific needs.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;üìö &lt;strong&gt;Effortless Research Kickoff&lt;/strong&gt;: Kickstart your research journey with unparalleled ease! Simply provide a list of relevant papers, and &lt;strong&gt;AI-Researcher&lt;/strong&gt; takes care of the rest‚Äîno need to upload files, contribute initial ideas, or navigate complex configurations. It's the ultimate tool to help you jumpstart your research process efficiently and effectively.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;üß† &lt;strong&gt;Minimal Domain Expertise Needed&lt;/strong&gt;: AI-Researcher simplifies the research process by autonomously identifying critical research gaps, proposing innovative approaches, and executing the entire research pipeline. While some domain understanding can enhance results, the tool is designed to empower users of all expertise levels to achieve impactful outcomes with ease.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;üì¶ &lt;strong&gt;Out-of-the-Box Functionality&lt;/strong&gt;: Experience seamless research automation right from the start. AI-Researcher is ready to use with minimal setup, giving you instant access to advanced capabilities. Skip the hassle of complex configurations and dive straight into accelerating your research process with ease and efficiency.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="how-to-use"&gt;&lt;/span&gt; 
&lt;h2&gt;üîç How to use AI-Researcher&lt;/h2&gt; 
&lt;h3&gt;1. Research Agent&lt;/h3&gt; 
&lt;p&gt;If you want to use research agent with the given idea (Level 1 tasks), conducting extensive survey and experiments, you can use the following command in the &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/research_agent/run_infer_level_1.sh"&gt;&lt;code&gt;research_agent/run_infer_level_1.sh&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;current_dir=$(dirname "$(readlink -f "$0")")
cd $current_dir
export DOCKER_WORKPLACE_NAME=workplace_paper

export BASE_IMAGES=tjbtech1/paperagent:latest

export COMPLETION_MODEL=claude-3-5-sonnet-20241022
export CHEEP_MODEL=claude-3-5-haiku-20241022

category=vq
instance_id=one_layer_vq
export GPUS='"device=0,1"'

python run_infer_plan.py --instance_path ../benchmark/final/${category}/${instance_id}.json --container_name paper_eval --task_level task1 --model $COMPLETION_MODEL --workplace_name workplace --cache_path cache --port 12372 --max_iter_times 0 --category ${category}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to just give the reference papers, and let the research agent to generate the idea then conduct the experiments (Level 2 tasks), you can use the following command in the &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/research_agent/run_infer_level_2.sh"&gt;&lt;code&gt;research_agent/run_infer_level_2.sh&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;current_dir=$(dirname "$(readlink -f "$0")")
cd $current_dir
export DOCKER_WORKPLACE_NAME=workplace_paper

export BASE_IMAGES=tjbtech1/paperagent:latest

export COMPLETION_MODEL=claude-3-5-sonnet-20241022
export CHEEP_MODEL=claude-3-5-haiku-20241022

category=vq
instance_id=one_layer_vq
export GPUS='"device=0,1"'

python run_infer_idea.py --instance_path ../benchmark/final/${category}/${instance_id}.json --container_name paper_eval --model $COMPLETION_MODEL --workplace_name workplace --cache_path cache --port 12372 --max_iter_times 0 --category ${category}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Paper Writing Agent&lt;/h3&gt; 
&lt;p&gt;If you want to generate the paper after the research agent has conducted the research, you can use the following command in the &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/paper_agent/run_paper.sh"&gt;&lt;code&gt;paper_agent/run_infer.sh&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;#!/bin/bash

cd path/to/AI-Researcher/paper_agent

export OPENAI_API_KEY=sk-SKlupNntta4WPmvDCRo7uuPbYGwOnUQcb25Twn8c718tPpXN


research_field=vq
instance_id=rotated_vq

python path/to/AI-Researcher/paper_agent/writing.py --research_field ${research_field} --instance_id ${instance_id}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Benchmark Data and Collection&lt;/h3&gt; 
&lt;p&gt;Our benchmark is also fully-open-sourced:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Detailed benchmark data is available in the &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/benchmark"&gt;&lt;code&gt;benchmark&lt;/code&gt;&lt;/a&gt; folder.&lt;/li&gt; 
 &lt;li&gt;Detailed benchmark collection process is available in the &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/benchmark_collection"&gt;&lt;code&gt;benchmark_collection&lt;/code&gt;&lt;/a&gt; folder.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="documentation"&gt;&lt;/span&gt; 
&lt;h2&gt;üìñ Documentation&lt;/h2&gt; 
&lt;p&gt;Comprehensive documentation is on its way üöÄ! Stay tuned for updates on our &lt;a href="https://auto-researcher.github.io/docs"&gt;Documentation&lt;/a&gt; page.&lt;/p&gt; 
&lt;span id="community"&gt;&lt;/span&gt; 
&lt;h2&gt;ü§ù Join the Community&lt;/h2&gt; 
&lt;p&gt;We aim to build a vibrant community around AI-Researcher and warmly invite everyone to join us. Here's how you can become part of our community:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://join.slack.com/t/ai-researchergroup/shared_invite/zt-30y5a070k-C0ajQt1zmVczFnfGkIicvA"&gt;Join our Slack workspace&lt;/a&gt; - Here we talk about research, architecture, and future development.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/ghSnKGkq"&gt;Join our Discord server&lt;/a&gt; - This is a community-run server for general discussion, questions, and feedback.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/HKUDS/AI-Researcher/issues"&gt;Read or post Github Issues&lt;/a&gt; - Check out the issues we're working on, or add your own ideas.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Misc&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/HKUDS/AI-Researcher/stargazers"&gt;&lt;img src="https://reporoster.com/stars/HKUDS/AI-Researcher" alt="Stargazers repo roster for @HKUDS/AI-Researcher" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/HKUDS/AI-Researcher/network/members"&gt;&lt;img src="https://reporoster.com/forks/HKUDS/AI-Researcher" alt="Forkers repo roster for @HKUDS/AI-Researcher" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://star-history.com/#HKUDS/AI-Researcher&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=HKUDS/AI-Researcher&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;span id="cite"&gt;&lt;/span&gt; 
&lt;h2&gt;üåü Cite&lt;/h2&gt; 
&lt;p&gt;A more detailed technical report will be released soon. üöÄ:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-tex"&gt;@misc{airesearcher,
      title={{AI-Researcher: Autonomous Scientific Innovation}},
      author={Jiabin Tang, Lianghao Xia, Zhonghang Li, Chao Huang},
      year={2025},
      eprint={2505.18705},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2505.18705},
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>bytedance/UI-TARS</title>
      <link>https://github.com/bytedance/UI-TARS</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/bytedance/UI-TARS/main/figures/writer.png" alt="Local Image" /&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; üåê &lt;a href="https://seed-tars.com/"&gt;Website&lt;/a&gt;&amp;nbsp;&amp;nbsp; | ü§ó &lt;a href="https://huggingface.co/ByteDance-Seed/UI-TARS-1.5-7B"&gt;Hugging Face Models&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; üîß &lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS/main/README_deploy.md"&gt;Deployment&lt;/a&gt; &amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; üìë &lt;a href="https://arxiv.org/abs/2501.12326"&gt;Paper&lt;/a&gt; &amp;nbsp;&amp;nbsp; |&amp;nbsp;&amp;nbsp; üñ•Ô∏è &lt;a href="https://github.com/bytedance/UI-TARS-desktop"&gt;UI-TARS-desktop&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;br /&gt;üèÑ &lt;a href="https://github.com/web-infra-dev/Midscene"&gt;Midscene (Browser Automation) &lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü´® &lt;a href="https://discord.gg/pTXwYVjfcs"&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/13561"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13561" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;We also offer a &lt;strong&gt;UI-TARS-desktop&lt;/strong&gt; version, which can operate on your &lt;strong&gt;local personal device&lt;/strong&gt;. To use it, please visit &lt;a href="https://github.com/bytedance/UI-TARS-desktop"&gt;https://github.com/bytedance/UI-TARS-desktop&lt;/a&gt;. To use UI-TARS in web automation, you may refer to the open-source project &lt;a href="https://github.com/web-infra-dev/Midscene"&gt;Midscene.js&lt;/a&gt;. &lt;strong&gt;‚ùóNotes&lt;/strong&gt;: Since Qwen 2.5vl based models ultilizes absolute coordinates to ground objects, please kindly refer to our illustration about how to process coordinates in this &lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS/main/README_coordinates.md"&gt;guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üåü 2025.04.16: We shared the latest progress of the UI-TARS-1.5 model in our &lt;a href="https://seed-tars.com/1.5"&gt;blog&lt;/a&gt;, which excels in playing games and performing GUI tasks, and we open-sourced the &lt;a href="https://huggingface.co/ByteDance-Seed/UI-TARS-1.5-7B"&gt;UI-TARS-1.5-7B&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;‚ú® 2025.03.23: We updated the OSWorld inference scripts from the original official &lt;a href="https://github.com/xlang-ai/OSWorld/raw/main/run_uitars.py"&gt;OSWorld repository&lt;/a&gt;. Now, you can use the OSWorld official inference scripts to reproduce our results.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;UI-TARS-1.5, an open-source multimodal agent built upon a powerful vision-language model. It is capable of effectively performing diverse tasks within virtual worlds.&lt;/p&gt; 
&lt;p&gt;Leveraging the foundational architecture introduced in &lt;a href="https://arxiv.org/abs/2501.12326"&gt;our recent paper&lt;/a&gt;, UI-TARS-1.5 integrates advanced reasoning enabled by reinforcement learning. This allows the model to reason through its thoughts before taking action, significantly enhancing its performance and adaptability, particularly in inference-time scaling. Our new 1.5 version achieves state-of-the-art results across a variety of standard benchmarks, demonstrating strong reasoning capabilities and notable improvements over prior models.&lt;/p&gt; 
&lt;!-- ![Local Image](figures/UI-TARS.png) --&gt; 
&lt;p align="center"&gt; 
 &lt;video controls width="480"&gt; 
  &lt;source src="https://huggingface.co/datasets/JjjFangg/Demo_video/resolve/main/GUI_demo.mp4" type="video/mp4" /&gt; 
 &lt;/video&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align="center"&gt; 
 &lt;video controls width="480"&gt; 
  &lt;source src="https://huggingface.co/datasets/JjjFangg/Demo_video/resolve/main/Game_demo.mp4" type="video/mp4" /&gt; 
 &lt;/video&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2&gt;üöÄ Quick Start Guide: Deploying and Using Our Model&lt;/h2&gt; 
&lt;p&gt;To help you get started quickly with our model, we recommend following the steps below in order. These steps will guide you through deployment, prediction post-processing to make the model take actions in your environment.&lt;/p&gt; 
&lt;h3&gt;‚úÖ Step 1: Deployment &amp;amp; Inference&lt;/h3&gt; 
&lt;p&gt;üëâ &lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS/main/README_deploy.md"&gt;Deployment and Inference&lt;/a&gt;. This includes instructions for model deployment using huggingface endpoint, and running your first prediction.&lt;/p&gt; 
&lt;h3&gt;‚úÖ Step 2: Post Processing&lt;/h3&gt; 
&lt;h4&gt;Installation&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install ui-tars
# or
uv pip install ui-tars
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Usage&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from ui_tars.action_parser import parse_action_to_structure_output, parsing_response_to_pyautogui_code

response = "Thought: Click the button\nAction: click(start_box='(100,200)')"
original_image_width, original_image_height = 1920, 1080
parsed_dict = parse_action_to_structure_output(
    response,
    factor=1000,
    origin_resized_height=original_image_height,
    origin_resized_width=original_image_width,
    model_type="qwen25vl"
)
print(parsed_dict)
parsed_pyautogui_code = parsing_response_to_pyautogui_code(
    responses=parsed_dict,
    image_height=original_image_height,
    image_width=original_image_width
)
print(parsed_pyautogui_code)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;FYI: Coordinates visualization&lt;/h5&gt; 
&lt;p&gt;To help you better understand the coordinate processing, we also provide a &lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS/main/README_coordinates.md"&gt;guide&lt;/a&gt; for coordinates processing visualization.&lt;/p&gt; 
&lt;h2&gt;Prompt Usage Guide&lt;/h2&gt; 
&lt;p&gt;To accommodate different device environments and task complexities, the following three prompt templates in &lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS/main/codes/ui_tars/prompt.py"&gt;codes/ui_tars/prompt.py&lt;/a&gt;. are designed to guide GUI agents in generating appropriate actions. Choose the template that best fits your use case:&lt;/p&gt; 
&lt;h3&gt;üñ•Ô∏è &lt;code&gt;COMPUTER_USE&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Recommended for&lt;/strong&gt;: GUI tasks on &lt;strong&gt;desktop environments&lt;/strong&gt; such as Windows, Linux, or macOS.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports common desktop operations: mouse clicks (single, double, right), drag actions, keyboard shortcuts, text input, scrolling, etc.&lt;/li&gt; 
 &lt;li&gt;Ideal for browser navigation, office software interaction, file management, and other desktop-based tasks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üì± &lt;code&gt;MOBILE_USE&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Recommended for&lt;/strong&gt;: GUI tasks on &lt;strong&gt;mobile devices or Android emulators&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Includes mobile-specific actions: &lt;code&gt;long_press&lt;/code&gt;, &lt;code&gt;open_app&lt;/code&gt;, &lt;code&gt;press_home&lt;/code&gt;, &lt;code&gt;press_back&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Suitable for launching apps, scrolling views, filling input fields, and navigating within mobile apps.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìå &lt;code&gt;GROUNDING&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Recommended for&lt;/strong&gt;: Lightweight tasks focused solely on &lt;strong&gt;action output&lt;/strong&gt;, or for use in model training and evaluation.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Only outputs the &lt;code&gt;Action&lt;/code&gt; without any reasoning (&lt;code&gt;Thought&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;Useful for evaluating grounding capability.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;When developing or evaluating multimodal interaction systems, choose the appropriate prompt template based on your target platform (desktop vs. mobile)&lt;/p&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Online Benchmark Evaluation&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Benchmark type&lt;/th&gt; 
   &lt;th&gt;Benchmark&lt;/th&gt; 
   &lt;th&gt;UI-TARS-1.5&lt;/th&gt; 
   &lt;th&gt;OpenAI CUA&lt;/th&gt; 
   &lt;th&gt;Claude 3.7&lt;/th&gt; 
   &lt;th&gt;Previous SOTA&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Computer Use&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2404.07972"&gt;OSworld&lt;/a&gt; (100 steps)&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;42.5&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;36.4&lt;/td&gt; 
   &lt;td&gt;28&lt;/td&gt; 
   &lt;td&gt;38.1 (200 step)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2409.08264"&gt;Windows Agent Arena&lt;/a&gt; (50 steps)&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;42.1&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;29.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Browser Use&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2401.13919"&gt;WebVoyager&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;84.8&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;87&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;84.1&lt;/td&gt; 
   &lt;td&gt;87&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2504.01382"&gt;Online-Mind2web&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;75.8&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;71&lt;/td&gt; 
   &lt;td&gt;62.9&lt;/td&gt; 
   &lt;td&gt;71&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Phone Use&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2405.14573"&gt;Android World&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;64.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;59.5&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Grounding Capability Evaluation&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Benchmark&lt;/th&gt; 
   &lt;th&gt;UI-TARS-1.5&lt;/th&gt; 
   &lt;th&gt;OpenAI CUA&lt;/th&gt; 
   &lt;th&gt;Claude 3.7&lt;/th&gt; 
   &lt;th&gt;Previous SOTA&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2410.23218"&gt;ScreenSpot-V2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;94.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;87.9&lt;/td&gt; 
   &lt;td&gt;87.6&lt;/td&gt; 
   &lt;td&gt;91.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2504.07981v1"&gt;ScreenSpotPro&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;61.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;23.4&lt;/td&gt; 
   &lt;td&gt;27.7&lt;/td&gt; 
   &lt;td&gt;43.6&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Poki Game&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/2048"&gt;2048&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/cubinko"&gt;cubinko&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/energy"&gt;energy&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/free-the-key"&gt;free-the-key&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/gem-11"&gt;Gem-11&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/hex-frvr"&gt;hex-frvr&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/infinity-loop"&gt;Infinity-Loop&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/maze-path-of-light"&gt;Maze:Path-of-Light&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/shapes"&gt;shapes&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/snake-solver"&gt;snake-solver&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/wood-blocks-3d"&gt;wood-blocks-3d&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/yarn-untangle"&gt;yarn-untangle&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/laser-maze-puzzle"&gt;laser-maze-puzzle&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/tiles-master"&gt;tiles-master&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI CUA&lt;/td&gt; 
   &lt;td&gt;31.04&lt;/td&gt; 
   &lt;td&gt;0.00&lt;/td&gt; 
   &lt;td&gt;32.80&lt;/td&gt; 
   &lt;td&gt;0.00&lt;/td&gt; 
   &lt;td&gt;46.27&lt;/td&gt; 
   &lt;td&gt;92.25&lt;/td&gt; 
   &lt;td&gt;23.08&lt;/td&gt; 
   &lt;td&gt;35.00&lt;/td&gt; 
   &lt;td&gt;52.18&lt;/td&gt; 
   &lt;td&gt;42.86&lt;/td&gt; 
   &lt;td&gt;2.02&lt;/td&gt; 
   &lt;td&gt;44.56&lt;/td&gt; 
   &lt;td&gt;80.00&lt;/td&gt; 
   &lt;td&gt;78.27&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Claude 3.7&lt;/td&gt; 
   &lt;td&gt;43.05&lt;/td&gt; 
   &lt;td&gt;0.00&lt;/td&gt; 
   &lt;td&gt;41.60&lt;/td&gt; 
   &lt;td&gt;0.00&lt;/td&gt; 
   &lt;td&gt;0.00&lt;/td&gt; 
   &lt;td&gt;30.76&lt;/td&gt; 
   &lt;td&gt;2.31&lt;/td&gt; 
   &lt;td&gt;82.00&lt;/td&gt; 
   &lt;td&gt;6.26&lt;/td&gt; 
   &lt;td&gt;42.86&lt;/td&gt; 
   &lt;td&gt;0.00&lt;/td&gt; 
   &lt;td&gt;13.77&lt;/td&gt; 
   &lt;td&gt;28.00&lt;/td&gt; 
   &lt;td&gt;52.18&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;UI-TARS-1.5&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;0.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Minecraft&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Task Type&lt;/th&gt; 
   &lt;th&gt;Task Name&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://openai.com/index/vpt/"&gt;VPT&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://www.nature.com/articles/s41586-025-08744-2"&gt;DreamerV3&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;Previous SOTA&lt;/th&gt; 
   &lt;th&gt;UI-TARS-1.5 w/o Thought&lt;/th&gt; 
   &lt;th&gt;UI-TARS-1.5 w/ Thought&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mine Blocks&lt;/td&gt; 
   &lt;td&gt;(oak_log)&lt;/td&gt; 
   &lt;td&gt;0.8&lt;/td&gt; 
   &lt;td&gt;1.0&lt;/td&gt; 
   &lt;td&gt;1.0&lt;/td&gt; 
   &lt;td&gt;1.0&lt;/td&gt; 
   &lt;td&gt;1.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;(obsidian)&lt;/td&gt; 
   &lt;td&gt;0.0&lt;/td&gt; 
   &lt;td&gt;0.0&lt;/td&gt; 
   &lt;td&gt;0.0&lt;/td&gt; 
   &lt;td&gt;0.2&lt;/td&gt; 
   &lt;td&gt;0.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;(white_bed)&lt;/td&gt; 
   &lt;td&gt;0.0&lt;/td&gt; 
   &lt;td&gt;0.0&lt;/td&gt; 
   &lt;td&gt;0.1&lt;/td&gt; 
   &lt;td&gt;0.4&lt;/td&gt; 
   &lt;td&gt;0.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;200 Tasks Avg.&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;0.06&lt;/td&gt; 
   &lt;td&gt;0.03&lt;/td&gt; 
   &lt;td&gt;0.32&lt;/td&gt; 
   &lt;td&gt;0.35&lt;/td&gt; 
   &lt;td&gt;0.42&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Kill Mobs&lt;/td&gt; 
   &lt;td&gt;(mooshroom)&lt;/td&gt; 
   &lt;td&gt;0.0&lt;/td&gt; 
   &lt;td&gt;0.0&lt;/td&gt; 
   &lt;td&gt;0.1&lt;/td&gt; 
   &lt;td&gt;0.3&lt;/td&gt; 
   &lt;td&gt;0.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;(zombie)&lt;/td&gt; 
   &lt;td&gt;0.4&lt;/td&gt; 
   &lt;td&gt;0.1&lt;/td&gt; 
   &lt;td&gt;0.6&lt;/td&gt; 
   &lt;td&gt;0.7&lt;/td&gt; 
   &lt;td&gt;0.9&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;(chicken)&lt;/td&gt; 
   &lt;td&gt;0.1&lt;/td&gt; 
   &lt;td&gt;0.0&lt;/td&gt; 
   &lt;td&gt;0.4&lt;/td&gt; 
   &lt;td&gt;0.5&lt;/td&gt; 
   &lt;td&gt;0.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;100 Tasks Avg.&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;0.04&lt;/td&gt; 
   &lt;td&gt;0.03&lt;/td&gt; 
   &lt;td&gt;0.18&lt;/td&gt; 
   &lt;td&gt;0.25&lt;/td&gt; 
   &lt;td&gt;0.31&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Model Scale Comparison&lt;/h2&gt; 
&lt;p&gt;Here we compare performance across different model scales of UI-TARS on the OSworld benchmark.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Benchmark Type&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Benchmark&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;UI-TARS-72B-DPO&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;UI-TARS-1.5-7B&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;UI-TARS-1.5&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Computer Use&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2404.07972"&gt;OSWorld&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;24.6&lt;/td&gt; 
   &lt;td&gt;27.5&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;42.5&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GUI Grounding&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2504.07981v1"&gt;ScreenSpotPro&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;38.1&lt;/td&gt; 
   &lt;td&gt;49.6&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;61.6&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Limitations&lt;/h3&gt; 
&lt;p&gt;While UI-TARS-1.5 represents a significant advancement in multimodal agent capabilities, we acknowledge several important limitations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Misuse:&lt;/strong&gt; Given its enhanced performance in GUI tasks, including successfully navigating authentication challenges like CAPTCHA, UI-TARS-1.5 could potentially be misused for unauthorized access or automation of protected content. To mitigate this risk, extensive internal safety evaluations are underway.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Computation:&lt;/strong&gt; UI-TARS-1.5 still requires substantial computational resources, particularly for large-scale tasks or extended gameplay scenarios.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Hallucination&lt;/strong&gt;: UI-TARS-1.5 may occasionally generate inaccurate descriptions, misidentify GUI elements, or take suboptimal actions based on incorrect inferences‚Äîespecially in ambiguous or unfamiliar environments.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model scale:&lt;/strong&gt; The released UI-TARS-1.5-7B focuses primarily on enhancing general computer use capabilities and is not specifically optimized for game-based scenarios, where the UI-TARS-1.5 still holds a significant advantage.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What's next&lt;/h2&gt; 
&lt;p&gt;We are providing early research access to our top-performing UI-TARS-1.5 model to facilitate collaborative research. Interested researchers can contact us at &lt;a href="mailto:TARS@bytedance.com"&gt;TARS@bytedance.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Looking ahead, we envision UI-TARS evolving into increasingly sophisticated agentic experiences capable of performing real-world actions, thereby empowering platforms such as &lt;a href="https://team.doubao.com/en/"&gt;doubao&lt;/a&gt; to accomplish more complex tasks for you :)&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#bytedance/UI-TARS&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=bytedance/UI-TARS&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find our paper and model useful in your research, feel free to give us a cite.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;@article{qin2025ui,
  title={UI-TARS: Pioneering Automated GUI Interaction with Native Agents},
  author={Qin, Yujia and Ye, Yining and Fang, Junjie and Wang, Haoming and Liang, Shihao and Tian, Shizuo and Zhang, Junda and Li, Jiahao and Li, Yunxin and Huang, Shijue and others},
  journal={arXiv preprint arXiv:2501.12326},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>microsoft/BitNet</title>
      <link>https://github.com/microsoft/BitNet</link>
      <description>&lt;p&gt;Official inference framework for 1-bit LLMs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;bitnet.cpp&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/version-1.0-blue" alt="version" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/header_model_release.png" alt="BitNet Model on Hugging Face" width="800" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Try it out via this &lt;a href="https://bitnet-demo.azurewebsites.net/"&gt;demo&lt;/a&gt;, or build and run it on your own &lt;a href="https://github.com/microsoft/BitNet?tab=readme-ov-file#build-from-source"&gt;CPU&lt;/a&gt; or &lt;a href="https://github.com/microsoft/BitNet/raw/main/gpu/README.md"&gt;GPU&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;bitnet.cpp is the official inference framework for 1-bit LLMs (e.g., BitNet b1.58). It offers a suite of optimized kernels, that support &lt;strong&gt;fast&lt;/strong&gt; and &lt;strong&gt;lossless&lt;/strong&gt; inference of 1.58-bit models on CPU and GPU (NPU support will coming next).&lt;/p&gt; 
&lt;p&gt;The first release of bitnet.cpp is to support inference on CPUs. bitnet.cpp achieves speedups of &lt;strong&gt;1.37x&lt;/strong&gt; to &lt;strong&gt;5.07x&lt;/strong&gt; on ARM CPUs, with larger models experiencing greater performance gains. Additionally, it reduces energy consumption by &lt;strong&gt;55.4%&lt;/strong&gt; to &lt;strong&gt;70.0%&lt;/strong&gt;, further boosting overall efficiency. On x86 CPUs, speedups range from &lt;strong&gt;2.37x&lt;/strong&gt; to &lt;strong&gt;6.17x&lt;/strong&gt; with energy reductions between &lt;strong&gt;71.9%&lt;/strong&gt; to &lt;strong&gt;82.2%&lt;/strong&gt;. Furthermore, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU, achieving speeds comparable to human reading (5-7 tokens per second), significantly enhancing the potential for running LLMs on local devices. Please refer to the &lt;a href="https://arxiv.org/abs/2410.16144"&gt;technical report&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/m2_performance.jpg" alt="m2_performance" width="800" /&gt; 
&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/intel_performance.jpg" alt="m2_performance" width="800" /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The tested models are dummy setups used in a research context to demonstrate the inference performance of bitnet.cpp.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;p&gt;A demo of bitnet.cpp running a BitNet b1.58 3B model on Apple M2:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1"&gt;https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What's New:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;05/20/2025 &lt;a href="https://github.com/microsoft/BitNet/raw/main/gpu/README.md"&gt;BitNet Official GPU inference kernel&lt;/a&gt; &lt;img src="https://img.shields.io/badge/NEW-red" alt="NEW" /&gt;&lt;/li&gt; 
 &lt;li&gt;04/14/2025 &lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;BitNet Official 2B Parameter Model on Hugging Face&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;02/18/2025 &lt;a href="https://arxiv.org/abs/2502.11880"&gt;Bitnet.cpp: Efficient Edge Inference for Ternary LLMs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;11/08/2024 &lt;a href="https://arxiv.org/abs/2411.04965"&gt;BitNet a4.8: 4-bit Activations for 1-bit LLMs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/21/2024 &lt;a href="https://arxiv.org/abs/2410.16144"&gt;1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/17/2024 bitnet.cpp 1.0 released.&lt;/li&gt; 
 &lt;li&gt;03/21/2024 &lt;a href="https://github.com/microsoft/unilm/raw/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf"&gt;The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;02/27/2024 &lt;a href="https://arxiv.org/abs/2402.17764"&gt;The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/17/2023 &lt;a href="https://arxiv.org/abs/2310.11453"&gt;BitNet: Scaling 1-bit Transformers for Large Language Models&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;This project is based on the &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt; framework. We would like to thank all the authors for their contributions to the open-source community. Also, bitnet.cpp's kernels are built on top of the Lookup Table methodologies pioneered in &lt;a href="https://github.com/microsoft/T-MAC/"&gt;T-MAC&lt;/a&gt;. For inference of general low-bit LLMs beyond ternary models, we recommend using T-MAC.&lt;/p&gt; 
&lt;h2&gt;Official Models&lt;/h2&gt; 
&lt;table&gt;  
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th rowspan="2"&gt;Model&lt;/th&gt; 
   &lt;th rowspan="2"&gt;Parameters&lt;/th&gt; 
   &lt;th rowspan="2"&gt;CPU&lt;/th&gt; 
   &lt;th colspan="3"&gt;Kernel&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;I2_S&lt;/th&gt; 
   &lt;th&gt;TL1&lt;/th&gt; 
   &lt;th&gt;TL2&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;BitNet-b1.58-2B-4T&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;2.4B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Supported Models&lt;/h2&gt; 
&lt;p&gt;‚ùóÔ∏è&lt;strong&gt;We use existing 1-bit LLMs available on &lt;a href="https://huggingface.co/"&gt;Hugging Face&lt;/a&gt; to demonstrate the inference capabilities of bitnet.cpp. We hope the release of bitnet.cpp will inspire the development of 1-bit LLMs in large-scale settings in terms of model size and training tokens.&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt;  
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th rowspan="2"&gt;Model&lt;/th&gt; 
   &lt;th rowspan="2"&gt;Parameters&lt;/th&gt; 
   &lt;th rowspan="2"&gt;CPU&lt;/th&gt; 
   &lt;th colspan="3"&gt;Kernel&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;I2_S&lt;/th&gt; 
   &lt;th&gt;TL1&lt;/th&gt; 
   &lt;th&gt;TL2&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/1bitLLM/bitnet_b1_58-large"&gt;bitnet_b1_58-large&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;0.7B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/1bitLLM/bitnet_b1_58-3B"&gt;bitnet_b1_58-3B&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;3.3B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens"&gt;Llama3-8B-1.58-100B-tokens&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;8.0B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026"&gt;Falcon3 Family&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;1B-10B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/collections/tiiuae/falcon-edge-series-6804fd13344d6d8a8fa71130"&gt;Falcon-E Family&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;1B-3B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;python&amp;gt;=3.9&lt;/li&gt; 
 &lt;li&gt;cmake&amp;gt;=3.22&lt;/li&gt; 
 &lt;li&gt;clang&amp;gt;=18 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;For Windows users, install &lt;a href="https://visualstudio.microsoft.com/downloads/"&gt;Visual Studio 2022&lt;/a&gt;. In the installer, toggle on at least the following options(this also automatically installs the required additional tools like CMake):&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Desktop-development with C++&lt;/li&gt; 
     &lt;li&gt;C++-CMake Tools for Windows&lt;/li&gt; 
     &lt;li&gt;Git for Windows&lt;/li&gt; 
     &lt;li&gt;C++-Clang Compiler for Windows&lt;/li&gt; 
     &lt;li&gt;MS-Build Support for LLVM-Toolset (clang)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;For Debian/Ubuntu users, you can download with &lt;a href="https://apt.llvm.org/"&gt;Automatic installation script&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash -c "$(wget -O - https://apt.llvm.org/llvm.sh)"&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;conda (highly recommend)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Build from source&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] If you are using Windows, please remember to always use a Developer Command Prompt / PowerShell for VS2022 for the following commands. Please refer to the FAQs below if you see any issues.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone the repo&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone --recursive https://github.com/microsoft/BitNet.git
cd BitNet
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install the dependencies&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# (Recommended) Create a new conda environment
conda create -n bitnet-cpp python=3.9
conda activate bitnet-cpp

pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Build the project&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Manually download the model and run with local path
huggingface-cli download microsoft/BitNet-b1.58-2B-4T-gguf --local-dir models/BitNet-b1.58-2B-4T
python setup_env.py -md models/BitNet-b1.58-2B-4T -q i2_s

&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;
usage: setup_env.py [-h] [--hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}] [--model-dir MODEL_DIR] [--log-dir LOG_DIR] [--quant-type {i2_s,tl1}] [--quant-embd]
                    [--use-pretuned]

Setup the environment for running inference

optional arguments:
  -h, --help            show this help message and exit
  --hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}, -hr {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}
                        Model used for inference
  --model-dir MODEL_DIR, -md MODEL_DIR
                        Directory to save/load the model
  --log-dir LOG_DIR, -ld LOG_DIR
                        Directory to save the logging info
  --quant-type {i2_s,tl1}, -q {i2_s,tl1}
                        Quantization type
  --quant-embd          Quantize the embeddings to f16
  --use-pretuned, -p    Use the pretuned kernel parameters
&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Basic usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run inference with the quantized model
python run_inference.py -m models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf -p "You are a helpful assistant" -cnv
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;
usage: run_inference.py [-h] [-m MODEL] [-n N_PREDICT] -p PROMPT [-t THREADS] [-c CTX_SIZE] [-temp TEMPERATURE] [-cnv]

Run inference

optional arguments:
  -h, --help            show this help message and exit
  -m MODEL, --model MODEL
                        Path to model file
  -n N_PREDICT, --n-predict N_PREDICT
                        Number of tokens to predict when generating text
  -p PROMPT, --prompt PROMPT
                        Prompt to generate text from
  -t THREADS, --threads THREADS
                        Number of threads to use
  -c CTX_SIZE, --ctx-size CTX_SIZE
                        Size of the prompt context
  -temp TEMPERATURE, --temperature TEMPERATURE
                        Temperature, a hyperparameter that controls the randomness of the generated text
  -cnv, --conversation  Whether to enable chat mode or not (for instruct models.)
                        (When this option is turned on, the prompt specified by -p will be used as the system prompt.)
&lt;/pre&gt; 
&lt;h3&gt;Benchmark&lt;/h3&gt; 
&lt;p&gt;We provide scripts to run the inference benchmark providing a model.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;usage: e2e_benchmark.py -m MODEL [-n N_TOKEN] [-p N_PROMPT] [-t THREADS]  
   
Setup the environment for running the inference  
   
required arguments:  
  -m MODEL, --model MODEL  
                        Path to the model file. 
   
optional arguments:  
  -h, --help  
                        Show this help message and exit. 
  -n N_TOKEN, --n-token N_TOKEN  
                        Number of generated tokens. 
  -p N_PROMPT, --n-prompt N_PROMPT  
                        Prompt to generate text from. 
  -t THREADS, --threads THREADS  
                        Number of threads to use. 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Here's a brief explanation of each argument:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-m&lt;/code&gt;, &lt;code&gt;--model&lt;/code&gt;: The path to the model file. This is a required argument that must be provided when running the script.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-n&lt;/code&gt;, &lt;code&gt;--n-token&lt;/code&gt;: The number of tokens to generate during the inference. It is an optional argument with a default value of 128.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-p&lt;/code&gt;, &lt;code&gt;--n-prompt&lt;/code&gt;: The number of prompt tokens to use for generating text. This is an optional argument with a default value of 512.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-t&lt;/code&gt;, &lt;code&gt;--threads&lt;/code&gt;: The number of threads to use for running the inference. It is an optional argument with a default value of 2.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-h&lt;/code&gt;, &lt;code&gt;--help&lt;/code&gt;: Show the help message and exit. Use this argument to display usage information.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python utils/e2e_benchmark.py -m /path/to/model -n 200 -p 256 -t 4  
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command would run the inference benchmark using the model located at &lt;code&gt;/path/to/model&lt;/code&gt;, generating 200 tokens from a 256 token prompt, utilizing 4 threads.&lt;/p&gt; 
&lt;p&gt;For the model layout that do not supported by any public model, we provide scripts to generate a dummy model with the given model layout, and run the benchmark on your machine:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python utils/generate-dummy-bitnet-model.py models/bitnet_b1_58-large --outfile models/dummy-bitnet-125m.tl1.gguf --outtype tl1 --model-size 125M

# Run benchmark with the generated model, use -m to specify the model path, -p to specify the prompt processed, -n to specify the number of token to generate
python utils/e2e_benchmark.py -m models/dummy-bitnet-125m.tl1.gguf -p 512 -n 128
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Convert from &lt;code&gt;.safetensors&lt;/code&gt; Checkpoints&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Prepare the .safetensors model file
huggingface-cli download microsoft/bitnet-b1.58-2B-4T-bf16 --local-dir ./models/bitnet-b1.58-2B-4T-bf16

# Convert to gguf model
python ./utils/convert-helper-bitnet.py ./models/bitnet-b1.58-2B-4T-bf16
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;FAQ (Frequently Asked Questions)üìå&lt;/h3&gt; 
&lt;h4&gt;Q1: The build dies with errors building llama.cpp due to issues with std::chrono in log.cpp?&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; This is an issue introduced in recent version of llama.cpp. Please refer to this &lt;a href="https://github.com/tinglou/llama.cpp/commit/4e3db1e3d78cc1bcd22bcb3af54bd2a4628dd323"&gt;commit&lt;/a&gt; in the &lt;a href="https://github.com/abetlen/llama-cpp-python/issues/1942"&gt;discussion&lt;/a&gt; to fix this issue.&lt;/p&gt; 
&lt;h4&gt;Q2: How to build with clang in conda environment on windows?&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Before building the project, verify your clang installation and access to Visual Studio tools by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;clang -v
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command checks that you are using the correct version of clang and that the Visual Studio tools are available. If you see an error message such as:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;'clang' is not recognized as an internal or external command, operable program or batch file.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It indicates that your command line window is not properly initialized for Visual Studio tools.&lt;/p&gt; 
&lt;p&gt;‚Ä¢ If you are using Command Prompt, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;"C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\VsDevCmd.bat" -startdir=none -arch=x64 -host_arch=x64
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;‚Ä¢ If you are using Windows PowerShell, run the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Import-Module "C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\Microsoft.VisualStudio.DevShell.dll" Enter-VsDevShell 3f0e31ad -SkipAutomaticLocation -DevCmdArguments "-arch=x64 -host_arch=x64"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These steps will initialize your environment and allow you to use the correct Visual Studio tools.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Shubhamsaboo/awesome-llm-apps</title>
      <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
      <description>&lt;p&gt;Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="http://www.theunwindai.com"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/unwind_black.png" width="900px" alt="Unwind AI" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.linkedin.com/in/shubhamsaboo/"&gt; &lt;img src="https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;amp;style=flat-square" alt="LinkedIn" /&gt; &lt;/a&gt; &lt;a href="https://twitter.com/Saboo_Shubham_"&gt; &lt;img src="https://img.shields.io/twitter/follow/Shubham_Saboo" alt="Twitter" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=de"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=es"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=fr"&gt;fran√ßais&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ja"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ko"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=pt"&gt;Portugu√™s&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ru"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=zh"&gt;‰∏≠Êñá&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üåü Awesome LLM Apps&lt;/h1&gt; 
&lt;p&gt;A curated collection of &lt;strong&gt;Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.&lt;/strong&gt; This repository features LLM apps that use models from OpenAI, Anthropic, Google, and open-source models like DeepSeek, Qwen or Llama that you can run locally on your computer.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/9876" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/9876" alt="Shubhamsaboo%2Fawesome-llm-apps | Trendshift" style="width: 250px; height: 55px;" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;ü§î Why Awesome LLM Apps?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üí° Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.&lt;/li&gt; 
 &lt;li&gt;üî• Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp;amp; RAG.&lt;/li&gt; 
 &lt;li&gt;üéì Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìÇ Featured AI Projects&lt;/h2&gt; 
&lt;h3&gt;AI Agents&lt;/h3&gt; 
&lt;h3&gt;üå± Starter AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_blog_to_podcast_agent/"&gt;üéôÔ∏è AI Blog to Podcast Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_breakup_recovery_agent/"&gt;‚ù§Ô∏è‚Äçü©π AI Breakup Recovery Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_data_analysis_agent/"&gt;üìä AI Data Analysis Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_medical_imaging_agent/"&gt;ü©ª AI Medical Imaging Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_meme_generator_agent_browseruse/"&gt;üòÇ AI Meme Generator Agent (Browser)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_music_generator_agent/"&gt;üéµ AI Music Generator Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_travel_agent/"&gt;üõ´ AI Travel Agent (Local &amp;amp; Cloud)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/gemini_multimodal_agent_demo/"&gt;‚ú® Gemini Multimodal Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/local_news_agent_openai_swarm/"&gt;üåê Local News Agent (OpenAI Swarm)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/mixture_of_agents/"&gt;üîÑ Mixture of Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/xai_finance_agent/"&gt;üìä xAI Finance Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/opeani_research_agent/"&gt;üîç OpenAI Research Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/web_scrapping_ai_agent/"&gt;üï∏Ô∏è Web Scrapping AI Agent (Local &amp;amp; Cloud)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üöÄ Advanced AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_deep_research_agent/"&gt;üîç AI Deep Research Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_consultant_agent"&gt;ü§ù AI Consultant Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_system_architect_r1/"&gt;üèóÔ∏è AI System Architect Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_lead_generation_agent/"&gt;üéØ AI Lead Generation Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/"&gt;üí∞ AI Financial Coach Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_movie_production_agent/"&gt;üé¨ AI Movie Production Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_investment_agent/"&gt;üìà AI Investment Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/"&gt;üèãÔ∏è‚Äç‚ôÇÔ∏è AI Health &amp;amp; Fitness Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/product_launch_intelligence_agent"&gt;üöÄ AI Product Launch Intelligence Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_journalist_agent/"&gt;üóûÔ∏è AI Journalist Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/"&gt;üß† AI Mental Wellbeing Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_meeting_agent/"&gt;üìë AI Meeting Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_Self-Evolving_agent/"&gt;üß¨ AI Self-Evolving Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_news_and_podcast_agents/"&gt;üéß AI Social Media News and Podcast Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üéÆ Autonomous Game Playing Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/"&gt;üéÆ AI 3D Pygame Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/"&gt;‚ôú AI Chess Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/"&gt;üé≤ AI Tic-Tac-Toe Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ü§ù Multi-agent Teams&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/"&gt;üß≤ AI Competitor Intelligence Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/"&gt;üí≤ AI Finance Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/"&gt;üé® AI Game Design Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/"&gt;üë®‚Äç‚öñÔ∏è AI Legal Agent Team (Cloud &amp;amp; Local)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/"&gt;üíº AI Recruitment Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_real_estate_agent_team"&gt;üè† AI Real Estate Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/"&gt;üë®‚Äçüíº AI Services Agency (CrewAI)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/"&gt;üë®‚Äçüè´ AI Teaching Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/"&gt;üíª Multimodal Coding Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/"&gt;‚ú® Multimodal Design Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_travel_planner_agent_team/"&gt;üåè AI Travel Planner Agent Team&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üó£Ô∏è Voice AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/ai_audio_tour_agent/"&gt;üó£Ô∏è AI Audio Tour Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/customer_support_voice_agent/"&gt;üìû Customer Support Voice Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/voice_rag_openaisdk/"&gt;üîä Voice RAG Agent (OpenAI SDK)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üåê MCP AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/browser_mcp_agent/"&gt;‚ôæÔ∏è Browser MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/github_mcp_agent/"&gt;üêô GitHub MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/notion_mcp_agent"&gt;üìë Notion MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/ai_travel_planner_mcp_agent_team"&gt;üåç AI Travel Planner MCP Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìÄ RAG (Retrieval Augmented Generation)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/agentic_rag/"&gt;üîó Agentic RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/agentic_rag_with_reasoning/"&gt;üßê Agentic RAG with Reasoning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/ai_blog_search/"&gt;üì∞ AI Blog Search (RAG)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/autonomous_rag/"&gt;üîç Autonomous RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/corrective_rag/"&gt;üîÑ Corrective RAG (CRAG)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/deepseek_local_rag_agent/"&gt;üêã Deepseek Local RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/gemini_agentic_rag/"&gt;ü§î Gemini Agentic RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/hybrid_search_rag/"&gt;üëÄ Hybrid Search RAG (Cloud)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/llama3.1_local_rag/"&gt;üîÑ Llama 3.1 Local RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/local_hybrid_search_rag/"&gt;üñ•Ô∏è Local Hybrid Search RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/local_rag_agent/"&gt;ü¶ô Local RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag-as-a-service/"&gt;üß© RAG-as-a-Service&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_agent_cohere/"&gt;‚ú® RAG Agent with Cohere&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_chain/"&gt;‚õìÔ∏è Basic RAG Chain&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_database_routing/"&gt;üì† RAG with Database Routing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/vision_rag/"&gt;üñºÔ∏è Vision RAG&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üíæ LLM Apps with Memory Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory/"&gt;üíæ AI ArXiv Agent with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/ai_travel_agent_memory/"&gt;üõ©Ô∏è AI Travel Agent with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/llama3_stateful_chat/"&gt;üí¨ Llama3 Stateful Chat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/llm_app_personalized_memory/"&gt;üìù LLM App with Personalized Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/local_chatgpt_with_memory/"&gt;üóÑÔ∏è Local ChatGPT Clone with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/multi_llm_memory/"&gt;üß† Multi-LLM Application with Shared Memory&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üí¨ Chat with X Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_github/"&gt;üí¨ Chat with GitHub (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_gmail/"&gt;üì® Chat with Gmail&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_pdf/"&gt;üìÑ Chat with PDF (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_research_papers/"&gt;üìö Chat with Research Papers (ArXiv) (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_substack/"&gt;üìù Chat with Substack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_youtube_videos/"&gt;üìΩÔ∏è Chat with YouTube Videos&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üîß LLM Fine-tuning Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_finetuning_tutorials/llama3.2_finetuning/"&gt;üîß Llama 3.2 Fine-tuning&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üßë‚Äçüè´ AI Agent Framework Crash Course&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/ai_agent_framework_crash_course/google_adk_crash_course/"&gt;Google ADK Crash Course&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Starter agent; model‚Äëagnostic (OpenAI, Claude)&lt;/li&gt; 
   &lt;li&gt;Structured outputs (Pydantic)&lt;/li&gt; 
   &lt;li&gt;Tools: built‚Äëin, function, third‚Äëparty, MCP tools&lt;/li&gt; 
   &lt;li&gt;Memory; callbacks; Plugins&lt;/li&gt; 
   &lt;li&gt;Simple multi‚Äëagent; Multi‚Äëagent patterns&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the repository&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Navigate to the desired project directory&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd awesome-llm-apps/starter_ai_agents/ai_travel_agent
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install the required dependencies&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Follow the project-specific instructions&lt;/strong&gt; in each project's &lt;code&gt;README.md&lt;/code&gt; file to set up and run the app.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ü§ù Contributing to Open Source&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! If you have any ideas, improvements, or new apps to add, please create a new &lt;a href="https://github.com/Shubhamsaboo/awesome-llm-apps/issues"&gt;GitHub Issue&lt;/a&gt; or submit a pull request. Make sure to follow the existing project structure and include a detailed &lt;code&gt;README.md&lt;/code&gt; for each new app.&lt;/p&gt; 
&lt;h3&gt;Thank You, Community, for the Support! üôè&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;üåü &lt;strong&gt;Don‚Äôt miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>huggingface/diffusers</title>
      <link>https://github.com/huggingface/diffusers</link>
      <description>&lt;p&gt;ü§ó Diffusers: State-of-the-art diffusion models for image, video, and audio generation in PyTorch and FLAX.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;br /&gt; &lt;img src="https://raw.githubusercontent.com/huggingface/diffusers/main/docs/source/en/imgs/diffusers_library.jpg" width="400" /&gt; &lt;br /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align="center"&gt; &lt;a href="https://github.com/huggingface/diffusers/raw/main/LICENSE"&gt;&lt;img alt="GitHub" src="https://img.shields.io/github/license/huggingface/datasets.svg?color=blue" /&gt;&lt;/a&gt; &lt;a href="https://github.com/huggingface/diffusers/releases"&gt;&lt;img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/diffusers.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/diffusers"&gt;&lt;img alt="GitHub release" src="https://static.pepy.tech/badge/diffusers/month" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/huggingface/diffusers/main/CODE_OF_CONDUCT.md"&gt;&lt;img alt="Contributor Covenant" src="https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/diffuserslib"&gt;&lt;img alt="X account" src="https://img.shields.io/twitter/url/https/twitter.com/diffuserslib.svg?style=social&amp;amp;label=Follow%20%40diffuserslib" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;ü§ó Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or training your own diffusion models, ü§ó Diffusers is a modular toolbox that supports both. Our library is designed with a focus on &lt;a href="https://huggingface.co/docs/diffusers/conceptual/philosophy#usability-over-performance"&gt;usability over performance&lt;/a&gt;, &lt;a href="https://huggingface.co/docs/diffusers/conceptual/philosophy#simple-over-easy"&gt;simple over easy&lt;/a&gt;, and &lt;a href="https://huggingface.co/docs/diffusers/conceptual/philosophy#tweakable-contributorfriendly-over-abstraction"&gt;customizability over abstractions&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;ü§ó Diffusers offers three core components:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;State-of-the-art &lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/overview"&gt;diffusion pipelines&lt;/a&gt; that can be run in inference with just a few lines of code.&lt;/li&gt; 
 &lt;li&gt;Interchangeable noise &lt;a href="https://huggingface.co/docs/diffusers/api/schedulers/overview"&gt;schedulers&lt;/a&gt; for different diffusion speeds and output quality.&lt;/li&gt; 
 &lt;li&gt;Pretrained &lt;a href="https://huggingface.co/docs/diffusers/api/models/overview"&gt;models&lt;/a&gt; that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;We recommend installing ü§ó Diffusers in a virtual environment from PyPI or Conda. For more details about installing &lt;a href="https://pytorch.org/get-started/locally/"&gt;PyTorch&lt;/a&gt; and &lt;a href="https://flax.readthedocs.io/en/latest/#installation"&gt;Flax&lt;/a&gt;, please refer to their official documentation.&lt;/p&gt; 
&lt;h3&gt;PyTorch&lt;/h3&gt; 
&lt;p&gt;With &lt;code&gt;pip&lt;/code&gt; (official package):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade diffusers[torch]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With &lt;code&gt;conda&lt;/code&gt; (maintained by the community):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;conda install -c conda-forge diffusers
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Flax&lt;/h3&gt; 
&lt;p&gt;With &lt;code&gt;pip&lt;/code&gt; (official package):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade diffusers[flax]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Apple Silicon (M1/M2) support&lt;/h3&gt; 
&lt;p&gt;Please refer to the &lt;a href="https://huggingface.co/docs/diffusers/optimization/mps"&gt;How to use Stable Diffusion in Apple Silicon&lt;/a&gt; guide.&lt;/p&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;Generating outputs is super easy with ü§ó Diffusers. To generate an image from text, use the &lt;code&gt;from_pretrained&lt;/code&gt; method to load any pretrained diffusion model (browse the &lt;a href="https://huggingface.co/models?library=diffusers&amp;amp;sort=downloads"&gt;Hub&lt;/a&gt; for 30,000+ checkpoints):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from diffusers import DiffusionPipeline
import torch

pipeline = DiffusionPipeline.from_pretrained("stable-diffusion-v1-5/stable-diffusion-v1-5", torch_dtype=torch.float16)
pipeline.to("cuda")
pipeline("An image of a squirrel in Picasso style").images[0]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also dig into the models and schedulers toolbox to build your own diffusion system:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from diffusers import DDPMScheduler, UNet2DModel
from PIL import Image
import torch

scheduler = DDPMScheduler.from_pretrained("google/ddpm-cat-256")
model = UNet2DModel.from_pretrained("google/ddpm-cat-256").to("cuda")
scheduler.set_timesteps(50)

sample_size = model.config.sample_size
noise = torch.randn((1, 3, sample_size, sample_size), device="cuda")
input = noise

for t in scheduler.timesteps:
    with torch.no_grad():
        noisy_residual = model(input, t).sample
        prev_noisy_sample = scheduler.step(noisy_residual, t, input).prev_sample
        input = prev_noisy_sample

image = (input / 2 + 0.5).clamp(0, 1)
image = image.cpu().permute(0, 2, 3, 1).numpy()[0]
image = Image.fromarray((image * 255).round().astype("uint8"))
image
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Check out the &lt;a href="https://huggingface.co/docs/diffusers/quicktour"&gt;Quickstart&lt;/a&gt; to launch your diffusion journey today!&lt;/p&gt; 
&lt;h2&gt;How to navigate the documentation&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;What can I learn?&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/tutorials/tutorial_overview"&gt;Tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A basic crash course for learning how to use the library's most important features like using models and schedulers to build your own diffusion system, and training your own diffusion model.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/using-diffusers/loading"&gt;Loading&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Guides for how to load and configure all the components (pipelines, models, and schedulers) of the library, as well as how to use different schedulers.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/using-diffusers/overview_techniques"&gt;Pipelines for inference&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Guides for how to use pipelines for different inference tasks, batched generation, controlling generated outputs and randomness, and how to contribute a pipeline to the library.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/optimization/fp16"&gt;Optimization&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Guides for how to optimize your diffusion model to run faster and consume less memory.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/training/overview"&gt;Training&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Guides for how to train a diffusion model for different tasks with different training techniques.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;p&gt;We ‚ù§Ô∏è contributions from the open-source community! If you want to contribute to this library, please check out our &lt;a href="https://github.com/huggingface/diffusers/raw/main/CONTRIBUTING.md"&gt;Contribution guide&lt;/a&gt;. You can look out for &lt;a href="https://github.com/huggingface/diffusers/issues"&gt;issues&lt;/a&gt; you'd like to tackle to contribute to the library.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;See &lt;a href="https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22"&gt;Good first issues&lt;/a&gt; for general opportunities to contribute&lt;/li&gt; 
 &lt;li&gt;See &lt;a href="https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3A%22New+pipeline%2Fmodel%22"&gt;New model/pipeline&lt;/a&gt; to contribute exciting new diffusion models / diffusion pipelines&lt;/li&gt; 
 &lt;li&gt;See &lt;a href="https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3A%22New+scheduler%22"&gt;New scheduler&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Also, say üëã in our public Discord channel &lt;a href="https://discord.gg/G7tWnz98XR"&gt;&lt;img alt="Join us on Discord" src="https://img.shields.io/discord/823813159592001537?color=5865F2&amp;amp;logo=discord&amp;amp;logoColor=white" /&gt;&lt;/a&gt;. We discuss the hottest trends about diffusion models, help each other with contributions, personal projects or just hang out ‚òï.&lt;/p&gt; 
&lt;h2&gt;Popular Tasks &amp;amp; Pipelines&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th&gt;Task&lt;/th&gt; 
   &lt;th&gt;Pipeline&lt;/th&gt; 
   &lt;th&gt;ü§ó Hub&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr style="border-top: 2px solid black"&gt; 
   &lt;td&gt;Unconditional Image Generation&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/ddpm"&gt; DDPM &lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/google/ddpm-ema-church-256"&gt; google/ddpm-ema-church-256 &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="border-top: 2px solid black"&gt; 
   &lt;td&gt;Text-to-Image&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/text2img"&gt;Stable Diffusion Text-to-Image&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5"&gt; stable-diffusion-v1-5/stable-diffusion-v1-5 &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Text-to-Image&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/unclip"&gt;unCLIP&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/kakaobrain/karlo-v1-alpha"&gt; kakaobrain/karlo-v1-alpha &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Text-to-Image&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/deepfloyd_if"&gt;DeepFloyd IF&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/DeepFloyd/IF-I-XL-v1.0"&gt; DeepFloyd/IF-I-XL-v1.0 &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Text-to-Image&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/kandinsky"&gt;Kandinsky&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder"&gt; kandinsky-community/kandinsky-2-2-decoder &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="border-top: 2px solid black"&gt; 
   &lt;td&gt;Text-guided Image-to-Image&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/controlnet"&gt;ControlNet&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/lllyasviel/sd-controlnet-canny"&gt; lllyasviel/sd-controlnet-canny &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Text-guided Image-to-Image&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/pix2pix"&gt;InstructPix2Pix&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/timbrooks/instruct-pix2pix"&gt; timbrooks/instruct-pix2pix &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Text-guided Image-to-Image&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/img2img"&gt;Stable Diffusion Image-to-Image&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5"&gt; stable-diffusion-v1-5/stable-diffusion-v1-5 &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="border-top: 2px solid black"&gt; 
   &lt;td&gt;Text-guided Image Inpainting&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/inpaint"&gt;Stable Diffusion Inpainting&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/runwayml/stable-diffusion-inpainting"&gt; runwayml/stable-diffusion-inpainting &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="border-top: 2px solid black"&gt; 
   &lt;td&gt;Image Variation&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/image_variation"&gt;Stable Diffusion Image Variation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/lambdalabs/sd-image-variations-diffusers"&gt; lambdalabs/sd-image-variations-diffusers &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="border-top: 2px solid black"&gt; 
   &lt;td&gt;Super Resolution&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/upscale"&gt;Stable Diffusion Upscale&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler"&gt; stabilityai/stable-diffusion-x4-upscaler &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Super Resolution&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/latent_upscale"&gt;Stable Diffusion Latent Upscale&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/stabilityai/sd-x2-latent-upscaler"&gt; stabilityai/sd-x2-latent-upscaler &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Popular libraries using üß® Diffusers&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/TaskMatrix"&gt;https://github.com/microsoft/TaskMatrix&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/invoke-ai/InvokeAI"&gt;https://github.com/invoke-ai/InvokeAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/InstantID/InstantID"&gt;https://github.com/InstantID/InstantID&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/apple/ml-stable-diffusion"&gt;https://github.com/apple/ml-stable-diffusion&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Sanster/lama-cleaner"&gt;https://github.com/Sanster/lama-cleaner&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/IDEA-Research/Grounded-Segment-Anything"&gt;https://github.com/IDEA-Research/Grounded-Segment-Anything&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ashawkey/stable-dreamfusion"&gt;https://github.com/ashawkey/stable-dreamfusion&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/deep-floyd/IF"&gt;https://github.com/deep-floyd/IF&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/bentoml/BentoML"&gt;https://github.com/bentoml/BentoML&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/bmaltais/kohya_ss"&gt;https://github.com/bmaltais/kohya_ss&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;+14,000 other amazing GitHub repositories üí™&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Thank you for using us ‚ù§Ô∏è.&lt;/p&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;p&gt;This library concretizes previous work by many different authors and would not have been possible without their great research and implementations. We'd like to thank, in particular, the following implementations which have helped us in our development and without which the API could not have been as polished today:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;@CompVis' latent diffusion models library, available &lt;a href="https://github.com/CompVis/latent-diffusion"&gt;here&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;@hojonathanho original DDPM implementation, available &lt;a href="https://github.com/hojonathanho/diffusion"&gt;here&lt;/a&gt; as well as the extremely useful translation into PyTorch by @pesser, available &lt;a href="https://github.com/pesser/pytorch_diffusion"&gt;here&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;@ermongroup's DDIM implementation, available &lt;a href="https://github.com/ermongroup/ddim"&gt;here&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;@yang-song's Score-VE and Score-VP implementations, available &lt;a href="https://github.com/yang-song/score_sde_pytorch"&gt;here&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We also want to thank @heejkoo for the very helpful overview of papers, code and resources on diffusion models, available &lt;a href="https://github.com/heejkoo/Awesome-Diffusion-Models"&gt;here&lt;/a&gt; as well as @crowsonkb and @rromb for useful discussions and insights.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{von-platen-etal-2022-diffusers,
  author = {Patrick von Platen and Suraj Patil and Anton Lozhkov and Pedro Cuenca and Nathan Lambert and Kashif Rasul and Mishig Davaadorj and Dhruv Nair and Sayak Paul and William Berman and Yiyi Xu and Steven Liu and Thomas Wolf},
  title = {Diffusers: State-of-the-art diffusion models},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/huggingface/diffusers}}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>livekit/agents</title>
      <link>https://github.com/livekit/agents</link>
      <description>&lt;p&gt;A powerful framework for building realtime voice AI agents ü§ñüéôÔ∏èüìπ&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="/.github/banner_dark.png" /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="/.github/banner_light.png" /&gt; 
 &lt;img style="width:100%;" alt="The LiveKit icon, the name of the repository and some sample code in the background." src="https://raw.githubusercontent.com/livekit/agents/main/.github/banner_light.png" /&gt; 
&lt;/picture&gt; 
&lt;!--END_BANNER_IMAGE--&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/pypi/v/livekit-agents" alt="PyPI - Version" /&gt; &lt;a href="https://pepy.tech/projects/livekit-agents"&gt;&lt;img src="https://static.pepy.tech/badge/livekit-agents/month" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://livekit.io/join-slack"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Flivekit.io%2Fbadges%2Fslack" alt="Slack community" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/livekit"&gt;&lt;img src="https://img.shields.io/twitter/follow/livekit" alt="Twitter Follow" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/livekit/agents"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki for understanding the codebase" /&gt;&lt;/a&gt; &lt;a href="https://github.com/livekit/livekit/raw/master/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/livekit/livekit" alt="License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;Looking for the JS/TS library? Check out &lt;a href="https://github.com/livekit/agents-js"&gt;AgentsJS&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What is Agents?&lt;/h2&gt; 
&lt;!--BEGIN_DESCRIPTION--&gt; 
&lt;p&gt;The Agent Framework is designed for building realtime, programmable participants that run on servers. Use it to create conversational, multi-modal voice agents that can see, hear, and understand.&lt;/p&gt; 
&lt;!--END_DESCRIPTION--&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible integrations&lt;/strong&gt;: A comprehensive ecosystem to mix and match the right STT, LLM, TTS, and Realtime API to suit your use case.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Integrated job scheduling&lt;/strong&gt;: Built-in task scheduling and distribution with &lt;a href="https://docs.livekit.io/agents/build/dispatch/"&gt;dispatch APIs&lt;/a&gt; to connect end users to agents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensive WebRTC clients&lt;/strong&gt;: Build client applications using LiveKit's open-source SDK ecosystem, supporting all major platforms.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Telephony integration&lt;/strong&gt;: Works seamlessly with LiveKit's &lt;a href="https://docs.livekit.io/sip/"&gt;telephony stack&lt;/a&gt;, allowing your agent to make calls to or receive calls from phones.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Exchange data with clients&lt;/strong&gt;: Use &lt;a href="https://docs.livekit.io/home/client/data/rpc/"&gt;RPCs&lt;/a&gt; and other &lt;a href="https://docs.livekit.io/home/client/data/"&gt;Data APIs&lt;/a&gt; to seamlessly exchange data with clients.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Semantic turn detection&lt;/strong&gt;: Uses a transformer model to detect when a user is done with their turn, helps to reduce interruptions.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP support&lt;/strong&gt;: Native support for MCP. Integrate tools provided by MCP servers with one loc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Builtin test framework&lt;/strong&gt;: Write tests and use judges to ensure your agent is performing as expected.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open-source&lt;/strong&gt;: Fully open-source, allowing you to run the entire stack on your own servers, including &lt;a href="https://github.com/livekit/livekit"&gt;LiveKit server&lt;/a&gt;, one of the most widely used WebRTC media servers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install the core Agents library, along with plugins for popular model providers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "livekit-agents[openai,silero,deepgram,cartesia,turn-detector]~=1.0"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Docs and guides&lt;/h2&gt; 
&lt;p&gt;Documentation on the framework and how to use it can be found &lt;a href="https://docs.livekit.io/agents/"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Core concepts&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Agent: An LLM-based application with defined instructions.&lt;/li&gt; 
 &lt;li&gt;AgentSession: A container for agents that manages interactions with end users.&lt;/li&gt; 
 &lt;li&gt;entrypoint: The starting point for an interactive session, similar to a request handler in a web server.&lt;/li&gt; 
 &lt;li&gt;Worker: The main process that coordinates job scheduling and launches agents for user sessions.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Simple voice agent&lt;/h3&gt; 
&lt;hr /&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from livekit.agents import (
    Agent,
    AgentSession,
    JobContext,
    RunContext,
    WorkerOptions,
    cli,
    function_tool,
)
from livekit.plugins import deepgram, elevenlabs, openai, silero

@function_tool
async def lookup_weather(
    context: RunContext,
    location: str,
):
    """Used to look up weather information."""

    return {"weather": "sunny", "temperature": 70}


async def entrypoint(ctx: JobContext):
    await ctx.connect()

    agent = Agent(
        instructions="You are a friendly voice assistant built by LiveKit.",
        tools=[lookup_weather],
    )
    session = AgentSession(
        vad=silero.VAD.load(),
        # any combination of STT, LLM, TTS, or realtime API can be used
        stt=deepgram.STT(model="nova-3"),
        llm=openai.LLM(model="gpt-4o-mini"),
        tts=elevenlabs.TTS(),
    )

    await session.start(agent=agent, room=ctx.room)
    await session.generate_reply(instructions="greet the user and ask about their day")


if __name__ == "__main__":
    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You'll need the following environment variables for this example:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;DEEPGRAM_API_KEY&lt;/li&gt; 
 &lt;li&gt;OPENAI_API_KEY&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Multi-agent handoff&lt;/h3&gt; 
&lt;hr /&gt; 
&lt;p&gt;This code snippet is abbreviated. For the full example, see &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/multi_agent.py"&gt;multi_agent.py&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;...
class IntroAgent(Agent):
    def __init__(self) -&amp;gt; None:
        super().__init__(
            instructions=f"You are a story teller. Your goal is to gather a few pieces of information from the user to make the story personalized and engaging."
            "Ask the user for their name and where they are from"
        )

    async def on_enter(self):
        self.session.generate_reply(instructions="greet the user and gather information")

    @function_tool
    async def information_gathered(
        self,
        context: RunContext,
        name: str,
        location: str,
    ):
        """Called when the user has provided the information needed to make the story personalized and engaging.

        Args:
            name: The name of the user
            location: The location of the user
        """

        context.userdata.name = name
        context.userdata.location = location

        story_agent = StoryAgent(name, location)
        return story_agent, "Let's start the story!"


class StoryAgent(Agent):
    def __init__(self, name: str, location: str) -&amp;gt; None:
        super().__init__(
            instructions=f"You are a storyteller. Use the user's information in order to make the story personalized."
            f"The user's name is {name}, from {location}"
            # override the default model, switching to Realtime API from standard LLMs
            llm=openai.realtime.RealtimeModel(voice="echo"),
            chat_ctx=chat_ctx,
        )

    async def on_enter(self):
        self.session.generate_reply()


async def entrypoint(ctx: JobContext):
    await ctx.connect()

    userdata = StoryData()
    session = AgentSession[StoryData](
        vad=silero.VAD.load(),
        stt=deepgram.STT(model="nova-3"),
        llm=openai.LLM(model="gpt-4o-mini"),
        tts=openai.TTS(voice="echo"),
        userdata=userdata,
    )

    await session.start(
        agent=IntroAgent(),
        room=ctx.room,
    )
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Testing&lt;/h3&gt; 
&lt;p&gt;Automated tests are essential for building reliable agents, especially with the non-deterministic behavior of LLMs. LiveKit Agents include native test integration to help you create dependable agents.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@pytest.mark.asyncio
async def test_no_availability() -&amp;gt; None:
    llm = google.LLM()
    async AgentSession(llm=llm) as sess:
        await sess.start(MyAgent())
        result = await sess.run(
            user_input="Hello, I need to place an order."
        )
        result.expect.skip_next_event_if(type="message", role="assistant")
        result.expect.next_event().is_function_call(name="start_order")
        result.expect.next_event().is_function_call_output()
        await (
            result.expect.next_event()
            .is_message(role="assistant")
            .judge(llm, intent="assistant should be asking the user what they would like")
        )

&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üéôÔ∏è Starter Agent&lt;/h3&gt; &lt;p&gt;A starter agent optimized for voice conversations.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/basic_agent.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üîÑ Multi-user push to talk&lt;/h3&gt; &lt;p&gt;Responds to multiple users in the room via push-to-talk.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/push_to_talk.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üéµ Background audio&lt;/h3&gt; &lt;p&gt;Background ambient and thinking audio to improve realism.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/background_audio.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üõ†Ô∏è Dynamic tool creation&lt;/h3&gt; &lt;p&gt;Creating function tools dynamically.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/dynamic_tool_creation.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;‚òéÔ∏è Outbound caller&lt;/h3&gt; &lt;p&gt;Agent that makes outbound phone calls&lt;/p&gt; &lt;p&gt; &lt;a href="https://github.com/livekit-examples/outbound-caller-python"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üìã Structured output&lt;/h3&gt; &lt;p&gt;Using structured output from LLM to guide TTS tone.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/structured_output.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üîå MCP support&lt;/h3&gt; &lt;p&gt;Use tools from MCP servers&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/mcp"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üí¨ Text-only agent&lt;/h3&gt; &lt;p&gt;Skip voice altogether and use the same code for text-only integrations&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/other/text_only.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üìù Multi-user transcriber&lt;/h3&gt; &lt;p&gt;Produce transcriptions from all users in the room&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/other/transcription/multi-user-transcriber.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üé• Video avatars&lt;/h3&gt; &lt;p&gt;Add an AI avatar with Tavus, Beyond Presence, and Bithuman&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/avatar_agents/"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üçΩÔ∏è Restaurant ordering and reservations&lt;/h3&gt; &lt;p&gt;Full example of an agent that handles calls for a restaurant.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/restaurant_agent.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üëÅÔ∏è Gemini Live vision&lt;/h3&gt; &lt;p&gt;Full example (including iOS app) of Gemini Live agent that can see.&lt;/p&gt; &lt;p&gt; &lt;a href="https://github.com/livekit-examples/vision-demo"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Running your agent&lt;/h2&gt; 
&lt;h3&gt;Testing in terminal&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python myagent.py console
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Runs your agent in terminal mode, enabling local audio input and output for testing. This mode doesn't require external servers or dependencies and is useful for quickly validating behavior.&lt;/p&gt; 
&lt;h3&gt;Developing with LiveKit clients&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python myagent.py dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Starts the agent server and enables hot reloading when files change. This mode allows each process to host multiple concurrent agents efficiently.&lt;/p&gt; 
&lt;p&gt;The agent connects to LiveKit Cloud or your self-hosted server. Set the following environment variables:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;LIVEKIT_URL&lt;/li&gt; 
 &lt;li&gt;LIVEKIT_API_KEY&lt;/li&gt; 
 &lt;li&gt;LIVEKIT_API_SECRET&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can connect using any LiveKit client SDK or telephony integration. To get started quickly, try the &lt;a href="https://agents-playground.livekit.io/"&gt;Agents Playground&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Running for production&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python myagent.py start
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Runs the agent with production-ready optimizations.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;The Agents framework is under active development in a rapidly evolving field. We welcome and appreciate contributions of any kind, be it feedback, bugfixes, features, new plugins and tools, or better documentation. You can file issues under this repo, open a PR, or chat with us in LiveKit's &lt;a href="https://livekit.io/join-slack"&gt;Slack community&lt;/a&gt;.&lt;/p&gt; 
&lt;!--BEGIN_REPO_NAV--&gt; 
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;table&gt; 
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th colspan="2"&gt;LiveKit Ecosystem&lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt;
   &lt;td&gt;LiveKit SDKs&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://github.com/livekit/client-sdk-js"&gt;Browser&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-swift"&gt;iOS/macOS/visionOS&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-android"&gt;Android&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-flutter"&gt;Flutter&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-react-native"&gt;React Native&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/rust-sdks"&gt;Rust&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/node-sdks"&gt;Node.js&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/python-sdks"&gt;Python&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-unity"&gt;Unity&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-unity-web"&gt;Unity (WebGL)&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-esp32"&gt;ESP32&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Server APIs&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://github.com/livekit/node-sdks"&gt;Node.js&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/server-sdk-go"&gt;Golang&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/server-sdk-ruby"&gt;Ruby&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/server-sdk-kotlin"&gt;Java/Kotlin&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/python-sdks"&gt;Python&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/rust-sdks"&gt;Rust&lt;/a&gt; ¬∑ &lt;a href="https://github.com/agence104/livekit-server-sdk-php"&gt;PHP (community)&lt;/a&gt; ¬∑ &lt;a href="https://github.com/pabloFuente/livekit-server-sdk-dotnet"&gt;.NET (community)&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;UI Components&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://github.com/livekit/components-js"&gt;React&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/components-android"&gt;Android Compose&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/components-swift"&gt;SwiftUI&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/components-flutter"&gt;Flutter&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Agents Frameworks&lt;/td&gt;
   &lt;td&gt;&lt;b&gt;Python&lt;/b&gt; ¬∑ &lt;a href="https://github.com/livekit/agents-js"&gt;Node.js&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/agent-playground"&gt;Playground&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Services&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://github.com/livekit/livekit"&gt;LiveKit server&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/egress"&gt;Egress&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/ingress"&gt;Ingress&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/sip"&gt;SIP&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Resources&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://docs.livekit.io"&gt;Docs&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit-examples"&gt;Example apps&lt;/a&gt; ¬∑ &lt;a href="https://livekit.io/cloud"&gt;Cloud&lt;/a&gt; ¬∑ &lt;a href="https://docs.livekit.io/home/self-hosting/deployment"&gt;Self-hosting&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/livekit-cli"&gt;CLI&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!--END_REPO_NAV--&gt;</description>
    </item>
    
    <item>
      <title>JaidedAI/EasyOCR</title>
      <link>https://github.com/JaidedAI/EasyOCR</link>
      <description>&lt;p&gt;Ready-to-use OCR with 80+ supported languages and all popular writing scripts including Latin, Chinese, Arabic, Devanagari, Cyrillic and etc.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;EasyOCR&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://badge.fury.io/py/easyocr"&gt;&lt;img src="https://badge.fury.io/py/easyocr.svg?sanitize=true" alt="PyPI Status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/JaidedAI/EasyOCR/raw/master/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true" alt="license" /&gt;&lt;/a&gt; &lt;a href="https://colab.to/easyocr"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open in Colab" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/intent/tweet?text=Check%20out%20this%20awesome%20library:%20EasyOCR%20https://github.com/JaidedAI/EasyOCR"&gt;&lt;img src="https://img.shields.io/twitter/url/https/github.com/JaidedAI/EasyOCR.svg?style=social" alt="Tweet" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/JaidedAI"&gt;&lt;img src="https://img.shields.io/badge/twitter-@JaidedAI-blue.svg?style=flat" alt="Twitter" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Ready-to-use OCR with 80+ &lt;a href="https://www.jaided.ai/easyocr"&gt;supported languages&lt;/a&gt; and all popular writing scripts including: Latin, Chinese, Arabic, Devanagari, Cyrillic, etc.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.jaided.ai/easyocr"&gt;Try Demo on our website&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Integrated into &lt;a href="https://huggingface.co/spaces"&gt;Huggingface Spaces ü§ó&lt;/a&gt; using &lt;a href="https://github.com/gradio-app/gradio"&gt;Gradio&lt;/a&gt;. Try out the Web Demo: &lt;a href="https://huggingface.co/spaces/tomofi/EasyOCR"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue" alt="Hugging Face Spaces" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What's new&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;24 September 2024 - Version 1.7.2&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fix several compatibilities&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/JaidedAI/EasyOCR/raw/master/releasenotes.md"&gt;Read all release notes&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What's coming next&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Handwritten text support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/JaidedAI/EasyOCR/master/examples/example.png" alt="example" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/JaidedAI/EasyOCR/master/examples/example2.png" alt="example2" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/JaidedAI/EasyOCR/master/examples/example3.png" alt="example3" /&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Install using &lt;code&gt;pip&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;For the latest stable release:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install easyocr
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For the latest development release:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install git+https://github.com/JaidedAI/EasyOCR.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note 1: For Windows, please install torch and torchvision first by following the official instructions here &lt;a href="https://pytorch.org"&gt;https://pytorch.org&lt;/a&gt;. On the pytorch website, be sure to select the right CUDA version you have. If you intend to run on CPU mode only, select &lt;code&gt;CUDA = None&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Note 2: We also provide a Dockerfile &lt;a href="https://github.com/JaidedAI/EasyOCR/raw/master/Dockerfile"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import easyocr
reader = easyocr.Reader(['ch_sim','en']) # this needs to run only once to load the model into memory
result = reader.readtext('chinese.jpg')
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The output will be in a list format, each item represents a bounding box, the text detected and confident level, respectively.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;[([[189, 75], [469, 75], [469, 165], [189, 165]], 'ÊÑöÂõ≠Ë∑Ø', 0.3754989504814148),
 ([[86, 80], [134, 80], [134, 128], [86, 128]], 'Ë•ø', 0.40452659130096436),
 ([[517, 81], [565, 81], [565, 123], [517, 123]], '‰∏ú', 0.9989598989486694),
 ([[78, 126], [136, 126], [136, 156], [78, 156]], '315', 0.8125889301300049),
 ([[514, 126], [574, 126], [574, 156], [514, 156]], '309', 0.4971577227115631),
 ([[226, 170], [414, 170], [414, 220], [226, 220]], 'Yuyuan Rd.', 0.8261902332305908),
 ([[79, 173], [125, 173], [125, 213], [79, 213]], 'W', 0.9848111271858215),
 ([[529, 173], [569, 173], [569, 213], [529, 213]], 'E', 0.8405593633651733)]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note 1: &lt;code&gt;['ch_sim','en']&lt;/code&gt; is the list of languages you want to read. You can pass several languages at once but not all languages can be used together. English is compatible with every language and languages that share common characters are usually compatible with each other.&lt;/p&gt; 
&lt;p&gt;Note 2: Instead of the filepath &lt;code&gt;chinese.jpg&lt;/code&gt;, you can also pass an OpenCV image object (numpy array) or an image file as bytes. A URL to a raw image is also acceptable.&lt;/p&gt; 
&lt;p&gt;Note 3: The line &lt;code&gt;reader = easyocr.Reader(['ch_sim','en'])&lt;/code&gt; is for loading a model into memory. It takes some time but it needs to be run only once.&lt;/p&gt; 
&lt;p&gt;You can also set &lt;code&gt;detail=0&lt;/code&gt; for simpler output.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;reader.readtext('chinese.jpg', detail = 0)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Result:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;['ÊÑöÂõ≠Ë∑Ø', 'Ë•ø', '‰∏ú', '315', '309', 'Yuyuan Rd.', 'W', 'E']
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Model weights for the chosen language will be automatically downloaded or you can download them manually from the &lt;a href="https://www.jaided.ai/easyocr/modelhub"&gt;model hub&lt;/a&gt; and put them in the '~/.EasyOCR/model' folder&lt;/p&gt; 
&lt;p&gt;In case you do not have a GPU, or your GPU has low memory, you can run the model in CPU-only mode by adding &lt;code&gt;gpu=False&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;reader = easyocr.Reader(['ch_sim','en'], gpu=False)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more information, read the &lt;a href="https://www.jaided.ai/easyocr/tutorial"&gt;tutorial&lt;/a&gt; and &lt;a href="https://www.jaided.ai/easyocr/documentation"&gt;API Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Run on command line&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;$ easyocr -l ch_sim en -f chinese.jpg --detail=1 --gpu=True
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Train/use your own model&lt;/h2&gt; 
&lt;p&gt;For recognition model, &lt;a href="https://github.com/JaidedAI/EasyOCR/raw/master/custom_model.md"&gt;Read here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For detection model (CRAFT), &lt;a href="https://github.com/JaidedAI/EasyOCR/raw/master/trainer/craft/README.md"&gt;Read here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Implementation Roadmap&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Handwritten support&lt;/li&gt; 
 &lt;li&gt;Restructure code to support swappable detection and recognition algorithms The api should be as easy as&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;reader = easyocr.Reader(['en'], detection='DB', recognition = 'Transformer')
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The idea is to be able to plug in any state-of-the-art model into EasyOCR. There are a lot of geniuses trying to make better detection/recognition models, but we are not trying to be geniuses here. We just want to make their works quickly accessible to the public ... for free. (well, we believe most geniuses want their work to create a positive impact as fast/big as possible) The pipeline should be something like the below diagram. Grey slots are placeholders for changeable light blue modules.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/JaidedAI/EasyOCR/master/examples/easyocr_framework.jpeg" alt="plan" /&gt;&lt;/p&gt; 
&lt;h2&gt;Acknowledgement and References&lt;/h2&gt; 
&lt;p&gt;This project is based on research and code from several papers and open-source repositories.&lt;/p&gt; 
&lt;p&gt;All deep learning execution is based on &lt;a href="https://pytorch.org"&gt;Pytorch&lt;/a&gt;. &lt;span&gt;‚ù§Ô∏è&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;Detection execution uses the CRAFT algorithm from this &lt;a href="https://github.com/clovaai/CRAFT-pytorch"&gt;official repository&lt;/a&gt; and their &lt;a href="https://arxiv.org/abs/1904.01941"&gt;paper&lt;/a&gt; (Thanks @YoungminBaek from &lt;a href="https://github.com/clovaai"&gt;@clovaai&lt;/a&gt;). We also use their pretrained model. Training script is provided by &lt;a href="https://github.com/gmuffiness"&gt;@gmuffiness&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The recognition model is a CRNN (&lt;a href="https://arxiv.org/abs/1507.05717"&gt;paper&lt;/a&gt;). It is composed of 3 main components: feature extraction (we are currently using &lt;a href="https://arxiv.org/abs/1512.03385"&gt;Resnet&lt;/a&gt;) and VGG, sequence labeling (&lt;a href="https://www.bioinf.jku.at/publications/older/2604.pdf"&gt;LSTM&lt;/a&gt;) and decoding (&lt;a href="https://www.cs.toronto.edu/~graves/icml_2006.pdf"&gt;CTC&lt;/a&gt;). The training pipeline for recognition execution is a modified version of the &lt;a href="https://github.com/clovaai/deep-text-recognition-benchmark"&gt;deep-text-recognition-benchmark&lt;/a&gt; framework. (Thanks &lt;a href="https://github.com/ku21fan"&gt;@ku21fan&lt;/a&gt; from &lt;a href="https://github.com/clovaai"&gt;@clovaai&lt;/a&gt;) This repository is a gem that deserves more recognition.&lt;/p&gt; 
&lt;p&gt;Beam search code is based on this &lt;a href="https://github.com/githubharald/CTCDecoder"&gt;repository&lt;/a&gt; and his &lt;a href="https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7"&gt;blog&lt;/a&gt;. (Thanks &lt;a href="https://github.com/githubharald"&gt;@githubharald&lt;/a&gt;)&lt;/p&gt; 
&lt;p&gt;Data synthesis is based on &lt;a href="https://github.com/Belval/TextRecognitionDataGenerator"&gt;TextRecognitionDataGenerator&lt;/a&gt;. (Thanks &lt;a href="https://github.com/Belval"&gt;@Belval&lt;/a&gt;)&lt;/p&gt; 
&lt;p&gt;And a good read about CTC from distill.pub &lt;a href="https://distill.pub/2017/ctc/"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Want To Contribute?&lt;/h2&gt; 
&lt;p&gt;Let's advance humanity together by making AI available to everyone!&lt;/p&gt; 
&lt;p&gt;3 ways to contribute:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Coder:&lt;/strong&gt; Please send a PR for small bugs/improvements. For bigger ones, discuss with us by opening an issue first. There is a list of possible bug/improvement issues tagged with &lt;a href="https://github.com/JaidedAI/EasyOCR/issues?q=is%3Aissue+is%3Aopen+label%3A%22PR+WELCOME%22"&gt;'PR WELCOME'&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;User:&lt;/strong&gt; Tell us how EasyOCR benefits you/your organization to encourage further development. Also post failure cases in &lt;a href="https://github.com/JaidedAI/EasyOCR/issues"&gt;Issue Section&lt;/a&gt; to help improve future models.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Tech leader/Guru:&lt;/strong&gt; If you found this library useful, please spread the word! (See &lt;a href="https://www.facebook.com/yann.lecun/posts/10157018122787143"&gt;Yann Lecun's post&lt;/a&gt; about EasyOCR)&lt;/p&gt; 
&lt;h2&gt;Guideline for new language request&lt;/h2&gt; 
&lt;p&gt;To request a new language, we need you to send a PR with the 2 following files:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;In folder &lt;a href="https://github.com/JaidedAI/EasyOCR/tree/master/easyocr/character"&gt;easyocr/character&lt;/a&gt;, we need 'yourlanguagecode_char.txt' that contains list of all characters. Please see format examples from other files in that folder.&lt;/li&gt; 
 &lt;li&gt;In folder &lt;a href="https://github.com/JaidedAI/EasyOCR/tree/master/easyocr/dict"&gt;easyocr/dict&lt;/a&gt;, we need 'yourlanguagecode.txt' that contains list of words in your language. On average, we have ~30000 words per language with more than 50000 words for more popular ones. More is better in this file.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;If your language has unique elements (such as 1. Arabic: characters change form when attached to each other + write from right to left 2. Thai: Some characters need to be above the line and some below), please educate us to the best of your ability and/or give useful links. It is important to take care of the detail to achieve a system that really works.&lt;/p&gt; 
&lt;p&gt;Lastly, please understand that our priority will have to go to popular languages or sets of languages that share large portions of their characters with each other (also tell us if this is the case for your language). It takes us at least a week to develop a new model, so you may have to wait a while for the new model to be released.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://github.com/JaidedAI/EasyOCR/issues/91"&gt;List of languages in development&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Github Issues&lt;/h2&gt; 
&lt;p&gt;Due to limited resources, an issue older than 6 months will be automatically closed. Please open an issue again if it is critical.&lt;/p&gt; 
&lt;h2&gt;Business Inquiries&lt;/h2&gt; 
&lt;p&gt;For Enterprise Support, &lt;a href="https://www.jaided.ai/"&gt;Jaided AI&lt;/a&gt; offers full service for custom OCR/AI systems from implementation, training/finetuning and deployment. Click &lt;a href="https://www.jaided.ai/contactus?ref=github"&gt;here&lt;/a&gt; to contact us.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/autogen</title>
      <link>https://github.com/microsoft/autogen</link>
      <description>&lt;p&gt;A programming framework for agentic AI ü§ñ PyPi: autogen-agentchat Discord: https://aka.ms/autogen-discord Office Hour: https://aka.ms/autogen-officehour&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://microsoft.github.io/autogen/0.2/img/ag.svg?sanitize=true" alt="AutoGen Logo" width="100" /&gt; 
 &lt;p&gt;&lt;a href="https://twitter.com/pyautogen"&gt;&lt;img src="https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&amp;amp;label=Follow%20%40pyautogen" alt="Twitter" /&gt;&lt;/a&gt; &lt;a href="https://www.linkedin.com/company/105812540"&gt;&lt;img src="https://img.shields.io/badge/LinkedIn-Company?style=flat&amp;amp;logo=linkedin&amp;amp;logoColor=white" alt="LinkedIn" /&gt;&lt;/a&gt; &lt;a href="https://aka.ms/autogen-discord"&gt;&lt;img src="https://img.shields.io/badge/discord-chat-green?logo=discord" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://microsoft.github.io/autogen/"&gt;&lt;img src="https://img.shields.io/badge/Documentation-AutoGen-blue?logo=read-the-docs" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://devblogs.microsoft.com/autogen/"&gt;&lt;img src="https://img.shields.io/badge/Blog-AutoGen-blue?logo=blogger" alt="Blog" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h1&gt;AutoGen&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;AutoGen&lt;/strong&gt; is a framework for creating multi-agent AI applications that can act autonomously or work alongside humans.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;AutoGen requires &lt;strong&gt;Python 3.10 or later&lt;/strong&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install AgentChat and OpenAI client from Extensions
pip install -U "autogen-agentchat" "autogen-ext[openai]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The current stable version can be found in the &lt;a href="https://github.com/microsoft/autogen/releases"&gt;releases&lt;/a&gt;. If you are upgrading from AutoGen v0.2, please refer to the &lt;a href="https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/migration-guide.html"&gt;Migration Guide&lt;/a&gt; for detailed instructions on how to update your code and configurations.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install AutoGen Studio for no-code GUI
pip install -U "autogenstudio"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;h3&gt;Hello World&lt;/h3&gt; 
&lt;p&gt;Create an assistant agent using OpenAI's GPT-4o model. See &lt;a href="https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/models.html"&gt;other supported models&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main() -&amp;gt; None:
    model_client = OpenAIChatCompletionClient(model="gpt-4.1")
    agent = AssistantAgent("assistant", model_client=model_client)
    print(await agent.run(task="Say 'Hello World!'"))
    await model_client.close()

asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;MCP Server&lt;/h3&gt; 
&lt;p&gt;Create a web browsing assistant agent that uses the Playwright MCP server.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# First run `npm install -g @playwright/mcp@latest` to install the MCP server.
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import McpWorkbench, StdioServerParams


async def main() -&amp;gt; None:
    model_client = OpenAIChatCompletionClient(model="gpt-4.1")
    server_params = StdioServerParams(
        command="npx",
        args=[
            "@playwright/mcp@latest",
            "--headless",
        ],
    )
    async with McpWorkbench(server_params) as mcp:
        agent = AssistantAgent(
            "web_browsing_assistant",
            model_client=model_client,
            workbench=mcp, # For multiple MCP servers, put them in a list.
            model_client_stream=True,
            max_tool_iterations=10,
        )
        await Console(agent.run_stream(task="Find out how many contributors for the microsoft/autogen repository"))


asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: Only connect to trusted MCP servers as they may execute commands in your local environment or expose sensitive information.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Multi-Agent Orchestration&lt;/h3&gt; 
&lt;p&gt;You can use &lt;code&gt;AgentTool&lt;/code&gt; to create a basic multi-agent orchestration setup.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.tools import AgentTool
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -&amp;gt; None:
    model_client = OpenAIChatCompletionClient(model="gpt-4.1")

    math_agent = AssistantAgent(
        "math_expert",
        model_client=model_client,
        system_message="You are a math expert.",
        description="A math expert assistant.",
        model_client_stream=True,
    )
    math_agent_tool = AgentTool(math_agent, return_value_as_last_message=True)

    chemistry_agent = AssistantAgent(
        "chemistry_expert",
        model_client=model_client,
        system_message="You are a chemistry expert.",
        description="A chemistry expert assistant.",
        model_client_stream=True,
    )
    chemistry_agent_tool = AgentTool(chemistry_agent, return_value_as_last_message=True)

    agent = AssistantAgent(
        "assistant",
        system_message="You are a general assistant. Use expert tools when needed.",
        model_client=model_client,
        model_client_stream=True,
        tools=[math_agent_tool, chemistry_agent_tool],
        max_tool_iterations=10,
    )
    await Console(agent.run_stream(task="What is the integral of x^2?"))
    await Console(agent.run_stream(task="What is the molecular weight of water?"))


asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more advanced multi-agent orchestrations and workflows, read &lt;a href="https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/index.html"&gt;AgentChat documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;AutoGen Studio&lt;/h3&gt; 
&lt;p&gt;Use AutoGen Studio to prototype and run multi-agent workflows without writing code.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run AutoGen Studio on http://localhost:8080
autogenstudio ui --port 8080 --appdir ./my-app
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Why Use AutoGen?&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/microsoft/autogen/main/autogen-landing.jpg" alt="AutoGen Landing" width="500" /&gt; 
&lt;/div&gt; 
&lt;p&gt;The AutoGen ecosystem provides everything you need to create AI agents, especially multi-agent workflows -- framework, developer tools, and applications.&lt;/p&gt; 
&lt;p&gt;The &lt;em&gt;framework&lt;/em&gt; uses a layered and extensible design. Layers have clearly divided responsibilities and build on top of layers below. This design enables you to use the framework at different levels of abstraction, from high-level APIs to low-level components.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/autogen/main/python/packages/autogen-core/"&gt;Core API&lt;/a&gt; implements message passing, event-driven agents, and local and distributed runtime for flexibility and power. It also support cross-language support for .NET and Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/autogen/main/python/packages/autogen-agentchat/"&gt;AgentChat API&lt;/a&gt; implements a simpler but opinionated&amp;nbsp;API for rapid prototyping. This API is built on top of the Core API and is closest to what users of v0.2 are familiar with and supports common multi-agent patterns such as two-agent chat or group chats.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/autogen/main/python/packages/autogen-ext/"&gt;Extensions API&lt;/a&gt; enables first- and third-party extensions continuously expanding framework capabilities. It support specific implementation of LLM clients (e.g., OpenAI, AzureOpenAI), and capabilities such as code execution.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The ecosystem also supports two essential &lt;em&gt;developer tools&lt;/em&gt;:&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://media.githubusercontent.com/media/microsoft/autogen/refs/heads/main/python/packages/autogen-studio/docs/ags_screen.png" alt="AutoGen Studio Screenshot" width="500" /&gt; 
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/autogen/main/python/packages/autogen-studio/"&gt;AutoGen Studio&lt;/a&gt; provides a no-code GUI for building multi-agent applications.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/autogen/main/python/packages/agbench/"&gt;AutoGen Bench&lt;/a&gt; provides a benchmarking suite for evaluating agent performance.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can use the AutoGen framework and developer tools to create applications for your domain. For example, &lt;a href="https://raw.githubusercontent.com/microsoft/autogen/main/python/packages/magentic-one-cli/"&gt;Magentic-One&lt;/a&gt; is a state-of-the-art multi-agent team built using AgentChat API and Extensions API that can handle a variety of tasks that require web browsing, code execution, and file handling.&lt;/p&gt; 
&lt;p&gt;With AutoGen you get to join and contribute to a thriving ecosystem. We host weekly office hours and talks with maintainers and community. We also have a &lt;a href="https://aka.ms/autogen-discord"&gt;Discord server&lt;/a&gt; for real-time chat, GitHub Discussions for Q&amp;amp;A, and a blog for tutorials and updates.&lt;/p&gt; 
&lt;h2&gt;Where to go next?&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/autogen/main/python"&gt;&lt;img src="https://img.shields.io/badge/AutoGen-Python-blue?logo=python&amp;amp;logoColor=white" alt="Python" /&gt;&lt;/a&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/autogen/main/dotnet"&gt;&lt;img src="https://img.shields.io/badge/AutoGen-.NET-green?logo=.net&amp;amp;logoColor=white" alt=".NET" /&gt;&lt;/a&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/autogen/main/python/packages/autogen-studio"&gt;&lt;img src="https://img.shields.io/badge/AutoGen-Studio-purple?logo=visual-studio&amp;amp;logoColor=white" alt="Studio" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Installation&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/installation.html"&gt;&lt;img src="https://img.shields.io/badge/Install-blue" alt="Installation" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://microsoft.github.io/autogen/dotnet/dev/core/installation.html"&gt;&lt;img src="https://img.shields.io/badge/Install-green" alt="Install" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/installation.html"&gt;&lt;img src="https://img.shields.io/badge/Install-purple" alt="Install" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Quickstart&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/quickstart.html#"&gt;&lt;img src="https://img.shields.io/badge/Quickstart-blue" alt="Quickstart" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://microsoft.github.io/autogen/dotnet/dev/core/index.html"&gt;&lt;img src="https://img.shields.io/badge/Quickstart-green" alt="Quickstart" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/usage.html#"&gt;&lt;img src="https://img.shields.io/badge/Quickstart-purple" alt="Usage" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Tutorial&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/index.html"&gt;&lt;img src="https://img.shields.io/badge/Tutorial-blue" alt="Tutorial" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://microsoft.github.io/autogen/dotnet/dev/core/tutorial.html"&gt;&lt;img src="https://img.shields.io/badge/Tutorial-green" alt="Tutorial" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/usage.html#"&gt;&lt;img src="https://img.shields.io/badge/Tutorial-purple" alt="Usage" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;API Reference&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://microsoft.github.io/autogen/stable/reference/index.html#"&gt;&lt;img src="https://img.shields.io/badge/Docs-blue" alt="API" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://microsoft.github.io/autogen/dotnet/dev/api/Microsoft.AutoGen.Contracts.html"&gt;&lt;img src="https://img.shields.io/badge/Docs-green" alt="API" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/usage.html"&gt;&lt;img src="https://img.shields.io/badge/Docs-purple" alt="API" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Packages&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://pypi.org/project/autogen-core/"&gt;&lt;img src="https://img.shields.io/badge/PyPi-autogen--core-blue?logo=pypi" alt="PyPi autogen-core" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://pypi.org/project/autogen-agentchat/"&gt;&lt;img src="https://img.shields.io/badge/PyPi-autogen--agentchat-blue?logo=pypi" alt="PyPi autogen-agentchat" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://pypi.org/project/autogen-ext/"&gt;&lt;img src="https://img.shields.io/badge/PyPi-autogen--ext-blue?logo=pypi" alt="PyPi autogen-ext" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://www.nuget.org/packages/Microsoft.AutoGen.Contracts/"&gt;&lt;img src="https://img.shields.io/badge/NuGet-Contracts-green?logo=nuget" alt="NuGet Contracts" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://www.nuget.org/packages/Microsoft.AutoGen.Core/"&gt;&lt;img src="https://img.shields.io/badge/NuGet-Core-green?logo=nuget" alt="NuGet Core" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://www.nuget.org/packages/Microsoft.AutoGen.Core.Grpc/"&gt;&lt;img src="https://img.shields.io/badge/NuGet-Core.Grpc-green?logo=nuget" alt="NuGet Core.Grpc" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://www.nuget.org/packages/Microsoft.AutoGen.RuntimeGateway.Grpc/"&gt;&lt;img src="https://img.shields.io/badge/NuGet-RuntimeGateway.Grpc-green?logo=nuget" alt="NuGet RuntimeGateway.Grpc" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://pypi.org/project/autogenstudio/"&gt;&lt;img src="https://img.shields.io/badge/PyPi-autogenstudio-purple?logo=pypi" alt="PyPi autogenstudio" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;p&gt;Interested in contributing? See &lt;a href="https://raw.githubusercontent.com/microsoft/autogen/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for guidelines on how to get started. We welcome contributions of all kinds, including bug fixes, new features, and documentation improvements. Join our community and help us make AutoGen better!&lt;/p&gt; 
&lt;p&gt;Have questions? Check out our &lt;a href="https://raw.githubusercontent.com/microsoft/autogen/main/FAQ.md"&gt;Frequently Asked Questions (FAQ)&lt;/a&gt; for answers to common queries. If you don't find what you're looking for, feel free to ask in our &lt;a href="https://github.com/microsoft/autogen/discussions"&gt;GitHub Discussions&lt;/a&gt; or join our &lt;a href="https://aka.ms/autogen-discord"&gt;Discord server&lt;/a&gt; for real-time support. You can also read our &lt;a href="https://devblogs.microsoft.com/autogen/"&gt;blog&lt;/a&gt; for updates.&lt;/p&gt; 
&lt;h2&gt;Legal Notices&lt;/h2&gt; 
&lt;p&gt;Microsoft and any contributors grant you a license to the Microsoft documentation and other content in this repository under the &lt;a href="https://creativecommons.org/licenses/by/4.0/legalcode"&gt;Creative Commons Attribution 4.0 International Public License&lt;/a&gt;, see the &lt;a href="https://raw.githubusercontent.com/microsoft/autogen/main/LICENSE"&gt;LICENSE&lt;/a&gt; file, and grant you a license to any code in the repository under the &lt;a href="https://opensource.org/licenses/MIT"&gt;MIT License&lt;/a&gt;, see the &lt;a href="https://raw.githubusercontent.com/microsoft/autogen/main/LICENSE-CODE"&gt;LICENSE-CODE&lt;/a&gt; file.&lt;/p&gt; 
&lt;p&gt;Microsoft, Windows, Microsoft Azure, and/or other Microsoft products and services referenced in the documentation may be either trademarks or registered trademarks of Microsoft in the United States and/or other countries. The licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks. Microsoft's general trademark guidelines can be found at &lt;a href="http://go.microsoft.com/fwlink/?LinkID=254653"&gt;http://go.microsoft.com/fwlink/?LinkID=254653&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Privacy information can be found at &lt;a href="https://go.microsoft.com/fwlink/?LinkId=521839"&gt;https://go.microsoft.com/fwlink/?LinkId=521839&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Microsoft and any contributors reserve all other rights, whether under their respective copyrights, patents, or trademarks, whether by implication, estoppel, or otherwise.&lt;/p&gt; 
&lt;p align="right" style="font-size: 14px; color: #555; margin-top: 20px;"&gt; &lt;a href="https://raw.githubusercontent.com/microsoft/autogen/main/#readme-top" style="text-decoration: none; color: blue; font-weight: bold;"&gt; ‚Üë Back to Top ‚Üë &lt;/a&gt; &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>HunxByts/GhostTrack</title>
      <link>https://github.com/HunxByts/GhostTrack</link>
      <description>&lt;p&gt;Useful tool to track location or mobile number&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GhostTrack&lt;/h1&gt; 
&lt;p&gt;Useful tool to track location or mobile number, so this tool can be called osint or also information gathering&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/raw/main/asset/bn.png" /&gt; 
&lt;p&gt;New update : &lt;code&gt;Version 2.2&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Instalation on Linux (deb)&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;sudo apt-get install git
sudo apt-get install python3
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Instalation on Termux&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;pkg install git
pkg install python3
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Usage Tool&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/HunxByts/GhostTrack.git
cd GhostTrack
pip3 install -r requirements.txt
python3 GhostTR.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Display on the menu &lt;code&gt;IP Tracker&lt;/code&gt;&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/blob/main/asset/ip.png " /&gt; 
&lt;p&gt;on the IP Track menu, you can combo with the seeker tool to get the target IP&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;span&gt;‚ö°&lt;/span&gt; Install Seeker :&lt;/summary&gt; - 
 &lt;strong&gt;&lt;a href="https://github.com/thewhiteh4t/seeker"&gt;Get Seeker&lt;/a&gt;&lt;/strong&gt; 
&lt;/details&gt; 
&lt;p&gt;Display on the menu &lt;code&gt;Phone Tracker&lt;/code&gt;&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/raw/main/asset/phone.png" /&gt; 
&lt;p&gt;on this menu you can search for information from the target phone number&lt;/p&gt; 
&lt;p&gt;Display on the menu &lt;code&gt;Username Tracker&lt;/code&gt;&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/raw/main/asset/User.png" /&gt; on this menu you can search for information from the target username on social media 
&lt;details&gt; 
 &lt;summary&gt;&lt;span&gt;‚ö°&lt;/span&gt; Author :&lt;/summary&gt; - 
 &lt;strong&gt;&lt;a href="https://github.com/HunxByts"&gt;HunxByts&lt;/a&gt;&lt;/strong&gt; 
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>PaddlePaddle/PaddleDetection</title>
      <link>https://github.com/PaddlePaddle/PaddleDetection</link>
      <description>&lt;p&gt;Object Detection toolkit based on PaddlePaddle. It supports object detection, instance segmentation, multiple object tracking and real-time multi-person keypoint detection.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;ÁÆÄ‰Ωì‰∏≠Êñá | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/README_en.md"&gt;English&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p align="center"&gt; &lt;img src="https://user-images.githubusercontent.com/48054808/160532560-34cf7a1f-d950-435e-90d2-4b0a679e5119.png" align="middle" width="800" /&gt; &lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-Apache%202-dfd.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://github.com/PaddlePaddle/PaddleDetection/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/PaddlePaddle/PaddleDetection?color=ffa" /&gt;&lt;/a&gt; &lt;a href=""&gt;&lt;img src="https://img.shields.io/badge/python-3.7+-aff.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href=""&gt;&lt;img src="https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://github.com/PaddlePaddle/PaddleDetection/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/PaddlePaddle/PaddleDetection?color=ccf" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üíåÁõÆÂΩï&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E7%9B%AE%E5%BD%95"&gt;üíåÁõÆÂΩï&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E7%AE%80%E4%BB%8B"&gt;üåàÁÆÄ‰ªã&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%95"&gt;üì£ÊúÄÊñ∞ËøõÂ±ï&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%EF%B8%8F%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B"&gt;‚ö°Ô∏èÂø´ÈÄüÂºÄÂßã&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E4%BD%8E%E4%BB%A3%E7%A0%81%E5%85%A8%E6%B5%81%E7%A8%8B%E5%BC%80%E5%8F%91"&gt;üî•‰Ωé‰ª£Á†ÅÂÖ®ÊµÅÁ®ãÂºÄÂèë&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E5%BC%80%E6%BA%90%E7%A4%BE%E5%8C%BA"&gt;üë´ÂºÄÊ∫êÁ§æÂå∫&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E4%B8%BB%E8%A6%81%E7%89%B9%E6%80%A7"&gt;‚ú®‰∏ªË¶ÅÁâπÊÄß&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E6%A8%A1%E5%9D%97%E5%8C%96%E8%AE%BE%E8%AE%A1"&gt;üß©Ê®°ÂùóÂåñËÆæËÆ°&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E4%B8%B0%E5%AF%8C%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%BA%93"&gt;üì±‰∏∞ÂØåÁöÑÊ®°ÂûãÂ∫ì&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%EF%B8%8F%E4%BA%A7%E4%B8%9A%E7%89%B9%E8%89%B2%E6%A8%A1%E5%9E%8B%E4%BA%A7%E4%B8%9A%E5%B7%A5%E5%85%B7"&gt;üéóÔ∏è‰∫ß‰∏öÁâπËâ≤Ê®°Âûã|‰∫ß‰∏öÂ∑•ÂÖ∑&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E4%BA%A7%E4%B8%9A%E7%BA%A7%E9%83%A8%E7%BD%B2%E5%AE%9E%E8%B7%B5"&gt;üí°üèÜ‰∫ß‰∏öÁ∫ßÈÉ®ÁΩ≤ÂÆûË∑µ&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E5%AE%89%E8%A3%85"&gt;üç±ÂÆâË£Ö&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E6%95%99%E7%A8%8B"&gt;üî•ÊïôÁ®ã&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#faq"&gt;üîëFAQ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E6%A8%A1%E5%9D%97%E7%BB%84%E4%BB%B6"&gt;üß©Ê®°ÂùóÁªÑ‰ª∂&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E6%A8%A1%E5%9E%8B%E5%BA%93"&gt;üì±Ê®°ÂûãÂ∫ì&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%EF%B8%8F%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94"&gt;‚öñÔ∏èÊ®°ÂûãÊÄßËÉΩÂØπÊØî&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%EF%B8%8F%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94"&gt;üñ•Ô∏èÊúçÂä°Âô®Á´ØÊ®°ÂûãÊÄßËÉΩÂØπÊØî&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%EF%B8%8F%E7%A7%BB%E5%8A%A8%E7%AB%AF%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94"&gt;‚åöÔ∏èÁßªÂä®Á´ØÊ®°ÂûãÊÄßËÉΩÂØπÊØî&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%EF%B8%8F%E4%BA%A7%E4%B8%9A%E7%89%B9%E8%89%B2%E6%A8%A1%E5%9E%8B%E4%BA%A7%E4%B8%9A%E5%B7%A5%E5%85%B7-1"&gt;üéóÔ∏è‰∫ß‰∏öÁâπËâ≤Ê®°Âûã|‰∫ß‰∏öÂ∑•ÂÖ∑&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#pp-yoloe-%E9%AB%98%E7%B2%BE%E5%BA%A6%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B"&gt;üíéPP-YOLOE È´òÁ≤æÂ∫¶ÁõÆÊ†áÊ£ÄÊµãÊ®°Âûã&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#pp-yoloe-r-%E9%AB%98%E6%80%A7%E8%83%BD%E6%97%8B%E8%BD%AC%E6%A1%86%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B"&gt;üíéPP-YOLOE-R È´òÊÄßËÉΩÊóãËΩ¨Ê°ÜÊ£ÄÊµãÊ®°Âûã&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#pp-yoloe-sod-%E9%AB%98%E7%B2%BE%E5%BA%A6%E5%B0%8F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B"&gt;üíéPP-YOLOE-SOD È´òÁ≤æÂ∫¶Â∞èÁõÆÊ†áÊ£ÄÊµãÊ®°Âûã&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#pp-picodet-%E8%B6%85%E8%BD%BB%E9%87%8F%E5%AE%9E%E6%97%B6%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B"&gt;üí´PP-PicoDet Ë∂ÖËΩªÈáèÂÆûÊó∂ÁõÆÊ†áÊ£ÄÊµãÊ®°Âûã&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#pp-tracking-%E5%AE%9E%E6%97%B6%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E7%B3%BB%E7%BB%9F"&gt;üì°PP-Tracking ÂÆûÊó∂Â§öÁõÆÊ†áË∑üË∏™Á≥ªÁªü&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%EF%B8%8Fpp-tinypose-%E4%BA%BA%E4%BD%93%E9%AA%A8%E9%AA%BC%E5%85%B3%E9%94%AE%E7%82%B9%E8%AF%86%E5%88%AB"&gt;‚õ∑Ô∏èPP-TinyPose ‰∫∫‰ΩìÈ™®È™ºÂÖ≥ÈîÆÁÇπËØÜÂà´&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#pp-human-%E5%AE%9E%E6%97%B6%E8%A1%8C%E4%BA%BA%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7"&gt;üèÉüèªPP-Human ÂÆûÊó∂Ë°å‰∫∫ÂàÜÊûêÂ∑•ÂÖ∑&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%EF%B8%8Fpp-vehicle-%E5%AE%9E%E6%97%B6%E8%BD%A6%E8%BE%86%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7"&gt;üèéÔ∏èPP-Vehicle ÂÆûÊó∂ËΩ¶ËæÜÂàÜÊûêÂ∑•ÂÖ∑&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E4%BA%A7%E4%B8%9A%E5%AE%9E%E8%B7%B5%E8%8C%83%E4%BE%8B"&gt;üí°‰∫ß‰∏öÂÆûË∑µËåÉ‰æã&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E4%BC%81%E4%B8%9A%E5%BA%94%E7%94%A8%E6%A1%88%E4%BE%8B"&gt;üèÜ‰ºÅ‰∏öÂ∫îÁî®Ê°à‰æã&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E8%AE%B8%E5%8F%AF%E8%AF%81%E4%B9%A6"&gt;üìùËÆ∏ÂèØËØÅ‰π¶&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E5%BC%95%E7%94%A8"&gt;üìåÂºïÁî®&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üåàÁÆÄ‰ªã&lt;/h2&gt; 
&lt;p&gt;PaddleDetectionÊòØ‰∏Ä‰∏™Âü∫‰∫éPaddlePaddleÁöÑÁõÆÊ†áÊ£ÄÊµãÁ´ØÂà∞Á´ØÂºÄÂèëÂ•ó‰ª∂ÔºåÂú®Êèê‰æõ‰∏∞ÂØåÁöÑÊ®°ÂûãÁªÑ‰ª∂ÂíåÊµãËØïÂü∫ÂáÜÁöÑÂêåÊó∂ÔºåÊ≥®ÈáçÁ´ØÂà∞Á´ØÁöÑ‰∫ß‰∏öËêΩÂú∞Â∫îÁî®ÔºåÈÄöËøáÊâìÈÄ†‰∫ß‰∏öÁ∫ßÁâπËâ≤Ê®°Âûã|Â∑•ÂÖ∑„ÄÅÂª∫ËÆæ‰∫ß‰∏öÂ∫îÁî®ËåÉ‰æãÁ≠âÊâãÊÆµÔºåÂ∏ÆÂä©ÂºÄÂèëËÄÖÂÆûÁé∞Êï∞ÊçÆÂáÜÂ§á„ÄÅÊ®°ÂûãÈÄâÂûã„ÄÅÊ®°ÂûãËÆ≠ÁªÉ„ÄÅÊ®°ÂûãÈÉ®ÁΩ≤ÁöÑÂÖ®ÊµÅÁ®ãÊâìÈÄöÔºåÂø´ÈÄüËøõË°åËêΩÂú∞Â∫îÁî®„ÄÇ&lt;/p&gt; 
&lt;p&gt;‰∏ªË¶ÅÊ®°ÂûãÊïàÊûúÁ§∫‰æãÂ¶Ç‰∏ãÔºàÁÇπÂáªÊ†áÈ¢òÂèØÂø´ÈÄüË∑≥ËΩ¨ÔºâÔºö&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#pp-yoloe-%E9%AB%98%E7%B2%BE%E5%BA%A6%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B"&gt;&lt;strong&gt;ÈÄöÁî®ÁõÆÊ†áÊ£ÄÊµã&lt;/strong&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#pp-yoloe-sod-%E9%AB%98%E7%B2%BE%E5%BA%A6%E5%B0%8F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B"&gt;&lt;strong&gt;Â∞èÁõÆÊ†áÊ£ÄÊµã&lt;/strong&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#pp-yoloe-r-%E9%AB%98%E6%80%A7%E8%83%BD%E6%97%8B%E8%BD%AC%E6%A1%86%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B"&gt;&lt;strong&gt;ÊóãËΩ¨Ê°ÜÊ£ÄÊµã&lt;/strong&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://github.com/PaddlePaddle/Paddle3D"&gt;&lt;strong&gt;3DÁõÆÊ†áÁâ©Ê£ÄÊµã&lt;/strong&gt;&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206095864-f174835d-4e9a-42f7-96b8-d684fc3a3687.png" height="126px" width="180px" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206095892-934be83a-f869-4a31-8e52-1074184149d1.jpg" height="126px" width="180px" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206111796-d9a9702a-c1a0-4647-b8e9-3e1307e9d34c.png" height="126px" width="180px" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206095622-cf6dbd26-5515-472f-9451-b39bbef5b1bf.gif" height="126px" width="180px" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E6%A8%A1%E5%9E%8B%E5%BA%93"&gt;&lt;strong&gt;‰∫∫ËÑ∏Ê£ÄÊµã&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%EF%B8%8Fpp-tinypose-%E4%BA%BA%E4%BD%93%E9%AA%A8%E9%AA%BC%E5%85%B3%E9%94%AE%E7%82%B9%E8%AF%86%E5%88%AB"&gt;&lt;strong&gt;2DÂÖ≥ÈîÆÁÇπÊ£ÄÊµã&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#pp-tracking-%E5%AE%9E%E6%97%B6%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E7%B3%BB%E7%BB%9F"&gt;&lt;strong&gt;Â§öÁõÆÊ†áËøΩË∏™&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E6%A8%A1%E5%9E%8B%E5%BA%93"&gt;&lt;strong&gt;ÂÆû‰æãÂàÜÂâ≤&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206095684-72f42233-c9c7-4bd8-9195-e34859bd08bf.jpg" height="126px" width="180px" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206100220-ab01d347-9ff9-4f17-9718-290ec14d4205.gif" height="126px" width="180px" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206111753-836e7827-968e-4c80-92ef-7a78766892fc.gif" height="126px" width="180px" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206095831-cc439557-1a23-4a99-b6b0-b6f2e97e8c57.jpg" height="126px" width="180px" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%EF%B8%8Fpp-vehicle-%E5%AE%9E%E6%97%B6%E8%BD%A6%E8%BE%86%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7"&gt;&lt;strong&gt;ËΩ¶ËæÜÂàÜÊûê‚Äî‚ÄîËΩ¶ÁâåËØÜÂà´&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%EF%B8%8Fpp-vehicle-%E5%AE%9E%E6%97%B6%E8%BD%A6%E8%BE%86%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7"&gt;&lt;strong&gt;ËΩ¶ËæÜÂàÜÊûê‚Äî‚ÄîËΩ¶ÊµÅÁªüËÆ°&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%EF%B8%8Fpp-vehicle-%E5%AE%9E%E6%97%B6%E8%BD%A6%E8%BE%86%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7"&gt;&lt;strong&gt;ËΩ¶ËæÜÂàÜÊûê‚Äî‚ÄîËøùÁ´†Ê£ÄÊµã&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%EF%B8%8Fpp-vehicle-%E5%AE%9E%E6%97%B6%E8%BD%A6%E8%BE%86%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7"&gt;&lt;strong&gt;ËΩ¶ËæÜÂàÜÊûê‚Äî‚ÄîÂ±ûÊÄßÂàÜÊûê&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206099328-2a1559e0-3b48-4424-9bad-d68f9ba5ba65.gif" height="126px" width="180px" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206095918-d0e7ad87-7bbb-40f1-bcc1-37844e2271ff.gif" height="126px" width="180px" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206100295-7762e1ab-ffce-44fb-b69d-45fb93657fa0.gif" height="126px" width="180px" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206095905-8255776a-d8e6-4af1-b6e9-8d9f97e5059d.gif" height="126px" width="180px" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#pp-human-%E5%AE%9E%E6%97%B6%E8%A1%8C%E4%BA%BA%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7"&gt;&lt;strong&gt;Ë°å‰∫∫ÂàÜÊûê‚Äî‚ÄîÈóØÂÖ•ÂàÜÊûê&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#pp-human-%E5%AE%9E%E6%97%B6%E8%A1%8C%E4%BA%BA%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7"&gt;&lt;strong&gt;Ë°å‰∫∫ÂàÜÊûê‚Äî‚ÄîË°å‰∏∫ÂàÜÊûê&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#pp-human-%E5%AE%9E%E6%97%B6%E8%A1%8C%E4%BA%BA%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7"&gt;&lt;strong&gt;Ë°å‰∫∫ÂàÜÊûê‚Äî‚ÄîÂ±ûÊÄßÂàÜÊûê&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#pp-human-%E5%AE%9E%E6%97%B6%E8%A1%8C%E4%BA%BA%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7"&gt;&lt;strong&gt;Ë°å‰∫∫ÂàÜÊûê‚Äî‚Äî‰∫∫ÊµÅÁªüËÆ°&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206095792-ae0ac107-cd8e-492a-8baa-32118fc82b04.gif" height="126px" width="180px" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206095778-fdd73e5d-9f91-48c7-9d3d-6f2e02ec3f79.gif" height="126px" width="180px" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206095709-2c3a209e-6626-45dd-be16-7f0bf4d48a14.gif" height="126px" width="180px" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206113351-cc59df79-8672-4d76-b521-a15acf69ae78.gif" height="126px" width="180px" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;ÂêåÊó∂ÔºåPaddleDetectionÊèê‰æõ‰∫ÜÊ®°ÂûãÁöÑÂú®Á∫ø‰ΩìÈ™åÂäüËÉΩÔºåÁî®Êà∑ÂèØ‰ª•ÈÄâÊã©Ëá™Â∑±ÁöÑÊï∞ÊçÆËøõË°åÂú®Á∫øÊé®ÁêÜ„ÄÇ&lt;/p&gt; 
&lt;h2&gt;üì£ÊúÄÊñ∞ËøõÂ±ï&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üî•2024.10.1 Ê∑ªÂä†ÁõÆÊ†áÊ£ÄÊµã„ÄÅÂÆû‰æãÂàÜÂâ≤È¢ÜÂüü‰∏ÄÁ´ôÂºèÂÖ®ÊµÅÁ®ãÂºÄÂèëËÉΩÂäõ&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;È£ûÊ°®‰Ωé‰ª£Á†ÅÂºÄÂèëÂ∑•ÂÖ∑PaddleXÔºå‰æùÊâò‰∫éPaddleDetectionÁöÑÂÖàËøõÊäÄÊúØÔºåÊîØÊåÅ‰∫ÜÁõÆÊ†áÊ£ÄÊµãÈ¢ÜÂüüÁöÑ&lt;strong&gt;‰∏ÄÁ´ôÂºèÂÖ®ÊµÅÁ®ã&lt;/strong&gt;ÂºÄÂèëËÉΩÂäõÔºö&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;üé® &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/paddlex/quick_start.md"&gt;&lt;strong&gt;Ê®°Âûã‰∏∞ÂØå‰∏ÄÈîÆË∞ÉÁî®&lt;/strong&gt;&lt;/a&gt;ÔºöÂ∞ÜÈÄöÁî®ÁõÆÊ†áÊ£ÄÊµã„ÄÅÂ∞èÁõÆÊ†áÊ£ÄÊµãÂíåÂÆû‰æãÂàÜÂâ≤Ê∂âÂèäÁöÑ&lt;strong&gt;55‰∏™Ê®°Âûã&lt;/strong&gt;Êï¥Âêà‰∏∫3Êù°Ê®°Âûã‰∫ßÁ∫øÔºåÈÄöËøáÊûÅÁÆÄÁöÑ&lt;strong&gt;Python API‰∏ÄÈîÆË∞ÉÁî®&lt;/strong&gt;ÔºåÂø´ÈÄü‰ΩìÈ™åÊ®°ÂûãÊïàÊûú„ÄÇÊ≠§Â§ñÔºåÂêå‰∏ÄÂ•óAPIÔºå‰πüÊîØÊåÅÂõæÂÉèÂàÜÁ±ª„ÄÅÂõæÂÉèÂàÜÂâ≤„ÄÅÊñáÊú¨ÂõæÂÉèÊô∫ËÉΩÂàÜÊûê„ÄÅÈÄöÁî®OCR„ÄÅÊó∂Â∫èÈ¢ÑÊµãÁ≠âÂÖ±ËÆ°&lt;strong&gt;200+Ê®°Âûã&lt;/strong&gt;ÔºåÂΩ¢Êàê20+ÂçïÂäüËÉΩÊ®°ÂùóÔºåÊñπ‰æøÂºÄÂèëËÄÖËøõË°å&lt;strong&gt;Ê®°ÂûãÁªÑÂêà‰ΩøÁî®&lt;/strong&gt;„ÄÇ&lt;/li&gt; 
     &lt;li&gt;üöÄ &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/paddlex/overview.md"&gt;&lt;strong&gt;ÊèêÈ´òÊïàÁéáÈôç‰ΩéÈó®Êßõ&lt;/strong&gt;&lt;/a&gt;ÔºöÊèê‰æõÂü∫‰∫é&lt;strong&gt;Áªü‰∏ÄÂëΩ‰ª§&lt;/strong&gt;Âíå&lt;strong&gt;ÂõæÂΩ¢ÁïåÈù¢&lt;/strong&gt;‰∏§ÁßçÊñπÂºèÔºåÂÆûÁé∞Ê®°ÂûãÁÆÄÊ¥ÅÈ´òÊïàÁöÑ‰ΩøÁî®„ÄÅÁªÑÂêà‰∏éÂÆöÂà∂„ÄÇÊîØÊåÅ&lt;strong&gt;È´òÊÄßËÉΩÈÉ®ÁΩ≤„ÄÅÊúçÂä°ÂåñÈÉ®ÁΩ≤ÂíåÁ´Ø‰æßÈÉ®ÁΩ≤&lt;/strong&gt;Á≠âÂ§öÁßçÈÉ®ÁΩ≤ÊñπÂºè„ÄÇÊ≠§Â§ñÔºåÂØπ‰∫éÂêÑÁßç‰∏ªÊµÅÁ°¨‰ª∂Â¶Ç&lt;strong&gt;Ëã±‰ºüËææGPU„ÄÅÊòÜ‰ªëËäØ„ÄÅÊòáËÖæ„ÄÅÂØíÊ≠¶Á∫™ÂíåÊµ∑ÂÖâ&lt;/strong&gt;Á≠âÔºåËøõË°åÊ®°ÂûãÂºÄÂèëÊó∂ÔºåÈÉΩÂèØ‰ª•&lt;strong&gt;Êó†ÁºùÂàáÊç¢&lt;/strong&gt;„ÄÇ&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Ê∑ªÂä†ÂÆû‰æãÂàÜÂâ≤SOTAÊ®°Âûã&lt;a href="https://github.com/PaddlePaddle/PaddleX/raw/release/3.0-beta1/docs/module_usage/tutorials/cv_modules/instance_segmentation.md"&gt;&lt;strong&gt;Mask-RT-DETR&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;üî•Ë∂ÖË∂äYOLOv8ÔºåÈ£ûÊ°®Êé®Âá∫Á≤æÂ∫¶ÊúÄÈ´òÁöÑÂÆûÊó∂Ê£ÄÊµãÂô®RT-DETRÔºÅ&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://github.com/PaddlePaddle/PaddleDetection/assets/17582080/196b0a10-d2e8-401c-9132-54b9126e0a33" height="500" caption="" /&gt; 
 &lt;p&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;RT-DETRËß£ËØªÊñáÁ´†‰º†ÈÄÅÈó®&lt;/code&gt;Ôºö 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/o03QM2rZNjHVto36gcV0Yw"&gt;„ÄäË∂ÖË∂äYOLOv8ÔºåÈ£ûÊ°®Êé®Âá∫Á≤æÂ∫¶ÊúÄÈ´òÁöÑÂÆûÊó∂Ê£ÄÊµãÂô®RT-DETRÔºÅ„Äã&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;‰ª£Á†Å‰º†ÈÄÅÈó®&lt;/code&gt;Ôºö&lt;a href="https://github.com/PaddlePaddle/PaddleDetection/tree/develop/configs/rtdetr"&gt;RT-DETR&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/paddlex/quick_start.md"&gt;‚ö°Ô∏èÂø´ÈÄüÂºÄÂßã&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/paddlex/overview.md"&gt;üî•‰Ωé‰ª£Á†ÅÂÖ®ÊµÅÁ®ãÂºÄÂèë&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;üë´ÂºÄÊ∫êÁ§æÂå∫&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üèÖÔ∏èÁ§æÂå∫Ë¥°ÁåÆÔºö&lt;/strong&gt; PaddleDetectionÈùûÂ∏∏Ê¨¢Ëøé‰Ω†Âä†ÂÖ•Âà∞È£ûÊ°®Á§æÂå∫ÁöÑÂºÄÊ∫êÂª∫ËÆæ‰∏≠ÔºåÂèÇ‰∏éË¥°ÁåÆÊñπÂºèÂèØ‰ª•ÂèÇËÄÉ&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/contribution/README.md"&gt;ÂºÄÊ∫êÈ°πÁõÆÂºÄÂèëÊåáÂçó&lt;/a&gt;„ÄÇ&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/community.md"&gt;&lt;strong&gt;üéàÁ§æÂå∫ËøëÊúüÊ¥ªÂä®&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚ú®‰∏ªË¶ÅÁâπÊÄß&lt;/h2&gt; 
&lt;h4&gt;üß©Ê®°ÂùóÂåñËÆæËÆ°&lt;/h4&gt; 
&lt;p&gt;PaddleDetectionÂ∞ÜÊ£ÄÊµãÊ®°ÂûãËß£ËÄ¶Êàê‰∏çÂêåÁöÑÊ®°ÂùóÁªÑ‰ª∂ÔºåÈÄöËøáËá™ÂÆö‰πâÊ®°ÂùóÁªÑ‰ª∂ÁªÑÂêàÔºåÁî®Êà∑ÂèØ‰ª•‰æøÊç∑È´òÊïàÂú∞ÂÆåÊàêÊ£ÄÊµãÊ®°ÂûãÁöÑÊê≠Âª∫„ÄÇ&lt;code&gt;‰º†ÈÄÅÈó®&lt;/code&gt;Ôºö&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E6%A8%A1%E5%9D%97%E7%BB%84%E4%BB%B6"&gt;üß©Ê®°ÂùóÁªÑ‰ª∂&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;h4&gt;üì±‰∏∞ÂØåÁöÑÊ®°ÂûãÂ∫ì&lt;/h4&gt; 
&lt;p&gt;PaddleDetectionÊîØÊåÅÂ§ßÈáèÁöÑÊúÄÊñ∞‰∏ªÊµÅÁöÑÁÆóÊ≥ïÂü∫ÂáÜ‰ª•ÂèäÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºåÊ∂µÁõñ2D/3DÁõÆÊ†áÊ£ÄÊµã„ÄÅÂÆû‰æãÂàÜÂâ≤„ÄÅ‰∫∫ËÑ∏Ê£ÄÊµã„ÄÅÂÖ≥ÈîÆÁÇπÊ£ÄÊµã„ÄÅÂ§öÁõÆÊ†áË∑üË∏™„ÄÅÂçäÁõëÁù£Â≠¶‰π†Á≠âÊñπÂêë„ÄÇ&lt;code&gt;‰º†ÈÄÅÈó®&lt;/code&gt;Ôºö&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E6%A8%A1%E5%9E%8B%E5%BA%93"&gt;üì±Ê®°ÂûãÂ∫ì&lt;/a&gt;„ÄÅ&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%EF%B8%8F%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94"&gt;‚öñÔ∏èÊ®°ÂûãÊÄßËÉΩÂØπÊØî&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;h4&gt;üéóÔ∏è‰∫ß‰∏öÁâπËâ≤Ê®°Âûã|‰∫ß‰∏öÂ∑•ÂÖ∑&lt;/h4&gt; 
&lt;p&gt;PaddleDetectionÊâìÈÄ†‰∫ß‰∏öÁ∫ßÁâπËâ≤Ê®°Âûã‰ª•ÂèäÂàÜÊûêÂ∑•ÂÖ∑ÔºöPP-YOLOE+„ÄÅPP-PicoDet„ÄÅPP-TinyPose„ÄÅPP-HumanV2„ÄÅPP-VehicleÁ≠âÔºåÈíàÂØπÈÄöÁî®„ÄÅÈ´òÈ¢ëÂûÇÁ±ªÂ∫îÁî®Âú∫ÊôØÊèê‰æõÊ∑±Â∫¶‰ºòÂåñËß£ÂÜ≥ÊñπÊ°à‰ª•ÂèäÈ´òÂ∫¶ÈõÜÊàêÁöÑÂàÜÊûêÂ∑•ÂÖ∑ÔºåÈôç‰ΩéÂºÄÂèëËÄÖÁöÑËØïÈîô„ÄÅÈÄâÊã©ÊàêÊú¨ÔºåÈíàÂØπ‰∏öÂä°Âú∫ÊôØÂø´ÈÄüÂ∫îÁî®ËêΩÂú∞„ÄÇ&lt;code&gt;‰º†ÈÄÅÈó®&lt;/code&gt;Ôºö&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%EF%B8%8F%E4%BA%A7%E4%B8%9A%E7%89%B9%E8%89%B2%E6%A8%A1%E5%9E%8B%E4%BA%A7%E4%B8%9A%E5%B7%A5%E5%85%B7-1"&gt;üéóÔ∏è‰∫ß‰∏öÁâπËâ≤Ê®°Âûã|‰∫ß‰∏öÂ∑•ÂÖ∑&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;h4&gt;üí°üèÜ‰∫ß‰∏öÁ∫ßÈÉ®ÁΩ≤ÂÆûË∑µ&lt;/h4&gt; 
&lt;p&gt;PaddleDetectionÊï¥ÁêÜÂ∑•‰∏ö„ÄÅÂÜú‰∏ö„ÄÅÊûó‰∏ö„ÄÅ‰∫§ÈÄö„ÄÅÂåªÁñó„ÄÅÈáëËûç„ÄÅËÉΩÊ∫êÁîµÂäõÁ≠âAIÂ∫îÁî®ËåÉ‰æãÔºåÊâìÈÄöÊï∞ÊçÆÊ†áÊ≥®-Ê®°ÂûãËÆ≠ÁªÉ-Ê®°ÂûãË∞É‰ºò-È¢ÑÊµãÈÉ®ÁΩ≤ÂÖ®ÊµÅÁ®ãÔºåÊåÅÁª≠Èôç‰ΩéÁõÆÊ†áÊ£ÄÊµãÊäÄÊúØ‰∫ß‰∏öËêΩÂú∞Èó®Êßõ„ÄÇ&lt;code&gt;‰º†ÈÄÅÈó®&lt;/code&gt;Ôºö&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E4%BA%A7%E4%B8%9A%E5%AE%9E%E8%B7%B5%E8%8C%83%E4%BE%8B"&gt;üí°‰∫ß‰∏öÂÆûË∑µËåÉ‰æã&lt;/a&gt;„ÄÅ&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E4%BC%81%E4%B8%9A%E5%BA%94%E7%94%A8%E6%A1%88%E4%BE%8B"&gt;üèÜ‰ºÅ‰∏öÂ∫îÁî®Ê°à‰æã&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p align="center"&gt; &lt;img src="https://user-images.githubusercontent.com/61035602/206431371-912a14c8-ce1e-48ec-ae6f-7267016b308e.png" align="middle" width="1280" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üç±ÂÆâË£Ö&lt;/h2&gt; 
&lt;p&gt;ÂèÇËÄÉ&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/tutorials/INSTALL_cn.md"&gt;ÂÆâË£ÖËØ¥Êòé&lt;/a&gt;ËøõË°åÂÆâË£Ö„ÄÇ&lt;/p&gt; 
&lt;h2&gt;üî•ÊïôÁ®ã&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Ê∑±Â∫¶Â≠¶‰π†ÂÖ•Èó®ÊïôÁ®ã&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.paddlepaddle.org.cn/tutorials/projectdetail/4676538"&gt;Èõ∂Âü∫Á°ÄÂÖ•Èó®Ê∑±Â∫¶Â≠¶‰π†&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.baidu.com/aistudio/education/group/info/1617"&gt;Èõ∂Âü∫Á°ÄÂÖ•Èó®ÁõÆÊ†áÊ£ÄÊµã&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Âø´ÈÄüÂºÄÂßã&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/tutorials/QUICK_STARTED_cn.md"&gt;Âø´ÈÄü‰ΩìÈ™å&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/tutorials/GETTING_STARTED_cn.md"&gt;Á§∫‰æãÔºö30ÂàÜÈíüÂø´ÈÄüÂºÄÂèë‰∫§ÈÄöÊ†áÂøóÊ£ÄÊµãÊ®°Âûã&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Êï∞ÊçÆÂáÜÂ§á&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/tutorials/data/README.md"&gt;Êï∞ÊçÆÂáÜÂ§á&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/advanced_tutorials/READER.md"&gt;Êï∞ÊçÆÂ§ÑÁêÜÊ®°Âùó&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;ÈÖçÁΩÆÊñá‰ª∂ËØ¥Êòé&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/tutorials/config_annotation/faster_rcnn_r50_fpn_1x_coco_annotation.md"&gt;RCNNÂèÇÊï∞ËØ¥Êòé&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/tutorials/config_annotation/ppyolo_r50vd_dcn_1x_coco_annotation.md"&gt;PP-YOLOÂèÇÊï∞ËØ¥Êòé&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Ê®°ÂûãÂºÄÂèë&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/advanced_tutorials/MODEL_TECHNICAL.md"&gt;Êñ∞Â¢ûÊ£ÄÊµãÊ®°Âûã&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;‰∫åÊ¨°ÂºÄÂèë 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/advanced_tutorials/customization/detection.md"&gt;ÁõÆÊ†áÊ£ÄÊµã&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/advanced_tutorials/customization/keypoint_detection.md"&gt;ÂÖ≥ÈîÆÁÇπÊ£ÄÊµã&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/advanced_tutorials/customization/pphuman_mot.md"&gt;Â§öÁõÆÊ†áË∑üË∏™&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/advanced_tutorials/customization/action_recognotion/"&gt;Ë°å‰∏∫ËØÜÂà´&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/advanced_tutorials/customization/pphuman_attribute.md"&gt;Â±ûÊÄßËØÜÂà´&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;ÈÉ®ÁΩ≤Êé®ÁêÜ&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/deploy/EXPORT_MODEL.md"&gt;Ê®°ÂûãÂØºÂá∫ÊïôÁ®ã&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleSlim"&gt;Ê®°ÂûãÂéãÁº©&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/slim"&gt;Ââ™Ë£Å/ÈáèÂåñ/Ëí∏È¶èÊïôÁ®ã&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/deploy/README.md"&gt;Paddle InferenceÈÉ®ÁΩ≤&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/deploy/python"&gt;PythonÁ´ØÊé®ÁêÜÈÉ®ÁΩ≤&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/deploy/cpp"&gt;C++Á´ØÊé®ÁêÜÈÉ®ÁΩ≤&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/deploy/lite"&gt;Paddle LiteÈÉ®ÁΩ≤&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/deploy/serving"&gt;Paddle ServingÈÉ®ÁΩ≤&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/deploy/EXPORT_ONNX_MODEL.md"&gt;ONNXÊ®°ÂûãÂØºÂá∫&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/deploy/BENCHMARK_INFER.md"&gt;Êé®ÁêÜbenchmark&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üîëFAQ&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/tutorials/FAQ"&gt;FAQ/Â∏∏ËßÅÈóÆÈ¢òÊ±áÊÄª&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üß©Ê®°ÂùóÁªÑ‰ª∂&lt;/h2&gt; 
&lt;table align="center"&gt; 
 &lt;tbody&gt; 
  &lt;tr align="center" valign="center"&gt; 
   &lt;td&gt; &lt;b&gt;Backbones&lt;/b&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;b&gt;Necks&lt;/b&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;b&gt;Loss&lt;/b&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;b&gt;Common&lt;/b&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;b&gt;Data Augmentation&lt;/b&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr valign="top"&gt; 
   &lt;td&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/resnet.py"&gt;ResNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/res2net.py"&gt;CSPResNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/senet.py"&gt;SENet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/res2net.py"&gt;Res2Net&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/hrnet.py"&gt;HRNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/lite_hrnet.py"&gt;Lite-HRNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/darknet.py"&gt;DarkNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/csp_darknet.py"&gt;CSPDarkNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/mobilenet_v1.py"&gt;MobileNetV1&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/mobilenet_v3.py"&gt;MobileNetV1&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/shufflenet_v2.py"&gt;ShuffleNetV2&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/ghostnet.py"&gt;GhostNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/blazenet.py"&gt;BlazeNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/dla.py"&gt;DLA&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/hardnet.py"&gt;HardNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/lcnet.py"&gt;LCNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/esnet.py"&gt;ESNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/swin_transformer.py"&gt;Swin-Transformer&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/convnext.py"&gt;ConvNeXt&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/vgg.py"&gt;VGG&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/vision_transformer.py"&gt;Vision Transformer&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/convnext"&gt;ConvNext&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
   &lt;td&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/necks/bifpn.py"&gt;BiFPN&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/necks/blazeface_fpn.py"&gt;BlazeFace-FPN&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/necks/centernet_fpn.py"&gt;CenterNet-FPN&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/necks/csp_pan.py"&gt;CSP-PAN&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/necks/custom_pan.py"&gt;Custom-PAN&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/necks/fpn.py"&gt;FPN&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/necks/es_pan.py"&gt;ES-PAN&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/necks/hrfpn.py"&gt;HRFPN&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/necks/lc_pan.py"&gt;LC-PAN&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/necks/ttf_fpn.py"&gt;TTF-FPN&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/necks/yolo_fpn.py"&gt;YOLO-FPN&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
   &lt;td&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/smooth_l1_loss.py"&gt;Smooth-L1&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/detr_loss.py"&gt;Detr Loss&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/fairmot_loss.py"&gt;Fairmot Loss&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/fcos_loss.py"&gt;Fcos Loss&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/gfocal_loss.py"&gt;GFocal Loss&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/jde_loss.py"&gt;JDE Loss&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/keypoint_loss.py"&gt;KeyPoint Loss&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/solov2_loss.py"&gt;SoloV2 Loss&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/focal_loss.py"&gt;Focal Loss&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/iou_loss.py"&gt;GIoU/DIoU/CIoU&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/iou_aware_loss.py"&gt;IoUAware&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/sparsercnn_loss.py"&gt;SparseRCNN Loss&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/ssd_loss.py"&gt;SSD Loss&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/focal_loss.py"&gt;YOLO Loss&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/yolo_loss.py"&gt;CT Focal Loss&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/varifocal_loss.py"&gt;VariFocal Loss&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
   &lt;td&gt;  &lt;li&gt;&lt;b&gt;Post-processing&lt;/b&gt;&lt;/li&gt; 
    &lt;ul&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/post_process.py"&gt;SoftNMS&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/post_process.py"&gt;MatrixNMS&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; 
    &lt;/ul&gt; &lt;li&gt;&lt;b&gt;Training&lt;/b&gt;&lt;/li&gt; 
    &lt;ul&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/tools/train.py#L62"&gt;FP16 training&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/tutorials/DistributedTraining_cn.md"&gt;Multi-machine training &lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; 
    &lt;/ul&gt; &lt;li&gt;&lt;b&gt;Common&lt;/b&gt;&lt;/li&gt; 
    &lt;ul&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/resnet.py#L41"&gt;Sync-BN&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/gn/README.md"&gt;Group Norm&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/dcn/README.md"&gt;DCNv2&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/optimizer/ema.py"&gt;EMA&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; 
    &lt;/ul&gt;&lt;/td&gt; 
   &lt;td&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/data/transform/operators.py"&gt;Resize&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/data/transform/operators.py"&gt;Lighting&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/data/transform/operators.py"&gt;Flipping&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/data/transform/operators.py"&gt;Expand&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/data/transform/operators.py"&gt;Crop&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/data/transform/operators.py"&gt;Color Distort&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/data/transform/operators.py"&gt;Random Erasing&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/data/transform/operators.py"&gt;Mixup &lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/data/transform/operators.py"&gt;AugmentHSV&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/data/transform/operators.py"&gt;Mosaic&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/data/transform/operators.py"&gt;Cutmix &lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/data/transform/operators.py"&gt;Grid Mask&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/data/transform/operators.py"&gt;Auto Augment&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/data/transform/operators.py"&gt;Random Perspective&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
  &lt;/tr&gt;   
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üì±Ê®°ÂûãÂ∫ì&lt;/h2&gt; 
&lt;table align="center"&gt; 
 &lt;tbody&gt; 
  &lt;tr align="center" valign="center"&gt; 
   &lt;td&gt; &lt;b&gt;2D Detection&lt;/b&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;b&gt;Multi Object Tracking&lt;/b&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;b&gt;KeyPoint Detection&lt;/b&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;b&gt;Others&lt;/b&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr valign="top"&gt; 
   &lt;td&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/faster_rcnn/README.md"&gt;Faster RCNN&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/necks/fpn.py"&gt;FPN&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/cascade_rcnn/README.md"&gt;Cascade-RCNN&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/rcnn_enhance"&gt;PSS-Det&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/retinanet/README.md"&gt;RetinaNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/yolov3/README.md"&gt;YOLOv3&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/yolof/README.md"&gt;YOLOF&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/yolox/README.md"&gt;YOLOX&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleYOLO/tree/develop/configs/yolov5"&gt;YOLOv5&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleYOLO/tree/develop/configs/yolov6"&gt;YOLOv6&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleYOLO/tree/develop/configs/yolov7"&gt;YOLOv7&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleYOLO/tree/develop/configs/yolov8"&gt;YOLOv8&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleYOLO/tree/develop/configs/rtmdet"&gt;RTMDet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ppyolo/README_cn.md"&gt;PP-YOLO&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ppyolo#pp-yolo-tiny"&gt;PP-YOLO-Tiny&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/picodet"&gt;PP-PicoDet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ppyolo/README_cn.md"&gt;PP-YOLOv2&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ppyoloe/README_legacy.md"&gt;PP-YOLOE&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ppyoloe/README_cn.md"&gt;PP-YOLOE+&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/smalldet"&gt;PP-YOLOE-SOD&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/rotate/README.md"&gt;PP-YOLOE-R&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ssd/README.md"&gt;SSD&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/centernet"&gt;CenterNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/fcos"&gt;FCOS&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/rotate/fcosr"&gt;FCOSR&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ttfnet"&gt;TTFNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/tood"&gt;TOOD&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/gfl"&gt;GFL&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/gfl/gflv2_r50_fpn_1x_coco.yml"&gt;GFLv2&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/detr"&gt;DETR&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/deformable_detr"&gt;Deformable DETR&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/sparse_rcnn"&gt;Sparse RCNN&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
   &lt;td&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/mot/jde"&gt;JDE&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/mot/fairmot"&gt;FairMOT&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/mot/deepsort"&gt;DeepSORT&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/mot/bytetrack"&gt;ByteTrack&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/mot/ocsort"&gt;OC-SORT&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/mot/botsort"&gt;BoT-SORT&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/mot/centertrack"&gt;CenterTrack&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
   &lt;td&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/keypoint/hrnet"&gt;HRNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/keypoint/higherhrnet"&gt;HigherHRNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/keypoint/lite_hrnet"&gt;Lite-HRNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/keypoint/tiny_pose"&gt;PP-TinyPose&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
   &lt;td&gt;  &lt;li&gt;&lt;b&gt;Instance Segmentation&lt;/b&gt;&lt;/li&gt; 
    &lt;ul&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/mask_rcnn"&gt;Mask RCNN&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/cascade_rcnn"&gt;Cascade Mask RCNN&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/solov2"&gt;SOLOv2&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; 
    &lt;/ul&gt; &lt;li&gt;&lt;b&gt;Face Detection&lt;/b&gt;&lt;/li&gt; 
    &lt;ul&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/face_detection"&gt;BlazeFace&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; 
    &lt;/ul&gt; &lt;li&gt;&lt;b&gt;Semi-Supervised Detection&lt;/b&gt;&lt;/li&gt; 
    &lt;ul&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/semi_det"&gt;DenseTeacher&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; 
    &lt;/ul&gt; &lt;li&gt;&lt;b&gt;3D Detection&lt;/b&gt;&lt;/li&gt; 
    &lt;ul&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/Paddle3D"&gt;Smoke&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/Paddle3D"&gt;CaDDN&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/Paddle3D"&gt;PointPillars&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/Paddle3D"&gt;CenterPoint&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/Paddle3D"&gt;SequeezeSegV3&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/Paddle3D"&gt;IA-SSD&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/Paddle3D"&gt;PETR&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; 
    &lt;/ul&gt; &lt;li&gt;&lt;b&gt;Vehicle Analysis Toolbox&lt;/b&gt;&lt;/li&gt; 
    &lt;ul&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/deploy/pipeline/README.md"&gt;PP-Vehicle&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; 
    &lt;/ul&gt; &lt;li&gt;&lt;b&gt;Human Analysis Toolbox&lt;/b&gt;&lt;/li&gt; 
    &lt;ul&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/deploy/pipeline/README.md"&gt;PP-Human&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/deploy/pipeline/README.md"&gt;PP-HumanV2&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; 
    &lt;/ul&gt; &lt;li&gt;&lt;b&gt;Sport Analysis Toolbox&lt;/b&gt;&lt;/li&gt; 
    &lt;ul&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleSports"&gt;PP-Sports&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; 
    &lt;/ul&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;‚öñÔ∏èÊ®°ÂûãÊÄßËÉΩÂØπÊØî&lt;/h2&gt; 
&lt;h4&gt;üñ•Ô∏èÊúçÂä°Âô®Á´ØÊ®°ÂûãÊÄßËÉΩÂØπÊØî&lt;/h4&gt; 
&lt;p&gt;ÂêÑÊ®°ÂûãÁªìÊûÑÂíåÈ™®Âπ≤ÁΩëÁªúÁöÑ‰ª£Ë°®Ê®°ÂûãÂú®COCOÊï∞ÊçÆÈõÜ‰∏äÁ≤æÂ∫¶mAPÂíåÂçïÂç°Tesla V100‰∏äÈ¢ÑÊµãÈÄüÂ∫¶(FPS)ÂØπÊØîÂõæ„ÄÇ&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://user-images.githubusercontent.com/61035602/206434766-caaa781b-b922-481f-af09-15faac9ed33b.png" width="800" /&gt; 
&lt;/div&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; ÊµãËØïËØ¥Êòé(ÁÇπÂáªÂ±ïÂºÄ)&lt;/b&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ViT‰∏∫ViT-Cascade-Faster-RCNNÊ®°ÂûãÔºåCOCOÊï∞ÊçÆÈõÜmAPÈ´òËææ55.7%&lt;/li&gt; 
  &lt;li&gt;Cascade-Faster-RCNN‰∏∫Cascade-Faster-RCNN-ResNet50vd-DCNÔºåPaddleDetectionÂ∞ÜÂÖ∂‰ºòÂåñÂà∞COCOÊï∞ÊçÆmAP‰∏∫47.8%Êó∂Êé®ÁêÜÈÄüÂ∫¶‰∏∫20FPS&lt;/li&gt; 
  &lt;li&gt;PP-YOLOEÊòØÂØπPP-YOLO v2Ê®°ÂûãÁöÑËøõ‰∏ÄÊ≠•‰ºòÂåñÔºåLÁâàÊú¨Âú®COCOÊï∞ÊçÆÈõÜmAP‰∏∫51.6%ÔºåTesla V100È¢ÑÊµãÈÄüÂ∫¶78.1FPS&lt;/li&gt; 
  &lt;li&gt;PP-YOLOE+ÊòØÂØπPPOLOEÊ®°ÂûãÁöÑËøõ‰∏ÄÊ≠•‰ºòÂåñÔºåLÁâàÊú¨Âú®COCOÊï∞ÊçÆÈõÜmAP‰∏∫53.3%ÔºåTesla V100È¢ÑÊµãÈÄüÂ∫¶78.1FPS&lt;/li&gt; 
  &lt;li&gt;YOLOXÂíåYOLOv5Âùá‰∏∫Âü∫‰∫éPaddleDetectionÂ§çÁé∞ÁÆóÊ≥ïÔºåYOLOv5‰ª£Á†ÅÂú®&lt;a href="https://github.com/PaddlePaddle/PaddleYOLO"&gt;PaddleYOLO&lt;/a&gt;‰∏≠ÔºåÂèÇÁÖß&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/feature_models/PaddleYOLO_MODEL.md"&gt;PaddleYOLO_MODEL&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Âõæ‰∏≠Ê®°ÂûãÂùáÂèØÂú®&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E6%A8%A1%E5%9E%8B%E5%BA%93"&gt;üì±Ê®°ÂûãÂ∫ì&lt;/a&gt;‰∏≠Ëé∑Âèñ&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h4&gt;‚åöÔ∏èÁßªÂä®Á´ØÊ®°ÂûãÊÄßËÉΩÂØπÊØî&lt;/h4&gt; 
&lt;p&gt;ÂêÑÁßªÂä®Á´ØÊ®°ÂûãÂú®COCOÊï∞ÊçÆÈõÜ‰∏äÁ≤æÂ∫¶mAPÂíåÈ´òÈÄöÈ™ÅÈæô865Â§ÑÁêÜÂô®‰∏äÈ¢ÑÊµãÈÄüÂ∫¶(FPS)ÂØπÊØîÂõæ„ÄÇ&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://user-images.githubusercontent.com/61035602/206434741-10460690-8fc3-4084-a11a-16fe4ce2fc85.png" width="550" /&gt; 
&lt;/div&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; ÊµãËØïËØ¥Êòé(ÁÇπÂáªÂ±ïÂºÄ)&lt;/b&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ÊµãËØïÊï∞ÊçÆÂùá‰ΩøÁî®È´òÈÄöÈ™ÅÈæô865(4xA77+4xA55)Â§ÑÁêÜÂô®Ôºåbatch size‰∏∫1, ÂºÄÂêØ4Á∫øÁ®ãÊµãËØïÔºåÊµãËØï‰ΩøÁî®NCNNÈ¢ÑÊµãÂ∫ìÔºåÊµãËØïËÑöÊú¨ËßÅ&lt;a href="https://github.com/JiweiMaster/MobileDetBenchmark"&gt;MobileDetBenchmark&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;PP-PicoDetÂèäPP-YOLO-Tiny‰∏∫PaddleDetectionËá™Á†îÊ®°ÂûãÔºåÂèØÂú®&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E6%A8%A1%E5%9E%8B%E5%BA%93"&gt;üì±Ê®°ÂûãÂ∫ì&lt;/a&gt;‰∏≠Ëé∑ÂèñÔºåÂÖ∂‰ΩôÊ®°ÂûãPaddleDetectionÊöÇÊú™Êèê‰æõ&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;üéóÔ∏è‰∫ß‰∏öÁâπËâ≤Ê®°Âûã|‰∫ß‰∏öÂ∑•ÂÖ∑&lt;/h2&gt; 
&lt;p&gt;‰∫ß‰∏öÁâπËâ≤Ê®°ÂûãÔΩú‰∫ß‰∏öÂ∑•ÂÖ∑ÊòØPaddleDetectionÈíàÂØπ‰∫ß‰∏öÈ´òÈ¢ëÂ∫îÁî®Âú∫ÊôØÊâìÈÄ†ÁöÑÂÖºÈ°æÁ≤æÂ∫¶ÂíåÈÄüÂ∫¶ÁöÑÊ®°Âûã‰ª•ÂèäÂ∑•ÂÖ∑ÁÆ±ÔºåÊ≥®Èáç‰ªéÊï∞ÊçÆÂ§ÑÁêÜ-Ê®°ÂûãËÆ≠ÁªÉ-Ê®°ÂûãË∞É‰ºò-Ê®°ÂûãÈÉ®ÁΩ≤ÁöÑÁ´ØÂà∞Á´ØÊâìÈÄöÔºå‰∏îÊèê‰æõ‰∫ÜÂÆûÈôÖÁîü‰∫ßÁéØÂ¢É‰∏≠ÁöÑÂÆûË∑µËåÉ‰æã‰ª£Á†ÅÔºåÂ∏ÆÂä©Êã•ÊúâÁ±ª‰ººÈúÄÊ±ÇÁöÑÂºÄÂèëËÄÖÈ´òÊïàÁöÑÂÆåÊàê‰∫ßÂìÅÂºÄÂèëËêΩÂú∞Â∫îÁî®„ÄÇ&lt;/p&gt; 
&lt;p&gt;ËØ•Á≥ªÂàóÊ®°ÂûãÔΩúÂ∑•ÂÖ∑ÂùáÂ∑≤PPÂâçÁºÄÂëΩÂêçÔºåÂÖ∑‰Ωì‰ªãÁªç„ÄÅÈ¢ÑËÆ≠ÁªÉÊ®°Âûã‰ª•Âèä‰∫ß‰∏öÂÆûË∑µËåÉ‰æã‰ª£Á†ÅÂ¶Ç‰∏ã„ÄÇ&lt;/p&gt; 
&lt;h3&gt;üíéPP-YOLOE È´òÁ≤æÂ∫¶ÁõÆÊ†áÊ£ÄÊµãÊ®°Âûã&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; ÁÆÄ‰ªã(ÁÇπÂáªÂ±ïÂºÄ)&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;PP-YOLOEÊòØÂü∫‰∫éPP-YOLOv2ÁöÑÂçìË∂äÁöÑÂçïÈò∂ÊÆµAnchor-freeÊ®°ÂûãÔºåË∂ÖË∂ä‰∫ÜÂ§öÁßçÊµÅË°åÁöÑYOLOÊ®°Âûã„ÄÇPP-YOLOEÈÅøÂÖç‰∫Ü‰ΩøÁî®ËØ∏Â¶ÇDeformable ConvolutionÊàñËÄÖMatrix NMS‰πãÁ±ªÁöÑÁâπÊÆäÁÆóÂ≠êÔºå‰ª•‰ΩøÂÖ∂ËÉΩËΩªÊùæÂú∞ÈÉ®ÁΩ≤Âú®Â§öÁßçÂ§öÊ†∑ÁöÑÁ°¨‰ª∂‰∏ä„ÄÇÂÖ∂‰ΩøÁî®Â§ßËßÑÊ®°Êï∞ÊçÆÈõÜobj365È¢ÑËÆ≠ÁªÉÊ®°ÂûãËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÂèØ‰ª•Âú®‰∏çÂêåÂú∫ÊôØÊï∞ÊçÆÈõÜ‰∏äÂø´ÈÄüË∞É‰ºòÊî∂Êïõ„ÄÇ&lt;/p&gt; 
 &lt;p&gt;&lt;code&gt;‰º†ÈÄÅÈó®&lt;/code&gt;Ôºö&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ppyoloe/README_cn.md"&gt;PP-YOLOEËØ¥Êòé&lt;/a&gt;„ÄÇ&lt;/p&gt; 
 &lt;p&gt;&lt;code&gt;‰º†ÈÄÅÈó®&lt;/code&gt;Ôºö&lt;a href="https://arxiv.org/abs/2203.16250"&gt;arXivËÆ∫Êñá&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; È¢ÑËÆ≠ÁªÉÊ®°Âûã(ÁÇπÂáªÂ±ïÂºÄ)&lt;/b&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;Ê®°ÂûãÂêçÁß∞&lt;/th&gt; 
    &lt;th align="center"&gt;COCOÁ≤æÂ∫¶ÔºàmAPÔºâ&lt;/th&gt; 
    &lt;th align="center"&gt;V100 TensorRT FP16ÈÄüÂ∫¶(FPS)&lt;/th&gt; 
    &lt;th align="center"&gt;Êé®ËçêÈÉ®ÁΩ≤Á°¨‰ª∂&lt;/th&gt; 
    &lt;th align="center"&gt;ÈÖçÁΩÆÊñá‰ª∂&lt;/th&gt; 
    &lt;th align="center"&gt;Ê®°Âûã‰∏ãËΩΩ&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;PP-YOLOE+_l&lt;/td&gt; 
    &lt;td align="center"&gt;53.3&lt;/td&gt; 
    &lt;td align="center"&gt;149.2&lt;/td&gt; 
    &lt;td align="center"&gt;ÊúçÂä°Âô®&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ppyoloe/ppyoloe_plus_crn_l_80e_coco.yml"&gt;ÈìæÊé•&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://paddledet.bj.bcebos.com/models/ppyoloe_plus_crn_m_80e_coco.pdparams"&gt;‰∏ãËΩΩÂú∞ÂùÄ&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;code&gt;‰º†ÈÄÅÈó®&lt;/code&gt;Ôºö&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ppyoloe/README_cn.md"&gt;ÂÖ®ÈÉ®È¢ÑËÆ≠ÁªÉÊ®°Âûã&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; ‰∫ß‰∏öÂ∫îÁî®‰ª£Á†ÅÁ§∫‰æã(ÁÇπÂáªÂ±ïÂºÄ)&lt;/b&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Ë°å‰∏ö&lt;/th&gt; 
    &lt;th&gt;Á±ªÂà´&lt;/th&gt; 
    &lt;th&gt;‰∫ÆÁÇπ&lt;/th&gt; 
    &lt;th&gt;ÊñáÊ°£ËØ¥Êòé&lt;/th&gt; 
    &lt;th&gt;Ê®°Âûã‰∏ãËΩΩ&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;ÂÜú‰∏ö&lt;/td&gt; 
    &lt;td&gt;ÂÜú‰ΩúÁâ©Ê£ÄÊµã&lt;/td&gt; 
    &lt;td&gt;Áî®‰∫éËë°ËêÑÊ†ΩÂüπ‰∏≠Âü∫‰∫éÂõæÂÉèÁöÑÁõëÊµãÂíåÁé∞Âú∫Êú∫Âô®‰∫∫ÊäÄÊúØÔºåÊèê‰æõ‰∫ÜÊù•Ëá™5Áßç‰∏çÂêåËë°ËêÑÂìÅÁßçÁöÑÂÆûÂú∞ÂÆû‰æã&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ppyoloe/application/README.md"&gt;PP-YOLOE+ ‰∏ãÊ∏∏‰ªªÂä°&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ppyoloe/application/README.md"&gt;‰∏ãËΩΩÈìæÊé•&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;ÈÄöÁî®&lt;/td&gt; 
    &lt;td&gt;‰ΩéÂÖâÂú∫ÊôØÊ£ÄÊµã&lt;/td&gt; 
    &lt;td&gt;‰ΩéÂÖâÊï∞ÊçÆÈõÜ‰ΩøÁî®ExDarkÔºåÂåÖÊã¨‰ªéÊûÅ‰ΩéÂÖâÁéØÂ¢ÉÂà∞ÊöÆÂÖâÁéØÂ¢ÉÁ≠â10Áßç‰∏çÂêåÂÖâÁÖßÊù°‰ª∂‰∏ãÁöÑÂõæÁâá„ÄÇ&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ppyoloe/application/README.md"&gt;PP-YOLOE+ ‰∏ãÊ∏∏‰ªªÂä°&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ppyoloe/application/README.md"&gt;‰∏ãËΩΩÈìæÊé•&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Â∑•‰∏ö&lt;/td&gt; 
    &lt;td&gt;PCBÁîµË∑ØÊùøÁëïÁñµÊ£ÄÊµã&lt;/td&gt; 
    &lt;td&gt;Â∑•‰∏öÊï∞ÊçÆÈõÜ‰ΩøÁî®PKU-Market-PCBÔºåËØ•Êï∞ÊçÆÈõÜÁî®‰∫éÂç∞Âà∑ÁîµË∑ØÊùøÔºàPCBÔºâÁöÑÁëïÁñµÊ£ÄÊµãÔºåÊèê‰æõ‰∫Ü6ÁßçÂ∏∏ËßÅÁöÑPCBÁº∫Èô∑&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ppyoloe/application/README.md"&gt;PP-YOLOE+ ‰∏ãÊ∏∏‰ªªÂä°&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ppyoloe/application/README.md"&gt;‰∏ãËΩΩÈìæÊé•&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;üíéPP-YOLOE-R È´òÊÄßËÉΩÊóãËΩ¨Ê°ÜÊ£ÄÊµãÊ®°Âûã&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; ÁÆÄ‰ªã(ÁÇπÂáªÂ±ïÂºÄ)&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;PP-YOLOE-RÊòØ‰∏Ä‰∏™È´òÊïàÁöÑÂçïÈò∂ÊÆµAnchor-freeÊóãËΩ¨Ê°ÜÊ£ÄÊµãÊ®°ÂûãÔºåÂü∫‰∫éPP-YOLOE+ÂºïÂÖ•‰∫Ü‰∏ÄÁ≥ªÂàóÊîπËøõÁ≠ñÁï•Êù•ÊèêÂçáÊ£ÄÊµãÁ≤æÂ∫¶„ÄÇÊ†πÊçÆ‰∏çÂêåÁöÑÁ°¨‰ª∂ÂØπÁ≤æÂ∫¶ÂíåÈÄüÂ∫¶ÁöÑË¶ÅÊ±ÇÔºåPP-YOLOE-RÂåÖÂê´s/m/l/xÂõõ‰∏™Â∞∫ÂØ∏ÁöÑÊ®°Âûã„ÄÇÂú®DOTA 1.0Êï∞ÊçÆÈõÜ‰∏äÔºåPP-YOLOE-R-lÂíåPP-YOLOE-R-xÂú®ÂçïÂ∞∫Â∫¶ËÆ≠ÁªÉÂíåÊµãËØïÁöÑÊÉÖÂÜµ‰∏ãÂàÜÂà´ËææÂà∞‰∫Ü78.14mAPÂíå78.28 mAPÔºåËøôÂú®ÂçïÂ∞∫Â∫¶ËØÑ‰º∞‰∏ãË∂ÖË∂ä‰∫ÜÂá†‰πéÊâÄÊúâÁöÑÊóãËΩ¨Ê°ÜÊ£ÄÊµãÊ®°Âûã„ÄÇÈÄöËøáÂ§öÂ∞∫Â∫¶ËÆ≠ÁªÉÂíåÊµãËØïÔºåPP-YOLOE-R-lÂíåPP-YOLOE-R-xÁöÑÊ£ÄÊµãÁ≤æÂ∫¶Ëøõ‰∏ÄÊ≠•ÊèêÂçáËá≥80.02mAPÂíå80.73 mAPÔºåË∂ÖË∂ä‰∫ÜÊâÄÊúâÁöÑAnchor-freeÊñπÊ≥ïÂπ∂‰∏îÂíåÊúÄÂÖàËøõÁöÑAnchor-basedÁöÑ‰∏§Èò∂ÊÆµÊ®°ÂûãÁ≤æÂ∫¶Âá†‰πéÁõ∏ÂΩì„ÄÇÂú®‰øùÊåÅÈ´òÁ≤æÂ∫¶ÁöÑÂêåÊó∂ÔºåPP-YOLOE-RÈÅøÂÖç‰ΩøÁî®ÁâπÊÆäÁöÑÁÆóÂ≠êÔºå‰æãÂ¶ÇDeformable ConvolutionÊàñRotated RoI AlignÔºå‰ΩøÂÖ∂ËÉΩËΩªÊùæÂú∞ÈÉ®ÁΩ≤Âú®Â§öÁßçÂ§öÊ†∑ÁöÑÁ°¨‰ª∂‰∏ä„ÄÇ&lt;/p&gt; 
 &lt;p&gt;&lt;code&gt;‰º†ÈÄÅÈó®&lt;/code&gt;Ôºö&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/rotate/ppyoloe_r"&gt;PP-YOLOE-RËØ¥Êòé&lt;/a&gt;„ÄÇ&lt;/p&gt; 
 &lt;p&gt;&lt;code&gt;‰º†ÈÄÅÈó®&lt;/code&gt;Ôºö&lt;a href="https://arxiv.org/abs/2211.02386"&gt;arXivËÆ∫Êñá&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; È¢ÑËÆ≠ÁªÉÊ®°Âûã(ÁÇπÂáªÂ±ïÂºÄ)&lt;/b&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;Ê®°Âûã&lt;/th&gt; 
    &lt;th align="center"&gt;Backbone&lt;/th&gt; 
    &lt;th align="center"&gt;mAP&lt;/th&gt; 
    &lt;th align="center"&gt;V100 TRT FP16 (FPS)&lt;/th&gt; 
    &lt;th align="center"&gt;RTX 2080 Ti TRT FP16 (FPS)&lt;/th&gt; 
    &lt;th align="center"&gt;Params (M)&lt;/th&gt; 
    &lt;th align="center"&gt;FLOPs (G)&lt;/th&gt; 
    &lt;th align="center"&gt;Â≠¶‰π†ÁéáÁ≠ñÁï•&lt;/th&gt; 
    &lt;th align="center"&gt;ËßíÂ∫¶Ë°®Á§∫&lt;/th&gt; 
    &lt;th align="center"&gt;Êï∞ÊçÆÂ¢ûÂπø&lt;/th&gt; 
    &lt;th align="center"&gt;GPUÊï∞ÁõÆ&lt;/th&gt; 
    &lt;th align="center"&gt;ÊØèGPUÂõæÁâáÊï∞ÁõÆ&lt;/th&gt; 
    &lt;th align="center"&gt;Ê®°Âûã‰∏ãËΩΩ&lt;/th&gt; 
    &lt;th align="center"&gt;ÈÖçÁΩÆÊñá‰ª∂&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;PP-YOLOE-R-l&lt;/td&gt; 
    &lt;td align="center"&gt;CRN-l&lt;/td&gt; 
    &lt;td align="center"&gt;80.02&lt;/td&gt; 
    &lt;td align="center"&gt;69.7&lt;/td&gt; 
    &lt;td align="center"&gt;48.3&lt;/td&gt; 
    &lt;td align="center"&gt;53.29&lt;/td&gt; 
    &lt;td align="center"&gt;281.65&lt;/td&gt; 
    &lt;td align="center"&gt;3x&lt;/td&gt; 
    &lt;td align="center"&gt;oc&lt;/td&gt; 
    &lt;td align="center"&gt;MS+RR&lt;/td&gt; 
    &lt;td align="center"&gt;4&lt;/td&gt; 
    &lt;td align="center"&gt;2&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://paddledet.bj.bcebos.com/models/ppyoloe_r_crn_l_3x_dota_ms.pdparams"&gt;model&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleDetection/tree/develop/configs/rotate/ppyoloe_r/ppyoloe_r_crn_l_3x_dota_ms.yml"&gt;config&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;code&gt;‰º†ÈÄÅÈó®&lt;/code&gt;Ôºö&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/rotate/ppyoloe_r"&gt;ÂÖ®ÈÉ®È¢ÑËÆ≠ÁªÉÊ®°Âûã&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; ‰∫ß‰∏öÂ∫îÁî®‰ª£Á†ÅÁ§∫‰æã(ÁÇπÂáªÂ±ïÂºÄ)&lt;/b&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Ë°å‰∏ö&lt;/th&gt; 
    &lt;th&gt;Á±ªÂà´&lt;/th&gt; 
    &lt;th&gt;‰∫ÆÁÇπ&lt;/th&gt; 
    &lt;th&gt;ÊñáÊ°£ËØ¥Êòé&lt;/th&gt; 
    &lt;th&gt;Ê®°Âûã‰∏ãËΩΩ&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;ÈÄöÁî®&lt;/td&gt; 
    &lt;td&gt;ÊóãËΩ¨Ê°ÜÊ£ÄÊµã&lt;/td&gt; 
    &lt;td&gt;ÊâãÊääÊâãÊïô‰Ω†‰∏äÊâãPP-YOLOE-RÊóãËΩ¨Ê°ÜÊ£ÄÊµãÔºå10ÂàÜÈíüÂ∞ÜËÑäÊü±Êï∞ÊçÆÈõÜÁ≤æÂ∫¶ËÆ≠ÁªÉËá≥95mAP&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/5058293"&gt;Âü∫‰∫éPP-YOLOE-RÁöÑÊóãËΩ¨Ê°ÜÊ£ÄÊµã&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/5058293"&gt;‰∏ãËΩΩÈìæÊé•&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;üíéPP-YOLOE-SOD È´òÁ≤æÂ∫¶Â∞èÁõÆÊ†áÊ£ÄÊµãÊ®°Âûã&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; ÁÆÄ‰ªã(ÁÇπÂáªÂ±ïÂºÄ)&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;PP-YOLOE-SOD(Small Object Detection)ÊòØPaddleDetectionÂõ¢ÈòüÈíàÂØπÂ∞èÁõÆÊ†áÊ£ÄÊµãÊèêÂá∫ÁöÑÊ£ÄÊµãÊñπÊ°àÔºåÂú®VisDrone-DETÊï∞ÊçÆÈõÜ‰∏äÂçïÊ®°ÂûãÁ≤æÂ∫¶ËææÂà∞38.5mAPÔºåËææÂà∞‰∫ÜSOTAÊÄßËÉΩ„ÄÇÂÖ∂ÂàÜÂà´Âü∫‰∫éÂàáÂõæÊãºÂõæÊµÅÁ®ã‰ºòÂåñÁöÑÂ∞èÁõÆÊ†áÊ£ÄÊµãÊñπÊ°à‰ª•ÂèäÂü∫‰∫éÂéüÂõæÊ®°ÂûãÁÆóÊ≥ï‰ºòÂåñÁöÑÂ∞èÁõÆÊ†áÊ£ÄÊµãÊñπÊ°à„ÄÇÂêåÊó∂Êèê‰æõ‰∫ÜÊï∞ÊçÆÈõÜËá™Âä®ÂàÜÊûêËÑöÊú¨ÔºåÂè™ÈúÄËæìÂÖ•Êï∞ÊçÆÈõÜÊ†áÊ≥®Êñá‰ª∂Ôºå‰æøÂèØÂæóÂà∞Êï∞ÊçÆÈõÜÁªüËÆ°ÁªìÊûúÔºåËæÖÂä©Âà§Êñ≠Êï∞ÊçÆÈõÜÊòØÂê¶ÊòØÂ∞èÁõÆÊ†áÊï∞ÊçÆÈõÜ‰ª•ÂèäÊòØÂê¶ÈúÄË¶ÅÈááÁî®ÂàáÂõæÁ≠ñÁï•ÔºåÂêåÊó∂ÁªôÂá∫ÁΩëÁªúË∂ÖÂèÇÊï∞ÂèÇËÄÉÂÄº„ÄÇ&lt;/p&gt; 
 &lt;p&gt;&lt;code&gt;‰º†ÈÄÅÈó®&lt;/code&gt;Ôºö&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/smalldet"&gt;PP-YOLOE-SOD Â∞èÁõÆÊ†áÊ£ÄÊµãÊ®°Âûã&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; È¢ÑËÆ≠ÁªÉÊ®°Âûã(ÁÇπÂáªÂ±ïÂºÄ)&lt;/b&gt;&lt;/summary&gt; - VisDroneÊï∞ÊçÆÈõÜÈ¢ÑËÆ≠ÁªÉÊ®°Âûã 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;Ê®°Âûã&lt;/th&gt; 
    &lt;th align="center"&gt;COCOAPI mAP&lt;sup&gt;val&lt;br /&gt;0.5:0.95&lt;/sup&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;COCOAPI mAP&lt;sup&gt;val&lt;br /&gt;0.5&lt;/sup&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;COCOAPI mAP&lt;sup&gt;test_dev&lt;br /&gt;0.5:0.95&lt;/sup&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;COCOAPI mAP&lt;sup&gt;test_dev&lt;br /&gt;0.5&lt;/sup&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;MatlabAPI mAP&lt;sup&gt;test_dev&lt;br /&gt;0.5:0.95&lt;/sup&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;MatlabAPI mAP&lt;sup&gt;test_dev&lt;br /&gt;0.5&lt;/sup&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;‰∏ãËΩΩ&lt;/th&gt; 
    &lt;th align="center"&gt;ÈÖçÁΩÆÊñá‰ª∂&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;strong&gt;PP-YOLOE+_SOD-l&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;31.9&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;52.1&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;25.6&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;43.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;30.25&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;51.18&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://paddledet.bj.bcebos.com/models/ppyoloe_plus_sod_crn_l_80e_visdrone.pdparams"&gt;‰∏ãËΩΩÈìæÊé•&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/visdrone/ppyoloe_plus_sod_crn_l_80e_visdrone.yml"&gt;ÈÖçÁΩÆÊñá‰ª∂&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;code&gt;‰º†ÈÄÅÈó®&lt;/code&gt;Ôºö&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/smalldet"&gt;ÂÖ®ÈÉ®È¢ÑËÆ≠ÁªÉÊ®°Âûã&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; ‰∫ß‰∏öÂ∫îÁî®‰ª£Á†ÅÁ§∫‰æã(ÁÇπÂáªÂ±ïÂºÄ)&lt;/b&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Ë°å‰∏ö&lt;/th&gt; 
    &lt;th&gt;Á±ªÂà´&lt;/th&gt; 
    &lt;th&gt;‰∫ÆÁÇπ&lt;/th&gt; 
    &lt;th&gt;ÊñáÊ°£ËØ¥Êòé&lt;/th&gt; 
    &lt;th&gt;Ê®°Âûã‰∏ãËΩΩ&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;ÈÄöÁî®&lt;/td&gt; 
    &lt;td&gt;Â∞èÁõÆÊ†áÊ£ÄÊµã&lt;/td&gt; 
    &lt;td&gt;Âü∫‰∫éPP-YOLOE-SODÁöÑÊó†‰∫∫Êú∫Ëà™ÊãçÂõæÂÉèÊ£ÄÊµãÊ°à‰æãÂÖ®ÊµÅÁ®ãÂÆûÊìç„ÄÇ&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/5036782"&gt;Âü∫‰∫éPP-YOLOE-SODÁöÑÊó†‰∫∫Êú∫Ëà™ÊãçÂõæÂÉèÊ£ÄÊµã&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/5036782"&gt;‰∏ãËΩΩÈìæÊé•&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;üí´PP-PicoDet Ë∂ÖËΩªÈáèÂÆûÊó∂ÁõÆÊ†áÊ£ÄÊµãÊ®°Âûã&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; ÁÆÄ‰ªã(ÁÇπÂáªÂ±ïÂºÄ)&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;ÂÖ®Êñ∞ÁöÑËΩªÈáèÁ∫ßÁ≥ªÂàóÊ®°ÂûãPP-PicoDetÔºåÂú®ÁßªÂä®Á´ØÂÖ∑ÊúâÂçìË∂äÁöÑÊÄßËÉΩÔºåÊàê‰∏∫ÂÖ®Êñ∞SOTAËΩªÈáèÁ∫ßÊ®°Âûã„ÄÇ&lt;/p&gt; 
 &lt;p&gt;&lt;code&gt;‰º†ÈÄÅÈó®&lt;/code&gt;Ôºö&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/picodet/README.md"&gt;PP-PicoDetËØ¥Êòé&lt;/a&gt;„ÄÇ&lt;/p&gt; 
 &lt;p&gt;&lt;code&gt;‰º†ÈÄÅÈó®&lt;/code&gt;Ôºö&lt;a href="https://arxiv.org/abs/2111.00902"&gt;arXivËÆ∫Êñá&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; È¢ÑËÆ≠ÁªÉÊ®°Âûã(ÁÇπÂáªÂ±ïÂºÄ)&lt;/b&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;Ê®°ÂûãÂêçÁß∞&lt;/th&gt; 
    &lt;th align="center"&gt;COCOÁ≤æÂ∫¶ÔºàmAPÔºâ&lt;/th&gt; 
    &lt;th align="center"&gt;È™ÅÈæô865 ÂõõÁ∫øÁ®ãÈÄüÂ∫¶(FPS)&lt;/th&gt; 
    &lt;th align="center"&gt;Êé®ËçêÈÉ®ÁΩ≤Á°¨‰ª∂&lt;/th&gt; 
    &lt;th align="center"&gt;ÈÖçÁΩÆÊñá‰ª∂&lt;/th&gt; 
    &lt;th align="center"&gt;Ê®°Âûã‰∏ãËΩΩ&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;PicoDet-L&lt;/td&gt; 
    &lt;td align="center"&gt;36.1&lt;/td&gt; 
    &lt;td align="center"&gt;39.7&lt;/td&gt; 
    &lt;td align="center"&gt;ÁßªÂä®Á´Ø„ÄÅÂµåÂÖ•Âºè&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/picodet/picodet_l_320_coco_lcnet.yml"&gt;ÈìæÊé•&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://paddledet.bj.bcebos.com/models/picodet_l_320_coco_lcnet.pdparams"&gt;‰∏ãËΩΩÂú∞ÂùÄ&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;code&gt;‰º†ÈÄÅÈó®&lt;/code&gt;Ôºö&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/picodet/README.md"&gt;ÂÖ®ÈÉ®È¢ÑËÆ≠ÁªÉÊ®°Âûã&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; ‰∫ß‰∏öÂ∫îÁî®‰ª£Á†ÅÁ§∫‰æã(ÁÇπÂáªÂ±ïÂºÄ)&lt;/b&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Ë°å‰∏ö&lt;/th&gt; 
    &lt;th&gt;Á±ªÂà´&lt;/th&gt; 
    &lt;th&gt;‰∫ÆÁÇπ&lt;/th&gt; 
    &lt;th&gt;ÊñáÊ°£ËØ¥Êòé&lt;/th&gt; 
    &lt;th&gt;Ê®°Âûã‰∏ãËΩΩ&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Êô∫ÊÖßÂüéÂ∏Ç&lt;/td&gt; 
    &lt;td&gt;ÈÅìË∑ØÂûÉÂúæÊ£ÄÊµã&lt;/td&gt; 
    &lt;td&gt;ÈÄöËøáÂú®Â∏ÇÊîøÁéØÂç´ËΩ¶ËæÜ‰∏äÂÆâË£ÖÊëÑÂÉèÂ§¥ÂØπË∑ØÈù¢ÂûÉÂúæÊ£ÄÊµãÂπ∂ÂàÜÊûêÔºåÂÆûÁé∞ÂØπË∑ØÈù¢ÈÅóÊííÁöÑÂûÉÂúæËøõË°åÁõëÊéßÔºåËÆ∞ÂΩïÂπ∂ÈÄöÁü•ÁéØÂç´‰∫∫ÂëòÊ∏ÖÁêÜÔºåÂ§ßÂ§ßÊèêÂçá‰∫ÜÁéØÂç´‰∫∫Êïà„ÄÇ&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/3846170?channelType=0&amp;amp;channel=0"&gt;Âü∫‰∫éPP-PicoDetÁöÑË∑ØÈù¢ÂûÉÂúæÊ£ÄÊµã&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/3846170?channelType=0&amp;amp;channel=0"&gt;‰∏ãËΩΩÈìæÊé•&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;üì°PP-Tracking ÂÆûÊó∂Â§öÁõÆÊ†áË∑üË∏™Á≥ªÁªü&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; ÁÆÄ‰ªã(ÁÇπÂáªÂ±ïÂºÄ)&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;PaddleDetectionÂõ¢ÈòüÊèê‰æõ‰∫ÜÂÆûÊó∂Â§öÁõÆÊ†áË∑üË∏™Á≥ªÁªüPP-TrackingÔºåÊòØÂü∫‰∫éPaddlePaddleÊ∑±Â∫¶Â≠¶‰π†Ê°ÜÊû∂ÁöÑ‰∏öÁïåÈ¶ñ‰∏™ÂºÄÊ∫êÁöÑÂÆûÊó∂Â§öÁõÆÊ†áË∑üË∏™Á≥ªÁªüÔºåÂÖ∑ÊúâÊ®°Âûã‰∏∞ÂØå„ÄÅÂ∫îÁî®ÂπøÊ≥õÂíåÈÉ®ÁΩ≤È´òÊïà‰∏âÂ§ß‰ºòÂäø„ÄÇ PP-TrackingÊîØÊåÅÂçïÈïúÂ§¥Ë∑üË∏™(MOT)ÂíåË∑®ÈïúÂ§¥Ë∑üË∏™(MTMCT)‰∏§ÁßçÊ®°ÂºèÔºåÈíàÂØπÂÆûÈôÖ‰∏öÂä°ÁöÑÈöæÁÇπÂíåÁóõÁÇπÔºåÊèê‰æõ‰∫ÜË°å‰∫∫Ë∑üË∏™„ÄÅËΩ¶ËæÜË∑üË∏™„ÄÅÂ§öÁ±ªÂà´Ë∑üË∏™„ÄÅÂ∞èÁõÆÊ†áË∑üË∏™„ÄÅÊµÅÈáèÁªüËÆ°‰ª•ÂèäË∑®ÈïúÂ§¥Ë∑üË∏™Á≠âÂêÑÁßçÂ§öÁõÆÊ†áË∑üË∏™ÂäüËÉΩÂíåÂ∫îÁî®ÔºåÈÉ®ÁΩ≤ÊñπÂºèÊîØÊåÅAPIË∞ÉÁî®ÂíåGUIÂèØËßÜÂåñÁïåÈù¢ÔºåÈÉ®ÁΩ≤ËØ≠Ë®ÄÊîØÊåÅPythonÂíåC++ÔºåÈÉ®ÁΩ≤Âπ≥Âè∞ÁéØÂ¢ÉÊîØÊåÅLinux„ÄÅNVIDIA JetsonÁ≠â„ÄÇ&lt;/p&gt; 
 &lt;p&gt;&lt;code&gt;‰º†ÈÄÅÈó®&lt;/code&gt;Ôºö&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/mot/README.md"&gt;PP-TrackingËØ¥Êòé&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; È¢ÑËÆ≠ÁªÉÊ®°Âûã(ÁÇπÂáªÂ±ïÂºÄ)&lt;/b&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;Ê®°ÂûãÂêçÁß∞&lt;/th&gt; 
    &lt;th align="center"&gt;Ê®°ÂûãÁÆÄ‰ªã&lt;/th&gt; 
    &lt;th align="center"&gt;Á≤æÂ∫¶&lt;/th&gt; 
    &lt;th align="center"&gt;ÈÄüÂ∫¶(FPS)&lt;/th&gt; 
    &lt;th align="center"&gt;Êé®ËçêÈÉ®ÁΩ≤Á°¨‰ª∂&lt;/th&gt; 
    &lt;th align="center"&gt;ÈÖçÁΩÆÊñá‰ª∂&lt;/th&gt; 
    &lt;th align="center"&gt;Ê®°Âûã‰∏ãËΩΩ&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;ByteTrack&lt;/td&gt; 
    &lt;td align="center"&gt;SDEÂ§öÁõÆÊ†áË∑üË∏™ÁÆóÊ≥ï ‰ªÖÂåÖÂê´Ê£ÄÊµãÊ®°Âûã&lt;/td&gt; 
    &lt;td align="center"&gt;MOT-17 test: 78.4&lt;/td&gt; 
    &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;td align="center"&gt;ÊúçÂä°Âô®„ÄÅÁßªÂä®Á´Ø„ÄÅÂµåÂÖ•Âºè&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/mot/bytetrack/bytetrack_yolox.yml"&gt;ÈìæÊé•&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/mot/yolox_x_24e_800x1440_mix_det.pdparams"&gt;‰∏ãËΩΩÂú∞ÂùÄ&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;FairMOT&lt;/td&gt; 
    &lt;td align="center"&gt;JDEÂ§öÁõÆÊ†áË∑üË∏™ÁÆóÊ≥ï Â§ö‰ªªÂä°ËÅîÂêàÂ≠¶‰π†ÊñπÊ≥ï&lt;/td&gt; 
    &lt;td align="center"&gt;MOT-16 test: 75.0&lt;/td&gt; 
    &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;td align="center"&gt;ÊúçÂä°Âô®„ÄÅÁßªÂä®Á´Ø„ÄÅÂµåÂÖ•Âºè&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/mot/fairmot/fairmot_dla34_30e_1088x608.yml"&gt;ÈìæÊé•&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://paddledet.bj.bcebos.com/models/mot/fairmot_dla34_30e_1088x608.pdparams"&gt;‰∏ãËΩΩÂú∞ÂùÄ&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;OC-SORT&lt;/td&gt; 
    &lt;td align="center"&gt;SDEÂ§öÁõÆÊ†áË∑üË∏™ÁÆóÊ≥ï ‰ªÖÂåÖÂê´Ê£ÄÊµãÊ®°Âûã&lt;/td&gt; 
    &lt;td align="center"&gt;MOT-17 half val: 75.5&lt;/td&gt; 
    &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;td align="center"&gt;ÊúçÂä°Âô®„ÄÅÁßªÂä®Á´Ø„ÄÅÂµåÂÖ•Âºè&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/mot/ocsort/ocsort_yolox.yml"&gt;ÈìæÊé•&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/mot/yolox_x_24e_800x1440_mix_mot_ch.pdparams"&gt;‰∏ãËΩΩÂú∞ÂùÄ&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; ‰∫ß‰∏öÂ∫îÁî®‰ª£Á†ÅÁ§∫‰æã(ÁÇπÂáªÂ±ïÂºÄ)&lt;/b&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Ë°å‰∏ö&lt;/th&gt; 
    &lt;th&gt;Á±ªÂà´&lt;/th&gt; 
    &lt;th&gt;‰∫ÆÁÇπ&lt;/th&gt; 
    &lt;th&gt;ÊñáÊ°£ËØ¥Êòé&lt;/th&gt; 
    &lt;th&gt;Ê®°Âûã‰∏ãËΩΩ&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;ÈÄöÁî®&lt;/td&gt; 
    &lt;td&gt;Â§öÁõÆÊ†áË∑üË∏™&lt;/td&gt; 
    &lt;td&gt;Âø´ÈÄü‰∏äÊâãÂçïÈïúÂ§¥„ÄÅÂ§öÈïúÂ§¥Ë∑üË∏™&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/3022582"&gt;PP-Tracking‰πãÊâãÊääÊâãÁé©ËΩ¨Â§öÁõÆÊ†áË∑üË∏™&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/3022582"&gt;‰∏ãËΩΩÈìæÊé•&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;‚õ∑Ô∏èPP-TinyPose ‰∫∫‰ΩìÈ™®È™ºÂÖ≥ÈîÆÁÇπËØÜÂà´&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; ÁÆÄ‰ªã(ÁÇπÂáªÂ±ïÂºÄ)&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;PaddleDetection ‰∏≠ÁöÑÂÖ≥ÈîÆÁÇπÊ£ÄÊµãÈÉ®ÂàÜÁ¥ßË∑üÊúÄÂÖàËøõÁöÑÁÆóÊ≥ïÔºåÂåÖÊã¨ Top-Down Âíå Bottom-Up ‰∏§ÁßçÊñπÊ≥ïÔºåÂèØ‰ª•Êª°Ë∂≥Áî®Êà∑ÁöÑ‰∏çÂêåÈúÄÊ±Ç„ÄÇÂêåÊó∂ÔºåPaddleDetection Êèê‰æõÈíàÂØπÁßªÂä®Á´ØËÆæÂ§á‰ºòÂåñÁöÑËá™Á†îÂÆûÊó∂ÂÖ≥ÈîÆÁÇπÊ£ÄÊµãÊ®°Âûã PP-TinyPose„ÄÇ&lt;/p&gt; 
 &lt;p&gt;&lt;code&gt;‰º†ÈÄÅÈó®&lt;/code&gt;Ôºö&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/keypoint/tiny_pose"&gt;PP-TinyPoseËØ¥Êòé&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; È¢ÑËÆ≠ÁªÉÊ®°Âûã(ÁÇπÂáªÂ±ïÂºÄ)&lt;/b&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;Ê®°ÂûãÂêçÁß∞&lt;/th&gt; 
    &lt;th align="center"&gt;Ê®°ÂûãÁÆÄ‰ªã&lt;/th&gt; 
    &lt;th align="center"&gt;COCOÁ≤æÂ∫¶ÔºàAPÔºâ&lt;/th&gt; 
    &lt;th align="center"&gt;ÈÄüÂ∫¶(FPS)&lt;/th&gt; 
    &lt;th align="center"&gt;Êé®ËçêÈÉ®ÁΩ≤Á°¨‰ª∂&lt;/th&gt; 
    &lt;th align="center"&gt;ÈÖçÁΩÆÊñá‰ª∂&lt;/th&gt; 
    &lt;th align="center"&gt;Ê®°Âûã‰∏ãËΩΩ&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;PP-TinyPose&lt;/td&gt; 
    &lt;td align="center"&gt;ËΩªÈáèÁ∫ßÂÖ≥ÈîÆÁÇπÁÆóÊ≥ï&lt;br /&gt;ËæìÂÖ•Â∞∫ÂØ∏256x192&lt;/td&gt; 
    &lt;td align="center"&gt;68.8&lt;/td&gt; 
    &lt;td align="center"&gt;È™ÅÈæô865 ÂõõÁ∫øÁ®ã: 158.7 FPS&lt;/td&gt; 
    &lt;td align="center"&gt;ÁßªÂä®Á´Ø„ÄÅÂµåÂÖ•Âºè&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/keypoint/tiny_pose/tinypose_256x192.yml"&gt;ÈìæÊé•&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/keypoint/tinypose_256x192.pdparams"&gt;‰∏ãËΩΩÂú∞ÂùÄ&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;code&gt;‰º†ÈÄÅÈó®&lt;/code&gt;Ôºö&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/keypoint/README.md"&gt;ÂÖ®ÈÉ®È¢ÑËÆ≠ÁªÉÊ®°Âûã&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; ‰∫ß‰∏öÂ∫îÁî®‰ª£Á†ÅÁ§∫‰æã(ÁÇπÂáªÂ±ïÂºÄ)&lt;/b&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Ë°å‰∏ö&lt;/th&gt; 
    &lt;th&gt;Á±ªÂà´&lt;/th&gt; 
    &lt;th&gt;‰∫ÆÁÇπ&lt;/th&gt; 
    &lt;th&gt;ÊñáÊ°£ËØ¥Êòé&lt;/th&gt; 
    &lt;th&gt;Ê®°Âûã‰∏ãËΩΩ&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;ËøêÂä®&lt;/td&gt; 
    &lt;td&gt;ÂÅ•Ë∫´&lt;/td&gt; 
    &lt;td&gt;Êèê‰æõ‰ªéÊ®°ÂûãÈÄâÂûã„ÄÅÊï∞ÊçÆÂáÜÂ§á„ÄÅÊ®°ÂûãËÆ≠ÁªÉ‰ºòÂåñÔºåÂà∞ÂêéÂ§ÑÁêÜÈÄªËæëÂíåÊ®°ÂûãÈÉ®ÁΩ≤ÁöÑÂÖ®ÊµÅÁ®ãÂèØÂ§çÁî®ÊñπÊ°àÔºåÊúâÊïàËß£ÂÜ≥‰∫ÜÂ§çÊùÇÂÅ•Ë∫´Âä®‰ΩúÁöÑÈ´òÊïàËØÜÂà´ÔºåÊâìÈÄ†AIËôöÊãüÂÅ•Ë∫´ÊïôÁªÉÔºÅ&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4385813"&gt;Âü∫‰∫éPP-TinyPoseÂ¢ûÂº∫ÁâàÁöÑÊô∫ËÉΩÂÅ•Ë∫´Âä®‰ΩúËØÜÂà´&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4385813"&gt;‰∏ãËΩΩÈìæÊé•&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;üèÉüèªPP-Human ÂÆûÊó∂Ë°å‰∫∫ÂàÜÊûêÂ∑•ÂÖ∑&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; ÁÆÄ‰ªã(ÁÇπÂáªÂ±ïÂºÄ)&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;PaddleDetectionÊ∑±ÂÖ•Êé¢Á¥¢Ê†∏ÂøÉË°å‰∏öÁöÑÈ´òÈ¢ëÂú∫ÊôØÔºåÊèê‰æõ‰∫ÜË°å‰∫∫ÂºÄÁÆ±Âç≥Áî®ÂàÜÊûêÂ∑•ÂÖ∑ÔºåÊîØÊåÅÂõæÁâá/ÂçïÈïúÂ§¥ËßÜÈ¢ë/Â§öÈïúÂ§¥ËßÜÈ¢ë/Âú®Á∫øËßÜÈ¢ëÊµÅÂ§öÁßçËæìÂÖ•ÊñπÂºèÔºåÂπøÊ≥õÂ∫îÁî®‰∫éÊô∫ÊÖß‰∫§ÈÄö„ÄÅÊô∫ÊÖßÂüéÂ∏Ç„ÄÅÂ∑•‰∏öÂ∑°Ê£ÄÁ≠âÈ¢ÜÂüü„ÄÇÊîØÊåÅÊúçÂä°Âô®Á´ØÈÉ®ÁΩ≤ÂèäTensorRTÂä†ÈÄüÔºåT4ÊúçÂä°Âô®‰∏äÂèØËææÂà∞ÂÆûÊó∂„ÄÇ PP-HumanÊîØÊåÅÂõõÂ§ß‰∫ß‰∏öÁ∫ßÂäüËÉΩÔºö‰∫îÂ§ßÂºÇÂ∏∏Ë°å‰∏∫ËØÜÂà´„ÄÅ26Áßç‰∫∫‰ΩìÂ±ûÊÄßÂàÜÊûê„ÄÅÂÆûÊó∂‰∫∫ÊµÅËÆ°Êï∞„ÄÅË∑®ÈïúÂ§¥ÔºàReIDÔºâË∑üË∏™„ÄÇ&lt;/p&gt; 
 &lt;p&gt;&lt;code&gt;‰º†ÈÄÅÈó®&lt;/code&gt;Ôºö&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/deploy/pipeline/README.md"&gt;PP-HumanË°å‰∫∫ÂàÜÊûêÂ∑•ÂÖ∑‰ΩøÁî®ÊåáÂçó&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; È¢ÑËÆ≠ÁªÉÊ®°Âûã(ÁÇπÂáªÂ±ïÂºÄ)&lt;/b&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;‰ªªÂä°&lt;/th&gt; 
    &lt;th align="center"&gt;T4 TensorRT FP16: ÈÄüÂ∫¶ÔºàFPSÔºâ&lt;/th&gt; 
    &lt;th align="center"&gt;Êé®ËçêÈÉ®ÁΩ≤Á°¨‰ª∂&lt;/th&gt; 
    &lt;th align="center"&gt;Ê®°Âûã‰∏ãËΩΩ&lt;/th&gt; 
    &lt;th align="center"&gt;Ê®°Âûã‰ΩìÁßØ&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Ë°å‰∫∫Ê£ÄÊµãÔºàÈ´òÁ≤æÂ∫¶Ôºâ&lt;/td&gt; 
    &lt;td align="center"&gt;39.8&lt;/td&gt; 
    &lt;td align="center"&gt;ÊúçÂä°Âô®&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip"&gt;ÁõÆÊ†áÊ£ÄÊµã&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;182M&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Ë°å‰∫∫Ë∑üË∏™ÔºàÈ´òÁ≤æÂ∫¶Ôºâ&lt;/td&gt; 
    &lt;td align="center"&gt;31.4&lt;/td&gt; 
    &lt;td align="center"&gt;ÊúçÂä°Âô®&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip"&gt;Â§öÁõÆÊ†áË∑üË∏™&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;182M&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Â±ûÊÄßËØÜÂà´ÔºàÈ´òÁ≤æÂ∫¶Ôºâ&lt;/td&gt; 
    &lt;td align="center"&gt;Âçï‰∫∫ 117.6&lt;/td&gt; 
    &lt;td align="center"&gt;ÊúçÂä°Âô®&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip"&gt;ÁõÆÊ†áÊ£ÄÊµã&lt;/a&gt;&lt;br /&gt; &lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/PPHGNet_small_person_attribute_954_infer.zip"&gt;Â±ûÊÄßËØÜÂà´&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;ÁõÆÊ†áÊ£ÄÊµãÔºö182M&lt;br /&gt;Â±ûÊÄßËØÜÂà´Ôºö86M&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;ÊëîÂÄíËØÜÂà´&lt;/td&gt; 
    &lt;td align="center"&gt;Âçï‰∫∫ 100&lt;/td&gt; 
    &lt;td align="center"&gt;ÊúçÂä°Âô®&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip"&gt;Â§öÁõÆÊ†áË∑üË∏™&lt;/a&gt; &lt;br /&gt; &lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/dark_hrnet_w32_256x192.zip"&gt;ÂÖ≥ÈîÆÁÇπÊ£ÄÊµã&lt;/a&gt; &lt;br /&gt; &lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/STGCN.zip"&gt;Âü∫‰∫éÂÖ≥ÈîÆÁÇπË°å‰∏∫ËØÜÂà´&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;Â§öÁõÆÊ†áË∑üË∏™Ôºö182M&lt;br /&gt;ÂÖ≥ÈîÆÁÇπÊ£ÄÊµãÔºö101M&lt;br /&gt;Âü∫‰∫éÂÖ≥ÈîÆÁÇπË°å‰∏∫ËØÜÂà´Ôºö21.8M&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;ÈóØÂÖ•ËØÜÂà´&lt;/td&gt; 
    &lt;td align="center"&gt;31.4&lt;/td&gt; 
    &lt;td align="center"&gt;ÊúçÂä°Âô®&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip"&gt;Â§öÁõÆÊ†áË∑üË∏™&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;182M&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;ÊâìÊû∂ËØÜÂà´&lt;/td&gt; 
    &lt;td align="center"&gt;50.8&lt;/td&gt; 
    &lt;td align="center"&gt;ÊúçÂä°Âô®&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip"&gt;ËßÜÈ¢ëÂàÜÁ±ª&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;90M&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;ÊäΩÁÉüËØÜÂà´&lt;/td&gt; 
    &lt;td align="center"&gt;340.1&lt;/td&gt; 
    &lt;td align="center"&gt;ÊúçÂä°Âô®&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip"&gt;ÁõÆÊ†áÊ£ÄÊµã&lt;/a&gt;&lt;br /&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/ppyoloe_crn_s_80e_smoking_visdrone.zip"&gt;Âü∫‰∫é‰∫∫‰ΩìidÁöÑÁõÆÊ†áÊ£ÄÊµã&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;ÁõÆÊ†áÊ£ÄÊµãÔºö182M&lt;br /&gt;Âü∫‰∫é‰∫∫‰ΩìidÁöÑÁõÆÊ†áÊ£ÄÊµãÔºö27M&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;ÊâìÁîµËØùËØÜÂà´&lt;/td&gt; 
    &lt;td align="center"&gt;166.7&lt;/td&gt; 
    &lt;td align="center"&gt;ÊúçÂä°Âô®&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip"&gt;ÁõÆÊ†áÊ£ÄÊµã&lt;/a&gt;&lt;br /&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/PPHGNet_tiny_calling_halfbody.zip"&gt;Âü∫‰∫é‰∫∫‰ΩìidÁöÑÂõæÂÉèÂàÜÁ±ª&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;ÁõÆÊ†áÊ£ÄÊµãÔºö182M&lt;br /&gt;Âü∫‰∫é‰∫∫‰ΩìidÁöÑÂõæÂÉèÂàÜÁ±ªÔºö45M&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;code&gt;‰º†ÈÄÅÈó®&lt;/code&gt;Ôºö&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/deploy/pipeline/README.md"&gt;ÂÆåÊï¥È¢ÑËÆ≠ÁªÉÊ®°Âûã&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; ‰∫ß‰∏öÂ∫îÁî®‰ª£Á†ÅÁ§∫‰æã(ÁÇπÂáªÂ±ïÂºÄ)&lt;/b&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Ë°å‰∏ö&lt;/th&gt; 
    &lt;th&gt;Á±ªÂà´&lt;/th&gt; 
    &lt;th&gt;‰∫ÆÁÇπ&lt;/th&gt; 
    &lt;th&gt;ÊñáÊ°£ËØ¥Êòé&lt;/th&gt; 
    &lt;th&gt;Ê®°Âûã‰∏ãËΩΩ&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Êô∫ËÉΩÂÆâÈò≤&lt;/td&gt; 
    &lt;td&gt;ÊëîÂÄíÊ£ÄÊµã&lt;/td&gt; 
    &lt;td&gt;È£ûÊ°®Ë°å‰∫∫ÂàÜÊûêPP-Human‰∏≠Êèê‰æõÁöÑÊëîÂÄíËØÜÂà´ÁÆóÊ≥ïÔºåÈááÁî®‰∫ÜÂÖ≥ÈîÆÁÇπ+Êó∂Á©∫ÂõæÂç∑ÁßØÁΩëÁªúÁöÑÊäÄÊúØÔºåÂØπÊëîÂÄíÂßøÂäøÊó†ÈôêÂà∂„ÄÅËÉåÊôØÁéØÂ¢ÉÊó†Ë¶ÅÊ±Ç„ÄÇ&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4606001"&gt;Âü∫‰∫éPP-Human v2ÁöÑÊëîÂÄíÊ£ÄÊµã&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4606001"&gt;‰∏ãËΩΩÈìæÊé•&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Êô∫ËÉΩÂÆâÈò≤&lt;/td&gt; 
    &lt;td&gt;ÊâìÊû∂ËØÜÂà´&lt;/td&gt; 
    &lt;td&gt;Êú¨È°πÁõÆÂü∫‰∫éPaddleVideoËßÜÈ¢ëÂºÄÂèëÂ•ó‰ª∂ËÆ≠ÁªÉÊâìÊû∂ËØÜÂà´Ê®°ÂûãÔºåÁÑ∂ÂêéÂ∞ÜËÆ≠ÁªÉÂ•ΩÁöÑÊ®°ÂûãÈõÜÊàêÂà∞PaddleDetectionÁöÑPP-Human‰∏≠ÔºåÂä©ÂäõË°å‰∫∫Ë°å‰∏∫ÂàÜÊûê„ÄÇ&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4086987?contributionType=1"&gt;Âü∫‰∫éPP-HumanÁöÑÊâìÊû∂ËØÜÂà´&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4086987?contributionType=1"&gt;‰∏ãËΩΩÈìæÊé•&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Êô∫ËÉΩÂÆâÈò≤&lt;/td&gt; 
    &lt;td&gt;ÊëîÂÄíÊ£ÄÊµã&lt;/td&gt; 
    &lt;td&gt;Âü∫‰∫éPP-HumanÂÆåÊàêÊù•ÂÆ¢ÂàÜÊûêÊï¥‰ΩìÊµÅÁ®ã„ÄÇ‰ΩøÁî®PP-HumanÂÆåÊàêÊù•ÂÆ¢ÂàÜÊûê‰∏≠ÈùûÂ∏∏Â∏∏ËßÅÁöÑÂú∫ÊôØÔºö 1. Êù•ÂÆ¢Â±ûÊÄßËØÜÂà´(ÂçïÈïúÂíåË∑®Â¢ÉÂèØËßÜÂåñÔºâÔºõ2. Êù•ÂÆ¢Ë°å‰∏∫ËØÜÂà´ÔºàÊëîÂÄíËØÜÂà´Ôºâ„ÄÇ&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4537344"&gt;Âü∫‰∫éPP-HumanÁöÑÊù•ÂÆ¢ÂàÜÊûêÊ°à‰æãÊïôÁ®ã&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4537344"&gt;‰∏ãËΩΩÈìæÊé•&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;üèéÔ∏èPP-Vehicle ÂÆûÊó∂ËΩ¶ËæÜÂàÜÊûêÂ∑•ÂÖ∑&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; ÁÆÄ‰ªã(ÁÇπÂáªÂ±ïÂºÄ)&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;PaddleDetectionÊ∑±ÂÖ•Êé¢Á¥¢Ê†∏ÂøÉË°å‰∏öÁöÑÈ´òÈ¢ëÂú∫ÊôØÔºåÊèê‰æõ‰∫ÜËΩ¶ËæÜÂºÄÁÆ±Âç≥Áî®ÂàÜÊûêÂ∑•ÂÖ∑ÔºåÊîØÊåÅÂõæÁâá/ÂçïÈïúÂ§¥ËßÜÈ¢ë/Â§öÈïúÂ§¥ËßÜÈ¢ë/Âú®Á∫øËßÜÈ¢ëÊµÅÂ§öÁßçËæìÂÖ•ÊñπÂºèÔºåÂπøÊ≥õÂ∫îÁî®‰∫éÊô∫ÊÖß‰∫§ÈÄö„ÄÅÊô∫ÊÖßÂüéÂ∏Ç„ÄÅÂ∑•‰∏öÂ∑°Ê£ÄÁ≠âÈ¢ÜÂüü„ÄÇÊîØÊåÅÊúçÂä°Âô®Á´ØÈÉ®ÁΩ≤ÂèäTensorRTÂä†ÈÄüÔºåT4ÊúçÂä°Âô®‰∏äÂèØËææÂà∞ÂÆûÊó∂„ÄÇ PP-VehicleÂõäÊã¨ÂõõÂ§ß‰∫§ÈÄöÂú∫ÊôØÊ†∏ÂøÉÂäüËÉΩÔºöËΩ¶ÁâåËØÜÂà´„ÄÅÂ±ûÊÄßËØÜÂà´„ÄÅËΩ¶ÊµÅÈáèÁªüËÆ°„ÄÅËøùÁ´†Ê£ÄÊµã„ÄÇ&lt;/p&gt; 
 &lt;p&gt;&lt;code&gt;‰º†ÈÄÅÈó®&lt;/code&gt;Ôºö&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/deploy/pipeline/README.md"&gt;PP-VehicleËΩ¶ËæÜÂàÜÊûêÂ∑•ÂÖ∑ÊåáÂçó&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; È¢ÑËÆ≠ÁªÉÊ®°Âûã(ÁÇπÂáªÂ±ïÂºÄ)&lt;/b&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;‰ªªÂä°&lt;/th&gt; 
    &lt;th align="center"&gt;T4 TensorRT FP16: ÈÄüÂ∫¶(FPS)&lt;/th&gt; 
    &lt;th align="center"&gt;Êé®ËçêÈÉ®ÁΩ≤Á°¨‰ª∂&lt;/th&gt; 
    &lt;th align="center"&gt;Ê®°ÂûãÊñπÊ°à&lt;/th&gt; 
    &lt;th align="center"&gt;Ê®°Âûã‰ΩìÁßØ&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;ËΩ¶ËæÜÊ£ÄÊµãÔºàÈ´òÁ≤æÂ∫¶Ôºâ&lt;/td&gt; 
    &lt;td align="center"&gt;38.9&lt;/td&gt; 
    &lt;td align="center"&gt;ÊúçÂä°Âô®&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_ppvehicle.zip"&gt;ÁõÆÊ†áÊ£ÄÊµã&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;182M&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;ËΩ¶ËæÜË∑üË∏™ÔºàÈ´òÁ≤æÂ∫¶Ôºâ&lt;/td&gt; 
    &lt;td align="center"&gt;25&lt;/td&gt; 
    &lt;td align="center"&gt;ÊúçÂä°Âô®&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_ppvehicle.zip"&gt;Â§öÁõÆÊ†áË∑üË∏™&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;182M&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;ËΩ¶ÁâåËØÜÂà´&lt;/td&gt; 
    &lt;td align="center"&gt;213.7&lt;/td&gt; 
    &lt;td align="center"&gt;ÊúçÂä°Âô®&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/ch_PP-OCRv3_det_infer.tar.gz"&gt;ËΩ¶ÁâåÊ£ÄÊµã&lt;/a&gt; &lt;br /&gt; &lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/ch_PP-OCRv3_rec_infer.tar.gz"&gt;ËΩ¶ÁâåËØÜÂà´&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;ËΩ¶ÁâåÊ£ÄÊµãÔºö3.9M &lt;br /&gt; ËΩ¶ÁâåÂ≠óÁ¨¶ËØÜÂà´Ôºö 12M&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;ËΩ¶ËæÜÂ±ûÊÄß&lt;/td&gt; 
    &lt;td align="center"&gt;136.8&lt;/td&gt; 
    &lt;td align="center"&gt;ÊúçÂä°Âô®&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/vehicle_attribute_model.zip"&gt;Â±ûÊÄßËØÜÂà´&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;7.2M&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;code&gt;‰º†ÈÄÅÈó®&lt;/code&gt;Ôºö&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/deploy/pipeline/README.md"&gt;ÂÆåÊï¥È¢ÑËÆ≠ÁªÉÊ®°Âûã&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; ‰∫ß‰∏öÂ∫îÁî®‰ª£Á†ÅÁ§∫‰æã(ÁÇπÂáªÂ±ïÂºÄ)&lt;/b&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Ë°å‰∏ö&lt;/th&gt; 
    &lt;th&gt;Á±ªÂà´&lt;/th&gt; 
    &lt;th&gt;‰∫ÆÁÇπ&lt;/th&gt; 
    &lt;th&gt;ÊñáÊ°£ËØ¥Êòé&lt;/th&gt; 
    &lt;th&gt;Ê®°Âûã‰∏ãËΩΩ&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Êô∫ÊÖß‰∫§ÈÄö&lt;/td&gt; 
    &lt;td&gt;‰∫§ÈÄöÁõëÊéßËΩ¶ËæÜÂàÜÊûê&lt;/td&gt; 
    &lt;td&gt;Êú¨È°πÁõÆÂü∫‰∫éPP-VehicleÊºîÁ§∫Êô∫ÊÖß‰∫§ÈÄö‰∏≠ÊúÄÂàöÈúÄÁöÑËΩ¶ÊµÅÈáèÁõëÊéß„ÄÅËΩ¶ËæÜËøùÂÅúÊ£ÄÊµã‰ª•ÂèäËΩ¶ËæÜÁªìÊûÑÂåñÔºàËΩ¶Áâå„ÄÅËΩ¶Âûã„ÄÅÈ¢úËâ≤ÔºâÂàÜÊûê‰∏âÂ§ßÂú∫ÊôØ„ÄÇ&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4512254"&gt;Âü∫‰∫éPP-VehicleÁöÑ‰∫§ÈÄöÁõëÊéßÂàÜÊûêÁ≥ªÁªü&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4512254"&gt;‰∏ãËΩΩÈìæÊé•&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h2&gt;üí°‰∫ß‰∏öÂÆûË∑µËåÉ‰æã&lt;/h2&gt; 
&lt;p&gt;‰∫ß‰∏öÂÆûË∑µËåÉ‰æãÊòØPaddleDetectionÈíàÂØπÈ´òÈ¢ëÁõÆÊ†áÊ£ÄÊµãÂ∫îÁî®Âú∫ÊôØÔºåÊèê‰æõÁöÑÁ´ØÂà∞Á´ØÂºÄÂèëÁ§∫‰æãÔºåÂ∏ÆÂä©ÂºÄÂèëËÄÖÊâìÈÄöÊï∞ÊçÆÊ†áÊ≥®-Ê®°ÂûãËÆ≠ÁªÉ-Ê®°ÂûãË∞É‰ºò-È¢ÑÊµãÈÉ®ÁΩ≤ÂÖ®ÊµÅÁ®ã„ÄÇ ÈíàÂØπÊØè‰∏™ËåÉ‰æãÊàë‰ª¨ÈÉΩÈÄöËøá&lt;a href="https://ai.baidu.com/ai-doc/AISTUDIO/Tk39ty6ho"&gt;AI-Studio&lt;/a&gt;Êèê‰æõ‰∫ÜÈ°πÁõÆ‰ª£Á†Å‰ª•ÂèäËØ¥ÊòéÔºåÁî®Êà∑ÂèØ‰ª•ÂêåÊ≠•ËøêË°å‰ΩìÈ™å„ÄÇ&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;‰º†ÈÄÅÈó®&lt;/code&gt;Ôºö&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/industrial_tutorial/README.md"&gt;‰∫ß‰∏öÂÆûË∑µËåÉ‰æãÂÆåÊï¥ÂàóË°®&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/5058293"&gt;Âü∫‰∫éPP-YOLOE-RÁöÑÊóãËΩ¨Ê°ÜÊ£ÄÊµã&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/5036782"&gt;Âü∫‰∫éPP-YOLOE-SODÁöÑÊó†‰∫∫Êú∫Ëà™ÊãçÂõæÂÉèÊ£ÄÊµã&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4512254"&gt;Âü∫‰∫éPP-VehicleÁöÑ‰∫§ÈÄöÁõëÊéßÂàÜÊûêÁ≥ªÁªü&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4606001"&gt;Âü∫‰∫éPP-Human v2ÁöÑÊëîÂÄíÊ£ÄÊµã&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4385813"&gt;Âü∫‰∫éPP-TinyPoseÂ¢ûÂº∫ÁâàÁöÑÊô∫ËÉΩÂÅ•Ë∫´Âä®‰ΩúËØÜÂà´&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4086987?contributionType=1"&gt;Âü∫‰∫éPP-HumanÁöÑÊâìÊû∂ËØÜÂà´&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/2571419"&gt;Âü∫‰∫éFaster-RCNNÁöÑÁì∑Á†ñË°®Èù¢ÁëïÁñµÊ£ÄÊµã&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/2367089"&gt;Âü∫‰∫éPaddleDetectionÁöÑPCBÁëïÁñµÊ£ÄÊµã&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/2421822"&gt;Âü∫‰∫éFairMOTÂÆûÁé∞‰∫∫ÊµÅÈáèÁªüËÆ°&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/2500639"&gt;Âü∫‰∫éYOLOv3ÂÆûÁé∞Ë∑åÂÄíÊ£ÄÊµã&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/3846170?channelType=0&amp;amp;channel=0"&gt;Âü∫‰∫éPP-PicoDetv2 ÁöÑË∑ØÈù¢ÂûÉÂúæÊ£ÄÊµã&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4061642?contributionType=1"&gt;Âü∫‰∫é‰∫∫‰ΩìÂÖ≥ÈîÆÁÇπÊ£ÄÊµãÁöÑÂêàËßÑÊ£ÄÊµã&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4537344"&gt;Âü∫‰∫éPP-HumanÁöÑÊù•ÂÆ¢ÂàÜÊûêÊ°à‰æãÊïôÁ®ã&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ÊåÅÁª≠Êõ¥Êñ∞‰∏≠...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üèÜ‰ºÅ‰∏öÂ∫îÁî®Ê°à‰æã&lt;/h2&gt; 
&lt;p&gt;‰ºÅ‰∏öÂ∫îÁî®Ê°à‰æãÊòØ‰ºÅ‰∏öÂú®ÂÆûÁîü‰∫ßÁéØÂ¢É‰∏ãËêΩÂú∞Â∫îÁî®PaddleDetectionÁöÑÊñπÊ°àÊÄùË∑ØÔºåÁõ∏ÊØî‰∫ß‰∏öÂÆûË∑µËåÉ‰æãÂÖ∂Êõ¥Â§öÂº∫Ë∞ÉÊï¥‰ΩìÊñπÊ°àËÆæËÆ°ÊÄùË∑ØÔºåÂèØ‰æõÂºÄÂèëËÄÖÂú®È°πÁõÆÊñπÊ°àËÆæËÆ°‰∏≠ÂÅöÂèÇËÄÉ„ÄÇ&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;‰º†ÈÄÅÈó®&lt;/code&gt;Ôºö&lt;a href="https://www.paddlepaddle.org.cn/customercase"&gt;‰ºÅ‰∏öÂ∫îÁî®Ê°à‰æãÂÆåÊï¥ÂàóË°®&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.paddlepaddle.org.cn/support/news?action=detail&amp;amp;id=2330"&gt;‰∏≠ÂõΩÂçóÊñπÁîµÁΩë‚Äî‚ÄîÂèòÁîµÁ´ôÊô∫ÊÖßÂ∑°Ê£Ä&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.paddlepaddle.org.cn/support/news?action=detail&amp;amp;id=2280"&gt;ÂõΩÈìÅÁîµÊ∞î‚Äî‚ÄîËΩ®ÈÅìÂú®Á∫øÊô∫ËÉΩÂ∑°Ê£ÄÁ≥ªÁªü&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.paddlepaddle.org.cn/support/news?action=detail&amp;amp;id=2611"&gt;‰∫¨‰∏úÁâ©ÊµÅ‚Äî‚ÄîÂõ≠Âå∫ËΩ¶ËæÜË°å‰∏∫ËØÜÂà´&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.paddlepaddle.org.cn/support/news?action=detail&amp;amp;id=2618"&gt;‰∏≠ÂÖ¥ÂÖãÊãâ‚ÄîÂéÇÂå∫‰º†Áªü‰ª™Ë°®ÁªüËÆ°ÁõëÊµã&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.paddlepaddle.org.cn/support/news?action=detail&amp;amp;id=2609"&gt;ÂÆÅÂæ∑Êó∂‰ª£‚ÄîÂä®ÂäõÁîµÊ±†È´òÁ≤æÂ∫¶Ë¥®ÈáèÊ£ÄÊµã&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.paddlepaddle.org.cn/support/news?action=detail&amp;amp;id=2483"&gt;‰∏≠ÂõΩÁßëÂ≠¶Èô¢Á©∫Â§©‰ø°ÊÅØÂàõÊñ∞Á†îÁ©∂Èô¢‚Äî‚ÄîÈ´òÂ∞îÂ§´ÁêÉÂú∫ÈÅ•ÊÑüÁõëÊµã&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.paddlepaddle.org.cn/support/news?action=detail&amp;amp;id=2481"&gt;Âæ°Ëà™Êô∫ËÉΩ‚Äî‚ÄîÂü∫‰∫éËæπÁºòÁöÑÊó†‰∫∫Êú∫Êô∫ËÉΩÂ∑°Ê£Ä&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.paddlepaddle.org.cn/support/news?action=detail&amp;amp;id=2121"&gt;ÊôÆÂÆôÊó†‰∫∫Êú∫‚Äî‚ÄîÈ´òÁ≤æÂ∫¶Ê£ÆÊûóÂ∑°Ê£Ä&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.paddlepaddle.org.cn/support/news?action=detail&amp;amp;id=2615"&gt;È¢ÜÈÇ¶Êô∫ËÉΩ‚Äî‚ÄîÁ∫¢Â§ñÊó†ÊÑüÊµãÊ∏©ÁõëÊéß&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/znrqaJmtA7CcjG0yQESWig"&gt;Âåó‰∫¨Âú∞ÈìÅ‚Äî‚ÄîÂè£ÁΩ©Ê£ÄÊµã&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.paddlepaddle.org.cn/support/news?action=detail&amp;amp;id=2288"&gt;Èü≥Êô∫Ëææ‚Äî‚ÄîÂ∑•ÂéÇ‰∫∫ÂëòËøùËßÑË°å‰∏∫Ê£ÄÊµã&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.paddlepaddle.org.cn/support/news?action=detail&amp;amp;id=2331"&gt;ÂçéÂ§èÂ§©‰ø°‚Äî‚ÄîËæìÁÖ§ÁöÆÂ∏¶Êú∫Âô®‰∫∫Êô∫ËÉΩÂ∑°Ê£Ä&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.paddlepaddle.org.cn/support/news?action=detail&amp;amp;id=2485"&gt;‰ºòÊÅ©Áâ©ËÅîÁΩë‚Äî‚ÄîÁ§æÂå∫‰ΩèÊà∑ÂàÜÁ±ªÊîØÊåÅÂπøÂëäÁ≤æÂáÜÊäïÊîæ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.paddlepaddle.org.cn/support/news?action=detail&amp;amp;id=2599"&gt;Ëû≥ËûÇÊÖßËßÜ‚Äî‚ÄîÂÆ§ÂÜÖ3DÁÇπ‰∫ëÂú∫ÊôØÁâ©‰ΩìÂàÜÂâ≤‰∏éÊ£ÄÊµã&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ÊåÅÁª≠Êõ¥Êñ∞‰∏≠...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìùËÆ∏ÂèØËØÅ‰π¶&lt;/h2&gt; 
&lt;p&gt;Êú¨È°πÁõÆÁöÑÂèëÂ∏ÉÂèó&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/LICENSE"&gt;Apache 2.0 license&lt;/a&gt;ËÆ∏ÂèØËÆ§ËØÅ„ÄÇ&lt;/p&gt; 
&lt;h2&gt;üìåÂºïÁî®&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@misc{ppdet2019,
title={PaddleDetection, Object detection and instance segmentation toolkit based on PaddlePaddle.},
author={PaddlePaddle Authors},
howpublished = {\url{https://github.com/PaddlePaddle/PaddleDetection}},
year={2019}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>NVIDIA/NeMo</title>
      <link>https://github.com/NVIDIA/NeMo</link>
      <description>&lt;p&gt;A scalable generative AI framework built for researchers and developers working on Large Language Models, Multimodal, and Speech AI (Automatic Speech Recognition and Text-to-Speech)&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="http://www.repostatus.org/#active"&gt;&lt;img src="http://www.repostatus.org/badges/latest/active.svg?sanitize=true" alt="Project Status: Active -- The project has reached a stable, usable state and is being actively developed." /&gt;&lt;/a&gt; &lt;a href="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/"&gt;&lt;img src="https://readthedocs.com/projects/nvidia-nemo/badge/?version=main" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://github.com/nvidia/nemo/actions/workflows/codeql.yml"&gt;&lt;img src="https://github.com/nvidia/nemo/actions/workflows/codeql.yml/badge.svg?branch=main&amp;amp;event=push" alt="CodeQL" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/NeMo/raw/master/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg?sanitize=true" alt="NeMo core license and license for collections in this repo" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/nemo-toolkit"&gt;&lt;img src="https://badge.fury.io/py/nemo-toolkit.svg?sanitize=true" alt="Release version" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/nemo-toolkit"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/nemo-toolkit.svg?sanitize=true" alt="Python version" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/nemo-toolkit"&gt;&lt;img src="https://static.pepy.tech/personalized-badge/nemo-toolkit?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=brightgreen&amp;amp;left_text=downloads" alt="PyPi total downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true" alt="Code style: black" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;NVIDIA NeMo Framework&lt;/strong&gt;&lt;/h1&gt; 
&lt;h2&gt;Latest News&lt;/h2&gt; 
&lt;!-- markdownlint-disable --&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;b&gt;Pretrain and finetune &lt;span&gt;ü§ó&lt;/span&gt;Hugging Face models via AutoModel&lt;/b&gt;&lt;/summary&gt; Nemo Framework's latest feature AutoModel enables broad support for 
 &lt;span&gt;ü§ó&lt;/span&gt;Hugging Face models, with 25.04 focusing on 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/transformers/v3.5.1/model_doc/auto.html#automodelforcausallm"&gt;AutoModelForCausalLM&lt;/a&gt;&lt;a&gt; in the &lt;/a&gt;&lt;a href="https://huggingface.co/models?pipeline_tag=text-generation&amp;amp;sort=trending"&gt;Text Generation&lt;/a&gt;&lt;a&gt; category&lt;/a&gt;&lt;/li&gt;
  &lt;a&gt; &lt;/a&gt;
  &lt;li&gt;&lt;a&gt;&lt;/a&gt;&lt;a href="https://huggingface.co/docs/transformers/main/model_doc/auto#transformers.AutoModelForImageTextToText"&gt;AutoModelForImageTextToText&lt;/a&gt;&lt;a&gt; in the &lt;/a&gt;&lt;a href="https://huggingface.co/models?pipeline_tag=image-text-to-text&amp;amp;sort=trending"&gt;Image-Text-to-Text&lt;/a&gt;&lt;a&gt; category&lt;/a&gt;&lt;/li&gt;
  &lt;a&gt; &lt;/a&gt;
 &lt;/ul&gt;
 &lt;a&gt; &lt;/a&gt;
 &lt;p&gt;&lt;a&gt;More Details in Blog: &lt;/a&gt;&lt;a href="https://developer.nvidia.com/blog/run-hugging-face-models-instantly-with-day-0-support-from-nvidia-nemo-framework"&gt;Run Hugging Face Models Instantly with Day-0 Support from NVIDIA NeMo Framework&lt;/a&gt;&lt;a&gt;. Future releases will enable support for more model families such as Video Generation models.(2025-05-19)&lt;/a&gt;&lt;/p&gt;
 &lt;a&gt; &lt;/a&gt;
&lt;/details&gt;
&lt;a&gt; &lt;/a&gt;
&lt;details open&gt;
 &lt;a&gt; &lt;summary&gt;&lt;b&gt;Training on Blackwell using Nemo&lt;/b&gt;&lt;/summary&gt; NeMo Framework has added Blackwell support, with &lt;/a&gt;
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance_summary.html"&gt;performance benchmarks on GB200 &amp;amp; B200&lt;/a&gt;
 &lt;a&gt;. More optimizations to come in the upcoming releases.(2025-05-19) &lt;/a&gt;
&lt;/details&gt;
&lt;a&gt; &lt;/a&gt;
&lt;details open&gt;
 &lt;a&gt; &lt;summary&gt;&lt;b&gt;Training Performance on GPU Tuning Guide&lt;/b&gt;&lt;/summary&gt; NeMo Framework has published &lt;/a&gt;
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance-guide.html"&gt;a comprehensive guide for performance tuning to achieve optimal throughput&lt;/a&gt;
 &lt;a&gt;! (2025-05-19) &lt;/a&gt;
&lt;/details&gt;
&lt;a&gt; &lt;/a&gt;
&lt;details open&gt;
 &lt;a&gt; &lt;summary&gt;&lt;b&gt;New Models Support&lt;/b&gt;&lt;/summary&gt; NeMo Framework has added support for latest community models - &lt;/a&gt;
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/vlms/llama4.html"&gt;Llama 4&lt;/a&gt;
 &lt;a&gt;, &lt;/a&gt;
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/vision/diffusionmodels/flux.html"&gt;Flux&lt;/a&gt;
 &lt;a&gt;, &lt;/a&gt;
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/llama_nemotron.html"&gt;Llama Nemotron&lt;/a&gt;
 &lt;a&gt;, &lt;/a&gt;
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/hyena.html#"&gt;Hyena &amp;amp; Evo2&lt;/a&gt;
 &lt;a&gt;, &lt;/a&gt;
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/vlms/qwen2vl.html"&gt;Qwen2-VL&lt;/a&gt;
 &lt;a&gt;, &lt;/a&gt;
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/qwen2.html"&gt;Qwen2.5&lt;/a&gt;
 &lt;a&gt;, Gemma3, Qwen3-30B&amp;amp;32B.(2025-05-19) &lt;/a&gt;
&lt;/details&gt;
&lt;a&gt; &lt;/a&gt;
&lt;details open&gt;
 &lt;a&gt; &lt;summary&gt;&lt;b&gt;NeMo Framework 2.0&lt;/b&gt;&lt;/summary&gt; We've released NeMo 2.0, an update on the NeMo Framework which prioritizes modularity and ease-of-use. Please refer to the &lt;/a&gt;
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/index.html"&gt;NeMo Framework User Guide&lt;/a&gt; to get started. 
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;b&gt;New Cosmos World Foundation Models Support&lt;/b&gt;&lt;/summary&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/advancing-physical-ai-with-nvidia-cosmos-world-foundation-model-platform"&gt;Advancing Physical AI with NVIDIA Cosmos World Foundation Model Platform &lt;/a&gt; (2025-01-09) &lt;/summary&gt; The end-to-end NVIDIA Cosmos platform accelerates world model development for physical AI systems. Built on CUDA, Cosmos combines state-of-the-art world foundation models, video tokenizers, and AI-accelerated data processing pipelines. Developers can accelerate world model development by fine-tuning Cosmos world foundation models or building new ones from the ground up. These models create realistic synthetic videos of environments and interactions, providing a scalable foundation for training complex systems, from simulating humanoid robots performing advanced actions to developing end-to-end autonomous driving models. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/accelerate-custom-video-foundation-model-pipelines-with-new-nvidia-nemo-framework-capabilities/"&gt; Accelerate Custom Video Foundation Model Pipelines with New NVIDIA NeMo Framework Capabilities &lt;/a&gt; (2025-01-07) &lt;/summary&gt; The NeMo Framework now supports training and customizing the 
  &lt;a href="https://github.com/NVIDIA/Cosmos"&gt;NVIDIA Cosmos&lt;/a&gt; collection of world foundation models. Cosmos leverages advanced text-to-world generation techniques to create fluid, coherent video content from natural language prompts. 
  &lt;br /&gt;
  &lt;br /&gt; You can also now accelerate your video processing step using the 
  &lt;a href="https://developer.nvidia.com/nemo-curator-video-processing-early-access"&gt;NeMo Curator&lt;/a&gt; library, which provides optimized video processing and captioning features that can deliver up to 89x faster video processing when compared to an unoptimized CPU pipeline. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;b&gt;Large Language Models and Multimodal Models&lt;/b&gt;&lt;/summary&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/state-of-the-art-multimodal-generative-ai-model-development-with-nvidia-nemo/"&gt; State-of-the-Art Multimodal Generative AI Model Development with NVIDIA NeMo &lt;/a&gt; (2024-11-06) &lt;/summary&gt; NVIDIA recently announced significant enhancements to the NeMo platform, focusing on multimodal generative AI models. The update includes NeMo Curator and the Cosmos tokenizer, which streamline the data curation process and enhance the quality of visual data. These tools are designed to handle large-scale data efficiently, making it easier to develop high-quality AI models for various applications, including robotics and autonomous driving. The Cosmos tokenizers, in particular, efficiently map visual data into compact, semantic tokens, which is crucial for training large-scale generative models. The tokenizer is available now on the 
  &lt;a href="http://github.com/NVIDIA/cosmos-tokenizer/NVIDIA/cosmos-tokenizer"&gt;NVIDIA/cosmos-tokenizer&lt;/a&gt; GitHub repo and on 
  &lt;a href="https://huggingface.co/nvidia/Cosmos-Tokenizer-CV8x8x8"&gt;Hugging Face&lt;/a&gt;. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/llama/index.html#new-llama-3-1-support for more information/"&gt; New Llama 3.1 Support &lt;/a&gt; (2024-07-23) &lt;/summary&gt; The NeMo Framework now supports training and customizing the Llama 3.1 collection of LLMs from Meta. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://aws.amazon.com/blogs/machine-learning/accelerate-your-generative-ai-distributed-training-workloads-with-the-nvidia-nemo-framework-on-amazon-eks/"&gt; Accelerate your Generative AI Distributed Training Workloads with the NVIDIA NeMo Framework on Amazon EKS &lt;/a&gt; (2024-07-16) &lt;/summary&gt; NVIDIA NeMo Framework now runs distributed training workloads on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. For step-by-step instructions on creating an EKS cluster and running distributed training workloads with NeMo, see the GitHub repository 
  &lt;a href="https://github.com/aws-samples/awsome-distributed-training/tree/main/3.test_cases/2.nemo-launcher/EKS/"&gt; here.&lt;/a&gt; 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/nvidia-nemo-accelerates-llm-innovation-with-hybrid-state-space-model-support/"&gt; NVIDIA NeMo Accelerates LLM Innovation with Hybrid State Space Model Support &lt;/a&gt; (2024/06/17) &lt;/summary&gt; NVIDIA NeMo and Megatron Core now support pre-training and fine-tuning of state space models (SSMs). NeMo also supports training models based on the Griffin architecture as described by Google DeepMind. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://huggingface.co/models?sort=trending&amp;amp;search=nvidia%2Fnemotron-4-340B"&gt; NVIDIA releases 340B base, instruct, and reward models pretrained on a total of 9T tokens. &lt;/a&gt; (2024-06-18) &lt;/summary&gt; See documentation and tutorials for SFT, PEFT, and PTQ with 
  &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/nemotron/index.html"&gt; Nemotron 340B &lt;/a&gt; in the NeMo Framework User Guide. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/nvidia-sets-new-generative-ai-performance-and-scale-records-in-mlperf-training-v4-0/"&gt; NVIDIA sets new generative AI performance and scale records in MLPerf Training v4.0 &lt;/a&gt; (2024/06/12) &lt;/summary&gt; Using NVIDIA NeMo Framework and NVIDIA Hopper GPUs NVIDIA was able to scale to 11,616 H100 GPUs and achieve near-linear performance scaling on LLM pretraining. NVIDIA also achieved the highest LLM fine-tuning performance and raised the bar for text-to-image training. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://cloud.google.com/blog/products/compute/gke-and-nvidia-nemo-framework-to-train-generative-ai-models"&gt; Accelerate your generative AI journey with NVIDIA NeMo Framework on GKE &lt;/a&gt; (2024/03/16) &lt;/summary&gt; An end-to-end walkthrough to train generative AI models on the Google Kubernetes Engine (GKE) using the NVIDIA NeMo Framework is available at https://github.com/GoogleCloudPlatform/nvidia-nemo-on-gke. The walkthrough includes detailed instructions on how to set up a Google Cloud Project and pre-train a GPT model using the NeMo Framework. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;b&gt;Speech Recognition&lt;/b&gt;&lt;/summary&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/accelerating-leaderboard-topping-asr-models-10x-with-nvidia-nemo/"&gt; Accelerating Leaderboard-Topping ASR Models 10x with NVIDIA NeMo &lt;/a&gt; (2024/09/24) &lt;/summary&gt; NVIDIA NeMo team released a number of inference optimizations for CTC, RNN-T, and TDT models that resulted in up to 10x inference speed-up. These models now exceed an inverse real-time factor (RTFx) of 2,000, with some reaching RTFx of even 6,000. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/new-standard-for-speech-recognition-and-translation-from-the-nvidia-nemo-canary-model/"&gt; New Standard for Speech Recognition and Translation from the NVIDIA NeMo Canary Model &lt;/a&gt; (2024/04/18) &lt;/summary&gt; The NeMo team just released Canary, a multilingual model that transcribes speech in English, Spanish, German, and French with punctuation and capitalization. Canary also provides bi-directional translation, between English and the three other supported languages. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/pushing-the-boundaries-of-speech-recognition-with-nemo-parakeet-asr-models/"&gt; Pushing the Boundaries of Speech Recognition with NVIDIA NeMo Parakeet ASR Models &lt;/a&gt; (2024/04/18) &lt;/summary&gt; NVIDIA NeMo, an end-to-end platform for the development of multimodal generative AI models at scale anywhere‚Äîon any cloud and on-premises‚Äîreleased the Parakeet family of automatic speech recognition (ASR) models. These state-of-the-art ASR models, developed in collaboration with Suno.ai, transcribe spoken English with exceptional accuracy. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/turbocharge-asr-accuracy-and-speed-with-nvidia-nemo-parakeet-tdt/"&gt; Turbocharge ASR Accuracy and Speed with NVIDIA NeMo Parakeet-TDT &lt;/a&gt; (2024/04/18) &lt;/summary&gt; NVIDIA NeMo, an end-to-end platform for developing multimodal generative AI models at scale anywhere‚Äîon any cloud and on-premises‚Äîrecently released Parakeet-TDT. This new addition to the ‚ÄØNeMo ASR Parakeet model family boasts better accuracy and 64% greater speed over the previously best model, Parakeet-RNNT-1.1B. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
&lt;/details&gt; 
&lt;!-- markdownlint-enable --&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;NVIDIA NeMo Framework is a scalable and cloud-native generative AI framework built for researchers and PyTorch developers working on Large Language Models (LLMs), Multimodal Models (MMs), Automatic Speech Recognition (ASR), Text to Speech (TTS), and Computer Vision (CV) domains. It is designed to help you efficiently create, customize, and deploy new generative AI models by leveraging existing code and pre-trained model checkpoints.&lt;/p&gt; 
&lt;p&gt;For technical documentation, please see the &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/index.html"&gt;NeMo Framework User Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;What's New in NeMo 2.0&lt;/h2&gt; 
&lt;p&gt;NVIDIA NeMo 2.0 introduces several significant improvements over its predecessor, NeMo 1.0, enhancing flexibility, performance, and scalability.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Python-Based Configuration&lt;/strong&gt; - NeMo 2.0 transitions from YAML files to a Python-based configuration, providing more flexibility and control. This shift makes it easier to extend and customize configurations programmatically.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Modular Abstractions&lt;/strong&gt; - By adopting PyTorch Lightning‚Äôs modular abstractions, NeMo 2.0 simplifies adaptation and experimentation. This modular approach allows developers to more easily modify and experiment with different components of their models.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Scalability&lt;/strong&gt; - NeMo 2.0 seamlessly scaling large-scale experiments across thousands of GPUs using &lt;a href="https://github.com/NVIDIA/NeMo-Run"&gt;NeMo-Run&lt;/a&gt;, a powerful tool designed to streamline the configuration, execution, and management of machine learning experiments across computing environments.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Overall, these enhancements make NeMo 2.0 a powerful, scalable, and user-friendly framework for AI model development.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT]&lt;br /&gt; NeMo 2.0 is currently supported by the LLM (large language model) and VLM (vision language model) collections.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Get Started with NeMo 2.0&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Refer to the &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/quickstart.html"&gt;Quickstart&lt;/a&gt; for examples of using NeMo-Run to launch NeMo 2.0 experiments locally and on a slurm cluster.&lt;/li&gt; 
 &lt;li&gt;For more information about NeMo 2.0, see the &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/index.html"&gt;NeMo Framework User Guide&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/NVIDIA/NeMo/raw/main/nemo/collections/llm/recipes"&gt;NeMo 2.0 Recipes&lt;/a&gt; contains additional examples of launching large-scale runs using NeMo 2.0 and NeMo-Run.&lt;/li&gt; 
 &lt;li&gt;For an in-depth exploration of the main features of NeMo 2.0, see the &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/features/index.html#feature-guide"&gt;Feature Guide&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;To transition from NeMo 1.0 to 2.0, see the &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/migration/index.html#migration-guide"&gt;Migration Guide&lt;/a&gt; for step-by-step instructions.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Get Started with Cosmos&lt;/h3&gt; 
&lt;p&gt;NeMo Curator and NeMo Framework support video curation and post-training of the Cosmos World Foundation Models, which are open and available on &lt;a href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/cosmos/collections/cosmos"&gt;NGC&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/nvidia/cosmos-6751e884dc10e013a0a0d8e6"&gt;Hugging Face&lt;/a&gt;. For more information on video datasets, refer to &lt;a href="https://developer.nvidia.com/nemo-curator"&gt;NeMo Curator&lt;/a&gt;. To post-train World Foundation Models using the NeMo Framework for your custom physical AI tasks, see the &lt;a href="https://github.com/NVIDIA/Cosmos/raw/main/cosmos1/models/diffusion/nemo/post_training/README.md"&gt;Cosmos Diffusion models&lt;/a&gt; and the &lt;a href="https://github.com/NVIDIA/Cosmos/raw/main/cosmos1/models/autoregressive/nemo/post_training/README.md"&gt;Cosmos Autoregressive models&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;LLMs and MMs Training, Alignment, and Customization&lt;/h2&gt; 
&lt;p&gt;All NeMo models are trained with &lt;a href="https://github.com/Lightning-AI/lightning"&gt;Lightning&lt;/a&gt;. Training is automatically scalable to 1000s of GPUs. You can check the performance benchmarks using the latest NeMo Framework container &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance_summary.html"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When applicable, NeMo models leverage cutting-edge distributed training techniques, incorporating &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/modeloverview.html"&gt;parallelism strategies&lt;/a&gt; to enable efficient training of very large models. These techniques include Tensor Parallelism (TP), Pipeline Parallelism (PP), Fully Sharded Data Parallelism (FSDP), Mixture-of-Experts (MoE), and Mixed Precision Training with BFloat16 and FP8, as well as others.&lt;/p&gt; 
&lt;p&gt;NeMo Transformer-based LLMs and MMs utilize &lt;a href="https://github.com/NVIDIA/TransformerEngine"&gt;NVIDIA Transformer Engine&lt;/a&gt; for FP8 training on NVIDIA Hopper GPUs, while leveraging &lt;a href="https://github.com/NVIDIA/Megatron-LM/tree/main/megatron/core"&gt;NVIDIA Megatron Core&lt;/a&gt; for scaling Transformer model training.&lt;/p&gt; 
&lt;p&gt;NeMo LLMs can be aligned with state-of-the-art methods such as SteerLM, Direct Preference Optimization (DPO), and Reinforcement Learning from Human Feedback (RLHF). See &lt;a href="https://github.com/NVIDIA/NeMo-Aligner"&gt;NVIDIA NeMo Aligner&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;p&gt;In addition to supervised fine-tuning (SFT), NeMo also supports the latest parameter efficient fine-tuning (PEFT) techniques such as LoRA, P-Tuning, Adapters, and IA3. Refer to the &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/sft_peft/index.html"&gt;NeMo Framework User Guide&lt;/a&gt; for the full list of supported models and techniques.&lt;/p&gt; 
&lt;h2&gt;LLMs and MMs Deployment and Optimization&lt;/h2&gt; 
&lt;p&gt;NeMo LLMs and MMs can be deployed and optimized with &lt;a href="https://developer.nvidia.com/nemo-microservices-early-access"&gt;NVIDIA NeMo Microservices&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Speech AI&lt;/h2&gt; 
&lt;p&gt;NeMo ASR and TTS models can be optimized for inference and deployed for production use cases with &lt;a href="https://developer.nvidia.com/riva"&gt;NVIDIA Riva&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;NeMo Framework Launcher&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT]&lt;br /&gt; NeMo Framework Launcher is compatible with NeMo version 1.0 only. &lt;a href="https://github.com/NVIDIA/NeMo-Run"&gt;NeMo-Run&lt;/a&gt; is recommended for launching experiments using NeMo 2.0.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://github.com/NVIDIA/NeMo-Megatron-Launcher"&gt;NeMo Framework Launcher&lt;/a&gt; is a cloud-native tool that streamlines the NeMo Framework experience. It is used for launching end-to-end NeMo Framework training jobs on CSPs and Slurm clusters.&lt;/p&gt; 
&lt;p&gt;The NeMo Framework Launcher includes extensive recipes, scripts, utilities, and documentation for training NeMo LLMs. It also includes the NeMo Framework &lt;a href="https://github.com/NVIDIA/NeMo-Megatron-Launcher#53-using-autoconfigurator-to-find-the-optimal-configuration"&gt;Autoconfigurator&lt;/a&gt;, which is designed to find the optimal model parallel configuration for training on a specific cluster.&lt;/p&gt; 
&lt;p&gt;To get started quickly with the NeMo Framework Launcher, please see the &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/index.html"&gt;NeMo Framework Playbooks&lt;/a&gt;. The NeMo Framework Launcher does not currently support ASR and TTS training, but it will soon.&lt;/p&gt; 
&lt;h2&gt;Get Started with NeMo Framework&lt;/h2&gt; 
&lt;p&gt;Getting started with NeMo Framework is easy. State-of-the-art pretrained NeMo models are freely available on &lt;a href="https://huggingface.co/models?library=nemo&amp;amp;sort=downloads&amp;amp;search=nvidia"&gt;Hugging Face Hub&lt;/a&gt; and &lt;a href="https://catalog.ngc.nvidia.com/models?query=nemo&amp;amp;orderBy=weightPopularDESC"&gt;NVIDIA NGC&lt;/a&gt;. These models can be used to generate text or images, transcribe audio, and synthesize speech in just a few lines of code.&lt;/p&gt; 
&lt;p&gt;We have extensive &lt;a href="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/starthere/tutorials.html"&gt;tutorials&lt;/a&gt; that can be run on &lt;a href="https://colab.research.google.com"&gt;Google Colab&lt;/a&gt; or with our &lt;a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo"&gt;NGC NeMo Framework Container&lt;/a&gt;. We also have &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/index.html"&gt;playbooks&lt;/a&gt; for users who want to train NeMo models with the NeMo Framework Launcher.&lt;/p&gt; 
&lt;p&gt;For advanced users who want to train NeMo models from scratch or fine-tune existing NeMo models, we have a full suite of &lt;a href="https://github.com/NVIDIA/NeMo/tree/main/examples"&gt;example scripts&lt;/a&gt; that support multi-GPU/multi-node training.&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo/main/nemo/collections/nlp/README.md"&gt;Large Language Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo/main/nemo/collections/multimodal/README.md"&gt;Multimodal&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo/main/nemo/collections/asr/README.md"&gt;Automatic Speech Recognition&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo/main/nemo/collections/tts/README.md"&gt;Text to Speech&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo/main/nemo/collections/vision/README.md"&gt;Computer Vision&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10 or above&lt;/li&gt; 
 &lt;li&gt;Pytorch 2.5 or above&lt;/li&gt; 
 &lt;li&gt;NVIDIA GPU (if you intend to do model training)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Developer Documentation&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Version&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Latest&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/"&gt;&lt;img src="https://readthedocs.com/projects/nvidia-nemo/badge/?version=main" alt="Documentation Status" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/"&gt;Documentation of the latest (i.e. main) branch.&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Stable&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/"&gt;&lt;img src="https://readthedocs.com/projects/nvidia-nemo/badge/?version=stable" alt="Documentation Status" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/"&gt;Documentation of the stable (i.e. most recent release)&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Install NeMo Framework&lt;/h2&gt; 
&lt;p&gt;The NeMo Framework can be installed in a variety of ways, depending on your needs. Depending on the domain, you may find one of the following installation methods more suitable.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo/main/#conda--pip"&gt;Conda / Pip&lt;/a&gt;: Install NeMo-Framework with native Pip into a virtual environment. 
  &lt;ul&gt; 
   &lt;li&gt;Used to explore NeMo on any supported platform.&lt;/li&gt; 
   &lt;li&gt;This is the recommended method for ASR and TTS domains.&lt;/li&gt; 
   &lt;li&gt;Limited feature-completeness for other domains.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo/main/#ngc-pytorch-container"&gt;NGC PyTorch container&lt;/a&gt;: Install NeMo-Framework from source with feature-completeness into a highly optimized container. 
  &lt;ul&gt; 
   &lt;li&gt;For users that want to install from source in a highly optimized container.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo/main/#ngc-nemo-container"&gt;NGC NeMo container&lt;/a&gt;: Ready-to-go solution of NeMo-Framework 
  &lt;ul&gt; 
   &lt;li&gt;For users that seek highest performance.&lt;/li&gt; 
   &lt;li&gt;Contains all dependencies installed and tested for performance and convergence.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Support matrix&lt;/h3&gt; 
&lt;p&gt;NeMo-Framework provides tiers of support based on OS / Platform and mode of installation. Please refer the following overview of support levels:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fully supported: Max performance and feature-completeness.&lt;/li&gt; 
 &lt;li&gt;Limited supported: Used to explore NeMo.&lt;/li&gt; 
 &lt;li&gt;No support yet: In development.&lt;/li&gt; 
 &lt;li&gt;Deprecated: Support has reached end of life.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Please refer to the following table for current support levels:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;OS / Platform&lt;/th&gt; 
   &lt;th&gt;Install from PyPi&lt;/th&gt; 
   &lt;th&gt;Source into NGC container&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;linux&lt;/code&gt; - &lt;code&gt;amd64/x84_64&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Limited support&lt;/td&gt; 
   &lt;td&gt;Full support&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;linux&lt;/code&gt; - &lt;code&gt;arm64&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Limited support&lt;/td&gt; 
   &lt;td&gt;Limited support&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;darwin&lt;/code&gt; - &lt;code&gt;amd64/x64_64&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Deprecated&lt;/td&gt; 
   &lt;td&gt;Deprecated&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;darwin&lt;/code&gt; - &lt;code&gt;arm64&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Limited support&lt;/td&gt; 
   &lt;td&gt;Limited support&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;windows&lt;/code&gt; - &lt;code&gt;amd64/x64_64&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;No support yet&lt;/td&gt; 
   &lt;td&gt;No support yet&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;windows&lt;/code&gt; - &lt;code&gt;arm64&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;No support yet&lt;/td&gt; 
   &lt;td&gt;No support yet&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Conda / Pip&lt;/h3&gt; 
&lt;p&gt;Install NeMo in a fresh Conda environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create --name nemo python==3.10.12
conda activate nemo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Pick the right version&lt;/h4&gt; 
&lt;p&gt;NeMo-Framework publishes pre-built wheels with each release. To install nemo_toolkit from such a wheel, use the following installation method:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "nemo_toolkit[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If a more specific version is desired, we recommend a Pip-VCS install. From &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo/main/github.com/NVIDIA/NeMo"&gt;NVIDIA/NeMo&lt;/a&gt;, fetch the commit, branch, or tag that you would like to install.&lt;br /&gt; To install nemo_toolkit from this Git reference &lt;code&gt;$REF&lt;/code&gt;, use the following installation method:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/NVIDIA/NeMo
cd NeMo
git checkout @${REF:-'main'}
pip install '.[all]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Install a specific Domain&lt;/h4&gt; 
&lt;p&gt;To install a specific domain of NeMo, you must first install the nemo_toolkit using the instructions listed above. Then, you run the following domain-specific commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install nemo_toolkit['all'] # or pip install "nemo_toolkit['all']@git+https://github.com/NVIDIA/NeMo@${REF:-'main'}"
pip install nemo_toolkit['asr'] # or pip install "nemo_toolkit['asr']@git+https://github.com/NVIDIA/NeMo@$REF:-'main'}"
pip install nemo_toolkit['nlp'] # or pip install "nemo_toolkit['nlp']@git+https://github.com/NVIDIA/NeMo@${REF:-'main'}"
pip install nemo_toolkit['tts'] # or pip install "nemo_toolkit['tts']@git+https://github.com/NVIDIA/NeMo@${REF:-'main'}"
pip install nemo_toolkit['vision'] # or pip install "nemo_toolkit['vision']@git+https://github.com/NVIDIA/NeMo@${REF:-'main'}"
pip install nemo_toolkit['multimodal'] # or pip install "nemo_toolkit['multimodal']@git+https://github.com/NVIDIA/NeMo@${REF:-'main'}"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;NGC PyTorch container&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;NOTE: The following steps are supported beginning with 24.04 (NeMo-Toolkit 2.3.0)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We recommended that you start with a base NVIDIA PyTorch container: nvcr.io/nvidia/pytorch:25.01-py3.&lt;/p&gt; 
&lt;p&gt;If starting with a base NVIDIA PyTorch container, you must first launch the container:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run \
  --gpus all \
  -it \
  --rm \
  --shm-size=16g \
  --ulimit memlock=-1 \
  --ulimit stack=67108864 \
  nvcr.io/nvidia/pytorch:${NV_PYTORCH_TAG:-'nvcr.io/nvidia/pytorch:25.01-py3'}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;From &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo/main/github.com/NVIDIA/NeMo"&gt;NVIDIA/NeMo&lt;/a&gt;, fetch the commit/branch/tag that you want to install.&lt;br /&gt; To install nemo_toolkit including all of its dependencies from this Git reference &lt;code&gt;$REF&lt;/code&gt;, use the following installation method:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd /opt
git clone https://github.com/NVIDIA/NeMo
cd NeMo
git checkout ${REF:-'main'}
bash docker/common/install_dep.sh --library all
pip install ".[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;NGC NeMo container&lt;/h2&gt; 
&lt;p&gt;NeMo containers are launched concurrently with NeMo version updates. NeMo Framework now supports LLMs, MMs, ASR, and TTS in a single consolidated Docker container. You can find additional information about released containers on the &lt;a href="https://github.com/NVIDIA/NeMo/releases"&gt;NeMo releases page&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To use a pre-built container, run the following code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run \
  --gpus all \
  -it \
  --rm \
  --shm-size=16g \
  --ulimit memlock=-1 \
  --ulimit stack=67108864 \
  nvcr.io/nvidia/pytorch:${NV_PYTORCH_TAG:-'nvcr.io/nvidia/nemo:25.02'}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Future Work&lt;/h2&gt; 
&lt;p&gt;The NeMo Framework Launcher does not currently support ASR and TTS training, but it will soon.&lt;/p&gt; 
&lt;h2&gt;Discussions Board&lt;/h2&gt; 
&lt;p&gt;FAQ can be found on the NeMo &lt;a href="https://github.com/NVIDIA/NeMo/discussions"&gt;Discussions board&lt;/a&gt;. You are welcome to ask questions or start discussions on the board.&lt;/p&gt; 
&lt;h2&gt;Contribute to NeMo&lt;/h2&gt; 
&lt;p&gt;We welcome community contributions! Please refer to &lt;a href="https://github.com/NVIDIA/NeMo/raw/stable/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for the process.&lt;/p&gt; 
&lt;h2&gt;Publications&lt;/h2&gt; 
&lt;p&gt;We provide an ever-growing list of &lt;a href="https://nvidia.github.io/NeMo/publications/"&gt;publications&lt;/a&gt; that utilize the NeMo Framework.&lt;/p&gt; 
&lt;p&gt;To contribute an article to the collection, please submit a pull request to the &lt;code&gt;gh-pages-src&lt;/code&gt; branch of this repository. For detailed information, please consult the README located at the &lt;a href="https://github.com/NVIDIA/NeMo/tree/gh-pages-src#readme"&gt;gh-pages-src branch&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Blogs&lt;/h2&gt; 
&lt;!-- markdownlint-disable --&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;b&gt;Large Language Models and Multimodal Models&lt;/b&gt;&lt;/summary&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://blogs.nvidia.com/blog/bria-builds-responsible-generative-ai-using-nemo-picasso/"&gt; Bria Builds Responsible Generative AI for Enterprises Using NVIDIA NeMo, Picasso &lt;/a&gt; (2024/03/06) &lt;/summary&gt; Bria, a Tel Aviv startup at the forefront of visual generative AI for enterprises now leverages the NVIDIA NeMo Framework. The Bria.ai platform uses reference implementations from the NeMo Multimodal collection, trained on NVIDIA Tensor Core GPUs, to enable high-throughput and low-latency image generation. Bria has also adopted NVIDIA Picasso, a foundry for visual generative AI models, to run inference. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/new-nvidia-nemo-framework-features-and-nvidia-h200-supercharge-llm-training-performance-and-versatility/"&gt; New NVIDIA NeMo Framework Features and NVIDIA H200 &lt;/a&gt; (2023/12/06) &lt;/summary&gt; NVIDIA NeMo Framework now includes several optimizations and enhancements, including: 1) Fully Sharded Data Parallelism (FSDP) to improve the efficiency of training large-scale AI models, 2) Mix of Experts (MoE)-based LLM architectures with expert parallelism for efficient LLM training at scale, 3) Reinforcement Learning from Human Feedback (RLHF) with TensorRT-LLM for inference stage acceleration, and 4) up to 4.2x speedups for Llama 2 pre-training on NVIDIA H200 Tensor Core GPUs. 
  &lt;br /&gt;
  &lt;br /&gt; 
  &lt;a href="https://developer.nvidia.com/blog/new-nvidia-nemo-framework-features-and-nvidia-h200-supercharge-llm-training-performance-and-versatility"&gt; &lt;img src="https://github.com/sbhavani/TransformerEngine/raw/main/docs/examples/H200-NeMo-performance.png" alt="H200-NeMo-performance" style="width: 600px;" /&gt;&lt;/a&gt; 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://blogs.nvidia.com/blog/nemo-amazon-titan/"&gt; NVIDIA now powers training for Amazon Titan Foundation models &lt;/a&gt; (2023/11/28) &lt;/summary&gt; NVIDIA NeMo Framework now empowers the Amazon Titan foundation models (FM) with efficient training of large language models (LLMs). The Titan FMs form the basis of Amazon‚Äôs generative AI service, Amazon Bedrock. The NeMo Framework provides a versatile framework for building, customizing, and running LLMs. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
&lt;/details&gt; 
&lt;!-- markdownlint-enable --&gt; 
&lt;h2&gt;Licenses&lt;/h2&gt; 
&lt;p&gt;NeMo is licensed under the &lt;a href="https://github.com/NVIDIA/NeMo?tab=Apache-2.0-1-ov-file"&gt;Apache License 2.0&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>