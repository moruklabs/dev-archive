<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Thu, 21 Aug 2025 01:37:11 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>laude-institute/terminal-bench</title>
      <link>https://github.com/laude-institute/terminal-bench</link>
      <description>&lt;p&gt;A benchmark for LLMs on complicated tasks in the terminal&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;terminal-bench&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;#####################################################################
#  _____                   _             _     ______________       #
# |_   _|__ _ __ _ __ ___ (_)_ __   __ _| |   ||            ||      #
#   | |/ _ \ '__| '_ ` _ \| | '_ \ / _` | |   || &amp;gt;          ||      #
#   | |  __/ |  | | | | | | | | | | (_| | |   ||            ||      #
#   |_|\___|_|  |_| |_| |_|_|_| |_|\__,_|_|   ||____________||      #
#   ____                  _                   |______________|      #
#  | __ )  ___ _ __   ___| |__                 \\############\\     #
#  |  _ \ / _ \ '_ \ / __| '_ \                 \\############\\    # 
#  | |_) |  __/ | | | (__| | | |                 \      ____    \   #
#  |____/ \___|_| |_|\___|_| |_|                  \_____\___\____\  #
#                                                                   #
#####################################################################
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/6xWPKhGDbA"&gt;&lt;img src="https://img.shields.io/badge/Join_our_discord-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://github.com/laude-institute/terminal-bench"&gt;&lt;img src="https://img.shields.io/badge/T--Bench-000000?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=000&amp;amp;logoColor=white" alt="Github" /&gt;&lt;/a&gt; &lt;a href="https://www.tbench.ai/docs"&gt;&lt;img src="https://img.shields.io/badge/Docs-000000?style=for-the-badge&amp;amp;logo=mdbook&amp;amp;color=105864" alt="Docs" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Terminal-Bench is the benchmark for testing AI agents in real terminal environments. From compiling code to training models and setting up servers, Terminal-Bench evaluates how well agents can handle real-world, end-to-end tasks - autonomously.&lt;/p&gt; 
&lt;p&gt;Whether you're building LLM agents, benchmarking frameworks, or stress-testing system-level reasoning, Terminal-Bench gives you a reproducible task suite and execution harness designed for practical, real-world evaluation.&lt;/p&gt; 
&lt;p&gt;Terminal-Bench consists of two parts: a &lt;strong&gt;dataset of tasks&lt;/strong&gt;, and an &lt;strong&gt;execution harness&lt;/strong&gt; that connects a language model to our terminal sandbox.&lt;/p&gt; 
&lt;p&gt;Terminal-Bench is currently in &lt;strong&gt;beta&lt;/strong&gt; with ~100 tasks. Over the coming months, we are going to expand Terminal-Bench into comprehensive testbed for AI agents in text-based environments. Any contributions are welcome, especially new and challenging tasks!&lt;/p&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;Our &lt;a href="https://www.tbench.ai/docs/installation"&gt;Quickstart Guide&lt;/a&gt; will walk you through installing the repo and contributing.&lt;/p&gt; 
&lt;p&gt;Terminal-Bench is distributed as a pip package and can be run using the Terminal-Bench CLI: &lt;code&gt;tb&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv tool install terminal-bench
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install terminal-bench
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Further Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.tbench.ai/tasks"&gt;Task Gallery&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.tbench.ai/docs/task-ideas"&gt;Task Ideas&lt;/a&gt; - Browse community-sourced task ideas&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.tbench.ai/docs/dashboard"&gt;Dashboard Documentation&lt;/a&gt; - Information about the Terminal-Bench dashboard&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Core Components&lt;/h2&gt; 
&lt;h3&gt;Dataset of Tasks&lt;/h3&gt; 
&lt;p&gt;Each task in Terminal-Bench includes&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;a instruction in English,&lt;/li&gt; 
 &lt;li&gt;a test script to verify if the language model / agent completed the task successfully,&lt;/li&gt; 
 &lt;li&gt;a reference ("oracle") solution that solves the task.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Tasks are located in the &lt;a href="https://raw.githubusercontent.com/laude-institute/terminal-bench/main/tasks"&gt;&lt;code&gt;tasks&lt;/code&gt;&lt;/a&gt; folder of the repository, and the aforementioned list of current tasks gives an overview that is easy to browse.&lt;/p&gt; 
&lt;h3&gt;Execution Harness&lt;/h3&gt; 
&lt;p&gt;The harness connects language models to a sandboxed terminal environment. After &lt;a href="https://www.tbench.ai/docs/installation"&gt;installing the terminal-bench package&lt;/a&gt; (along with the dependencies &lt;code&gt;uv&lt;/code&gt; and &lt;code&gt;Docker&lt;/code&gt;) you can view how to run the harness using:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tb run --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For detailed information about running the harness and its options, see the &lt;a href="https://www.tbench.ai/docs/first-steps"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Submit to Our Leaderboard&lt;/h3&gt; 
&lt;p&gt;Terminal-Bench-Core v0.1.1 is the set of tasks for Terminal-Bench's beta release and corresponds to the current leaderboard. To evaluate on it pass &lt;code&gt;--dataset-name terminal-bench-core&lt;/code&gt; and &lt;code&gt;--dataset-version 0.1.1&lt;/code&gt; to the harness. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tb run \
    --agent terminus \
    --model-name anthropic/claude-3-7-latest \
    --dataset-name terminal-bench-core
    --dataset-version 0.1.1
    --n-concurrent 8
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more detailed instructions on submitting to the leaderboard, view our &lt;a href="https://www.tbench.ai/docs/submitting-to-leaderboard"&gt;leaderboard submission guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For more information on Terminal-Bench datasets and versioning view our &lt;a href="https://www.tbench.ai/docs/registry"&gt;registry overview&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Creating New Tasks&lt;/h2&gt; 
&lt;p&gt;View our &lt;a href="https://www.tbench.ai/docs/task-quickstart"&gt;task contribution quickstart&lt;/a&gt; to create a new task.&lt;/p&gt; 
&lt;h2&gt;Citing Us&lt;/h2&gt; 
&lt;p&gt;If you found Terminal-Bench useful, please cite us as:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{tbench_2025,
      title={Terminal-Bench: A Benchmark for AI Agents in Terminal Environments}, 
      url={https://github.com/laude-institute/terminal-bench}, 
      author={The Terminal-Bench Team}, year={2025}, month={Apr}} 
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>prowler-cloud/prowler</title>
      <link>https://github.com/prowler-cloud/prowler</link>
      <description>&lt;p&gt;Prowler is the Open Cloud Security platform for AWS, Azure, GCP, Kubernetes, M365 and more. It helps for continuous monitoring, security assessments &amp; audits, incident response, compliance, hardening and forensics readiness. Includes CIS, NIST 800, NIST CSF, CISA, FedRAMP, PCI-DSS, GDPR, HIPAA, FFIEC, SOC2, ENS and more&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img align="center" src="https://github.com/prowler-cloud/prowler/raw/master/docs/img/prowler-logo-black.png#gh-light-mode-only" width="50%" height="50%" /&gt; &lt;img align="center" src="https://github.com/prowler-cloud/prowler/raw/master/docs/img/prowler-logo-white.png#gh-dark-mode-only" width="50%" height="50%" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;b&gt;&lt;i&gt;Prowler&lt;/i&gt;&lt;/b&gt;&lt;i&gt; is the Open Cloud Security platform trusted by thousands to automate security and compliance in any cloud environment. With hundreds of ready-to-use checks and compliance frameworks, Prowler delivers real-time, customizable monitoring and seamless integrations, making cloud security simple, scalable, and cost-effective for organizations of any size. &lt;/i&gt;&lt;/p&gt;
&lt;i&gt; &lt;/i&gt;
&lt;p align="center"&gt;&lt;i&gt; &lt;b&gt;Learn more at &lt;a href="https://prowler.com"&gt;prowler.com&lt;/a&gt;&lt;/b&gt;&lt;/i&gt;&lt;a href="https://prowler.com"&gt; &lt;/a&gt;&lt;/p&gt;
&lt;a href="https://prowler.com"&gt; &lt;/a&gt;
&lt;p align="center"&gt;&lt;a href="https://prowler.com"&gt; &lt;/a&gt;&lt;a href="https://goto.prowler.com/slack"&gt;&lt;img width="30" height="30" alt="Prowler community on Slack" src="https://github.com/prowler-cloud/prowler/assets/38561120/3c8b4ec5-6849-41a5-b5e1-52bbb94af73a" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://goto.prowler.com/slack"&gt;Join our Prowler community!&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; &lt;a href="https://goto.prowler.com/slack"&gt;&lt;img alt="Slack Shield" src="https://img.shields.io/badge/slack-prowler-brightgreen.svg?logo=slack" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/prowler/"&gt;&lt;img alt="Python Version" src="https://img.shields.io/pypi/v/prowler.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/prowler/"&gt;&lt;img alt="Python Version" src="https://img.shields.io/pypi/pyversions/prowler.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://pypistats.org/packages/prowler"&gt;&lt;img alt="PyPI Prowler Downloads" src="https://img.shields.io/pypi/dw/prowler.svg?label=prowler%20downloads" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/toniblyx/prowler"&gt;&lt;img alt="Docker Pulls" src="https://img.shields.io/docker/pulls/toniblyx/prowler" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/toniblyx/prowler"&gt;&lt;img alt="Docker" src="https://img.shields.io/docker/cloud/build/toniblyx/prowler" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/toniblyx/prowler"&gt;&lt;img alt="Docker" src="https://img.shields.io/docker/image-size/toniblyx/prowler" /&gt;&lt;/a&gt; &lt;a href="https://gallery.ecr.aws/prowler-cloud/prowler"&gt;&lt;img width="120" height="19&amp;quot;" alt="AWS ECR Gallery" src="https://user-images.githubusercontent.com/3985464/151531396-b6535a68-c907-44eb-95a1-a09508178616.png" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/prowler-cloud/prowler"&gt;&lt;img src="https://codecov.io/gh/prowler-cloud/prowler/graph/badge.svg?token=OflBGsdpDl" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/prowler-cloud/prowler"&gt;&lt;img alt="Repo size" src="https://img.shields.io/github/repo-size/prowler-cloud/prowler" /&gt;&lt;/a&gt; &lt;a href="https://github.com/prowler-cloud/prowler/issues"&gt;&lt;img alt="Issues" src="https://img.shields.io/github/issues/prowler-cloud/prowler" /&gt;&lt;/a&gt; &lt;a href="https://github.com/prowler-cloud/prowler/releases"&gt;&lt;img alt="Version" src="https://img.shields.io/github/v/release/prowler-cloud/prowler" /&gt;&lt;/a&gt; &lt;a href="https://github.com/prowler-cloud/prowler/releases"&gt;&lt;img alt="Version" src="https://img.shields.io/github/release-date/prowler-cloud/prowler" /&gt;&lt;/a&gt; &lt;a href="https://github.com/prowler-cloud/prowler"&gt;&lt;img alt="Contributors" src="https://img.shields.io/github/contributors-anon/prowler-cloud/prowler" /&gt;&lt;/a&gt; &lt;a href="https://github.com/prowler-cloud/prowler"&gt;&lt;img alt="License" src="https://img.shields.io/github/license/prowler-cloud/prowler" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/ToniBlyx"&gt;&lt;img alt="Twitter" src="https://img.shields.io/twitter/follow/toniblyx?style=social" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/prowlercloud"&gt;&lt;img alt="Twitter" src="https://img.shields.io/twitter/follow/prowlercloud?style=social" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; &lt;img align="center" src="https://raw.githubusercontent.com/prowler-cloud/prowler/master/docs/img/prowler-cli-quick.gif" width="100%" height="100%" /&gt; &lt;/p&gt; 
&lt;h1&gt;Description&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Prowler&lt;/strong&gt; is an open-source security tool designed to assess and enforce security best practices across AWS, Azure, Google Cloud, and Kubernetes. It supports tasks such as security audits, incident response, continuous monitoring, system hardening, forensic readiness, and remediation processes.&lt;/p&gt; 
&lt;p&gt;Prowler includes hundreds of built-in controls to ensure compliance with standards and frameworks, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Industry Standards:&lt;/strong&gt; CIS, NIST 800, NIST CSF, and CISA&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Regulatory Compliance and Governance:&lt;/strong&gt; RBI, FedRAMP, and PCI-DSS&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Frameworks for Sensitive Data and Privacy:&lt;/strong&gt; GDPR, HIPAA, and FFIEC&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Frameworks for Organizational Governance and Quality Control:&lt;/strong&gt; SOC2 and GXP&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AWS-Specific Frameworks:&lt;/strong&gt; AWS Foundational Technical Review (FTR) and AWS Well-Architected Framework (Security Pillar)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;National Security Standards:&lt;/strong&gt; ENS (Spanish National Security Scheme)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Custom Security Frameworks:&lt;/strong&gt; Tailored to your needs&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Prowler CLI and Prowler Cloud&lt;/h2&gt; 
&lt;p&gt;Prowler offers a Command Line Interface (CLI), known as Prowler Open Source, and an additional service built on top of it, called &lt;a href="https://prowler.com"&gt;Prowler Cloud&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Prowler App&lt;/h2&gt; 
&lt;p&gt;Prowler App is a web-based application that simplifies running Prowler across your cloud provider accounts. It provides a user-friendly interface to visualize the results and streamline your security assessments.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/prowler-cloud/prowler/master/docs/img/overview.png" alt="Prowler App" /&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For more details, refer to the &lt;a href="https://docs.prowler.com/projects/prowler-open-source/en/latest/#prowler-app-installation"&gt;Prowler App Documentation&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Prowler CLI&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;prowler &amp;lt;provider&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/prowler-cloud/prowler/master/docs/img/short-display.png" alt="Prowler CLI Execution" /&gt;&lt;/p&gt; 
&lt;h2&gt;Prowler Dashboard&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;prowler dashboard
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/prowler-cloud/prowler/master/docs/img/dashboard.png" alt="Prowler Dashboard" /&gt;&lt;/p&gt; 
&lt;h1&gt;Prowler at a Glance&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Provider&lt;/th&gt; 
   &lt;th&gt;Checks&lt;/th&gt; 
   &lt;th&gt;Services&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://docs.prowler.com/projects/prowler-open-source/en/latest/tutorials/compliance/"&gt;Compliance Frameworks&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://docs.prowler.com/projects/prowler-open-source/en/latest/tutorials/misc/#categories"&gt;Categories&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AWS&lt;/td&gt; 
   &lt;td&gt;571&lt;/td&gt; 
   &lt;td&gt;82&lt;/td&gt; 
   &lt;td&gt;36&lt;/td&gt; 
   &lt;td&gt;10&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GCP&lt;/td&gt; 
   &lt;td&gt;79&lt;/td&gt; 
   &lt;td&gt;13&lt;/td&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Azure&lt;/td&gt; 
   &lt;td&gt;162&lt;/td&gt; 
   &lt;td&gt;19&lt;/td&gt; 
   &lt;td&gt;11&lt;/td&gt; 
   &lt;td&gt;4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Kubernetes&lt;/td&gt; 
   &lt;td&gt;83&lt;/td&gt; 
   &lt;td&gt;7&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;7&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GitHub&lt;/td&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;M365&lt;/td&gt; 
   &lt;td&gt;70&lt;/td&gt; 
   &lt;td&gt;7&lt;/td&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;NHN (Unofficial)&lt;/td&gt; 
   &lt;td&gt;6&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;0&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] The numbers in the table are updated periodically.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Tip] For the most accurate and up-to-date information about checks, services, frameworks, and categories, visit &lt;a href="https://hub.prowler.com"&gt;&lt;strong&gt;Prowler Hub&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] Use the following commands to list Prowler's available checks, services, compliance frameworks, and categories: &lt;code&gt;prowler &amp;lt;provider&amp;gt; --list-checks&lt;/code&gt;, &lt;code&gt;prowler &amp;lt;provider&amp;gt; --list-services&lt;/code&gt;, &lt;code&gt;prowler &amp;lt;provider&amp;gt; --list-compliance&lt;/code&gt; and &lt;code&gt;prowler &amp;lt;provider&amp;gt; --list-categories&lt;/code&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;💻 Installation&lt;/h1&gt; 
&lt;h2&gt;Prowler App&lt;/h2&gt; 
&lt;p&gt;Prowler App offers flexible installation methods tailored to various environments:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For detailed instructions on using Prowler App, refer to the &lt;a href="https://docs.prowler.com/projects/prowler-open-source/en/latest/tutorials/prowler-app/"&gt;Prowler App Usage Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Docker Compose&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;Docker Compose&lt;/code&gt; installed: &lt;a href="https://docs.docker.com/compose/install/"&gt;https://docs.docker.com/compose/install/&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Commands&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;curl -LO https://raw.githubusercontent.com/prowler-cloud/prowler/refs/heads/master/docker-compose.yml
curl -LO https://raw.githubusercontent.com/prowler-cloud/prowler/refs/heads/master/.env
docker compose up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Containers are built for &lt;code&gt;linux/amd64&lt;/code&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Configuring Your Workstation for Prowler App&lt;/h3&gt; 
&lt;p&gt;If your workstation's architecture is incompatible, you can resolve this by:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Setting the environment variable&lt;/strong&gt;: &lt;code&gt;DOCKER_DEFAULT_PLATFORM=linux/amd64&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Using the following flag in your Docker command&lt;/strong&gt;: &lt;code&gt;--platform linux/amd64&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Once configured, access the Prowler App at &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt;. Sign up using your email and password to get started.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Common Issues with Docker Pull Installation&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] If you want to use AWS role assumption (e.g., with the "Connect assuming IAM Role" option), you may need to mount your local &lt;code&gt;.aws&lt;/code&gt; directory into the container as a volume (e.g., &lt;code&gt;- "${HOME}/.aws:/home/prowler/.aws:ro"&lt;/code&gt;). There are several ways to configure credentials for Docker containers. See the &lt;a href="https://raw.githubusercontent.com/prowler-cloud/prowler/master/docs/troubleshooting.md"&gt;Troubleshooting&lt;/a&gt; section for more details and examples.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;You can find more information in the &lt;a href="https://raw.githubusercontent.com/prowler-cloud/prowler/master/docs/troubleshooting.md"&gt;Troubleshooting&lt;/a&gt; section.&lt;/p&gt; 
&lt;h3&gt;From GitHub&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;git&lt;/code&gt; installed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;poetry&lt;/code&gt; v2 installed: &lt;a href="https://python-poetry.org/docs/#installation"&gt;poetry installation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;npm&lt;/code&gt; installed: &lt;a href="https://docs.npmjs.com/downloading-and-installing-node-js-and-npm"&gt;npm installation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Docker Compose&lt;/code&gt; installed: &lt;a href="https://docs.docker.com/compose/install/"&gt;https://docs.docker.com/compose/install/&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Commands to run the API&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;git clone https://github.com/prowler-cloud/prowler
cd prowler/api
poetry install
eval $(poetry env activate)
set -a
source .env
docker compose up postgres valkey -d
cd src/backend
python manage.py migrate --database admin
gunicorn -c config/guniconf.py config.wsgi:application
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] As of Poetry v2.0.0, the &lt;code&gt;poetry shell&lt;/code&gt; command has been deprecated. Use &lt;code&gt;poetry env activate&lt;/code&gt; instead for environment activation.&lt;/p&gt; 
 &lt;p&gt;If your Poetry version is below v2.0.0, continue using &lt;code&gt;poetry shell&lt;/code&gt; to activate your environment. For further guidance, refer to the Poetry Environment Activation Guide &lt;a href="https://python-poetry.org/docs/managing-environments/#activating-the-environment"&gt;https://python-poetry.org/docs/managing-environments/#activating-the-environment&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;After completing the setup, access the API documentation at &lt;a href="http://localhost:8080/api/v1/docs"&gt;http://localhost:8080/api/v1/docs&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Commands to run the API Worker&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;git clone https://github.com/prowler-cloud/prowler
cd prowler/api
poetry install
eval $(poetry env activate)
set -a
source .env
cd src/backend
python -m celery -A config.celery worker -l info -E
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Commands to run the API Scheduler&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;git clone https://github.com/prowler-cloud/prowler
cd prowler/api
poetry install
eval $(poetry env activate)
set -a
source .env
cd src/backend
python -m celery -A config.celery beat -l info --scheduler django_celery_beat.schedulers:DatabaseScheduler
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Commands to run the UI&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;git clone https://github.com/prowler-cloud/prowler
cd prowler/ui
npm install
npm run build
npm start
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Once configured, access the Prowler App at &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt;. Sign up using your email and password to get started.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Prowler CLI&lt;/h2&gt; 
&lt;h3&gt;Pip package&lt;/h3&gt; 
&lt;p&gt;Prowler CLI is available as a project in &lt;a href="https://pypi.org/project/prowler-cloud/"&gt;PyPI&lt;/a&gt;. Consequently, it can be installed using pip with Python &amp;gt;3.9.1, &amp;lt;3.13:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;pip install prowler
prowler -v
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For further guidance, refer to &lt;a href="https://docs.prowler.com/projects/prowler-open-source/en/latest/#prowler-cli-installation"&gt;https://docs.prowler.com&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Containers&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Available Versions of Prowler CLI&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The following versions of Prowler CLI are available, depending on your requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;latest&lt;/code&gt;: Synchronizes with the &lt;code&gt;master&lt;/code&gt; branch. Note that this version is not stable.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;v4-latest&lt;/code&gt;: Synchronizes with the &lt;code&gt;v4&lt;/code&gt; branch. Note that this version is not stable.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;v3-latest&lt;/code&gt;: Synchronizes with the &lt;code&gt;v3&lt;/code&gt; branch. Note that this version is not stable.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;x.y.z&amp;gt;&lt;/code&gt; (release): Stable releases corresponding to specific versions. You can find the complete list of releases &lt;a href="https://github.com/prowler-cloud/prowler/releases"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;stable&lt;/code&gt;: Always points to the latest release.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;v4-stable&lt;/code&gt;: Always points to the latest release for v4.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;v3-stable&lt;/code&gt;: Always points to the latest release for v3.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The container images are available here:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Prowler CLI: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://hub.docker.com/r/toniblyx/prowler/tags"&gt;DockerHub&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://gallery.ecr.aws/prowler-cloud/prowler"&gt;AWS Public ECR&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Prowler App: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://hub.docker.com/r/prowlercloud/prowler-ui/tags"&gt;DockerHub - Prowler UI&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://hub.docker.com/r/prowlercloud/prowler-api/tags"&gt;DockerHub - Prowler API&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;From GitHub&lt;/h3&gt; 
&lt;p&gt;Python &amp;gt;3.9.1, &amp;lt;3.13 is required with pip and Poetry:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;git clone https://github.com/prowler-cloud/prowler
cd prowler
eval $(poetry env activate)
poetry install
python prowler-cli.py -v
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] To clone Prowler on Windows, configure Git to support long file paths by running the following command: &lt;code&gt;git config core.longpaths true&lt;/code&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] As of Poetry v2.0.0, the &lt;code&gt;poetry shell&lt;/code&gt; command has been deprecated. Use &lt;code&gt;poetry env activate&lt;/code&gt; instead for environment activation.&lt;/p&gt; 
 &lt;p&gt;If your Poetry version is below v2.0.0, continue using &lt;code&gt;poetry shell&lt;/code&gt; to activate your environment. For further guidance, refer to the Poetry Environment Activation Guide &lt;a href="https://python-poetry.org/docs/managing-environments/#activating-the-environment"&gt;https://python-poetry.org/docs/managing-environments/#activating-the-environment&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;✏️ High level architecture&lt;/h1&gt; 
&lt;h2&gt;Prowler App&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Prowler App&lt;/strong&gt; is composed of three key components:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Prowler UI&lt;/strong&gt;: A web-based interface, built with Next.js, providing a user-friendly experience for executing Prowler scans and visualizing results.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Prowler API&lt;/strong&gt;: A backend service, developed with Django REST Framework, responsible for running Prowler scans and storing the generated results.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Prowler SDK&lt;/strong&gt;: A Python SDK designed to extend the functionality of the Prowler CLI for advanced capabilities.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/prowler-cloud/prowler/master/docs/img/prowler-app-architecture.png" alt="Prowler App Architecture" /&gt;&lt;/p&gt; 
&lt;h2&gt;Prowler CLI&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Running Prowler&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Prowler can be executed across various environments, offering flexibility to meet your needs. It can be run from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Your own workstation&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;A Kubernetes Job&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Google Compute Engine&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Azure Virtual Machines (VMs)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Amazon EC2 instances&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;AWS Fargate or other container platforms&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;CloudShell&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;And many more environments.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/prowler-cloud/prowler/master/docs/img/architecture.png" alt="Architecture" /&gt;&lt;/p&gt; 
&lt;h1&gt;Deprecations from v3&lt;/h1&gt; 
&lt;h2&gt;General&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;Allowlist&lt;/code&gt; now is called &lt;code&gt;Mutelist&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;The &lt;code&gt;--quiet&lt;/code&gt; option has been deprecated. Use the &lt;code&gt;--status&lt;/code&gt; flag to filter findings based on their status: PASS, FAIL, or MANUAL.&lt;/li&gt; 
 &lt;li&gt;All findings with an &lt;code&gt;INFO&lt;/code&gt; status have been reclassified as &lt;code&gt;MANUAL&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;The CSV output format is standardized across all providers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Deprecated Output Formats&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The following formats are now deprecated:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Native JSON has been replaced with JSON in [OCSF] v1.1.0 format, which is standardized across all providers (&lt;a href="https://schema.ocsf.io/"&gt;https://schema.ocsf.io/&lt;/a&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;AWS&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;AWS Flag Deprecation&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The flag --sts-endpoint-region has been deprecated due to the adoption of AWS STS regional tokens.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Sending FAIL Results to AWS Security Hub&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;To send only FAILS to AWS Security Hub, use one of the following options: &lt;code&gt;--send-sh-only-fails&lt;/code&gt; or &lt;code&gt;--security-hub --status FAIL&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;📖 Documentation&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Documentation Resources&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;For installation instructions, usage details, tutorials, and the Developer Guide, visit &lt;a href="https://docs.prowler.com/"&gt;https://docs.prowler.com/&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;📃 License&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Prowler License Information&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Prowler is licensed under the Apache License 2.0, as indicated in each file within the repository. Obtaining a Copy of the License&lt;/p&gt; 
&lt;p&gt;A copy of the License is available at &lt;a href="http://www.apache.org/licenses/LICENSE-2.0"&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>HKUDS/AI-Researcher</title>
      <link>https://github.com/HKUDS/AI-Researcher</link>
      <description>&lt;p&gt;"AI-Researcher: Autonomous Scientific Innovation"&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/assets/ai-researcher.png" alt="Logo" width="400" /&gt; 
 &lt;h1 align="center"&gt;"AI-Researcher: Autonomous Scientific Innovation" &lt;/h1&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/14638" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14638" alt="HKUDS%2FAI-Researcher | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://autoresearcher.github.io"&gt;&lt;img src="https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&amp;amp;color=FFE165&amp;amp;logo=homepage&amp;amp;logoColor=white" alt="Project Page" /&gt;&lt;/a&gt; 
 &lt;a href="https://join.slack.com/t/ai-researchergroup/shared_invite/zt-30y5a070k-C0ajQt1zmVczFnfGkIicvA"&gt;&lt;img src="https://img.shields.io/badge/Slack-Join%20Us-red?logo=slack&amp;amp;logoColor=white&amp;amp;style=for-the-badge" alt="Join our Slack community" /&gt;&lt;/a&gt; 
 &lt;a href="https://discord.gg/zBNYTk5q2g"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20Us-purple?logo=discord&amp;amp;logoColor=white&amp;amp;style=for-the-badge" alt="Join our Discord community" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://autoresearcher.github.io/docs"&gt;&lt;img src="https://img.shields.io/badge/Documentation-000?logo=googledocs&amp;amp;logoColor=FFE165&amp;amp;style=for-the-badge" alt="Check out the documentation" /&gt;&lt;/a&gt; 
 &lt;a href="https://arxiv.org/abs/2505.18705"&gt;&lt;img src="https://img.shields.io/badge/Paper%20on%20Arxiv-000?logoColor=FFE165&amp;amp;logo=arxiv&amp;amp;style=for-the-badge" alt="Paper" /&gt;&lt;/a&gt; 
 &lt;a href="https://autoresearcher.github.io/leaderboard"&gt;&lt;img src="https://img.shields.io/badge/DATASETS-000?logoColor=FFE165&amp;amp;logo=huggingface&amp;amp;style=for-the-badge" alt="Benchmark" /&gt;&lt;/a&gt; 
 &lt;hr /&gt; 
&lt;/div&gt; 
&lt;p&gt;Welcome to &lt;strong&gt;AI-Researcher&lt;/strong&gt;🤗 AI-Researcher introduces a revolutionary breakthrough in &lt;strong&gt;Automated Scientific Discovery&lt;/strong&gt;🔬, presenting a new system that fundamentally &lt;strong&gt;Reshapes the Traditional Research Paradigm&lt;/strong&gt;. This state-of-the-art platform empowers researchers with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🎯 &lt;strong&gt;Full Autonomy&lt;/strong&gt;: Complete end-to-end research automation&lt;/li&gt; 
 &lt;li&gt;🔄 &lt;strong&gt;Seamless Orchestration&lt;/strong&gt;: From concept to publication&lt;/li&gt; 
 &lt;li&gt;🧠 &lt;strong&gt;Advanced AI Integration&lt;/strong&gt;: Powered by cutting-edge AI agents&lt;/li&gt; 
 &lt;li&gt;🚀 &lt;strong&gt;Research Acceleration&lt;/strong&gt;: Streamlined scientific innovation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;✨ The AI-Researcher system accepts user input queries at two distinct levels ✨&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Level 1: Detailed Idea Description&lt;/strong&gt; &lt;br /&gt; At this level, users provide comprehensive descriptions of their specific research ideas. The system processes these detailed inputs to develop implementation strategies based on the user's explicit requirements.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Level 2: Reference-Based Ideation&lt;/strong&gt; &lt;br /&gt; This simpler level involves users submitting reference papers without a specific idea in mind. The user query typically follows the format: "I have some reference papers, please come up with an innovative idea and implement it with these papers." The system then analyzes the provided references to generate and develop novel research concepts.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;🌟&lt;strong&gt;Core Capabilities &amp;amp; Integration&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;AI-Researcher&lt;/strong&gt; delivers a &lt;strong&gt;Comprehensive Research Ecosystem&lt;/strong&gt; through seamless integration of critical components:&lt;/p&gt; 
&lt;p&gt;🚀&lt;strong&gt;Primary Research Functions&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;📚 &lt;strong&gt;Literature Review&lt;/strong&gt;: Conducts comprehensive analysis and synthesis of existing research.&lt;/li&gt; 
 &lt;li&gt;📊 &lt;strong&gt;Idea Generation&lt;/strong&gt;: Systematically gathers, organizes, and formulates novel research directions.&lt;/li&gt; 
 &lt;li&gt;🧪 &lt;strong&gt;Algorithm Design and Implementation&lt;/strong&gt;: Develops methodologies and transforms ideas into functional implementations.&lt;/li&gt; 
 &lt;li&gt;💻 &lt;strong&gt;Algorithm Validation and Refinement&lt;/strong&gt;: Automates testing, performance evaluation, and iterative optimization.&lt;/li&gt; 
 &lt;li&gt;📈 &lt;strong&gt;Result Analysis&lt;/strong&gt;: Delivers advanced interpretation of experimental data and insights.&lt;/li&gt; 
 &lt;li&gt;✍️ &lt;strong&gt;Manuscript Creation&lt;/strong&gt;: Automatically generates polished, full-length academic papers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;!-- &lt;img src="./assets/AI-Researchernew-intro.pdf" alt="Logo" width="100%"&gt; --&gt; 
 &lt;figure&gt; 
  &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/assets/AI-Researcher-Framework.png" alt="Logo" style="max-width: 100%; height: auto;" /&gt; 
  &lt;br /&gt; 
  &lt;figcaption&gt;
   &lt;em&gt;Quick Overview of AI-Researcher.&lt;/em&gt;
  &lt;/figcaption&gt; 
 &lt;/figure&gt; 
&lt;/div&gt; 
&lt;span id="news"&gt;&lt;/span&gt; 
&lt;h2&gt;🔥 News&lt;/h2&gt; 
&lt;div class="scrollable"&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;[2025, May 24]&lt;/strong&gt;: &amp;nbsp;🎉🎉 &lt;b&gt;Major Release! AI-Researcher Comprehensive Upgrade!&lt;/b&gt; 🚀 &lt;br /&gt;We are excited to announce a significant milestone for AI-Researcher: 
   &lt;ul&gt; 
    &lt;li&gt;📄 &lt;b&gt;&lt;a href="https://arxiv.org/abs/2505.18705"&gt;Academic Paper Release&lt;/a&gt;&lt;/b&gt;: Detailed exposition of our innovative methods and experimental results&lt;/li&gt; 
    &lt;li&gt;📊 &lt;b&gt;&lt;a href="https://autoresearcher.github.io/leaderboard"&gt;Benchmark Suite&lt;/a&gt;&lt;/b&gt;: Comprehensive evaluation framework and datasets&lt;/li&gt; 
    &lt;li&gt;🖥️ &lt;b&gt;Web GUI Interface&lt;/b&gt;: User-friendly graphical interface making research more convenient&lt;/li&gt; 
   &lt;/ul&gt; &lt;b&gt;🤝 Join Us!&lt;/b&gt; We welcome researchers, developers, and AI enthusiasts to contribute together and advance AI research development. Whether it's code contributions, bug reports, feature suggestions, or documentation improvements, every contribution is valuable! &lt;br /&gt;💡 &lt;i&gt;Let's build a smarter AI research assistant together!&lt;/i&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2025, Mar 04]&lt;/strong&gt;: &amp;nbsp;🎉🎉We've launched &lt;b&gt;AI-Researcher!&lt;/b&gt;, The release includes the complete framework, datasets, benchmark construction pipeline, and much more. Stay tuned—there's plenty more to come! 🚀&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;span id="table-of-contents"&gt;&lt;/span&gt; 
&lt;h2&gt;📑 Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#news"&gt;🔥 News&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#quick-start"&gt;⚡ Quick Start&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#api-keys-setup"&gt;API Keys Setup&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#examples"&gt;⬇️ Examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#how-it-works"&gt;✨ How AI-Researcher works&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#how-to-use"&gt;🔍 How to use AI-Researcher&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#documentation"&gt;📖 Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#community"&gt;🤝 Join the Community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#acknowledgements"&gt;🙏 Acknowledgements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#cite"&gt;🌟 Cite&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="quick-start"&gt;&lt;/span&gt; 
&lt;h2&gt;⚡ Quick Start&lt;/h2&gt; 
&lt;span id="installation"&gt;&lt;/span&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;h4&gt;AI Installation&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Using &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;We recommend to use &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt; to manage packages in our project (Much more faster than conda)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# install uv
curl -LsSf https://astral.sh/uv/install.sh | sh
source ~/.bashrc

# clone the project
git clone https://github.com/HKUDS/AI-Researcher.git
cd AI-Researcher

# install and activate enviroment
uv venv --python 3.11
source ./.venv/bin/activate
uv pip install -e .
playwright install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Docker Installation&lt;/h4&gt; 
&lt;p&gt;To set up the agent-interactive environment, we use Docker for containerization. Please ensure you have &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt; installed on your system before proceeding. For running the research agent, we utilize the Docker image 'tjbtech1/airesearcher:v1t'. You can pull this image by executing the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker pull tjbtech1/airesearcher:v1
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or you can build the docker image from our provided &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/docker/Dockerfile"&gt;Dockerfile&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd ./docker &amp;amp;&amp;amp; docker build -t tjbtech1/airesearcher:v1 .
&lt;/code&gt;&lt;/pre&gt; 
&lt;span id="api-keys-setup"&gt;&lt;/span&gt; 
&lt;h3&gt;API Keys Setup&lt;/h3&gt; 
&lt;p&gt;Create an environment variable file based on the provided '.env.template' file. In this file, you should set the configuration including api key, instance id of the test case.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;
# ================ container configuration ================
# workplace of the research agent
DOCKER_WORKPLACE_NAME=workplace_paper
# base image of the research agent
BASE_IMAGES=tjbtech1/airesearcher:v1
# completion model name, configuration details see: https://docs.litellm.ai/docs/
COMPLETION_MODEL=openrouter/google/gemini-2.5-pro-preview-05-20
# cheep model name, configuration details see: https://docs.litellm.ai/docs/
CHEEP_MODEL=openrouter/google/gemini-2.5-pro-preview-05-20
# specific gpu of the research agent, can be: 
# '"device=0"' using the first gpu
# '"device=0,1"' using the first and second gpu
# '"all"' using all gpus
# None for no gpu
GPUS='"device=0"'
# name of the container
CONTAINER_NAME=paper_eval
# name of the workplace
WORKPLACE_NAME=workplace
# path of the cache
CACHE_PATH=cache
# port of the research agent
PORT=7020
# platform of the research agent
PLATFORM=linux/amd64

# ================ llm configuration ================
# github ai token of the research agent
GITHUB_AI_TOKEN=your_github_ai_token
# openrouter api key of the research agent
OPENROUTER_API_KEY=your_openrouter_api_key
# openrouter api base url of the research agent
OPENROUTER_API_BASE=https://openrouter.ai/api/v1

# ================ task configuration ================
# category of the research agent, based on: ./benchmark/final. Can be: 
# diffu_flow
# gnn
# reasoning
# recommendation
# vq
# example: ./benchmark/final/vq
CATEGORY=vq
# instance id of the research agent, example: ./benchmark/final/vq/one_layer_vq.json
INSTANCE_ID=one_layer_vq
# task level of the research agent, can be: 
# task1
# task2
TASK_LEVEL=task1
# maximum iteration times of the research agent
MAX_ITER_TIMES=0
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;🔥 Web GUI&lt;/h3&gt; 
&lt;p&gt;We add a webgui based on gradio. Just run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python web_ai_researcher.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/assets/webgui/image-20250606135137558.png" alt="image-20250606135137558" /&gt;&lt;/p&gt; 
&lt;p&gt;You can configure the environment variables in the following tab:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/assets/webgui/image-20250606135325373.png" alt="image-20250606135325373" /&gt;&lt;/p&gt; 
&lt;p&gt;Select the following example to run our AI-Researcher:&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/assets/webgui/image-20250606135507970.png" alt="image-20250606135507970" style="zoom:67%;" /&gt; 
&lt;span id="examples"&gt;&lt;/span&gt; 
&lt;h2&gt;⬇️ Examples&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;⚠️ &lt;strong&gt;ALERT&lt;/strong&gt;: The GIFs below are large files and may &lt;strong&gt;take some time to load&lt;/strong&gt;. &lt;strong&gt;Please be patient while they render completely&lt;/strong&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Example 1 (Vector Quantized)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed model designed in this paper is designed to improve the performance of Vector Quantized Variational AutoEncoders (VQ-VAEs) by addressing issues with gradient propagation through the non-differentiable vector quantization layer.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;The core methodologies utilized include: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Rotation and Rescaling Transformation&lt;/strong&gt;: A linear transformation that alters the encoder output to align it with the nearest codebook vector without changing the forward pass output.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Gradient Propagation Method&lt;/strong&gt;: The proposed model ensures that gradients flow from the decoder to the encoder while preserving the angle between the gradient and codebook vector.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Codebook Management&lt;/strong&gt;: Utilizes the connection between the encoder output and the corresponding codebook vectors to mitigate codebook collapse and improve utilization.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;The primary functions of these components are: 
    &lt;ul&gt; 
     &lt;li&gt;The rotation and rescaling transformation modifies how the encoder output is quantized and how information is retained during backpropagation, enabling gradients to reflect the true positioning of the encoder output relative to the codebook vectors.&lt;/li&gt; 
     &lt;li&gt;The gradient propagation method redefines how gradients are transported back to the encoder, allowing for an enhanced and nuanced movement through the quantization layer, which leads to a better performance during training.&lt;/li&gt; 
     &lt;li&gt;Codebook management practices help in maintaining a diverse set of codebook vectors throughout training, avoiding scenarios where multiple vectors become redundant or unused.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Implementation details for each component: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Key Parameters&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Codebook size should be configured based on the complexity of the dataset (e.g., 1024 or 8192).&lt;/li&gt; 
       &lt;li&gt;Commitment loss coefficient (β) is typically set within [0.25, 2].&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Input/Output Specifications&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Input to the encoder is a continuous high-dimensional vector, while the output is a corresponding quantized vector from the codebook.&lt;/li&gt; 
       &lt;li&gt;The output for reconstruction is generated using the decoder applied to the transformed codebook vectors.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Important Constraints&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Ensure that the codebook is updated correctly with an exponential moving average procedure, and treat both rotation and rescaling during the forward pass as constants with respect to the gradient.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Step-by-Step Integration of Components: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: Input the data vector into the encoder to obtain the continuous representation.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: Identify the nearest codebook vector to the encoder output.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 3&lt;/strong&gt;: Compute the rotation matrix that aligns the encoder output to the codebook vector.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 4&lt;/strong&gt;: Apply the rotation and rescaling transformation to obtain the modified output for the decoder (i.e., `˜ q`).&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 5&lt;/strong&gt;: Feed `˜ q` into the decoder to produce the reconstructed output.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 6&lt;/strong&gt;: Compute the loss using the reconstruction and apply backpropagation.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 7&lt;/strong&gt;: During backpropagation, modify the gradient transfer process to maintain the angle using the proposed model, replacing traditional shortcuts in gradient computation.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Critical implementation details affecting performance: 
    &lt;ul&gt; 
     &lt;li&gt;The choice of rotation matrix calculation should ensure computational efficiency—using Householder transformations to minimize resource demands.&lt;/li&gt; 
     &lt;li&gt;The deployment of the stop-gradient technique effectively turns off the back-propagation through the quantization layer, which is essential to reflect the intended change without inducing undesired noise in the gradient updates.&lt;/li&gt; 
     &lt;li&gt;Monitor the codebook usage regularly during training to detect any potential collapse early and adjust the training dynamics (e.g., learning rate) accordingly to maintain effective utilization throughout the training period.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Neural discrete representation learning&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;Neural discrete representation learning&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Straightening out the straight-through estimator: Overcoming optimization challenges in vector quantized networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Estimating or propagating gradients through stochastic neurons for conditional computation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;High-resolution image synthesis with latent diffusion models&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Finite scalar quantization: Vq-vae made simple&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Elements of information theory&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Vector-quantized image modeling with improved vqgan&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Uvim: A unified modeling approach for vision with learned guiding codes&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Auto-encoding variational bayes&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Categorical reparameterization with gumbel-softmax&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/rotation_vq/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/rotation_vq/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/rotation_vq/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/rotation_vq/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Example 2 (Category: Vector Quantized)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed model focuses on discrete representation learning for tasks such as image generation, depth estimation, colorization, and segmentation using the proposed approach integrated into architectures like autoregressive transformers.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;Core Techniques: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Simplified Quantization&lt;/strong&gt;: Use a simplified quantization approach utilizing scalar quantization instead of VQ.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Dimensionality Projection&lt;/strong&gt;: Define a function to project the encoder output to a manageable dimensionality (typically between 3 to 10).&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Gradient Propagation&lt;/strong&gt;: Implement the Straight-Through Estimator (STE) for gradient propagation through the quantization operation.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Technical Components: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Bounding Function&lt;/strong&gt;: This compresses data dimensionality and confines values to a desired range. Use a function like \(f(z) = \left\lfloor \frac{L}{2} \right\rfloor \tanh(z)\) to project the data, where \(L\) is the number of quantization levels.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Quantization process&lt;/strong&gt;: Round each bounded dimension to its nearest integer to yield the quantized output.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Loss function&lt;/strong&gt;: Operate under a reconstruction loss paradigm typical in VAEs to optimize the proposed model parameters.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Key Parameters&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Number of dimensions \(d\) and levels \(L\) per dimension should be defined based on the codebook size you aim to replicate (e.g., set \(L_i \geq 5\) for all \(i\)).&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Input/Output Specifications&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;The input to the bounding function will be the output from the final encoder layer; the output after quantization will be in the format \(\hat{z}\), with shape matching the original \(z\).&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Constraints&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Ensure all inputs are preprocessed adequately to be within the functioning range of the bounding function.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Step-by-Step Integration: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: Train a standard VAE model and obtain its encoder output \(z\).&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: Apply the bounding function \(f\) on \(z\) to limit the output dimensions to usable values.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 3&lt;/strong&gt;: Quantize the resultant bounded \(z\) using the rounding procedure to generate \( \hat{z} \).&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 4&lt;/strong&gt;: Use the original \(z\) and \(\hat{z}\) in conjunction with the reconstruction loss to backpropagate through the network using the STE for gradient calculation.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Critical Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;Ensure the rounding process is correctly differentiable; utilize the STE to maintain gradient flow during backpropagation.&lt;/li&gt; 
     &lt;li&gt;Maintain high codebook utilization by selecting optimal dimensions and levels based on empirical trials, and monitor performance to refine the parameters if needed.&lt;/li&gt; 
     &lt;li&gt;Adjust the proposed model configurations (number of epochs, batch size) based on the structures laid out in this paper, ensuring hyperparameters match those recommended for the proposed approach integration.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Neural discrete representation learning&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;Neural discrete representation learning&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Conditional probability models for deep image compression&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;High-fidelity generative image compression&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;End-to-end optimized image compression&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Taming transformers for high-resolution image generation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;An algorithm for vector quantizer design&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Joint autoregressive and hierarchical priors for learned image compression&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Assessing generative models via precision and recall&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Variational bayes on discrete representation with self-annealed stochastic quantization&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;High quality monocular depth estimation via transfer learning&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/fsq/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/fsq/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/fsq/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/fsq/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Example 3 (Category: Recommendation)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed model aims to improve user-item interaction predictions in recommendation systems by leveraging heterogeneous relational information.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;Core Techniques/Algorithms: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Heterogeneous Graph Neural Networks (GNNs)&lt;/strong&gt;: Used for embedding initialization and message propagation across different types of user-item and user-user/item-item graphs.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Contrastive Learning&lt;/strong&gt;: Specifically, a cross-view contrastive learning framework is utilized to enhance representation learning by aligning embeddings from auxiliary views with user-item interaction embeddings.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Meta Networks&lt;/strong&gt;: Employed to extract personalized knowledge and facilitate customized knowledge transfer between auxiliary views and the user-item interaction view.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Purpose and Function of Each Major Component: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Heterogeneous GNN&lt;/strong&gt;: Encodes user and item relationships into embeddings that capture the semantics of various interactions.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Contrastive Learning&lt;/strong&gt;: Provides self-supervision signals to enhance the robustness of learned representations, allowing the proposed model to distinguish between relevant and irrelevant interactions.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Meta Network&lt;/strong&gt;: Models personalized characteristics to facilitate adaptive knowledge transfer, ensuring that the influence of auxiliary information is tailored to individual users and items.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Heterogeneous GNN&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;&lt;strong&gt;Key Parameters&lt;/strong&gt;: Use Xavier initializer for embedding initialization; set the hidden dimensionality &lt;code&gt;d&lt;/code&gt;.&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Input/Output&lt;/strong&gt;: Take adjacency matrices for user-item, user-user, and item-item graphs as input; output relation-aware embeddings.&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Constraints&lt;/strong&gt;: Ensure that the GNN can handle varying types of nodes and relations.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Contrastive Learning&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;&lt;strong&gt;Key Parameters&lt;/strong&gt;: Use cosine similarity as the similarity function; define a temperature coefficient for handling negative samples.&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Input/Output&lt;/strong&gt;: Input embeddings from the meta network and user/item views; output contrastive loss values.&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Constraints&lt;/strong&gt;: Maintain diverse representations to avoid overfitting.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Meta Network&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;&lt;strong&gt;Key Parameters&lt;/strong&gt;: Set up fully connected layers with PReLU activation to generate personalized transformation matrices.&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Input/Output&lt;/strong&gt;: Input user and item embeddings; output transformed embeddings for personalized knowledge transfer.&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Constraints&lt;/strong&gt;: Ensure low-rank decomposition of transformation matrices to reduce parameter count.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Step-by-Step Interaction: 
    &lt;ul&gt; 
     &lt;li&gt;Initialize user and item embeddings using a heterogeneous GNN.&lt;/li&gt; 
     &lt;li&gt;Perform heterogeneous message propagation to refine embeddings iteratively across user-item, user-user, and item-item graphs.&lt;/li&gt; 
     &lt;li&gt;Aggregate the refined embeddings from various views using a mean pooling function to retain heterogeneous semantics.&lt;/li&gt; 
     &lt;li&gt;Extract meta knowledge from the learned embeddings to create personalized mapping functions using the meta network.&lt;/li&gt; 
     &lt;li&gt;Apply contrastive learning to align embeddings from auxiliary views with the user-item interaction embeddings, generating a contrastive loss.&lt;/li&gt; 
     &lt;li&gt;Combine the contrastive loss with a pairwise loss function (like Bayesian Personalized Ranking) to optimize the proposed model.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Critical Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;Choose appropriate hyperparameters such as embedding size, learning rate, and the number of GNN layers through systematic experimentation.&lt;/li&gt; 
     &lt;li&gt;Monitor the proposed model for signs of overfitting, especially when increasing the number of GNN layers or embedding dimensions.&lt;/li&gt; 
     &lt;li&gt;Ensure diverse user-item interaction patterns are captured through sufficient training data and effective augmentation techniques.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Revisiting Graph Based Collaborative Filtering: A Linear Residual Graph Convolutional Network Approach&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;Revisiting Graph Based Collaborative Filtering: A Linear Residual Graph Convolutional Network Approach&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Graph Neural Networks for Social Recommendation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Improving Graph Collaborative Filtering with Neighborhood-enriched Contrastive Learning&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Knowledge-aware Coupled Graph Neural Network for Social Recommendation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Heterogeneous Graph Transformer&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Sequential Recommendation with Graph Neural Networks&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/hgcl/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/hgcl/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/hgcl/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/hgcl/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Example 4 (Category: Recommendation)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed model focuses on collaborative filtering for recommendation systems by leveraging graph neural networks (GNNs) and contrastive learning to address the issue of sparse user-item interactions.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;Core Techniques: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Graph Neural Networks&lt;/strong&gt;: Utilize GNNs for message passing to learn user and item embeddings from the interaction graph.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Disentangled Representations&lt;/strong&gt;: Implement a mechanism to model multiple latent intent factors driving user-item interactions.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Contrastive Learning&lt;/strong&gt;: Use contrastive learning techniques to generate adaptive self-supervised signals from augmented views of user-item interactions.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Purpose of Components: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;GNN Layers&lt;/strong&gt;: Capture high-order interactions among users and items through iterative message passing.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Intent Encoding&lt;/strong&gt;: Differentiate latent intents to improve the representation of user preferences.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Adaptive Augmentation&lt;/strong&gt;: Generate contrastive views that account for both local and global dependencies to enhance robustness against noise.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Graph Construction&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;&lt;strong&gt;Input&lt;/strong&gt;: User-item interaction matrix \( A \) of size \( I \times J \) (where \( I \) is the number of users and \( J \) is the number of items).&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: Normalized adjacency matrix \( \bar{A} \).&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;GNN Configuration&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Number of layers \( L \): Choose based on your dataset, typically 2 or 3 layers.&lt;/li&gt; 
       &lt;li&gt;Dimensionality \( d \) of embeddings: Start with \( d = 32 \).&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Intent Prototypes&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Number of intents \( K \): Experiment with values from {32, 64, 128, 256}, starting with \( K = 128 \).&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Learning Rate&lt;/strong&gt;: Use Adam optimizer with a learning rate around \( 1e-3 \).&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Loss Functions&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Use Bayesian Personalized Ranking (BPR) loss for the recommendation task.&lt;/li&gt; 
       &lt;li&gt;Implement InfoNCE loss for contrastive learning, incorporating both local and global augmented views.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Step-by-Step Interaction: 
    &lt;ul&gt; 
     &lt;li&gt;Construct the interaction graph from the user-item matrix.&lt;/li&gt; 
     &lt;li&gt;For each GNN layer: 
      &lt;ul&gt; 
       &lt;li&gt;Compute the aggregated embeddings \( Z(u) \) and \( Z(v) \) using the normalized adjacency matrix.&lt;/li&gt; 
       &lt;li&gt;Update user and item embeddings using residual connections to prevent over-smoothing.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;Generate intent-aware representations by aggregating embeddings over the latent intents.&lt;/li&gt; 
     &lt;li&gt;Apply the learned parameterized masks for adaptive augmentation during message passing to create multiple contrastive views.&lt;/li&gt; 
     &lt;li&gt;Calculate contrastive learning signals using the generated augmented representations and optimize using the combined loss function.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Critical Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;Ensure that the augmentation matrices are learned adaptively based on the current user-item embeddings to differentiate the importance of interactions.&lt;/li&gt; 
     &lt;li&gt;Monitor the performance with different numbers of latent intents \( K \) to find an optimal balance between expressiveness and noise.&lt;/li&gt; 
     &lt;li&gt;Regularly assess the proposed model for over-smoothing by checking the Mean Average Distance (MAD) metric on the embeddings.&lt;/li&gt; 
     &lt;li&gt;Tune hyperparameters \( \lambda_1, \lambda_2, \lambda_3 \) for the multi-task loss to balance the contribution of the self-supervised learning signals.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Lightgcn: Simplifying and powering graph convolution network for recommendation&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;Lightgcn: Simplifying and powering graph convolution network for recommendation&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Neural collaborative filtering&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Disentangled contrastive learning on graphs&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Improving Graph Collaborative Filtering with Neighborhood-enriched Contrastive Learning&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Curriculum Disentangled Recommendation with Noisy Multi-feedback&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Disentangled heterogeneous graph attention network for recommendation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Learning intents behind interactions with knowledge graph for recommendation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;LightGCL: Simple Yet Effective Graph Contrastive Learning for Recommendation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Self-supervised graph learning for recommendation&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/dccf/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/dccf/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/dccf/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/dccf/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Example 5 (Category: Diffusion and Flow Matching)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed model presented in this paper focuses on the task of generative modeling through the framework of Continuous Normalizing Flows (CNFs) to define straight flows between noise and data samples.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;Architecture: 
    &lt;ul&gt; 
     &lt;li&gt;Implement a neural network to parameterize the velocity field \( v_{\theta}(t, x) \) that maps from noise to data distributions.&lt;/li&gt; 
     &lt;li&gt;Use architectures suitable for continuous functions, such as feedforward or convolutional networks.&lt;/li&gt; 
     &lt;li&gt;Each layer should have non-linear activation functions (e.g., ReLU, Tanh).&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Loss Functions: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Velocity Consistency Loss&lt;/strong&gt;: This should be structured as: \[ L_{\theta} = E_{t \sim U} E_{x_t, x_{t+\Delta t}} \| f_{\theta}(t, x_t) - f_{\theta}(t+\Delta t, x_{t+\Delta t}) \|^2_2 + \alpha \| v_{\theta}(t, x_t) - v_{\theta}(t+\Delta t, x_{t+\Delta t}) \|^2_2 \] where \( f_{\theta}(t, x_t) = x_t + (1 - t) v_{\theta}(t, x_t) \). Choose \( \alpha \) based on cross-validation performance. &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Training Procedure: 
    &lt;ul&gt; 
     &lt;li&gt;Sample \( x_0 \) from the noise distribution \( p_0 \).&lt;/li&gt; 
     &lt;li&gt;For multiple time segments, define intervals and compute velocity fields iteratively.&lt;/li&gt; 
     &lt;li&gt;Use the weights of the proposed approach in an exponential moving average to stabilize training.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Sampling Process: 
    &lt;ul&gt; 
     &lt;li&gt;For single-step or multi-step generation, heuristically sample from the noise distribution and use the learned velocity field as follows: \[ x_{i/k} = x_{(i-1)/k} + \frac{1}{k} v_{i\theta}((i-1)/k, x_{(i-1)/k}) \] &lt;/li&gt; 
     &lt;li&gt;Apply the Euler method for iterative updates: \[ x_{t + \Delta t} = x_t + \Delta t v_i(t, x_t) \] where \( t \in [i/k, (i + 1)/k - \Delta t] \). &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Key Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;Ensure the network is equipped with a suitable optimizer such as Adam with a learning rate around \( 2 \times 10^{-4} \).&lt;/li&gt; 
     &lt;li&gt;The batch size should be appropriately set (e.g., 512 for CIFAR-10).&lt;/li&gt; 
     &lt;li&gt;Employ an ODE solver, suggested as Euler's method, during the training and sampling processes.&lt;/li&gt; 
     &lt;li&gt;Maintain a uniform distribution for sampling time intervals \( U \).&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Performance Considerations: 
    &lt;ul&gt; 
     &lt;li&gt;Monitor convergence rates and empirically validate parameter configurations through experiments. Start with fewer segments and gradually increase to capture complex distributions better.&lt;/li&gt; 
     &lt;li&gt;Adjust the decay rate for the EMA based on the stability of convergence (commonly around 0.999).&lt;/li&gt; 
     &lt;li&gt;Analyze the trade-offs between sampling efficiency and sample quality, ensuring a balance during proposed model development.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Flow matching for generative modeling&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;Flow matching for generative modeling&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Consistency models&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Rectified Flow&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Denoising diffusion probabilistic models&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Optimal flow matching: Learning straight trajectories in just one step&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Maximum likelihood training of score-based diffusion models&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/con_flowmatching/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/con_flowmatching/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/con_flowmatching/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/con_flowmatching/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Example 6 (Category: Graph Neural Networks)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed model focuses on the task of node classification in large graphs, addressing challenges like scalability, heterophily, long-range dependencies, and the absence of edges.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;The core techniques used in this study include a kernelized Gumbel-Softmax operator for all-pair message passing, which reduces computational complexity to linear (O(N)), and a Transformer-style network architecture designed for layer-wise learning of latent graph structures.&lt;/li&gt; 
   &lt;li&gt;The purpose of the kernelized Gumbel-Softmax operator is to enable differentiable learning of discrete graph structures by approximating categorical distributions. The Transformer-style architecture facilitates information propagation between arbitrary pairs of nodes through learned latent graphs.&lt;/li&gt; 
   &lt;li&gt;Implementation details for each component: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Kernelized Gumbel-Softmax Operator&lt;/strong&gt;: Set the temperature parameter (τ) to a range typically between 0.25 and 0.4 for training. It operates on node feature representations (D-dimensional feature vectors). The output of this operator is a distribution over node connections, facilitating the selection of neighbors for message passing.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Node Feature Input&lt;/strong&gt;: Each node input should be represented as a feature vector (e.g., {x_u} ∈ R^D), and the output is an updated representation of the node embedding after message passing.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Relational Bias (if applicable)&lt;/strong&gt;: Introduces activation (e.g., sigmoid) to adjust the message passing weights based on an observed adjacency matrix, which enhances weight assignment for connected nodes.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Edge Regularization Loss&lt;/strong&gt;: Combines categorical edge probabilities with a supervised classification loss, encouraging the network to maintain predicted edges consistent with observed edges.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;The step-by-step interaction of these components includes: 
    &lt;ul&gt; 
     &lt;li&gt;Begin with an input matrix of node embeddings (X) and, if available, an adjacency matrix (A).&lt;/li&gt; 
     &lt;li&gt;Apply the kernelized Gumbel-Softmax operator to the embedding matrix to generate a probability distribution over neighbor selection for each node.&lt;/li&gt; 
     &lt;li&gt;Use these probabilities to sample neighbors, allowing for message passing where each node aggregates information from its selected neighbors.&lt;/li&gt; 
     &lt;li&gt;Update the node embeddings using an attention mechanism, which can be enhanced by relational bias if edges are available.&lt;/li&gt; 
     &lt;li&gt;After K iterations of neighbor sampling, apply loss functions comprising a supervised classification loss and, if applicable, edge-level regularization loss to optimize the embedding representations.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Critical implementation details affecting performance involve: 
    &lt;ul&gt; 
     &lt;li&gt;Careful tuning of the temperature parameter (τ) in the Gumbel-Softmax operator, as it significantly influences the proposed approach's capacity to capture the discrete nature of graph structures.&lt;/li&gt; 
     &lt;li&gt;Utilizing appropriate batch sizes for large-scale graphs, ensuring enough memory is available while also maintaining computational efficiency.&lt;/li&gt; 
     &lt;li&gt;Choosing the correct dimensionality for random features in the kernel approximation, balancing model expressiveness and training stability.&lt;/li&gt; 
     &lt;li&gt;The use of dropout or other regularization techniques such as edge-level regularization can influence the proposed model's generalization capabilities on unseen data.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;On the bottleneck of graph neural networks and its practical implications&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;On the bottleneck of graph neural networks and its practical implications&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Semi-supervised classification with graph convolutional networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Categorical reparameterization with gumbel-softmax&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Learning discrete structures for graph neural networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Graph attention networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Geometric deep learning: going beyond euclidean data&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Graph structure learning for robust graph neural networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Geom-gcn: Geometric graph convolutional networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;New benchmarks for learning on non-homophilous graphs&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Latent patient network learning for automatic diagnosis&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Few-shot learning with graph neural networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;The graph neural network model&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Characteristic functions on graphs: Birds of a feather, from statistical descriptors to parametric models&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Beyond homophily in graph neural networks: Current limitations and effective designs&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_nodeformer/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_nodeformer/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_nodeformer/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_nodeformer/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Example 7 (Category: Graph Neural Networks)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed approach works on the task of uncovering data dependencies and learning instance representations from datasets that may not have complete or reliable relationships, particularly in semi-supervised contexts like node classification, image/text classification, and spatial-temporal dynamics prediction.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;The core techniques/algorithms used in this paper include an energy-constrained diffusion model represented as a partial differential equation (PDE), an explicit Euler scheme for numerical solutions, and a form of adaptive diffusivity function based on the energy function. The proposed architecture utilizes a diffusion-based Transformer framework that allows for all-pair feature propagation among instances.&lt;/li&gt; 
   &lt;li&gt;The major technical components serve the following purposes: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Diffusion Process:&lt;/strong&gt; Encodes instances into evolving states by modeling information flow, where instance representations evolve according to a PDE illuminating the relationships among the instances.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Energy Function:&lt;/strong&gt; Provides constraints to regularize the diffusion process and guide the proposed model towards desired low-energy embeddings, enhancing the quality of representations.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Diffusivity Function:&lt;/strong&gt; Specifies the strength of information flow between instances, adapting based on the instance states, and allows for flexible and efficient propagation strategies.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Implementation details for each component: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Diffusion Process Input:&lt;/strong&gt; Requires a batch of instances represented as a matrix of size \(N \times D\), where \(N\) is the number of instances and \(D\) is the input feature dimension.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Diffusion Process Output:&lt;/strong&gt; Produces the updated instance representations after \(K\) propagation steps. The step size \(\tau\) should be set within the range (0, 1).&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Energy Function:&lt;/strong&gt; Implemented as \(E(Z, k; \delta) = ||Z - Z^{(k)}||^2_F + \lambda \sum_{i,j} \delta(||z_i - z_j||^2_2)\), with \(\delta\) being a non-decreasing, concave function.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Key Parameters:&lt;/strong&gt; 
      &lt;ul&gt; 
       &lt;li&gt;Step size \(\tau\)&lt;/li&gt; 
       &lt;li&gt;Layer number \(K\) (number of diffusion propagation steps)&lt;/li&gt; 
       &lt;li&gt;Regularization weight \(\lambda\).&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Step-by-step description of interactions: 
    &lt;ul&gt; 
     &lt;li&gt;Start by initializing the instance representations.&lt;/li&gt; 
     &lt;li&gt;For each layer of diffusion, compute the diffusivity \(S(k)\) based on current embeddings through a function \(f\) which can be defined differently depending on the proposed model implementation.&lt;/li&gt; 
     &lt;li&gt;Update the instance representations using the defined diffusion equations, ensuring to conserve states and introduce propagation according to the computed diffusivity.&lt;/li&gt; 
     &lt;li&gt;After \(K\) layers of diffusion, apply a final output layer to produce logits for predictions.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Critical implementation details that affect performance: 
    &lt;ul&gt; 
     &lt;li&gt;The choice of diffusivity function \(f\) greatly impacts the proposed model's capacity to learn complex dependencies, where specific formulations (like linear or logistic) yield different abilities in capturing inter-instance relationships.&lt;/li&gt; 
     &lt;li&gt;Ensure that the values of \(\tau\) and \(\lambda\) are set appropriately to balance convergence speed and representation quality; using a smaller \(\tau\) may require deeper layers to learn effectively.&lt;/li&gt; 
     &lt;li&gt;Optimization parameters like learning rate and early stopping criteria are essential, particularly for large-scale datasets where convergence behavior can vary widely depending on architecture size and complexity.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Diffusion-convolutional neural networks&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;Diffusion-convolutional neural networks&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Semi-supervised classification with graph convolutional networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Manifold regularization: A geometric framework for learning from labeled and unlabeled examples&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Geometric deep learning: going beyond euclidean data&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Artificial neural networks for solving ordinary and partial differential equations&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Scaling graph neural networks with approximate pagerank&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Learning discrete structures for graph neural networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Semi-supervised learning using gaussian fields and harmonic functions&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Graph convolutional networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Deep learning via semi-supervised embedding&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;A generalization of transformer networks to graphs&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Graph Convolution and Quadratic Time Complexity&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Bayesian graph convolutional neural networks for semi-supervised classification&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Do transformers really perform bad for graph representation?&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Big bird: Transformers for longer sequences&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Adaptive graph diffusion networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Transformers are RNNs&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Collective classification in network data&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;NodeFormer: A scalable graph structure learning transformer for node classification&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_difformer/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_difformer/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_difformer/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_difformer/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;span id="how-it-works"&gt;&lt;/span&gt; 
&lt;h2&gt;✨How AI-Researcher works&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;🔄 &lt;strong&gt;End-to-End Scientific Research Automation System&lt;/strong&gt; &lt;br /&gt;Our &lt;strong&gt;AI-Researcher&lt;/strong&gt; provides comprehensive automation for the complete scientific research lifecycle through an integrated pipeline. The system orchestrates research activities across three strategic phases: 
  &lt;ol&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Literature Review &amp;amp; Idea Generation&lt;/strong&gt; 📚💡&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt; &lt;p&gt;🔍 &lt;strong&gt;Resource Collector&lt;/strong&gt;: Systematically gathers comprehensive research materials across multiple scientific domains through automated collection from major academic databases (e.g., arXiv, IEEE Xplore, ACM Digital Library, and Google Scholar), code platforms (e.g., GitHub, Hugging Face), and open datasets across scientific domains.&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;🧠 &lt;strong&gt;Resource Filter&lt;/strong&gt;: Evaluates and selects high-impact papers, well-maintained code implementations, and benchmark datasets through quality metrics (e.g., citation count, code maintenance, data completeness) and relevance assessment.&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;💭 &lt;strong&gt;Idea Generator&lt;/strong&gt;: Leveraging the identified research resources, including high-impact papers and code repositories, the Idea Generator systematically formulates novel research directions through comprehensive analysis. It automatedly evaluates current methodological limitations, map emerging technological trends, and explore uncharted research territories.&lt;/p&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;New Algorithm Design, Implementation &amp;amp; Validation&lt;/strong&gt; 🧪💻 &lt;br /&gt;&lt;strong&gt;Design → Implementation → Validation → Refinement&lt;/strong&gt;&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt; &lt;p&gt;📝&lt;strong&gt;Design Phase&lt;/strong&gt;: The initial phase focuses on conceptual development, where novel algorithmic ideas are formulated and theoretical foundations are established. During this stage, we carefully plan the implementation strategy, ensuring the proposed solution advances beyond existing approaches while maintaining practical feasibility.&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;⚙️&lt;strong&gt;Implementation Phase&lt;/strong&gt;: proceed to transform abstract concepts into concrete code implementations. This phase involves developing functional modules, establishing a robust testing environment, and creating necessary infrastructure for experimental validation.&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;🔬&lt;strong&gt;Validation Phase&lt;/strong&gt;: Systematic experimentation forms the core of our validation process. We execute comprehensive tests to evaluate algorithm performance, collect metrics, and document all findings. This phase ensures rigorous implementation verification with practical requirements.&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;🔧&lt;strong&gt;Refinement Phase&lt;/strong&gt; 🔬: Based on validation results, we enter an iterative refinement cycle. This phase involves identifying areas for improvement, optimizing code efficiency, and implementing necessary enhancements. We carefully analyze performance bottlenecks and plan strategic improvements for the next development iteration.&lt;/p&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Paper Writing&lt;/strong&gt; ✍️📝&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Writer Agent&lt;/strong&gt; 📄: Automatically generates full-length academic papers by integrating research ideas, motivations, newly designed algorithm frameworks, and algorithm validation performance. Leveraging a hierarchical writing approach, it creates polished manuscripts with precision and clarity.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;🚀 This fully automated system removes the need for manual intervention across the entire research lifecycle, enabling effortless and seamless scientific discovery—from initial concept to final publication. 🚀 It serves as an excellent research assistant, aiding researchers in achieving their goals efficiently and effectively.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;🔬 &lt;strong&gt;Comprehensive Benchmark Suite&lt;/strong&gt; &lt;br /&gt;We have developed a comprehensive and standardized evaluation framework to objectively assess the academic capabilities of AI researchers and the quality of their scholarly work, integrating several key innovations to ensure thorough and reliable evaluation.&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt; &lt;p&gt;👨‍🔬 &lt;strong&gt;Expert-Level Ground Truth&lt;/strong&gt;: TThe benchmark leverages human expert-written papers as ground truth references, establishing a high-quality standard for comparison and validation.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;🌈 &lt;strong&gt;Multi-Domain Coverage&lt;/strong&gt;: Our benchmark is designed to comprehensively span 4 major research domains, ensuring broad applicability: Computer Vision (CV), Nature Language Processing (NLP), Data Mining (DM), and Information Retrieval (IR).&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;🌐 &lt;strong&gt;Fully Open-Source Benchmark Construction&lt;/strong&gt;: We have fully open-sourced the methodology and process for building the benchmark, including complete access to processed datasets, data collection pipelines, and processing code. This ensures &lt;strong&gt;Transparency in Evaluation&lt;/strong&gt; while empowering the community to customize and construct benchmarks tailored to their specific domains for testing AI researchers.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;📊 &lt;strong&gt;Comprehensive Evaluation Metrics&lt;/strong&gt;: Our evaluation framework adopts a hierarchical and systematic approach, where tasks are organized into two levels based on the extent of idea provision. Leveraging specialized &lt;strong&gt;Evaluator Agents&lt;/strong&gt;, the framework conducts thorough assessments across multiple dimensions, ensuring a robust and comprehensive evaluation. Key evaluation metrics include: 1) &lt;strong&gt;Novelty&lt;/strong&gt;: Assessing the innovation and uniqueness of the research work. 2) &lt;strong&gt;Experimental Comprehensiveness&lt;/strong&gt;: Evaluating the design, execution, and rigor of the experiments. 3) &lt;strong&gt;Theoretical Foundation&lt;/strong&gt;: Measuring the strength of the theoretical background and foundations. 4) &lt;strong&gt;Result Analysis&lt;/strong&gt;: Analyzing the depth and accuracy of result interpretation. 5) &lt;strong&gt;Writing Quality&lt;/strong&gt;: Reviewing the clarity, coherence, and structure of the written report.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;🚀 &lt;strong&gt;Advancing Research Automation&lt;/strong&gt;. This benchmark suite provides an objective framework for assessing research automation capabilities. It is designed to evolve continuously, incorporating new advancements and expanding its scope to meet the growing demands of the research community.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;🌟 &lt;strong&gt;Easy-to-Use AI Research Assistant&lt;/strong&gt; &lt;br /&gt;&lt;strong&gt;AI-Researcher&lt;/strong&gt;E delivers a truly seamless and accessible experience for research automation, empowering users to focus on innovation without technical barriers. Key features include:&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt; &lt;p&gt;🌐 &lt;strong&gt;Multi-LLM Provider Support&lt;/strong&gt;: Effortlessly integrates with leading language model providers such as Claude, OpenAI, Deepseek, and more. Researchers can select the most suitable AI capabilities for their specific needs.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;📚 &lt;strong&gt;Effortless Research Kickoff&lt;/strong&gt;: Kickstart your research journey with unparalleled ease! Simply provide a list of relevant papers, and &lt;strong&gt;AI-Researcher&lt;/strong&gt; takes care of the rest—no need to upload files, contribute initial ideas, or navigate complex configurations. It's the ultimate tool to help you jumpstart your research process efficiently and effectively.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;🧠 &lt;strong&gt;Minimal Domain Expertise Needed&lt;/strong&gt;: AI-Researcher simplifies the research process by autonomously identifying critical research gaps, proposing innovative approaches, and executing the entire research pipeline. While some domain understanding can enhance results, the tool is designed to empower users of all expertise levels to achieve impactful outcomes with ease.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;📦 &lt;strong&gt;Out-of-the-Box Functionality&lt;/strong&gt;: Experience seamless research automation right from the start. AI-Researcher is ready to use with minimal setup, giving you instant access to advanced capabilities. Skip the hassle of complex configurations and dive straight into accelerating your research process with ease and efficiency.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="how-to-use"&gt;&lt;/span&gt; 
&lt;h2&gt;🔍 How to use AI-Researcher&lt;/h2&gt; 
&lt;h3&gt;1. Research Agent&lt;/h3&gt; 
&lt;p&gt;If you want to use research agent with the given idea (Level 1 tasks), conducting extensive survey and experiments, you can use the following command in the &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/research_agent/run_infer_level_1.sh"&gt;&lt;code&gt;research_agent/run_infer_level_1.sh&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;current_dir=$(dirname "$(readlink -f "$0")")
cd $current_dir
export DOCKER_WORKPLACE_NAME=workplace_paper

export BASE_IMAGES=tjbtech1/paperagent:latest

export COMPLETION_MODEL=claude-3-5-sonnet-20241022
export CHEEP_MODEL=claude-3-5-haiku-20241022

category=vq
instance_id=one_layer_vq
export GPUS='"device=0,1"'

python run_infer_plan.py --instance_path ../benchmark/final/${category}/${instance_id}.json --container_name paper_eval --task_level task1 --model $COMPLETION_MODEL --workplace_name workplace --cache_path cache --port 12372 --max_iter_times 0 --category ${category}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to just give the reference papers, and let the research agent to generate the idea then conduct the experiments (Level 2 tasks), you can use the following command in the &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/research_agent/run_infer_level_2.sh"&gt;&lt;code&gt;research_agent/run_infer_level_2.sh&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;current_dir=$(dirname "$(readlink -f "$0")")
cd $current_dir
export DOCKER_WORKPLACE_NAME=workplace_paper

export BASE_IMAGES=tjbtech1/paperagent:latest

export COMPLETION_MODEL=claude-3-5-sonnet-20241022
export CHEEP_MODEL=claude-3-5-haiku-20241022

category=vq
instance_id=one_layer_vq
export GPUS='"device=0,1"'

python run_infer_idea.py --instance_path ../benchmark/final/${category}/${instance_id}.json --container_name paper_eval --model $COMPLETION_MODEL --workplace_name workplace --cache_path cache --port 12372 --max_iter_times 0 --category ${category}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Paper Writing Agent&lt;/h3&gt; 
&lt;p&gt;If you want to generate the paper after the research agent has conducted the research, you can use the following command in the &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/paper_agent/run_paper.sh"&gt;&lt;code&gt;paper_agent/run_infer.sh&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;#!/bin/bash

cd path/to/AI-Researcher/paper_agent

export OPENAI_API_KEY=sk-SKlupNntta4WPmvDCRo7uuPbYGwOnUQcb25Twn8c718tPpXN


research_field=vq
instance_id=rotated_vq

python path/to/AI-Researcher/paper_agent/writing.py --research_field ${research_field} --instance_id ${instance_id}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Benchmark Data and Collection&lt;/h3&gt; 
&lt;p&gt;Our benchmark is also fully-open-sourced:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Detailed benchmark data is available in the &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/benchmark"&gt;&lt;code&gt;benchmark&lt;/code&gt;&lt;/a&gt; folder.&lt;/li&gt; 
 &lt;li&gt;Detailed benchmark collection process is available in the &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/benchmark_collection"&gt;&lt;code&gt;benchmark_collection&lt;/code&gt;&lt;/a&gt; folder.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="documentation"&gt;&lt;/span&gt; 
&lt;h2&gt;📖 Documentation&lt;/h2&gt; 
&lt;p&gt;Comprehensive documentation is on its way 🚀! Stay tuned for updates on our &lt;a href="https://auto-researcher.github.io/docs"&gt;Documentation&lt;/a&gt; page.&lt;/p&gt; 
&lt;span id="community"&gt;&lt;/span&gt; 
&lt;h2&gt;🤝 Join the Community&lt;/h2&gt; 
&lt;p&gt;We aim to build a vibrant community around AI-Researcher and warmly invite everyone to join us. Here's how you can become part of our community:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://join.slack.com/t/ai-researchergroup/shared_invite/zt-30y5a070k-C0ajQt1zmVczFnfGkIicvA"&gt;Join our Slack workspace&lt;/a&gt; - Here we talk about research, architecture, and future development.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/ghSnKGkq"&gt;Join our Discord server&lt;/a&gt; - This is a community-run server for general discussion, questions, and feedback.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/HKUDS/AI-Researcher/issues"&gt;Read or post Github Issues&lt;/a&gt; - Check out the issues we're working on, or add your own ideas.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Misc&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/HKUDS/AI-Researcher/stargazers"&gt;&lt;img src="https://reporoster.com/stars/HKUDS/AI-Researcher" alt="Stargazers repo roster for @HKUDS/AI-Researcher" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/HKUDS/AI-Researcher/network/members"&gt;&lt;img src="https://reporoster.com/forks/HKUDS/AI-Researcher" alt="Forkers repo roster for @HKUDS/AI-Researcher" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://star-history.com/#HKUDS/AI-Researcher&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=HKUDS/AI-Researcher&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;span id="cite"&gt;&lt;/span&gt; 
&lt;h2&gt;🌟 Cite&lt;/h2&gt; 
&lt;p&gt;A more detailed technical report will be released soon. 🚀:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-tex"&gt;@misc{airesearcher,
      title={{AI-Researcher: Autonomous Scientific Innovation}},
      author={Jiabin Tang, Lianghao Xia, Zhonghang Li, Chao Huang},
      year={2025},
      eprint={2505.18705},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2505.18705},
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>bytedance/UI-TARS</title>
      <link>https://github.com/bytedance/UI-TARS</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/bytedance/UI-TARS/main/figures/writer.png" alt="Local Image" /&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; 🌐 &lt;a href="https://seed-tars.com/"&gt;Website&lt;/a&gt;&amp;nbsp;&amp;nbsp; | 🤗 &lt;a href="https://huggingface.co/ByteDance-Seed/UI-TARS-1.5-7B"&gt;Hugging Face Models&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; 🔧 &lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS/main/README_deploy.md"&gt;Deployment&lt;/a&gt; &amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; 📑 &lt;a href="https://arxiv.org/abs/2501.12326"&gt;Paper&lt;/a&gt; &amp;nbsp;&amp;nbsp; |&amp;nbsp;&amp;nbsp; 🖥️ &lt;a href="https://github.com/bytedance/UI-TARS-desktop"&gt;UI-TARS-desktop&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;br /&gt;🏄 &lt;a href="https://github.com/web-infra-dev/Midscene"&gt;Midscene (Browser Automation) &lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;🫨 &lt;a href="https://discord.gg/pTXwYVjfcs"&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/13561"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13561" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;We also offer a &lt;strong&gt;UI-TARS-desktop&lt;/strong&gt; version, which can operate on your &lt;strong&gt;local personal device&lt;/strong&gt;. To use it, please visit &lt;a href="https://github.com/bytedance/UI-TARS-desktop"&gt;https://github.com/bytedance/UI-TARS-desktop&lt;/a&gt;. To use UI-TARS in web automation, you may refer to the open-source project &lt;a href="https://github.com/web-infra-dev/Midscene"&gt;Midscene.js&lt;/a&gt;. &lt;strong&gt;❗Notes&lt;/strong&gt;: Since Qwen 2.5vl based models ultilizes absolute coordinates to ground objects, please kindly refer to our illustration about how to process coordinates in this &lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS/main/README_coordinates.md"&gt;guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;🌟 2025.04.16: We shared the latest progress of the UI-TARS-1.5 model in our &lt;a href="https://seed-tars.com/1.5"&gt;blog&lt;/a&gt;, which excels in playing games and performing GUI tasks, and we open-sourced the &lt;a href="https://huggingface.co/ByteDance-Seed/UI-TARS-1.5-7B"&gt;UI-TARS-1.5-7B&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;✨ 2025.03.23: We updated the OSWorld inference scripts from the original official &lt;a href="https://github.com/xlang-ai/OSWorld/raw/main/run_uitars.py"&gt;OSWorld repository&lt;/a&gt;. Now, you can use the OSWorld official inference scripts to reproduce our results.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;UI-TARS-1.5, an open-source multimodal agent built upon a powerful vision-language model. It is capable of effectively performing diverse tasks within virtual worlds.&lt;/p&gt; 
&lt;p&gt;Leveraging the foundational architecture introduced in &lt;a href="https://arxiv.org/abs/2501.12326"&gt;our recent paper&lt;/a&gt;, UI-TARS-1.5 integrates advanced reasoning enabled by reinforcement learning. This allows the model to reason through its thoughts before taking action, significantly enhancing its performance and adaptability, particularly in inference-time scaling. Our new 1.5 version achieves state-of-the-art results across a variety of standard benchmarks, demonstrating strong reasoning capabilities and notable improvements over prior models.&lt;/p&gt; 
&lt;!-- ![Local Image](figures/UI-TARS.png) --&gt; 
&lt;p align="center"&gt; 
 &lt;video controls width="480"&gt; 
  &lt;source src="https://huggingface.co/datasets/JjjFangg/Demo_video/resolve/main/GUI_demo.mp4" type="video/mp4" /&gt; 
 &lt;/video&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align="center"&gt; 
 &lt;video controls width="480"&gt; 
  &lt;source src="https://huggingface.co/datasets/JjjFangg/Demo_video/resolve/main/Game_demo.mp4" type="video/mp4" /&gt; 
 &lt;/video&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2&gt;🚀 Quick Start Guide: Deploying and Using Our Model&lt;/h2&gt; 
&lt;p&gt;To help you get started quickly with our model, we recommend following the steps below in order. These steps will guide you through deployment, prediction post-processing to make the model take actions in your environment.&lt;/p&gt; 
&lt;h3&gt;✅ Step 1: Deployment &amp;amp; Inference&lt;/h3&gt; 
&lt;p&gt;👉 &lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS/main/README_deploy.md"&gt;Deployment and Inference&lt;/a&gt;. This includes instructions for model deployment using huggingface endpoint, and running your first prediction.&lt;/p&gt; 
&lt;h3&gt;✅ Step 2: Post Processing&lt;/h3&gt; 
&lt;h4&gt;Installation&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install ui-tars
# or
uv pip install ui-tars
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Usage&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from ui_tars.action_parser import parse_action_to_structure_output, parsing_response_to_pyautogui_code

response = "Thought: Click the button\nAction: click(start_box='(100,200)')"
original_image_width, original_image_height = 1920, 1080
parsed_dict = parse_action_to_structure_output(
    response,
    factor=1000,
    origin_resized_height=original_image_height,
    origin_resized_width=original_image_width,
    model_type="qwen25vl"
)
print(parsed_dict)
parsed_pyautogui_code = parsing_response_to_pyautogui_code(
    responses=parsed_dict,
    image_height=original_image_height,
    image_width=original_image_width
)
print(parsed_pyautogui_code)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;FYI: Coordinates visualization&lt;/h5&gt; 
&lt;p&gt;To help you better understand the coordinate processing, we also provide a &lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS/main/README_coordinates.md"&gt;guide&lt;/a&gt; for coordinates processing visualization.&lt;/p&gt; 
&lt;h2&gt;Prompt Usage Guide&lt;/h2&gt; 
&lt;p&gt;To accommodate different device environments and task complexities, the following three prompt templates in &lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS/main/codes/ui_tars/prompt.py"&gt;codes/ui_tars/prompt.py&lt;/a&gt;. are designed to guide GUI agents in generating appropriate actions. Choose the template that best fits your use case:&lt;/p&gt; 
&lt;h3&gt;🖥️ &lt;code&gt;COMPUTER_USE&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Recommended for&lt;/strong&gt;: GUI tasks on &lt;strong&gt;desktop environments&lt;/strong&gt; such as Windows, Linux, or macOS.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports common desktop operations: mouse clicks (single, double, right), drag actions, keyboard shortcuts, text input, scrolling, etc.&lt;/li&gt; 
 &lt;li&gt;Ideal for browser navigation, office software interaction, file management, and other desktop-based tasks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;📱 &lt;code&gt;MOBILE_USE&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Recommended for&lt;/strong&gt;: GUI tasks on &lt;strong&gt;mobile devices or Android emulators&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Includes mobile-specific actions: &lt;code&gt;long_press&lt;/code&gt;, &lt;code&gt;open_app&lt;/code&gt;, &lt;code&gt;press_home&lt;/code&gt;, &lt;code&gt;press_back&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Suitable for launching apps, scrolling views, filling input fields, and navigating within mobile apps.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;📌 &lt;code&gt;GROUNDING&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Recommended for&lt;/strong&gt;: Lightweight tasks focused solely on &lt;strong&gt;action output&lt;/strong&gt;, or for use in model training and evaluation.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Only outputs the &lt;code&gt;Action&lt;/code&gt; without any reasoning (&lt;code&gt;Thought&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;Useful for evaluating grounding capability.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;When developing or evaluating multimodal interaction systems, choose the appropriate prompt template based on your target platform (desktop vs. mobile)&lt;/p&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Online Benchmark Evaluation&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Benchmark type&lt;/th&gt; 
   &lt;th&gt;Benchmark&lt;/th&gt; 
   &lt;th&gt;UI-TARS-1.5&lt;/th&gt; 
   &lt;th&gt;OpenAI CUA&lt;/th&gt; 
   &lt;th&gt;Claude 3.7&lt;/th&gt; 
   &lt;th&gt;Previous SOTA&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Computer Use&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2404.07972"&gt;OSworld&lt;/a&gt; (100 steps)&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;42.5&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;36.4&lt;/td&gt; 
   &lt;td&gt;28&lt;/td&gt; 
   &lt;td&gt;38.1 (200 step)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2409.08264"&gt;Windows Agent Arena&lt;/a&gt; (50 steps)&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;42.1&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;29.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Browser Use&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2401.13919"&gt;WebVoyager&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;84.8&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;87&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;84.1&lt;/td&gt; 
   &lt;td&gt;87&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2504.01382"&gt;Online-Mind2web&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;75.8&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;71&lt;/td&gt; 
   &lt;td&gt;62.9&lt;/td&gt; 
   &lt;td&gt;71&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Phone Use&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2405.14573"&gt;Android World&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;64.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;59.5&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Grounding Capability Evaluation&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Benchmark&lt;/th&gt; 
   &lt;th&gt;UI-TARS-1.5&lt;/th&gt; 
   &lt;th&gt;OpenAI CUA&lt;/th&gt; 
   &lt;th&gt;Claude 3.7&lt;/th&gt; 
   &lt;th&gt;Previous SOTA&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2410.23218"&gt;ScreenSpot-V2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;94.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;87.9&lt;/td&gt; 
   &lt;td&gt;87.6&lt;/td&gt; 
   &lt;td&gt;91.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2504.07981v1"&gt;ScreenSpotPro&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;61.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;23.4&lt;/td&gt; 
   &lt;td&gt;27.7&lt;/td&gt; 
   &lt;td&gt;43.6&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Poki Game&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/2048"&gt;2048&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/cubinko"&gt;cubinko&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/energy"&gt;energy&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/free-the-key"&gt;free-the-key&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/gem-11"&gt;Gem-11&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/hex-frvr"&gt;hex-frvr&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/infinity-loop"&gt;Infinity-Loop&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/maze-path-of-light"&gt;Maze:Path-of-Light&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/shapes"&gt;shapes&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/snake-solver"&gt;snake-solver&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/wood-blocks-3d"&gt;wood-blocks-3d&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/yarn-untangle"&gt;yarn-untangle&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/laser-maze-puzzle"&gt;laser-maze-puzzle&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/tiles-master"&gt;tiles-master&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI CUA&lt;/td&gt; 
   &lt;td&gt;31.04&lt;/td&gt; 
   &lt;td&gt;0.00&lt;/td&gt; 
   &lt;td&gt;32.80&lt;/td&gt; 
   &lt;td&gt;0.00&lt;/td&gt; 
   &lt;td&gt;46.27&lt;/td&gt; 
   &lt;td&gt;92.25&lt;/td&gt; 
   &lt;td&gt;23.08&lt;/td&gt; 
   &lt;td&gt;35.00&lt;/td&gt; 
   &lt;td&gt;52.18&lt;/td&gt; 
   &lt;td&gt;42.86&lt;/td&gt; 
   &lt;td&gt;2.02&lt;/td&gt; 
   &lt;td&gt;44.56&lt;/td&gt; 
   &lt;td&gt;80.00&lt;/td&gt; 
   &lt;td&gt;78.27&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Claude 3.7&lt;/td&gt; 
   &lt;td&gt;43.05&lt;/td&gt; 
   &lt;td&gt;0.00&lt;/td&gt; 
   &lt;td&gt;41.60&lt;/td&gt; 
   &lt;td&gt;0.00&lt;/td&gt; 
   &lt;td&gt;0.00&lt;/td&gt; 
   &lt;td&gt;30.76&lt;/td&gt; 
   &lt;td&gt;2.31&lt;/td&gt; 
   &lt;td&gt;82.00&lt;/td&gt; 
   &lt;td&gt;6.26&lt;/td&gt; 
   &lt;td&gt;42.86&lt;/td&gt; 
   &lt;td&gt;0.00&lt;/td&gt; 
   &lt;td&gt;13.77&lt;/td&gt; 
   &lt;td&gt;28.00&lt;/td&gt; 
   &lt;td&gt;52.18&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;UI-TARS-1.5&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;0.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Minecraft&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Task Type&lt;/th&gt; 
   &lt;th&gt;Task Name&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://openai.com/index/vpt/"&gt;VPT&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://www.nature.com/articles/s41586-025-08744-2"&gt;DreamerV3&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;Previous SOTA&lt;/th&gt; 
   &lt;th&gt;UI-TARS-1.5 w/o Thought&lt;/th&gt; 
   &lt;th&gt;UI-TARS-1.5 w/ Thought&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mine Blocks&lt;/td&gt; 
   &lt;td&gt;(oak_log)&lt;/td&gt; 
   &lt;td&gt;0.8&lt;/td&gt; 
   &lt;td&gt;1.0&lt;/td&gt; 
   &lt;td&gt;1.0&lt;/td&gt; 
   &lt;td&gt;1.0&lt;/td&gt; 
   &lt;td&gt;1.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;(obsidian)&lt;/td&gt; 
   &lt;td&gt;0.0&lt;/td&gt; 
   &lt;td&gt;0.0&lt;/td&gt; 
   &lt;td&gt;0.0&lt;/td&gt; 
   &lt;td&gt;0.2&lt;/td&gt; 
   &lt;td&gt;0.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;(white_bed)&lt;/td&gt; 
   &lt;td&gt;0.0&lt;/td&gt; 
   &lt;td&gt;0.0&lt;/td&gt; 
   &lt;td&gt;0.1&lt;/td&gt; 
   &lt;td&gt;0.4&lt;/td&gt; 
   &lt;td&gt;0.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;200 Tasks Avg.&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;0.06&lt;/td&gt; 
   &lt;td&gt;0.03&lt;/td&gt; 
   &lt;td&gt;0.32&lt;/td&gt; 
   &lt;td&gt;0.35&lt;/td&gt; 
   &lt;td&gt;0.42&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Kill Mobs&lt;/td&gt; 
   &lt;td&gt;(mooshroom)&lt;/td&gt; 
   &lt;td&gt;0.0&lt;/td&gt; 
   &lt;td&gt;0.0&lt;/td&gt; 
   &lt;td&gt;0.1&lt;/td&gt; 
   &lt;td&gt;0.3&lt;/td&gt; 
   &lt;td&gt;0.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;(zombie)&lt;/td&gt; 
   &lt;td&gt;0.4&lt;/td&gt; 
   &lt;td&gt;0.1&lt;/td&gt; 
   &lt;td&gt;0.6&lt;/td&gt; 
   &lt;td&gt;0.7&lt;/td&gt; 
   &lt;td&gt;0.9&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;(chicken)&lt;/td&gt; 
   &lt;td&gt;0.1&lt;/td&gt; 
   &lt;td&gt;0.0&lt;/td&gt; 
   &lt;td&gt;0.4&lt;/td&gt; 
   &lt;td&gt;0.5&lt;/td&gt; 
   &lt;td&gt;0.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;100 Tasks Avg.&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;0.04&lt;/td&gt; 
   &lt;td&gt;0.03&lt;/td&gt; 
   &lt;td&gt;0.18&lt;/td&gt; 
   &lt;td&gt;0.25&lt;/td&gt; 
   &lt;td&gt;0.31&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Model Scale Comparison&lt;/h2&gt; 
&lt;p&gt;Here we compare performance across different model scales of UI-TARS on the OSworld benchmark.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Benchmark Type&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Benchmark&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;UI-TARS-72B-DPO&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;UI-TARS-1.5-7B&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;UI-TARS-1.5&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Computer Use&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2404.07972"&gt;OSWorld&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;24.6&lt;/td&gt; 
   &lt;td&gt;27.5&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;42.5&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GUI Grounding&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2504.07981v1"&gt;ScreenSpotPro&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;38.1&lt;/td&gt; 
   &lt;td&gt;49.6&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;61.6&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Limitations&lt;/h3&gt; 
&lt;p&gt;While UI-TARS-1.5 represents a significant advancement in multimodal agent capabilities, we acknowledge several important limitations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Misuse:&lt;/strong&gt; Given its enhanced performance in GUI tasks, including successfully navigating authentication challenges like CAPTCHA, UI-TARS-1.5 could potentially be misused for unauthorized access or automation of protected content. To mitigate this risk, extensive internal safety evaluations are underway.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Computation:&lt;/strong&gt; UI-TARS-1.5 still requires substantial computational resources, particularly for large-scale tasks or extended gameplay scenarios.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Hallucination&lt;/strong&gt;: UI-TARS-1.5 may occasionally generate inaccurate descriptions, misidentify GUI elements, or take suboptimal actions based on incorrect inferences—especially in ambiguous or unfamiliar environments.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model scale:&lt;/strong&gt; The released UI-TARS-1.5-7B focuses primarily on enhancing general computer use capabilities and is not specifically optimized for game-based scenarios, where the UI-TARS-1.5 still holds a significant advantage.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What's next&lt;/h2&gt; 
&lt;p&gt;We are providing early research access to our top-performing UI-TARS-1.5 model to facilitate collaborative research. Interested researchers can contact us at &lt;a href="mailto:TARS@bytedance.com"&gt;TARS@bytedance.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Looking ahead, we envision UI-TARS evolving into increasingly sophisticated agentic experiences capable of performing real-world actions, thereby empowering platforms such as &lt;a href="https://team.doubao.com/en/"&gt;doubao&lt;/a&gt; to accomplish more complex tasks for you :)&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#bytedance/UI-TARS&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=bytedance/UI-TARS&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find our paper and model useful in your research, feel free to give us a cite.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;@article{qin2025ui,
  title={UI-TARS: Pioneering Automated GUI Interaction with Native Agents},
  author={Qin, Yujia and Ye, Yining and Fang, Junjie and Wang, Haoming and Liang, Shihao and Tian, Shizuo and Zhang, Junda and Li, Jiahao and Li, Yunxin and Huang, Shijue and others},
  journal={arXiv preprint arXiv:2501.12326},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>microsoft/BitNet</title>
      <link>https://github.com/microsoft/BitNet</link>
      <description>&lt;p&gt;Official inference framework for 1-bit LLMs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;bitnet.cpp&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/version-1.0-blue" alt="version" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/header_model_release.png" alt="BitNet Model on Hugging Face" width="800" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Try it out via this &lt;a href="https://bitnet-demo.azurewebsites.net/"&gt;demo&lt;/a&gt;, or build and run it on your own &lt;a href="https://github.com/microsoft/BitNet?tab=readme-ov-file#build-from-source"&gt;CPU&lt;/a&gt; or &lt;a href="https://github.com/microsoft/BitNet/raw/main/gpu/README.md"&gt;GPU&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;bitnet.cpp is the official inference framework for 1-bit LLMs (e.g., BitNet b1.58). It offers a suite of optimized kernels, that support &lt;strong&gt;fast&lt;/strong&gt; and &lt;strong&gt;lossless&lt;/strong&gt; inference of 1.58-bit models on CPU and GPU (NPU support will coming next).&lt;/p&gt; 
&lt;p&gt;The first release of bitnet.cpp is to support inference on CPUs. bitnet.cpp achieves speedups of &lt;strong&gt;1.37x&lt;/strong&gt; to &lt;strong&gt;5.07x&lt;/strong&gt; on ARM CPUs, with larger models experiencing greater performance gains. Additionally, it reduces energy consumption by &lt;strong&gt;55.4%&lt;/strong&gt; to &lt;strong&gt;70.0%&lt;/strong&gt;, further boosting overall efficiency. On x86 CPUs, speedups range from &lt;strong&gt;2.37x&lt;/strong&gt; to &lt;strong&gt;6.17x&lt;/strong&gt; with energy reductions between &lt;strong&gt;71.9%&lt;/strong&gt; to &lt;strong&gt;82.2%&lt;/strong&gt;. Furthermore, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU, achieving speeds comparable to human reading (5-7 tokens per second), significantly enhancing the potential for running LLMs on local devices. Please refer to the &lt;a href="https://arxiv.org/abs/2410.16144"&gt;technical report&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/m2_performance.jpg" alt="m2_performance" width="800" /&gt; 
&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/intel_performance.jpg" alt="m2_performance" width="800" /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The tested models are dummy setups used in a research context to demonstrate the inference performance of bitnet.cpp.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;p&gt;A demo of bitnet.cpp running a BitNet b1.58 3B model on Apple M2:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1"&gt;https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What's New:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;05/20/2025 &lt;a href="https://github.com/microsoft/BitNet/raw/main/gpu/README.md"&gt;BitNet Official GPU inference kernel&lt;/a&gt; &lt;img src="https://img.shields.io/badge/NEW-red" alt="NEW" /&gt;&lt;/li&gt; 
 &lt;li&gt;04/14/2025 &lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;BitNet Official 2B Parameter Model on Hugging Face&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;02/18/2025 &lt;a href="https://arxiv.org/abs/2502.11880"&gt;Bitnet.cpp: Efficient Edge Inference for Ternary LLMs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;11/08/2024 &lt;a href="https://arxiv.org/abs/2411.04965"&gt;BitNet a4.8: 4-bit Activations for 1-bit LLMs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/21/2024 &lt;a href="https://arxiv.org/abs/2410.16144"&gt;1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/17/2024 bitnet.cpp 1.0 released.&lt;/li&gt; 
 &lt;li&gt;03/21/2024 &lt;a href="https://github.com/microsoft/unilm/raw/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf"&gt;The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;02/27/2024 &lt;a href="https://arxiv.org/abs/2402.17764"&gt;The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/17/2023 &lt;a href="https://arxiv.org/abs/2310.11453"&gt;BitNet: Scaling 1-bit Transformers for Large Language Models&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;This project is based on the &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt; framework. We would like to thank all the authors for their contributions to the open-source community. Also, bitnet.cpp's kernels are built on top of the Lookup Table methodologies pioneered in &lt;a href="https://github.com/microsoft/T-MAC/"&gt;T-MAC&lt;/a&gt;. For inference of general low-bit LLMs beyond ternary models, we recommend using T-MAC.&lt;/p&gt; 
&lt;h2&gt;Official Models&lt;/h2&gt; 
&lt;table&gt;  
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th rowspan="2"&gt;Model&lt;/th&gt; 
   &lt;th rowspan="2"&gt;Parameters&lt;/th&gt; 
   &lt;th rowspan="2"&gt;CPU&lt;/th&gt; 
   &lt;th colspan="3"&gt;Kernel&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;I2_S&lt;/th&gt; 
   &lt;th&gt;TL1&lt;/th&gt; 
   &lt;th&gt;TL2&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;BitNet-b1.58-2B-4T&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;2.4B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Supported Models&lt;/h2&gt; 
&lt;p&gt;❗️&lt;strong&gt;We use existing 1-bit LLMs available on &lt;a href="https://huggingface.co/"&gt;Hugging Face&lt;/a&gt; to demonstrate the inference capabilities of bitnet.cpp. We hope the release of bitnet.cpp will inspire the development of 1-bit LLMs in large-scale settings in terms of model size and training tokens.&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt;  
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th rowspan="2"&gt;Model&lt;/th&gt; 
   &lt;th rowspan="2"&gt;Parameters&lt;/th&gt; 
   &lt;th rowspan="2"&gt;CPU&lt;/th&gt; 
   &lt;th colspan="3"&gt;Kernel&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;I2_S&lt;/th&gt; 
   &lt;th&gt;TL1&lt;/th&gt; 
   &lt;th&gt;TL2&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/1bitLLM/bitnet_b1_58-large"&gt;bitnet_b1_58-large&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;0.7B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/1bitLLM/bitnet_b1_58-3B"&gt;bitnet_b1_58-3B&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;3.3B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens"&gt;Llama3-8B-1.58-100B-tokens&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;8.0B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026"&gt;Falcon3 Family&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;1B-10B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/collections/tiiuae/falcon-edge-series-6804fd13344d6d8a8fa71130"&gt;Falcon-E Family&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;1B-3B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;python&amp;gt;=3.9&lt;/li&gt; 
 &lt;li&gt;cmake&amp;gt;=3.22&lt;/li&gt; 
 &lt;li&gt;clang&amp;gt;=18 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;For Windows users, install &lt;a href="https://visualstudio.microsoft.com/downloads/"&gt;Visual Studio 2022&lt;/a&gt;. In the installer, toggle on at least the following options(this also automatically installs the required additional tools like CMake):&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Desktop-development with C++&lt;/li&gt; 
     &lt;li&gt;C++-CMake Tools for Windows&lt;/li&gt; 
     &lt;li&gt;Git for Windows&lt;/li&gt; 
     &lt;li&gt;C++-Clang Compiler for Windows&lt;/li&gt; 
     &lt;li&gt;MS-Build Support for LLVM-Toolset (clang)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;For Debian/Ubuntu users, you can download with &lt;a href="https://apt.llvm.org/"&gt;Automatic installation script&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash -c "$(wget -O - https://apt.llvm.org/llvm.sh)"&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;conda (highly recommend)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Build from source&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] If you are using Windows, please remember to always use a Developer Command Prompt / PowerShell for VS2022 for the following commands. Please refer to the FAQs below if you see any issues.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone the repo&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone --recursive https://github.com/microsoft/BitNet.git
cd BitNet
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install the dependencies&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# (Recommended) Create a new conda environment
conda create -n bitnet-cpp python=3.9
conda activate bitnet-cpp

pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Build the project&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Manually download the model and run with local path
huggingface-cli download microsoft/BitNet-b1.58-2B-4T-gguf --local-dir models/BitNet-b1.58-2B-4T
python setup_env.py -md models/BitNet-b1.58-2B-4T -q i2_s

&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;
usage: setup_env.py [-h] [--hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}] [--model-dir MODEL_DIR] [--log-dir LOG_DIR] [--quant-type {i2_s,tl1}] [--quant-embd]
                    [--use-pretuned]

Setup the environment for running inference

optional arguments:
  -h, --help            show this help message and exit
  --hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}, -hr {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}
                        Model used for inference
  --model-dir MODEL_DIR, -md MODEL_DIR
                        Directory to save/load the model
  --log-dir LOG_DIR, -ld LOG_DIR
                        Directory to save the logging info
  --quant-type {i2_s,tl1}, -q {i2_s,tl1}
                        Quantization type
  --quant-embd          Quantize the embeddings to f16
  --use-pretuned, -p    Use the pretuned kernel parameters
&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Basic usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run inference with the quantized model
python run_inference.py -m models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf -p "You are a helpful assistant" -cnv
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;
usage: run_inference.py [-h] [-m MODEL] [-n N_PREDICT] -p PROMPT [-t THREADS] [-c CTX_SIZE] [-temp TEMPERATURE] [-cnv]

Run inference

optional arguments:
  -h, --help            show this help message and exit
  -m MODEL, --model MODEL
                        Path to model file
  -n N_PREDICT, --n-predict N_PREDICT
                        Number of tokens to predict when generating text
  -p PROMPT, --prompt PROMPT
                        Prompt to generate text from
  -t THREADS, --threads THREADS
                        Number of threads to use
  -c CTX_SIZE, --ctx-size CTX_SIZE
                        Size of the prompt context
  -temp TEMPERATURE, --temperature TEMPERATURE
                        Temperature, a hyperparameter that controls the randomness of the generated text
  -cnv, --conversation  Whether to enable chat mode or not (for instruct models.)
                        (When this option is turned on, the prompt specified by -p will be used as the system prompt.)
&lt;/pre&gt; 
&lt;h3&gt;Benchmark&lt;/h3&gt; 
&lt;p&gt;We provide scripts to run the inference benchmark providing a model.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;usage: e2e_benchmark.py -m MODEL [-n N_TOKEN] [-p N_PROMPT] [-t THREADS]  
   
Setup the environment for running the inference  
   
required arguments:  
  -m MODEL, --model MODEL  
                        Path to the model file. 
   
optional arguments:  
  -h, --help  
                        Show this help message and exit. 
  -n N_TOKEN, --n-token N_TOKEN  
                        Number of generated tokens. 
  -p N_PROMPT, --n-prompt N_PROMPT  
                        Prompt to generate text from. 
  -t THREADS, --threads THREADS  
                        Number of threads to use. 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Here's a brief explanation of each argument:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-m&lt;/code&gt;, &lt;code&gt;--model&lt;/code&gt;: The path to the model file. This is a required argument that must be provided when running the script.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-n&lt;/code&gt;, &lt;code&gt;--n-token&lt;/code&gt;: The number of tokens to generate during the inference. It is an optional argument with a default value of 128.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-p&lt;/code&gt;, &lt;code&gt;--n-prompt&lt;/code&gt;: The number of prompt tokens to use for generating text. This is an optional argument with a default value of 512.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-t&lt;/code&gt;, &lt;code&gt;--threads&lt;/code&gt;: The number of threads to use for running the inference. It is an optional argument with a default value of 2.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-h&lt;/code&gt;, &lt;code&gt;--help&lt;/code&gt;: Show the help message and exit. Use this argument to display usage information.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python utils/e2e_benchmark.py -m /path/to/model -n 200 -p 256 -t 4  
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command would run the inference benchmark using the model located at &lt;code&gt;/path/to/model&lt;/code&gt;, generating 200 tokens from a 256 token prompt, utilizing 4 threads.&lt;/p&gt; 
&lt;p&gt;For the model layout that do not supported by any public model, we provide scripts to generate a dummy model with the given model layout, and run the benchmark on your machine:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python utils/generate-dummy-bitnet-model.py models/bitnet_b1_58-large --outfile models/dummy-bitnet-125m.tl1.gguf --outtype tl1 --model-size 125M

# Run benchmark with the generated model, use -m to specify the model path, -p to specify the prompt processed, -n to specify the number of token to generate
python utils/e2e_benchmark.py -m models/dummy-bitnet-125m.tl1.gguf -p 512 -n 128
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Convert from &lt;code&gt;.safetensors&lt;/code&gt; Checkpoints&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Prepare the .safetensors model file
huggingface-cli download microsoft/bitnet-b1.58-2B-4T-bf16 --local-dir ./models/bitnet-b1.58-2B-4T-bf16

# Convert to gguf model
python ./utils/convert-helper-bitnet.py ./models/bitnet-b1.58-2B-4T-bf16
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;FAQ (Frequently Asked Questions)📌&lt;/h3&gt; 
&lt;h4&gt;Q1: The build dies with errors building llama.cpp due to issues with std::chrono in log.cpp?&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; This is an issue introduced in recent version of llama.cpp. Please refer to this &lt;a href="https://github.com/tinglou/llama.cpp/commit/4e3db1e3d78cc1bcd22bcb3af54bd2a4628dd323"&gt;commit&lt;/a&gt; in the &lt;a href="https://github.com/abetlen/llama-cpp-python/issues/1942"&gt;discussion&lt;/a&gt; to fix this issue.&lt;/p&gt; 
&lt;h4&gt;Q2: How to build with clang in conda environment on windows?&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Before building the project, verify your clang installation and access to Visual Studio tools by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;clang -v
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command checks that you are using the correct version of clang and that the Visual Studio tools are available. If you see an error message such as:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;'clang' is not recognized as an internal or external command, operable program or batch file.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It indicates that your command line window is not properly initialized for Visual Studio tools.&lt;/p&gt; 
&lt;p&gt;• If you are using Command Prompt, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;"C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\VsDevCmd.bat" -startdir=none -arch=x64 -host_arch=x64
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;• If you are using Windows PowerShell, run the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Import-Module "C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\Microsoft.VisualStudio.DevShell.dll" Enter-VsDevShell 3f0e31ad -SkipAutomaticLocation -DevCmdArguments "-arch=x64 -host_arch=x64"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These steps will initialize your environment and allow you to use the correct Visual Studio tools.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Shubhamsaboo/awesome-llm-apps</title>
      <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
      <description>&lt;p&gt;Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="http://www.theunwindai.com"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/unwind_black.png" width="900px" alt="Unwind AI" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.linkedin.com/in/shubhamsaboo/"&gt; &lt;img src="https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;amp;style=flat-square" alt="LinkedIn" /&gt; &lt;/a&gt; &lt;a href="https://twitter.com/Saboo_Shubham_"&gt; &lt;img src="https://img.shields.io/twitter/follow/Shubham_Saboo" alt="Twitter" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=de"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=es"&gt;Español&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=fr"&gt;français&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ja"&gt;日本語&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ko"&gt;한국어&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=pt"&gt;Português&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ru"&gt;Русский&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=zh"&gt;中文&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;🌟 Awesome LLM Apps&lt;/h1&gt; 
&lt;p&gt;A curated collection of &lt;strong&gt;Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.&lt;/strong&gt; This repository features LLM apps that use models from OpenAI, Anthropic, Google, and open-source models like DeepSeek, Qwen or Llama that you can run locally on your computer.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/9876" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/9876" alt="Shubhamsaboo%2Fawesome-llm-apps | Trendshift" style="width: 250px; height: 55px;" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;🤔 Why Awesome LLM Apps?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;💡 Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.&lt;/li&gt; 
 &lt;li&gt;🔥 Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp;amp; RAG.&lt;/li&gt; 
 &lt;li&gt;🎓 Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;📂 Featured AI Projects&lt;/h2&gt; 
&lt;h3&gt;AI Agents&lt;/h3&gt; 
&lt;h3&gt;🌱 Starter AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_blog_to_podcast_agent/"&gt;🎙️ AI Blog to Podcast Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_breakup_recovery_agent/"&gt;❤️‍🩹 AI Breakup Recovery Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_data_analysis_agent/"&gt;📊 AI Data Analysis Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_medical_imaging_agent/"&gt;🩻 AI Medical Imaging Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_meme_generator_agent_browseruse/"&gt;😂 AI Meme Generator Agent (Browser)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_music_generator_agent/"&gt;🎵 AI Music Generator Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_travel_agent/"&gt;🛫 AI Travel Agent (Local &amp;amp; Cloud)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/gemini_multimodal_agent_demo/"&gt;✨ Gemini Multimodal Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/local_news_agent_openai_swarm/"&gt;🌐 Local News Agent (OpenAI Swarm)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/mixture_of_agents/"&gt;🔄 Mixture of Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/xai_finance_agent/"&gt;📊 xAI Finance Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/opeani_research_agent/"&gt;🔍 OpenAI Research Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/web_scrapping_ai_agent/"&gt;🕸️ Web Scrapping AI Agent (Local &amp;amp; Cloud)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🚀 Advanced AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_deep_research_agent/"&gt;🔍 AI Deep Research Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_consultant_agent"&gt;🤝 AI Consultant Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_system_architect_r1/"&gt;🏗️ AI System Architect Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_lead_generation_agent/"&gt;🎯 AI Lead Generation Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/"&gt;💰 AI Financial Coach Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_movie_production_agent/"&gt;🎬 AI Movie Production Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_investment_agent/"&gt;📈 AI Investment Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/"&gt;🏋️‍♂️ AI Health &amp;amp; Fitness Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/product_launch_intelligence_agent"&gt;🚀 AI Product Launch Intelligence Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_journalist_agent/"&gt;🗞️ AI Journalist Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/"&gt;🧠 AI Mental Wellbeing Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_meeting_agent/"&gt;📑 AI Meeting Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_Self-Evolving_agent/"&gt;🧬 AI Self-Evolving Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_news_and_podcast_agents/"&gt;🎧 AI Social Media News and Podcast Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🎮 Autonomous Game Playing Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/"&gt;🎮 AI 3D Pygame Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/"&gt;♜ AI Chess Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/"&gt;🎲 AI Tic-Tac-Toe Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🤝 Multi-agent Teams&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/"&gt;🧲 AI Competitor Intelligence Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/"&gt;💲 AI Finance Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/"&gt;🎨 AI Game Design Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/"&gt;👨‍⚖️ AI Legal Agent Team (Cloud &amp;amp; Local)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/"&gt;💼 AI Recruitment Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_real_estate_agent_team"&gt;🏠 AI Real Estate Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/"&gt;👨‍💼 AI Services Agency (CrewAI)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/"&gt;👨‍🏫 AI Teaching Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/"&gt;💻 Multimodal Coding Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/"&gt;✨ Multimodal Design Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_travel_planner_agent_team/"&gt;🌏 AI Travel Planner Agent Team&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🗣️ Voice AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/ai_audio_tour_agent/"&gt;🗣️ AI Audio Tour Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/customer_support_voice_agent/"&gt;📞 Customer Support Voice Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/voice_rag_openaisdk/"&gt;🔊 Voice RAG Agent (OpenAI SDK)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🌐 MCP AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/browser_mcp_agent/"&gt;♾️ Browser MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/github_mcp_agent/"&gt;🐙 GitHub MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/notion_mcp_agent"&gt;📑 Notion MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/ai_travel_planner_mcp_agent_team"&gt;🌍 AI Travel Planner MCP Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;📀 RAG (Retrieval Augmented Generation)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/agentic_rag/"&gt;🔗 Agentic RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/agentic_rag_with_reasoning/"&gt;🧐 Agentic RAG with Reasoning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/ai_blog_search/"&gt;📰 AI Blog Search (RAG)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/autonomous_rag/"&gt;🔍 Autonomous RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/corrective_rag/"&gt;🔄 Corrective RAG (CRAG)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/deepseek_local_rag_agent/"&gt;🐋 Deepseek Local RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/gemini_agentic_rag/"&gt;🤔 Gemini Agentic RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/hybrid_search_rag/"&gt;👀 Hybrid Search RAG (Cloud)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/llama3.1_local_rag/"&gt;🔄 Llama 3.1 Local RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/local_hybrid_search_rag/"&gt;🖥️ Local Hybrid Search RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/local_rag_agent/"&gt;🦙 Local RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag-as-a-service/"&gt;🧩 RAG-as-a-Service&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_agent_cohere/"&gt;✨ RAG Agent with Cohere&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_chain/"&gt;⛓️ Basic RAG Chain&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_database_routing/"&gt;📠 RAG with Database Routing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/vision_rag/"&gt;🖼️ Vision RAG&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;💾 LLM Apps with Memory Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory/"&gt;💾 AI ArXiv Agent with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/ai_travel_agent_memory/"&gt;🛩️ AI Travel Agent with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/llama3_stateful_chat/"&gt;💬 Llama3 Stateful Chat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/llm_app_personalized_memory/"&gt;📝 LLM App with Personalized Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/local_chatgpt_with_memory/"&gt;🗄️ Local ChatGPT Clone with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/multi_llm_memory/"&gt;🧠 Multi-LLM Application with Shared Memory&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;💬 Chat with X Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_github/"&gt;💬 Chat with GitHub (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_gmail/"&gt;📨 Chat with Gmail&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_pdf/"&gt;📄 Chat with PDF (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_research_papers/"&gt;📚 Chat with Research Papers (ArXiv) (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_substack/"&gt;📝 Chat with Substack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_youtube_videos/"&gt;📽️ Chat with YouTube Videos&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🔧 LLM Fine-tuning Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_finetuning_tutorials/llama3.2_finetuning/"&gt;🔧 Llama 3.2 Fine-tuning&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🧑‍🏫 AI Agent Framework Crash Course&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/ai_agent_framework_crash_course/google_adk_crash_course/"&gt;Google ADK Crash Course&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Starter agent; model‑agnostic (OpenAI, Claude)&lt;/li&gt; 
   &lt;li&gt;Structured outputs (Pydantic)&lt;/li&gt; 
   &lt;li&gt;Tools: built‑in, function, third‑party, MCP tools&lt;/li&gt; 
   &lt;li&gt;Memory; callbacks; Plugins&lt;/li&gt; 
   &lt;li&gt;Simple multi‑agent; Multi‑agent patterns&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🚀 Getting Started&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the repository&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Navigate to the desired project directory&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd awesome-llm-apps/starter_ai_agents/ai_travel_agent
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install the required dependencies&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Follow the project-specific instructions&lt;/strong&gt; in each project's &lt;code&gt;README.md&lt;/code&gt; file to set up and run the app.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;🤝 Contributing to Open Source&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! If you have any ideas, improvements, or new apps to add, please create a new &lt;a href="https://github.com/Shubhamsaboo/awesome-llm-apps/issues"&gt;GitHub Issue&lt;/a&gt; or submit a pull request. Make sure to follow the existing project structure and include a detailed &lt;code&gt;README.md&lt;/code&gt; for each new app.&lt;/p&gt; 
&lt;h3&gt;Thank You, Community, for the Support! 🙏&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;🌟 &lt;strong&gt;Don’t miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>huggingface/diffusers</title>
      <link>https://github.com/huggingface/diffusers</link>
      <description>&lt;p&gt;🤗 Diffusers: State-of-the-art diffusion models for image, video, and audio generation in PyTorch and FLAX.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;br /&gt; &lt;img src="https://raw.githubusercontent.com/huggingface/diffusers/main/docs/source/en/imgs/diffusers_library.jpg" width="400" /&gt; &lt;br /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align="center"&gt; &lt;a href="https://github.com/huggingface/diffusers/raw/main/LICENSE"&gt;&lt;img alt="GitHub" src="https://img.shields.io/github/license/huggingface/datasets.svg?color=blue" /&gt;&lt;/a&gt; &lt;a href="https://github.com/huggingface/diffusers/releases"&gt;&lt;img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/diffusers.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/diffusers"&gt;&lt;img alt="GitHub release" src="https://static.pepy.tech/badge/diffusers/month" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/huggingface/diffusers/main/CODE_OF_CONDUCT.md"&gt;&lt;img alt="Contributor Covenant" src="https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/diffuserslib"&gt;&lt;img alt="X account" src="https://img.shields.io/twitter/url/https/twitter.com/diffuserslib.svg?style=social&amp;amp;label=Follow%20%40diffuserslib" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;🤗 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or training your own diffusion models, 🤗 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on &lt;a href="https://huggingface.co/docs/diffusers/conceptual/philosophy#usability-over-performance"&gt;usability over performance&lt;/a&gt;, &lt;a href="https://huggingface.co/docs/diffusers/conceptual/philosophy#simple-over-easy"&gt;simple over easy&lt;/a&gt;, and &lt;a href="https://huggingface.co/docs/diffusers/conceptual/philosophy#tweakable-contributorfriendly-over-abstraction"&gt;customizability over abstractions&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;🤗 Diffusers offers three core components:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;State-of-the-art &lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/overview"&gt;diffusion pipelines&lt;/a&gt; that can be run in inference with just a few lines of code.&lt;/li&gt; 
 &lt;li&gt;Interchangeable noise &lt;a href="https://huggingface.co/docs/diffusers/api/schedulers/overview"&gt;schedulers&lt;/a&gt; for different diffusion speeds and output quality.&lt;/li&gt; 
 &lt;li&gt;Pretrained &lt;a href="https://huggingface.co/docs/diffusers/api/models/overview"&gt;models&lt;/a&gt; that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;We recommend installing 🤗 Diffusers in a virtual environment from PyPI or Conda. For more details about installing &lt;a href="https://pytorch.org/get-started/locally/"&gt;PyTorch&lt;/a&gt; and &lt;a href="https://flax.readthedocs.io/en/latest/#installation"&gt;Flax&lt;/a&gt;, please refer to their official documentation.&lt;/p&gt; 
&lt;h3&gt;PyTorch&lt;/h3&gt; 
&lt;p&gt;With &lt;code&gt;pip&lt;/code&gt; (official package):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade diffusers[torch]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With &lt;code&gt;conda&lt;/code&gt; (maintained by the community):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;conda install -c conda-forge diffusers
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Flax&lt;/h3&gt; 
&lt;p&gt;With &lt;code&gt;pip&lt;/code&gt; (official package):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade diffusers[flax]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Apple Silicon (M1/M2) support&lt;/h3&gt; 
&lt;p&gt;Please refer to the &lt;a href="https://huggingface.co/docs/diffusers/optimization/mps"&gt;How to use Stable Diffusion in Apple Silicon&lt;/a&gt; guide.&lt;/p&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;Generating outputs is super easy with 🤗 Diffusers. To generate an image from text, use the &lt;code&gt;from_pretrained&lt;/code&gt; method to load any pretrained diffusion model (browse the &lt;a href="https://huggingface.co/models?library=diffusers&amp;amp;sort=downloads"&gt;Hub&lt;/a&gt; for 30,000+ checkpoints):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from diffusers import DiffusionPipeline
import torch

pipeline = DiffusionPipeline.from_pretrained("stable-diffusion-v1-5/stable-diffusion-v1-5", torch_dtype=torch.float16)
pipeline.to("cuda")
pipeline("An image of a squirrel in Picasso style").images[0]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also dig into the models and schedulers toolbox to build your own diffusion system:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from diffusers import DDPMScheduler, UNet2DModel
from PIL import Image
import torch

scheduler = DDPMScheduler.from_pretrained("google/ddpm-cat-256")
model = UNet2DModel.from_pretrained("google/ddpm-cat-256").to("cuda")
scheduler.set_timesteps(50)

sample_size = model.config.sample_size
noise = torch.randn((1, 3, sample_size, sample_size), device="cuda")
input = noise

for t in scheduler.timesteps:
    with torch.no_grad():
        noisy_residual = model(input, t).sample
        prev_noisy_sample = scheduler.step(noisy_residual, t, input).prev_sample
        input = prev_noisy_sample

image = (input / 2 + 0.5).clamp(0, 1)
image = image.cpu().permute(0, 2, 3, 1).numpy()[0]
image = Image.fromarray((image * 255).round().astype("uint8"))
image
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Check out the &lt;a href="https://huggingface.co/docs/diffusers/quicktour"&gt;Quickstart&lt;/a&gt; to launch your diffusion journey today!&lt;/p&gt; 
&lt;h2&gt;How to navigate the documentation&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;What can I learn?&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/tutorials/tutorial_overview"&gt;Tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A basic crash course for learning how to use the library's most important features like using models and schedulers to build your own diffusion system, and training your own diffusion model.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/using-diffusers/loading"&gt;Loading&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Guides for how to load and configure all the components (pipelines, models, and schedulers) of the library, as well as how to use different schedulers.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/using-diffusers/overview_techniques"&gt;Pipelines for inference&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Guides for how to use pipelines for different inference tasks, batched generation, controlling generated outputs and randomness, and how to contribute a pipeline to the library.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/optimization/fp16"&gt;Optimization&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Guides for how to optimize your diffusion model to run faster and consume less memory.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/training/overview"&gt;Training&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Guides for how to train a diffusion model for different tasks with different training techniques.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;p&gt;We ❤️ contributions from the open-source community! If you want to contribute to this library, please check out our &lt;a href="https://github.com/huggingface/diffusers/raw/main/CONTRIBUTING.md"&gt;Contribution guide&lt;/a&gt;. You can look out for &lt;a href="https://github.com/huggingface/diffusers/issues"&gt;issues&lt;/a&gt; you'd like to tackle to contribute to the library.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;See &lt;a href="https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22"&gt;Good first issues&lt;/a&gt; for general opportunities to contribute&lt;/li&gt; 
 &lt;li&gt;See &lt;a href="https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3A%22New+pipeline%2Fmodel%22"&gt;New model/pipeline&lt;/a&gt; to contribute exciting new diffusion models / diffusion pipelines&lt;/li&gt; 
 &lt;li&gt;See &lt;a href="https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3A%22New+scheduler%22"&gt;New scheduler&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Also, say 👋 in our public Discord channel &lt;a href="https://discord.gg/G7tWnz98XR"&gt;&lt;img alt="Join us on Discord" src="https://img.shields.io/discord/823813159592001537?color=5865F2&amp;amp;logo=discord&amp;amp;logoColor=white" /&gt;&lt;/a&gt;. We discuss the hottest trends about diffusion models, help each other with contributions, personal projects or just hang out ☕.&lt;/p&gt; 
&lt;h2&gt;Popular Tasks &amp;amp; Pipelines&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th&gt;Task&lt;/th&gt; 
   &lt;th&gt;Pipeline&lt;/th&gt; 
   &lt;th&gt;🤗 Hub&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr style="border-top: 2px solid black"&gt; 
   &lt;td&gt;Unconditional Image Generation&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/ddpm"&gt; DDPM &lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/google/ddpm-ema-church-256"&gt; google/ddpm-ema-church-256 &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="border-top: 2px solid black"&gt; 
   &lt;td&gt;Text-to-Image&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/text2img"&gt;Stable Diffusion Text-to-Image&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5"&gt; stable-diffusion-v1-5/stable-diffusion-v1-5 &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Text-to-Image&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/unclip"&gt;unCLIP&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/kakaobrain/karlo-v1-alpha"&gt; kakaobrain/karlo-v1-alpha &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Text-to-Image&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/deepfloyd_if"&gt;DeepFloyd IF&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/DeepFloyd/IF-I-XL-v1.0"&gt; DeepFloyd/IF-I-XL-v1.0 &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Text-to-Image&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/kandinsky"&gt;Kandinsky&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder"&gt; kandinsky-community/kandinsky-2-2-decoder &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="border-top: 2px solid black"&gt; 
   &lt;td&gt;Text-guided Image-to-Image&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/controlnet"&gt;ControlNet&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/lllyasviel/sd-controlnet-canny"&gt; lllyasviel/sd-controlnet-canny &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Text-guided Image-to-Image&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/pix2pix"&gt;InstructPix2Pix&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/timbrooks/instruct-pix2pix"&gt; timbrooks/instruct-pix2pix &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Text-guided Image-to-Image&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/img2img"&gt;Stable Diffusion Image-to-Image&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5"&gt; stable-diffusion-v1-5/stable-diffusion-v1-5 &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="border-top: 2px solid black"&gt; 
   &lt;td&gt;Text-guided Image Inpainting&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/inpaint"&gt;Stable Diffusion Inpainting&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/runwayml/stable-diffusion-inpainting"&gt; runwayml/stable-diffusion-inpainting &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="border-top: 2px solid black"&gt; 
   &lt;td&gt;Image Variation&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/image_variation"&gt;Stable Diffusion Image Variation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/lambdalabs/sd-image-variations-diffusers"&gt; lambdalabs/sd-image-variations-diffusers &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="border-top: 2px solid black"&gt; 
   &lt;td&gt;Super Resolution&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/upscale"&gt;Stable Diffusion Upscale&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler"&gt; stabilityai/stable-diffusion-x4-upscaler &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Super Resolution&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/latent_upscale"&gt;Stable Diffusion Latent Upscale&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/stabilityai/sd-x2-latent-upscaler"&gt; stabilityai/sd-x2-latent-upscaler &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Popular libraries using 🧨 Diffusers&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/TaskMatrix"&gt;https://github.com/microsoft/TaskMatrix&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/invoke-ai/InvokeAI"&gt;https://github.com/invoke-ai/InvokeAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/InstantID/InstantID"&gt;https://github.com/InstantID/InstantID&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/apple/ml-stable-diffusion"&gt;https://github.com/apple/ml-stable-diffusion&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Sanster/lama-cleaner"&gt;https://github.com/Sanster/lama-cleaner&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/IDEA-Research/Grounded-Segment-Anything"&gt;https://github.com/IDEA-Research/Grounded-Segment-Anything&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ashawkey/stable-dreamfusion"&gt;https://github.com/ashawkey/stable-dreamfusion&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/deep-floyd/IF"&gt;https://github.com/deep-floyd/IF&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/bentoml/BentoML"&gt;https://github.com/bentoml/BentoML&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/bmaltais/kohya_ss"&gt;https://github.com/bmaltais/kohya_ss&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;+14,000 other amazing GitHub repositories 💪&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Thank you for using us ❤️.&lt;/p&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;p&gt;This library concretizes previous work by many different authors and would not have been possible without their great research and implementations. We'd like to thank, in particular, the following implementations which have helped us in our development and without which the API could not have been as polished today:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;@CompVis' latent diffusion models library, available &lt;a href="https://github.com/CompVis/latent-diffusion"&gt;here&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;@hojonathanho original DDPM implementation, available &lt;a href="https://github.com/hojonathanho/diffusion"&gt;here&lt;/a&gt; as well as the extremely useful translation into PyTorch by @pesser, available &lt;a href="https://github.com/pesser/pytorch_diffusion"&gt;here&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;@ermongroup's DDIM implementation, available &lt;a href="https://github.com/ermongroup/ddim"&gt;here&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;@yang-song's Score-VE and Score-VP implementations, available &lt;a href="https://github.com/yang-song/score_sde_pytorch"&gt;here&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We also want to thank @heejkoo for the very helpful overview of papers, code and resources on diffusion models, available &lt;a href="https://github.com/heejkoo/Awesome-Diffusion-Models"&gt;here&lt;/a&gt; as well as @crowsonkb and @rromb for useful discussions and insights.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{von-platen-etal-2022-diffusers,
  author = {Patrick von Platen and Suraj Patil and Anton Lozhkov and Pedro Cuenca and Nathan Lambert and Kashif Rasul and Mishig Davaadorj and Dhruv Nair and Sayak Paul and William Berman and Yiyi Xu and Steven Liu and Thomas Wolf},
  title = {Diffusers: State-of-the-art diffusion models},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/huggingface/diffusers}}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>livekit/agents</title>
      <link>https://github.com/livekit/agents</link>
      <description>&lt;p&gt;A powerful framework for building realtime voice AI agents 🤖🎙️📹&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="/.github/banner_dark.png" /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="/.github/banner_light.png" /&gt; 
 &lt;img style="width:100%;" alt="The LiveKit icon, the name of the repository and some sample code in the background." src="https://raw.githubusercontent.com/livekit/agents/main/.github/banner_light.png" /&gt; 
&lt;/picture&gt; 
&lt;!--END_BANNER_IMAGE--&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/pypi/v/livekit-agents" alt="PyPI - Version" /&gt; &lt;a href="https://pepy.tech/projects/livekit-agents"&gt;&lt;img src="https://static.pepy.tech/badge/livekit-agents/month" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://livekit.io/join-slack"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Flivekit.io%2Fbadges%2Fslack" alt="Slack community" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/livekit"&gt;&lt;img src="https://img.shields.io/twitter/follow/livekit" alt="Twitter Follow" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/livekit/agents"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki for understanding the codebase" /&gt;&lt;/a&gt; &lt;a href="https://github.com/livekit/livekit/raw/master/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/livekit/livekit" alt="License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;Looking for the JS/TS library? Check out &lt;a href="https://github.com/livekit/agents-js"&gt;AgentsJS&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What is Agents?&lt;/h2&gt; 
&lt;!--BEGIN_DESCRIPTION--&gt; 
&lt;p&gt;The Agent Framework is designed for building realtime, programmable participants that run on servers. Use it to create conversational, multi-modal voice agents that can see, hear, and understand.&lt;/p&gt; 
&lt;!--END_DESCRIPTION--&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible integrations&lt;/strong&gt;: A comprehensive ecosystem to mix and match the right STT, LLM, TTS, and Realtime API to suit your use case.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Integrated job scheduling&lt;/strong&gt;: Built-in task scheduling and distribution with &lt;a href="https://docs.livekit.io/agents/build/dispatch/"&gt;dispatch APIs&lt;/a&gt; to connect end users to agents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensive WebRTC clients&lt;/strong&gt;: Build client applications using LiveKit's open-source SDK ecosystem, supporting all major platforms.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Telephony integration&lt;/strong&gt;: Works seamlessly with LiveKit's &lt;a href="https://docs.livekit.io/sip/"&gt;telephony stack&lt;/a&gt;, allowing your agent to make calls to or receive calls from phones.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Exchange data with clients&lt;/strong&gt;: Use &lt;a href="https://docs.livekit.io/home/client/data/rpc/"&gt;RPCs&lt;/a&gt; and other &lt;a href="https://docs.livekit.io/home/client/data/"&gt;Data APIs&lt;/a&gt; to seamlessly exchange data with clients.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Semantic turn detection&lt;/strong&gt;: Uses a transformer model to detect when a user is done with their turn, helps to reduce interruptions.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP support&lt;/strong&gt;: Native support for MCP. Integrate tools provided by MCP servers with one loc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Builtin test framework&lt;/strong&gt;: Write tests and use judges to ensure your agent is performing as expected.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open-source&lt;/strong&gt;: Fully open-source, allowing you to run the entire stack on your own servers, including &lt;a href="https://github.com/livekit/livekit"&gt;LiveKit server&lt;/a&gt;, one of the most widely used WebRTC media servers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install the core Agents library, along with plugins for popular model providers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "livekit-agents[openai,silero,deepgram,cartesia,turn-detector]~=1.0"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Docs and guides&lt;/h2&gt; 
&lt;p&gt;Documentation on the framework and how to use it can be found &lt;a href="https://docs.livekit.io/agents/"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Core concepts&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Agent: An LLM-based application with defined instructions.&lt;/li&gt; 
 &lt;li&gt;AgentSession: A container for agents that manages interactions with end users.&lt;/li&gt; 
 &lt;li&gt;entrypoint: The starting point for an interactive session, similar to a request handler in a web server.&lt;/li&gt; 
 &lt;li&gt;Worker: The main process that coordinates job scheduling and launches agents for user sessions.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Simple voice agent&lt;/h3&gt; 
&lt;hr /&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from livekit.agents import (
    Agent,
    AgentSession,
    JobContext,
    RunContext,
    WorkerOptions,
    cli,
    function_tool,
)
from livekit.plugins import deepgram, elevenlabs, openai, silero

@function_tool
async def lookup_weather(
    context: RunContext,
    location: str,
):
    """Used to look up weather information."""

    return {"weather": "sunny", "temperature": 70}


async def entrypoint(ctx: JobContext):
    await ctx.connect()

    agent = Agent(
        instructions="You are a friendly voice assistant built by LiveKit.",
        tools=[lookup_weather],
    )
    session = AgentSession(
        vad=silero.VAD.load(),
        # any combination of STT, LLM, TTS, or realtime API can be used
        stt=deepgram.STT(model="nova-3"),
        llm=openai.LLM(model="gpt-4o-mini"),
        tts=elevenlabs.TTS(),
    )

    await session.start(agent=agent, room=ctx.room)
    await session.generate_reply(instructions="greet the user and ask about their day")


if __name__ == "__main__":
    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You'll need the following environment variables for this example:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;DEEPGRAM_API_KEY&lt;/li&gt; 
 &lt;li&gt;OPENAI_API_KEY&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Multi-agent handoff&lt;/h3&gt; 
&lt;hr /&gt; 
&lt;p&gt;This code snippet is abbreviated. For the full example, see &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/multi_agent.py"&gt;multi_agent.py&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;...
class IntroAgent(Agent):
    def __init__(self) -&amp;gt; None:
        super().__init__(
            instructions=f"You are a story teller. Your goal is to gather a few pieces of information from the user to make the story personalized and engaging."
            "Ask the user for their name and where they are from"
        )

    async def on_enter(self):
        self.session.generate_reply(instructions="greet the user and gather information")

    @function_tool
    async def information_gathered(
        self,
        context: RunContext,
        name: str,
        location: str,
    ):
        """Called when the user has provided the information needed to make the story personalized and engaging.

        Args:
            name: The name of the user
            location: The location of the user
        """

        context.userdata.name = name
        context.userdata.location = location

        story_agent = StoryAgent(name, location)
        return story_agent, "Let's start the story!"


class StoryAgent(Agent):
    def __init__(self, name: str, location: str) -&amp;gt; None:
        super().__init__(
            instructions=f"You are a storyteller. Use the user's information in order to make the story personalized."
            f"The user's name is {name}, from {location}"
            # override the default model, switching to Realtime API from standard LLMs
            llm=openai.realtime.RealtimeModel(voice="echo"),
            chat_ctx=chat_ctx,
        )

    async def on_enter(self):
        self.session.generate_reply()


async def entrypoint(ctx: JobContext):
    await ctx.connect()

    userdata = StoryData()
    session = AgentSession[StoryData](
        vad=silero.VAD.load(),
        stt=deepgram.STT(model="nova-3"),
        llm=openai.LLM(model="gpt-4o-mini"),
        tts=openai.TTS(voice="echo"),
        userdata=userdata,
    )

    await session.start(
        agent=IntroAgent(),
        room=ctx.room,
    )
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Testing&lt;/h3&gt; 
&lt;p&gt;Automated tests are essential for building reliable agents, especially with the non-deterministic behavior of LLMs. LiveKit Agents include native test integration to help you create dependable agents.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@pytest.mark.asyncio
async def test_no_availability() -&amp;gt; None:
    llm = google.LLM()
    async AgentSession(llm=llm) as sess:
        await sess.start(MyAgent())
        result = await sess.run(
            user_input="Hello, I need to place an order."
        )
        result.expect.skip_next_event_if(type="message", role="assistant")
        result.expect.next_event().is_function_call(name="start_order")
        result.expect.next_event().is_function_call_output()
        await (
            result.expect.next_event()
            .is_message(role="assistant")
            .judge(llm, intent="assistant should be asking the user what they would like")
        )

&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;🎙️ Starter Agent&lt;/h3&gt; &lt;p&gt;A starter agent optimized for voice conversations.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/basic_agent.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;🔄 Multi-user push to talk&lt;/h3&gt; &lt;p&gt;Responds to multiple users in the room via push-to-talk.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/push_to_talk.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;🎵 Background audio&lt;/h3&gt; &lt;p&gt;Background ambient and thinking audio to improve realism.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/background_audio.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;🛠️ Dynamic tool creation&lt;/h3&gt; &lt;p&gt;Creating function tools dynamically.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/dynamic_tool_creation.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;☎️ Outbound caller&lt;/h3&gt; &lt;p&gt;Agent that makes outbound phone calls&lt;/p&gt; &lt;p&gt; &lt;a href="https://github.com/livekit-examples/outbound-caller-python"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;📋 Structured output&lt;/h3&gt; &lt;p&gt;Using structured output from LLM to guide TTS tone.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/structured_output.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;🔌 MCP support&lt;/h3&gt; &lt;p&gt;Use tools from MCP servers&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/mcp"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;💬 Text-only agent&lt;/h3&gt; &lt;p&gt;Skip voice altogether and use the same code for text-only integrations&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/other/text_only.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;📝 Multi-user transcriber&lt;/h3&gt; &lt;p&gt;Produce transcriptions from all users in the room&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/other/transcription/multi-user-transcriber.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;🎥 Video avatars&lt;/h3&gt; &lt;p&gt;Add an AI avatar with Tavus, Beyond Presence, and Bithuman&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/avatar_agents/"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;🍽️ Restaurant ordering and reservations&lt;/h3&gt; &lt;p&gt;Full example of an agent that handles calls for a restaurant.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/restaurant_agent.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;👁️ Gemini Live vision&lt;/h3&gt; &lt;p&gt;Full example (including iOS app) of Gemini Live agent that can see.&lt;/p&gt; &lt;p&gt; &lt;a href="https://github.com/livekit-examples/vision-demo"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Running your agent&lt;/h2&gt; 
&lt;h3&gt;Testing in terminal&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python myagent.py console
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Runs your agent in terminal mode, enabling local audio input and output for testing. This mode doesn't require external servers or dependencies and is useful for quickly validating behavior.&lt;/p&gt; 
&lt;h3&gt;Developing with LiveKit clients&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python myagent.py dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Starts the agent server and enables hot reloading when files change. This mode allows each process to host multiple concurrent agents efficiently.&lt;/p&gt; 
&lt;p&gt;The agent connects to LiveKit Cloud or your self-hosted server. Set the following environment variables:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;LIVEKIT_URL&lt;/li&gt; 
 &lt;li&gt;LIVEKIT_API_KEY&lt;/li&gt; 
 &lt;li&gt;LIVEKIT_API_SECRET&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can connect using any LiveKit client SDK or telephony integration. To get started quickly, try the &lt;a href="https://agents-playground.livekit.io/"&gt;Agents Playground&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Running for production&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python myagent.py start
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Runs the agent with production-ready optimizations.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;The Agents framework is under active development in a rapidly evolving field. We welcome and appreciate contributions of any kind, be it feedback, bugfixes, features, new plugins and tools, or better documentation. You can file issues under this repo, open a PR, or chat with us in LiveKit's &lt;a href="https://livekit.io/join-slack"&gt;Slack community&lt;/a&gt;.&lt;/p&gt; 
&lt;!--BEGIN_REPO_NAV--&gt; 
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;table&gt; 
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th colspan="2"&gt;LiveKit Ecosystem&lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt;
   &lt;td&gt;LiveKit SDKs&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://github.com/livekit/client-sdk-js"&gt;Browser&lt;/a&gt; · &lt;a href="https://github.com/livekit/client-sdk-swift"&gt;iOS/macOS/visionOS&lt;/a&gt; · &lt;a href="https://github.com/livekit/client-sdk-android"&gt;Android&lt;/a&gt; · &lt;a href="https://github.com/livekit/client-sdk-flutter"&gt;Flutter&lt;/a&gt; · &lt;a href="https://github.com/livekit/client-sdk-react-native"&gt;React Native&lt;/a&gt; · &lt;a href="https://github.com/livekit/rust-sdks"&gt;Rust&lt;/a&gt; · &lt;a href="https://github.com/livekit/node-sdks"&gt;Node.js&lt;/a&gt; · &lt;a href="https://github.com/livekit/python-sdks"&gt;Python&lt;/a&gt; · &lt;a href="https://github.com/livekit/client-sdk-unity"&gt;Unity&lt;/a&gt; · &lt;a href="https://github.com/livekit/client-sdk-unity-web"&gt;Unity (WebGL)&lt;/a&gt; · &lt;a href="https://github.com/livekit/client-sdk-esp32"&gt;ESP32&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Server APIs&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://github.com/livekit/node-sdks"&gt;Node.js&lt;/a&gt; · &lt;a href="https://github.com/livekit/server-sdk-go"&gt;Golang&lt;/a&gt; · &lt;a href="https://github.com/livekit/server-sdk-ruby"&gt;Ruby&lt;/a&gt; · &lt;a href="https://github.com/livekit/server-sdk-kotlin"&gt;Java/Kotlin&lt;/a&gt; · &lt;a href="https://github.com/livekit/python-sdks"&gt;Python&lt;/a&gt; · &lt;a href="https://github.com/livekit/rust-sdks"&gt;Rust&lt;/a&gt; · &lt;a href="https://github.com/agence104/livekit-server-sdk-php"&gt;PHP (community)&lt;/a&gt; · &lt;a href="https://github.com/pabloFuente/livekit-server-sdk-dotnet"&gt;.NET (community)&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;UI Components&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://github.com/livekit/components-js"&gt;React&lt;/a&gt; · &lt;a href="https://github.com/livekit/components-android"&gt;Android Compose&lt;/a&gt; · &lt;a href="https://github.com/livekit/components-swift"&gt;SwiftUI&lt;/a&gt; · &lt;a href="https://github.com/livekit/components-flutter"&gt;Flutter&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Agents Frameworks&lt;/td&gt;
   &lt;td&gt;&lt;b&gt;Python&lt;/b&gt; · &lt;a href="https://github.com/livekit/agents-js"&gt;Node.js&lt;/a&gt; · &lt;a href="https://github.com/livekit/agent-playground"&gt;Playground&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Services&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://github.com/livekit/livekit"&gt;LiveKit server&lt;/a&gt; · &lt;a href="https://github.com/livekit/egress"&gt;Egress&lt;/a&gt; · &lt;a href="https://github.com/livekit/ingress"&gt;Ingress&lt;/a&gt; · &lt;a href="https://github.com/livekit/sip"&gt;SIP&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Resources&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://docs.livekit.io"&gt;Docs&lt;/a&gt; · &lt;a href="https://github.com/livekit-examples"&gt;Example apps&lt;/a&gt; · &lt;a href="https://livekit.io/cloud"&gt;Cloud&lt;/a&gt; · &lt;a href="https://docs.livekit.io/home/self-hosting/deployment"&gt;Self-hosting&lt;/a&gt; · &lt;a href="https://github.com/livekit/livekit-cli"&gt;CLI&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!--END_REPO_NAV--&gt;</description>
    </item>
    
    <item>
      <title>JaidedAI/EasyOCR</title>
      <link>https://github.com/JaidedAI/EasyOCR</link>
      <description>&lt;p&gt;Ready-to-use OCR with 80+ supported languages and all popular writing scripts including Latin, Chinese, Arabic, Devanagari, Cyrillic and etc.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;EasyOCR&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://badge.fury.io/py/easyocr"&gt;&lt;img src="https://badge.fury.io/py/easyocr.svg?sanitize=true" alt="PyPI Status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/JaidedAI/EasyOCR/raw/master/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true" alt="license" /&gt;&lt;/a&gt; &lt;a href="https://colab.to/easyocr"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open in Colab" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/intent/tweet?text=Check%20out%20this%20awesome%20library:%20EasyOCR%20https://github.com/JaidedAI/EasyOCR"&gt;&lt;img src="https://img.shields.io/twitter/url/https/github.com/JaidedAI/EasyOCR.svg?style=social" alt="Tweet" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/JaidedAI"&gt;&lt;img src="https://img.shields.io/badge/twitter-@JaidedAI-blue.svg?style=flat" alt="Twitter" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Ready-to-use OCR with 80+ &lt;a href="https://www.jaided.ai/easyocr"&gt;supported languages&lt;/a&gt; and all popular writing scripts including: Latin, Chinese, Arabic, Devanagari, Cyrillic, etc.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.jaided.ai/easyocr"&gt;Try Demo on our website&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Integrated into &lt;a href="https://huggingface.co/spaces"&gt;Huggingface Spaces 🤗&lt;/a&gt; using &lt;a href="https://github.com/gradio-app/gradio"&gt;Gradio&lt;/a&gt;. Try out the Web Demo: &lt;a href="https://huggingface.co/spaces/tomofi/EasyOCR"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue" alt="Hugging Face Spaces" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What's new&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;24 September 2024 - Version 1.7.2&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fix several compatibilities&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/JaidedAI/EasyOCR/raw/master/releasenotes.md"&gt;Read all release notes&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What's coming next&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Handwritten text support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/JaidedAI/EasyOCR/master/examples/example.png" alt="example" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/JaidedAI/EasyOCR/master/examples/example2.png" alt="example2" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/JaidedAI/EasyOCR/master/examples/example3.png" alt="example3" /&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Install using &lt;code&gt;pip&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;For the latest stable release:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install easyocr
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For the latest development release:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install git+https://github.com/JaidedAI/EasyOCR.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note 1: For Windows, please install torch and torchvision first by following the official instructions here &lt;a href="https://pytorch.org"&gt;https://pytorch.org&lt;/a&gt;. On the pytorch website, be sure to select the right CUDA version you have. If you intend to run on CPU mode only, select &lt;code&gt;CUDA = None&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Note 2: We also provide a Dockerfile &lt;a href="https://github.com/JaidedAI/EasyOCR/raw/master/Dockerfile"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import easyocr
reader = easyocr.Reader(['ch_sim','en']) # this needs to run only once to load the model into memory
result = reader.readtext('chinese.jpg')
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The output will be in a list format, each item represents a bounding box, the text detected and confident level, respectively.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;[([[189, 75], [469, 75], [469, 165], [189, 165]], '愚园路', 0.3754989504814148),
 ([[86, 80], [134, 80], [134, 128], [86, 128]], '西', 0.40452659130096436),
 ([[517, 81], [565, 81], [565, 123], [517, 123]], '东', 0.9989598989486694),
 ([[78, 126], [136, 126], [136, 156], [78, 156]], '315', 0.8125889301300049),
 ([[514, 126], [574, 126], [574, 156], [514, 156]], '309', 0.4971577227115631),
 ([[226, 170], [414, 170], [414, 220], [226, 220]], 'Yuyuan Rd.', 0.8261902332305908),
 ([[79, 173], [125, 173], [125, 213], [79, 213]], 'W', 0.9848111271858215),
 ([[529, 173], [569, 173], [569, 213], [529, 213]], 'E', 0.8405593633651733)]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note 1: &lt;code&gt;['ch_sim','en']&lt;/code&gt; is the list of languages you want to read. You can pass several languages at once but not all languages can be used together. English is compatible with every language and languages that share common characters are usually compatible with each other.&lt;/p&gt; 
&lt;p&gt;Note 2: Instead of the filepath &lt;code&gt;chinese.jpg&lt;/code&gt;, you can also pass an OpenCV image object (numpy array) or an image file as bytes. A URL to a raw image is also acceptable.&lt;/p&gt; 
&lt;p&gt;Note 3: The line &lt;code&gt;reader = easyocr.Reader(['ch_sim','en'])&lt;/code&gt; is for loading a model into memory. It takes some time but it needs to be run only once.&lt;/p&gt; 
&lt;p&gt;You can also set &lt;code&gt;detail=0&lt;/code&gt; for simpler output.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;reader.readtext('chinese.jpg', detail = 0)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Result:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;['愚园路', '西', '东', '315', '309', 'Yuyuan Rd.', 'W', 'E']
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Model weights for the chosen language will be automatically downloaded or you can download them manually from the &lt;a href="https://www.jaided.ai/easyocr/modelhub"&gt;model hub&lt;/a&gt; and put them in the '~/.EasyOCR/model' folder&lt;/p&gt; 
&lt;p&gt;In case you do not have a GPU, or your GPU has low memory, you can run the model in CPU-only mode by adding &lt;code&gt;gpu=False&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;reader = easyocr.Reader(['ch_sim','en'], gpu=False)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more information, read the &lt;a href="https://www.jaided.ai/easyocr/tutorial"&gt;tutorial&lt;/a&gt; and &lt;a href="https://www.jaided.ai/easyocr/documentation"&gt;API Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Run on command line&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;$ easyocr -l ch_sim en -f chinese.jpg --detail=1 --gpu=True
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Train/use your own model&lt;/h2&gt; 
&lt;p&gt;For recognition model, &lt;a href="https://github.com/JaidedAI/EasyOCR/raw/master/custom_model.md"&gt;Read here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For detection model (CRAFT), &lt;a href="https://github.com/JaidedAI/EasyOCR/raw/master/trainer/craft/README.md"&gt;Read here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Implementation Roadmap&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Handwritten support&lt;/li&gt; 
 &lt;li&gt;Restructure code to support swappable detection and recognition algorithms The api should be as easy as&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;reader = easyocr.Reader(['en'], detection='DB', recognition = 'Transformer')
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The idea is to be able to plug in any state-of-the-art model into EasyOCR. There are a lot of geniuses trying to make better detection/recognition models, but we are not trying to be geniuses here. We just want to make their works quickly accessible to the public ... for free. (well, we believe most geniuses want their work to create a positive impact as fast/big as possible) The pipeline should be something like the below diagram. Grey slots are placeholders for changeable light blue modules.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/JaidedAI/EasyOCR/master/examples/easyocr_framework.jpeg" alt="plan" /&gt;&lt;/p&gt; 
&lt;h2&gt;Acknowledgement and References&lt;/h2&gt; 
&lt;p&gt;This project is based on research and code from several papers and open-source repositories.&lt;/p&gt; 
&lt;p&gt;All deep learning execution is based on &lt;a href="https://pytorch.org"&gt;Pytorch&lt;/a&gt;. &lt;span&gt;❤️&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;Detection execution uses the CRAFT algorithm from this &lt;a href="https://github.com/clovaai/CRAFT-pytorch"&gt;official repository&lt;/a&gt; and their &lt;a href="https://arxiv.org/abs/1904.01941"&gt;paper&lt;/a&gt; (Thanks @YoungminBaek from &lt;a href="https://github.com/clovaai"&gt;@clovaai&lt;/a&gt;). We also use their pretrained model. Training script is provided by &lt;a href="https://github.com/gmuffiness"&gt;@gmuffiness&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The recognition model is a CRNN (&lt;a href="https://arxiv.org/abs/1507.05717"&gt;paper&lt;/a&gt;). It is composed of 3 main components: feature extraction (we are currently using &lt;a href="https://arxiv.org/abs/1512.03385"&gt;Resnet&lt;/a&gt;) and VGG, sequence labeling (&lt;a href="https://www.bioinf.jku.at/publications/older/2604.pdf"&gt;LSTM&lt;/a&gt;) and decoding (&lt;a href="https://www.cs.toronto.edu/~graves/icml_2006.pdf"&gt;CTC&lt;/a&gt;). The training pipeline for recognition execution is a modified version of the &lt;a href="https://github.com/clovaai/deep-text-recognition-benchmark"&gt;deep-text-recognition-benchmark&lt;/a&gt; framework. (Thanks &lt;a href="https://github.com/ku21fan"&gt;@ku21fan&lt;/a&gt; from &lt;a href="https://github.com/clovaai"&gt;@clovaai&lt;/a&gt;) This repository is a gem that deserves more recognition.&lt;/p&gt; 
&lt;p&gt;Beam search code is based on this &lt;a href="https://github.com/githubharald/CTCDecoder"&gt;repository&lt;/a&gt; and his &lt;a href="https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7"&gt;blog&lt;/a&gt;. (Thanks &lt;a href="https://github.com/githubharald"&gt;@githubharald&lt;/a&gt;)&lt;/p&gt; 
&lt;p&gt;Data synthesis is based on &lt;a href="https://github.com/Belval/TextRecognitionDataGenerator"&gt;TextRecognitionDataGenerator&lt;/a&gt;. (Thanks &lt;a href="https://github.com/Belval"&gt;@Belval&lt;/a&gt;)&lt;/p&gt; 
&lt;p&gt;And a good read about CTC from distill.pub &lt;a href="https://distill.pub/2017/ctc/"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Want To Contribute?&lt;/h2&gt; 
&lt;p&gt;Let's advance humanity together by making AI available to everyone!&lt;/p&gt; 
&lt;p&gt;3 ways to contribute:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Coder:&lt;/strong&gt; Please send a PR for small bugs/improvements. For bigger ones, discuss with us by opening an issue first. There is a list of possible bug/improvement issues tagged with &lt;a href="https://github.com/JaidedAI/EasyOCR/issues?q=is%3Aissue+is%3Aopen+label%3A%22PR+WELCOME%22"&gt;'PR WELCOME'&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;User:&lt;/strong&gt; Tell us how EasyOCR benefits you/your organization to encourage further development. Also post failure cases in &lt;a href="https://github.com/JaidedAI/EasyOCR/issues"&gt;Issue Section&lt;/a&gt; to help improve future models.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Tech leader/Guru:&lt;/strong&gt; If you found this library useful, please spread the word! (See &lt;a href="https://www.facebook.com/yann.lecun/posts/10157018122787143"&gt;Yann Lecun's post&lt;/a&gt; about EasyOCR)&lt;/p&gt; 
&lt;h2&gt;Guideline for new language request&lt;/h2&gt; 
&lt;p&gt;To request a new language, we need you to send a PR with the 2 following files:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;In folder &lt;a href="https://github.com/JaidedAI/EasyOCR/tree/master/easyocr/character"&gt;easyocr/character&lt;/a&gt;, we need 'yourlanguagecode_char.txt' that contains list of all characters. Please see format examples from other files in that folder.&lt;/li&gt; 
 &lt;li&gt;In folder &lt;a href="https://github.com/JaidedAI/EasyOCR/tree/master/easyocr/dict"&gt;easyocr/dict&lt;/a&gt;, we need 'yourlanguagecode.txt' that contains list of words in your language. On average, we have ~30000 words per language with more than 50000 words for more popular ones. More is better in this file.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;If your language has unique elements (such as 1. Arabic: characters change form when attached to each other + write from right to left 2. Thai: Some characters need to be above the line and some below), please educate us to the best of your ability and/or give useful links. It is important to take care of the detail to achieve a system that really works.&lt;/p&gt; 
&lt;p&gt;Lastly, please understand that our priority will have to go to popular languages or sets of languages that share large portions of their characters with each other (also tell us if this is the case for your language). It takes us at least a week to develop a new model, so you may have to wait a while for the new model to be released.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://github.com/JaidedAI/EasyOCR/issues/91"&gt;List of languages in development&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Github Issues&lt;/h2&gt; 
&lt;p&gt;Due to limited resources, an issue older than 6 months will be automatically closed. Please open an issue again if it is critical.&lt;/p&gt; 
&lt;h2&gt;Business Inquiries&lt;/h2&gt; 
&lt;p&gt;For Enterprise Support, &lt;a href="https://www.jaided.ai/"&gt;Jaided AI&lt;/a&gt; offers full service for custom OCR/AI systems from implementation, training/finetuning and deployment. Click &lt;a href="https://www.jaided.ai/contactus?ref=github"&gt;here&lt;/a&gt; to contact us.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/autogen</title>
      <link>https://github.com/microsoft/autogen</link>
      <description>&lt;p&gt;A programming framework for agentic AI 🤖 PyPi: autogen-agentchat Discord: https://aka.ms/autogen-discord Office Hour: https://aka.ms/autogen-officehour&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://microsoft.github.io/autogen/0.2/img/ag.svg?sanitize=true" alt="AutoGen Logo" width="100" /&gt; 
 &lt;p&gt;&lt;a href="https://twitter.com/pyautogen"&gt;&lt;img src="https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&amp;amp;label=Follow%20%40pyautogen" alt="Twitter" /&gt;&lt;/a&gt; &lt;a href="https://www.linkedin.com/company/105812540"&gt;&lt;img src="https://img.shields.io/badge/LinkedIn-Company?style=flat&amp;amp;logo=linkedin&amp;amp;logoColor=white" alt="LinkedIn" /&gt;&lt;/a&gt; &lt;a href="https://aka.ms/autogen-discord"&gt;&lt;img src="https://img.shields.io/badge/discord-chat-green?logo=discord" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://microsoft.github.io/autogen/"&gt;&lt;img src="https://img.shields.io/badge/Documentation-AutoGen-blue?logo=read-the-docs" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://devblogs.microsoft.com/autogen/"&gt;&lt;img src="https://img.shields.io/badge/Blog-AutoGen-blue?logo=blogger" alt="Blog" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h1&gt;AutoGen&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;AutoGen&lt;/strong&gt; is a framework for creating multi-agent AI applications that can act autonomously or work alongside humans.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;AutoGen requires &lt;strong&gt;Python 3.10 or later&lt;/strong&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install AgentChat and OpenAI client from Extensions
pip install -U "autogen-agentchat" "autogen-ext[openai]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The current stable version can be found in the &lt;a href="https://github.com/microsoft/autogen/releases"&gt;releases&lt;/a&gt;. If you are upgrading from AutoGen v0.2, please refer to the &lt;a href="https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/migration-guide.html"&gt;Migration Guide&lt;/a&gt; for detailed instructions on how to update your code and configurations.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install AutoGen Studio for no-code GUI
pip install -U "autogenstudio"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;h3&gt;Hello World&lt;/h3&gt; 
&lt;p&gt;Create an assistant agent using OpenAI's GPT-4o model. See &lt;a href="https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/models.html"&gt;other supported models&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main() -&amp;gt; None:
    model_client = OpenAIChatCompletionClient(model="gpt-4.1")
    agent = AssistantAgent("assistant", model_client=model_client)
    print(await agent.run(task="Say 'Hello World!'"))
    await model_client.close()

asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;MCP Server&lt;/h3&gt; 
&lt;p&gt;Create a web browsing assistant agent that uses the Playwright MCP server.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# First run `npm install -g @playwright/mcp@latest` to install the MCP server.
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import McpWorkbench, StdioServerParams


async def main() -&amp;gt; None:
    model_client = OpenAIChatCompletionClient(model="gpt-4.1")
    server_params = StdioServerParams(
        command="npx",
        args=[
            "@playwright/mcp@latest",
            "--headless",
        ],
    )
    async with McpWorkbench(server_params) as mcp:
        agent = AssistantAgent(
            "web_browsing_assistant",
            model_client=model_client,
            workbench=mcp, # For multiple MCP servers, put them in a list.
            model_client_stream=True,
            max_tool_iterations=10,
        )
        await Console(agent.run_stream(task="Find out how many contributors for the microsoft/autogen repository"))


asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: Only connect to trusted MCP servers as they may execute commands in your local environment or expose sensitive information.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Multi-Agent Orchestration&lt;/h3&gt; 
&lt;p&gt;You can use &lt;code&gt;AgentTool&lt;/code&gt; to create a basic multi-agent orchestration setup.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.tools import AgentTool
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -&amp;gt; None:
    model_client = OpenAIChatCompletionClient(model="gpt-4.1")

    math_agent = AssistantAgent(
        "math_expert",
        model_client=model_client,
        system_message="You are a math expert.",
        description="A math expert assistant.",
        model_client_stream=True,
    )
    math_agent_tool = AgentTool(math_agent, return_value_as_last_message=True)

    chemistry_agent = AssistantAgent(
        "chemistry_expert",
        model_client=model_client,
        system_message="You are a chemistry expert.",
        description="A chemistry expert assistant.",
        model_client_stream=True,
    )
    chemistry_agent_tool = AgentTool(chemistry_agent, return_value_as_last_message=True)

    agent = AssistantAgent(
        "assistant",
        system_message="You are a general assistant. Use expert tools when needed.",
        model_client=model_client,
        model_client_stream=True,
        tools=[math_agent_tool, chemistry_agent_tool],
        max_tool_iterations=10,
    )
    await Console(agent.run_stream(task="What is the integral of x^2?"))
    await Console(agent.run_stream(task="What is the molecular weight of water?"))


asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more advanced multi-agent orchestrations and workflows, read &lt;a href="https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/index.html"&gt;AgentChat documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;AutoGen Studio&lt;/h3&gt; 
&lt;p&gt;Use AutoGen Studio to prototype and run multi-agent workflows without writing code.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run AutoGen Studio on http://localhost:8080
autogenstudio ui --port 8080 --appdir ./my-app
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Why Use AutoGen?&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/microsoft/autogen/main/autogen-landing.jpg" alt="AutoGen Landing" width="500" /&gt; 
&lt;/div&gt; 
&lt;p&gt;The AutoGen ecosystem provides everything you need to create AI agents, especially multi-agent workflows -- framework, developer tools, and applications.&lt;/p&gt; 
&lt;p&gt;The &lt;em&gt;framework&lt;/em&gt; uses a layered and extensible design. Layers have clearly divided responsibilities and build on top of layers below. This design enables you to use the framework at different levels of abstraction, from high-level APIs to low-level components.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/autogen/main/python/packages/autogen-core/"&gt;Core API&lt;/a&gt; implements message passing, event-driven agents, and local and distributed runtime for flexibility and power. It also support cross-language support for .NET and Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/autogen/main/python/packages/autogen-agentchat/"&gt;AgentChat API&lt;/a&gt; implements a simpler but opinionated&amp;nbsp;API for rapid prototyping. This API is built on top of the Core API and is closest to what users of v0.2 are familiar with and supports common multi-agent patterns such as two-agent chat or group chats.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/autogen/main/python/packages/autogen-ext/"&gt;Extensions API&lt;/a&gt; enables first- and third-party extensions continuously expanding framework capabilities. It support specific implementation of LLM clients (e.g., OpenAI, AzureOpenAI), and capabilities such as code execution.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The ecosystem also supports two essential &lt;em&gt;developer tools&lt;/em&gt;:&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://media.githubusercontent.com/media/microsoft/autogen/refs/heads/main/python/packages/autogen-studio/docs/ags_screen.png" alt="AutoGen Studio Screenshot" width="500" /&gt; 
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/autogen/main/python/packages/autogen-studio/"&gt;AutoGen Studio&lt;/a&gt; provides a no-code GUI for building multi-agent applications.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/autogen/main/python/packages/agbench/"&gt;AutoGen Bench&lt;/a&gt; provides a benchmarking suite for evaluating agent performance.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can use the AutoGen framework and developer tools to create applications for your domain. For example, &lt;a href="https://raw.githubusercontent.com/microsoft/autogen/main/python/packages/magentic-one-cli/"&gt;Magentic-One&lt;/a&gt; is a state-of-the-art multi-agent team built using AgentChat API and Extensions API that can handle a variety of tasks that require web browsing, code execution, and file handling.&lt;/p&gt; 
&lt;p&gt;With AutoGen you get to join and contribute to a thriving ecosystem. We host weekly office hours and talks with maintainers and community. We also have a &lt;a href="https://aka.ms/autogen-discord"&gt;Discord server&lt;/a&gt; for real-time chat, GitHub Discussions for Q&amp;amp;A, and a blog for tutorials and updates.&lt;/p&gt; 
&lt;h2&gt;Where to go next?&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/autogen/main/python"&gt;&lt;img src="https://img.shields.io/badge/AutoGen-Python-blue?logo=python&amp;amp;logoColor=white" alt="Python" /&gt;&lt;/a&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/autogen/main/dotnet"&gt;&lt;img src="https://img.shields.io/badge/AutoGen-.NET-green?logo=.net&amp;amp;logoColor=white" alt=".NET" /&gt;&lt;/a&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/autogen/main/python/packages/autogen-studio"&gt;&lt;img src="https://img.shields.io/badge/AutoGen-Studio-purple?logo=visual-studio&amp;amp;logoColor=white" alt="Studio" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Installation&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/installation.html"&gt;&lt;img src="https://img.shields.io/badge/Install-blue" alt="Installation" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://microsoft.github.io/autogen/dotnet/dev/core/installation.html"&gt;&lt;img src="https://img.shields.io/badge/Install-green" alt="Install" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/installation.html"&gt;&lt;img src="https://img.shields.io/badge/Install-purple" alt="Install" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Quickstart&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/quickstart.html#"&gt;&lt;img src="https://img.shields.io/badge/Quickstart-blue" alt="Quickstart" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://microsoft.github.io/autogen/dotnet/dev/core/index.html"&gt;&lt;img src="https://img.shields.io/badge/Quickstart-green" alt="Quickstart" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/usage.html#"&gt;&lt;img src="https://img.shields.io/badge/Quickstart-purple" alt="Usage" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Tutorial&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/index.html"&gt;&lt;img src="https://img.shields.io/badge/Tutorial-blue" alt="Tutorial" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://microsoft.github.io/autogen/dotnet/dev/core/tutorial.html"&gt;&lt;img src="https://img.shields.io/badge/Tutorial-green" alt="Tutorial" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/usage.html#"&gt;&lt;img src="https://img.shields.io/badge/Tutorial-purple" alt="Usage" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;API Reference&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://microsoft.github.io/autogen/stable/reference/index.html#"&gt;&lt;img src="https://img.shields.io/badge/Docs-blue" alt="API" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://microsoft.github.io/autogen/dotnet/dev/api/Microsoft.AutoGen.Contracts.html"&gt;&lt;img src="https://img.shields.io/badge/Docs-green" alt="API" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/usage.html"&gt;&lt;img src="https://img.shields.io/badge/Docs-purple" alt="API" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Packages&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://pypi.org/project/autogen-core/"&gt;&lt;img src="https://img.shields.io/badge/PyPi-autogen--core-blue?logo=pypi" alt="PyPi autogen-core" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://pypi.org/project/autogen-agentchat/"&gt;&lt;img src="https://img.shields.io/badge/PyPi-autogen--agentchat-blue?logo=pypi" alt="PyPi autogen-agentchat" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://pypi.org/project/autogen-ext/"&gt;&lt;img src="https://img.shields.io/badge/PyPi-autogen--ext-blue?logo=pypi" alt="PyPi autogen-ext" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://www.nuget.org/packages/Microsoft.AutoGen.Contracts/"&gt;&lt;img src="https://img.shields.io/badge/NuGet-Contracts-green?logo=nuget" alt="NuGet Contracts" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://www.nuget.org/packages/Microsoft.AutoGen.Core/"&gt;&lt;img src="https://img.shields.io/badge/NuGet-Core-green?logo=nuget" alt="NuGet Core" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://www.nuget.org/packages/Microsoft.AutoGen.Core.Grpc/"&gt;&lt;img src="https://img.shields.io/badge/NuGet-Core.Grpc-green?logo=nuget" alt="NuGet Core.Grpc" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://www.nuget.org/packages/Microsoft.AutoGen.RuntimeGateway.Grpc/"&gt;&lt;img src="https://img.shields.io/badge/NuGet-RuntimeGateway.Grpc-green?logo=nuget" alt="NuGet RuntimeGateway.Grpc" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://pypi.org/project/autogenstudio/"&gt;&lt;img src="https://img.shields.io/badge/PyPi-autogenstudio-purple?logo=pypi" alt="PyPi autogenstudio" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;p&gt;Interested in contributing? See &lt;a href="https://raw.githubusercontent.com/microsoft/autogen/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for guidelines on how to get started. We welcome contributions of all kinds, including bug fixes, new features, and documentation improvements. Join our community and help us make AutoGen better!&lt;/p&gt; 
&lt;p&gt;Have questions? Check out our &lt;a href="https://raw.githubusercontent.com/microsoft/autogen/main/FAQ.md"&gt;Frequently Asked Questions (FAQ)&lt;/a&gt; for answers to common queries. If you don't find what you're looking for, feel free to ask in our &lt;a href="https://github.com/microsoft/autogen/discussions"&gt;GitHub Discussions&lt;/a&gt; or join our &lt;a href="https://aka.ms/autogen-discord"&gt;Discord server&lt;/a&gt; for real-time support. You can also read our &lt;a href="https://devblogs.microsoft.com/autogen/"&gt;blog&lt;/a&gt; for updates.&lt;/p&gt; 
&lt;h2&gt;Legal Notices&lt;/h2&gt; 
&lt;p&gt;Microsoft and any contributors grant you a license to the Microsoft documentation and other content in this repository under the &lt;a href="https://creativecommons.org/licenses/by/4.0/legalcode"&gt;Creative Commons Attribution 4.0 International Public License&lt;/a&gt;, see the &lt;a href="https://raw.githubusercontent.com/microsoft/autogen/main/LICENSE"&gt;LICENSE&lt;/a&gt; file, and grant you a license to any code in the repository under the &lt;a href="https://opensource.org/licenses/MIT"&gt;MIT License&lt;/a&gt;, see the &lt;a href="https://raw.githubusercontent.com/microsoft/autogen/main/LICENSE-CODE"&gt;LICENSE-CODE&lt;/a&gt; file.&lt;/p&gt; 
&lt;p&gt;Microsoft, Windows, Microsoft Azure, and/or other Microsoft products and services referenced in the documentation may be either trademarks or registered trademarks of Microsoft in the United States and/or other countries. The licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks. Microsoft's general trademark guidelines can be found at &lt;a href="http://go.microsoft.com/fwlink/?LinkID=254653"&gt;http://go.microsoft.com/fwlink/?LinkID=254653&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Privacy information can be found at &lt;a href="https://go.microsoft.com/fwlink/?LinkId=521839"&gt;https://go.microsoft.com/fwlink/?LinkId=521839&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Microsoft and any contributors reserve all other rights, whether under their respective copyrights, patents, or trademarks, whether by implication, estoppel, or otherwise.&lt;/p&gt; 
&lt;p align="right" style="font-size: 14px; color: #555; margin-top: 20px;"&gt; &lt;a href="https://raw.githubusercontent.com/microsoft/autogen/main/#readme-top" style="text-decoration: none; color: blue; font-weight: bold;"&gt; ↑ Back to Top ↑ &lt;/a&gt; &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>HunxByts/GhostTrack</title>
      <link>https://github.com/HunxByts/GhostTrack</link>
      <description>&lt;p&gt;Useful tool to track location or mobile number&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GhostTrack&lt;/h1&gt; 
&lt;p&gt;Useful tool to track location or mobile number, so this tool can be called osint or also information gathering&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/raw/main/asset/bn.png" /&gt; 
&lt;p&gt;New update : &lt;code&gt;Version 2.2&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Instalation on Linux (deb)&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;sudo apt-get install git
sudo apt-get install python3
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Instalation on Termux&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;pkg install git
pkg install python3
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Usage Tool&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/HunxByts/GhostTrack.git
cd GhostTrack
pip3 install -r requirements.txt
python3 GhostTR.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Display on the menu &lt;code&gt;IP Tracker&lt;/code&gt;&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/blob/main/asset/ip.png " /&gt; 
&lt;p&gt;on the IP Track menu, you can combo with the seeker tool to get the target IP&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;span&gt;⚡&lt;/span&gt; Install Seeker :&lt;/summary&gt; - 
 &lt;strong&gt;&lt;a href="https://github.com/thewhiteh4t/seeker"&gt;Get Seeker&lt;/a&gt;&lt;/strong&gt; 
&lt;/details&gt; 
&lt;p&gt;Display on the menu &lt;code&gt;Phone Tracker&lt;/code&gt;&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/raw/main/asset/phone.png" /&gt; 
&lt;p&gt;on this menu you can search for information from the target phone number&lt;/p&gt; 
&lt;p&gt;Display on the menu &lt;code&gt;Username Tracker&lt;/code&gt;&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/raw/main/asset/User.png" /&gt; on this menu you can search for information from the target username on social media 
&lt;details&gt; 
 &lt;summary&gt;&lt;span&gt;⚡&lt;/span&gt; Author :&lt;/summary&gt; - 
 &lt;strong&gt;&lt;a href="https://github.com/HunxByts"&gt;HunxByts&lt;/a&gt;&lt;/strong&gt; 
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>PaddlePaddle/PaddleDetection</title>
      <link>https://github.com/PaddlePaddle/PaddleDetection</link>
      <description>&lt;p&gt;Object Detection toolkit based on PaddlePaddle. It supports object detection, instance segmentation, multiple object tracking and real-time multi-person keypoint detection.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;简体中文 | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/README_en.md"&gt;English&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p align="center"&gt; &lt;img src="https://user-images.githubusercontent.com/48054808/160532560-34cf7a1f-d950-435e-90d2-4b0a679e5119.png" align="middle" width="800" /&gt; &lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-Apache%202-dfd.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://github.com/PaddlePaddle/PaddleDetection/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/PaddlePaddle/PaddleDetection?color=ffa" /&gt;&lt;/a&gt; &lt;a href=""&gt;&lt;img src="https://img.shields.io/badge/python-3.7+-aff.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href=""&gt;&lt;img src="https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://github.com/PaddlePaddle/PaddleDetection/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/PaddlePaddle/PaddleDetection?color=ccf" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;💌目录&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E7%9B%AE%E5%BD%95"&gt;💌目录&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E7%AE%80%E4%BB%8B"&gt;🌈简介&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%95"&gt;📣最新进展&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%EF%B8%8F%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B"&gt;⚡️快速开始&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E4%BD%8E%E4%BB%A3%E7%A0%81%E5%85%A8%E6%B5%81%E7%A8%8B%E5%BC%80%E5%8F%91"&gt;🔥低代码全流程开发&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E5%BC%80%E6%BA%90%E7%A4%BE%E5%8C%BA"&gt;👫开源社区&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E4%B8%BB%E8%A6%81%E7%89%B9%E6%80%A7"&gt;✨主要特性&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E6%A8%A1%E5%9D%97%E5%8C%96%E8%AE%BE%E8%AE%A1"&gt;🧩模块化设计&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E4%B8%B0%E5%AF%8C%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%BA%93"&gt;📱丰富的模型库&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%EF%B8%8F%E4%BA%A7%E4%B8%9A%E7%89%B9%E8%89%B2%E6%A8%A1%E5%9E%8B%E4%BA%A7%E4%B8%9A%E5%B7%A5%E5%85%B7"&gt;🎗️产业特色模型|产业工具&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E4%BA%A7%E4%B8%9A%E7%BA%A7%E9%83%A8%E7%BD%B2%E5%AE%9E%E8%B7%B5"&gt;💡🏆产业级部署实践&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E5%AE%89%E8%A3%85"&gt;🍱安装&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E6%95%99%E7%A8%8B"&gt;🔥教程&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#faq"&gt;🔑FAQ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E6%A8%A1%E5%9D%97%E7%BB%84%E4%BB%B6"&gt;🧩模块组件&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E6%A8%A1%E5%9E%8B%E5%BA%93"&gt;📱模型库&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%EF%B8%8F%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94"&gt;⚖️模型性能对比&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%EF%B8%8F%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94"&gt;🖥️服务器端模型性能对比&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%EF%B8%8F%E7%A7%BB%E5%8A%A8%E7%AB%AF%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94"&gt;⌚️移动端模型性能对比&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%EF%B8%8F%E4%BA%A7%E4%B8%9A%E7%89%B9%E8%89%B2%E6%A8%A1%E5%9E%8B%E4%BA%A7%E4%B8%9A%E5%B7%A5%E5%85%B7-1"&gt;🎗️产业特色模型|产业工具&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#pp-yoloe-%E9%AB%98%E7%B2%BE%E5%BA%A6%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B"&gt;💎PP-YOLOE 高精度目标检测模型&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#pp-yoloe-r-%E9%AB%98%E6%80%A7%E8%83%BD%E6%97%8B%E8%BD%AC%E6%A1%86%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B"&gt;💎PP-YOLOE-R 高性能旋转框检测模型&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#pp-yoloe-sod-%E9%AB%98%E7%B2%BE%E5%BA%A6%E5%B0%8F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B"&gt;💎PP-YOLOE-SOD 高精度小目标检测模型&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#pp-picodet-%E8%B6%85%E8%BD%BB%E9%87%8F%E5%AE%9E%E6%97%B6%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B"&gt;💫PP-PicoDet 超轻量实时目标检测模型&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#pp-tracking-%E5%AE%9E%E6%97%B6%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E7%B3%BB%E7%BB%9F"&gt;📡PP-Tracking 实时多目标跟踪系统&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%EF%B8%8Fpp-tinypose-%E4%BA%BA%E4%BD%93%E9%AA%A8%E9%AA%BC%E5%85%B3%E9%94%AE%E7%82%B9%E8%AF%86%E5%88%AB"&gt;⛷️PP-TinyPose 人体骨骼关键点识别&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#pp-human-%E5%AE%9E%E6%97%B6%E8%A1%8C%E4%BA%BA%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7"&gt;🏃🏻PP-Human 实时行人分析工具&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%EF%B8%8Fpp-vehicle-%E5%AE%9E%E6%97%B6%E8%BD%A6%E8%BE%86%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7"&gt;🏎️PP-Vehicle 实时车辆分析工具&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E4%BA%A7%E4%B8%9A%E5%AE%9E%E8%B7%B5%E8%8C%83%E4%BE%8B"&gt;💡产业实践范例&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E4%BC%81%E4%B8%9A%E5%BA%94%E7%94%A8%E6%A1%88%E4%BE%8B"&gt;🏆企业应用案例&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E8%AE%B8%E5%8F%AF%E8%AF%81%E4%B9%A6"&gt;📝许可证书&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E5%BC%95%E7%94%A8"&gt;📌引用&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🌈简介&lt;/h2&gt; 
&lt;p&gt;PaddleDetection是一个基于PaddlePaddle的目标检测端到端开发套件，在提供丰富的模型组件和测试基准的同时，注重端到端的产业落地应用，通过打造产业级特色模型|工具、建设产业应用范例等手段，帮助开发者实现数据准备、模型选型、模型训练、模型部署的全流程打通，快速进行落地应用。&lt;/p&gt; 
&lt;p&gt;主要模型效果示例如下（点击标题可快速跳转）：&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#pp-yoloe-%E9%AB%98%E7%B2%BE%E5%BA%A6%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B"&gt;&lt;strong&gt;通用目标检测&lt;/strong&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#pp-yoloe-sod-%E9%AB%98%E7%B2%BE%E5%BA%A6%E5%B0%8F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B"&gt;&lt;strong&gt;小目标检测&lt;/strong&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#pp-yoloe-r-%E9%AB%98%E6%80%A7%E8%83%BD%E6%97%8B%E8%BD%AC%E6%A1%86%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B"&gt;&lt;strong&gt;旋转框检测&lt;/strong&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://github.com/PaddlePaddle/Paddle3D"&gt;&lt;strong&gt;3D目标物检测&lt;/strong&gt;&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206095864-f174835d-4e9a-42f7-96b8-d684fc3a3687.png" height="126px" width="180px" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206095892-934be83a-f869-4a31-8e52-1074184149d1.jpg" height="126px" width="180px" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206111796-d9a9702a-c1a0-4647-b8e9-3e1307e9d34c.png" height="126px" width="180px" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206095622-cf6dbd26-5515-472f-9451-b39bbef5b1bf.gif" height="126px" width="180px" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E6%A8%A1%E5%9E%8B%E5%BA%93"&gt;&lt;strong&gt;人脸检测&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%EF%B8%8Fpp-tinypose-%E4%BA%BA%E4%BD%93%E9%AA%A8%E9%AA%BC%E5%85%B3%E9%94%AE%E7%82%B9%E8%AF%86%E5%88%AB"&gt;&lt;strong&gt;2D关键点检测&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#pp-tracking-%E5%AE%9E%E6%97%B6%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E7%B3%BB%E7%BB%9F"&gt;&lt;strong&gt;多目标追踪&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E6%A8%A1%E5%9E%8B%E5%BA%93"&gt;&lt;strong&gt;实例分割&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206095684-72f42233-c9c7-4bd8-9195-e34859bd08bf.jpg" height="126px" width="180px" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206100220-ab01d347-9ff9-4f17-9718-290ec14d4205.gif" height="126px" width="180px" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206111753-836e7827-968e-4c80-92ef-7a78766892fc.gif" height="126px" width="180px" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206095831-cc439557-1a23-4a99-b6b0-b6f2e97e8c57.jpg" height="126px" width="180px" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%EF%B8%8Fpp-vehicle-%E5%AE%9E%E6%97%B6%E8%BD%A6%E8%BE%86%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7"&gt;&lt;strong&gt;车辆分析——车牌识别&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%EF%B8%8Fpp-vehicle-%E5%AE%9E%E6%97%B6%E8%BD%A6%E8%BE%86%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7"&gt;&lt;strong&gt;车辆分析——车流统计&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%EF%B8%8Fpp-vehicle-%E5%AE%9E%E6%97%B6%E8%BD%A6%E8%BE%86%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7"&gt;&lt;strong&gt;车辆分析——违章检测&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%EF%B8%8Fpp-vehicle-%E5%AE%9E%E6%97%B6%E8%BD%A6%E8%BE%86%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7"&gt;&lt;strong&gt;车辆分析——属性分析&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206099328-2a1559e0-3b48-4424-9bad-d68f9ba5ba65.gif" height="126px" width="180px" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206095918-d0e7ad87-7bbb-40f1-bcc1-37844e2271ff.gif" height="126px" width="180px" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206100295-7762e1ab-ffce-44fb-b69d-45fb93657fa0.gif" height="126px" width="180px" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206095905-8255776a-d8e6-4af1-b6e9-8d9f97e5059d.gif" height="126px" width="180px" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#pp-human-%E5%AE%9E%E6%97%B6%E8%A1%8C%E4%BA%BA%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7"&gt;&lt;strong&gt;行人分析——闯入分析&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#pp-human-%E5%AE%9E%E6%97%B6%E8%A1%8C%E4%BA%BA%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7"&gt;&lt;strong&gt;行人分析——行为分析&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#pp-human-%E5%AE%9E%E6%97%B6%E8%A1%8C%E4%BA%BA%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7"&gt;&lt;strong&gt;行人分析——属性分析&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#pp-human-%E5%AE%9E%E6%97%B6%E8%A1%8C%E4%BA%BA%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7"&gt;&lt;strong&gt;行人分析——人流统计&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206095792-ae0ac107-cd8e-492a-8baa-32118fc82b04.gif" height="126px" width="180px" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206095778-fdd73e5d-9f91-48c7-9d3d-6f2e02ec3f79.gif" height="126px" width="180px" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206095709-2c3a209e-6626-45dd-be16-7f0bf4d48a14.gif" height="126px" width="180px" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/61035602/206113351-cc59df79-8672-4d76-b521-a15acf69ae78.gif" height="126px" width="180px" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;同时，PaddleDetection提供了模型的在线体验功能，用户可以选择自己的数据进行在线推理。&lt;/p&gt; 
&lt;h2&gt;📣最新进展&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;🔥2024.10.1 添加目标检测、实例分割领域一站式全流程开发能力&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;飞桨低代码开发工具PaddleX，依托于PaddleDetection的先进技术，支持了目标检测领域的&lt;strong&gt;一站式全流程&lt;/strong&gt;开发能力：&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;🎨 &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/paddlex/quick_start.md"&gt;&lt;strong&gt;模型丰富一键调用&lt;/strong&gt;&lt;/a&gt;：将通用目标检测、小目标检测和实例分割涉及的&lt;strong&gt;55个模型&lt;/strong&gt;整合为3条模型产线，通过极简的&lt;strong&gt;Python API一键调用&lt;/strong&gt;，快速体验模型效果。此外，同一套API，也支持图像分类、图像分割、文本图像智能分析、通用OCR、时序预测等共计&lt;strong&gt;200+模型&lt;/strong&gt;，形成20+单功能模块，方便开发者进行&lt;strong&gt;模型组合使用&lt;/strong&gt;。&lt;/li&gt; 
     &lt;li&gt;🚀 &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/paddlex/overview.md"&gt;&lt;strong&gt;提高效率降低门槛&lt;/strong&gt;&lt;/a&gt;：提供基于&lt;strong&gt;统一命令&lt;/strong&gt;和&lt;strong&gt;图形界面&lt;/strong&gt;两种方式，实现模型简洁高效的使用、组合与定制。支持&lt;strong&gt;高性能部署、服务化部署和端侧部署&lt;/strong&gt;等多种部署方式。此外，对于各种主流硬件如&lt;strong&gt;英伟达GPU、昆仑芯、昇腾、寒武纪和海光&lt;/strong&gt;等，进行模型开发时，都可以&lt;strong&gt;无缝切换&lt;/strong&gt;。&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;添加实例分割SOTA模型&lt;a href="https://github.com/PaddlePaddle/PaddleX/raw/release/3.0-beta1/docs/module_usage/tutorials/cv_modules/instance_segmentation.md"&gt;&lt;strong&gt;Mask-RT-DETR&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;🔥超越YOLOv8，飞桨推出精度最高的实时检测器RT-DETR！&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://github.com/PaddlePaddle/PaddleDetection/assets/17582080/196b0a10-d2e8-401c-9132-54b9126e0a33" height="500" caption="" /&gt; 
 &lt;p&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;RT-DETR解读文章传送门&lt;/code&gt;： 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/o03QM2rZNjHVto36gcV0Yw"&gt;《超越YOLOv8，飞桨推出精度最高的实时检测器RT-DETR！》&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;代码传送门&lt;/code&gt;：&lt;a href="https://github.com/PaddlePaddle/PaddleDetection/tree/develop/configs/rtdetr"&gt;RT-DETR&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/paddlex/quick_start.md"&gt;⚡️快速开始&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/paddlex/overview.md"&gt;🔥低代码全流程开发&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;👫开源社区&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;🏅️社区贡献：&lt;/strong&gt; PaddleDetection非常欢迎你加入到飞桨社区的开源建设中，参与贡献方式可以参考&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/contribution/README.md"&gt;开源项目开发指南&lt;/a&gt;。&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/community.md"&gt;&lt;strong&gt;🎈社区近期活动&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;✨主要特性&lt;/h2&gt; 
&lt;h4&gt;🧩模块化设计&lt;/h4&gt; 
&lt;p&gt;PaddleDetection将检测模型解耦成不同的模块组件，通过自定义模块组件组合，用户可以便捷高效地完成检测模型的搭建。&lt;code&gt;传送门&lt;/code&gt;：&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E6%A8%A1%E5%9D%97%E7%BB%84%E4%BB%B6"&gt;🧩模块组件&lt;/a&gt;。&lt;/p&gt; 
&lt;h4&gt;📱丰富的模型库&lt;/h4&gt; 
&lt;p&gt;PaddleDetection支持大量的最新主流的算法基准以及预训练模型，涵盖2D/3D目标检测、实例分割、人脸检测、关键点检测、多目标跟踪、半监督学习等方向。&lt;code&gt;传送门&lt;/code&gt;：&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E6%A8%A1%E5%9E%8B%E5%BA%93"&gt;📱模型库&lt;/a&gt;、&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%EF%B8%8F%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94"&gt;⚖️模型性能对比&lt;/a&gt;。&lt;/p&gt; 
&lt;h4&gt;🎗️产业特色模型|产业工具&lt;/h4&gt; 
&lt;p&gt;PaddleDetection打造产业级特色模型以及分析工具：PP-YOLOE+、PP-PicoDet、PP-TinyPose、PP-HumanV2、PP-Vehicle等，针对通用、高频垂类应用场景提供深度优化解决方案以及高度集成的分析工具，降低开发者的试错、选择成本，针对业务场景快速应用落地。&lt;code&gt;传送门&lt;/code&gt;：&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%EF%B8%8F%E4%BA%A7%E4%B8%9A%E7%89%B9%E8%89%B2%E6%A8%A1%E5%9E%8B%E4%BA%A7%E4%B8%9A%E5%B7%A5%E5%85%B7-1"&gt;🎗️产业特色模型|产业工具&lt;/a&gt;。&lt;/p&gt; 
&lt;h4&gt;💡🏆产业级部署实践&lt;/h4&gt; 
&lt;p&gt;PaddleDetection整理工业、农业、林业、交通、医疗、金融、能源电力等AI应用范例，打通数据标注-模型训练-模型调优-预测部署全流程，持续降低目标检测技术产业落地门槛。&lt;code&gt;传送门&lt;/code&gt;：&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E4%BA%A7%E4%B8%9A%E5%AE%9E%E8%B7%B5%E8%8C%83%E4%BE%8B"&gt;💡产业实践范例&lt;/a&gt;、&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E4%BC%81%E4%B8%9A%E5%BA%94%E7%94%A8%E6%A1%88%E4%BE%8B"&gt;🏆企业应用案例&lt;/a&gt;。&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p align="center"&gt; &lt;img src="https://user-images.githubusercontent.com/61035602/206431371-912a14c8-ce1e-48ec-ae6f-7267016b308e.png" align="middle" width="1280" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;🍱安装&lt;/h2&gt; 
&lt;p&gt;参考&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/tutorials/INSTALL_cn.md"&gt;安装说明&lt;/a&gt;进行安装。&lt;/p&gt; 
&lt;h2&gt;🔥教程&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;深度学习入门教程&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.paddlepaddle.org.cn/tutorials/projectdetail/4676538"&gt;零基础入门深度学习&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.baidu.com/aistudio/education/group/info/1617"&gt;零基础入门目标检测&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;快速开始&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/tutorials/QUICK_STARTED_cn.md"&gt;快速体验&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/tutorials/GETTING_STARTED_cn.md"&gt;示例：30分钟快速开发交通标志检测模型&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;数据准备&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/tutorials/data/README.md"&gt;数据准备&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/advanced_tutorials/READER.md"&gt;数据处理模块&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;配置文件说明&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/tutorials/config_annotation/faster_rcnn_r50_fpn_1x_coco_annotation.md"&gt;RCNN参数说明&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/tutorials/config_annotation/ppyolo_r50vd_dcn_1x_coco_annotation.md"&gt;PP-YOLO参数说明&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;模型开发&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/advanced_tutorials/MODEL_TECHNICAL.md"&gt;新增检测模型&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;二次开发 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/advanced_tutorials/customization/detection.md"&gt;目标检测&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/advanced_tutorials/customization/keypoint_detection.md"&gt;关键点检测&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/advanced_tutorials/customization/pphuman_mot.md"&gt;多目标跟踪&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/advanced_tutorials/customization/action_recognotion/"&gt;行为识别&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/advanced_tutorials/customization/pphuman_attribute.md"&gt;属性识别&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;部署推理&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/deploy/EXPORT_MODEL.md"&gt;模型导出教程&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleSlim"&gt;模型压缩&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/slim"&gt;剪裁/量化/蒸馏教程&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/deploy/README.md"&gt;Paddle Inference部署&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/deploy/python"&gt;Python端推理部署&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/deploy/cpp"&gt;C++端推理部署&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/deploy/lite"&gt;Paddle Lite部署&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/deploy/serving"&gt;Paddle Serving部署&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/deploy/EXPORT_ONNX_MODEL.md"&gt;ONNX模型导出&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/deploy/BENCHMARK_INFER.md"&gt;推理benchmark&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🔑FAQ&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/tutorials/FAQ"&gt;FAQ/常见问题汇总&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🧩模块组件&lt;/h2&gt; 
&lt;table align="center"&gt; 
 &lt;tbody&gt; 
  &lt;tr align="center" valign="center"&gt; 
   &lt;td&gt; &lt;b&gt;Backbones&lt;/b&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;b&gt;Necks&lt;/b&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;b&gt;Loss&lt;/b&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;b&gt;Common&lt;/b&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;b&gt;Data Augmentation&lt;/b&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr valign="top"&gt; 
   &lt;td&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/resnet.py"&gt;ResNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/res2net.py"&gt;CSPResNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/senet.py"&gt;SENet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/res2net.py"&gt;Res2Net&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/hrnet.py"&gt;HRNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/lite_hrnet.py"&gt;Lite-HRNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/darknet.py"&gt;DarkNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/csp_darknet.py"&gt;CSPDarkNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/mobilenet_v1.py"&gt;MobileNetV1&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/mobilenet_v3.py"&gt;MobileNetV1&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/shufflenet_v2.py"&gt;ShuffleNetV2&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/ghostnet.py"&gt;GhostNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/blazenet.py"&gt;BlazeNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/dla.py"&gt;DLA&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/hardnet.py"&gt;HardNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/lcnet.py"&gt;LCNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/esnet.py"&gt;ESNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/swin_transformer.py"&gt;Swin-Transformer&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/convnext.py"&gt;ConvNeXt&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/vgg.py"&gt;VGG&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/vision_transformer.py"&gt;Vision Transformer&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/convnext"&gt;ConvNext&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
   &lt;td&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/necks/bifpn.py"&gt;BiFPN&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/necks/blazeface_fpn.py"&gt;BlazeFace-FPN&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/necks/centernet_fpn.py"&gt;CenterNet-FPN&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/necks/csp_pan.py"&gt;CSP-PAN&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/necks/custom_pan.py"&gt;Custom-PAN&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/necks/fpn.py"&gt;FPN&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/necks/es_pan.py"&gt;ES-PAN&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/necks/hrfpn.py"&gt;HRFPN&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/necks/lc_pan.py"&gt;LC-PAN&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/necks/ttf_fpn.py"&gt;TTF-FPN&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/necks/yolo_fpn.py"&gt;YOLO-FPN&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
   &lt;td&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/smooth_l1_loss.py"&gt;Smooth-L1&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/detr_loss.py"&gt;Detr Loss&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/fairmot_loss.py"&gt;Fairmot Loss&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/fcos_loss.py"&gt;Fcos Loss&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/gfocal_loss.py"&gt;GFocal Loss&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/jde_loss.py"&gt;JDE Loss&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/keypoint_loss.py"&gt;KeyPoint Loss&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/solov2_loss.py"&gt;SoloV2 Loss&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/focal_loss.py"&gt;Focal Loss&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/iou_loss.py"&gt;GIoU/DIoU/CIoU&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/iou_aware_loss.py"&gt;IoUAware&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/sparsercnn_loss.py"&gt;SparseRCNN Loss&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/ssd_loss.py"&gt;SSD Loss&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/focal_loss.py"&gt;YOLO Loss&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/yolo_loss.py"&gt;CT Focal Loss&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/losses/varifocal_loss.py"&gt;VariFocal Loss&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
   &lt;td&gt;  &lt;li&gt;&lt;b&gt;Post-processing&lt;/b&gt;&lt;/li&gt; 
    &lt;ul&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/post_process.py"&gt;SoftNMS&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/post_process.py"&gt;MatrixNMS&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; 
    &lt;/ul&gt; &lt;li&gt;&lt;b&gt;Training&lt;/b&gt;&lt;/li&gt; 
    &lt;ul&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/tools/train.py#L62"&gt;FP16 training&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/tutorials/DistributedTraining_cn.md"&gt;Multi-machine training &lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; 
    &lt;/ul&gt; &lt;li&gt;&lt;b&gt;Common&lt;/b&gt;&lt;/li&gt; 
    &lt;ul&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/backbones/resnet.py#L41"&gt;Sync-BN&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/gn/README.md"&gt;Group Norm&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/dcn/README.md"&gt;DCNv2&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/optimizer/ema.py"&gt;EMA&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; 
    &lt;/ul&gt;&lt;/td&gt; 
   &lt;td&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/data/transform/operators.py"&gt;Resize&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/data/transform/operators.py"&gt;Lighting&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/data/transform/operators.py"&gt;Flipping&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/data/transform/operators.py"&gt;Expand&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/data/transform/operators.py"&gt;Crop&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/data/transform/operators.py"&gt;Color Distort&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/data/transform/operators.py"&gt;Random Erasing&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/data/transform/operators.py"&gt;Mixup &lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/data/transform/operators.py"&gt;AugmentHSV&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/data/transform/operators.py"&gt;Mosaic&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/data/transform/operators.py"&gt;Cutmix &lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/data/transform/operators.py"&gt;Grid Mask&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/data/transform/operators.py"&gt;Auto Augment&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/data/transform/operators.py"&gt;Random Perspective&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
  &lt;/tr&gt;   
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;📱模型库&lt;/h2&gt; 
&lt;table align="center"&gt; 
 &lt;tbody&gt; 
  &lt;tr align="center" valign="center"&gt; 
   &lt;td&gt; &lt;b&gt;2D Detection&lt;/b&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;b&gt;Multi Object Tracking&lt;/b&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;b&gt;KeyPoint Detection&lt;/b&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;b&gt;Others&lt;/b&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr valign="top"&gt; 
   &lt;td&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/faster_rcnn/README.md"&gt;Faster RCNN&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/ppdet/modeling/necks/fpn.py"&gt;FPN&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/cascade_rcnn/README.md"&gt;Cascade-RCNN&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/rcnn_enhance"&gt;PSS-Det&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/retinanet/README.md"&gt;RetinaNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/yolov3/README.md"&gt;YOLOv3&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/yolof/README.md"&gt;YOLOF&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/yolox/README.md"&gt;YOLOX&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleYOLO/tree/develop/configs/yolov5"&gt;YOLOv5&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleYOLO/tree/develop/configs/yolov6"&gt;YOLOv6&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleYOLO/tree/develop/configs/yolov7"&gt;YOLOv7&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleYOLO/tree/develop/configs/yolov8"&gt;YOLOv8&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleYOLO/tree/develop/configs/rtmdet"&gt;RTMDet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ppyolo/README_cn.md"&gt;PP-YOLO&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ppyolo#pp-yolo-tiny"&gt;PP-YOLO-Tiny&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/picodet"&gt;PP-PicoDet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ppyolo/README_cn.md"&gt;PP-YOLOv2&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ppyoloe/README_legacy.md"&gt;PP-YOLOE&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ppyoloe/README_cn.md"&gt;PP-YOLOE+&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/smalldet"&gt;PP-YOLOE-SOD&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/rotate/README.md"&gt;PP-YOLOE-R&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ssd/README.md"&gt;SSD&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/centernet"&gt;CenterNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/fcos"&gt;FCOS&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/rotate/fcosr"&gt;FCOSR&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ttfnet"&gt;TTFNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/tood"&gt;TOOD&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/gfl"&gt;GFL&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/gfl/gflv2_r50_fpn_1x_coco.yml"&gt;GFLv2&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/detr"&gt;DETR&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/deformable_detr"&gt;Deformable DETR&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/sparse_rcnn"&gt;Sparse RCNN&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
   &lt;td&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/mot/jde"&gt;JDE&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/mot/fairmot"&gt;FairMOT&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/mot/deepsort"&gt;DeepSORT&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/mot/bytetrack"&gt;ByteTrack&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/mot/ocsort"&gt;OC-SORT&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/mot/botsort"&gt;BoT-SORT&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/mot/centertrack"&gt;CenterTrack&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
   &lt;td&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/keypoint/hrnet"&gt;HRNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/keypoint/higherhrnet"&gt;HigherHRNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/keypoint/lite_hrnet"&gt;Lite-HRNet&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/keypoint/tiny_pose"&gt;PP-TinyPose&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
   &lt;td&gt;  &lt;li&gt;&lt;b&gt;Instance Segmentation&lt;/b&gt;&lt;/li&gt; 
    &lt;ul&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/mask_rcnn"&gt;Mask RCNN&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/cascade_rcnn"&gt;Cascade Mask RCNN&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/solov2"&gt;SOLOv2&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; 
    &lt;/ul&gt; &lt;li&gt;&lt;b&gt;Face Detection&lt;/b&gt;&lt;/li&gt; 
    &lt;ul&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/face_detection"&gt;BlazeFace&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; 
    &lt;/ul&gt; &lt;li&gt;&lt;b&gt;Semi-Supervised Detection&lt;/b&gt;&lt;/li&gt; 
    &lt;ul&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/semi_det"&gt;DenseTeacher&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; 
    &lt;/ul&gt; &lt;li&gt;&lt;b&gt;3D Detection&lt;/b&gt;&lt;/li&gt; 
    &lt;ul&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/Paddle3D"&gt;Smoke&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/Paddle3D"&gt;CaDDN&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/Paddle3D"&gt;PointPillars&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/Paddle3D"&gt;CenterPoint&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/Paddle3D"&gt;SequeezeSegV3&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/Paddle3D"&gt;IA-SSD&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/Paddle3D"&gt;PETR&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; 
    &lt;/ul&gt; &lt;li&gt;&lt;b&gt;Vehicle Analysis Toolbox&lt;/b&gt;&lt;/li&gt; 
    &lt;ul&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/deploy/pipeline/README.md"&gt;PP-Vehicle&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; 
    &lt;/ul&gt; &lt;li&gt;&lt;b&gt;Human Analysis Toolbox&lt;/b&gt;&lt;/li&gt; 
    &lt;ul&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/deploy/pipeline/README.md"&gt;PP-Human&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/deploy/pipeline/README.md"&gt;PP-HumanV2&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; 
    &lt;/ul&gt; &lt;li&gt;&lt;b&gt;Sport Analysis Toolbox&lt;/b&gt;&lt;/li&gt; 
    &lt;ul&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleSports"&gt;PP-Sports&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; 
    &lt;/ul&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;⚖️模型性能对比&lt;/h2&gt; 
&lt;h4&gt;🖥️服务器端模型性能对比&lt;/h4&gt; 
&lt;p&gt;各模型结构和骨干网络的代表模型在COCO数据集上精度mAP和单卡Tesla V100上预测速度(FPS)对比图。&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://user-images.githubusercontent.com/61035602/206434766-caaa781b-b922-481f-af09-15faac9ed33b.png" width="800" /&gt; 
&lt;/div&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; 测试说明(点击展开)&lt;/b&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ViT为ViT-Cascade-Faster-RCNN模型，COCO数据集mAP高达55.7%&lt;/li&gt; 
  &lt;li&gt;Cascade-Faster-RCNN为Cascade-Faster-RCNN-ResNet50vd-DCN，PaddleDetection将其优化到COCO数据mAP为47.8%时推理速度为20FPS&lt;/li&gt; 
  &lt;li&gt;PP-YOLOE是对PP-YOLO v2模型的进一步优化，L版本在COCO数据集mAP为51.6%，Tesla V100预测速度78.1FPS&lt;/li&gt; 
  &lt;li&gt;PP-YOLOE+是对PPOLOE模型的进一步优化，L版本在COCO数据集mAP为53.3%，Tesla V100预测速度78.1FPS&lt;/li&gt; 
  &lt;li&gt;YOLOX和YOLOv5均为基于PaddleDetection复现算法，YOLOv5代码在&lt;a href="https://github.com/PaddlePaddle/PaddleYOLO"&gt;PaddleYOLO&lt;/a&gt;中，参照&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/docs/feature_models/PaddleYOLO_MODEL.md"&gt;PaddleYOLO_MODEL&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;图中模型均可在&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E6%A8%A1%E5%9E%8B%E5%BA%93"&gt;📱模型库&lt;/a&gt;中获取&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h4&gt;⌚️移动端模型性能对比&lt;/h4&gt; 
&lt;p&gt;各移动端模型在COCO数据集上精度mAP和高通骁龙865处理器上预测速度(FPS)对比图。&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://user-images.githubusercontent.com/61035602/206434741-10460690-8fc3-4084-a11a-16fe4ce2fc85.png" width="550" /&gt; 
&lt;/div&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; 测试说明(点击展开)&lt;/b&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;测试数据均使用高通骁龙865(4xA77+4xA55)处理器，batch size为1, 开启4线程测试，测试使用NCNN预测库，测试脚本见&lt;a href="https://github.com/JiweiMaster/MobileDetBenchmark"&gt;MobileDetBenchmark&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;PP-PicoDet及PP-YOLO-Tiny为PaddleDetection自研模型，可在&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/#%E6%A8%A1%E5%9E%8B%E5%BA%93"&gt;📱模型库&lt;/a&gt;中获取，其余模型PaddleDetection暂未提供&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;🎗️产业特色模型|产业工具&lt;/h2&gt; 
&lt;p&gt;产业特色模型｜产业工具是PaddleDetection针对产业高频应用场景打造的兼顾精度和速度的模型以及工具箱，注重从数据处理-模型训练-模型调优-模型部署的端到端打通，且提供了实际生产环境中的实践范例代码，帮助拥有类似需求的开发者高效的完成产品开发落地应用。&lt;/p&gt; 
&lt;p&gt;该系列模型｜工具均已PP前缀命名，具体介绍、预训练模型以及产业实践范例代码如下。&lt;/p&gt; 
&lt;h3&gt;💎PP-YOLOE 高精度目标检测模型&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; 简介(点击展开)&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;PP-YOLOE是基于PP-YOLOv2的卓越的单阶段Anchor-free模型，超越了多种流行的YOLO模型。PP-YOLOE避免了使用诸如Deformable Convolution或者Matrix NMS之类的特殊算子，以使其能轻松地部署在多种多样的硬件上。其使用大规模数据集obj365预训练模型进行预训练，可以在不同场景数据集上快速调优收敛。&lt;/p&gt; 
 &lt;p&gt;&lt;code&gt;传送门&lt;/code&gt;：&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ppyoloe/README_cn.md"&gt;PP-YOLOE说明&lt;/a&gt;。&lt;/p&gt; 
 &lt;p&gt;&lt;code&gt;传送门&lt;/code&gt;：&lt;a href="https://arxiv.org/abs/2203.16250"&gt;arXiv论文&lt;/a&gt;。&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; 预训练模型(点击展开)&lt;/b&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;模型名称&lt;/th&gt; 
    &lt;th align="center"&gt;COCO精度（mAP）&lt;/th&gt; 
    &lt;th align="center"&gt;V100 TensorRT FP16速度(FPS)&lt;/th&gt; 
    &lt;th align="center"&gt;推荐部署硬件&lt;/th&gt; 
    &lt;th align="center"&gt;配置文件&lt;/th&gt; 
    &lt;th align="center"&gt;模型下载&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;PP-YOLOE+_l&lt;/td&gt; 
    &lt;td align="center"&gt;53.3&lt;/td&gt; 
    &lt;td align="center"&gt;149.2&lt;/td&gt; 
    &lt;td align="center"&gt;服务器&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ppyoloe/ppyoloe_plus_crn_l_80e_coco.yml"&gt;链接&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://paddledet.bj.bcebos.com/models/ppyoloe_plus_crn_m_80e_coco.pdparams"&gt;下载地址&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;code&gt;传送门&lt;/code&gt;：&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ppyoloe/README_cn.md"&gt;全部预训练模型&lt;/a&gt;。&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; 产业应用代码示例(点击展开)&lt;/b&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;行业&lt;/th&gt; 
    &lt;th&gt;类别&lt;/th&gt; 
    &lt;th&gt;亮点&lt;/th&gt; 
    &lt;th&gt;文档说明&lt;/th&gt; 
    &lt;th&gt;模型下载&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;农业&lt;/td&gt; 
    &lt;td&gt;农作物检测&lt;/td&gt; 
    &lt;td&gt;用于葡萄栽培中基于图像的监测和现场机器人技术，提供了来自5种不同葡萄品种的实地实例&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ppyoloe/application/README.md"&gt;PP-YOLOE+ 下游任务&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ppyoloe/application/README.md"&gt;下载链接&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;通用&lt;/td&gt; 
    &lt;td&gt;低光场景检测&lt;/td&gt; 
    &lt;td&gt;低光数据集使用ExDark，包括从极低光环境到暮光环境等10种不同光照条件下的图片。&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ppyoloe/application/README.md"&gt;PP-YOLOE+ 下游任务&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ppyoloe/application/README.md"&gt;下载链接&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;工业&lt;/td&gt; 
    &lt;td&gt;PCB电路板瑕疵检测&lt;/td&gt; 
    &lt;td&gt;工业数据集使用PKU-Market-PCB，该数据集用于印刷电路板（PCB）的瑕疵检测，提供了6种常见的PCB缺陷&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ppyoloe/application/README.md"&gt;PP-YOLOE+ 下游任务&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/ppyoloe/application/README.md"&gt;下载链接&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;💎PP-YOLOE-R 高性能旋转框检测模型&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; 简介(点击展开)&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;PP-YOLOE-R是一个高效的单阶段Anchor-free旋转框检测模型，基于PP-YOLOE+引入了一系列改进策略来提升检测精度。根据不同的硬件对精度和速度的要求，PP-YOLOE-R包含s/m/l/x四个尺寸的模型。在DOTA 1.0数据集上，PP-YOLOE-R-l和PP-YOLOE-R-x在单尺度训练和测试的情况下分别达到了78.14mAP和78.28 mAP，这在单尺度评估下超越了几乎所有的旋转框检测模型。通过多尺度训练和测试，PP-YOLOE-R-l和PP-YOLOE-R-x的检测精度进一步提升至80.02mAP和80.73 mAP，超越了所有的Anchor-free方法并且和最先进的Anchor-based的两阶段模型精度几乎相当。在保持高精度的同时，PP-YOLOE-R避免使用特殊的算子，例如Deformable Convolution或Rotated RoI Align，使其能轻松地部署在多种多样的硬件上。&lt;/p&gt; 
 &lt;p&gt;&lt;code&gt;传送门&lt;/code&gt;：&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/rotate/ppyoloe_r"&gt;PP-YOLOE-R说明&lt;/a&gt;。&lt;/p&gt; 
 &lt;p&gt;&lt;code&gt;传送门&lt;/code&gt;：&lt;a href="https://arxiv.org/abs/2211.02386"&gt;arXiv论文&lt;/a&gt;。&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; 预训练模型(点击展开)&lt;/b&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;模型&lt;/th&gt; 
    &lt;th align="center"&gt;Backbone&lt;/th&gt; 
    &lt;th align="center"&gt;mAP&lt;/th&gt; 
    &lt;th align="center"&gt;V100 TRT FP16 (FPS)&lt;/th&gt; 
    &lt;th align="center"&gt;RTX 2080 Ti TRT FP16 (FPS)&lt;/th&gt; 
    &lt;th align="center"&gt;Params (M)&lt;/th&gt; 
    &lt;th align="center"&gt;FLOPs (G)&lt;/th&gt; 
    &lt;th align="center"&gt;学习率策略&lt;/th&gt; 
    &lt;th align="center"&gt;角度表示&lt;/th&gt; 
    &lt;th align="center"&gt;数据增广&lt;/th&gt; 
    &lt;th align="center"&gt;GPU数目&lt;/th&gt; 
    &lt;th align="center"&gt;每GPU图片数目&lt;/th&gt; 
    &lt;th align="center"&gt;模型下载&lt;/th&gt; 
    &lt;th align="center"&gt;配置文件&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;PP-YOLOE-R-l&lt;/td&gt; 
    &lt;td align="center"&gt;CRN-l&lt;/td&gt; 
    &lt;td align="center"&gt;80.02&lt;/td&gt; 
    &lt;td align="center"&gt;69.7&lt;/td&gt; 
    &lt;td align="center"&gt;48.3&lt;/td&gt; 
    &lt;td align="center"&gt;53.29&lt;/td&gt; 
    &lt;td align="center"&gt;281.65&lt;/td&gt; 
    &lt;td align="center"&gt;3x&lt;/td&gt; 
    &lt;td align="center"&gt;oc&lt;/td&gt; 
    &lt;td align="center"&gt;MS+RR&lt;/td&gt; 
    &lt;td align="center"&gt;4&lt;/td&gt; 
    &lt;td align="center"&gt;2&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://paddledet.bj.bcebos.com/models/ppyoloe_r_crn_l_3x_dota_ms.pdparams"&gt;model&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleDetection/tree/develop/configs/rotate/ppyoloe_r/ppyoloe_r_crn_l_3x_dota_ms.yml"&gt;config&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;code&gt;传送门&lt;/code&gt;：&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/rotate/ppyoloe_r"&gt;全部预训练模型&lt;/a&gt;。&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; 产业应用代码示例(点击展开)&lt;/b&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;行业&lt;/th&gt; 
    &lt;th&gt;类别&lt;/th&gt; 
    &lt;th&gt;亮点&lt;/th&gt; 
    &lt;th&gt;文档说明&lt;/th&gt; 
    &lt;th&gt;模型下载&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;通用&lt;/td&gt; 
    &lt;td&gt;旋转框检测&lt;/td&gt; 
    &lt;td&gt;手把手教你上手PP-YOLOE-R旋转框检测，10分钟将脊柱数据集精度训练至95mAP&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/5058293"&gt;基于PP-YOLOE-R的旋转框检测&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/5058293"&gt;下载链接&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;💎PP-YOLOE-SOD 高精度小目标检测模型&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; 简介(点击展开)&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;PP-YOLOE-SOD(Small Object Detection)是PaddleDetection团队针对小目标检测提出的检测方案，在VisDrone-DET数据集上单模型精度达到38.5mAP，达到了SOTA性能。其分别基于切图拼图流程优化的小目标检测方案以及基于原图模型算法优化的小目标检测方案。同时提供了数据集自动分析脚本，只需输入数据集标注文件，便可得到数据集统计结果，辅助判断数据集是否是小目标数据集以及是否需要采用切图策略，同时给出网络超参数参考值。&lt;/p&gt; 
 &lt;p&gt;&lt;code&gt;传送门&lt;/code&gt;：&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/smalldet"&gt;PP-YOLOE-SOD 小目标检测模型&lt;/a&gt;。&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; 预训练模型(点击展开)&lt;/b&gt;&lt;/summary&gt; - VisDrone数据集预训练模型 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;模型&lt;/th&gt; 
    &lt;th align="center"&gt;COCOAPI mAP&lt;sup&gt;val&lt;br /&gt;0.5:0.95&lt;/sup&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;COCOAPI mAP&lt;sup&gt;val&lt;br /&gt;0.5&lt;/sup&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;COCOAPI mAP&lt;sup&gt;test_dev&lt;br /&gt;0.5:0.95&lt;/sup&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;COCOAPI mAP&lt;sup&gt;test_dev&lt;br /&gt;0.5&lt;/sup&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;MatlabAPI mAP&lt;sup&gt;test_dev&lt;br /&gt;0.5:0.95&lt;/sup&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;MatlabAPI mAP&lt;sup&gt;test_dev&lt;br /&gt;0.5&lt;/sup&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;下载&lt;/th&gt; 
    &lt;th align="center"&gt;配置文件&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;strong&gt;PP-YOLOE+_SOD-l&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;31.9&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;52.1&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;25.6&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;43.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;30.25&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;51.18&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://paddledet.bj.bcebos.com/models/ppyoloe_plus_sod_crn_l_80e_visdrone.pdparams"&gt;下载链接&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/visdrone/ppyoloe_plus_sod_crn_l_80e_visdrone.yml"&gt;配置文件&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;code&gt;传送门&lt;/code&gt;：&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/smalldet"&gt;全部预训练模型&lt;/a&gt;。&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; 产业应用代码示例(点击展开)&lt;/b&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;行业&lt;/th&gt; 
    &lt;th&gt;类别&lt;/th&gt; 
    &lt;th&gt;亮点&lt;/th&gt; 
    &lt;th&gt;文档说明&lt;/th&gt; 
    &lt;th&gt;模型下载&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;通用&lt;/td&gt; 
    &lt;td&gt;小目标检测&lt;/td&gt; 
    &lt;td&gt;基于PP-YOLOE-SOD的无人机航拍图像检测案例全流程实操。&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/5036782"&gt;基于PP-YOLOE-SOD的无人机航拍图像检测&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/5036782"&gt;下载链接&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;💫PP-PicoDet 超轻量实时目标检测模型&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; 简介(点击展开)&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;全新的轻量级系列模型PP-PicoDet，在移动端具有卓越的性能，成为全新SOTA轻量级模型。&lt;/p&gt; 
 &lt;p&gt;&lt;code&gt;传送门&lt;/code&gt;：&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/picodet/README.md"&gt;PP-PicoDet说明&lt;/a&gt;。&lt;/p&gt; 
 &lt;p&gt;&lt;code&gt;传送门&lt;/code&gt;：&lt;a href="https://arxiv.org/abs/2111.00902"&gt;arXiv论文&lt;/a&gt;。&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; 预训练模型(点击展开)&lt;/b&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;模型名称&lt;/th&gt; 
    &lt;th align="center"&gt;COCO精度（mAP）&lt;/th&gt; 
    &lt;th align="center"&gt;骁龙865 四线程速度(FPS)&lt;/th&gt; 
    &lt;th align="center"&gt;推荐部署硬件&lt;/th&gt; 
    &lt;th align="center"&gt;配置文件&lt;/th&gt; 
    &lt;th align="center"&gt;模型下载&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;PicoDet-L&lt;/td&gt; 
    &lt;td align="center"&gt;36.1&lt;/td&gt; 
    &lt;td align="center"&gt;39.7&lt;/td&gt; 
    &lt;td align="center"&gt;移动端、嵌入式&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/picodet/picodet_l_320_coco_lcnet.yml"&gt;链接&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://paddledet.bj.bcebos.com/models/picodet_l_320_coco_lcnet.pdparams"&gt;下载地址&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;code&gt;传送门&lt;/code&gt;：&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/picodet/README.md"&gt;全部预训练模型&lt;/a&gt;。&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; 产业应用代码示例(点击展开)&lt;/b&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;行业&lt;/th&gt; 
    &lt;th&gt;类别&lt;/th&gt; 
    &lt;th&gt;亮点&lt;/th&gt; 
    &lt;th&gt;文档说明&lt;/th&gt; 
    &lt;th&gt;模型下载&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;智慧城市&lt;/td&gt; 
    &lt;td&gt;道路垃圾检测&lt;/td&gt; 
    &lt;td&gt;通过在市政环卫车辆上安装摄像头对路面垃圾检测并分析，实现对路面遗撒的垃圾进行监控，记录并通知环卫人员清理，大大提升了环卫人效。&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/3846170?channelType=0&amp;amp;channel=0"&gt;基于PP-PicoDet的路面垃圾检测&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/3846170?channelType=0&amp;amp;channel=0"&gt;下载链接&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;📡PP-Tracking 实时多目标跟踪系统&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; 简介(点击展开)&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;PaddleDetection团队提供了实时多目标跟踪系统PP-Tracking，是基于PaddlePaddle深度学习框架的业界首个开源的实时多目标跟踪系统，具有模型丰富、应用广泛和部署高效三大优势。 PP-Tracking支持单镜头跟踪(MOT)和跨镜头跟踪(MTMCT)两种模式，针对实际业务的难点和痛点，提供了行人跟踪、车辆跟踪、多类别跟踪、小目标跟踪、流量统计以及跨镜头跟踪等各种多目标跟踪功能和应用，部署方式支持API调用和GUI可视化界面，部署语言支持Python和C++，部署平台环境支持Linux、NVIDIA Jetson等。&lt;/p&gt; 
 &lt;p&gt;&lt;code&gt;传送门&lt;/code&gt;：&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/mot/README.md"&gt;PP-Tracking说明&lt;/a&gt;。&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; 预训练模型(点击展开)&lt;/b&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;模型名称&lt;/th&gt; 
    &lt;th align="center"&gt;模型简介&lt;/th&gt; 
    &lt;th align="center"&gt;精度&lt;/th&gt; 
    &lt;th align="center"&gt;速度(FPS)&lt;/th&gt; 
    &lt;th align="center"&gt;推荐部署硬件&lt;/th&gt; 
    &lt;th align="center"&gt;配置文件&lt;/th&gt; 
    &lt;th align="center"&gt;模型下载&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;ByteTrack&lt;/td&gt; 
    &lt;td align="center"&gt;SDE多目标跟踪算法 仅包含检测模型&lt;/td&gt; 
    &lt;td align="center"&gt;MOT-17 test: 78.4&lt;/td&gt; 
    &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;td align="center"&gt;服务器、移动端、嵌入式&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/mot/bytetrack/bytetrack_yolox.yml"&gt;链接&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/mot/yolox_x_24e_800x1440_mix_det.pdparams"&gt;下载地址&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;FairMOT&lt;/td&gt; 
    &lt;td align="center"&gt;JDE多目标跟踪算法 多任务联合学习方法&lt;/td&gt; 
    &lt;td align="center"&gt;MOT-16 test: 75.0&lt;/td&gt; 
    &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;td align="center"&gt;服务器、移动端、嵌入式&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/mot/fairmot/fairmot_dla34_30e_1088x608.yml"&gt;链接&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://paddledet.bj.bcebos.com/models/mot/fairmot_dla34_30e_1088x608.pdparams"&gt;下载地址&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;OC-SORT&lt;/td&gt; 
    &lt;td align="center"&gt;SDE多目标跟踪算法 仅包含检测模型&lt;/td&gt; 
    &lt;td align="center"&gt;MOT-17 half val: 75.5&lt;/td&gt; 
    &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;td align="center"&gt;服务器、移动端、嵌入式&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/mot/ocsort/ocsort_yolox.yml"&gt;链接&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/mot/yolox_x_24e_800x1440_mix_mot_ch.pdparams"&gt;下载地址&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; 产业应用代码示例(点击展开)&lt;/b&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;行业&lt;/th&gt; 
    &lt;th&gt;类别&lt;/th&gt; 
    &lt;th&gt;亮点&lt;/th&gt; 
    &lt;th&gt;文档说明&lt;/th&gt; 
    &lt;th&gt;模型下载&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;通用&lt;/td&gt; 
    &lt;td&gt;多目标跟踪&lt;/td&gt; 
    &lt;td&gt;快速上手单镜头、多镜头跟踪&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/3022582"&gt;PP-Tracking之手把手玩转多目标跟踪&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/3022582"&gt;下载链接&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;⛷️PP-TinyPose 人体骨骼关键点识别&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; 简介(点击展开)&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;PaddleDetection 中的关键点检测部分紧跟最先进的算法，包括 Top-Down 和 Bottom-Up 两种方法，可以满足用户的不同需求。同时，PaddleDetection 提供针对移动端设备优化的自研实时关键点检测模型 PP-TinyPose。&lt;/p&gt; 
 &lt;p&gt;&lt;code&gt;传送门&lt;/code&gt;：&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/keypoint/tiny_pose"&gt;PP-TinyPose说明&lt;/a&gt;。&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; 预训练模型(点击展开)&lt;/b&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;模型名称&lt;/th&gt; 
    &lt;th align="center"&gt;模型简介&lt;/th&gt; 
    &lt;th align="center"&gt;COCO精度（AP）&lt;/th&gt; 
    &lt;th align="center"&gt;速度(FPS)&lt;/th&gt; 
    &lt;th align="center"&gt;推荐部署硬件&lt;/th&gt; 
    &lt;th align="center"&gt;配置文件&lt;/th&gt; 
    &lt;th align="center"&gt;模型下载&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;PP-TinyPose&lt;/td&gt; 
    &lt;td align="center"&gt;轻量级关键点算法&lt;br /&gt;输入尺寸256x192&lt;/td&gt; 
    &lt;td align="center"&gt;68.8&lt;/td&gt; 
    &lt;td align="center"&gt;骁龙865 四线程: 158.7 FPS&lt;/td&gt; 
    &lt;td align="center"&gt;移动端、嵌入式&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/keypoint/tiny_pose/tinypose_256x192.yml"&gt;链接&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/keypoint/tinypose_256x192.pdparams"&gt;下载地址&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;code&gt;传送门&lt;/code&gt;：&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/configs/keypoint/README.md"&gt;全部预训练模型&lt;/a&gt;。&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; 产业应用代码示例(点击展开)&lt;/b&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;行业&lt;/th&gt; 
    &lt;th&gt;类别&lt;/th&gt; 
    &lt;th&gt;亮点&lt;/th&gt; 
    &lt;th&gt;文档说明&lt;/th&gt; 
    &lt;th&gt;模型下载&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;运动&lt;/td&gt; 
    &lt;td&gt;健身&lt;/td&gt; 
    &lt;td&gt;提供从模型选型、数据准备、模型训练优化，到后处理逻辑和模型部署的全流程可复用方案，有效解决了复杂健身动作的高效识别，打造AI虚拟健身教练！&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4385813"&gt;基于PP-TinyPose增强版的智能健身动作识别&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4385813"&gt;下载链接&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;🏃🏻PP-Human 实时行人分析工具&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; 简介(点击展开)&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;PaddleDetection深入探索核心行业的高频场景，提供了行人开箱即用分析工具，支持图片/单镜头视频/多镜头视频/在线视频流多种输入方式，广泛应用于智慧交通、智慧城市、工业巡检等领域。支持服务器端部署及TensorRT加速，T4服务器上可达到实时。 PP-Human支持四大产业级功能：五大异常行为识别、26种人体属性分析、实时人流计数、跨镜头（ReID）跟踪。&lt;/p&gt; 
 &lt;p&gt;&lt;code&gt;传送门&lt;/code&gt;：&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/deploy/pipeline/README.md"&gt;PP-Human行人分析工具使用指南&lt;/a&gt;。&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; 预训练模型(点击展开)&lt;/b&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;任务&lt;/th&gt; 
    &lt;th align="center"&gt;T4 TensorRT FP16: 速度（FPS）&lt;/th&gt; 
    &lt;th align="center"&gt;推荐部署硬件&lt;/th&gt; 
    &lt;th align="center"&gt;模型下载&lt;/th&gt; 
    &lt;th align="center"&gt;模型体积&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;行人检测（高精度）&lt;/td&gt; 
    &lt;td align="center"&gt;39.8&lt;/td&gt; 
    &lt;td align="center"&gt;服务器&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip"&gt;目标检测&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;182M&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;行人跟踪（高精度）&lt;/td&gt; 
    &lt;td align="center"&gt;31.4&lt;/td&gt; 
    &lt;td align="center"&gt;服务器&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip"&gt;多目标跟踪&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;182M&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;属性识别（高精度）&lt;/td&gt; 
    &lt;td align="center"&gt;单人 117.6&lt;/td&gt; 
    &lt;td align="center"&gt;服务器&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip"&gt;目标检测&lt;/a&gt;&lt;br /&gt; &lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/PPHGNet_small_person_attribute_954_infer.zip"&gt;属性识别&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;目标检测：182M&lt;br /&gt;属性识别：86M&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;摔倒识别&lt;/td&gt; 
    &lt;td align="center"&gt;单人 100&lt;/td&gt; 
    &lt;td align="center"&gt;服务器&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip"&gt;多目标跟踪&lt;/a&gt; &lt;br /&gt; &lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/dark_hrnet_w32_256x192.zip"&gt;关键点检测&lt;/a&gt; &lt;br /&gt; &lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/STGCN.zip"&gt;基于关键点行为识别&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;多目标跟踪：182M&lt;br /&gt;关键点检测：101M&lt;br /&gt;基于关键点行为识别：21.8M&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;闯入识别&lt;/td&gt; 
    &lt;td align="center"&gt;31.4&lt;/td&gt; 
    &lt;td align="center"&gt;服务器&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip"&gt;多目标跟踪&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;182M&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;打架识别&lt;/td&gt; 
    &lt;td align="center"&gt;50.8&lt;/td&gt; 
    &lt;td align="center"&gt;服务器&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip"&gt;视频分类&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;90M&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;抽烟识别&lt;/td&gt; 
    &lt;td align="center"&gt;340.1&lt;/td&gt; 
    &lt;td align="center"&gt;服务器&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip"&gt;目标检测&lt;/a&gt;&lt;br /&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/ppyoloe_crn_s_80e_smoking_visdrone.zip"&gt;基于人体id的目标检测&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;目标检测：182M&lt;br /&gt;基于人体id的目标检测：27M&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;打电话识别&lt;/td&gt; 
    &lt;td align="center"&gt;166.7&lt;/td&gt; 
    &lt;td align="center"&gt;服务器&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip"&gt;目标检测&lt;/a&gt;&lt;br /&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/PPHGNet_tiny_calling_halfbody.zip"&gt;基于人体id的图像分类&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;目标检测：182M&lt;br /&gt;基于人体id的图像分类：45M&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;code&gt;传送门&lt;/code&gt;：&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/deploy/pipeline/README.md"&gt;完整预训练模型&lt;/a&gt;。&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; 产业应用代码示例(点击展开)&lt;/b&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;行业&lt;/th&gt; 
    &lt;th&gt;类别&lt;/th&gt; 
    &lt;th&gt;亮点&lt;/th&gt; 
    &lt;th&gt;文档说明&lt;/th&gt; 
    &lt;th&gt;模型下载&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;智能安防&lt;/td&gt; 
    &lt;td&gt;摔倒检测&lt;/td&gt; 
    &lt;td&gt;飞桨行人分析PP-Human中提供的摔倒识别算法，采用了关键点+时空图卷积网络的技术，对摔倒姿势无限制、背景环境无要求。&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4606001"&gt;基于PP-Human v2的摔倒检测&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4606001"&gt;下载链接&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;智能安防&lt;/td&gt; 
    &lt;td&gt;打架识别&lt;/td&gt; 
    &lt;td&gt;本项目基于PaddleVideo视频开发套件训练打架识别模型，然后将训练好的模型集成到PaddleDetection的PP-Human中，助力行人行为分析。&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4086987?contributionType=1"&gt;基于PP-Human的打架识别&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4086987?contributionType=1"&gt;下载链接&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;智能安防&lt;/td&gt; 
    &lt;td&gt;摔倒检测&lt;/td&gt; 
    &lt;td&gt;基于PP-Human完成来客分析整体流程。使用PP-Human完成来客分析中非常常见的场景： 1. 来客属性识别(单镜和跨境可视化）；2. 来客行为识别（摔倒识别）。&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4537344"&gt;基于PP-Human的来客分析案例教程&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4537344"&gt;下载链接&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;🏎️PP-Vehicle 实时车辆分析工具&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; 简介(点击展开)&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;PaddleDetection深入探索核心行业的高频场景，提供了车辆开箱即用分析工具，支持图片/单镜头视频/多镜头视频/在线视频流多种输入方式，广泛应用于智慧交通、智慧城市、工业巡检等领域。支持服务器端部署及TensorRT加速，T4服务器上可达到实时。 PP-Vehicle囊括四大交通场景核心功能：车牌识别、属性识别、车流量统计、违章检测。&lt;/p&gt; 
 &lt;p&gt;&lt;code&gt;传送门&lt;/code&gt;：&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/deploy/pipeline/README.md"&gt;PP-Vehicle车辆分析工具指南&lt;/a&gt;。&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; 预训练模型(点击展开)&lt;/b&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;任务&lt;/th&gt; 
    &lt;th align="center"&gt;T4 TensorRT FP16: 速度(FPS)&lt;/th&gt; 
    &lt;th align="center"&gt;推荐部署硬件&lt;/th&gt; 
    &lt;th align="center"&gt;模型方案&lt;/th&gt; 
    &lt;th align="center"&gt;模型体积&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;车辆检测（高精度）&lt;/td&gt; 
    &lt;td align="center"&gt;38.9&lt;/td&gt; 
    &lt;td align="center"&gt;服务器&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_ppvehicle.zip"&gt;目标检测&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;182M&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;车辆跟踪（高精度）&lt;/td&gt; 
    &lt;td align="center"&gt;25&lt;/td&gt; 
    &lt;td align="center"&gt;服务器&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_ppvehicle.zip"&gt;多目标跟踪&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;182M&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;车牌识别&lt;/td&gt; 
    &lt;td align="center"&gt;213.7&lt;/td&gt; 
    &lt;td align="center"&gt;服务器&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/ch_PP-OCRv3_det_infer.tar.gz"&gt;车牌检测&lt;/a&gt; &lt;br /&gt; &lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/ch_PP-OCRv3_rec_infer.tar.gz"&gt;车牌识别&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;车牌检测：3.9M &lt;br /&gt; 车牌字符识别： 12M&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;车辆属性&lt;/td&gt; 
    &lt;td align="center"&gt;136.8&lt;/td&gt; 
    &lt;td align="center"&gt;服务器&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://bj.bcebos.com/v1/paddledet/models/pipeline/vehicle_attribute_model.zip"&gt;属性识别&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;7.2M&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;code&gt;传送门&lt;/code&gt;：&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/deploy/pipeline/README.md"&gt;完整预训练模型&lt;/a&gt;。&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt; 产业应用代码示例(点击展开)&lt;/b&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;行业&lt;/th&gt; 
    &lt;th&gt;类别&lt;/th&gt; 
    &lt;th&gt;亮点&lt;/th&gt; 
    &lt;th&gt;文档说明&lt;/th&gt; 
    &lt;th&gt;模型下载&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;智慧交通&lt;/td&gt; 
    &lt;td&gt;交通监控车辆分析&lt;/td&gt; 
    &lt;td&gt;本项目基于PP-Vehicle演示智慧交通中最刚需的车流量监控、车辆违停检测以及车辆结构化（车牌、车型、颜色）分析三大场景。&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4512254"&gt;基于PP-Vehicle的交通监控分析系统&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4512254"&gt;下载链接&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h2&gt;💡产业实践范例&lt;/h2&gt; 
&lt;p&gt;产业实践范例是PaddleDetection针对高频目标检测应用场景，提供的端到端开发示例，帮助开发者打通数据标注-模型训练-模型调优-预测部署全流程。 针对每个范例我们都通过&lt;a href="https://ai.baidu.com/ai-doc/AISTUDIO/Tk39ty6ho"&gt;AI-Studio&lt;/a&gt;提供了项目代码以及说明，用户可以同步运行体验。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;传送门&lt;/code&gt;：&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/industrial_tutorial/README.md"&gt;产业实践范例完整列表&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/5058293"&gt;基于PP-YOLOE-R的旋转框检测&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/5036782"&gt;基于PP-YOLOE-SOD的无人机航拍图像检测&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4512254"&gt;基于PP-Vehicle的交通监控分析系统&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4606001"&gt;基于PP-Human v2的摔倒检测&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4385813"&gt;基于PP-TinyPose增强版的智能健身动作识别&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4086987?contributionType=1"&gt;基于PP-Human的打架识别&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/2571419"&gt;基于Faster-RCNN的瓷砖表面瑕疵检测&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/2367089"&gt;基于PaddleDetection的PCB瑕疵检测&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/2421822"&gt;基于FairMOT实现人流量统计&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/2500639"&gt;基于YOLOv3实现跌倒检测&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/3846170?channelType=0&amp;amp;channel=0"&gt;基于PP-PicoDetv2 的路面垃圾检测&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4061642?contributionType=1"&gt;基于人体关键点检测的合规检测&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.baidu.com/aistudio/projectdetail/4537344"&gt;基于PP-Human的来客分析案例教程&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;持续更新中...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🏆企业应用案例&lt;/h2&gt; 
&lt;p&gt;企业应用案例是企业在实生产环境下落地应用PaddleDetection的方案思路，相比产业实践范例其更多强调整体方案设计思路，可供开发者在项目方案设计中做参考。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;传送门&lt;/code&gt;：&lt;a href="https://www.paddlepaddle.org.cn/customercase"&gt;企业应用案例完整列表&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.paddlepaddle.org.cn/support/news?action=detail&amp;amp;id=2330"&gt;中国南方电网——变电站智慧巡检&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.paddlepaddle.org.cn/support/news?action=detail&amp;amp;id=2280"&gt;国铁电气——轨道在线智能巡检系统&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.paddlepaddle.org.cn/support/news?action=detail&amp;amp;id=2611"&gt;京东物流——园区车辆行为识别&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.paddlepaddle.org.cn/support/news?action=detail&amp;amp;id=2618"&gt;中兴克拉—厂区传统仪表统计监测&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.paddlepaddle.org.cn/support/news?action=detail&amp;amp;id=2609"&gt;宁德时代—动力电池高精度质量检测&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.paddlepaddle.org.cn/support/news?action=detail&amp;amp;id=2483"&gt;中国科学院空天信息创新研究院——高尔夫球场遥感监测&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.paddlepaddle.org.cn/support/news?action=detail&amp;amp;id=2481"&gt;御航智能——基于边缘的无人机智能巡检&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.paddlepaddle.org.cn/support/news?action=detail&amp;amp;id=2121"&gt;普宙无人机——高精度森林巡检&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.paddlepaddle.org.cn/support/news?action=detail&amp;amp;id=2615"&gt;领邦智能——红外无感测温监控&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/znrqaJmtA7CcjG0yQESWig"&gt;北京地铁——口罩检测&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.paddlepaddle.org.cn/support/news?action=detail&amp;amp;id=2288"&gt;音智达——工厂人员违规行为检测&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.paddlepaddle.org.cn/support/news?action=detail&amp;amp;id=2331"&gt;华夏天信——输煤皮带机器人智能巡检&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.paddlepaddle.org.cn/support/news?action=detail&amp;amp;id=2485"&gt;优恩物联网——社区住户分类支持广告精准投放&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.paddlepaddle.org.cn/support/news?action=detail&amp;amp;id=2599"&gt;螳螂慧视——室内3D点云场景物体分割与检测&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;持续更新中...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;📝许可证书&lt;/h2&gt; 
&lt;p&gt;本项目的发布受&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.8.1/LICENSE"&gt;Apache 2.0 license&lt;/a&gt;许可认证。&lt;/p&gt; 
&lt;h2&gt;📌引用&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@misc{ppdet2019,
title={PaddleDetection, Object detection and instance segmentation toolkit based on PaddlePaddle.},
author={PaddlePaddle Authors},
howpublished = {\url{https://github.com/PaddlePaddle/PaddleDetection}},
year={2019}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>NVIDIA/NeMo</title>
      <link>https://github.com/NVIDIA/NeMo</link>
      <description>&lt;p&gt;A scalable generative AI framework built for researchers and developers working on Large Language Models, Multimodal, and Speech AI (Automatic Speech Recognition and Text-to-Speech)&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="http://www.repostatus.org/#active"&gt;&lt;img src="http://www.repostatus.org/badges/latest/active.svg?sanitize=true" alt="Project Status: Active -- The project has reached a stable, usable state and is being actively developed." /&gt;&lt;/a&gt; &lt;a href="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/"&gt;&lt;img src="https://readthedocs.com/projects/nvidia-nemo/badge/?version=main" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://github.com/nvidia/nemo/actions/workflows/codeql.yml"&gt;&lt;img src="https://github.com/nvidia/nemo/actions/workflows/codeql.yml/badge.svg?branch=main&amp;amp;event=push" alt="CodeQL" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/NeMo/raw/master/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg?sanitize=true" alt="NeMo core license and license for collections in this repo" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/nemo-toolkit"&gt;&lt;img src="https://badge.fury.io/py/nemo-toolkit.svg?sanitize=true" alt="Release version" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/nemo-toolkit"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/nemo-toolkit.svg?sanitize=true" alt="Python version" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/nemo-toolkit"&gt;&lt;img src="https://static.pepy.tech/personalized-badge/nemo-toolkit?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=brightgreen&amp;amp;left_text=downloads" alt="PyPi total downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true" alt="Code style: black" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;NVIDIA NeMo Framework&lt;/strong&gt;&lt;/h1&gt; 
&lt;h2&gt;Latest News&lt;/h2&gt; 
&lt;!-- markdownlint-disable --&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;b&gt;Pretrain and finetune &lt;span&gt;🤗&lt;/span&gt;Hugging Face models via AutoModel&lt;/b&gt;&lt;/summary&gt; Nemo Framework's latest feature AutoModel enables broad support for 
 &lt;span&gt;🤗&lt;/span&gt;Hugging Face models, with 25.04 focusing on 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/transformers/v3.5.1/model_doc/auto.html#automodelforcausallm"&gt;AutoModelForCausalLM&lt;/a&gt;&lt;a&gt; in the &lt;/a&gt;&lt;a href="https://huggingface.co/models?pipeline_tag=text-generation&amp;amp;sort=trending"&gt;Text Generation&lt;/a&gt;&lt;a&gt; category&lt;/a&gt;&lt;/li&gt;
  &lt;a&gt; &lt;/a&gt;
  &lt;li&gt;&lt;a&gt;&lt;/a&gt;&lt;a href="https://huggingface.co/docs/transformers/main/model_doc/auto#transformers.AutoModelForImageTextToText"&gt;AutoModelForImageTextToText&lt;/a&gt;&lt;a&gt; in the &lt;/a&gt;&lt;a href="https://huggingface.co/models?pipeline_tag=image-text-to-text&amp;amp;sort=trending"&gt;Image-Text-to-Text&lt;/a&gt;&lt;a&gt; category&lt;/a&gt;&lt;/li&gt;
  &lt;a&gt; &lt;/a&gt;
 &lt;/ul&gt;
 &lt;a&gt; &lt;/a&gt;
 &lt;p&gt;&lt;a&gt;More Details in Blog: &lt;/a&gt;&lt;a href="https://developer.nvidia.com/blog/run-hugging-face-models-instantly-with-day-0-support-from-nvidia-nemo-framework"&gt;Run Hugging Face Models Instantly with Day-0 Support from NVIDIA NeMo Framework&lt;/a&gt;&lt;a&gt;. Future releases will enable support for more model families such as Video Generation models.(2025-05-19)&lt;/a&gt;&lt;/p&gt;
 &lt;a&gt; &lt;/a&gt;
&lt;/details&gt;
&lt;a&gt; &lt;/a&gt;
&lt;details open&gt;
 &lt;a&gt; &lt;summary&gt;&lt;b&gt;Training on Blackwell using Nemo&lt;/b&gt;&lt;/summary&gt; NeMo Framework has added Blackwell support, with &lt;/a&gt;
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance_summary.html"&gt;performance benchmarks on GB200 &amp;amp; B200&lt;/a&gt;
 &lt;a&gt;. More optimizations to come in the upcoming releases.(2025-05-19) &lt;/a&gt;
&lt;/details&gt;
&lt;a&gt; &lt;/a&gt;
&lt;details open&gt;
 &lt;a&gt; &lt;summary&gt;&lt;b&gt;Training Performance on GPU Tuning Guide&lt;/b&gt;&lt;/summary&gt; NeMo Framework has published &lt;/a&gt;
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance-guide.html"&gt;a comprehensive guide for performance tuning to achieve optimal throughput&lt;/a&gt;
 &lt;a&gt;! (2025-05-19) &lt;/a&gt;
&lt;/details&gt;
&lt;a&gt; &lt;/a&gt;
&lt;details open&gt;
 &lt;a&gt; &lt;summary&gt;&lt;b&gt;New Models Support&lt;/b&gt;&lt;/summary&gt; NeMo Framework has added support for latest community models - &lt;/a&gt;
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/vlms/llama4.html"&gt;Llama 4&lt;/a&gt;
 &lt;a&gt;, &lt;/a&gt;
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/vision/diffusionmodels/flux.html"&gt;Flux&lt;/a&gt;
 &lt;a&gt;, &lt;/a&gt;
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/llama_nemotron.html"&gt;Llama Nemotron&lt;/a&gt;
 &lt;a&gt;, &lt;/a&gt;
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/hyena.html#"&gt;Hyena &amp;amp; Evo2&lt;/a&gt;
 &lt;a&gt;, &lt;/a&gt;
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/vlms/qwen2vl.html"&gt;Qwen2-VL&lt;/a&gt;
 &lt;a&gt;, &lt;/a&gt;
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/qwen2.html"&gt;Qwen2.5&lt;/a&gt;
 &lt;a&gt;, Gemma3, Qwen3-30B&amp;amp;32B.(2025-05-19) &lt;/a&gt;
&lt;/details&gt;
&lt;a&gt; &lt;/a&gt;
&lt;details open&gt;
 &lt;a&gt; &lt;summary&gt;&lt;b&gt;NeMo Framework 2.0&lt;/b&gt;&lt;/summary&gt; We've released NeMo 2.0, an update on the NeMo Framework which prioritizes modularity and ease-of-use. Please refer to the &lt;/a&gt;
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/index.html"&gt;NeMo Framework User Guide&lt;/a&gt; to get started. 
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;b&gt;New Cosmos World Foundation Models Support&lt;/b&gt;&lt;/summary&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/advancing-physical-ai-with-nvidia-cosmos-world-foundation-model-platform"&gt;Advancing Physical AI with NVIDIA Cosmos World Foundation Model Platform &lt;/a&gt; (2025-01-09) &lt;/summary&gt; The end-to-end NVIDIA Cosmos platform accelerates world model development for physical AI systems. Built on CUDA, Cosmos combines state-of-the-art world foundation models, video tokenizers, and AI-accelerated data processing pipelines. Developers can accelerate world model development by fine-tuning Cosmos world foundation models or building new ones from the ground up. These models create realistic synthetic videos of environments and interactions, providing a scalable foundation for training complex systems, from simulating humanoid robots performing advanced actions to developing end-to-end autonomous driving models. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/accelerate-custom-video-foundation-model-pipelines-with-new-nvidia-nemo-framework-capabilities/"&gt; Accelerate Custom Video Foundation Model Pipelines with New NVIDIA NeMo Framework Capabilities &lt;/a&gt; (2025-01-07) &lt;/summary&gt; The NeMo Framework now supports training and customizing the 
  &lt;a href="https://github.com/NVIDIA/Cosmos"&gt;NVIDIA Cosmos&lt;/a&gt; collection of world foundation models. Cosmos leverages advanced text-to-world generation techniques to create fluid, coherent video content from natural language prompts. 
  &lt;br /&gt;
  &lt;br /&gt; You can also now accelerate your video processing step using the 
  &lt;a href="https://developer.nvidia.com/nemo-curator-video-processing-early-access"&gt;NeMo Curator&lt;/a&gt; library, which provides optimized video processing and captioning features that can deliver up to 89x faster video processing when compared to an unoptimized CPU pipeline. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;b&gt;Large Language Models and Multimodal Models&lt;/b&gt;&lt;/summary&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/state-of-the-art-multimodal-generative-ai-model-development-with-nvidia-nemo/"&gt; State-of-the-Art Multimodal Generative AI Model Development with NVIDIA NeMo &lt;/a&gt; (2024-11-06) &lt;/summary&gt; NVIDIA recently announced significant enhancements to the NeMo platform, focusing on multimodal generative AI models. The update includes NeMo Curator and the Cosmos tokenizer, which streamline the data curation process and enhance the quality of visual data. These tools are designed to handle large-scale data efficiently, making it easier to develop high-quality AI models for various applications, including robotics and autonomous driving. The Cosmos tokenizers, in particular, efficiently map visual data into compact, semantic tokens, which is crucial for training large-scale generative models. The tokenizer is available now on the 
  &lt;a href="http://github.com/NVIDIA/cosmos-tokenizer/NVIDIA/cosmos-tokenizer"&gt;NVIDIA/cosmos-tokenizer&lt;/a&gt; GitHub repo and on 
  &lt;a href="https://huggingface.co/nvidia/Cosmos-Tokenizer-CV8x8x8"&gt;Hugging Face&lt;/a&gt;. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/llama/index.html#new-llama-3-1-support for more information/"&gt; New Llama 3.1 Support &lt;/a&gt; (2024-07-23) &lt;/summary&gt; The NeMo Framework now supports training and customizing the Llama 3.1 collection of LLMs from Meta. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://aws.amazon.com/blogs/machine-learning/accelerate-your-generative-ai-distributed-training-workloads-with-the-nvidia-nemo-framework-on-amazon-eks/"&gt; Accelerate your Generative AI Distributed Training Workloads with the NVIDIA NeMo Framework on Amazon EKS &lt;/a&gt; (2024-07-16) &lt;/summary&gt; NVIDIA NeMo Framework now runs distributed training workloads on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. For step-by-step instructions on creating an EKS cluster and running distributed training workloads with NeMo, see the GitHub repository 
  &lt;a href="https://github.com/aws-samples/awsome-distributed-training/tree/main/3.test_cases/2.nemo-launcher/EKS/"&gt; here.&lt;/a&gt; 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/nvidia-nemo-accelerates-llm-innovation-with-hybrid-state-space-model-support/"&gt; NVIDIA NeMo Accelerates LLM Innovation with Hybrid State Space Model Support &lt;/a&gt; (2024/06/17) &lt;/summary&gt; NVIDIA NeMo and Megatron Core now support pre-training and fine-tuning of state space models (SSMs). NeMo also supports training models based on the Griffin architecture as described by Google DeepMind. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://huggingface.co/models?sort=trending&amp;amp;search=nvidia%2Fnemotron-4-340B"&gt; NVIDIA releases 340B base, instruct, and reward models pretrained on a total of 9T tokens. &lt;/a&gt; (2024-06-18) &lt;/summary&gt; See documentation and tutorials for SFT, PEFT, and PTQ with 
  &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/nemotron/index.html"&gt; Nemotron 340B &lt;/a&gt; in the NeMo Framework User Guide. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/nvidia-sets-new-generative-ai-performance-and-scale-records-in-mlperf-training-v4-0/"&gt; NVIDIA sets new generative AI performance and scale records in MLPerf Training v4.0 &lt;/a&gt; (2024/06/12) &lt;/summary&gt; Using NVIDIA NeMo Framework and NVIDIA Hopper GPUs NVIDIA was able to scale to 11,616 H100 GPUs and achieve near-linear performance scaling on LLM pretraining. NVIDIA also achieved the highest LLM fine-tuning performance and raised the bar for text-to-image training. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://cloud.google.com/blog/products/compute/gke-and-nvidia-nemo-framework-to-train-generative-ai-models"&gt; Accelerate your generative AI journey with NVIDIA NeMo Framework on GKE &lt;/a&gt; (2024/03/16) &lt;/summary&gt; An end-to-end walkthrough to train generative AI models on the Google Kubernetes Engine (GKE) using the NVIDIA NeMo Framework is available at https://github.com/GoogleCloudPlatform/nvidia-nemo-on-gke. The walkthrough includes detailed instructions on how to set up a Google Cloud Project and pre-train a GPT model using the NeMo Framework. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;b&gt;Speech Recognition&lt;/b&gt;&lt;/summary&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/accelerating-leaderboard-topping-asr-models-10x-with-nvidia-nemo/"&gt; Accelerating Leaderboard-Topping ASR Models 10x with NVIDIA NeMo &lt;/a&gt; (2024/09/24) &lt;/summary&gt; NVIDIA NeMo team released a number of inference optimizations for CTC, RNN-T, and TDT models that resulted in up to 10x inference speed-up. These models now exceed an inverse real-time factor (RTFx) of 2,000, with some reaching RTFx of even 6,000. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/new-standard-for-speech-recognition-and-translation-from-the-nvidia-nemo-canary-model/"&gt; New Standard for Speech Recognition and Translation from the NVIDIA NeMo Canary Model &lt;/a&gt; (2024/04/18) &lt;/summary&gt; The NeMo team just released Canary, a multilingual model that transcribes speech in English, Spanish, German, and French with punctuation and capitalization. Canary also provides bi-directional translation, between English and the three other supported languages. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/pushing-the-boundaries-of-speech-recognition-with-nemo-parakeet-asr-models/"&gt; Pushing the Boundaries of Speech Recognition with NVIDIA NeMo Parakeet ASR Models &lt;/a&gt; (2024/04/18) &lt;/summary&gt; NVIDIA NeMo, an end-to-end platform for the development of multimodal generative AI models at scale anywhere—on any cloud and on-premises—released the Parakeet family of automatic speech recognition (ASR) models. These state-of-the-art ASR models, developed in collaboration with Suno.ai, transcribe spoken English with exceptional accuracy. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/turbocharge-asr-accuracy-and-speed-with-nvidia-nemo-parakeet-tdt/"&gt; Turbocharge ASR Accuracy and Speed with NVIDIA NeMo Parakeet-TDT &lt;/a&gt; (2024/04/18) &lt;/summary&gt; NVIDIA NeMo, an end-to-end platform for developing multimodal generative AI models at scale anywhere—on any cloud and on-premises—recently released Parakeet-TDT. This new addition to the  NeMo ASR Parakeet model family boasts better accuracy and 64% greater speed over the previously best model, Parakeet-RNNT-1.1B. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
&lt;/details&gt; 
&lt;!-- markdownlint-enable --&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;NVIDIA NeMo Framework is a scalable and cloud-native generative AI framework built for researchers and PyTorch developers working on Large Language Models (LLMs), Multimodal Models (MMs), Automatic Speech Recognition (ASR), Text to Speech (TTS), and Computer Vision (CV) domains. It is designed to help you efficiently create, customize, and deploy new generative AI models by leveraging existing code and pre-trained model checkpoints.&lt;/p&gt; 
&lt;p&gt;For technical documentation, please see the &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/index.html"&gt;NeMo Framework User Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;What's New in NeMo 2.0&lt;/h2&gt; 
&lt;p&gt;NVIDIA NeMo 2.0 introduces several significant improvements over its predecessor, NeMo 1.0, enhancing flexibility, performance, and scalability.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Python-Based Configuration&lt;/strong&gt; - NeMo 2.0 transitions from YAML files to a Python-based configuration, providing more flexibility and control. This shift makes it easier to extend and customize configurations programmatically.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Modular Abstractions&lt;/strong&gt; - By adopting PyTorch Lightning’s modular abstractions, NeMo 2.0 simplifies adaptation and experimentation. This modular approach allows developers to more easily modify and experiment with different components of their models.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Scalability&lt;/strong&gt; - NeMo 2.0 seamlessly scaling large-scale experiments across thousands of GPUs using &lt;a href="https://github.com/NVIDIA/NeMo-Run"&gt;NeMo-Run&lt;/a&gt;, a powerful tool designed to streamline the configuration, execution, and management of machine learning experiments across computing environments.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Overall, these enhancements make NeMo 2.0 a powerful, scalable, and user-friendly framework for AI model development.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT]&lt;br /&gt; NeMo 2.0 is currently supported by the LLM (large language model) and VLM (vision language model) collections.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Get Started with NeMo 2.0&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Refer to the &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/quickstart.html"&gt;Quickstart&lt;/a&gt; for examples of using NeMo-Run to launch NeMo 2.0 experiments locally and on a slurm cluster.&lt;/li&gt; 
 &lt;li&gt;For more information about NeMo 2.0, see the &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/index.html"&gt;NeMo Framework User Guide&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/NVIDIA/NeMo/raw/main/nemo/collections/llm/recipes"&gt;NeMo 2.0 Recipes&lt;/a&gt; contains additional examples of launching large-scale runs using NeMo 2.0 and NeMo-Run.&lt;/li&gt; 
 &lt;li&gt;For an in-depth exploration of the main features of NeMo 2.0, see the &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/features/index.html#feature-guide"&gt;Feature Guide&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;To transition from NeMo 1.0 to 2.0, see the &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/migration/index.html#migration-guide"&gt;Migration Guide&lt;/a&gt; for step-by-step instructions.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Get Started with Cosmos&lt;/h3&gt; 
&lt;p&gt;NeMo Curator and NeMo Framework support video curation and post-training of the Cosmos World Foundation Models, which are open and available on &lt;a href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/cosmos/collections/cosmos"&gt;NGC&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/nvidia/cosmos-6751e884dc10e013a0a0d8e6"&gt;Hugging Face&lt;/a&gt;. For more information on video datasets, refer to &lt;a href="https://developer.nvidia.com/nemo-curator"&gt;NeMo Curator&lt;/a&gt;. To post-train World Foundation Models using the NeMo Framework for your custom physical AI tasks, see the &lt;a href="https://github.com/NVIDIA/Cosmos/raw/main/cosmos1/models/diffusion/nemo/post_training/README.md"&gt;Cosmos Diffusion models&lt;/a&gt; and the &lt;a href="https://github.com/NVIDIA/Cosmos/raw/main/cosmos1/models/autoregressive/nemo/post_training/README.md"&gt;Cosmos Autoregressive models&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;LLMs and MMs Training, Alignment, and Customization&lt;/h2&gt; 
&lt;p&gt;All NeMo models are trained with &lt;a href="https://github.com/Lightning-AI/lightning"&gt;Lightning&lt;/a&gt;. Training is automatically scalable to 1000s of GPUs. You can check the performance benchmarks using the latest NeMo Framework container &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance_summary.html"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When applicable, NeMo models leverage cutting-edge distributed training techniques, incorporating &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/modeloverview.html"&gt;parallelism strategies&lt;/a&gt; to enable efficient training of very large models. These techniques include Tensor Parallelism (TP), Pipeline Parallelism (PP), Fully Sharded Data Parallelism (FSDP), Mixture-of-Experts (MoE), and Mixed Precision Training with BFloat16 and FP8, as well as others.&lt;/p&gt; 
&lt;p&gt;NeMo Transformer-based LLMs and MMs utilize &lt;a href="https://github.com/NVIDIA/TransformerEngine"&gt;NVIDIA Transformer Engine&lt;/a&gt; for FP8 training on NVIDIA Hopper GPUs, while leveraging &lt;a href="https://github.com/NVIDIA/Megatron-LM/tree/main/megatron/core"&gt;NVIDIA Megatron Core&lt;/a&gt; for scaling Transformer model training.&lt;/p&gt; 
&lt;p&gt;NeMo LLMs can be aligned with state-of-the-art methods such as SteerLM, Direct Preference Optimization (DPO), and Reinforcement Learning from Human Feedback (RLHF). See &lt;a href="https://github.com/NVIDIA/NeMo-Aligner"&gt;NVIDIA NeMo Aligner&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;p&gt;In addition to supervised fine-tuning (SFT), NeMo also supports the latest parameter efficient fine-tuning (PEFT) techniques such as LoRA, P-Tuning, Adapters, and IA3. Refer to the &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/sft_peft/index.html"&gt;NeMo Framework User Guide&lt;/a&gt; for the full list of supported models and techniques.&lt;/p&gt; 
&lt;h2&gt;LLMs and MMs Deployment and Optimization&lt;/h2&gt; 
&lt;p&gt;NeMo LLMs and MMs can be deployed and optimized with &lt;a href="https://developer.nvidia.com/nemo-microservices-early-access"&gt;NVIDIA NeMo Microservices&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Speech AI&lt;/h2&gt; 
&lt;p&gt;NeMo ASR and TTS models can be optimized for inference and deployed for production use cases with &lt;a href="https://developer.nvidia.com/riva"&gt;NVIDIA Riva&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;NeMo Framework Launcher&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT]&lt;br /&gt; NeMo Framework Launcher is compatible with NeMo version 1.0 only. &lt;a href="https://github.com/NVIDIA/NeMo-Run"&gt;NeMo-Run&lt;/a&gt; is recommended for launching experiments using NeMo 2.0.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://github.com/NVIDIA/NeMo-Megatron-Launcher"&gt;NeMo Framework Launcher&lt;/a&gt; is a cloud-native tool that streamlines the NeMo Framework experience. It is used for launching end-to-end NeMo Framework training jobs on CSPs and Slurm clusters.&lt;/p&gt; 
&lt;p&gt;The NeMo Framework Launcher includes extensive recipes, scripts, utilities, and documentation for training NeMo LLMs. It also includes the NeMo Framework &lt;a href="https://github.com/NVIDIA/NeMo-Megatron-Launcher#53-using-autoconfigurator-to-find-the-optimal-configuration"&gt;Autoconfigurator&lt;/a&gt;, which is designed to find the optimal model parallel configuration for training on a specific cluster.&lt;/p&gt; 
&lt;p&gt;To get started quickly with the NeMo Framework Launcher, please see the &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/index.html"&gt;NeMo Framework Playbooks&lt;/a&gt;. The NeMo Framework Launcher does not currently support ASR and TTS training, but it will soon.&lt;/p&gt; 
&lt;h2&gt;Get Started with NeMo Framework&lt;/h2&gt; 
&lt;p&gt;Getting started with NeMo Framework is easy. State-of-the-art pretrained NeMo models are freely available on &lt;a href="https://huggingface.co/models?library=nemo&amp;amp;sort=downloads&amp;amp;search=nvidia"&gt;Hugging Face Hub&lt;/a&gt; and &lt;a href="https://catalog.ngc.nvidia.com/models?query=nemo&amp;amp;orderBy=weightPopularDESC"&gt;NVIDIA NGC&lt;/a&gt;. These models can be used to generate text or images, transcribe audio, and synthesize speech in just a few lines of code.&lt;/p&gt; 
&lt;p&gt;We have extensive &lt;a href="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/starthere/tutorials.html"&gt;tutorials&lt;/a&gt; that can be run on &lt;a href="https://colab.research.google.com"&gt;Google Colab&lt;/a&gt; or with our &lt;a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo"&gt;NGC NeMo Framework Container&lt;/a&gt;. We also have &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/index.html"&gt;playbooks&lt;/a&gt; for users who want to train NeMo models with the NeMo Framework Launcher.&lt;/p&gt; 
&lt;p&gt;For advanced users who want to train NeMo models from scratch or fine-tune existing NeMo models, we have a full suite of &lt;a href="https://github.com/NVIDIA/NeMo/tree/main/examples"&gt;example scripts&lt;/a&gt; that support multi-GPU/multi-node training.&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo/main/nemo/collections/nlp/README.md"&gt;Large Language Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo/main/nemo/collections/multimodal/README.md"&gt;Multimodal&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo/main/nemo/collections/asr/README.md"&gt;Automatic Speech Recognition&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo/main/nemo/collections/tts/README.md"&gt;Text to Speech&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo/main/nemo/collections/vision/README.md"&gt;Computer Vision&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10 or above&lt;/li&gt; 
 &lt;li&gt;Pytorch 2.5 or above&lt;/li&gt; 
 &lt;li&gt;NVIDIA GPU (if you intend to do model training)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Developer Documentation&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Version&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Latest&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/"&gt;&lt;img src="https://readthedocs.com/projects/nvidia-nemo/badge/?version=main" alt="Documentation Status" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/"&gt;Documentation of the latest (i.e. main) branch.&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Stable&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/"&gt;&lt;img src="https://readthedocs.com/projects/nvidia-nemo/badge/?version=stable" alt="Documentation Status" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/"&gt;Documentation of the stable (i.e. most recent release)&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Install NeMo Framework&lt;/h2&gt; 
&lt;p&gt;The NeMo Framework can be installed in a variety of ways, depending on your needs. Depending on the domain, you may find one of the following installation methods more suitable.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo/main/#conda--pip"&gt;Conda / Pip&lt;/a&gt;: Install NeMo-Framework with native Pip into a virtual environment. 
  &lt;ul&gt; 
   &lt;li&gt;Used to explore NeMo on any supported platform.&lt;/li&gt; 
   &lt;li&gt;This is the recommended method for ASR and TTS domains.&lt;/li&gt; 
   &lt;li&gt;Limited feature-completeness for other domains.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo/main/#ngc-pytorch-container"&gt;NGC PyTorch container&lt;/a&gt;: Install NeMo-Framework from source with feature-completeness into a highly optimized container. 
  &lt;ul&gt; 
   &lt;li&gt;For users that want to install from source in a highly optimized container.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo/main/#ngc-nemo-container"&gt;NGC NeMo container&lt;/a&gt;: Ready-to-go solution of NeMo-Framework 
  &lt;ul&gt; 
   &lt;li&gt;For users that seek highest performance.&lt;/li&gt; 
   &lt;li&gt;Contains all dependencies installed and tested for performance and convergence.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Support matrix&lt;/h3&gt; 
&lt;p&gt;NeMo-Framework provides tiers of support based on OS / Platform and mode of installation. Please refer the following overview of support levels:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fully supported: Max performance and feature-completeness.&lt;/li&gt; 
 &lt;li&gt;Limited supported: Used to explore NeMo.&lt;/li&gt; 
 &lt;li&gt;No support yet: In development.&lt;/li&gt; 
 &lt;li&gt;Deprecated: Support has reached end of life.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Please refer to the following table for current support levels:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;OS / Platform&lt;/th&gt; 
   &lt;th&gt;Install from PyPi&lt;/th&gt; 
   &lt;th&gt;Source into NGC container&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;linux&lt;/code&gt; - &lt;code&gt;amd64/x84_64&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Limited support&lt;/td&gt; 
   &lt;td&gt;Full support&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;linux&lt;/code&gt; - &lt;code&gt;arm64&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Limited support&lt;/td&gt; 
   &lt;td&gt;Limited support&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;darwin&lt;/code&gt; - &lt;code&gt;amd64/x64_64&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Deprecated&lt;/td&gt; 
   &lt;td&gt;Deprecated&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;darwin&lt;/code&gt; - &lt;code&gt;arm64&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Limited support&lt;/td&gt; 
   &lt;td&gt;Limited support&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;windows&lt;/code&gt; - &lt;code&gt;amd64/x64_64&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;No support yet&lt;/td&gt; 
   &lt;td&gt;No support yet&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;windows&lt;/code&gt; - &lt;code&gt;arm64&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;No support yet&lt;/td&gt; 
   &lt;td&gt;No support yet&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Conda / Pip&lt;/h3&gt; 
&lt;p&gt;Install NeMo in a fresh Conda environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create --name nemo python==3.10.12
conda activate nemo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Pick the right version&lt;/h4&gt; 
&lt;p&gt;NeMo-Framework publishes pre-built wheels with each release. To install nemo_toolkit from such a wheel, use the following installation method:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "nemo_toolkit[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If a more specific version is desired, we recommend a Pip-VCS install. From &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo/main/github.com/NVIDIA/NeMo"&gt;NVIDIA/NeMo&lt;/a&gt;, fetch the commit, branch, or tag that you would like to install.&lt;br /&gt; To install nemo_toolkit from this Git reference &lt;code&gt;$REF&lt;/code&gt;, use the following installation method:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/NVIDIA/NeMo
cd NeMo
git checkout @${REF:-'main'}
pip install '.[all]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Install a specific Domain&lt;/h4&gt; 
&lt;p&gt;To install a specific domain of NeMo, you must first install the nemo_toolkit using the instructions listed above. Then, you run the following domain-specific commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install nemo_toolkit['all'] # or pip install "nemo_toolkit['all']@git+https://github.com/NVIDIA/NeMo@${REF:-'main'}"
pip install nemo_toolkit['asr'] # or pip install "nemo_toolkit['asr']@git+https://github.com/NVIDIA/NeMo@$REF:-'main'}"
pip install nemo_toolkit['nlp'] # or pip install "nemo_toolkit['nlp']@git+https://github.com/NVIDIA/NeMo@${REF:-'main'}"
pip install nemo_toolkit['tts'] # or pip install "nemo_toolkit['tts']@git+https://github.com/NVIDIA/NeMo@${REF:-'main'}"
pip install nemo_toolkit['vision'] # or pip install "nemo_toolkit['vision']@git+https://github.com/NVIDIA/NeMo@${REF:-'main'}"
pip install nemo_toolkit['multimodal'] # or pip install "nemo_toolkit['multimodal']@git+https://github.com/NVIDIA/NeMo@${REF:-'main'}"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;NGC PyTorch container&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;NOTE: The following steps are supported beginning with 24.04 (NeMo-Toolkit 2.3.0)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We recommended that you start with a base NVIDIA PyTorch container: nvcr.io/nvidia/pytorch:25.01-py3.&lt;/p&gt; 
&lt;p&gt;If starting with a base NVIDIA PyTorch container, you must first launch the container:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run \
  --gpus all \
  -it \
  --rm \
  --shm-size=16g \
  --ulimit memlock=-1 \
  --ulimit stack=67108864 \
  nvcr.io/nvidia/pytorch:${NV_PYTORCH_TAG:-'nvcr.io/nvidia/pytorch:25.01-py3'}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;From &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo/main/github.com/NVIDIA/NeMo"&gt;NVIDIA/NeMo&lt;/a&gt;, fetch the commit/branch/tag that you want to install.&lt;br /&gt; To install nemo_toolkit including all of its dependencies from this Git reference &lt;code&gt;$REF&lt;/code&gt;, use the following installation method:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd /opt
git clone https://github.com/NVIDIA/NeMo
cd NeMo
git checkout ${REF:-'main'}
bash docker/common/install_dep.sh --library all
pip install ".[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;NGC NeMo container&lt;/h2&gt; 
&lt;p&gt;NeMo containers are launched concurrently with NeMo version updates. NeMo Framework now supports LLMs, MMs, ASR, and TTS in a single consolidated Docker container. You can find additional information about released containers on the &lt;a href="https://github.com/NVIDIA/NeMo/releases"&gt;NeMo releases page&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To use a pre-built container, run the following code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run \
  --gpus all \
  -it \
  --rm \
  --shm-size=16g \
  --ulimit memlock=-1 \
  --ulimit stack=67108864 \
  nvcr.io/nvidia/pytorch:${NV_PYTORCH_TAG:-'nvcr.io/nvidia/nemo:25.02'}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Future Work&lt;/h2&gt; 
&lt;p&gt;The NeMo Framework Launcher does not currently support ASR and TTS training, but it will soon.&lt;/p&gt; 
&lt;h2&gt;Discussions Board&lt;/h2&gt; 
&lt;p&gt;FAQ can be found on the NeMo &lt;a href="https://github.com/NVIDIA/NeMo/discussions"&gt;Discussions board&lt;/a&gt;. You are welcome to ask questions or start discussions on the board.&lt;/p&gt; 
&lt;h2&gt;Contribute to NeMo&lt;/h2&gt; 
&lt;p&gt;We welcome community contributions! Please refer to &lt;a href="https://github.com/NVIDIA/NeMo/raw/stable/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for the process.&lt;/p&gt; 
&lt;h2&gt;Publications&lt;/h2&gt; 
&lt;p&gt;We provide an ever-growing list of &lt;a href="https://nvidia.github.io/NeMo/publications/"&gt;publications&lt;/a&gt; that utilize the NeMo Framework.&lt;/p&gt; 
&lt;p&gt;To contribute an article to the collection, please submit a pull request to the &lt;code&gt;gh-pages-src&lt;/code&gt; branch of this repository. For detailed information, please consult the README located at the &lt;a href="https://github.com/NVIDIA/NeMo/tree/gh-pages-src#readme"&gt;gh-pages-src branch&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Blogs&lt;/h2&gt; 
&lt;!-- markdownlint-disable --&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;b&gt;Large Language Models and Multimodal Models&lt;/b&gt;&lt;/summary&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://blogs.nvidia.com/blog/bria-builds-responsible-generative-ai-using-nemo-picasso/"&gt; Bria Builds Responsible Generative AI for Enterprises Using NVIDIA NeMo, Picasso &lt;/a&gt; (2024/03/06) &lt;/summary&gt; Bria, a Tel Aviv startup at the forefront of visual generative AI for enterprises now leverages the NVIDIA NeMo Framework. The Bria.ai platform uses reference implementations from the NeMo Multimodal collection, trained on NVIDIA Tensor Core GPUs, to enable high-throughput and low-latency image generation. Bria has also adopted NVIDIA Picasso, a foundry for visual generative AI models, to run inference. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/new-nvidia-nemo-framework-features-and-nvidia-h200-supercharge-llm-training-performance-and-versatility/"&gt; New NVIDIA NeMo Framework Features and NVIDIA H200 &lt;/a&gt; (2023/12/06) &lt;/summary&gt; NVIDIA NeMo Framework now includes several optimizations and enhancements, including: 1) Fully Sharded Data Parallelism (FSDP) to improve the efficiency of training large-scale AI models, 2) Mix of Experts (MoE)-based LLM architectures with expert parallelism for efficient LLM training at scale, 3) Reinforcement Learning from Human Feedback (RLHF) with TensorRT-LLM for inference stage acceleration, and 4) up to 4.2x speedups for Llama 2 pre-training on NVIDIA H200 Tensor Core GPUs. 
  &lt;br /&gt;
  &lt;br /&gt; 
  &lt;a href="https://developer.nvidia.com/blog/new-nvidia-nemo-framework-features-and-nvidia-h200-supercharge-llm-training-performance-and-versatility"&gt; &lt;img src="https://github.com/sbhavani/TransformerEngine/raw/main/docs/examples/H200-NeMo-performance.png" alt="H200-NeMo-performance" style="width: 600px;" /&gt;&lt;/a&gt; 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://blogs.nvidia.com/blog/nemo-amazon-titan/"&gt; NVIDIA now powers training for Amazon Titan Foundation models &lt;/a&gt; (2023/11/28) &lt;/summary&gt; NVIDIA NeMo Framework now empowers the Amazon Titan foundation models (FM) with efficient training of large language models (LLMs). The Titan FMs form the basis of Amazon’s generative AI service, Amazon Bedrock. The NeMo Framework provides a versatile framework for building, customizing, and running LLMs. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
&lt;/details&gt; 
&lt;!-- markdownlint-enable --&gt; 
&lt;h2&gt;Licenses&lt;/h2&gt; 
&lt;p&gt;NeMo is licensed under the &lt;a href="https://github.com/NVIDIA/NeMo?tab=Apache-2.0-1-ov-file"&gt;Apache License 2.0&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>