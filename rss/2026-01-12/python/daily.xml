<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Sun, 11 Jan 2026 01:37:54 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>Lightricks/ComfyUI-LTXVideo</title>
      <link>https://github.com/Lightricks/ComfyUI-LTXVideo</link>
      <description>&lt;p&gt;LTX-Video Support for ComfyUI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ComfyUI-LTXVideo&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/Lightricks/LTX-2"&gt;&lt;img src="https://img.shields.io/badge/LTX-Repo-blue?logo=github" alt="GitHub" /&gt;&lt;/a&gt; &lt;a href="https://ltx.io/model"&gt;&lt;img src="https://img.shields.io/badge/Website-LTX-181717?logo=google-chrome" alt="Website" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/Lightricks/LTX-2"&gt;&lt;img src="https://img.shields.io/badge/HuggingFace-Model-orange?logo=huggingface" alt="Model" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Lightricks/LTX-2/tree/main/packages/ltx-trainer"&gt;&lt;img src="https://img.shields.io/badge/LTX-Trainer%20Repo-9146FF" alt="LTXV Trainer" /&gt;&lt;/a&gt; &lt;a href="https://app.ltx.studio/ltx-2-playground/i2v"&gt;&lt;img src="https://img.shields.io/badge/Demo-Try%20Now-brightgreen?logo=vercel" alt="Demo" /&gt;&lt;/a&gt; &lt;a href="https://videos.ltx.io/LTX-2/grants/LTX_2_Technical_Report_compressed.pdf"&gt;&lt;img src="https://img.shields.io/badge/Paper-arXiv-B31B1B?logo=arxiv" alt="Paper" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/ltxplatform"&gt;&lt;img src="https://img.shields.io/badge/Join-Discord-5865F2?logo=discord" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;A collection of powerful custom nodes that extend ComfyUI's capabilities for the LTX-2 video generation model.&lt;/p&gt; 
&lt;p&gt;LTX-2 is built into ComfyUI core (&lt;a href="https://github.com/comfyanonymous/ComfyUI/tree/master/comfy/ldm/lightricks"&gt;see it here&lt;/a&gt;), making it readily accessible to all ComfyUI users. This repository hosts additional nodes and workflows to help you get the most out of LTX-2's advanced features.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;To learn more about LTX-2&lt;/strong&gt; See the &lt;a href="https://github.com/Lightricks/LTX-2"&gt;main LTX-2 repository&lt;/a&gt; for model details and additional resources.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;Before you begin using an LTX-2 workflow in ComfyUI, make sure you have:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ComfyUI installed (Download here](&lt;a href="https://www.comfy.org/download"&gt;https://www.comfy.org/download&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;CUDA-compatible GPU with 32GB+ VRAM&lt;/li&gt; 
 &lt;li&gt;100GB+ free disk space for models and cache&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start üöÄ&lt;/h2&gt; 
&lt;p&gt;We recommend using the LTX-2 workflows available in Comfy Manager.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open ComfyUI&lt;/li&gt; 
 &lt;li&gt;Click the Manager button (or press Ctrl+M)&lt;/li&gt; 
 &lt;li&gt;Select Install Custom Nodes&lt;/li&gt; 
 &lt;li&gt;Search for ‚ÄúLTXVideo‚Äù&lt;/li&gt; 
 &lt;li&gt;Click Install&lt;/li&gt; 
 &lt;li&gt;Wait for installation to complete&lt;/li&gt; 
 &lt;li&gt;Restart ComfyUI&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The nodes will appear in your node menu under the ‚ÄúLTXVideo‚Äù category. Required models will be downloaded on first use.&lt;/p&gt; 
&lt;h2&gt;Example Workflows&lt;/h2&gt; 
&lt;p&gt;The ComfyUI-LTXVideo installation includes several example workflows. You can see them all at: ''' ComfyUI/custom_nodes/ComfyUI-LTXVideo/example_workflows/ '''&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/ComfyUI-LTXVideo/master/example_workflows/LTX-2_T2V_Full_wLora.json"&gt;&lt;code&gt;Text to video full model&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/ComfyUI-LTXVideo/master/example_workflows/LTX-2_T2V_Distilled_wLora.json"&gt;&lt;code&gt;Text to video distilled model (Fast)&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/ComfyUI-LTXVideo/master/example_workflows/LTX-2_I2V_Full_wLora.json"&gt;&lt;code&gt;Image to video full model&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/ComfyUI-LTXVideo/master/example_workflows/LTX-2_I2V_Distilled_wLora.json"&gt;&lt;code&gt;Image to video distilled model (Fast)&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/ComfyUI-LTXVideo/master/example_workflows/LTX-2_V2V_Detailer.json"&gt;&lt;code&gt;Video to video detailer&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/ComfyUI-LTXVideo/master/example_workflows/LTX-2_ICLoRA_All_Distilled.json"&gt;&lt;code&gt;IC-LoRA distilled model (depth + human pose + edges)&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Required Models&lt;/h2&gt; 
&lt;p&gt;Download the following models:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;LTX-2 Model Checkpoint&lt;/strong&gt; - Choose and download one of the models to &lt;code&gt;COMFYUI_ROOT_FOLDER/models/checkpoints&lt;/code&gt; folder.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-dev-fp8.safetensors"&gt;&lt;code&gt;ltx-2-19b-dev-fp8.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-distilled-fp8.safetensors"&gt;&lt;code&gt;ltx-2-19b-distilled-fp8.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-dev.safetensors"&gt;&lt;code&gt;ltx-2-19b-dev.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-distilled.safetensors"&gt;&lt;code&gt;ltx-2-19b-distilled.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Spatial Upscaler&lt;/strong&gt; - Required for current two-stage pipeline implementations in this repository. Download to &lt;code&gt;COMFYUI_ROOT_FOLDER/models/latent_upscale_models&lt;/code&gt; folder.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-spatial-upscaler-x2-1.0.safetensors"&gt;&lt;code&gt;ltx-2-spatial-upscaler-x2-1.0.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Temporal Upscaler&lt;/strong&gt; - Required for current two-stage pipeline implementations in this repository. Download to &lt;code&gt;COMFYUI_ROOT_FOLDER/models/latent_upscale_models&lt;/code&gt; folder.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-temporal-upscaler-x2-1.0.safetensors"&gt;&lt;code&gt;ltx-2-temporal-upscaler-x2-1.0.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Distilled LoRA&lt;/strong&gt; - Required for current two-stage pipeline implementations in this repository (except DistilledPipeline and ICLoraPipeline). Download to &lt;code&gt;COMFYUI_ROOT_FOLDER/models/loras&lt;/code&gt; folder.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-distilled-lora-384.safetensors"&gt;&lt;code&gt;ltx-2-19b-distilled-lora-384.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Gemma Text Encoder&lt;/strong&gt; Download all files from the repository to &lt;code&gt;COMFYUI_ROOT_FOLDER/models/text_encoders/gemma-3-12b-it-qat-q4_0-unquantized&lt;/code&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-unquantized"&gt;&lt;code&gt;Gemma 3&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;LoRAs&lt;/strong&gt; Choose and download to &lt;code&gt;COMFYUI_ROOT_FOLDER/models/loras&lt;/code&gt; folder.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Canny-Control/blob/main/ltx-2-19b-ic-lora-canny-control.safetensors"&gt;&lt;code&gt;ltx-2-19b-ic-lora-canny-control.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Depth-Control/blob/main/ltx-2-19b-ic-lora-depth-control.safetensors"&gt;&lt;code&gt;ltx-2-19b-ic-lora-depth-control.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Detailer/blob/main/ltx-2-19b-ic-lora-detailer.safetensors"&gt;&lt;code&gt;ltx-2-19b-ic-lora-detailer.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Pose-Control/blob/main/ltx-2-19b-ic-lora-pose-control.safetensors"&gt;&lt;code&gt;ltx-2-19b-ic-lora-pose-control.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-In/blob/main/ltx-2-19b-lora-camera-control-dolly-in.safetensors"&gt;&lt;code&gt;ltx-2-19b-lora-camera-control-dolly-in.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-Left/blob/main/ltx-2-19b-lora-camera-control-dolly-left.safetensors"&gt;&lt;code&gt;ltx-2-19b-lora-camera-control-dolly-left.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-Out/blob/main/ltx-2-19b-lora-camera-control-dolly-out.safetensors"&gt;&lt;code&gt;ltx-2-19b-lora-camera-control-dolly-out.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-Right/blob/main/ltx-2-19b-lora-camera-control-dolly-right.safetensors"&gt;&lt;code&gt;ltx-2-19b-lora-camera-control-dolly-right.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Jib-Down/blob/main/ltx-2-19b-lora-camera-control-jib-down.safetensors"&gt;&lt;code&gt;ltx-2-19b-lora-camera-control-jib-down.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Jib-Up/blob/main/ltx-2-19b-lora-camera-control-jib-up.safetensors"&gt;&lt;code&gt;ltx-2-19b-lora-camera-control-jib-up.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Static/blob/main/ltx-2-19b-lora-camera-control-static.safetensors"&gt;&lt;code&gt;ltx-2-19b-lora-camera-control-static.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Advanced Techniques&lt;/h2&gt; 
&lt;h3&gt;Low VRAM&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;For systems with low VRAM you can use the model loader nodes from &lt;a href="https://raw.githubusercontent.com/Lightricks/ComfyUI-LTXVideo/master/low_vram_loaders.py"&gt;low_vram_loaders.py&lt;/a&gt;. Those nodes ensure the correct order of execution and perform the model offloading such that generation fits in 32 GB VRAM.&lt;/li&gt; 
 &lt;li&gt;Use --reserve-vram ComfyUI parameter: &lt;code&gt;python -m main --reserve-vram 5&lt;/code&gt; (or other number in GB).&lt;/li&gt; 
 &lt;li&gt;For complete information about using LTX-2 models, workflows, and nodes in ComfyUI, please visit our &lt;a href="https://docs.ltx.video/open-source-model/integration-tools/comfy-ui"&gt;Open Source documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>browser-use/browser-use</title>
      <link>https://github.com/browser-use/browser-use</link>
      <description>&lt;p&gt;üåê Make websites accessible for AI agents. Automate tasks online with ease.&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="https://github.com/user-attachments/assets/2ccdb752-22fb-41c7-8948-857fc1ad7e24" " /&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/user-attachments/assets/774a46d5-27a0-490c-b7d0-e65fcbbfa358" /&gt; 
 &lt;img alt="Shows a black Browser Use Logo in light color mode and a white one in dark color mode." src="https://github.com/user-attachments/assets/2ccdb752-22fb-41c7-8948-857fc1ad7e24" width="full" /&gt; 
&lt;/picture&gt; 
&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://github.com/user-attachments/assets/9955dda9-ede3-4971-8ee0-91cbc3850125" " /&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/user-attachments/assets/6797d09b-8ac3-4cb9-ba07-b289e080765a" /&gt; 
  &lt;img alt="The AI browser agent." src="https://github.com/user-attachments/assets/9955dda9-ede3-4971-8ee0-91cbc3850125" width="400" /&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://cloud.browser-use.com"&gt;&lt;img src="https://media.browser-use.tools/badges/package" height="48" alt="Browser-Use Package Download Statistics" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://raw.githubusercontent.com/browser-use/browser-use/main/#demos"&gt;&lt;img src="https://media.browser-use.tools/badges/demos" alt="Demos" /&gt;&lt;/a&gt; 
 &lt;img width="16" height="1" alt="" /&gt; 
 &lt;a href="https://docs.browser-use.com"&gt;&lt;img src="https://media.browser-use.tools/badges/docs" alt="Docs" /&gt;&lt;/a&gt; 
 &lt;img width="16" height="1" alt="" /&gt; 
 &lt;a href="https://browser-use.com/posts"&gt;&lt;img src="https://media.browser-use.tools/badges/blog" alt="Blog" /&gt;&lt;/a&gt; 
 &lt;img width="16" height="1" alt="" /&gt; 
 &lt;a href="https://browsermerch.com"&gt;&lt;img src="https://media.browser-use.tools/badges/merch" alt="Merch" /&gt;&lt;/a&gt; 
 &lt;img width="100" height="1" alt="" /&gt; 
 &lt;a href="https://github.com/browser-use/browser-use"&gt;&lt;img src="https://media.browser-use.tools/badges/github" alt="Github Stars" /&gt;&lt;/a&gt; 
 &lt;img width="4" height="1" alt="" /&gt; 
 &lt;a href="https://x.com/intent/user?screen_name=browser_use"&gt;&lt;img src="https://media.browser-use.tools/badges/twitter" alt="Twitter" /&gt;&lt;/a&gt; 
 &lt;img width="4 height=" 1" alt="" /&gt; 
 &lt;a href="https://link.browser-use.com/discord"&gt;&lt;img src="https://media.browser-use.tools/badges/discord" alt="Discord" /&gt;&lt;/a&gt; 
 &lt;img width="4" height="1" alt="" /&gt; 
 &lt;a href="https://cloud.browser-use.com"&gt;&lt;img src="https://media.browser-use.tools/badges/cloud" height="48" alt="Browser-Use Cloud" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p&gt;üå§Ô∏è Want to skip the setup? Use our &lt;b&gt;&lt;a href="https://cloud.browser-use.com"&gt;cloud&lt;/a&gt;&lt;/b&gt; for faster, scalable, stealth-enabled browser automation!&lt;/p&gt; 
&lt;h1&gt;ü§ñ LLM Quickstart&lt;/h1&gt; 
&lt;ol&gt; 
 &lt;li&gt;Direct your favorite coding agent (Cursor, Claude Code, etc) to &lt;a href="https://docs.browser-use.com/llms-full.txt"&gt;Agents.md&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Prompt away!&lt;/li&gt; 
&lt;/ol&gt; 
&lt;br /&gt; 
&lt;h1&gt;üëã Human Quickstart&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;1. Create environment with &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt; (Python&amp;gt;=3.11):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv init
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;2. Install Browser-Use package:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;#  We ship every day - use the latest version!
uv add browser-use
uv sync
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;3. Get your API key from &lt;a href="https://cloud.browser-use.com/new-api-key"&gt;Browser Use Cloud&lt;/a&gt; and add it to your &lt;code&gt;.env&lt;/code&gt; file (new signups get $10 free credits):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# .env
BROWSER_USE_API_KEY=your-key
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;4. Install Chromium browser:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx browser-use install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;5. Run your first agent:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from browser_use import Agent, Browser, ChatBrowserUse
import asyncio

async def example():
    browser = Browser(
        # use_cloud=True,  # Uncomment to use a stealth browser on Browser Use Cloud
    )

    llm = ChatBrowserUse()

    agent = Agent(
        task="Find the number of stars of the browser-use repo",
        llm=llm,
        browser=browser,
    )

    history = await agent.run()
    return history

if __name__ == "__main__":
    history = asyncio.run(example())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Check out the &lt;a href="https://docs.browser-use.com"&gt;library docs&lt;/a&gt; and the &lt;a href="https://docs.cloud.browser-use.com"&gt;cloud docs&lt;/a&gt; for more!&lt;/p&gt; 
&lt;br /&gt; 
&lt;h1&gt;üî• Deploy on Sandboxes&lt;/h1&gt; 
&lt;p&gt;We handle agents, browsers, persistence, auth, cookies, and LLMs. The agent runs right next to the browser for minimal latency.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from browser_use import Browser, sandbox, ChatBrowserUse
from browser_use.agent.service import Agent
import asyncio

@sandbox()
async def my_task(browser: Browser):
    agent = Agent(task="Find the top HN post", browser=browser, llm=ChatBrowserUse())
    await agent.run()

# Just call it like any async function
asyncio.run(my_task())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://docs.browser-use.com/production"&gt;Going to Production&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h1&gt;üöÄ Template Quickstart&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Want to get started even faster?&lt;/strong&gt; Generate a ready-to-run template:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx browser-use init --template default
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This creates a &lt;code&gt;browser_use_default.py&lt;/code&gt; file with a working example. Available templates:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;default&lt;/code&gt; - Minimal setup to get started quickly&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;advanced&lt;/code&gt; - All configuration options with detailed comments&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tools&lt;/code&gt; - Examples of custom tools and extending the agent&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can also specify a custom output path:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx browser-use init --template default --output my_agent.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt; 
&lt;h1&gt;Demos&lt;/h1&gt; 
&lt;h3&gt;üìã Form-Filling&lt;/h3&gt; 
&lt;h4&gt;Task = "Fill in this job application with my resume and information."&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/57865ee6-6004-49d5-b2c2-6dff39ec2ba9" alt="Job Application Demo" /&gt; &lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/use-cases/apply_to_job.py"&gt;Example code ‚Üó&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;üçé Grocery-Shopping&lt;/h3&gt; 
&lt;h4&gt;Task = "Put this list of items into my instacart."&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/a6813fa7-4a7c-40a6-b4aa-382bf88b1850"&gt;https://github.com/user-attachments/assets/a6813fa7-4a7c-40a6-b4aa-382bf88b1850&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/use-cases/buy_groceries.py"&gt;Example code ‚Üó&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;üíª Personal-Assistant.&lt;/h3&gt; 
&lt;h4&gt;Task = "Help me find parts for a custom PC."&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/ac34f75c-057a-43ef-ad06-5b2c9d42bf06"&gt;https://github.com/user-attachments/assets/ac34f75c-057a-43ef-ad06-5b2c9d42bf06&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/use-cases/pcpartpicker.py"&gt;Example code ‚Üó&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;üí°See &lt;a href="https://docs.browser-use.com/examples"&gt;more examples here ‚Üó&lt;/a&gt; and give us a star!&lt;/h3&gt; 
&lt;br /&gt; 
&lt;h2&gt;Integrations, hosting, custom tools, MCP, and more on our &lt;a href="https://docs.browser-use.com"&gt;Docs ‚Üó&lt;/a&gt;&lt;/h2&gt; 
&lt;br /&gt; 
&lt;h1&gt;FAQ&lt;/h1&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;What's the best model to use?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;We optimized &lt;strong&gt;ChatBrowserUse()&lt;/strong&gt; specifically for browser automation tasks. On avg it completes tasks 3-5x faster than other models with SOTA accuracy.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Pricing (per 1M tokens):&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Input tokens: $0.20&lt;/li&gt; 
  &lt;li&gt;Cached input tokens: $0.02&lt;/li&gt; 
  &lt;li&gt;Output tokens: $2.00&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;For other LLM providers, see our &lt;a href="https://docs.browser-use.com/supported-models"&gt;supported models documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Can I use custom tools with the agent?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Yes! You can add custom tools to extend the agent's capabilities:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from browser_use import Tools

tools = Tools()

@tools.action(description='Description of what this tool does.')
def custom_tool(param: str) -&amp;gt; str:
    return f"Result: {param}"

agent = Agent(
    task="Your task",
    llm=llm,
    browser=browser,
    tools=tools,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Can I use this for free?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Yes! Browser-Use is open source and free to use. You only need to choose an LLM provider (like OpenAI, Google, ChatBrowserUse, or run local models with Ollama).&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;How do I handle authentication?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Check out our authentication examples:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/browser/real_browser.py"&gt;Using real browser profiles&lt;/a&gt; - Reuse your existing Chrome profile with saved logins&lt;/li&gt; 
  &lt;li&gt;If you want to use temporary accounts with inbox, choose AgentMail&lt;/li&gt; 
  &lt;li&gt;To sync your auth profile with the remote browser, run &lt;code&gt;curl -fsSL https://browser-use.com/profile.sh | BROWSER_USE_API_KEY=XXXX sh&lt;/code&gt; (replace XXXX with your API key)&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;These examples show how to maintain sessions and handle authentication seamlessly.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;How do I solve CAPTCHAs?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;For CAPTCHA handling, you need better browser fingerprinting and proxies. Use &lt;a href="https://cloud.browser-use.com"&gt;Browser Use Cloud&lt;/a&gt; which provides stealth browsers designed to avoid detection and CAPTCHA challenges.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;How do I go into production?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Chrome can consume a lot of memory, and running many agents in parallel can be tricky to manage.&lt;/p&gt; 
 &lt;p&gt;For production use cases, use our &lt;a href="https://cloud.browser-use.com"&gt;Browser Use Cloud API&lt;/a&gt; which handles:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Scalable browser infrastructure&lt;/li&gt; 
  &lt;li&gt;Memory management&lt;/li&gt; 
  &lt;li&gt;Proxy rotation&lt;/li&gt; 
  &lt;li&gt;Stealth browser fingerprinting&lt;/li&gt; 
  &lt;li&gt;High-performance parallel execution&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;Tell your computer what to do, and it gets it done.&lt;/strong&gt;&lt;/p&gt; 
 &lt;img src="https://github.com/user-attachments/assets/06fa3078-8461-4560-b434-445510c1766f" width="400" /&gt; 
 &lt;p&gt;&lt;a href="https://x.com/intent/user?screen_name=mamagnus00"&gt;&lt;img src="https://img.shields.io/twitter/follow/Magnus?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt; ‚ÄÉ‚ÄÉ‚ÄÉ &lt;a href="https://x.com/intent/user?screen_name=gregpr07"&gt;&lt;img src="https://img.shields.io/twitter/follow/Gregor?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt;
  Made with ‚ù§Ô∏è in Zurich and San Francisco 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>hacksider/Deep-Live-Cam</title>
      <link>https://github.com/hacksider/Deep-Live-Cam</link>
      <description>&lt;p&gt;real time face swap and one-click video deepfake with only a single image&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt;Deep-Live-Cam 2.0.1c&lt;/h1&gt; 
&lt;p align="center"&gt; Real-time face swap and video deepfake with a single click and only a single image. &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/11395" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/11395" alt="hacksider%2FDeep-Live-Cam | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/demo.gif" alt="Demo GIF" width="800" /&gt; &lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This deepfake software is designed to be a productive tool for the AI-generated media industry. It can assist artists in animating custom characters, creating engaging content, and even using models for clothing design.&lt;/p&gt; 
&lt;p&gt;We are aware of the potential for unethical applications and are committed to preventative measures. A built-in check prevents the program from processing inappropriate media (nudity, graphic content, sensitive material like war footage, etc.). We will continue to develop this project responsibly, adhering to the law and ethics. We may shut down the project or add watermarks if legally required.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Ethical Use: Users are expected to use this software responsibly and legally. If using a real person's face, obtain their consent and clearly label any output as a deepfake when sharing online.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Content Restrictions: The software includes built-in checks to prevent processing inappropriate media, such as nudity, graphic content, or sensitive material.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Legal Compliance: We adhere to all relevant laws and ethical guidelines. If legally required, we may shut down the project or add watermarks to the output.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;User Responsibility: We are not responsible for end-user actions. Users must ensure their use of the software aligns with ethical standards and legal requirements.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By using this software, you agree to these terms and commit to using it in a manner that respects the rights and dignity of others.&lt;/p&gt; 
&lt;p&gt;Users are expected to use this software responsibly and legally. If using a real person's face, obtain their consent and clearly label any output as a deepfake when sharing online. We are not responsible for end-user actions.&lt;/p&gt; 
&lt;h2&gt;Exclusive v2.4 Quick Start - Pre-built (Windows/Mac Silicon)&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://deeplivecam.net/index.php/quickstart"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/Download.png" width="285" height="77" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;a href="https://deeplivecam.net/index.php/quickstart"&gt; &lt;h5&gt;This is the fastest build you can get if you have a discrete NVIDIA or AMD GPU or Mac Silicon, And you'll receive special priority support.&lt;/h5&gt; &lt;h6&gt;These Pre-builts are perfect for non-technical users or those who don't have time to, or can't manually install all the requirements. Just a heads-up: this is an open-source project, so you can also install it manually.&lt;/h6&gt; &lt;h2&gt;TLDR; Live Deepfake in just 3 Clicks&lt;/h2&gt; &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/af825228-852c-411b-b787-ffd9aac72fc6" alt="easysteps" /&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Select a face&lt;/li&gt; 
  &lt;li&gt;Select which camera to use&lt;/li&gt; 
  &lt;li&gt;Press live!&lt;/li&gt; 
 &lt;/ol&gt; &lt;h2&gt;Features &amp;amp; Uses - Everything is in real-time&lt;/h2&gt; &lt;h3&gt;Mouth Mask&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Retain your original mouth for accurate movement using Mouth Mask&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/ludwig.gif" alt="resizable-gif" /&gt; &lt;/p&gt; &lt;h3&gt;Face Mapping&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Use different faces on multiple subjects simultaneously&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/streamers.gif" alt="face_mapping_source" /&gt; &lt;/p&gt; &lt;h3&gt;Your Movie, Your Face&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Watch movies with any face in real-time&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/movie.gif" alt="movie" /&gt; &lt;/p&gt; &lt;h3&gt;Live Show&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Run Live shows and performances&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/live_show.gif" alt="show" /&gt; &lt;/p&gt; &lt;h3&gt;Memes&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Create Your Most Viral Meme Yet&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/meme.gif" alt="show" width="450" /&gt; &lt;br /&gt; &lt;sub&gt;Created using Many Faces feature in Deep-Live-Cam&lt;/sub&gt; &lt;/p&gt; &lt;h3&gt;Omegle&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Surprise people on Omegle&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; 
  &lt;video src="https://github.com/user-attachments/assets/2e9b9b82-fa04-4b70-9f56-b1f68e7672d0" width="450" controls&gt;&lt;/video&gt; &lt;/p&gt; &lt;h2&gt;Installation (Manual)&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Please be aware that the installation requires technical skills and is not for beginners. Consider downloading the quickstart version.&lt;/strong&gt;&lt;/p&gt; &lt;/a&gt;
&lt;details&gt;
 &lt;a href="https://deeplivecam.net/index.php/quickstart"&gt; &lt;summary&gt;Click to see the process&lt;/summary&gt; &lt;h3&gt;Installation&lt;/h3&gt; &lt;p&gt;This is more likely to work on your computer but will be slower as it utilizes the CPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Set up Your Platform&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Python (3.11 recommended)&lt;/li&gt; 
   &lt;li&gt;pip&lt;/li&gt; 
   &lt;li&gt;git&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=OlNWCpFdVMA"&gt;ffmpeg&lt;/a&gt; - &lt;code&gt;iex (irm ffmpeg.tc.ht)&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://visualstudio.microsoft.com/visual-cpp-build-tools/"&gt;Visual Studio 2022 Runtimes (Windows)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt;&lt;/a&gt; 
 &lt;p&gt;&lt;strong&gt;2. Clone the Repository&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/hacksider/Deep-Live-Cam.git
cd Deep-Live-Cam
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;3. Download the Models&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth"&gt;GFPGANv1.4&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx"&gt;inswapper_128_fp16.onnx&lt;/a&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;Place these files in the "&lt;strong&gt;models&lt;/strong&gt;" folder.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;4. Install Dependencies&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;We highly recommend using a &lt;code&gt;venv&lt;/code&gt; to avoid issues.&lt;/p&gt; 
 &lt;p&gt;For Windows:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;For Linux:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Ensure you use the installed Python 3.10
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;For macOS:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Apple Silicon (M1/M2/M3) requires specific setup:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Install Python 3.11 (specific version is important)
brew install python@3.11

# Install tkinter package (required for the GUI)
brew install python-tk@3.10

# Create and activate virtual environment with Python 3.11
python3.11 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;** In case something goes wrong and you need to reinstall the virtual environment **&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Deactivate the virtual environment
rm -rf venv

# Reinstall the virtual environment
python -m venv venv
source venv/bin/activate

# install the dependencies again
pip install -r requirements.txt

# gfpgan and basicsrs issue fix
pip install git+https://github.com/xinntao/BasicSR.git@master
pip uninstall gfpgan -y
pip install git+https://github.com/TencentARC/GFPGAN.git@master
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Run:&lt;/strong&gt; If you don't have a GPU, you can run Deep-Live-Cam using &lt;code&gt;python run.py&lt;/code&gt;. Note that initial execution will download models (~300MB).&lt;/p&gt; 
 &lt;h3&gt;GPU Acceleration&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;CUDA Execution Provider (Nvidia)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install &lt;a href="https://developer.nvidia.com/cuda-12-8-0-download-archive"&gt;CUDA Toolkit 12.8.0&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Install &lt;a href="https://developer.nvidia.com/rdp/cudnn-archive"&gt;cuDNN v8.9.7 for CUDA 12.x&lt;/a&gt; (required for onnxruntime-gpu): 
   &lt;ul&gt; 
    &lt;li&gt;Download cuDNN v8.9.7 for CUDA 12.x&lt;/li&gt; 
    &lt;li&gt;Make sure the cuDNN bin directory is in your system PATH&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
pip uninstall onnxruntime onnxruntime-gpu
pip install onnxruntime-gpu==1.21.0
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="3"&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python run.py --execution-provider cuda
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;CoreML Execution Provider (Apple Silicon)&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Apple Silicon (M1/M2/M3) specific installation:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Make sure you've completed the macOS setup above using Python 3.10.&lt;/li&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall onnxruntime onnxruntime-silicon
pip install onnxruntime-silicon==1.13.1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="3"&gt; 
  &lt;li&gt;Usage (important: specify Python 3.10):&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python3.10 run.py --execution-provider coreml
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Important Notes for macOS:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;You &lt;strong&gt;must&lt;/strong&gt; use Python 3.10, not newer versions like 3.11 or 3.13&lt;/li&gt; 
  &lt;li&gt;Always run with &lt;code&gt;python3.10&lt;/code&gt; command not just &lt;code&gt;python&lt;/code&gt; if you have multiple Python versions installed&lt;/li&gt; 
  &lt;li&gt;If you get error about &lt;code&gt;_tkinter&lt;/code&gt; missing, reinstall the tkinter package: &lt;code&gt;brew reinstall python-tk@3.10&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;If you get model loading errors, check that your models are in the correct folder&lt;/li&gt; 
  &lt;li&gt;If you encounter conflicts with other Python versions, consider uninstalling them: &lt;pre&gt;&lt;code class="language-bash"&gt;# List all installed Python versions
brew list | grep python

# Uninstall conflicting versions if needed
brew uninstall --ignore-dependencies python@3.11 python@3.13

# Keep only Python 3.11
brew cleanup
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;CoreML Execution Provider (Apple Legacy)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall onnxruntime onnxruntime-coreml
pip install onnxruntime-coreml==1.21.0
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python run.py --execution-provider coreml
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;DirectML Execution Provider (Windows)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall onnxruntime onnxruntime-directml
pip install onnxruntime-directml==1.21.0
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python run.py --execution-provider directml
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;OpenVINO‚Ñ¢ Execution Provider (Intel)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall onnxruntime onnxruntime-openvino
pip install onnxruntime-openvino==1.21.0
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python run.py --execution-provider openvino
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;1. Image/Video Mode&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Execute &lt;code&gt;python run.py&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Choose a source face image and a target image/video.&lt;/li&gt; 
 &lt;li&gt;Click "Start".&lt;/li&gt; 
 &lt;li&gt;The output will be saved in a directory named after the target video.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2. Webcam Mode&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Execute &lt;code&gt;python run.py&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Select a source face image.&lt;/li&gt; 
 &lt;li&gt;Click "Live".&lt;/li&gt; 
 &lt;li&gt;Wait for the preview to appear (10-30 seconds).&lt;/li&gt; 
 &lt;li&gt;Use a screen capture tool like OBS to stream.&lt;/li&gt; 
 &lt;li&gt;To change the face, select a new source image.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Command Line Arguments (Unmaintained)&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;options:
  -h, --help                                               show this help message and exit
  -s SOURCE_PATH, --source SOURCE_PATH                     select a source image
  -t TARGET_PATH, --target TARGET_PATH                     select a target image or video
  -o OUTPUT_PATH, --output OUTPUT_PATH                     select output file or directory
  --frame-processor FRAME_PROCESSOR [FRAME_PROCESSOR ...]  frame processors (choices: face_swapper, face_enhancer, ...)
  --keep-fps                                               keep original fps
  --keep-audio                                             keep original audio
  --keep-frames                                            keep temporary frames
  --many-faces                                             process every face
  --map-faces                                              map source target faces
  --mouth-mask                                             mask the mouth region
  --video-encoder {libx264,libx265,libvpx-vp9}             adjust output video encoder
  --video-quality [0-51]                                   adjust output video quality
  --live-mirror                                            the live camera display as you see it in the front-facing camera frame
  --live-resizable                                         the live camera frame is resizable
  --max-memory MAX_MEMORY                                  maximum amount of RAM in GB
  --execution-provider {cpu} [{cpu} ...]                   available execution provider (choices: cpu, ...)
  --execution-threads EXECUTION_THREADS                    number of execution threads
  -v, --version                                            show program's version number and exit
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Looking for a CLI mode? Using the -s/--source argument will make the run program in cli mode.&lt;/p&gt; 
&lt;h2&gt;Press&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;We are always open to criticism and are ready to improve, that's why we didn't cherry-pick anything.&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://arstechnica.com/information-technology/2024/08/new-ai-tool-enables-real-time-face-swapping-on-webcams-raising-fraud-concerns/"&gt;&lt;em&gt;"Deep-Live-Cam goes viral, allowing anyone to become a digital doppelganger"&lt;/em&gt;&lt;/a&gt; - Ars Technica&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://dataconomy.com/2024/08/15/what-is-deep-live-cam-github-deepfake/"&gt;&lt;em&gt;"Thanks Deep Live Cam, shapeshifters are among us now"&lt;/em&gt;&lt;/a&gt; - Dataconomy&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.newsbytesapp.com/news/science/deep-live-cam-ai-impersonation-tool-goes-viral/story"&gt;&lt;em&gt;"This free AI tool lets you become anyone during video-calls"&lt;/em&gt;&lt;/a&gt; - NewsBytes&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.creativebloq.com/ai/ok-this-viral-ai-live-stream-software-is-truly-terrifying"&gt;&lt;em&gt;"OK, this viral AI live stream software is truly terrifying"&lt;/em&gt;&lt;/a&gt; - Creative Bloq&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://petapixel.com/2024/08/14/deep-live-cam-deepfake-ai-tool-lets-you-become-anyone-in-a-video-call-with-single-photo-mark-zuckerberg-jd-vance-elon-musk/"&gt;&lt;em&gt;"Deepfake AI Tool Lets You Become Anyone in a Video Call With Single Photo"&lt;/em&gt;&lt;/a&gt; - PetaPixel&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.techeblog.com/deep-live-cam-ai-transform-face/"&gt;&lt;em&gt;"Deep-Live-Cam Uses AI to Transform Your Face in Real-Time, Celebrities Included"&lt;/em&gt;&lt;/a&gt; - TechEBlog&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://telegrafi.com/en/a-tool-that-makes-you-look-like-anyone-during-a-video-call-is-going-viral-on-the-Internet/"&gt;&lt;em&gt;"An AI tool that "makes you look like anyone" during a video call is going viral online"&lt;/em&gt;&lt;/a&gt; - Telegrafi&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://decrypt.co/244565/this-deepfake-tool-turning-images-into-livestreams-is-topping-the-github-charts"&gt;&lt;em&gt;"This Deepfake Tool Turning Images Into Livestreams is Topping the GitHub Charts"&lt;/em&gt;&lt;/a&gt; - Emerge&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.digitalmusicnews.com/2024/08/15/face-swapping-ai-real-time-mimic/"&gt;&lt;em&gt;"New Real-Time Face-Swapping AI Allows Anyone to Mimic Famous Faces"&lt;/em&gt;&lt;/a&gt; - Digital Music News&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.diyphotography.net/this-real-time-webcam-deepfake-tool-raises-alarms-about-the-future-of-identity-theft/"&gt;&lt;em&gt;"This real-time webcam deepfake tool raises alarms about the future of identity theft"&lt;/em&gt;&lt;/a&gt; - DIYPhotography&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/watch?time_continue=1074&amp;amp;v=py4Tc-Y8BcY"&gt;&lt;em&gt;"That's Crazy, Oh God. That's Fucking Freaky Dude... That's So Wild Dude"&lt;/em&gt;&lt;/a&gt; - SomeOrdinaryGamers&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/live/mFsCe7AIxq8?feature=shared&amp;amp;t=2686"&gt;&lt;em&gt;"Alright look look look, now look chat, we can do any face we want to look like chat"&lt;/em&gt;&lt;/a&gt; - IShowSpeed&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=wnCghLjqv3s&amp;amp;t=551s"&gt;&lt;em&gt;"They do a pretty good job matching poses, expression and even the lighting"&lt;/em&gt;&lt;/a&gt; - TechLinked (LTT)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.golem.de/news/deepfakes-als-sean-connery-an-der-redaktionskonferenz-teilnahm-2408-188172.html"&gt;&lt;em&gt;"Als Sean Connery an der Redaktionskonferenz teilnahm"&lt;/em&gt;&lt;/a&gt; - Golem.de (German)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://youtu.be/JbUPRmXRUtE?t=3964"&gt;&lt;em&gt;"What the F&lt;/em&gt;**! Why do I look like Vinny Jr? I look exactly like Vinny Jr!? No, this shit is crazy! Bro This is F*** Crazy! "*&lt;/a&gt; - IShowSpeed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://ffmpeg.org/"&gt;ffmpeg&lt;/a&gt;: for making video-related operations easy&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/henryruhs"&gt;Henry&lt;/a&gt;: One of the major contributor in this repo&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/deepinsight"&gt;deepinsight&lt;/a&gt;: for their &lt;a href="https://github.com/deepinsight/insightface"&gt;insightface&lt;/a&gt; project which provided a well-made library and models. Please be reminded that the &lt;a href="https://github.com/deepinsight/insightface?tab=readme-ov-file#license"&gt;use of the model is for non-commercial research purposes only&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/havok2-htwo"&gt;havok2-htwo&lt;/a&gt;: for sharing the code for webcam&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/GosuDRM"&gt;GosuDRM&lt;/a&gt;: for the open version of roop&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pereiraroland26"&gt;pereiraroland26&lt;/a&gt;: Multiple faces support&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vic4key"&gt;vic4key&lt;/a&gt;: For supporting/contributing to this project&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kier007"&gt;kier007&lt;/a&gt;: for improving the user experience&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/qitianai"&gt;qitianai&lt;/a&gt;: for multi-lingual support&lt;/li&gt; 
 &lt;li&gt;and &lt;a href="https://github.com/hacksider/Deep-Live-Cam/graphs/contributors"&gt;all developers&lt;/a&gt; behind libraries used in this project.&lt;/li&gt; 
 &lt;li&gt;Footnote: Please be informed that the base author of the code is &lt;a href="https://github.com/s0md3v/roop"&gt;s0md3v&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;All the wonderful users who helped make this project go viral by starring the repo ‚ù§Ô∏è&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://github.com/hacksider/Deep-Live-Cam/stargazers"&gt;&lt;img src="https://reporoster.com/stars/hacksider/Deep-Live-Cam" alt="Stargazers" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributions&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://repobeats.axiom.co/api/embed/fec8e29c45dfdb9c5916f3a7830e1249308d20e1.svg?sanitize=true" alt="Alt" title="Repobeats analytics image" /&gt;&lt;/p&gt; 
&lt;h2&gt;Stars to the Moon üöÄ&lt;/h2&gt; 
&lt;a href="https://star-history.com/#hacksider/deep-live-cam&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>bunkerity/bunkerweb</title>
      <link>https://github.com/bunkerity/bunkerweb</link>
      <description>&lt;p&gt;üõ°Ô∏è Open-source and next-generation Web Application Firewall (WAF)&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img alt="BunkerWeb logo" src="https://github.com/bunkerity/bunkerweb/raw/v1.6.7/misc/logo.png" height="100" width="350" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://img.shields.io/github/v/release/bunkerity/bunkerweb?label=stable" /&gt; &lt;img src="https://img.shields.io/github/v/release/bunkerity/bunkerweb?include_prereleases&amp;amp;label=latest" /&gt; &lt;br /&gt; &lt;img src="https://img.shields.io/github/last-commit/bunkerity/bunkerweb" /&gt; &lt;img src="https://img.shields.io/github/issues/bunkerity/bunkerweb" /&gt; &lt;img src="https://img.shields.io/github/issues-pr/bunkerity/bunkerweb" /&gt; &lt;br /&gt; &lt;img src="https://img.shields.io/github/actions/workflow/status/bunkerity/bunkerweb/dev.yml?branch=dev&amp;amp;label=CI%2FCD%20dev" /&gt; &lt;img src="https://img.shields.io/github/actions/workflow/status/bunkerity/bunkerweb/staging.yml?branch=staging&amp;amp;label=CI%2FCD%20staging" /&gt; &lt;a href="https://www.bestpractices.dev/projects/8001"&gt; &lt;img src="https://www.bestpractices.dev/projects/8001/badge" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; üåê &lt;a href="https://www.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=github"&gt;Website&lt;/a&gt; | ü§ù &lt;a href="https://panel.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=github"&gt;Panel&lt;/a&gt; | üìì &lt;a href="https://docs.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=github"&gt;Documentation&lt;/a&gt; | üë®‚Äçüíª &lt;a href="https://demo.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=github"&gt;Demo&lt;/a&gt; | üì± &lt;a href="https://demo-ui.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=github"&gt;Demo UI&lt;/a&gt; | üß© &lt;a href="https://github.com/bunkerity/bunkerweb-templates"&gt;Templates&lt;/a&gt; | üõ°Ô∏è &lt;a href="https://github.com/bunkerity/bunkerweb/raw/v1.6.7/examples"&gt;Examples&lt;/a&gt; &lt;br /&gt; üí¨ &lt;a href="https://discord.com/invite/fTf46FmtyD"&gt;Chat&lt;/a&gt; | üìù &lt;a href="https://github.com/bunkerity/bunkerweb/discussions"&gt;Forum&lt;/a&gt; | üó∫Ô∏è &lt;a href="https://www.bunkerweb.io/threatmap/?utm_campaign=self&amp;amp;utm_source=github"&gt;Threatmap&lt;/a&gt; | üìä &lt;a href="https://status.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=github"&gt;Status&lt;/a&gt; | üîé &lt;a href="https://forms.gle/e3VgymAteYPnwM1j9"&gt;Feedback&lt;/a&gt; &lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üõ°Ô∏è Make security by default great again!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;BunkerWeb&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img alt="Overview banner" src="https://github.com/bunkerity/bunkerweb/raw/v1.6.7/docs/assets/img/intro-overview.svg?sanitize=true" /&gt; &lt;/p&gt; 
&lt;p&gt;BunkerWeb is a next-generation, open-source Web Application Firewall (WAF).&lt;/p&gt; 
&lt;p&gt;Being a full-featured web server (based on &lt;a href="https://nginx.org/"&gt;NGINX&lt;/a&gt; under the hood), it will protect your web services to make them "secure by default." BunkerWeb integrates seamlessly into your existing environments (&lt;a href="https://docs.bunkerweb.io/1.6.7/integrations/?utm_campaign=self&amp;amp;utm_source=github#linux"&gt;Linux&lt;/a&gt;, &lt;a href="https://docs.bunkerweb.io/1.6.7/integrations/?utm_campaign=self&amp;amp;utm_source=github#docker"&gt;Docker&lt;/a&gt;, &lt;a href="https://docs.bunkerweb.io/1.6.7/integrations/?utm_campaign=self&amp;amp;utm_source=github#swarm"&gt;Swarm&lt;/a&gt;, &lt;a href="https://docs.bunkerweb.io/1.6.7/integrations/?utm_campaign=self&amp;amp;utm_source=github#kubernetes"&gt;Kubernetes&lt;/a&gt;, ‚Ä¶) as a reverse proxy and is fully configurable (don't panic, there is an &lt;a href="https://docs.bunkerweb.io/1.6.7/web-ui/?utm_campaign=self&amp;amp;utm_source=github"&gt;awesome web UI&lt;/a&gt; if you don't like the CLI) to meet your own use cases. In other words, cybersecurity is no longer a hassle.&lt;/p&gt; 
&lt;p&gt;BunkerWeb contains primary &lt;a href="https://docs.bunkerweb.io/1.6.7/advanced/?utm_campaign=self&amp;amp;utm_source=github#security-tuning"&gt;security features&lt;/a&gt; as part of the core but can be easily extended with additional ones thanks to a &lt;a href="https://docs.bunkerweb.io/1.6.7/plugins/?utm_campaign=self&amp;amp;utm_source=github"&gt;plugin system&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Why BunkerWeb?&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/c3fed740-28d8-4335-ab05-113a9e815b4f"&gt;https://github.com/user-attachments/assets/c3fed740-28d8-4335-ab05-113a9e815b4f&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Easy integration into existing environments&lt;/strong&gt;: Seamlessly integrate BunkerWeb into various environments such as Linux, Docker, Swarm, Kubernetes, and more. Enjoy a smooth transition and hassle-free implementation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Highly customizable&lt;/strong&gt;: Tailor BunkerWeb to your specific requirements with ease. Enable, disable, and configure features effortlessly, allowing you to customize the security settings according to your unique use case.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Secure by default&lt;/strong&gt;: BunkerWeb provides out-of-the-box, hassle-free minimal security for your web services. Experience peace of mind and enhanced protection right from the start.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Awesome web UI&lt;/strong&gt;: Take control of BunkerWeb more efficiently with the exceptional web user interface (UI). Navigate settings and configurations effortlessly through a user-friendly graphical interface, eliminating the need for the command-line interface (CLI).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Plugin system&lt;/strong&gt;: Extend the capabilities of BunkerWeb to meet your own use cases. Seamlessly integrate additional security measures and customize the functionality of BunkerWeb according to your specific requirements.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Free as in "freedom"&lt;/strong&gt;: BunkerWeb is licensed under the free &lt;a href="https://www.gnu.org/licenses/agpl-3.0.en.html"&gt;AGPLv3 license&lt;/a&gt;, embracing the principles of freedom and openness. Enjoy the freedom to use, modify, and distribute the software, backed by a supportive community.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Professional services&lt;/strong&gt;: Get technical support, tailored consulting, and custom development directly from the maintainers of BunkerWeb. Visit the &lt;a href="https://panel.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=github"&gt;Bunker Panel&lt;/a&gt; for more information.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Security features&lt;/h2&gt; 
&lt;p&gt;A non-exhaustive list of security features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;HTTPS&lt;/strong&gt; support with transparent &lt;strong&gt;Let's Encrypt&lt;/strong&gt; automation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;State-of-the-art web security&lt;/strong&gt;: HTTP security headers, prevent leaks, TLS hardening, ...&lt;/li&gt; 
 &lt;li&gt;Integrated &lt;strong&gt;ModSecurity WAF&lt;/strong&gt; with the &lt;strong&gt;OWASP Core Rule Set&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic ban&lt;/strong&gt; of strange behaviors based on HTTP status codes&lt;/li&gt; 
 &lt;li&gt;Apply &lt;strong&gt;connection and request limits&lt;/strong&gt; for clients&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Block bots&lt;/strong&gt; by asking them to solve a &lt;strong&gt;challenge&lt;/strong&gt; (e.g., cookie, JavaScript, captcha, hCaptcha, or reCAPTCHA)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Block known bad IPs&lt;/strong&gt; with external blacklists and DNSBL&lt;/li&gt; 
 &lt;li&gt;And much more...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Learn more about the core security features in the &lt;a href="https://docs.bunkerweb.io/1.6.7/advanced/?utm_campaign=self&amp;amp;utm_source=github#security-tuning"&gt;security tuning&lt;/a&gt; section of the documentation.&lt;/p&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/6fc0e3c1-d353-4a84-bad0-15bf9b6623a5"&gt;https://github.com/user-attachments/assets/6fc0e3c1-d353-4a84-bad0-15bf9b6623a5&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;A demo website protected with BunkerWeb is available at &lt;a href="https://demo.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=github"&gt;demo.bunkerweb.io&lt;/a&gt;. Feel free to visit it and perform some security tests.&lt;/p&gt; 
&lt;h2&gt;Web UI&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/a3ed56f8-c124-4ca9-b8b3-4be0913b3078"&gt;https://github.com/user-attachments/assets/a3ed56f8-c124-4ca9-b8b3-4be0913b3078&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;BunkerWeb offers an optional &lt;a href="https://raw.githubusercontent.com/bunkerity/bunkerweb/master/web-ui.md"&gt;user interface&lt;/a&gt; to manage your instances and their configurations. An online read-only demo is available at &lt;a href="https://demo-ui.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=doc"&gt;demo-ui.bunkerweb.io&lt;/a&gt;, feel free to test it yourself.&lt;/p&gt; 
&lt;h2&gt;BunkerWeb Cloud&lt;/h2&gt; 
&lt;p&gt;Don't want to self-host and manage your own BunkerWeb instance(s)? You might be interested in BunkerWeb Cloud, our fully managed SaaS offering for BunkerWeb.&lt;/p&gt; 
&lt;p&gt;Order your &lt;a href="https://panel.bunkerweb.io/store/bunkerweb-cloud?utm_campaign=self&amp;amp;utm_source=doc"&gt;BunkerWeb Cloud instance&lt;/a&gt; and get access to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A fully managed BunkerWeb instance hosted in our cloud&lt;/li&gt; 
 &lt;li&gt;All BunkerWeb features, including PRO ones&lt;/li&gt; 
 &lt;li&gt;A monitoring platform with dashboards and alerts&lt;/li&gt; 
 &lt;li&gt;Technical support to assist you with configuration&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you are interested in the BunkerWeb Cloud offering, don't hesitate to &lt;a href="https://panel.bunkerweb.io/contact.php?utm_campaign=self&amp;amp;utm_source=doc"&gt;contact us&lt;/a&gt; so we can discuss your needs.&lt;/p&gt; 
&lt;h2&gt;PRO version&lt;/h2&gt; 
&lt;p&gt;Want to quickly test BunkerWeb PRO for one month? Use the code &lt;code&gt;freetrial&lt;/code&gt; when placing your order on the &lt;a href="https://panel.bunkerweb.io/store/bunkerweb-pro?utm_campaign=self&amp;amp;utm_source=doc"&gt;BunkerWeb panel&lt;/a&gt; or by clicking &lt;a href="https://panel.bunkerweb.io/cart.php?a=add&amp;amp;pid=19&amp;amp;promocode=freetrial&amp;amp;utm_campaign=self&amp;amp;utm_source=doc"&gt;here&lt;/a&gt; to directly to apply the promo code (will be effective at checkout).&lt;/p&gt; 
&lt;p&gt;When using BunkerWeb, you have the choice of the version you want to use: open-source or PRO.&lt;/p&gt; 
&lt;p&gt;Whether it's enhanced security, an enriched user experience, or technical monitoring, the BunkerWeb PRO version allows you to fully benefit from BunkerWeb and meet your professional needs.&lt;/p&gt; 
&lt;p&gt;In the documentation or the user interface, PRO features are annotated with a crown &lt;img src="https://docs.bunkerweb.io/1.6.7/assets/img/pro-icon.svg?sanitize=true" alt="crown pro icon" height="32px" width="32px" /&gt; to distinguish them from those integrated into the open-source version.&lt;/p&gt; 
&lt;p&gt;You can upgrade from the open-source version to the PRO one easily and at any time. The process is straightforward:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Claim your &lt;a href="https://panel.bunkerweb.io/store/bunkerweb-pro?utm_campaign=self&amp;amp;utm_source=doc"&gt;free trial on the BunkerWeb panel&lt;/a&gt; by using the &lt;code&gt;freetrial&lt;/code&gt; promo code at checkout&lt;/li&gt; 
 &lt;li&gt;Once connected to the client area, copy your PRO license key&lt;/li&gt; 
 &lt;li&gt;Paste your license key into BunkerWeb using the &lt;a href="https://docs.bunkerweb.io/1.6.7/web-ui/#upgrade-to-pro"&gt;web UI&lt;/a&gt; or a &lt;a href="https://docs.bunkerweb.io/1.6.7/features/#pro"&gt;specific setting&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Do not hesitate to visit the &lt;a href="https://panel.bunkerweb.io/knowledgebase?utm_campaign=self&amp;amp;utm_source=doc"&gt;BunkerWeb panel&lt;/a&gt; or &lt;a href="https://panel.bunkerweb.io/contact.php?utm_campaign=self&amp;amp;utm_source=doc"&gt;contact us&lt;/a&gt; if you have any questions regarding the PRO version.&lt;/p&gt; 
&lt;h2&gt;Professional services&lt;/h2&gt; 
&lt;p&gt;Get the most out of BunkerWeb by getting professional services directly from the maintainers of the project. From technical support to tailored consulting and development, we are here to assist you in the security of your web services.&lt;/p&gt; 
&lt;p&gt;You will find more information by visiting the &lt;a href="https://panel.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=doc"&gt;BunkerWeb Panel&lt;/a&gt;, our dedicated platform for professional services.&lt;/p&gt; 
&lt;p&gt;Don't hesitate to &lt;a href="https://panel.bunkerweb.io/contact.php?utm_campaign=self&amp;amp;utm_source=doc"&gt;contact us&lt;/a&gt; if you have any questions; we will be more than happy to respond to your needs.&lt;/p&gt; 
&lt;h2&gt;Ecosystem, community, and resources&lt;/h2&gt; 
&lt;p&gt;Official websites, tools, and resources about BunkerWeb:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=doc"&gt;&lt;strong&gt;Website&lt;/strong&gt;&lt;/a&gt;: get more information, news, and articles about BunkerWeb&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://panel.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=doc"&gt;&lt;strong&gt;Panel&lt;/strong&gt;&lt;/a&gt;: dedicated platform to order and manage professional services (e.g., technical support) around BunkerWeb&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.bunkerweb.io"&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/a&gt;: technical documentation of the BunkerWeb solution&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://demo.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=doc"&gt;&lt;strong&gt;Demo&lt;/strong&gt;&lt;/a&gt;: demonstration website of BunkerWeb, don't hesitate to attempt attacks to test the robustness of the solution&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://demo-ui.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=doc"&gt;&lt;strong&gt;Web UI&lt;/strong&gt;&lt;/a&gt;: online read-only demo of the web UI of BunkerWeb&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.bunkerweb.io/threatmap/?utm_campaign=self&amp;amp;utm_source=doc"&gt;&lt;strong&gt;Threatmap&lt;/strong&gt;&lt;/a&gt;: live cyber attack blocked by BunkerWeb instances all around the world&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Community and social networks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://discord.com/invite/fTf46FmtyD"&gt;&lt;strong&gt;Discord&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.linkedin.com/company/bunkerity/"&gt;&lt;strong&gt;LinkedIn&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/bunkerity"&gt;&lt;strong&gt;Twitter&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.reddit.com/r/BunkerWeb/"&gt;&lt;strong&gt;Reddit&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Concepts&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img alt="Concepts banner" src="https://github.com/bunkerity/bunkerweb/raw/v1.6.7/docs/assets/img/concepts.svg?sanitize=true" /&gt; &lt;/p&gt; 
&lt;p&gt;You will find more information about the key concepts of BunkerWeb in the &lt;a href="https://docs.bunkerweb.io/1.6.7/concepts/?utm_campaign=self&amp;amp;utm_source=github"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Integrations&lt;/h2&gt; 
&lt;p&gt;The first concept is the integration of BunkerWeb into the target environment. We prefer to use the word "integration" instead of "installation" because one of the goals of BunkerWeb is to integrate seamlessly into existing environments.&lt;/p&gt; 
&lt;p&gt;The following integrations are officially supported:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.bunkerweb.io/1.6.7/integrations/?utm_campaign=self&amp;amp;utm_source=github#docker"&gt;Docker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.bunkerweb.io/1.6.7/integrations/?utm_campaign=self&amp;amp;utm_source=github#linux"&gt;Linux&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.bunkerweb.io/1.6.7/integrations/?utm_campaign=self&amp;amp;utm_source=github#docker-autoconf"&gt;Docker autoconf&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.bunkerweb.io/1.6.7/integrations/?utm_campaign=self&amp;amp;utm_source=github#kubernetes"&gt;Kubernetes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.bunkerweb.io/1.6.7/integrations/?utm_campaign=self&amp;amp;utm_source=github#swarm"&gt;Swarm&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.bunkerweb.io/1.6.7/integrations/?utm_campaign=self&amp;amp;utm_source=github#microsoft-azure"&gt;Microsoft Azure&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Settings&lt;/h2&gt; 
&lt;p&gt;Once BunkerWeb is integrated into your environment, you will need to configure it to serve and protect your web applications.&lt;/p&gt; 
&lt;p&gt;The configuration of BunkerWeb is done by using what we call the "settings" or "variables." Each setting is identified by a name such as &lt;code&gt;AUTO_LETS_ENCRYPT&lt;/code&gt; or &lt;code&gt;USE_ANTIBOT&lt;/code&gt;. You can assign values to the settings to configure BunkerWeb.&lt;/p&gt; 
&lt;p&gt;Here is a dummy example of a BunkerWeb configuration:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-conf"&gt;SERVER_NAME=www.example.com
AUTO_LETS_ENCRYPT=yes
USE_ANTIBOT=captcha
REFERRER_POLICY=no-referrer
USE_MODSECURITY=no
USE_GZIP=yes
USE_BROTLI=no
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Multisite mode&lt;/h2&gt; 
&lt;p&gt;The multisite mode is a crucial concept to understand when using BunkerWeb. Because the goal is to protect web applications, we intrinsically inherit the concept of "virtual host" or "vhost" (more info &lt;a href="https://en.wikipedia.org/wiki/Virtual_hosting"&gt;here&lt;/a&gt;) which makes it possible to serve multiple web applications from a single (or a cluster of) instance.&lt;/p&gt; 
&lt;p&gt;By default, the multisite mode of BunkerWeb is disabled, which means that only one web application will be served and all the settings will be applied to it. The typical use case is when you have a single application to protect: you don't have to worry about the multisite, and the default behavior should be the right one for you.&lt;/p&gt; 
&lt;p&gt;When multisite mode is enabled, BunkerWeb will serve and protect multiple web applications. Each web application is identified by a unique server name and has its own set of settings. The typical use case is when you have multiple applications to protect and you want to use a single (or a cluster depending on the integration) instance of BunkerWeb.&lt;/p&gt; 
&lt;h2&gt;Custom configurations&lt;/h2&gt; 
&lt;p&gt;Because meeting all the use cases only using the settings is not an option (even with &lt;a href="https://docs.bunkerweb.io/1.6.7/plugins/?utm_campaign=self&amp;amp;utm_source=github"&gt;external plugins&lt;/a&gt;), you can use custom configurations to solve your specific challenges.&lt;/p&gt; 
&lt;p&gt;Under the hood, BunkerWeb uses the notorious NGINX web server, that's why you can leverage its configuration system for your specific needs. Custom NGINX configurations can be included in different &lt;a href="https://docs.nginx.com/nginx/admin-guide/basic-functionality/managing-configuration-files/#contexts"&gt;contexts&lt;/a&gt; like HTTP or server (all servers and/or specific server block).&lt;/p&gt; 
&lt;p&gt;Another core component of BunkerWeb is the ModSecurity Web Application Firewall: you can also use custom configurations to fix some false positives or add custom rules, for example.&lt;/p&gt; 
&lt;h2&gt;Database&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img alt="Database model" src="https://github.com/bunkerity/bunkerweb/raw/v1.6.7/docs/assets/img/bunkerweb_db.svg?sanitize=true" /&gt; &lt;/p&gt; 
&lt;p&gt;The state of the current configuration of BunkerWeb is stored in a backend database which contains the following data:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Settings defined for all the services&lt;/li&gt; 
 &lt;li&gt;Custom configurations&lt;/li&gt; 
 &lt;li&gt;BunkerWeb instances&lt;/li&gt; 
 &lt;li&gt;Metadata about job execution&lt;/li&gt; 
 &lt;li&gt;Cached files&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The following backend databases are supported: SQLite, MariaDB, MySQL, and PostgreSQL.&lt;/p&gt; 
&lt;h2&gt;Scheduler&lt;/h2&gt; 
&lt;p&gt;To make things automagically work together, a dedicated service called the scheduler is in charge of:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Storing the settings and custom configurations inside the database&lt;/li&gt; 
 &lt;li&gt;Executing various tasks (called jobs)&lt;/li&gt; 
 &lt;li&gt;Generating a configuration which is understood by BunkerWeb&lt;/li&gt; 
 &lt;li&gt;Being the intermediary for other services (like web UI or autoconf)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In other words, the scheduler is the brain of BunkerWeb.&lt;/p&gt; 
&lt;h1&gt;Setup&lt;/h1&gt; 
&lt;!--## BunkerWeb Cloud

&lt;p align="center"&gt;
	&lt;img alt="Docker banner" src="https://github.com/bunkerity/bunkerweb/raw/v1.6.7/docs/assets/img/bunkerweb-cloud.webp" /&gt;
&lt;/p&gt;

BunkerWeb Cloud is the easiest way to get started with BunkerWeb. It offers you a fully managed BunkerWeb service with no hassle. Think of it like a BunkerWeb-as-a-Service!

You will find more information about BunkerWeb Cloud beta [here](https://www.bunkerweb.io/cloud?utm_campaign=self&amp;utm_source=docs) and you can apply for free [in the BunkerWeb panel](https://panel.bunkerweb.io/store/bunkerweb-cloud?utm_campaign=self&amp;utm_source=docs).
--&gt; 
&lt;h2&gt;Linux&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img alt="Linux banner" src="https://github.com/bunkerity/bunkerweb/raw/v1.6.7/docs/assets/img/integration-linux.svg?sanitize=true" /&gt; &lt;/p&gt; 
&lt;p&gt;List of supported Linux distros:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Debian 12 "Bookworm"&lt;/li&gt; 
 &lt;li&gt;Debian 13 "Trixie"&lt;/li&gt; 
 &lt;li&gt;Ubuntu 22.04 "Jammy"&lt;/li&gt; 
 &lt;li&gt;Ubuntu 24.04 "Noble"&lt;/li&gt; 
 &lt;li&gt;Fedora 42&lt;/li&gt; 
 &lt;li&gt;Fedora 43&lt;/li&gt; 
 &lt;li&gt;RHEL 8.10&lt;/li&gt; 
 &lt;li&gt;RHEL 9.6&lt;/li&gt; 
 &lt;li&gt;RHEL 10.0&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You will find more information in the &lt;a href="https://docs.bunkerweb.io/1.5.10/integrations/?utm_campaign=self&amp;amp;utm_source=github#linux"&gt;Linux section&lt;/a&gt; of the documentation.&lt;/p&gt; 
&lt;h2&gt;Docker&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img alt="Docker banner" src="https://github.com/bunkerity/bunkerweb/raw/v1.6.7/docs/assets/img/integration-docker.svg?sanitize=true" /&gt; &lt;/p&gt; 
&lt;p&gt;We provide ready-to-use prebuilt images for x64, x86, armv7, and arm64 platforms on &lt;a href="https://hub.docker.com/u/bunkerity"&gt;Docker Hub&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Docker integration key concepts are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Environment variables&lt;/strong&gt; to configure BunkerWeb&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scheduler&lt;/strong&gt; container to store configuration and execute jobs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Networks&lt;/strong&gt; to expose ports for clients and connect to upstream web services&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You will find more information in the &lt;a href="https://docs.bunkerweb.io/1.6.7/integrations/?utm_campaign=self&amp;amp;utm_source=github#docker"&gt;Docker integration section&lt;/a&gt; of the documentation.&lt;/p&gt; 
&lt;h2&gt;Docker autoconf&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img alt="Docker autoconf banner" src="https://github.com/bunkerity/bunkerweb/raw/v1.6.7/docs/assets/img/integration-autoconf.svg?sanitize=true" /&gt; &lt;/p&gt; 
&lt;p&gt;The downside of using environment variables is that the container needs to be recreated each time there is an update, which is not very convenient. To counter that issue, you can use another image called &lt;strong&gt;autoconf&lt;/strong&gt; which will listen for Docker events and automatically reconfigure BunkerWeb in real-time without recreating the container.&lt;/p&gt; 
&lt;p&gt;Instead of defining environment variables for the BunkerWeb container, you simply add &lt;strong&gt;labels&lt;/strong&gt; to your web applications containers and the &lt;strong&gt;autoconf&lt;/strong&gt; will "automagically" take care of the rest.&lt;/p&gt; 
&lt;p&gt;You will find more information in the &lt;a href="https://docs.bunkerweb.io/1.6.7/integrations/?utm_campaign=self&amp;amp;utm_source=github#docker-autoconf"&gt;Docker autoconf section&lt;/a&gt; of the documentation.&lt;/p&gt; 
&lt;h2&gt;Kubernetes&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img alt="Kubernetes banner" src="https://github.com/bunkerity/bunkerweb/raw/v1.6.7/docs/assets/img/integration-kubernetes.svg?sanitize=true" /&gt; &lt;/p&gt; 
&lt;p&gt;The autoconf acts as an &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/"&gt;Ingress controller&lt;/a&gt; and will configure the BunkerWeb instances according to the &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/"&gt;Ingress resources&lt;/a&gt;. It also monitors other Kubernetes objects like &lt;a href="https://kubernetes.io/docs/concepts/configuration/configmap/"&gt;ConfigMap&lt;/a&gt; for custom configurations.&lt;/p&gt; 
&lt;p&gt;The official &lt;a href="https://helm.sh/"&gt;Helm chart&lt;/a&gt; for BunkerWeb is available in the &lt;a href="https://github.com/bunkerity/bunkerweb-helm"&gt;bunkerity/bunkerweb-helm repository&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You will find more information in the &lt;a href="https://docs.bunkerweb.io/1.6.7/integrations/?utm_campaign=self&amp;amp;utm_source=github#kubernetes"&gt;Kubernetes section&lt;/a&gt; of the documentation.&lt;/p&gt; 
&lt;h2&gt;Microsoft Azure&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img alt="Azure banner" src="https://github.com/bunkerity/bunkerweb/raw/v1.6.7/docs/assets/img/integration-azure.webp" /&gt; &lt;/p&gt; 
&lt;p&gt;BunkerWeb is referenced in the &lt;a href="https://azuremarketplace.microsoft.com/fr-fr/marketplace/apps/bunkerity.bunkerweb?tab=Overview"&gt;Azure Marketplace&lt;/a&gt; and an ARM template is available in the &lt;a href="https://github.com/bunkerity/bunkerweb/raw/v1.6.7/misc/integrations/azure-arm-template.json"&gt;misc folder&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You will find more information in the &lt;a href="https://docs.bunkerweb.io/1.6.7/integrations/?utm_campaign=self&amp;amp;utm_source=github#microsoft-azure"&gt;Microsoft Azure section&lt;/a&gt; of the documentation.&lt;/p&gt; 
&lt;h2&gt;Swarm&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img alt="Swarm banner" src="https://github.com/bunkerity/bunkerweb/raw/v1.6.7/docs/assets/img/integration-swarm.svg?sanitize=true" /&gt; &lt;/p&gt; 
&lt;p&gt;To automatically configure BunkerWeb instances, a special service, called &lt;strong&gt;autoconf&lt;/strong&gt; will listen for Docker Swarm events like service creation or deletion and automatically configure the &lt;strong&gt;BunkerWeb instances&lt;/strong&gt; in real-time without downtime.&lt;/p&gt; 
&lt;p&gt;Like the &lt;a href="https://docs.bunkerweb.io/1.6.7/integrations/?utm_campaign=self&amp;amp;utm_source=github#docker-autoconf"&gt;Docker autoconf integration&lt;/a&gt;, configuration for web services is defined using labels starting with the special &lt;strong&gt;bunkerweb.&lt;/strong&gt; prefix.&lt;/p&gt; 
&lt;p&gt;You will find more information in the &lt;a href="https://docs.bunkerweb.io/1.6.7/integrations/?utm_campaign=self&amp;amp;utm_source=github#swarm"&gt;Swarm section&lt;/a&gt; of the documentation.&lt;/p&gt; 
&lt;h1&gt;Quickstart guide&lt;/h1&gt; 
&lt;p&gt;Once you have set up BunkerWeb with the integration of your choice, you can follow the &lt;a href="https://docs.bunkerweb.io/1.6.7/quickstart-guide/?utm_campaign=self&amp;amp;utm_source=github"&gt;quickstart guide&lt;/a&gt; that will cover the installation and first configuration to protect a web service.&lt;/p&gt; 
&lt;h1&gt;Security tuning&lt;/h1&gt; 
&lt;p&gt;BunkerWeb offers many security features that you can configure with &lt;a href="https://docs.bunkerweb.io/1.6.7/features/?utm_campaign=self&amp;amp;utm_source=github"&gt;features&lt;/a&gt;. Even if the default values of settings ensure a minimal "security by default," we strongly recommend you to tune them. By doing so, you will be able to ensure a security level of your choice but also manage false positives.&lt;/p&gt; 
&lt;p&gt;You will find more information in the &lt;a href="https://docs.bunkerweb.io/1.6.7/advanced/?utm_campaign=self&amp;amp;utm_source=github#security-tuning"&gt;security tuning section&lt;/a&gt; of the documentation.&lt;/p&gt; 
&lt;h1&gt;Settings&lt;/h1&gt; 
&lt;p&gt;As a general rule, when multisite mode is enabled, if you want to apply settings with multisite context to a specific server, you will need to add the primary (first) server name as a prefix like &lt;code&gt;www.example.com_USE_ANTIBOT=captcha&lt;/code&gt; or &lt;code&gt;myapp.example.com_USE_GZIP=yes&lt;/code&gt;, for example.&lt;/p&gt; 
&lt;p&gt;When settings are considered as "multiple," it means that you can have multiple groups of settings for the same feature by adding numbers as suffixes like &lt;code&gt;REVERSE_PROXY_URL_1=/subdir&lt;/code&gt;, &lt;code&gt;REVERSE_PROXY_HOST_1=http://myhost1&lt;/code&gt;, &lt;code&gt;REVERSE_PROXY_URL_2=/anotherdir&lt;/code&gt;, &lt;code&gt;REVERSE_PROXY_HOST_2=http://myhost2&lt;/code&gt;, ... for example.&lt;/p&gt; 
&lt;p&gt;Check the &lt;a href="https://docs.bunkerweb.io/1.6.7/features/?utm_campaign=self&amp;amp;utm_source=github"&gt;features section&lt;/a&gt; of the documentation to get the full list.&lt;/p&gt; 
&lt;h1&gt;Web UI&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/a3ed56f8-c124-4ca9-b8b3-4be0913b3078"&gt;https://github.com/user-attachments/assets/a3ed56f8-c124-4ca9-b8b3-4be0913b3078&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The "Web UI" is a web application that helps you manage your BunkerWeb instance using a user-friendly interface instead of the command-line one.&lt;/p&gt; 
&lt;p&gt;Here is the list of features offered by the web UI:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Get a comprehensive view of the blocked attacks&lt;/li&gt; 
 &lt;li&gt;Start, stop, restart, and reload your BunkerWeb instance&lt;/li&gt; 
 &lt;li&gt;Add, edit, and delete settings for your web applications&lt;/li&gt; 
 &lt;li&gt;Add, edit, and delete custom configurations for NGINX and ModSecurity&lt;/li&gt; 
 &lt;li&gt;Install and uninstall external plugins&lt;/li&gt; 
 &lt;li&gt;Explore the cached files&lt;/li&gt; 
 &lt;li&gt;Monitor job execution and restart them when needed&lt;/li&gt; 
 &lt;li&gt;View the logs and search patterns&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You will find more information in the &lt;a href="https://docs.bunkerweb.io/1.6.7/web-ui/?utm_campaign=self&amp;amp;utm_source=github"&gt;Web UI section&lt;/a&gt; of the documentation.&lt;/p&gt; 
&lt;h1&gt;Plugins&lt;/h1&gt; 
&lt;p&gt;BunkerWeb comes with a plugin system to make it possible to easily add new features. Once a plugin is installed, you can manage it using additional settings defined by the plugin.&lt;/p&gt; 
&lt;p&gt;Here is the list of "official" plugins that we maintain (see the &lt;a href="https://github.com/bunkerity/bunkerweb-plugins/?utm_campaign=self&amp;amp;utm_source=github"&gt;bunkerweb-plugins&lt;/a&gt; repository for more information):&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Name&lt;/th&gt; 
   &lt;th align="center"&gt;Version&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;th align="center"&gt;Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;ClamAV&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;1.9&lt;/td&gt; 
   &lt;td align="left"&gt;Automatically scans uploaded files with the ClamAV antivirus engine and denies the request when a file is detected as malicious.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/bunkerity/bunkerweb-plugins/tree/main/clamav"&gt;bunkerweb-plugins/clamav&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Coraza&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;1.9&lt;/td&gt; 
   &lt;td align="left"&gt;Inspect requests using the Coraza WAF (alternative of ModSecurity).&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/bunkerity/bunkerweb-plugins/tree/main/coraza"&gt;bunkerweb-plugins/coraza&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Discord&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;1.9&lt;/td&gt; 
   &lt;td align="left"&gt;Send security notifications to a Discord channel using a Webhook.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/bunkerity/bunkerweb-plugins/tree/main/discord"&gt;bunkerweb-plugins/discord&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Slack&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;1.9&lt;/td&gt; 
   &lt;td align="left"&gt;Send security notifications to a Slack channel using a Webhook.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/bunkerity/bunkerweb-plugins/tree/main/slack"&gt;bunkerweb-plugins/slack&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;VirusTotal&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;1.9&lt;/td&gt; 
   &lt;td align="left"&gt;Automatically scans uploaded files with the VirusTotal API and denies the request when a file is detected as malicious.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/bunkerity/bunkerweb-plugins/tree/main/virustotal"&gt;bunkerweb-plugins/virustotal&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;WebHook&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;1.9&lt;/td&gt; 
   &lt;td align="left"&gt;Send security notifications to a custom HTTP endpoint using a Webhook.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/bunkerity/bunkerweb-plugins/tree/main/webhook"&gt;bunkerweb-plugins/webhook&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;You will find more information in the &lt;a href="https://docs.bunkerweb.io/1.6.7/plugins/?utm_campaign=self&amp;amp;utm_source=github"&gt;plugins section&lt;/a&gt; of the documentation.&lt;/p&gt; 
&lt;h1&gt;Language Support &amp;amp; Localization&lt;/h1&gt; 
&lt;p&gt;BunkerWeb UI supports multiple languages. Translations are managed in the &lt;code&gt;src/ui/app/static/locales&lt;/code&gt; directory. The following languages are currently available:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;English (en)&lt;/li&gt; 
 &lt;li&gt;French (fr)&lt;/li&gt; 
 &lt;li&gt;Arabic (ar)&lt;/li&gt; 
 &lt;li&gt;Bengali (bn)&lt;/li&gt; 
 &lt;li&gt;Spanish (es)&lt;/li&gt; 
 &lt;li&gt;Hindi (hi)&lt;/li&gt; 
 &lt;li&gt;Portuguese (pt)&lt;/li&gt; 
 &lt;li&gt;Russian (ru)&lt;/li&gt; 
 &lt;li&gt;Urdu (ur)&lt;/li&gt; 
 &lt;li&gt;Chinese (zh)&lt;/li&gt; 
 &lt;li&gt;German (de)&lt;/li&gt; 
 &lt;li&gt;Italian (it)&lt;/li&gt; 
 &lt;li&gt;Turkish (tr)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/bunkerity/bunkerweb/raw/v1.6.7/src/ui/app/static/locales/README.md"&gt;locales/README.md&lt;/a&gt; for details on translation provenance and review status.&lt;/p&gt; 
&lt;h2&gt;Contributing Translations&lt;/h2&gt; 
&lt;p&gt;We welcome contributions to improve or add new locale files!&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;How to contribute a translation:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Edit the &lt;code&gt;src/ui/app/lang_config.py&lt;/code&gt; file to add your language (code, name, flag, english_name).&lt;/li&gt; 
 &lt;li&gt;Copy &lt;code&gt;en.json&lt;/code&gt; as a template in &lt;code&gt;src/ui/app/static/locales/&lt;/code&gt;, rename it to your language code (e.g., &lt;code&gt;de.json&lt;/code&gt; for German).&lt;/li&gt; 
 &lt;li&gt;Translate the values in your new file.&lt;/li&gt; 
 &lt;li&gt;Update the table in &lt;code&gt;locales/README.md&lt;/code&gt; to add your language and indicate who created/reviewed it.&lt;/li&gt; 
 &lt;li&gt;Submit a pull request.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;For updates, edit the relevant file and update the provenance table as needed.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/bunkerity/bunkerweb/raw/v1.6.7/src/ui/app/static/locales/README.md"&gt;locales/README.md&lt;/a&gt; for full guidelines.&lt;/p&gt; 
&lt;h1&gt;Support&lt;/h1&gt; 
&lt;h2&gt;Professional&lt;/h2&gt; 
&lt;p&gt;Get technical support directly from the BunkerWeb maintainers. You will find more information by visiting the &lt;a href="https://panel.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=github"&gt;BunkerWeb Panel&lt;/a&gt;, our dedicated platform for professional services.&lt;/p&gt; 
&lt;p&gt;Don't hesitate to &lt;a href="https://panel.bunkerweb.io/contact.php?utm_campaign=self&amp;amp;utm_source=github"&gt;contact us&lt;/a&gt; if you have any questions; we will be more than happy to respond to your needs.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;To get free community support, you can use the following media:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The #help channel of BunkerWeb in the &lt;a href="https://discord.com/invite/fTf46FmtyD"&gt;Discord server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;The help category of &lt;a href="https://github.com/bunkerity/bunkerweb/discussions"&gt;GitHub discussions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;The &lt;a href="https://www.reddit.com/r/BunkerWeb"&gt;/r/BunkerWeb&lt;/a&gt; subreddit&lt;/li&gt; 
 &lt;li&gt;The &lt;a href="https://serverfault.com/"&gt;Server Fault&lt;/a&gt; and &lt;a href="https://superuser.com/"&gt;Super User&lt;/a&gt; forums&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Please don't use &lt;a href="https://github.com/bunkerity/bunkerweb/issues"&gt;GitHub issues&lt;/a&gt; to ask for help; use it only for bug reports and feature requests.&lt;/p&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;This project is licensed under the terms of the &lt;a href="https://github.com/bunkerity/bunkerweb/raw/v1.6.7/LICENSE.md"&gt;GNU Affero General Public License (AGPL) version 3&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Contribute&lt;/h1&gt; 
&lt;p&gt;If you would like to contribute to the plugins, you can read the &lt;a href="https://github.com/bunkerity/bunkerweb/raw/v1.6.7/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;h1&gt;Security policy&lt;/h1&gt; 
&lt;p&gt;We take security bugs as serious issues and encourage responsible disclosure; see our &lt;a href="https://github.com/bunkerity/bunkerweb/raw/v1.6.7/SECURITY.md"&gt;security policy&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h1&gt;Star History&lt;/h1&gt; 
&lt;a href="https://star-history.com/#bunkerity/bunkerweb&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=bunkerity/bunkerweb&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=bunkerity/bunkerweb&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=bunkerity/bunkerweb&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>PostHog/posthog</title>
      <link>https://github.com/PostHog/posthog</link>
      <description>&lt;p&gt;ü¶î PostHog is an all-in-one developer platform for building successful products. We offer product analytics, web analytics, session replay, error tracking, feature flags, experimentation, surveys, data warehouse, a CDP, and an AI product assistant to help debug your code, ship features faster, and keep all your usage and customer data in one stack.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img alt="posthoglogo" src="https://user-images.githubusercontent.com/65415371/205059737-c8a4f836-4889-4654-902e-f302b187b6a0.png" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://posthog.com/contributors"&gt;&lt;img alt="GitHub contributors" src="https://img.shields.io/github/contributors/posthog/posthog" /&gt;&lt;/a&gt; &lt;a href="http://makeapullrequest.com"&gt;&lt;img alt="PRs Welcome" src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=shields" /&gt;&lt;/a&gt; &lt;img alt="Docker Pulls" src="https://img.shields.io/docker/pulls/posthog/posthog" /&gt; &lt;a href="https://github.com/PostHog/posthog/commits/master"&gt;&lt;img alt="GitHub commit activity" src="https://img.shields.io/github/commit-activity/m/posthog/posthog" /&gt; &lt;/a&gt; &lt;a href="https://github.com/PostHog/posthog/issues?q=is%3Aissue%20state%3Aclosed"&gt;&lt;img alt="GitHub closed issues" src="https://img.shields.io/github/issues-closed/posthog/posthog" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://posthog.com/docs"&gt;Docs&lt;/a&gt; - &lt;a href="https://posthog.com/community"&gt;Community&lt;/a&gt; - &lt;a href="https://posthog.com/roadmap"&gt;Roadmap&lt;/a&gt; - &lt;a href="https://posthog.com/why"&gt;Why PostHog?&lt;/a&gt; - &lt;a href="https://posthog.com/changelog"&gt;Changelog&lt;/a&gt; - &lt;a href="https://github.com/PostHog/posthog/issues/new?assignees=&amp;amp;labels=bug&amp;amp;template=bug_report.yml"&gt;Bug reports&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.youtube.com/watch?v=2jQco8hEvTI"&gt; &lt;img src="https://res.cloudinary.com/dmukukwp6/image/upload/demo_thumb_68d0d8d56d" alt="PostHog Demonstration" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;PostHog is an all-in-one, open source platform for building successful products&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://posthog.com/"&gt;PostHog&lt;/a&gt; provides every tool you need to build a successful product including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/product-analytics"&gt;Product Analytics&lt;/a&gt;: Autocapture or manually instrument event-based analytics to understand user behavior and analyze data with visualization or SQL.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/web-analytics"&gt;Web Analytics&lt;/a&gt;: Monitor web traffic and user sessions with a GA-like dashboard. Easily monitor conversion, web vitals, and revenue.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/session-replay"&gt;Session Replays&lt;/a&gt;: Watch real user sessions of interactions with your website or mobile app to diagnose issues and understand user behavior.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/feature-flags"&gt;Feature Flags&lt;/a&gt;: Safely roll out features to select users or cohorts with feature flags.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/experiments"&gt;Experiments&lt;/a&gt;: Test changes and measure their statistical impact on goal metrics. Set up experiments with no-code too.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/error-tracking"&gt;Error Tracking&lt;/a&gt;: Track errors, get alerts, and resolve issues to improve your product.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/surveys"&gt;Surveys&lt;/a&gt;: Ask anything with our collection of no-code survey templates, or build custom surveys with our survey builder.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/data-warehouse"&gt;Data warehouse&lt;/a&gt;: Sync data from external tools like Stripe, Hubspot, your data warehouse, and more. Query it alongside your product data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/cdp"&gt;Data pipelines&lt;/a&gt;: Run custom filters and transformations on your incoming data. Send it to 25+ tools or any webhook in real time or batch export large amounts to your warehouse.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/docs/llm-analytics"&gt;LLM analytics&lt;/a&gt;: Capture traces, generations, latency, and cost for your LLM-powered app.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/docs/workflows"&gt;Workflows&lt;/a&gt;: Create workflows that automate actions or send messages to your users.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Best of all, all of this is free to use with a &lt;a href="https://posthog.com/pricing"&gt;generous monthly free tier&lt;/a&gt; for each product. Get started by signing up for &lt;a href="https://us.posthog.com/signup"&gt;PostHog Cloud US&lt;/a&gt; or &lt;a href="https://eu.posthog.com/signup"&gt;PostHog Cloud EU&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PostHog/posthog/master/#posthog-is-an-all-in-one-open-source-platform-for-building-successful-products"&gt;PostHog is an all-in-one, open source platform for building successful products&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PostHog/posthog/master/#table-of-contents"&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PostHog/posthog/master/#getting-started-with-posthog"&gt;Getting started with PostHog&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PostHog/posthog/master/#posthog-cloud-recommended"&gt;PostHog Cloud (Recommended)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PostHog/posthog/master/#self-hosting-the-open-source-hobby-deploy-advanced"&gt;Self-hosting the open-source hobby deploy (Advanced)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PostHog/posthog/master/#setting-up-posthog"&gt;Setting up PostHog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PostHog/posthog/master/#learning-more-about-posthog"&gt;Learning more about PostHog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PostHog/posthog/master/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PostHog/posthog/master/#open-source-vs-paid"&gt;Open-source vs. paid&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PostHog/posthog/master/#were-hiring"&gt;We‚Äôre hiring!&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting started with PostHog&lt;/h2&gt; 
&lt;h3&gt;PostHog Cloud (Recommended)&lt;/h3&gt; 
&lt;p&gt;The fastest and most reliable way to get started with PostHog is signing up for free to&amp;nbsp;&lt;a href="https://us.posthog.com/signup"&gt;PostHog Cloud&lt;/a&gt; or &lt;a href="https://eu.posthog.com/signup"&gt;PostHog Cloud EU&lt;/a&gt;. Your first 1 million events, 5k recordings, 1M flag requests, 100k exceptions, and 1500 survey responses are free every month, after which you pay based on usage.&lt;/p&gt; 
&lt;h3&gt;Self-hosting the open-source hobby deploy (Advanced)&lt;/h3&gt; 
&lt;p&gt;If you want to self-host PostHog, you can deploy a hobby instance in one line on Linux with Docker (recommended 4GB memory):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/posthog/posthog/HEAD/bin/deploy-hobby)"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open source deployments should scale to approximately 100k events per month, after which we recommend &lt;a href="https://posthog.com/docs/migrate/migrate-to-cloud"&gt;migrating to a PostHog Cloud&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We &lt;em&gt;do not&lt;/em&gt; provide customer support or offer guarantees for open source deployments. See our &lt;a href="https://posthog.com/docs/self-host"&gt;self-hosting docs&lt;/a&gt;, &lt;a href="https://posthog.com/docs/self-host/deploy/troubleshooting"&gt;troubleshooting guide&lt;/a&gt;, and &lt;a href="https://posthog.com/docs/self-host/open-source/disclaimer"&gt;disclaimer&lt;/a&gt; for more info.&lt;/p&gt; 
&lt;h2&gt;Setting up PostHog&lt;/h2&gt; 
&lt;p&gt;Once you've got a PostHog instance, you can set it up by installing our &lt;a href="https://posthog.com/docs/getting-started/install?tab=snippet"&gt;JavaScript web snippet&lt;/a&gt;, one of &lt;a href="https://posthog.com/docs/getting-started/install?tab=sdks"&gt;our SDKs&lt;/a&gt;, or by &lt;a href="https://posthog.com/docs/getting-started/install?tab=api"&gt;using our API&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We have SDKs and libraries for popular languages and frameworks like:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Frontend&lt;/th&gt; 
   &lt;th&gt;Mobile&lt;/th&gt; 
   &lt;th&gt;Backend&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/js"&gt;JavaScript&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/react-native"&gt;React Native&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/python"&gt;Python&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/next-js"&gt;Next.js&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/android"&gt;Android&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/node"&gt;Node&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/react"&gt;React&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/ios"&gt;iOS&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/php"&gt;PHP&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/vue-js"&gt;Vue&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/flutter"&gt;Flutter&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/ruby"&gt;Ruby&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Beyond this, we have docs and guides for &lt;a href="https://posthog.com/docs/libraries/go"&gt;Go&lt;/a&gt;, &lt;a href="https://posthog.com/docs/libraries/dotnet"&gt;.NET/C#&lt;/a&gt;, &lt;a href="https://posthog.com/docs/libraries/django"&gt;Django&lt;/a&gt;, &lt;a href="https://posthog.com/docs/libraries/angular"&gt;Angular&lt;/a&gt;, &lt;a href="https://posthog.com/docs/libraries/wordpress"&gt;WordPress&lt;/a&gt;, &lt;a href="https://posthog.com/docs/libraries/webflow"&gt;Webflow&lt;/a&gt;, and more.&lt;/p&gt; 
&lt;p&gt;Once you've installed PostHog, see our &lt;a href="https://posthog.com/docs/product-os"&gt;product docs&lt;/a&gt; for more information on how to set up &lt;a href="https://posthog.com/docs/product-analytics/capture-events"&gt;product analytics&lt;/a&gt;, &lt;a href="https://posthog.com/docs/web-analytics/getting-started"&gt;web analytics&lt;/a&gt;, &lt;a href="https://posthog.com/docs/session-replay/how-to-watch-recordings"&gt;session replays&lt;/a&gt;, &lt;a href="https://posthog.com/docs/feature-flags/creating-feature-flags"&gt;feature flags&lt;/a&gt;, &lt;a href="https://posthog.com/docs/experiments/creating-an-experiment"&gt;experiments&lt;/a&gt;, &lt;a href="https://posthog.com/docs/error-tracking/installation#setting-up-exception-autocapture"&gt;error tracking&lt;/a&gt;, &lt;a href="https://posthog.com/docs/surveys/installation"&gt;surveys&lt;/a&gt;, &lt;a href="https://posthog.com/docs/cdp/sources"&gt;data warehouse&lt;/a&gt;, and more.&lt;/p&gt; 
&lt;h2&gt;Learning more about PostHog&lt;/h2&gt; 
&lt;p&gt;Our code isn't the only thing that's open source üò≥. We also open source our &lt;a href="https://posthog.com/handbook"&gt;company handbook&lt;/a&gt; which details our &lt;a href="https://posthog.com/handbook/why-does-posthog-exist"&gt;strategy&lt;/a&gt;, &lt;a href="https://posthog.com/handbook/company/culture"&gt;ways of working&lt;/a&gt;, and &lt;a href="https://posthog.com/handbook/team-structure"&gt;processes&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Curious about how to make the most of PostHog? We wrote a guide to &lt;a href="https://posthog.com/docs/new-to-posthog/getting-hogpilled"&gt;winning with PostHog&lt;/a&gt; which walks you through the basics of &lt;a href="https://posthog.com/docs/new-to-posthog/activation"&gt;measuring activation&lt;/a&gt;, &lt;a href="https://posthog.com/docs/new-to-posthog/retention"&gt;tracking retention&lt;/a&gt;, and &lt;a href="https://posthog.com/docs/new-to-posthog/revenue"&gt;capturing revenue&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We &amp;lt;3 contributions big and small:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Vote on features or get early access to beta functionality in our &lt;a href="https://posthog.com/roadmap"&gt;roadmap&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Open a PR (see our instructions on &lt;a href="https://posthog.com/handbook/engineering/developing-locally"&gt;developing PostHog locally&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Submit a &lt;a href="https://github.com/PostHog/posthog/issues/new?assignees=&amp;amp;labels=enhancement%2C+feature&amp;amp;template=feature_request.yml"&gt;feature request&lt;/a&gt; or &lt;a href="https://github.com/PostHog/posthog/issues/new?assignees=&amp;amp;labels=bug&amp;amp;template=bug_report.yml"&gt;bug report&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Open-source vs. paid&lt;/h2&gt; 
&lt;p&gt;This repo is available under the &lt;a href="https://github.com/PostHog/posthog/raw/master/LICENSE"&gt;MIT expat license&lt;/a&gt;, except for the &lt;code&gt;ee&lt;/code&gt; directory (which has its &lt;a href="https://github.com/PostHog/posthog/raw/master/ee/LICENSE"&gt;license here&lt;/a&gt;) if applicable.&lt;/p&gt; 
&lt;p&gt;Need &lt;em&gt;absolutely üíØ% FOSS&lt;/em&gt;? Check out our &lt;a href="https://github.com/PostHog/posthog-foss"&gt;posthog-foss&lt;/a&gt; repository, which is purged of all proprietary code and features.&lt;/p&gt; 
&lt;p&gt;The pricing for our paid plan is completely transparent and available on &lt;a href="https://posthog.com/pricing"&gt;our pricing page&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;We're hiring!&lt;/h2&gt; 
&lt;img src="https://res.cloudinary.com/dmukukwp6/image/upload/v1/posthog.com/src/components/Home/images/mission-control-hog" alt="Hedgehog working on a Mission Control Center" width="350px" /&gt; 
&lt;p&gt;Hey! If you're reading this, you've proven yourself as a dedicated README reader.&lt;/p&gt; 
&lt;p&gt;You might also make a great addition to our team. We're growing fast &lt;a href="https://posthog.com/careers"&gt;and would love for you to join us&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>usestrix/strix</title>
      <link>https://github.com/usestrix/strix</link>
      <description>&lt;p&gt;Open-source AI agents for penetration testing&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://strix.ai/"&gt; &lt;img src="https://raw.githubusercontent.com/usestrix/strix/main/.github/logo.png" width="150" alt="Strix Logo" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h1 align="center"&gt;Strix&lt;/h1&gt; 
&lt;h2 align="center"&gt;Open-source AI Hackers to secure your Apps&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://pypi.org/project/strix-agent/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/strix-agent?color=3776AB" alt="Python" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/strix-agent/"&gt;&lt;img src="https://img.shields.io/pypi/v/strix-agent?color=10b981" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/usestrix/strix/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-Apache%202.0-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://docs.strix.ai"&gt;&lt;img src="https://img.shields.io/badge/Docs-docs.strix.ai-10b981.svg?sanitize=true" alt="Docs" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/usestrix/strix"&gt;&lt;img src="https://img.shields.io/github/stars/usestrix/strix" alt="GitHub Stars" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/YjKFvEZSdZ"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://strix.ai"&gt;&lt;img src="https://img.shields.io/badge/Website-strix.ai-2d3748.svg?sanitize=true" alt="Website" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/15362" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/15362" alt="usestrix%2Fstrix | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://deepwiki.com/usestrix/strix"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/usestrix/strix/main/.github/screenshot.png" alt="Strix Demo" width="800" style="border-radius: 16px;" /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] &lt;strong&gt;New!&lt;/strong&gt; Strix now integrates seamlessly with GitHub Actions and CI/CD pipelines. Automatically scan for vulnerabilities on every pull request and block insecure code before it reaches production!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü¶â Strix Overview&lt;/h2&gt; 
&lt;p&gt;Strix are autonomous AI agents that act just like real hackers - they run your code dynamically, find vulnerabilities, and validate them through actual proof-of-concepts. Built for developers and security teams who need fast, accurate security testing without the overhead of manual pentesting or the false positives of static analysis tools.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key Capabilities:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üîß &lt;strong&gt;Full hacker toolkit&lt;/strong&gt; out of the box&lt;/li&gt; 
 &lt;li&gt;ü§ù &lt;strong&gt;Teams of agents&lt;/strong&gt; that collaborate and scale&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Real validation&lt;/strong&gt; with PoCs, not false positives&lt;/li&gt; 
 &lt;li&gt;üíª &lt;strong&gt;Developer‚Äëfirst&lt;/strong&gt; CLI with actionable reports&lt;/li&gt; 
 &lt;li&gt;üîÑ &lt;strong&gt;Auto‚Äëfix &amp;amp; reporting&lt;/strong&gt; to accelerate remediation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üéØ Use Cases&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Application Security Testing&lt;/strong&gt; - Detect and validate critical vulnerabilities in your applications&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Rapid Penetration Testing&lt;/strong&gt; - Get penetration tests done in hours, not weeks, with compliance reports&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bug Bounty Automation&lt;/strong&gt; - Automate bug bounty research and generate PoCs for faster reporting&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CI/CD Integration&lt;/strong&gt; - Run tests in CI/CD to block vulnerabilities before reaching production&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Prerequisites:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Docker (running)&lt;/li&gt; 
 &lt;li&gt;An LLM provider key (e.g. &lt;a href="https://platform.openai.com/api-keys"&gt;get OpenAI API key&lt;/a&gt; or use a local LLM)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installation &amp;amp; First Scan&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install Strix
curl -sSL https://strix.ai/install | bash

# Or via pipx
pipx install strix-agent

# Configure your AI provider
export STRIX_LLM="openai/gpt-5"
export LLM_API_KEY="your-api-key"

# Run your first security assessment
strix --target ./app-directory
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] First run automatically pulls the sandbox Docker image. Results are saved to &lt;code&gt;strix_runs/&amp;lt;run-name&amp;gt;&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;‚òÅÔ∏è Run Strix in Cloud&lt;/h2&gt; 
&lt;p&gt;Want to skip the local setup, API keys, and unpredictable LLM costs? Run the hosted cloud version of Strix at &lt;strong&gt;&lt;a href="https://strix.ai"&gt;app.strix.ai&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;Launch a scan in just a few minutes‚Äîno setup or configuration required‚Äîand you‚Äôll get:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;A full pentest report&lt;/strong&gt; with validated findings and clear remediation steps&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Shareable dashboards&lt;/strong&gt; your team can use to track fixes over time&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CI/CD and GitHub integrations&lt;/strong&gt; to block risky changes before production&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Continuous monitoring&lt;/strong&gt; so new vulnerabilities are caught quickly&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://strix.ai"&gt;&lt;strong&gt;Run your first pentest now ‚Üí&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚ú® Features&lt;/h2&gt; 
&lt;h3&gt;üõ†Ô∏è Agentic Security Tools&lt;/h3&gt; 
&lt;p&gt;Strix agents come equipped with a comprehensive security testing toolkit:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Full HTTP Proxy&lt;/strong&gt; - Full request/response manipulation and analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Browser Automation&lt;/strong&gt; - Multi-tab browser for testing of XSS, CSRF, auth flows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Terminal Environments&lt;/strong&gt; - Interactive shells for command execution and testing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Python Runtime&lt;/strong&gt; - Custom exploit development and validation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reconnaissance&lt;/strong&gt; - Automated OSINT and attack surface mapping&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code Analysis&lt;/strong&gt; - Static and dynamic analysis capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Knowledge Management&lt;/strong&gt; - Structured findings and attack documentation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üéØ Comprehensive Vulnerability Detection&lt;/h3&gt; 
&lt;p&gt;Strix can identify and validate a wide range of security vulnerabilities:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Access Control&lt;/strong&gt; - IDOR, privilege escalation, auth bypass&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Injection Attacks&lt;/strong&gt; - SQL, NoSQL, command injection&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Server-Side&lt;/strong&gt; - SSRF, XXE, deserialization flaws&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Client-Side&lt;/strong&gt; - XSS, prototype pollution, DOM vulnerabilities&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Business Logic&lt;/strong&gt; - Race conditions, workflow manipulation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Authentication&lt;/strong&gt; - JWT vulnerabilities, session management&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Infrastructure&lt;/strong&gt; - Misconfigurations, exposed services&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üï∏Ô∏è Graph of Agents&lt;/h3&gt; 
&lt;p&gt;Advanced multi-agent orchestration for comprehensive security testing:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Distributed Workflows&lt;/strong&gt; - Specialized agents for different attacks and assets&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scalable Testing&lt;/strong&gt; - Parallel execution for fast comprehensive coverage&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dynamic Coordination&lt;/strong&gt; - Agents collaborate and share discoveries&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üíª Usage Examples&lt;/h2&gt; 
&lt;h3&gt;Basic Usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Scan a local codebase
strix --target ./app-directory

# Security review of a GitHub repository
strix --target https://github.com/org/repo

# Black-box web application assessment
strix --target https://your-app.com
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Advanced Testing Scenarios&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Grey-box authenticated testing
strix --target https://your-app.com --instruction "Perform authenticated testing using credentials: user:pass"

# Multi-target testing (source code + deployed app)
strix -t https://github.com/org/app -t https://your-app.com

# Focused testing with custom instructions
strix --target api.your-app.com --instruction "Focus on business logic flaws and IDOR vulnerabilities"

# Provide detailed instructions through file (e.g., rules of engagement, scope, exclusions)
strix --target api.your-app.com --instruction-file ./instruction.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ü§ñ Headless Mode&lt;/h3&gt; 
&lt;p&gt;Run Strix programmatically without interactive UI using the &lt;code&gt;-n/--non-interactive&lt;/code&gt; flag‚Äîperfect for servers and automated jobs. The CLI prints real-time vulnerability findings, and the final report before exiting. Exits with non-zero code when vulnerabilities are found.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;strix -n --target https://your-app.com
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üîÑ CI/CD (GitHub Actions)&lt;/h3&gt; 
&lt;p&gt;Strix can be added to your pipeline to run a security test on pull requests with a lightweight GitHub Actions workflow:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;name: strix-penetration-test

on:
  pull_request:

jobs:
  security-scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v6

      - name: Install Strix
        run: curl -sSL https://strix.ai/install | bash

      - name: Run Strix
        env:
          STRIX_LLM: ${{ secrets.STRIX_LLM }}
          LLM_API_KEY: ${{ secrets.LLM_API_KEY }}

        run: strix -n -t ./ --scan-mode quick
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;‚öôÔ∏è Configuration&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export STRIX_LLM="openai/gpt-5"
export LLM_API_KEY="your-api-key"

# Optional
export LLM_API_BASE="your-api-base-url"  # if using a local model, e.g. Ollama, LMStudio
export PERPLEXITY_API_KEY="your-api-key"  # for search capabilities
export STRIX_REASONING_EFFORT="high"  # control thinking effort (default: high, quick scan: medium)
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Strix automatically saves your configuration to &lt;code&gt;~/.strix/cli-config.json&lt;/code&gt;, so you don't have to re-enter it on every run.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Recommended models for best results:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://openai.com/api/"&gt;OpenAI GPT-5&lt;/a&gt; ‚Äî &lt;code&gt;openai/gpt-5&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://claude.com/platform/api"&gt;Anthropic Claude Sonnet 4.5&lt;/a&gt; ‚Äî &lt;code&gt;anthropic/claude-sonnet-4-5&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cloud.google.com/vertex-ai"&gt;Google Gemini 3 Pro Preview&lt;/a&gt; ‚Äî &lt;code&gt;vertex_ai/gemini-3-pro-preview&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the &lt;a href="https://docs.strix.ai/llm-providers/overview"&gt;LLM Providers documentation&lt;/a&gt; for all supported providers including Vertex AI, Bedrock, Azure, and local models.&lt;/p&gt; 
&lt;h2&gt;üìö Documentation&lt;/h2&gt; 
&lt;p&gt;Full documentation is available at &lt;strong&gt;&lt;a href="https://docs.strix.ai"&gt;docs.strix.ai&lt;/a&gt;&lt;/strong&gt; ‚Äî including detailed guides for usage, CI/CD integrations, skills, and advanced configuration.&lt;/p&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions of code, docs, and new skills - check out our &lt;a href="https://docs.strix.ai/contributing"&gt;Contributing Guide&lt;/a&gt; to get started or open a &lt;a href="https://github.com/usestrix/strix/pulls"&gt;pull request&lt;/a&gt;/&lt;a href="https://github.com/usestrix/strix/issues"&gt;issue&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üë• Join Our Community&lt;/h2&gt; 
&lt;p&gt;Have questions? Found a bug? Want to contribute? &lt;strong&gt;&lt;a href="https://discord.gg/YjKFvEZSdZ"&gt;Join our Discord!&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;üåü Support the Project&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Love Strix?&lt;/strong&gt; Give us a ‚≠ê on GitHub!&lt;/p&gt; 
&lt;h2&gt;üôè Acknowledgements&lt;/h2&gt; 
&lt;p&gt;Strix builds on the incredible work of open-source projects like &lt;a href="https://github.com/BerriAI/litellm"&gt;LiteLLM&lt;/a&gt;, &lt;a href="https://github.com/caido/caido"&gt;Caido&lt;/a&gt;, &lt;a href="https://github.com/projectdiscovery"&gt;ProjectDiscovery&lt;/a&gt;, &lt;a href="https://github.com/microsoft/playwright"&gt;Playwright&lt;/a&gt;, and &lt;a href="https://github.com/Textualize/textual"&gt;Textual&lt;/a&gt;. Huge thanks to their maintainers!&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] Only test apps you own or have permission to test. You are responsible for using Strix ethically and legally.&lt;/p&gt; 
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>NVIDIA-NeMo/NeMo</title>
      <link>https://github.com/NVIDIA-NeMo/NeMo</link>
      <description>&lt;p&gt;A scalable generative AI framework built for researchers and developers working on Large Language Models, Multimodal, and Speech AI (Automatic Speech Recognition and Text-to-Speech)&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="http://www.repostatus.org/#active"&gt;&lt;img src="http://www.repostatus.org/badges/latest/active.svg?sanitize=true" alt="Project Status: Active -- The project has reached a stable, usable state and is being actively developed." /&gt;&lt;/a&gt; &lt;a href="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/"&gt;&lt;img src="https://readthedocs.com/projects/nvidia-nemo/badge/?version=main" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://github.com/nvidia/nemo/actions/workflows/codeql.yml"&gt;&lt;img src="https://github.com/nvidia/nemo/actions/workflows/codeql.yml/badge.svg?branch=main&amp;amp;event=push" alt="CodeQL" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/NeMo/raw/master/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg?sanitize=true" alt="NeMo core license and license for collections in this repo" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/nemo-toolkit"&gt;&lt;img src="https://badge.fury.io/py/nemo-toolkit.svg?sanitize=true" alt="Release version" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/nemo-toolkit"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/nemo-toolkit.svg?sanitize=true" alt="Python version" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/nemo-toolkit"&gt;&lt;img src="https://static.pepy.tech/personalized-badge/nemo-toolkit?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=brightgreen&amp;amp;left_text=downloads" alt="PyPi total downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true" alt="Code style: black" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;NVIDIA NeMo Speech Collection&lt;/strong&gt;&lt;/h1&gt; 
&lt;h2&gt;Latest News&lt;/h2&gt; 
&lt;!-- markdownlint-disable --&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;b&gt;&lt;a href="https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16"&gt;NVIDIA-Nemotron-3-Nano-30B-A3B&lt;/a&gt; is out with full reproducible script and recipes! Check out &lt;a href="https://github.com/NVIDIA-NeMo/Megatron-Bridge/tree/nano-v3"&gt;NeMo Megatron-Bridge&lt;/a&gt;, &lt;a href="https://github.com/NVIDIA-NeMo/AutoModel/raw/main/examples/llm_finetune/nemotron/nemotron_nano_v3_squad.yaml"&gt;NeMo AutoModel&lt;/a&gt;, &lt;a href="https://github.com/NVIDIA-NeMo/RL"&gt;NeMo-RL&lt;/a&gt; and &lt;a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo?version=25.11.nemotron_3_nano"&gt;NGC container&lt;/a&gt; to try them!&lt;/b&gt; (2025-12-15) &lt;/summary&gt;
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;b&gt;‚ö†Ô∏è Pivot notice: This repo will pivot to focus on speech models collections only. Please refer to &lt;a href="https://github.com/NVIDIA-NeMo"&gt;NeMo Framework Github Org&lt;/a&gt; for the complete list of repos under NeMo Framework&lt;/b&gt;&lt;/summary&gt; NeMo 2.0, with its support for LLMs and VLMs will be deprecated by 25.11, and replaced by 
 &lt;a href="https://github.com/NVIDIA-NeMo/Megatron-Bridge"&gt;NeMo Megatron-Bridge&lt;/a&gt; and 
 &lt;a href="https://github.com/NVIDIA-NeMo/AutoModel"&gt;NeMo AutoModel&lt;/a&gt;. More details can be found in the 
 &lt;a href="https://github.com/NVIDIA-NeMo"&gt;NeMo Framework GitHub org readme&lt;/a&gt;. (2025-10-10) 
 &lt;pre&gt;&lt;code&gt;  Following collections are deprecated and will be removed in a later release, please use previous versions if you are using them:
  - nlp
  - llm
  - vlm
  - vision
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details closed&gt; 
 &lt;summary&gt;&lt;b&gt;Pretrain and finetune &lt;span&gt;ü§ó&lt;/span&gt;Hugging Face models via AutoModel&lt;/b&gt;&lt;/summary&gt; NeMo Framework's latest feature AutoModel enables broad support for 
 &lt;span&gt;ü§ó&lt;/span&gt;Hugging Face models, with 25.04 focusing on 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/transformers/v3.5.1/model_doc/auto.html#automodelforcausallm"&gt;AutoModelForCausalLM&lt;/a&gt; in the &lt;a href="https://huggingface.co/models?pipeline_tag=text-generation&amp;amp;sort=trending"&gt;Text Generation&lt;/a&gt; category&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/docs/transformers/main/model_doc/auto#transformers.AutoModelForImageTextToText"&gt;AutoModelForImageTextToText&lt;/a&gt; in the &lt;a href="https://huggingface.co/models?pipeline_tag=image-text-to-text&amp;amp;sort=trending"&gt;Image-Text-to-Text&lt;/a&gt; category&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;More Details in Blog: &lt;a href="https://developer.nvidia.com/blog/run-hugging-face-models-instantly-with-day-0-support-from-nvidia-nemo-framework"&gt;Run Hugging Face Models Instantly with Day-0 Support from NVIDIA NeMo Framework&lt;/a&gt;. Future releases will enable support for more model families such as Video Generation models.(2025-05-19)&lt;/p&gt; 
&lt;/details&gt; 
&lt;details closed&gt; 
 &lt;summary&gt;&lt;b&gt;Training on Blackwell using NeMo&lt;/b&gt;&lt;/summary&gt; NeMo Framework has added Blackwell support, with 
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance_summary.html"&gt;performance benchmarks on GB200 &amp;amp; B200&lt;/a&gt;. More optimizations to come in the upcoming releases.(2025-05-19) 
&lt;/details&gt; 
&lt;details closed&gt; 
 &lt;summary&gt;&lt;b&gt;Training Performance on GPU Tuning Guide&lt;/b&gt;&lt;/summary&gt; NeMo Framework has published 
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance-guide.html"&gt;a comprehensive guide for performance tuning to achieve optimal throughput&lt;/a&gt;! (2025-05-19) 
&lt;/details&gt; 
&lt;details closed&gt; 
 &lt;summary&gt;&lt;b&gt;New Models Support&lt;/b&gt;&lt;/summary&gt; NeMo Framework has added support for latest community models - 
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/vlms/llama4.html"&gt;Llama 4&lt;/a&gt;, 
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/vision/diffusionmodels/flux.html"&gt;Flux&lt;/a&gt;, 
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/llama_nemotron.html"&gt;Llama Nemotron&lt;/a&gt;, 
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/hyena.html#"&gt;Hyena &amp;amp; Evo2&lt;/a&gt;, 
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/vlms/qwen2vl.html"&gt;Qwen2-VL&lt;/a&gt;, 
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/qwen2.html"&gt;Qwen2.5&lt;/a&gt;, Gemma3, Qwen3-30B&amp;amp;32B.(2025-05-19) 
&lt;/details&gt; 
&lt;details closed&gt; 
 &lt;summary&gt;&lt;b&gt;NeMo Framework 2.0&lt;/b&gt;&lt;/summary&gt; We've released NeMo 2.0, an update on the NeMo Framework which prioritizes modularity and ease-of-use. Please refer to the 
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/index.html"&gt;NeMo Framework User Guide&lt;/a&gt; to get started. 
&lt;/details&gt; 
&lt;details closed&gt; 
 &lt;summary&gt;&lt;b&gt;New Cosmos World Foundation Models Support&lt;/b&gt;&lt;/summary&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/advancing-physical-ai-with-nvidia-cosmos-world-foundation-model-platform"&gt;Advancing Physical AI with NVIDIA Cosmos World Foundation Model Platform &lt;/a&gt; (2025-01-09) &lt;/summary&gt; The end-to-end NVIDIA Cosmos platform accelerates world model development for physical AI systems. Built on CUDA, Cosmos combines state-of-the-art world foundation models, video tokenizers, and AI-accelerated data processing pipelines. Developers can accelerate world model development by fine-tuning Cosmos world foundation models or building new ones from the ground up. These models create realistic synthetic videos of environments and interactions, providing a scalable foundation for training complex systems, from simulating humanoid robots performing advanced actions to developing end-to-end autonomous driving models. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/accelerate-custom-video-foundation-model-pipelines-with-new-nvidia-nemo-framework-capabilities/"&gt; Accelerate Custom Video Foundation Model Pipelines with New NVIDIA NeMo Framework Capabilities &lt;/a&gt; (2025-01-07) &lt;/summary&gt; The NeMo Framework now supports training and customizing the 
  &lt;a href="https://github.com/NVIDIA/Cosmos"&gt;NVIDIA Cosmos&lt;/a&gt; collection of world foundation models. Cosmos leverages advanced text-to-world generation techniques to create fluid, coherent video content from natural language prompts. 
  &lt;br /&gt;
  &lt;br /&gt; You can also now accelerate your video processing step using the 
  &lt;a href="https://developer.nvidia.com/nemo-curator-video-processing-early-access"&gt;NeMo Curator&lt;/a&gt; library, which provides optimized video processing and captioning features that can deliver up to 89x faster video processing when compared to an unoptimized CPU pipeline. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
&lt;/details&gt; 
&lt;details closed&gt; 
 &lt;summary&gt;&lt;b&gt;Large Language Models and Multimodal Models&lt;/b&gt;&lt;/summary&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/state-of-the-art-multimodal-generative-ai-model-development-with-nvidia-nemo/"&gt; State-of-the-Art Multimodal Generative AI Model Development with NVIDIA NeMo &lt;/a&gt; (2024-11-06) &lt;/summary&gt; NVIDIA recently announced significant enhancements to the NeMo platform, focusing on multimodal generative AI models. The update includes NeMo Curator and the Cosmos tokenizer, which streamline the data curation process and enhance the quality of visual data. These tools are designed to handle large-scale data efficiently, making it easier to develop high-quality AI models for various applications, including robotics and autonomous driving. The Cosmos tokenizers, in particular, efficiently map visual data into compact, semantic tokens, which is crucial for training large-scale generative models. The tokenizer is available now on the 
  &lt;a href="https://github.com/NVIDIA/cosmos-tokenizer"&gt;NVIDIA/cosmos-tokenizer&lt;/a&gt; GitHub repo and on 
  &lt;a href="https://huggingface.co/nvidia/Cosmos-Tokenizer-CV8x8x8"&gt;Hugging Face&lt;/a&gt;. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/llama/index.html#new-llama-3-1-support for more information/"&gt; New Llama 3.1 Support &lt;/a&gt; (2024-07-23) &lt;/summary&gt; The NeMo Framework now supports training and customizing the Llama 3.1 collection of LLMs from Meta. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://aws.amazon.com/blogs/machine-learning/accelerate-your-generative-ai-distributed-training-workloads-with-the-nvidia-nemo-framework-on-amazon-eks/"&gt; Accelerate your Generative AI Distributed Training Workloads with the NVIDIA NeMo Framework on Amazon EKS &lt;/a&gt; (2024-07-16) &lt;/summary&gt; NVIDIA NeMo Framework now runs distributed training workloads on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. For step-by-step instructions on creating an EKS cluster and running distributed training workloads with NeMo, see the GitHub repository 
  &lt;a href="https://github.com/aws-samples/awsome-distributed-training/tree/main/3.test_cases/2.nemo-launcher/EKS/"&gt; here.&lt;/a&gt; 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/nvidia-nemo-accelerates-llm-innovation-with-hybrid-state-space-model-support/"&gt; NVIDIA NeMo Accelerates LLM Innovation with Hybrid State Space Model Support &lt;/a&gt; (2024/06/17) &lt;/summary&gt; NVIDIA NeMo and Megatron Core now support pre-training and fine-tuning of state space models (SSMs). NeMo also supports training models based on the Griffin architecture as described by Google DeepMind. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://huggingface.co/models?sort=trending&amp;amp;search=nvidia%2Fnemotron-4-340B"&gt; NVIDIA releases 340B base, instruct, and reward models pretrained on a total of 9T tokens. &lt;/a&gt; (2024-06-18) &lt;/summary&gt; See documentation and tutorials for SFT, PEFT, and PTQ with 
  &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/nemotron/index.html"&gt; Nemotron 340B &lt;/a&gt; in the NeMo Framework User Guide. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/nvidia-sets-new-generative-ai-performance-and-scale-records-in-mlperf-training-v4-0/"&gt; NVIDIA sets new generative AI performance and scale records in MLPerf Training v4.0 &lt;/a&gt; (2024/06/12) &lt;/summary&gt; Using NVIDIA NeMo Framework and NVIDIA Hopper GPUs NVIDIA was able to scale to 11,616 H100 GPUs and achieve near-linear performance scaling on LLM pretraining. NVIDIA also achieved the highest LLM fine-tuning performance and raised the bar for text-to-image training. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://cloud.google.com/blog/products/compute/gke-and-nvidia-nemo-framework-to-train-generative-ai-models"&gt; Accelerate your generative AI journey with NVIDIA NeMo Framework on GKE &lt;/a&gt; (2024/03/16) &lt;/summary&gt; An end-to-end walkthrough to train generative AI models on the Google Kubernetes Engine (GKE) using the NVIDIA NeMo Framework is available at https://github.com/GoogleCloudPlatform/nvidia-nemo-on-gke. The walkthrough includes detailed instructions on how to set up a Google Cloud Project and pre-train a GPT model using the NeMo Framework. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
&lt;/details&gt; 
&lt;details closed&gt; 
 &lt;summary&gt;&lt;b&gt;Speech Recognition&lt;/b&gt;&lt;/summary&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/accelerating-leaderboard-topping-asr-models-10x-with-nvidia-nemo/"&gt; Accelerating Leaderboard-Topping ASR Models 10x with NVIDIA NeMo &lt;/a&gt; (2024/09/24) &lt;/summary&gt; NVIDIA NeMo team released a number of inference optimizations for CTC, RNN-T, and TDT models that resulted in up to 10x inference speed-up. These models now exceed an inverse real-time factor (RTFx) of 2,000, with some reaching RTFx of even 6,000. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/new-standard-for-speech-recognition-and-translation-from-the-nvidia-nemo-canary-model/"&gt; New Standard for Speech Recognition and Translation from the NVIDIA NeMo Canary Model &lt;/a&gt; (2024/04/18) &lt;/summary&gt; The NeMo team just released Canary, a multilingual model that transcribes speech in English, Spanish, German, and French with punctuation and capitalization. Canary also provides bi-directional translation, between English and the three other supported languages. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/pushing-the-boundaries-of-speech-recognition-with-nemo-parakeet-asr-models/"&gt; Pushing the Boundaries of Speech Recognition with NVIDIA NeMo Parakeet ASR Models &lt;/a&gt; (2024/04/18) &lt;/summary&gt; NVIDIA NeMo, an end-to-end platform for the development of multimodal generative AI models at scale anywhere‚Äîon any cloud and on-premises‚Äîreleased the Parakeet family of automatic speech recognition (ASR) models. These state-of-the-art ASR models, developed in collaboration with Suno.ai, transcribe spoken English with exceptional accuracy. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/turbocharge-asr-accuracy-and-speed-with-nvidia-nemo-parakeet-tdt/"&gt; Turbocharge ASR Accuracy and Speed with NVIDIA NeMo Parakeet-TDT &lt;/a&gt; (2024/04/18) &lt;/summary&gt; NVIDIA NeMo, an end-to-end platform for developing multimodal generative AI models at scale anywhere‚Äîon any cloud and on-premises‚Äîrecently released Parakeet-TDT. This new addition to the ‚ÄØNeMo ASR Parakeet model family boasts better accuracy and 64% greater speed over the previously best model, Parakeet-RNNT-1.1B. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
&lt;/details&gt; 
&lt;!-- markdownlint-enable --&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;NVIDIA NeMo Framework is a scalable and cloud-native generative AI framework built for researchers and PyTorch developers working on Large Language Models (LLMs), Multimodal Models (MMs), Automatic Speech Recognition (ASR), Text to Speech (TTS), and Computer Vision (CV) domains. It is designed to help you efficiently create, customize, and deploy new generative AI models by leveraging existing code and pre-trained model checkpoints.&lt;/p&gt; 
&lt;p&gt;For technical documentation, please see the &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/index.html"&gt;NeMo Framework User Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;What's New in NeMo 2.0&lt;/h2&gt; 
&lt;p&gt;NVIDIA NeMo 2.0 introduces several significant improvements over its predecessor, NeMo 1.0, enhancing flexibility, performance, and scalability.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Python-Based Configuration&lt;/strong&gt; - NeMo 2.0 transitions from YAML files to a Python-based configuration, providing more flexibility and control. This shift makes it easier to extend and customize configurations programmatically.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Modular Abstractions&lt;/strong&gt; - By adopting PyTorch Lightning‚Äôs modular abstractions, NeMo 2.0 simplifies adaptation and experimentation. This modular approach allows developers to more easily modify and experiment with different components of their models.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Scalability&lt;/strong&gt; - NeMo 2.0 seamlessly scaling large-scale experiments across thousands of GPUs using &lt;a href="https://github.com/NVIDIA/NeMo-Run"&gt;NeMo-Run&lt;/a&gt;, a powerful tool designed to streamline the configuration, execution, and management of machine learning experiments across computing environments.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Overall, these enhancements make NeMo 2.0 a powerful, scalable, and user-friendly framework for AI model development.&lt;/p&gt; 
&lt;h3&gt;Get Started with NeMo 2.0&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Refer to the &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/quickstart.html"&gt;Quickstart&lt;/a&gt; for examples of using NeMo-Run to launch NeMo 2.0 experiments locally and on a slurm cluster.&lt;/li&gt; 
 &lt;li&gt;For more information about NeMo 2.0, see the &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/index.html"&gt;NeMo Framework User Guide&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;For an in-depth exploration of the main features of NeMo 2.0, see the &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/features/index.html#feature-guide"&gt;Feature Guide&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;To transition from NeMo 1.0 to 2.0, see the &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/migration/index.html#migration-guide"&gt;Migration Guide&lt;/a&gt; for step-by-step instructions.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Training and Customization&lt;/h2&gt; 
&lt;p&gt;All NeMo models are trained with &lt;a href="https://github.com/Lightning-AI/lightning"&gt;Lightning&lt;/a&gt;. Training is automatically scalable to 1000s of GPUs. You can check the performance benchmarks using the latest NeMo Framework container &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance_summary.html"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When applicable, NeMo models leverage cutting-edge distributed training techniques, incorporating &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/modeloverview.html"&gt;parallelism strategies&lt;/a&gt; to enable efficient training of very large models. These techniques include Tensor Parallelism (TP), Pipeline Parallelism (PP), Fully Sharded Data Parallelism (FSDP), Mixture-of-Experts (MoE), and Mixed Precision Training with BFloat16 and FP8, as well as others.&lt;/p&gt; 
&lt;p&gt;In addition to supervised fine-tuning (SFT), NeMo also supports the latest parameter efficient fine-tuning (PEFT) techniques such as LoRA, P-Tuning, Adapters, and IA3.&lt;/p&gt; 
&lt;h2&gt;Speech AI&lt;/h2&gt; 
&lt;p&gt;NeMo ASR and TTS models can be optimized for inference and deployed for production use cases with &lt;a href="https://developer.nvidia.com/riva"&gt;NVIDIA Riva&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Get Started with NeMo Framework&lt;/h2&gt; 
&lt;p&gt;Getting started with NeMo Framework is easy. State-of-the-art pretrained NeMo models are freely available on &lt;a href="https://huggingface.co/models?library=nemo&amp;amp;sort=downloads&amp;amp;search=nvidia"&gt;Hugging Face Hub&lt;/a&gt; and &lt;a href="https://catalog.ngc.nvidia.com/models?query=nemo&amp;amp;orderBy=weightPopularDESC"&gt;NVIDIA NGC&lt;/a&gt;. These models can be used to generate text or images, transcribe audio, and synthesize speech in just a few lines of code.&lt;/p&gt; 
&lt;p&gt;We have extensive &lt;a href="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/starthere/tutorials.html"&gt;tutorials&lt;/a&gt; that can be run on &lt;a href="https://colab.research.google.com"&gt;Google Colab&lt;/a&gt; or with our &lt;a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo"&gt;NGC NeMo Framework Container&lt;/a&gt;. We also have &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/index.html"&gt;playbooks&lt;/a&gt; for users who want to train NeMo models with the NeMo Framework Launcher.&lt;/p&gt; 
&lt;p&gt;For advanced users who want to train NeMo models from scratch or fine-tune existing NeMo models, we have a full suite of &lt;a href="https://github.com/NVIDIA/NeMo/tree/main/examples"&gt;example scripts&lt;/a&gt; that support multi-GPU/multi-node training.&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/NeMo/main/nemo/collections/multimodal/README.md"&gt;Multimodal&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/NeMo/main/nemo/collections/asr/README.md"&gt;Automatic Speech Recognition&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/NeMo/main/nemo/collections/tts/README.md"&gt;Text to Speech&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10 or above&lt;/li&gt; 
 &lt;li&gt;Pytorch 2.5 or above&lt;/li&gt; 
 &lt;li&gt;NVIDIA GPU (if you intend to do model training)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Developer Documentation&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Version&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Latest&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/"&gt;&lt;img src="https://readthedocs.com/projects/nvidia-nemo/badge/?version=main" alt="Documentation Status" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/"&gt;Documentation of the latest (i.e. main) branch.&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Stable&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/"&gt;&lt;img src="https://readthedocs.com/projects/nvidia-nemo/badge/?version=stable" alt="Documentation Status" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/"&gt;Documentation of the stable (i.e. most recent release)&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Install NeMo Framework&lt;/h2&gt; 
&lt;p&gt;The NeMo Framework can be installed in a variety of ways, depending on your needs. Depending on the domain, you may find one of the following installation methods more suitable.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/NeMo/main/#conda--pip"&gt;Conda / Pip&lt;/a&gt;: Install NeMo-Framework with native Pip into a virtual environment. 
  &lt;ul&gt; 
   &lt;li&gt;Used to explore NeMo on any supported platform.&lt;/li&gt; 
   &lt;li&gt;This is the recommended method for ASR and TTS domains.&lt;/li&gt; 
   &lt;li&gt;Limited feature-completeness for other domains.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/NeMo/main/#ngc-pytorch-container"&gt;NGC PyTorch container&lt;/a&gt;: Install NeMo-Framework from source with feature-completeness into a highly optimized container. 
  &lt;ul&gt; 
   &lt;li&gt;For users that want to install from source in a highly optimized container.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/NeMo/main/#ngc-nemo-container"&gt;NGC NeMo container&lt;/a&gt;: Ready-to-go solution of NeMo-Framework 
  &lt;ul&gt; 
   &lt;li&gt;For users that seek highest performance.&lt;/li&gt; 
   &lt;li&gt;Contains all dependencies installed and tested for performance and convergence.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Support matrix&lt;/h3&gt; 
&lt;p&gt;NeMo-Framework provides tiers of support based on OS / Platform and mode of installation. Please refer the following overview of support levels:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fully supported: Max performance and feature-completeness.&lt;/li&gt; 
 &lt;li&gt;Limited supported: Used to explore NeMo.&lt;/li&gt; 
 &lt;li&gt;No support yet: In development.&lt;/li&gt; 
 &lt;li&gt;Deprecated: Support has reached end of life.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Please refer to the following table for current support levels:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;OS / Platform&lt;/th&gt; 
   &lt;th&gt;Install from PyPi&lt;/th&gt; 
   &lt;th&gt;Source into NGC container&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;linux&lt;/code&gt; - &lt;code&gt;amd64/x84_64&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Limited support&lt;/td&gt; 
   &lt;td&gt;Full support&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;linux&lt;/code&gt; - &lt;code&gt;arm64&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Limited support&lt;/td&gt; 
   &lt;td&gt;Limited support&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;darwin&lt;/code&gt; - &lt;code&gt;amd64/x64_64&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Deprecated&lt;/td&gt; 
   &lt;td&gt;Deprecated&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;darwin&lt;/code&gt; - &lt;code&gt;arm64&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Limited support&lt;/td&gt; 
   &lt;td&gt;Limited support&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;windows&lt;/code&gt; - &lt;code&gt;amd64/x64_64&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;No support yet&lt;/td&gt; 
   &lt;td&gt;No support yet&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;windows&lt;/code&gt; - &lt;code&gt;arm64&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;No support yet&lt;/td&gt; 
   &lt;td&gt;No support yet&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Conda / Pip&lt;/h3&gt; 
&lt;p&gt;Install NeMo in a fresh Conda environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create --name nemo python==3.10.12
conda activate nemo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Pick the right version&lt;/h4&gt; 
&lt;p&gt;NeMo-Framework publishes pre-built wheels with each release. To install nemo_toolkit from such a wheel, use the following installation method:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "nemo_toolkit[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If a more specific version is desired, we recommend a Pip-VCS install. From &lt;a href="https://github.com/NVIDIA/NeMo"&gt;NVIDIA/NeMo&lt;/a&gt;, fetch the commit, branch, or tag that you would like to install.&lt;br /&gt; To install nemo_toolkit from this Git reference &lt;code&gt;$REF&lt;/code&gt;, use the following installation method:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/NVIDIA/NeMo
cd NeMo
git checkout @${REF:-'main'}
pip install '.[all]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Install a specific Domain&lt;/h4&gt; 
&lt;p&gt;To install a specific domain of NeMo, you must first install the nemo_toolkit using the instructions listed above. Then, you run the following domain-specific commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install nemo_toolkit['all'] # or pip install "nemo_toolkit['all']@git+https://github.com/NVIDIA/NeMo@${REF:-'main'}"
pip install nemo_toolkit['asr'] # or pip install "nemo_toolkit['asr']@git+https://github.com/NVIDIA/NeMo@$REF:-'main'}"
pip install nemo_toolkit['tts'] # or pip install "nemo_toolkit['tts']@git+https://github.com/NVIDIA/NeMo@${REF:-'main'}"
pip install nemo_toolkit['multimodal'] # or pip install "nemo_toolkit['multimodal']@git+https://github.com/NVIDIA/NeMo@${REF:-'main'}"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;NGC PyTorch container&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;NOTE: The following steps are supported beginning with 25.09 (NeMo-Toolkit 2.6.0)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We recommended that you start with a base NVIDIA PyTorch container: nvcr.io/nvidia/pytorch:25.09-py3.&lt;/p&gt; 
&lt;p&gt;If starting with a base NVIDIA PyTorch container, you must first launch the container:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run \
  --gpus all \
  -it \
  --rm \
  --shm-size=16g \
  --ulimit memlock=-1 \
  --ulimit stack=67108864 \
  ${NV_PYTORCH_TAG:-'nvcr.io/nvidia/pytorch:25.09-py3'}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;From &lt;a href="https://github.com/NVIDIA/NeMo"&gt;NVIDIA/NeMo&lt;/a&gt;, fetch the commit/branch/tag that you want to install.&lt;br /&gt; To install nemo_toolkit including all of its dependencies from this Git reference &lt;code&gt;$REF&lt;/code&gt;, use the following installation method:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd /opt
git clone https://github.com/NVIDIA/NeMo
cd NeMo
git checkout ${REF:-'main'}
bash docker/common/install_dep.sh --library all
pip install ".[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;NGC NeMo container&lt;/h2&gt; 
&lt;p&gt;NeMo containers are launched concurrently with NeMo version updates. NeMo Framework now supports LLMs, MMs, ASR, and TTS in a single consolidated Docker container. The latest container is based on NeMo 2.6.0. You can find additional information about released containers on the &lt;a href="https://github.com/NVIDIA/NeMo/releases"&gt;NeMo releases page&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To use a pre-built container, run the following code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run \
  --gpus all \
  -it \
  --rm \
  --shm-size=16g \
  --ulimit memlock=-1 \
  --ulimit stack=67108864 \
  nvcr.io/nvidia/nemo:25.11.01
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Discussions Board&lt;/h2&gt; 
&lt;p&gt;FAQ can be found on the NeMo &lt;a href="https://github.com/NVIDIA/NeMo/discussions"&gt;Discussions board&lt;/a&gt;. You are welcome to ask questions or start discussions on the board.&lt;/p&gt; 
&lt;h2&gt;Contribute to NeMo&lt;/h2&gt; 
&lt;p&gt;We welcome community contributions! Please refer to &lt;a href="https://github.com/NVIDIA/NeMo/raw/stable/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for the process.&lt;/p&gt; 
&lt;h2&gt;Publications&lt;/h2&gt; 
&lt;p&gt;We provide an ever-growing list of &lt;a href="https://nvidia.github.io/NeMo/publications/"&gt;publications&lt;/a&gt; that utilize the NeMo Framework.&lt;/p&gt; 
&lt;p&gt;To contribute an article to the collection, please submit a pull request to the &lt;code&gt;gh-pages-src&lt;/code&gt; branch of this repository. For detailed information, please consult the README located at the &lt;a href="https://github.com/NVIDIA/NeMo/tree/gh-pages-src#readme"&gt;gh-pages-src branch&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Licenses&lt;/h2&gt; 
&lt;p&gt;NeMo is licensed under the &lt;a href="https://github.com/NVIDIA/NeMo?tab=Apache-2.0-1-ov-file"&gt;Apache License 2.0&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>alibaba/ROLL</title>
      <link>https://github.com/alibaba/ROLL</link>
      <description>&lt;p&gt;An Efficient and User-Friendly Scaling Library for Reinforcement Learning with Large Language Models&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/alibaba/ROLL/main/assets/roll.jpeg" width="40%" alt="ROLL Logo" /&gt; 
 &lt;h1&gt;ROLL: Reinforcement Learning Optimization for Large-Scale Learning&lt;/h1&gt; 
 &lt;h4&gt;üöÄ An Efficient and User-Friendly Scaling Library for Reinforcement Learning with Large Language Models üöÄ&lt;/h4&gt; 
 &lt;p&gt; &lt;a href="https://github.com/alibaba/ROLL/raw/main/LICENSE"&gt; &lt;img src="https://img.shields.io/badge/license-Apache%202.0-blue.svg?sanitize=true" alt="License" /&gt; &lt;/a&gt; &lt;a href="https://github.com/alibaba/ROLL/issues"&gt; &lt;img src="https://img.shields.io/github/issues/alibaba/ROLL" alt="GitHub issues" /&gt; &lt;/a&gt; &lt;a href="https://github.com/alibaba/ROLL/stargazers"&gt; &lt;img src="https://img.shields.io/github/stars/alibaba/ROLL?style=social" alt="Repo stars" /&gt; &lt;/a&gt; &lt;a href="https://arxiv.org/abs/2506.06122"&gt;&lt;img src="https://img.shields.io/static/v1?label=arXiv&amp;amp;message=Paper&amp;amp;color=red" /&gt;&lt;/a&gt; 
  &lt;!-- ÁªÑÁªá‰∏ªÈ°µÔºöÁÇπÂáªË∑≥ËΩ¨Âà∞ https://github.com/alibaba --&gt; &lt;a href="https://raw.githubusercontent.com/alibaba/ROLL/main/assets/roll_wechat.png" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/WeChat-green?logo=wechat" alt="WeChat QR" /&gt; &lt;/a&gt; &lt;a href="https://deepwiki.com/alibaba/ROLL" target="_blank"&gt; &lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt; &lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/alibaba/ROLL/main/assets/future_lab.png" target="_blank"&gt; &lt;img src="https://img.shields.io/twitter/follow/FutureLab2025?style=social" alt="X QR" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;ROLL is an efficient and user-friendly RL library designed for Large Language Models (LLMs) utilizing Large Scale GPU resources. It significantly enhances LLM performance in key areas such as human preference alignment, complex reasoning, and multi-turn agentic interaction scenarios.&lt;/p&gt; 
&lt;p&gt;Leveraging a multi-role distributed architecture with Ray for flexible resource allocation and heterogeneous task scheduling, ROLL integrates cutting-edge technologies like Megatron-Core, SGLang and vLLM to accelerate model training and inference.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üì¢ News&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;üì£ Updates&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;[01/01/2026]&lt;/strong&gt; üéâ Our &lt;a href="https://arxiv.org/abs/2512.24873"&gt;Let It Flow: Agentic Crafting on Rock and Roll&lt;/a&gt; report released! Introducing ALE ecosystem and ROME, an open-source agentic model with novel IPA algorithm.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;[11/08/2025]&lt;/strong&gt; üéâ Our &lt;a href="https://github.com/alibaba/ROCK"&gt;ROCK: Reinforcement Open Construction Kit&lt;/a&gt; released, Explore the new capabilities!.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;[10/23/2025]&lt;/strong&gt; üéâ Our Papers released, see &lt;a href="https://arxiv.org/abs/2510.01656"&gt;Asymmetric Proximal Policy Optimization: mini-critics boost LLM reasoning&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/2510.13554"&gt;Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm Enables Fine-Grained Policy Optimization&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;[10/14/2025]&lt;/strong&gt; üéâ Our Paper released, see &lt;a href="https://arxiv.org/abs/2510.11345"&gt;Part II: ROLL Flash -- Accelerating RLVR and Agentic Training with Asynchrony&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;[09/28/2025]&lt;/strong&gt; üéâ Ascend NPU support ‚Äî see &lt;a href="https://alibaba.github.io/ROLL/docs/User%20Guides/Hardware%20Support/ascend_usage"&gt;usage guide&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;[09/25/2025]&lt;/strong&gt; üéâ Our Paper released, see &lt;a href="https://arxiv.org/abs/2509.21009"&gt;RollPacker: Mitigating Long-Tail Rollouts for Fast, Synchronous RL Post-Training&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;[09/24/2025]&lt;/strong&gt; üéâ Support &lt;a href="https://raw.githubusercontent.com/alibaba/ROLL/main/examples/wan2.2-14B-reward_fl_ds/reward_fl_config.yaml"&gt;Wan2_2 Reward FL pipeline&lt;/a&gt;. Explore the new capabilities!&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;[09/23/2025]&lt;/strong&gt; üéâ ROLL aligns with GEM environment definition, providing agentic Tool Use training capabilities, &lt;a href="https://raw.githubusercontent.com/alibaba/ROLL/main/docs_roll/docs/English/UserGuide/agentic/Tool_Use.md"&gt;ToolUse docs&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;[09/16/2025]&lt;/strong&gt; üéâ Qwen3-Next model training is supported, refer to &lt;a href="https://raw.githubusercontent.com/alibaba/ROLL/main/examples/qwen3-next-80BA3B-rlvr_megatron/rlvr_config.yaml"&gt;configuration&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;[09/04/2025]&lt;/strong&gt; üéâ ROLL supports vLLM dynamic FP8 rollout and remove_padding for acceleration.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;[08/28/2025]&lt;/strong&gt; üéâ ROLL supports SFT pipeline, refer to &lt;a href="https://raw.githubusercontent.com/alibaba/ROLL/main/examples/qwen2.5-7B-sft_megatron/sft_config.yaml"&gt;configuration&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;[08/13/2025]&lt;/strong&gt; üéâ ROLL supports AMD GPUs with out-of-box image docker and Dockerfile and specific yamls under &lt;code&gt;examples/&lt;/code&gt; directory. Please refer to &lt;a href="https://alibaba.github.io/ROLL/docs/Getting%20Started/Installation/"&gt;Installation&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;[08/11/2025]&lt;/strong&gt; üéâ Our Paper released, see &lt;a href="https://arxiv.org/abs/2508.08221"&gt;Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;[08/10/2025]&lt;/strong&gt; üéâ Agentic RL supports &lt;a href="https://raw.githubusercontent.com/alibaba/ROLL/main/examples/qwen2.5-0.5B-agentic/agent_val_frozen_lake_gigpo.yaml"&gt;stepwise learning&lt;/a&gt;, like &lt;a href="https://arxiv.org/abs/2505.10978"&gt;GiGPO&lt;/a&gt;; Distill supports &lt;a href="https://raw.githubusercontent.com/alibaba/ROLL/main/examples/qwen2.5-vl-7B-distill/distill_vl_megatron.yaml"&gt;VLM&lt;/a&gt;. Explore the new capabilities!&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;[08/06/2025]&lt;/strong&gt; üéâ ROLL PPT is now available, &lt;a href="https://raw.githubusercontent.com/alibaba/ROLL/main/assets/ROLL%20%E9%AB%98%E6%95%88%E4%B8%94%E7%94%A8%E6%88%B7%E5%8F%8B%E5%A5%BD%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8BRL%E8%AE%AD%E7%BB%83%E6%A1%86%E6%9E%B6.pdf"&gt;Slides&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;[07/31/2025]&lt;/strong&gt; üéâ Refactor agentic rl design. Support agentic rl &lt;a href="https://raw.githubusercontent.com/alibaba/ROLL/main/examples/qwen2.5-0.5B-agentic/agent_val_frozen_lake_async.yaml"&gt;async training&lt;/a&gt;. Explore the new capabilities!&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;[07/31/2025]&lt;/strong&gt; üéâ Support &lt;a href="https://raw.githubusercontent.com/alibaba/ROLL/main/examples/qwen2.5-7B-distill_megatron/run_distill_pipeline.sh"&gt;DistillPipeline&lt;/a&gt;/&lt;a href="https://raw.githubusercontent.com/alibaba/ROLL/main/examples/qwen2.5-3B-dpo_megatron/run_dpo_pipeline.sh"&gt;DpoPipeline&lt;/a&gt;. Support &lt;a href="https://raw.githubusercontent.com/alibaba/ROLL/main/examples/qwen2.5-7B-rlvr_megatron/rlvr_lora_zero3.yaml"&gt;lora&lt;/a&gt;. Support &lt;a href="https://arxiv.org/abs/2507.18071"&gt;GSPO&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;[06/25/2025]&lt;/strong&gt; üéâ Support thread env for env scaling and support &lt;a href="https://raw.githubusercontent.com/alibaba/ROLL/main/examples/qwen2.5-vl-3B-agentic/agentic_val_sokoban.yaml"&gt;qwen2.5 VL agentic pipeline&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;[06/13/2025]&lt;/strong&gt; üéâ Support &lt;a href="https://raw.githubusercontent.com/alibaba/ROLL/main/examples/qwen2.5-vl-7B-rlvr/rlvr_megatron.yaml"&gt;Qwen2.5 VL rlvr pipeline&lt;/a&gt; and upgrade mcore to 0.12 version.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;[06/09/2025]&lt;/strong&gt; üéâ ROLL tech report is now available! Access the report &lt;a href="https://arxiv.org/abs/2506.06122"&gt;here&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;[06/08/2025]&lt;/strong&gt; üéâSupports Qwen3(&lt;a href="https://raw.githubusercontent.com/alibaba/ROLL/main/examples/qwen3-8B-rlvr_megatron/rlvr_config.yaml"&gt;8B&lt;/a&gt;/14B/32B), Qwen3-MoE(&lt;a href="https://raw.githubusercontent.com/alibaba/ROLL/main/examples/qwen3-30BA3B-rlvr_megatron/rlvr_config.yaml"&gt;30A3&lt;/a&gt;/&lt;a href="https://raw.githubusercontent.com/alibaba/ROLL/main/examples/qwen3-235BA22B-rlvr_megatron/rlvr_config.yaml"&gt;235A22&lt;/a&gt;), Qwen2.5(&lt;a href="https://raw.githubusercontent.com/alibaba/ROLL/main/examples/qwen2.5-7B-rlvr_megatron/rlvr_config.yaml"&gt;7B&lt;/a&gt;/14B/32B/72B) LLM models.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;[05/30/2025]&lt;/strong&gt; üéâ Training &lt;a href="https://raw.githubusercontent.com/alibaba/ROLL/main/examples/qwen2.5-7B-rlvr_megatron/rlvr_config.yaml"&gt;RLVR&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/alibaba/ROLL/main/examples/qwen2.5-0.5B-agentic/agent_val_frozen_lake.yaml"&gt;Agentic RL&lt;/a&gt; with ROLL is now available! Explore the new capabilities.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ Get Started&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://alibaba.github.io/ROLL/"&gt;Documents&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://alibaba.github.io/ROLL/docs/Getting%20Started/Installation/"&gt;Installation&lt;/a&gt;&lt;br /&gt; &lt;a href="https://alibaba.github.io/ROLL/docs/User%20Guides/Configuration/config_system"&gt;Config System Explanation&lt;/a&gt;&lt;br /&gt; &lt;a href="https://alibaba.github.io/ROLL/docs/Getting%20Started/Debugging%20Guide/debug_guide"&gt;Debugging Guide&lt;/a&gt;&lt;br /&gt; &lt;a href="https://alibaba.github.io/ROLL/docs/User%20Guides/Tracker%20&amp;amp;%20Metrics/trackers_and_metrics"&gt;Trackers and Metrics&lt;/a&gt;&lt;br /&gt; &lt;a href="https://alibaba.github.io/ROLL/docs/User%20Guides/Advanced%20Features/checkpoint_and_resume"&gt;Checkpoint Saving and Resuming Guide&lt;/a&gt;&lt;br /&gt; &lt;a href="https://alibaba.github.io/ROLL/docs/User%20Guides/Advanced%20Features/megatron_convert_2_hf"&gt;Converting MCoreAdapter Models to Hugging Face Format&lt;/a&gt;&lt;br /&gt; &lt;a href="https://alibaba.github.io/ROLL/docs/Getting%20Started/Quick%20Start/single_node_quick_start"&gt;Quick Start: Single-Node Deployment Guide&lt;/a&gt;&lt;br /&gt; &lt;a href="https://alibaba.github.io/ROLL/docs/Getting%20Started/Quick%20Start/multi_nodes_quick_start"&gt;Quick Start: Multi-Node Deployment Guide&lt;/a&gt;&lt;br /&gt; &lt;a href="https://alibaba.github.io/ROLL/docs/Getting%20Started/Quick%20Start/aliyun_serverless_devpod_quick_start"&gt;Quick Start: Using Alibaba Cloud Function Compute DevPod for Rapid Development&lt;/a&gt; &lt;a href="https://alibaba.github.io/ROLL/docs/Getting%20Started/FAQ/qa_issues"&gt;Frequently Asked Questions&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;UserGuide&lt;/h3&gt; 
&lt;h4&gt;Pipeline Step by Step&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://alibaba.github.io/ROLL/docs/User%20Guides/Pipeline/rlvr_pipeline_start"&gt;RLVR Pipeline&lt;/a&gt;&lt;br /&gt; &lt;a href="https://alibaba.github.io/ROLL/docs/User%20Guides/Pipeline/agentic_pipeline_start"&gt;Agentic Pipeline&lt;/a&gt;&lt;br /&gt; &lt;a href="https://alibaba.github.io/ROLL/docs/User%20Guides/Pipeline/agent_pipeline_start"&gt;Agentic Comprehensive Guide&lt;/a&gt;&lt;br /&gt; &lt;a href="https://alibaba.github.io/ROLL/docs/User%20Guides/Pipeline/distill_pipeline_start"&gt;Distill Pipeline&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Algorithms&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://alibaba.github.io/ROLL/docs/User%20Guides/Algorithms/Reinforce_Plus_Plus"&gt;Reinforce++&lt;/a&gt;&lt;br /&gt; &lt;a href="https://alibaba.github.io/ROLL/docs/User%20Guides/Algorithms/TOPR"&gt;TOPR&lt;/a&gt;&lt;br /&gt; &lt;a href="https://alibaba.github.io/ROLL/docs/User%20Guides/Agentic/agentic_GiGPO"&gt;GiGPO&lt;/a&gt;&lt;br /&gt; &lt;a href="https://alibaba.github.io/ROLL/docs/User%20Guides/Algorithms/PPO"&gt;PPO&lt;/a&gt;&lt;br /&gt; &lt;a href="https://alibaba.github.io/ROLL/docs/User%20Guides/Algorithms/LitePPO"&gt;Lite PPO&lt;/a&gt;&lt;br /&gt; &lt;a href="https://alibaba.github.io/ROLL/docs/User%20Guides/Algorithms/GRPO"&gt;GRPO&lt;/a&gt;&lt;br /&gt; &lt;a href="https://alibaba.github.io/ROLL/docs/User%20Guides/Algorithms/GSPO"&gt;GSPO&lt;/a&gt;&lt;br /&gt; &lt;a href="https://alibaba.github.io/ROLL/docs/User%20Guides/Algorithms/RAFT_Plus_Plus"&gt;RAFT++&lt;/a&gt;&lt;br /&gt; &lt;a href="https://alibaba.github.io/ROLL/docs/User%20Guides/Agentic/agentic_StarPO"&gt;StarPO&lt;/a&gt;&lt;br /&gt; &lt;a href="https://alibaba.github.io/ROLL/docs/User%20Guides/Algorithms/Reward_FL"&gt;RewardFL&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Backend&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://alibaba.github.io/ROLL/docs/User%20Guides/Configuration/deepspeed"&gt;DeepSeed&lt;/a&gt;&lt;br /&gt; &lt;a href="https://alibaba.github.io/ROLL/docs/User%20Guides/Configuration/megatron"&gt;Megatron&lt;/a&gt;&lt;br /&gt; &lt;a href="https://alibaba.github.io/ROLL/docs/User%20Guides/Configuration/vllm"&gt;vLLM&lt;/a&gt;&lt;br /&gt; &lt;a href="https://alibaba.github.io/ROLL/docs/User%20Guides/Configuration/sglang"&gt;SGLang&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Advanced Features&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://alibaba.github.io/ROLL/docs/User%20Guides/Advanced%20Features/async_parallel_rollout"&gt;Asynchronous Parallel Rollout&lt;/a&gt;&lt;br /&gt; &lt;a href="https://alibaba.github.io/ROLL/docs/User%20Guides/Advanced%20Features/async_training"&gt;Asynchronous Training Feature&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Performance Optimization &amp;amp; Resource Management&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://alibaba.github.io/ROLL/docs/User%20Guides/Configuration/device_mapping"&gt;Resource Config&lt;/a&gt;&lt;br /&gt; &lt;a href="https://alibaba.github.io/ROLL/docs/User%20Guides/Advanced%20Features/offload_reload_control"&gt;GPU Time-Division Multiplexing Control&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;ROLL x Ascend&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://alibaba.github.io/ROLL/docs/User%20Guides/Hardware%20Support/ascend_usage"&gt;Ascend Usage Guide&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚ú® Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-task RL Training (RLVR):&lt;/strong&gt; Covers mathematics, coding, general reasoning, open-ended Q&amp;amp;A, instruction following, etc. 
  &lt;ul&gt; 
   &lt;li&gt;Flexible &lt;code&gt;domain_batch_size&lt;/code&gt; distribution control.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Sample-level asynchronous parallel Rollout&lt;/strong&gt;, asynchronous reward calculation, and dynamic sampling.&lt;/li&gt; 
   &lt;li&gt;Asynchronous training under implementation.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Agentic RL:&lt;/strong&gt; Multi-turn interaction capabilities for games, multi-turn dialogues, tool use, etc. 
  &lt;ul&gt; 
   &lt;li&gt;Environment-level &lt;strong&gt;asynchronous parallel rollout&lt;/strong&gt;.&lt;/li&gt; 
   &lt;li&gt;Supports &lt;strong&gt;asynchronous training&lt;/strong&gt;.&lt;/li&gt; 
   &lt;li&gt;Multi-turn interaction rollout supports &lt;strong&gt;local debugging&lt;/strong&gt;, improving multi-turn interaction business development efficiency.&lt;/li&gt; 
   &lt;li&gt;Supports &lt;strong&gt;TrajectoryWise (StartPO)&lt;/strong&gt; and &lt;strong&gt;StepWise (GiGPO)&lt;/strong&gt; training paradigms.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Algorithm-Friendly:&lt;/strong&gt; Provides flexible and rich RL strategy configurations by default. 
  &lt;ul&gt; 
   &lt;li&gt;Over 20 rich reinforcement learning strategy options, such as reward normalization, reward clipping, various advantage estimation methods, etc.&lt;/li&gt; 
   &lt;li&gt;Out-of-the-box support for reinforcement learning algorithms, such as &lt;strong&gt;PPO, GRPO, Reinforce++, TOPR, RAFT++, GSPO&lt;/strong&gt;, etc.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Rich Training and Inference Engine:&lt;/strong&gt; Ray-based multi-role distributed architecture; Strategy abstraction unifies various backends, enabling easy operation from single machines to thousands-of-GPU clusters. 
  &lt;ul&gt; 
   &lt;li&gt;Inference/Generation supports vLLM, SGLang.&lt;/li&gt; 
   &lt;li&gt;Training supports DeepSpeed (ZeRO), Megatron-LM 5D parallelism (mcore-adapter, dp/tp/pp/cp/ep), FSDP under implementation.&lt;/li&gt; 
   &lt;li&gt;Extreme offload/reload capabilities.&lt;/li&gt; 
   &lt;li&gt;Supports &lt;a href="https://alibaba.github.io/ROLL/docs/User%20Guides/Configuration/lora"&gt;LoRA&lt;/a&gt; training.&lt;/li&gt; 
   &lt;li&gt;Supports FP8 rollout (FP8 inference for LLM as judge, FP8 rollout with BF16 training under development).&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AutoDeviceMapping:&lt;/strong&gt; Supports custom device mapping for different roles, flexibly managing colocated and disaggregated deployments.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Observability:&lt;/strong&gt; Integrated with SwanLab / WandB / TensorBoard, tracking of performance for each domain and reward type.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Rich Post-training Technical Support:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Agentic RL LLM &amp;amp; VLM&lt;/li&gt; 
   &lt;li&gt;RLVR LLM &amp;amp; VLM&lt;/li&gt; 
   &lt;li&gt;Distill Pipeline LLM &amp;amp; VLM&lt;/li&gt; 
   &lt;li&gt;DPO Pipeline&lt;/li&gt; 
   &lt;li&gt;SFT Pipeline under development&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üîÆ Upcoming Features&lt;/h2&gt; 
&lt;p&gt;We are continuously working to expand ROLL's capabilities:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚è±Ô∏è &lt;strong&gt;Async RLVR pipeline&lt;/strong&gt;: For even more efficient and streamlined asynchronous operations.&lt;/li&gt; 
 &lt;li&gt;‚öôÔ∏è &lt;strong&gt;FSDP2&lt;/strong&gt;: Integrating the latest Fully Sharded Data Parallel techniques.&lt;/li&gt; 
 &lt;li&gt;üîç &lt;strong&gt;Support DeepseekV3&lt;/strong&gt;: Adding compatibility for the newest Deepseek models.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üèÜ Notable work based on ROLL&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2512.24957"&gt;STAgent&lt;/a&gt;: An agentic LLM specialized for spatio-temporal understanding and complex tasks like constrained POI discovery and itinerary planning, featuring hierarchical data curation with 1:10,000 filter ratio and cascaded training (seed SFT + difficulty-aware SFT + RL), achieving strong performance on TravelBench while preserving general capabilities.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2510.14255"&gt;IPRO&lt;/a&gt;: A novel video diffusion framework using reinforcement learning to enhance identity preservation in human-centric I2V generation, optimizing diffusion models with face identity scorer and KL-divergence regularization.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2510.07972"&gt;TaoSR-SHE&lt;/a&gt;: Stepwise Hybrid Examination Reinforcement Learning Framework for Taobao Search Relevance, with SRPO (hybrid reward model + offline verifier), diversified data filtering, and multi-stage curriculum learning.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2510.05943"&gt;EARL&lt;/a&gt;: Efficient Agentic RL Systems for LLMs, introducing a dynamic parallelism selector and a layout-aware data dispatcher to boost throughput, reduce memory and data movement bottlenecks, enabling stable large-scale agentic RL without hard context-length limits.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2510.07685"&gt;LiveThinking&lt;/a&gt;: Real-time reasoning for AI-powered livestreaming by distilling a 670B teacher LLM to a 30B MoE (3B active) via Rejection Sampling Fine-Tuning, then compressing reasoning with GRPO; delivers sub-second latency and ~30x compute reduction, with gains in response correctness (3.3%), helpfulness (21.8%), and GMV in Taobao Live.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.arxiv.org/abs/2510.08048"&gt;TaoSR-AGRL&lt;/a&gt;: Adaptive Guided Reinforcement Learning for LLM-based e-commerce relevance, introducing Rule-aware Reward Shaping and Adaptive Guided Replay to improve long-horizon reasoning, rule adherence, and training stability in Taobao Search; deployed in main search handling hundreds of millions of users.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.arxiv.org/abs/2507.22879"&gt;RecGPT&lt;/a&gt;: a next-generation, LLM-driven framework that places user intent at the core of recommender systems, fostering a more sustainable and mutually beneficial ecosystem.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2508.12365"&gt;TaoSR1&lt;/a&gt;: A novel LLM framework directly deploying Chain-of-Thought (CoT) reasoning for e-commerce query-product relevance prediction, overcoming deployment challenges for superior performance.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.arxiv.org/abs/2509.15927"&gt;AIGB-Pearl&lt;/a&gt;: a novel auto-bidding method that integrates generative planning and policy optimization, utilizing an LLM-enhanced trajectory evaluator to iteratively refine bidding strategies for state-of-the-art advertising performance.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üôè Citation and Acknowledgement&lt;/h2&gt; 
&lt;p&gt;ROLL is inspired by the design of OpenRLHF, VeRL, Nemo-Aligner, and RAGEN. The project is developed by Alibaba TAOBAO &amp;amp; TMALL Group and Alibaba Group. The code is distributed under the Apache License (Version 2.0). This product contains various third-party components under other open-source licenses. See the &lt;code&gt;NOTICE&lt;/code&gt; file for more information.&lt;/p&gt; 
&lt;p&gt;The following repositories have been used in ROLL, either in their close-to-original form or as an inspiration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/NVIDIA/Megatron-LM"&gt;NVIDIA/Megatron-LM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/DeepSpeed"&gt;microsoft/DeepSpeed&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sgl-project/sglang"&gt;sgl-project/sglang&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vllm-project/vllm"&gt;vllm-project/vllm&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/modelscope/DiffSynth-Studio"&gt;modelscope/DiffSynth-Studio&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you use ROLL in your research or project, please consider citing us:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{wang2025reinforcement,
  title={Reinforcement Learning Optimization for Large-Scale Learning: An Efficient and User-Friendly Scaling Library},
  author={Wang, Weixun and Xiong, Shaopan and Chen, Gengru and Gao, Wei and Guo, Sheng and He, Yancheng and Huang, Ju and Liu, Jiaheng and Li, Zhendong and Li, Xiaoyang and others},
  journal={arXiv preprint arXiv:2506.06122},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§ù About [ROCK &amp;amp; ROLL Team]&lt;/h2&gt; 
&lt;p&gt;ROLL is a project jointly developed by Taotian Future Living Lab and Alibaba AI Engine Team, with a strong emphasis on pioneering the future of Reinforcement Learning (RL). Our mission is to explore and shape innovative forms of future living powered by advanced RL technologies. If you are passionate about the future of RL and want to be part of its evolution, we warmly welcome you to join us!üëá&lt;/p&gt; 
&lt;a href="https://raw.githubusercontent.com/alibaba/ROLL/main/assets/roll_wechat.png" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/WeChat-green?logo=wechat" alt="WeChat QR" /&gt; &lt;/a&gt; 
&lt;a href="https://raw.githubusercontent.com/alibaba/ROLL/main/assets/future_lab.png" target="_blank"&gt; &lt;img src="https://img.shields.io/twitter/follow/FutureLab2025?style=social" alt="X QR" /&gt; &lt;/a&gt; 
&lt;hr /&gt; 
&lt;p&gt;We are HIRING!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Post Training Infra Á†îÂèëÂ∑•Á®ãÂ∏à &lt;a href="https://talent-holding.alibaba.com/off-campus/position-detail?lang=zh&amp;amp;positionId=7000016304"&gt;JD link&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Â§ßÊ®°ÂûãËÆ≠ÁªÉ‰∏ìÂÆ∂Ôºö 
  &lt;ul&gt; 
   &lt;li&gt;ÔºàÁ§æÊãõÔºâ&lt;a href="https://talent.taotian.com/off-campus/position-detail?lang=zh&amp;amp;positionId=7000024203"&gt;JD link&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;ÔºàÊ†°ÊãõÔºâ&lt;a href="https://talent.taotian.com/campus/position-detail?positionId=199900140053"&gt;JD link&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Infra Á†îÁ©∂ÂûãÂÆû‰π†Áîü &lt;a href="https://talent-holding.alibaba.com/campus/position-detail?lang=zh&amp;amp;positionId=59900004115"&gt;JD link&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt;
  We welcome contributions from the community! ü§ù 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Arindam200/awesome-ai-apps</title>
      <link>https://github.com/Arindam200/awesome-ai-apps</link>
      <description>&lt;p&gt;A collection of projects showcasing RAG, agents, workflows, and other AI use cases&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/assets/banner_new.png" alt="Banner" /&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h1&gt;Awesome AI Apps &lt;a href="https://awesome.re"&gt;&lt;img src="https://awesome.re/badge.svg?sanitize=true" alt="Awesome" /&gt;&lt;/a&gt;&lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/14662" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14662" alt="Arindam200%2Fawesome-ai-apps | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;This repository is a comprehensive collection of &lt;strong&gt;70+ practical examples, tutorials, and recipes&lt;/strong&gt; for building powerful LLM-powered applications. From simple chatbots to advanced AI agents, these projects serve as a guide for developers working with various AI frameworks and tools.&lt;/p&gt; 
&lt;h2&gt;üìã Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/#-courses"&gt;üéì Courses&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/#-featured-ai-apps"&gt;üöÄ Featured AI Apps&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/#-starter-agents"&gt;üß© Starter Agents&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/#-simple-agents"&gt;ü™∂ Simple Agents&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/#%EF%B8%8F-mcp-agents"&gt;üóÇÔ∏è MCP Agents&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/#-memory-agents"&gt;üß† Memory Agents&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/#-rag-applications"&gt;üìö RAG Applications&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/#-advanced-agents"&gt;üî¨ Advanced Agents&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/#-tutorials--videos"&gt;üì∫ Tutorials &amp;amp; Videos&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/#getting-started"&gt;üöÄ Getting Started&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/#-contributing"&gt;ü§ù Contributing&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;h2&gt;üíé Sponsors&lt;/h2&gt; 
 &lt;p align="center"&gt; A huge thank you to our sponsors for their generous support! &lt;/p&gt; 
 &lt;table align="center" cellpadding="10" style="width:100%; border-collapse:collapse;"&gt; 
  &lt;tbody&gt;
   &lt;tr align="center"&gt; 
    &lt;td width="300" valign="middle" align="center"&gt; &lt;a href="https://dub.sh/brightdata" target="_blank" title="Visit Bright Data"&gt; &lt;img src="https://mintlify.s3.us-west-1.amazonaws.com/brightdata/logo/light.svg?sanitize=true" height="35" style="max-width:180px;" alt="Bright Data - Web Data Platform" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;sub&gt; &lt;span style="white-space:nowrap;"&gt;Web Data Platform&lt;/span&gt; &lt;br /&gt; &lt;a href="https://dub.sh/brightdata" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/Visit%20Site-blue?style=flat-square" alt="Visit Bright Data website" /&gt; &lt;/a&gt; &lt;/sub&gt; &lt;/td&gt; 
    &lt;td width="300" valign="middle" align="center"&gt; &lt;a href="https://dub.sh/nebius" target="_blank" title="Visit Nebius Token Factory"&gt; &lt;img src="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/assets/nebius.png" height="36" style="max-width:180px;" alt="Nebius Token Factory" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;sub&gt; &lt;span style="white-space:nowrap;"&gt;AI Inference Provider&lt;/span&gt; &lt;br /&gt; &lt;a href="https://dub.sh/nebius" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/Visit%20Site-blue?style=flat-square" alt="Visit Nebius Token Factory" /&gt; &lt;/a&gt; &lt;/sub&gt; &lt;/td&gt; 
    &lt;td width="300" valign="middle" align="center"&gt; &lt;a href="https://dub.sh/scrapegraphai" target="_blank" title="Visit ScrapeGraphAI on GitHub"&gt; &lt;img src="https://raw.githubusercontent.com/ScrapeGraphAI/ScrapeGraph-AI/main/docs/assets/scrapegraphai_logo.png" height="44" style="max-width:180px;" alt="ScrapeGraphAI - Web Scraping Library" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;sub&gt; &lt;span style="white-space:nowrap;"&gt;AI Web Scraping framework&lt;/span&gt; &lt;br /&gt; &lt;a href="https://dub.sh/scrapegraphai" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/Visit%20Site-blue?style=flat-square" alt="View ScrapeGraphAI on GitHub" /&gt; &lt;/a&gt; &lt;/sub&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr align="center"&gt; 
    &lt;td width="300" valign="middle" align="center"&gt; &lt;a href="https://dub.sh/memorilabs" target="_blank" title="Visit Memorilabs"&gt; &lt;img src="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/assets/memori.png" height="36" style="max-width:180px;" alt="Memori - SQL Native Memory for AI" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;sub&gt; &lt;span style="white-space:nowrap;"&gt;SQL Native Memory for AI&lt;/span&gt; &lt;br /&gt; &lt;a href="https://dub.sh/memorilabs" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/Visit%20Site-blue?style=flat-square" alt="Visit Memorilabs website" /&gt; &lt;/a&gt; &lt;/sub&gt; &lt;/td&gt; 
    &lt;td width="300" valign="middle" align="center"&gt; &lt;a href="https://dub.sh/copilotkit" target="_blank" title="Visit CopilotKit"&gt; &lt;img src="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/assets/copilot-kit-logo.svg?sanitize=true" height="36" style="max-width:180px;" alt="CopilotKit - Agentic Application Platform" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;sub&gt; &lt;span style="white-space:nowrap;"&gt;Agentic Application Platform&lt;/span&gt; &lt;br /&gt; &lt;a href="https://dub.sh/copilotkit" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/Visit%20Site-blue?style=flat-square" alt="Visit CopilotKit website" /&gt; &lt;/a&gt; &lt;/sub&gt; &lt;/td&gt; 
    &lt;td width="300" valign="middle" align="center"&gt; &lt;a href="https://dub.sh/scalekitt" target="_blank" title="Visit ScaleKit"&gt; &lt;img src="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/assets/scalekit.svg?sanitize=true" height="36" style="max-width:180px;" alt="ScaleKit - Auth Stack for AI" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;sub&gt; &lt;span style="white-space:nowrap;"&gt;Auth Stack for AI&lt;/span&gt; &lt;br /&gt; &lt;a href="https://dub.sh/scalekitt" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/Visit%20Site-blue?style=flat-square" alt="Visit ScaleKit website" /&gt; &lt;/a&gt; &lt;/sub&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;!-- &lt;tr align="center"&gt;
    &lt;td width="200" valign="middle" align="center"&gt;
      &lt;a href="https://okahu.ai" target="_blank" title="Visit Okahu"&gt;
        &lt;img src="assets/okahu.png" height="36" style="max-width:180px;" alt="Okahu - AI Platform"&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;sub&gt;
        &lt;span style="white-space:nowrap;"&gt;AI Platform&lt;/span&gt;
        &lt;br&gt;
        &lt;a href="https://okahu.ai" target="_blank"&gt;
          &lt;img src="https://img.shields.io/badge/Visit%20Site-blue?style=flat-square" alt="Visit Okahu website"&gt;
        &lt;/a&gt;
      &lt;/sub&gt;
    &lt;/td&gt;
  &lt;/tr&gt; --&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;h3&gt;üíé Become a Sponsor&lt;/h3&gt; 
 &lt;p align="center"&gt; Interested in sponsoring this project? Feel free to reach out! &lt;br /&gt; &lt;a href="https://dub.sh/arindam-linkedin" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&amp;amp;logo=linkedin&amp;amp;logoColor=white" alt="LinkedIn" /&gt; &lt;/a&gt; &lt;a href="mailto:arindammajumder2020@gmail.com"&gt; &lt;img src="https://img.shields.io/badge/Email-D14836?style=for-the-badge&amp;amp;logo=gmail&amp;amp;logoColor=white" alt="Email" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üéì Courses&lt;/h2&gt; 
&lt;h3&gt;AWS Strands Course for Beginners&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Comprehensive hands-on course on building AI agents with AWS Strands SDK:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/course/aws_strands"&gt;&lt;strong&gt;AWS Strands Course&lt;/strong&gt;&lt;/a&gt; - Complete 8-lesson course covering agent fundamentals to production patterns 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Foundation&lt;/strong&gt;: Basic agents, session management, structured output&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Integration&lt;/strong&gt;: MCP agents, human-in-the-loop patterns&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Multi-Agent&lt;/strong&gt;: Orchestrator agents, swarm intelligence, graph workflows&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Production&lt;/strong&gt;: Observability, safety guardrails, and best practices&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üöÄ Featured AI Apps&lt;/h2&gt; 
&lt;h3&gt;üß© Starter Agents&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Quick-start agents for learning and extending different AI frameworks.&lt;/strong&gt; &lt;em&gt;12 projects&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/starter_ai_agents/agno_starter"&gt;Agno HackerNews Analysis&lt;/a&gt; - Agno-based agent for trend analysis on HackerNews&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/starter_ai_agents/openai_agents_sdk"&gt;OpenAI SDK Starter&lt;/a&gt; - OpenAI Agents SDK with email helper &amp;amp; haiku writer examples&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/starter_ai_agents/llamaindex_starter"&gt;LlamaIndex Task Manager&lt;/a&gt; - LlamaIndex-powered task assistant&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/starter_ai_agents/crewai_starter"&gt;CrewAI Research Crew&lt;/a&gt; - Multi-agent research team example&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/starter_ai_agents/pydantic_starter"&gt;PydanticAI Weather Bot&lt;/a&gt; - Real-time weather information agent&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/starter_ai_agents/langchain_langgraph_starter"&gt;LangChain-LangGraph Starter&lt;/a&gt; - LangChain + LangGraph workflow starter&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/starter_ai_agents/aws_strands_starter"&gt;AWS Strands Agent Starter&lt;/a&gt; - Weather report agent using AWS Strands SDK&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/starter_ai_agents/camel_ai_starter"&gt;Camel AI Starter&lt;/a&gt; - Performance benchmarking tool comparing various AI models&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/starter_ai_agents/dspy_starter"&gt;DSPy Starter&lt;/a&gt; - DSPy framework for building and optimizing AI systems&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/starter_ai_agents/google_adk_starter"&gt;Google ADK Starter&lt;/a&gt; - Google Agent Development Kit starter template&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/starter_ai_agents/cagent_starter"&gt;cagent Starter&lt;/a&gt; - Open-source customizable multi-agent runtime by Docker&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/starter_ai_agents/sayna_starter"&gt;Sayna Voice Agent&lt;/a&gt; - Real-time voice infrastructure with multi-provider STT/TTS (Deepgram, ElevenLabs, Azure, Google) and WebSocket streaming&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ü™∂ Simple Agents&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Straightforward, practical use-cases for everyday AI applications.&lt;/strong&gt; &lt;em&gt;13 projects&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/agno_ai_examples"&gt;Agno AI Examples&lt;/a&gt; - Simple to multi-agent examples with web search &amp;amp; knowledge base&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/finance_agent"&gt;Finance Agent&lt;/a&gt; - Real-time stock &amp;amp; market data tracking agent&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/human_in_the_loop_agent"&gt;Human-in-the-Loop Agent&lt;/a&gt; - HITL actions for safe AI task execution&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/newsletter_agent"&gt;Newsletter Generator&lt;/a&gt; - AI-powered newsletter builder with Firecrawl integration&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/reasoning_agent"&gt;Reasoning Agent&lt;/a&gt; - Step-by-step financial reasoning demonstration&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/agno_ui_agent"&gt;Agno UI Example&lt;/a&gt; - Interactive UI for web &amp;amp; finance agents&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/mastra_ai_weather_agent"&gt;Mastra Weather Bot&lt;/a&gt; - Weather updates using Mastra AI framework&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/cal_scheduling_agent"&gt;Calendar Assistant&lt;/a&gt; - Calendar scheduling integration with Cal.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/email_to_calendar_scheduler"&gt;Smart Scheduler Assistant&lt;/a&gt; - AI-powered Gmail reader and Google Calendar manager&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/browser_agent"&gt;Web Automation Agent&lt;/a&gt; - Browser automation agent using Nebius &amp;amp; browser-use&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/nebius_chat"&gt;Nebius Chat&lt;/a&gt; - Chat interface for Nebius Token Factory&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/llm_router"&gt;RouteLLM Chat&lt;/a&gt; - Intelligent model routing with RouteLLM (GPT-4o-mini vs Nebius Llama) for cost optimization&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/talk_to_db"&gt;Talk to Your DB&lt;/a&gt; - Natural language database queries with GibsonAI &amp;amp; LangChain&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üóÇÔ∏è MCP Agents&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Examples using Model Context Protocol for external tool integration.&lt;/strong&gt; &lt;em&gt;11 projects&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/mcp_ai_agents/doc_mcp"&gt;Doc-MCP&lt;/a&gt; - Semantic RAG documentation &amp;amp; Q&amp;amp;A system&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/mcp_ai_agents/langchain_langgraph_mcp_agent"&gt;LangGraph MCP Agent&lt;/a&gt; - LangChain ReAct agent with Couchbase integration&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/mcp_ai_agents/github_mcp_agent"&gt;GitHub MCP Agent&lt;/a&gt; - Repository insights and analysis via MCP&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/mcp_ai_agents/mcp_starter"&gt;MCP Starter&lt;/a&gt; - GitHub repository analyzer starter template&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/mcp_ai_agents/docs_qna_agent"&gt;Talk to your Docs&lt;/a&gt; - Documentation Q&amp;amp;A agent with MCP&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/mcp_ai_agents/database_mcp_agent"&gt;Database MCP Agent&lt;/a&gt; - Conversational AI agent for managing GibsonAI database projects and schemas&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/mcp_ai_agents/hotel_finder_agent"&gt;Hotel Finder Agent&lt;/a&gt; - Hotel search and booking using MCP integration&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/mcp_ai_agents/custom_mcp_server"&gt;Custom MCP Server&lt;/a&gt; - Custom MCP server implementation example&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/mcp_ai_agents/couchbase_mcp_server"&gt;Couchbase MCP Server&lt;/a&gt; - Couchbase database integration with MCP protocol&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/mcp_ai_agents/scalekit-exa-mcp-security"&gt;ScaleKit Exa MCP Security&lt;/a&gt; - Security-focused MCP integration with Exa search&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/mcp_ai_agents/e2b_docker_mcp_agent"&gt;Docker E2B MCP Agent&lt;/a&gt; - Secure AI agent for running agents in sandboxed Docker environments via MCP Gateway&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üß† Memory Agents&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Agents with advanced memory capabilities for context retention and personalization.&lt;/strong&gt; &lt;em&gt;12 projects&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/memory_agents/agno_memory_agent"&gt;Agno Memory Agent&lt;/a&gt; - Agno-based agent with persistent memory capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/memory_agents/arxiv_researcher_agent_with_memori"&gt;arXiv Researcher Agent with Memori&lt;/a&gt; - Research assistant using OpenAI Agents and GibsonAI Memori&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/memory_agents/aws_strands_agent_with_memori"&gt;AWS Strands Agent with Memori&lt;/a&gt; - AWS Strands agent enhanced with Memori memory system&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/memory_agents/blog_writing_agent"&gt;Blog Writing Agent&lt;/a&gt; - Personalized blog writing agent with memory for style consistency&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/memory_agents/social_media_agent"&gt;Social Media Agent&lt;/a&gt; - Social media automation agent with memory for brand voice&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/memory_agents/job_search_agent"&gt;Job Search Agent&lt;/a&gt; - Job search agent with memory for preference tracking&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/memory_agents/brand_reputation_monitor"&gt;Brand Reputation Monitor&lt;/a&gt; - AI-powered brand reputation monitoring with news analysis and sentiment tracking&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/memory_agents/product_launch_agent"&gt;Product Launch Agent&lt;/a&gt; - Competitive intelligence tool for analyzing competitor product launches&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/memory_agents/ai_consultant_agent/"&gt;AI Consultant Agent&lt;/a&gt; - AI-powered consulting agent using &lt;strong&gt;Memori v3&lt;/strong&gt; as long-term memory fabric and &lt;strong&gt;ExaAI&lt;/strong&gt; for research&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/memory_agents/customer_support_voice_agent"&gt;Customer Support Voice Agent&lt;/a&gt; - Voice-enabled customer support assistant with Memori v3 and Firecrawl for knowledge base management&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/memory_agents/youtube_trend_agent"&gt;YouTube Trend Agent&lt;/a&gt; - YouTube channel analysis agent with Memori, Agno, and Exa for trend analysis and video ideas&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/memory_agents/study_coach_agent"&gt;Study Coach Agent&lt;/a&gt; - AI-powered study coach with Memori v3 and LangGraph for multi-step verification of understanding&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìö RAG Applications&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Retrieve-augmented generation examples for document understanding and knowledge bases.&lt;/strong&gt; &lt;em&gt;11 projects&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/rag_apps/agentic_rag"&gt;Agentic RAG&lt;/a&gt; - Agentic RAG implementation with Agno &amp;amp; GPT-5&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/rag_apps/agentic_rag_with_web_search"&gt;Agentic RAG with Web Search&lt;/a&gt; - Advanced RAG with CrewAI, Qdrant, and Exa for hybrid search capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/rag_apps/resume_optimizer"&gt;Resume Optimizer&lt;/a&gt; - AI-powered resume optimization and enhancement tool&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/rag_apps/llamaIndex_starter"&gt;LlamaIndex RAG Starter&lt;/a&gt; - LlamaIndex + Nebius RAG starter template&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/rag_apps/pdf_rag_analyser"&gt;PDF RAG Analyzer&lt;/a&gt; - Multi-PDF chat and analysis system&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/rag_apps/qwen3_rag"&gt;Qwen3 RAG Chat&lt;/a&gt; - PDF chatbot interface built with Streamlit&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/rag_apps/chat_with_code"&gt;Chat with Code&lt;/a&gt; - Conversational code explorer and documentation assistant&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/rag_apps/gemma_ocr/"&gt;Gemma3 OCR&lt;/a&gt; - OCR-based document and image processor using Gemma3 model&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/rag_apps/nvidia_ocr/"&gt;Nvidia Nemotron OCR&lt;/a&gt; - OCR-based document and image parsing using Nvidia Nemotron-Nano-V2-12b&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/rag_apps/contextual_ai_rag"&gt;Contextual AI RAG&lt;/a&gt; - Enterprise-level RAG with managed datastores and quality evaluation&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/rag_apps/simple_rag"&gt;Simple RAG&lt;/a&gt; - Basic RAG implementation with Nebius for quick starts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üî¨ Advanced Agents&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Complex multi-agent pipelines for production-ready end-to-end workflows.&lt;/strong&gt; &lt;em&gt;14 projects&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/advance_ai_agents/deep_researcher_agent"&gt;Deep Researcher&lt;/a&gt; - Multi-stage research agent with Agno &amp;amp; ScrapeGraph AI&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/advance_ai_agents/candidate_analyser"&gt;Candilyzer&lt;/a&gt; - Candidate analysis tool for GitHub/LinkedIn profiles&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/advance_ai_agents/job_finder_agent"&gt;Job Finder&lt;/a&gt; - LinkedIn job search automation with Bright Data integration&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/advance_ai_agents/trend_analyzer_agent"&gt;AI Trend Analyzer&lt;/a&gt; - AI trend mining and analysis with Google ADK&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/advance_ai_agents/conference_talk_abstract_generator"&gt;Conference Talk Generator&lt;/a&gt; - Automated talk abstract generation with Google ADK &amp;amp; Couchbase&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/advance_ai_agents/finance_service_agent"&gt;Finance Service Agent&lt;/a&gt; - FastAPI server for stock data and predictions with Agno&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/advance_ai_agents/price_monitoring_agent"&gt;Price Monitoring Agent&lt;/a&gt; - Price monitoring and alerting agent powered by CrewAI, Twilio &amp;amp; Nebius&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/advance_ai_agents/startup_idea_validator_agent"&gt;Startup Idea Validator Agent&lt;/a&gt; - Agentic workflow to validate and analyze startup ideas&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/advance_ai_agents/meeting_assistant_agent"&gt;Meeting Assistant Agent&lt;/a&gt; - Automated meeting notes and task creation from conversations&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/advance_ai_agents/ai-hedgefund"&gt;AI Hedgefund&lt;/a&gt; - Agentic workflow for comprehensive financial analysis&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/advance_ai_agents/smart_gtm_agent"&gt;Smart GTM Agent&lt;/a&gt; - Go-to-market strategy and competitive analysis agent&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/advance_ai_agents/conference_agnositc_cfp_generator"&gt;Conference Agnostic CFP Generator&lt;/a&gt; - Automated conference proposal generation system&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/advance_ai_agents/car_finder_agent"&gt;Car Finder Agent&lt;/a&gt; - AI-powered used car recommendation system with CrewAI and MongoDB&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/advance_ai_agents/content_team_agent"&gt;Content Team Agent&lt;/a&gt; - SEO content optimization workflow with Agno &amp;amp; SerpAPI for Google AI Search ranking&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üì∫ Tutorials &amp;amp; Videos&lt;/h2&gt; 
&lt;h3&gt;üéì Course Playlists&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLMZM1DAlf0Lrc43ZtUXAwYu9DhnqxzRKZ"&gt;&lt;strong&gt;AWS Strands Course&lt;/strong&gt;&lt;/a&gt; - Complete 8-lesson course on building AI agents with AWS Strands SDK&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üîß Framework Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLMZM1DAlf0Lolxax4L2HS54Me8gn1gkz4"&gt;&lt;strong&gt;Build with MCP&lt;/strong&gt;&lt;/a&gt; - Model Context Protocol tutorials and examples&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLMZM1DAlf0LqixhAG9BDk4O_FjqnaogK8"&gt;&lt;strong&gt;Build AI Agents&lt;/strong&gt;&lt;/a&gt; - General AI agent development tutorials&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PL2ambAOfYA6-LDz0KpVKu9vJKAqhv0KKI"&gt;&lt;strong&gt;AI Agents, MCP and more...&lt;/strong&gt;&lt;/a&gt; - Mixed tutorials and project demos&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;h2&gt;üì• Stay Updated with Daily AI Insight!&lt;/h2&gt; 
 &lt;p&gt;Get easy-to-follow weekly tutorials and deep dives on AI, LLMs, and agent frameworks. Perfect for developers who want to learn, build, and stay ahead with new tech. Subscribe our Newsletter!&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://mranand.substack.com/subscribe"&gt;&lt;img src="https://github.com/user-attachments/assets/990d1947-337b-4e87-a7e6-e619ec19dee6" alt="Subscribe to our Newsletter" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Python 3.10+&lt;/strong&gt; (Python 3.11+ recommended for newer projects)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Git&lt;/strong&gt; for cloning the repository&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Package Manager&lt;/strong&gt;: &lt;code&gt;pip&lt;/code&gt; or &lt;code&gt;uv&lt;/code&gt; (recommended for faster installs)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;API Keys&lt;/strong&gt;: Most projects require API keys (see individual project READMEs)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the repository&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Arindam200/awesome-ai-apps.git
cd awesome-ai-apps
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Choose a project&lt;/strong&gt; and navigate to its directory&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd starter_ai_agents/agno_starter  # Example: Start with Agno starter
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Set up environment variables&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cp .env.example .env  # Copy example environment file
# Edit .env with your API keys
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install dependencies&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Using pip
pip install -r requirements.txt

# OR using uv (recommended - faster)
uv sync
# or
uv pip install -e .
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run the project&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python main.py
# or for Streamlit apps
streamlit run app.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Here's how you can help:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üêõ &lt;strong&gt;Report bugs&lt;/strong&gt; or suggest improvements via &lt;a href="https://github.com/Arindam200/awesome-ai-apps/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üí° &lt;strong&gt;Add new projects&lt;/strong&gt; - Submit your own AI agent examples&lt;/li&gt; 
 &lt;li&gt;üìù &lt;strong&gt;Improve documentation&lt;/strong&gt; - Help make projects more accessible&lt;/li&gt; 
 &lt;li&gt;üîß &lt;strong&gt;Fix issues&lt;/strong&gt; - Contribute code improvements and bug fixes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Before contributing:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Read our &lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/CONTRIBUTING.md"&gt;Contributing Guidelines&lt;/a&gt; for detailed information&lt;/li&gt; 
 &lt;li&gt;Check existing issues to avoid duplicates&lt;/li&gt; 
 &lt;li&gt;Follow the project structure and naming conventions&lt;/li&gt; 
 &lt;li&gt;Ensure your project includes a comprehensive README.md&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; This project follows a &lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/CODE_OF_CONDUCT.md"&gt;Contributor Code of Conduct&lt;/a&gt;. By participating, you agree to abide by its terms.&lt;/p&gt; 
&lt;h2&gt;üìú License&lt;/h2&gt; 
&lt;p&gt;This repository is licensed under the &lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/LICENSE"&gt;MIT License&lt;/a&gt;. Feel free to use and modify the examples for your projects.&lt;/p&gt; 
&lt;h2&gt;Thank You for the Support! üôè&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#Arindam200/awesome-ai-apps&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Arindam200/awesome-ai-apps&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>anthropics/claude-agent-sdk-python</title>
      <link>https://github.com/anthropics/claude-agent-sdk-python</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Claude Agent SDK for Python&lt;/h1&gt; 
&lt;p&gt;Python SDK for Claude Agent. See the &lt;a href="https://platform.claude.com/docs/en/agent-sdk/python"&gt;Claude Agent SDK documentation&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install claude-agent-sdk
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Prerequisites:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The Claude Code CLI is automatically bundled with the package - no separate installation required! The SDK will use the bundled CLI by default. If you prefer to use a system-wide installation or a specific version, you can:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install Claude Code separately: &lt;code&gt;curl -fsSL https://claude.ai/install.sh | bash&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Specify a custom path: &lt;code&gt;ClaudeAgentOptions(cli_path="/path/to/claude")&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import anyio
from claude_agent_sdk import query

async def main():
    async for message in query(prompt="What is 2 + 2?"):
        print(message)

anyio.run(main)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Basic Usage: query()&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;query()&lt;/code&gt; is an async function for querying Claude Code. It returns an &lt;code&gt;AsyncIterator&lt;/code&gt; of response messages. See &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/src/claude_agent_sdk/query.py"&gt;src/claude_agent_sdk/query.py&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from claude_agent_sdk import query, ClaudeAgentOptions, AssistantMessage, TextBlock

# Simple query
async for message in query(prompt="Hello Claude"):
    if isinstance(message, AssistantMessage):
        for block in message.content:
            if isinstance(block, TextBlock):
                print(block.text)

# With options
options = ClaudeAgentOptions(
    system_prompt="You are a helpful assistant",
    max_turns=1
)

async for message in query(prompt="Tell me a joke", options=options):
    print(message)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using Tools&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;options = ClaudeAgentOptions(
    allowed_tools=["Read", "Write", "Bash"],
    permission_mode='acceptEdits'  # auto-accept file edits
)

async for message in query(
    prompt="Create a hello.py file",
    options=options
):
    # Process tool use and results
    pass
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Working Directory&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from pathlib import Path

options = ClaudeAgentOptions(
    cwd="/path/to/project"  # or Path("/path/to/project")
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ClaudeSDKClient&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;ClaudeSDKClient&lt;/code&gt; supports bidirectional, interactive conversations with Claude Code. See &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/src/claude_agent_sdk/client.py"&gt;src/claude_agent_sdk/client.py&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Unlike &lt;code&gt;query()&lt;/code&gt;, &lt;code&gt;ClaudeSDKClient&lt;/code&gt; additionally enables &lt;strong&gt;custom tools&lt;/strong&gt; and &lt;strong&gt;hooks&lt;/strong&gt;, both of which can be defined as Python functions.&lt;/p&gt; 
&lt;h3&gt;Custom Tools (as In-Process SDK MCP Servers)&lt;/h3&gt; 
&lt;p&gt;A &lt;strong&gt;custom tool&lt;/strong&gt; is a Python function that you can offer to Claude, for Claude to invoke as needed.&lt;/p&gt; 
&lt;p&gt;Custom tools are implemented in-process MCP servers that run directly within your Python application, eliminating the need for separate processes that regular MCP servers require.&lt;/p&gt; 
&lt;p&gt;For an end-to-end example, see &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/examples/mcp_calculator.py"&gt;MCP Calculator&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Creating a Simple Tool&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from claude_agent_sdk import tool, create_sdk_mcp_server, ClaudeAgentOptions, ClaudeSDKClient

# Define a tool using the @tool decorator
@tool("greet", "Greet a user", {"name": str})
async def greet_user(args):
    return {
        "content": [
            {"type": "text", "text": f"Hello, {args['name']}!"}
        ]
    }

# Create an SDK MCP server
server = create_sdk_mcp_server(
    name="my-tools",
    version="1.0.0",
    tools=[greet_user]
)

# Use it with Claude
options = ClaudeAgentOptions(
    mcp_servers={"tools": server},
    allowed_tools=["mcp__tools__greet"]
)

async with ClaudeSDKClient(options=options) as client:
    await client.query("Greet Alice")

    # Extract and print response
    async for msg in client.receive_response():
        print(msg)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Benefits Over External MCP Servers&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;No subprocess management&lt;/strong&gt; - Runs in the same process as your application&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Better performance&lt;/strong&gt; - No IPC overhead for tool calls&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Simpler deployment&lt;/strong&gt; - Single Python process instead of multiple&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easier debugging&lt;/strong&gt; - All code runs in the same process&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Type safety&lt;/strong&gt; - Direct Python function calls with type hints&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Migration from External Servers&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# BEFORE: External MCP server (separate process)
options = ClaudeAgentOptions(
    mcp_servers={
        "calculator": {
            "type": "stdio",
            "command": "python",
            "args": ["-m", "calculator_server"]
        }
    }
)

# AFTER: SDK MCP server (in-process)
from my_tools import add, subtract  # Your tool functions

calculator = create_sdk_mcp_server(
    name="calculator",
    tools=[add, subtract]
)

options = ClaudeAgentOptions(
    mcp_servers={"calculator": calculator}
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Mixed Server Support&lt;/h4&gt; 
&lt;p&gt;You can use both SDK and external MCP servers together:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;options = ClaudeAgentOptions(
    mcp_servers={
        "internal": sdk_server,      # In-process SDK server
        "external": {                # External subprocess server
            "type": "stdio",
            "command": "external-server"
        }
    }
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Hooks&lt;/h3&gt; 
&lt;p&gt;A &lt;strong&gt;hook&lt;/strong&gt; is a Python function that the Claude Code &lt;em&gt;application&lt;/em&gt; (&lt;em&gt;not&lt;/em&gt; Claude) invokes at specific points of the Claude agent loop. Hooks can provide deterministic processing and automated feedback for Claude. Read more in &lt;a href="https://docs.anthropic.com/en/docs/claude-code/hooks"&gt;Claude Code Hooks Reference&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For more examples, see examples/hooks.py.&lt;/p&gt; 
&lt;h4&gt;Example&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from claude_agent_sdk import ClaudeAgentOptions, ClaudeSDKClient, HookMatcher

async def check_bash_command(input_data, tool_use_id, context):
    tool_name = input_data["tool_name"]
    tool_input = input_data["tool_input"]
    if tool_name != "Bash":
        return {}
    command = tool_input.get("command", "")
    block_patterns = ["foo.sh"]
    for pattern in block_patterns:
        if pattern in command:
            return {
                "hookSpecificOutput": {
                    "hookEventName": "PreToolUse",
                    "permissionDecision": "deny",
                    "permissionDecisionReason": f"Command contains invalid pattern: {pattern}",
                }
            }
    return {}

options = ClaudeAgentOptions(
    allowed_tools=["Bash"],
    hooks={
        "PreToolUse": [
            HookMatcher(matcher="Bash", hooks=[check_bash_command]),
        ],
    }
)

async with ClaudeSDKClient(options=options) as client:
    # Test 1: Command with forbidden pattern (will be blocked)
    await client.query("Run the bash command: ./foo.sh --help")
    async for msg in client.receive_response():
        print(msg)

    print("\n" + "=" * 50 + "\n")

    # Test 2: Safe command that should work
    await client.query("Run the bash command: echo 'Hello from hooks example!'")
    async for msg in client.receive_response():
        print(msg)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Types&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/src/claude_agent_sdk/types.py"&gt;src/claude_agent_sdk/types.py&lt;/a&gt; for complete type definitions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;ClaudeAgentOptions&lt;/code&gt; - Configuration options&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;AssistantMessage&lt;/code&gt;, &lt;code&gt;UserMessage&lt;/code&gt;, &lt;code&gt;SystemMessage&lt;/code&gt;, &lt;code&gt;ResultMessage&lt;/code&gt; - Message types&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;TextBlock&lt;/code&gt;, &lt;code&gt;ToolUseBlock&lt;/code&gt;, &lt;code&gt;ToolResultBlock&lt;/code&gt; - Content blocks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Error Handling&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from claude_agent_sdk import (
    ClaudeSDKError,      # Base error
    CLINotFoundError,    # Claude Code not installed
    CLIConnectionError,  # Connection issues
    ProcessError,        # Process failed
    CLIJSONDecodeError,  # JSON parsing issues
)

try:
    async for message in query(prompt="Hello"):
        pass
except CLINotFoundError:
    print("Please install Claude Code")
except ProcessError as e:
    print(f"Process failed with exit code: {e.exit_code}")
except CLIJSONDecodeError as e:
    print(f"Failed to parse response: {e}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/src/claude_agent_sdk/_errors.py"&gt;src/claude_agent_sdk/_errors.py&lt;/a&gt; for all error types.&lt;/p&gt; 
&lt;h2&gt;Available Tools&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://docs.anthropic.com/en/docs/claude-code/settings#tools-available-to-claude"&gt;Claude Code documentation&lt;/a&gt; for a complete list of available tools.&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/examples/quick_start.py"&gt;examples/quick_start.py&lt;/a&gt; for a complete working example.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/examples/streaming_mode.py"&gt;examples/streaming_mode.py&lt;/a&gt; for comprehensive examples involving &lt;code&gt;ClaudeSDKClient&lt;/code&gt;. You can even run interactive examples in IPython from &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/examples/streaming_mode_ipython.py"&gt;examples/streaming_mode_ipython.py&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Migrating from Claude Code SDK&lt;/h2&gt; 
&lt;p&gt;If you're upgrading from the Claude Code SDK (versions &amp;lt; 0.1.0), please see the &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/CHANGELOG.md#010"&gt;CHANGELOG.md&lt;/a&gt; for details on breaking changes and new features, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;ClaudeCodeOptions&lt;/code&gt; ‚Üí &lt;code&gt;ClaudeAgentOptions&lt;/code&gt; rename&lt;/li&gt; 
 &lt;li&gt;Merged system prompt configuration&lt;/li&gt; 
 &lt;li&gt;Settings isolation and explicit control&lt;/li&gt; 
 &lt;li&gt;New programmatic subagents and session forking features&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;If you're contributing to this project, run the initial setup script to install git hooks:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./scripts/initial-setup.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This installs a pre-push hook that runs lint checks before pushing, matching the CI workflow. To skip the hook temporarily, use &lt;code&gt;git push --no-verify&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Building Wheels Locally&lt;/h3&gt; 
&lt;p&gt;To build wheels with the bundled Claude Code CLI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install build dependencies
pip install build twine

# Build wheel with bundled CLI
python scripts/build_wheel.py

# Build with specific version
python scripts/build_wheel.py --version 0.1.4

# Build with specific CLI version
python scripts/build_wheel.py --cli-version 2.0.0

# Clean bundled CLI after building
python scripts/build_wheel.py --clean

# Skip CLI download (use existing)
python scripts/build_wheel.py --skip-download
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The build script:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Downloads Claude Code CLI for your platform&lt;/li&gt; 
 &lt;li&gt;Bundles it in the wheel&lt;/li&gt; 
 &lt;li&gt;Builds both wheel and source distribution&lt;/li&gt; 
 &lt;li&gt;Checks the package with twine&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;See &lt;code&gt;python scripts/build_wheel.py --help&lt;/code&gt; for all options.&lt;/p&gt; 
&lt;h3&gt;Release Workflow&lt;/h3&gt; 
&lt;p&gt;The package is published to PyPI via the GitHub Actions workflow in &lt;code&gt;.github/workflows/publish.yml&lt;/code&gt;. To create a new release:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Trigger the workflow&lt;/strong&gt; manually from the Actions tab with two inputs:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;version&lt;/code&gt;: The package version to publish (e.g., &lt;code&gt;0.1.5&lt;/code&gt;)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;claude_code_version&lt;/code&gt;: The Claude Code CLI version to bundle (e.g., &lt;code&gt;2.0.0&lt;/code&gt; or &lt;code&gt;latest&lt;/code&gt;)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;The workflow will&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Build platform-specific wheels for macOS, Linux, and Windows&lt;/li&gt; 
   &lt;li&gt;Bundle the specified Claude Code CLI version in each wheel&lt;/li&gt; 
   &lt;li&gt;Build a source distribution&lt;/li&gt; 
   &lt;li&gt;Publish all artifacts to PyPI&lt;/li&gt; 
   &lt;li&gt;Create a release branch with version updates&lt;/li&gt; 
   &lt;li&gt;Open a PR to main with: 
    &lt;ul&gt; 
     &lt;li&gt;Updated &lt;code&gt;pyproject.toml&lt;/code&gt; version&lt;/li&gt; 
     &lt;li&gt;Updated &lt;code&gt;src/claude_agent_sdk/_version.py&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;Updated &lt;code&gt;src/claude_agent_sdk/_cli_version.py&lt;/code&gt; with bundled CLI version&lt;/li&gt; 
     &lt;li&gt;Auto-generated &lt;code&gt;CHANGELOG.md&lt;/code&gt; entry&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Review and merge&lt;/strong&gt; the release PR to update main with the new version information&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The workflow tracks both the package version and the bundled CLI version separately, allowing you to release a new package version with an updated CLI without code changes.&lt;/p&gt; 
&lt;h2&gt;License and terms&lt;/h2&gt; 
&lt;p&gt;Use of this SDK is governed by Anthropic's &lt;a href="https://www.anthropic.com/legal/commercial-terms"&gt;Commercial Terms of Service&lt;/a&gt;, including when you use it to power products and services that you make available to your own customers and end users, except to the extent a specific component or dependency is covered by a different license as indicated in that component's LICENSE file.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>NevaMind-AI/memU</title>
      <link>https://github.com/NevaMind-AI/memU</link>
      <description>&lt;p&gt;Memory infrastructure for LLMs and AI agents&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/banner.png" alt="MemU Banner" /&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h1&gt;MemU&lt;/h1&gt; 
 &lt;h3&gt;A Future-Oriented Agentic Memory System&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://badge.fury.io/py/memu-py"&gt;&lt;img src="https://badge.fury.io/py/memu-py.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true" alt="License: Apache 2.0" /&gt;&lt;/a&gt; &lt;a href="https://www.python.org/downloads/"&gt;&lt;img src="https://img.shields.io/badge/python-3.13+-blue.svg?sanitize=true" alt="Python 3.13+" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/memu"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20Chat-5865F2?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://x.com/memU_ai"&gt;&lt;img src="https://img.shields.io/badge/Twitter-Follow-1DA1F2?logo=x&amp;amp;logoColor=white" alt="Twitter" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/17374" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/17374" alt="NevaMind-AI%2FmemU | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;MemU is an agentic memory framework for LLM and AI agent backends. It receives &lt;strong&gt;multimodal inputs&lt;/strong&gt; (conversations, documents, images), extracts them into structured memory, and organizes them into a &lt;strong&gt;hierarchical file system&lt;/strong&gt; that supports both &lt;strong&gt;embedding-based (RAG)&lt;/strong&gt; and &lt;strong&gt;non-embedding (LLM)&lt;/strong&gt; retrieval.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚≠êÔ∏è Star the repository&lt;/h2&gt; 
&lt;img width="100%" src="https://github.com/NevaMind-AI/memU/raw/main/assets/star.gif" /&gt; If you find memU useful or interesting, a GitHub Star ‚≠êÔ∏è would be greatly appreciated. 
&lt;hr /&gt; 
&lt;p&gt;MemU is collaborating with four open-source projects to launch the 2026 New Year Challenge. üéâBetween January 8‚Äì18, contributors can submit PRs to memU and earn cash rewards, community recognition, and platform credits. üéÅ&lt;a href="https://discord.gg/KaWy6SBAsx"&gt;Learn more &amp;amp; get involved&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;‚ú® Core Features&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üóÇÔ∏è &lt;strong&gt;Hierarchical File System&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Three-layer architecture: Resource ‚Üí Item ‚Üí Category with full traceability&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üîç &lt;strong&gt;Dual Retrieval Methods&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;RAG (embedding-based) for speed, LLM (non-embedding) for deep semantic understanding&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üé® &lt;strong&gt;Multimodal Support&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Process conversations, documents, images, audio, and video&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üîÑ &lt;strong&gt;Self-Evolving Memory&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Memory structure adapts and improves based on usage patterns&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üóÇÔ∏è Hierarchical File System&lt;/h2&gt; 
&lt;p&gt;MemU organizes memory using a &lt;strong&gt;three-layer architecture&lt;/strong&gt; inspired by hierarchical storage systems:&lt;/p&gt; 
&lt;img width="100%" alt="structure" src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/structure.png" /&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Layer&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Examples&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Resource&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Raw multimodal data warehouse&lt;/td&gt; 
   &lt;td&gt;JSON conversations, text documents, images, videos&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Item&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Discrete extracted memory units&lt;/td&gt; 
   &lt;td&gt;Individual preferences, skills, opinions, habits&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Category&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Aggregated textual memory with summaries&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;preferences.md&lt;/code&gt;, &lt;code&gt;work_life.md&lt;/code&gt;, &lt;code&gt;relationships.md&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Key Benefits:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Full Traceability&lt;/strong&gt;: Track from raw data ‚Üí items ‚Üí categories and back&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Progressive Summarization&lt;/strong&gt;: Each layer provides increasingly abstracted views&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible Organization&lt;/strong&gt;: Categories evolve based on content patterns&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üé® Multimodal Support&lt;/h2&gt; 
&lt;p&gt;MemU processes diverse content types into unified memory:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Modality&lt;/th&gt; 
   &lt;th&gt;Input&lt;/th&gt; 
   &lt;th&gt;Processing&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;conversation&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;JSON chat logs&lt;/td&gt; 
   &lt;td&gt;Extract preferences, opinions, habits, relationships&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;document&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Text files (.txt, .md)&lt;/td&gt; 
   &lt;td&gt;Extract knowledge, skills, facts&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;image&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;PNG, JPG, etc.&lt;/td&gt; 
   &lt;td&gt;Vision model extracts visual concepts and descriptions&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;video&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Video files&lt;/td&gt; 
   &lt;td&gt;Frame extraction + vision analysis&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;audio&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Audio files&lt;/td&gt; 
   &lt;td&gt;Transcription + text processing&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;All modalities are unified into the same three-layer hierarchy, enabling cross-modal retrieval.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;h3&gt;Option 1: Cloud Version&lt;/h3&gt; 
&lt;p&gt;Try MemU instantly without any setup:&lt;/p&gt; 
&lt;p&gt;üëâ &lt;strong&gt;&lt;a href="https://memu.so"&gt;memu.so&lt;/a&gt;&lt;/strong&gt; - Hosted cloud service with full API access&lt;/p&gt; 
&lt;p&gt;For enterprise deployment and custom solutions, contact &lt;strong&gt;&lt;a href="mailto:info@nevamind.ai"&gt;info@nevamind.ai&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h4&gt;Cloud API (v3)&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Base URL&lt;/th&gt; 
   &lt;th&gt;&lt;code&gt;https://api.memu.so&lt;/code&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Auth&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;Authorization: Bearer YOUR_API_KEY&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Endpoint&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;POST&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/api/v3/memory/memorize&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register a memorization task&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;GET&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/api/v3/memory/memorize/status/{task_id}&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Get task status&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;POST&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/api/v3/memory/categories&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;List memory categories&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;POST&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/api/v3/memory/retrieve&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Retrieve memories (semantic search)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;üìö &lt;strong&gt;&lt;a href="https://memu.pro/docs#cloud-version"&gt;Full API Documentation&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Option 2: Self-Hosted&lt;/h3&gt; 
&lt;h4&gt;Installation&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Basic Example&lt;/h4&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;: Python 3.13+ and an OpenAI API key&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Test with In-Memory Storage&lt;/strong&gt; (no database required):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=your_api_key
cd tests
python test_inmemory.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Test with PostgreSQL Storage&lt;/strong&gt; (requires pgvector):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start PostgreSQL with pgvector
docker run -d \
  --name memu-postgres \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=postgres \
  -e POSTGRES_DB=memu \
  -p 5432:5432 \
  pgvector/pgvector:pg16

# Run the test
export OPENAI_API_KEY=your_api_key
cd tests
python test_postgres.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Both examples demonstrate the complete workflow:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Memorize&lt;/strong&gt;: Process a conversation file and extract structured memory&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Retrieve (RAG)&lt;/strong&gt;: Fast embedding-based search&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Retrieve (LLM)&lt;/strong&gt;: Deep semantic understanding search&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/tests/test_inmemory.py"&gt;&lt;code&gt;tests/test_inmemory.py&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/tests/test_postgres.py"&gt;&lt;code&gt;tests/test_postgres.py&lt;/code&gt;&lt;/a&gt; for the full source code.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Custom LLM and Embedding Providers&lt;/h3&gt; 
&lt;p&gt;MemU supports custom LLM and embedding providers beyond OpenAI. Configure them via &lt;code&gt;llm_profiles&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from memu import MemUService

service = MemUService(
    llm_profiles={
        # Default profile for LLM operations
        "default": {
            "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
            "api_key": "your_api_key",
            "chat_model": "qwen3-max",
            "client_backend": "sdk"  # "sdk" or "http"
        },
        # Separate profile for embeddings
        "embedding": {
            "base_url": "https://api.voyageai.com/v1",
            "api_key": "your_voyage_api_key",
            "embed_model": "voyage-3.5-lite"
        }
    },
    # ... other configuration
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìñ Core APIs&lt;/h2&gt; 
&lt;h3&gt;&lt;code&gt;memorize()&lt;/code&gt; - Extract and Store Memory&lt;/h3&gt; 
&lt;p&gt;Processes input resources and extracts structured memory:&lt;/p&gt; 
&lt;img width="100%" alt="memorize" src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/memorize.png" /&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;result = await service.memorize(
    resource_url="path/to/file.json",  # File path or URL
    modality="conversation",            # conversation | document | image | video | audio
    user={"user_id": "123"}             # Optional: scope to a user
)

# Returns:
{
    "resource": {...},      # Stored resource metadata
    "items": [...],         # Extracted memory items
    "categories": [...]     # Updated category summaries
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;code&gt;retrieve()&lt;/code&gt; - Query Memory&lt;/h3&gt; 
&lt;p&gt;Retrieves relevant memory based on queries. MemU supports &lt;strong&gt;two retrieval strategies&lt;/strong&gt;:&lt;/p&gt; 
&lt;img width="100%" alt="retrieve" src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/retrieve.png" /&gt; 
&lt;h4&gt;RAG-based Retrieval (&lt;code&gt;method="rag"&lt;/code&gt;)&lt;/h4&gt; 
&lt;p&gt;Fast &lt;strong&gt;embedding vector search&lt;/strong&gt; using cosine similarity:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Fast&lt;/strong&gt;: Pure vector computation&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Scalable&lt;/strong&gt;: Efficient for large memory stores&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Returns scores&lt;/strong&gt;: Each result includes similarity score&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;LLM-based Retrieval (&lt;code&gt;method="llm"&lt;/code&gt;)&lt;/h4&gt; 
&lt;p&gt;Deep &lt;strong&gt;semantic understanding&lt;/strong&gt; through direct LLM reasoning:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Deep understanding&lt;/strong&gt;: LLM comprehends context and nuance&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Query rewriting&lt;/strong&gt;: Automatically refines query at each tier&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Adaptive&lt;/strong&gt;: Stops early when sufficient information is found&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Comparison&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Aspect&lt;/th&gt; 
   &lt;th&gt;RAG&lt;/th&gt; 
   &lt;th&gt;LLM&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Speed&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚ö° Fast&lt;/td&gt; 
   &lt;td&gt;üê¢ Slower&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Cost&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;üí∞ Low&lt;/td&gt; 
   &lt;td&gt;üí∞üí∞ Higher&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Semantic depth&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Medium&lt;/td&gt; 
   &lt;td&gt;Deep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Tier 2 scope&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;All items&lt;/td&gt; 
   &lt;td&gt;Only items in relevant categories&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Output&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;With similarity scores&lt;/td&gt; 
   &lt;td&gt;Ranked by LLM reasoning&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Both methods support:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Context-aware rewriting&lt;/strong&gt;: Resolves pronouns using conversation history&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Progressive search&lt;/strong&gt;: Categories ‚Üí Items ‚Üí Resources&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Sufficiency checking&lt;/strong&gt;: Stops when enough information is retrieved&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Usage&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;result = await service.retrieve(
    queries=[
        {"role": "user", "content": {"text": "What are their preferences?"}},
        {"role": "user", "content": {"text": "Tell me about work habits"}}
    ],
    where={"user_id": "123"}  # Optional: scope filter
)

# Returns:
{
    "categories": [...],     # Relevant categories (with scores for RAG)
    "items": [...],          # Relevant memory items
    "resources": [...],      # Related raw resources
    "next_step_query": "..." # Rewritten query for follow-up (if applicable)
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Scope Filtering&lt;/strong&gt;: Use &lt;code&gt;where&lt;/code&gt; to filter by user model fields:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;where={"user_id": "123"}&lt;/code&gt; - exact match&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;where={"agent_id__in": ["1", "2"]}&lt;/code&gt; - match any in list&lt;/li&gt; 
 &lt;li&gt;Omit &lt;code&gt;where&lt;/code&gt; to retrieve across all scopes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üìö &lt;strong&gt;For complete API documentation&lt;/strong&gt;, see &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/docs/SERVICE_API.md"&gt;SERVICE_API.md&lt;/a&gt; - includes all methods, CRUD operations, pipeline configuration, and configuration types.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üí° Use Cases&lt;/h2&gt; 
&lt;h3&gt;Example 1: Conversation Memory&lt;/h3&gt; 
&lt;p&gt;Extract and organize memory from multi-turn conversations:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=your_api_key
python examples/example_1_conversation_memory.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Processes multiple conversation JSON files&lt;/li&gt; 
 &lt;li&gt;Extracts memory items (preferences, habits, opinions, relationships)&lt;/li&gt; 
 &lt;li&gt;Generates category markdown files (&lt;code&gt;preferences.md&lt;/code&gt;, &lt;code&gt;work_life.md&lt;/code&gt;, etc.)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt; Personal AI assistants, customer support bots, social chatbots&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Example 2: Skill Extraction from Logs&lt;/h3&gt; 
&lt;p&gt;Extract skills and lessons learned from agent execution logs:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=your_api_key
python examples/example_2_skill_extraction.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Processes agent logs sequentially&lt;/li&gt; 
 &lt;li&gt;Extracts actions, outcomes, and lessons learned&lt;/li&gt; 
 &lt;li&gt;Demonstrates &lt;strong&gt;incremental learning&lt;/strong&gt; - memory evolves with each file&lt;/li&gt; 
 &lt;li&gt;Generates evolving skill guides (&lt;code&gt;log_1.md&lt;/code&gt; ‚Üí &lt;code&gt;log_2.md&lt;/code&gt; ‚Üí &lt;code&gt;skill.md&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt; DevOps teams, agent self-improvement, knowledge management&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Example 3: Multimodal Memory&lt;/h3&gt; 
&lt;p&gt;Process diverse content types into unified memory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=your_api_key
python examples/example_3_multimodal_memory.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Processes documents and images together&lt;/li&gt; 
 &lt;li&gt;Extracts memory from different content types&lt;/li&gt; 
 &lt;li&gt;Unifies into cross-modal categories (&lt;code&gt;technical_documentation&lt;/code&gt;, &lt;code&gt;visual_diagrams&lt;/code&gt;, etc.)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt; Documentation systems, learning platforms, research tools&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìä Performance&lt;/h2&gt; 
&lt;p&gt;MemU achieves &lt;strong&gt;92.09% average accuracy&lt;/strong&gt; on the Locomo benchmark across all reasoning tasks.&lt;/p&gt; 
&lt;img width="100%" alt="benchmark" src="https://github.com/user-attachments/assets/6fec4884-94e5-4058-ad5c-baac3d7e76d9" /&gt; 
&lt;p&gt;View detailed experimental data: &lt;a href="https://github.com/NevaMind-AI/memU-experiment"&gt;memU-experiment&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üß© Ecosystem&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Repository&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href="https://github.com/NevaMind-AI/memU"&gt;memU&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Core algorithm engine&lt;/td&gt; 
   &lt;td&gt;Embed AI memory into your product&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href="https://github.com/NevaMind-AI/memU-server"&gt;memU-server&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Backend service with CRUD, user system, RBAC&lt;/td&gt; 
   &lt;td&gt;Self-host a memory backend&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href="https://github.com/NevaMind-AI/memU-ui"&gt;memU-ui&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Visual dashboard&lt;/td&gt; 
   &lt;td&gt;Ready-to-use memory console&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Quick Links:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üöÄ &lt;a href="https://app.memu.so/quick-start"&gt;Try MemU Cloud&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìö &lt;a href="https://memu.pro/docs"&gt;API Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üí¨ &lt;a href="https://discord.gg/memu"&gt;Discord Community&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§ù Partners&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/TEN-framework/ten-framework"&gt;&lt;img src="https://avatars.githubusercontent.com/u/113095513?s=200&amp;amp;v=4" alt="Ten" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://openagents.org"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/openagents.png" alt="OpenAgents" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/milvus-io/milvus"&gt;&lt;img src="https://miro.medium.com/v2/resize:fit:2400/1*-VEGyAgcIBD62XtZWavy8w.png" alt="Milvus" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://xroute.ai/"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/xroute.png" alt="xRoute" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://jaaz.app/"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/jazz.png" alt="Jazz" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Buddie-AI/Buddie"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/buddie.png" alt="Buddie" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/bytebase/bytebase"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/bytebase.png" alt="Bytebase" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/LazyAGI/LazyLLM"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/LazyLLM.png" alt="LazyLLM" height="40" style="margin: 10px;" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§ù How to Contribute&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Whether you're fixing bugs, adding features, or improving documentation, your help is appreciated.&lt;/p&gt; 
&lt;h3&gt;Getting Started&lt;/h3&gt; 
&lt;p&gt;To start contributing to MemU, you'll need to set up your development environment:&lt;/p&gt; 
&lt;h4&gt;Prerequisites&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.13+&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/astral-sh/uv"&gt;uv&lt;/a&gt; (Python package manager)&lt;/li&gt; 
 &lt;li&gt;Git&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Setup Development Environment&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Fork and clone the repository
git clone https://github.com/YOUR_USERNAME/memU.git
cd memU

# 2. Install development dependencies
make install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;make install&lt;/code&gt; command will:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create a virtual environment using &lt;code&gt;uv&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install all project dependencies&lt;/li&gt; 
 &lt;li&gt;Set up pre-commit hooks for code quality checks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Running Quality Checks&lt;/h4&gt; 
&lt;p&gt;Before submitting your contribution, ensure your code passes all quality checks:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make check
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;make check&lt;/code&gt; command runs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Lock file verification&lt;/strong&gt;: Ensures &lt;code&gt;pyproject.toml&lt;/code&gt; consistency&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Pre-commit hooks&lt;/strong&gt;: Lints code with Ruff, formats with Black&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Type checking&lt;/strong&gt;: Runs &lt;code&gt;mypy&lt;/code&gt; for static type analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dependency analysis&lt;/strong&gt;: Uses &lt;code&gt;deptry&lt;/code&gt; to find obsolete dependencies&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing Guidelines&lt;/h3&gt; 
&lt;p&gt;For detailed contribution guidelines, code standards, and development practices, please see &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Quick tips:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create a new branch for each feature or bug fix&lt;/li&gt; 
 &lt;li&gt;Write clear commit messages&lt;/li&gt; 
 &lt;li&gt;Add tests for new functionality&lt;/li&gt; 
 &lt;li&gt;Update documentation as needed&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;make check&lt;/code&gt; before pushing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/LICENSE.txt"&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üåç Community&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Issues&lt;/strong&gt;: &lt;a href="https://github.com/NevaMind-AI/memU/issues"&gt;Report bugs &amp;amp; request features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Discord&lt;/strong&gt;: &lt;a href="https://discord.com/invite/hQZntfGsbJ"&gt;Join the community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;X (Twitter)&lt;/strong&gt;: &lt;a href="https://x.com/memU_ai"&gt;Follow @memU_ai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Contact&lt;/strong&gt;: &lt;a href="mailto:info@nevamind.ai"&gt;info@nevamind.ai&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;‚≠ê &lt;strong&gt;Star us on GitHub&lt;/strong&gt; to get notified about new releases!&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>EveryInc/compound-engineering-plugin</title>
      <link>https://github.com/EveryInc/compound-engineering-plugin</link>
      <description>&lt;p&gt;Official Claude Code compound engineering plugin&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Compound Engineering Plugin&lt;/h1&gt; 
&lt;p&gt;A Claude Code plugin that makes each unit of engineering work easier than the last.&lt;/p&gt; 
&lt;h2&gt;Install&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;/plugin marketplace add https://github.com/EveryInc/compound-engineering-plugin
/plugin install compound-engineering
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Workflow&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;Plan ‚Üí Work ‚Üí Review ‚Üí Compound ‚Üí Repeat
&lt;/code&gt;&lt;/pre&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Command&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;/workflows:plan&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Turn feature ideas into detailed implementation plans&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;/workflows:work&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Execute plans with worktrees and task tracking&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;/workflows:review&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Multi-agent code review before merging&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;/workflows:compound&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Document learnings to make future work easier&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Each cycle compounds: plans inform future plans, reviews catch more issues, patterns get documented.&lt;/p&gt; 
&lt;h2&gt;Philosophy&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Each unit of engineering work should make subsequent units easier‚Äînot harder.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Traditional development accumulates technical debt. Every feature adds complexity. The codebase becomes harder to work with over time.&lt;/p&gt; 
&lt;p&gt;Compound engineering inverts this. 80% is in planning and review, 20% is in execution:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Plan thoroughly before writing code&lt;/li&gt; 
 &lt;li&gt;Review to catch issues and capture learnings&lt;/li&gt; 
 &lt;li&gt;Codify knowledge so it's reusable&lt;/li&gt; 
 &lt;li&gt;Keep quality high so future changes are easy&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Learn More&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EveryInc/compound-engineering-plugin/main/plugins/compound-engineering/README.md"&gt;Full component reference&lt;/a&gt; - all agents, commands, skills&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://every.to/chain-of-thought/compound-engineering-how-every-codes-with-agents"&gt;Compound engineering: how Every codes with agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://every.to/source-code/my-ai-had-already-fixed-the-code-before-i-saw-it"&gt;The story behind compounding engineering&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>getzep/graphiti</title>
      <link>https://github.com/getzep/graphiti</link>
      <description>&lt;p&gt;Build Real-Time Knowledge Graphs for AI Agents&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://www.getzep.com/"&gt; &lt;img src="https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73" width="150" alt="Zep Logo" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h1 align="center"&gt; Graphiti &lt;/h1&gt; 
&lt;h2 align="center"&gt; Build Real-Time Knowledge Graphs for AI Agents&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/getzep/Graphiti/actions/workflows/lint.yml"&gt;&lt;img src="https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat" alt="Lint" /&gt;&lt;/a&gt; &lt;a href="https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml"&gt;&lt;img src="https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg?sanitize=true" alt="Unit Tests" /&gt;&lt;/a&gt; &lt;a href="https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml"&gt;&lt;img src="https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg?sanitize=true" alt="MyPy Check" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://img.shields.io/github/stars/getzep/graphiti" alt="GitHub Repo stars" /&gt; &lt;a href="https://discord.com/invite/W8Kw6bsgXQ"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2501.13956"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://github.com/getzep/graphiti/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/getzep/graphiti?style=flat&amp;amp;label=Release&amp;amp;color=limegreen" alt="Release" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/12986" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/12986" alt="getzep%2Fgraphiti | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span&gt;‚≠ê&lt;/span&gt; &lt;em&gt;Help us reach more developers and grow the Graphiti community. Star this repo!&lt;/em&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Check out the new &lt;a href="https://raw.githubusercontent.com/getzep/graphiti/main/mcp_server/README.md"&gt;MCP server for Graphiti&lt;/a&gt;! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Graphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.&lt;/p&gt; 
&lt;p&gt;Use Graphiti to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Integrate and maintain dynamic user interactions and business data.&lt;/li&gt; 
 &lt;li&gt;Facilitate state-based reasoning and task automation for agents.&lt;/li&gt; 
 &lt;li&gt;Query complex, evolving data with semantic, keyword, and graph-based search methods.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/getzep/graphiti/main/images/graphiti-graph-intro.gif" alt="Graphiti temporal walkthrough" width="700px" /&gt; &lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;A knowledge graph is a network of interconnected facts, such as &lt;em&gt;"Kendra loves Adidas shoes."&lt;/em&gt; Each fact is a "triplet" represented by two entities, or nodes ("Kendra", "Adidas shoes"), and their relationship, or edge ("loves"). Knowledge Graphs have been explored extensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph while handling changing relationships and maintaining historical context.&lt;/p&gt; 
&lt;h2&gt;Graphiti and Zep's Context Engineering Platform.&lt;/h2&gt; 
&lt;p&gt;Graphiti powers the core of &lt;a href="https://www.getzep.com"&gt;Zep's context engineering platform&lt;/a&gt; for AI Agents. Zep offers agent memory, Graph RAG for dynamic data, and context retrieval and assembly.&lt;/p&gt; 
&lt;p&gt;Using Graphiti, we've demonstrated Zep is the &lt;a href="https://blog.getzep.com/state-of-the-art-agent-memory/"&gt;State of the Art in Agent Memory&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Read our paper: &lt;a href="https://arxiv.org/abs/2501.13956"&gt;Zep: A Temporal Knowledge Graph Architecture for Agent Memory&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We're excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://arxiv.org/abs/2501.13956"&gt;&lt;img src="https://raw.githubusercontent.com/getzep/graphiti/main/images/arxiv-screenshot.png" alt="Zep: A Temporal Knowledge Graph Architecture for Agent Memory" width="700px" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;Zep vs Graphiti&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Aspect&lt;/th&gt; 
   &lt;th&gt;Zep&lt;/th&gt; 
   &lt;th&gt;Graphiti&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;What they are&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Fully managed platform for context engineering and AI memory&lt;/td&gt; 
   &lt;td&gt;Open-source graph framework&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;User &amp;amp; conversation management&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Built-in users, threads, and message storage&lt;/td&gt; 
   &lt;td&gt;Build your own&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Retrieval &amp;amp; performance&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Pre-configured, production-ready retrieval with sub-200ms performance at scale&lt;/td&gt; 
   &lt;td&gt;Custom implementation required; performance depends on your setup&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Developer tools&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Dashboard with graph visualization, debug logs, API logs; SDKs for Python, TypeScript, and Go&lt;/td&gt; 
   &lt;td&gt;Build your own tools&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Enterprise features&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;SLAs, support, security guarantees&lt;/td&gt; 
   &lt;td&gt;Self-managed&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Deployment&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Fully managed or in your cloud&lt;/td&gt; 
   &lt;td&gt;Self-hosted only&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;When to choose which&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Choose Zep&lt;/strong&gt; if you want a turnkey, enterprise-grade platform with security, performance, and support baked in.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Choose Graphiti&lt;/strong&gt; if you want a flexible OSS core and you're comfortable building/operating the surrounding system.&lt;/p&gt; 
&lt;h2&gt;Why Graphiti?&lt;/h2&gt; 
&lt;p&gt;Traditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Real-Time Incremental Updates:&lt;/strong&gt; Immediate integration of new data episodes without batch recomputation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bi-Temporal Data Model:&lt;/strong&gt; Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Efficient Hybrid Retrieval:&lt;/strong&gt; Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Custom Entity Definitions:&lt;/strong&gt; Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scalability:&lt;/strong&gt; Efficiently manages large datasets with parallel processing, suitable for enterprise environments.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/getzep/graphiti/main/images/graphiti-intro-slides-stock-2.gif" alt="Graphiti structured + unstructured demo" width="700px" /&gt; &lt;/p&gt; 
&lt;h2&gt;Graphiti vs. GraphRAG&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Aspect&lt;/th&gt; 
   &lt;th&gt;GraphRAG&lt;/th&gt; 
   &lt;th&gt;Graphiti&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Primary Use&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Static document summarization&lt;/td&gt; 
   &lt;td&gt;Dynamic data management&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Data Handling&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Batch-oriented processing&lt;/td&gt; 
   &lt;td&gt;Continuous, incremental updates&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Knowledge Structure&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Entity clusters &amp;amp; community summaries&lt;/td&gt; 
   &lt;td&gt;Episodic data, semantic entities, communities&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Retrieval Method&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Sequential LLM summarization&lt;/td&gt; 
   &lt;td&gt;Hybrid semantic, keyword, and graph-based search&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Adaptability&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Low&lt;/td&gt; 
   &lt;td&gt;High&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Temporal Handling&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Basic timestamp tracking&lt;/td&gt; 
   &lt;td&gt;Explicit bi-temporal tracking&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Contradiction Handling&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;LLM-driven summarization judgments&lt;/td&gt; 
   &lt;td&gt;Temporal edge invalidation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Query Latency&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Seconds to tens of seconds&lt;/td&gt; 
   &lt;td&gt;Typically sub-second latency&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Custom Entity Types&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;Yes, customizable&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Scalability&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Moderate&lt;/td&gt; 
   &lt;td&gt;High, optimized for large datasets&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Graphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10 or higher&lt;/li&gt; 
 &lt;li&gt;Neo4j 5.26 / FalkorDB 1.1.2 / Kuzu 0.11.2 / Amazon Neptune Database Cluster or Neptune Analytics Graph + Amazon OpenSearch Serverless collection (serves as the full text search backend)&lt;/li&gt; 
 &lt;li&gt;OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini). Using other services may result in incorrect output schemas and ingestion failures. This is particularly problematic when using smaller models.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Optional:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Google Gemini, Anthropic, or Groq API key (for alternative LLM providers)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] The simplest way to install Neo4j is via &lt;a href="https://neo4j.com/download/"&gt;Neo4j Desktop&lt;/a&gt;. It provides a user-friendly interface to manage Neo4j instances and databases. Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example:&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest

&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install graphiti-core
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add graphiti-core
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Installing with FalkorDB Support&lt;/h3&gt; 
&lt;p&gt;If you plan to use FalkorDB as your graph database backend, install with the FalkorDB extra:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install graphiti-core[falkordb]

# or with uv
uv add graphiti-core[falkordb]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Installing with Kuzu Support&lt;/h3&gt; 
&lt;p&gt;If you plan to use Kuzu as your graph database backend, install with the Kuzu extra:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install graphiti-core[kuzu]

# or with uv
uv add graphiti-core[kuzu]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Installing with Amazon Neptune Support&lt;/h3&gt; 
&lt;p&gt;If you plan to use Amazon Neptune as your graph database backend, install with the Amazon Neptune extra:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install graphiti-core[neptune]

# or with uv
uv add graphiti-core[neptune]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;You can also install optional LLM providers as extras:&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install with Anthropic support
pip install graphiti-core[anthropic]

# Install with Groq support
pip install graphiti-core[groq]

# Install with Google Gemini support
pip install graphiti-core[google-genai]

# Install with multiple providers
pip install graphiti-core[anthropic,groq,google-genai]

# Install with FalkorDB and LLM providers
pip install graphiti-core[falkordb,anthropic,google-genai]

# Install with Amazon Neptune
pip install graphiti-core[neptune]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Default to Low Concurrency; LLM Provider 429 Rate Limit Errors&lt;/h2&gt; 
&lt;p&gt;Graphiti's ingestion pipelines are designed for high concurrency. By default, concurrency is set low to avoid LLM Provider 429 Rate Limit Errors. If you find Graphiti slow, please increase concurrency as described below.&lt;/p&gt; 
&lt;p&gt;Concurrency controlled by the &lt;code&gt;SEMAPHORE_LIMIT&lt;/code&gt; environment variable. By default, &lt;code&gt;SEMAPHORE_LIMIT&lt;/code&gt; is set to &lt;code&gt;10&lt;/code&gt; concurrent operations to help prevent &lt;code&gt;429&lt;/code&gt; rate limit errors from your LLM provider. If you encounter such errors, try lowering this value.&lt;/p&gt; 
&lt;p&gt;If your LLM provider allows higher throughput, you can increase &lt;code&gt;SEMAPHORE_LIMIT&lt;/code&gt; to boost episode ingestion performance.&lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; is set in your environment. Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI compatible APIs.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;For a complete working example, see the &lt;a href="https://raw.githubusercontent.com/getzep/graphiti/main/examples/quickstart/README.md"&gt;Quickstart Example&lt;/a&gt; in the examples directory. The quickstart demonstrates:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Connecting to a Neo4j, Amazon Neptune, FalkorDB, or Kuzu database&lt;/li&gt; 
 &lt;li&gt;Initializing Graphiti indices and constraints&lt;/li&gt; 
 &lt;li&gt;Adding episodes to the graph (both text and structured JSON)&lt;/li&gt; 
 &lt;li&gt;Searching for relationships (edges) using hybrid search&lt;/li&gt; 
 &lt;li&gt;Reranking search results using graph distance&lt;/li&gt; 
 &lt;li&gt;Searching for nodes using predefined search recipes&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The example is fully documented with clear explanations of each functionality and includes a comprehensive README with setup instructions and next steps.&lt;/p&gt; 
&lt;h3&gt;Running with Docker Compose&lt;/h3&gt; 
&lt;p&gt;You can use Docker Compose to quickly start the required services:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Neo4j Docker:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;docker compose up
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will start the Neo4j Docker service and related components.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;FalkorDB Docker:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;docker compose --profile falkordb up
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will start the FalkorDB Docker service and related components.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;MCP Server&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;mcp_server&lt;/code&gt; directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server allows AI assistants to interact with Graphiti's knowledge graph capabilities through the MCP protocol.&lt;/p&gt; 
&lt;p&gt;Key features of the MCP server include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Episode management (add, retrieve, delete)&lt;/li&gt; 
 &lt;li&gt;Entity management and relationship handling&lt;/li&gt; 
 &lt;li&gt;Semantic and hybrid search capabilities&lt;/li&gt; 
 &lt;li&gt;Group management for organizing related data&lt;/li&gt; 
 &lt;li&gt;Graph maintenance operations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant workflows.&lt;/p&gt; 
&lt;p&gt;For detailed setup instructions and usage examples, see the &lt;a href="https://raw.githubusercontent.com/getzep/graphiti/main/mcp_server/README.md"&gt;MCP server README&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;REST Service&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;server&lt;/code&gt; directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.&lt;/p&gt; 
&lt;p&gt;Please see the &lt;a href="https://raw.githubusercontent.com/getzep/graphiti/main/server/README.md"&gt;server README&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Optional Environment Variables&lt;/h2&gt; 
&lt;p&gt;In addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables. If you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables must be set.&lt;/p&gt; 
&lt;h3&gt;Database Configuration&lt;/h3&gt; 
&lt;p&gt;Database names are configured directly in the driver constructors:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Neo4j&lt;/strong&gt;: Database name defaults to &lt;code&gt;neo4j&lt;/code&gt; (hardcoded in Neo4jDriver)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;FalkorDB&lt;/strong&gt;: Database name defaults to &lt;code&gt;default_db&lt;/code&gt; (hardcoded in FalkorDriver)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;As of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the &lt;code&gt;graph_driver&lt;/code&gt; parameter.&lt;/p&gt; 
&lt;h4&gt;Neo4j with Custom Database Name&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from graphiti_core import Graphiti
from graphiti_core.driver.neo4j_driver import Neo4jDriver

# Create a Neo4j driver with custom database name
driver = Neo4jDriver(
    uri="bolt://localhost:7687",
    user="neo4j",
    password="password",
    database="my_custom_database"  # Custom database name
)

# Pass the driver to Graphiti
graphiti = Graphiti(graph_driver=driver)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;FalkorDB with Custom Database Name&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from graphiti_core import Graphiti
from graphiti_core.driver.falkordb_driver import FalkorDriver

# Create a FalkorDB driver with custom database name
driver = FalkorDriver(
    host="localhost",
    port=6379,
    username="falkor_user",  # Optional
    password="falkor_password",  # Optional
    database="my_custom_graph"  # Custom database name
)

# Pass the driver to Graphiti
graphiti = Graphiti(graph_driver=driver)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Kuzu&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from graphiti_core import Graphiti
from graphiti_core.driver.kuzu_driver import KuzuDriver

# Create a Kuzu driver
driver = KuzuDriver(db="/tmp/graphiti.kuzu")

# Pass the driver to Graphiti
graphiti = Graphiti(graph_driver=driver)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Amazon Neptune&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from graphiti_core import Graphiti
from graphiti_core.driver.neptune_driver import NeptuneDriver

# Create a FalkorDB driver with custom database name
driver = NeptuneDriver(
    host= &amp;lt; NEPTUNE
ENDPOINT &amp;gt;,
aoss_host = &amp;lt; Amazon
OpenSearch
Serverless
Host &amp;gt;,
port = &amp;lt; PORT &amp;gt;  # Optional, defaults to 8182,
         aoss_port = &amp;lt; PORT &amp;gt;  # Optional, defaults to 443
)

driver = NeptuneDriver(host=neptune_uri, aoss_host=aoss_host, port=neptune_port)

# Pass the driver to Graphiti
graphiti = Graphiti(graph_driver=driver)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Using Graphiti with Azure OpenAI&lt;/h2&gt; 
&lt;p&gt;Graphiti supports Azure OpenAI for both LLM inference and embeddings using Azure's OpenAI v1 API compatibility layer.&lt;/p&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import AsyncOpenAI
from graphiti_core import Graphiti
from graphiti_core.llm_client.azure_openai_client import AzureOpenAILLMClient
from graphiti_core.llm_client.config import LLMConfig
from graphiti_core.embedder.azure_openai import AzureOpenAIEmbedderClient

# Initialize Azure OpenAI client using the standard OpenAI client
# with Azure's v1 API endpoint
azure_client = AsyncOpenAI(
    base_url="https://your-resource-name.openai.azure.com/openai/v1/",
    api_key="your-api-key",
)

# Create LLM and Embedder clients
llm_client = AzureOpenAILLMClient(
    azure_client=azure_client,
    config=LLMConfig(model="gpt-5-mini", small_model="gpt-5-mini")  # Your Azure deployment name
)
embedder_client = AzureOpenAIEmbedderClient(
    azure_client=azure_client,
    model="text-embedding-3-small"  # Your Azure embedding deployment name
)

# Initialize Graphiti with Azure OpenAI clients
graphiti = Graphiti(
    "bolt://localhost:7687",
    "neo4j",
    "password",
    llm_client=llm_client,
    embedder=embedder_client,
)

# Now you can use Graphiti with Azure OpenAI
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Key Points:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use the standard &lt;code&gt;AsyncOpenAI&lt;/code&gt; client with Azure's v1 API endpoint format: &lt;code&gt;https://your-resource-name.openai.azure.com/openai/v1/&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;The deployment names (e.g., &lt;code&gt;gpt-5-mini&lt;/code&gt;, &lt;code&gt;text-embedding-3-small&lt;/code&gt;) should match your Azure OpenAI deployment names&lt;/li&gt; 
 &lt;li&gt;See &lt;code&gt;examples/azure-openai/&lt;/code&gt; for a complete working example&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Make sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names.&lt;/p&gt; 
&lt;h2&gt;Using Graphiti with Google Gemini&lt;/h2&gt; 
&lt;p&gt;Graphiti supports Google's Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini, you'll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.&lt;/p&gt; 
&lt;p&gt;Install Graphiti:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add "graphiti-core[google-genai]"

# or

pip install "graphiti-core[google-genai]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from graphiti_core import Graphiti
from graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig
from graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig
from graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient

# Google API key configuration
api_key = "&amp;lt;your-google-api-key&amp;gt;"

# Initialize Graphiti with Gemini clients
graphiti = Graphiti(
    "bolt://localhost:7687",
    "neo4j",
    "password",
    llm_client=GeminiClient(
        config=LLMConfig(
            api_key=api_key,
            model="gemini-2.0-flash"
        )
    ),
    embedder=GeminiEmbedder(
        config=GeminiEmbedderConfig(
            api_key=api_key,
            embedding_model="embedding-001"
        )
    ),
    cross_encoder=GeminiRerankerClient(
        config=LLMConfig(
            api_key=api_key,
            model="gemini-2.5-flash-lite"
        )
    )
)

# Now you can use Graphiti with Google Gemini for all components
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The Gemini reranker uses the &lt;code&gt;gemini-2.5-flash-lite&lt;/code&gt; model by default, which is optimized for cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI reranker, leveraging Gemini's log probabilities feature to rank passage relevance.&lt;/p&gt; 
&lt;h2&gt;Using Graphiti with Ollama (Local LLM)&lt;/h2&gt; 
&lt;p&gt;Graphiti supports Ollama for running local LLMs and embedding models via Ollama's OpenAI-compatible API. This is ideal for privacy-focused applications or when you want to avoid API costs.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Use &lt;code&gt;OpenAIGenericClient&lt;/code&gt; (not &lt;code&gt;OpenAIClient&lt;/code&gt;) for Ollama and other OpenAI-compatible providers like LM Studio. The &lt;code&gt;OpenAIGenericClient&lt;/code&gt; is optimized for local models with a higher default max token limit (16K vs 8K) and full support for structured outputs.&lt;/p&gt; 
&lt;p&gt;Install the models:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ollama pull deepseek-r1:7b # LLM
ollama pull nomic-embed-text # embeddings
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from graphiti_core import Graphiti
from graphiti_core.llm_client.config import LLMConfig
from graphiti_core.llm_client.openai_generic_client import OpenAIGenericClient
from graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig
from graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient

# Configure Ollama LLM client
llm_config = LLMConfig(
    api_key="ollama",  # Ollama doesn't require a real API key, but some placeholder is needed
    model="deepseek-r1:7b",
    small_model="deepseek-r1:7b",
    base_url="http://localhost:11434/v1",  # Ollama's OpenAI-compatible endpoint
)

llm_client = OpenAIGenericClient(config=llm_config)

# Initialize Graphiti with Ollama clients
graphiti = Graphiti(
    "bolt://localhost:7687",
    "neo4j",
    "password",
    llm_client=llm_client,
    embedder=OpenAIEmbedder(
        config=OpenAIEmbedderConfig(
            api_key="ollama",  # Placeholder API key
            embedding_model="nomic-embed-text",
            embedding_dim=768,
            base_url="http://localhost:11434/v1",
        )
    ),
    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),
)

# Now you can use Graphiti with local Ollama models
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Ensure Ollama is running (&lt;code&gt;ollama serve&lt;/code&gt;) and that you have pulled the models you want to use.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://help.getzep.com/graphiti"&gt;Guides and API documentation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://help.getzep.com/graphiti/graphiti/quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://help.getzep.com/graphiti/integrations/lang-graph-agent"&gt;Building an agent with LangChain's LangGraph and Graphiti&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Telemetry&lt;/h2&gt; 
&lt;p&gt;Graphiti collects anonymous usage statistics to help us understand how the framework is being used and improve it for everyone. We believe transparency is important, so here's exactly what we collect and why.&lt;/p&gt; 
&lt;h3&gt;What We Collect&lt;/h3&gt; 
&lt;p&gt;When you initialize a Graphiti instance, we collect:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Anonymous identifier&lt;/strong&gt;: A randomly generated UUID stored locally in &lt;code&gt;~/.cache/graphiti/telemetry_anon_id&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;System information&lt;/strong&gt;: Operating system, Python version, and system architecture&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Graphiti version&lt;/strong&gt;: The version you're using&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configuration choices&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;LLM provider type (OpenAI, Azure, Anthropic, etc.)&lt;/li&gt; 
   &lt;li&gt;Database backend (Neo4j, FalkorDB, Kuzu, Amazon Neptune Database or Neptune Analytics)&lt;/li&gt; 
   &lt;li&gt;Embedder provider (OpenAI, Azure, Voyage, etc.)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;What We Don't Collect&lt;/h3&gt; 
&lt;p&gt;We are committed to protecting your privacy. We &lt;strong&gt;never&lt;/strong&gt; collect:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Personal information or identifiers&lt;/li&gt; 
 &lt;li&gt;API keys or credentials&lt;/li&gt; 
 &lt;li&gt;Your actual data, queries, or graph content&lt;/li&gt; 
 &lt;li&gt;IP addresses or hostnames&lt;/li&gt; 
 &lt;li&gt;File paths or system-specific information&lt;/li&gt; 
 &lt;li&gt;Any content from your episodes, nodes, or edges&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Why We Collect This Data&lt;/h3&gt; 
&lt;p&gt;This information helps us:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Understand which configurations are most popular to prioritize support and testing&lt;/li&gt; 
 &lt;li&gt;Identify which LLM and database providers to focus development efforts on&lt;/li&gt; 
 &lt;li&gt;Track adoption patterns to guide our roadmap&lt;/li&gt; 
 &lt;li&gt;Ensure compatibility across different Python versions and operating systems&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By sharing this anonymous information, you help us make Graphiti better for everyone in the community.&lt;/p&gt; 
&lt;h3&gt;View the Telemetry Code&lt;/h3&gt; 
&lt;p&gt;The Telemetry code &lt;a href="https://raw.githubusercontent.com/getzep/graphiti/main/graphiti_core/telemetry/telemetry.py"&gt;may be found here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;How to Disable Telemetry&lt;/h3&gt; 
&lt;p&gt;Telemetry is &lt;strong&gt;opt-out&lt;/strong&gt; and can be disabled at any time. To disable telemetry collection:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Option 1: Environment Variable&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export GRAPHITI_TELEMETRY_ENABLED=false
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option 2: Set in your shell profile&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# For bash users (~/.bashrc or ~/.bash_profile)
echo 'export GRAPHITI_TELEMETRY_ENABLED=false' &amp;gt;&amp;gt; ~/.bashrc

# For zsh users (~/.zshrc)
echo 'export GRAPHITI_TELEMETRY_ENABLED=false' &amp;gt;&amp;gt; ~/.zshrc
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option 3: Set for a specific Python session&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os

os.environ['GRAPHITI_TELEMETRY_ENABLED'] = 'false'

# Then initialize Graphiti as usual
from graphiti_core import Graphiti

graphiti = Graphiti(...)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Telemetry is automatically disabled during test runs (when &lt;code&gt;pytest&lt;/code&gt; is detected).&lt;/p&gt; 
&lt;h3&gt;Technical Details&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Telemetry uses PostHog for anonymous analytics collection&lt;/li&gt; 
 &lt;li&gt;All telemetry operations are designed to fail silently - they will never interrupt your application or affect Graphiti functionality&lt;/li&gt; 
 &lt;li&gt;The anonymous ID is stored locally and is not tied to any personal information&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Status and Roadmap&lt;/h2&gt; 
&lt;p&gt;Graphiti is under active development. We aim to maintain API stability while working on:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Supporting custom graph schemas: 
  &lt;ul&gt; 
   &lt;li&gt;Allow developers to provide their own defined node and edge classes when ingesting episodes&lt;/li&gt; 
   &lt;li&gt;Enable more flexible knowledge representation tailored to specific use cases&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Enhancing retrieval capabilities with more robust and configurable options&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Graphiti MCP Server&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Expanding test coverage to ensure reliability and catch edge cases&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or answering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer to &lt;a href="https://raw.githubusercontent.com/getzep/graphiti/main/CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;Join the &lt;a href="https://discord.com/invite/W8Kw6bsgXQ"&gt;Zep Discord server&lt;/a&gt; and make your way to the &lt;strong&gt;#Graphiti&lt;/strong&gt; channel!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>HKUDS/VideoRAG</title>
      <link>https://github.com/HKUDS/VideoRAG</link>
      <description>&lt;p&gt;[KDD'2026] "VideoRAG: Chat with Your Videos"&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;img src="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/cover.png" width="80%" style="border: none; box-shadow: none;" alt="Vimo: Chat with Your Videos" /&gt; 
 &lt;/picture&gt; 
 &lt;h1&gt; &lt;strong&gt;VideoRAG: Chat with Your Videos&lt;/strong&gt; ‚Ä¢ &lt;strong&gt;Vimo Desktop&lt;/strong&gt; &lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/16146" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/16146" alt="HKUDS%2FVideoRAG | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://arxiv.org/abs/2502.01549"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2502.01549-b31b1b" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/VideoRAG/issues/1"&gt;&lt;img src="https://img.shields.io/badge/Áæ§ËÅä-wechat/feishu-green" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/ZzU55kz3"&gt;&lt;img src="https://discordapp.com/api/guilds/1296348098003734629/widget.png?style=shield" /&gt;&lt;/a&gt; &lt;a href="https://www.youtube.com/watch?v=D5vsxcp4QZI"&gt;&lt;img src="https://img.shields.io/badge/YouTube-Watch%20Demo-red?style=flat&amp;amp;logo=youtube" /&gt;&lt;/a&gt; &lt;a href="https://learnopencv.com/videorag-long-context-video-comprehension/"&gt;&lt;img src="https://img.shields.io/badge/Blog-LearnOpenCV-blue?style=flat&amp;amp;logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAJYAAACWCAMAAAAL34HQAAAAilBMVEVHcEwuLi4qKio3NzdQUFBjY2NoaGhiYmJ8fHx4eHiGhoabm5t1dXW2tranp6eOjo7Pz8/////9/f35+fn29vbz8/Pv7+/t7e3q6urn5+fj4+Pg4OA4svIyrfAsp+0noesinekemOcalOUchMVjZGQXaZxOT08OT3oMPFwvMTIONEoKIS8OFx0DAwPBWB/1AAAAEXRSTlMACxw5ZXqQmqG1vdfv8ff7/XwvPHUAAAnaSURBVHja3ZyJkqI6GIUbXHpcwHSUbtuMdtuiLcu8/+vdCIRDjElYxLLuz701W03NV+ccAmT5XzqV4w6Go/FkPl/4RS3m88l4NBy4zsuDC0Sv05nnEcLYhvH/ebHsp4R43mz6+ng2ZzD8M+NAOcaGI6Hy38ngZn+Gg4eRuYPR1LsQVWk+iwtVoE1HA/cROo1nORPLcW4WBwSaNxv3rJk7nGRMGwlpfV2CrSDbEG8ydPsTajQjFZXWJc/H+gMFuAteqdls1I9k3D1Sca4AQr3zyn4EXa6bAOvBS2fw6mdQYCpheAWVKyuwZaIVYP7rfcHcUqkKkwC6UQWeICsl44q595NqOCUQCkwFwuq6gJaTQbINBxs6dwrVxMuUUphAtFwti1ot+a8A914hK7z0JoN7+Dea5VLlUIJpFeQ8uip0WwkyDgbBRm53qWSokglItKg3KqpkA5kM1lEwZzgjhX+AqjDREgklACtkAMuc7Jowd+wJqQAlmMBzq3I2QQYwIZjX/pYcTAkT/mVQEMqIBDReQrIMDE4yMm1pZGkgoCDUW80CGcAuXGzDjWwVK5+xMlWZfRDKjqOC5VZCMMb8odOcymOsTBWkAlMzslIwJIwxrymXM/JgoALVFQxcI6cR1VihAlQXMImLlzd2GlMVBopUAaotmJQwnlpwNXDQKFV3wYRe8NGadpUKUF3qysiMq27uVSpIdRfBwFXcj7Wo/AqVRqqugkl6Mb8G12CWj1f9UMFI5Iux2cBG5U4J02jVHxeZurahgfSmFbjeJK5PzjV2LHHXUPWslzc0B0s8BzlVAKoe9BK5z8d7Y7zc7A25ORWly+DyD/AnQu2/UXCJYYJM9PEaIVh1qehqTfxFGJ6irE5huPDJekXr6gWukcVCBItSGxPxwyhN0yQ+FxUn/JdR6BMbGaWIl9FGBxbWolqyxSlKk/Pv8XD4+fnZ/+z3/IfD4fh7TtLotGDLmlzCRsdwF66FhRaqFZlzJo70c6M4Giebk5WZCzbq70Z3imBZqQISRskZTLfIzkkUksDOVdo4dTUDqWShwT4ScqHApCPjkoXEYOW1jWNHm3d7sOjHIkoBZQZLo8UHtcZLm3rnNRfLbiElpzQGlKWOcXoi1G5jLtercy2WX9PCwI/gX536TSI/qGmjP1CTVSvv6/lFqv1PgzrE6XxdJ/VIF5IliaWjoixMz6pUe7kUrnMaMqrjglxqukYQy2QhO6W/GiQ9GNf2Nz0xk42QaySNWRDLYCHlVMfbUN9l3SY7ci6qt1HIxdjMrQ7wuA0NYilUQEIBTeEyyIWbcVh9GmLM0ou1DmWqK6av7JLIZK5wbZQrd5FNHATew5ilFSuYy7kC1NdVAUwaKNJ5oJULY5eH0IvRwXQbUj89a6DAZQQ7pz413YxijFACbxCLkig+qFSCYycKZArXIY4INcglxghX8VAv1scp0VJdaPiV/8eropjMlZzWerngIgYtW+CXi2qwAAWdpJIEq8ZrsbSEHkOXM7V7KCxUqQTXtrgA9q1ycRvtLk6duh4GYSpTyVJtqyUpJrgwSgR1XRzKHt7MOyxUqS4ogKqgcSrBBRs9nVxwcZh5+Mfq4Sqs5D2nunBdKfWXX5cSZn5Br0rqw5XVxT8OhgeThyRSxZKp/qK2kl6KXBGxujhzRbSMT+nlXIgFKkBlTIASYLutFC/INV/q70WEyx4tyiKM79AKVGqVgsFGyMVqheuVWKJFF+nxplgyFMfYKlyqjcd0QW/KhXCRV4xa+mitTvBQthBUh2Oc8IqPh62GCy6eVtpwYeRC4hEtNfAQq7QQWu2OcfqvqDQ+7gQXbIRcCL0hXDM3T7wxWj48hFg80YLqEP+TKj6AC3LBxZsvEteZH3oi8StgqYOWilVSJf+uKgFXRS5p6DJn3hviOa1L/Do6yx5CLFDJlR5kuWQXz9H6Ta1sQMXTekwsiceDRxYLVGol4FJd/E2JJfNk/DJhFiwpWhArp9oiV1LFWe4hVwVrz8NlwWKTl7nu0YNR66B6KCw8/tPUUWCp9+IBI5fu8TN/mW/M4wMNEwULHsY6rHgLFwUWMn8Ta1libeYvCwnrRhTDWI1WmfcUIJrU3whXHC7NI8Rm8eIzM1ZwOktYiod2F3fXWKfgJhZM9DmWeTR9j4Bl9xAVq5nHCPGuVas9FkzcJnqspBqub+lWPEcfdixupWmQX19hfVdM3JmwdsiWota6o1oq1ldzLFUtYKEUtYxYH7fU2tYx0aDWR2e1Ak227JHXZ8se+f4HCHXcOp8CG9YzDacYtxZ9P3x2moePeZSf9/uohloNH9WtXmyAtbO+2Kg3Yq0XmzFhbV4Dd+1fA481XgNbvzRvDVyJ8aU51gzy0kszPjGCOp8Y+KQ2f2KoHlpmR+RPjBYfZMCyfZDtWn+Qtf983dk/X7cNP18xZdPxY/+v6WN/1/5jXz81grrn1MgeUyNKtDA1UmuOUjuRZJ5H6jSRhMxrw0XnycE0GajHajvtdqdJShkJWrWdpKw3pbtUp3QFlzynK5hA1XxKl2VTuoZwocwT4CBTqNpNgNdcLqDm5YILh4Sko9ofk3rLBd0XV+TaVplaL67UXoqi+qWoL4WJU+06LkWpLi7rLdxh3RVkQLrHwp3TeZlzl7Hxy7DMua+9zNl1URhg+kXhfeNF4WZL6Cv9Evp3myV0lLKEfr8NB9hvAChIVXvDwYZNnIbbM+iDt2d038zyvecsF572m1kQeJRj2foDLuQLZGope7iglVLK1p+eNkr9dN4ohaq5rYzm28rU2udX921lLTfhrRpuwju02YSHqr1lcdloy+Kh5ZbF/jd4rhts8Oy0HZY+YDssxq4mm4e9MMKGZt225ij0mmwenrlPu9W6+8Z0io3pYOu6Mb37Nn7+h5238cPCzWzwDIceqHLo4RmOiLypR0Se4EANrXGgBuU86PjRm3r86AkOa1GJCgOpvtxp71yqVmTqPsVBwJWgWuMgYKNjk8FDjk0+3SFTxP1JjuSCyml6BL3/A8xNDqI7Y3DdTzBACSoc936Gw/FBcTj+c8NA9RStBODgczZeePY2Fd2betCuTT1Ed5b/SwsUNIxhbRvGUEPDGIaGMQ9qr0PL0rfX2XRt4OQMJkQIBjCQAa1aS6nlD6CEVGQy6Kl1U6C0bspYUCswaVo39d/oSuHpt9EV2oIVYCBDozK5JVi1LxiEQr8ypOrxTdT4b9uaqPXfck5uOlcUWs4BCi3nHtKgL4MoWN4/QASmDRr03b2kdoY5GuBQ6LX4iHaGaP4IMgH3WW2tiOaPn4IJzR/7AuNeVttSantlPqhVJsqRGosKPFwbILFHNBYFGNqwSnQA0rRhfXDTWtTmPk1rn7TF739gu3see8j9YQAAAABJRU5ErkJggg==" alt="Blog" /&gt;&lt;/a&gt; &lt;a href=""&gt;&lt;img src="https://img.shields.io/badge/platform-macOS%20%7C%20Windows%20%7C%20Linux-lightgrey.svg?sanitize=true" alt="Platform" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;üé¨ Intelligent Video Conversations | Powered by Advanced AI | Extreme Long-Context Processing&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;img src="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/VideoRAG-algorithm/VideoRAG_cover.png" /&gt; 
&lt;p&gt;Vimo is a revolutionary desktop application that lets you &lt;strong&gt;chat with your videos&lt;/strong&gt; using cutting-edge AI technology. Built on the powerful &lt;a href="https://arxiv.org/abs/2502.01549"&gt;VideoRAG framework&lt;/a&gt;, Vimo can understand and analyze videos of any length - from short clips to hundreds of hours of content - and answer your questions with remarkable accuracy.&lt;/p&gt; 
&lt;h3&gt;üé• Watch Vimo in Action&lt;/h3&gt; 
&lt;p&gt;See how Vimo transforms video interaction with intelligent conversations and deep understanding capabilities.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://www.youtube.com/watch?v=D5vsxcp4QZI"&gt; &lt;img src="https://img.youtube.com/vi/D5vsxcp4QZI/maxresdefault.jpg" width="80%" alt="Vimo Introduction Video" /&gt; &lt;/a&gt; 
 &lt;p&gt;&lt;em&gt;üëÜ Click to watch the Vimo demo video&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;‚ú® Key Features&lt;/h2&gt; 
&lt;h3&gt;For Everyone&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Drag &amp;amp; Drop Upload&lt;/strong&gt;: Simply drag video files into Vimo&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Conversations&lt;/strong&gt;: Ask questions in natural language&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Format Support&lt;/strong&gt;: Works with MP4, MKV, AVI, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cross-Platform&lt;/strong&gt;: Available on macOS, Windows, and Linux&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;For Power Users&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Extreme Long Videos&lt;/strong&gt;: Process videos up to hundreds of hours&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Video Analysis&lt;/strong&gt;: Compare and analyze multiple videos simultaneously&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced Retrieval&lt;/strong&gt;: Find specific moments and scenes with precision&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Export Capabilities&lt;/strong&gt;: Save insights and references for later use&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;For Researchers&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;VideoRAG Framework&lt;/strong&gt;: Access to cutting-edge retrieval-augmented generation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Benchmark Dataset&lt;/strong&gt;: LongerVideos benchmark with 134+ hours of content&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Performance Metrics&lt;/strong&gt;: Detailed evaluation against existing methods&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensible Architecture&lt;/strong&gt;: Build upon our open-source foundation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üåü Why Vimo?&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;For Video Enthusiasts &amp;amp; Professionals:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Effortless Video Analysis&lt;/strong&gt;: Upload any video and start asking questions immediately&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Natural Conversations&lt;/strong&gt;: Chat with your videos as if talking to a human expert&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No Length Limits&lt;/strong&gt;: Process everything from 30-second clips to 100+ hour documentaries&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Deep Understanding&lt;/strong&gt;: Combines visual content, audio, and context for comprehensive answers&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;For Researchers &amp;amp; Developers:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;State-of-the-Art Algorithm&lt;/strong&gt;: Built on VideoRAG, featuring graph-driven knowledge indexing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Benchmark Performance&lt;/strong&gt;: Evaluated on 134+ hours across lectures, documentaries, and entertainment&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open Source&lt;/strong&gt;: Full access to VideoRAG implementation and research findings&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scalable Architecture&lt;/strong&gt;: Efficient processing with single GPU (RTX 3090) capability&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìã Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/#-quick-start"&gt;üöÄ Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/#-key-features"&gt;‚ú® Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/#-videorag-algorithm"&gt;üî¨ VideoRAG Algorithm&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/#%EF%B8%8F-development-setup"&gt;üõ†Ô∏è Development Setup&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/#-benchmarks--evaluation"&gt;üß™ Benchmarks &amp;amp; Evaluation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/#-citation"&gt;üìñ Citation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/#-contributing"&gt;ü§ù Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/#-acknowledgement"&gt;üôè Acknowledgement&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üöÄ Quick Start of Vimo&lt;/h2&gt; 
&lt;h3&gt;Option 1: Download Vimo App (Coming Soon)&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] We are preparing the &lt;strong&gt;Beta release&lt;/strong&gt; for macOS Apple Silicon first, with Windows and Linux versions coming soon!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;div align="left"&gt; 
 &lt;a href="https://github.com/HKUDS/Vimo/releases"&gt; &lt;img src="https://img.shields.io/badge/Coming%20Soon-Mac%20Download-007ACC?style=for-the-badge&amp;amp;logo=apple&amp;amp;logoColor=white" alt="Coming Soon - Mac Release" height="50" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h3&gt;Option 2: Run from Source Code&lt;/h3&gt; 
&lt;p&gt;For detailed setup instructions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Vimo Desktop App&lt;/strong&gt;: See &lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/Vimo-desktop"&gt;Vimo-desktop&lt;/a&gt; for complete installation and configuration steps&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Quick Overview:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Set up the Python backend environment and start the VideoRAG server&lt;/li&gt; 
 &lt;li&gt;Launch the Electron frontend application&lt;/li&gt; 
 &lt;li&gt;Start chatting with your videos!&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;üî¨ VideoRAG Algorithm&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/VideoRAG-algorithm/VideoRAG.png" alt="VideoRAG Architecture" width="80%" /&gt; &lt;/p&gt; 
&lt;p&gt;VideoRAG introduces a novel dual-channel architecture that combines:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Graph-Driven Knowledge Indexing&lt;/strong&gt;: Multi-modal knowledge graphs for structured video understanding&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Hierarchical Context Encoding&lt;/strong&gt;: Preserves spatiotemporal visual patterns across long sequences&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Adaptive Retrieval&lt;/strong&gt;: Dynamic retrieval mechanisms optimized for video content&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cross-Video Understanding&lt;/strong&gt;: Semantic relationship modeling across multiple videos&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Technical Highlights&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Efficient Processing&lt;/strong&gt;: Handle hundreds of hours on a single RTX 3090 (24GB)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Structured Indexing&lt;/strong&gt;: Distill long videos into concise knowledge representations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Modal Retrieval&lt;/strong&gt;: Align textual queries with visual and audio content&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LongerVideos Benchmark&lt;/strong&gt;: 160+ videos, 134+ hours across diverse domains&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Performance Comparison&lt;/h3&gt; 
&lt;p&gt;Our VideoRAG algorithm significantly outperforms existing methods in long-context video understanding:&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/Vimo-desktop/figures/table.png" width="80%" alt="Performance Comparison" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;Experiments and Evaluation&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/VideoRAG-algorithm"&gt;VideoRAG-algorithm&lt;/a&gt; for detailed development setup including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Conda environment creation&lt;/li&gt; 
 &lt;li&gt;Model checkpoints download&lt;/li&gt; 
 &lt;li&gt;Dependencies installation&lt;/li&gt; 
 &lt;li&gt;Evaluation scripts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üß™ LongerVideos Benchmark&lt;/h2&gt; 
&lt;p&gt;We created the LongerVideos benchmark to evaluate long-context video understanding:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Video Type&lt;/th&gt; 
   &lt;th&gt;#Collections&lt;/th&gt; 
   &lt;th&gt;#Videos&lt;/th&gt; 
   &lt;th&gt;#Queries&lt;/th&gt; 
   &lt;th&gt;Avg. Duration&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Lectures&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td&gt;135&lt;/td&gt; 
   &lt;td&gt;376&lt;/td&gt; 
   &lt;td&gt;~64.3 hours&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Documentaries&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td&gt;114&lt;/td&gt; 
   &lt;td&gt;~28.5 hours&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Entertainment&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;112&lt;/td&gt; 
   &lt;td&gt;~41.9 hours&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;22&lt;/td&gt; 
   &lt;td&gt;164&lt;/td&gt; 
   &lt;td&gt;602&lt;/td&gt; 
   &lt;td&gt;~134.6 hours&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;For detailed evaluation instructions and reproduction scripts, see &lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/VideoRAG-algorithm/reproduce"&gt;VideoRAG-algorithm/reproduce&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üìñ Citation&lt;/h2&gt; 
&lt;p&gt;If you find Vimo or VideoRAG helpful in your research, please cite our paper:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{VideoRAG,
  title={VideoRAG: Retrieval-Augmented Generation with Extreme Long-Context Videos},
  author={Ren, Xubin and Xu, Lingrui and Xia, Long and Wang, Shuaiqiang and Yin, Dawei and Huang, Chao},
  journal={arXiv preprint arXiv:2502.01549},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Whether you're:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Reporting bugs&lt;/strong&gt; or suggesting features for Vimo&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Improving VideoRAG algorithms&lt;/strong&gt; or adding new capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhancing documentation&lt;/strong&gt; or creating tutorials&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Designing UI/UX improvements&lt;/strong&gt; for better user experience&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Feel free to submit issues and pull requests. Together, we're building the future of intelligent video interaction!&lt;/p&gt; 
&lt;h2&gt;üôè Acknowledgement&lt;/h2&gt; 
&lt;p&gt;Vimo builds upon the incredible work of the open-source community:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://arxiv.org/abs/2502.01549"&gt;VideoRAG&lt;/a&gt;&lt;/strong&gt;: The core algorithm powering Vimo's intelligence&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/gusye1234/nano-graphrag"&gt;nano-graphrag&lt;/a&gt;&lt;/strong&gt; &amp;amp; &lt;strong&gt;&lt;a href="https://github.com/HKUDS/LightRAG"&gt;LightRAG&lt;/a&gt;&lt;/strong&gt;: Graph-based retrieval foundations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/facebookresearch/ImageBind"&gt;ImageBind&lt;/a&gt;&lt;/strong&gt;: Multi-modal representation learning&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/bytedance/UI-TARS-desktop"&gt;uitars-desktop&lt;/a&gt;&lt;/strong&gt;: Desktop application architecture inspiration&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;üåü Transform how you interact with videos. Start your journey with Vimo today!&lt;/strong&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;sub&gt;Built with ‚ù§Ô∏è by the VideoRAG@HKUDS team.&lt;/sub&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>MiroMindAI/MiroThinker</title>
      <link>https://github.com/MiroMindAI/MiroThinker</link>
      <description>&lt;p&gt;MiroThinker is an open-source search agent model, built for tool-augmented reasoning and real-world information seeking, aiming to match the deep research experience of OpenAI Deep Research and Gemini Deep Research.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/assets/miro_thinker.png" width="55%" alt="MiroThinker" /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://dr.miromind.ai/"&gt;&lt;img src="https://img.shields.io/badge/Demo-FFB300?style=for-the-badge&amp;amp;logo=airplayvideo&amp;amp;logoColor=white" alt="DEMO" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/collections/miromind-ai/mirothinker-v15"&gt;&lt;img src="https://img.shields.io/badge/Models-5EDDD2?style=for-the-badge&amp;amp;logo=huggingface&amp;amp;logoColor=ffffff&amp;amp;labelColor" alt="MODELS" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2511.11793"&gt;&lt;img src="https://img.shields.io/badge/Paper-B31B1B?style=for-the-badge&amp;amp;logo=arxiv&amp;amp;logoColor=white" alt="Paper" /&gt;&lt;/a&gt; &lt;a href="https://miromind.ai/#blog"&gt;&lt;img src="https://img.shields.io/badge/Blog-4285F4?style=for-the-badge&amp;amp;logo=google-chrome&amp;amp;logoColor=white" alt="Blog" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.1"&gt;&lt;img src="https://img.shields.io/badge/Data-0040A1?style=for-the-badge&amp;amp;logo=huggingface&amp;amp;logoColor=ffffff&amp;amp;labelColor" alt="DATA" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/MiroMindAI"&gt;&lt;img src="https://img.shields.io/badge/Github-24292F?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" alt="GITHUB" /&gt;&lt;/a&gt; &lt;a href="https://miromind.ai/"&gt;&lt;img src="https://img.shields.io/badge/Website-4285F4?style=for-the-badge&amp;amp;logo=google-chrome&amp;amp;logoColor=white" alt="WEBSITE" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/GPqEnkzQZd"&gt;&lt;img src="https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="DISCORD" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/refs/heads/main/assets/miromind_wechat.png"&gt;&lt;img src="https://img.shields.io/badge/WeChat-07C160?style=for-the-badge&amp;amp;logo=wechat&amp;amp;logoColor=white" alt="WeChat" /&gt;&lt;/a&gt; &lt;a href="https://www.xiaohongshu.com/user/profile/5e353bd80000000001000239"&gt;&lt;img src="https://img.shields.io/badge/RedNote-FF2442?style=for-the-badge&amp;amp;logo=revoltdotchat&amp;amp;logoColor=white" alt="RedNote" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;üöÄ &lt;a href="https://dr.miromind.ai/"&gt;Try our Demo!&lt;/a&gt;&lt;/h3&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;MiroThinker&lt;/strong&gt; is MiroMind's Flagship Research Agent Model. It is an open-source search model designed to advance tool-augmented reasoning and information-seeking capabilities, enabling complex real-world research workflows across diverse challenges.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;The project currently comprises four key components:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üí° &lt;strong&gt;MiroThinker&lt;/strong&gt;: An open-source search &lt;strong&gt;model&lt;/strong&gt; that natively supports tool-assisted reasoning, achieving leading performance across multiple benchmarks (e.g., HLE, HLE-Text-2158, HLE-Text-500, BrowseComp, BrowseComp-ZH, GAIA, XBench-DeepSearch, FutureX, and Frames). See &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-quick-start"&gt;Quick Start&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;ü§ñ &lt;strong&gt;MiroFlow&lt;/strong&gt;: An open-source research agent framework that offers reproducible state-of-the-art performance across multiple benchmarks. See &lt;a href="https://github.com/MiroMindAI/MiroFlow"&gt;MiroFlow&lt;/a&gt; for details.&lt;/li&gt; 
 &lt;li&gt;üìö &lt;strong&gt;MiroVerse&lt;/strong&gt;: A premium open-source training dataset with 147k samples supporting research agent training. See &lt;a href="https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.1"&gt;MiroVerse&lt;/a&gt; on HuggingFace.&lt;/li&gt; 
 &lt;li&gt;üîß &lt;strong&gt;MiroTrain / MiroRL&lt;/strong&gt;: Training infrastructure that supports stable and efficient training for research agent models. See &lt;a href="https://github.com/MiroMindAI/MiroTrain"&gt;MiroTrain&lt;/a&gt; and &lt;a href="https://github.com/MiroMindAI/MiroRL"&gt;MiroRL&lt;/a&gt; for details.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìã Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üì∞ &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-news--updates"&gt;News &amp;amp; Updates&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;‚ú® &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-key-features"&gt;Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìà &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-performance-on-benchmarks"&gt;Performance on Benchmarks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üöÄ &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìä &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-benchmark-evaluation"&gt;Benchmark Evaluation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üî¨ &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-trace-collection"&gt;Trace Collection&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;‚ùì &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-faq--troubleshooting"&gt;FAQ &amp;amp; Troubleshooting&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìÑ &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-license"&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üôè &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-acknowledgments"&gt;Acknowledgments&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üì∞ News &amp;amp; Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;[2026-01-05]&lt;/strong&gt; üéâüéâ We release &lt;a href="https://huggingface.co/collections/miromind-ai/mirothinker-v15"&gt;MiroThinker-v1.5&lt;/a&gt;, a world-leading open-source search agent. &lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.5-30B"&gt;MiroThinker-v1.5-30B&lt;/a&gt; surpasses Kimi-K2-Thinking on BrowseComp-ZH at much lower cost, using only 1/30 of the parameters. &lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.5-235B"&gt;MiroThinker-v1.5-235B&lt;/a&gt; scores 39.2% on HLE-Text, 69.8% on BrowseComp, 71.5% on BrowseComp-ZH, and 80.8% on GAIA-Val-165, setting a new state-of-the-art among search agents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-11-13]&lt;/strong&gt; üéâ &lt;a href="https://huggingface.co/collections/miromind-ai/mirothinker-v10"&gt;MiroThinker-v1.0&lt;/a&gt; is now released! Introducing &lt;strong&gt;interactive scaling&lt;/strong&gt; as a third dimension of performance improvement, MiroThinker v1.0 supports 256K context window and up to 600 tool calls per task. Available in 8B, 30B, and 72B parameter scales, achieving 37.7%, 47.1%, 55.6%, and 81.9% on HLE-Text, BrowseComp, BrowseComp-ZH, and GAIA-Text-103, respectively. See &lt;a href="https://arxiv.org/abs/2511.11793"&gt;Technical Report&lt;/a&gt; for more details.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-09-11]&lt;/strong&gt; MiroThinker-72B-Preview ranked 4th in this week's FutureX benchmark. See &lt;a href="https://futurex-ai.github.io/"&gt;FutureX&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;üìú Click to expand older updates&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;[2025-09-08]&lt;/strong&gt; &lt;a href="https://huggingface.co/collections/miromind-ai/mirothinker-v02"&gt;MiroThinker-v0.2&lt;/a&gt; is now released, achieving open-source SOTA performance across multiple benchmarks, including HLE (17.8%), HLE-Text-Only (19.1%), BrowseComp-EN (17.2%), BrowseComp-ZH (29.4%), XBench-DeepSearch (56.0%), and Frames (74.8%).&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2025-09-07]&lt;/strong&gt; We supported more benchmarks, including &lt;a href="https://arxiv.org/abs/2504.19314"&gt;BrowseComp-ZH&lt;/a&gt;, &lt;a href="https://xbench.org/agi/aisearch"&gt;XBench-DeepSearch&lt;/a&gt;, and &lt;a href="https://futurex-ai.github.io/"&gt;FutureX&lt;/a&gt;. We plan to add more benchmarks in the future.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2025-08-22]&lt;/strong&gt; Introducing streamlined deployment options for MiroThinker models with optimized resource usage and faster startup times. Experience the interactive demo: &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/apps/gradio-demo"&gt;üöÄ Try Gradio Demo&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2025-08-08]&lt;/strong&gt; &lt;a href="https://huggingface.co/collections/miromind-ai/mirothinker-v01-689301b6d0563321862d44a1"&gt;MiroThinker-v0.1&lt;/a&gt; released. Models, framework, and data are now fully open-sourced!&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;üìù Introduction&lt;/h2&gt; 
&lt;h3&gt;MiroThinker-v1.5&lt;/h3&gt; 
&lt;p&gt;MiroThinker v1.5 is the world-leading open-source search agent that advances tool-augmented reasoning through &lt;strong&gt;interactive scaling&lt;/strong&gt; ‚Äî training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement, beyond model size and context length.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/mirothinker_v1.5_framework.png" alt="image" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key Features&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üöÄ MiroThinker v1.5 supports a 256K context window, long-horizon reasoning, and deep multi-step analysis.&lt;/li&gt; 
 &lt;li&gt;üîß Handles up to 400 tool calls per task ‚Äî a substantial improvement over previous open-source research agents.&lt;/li&gt; 
 &lt;li&gt;üì¶ Released in 30B and 235B parameter scales, accompanied by a comprehensive suite of tools and workflows to flexibly support diverse research settings and compute budgets.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;Model Name&lt;/th&gt; 
    &lt;th align="center"&gt;Base Model&lt;/th&gt; 
    &lt;th align="center"&gt;Max Context&lt;/th&gt; 
    &lt;th align="center"&gt;Max Tool Calls&lt;/th&gt; 
    &lt;th align="center"&gt;HF Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;MiroThinker-v1.5-30B&lt;/td&gt; 
    &lt;td align="center"&gt;Qwen3-30B-A3B-Thinking-2507&lt;/td&gt; 
    &lt;td align="center"&gt;256K&lt;/td&gt; 
    &lt;td align="center"&gt;400&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.5-30B"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;MiroThinker-v1.5-235B&lt;/td&gt; 
    &lt;td align="center"&gt;Qwen3-235B-A22B-Thinking-2507&lt;/td&gt; 
    &lt;td align="center"&gt;256K&lt;/td&gt; 
    &lt;td align="center"&gt;400&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.5-235B"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;p&gt;MiroThinker v1.5 demonstrates strong general-research performance across a broad range of benchmarks, achieving&amp;nbsp;39.2%,&amp;nbsp;69.8%, 71.5%, and&amp;nbsp;80.8%&amp;nbsp;on HLE-Text, BrowseComp, BrowseComp-ZH, and GAIA-Val-165, respectively. These results surpass previous open-source agents and set the new world-leading BrowseComp performance.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/mirothinker_v1.5_browsecomp.png" alt="image" /&gt;&lt;/p&gt; 
&lt;h3&gt;MiroThinker-v1.0&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;üì¶ Click to expand MiroThinker-v1.0 details&lt;/summary&gt; 
 &lt;p&gt;Unlike previous agents that scale only model size or context length, MiroThinker v1.0 introduces &lt;strong&gt;interactive scaling&lt;/strong&gt; at the model level, systematically training the model to handle deeper and more frequent agent‚Äìenvironment interactions as a third dimension of performance improvement. Interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/MiroThinker_v1.0_Overall.png" alt="image" /&gt;&lt;/p&gt; 
 &lt;h3&gt;‚ú® Key Features&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;üöÄ &lt;strong&gt;256K Context Window&lt;/strong&gt;: Supports long-horizon reasoning and deep multi-step analysis&lt;/li&gt; 
  &lt;li&gt;üîß &lt;strong&gt;600 Tool Calls&lt;/strong&gt;: Handles up to 600 tool calls per task ‚Äî a substantial improvement over previous open-source research agents&lt;/li&gt; 
  &lt;li&gt;üì¶ &lt;strong&gt;Multiple Scales&lt;/strong&gt;: Released in 8B, 30B, and 72B parameter scales, accompanied by a comprehensive suite of tools and workflows to flexibly support diverse research settings and compute budgets&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;div align="center"&gt; 
  &lt;table&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="center"&gt;Model Name&lt;/th&gt; 
     &lt;th align="center"&gt;Base Model&lt;/th&gt; 
     &lt;th align="center"&gt;Max Context&lt;/th&gt; 
     &lt;th align="center"&gt;Max Tool Calls&lt;/th&gt; 
     &lt;th align="center"&gt;HF Link&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-v1.0-8B&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-8B&lt;/td&gt; 
     &lt;td align="center"&gt;256K&lt;/td&gt; 
     &lt;td align="center"&gt;600&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.0-8B"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-v1.0-30B&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-30B-A3B-Thinking-2507&lt;/td&gt; 
     &lt;td align="center"&gt;256K&lt;/td&gt; 
     &lt;td align="center"&gt;600&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.0-30B"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-v1.0-72B&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen2.5-72B-Instruct&lt;/td&gt; 
     &lt;td align="center"&gt;256K&lt;/td&gt; 
     &lt;td align="center"&gt;600&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.0-72B"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; 
 &lt;p&gt;MiroThinker v1.0 demonstrates strong general-research performance across a broad range of benchmarks, achieving &lt;strong&gt;37.7%&lt;/strong&gt;, &lt;strong&gt;47.1%&lt;/strong&gt;, &lt;strong&gt;55.6%&lt;/strong&gt;, and &lt;strong&gt;81.9%&lt;/strong&gt; on HLE-Text, BrowseComp, BrowseComp-ZH, and GAIA-Text-103, respectively. These results surpass previous open-source agents and narrow the gap with commercial counterparts such as &lt;strong&gt;GPT-5-high&lt;/strong&gt;.&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/MiroThinker_v1.0_Performance_1.png" width="100%" alt="MiroThinker" /&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;h3&gt;MiroThinker-v0.2&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;üì¶ Click to expand MiroThinker-v0.2 details&lt;/summary&gt; 
 &lt;p&gt;In this new version, we introduced three key improvements:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;üìö &lt;strong&gt;Richer training data&lt;/strong&gt; from both English and Chinese sources, yielding significant gains in benchmark performance and generalization&lt;/li&gt; 
  &lt;li&gt;üéØ &lt;strong&gt;Unified DPO training&lt;/strong&gt; with a single preference dataset across all models&lt;/li&gt; 
  &lt;li&gt;üìè &lt;strong&gt;Extended context length&lt;/strong&gt; from 40k to 64k for more challenging multi-turn tool-use tasks&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Compared to v0.1, MiroThinker v0.2 delivers consistent gains across benchmarks. For example, scores improved from &lt;strong&gt;57.3 ‚Üí 64.1&lt;/strong&gt; on &lt;strong&gt;GAIA-Text-103&lt;/strong&gt; and from &lt;strong&gt;17.0 ‚Üí 29.4&lt;/strong&gt; on &lt;strong&gt;BrowseComp-ZH&lt;/strong&gt;, reflecting substantial advancements in the model‚Äôs general research agent capabilities.&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="center"&gt;Model Name&lt;/th&gt; 
     &lt;th align="center"&gt;Base Model&lt;/th&gt; 
     &lt;th align="center"&gt;Max Context&lt;/th&gt; 
     &lt;th align="center"&gt;HF Link&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-4B-SFT-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-4B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-4B-SFT-v0.2"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-4B-DPO-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-4B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-4B-DPO-v0.2"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-8B-SFT-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-8B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-8B-SFT-v0.2"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-8B-DPO-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-8B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-8B-DPO-v0.2"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-14B-SFT-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-14B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-14B-SFT-v0.2"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-14B-DPO-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-14B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-14B-DPO-v0.2"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-32B-SFT-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-32B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-32B-SFT-v0.2"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-32B-DPO-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-32B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-32B-DPO-v0.2"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;h3&gt;MiroThinker-v0.1&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;üì¶ Click to expand MiroThinker-v0.1 details&lt;/summary&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/assets/gaia_text_103.png" width="98%" alt="MiroFlow Performance on GAIA-Validation" /&gt; 
  &lt;p&gt;&lt;strong&gt;Performance of Open-Source Models on GAIA-Validation Benchmark.&lt;/strong&gt;&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;p&gt;We have released the &lt;strong&gt;MiroThinker v0.1&lt;/strong&gt; series, including both SFT and DPO variants at parameter scales of &lt;strong&gt;8B&lt;/strong&gt;, &lt;strong&gt;14B&lt;/strong&gt;, and &lt;strong&gt;32B&lt;/strong&gt;. Notably, MiroThinker v0.1 achieves &lt;strong&gt;state-of-the-art performance&lt;/strong&gt; among open-source models on the &lt;a href="https://huggingface.co/datasets/gaia-benchmark/GAIA"&gt;GAIA benchmark&lt;/a&gt;, a rigorous evaluation suite for advanced agentic capabilities, demonstrating its strength in long-context, decision-intensive, and real-world task scenarios.&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="center"&gt;Model Name&lt;/th&gt; 
     &lt;th align="center"&gt;Base Model&lt;/th&gt; 
     &lt;th align="center"&gt;Max Context&lt;/th&gt; 
     &lt;th align="center"&gt;HF Link&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-8B-SFT-v0.1&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-8B&lt;/td&gt; 
     &lt;td align="center"&gt;40K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-8B-SFT-v0.1"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-8B-DPO-v0.1&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-8B&lt;/td&gt; 
     &lt;td align="center"&gt;40K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-8B-DPO-v0.1"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-14B-SFT-v0.1&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-14B&lt;/td&gt; 
     &lt;td align="center"&gt;40K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-14B-SFT-v0.1"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-14B-DPO-v0.1&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-14B&lt;/td&gt; 
     &lt;td align="center"&gt;40K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-14B-DPO-v0.1"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-32B-SFT-v0.1&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-32B&lt;/td&gt; 
     &lt;td align="center"&gt;40K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-32B-SFT-v0.1"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-32B-DPO-v0.1&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-32B&lt;/td&gt; 
     &lt;td align="center"&gt;40K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-32B-DPO-v0.1"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚ú® Key Features&lt;/h2&gt; 
&lt;h3&gt;ü§ñ &lt;strong&gt;MiroThinker-Optimized Framework&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üîì &lt;strong&gt;Fully Open-Source Agent Framework&lt;/strong&gt;: Complete transparency with open framework and open models&lt;/li&gt; 
 &lt;li&gt;üîó &lt;strong&gt;Tool Integration&lt;/strong&gt;: Seamless integration with external tools and APIs&lt;/li&gt; 
 &lt;li&gt;üìù &lt;strong&gt;Trace Collection&lt;/strong&gt;: Comprehensive logging and analysis of agent interactions with elapsed time and estimated completion time displayed in minutes. Ready for SFT and DPO&lt;/li&gt; 
 &lt;li&gt;üìä &lt;strong&gt;Benchmark Evaluation&lt;/strong&gt;: Extensive testing across multiple benchmark datasets&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìä &lt;strong&gt;Comprehensive Benchmark Suite&lt;/strong&gt;&lt;/h3&gt; 
&lt;details open&gt; 
 &lt;summary&gt;üìã Click to expand benchmark list&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;GAIA Validation&lt;/strong&gt;: A benchmark for General AI Assistants. (&lt;a href="https://arxiv.org/abs/2311.12983"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;GAIA-Text-103&lt;/strong&gt;: A subset of GAIA Validation for text-only tasks. (&lt;a href="https://arxiv.org/abs/2505.22648"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;HLE&lt;/strong&gt;: Humanity's Last Exam. (&lt;a href="https://arxiv.org/abs/2501.14249"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;HLE-Text-2158&lt;/strong&gt;: A subset of HLE for text-only tasks. (&lt;a href="https://arxiv.org/abs/2501.14249"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;HLE-Text-500&lt;/strong&gt;: A subset of HLE for text-only tasks, created by &lt;a href="https://arxiv.org/pdf/2504.21776"&gt;WebThinker&lt;/a&gt;. (&lt;a href="https://arxiv.org/pdf/2504.21776"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;BrowseComp-EN&lt;/strong&gt;: Web browsing and comprehension tasks. (&lt;a href="https://arxiv.org/abs/2504.12516"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;BrowseComp-ZH&lt;/strong&gt;: A Chinese version of BrowseComp. (&lt;a href="https://arxiv.org/abs/2504.19314"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;WebWalkerQA&lt;/strong&gt;: Web navigation and question answering. (&lt;a href="https://arxiv.org/abs/2501.07572"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Frames&lt;/strong&gt;: Factuality, Retrieval, And reasoning MEasurement Set. (&lt;a href="https://arxiv.org/abs/2409.12941"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;XBench-DeepSearch&lt;/strong&gt;: A benchmark for deep research agents. (&lt;a href="https://xbench.org/agi/aisearch"&gt;website&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;FutureX&lt;/strong&gt;: A live benchmark designed for predicting unknown future. (&lt;a href="https://futurex-ai.github.io/"&gt;website&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;SEAL-0&lt;/strong&gt;: A benchmark for evaluating LLMs on conflicting-evidence web questions. (&lt;a href="https://arxiv.org/abs/2506.01062"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;AIME2025&lt;/strong&gt;: American Invitational Mathematics Examination 2025. (&lt;a href="https://artificialanalysis.ai/evaluations/aime-2025"&gt;website&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;DeepSearchQA&lt;/strong&gt;: Google's Deep Search Question Answering benchmark. (&lt;a href="https://arxiv.org/abs/2505.20827"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;üìà Performance on Benchmarks&lt;/h2&gt; 
&lt;h3&gt;MiroThinker-v1.5&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;To prevent potential information leakage (e.g., searching benchmark answers from HuggingFace), access to HuggingFace has been explicitly disabled in these tools.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;We further perform canary string testing on the tool outputs of all trajectories and disregard any trajectory found to be contaminated, treating it as an incorrect answer.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;div&gt; 
 &lt;img src="https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/mirothinker_v1.5_performance.png" width="100%" alt="MiroThinker" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;MiroThinker-v1.0&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;üì¶ Click to expand MiroThinker-v1.0 details&lt;/summary&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://github.com/user-attachments/assets/108a2105-4e1d-499e-a001-4713a03fd8ac" width="100%" alt="MiroThinker" /&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;h3&gt;MiroThinker-v0.2&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;üì¶ Click to expand MiroThinker-v0.2 details&lt;/summary&gt; 
 &lt;h4&gt;Comparison with SOTA Research Agents&lt;/h4&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/MiroThinker_v0.2_Performance_2.png" width="90%" alt="MiroThinker" /&gt; 
 &lt;/div&gt; 
 &lt;h4&gt;GAIA Benchmark&lt;/h4&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/MiroThinker_v0.2_Performance_1.png" width="80%" alt="MiroThinker" /&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;h3&gt;MiroThinker-v0.1&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;üì¶ Click to expand MiroThinker-v0.1 details&lt;/summary&gt; 
 &lt;h4&gt;GAIA Benchmark&lt;/h4&gt; 
 &lt;div align="center"&gt; 
  &lt;table&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th&gt;&lt;strong&gt;Method&lt;/strong&gt;&lt;/th&gt; 
     &lt;th align="center"&gt;Text-103&lt;br /&gt;Best Pass@1&lt;/th&gt; 
     &lt;th align="center"&gt;Text-103&lt;br /&gt;Pass@1 (Avg@8)&lt;/th&gt; 
     &lt;th align="center"&gt;Val-165&lt;br /&gt;Best Pass@1&lt;/th&gt; 
     &lt;th align="center"&gt;Val-165&lt;br /&gt;Pass@1 (Avg@8)&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;üîπ‚Äî‚Äî 7B/8B Models ‚Äî‚Äî&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;Search-o1-7B&lt;/td&gt; 
     &lt;td align="center"&gt;17.5&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;R1-Searcher-7B&lt;/td&gt; 
     &lt;td align="center"&gt;20.4&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebDancer-7B&lt;/td&gt; 
     &lt;td align="center"&gt;31.0&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebSailor-7B&lt;/td&gt; 
     &lt;td align="center"&gt;37.9&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;CK-Pro-8B&lt;/td&gt; 
     &lt;td align="center"&gt;40.3&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;32.7&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-8B-SFT-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;44.7&lt;/td&gt; 
     &lt;td align="center"&gt;40.1&lt;/td&gt; 
     &lt;td align="center"&gt;34.6&lt;/td&gt; 
     &lt;td align="center"&gt;31.8&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;+ Commercial Tools&lt;/td&gt; 
     &lt;td align="center"&gt;46.6&lt;/td&gt; 
     &lt;td align="center"&gt;42.1&lt;/td&gt; 
     &lt;td align="center"&gt;37.6&lt;/td&gt; 
     &lt;td align="center"&gt;33.9&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-8B-DPO-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;46.6&lt;/td&gt; 
     &lt;td align="center"&gt;44.8&lt;/td&gt; 
     &lt;td align="center"&gt;37.0&lt;/td&gt; 
     &lt;td align="center"&gt;35.4&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;+ Commercial Tools&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;50.5&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;46.7&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;38.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;35.9&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;üîπ‚Äî‚Äî 14B Models ‚Äî‚Äî&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-14B-SFT-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;47.6&lt;/td&gt; 
     &lt;td align="center"&gt;44.4&lt;/td&gt; 
     &lt;td align="center"&gt;37.0&lt;/td&gt; 
     &lt;td align="center"&gt;34.4&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;+ Commercial Tools&lt;/td&gt; 
     &lt;td align="center"&gt;49.5&lt;/td&gt; 
     &lt;td align="center"&gt;47.5&lt;/td&gt; 
     &lt;td align="center"&gt;41.8&lt;/td&gt; 
     &lt;td align="center"&gt;39.8&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-14B-DPO-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;48.5&lt;/td&gt; 
     &lt;td align="center"&gt;46.6&lt;/td&gt; 
     &lt;td align="center"&gt;42.4&lt;/td&gt; 
     &lt;td align="center"&gt;39.2&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;+ Commercial Tools&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;52.4&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;48.5&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;45.5&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;42.0&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;üîπ‚Äî‚Äî 32B Models ‚Äî‚Äî&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;Qwen3-32B&lt;/td&gt; 
     &lt;td align="center"&gt;31.1&lt;/td&gt; 
     &lt;td align="center"&gt;26.7&lt;/td&gt; 
     &lt;td align="center"&gt;29.7&lt;/td&gt; 
     &lt;td align="center"&gt;26.4&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;Search-o1-32B&lt;/td&gt; 
     &lt;td align="center"&gt;28.2&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebThinker-32B-RL&lt;/td&gt; 
     &lt;td align="center"&gt;48.5&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebDancer-QwQ-32B&lt;/td&gt; 
     &lt;td align="center"&gt;51.5&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebSailor-32B&lt;/td&gt; 
     &lt;td align="center"&gt;53.2&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebShaper-QwQ-32B&lt;/td&gt; 
     &lt;td align="center"&gt;53.3&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-32B-SFT-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;55.3&lt;/td&gt; 
     &lt;td align="center"&gt;51.3&lt;/td&gt; 
     &lt;td align="center"&gt;44.9&lt;/td&gt; 
     &lt;td align="center"&gt;42.7&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;+ Commercial Tools&lt;/td&gt; 
     &lt;td align="center"&gt;58.3&lt;/td&gt; 
     &lt;td align="center"&gt;54.2&lt;/td&gt; 
     &lt;td align="center"&gt;48.5&lt;/td&gt; 
     &lt;td align="center"&gt;45.8&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-32B-DPO-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;57.3&lt;/td&gt; 
     &lt;td align="center"&gt;54.1&lt;/td&gt; 
     &lt;td align="center"&gt;48.5&lt;/td&gt; 
     &lt;td align="center"&gt;45.9&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;+ Commercial Tools&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;60.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;57.9&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;50.9&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;48.9&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;Following the practices of WebThinker, WebAgents, and CognitiveKernel, we report the Best Pass@1, the highest score across three runs, which often reflects stronger performance, though it may exhibit some variability. To provide a more stable measure, we additionally report Pass@1 (Avg@8), which offers greater consistency at the cost of slightly lower scores.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;For consistency with prior open-source works, we evaluate GAIA-Text-103 using the WebAgents LLM-as-a-Judge template, and report results on GAIA-Val-165 using the official GAIA scorer script.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;By default, we use open-source tools wherever possible, except for the code tool &lt;a href="https://github.com/e2b-dev/E2B"&gt;E2B&lt;/a&gt; and the Google search tool &lt;a href="https://serper.dev/"&gt;Serper&lt;/a&gt;. We use &lt;a href="https://huggingface.co/openai/whisper-large-v3-turbo"&gt;Whisper&lt;/a&gt;, &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct"&gt;Qwen2.5-VL-72B-Instruct&lt;/a&gt;, and &lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507"&gt;Qwen3-235B-A22B-Thinking-2507&lt;/a&gt; in our implementation. The framework can be easily extended to other open-source tools of your choice.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Replacing these open-source tools with commercial alternatives can yield performance gains. Commercial tools were mainly used for multimodal capabilities and certain complex reasoning subtasks. The majority of tasks, including planning, browsing, refinement, navigation, and more, were handled by our models.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h4&gt;More Benchmarks&lt;/h4&gt; 
 &lt;div align="center"&gt; 
  &lt;table&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th&gt;Method&lt;/th&gt; 
     &lt;th align="center"&gt;HLE&lt;br /&gt;Pass@1&lt;/th&gt; 
     &lt;th align="center"&gt;Frames&lt;br /&gt;Pass@1&lt;/th&gt; 
     &lt;th align="center"&gt;BrowseComp&lt;br /&gt;Pass@1&lt;/th&gt; 
     &lt;th align="center"&gt;BrowseComp-ZH&lt;br /&gt;Pass@1&lt;/th&gt; 
     &lt;th align="center"&gt;WebWalkerQA&lt;br /&gt;Pass@1&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody&gt; 
    &lt;tr&gt; 
     &lt;td&gt;OpenAI Deep Research&lt;/td&gt; 
     &lt;td align="center"&gt;26.6&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;51.5&lt;/td&gt; 
     &lt;td align="center"&gt;42.9&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;Gemini Deep Research&lt;/td&gt; 
     &lt;td align="center"&gt;26.9&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;Kimi-Researcher&lt;/td&gt; 
     &lt;td align="center"&gt;26.9&lt;/td&gt; 
     &lt;td align="center"&gt;78.8&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebDancer-7B&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;36.0&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebSailor-7B&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;6.7&lt;/td&gt; 
     &lt;td align="center"&gt;14.2&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-8B-SFT-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;58.0&lt;/td&gt; 
     &lt;td align="center"&gt;5.5&lt;/td&gt; 
     &lt;td align="center"&gt;9.3&lt;/td&gt; 
     &lt;td align="center"&gt;41.3&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-8B-DPO-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;64.4&lt;/td&gt; 
     &lt;td align="center"&gt;8.7&lt;/td&gt; 
     &lt;td align="center"&gt;13.6&lt;/td&gt; 
     &lt;td align="center"&gt;45.7&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebThinker-32B-RL&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;46.5&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebDancer-QwQ-32B&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;3.8&lt;/td&gt; 
     &lt;td align="center"&gt;18.0&lt;/td&gt; 
     &lt;td align="center"&gt;47.9&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebSailor-32B&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;10.5&lt;/td&gt; 
     &lt;td align="center"&gt;25.5&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebShaper-32B&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;51.4&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-32B-SFT-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;10.2&lt;/td&gt; 
     &lt;td align="center"&gt;70.4&lt;/td&gt; 
     &lt;td align="center"&gt;10.6&lt;/td&gt; 
     &lt;td align="center"&gt;13.8&lt;/td&gt; 
     &lt;td align="center"&gt;45.7&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-32B-DPO-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;11.8&lt;/td&gt; 
     &lt;td align="center"&gt;71.7&lt;/td&gt; 
     &lt;td align="center"&gt;13.0&lt;/td&gt; 
     &lt;td align="center"&gt;17.0&lt;/td&gt; 
     &lt;td align="center"&gt;49.3&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;MiroThinker‚Äôs performance was tested with this repository and open-source tools; other models‚Äô results are from their papers and official sites.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;As &lt;a href="https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.1"&gt;MiroVerse-v0.1&lt;/a&gt; mainly contains English data, the model‚Äôs Chinese capability is limited. We plan to add more Chinese data to improve performance in the next version.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üêç &lt;strong&gt;Python 3.10+&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;üì¶ &lt;strong&gt;uv package manager&lt;/strong&gt; (&lt;a href="https://github.com/astral-sh/uv"&gt;Installation guide&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;üîë &lt;strong&gt;Required API keys&lt;/strong&gt; (see configuration section below)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the repository
git clone https://github.com/MiroMindAI/MiroThinker
cd MiroThinker

# Setup environment
cd apps/miroflow-agent
uv sync

# Configure API keys
cp .env.example .env
# Edit .env with your API keys (SERPER_API_KEY, JINA_API_KEY, E2B_API_KEY, etc.)
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üìù Environment Variables&lt;/strong&gt;: See &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#tool-configuration"&gt;Tool Configuration&lt;/a&gt; section for required API keys.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Tool Configuration&lt;/h3&gt; 
&lt;h4&gt;Minimal Configuration for MiroThinker v1.5 and v1.0&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Server&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;th align="left"&gt;Tools Provided&lt;/th&gt; 
   &lt;th align="left"&gt;Required Environment Variables&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;tool-python&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Execution environment and file management (E2B sandbox)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;create_sandbox&lt;/code&gt;, &lt;code&gt;run_command&lt;/code&gt;, &lt;code&gt;run_python_code&lt;/code&gt;, &lt;code&gt;upload_file_from_local_to_sandbox&lt;/code&gt;, &lt;code&gt;download_file_from_sandbox_to_local&lt;/code&gt;, &lt;code&gt;download_file_from_internet_to_sandbox&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;E2B_API_KEY&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;search_and_scrape_webpage&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Google search via Serper API&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;google_search&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;SERPER_API_KEY&lt;/code&gt;, &lt;code&gt;SERPER_BASE_URL&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;jina_scrape_llm_summary&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Web scraping with LLM-based information extraction&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;scrape_and_extract_info&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;JINA_API_KEY&lt;/code&gt;, &lt;code&gt;JINA_BASE_URL&lt;/code&gt;, &lt;code&gt;SUMMARY_LLM_BASE_URL&lt;/code&gt;, &lt;code&gt;SUMMARY_LLM_MODEL_NAME&lt;/code&gt;, &lt;code&gt;SUMMARY_LLM_API_KEY&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Minimal &lt;code&gt;.env&lt;/code&gt; configuration example:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Required for MiroThinker v1.5 and v1.0 (minimal setup)
SERPER_API_KEY=your_serper_key
SERPER_BASE_URL="https://google.serper.dev"
JINA_API_KEY=your_jina_key
JINA_BASE_URL="https://r.jina.ai"
E2B_API_KEY=your_e2b_key

# Required for jina_scrape_llm_summary
# Note: Summary LLM can be a small model (e.g., Qwen3-14B or GPT-5-Nano)
# The choice has minimal impact on performance, use what's most convenient
SUMMARY_LLM_BASE_URL="https://your_summary_llm_base_url/v1/chat/completions"
SUMMARY_LLM_MODEL_NAME=your_llm_model_name  # e.g., "Qwen/Qwen3-14B" or "gpt-5-nano"
SUMMARY_LLM_API_KEY=your_llm_api_key  # Optional, depends on LLM provider

# Required for benchmark evaluation (LLM-as-a-Judge)
OPENAI_API_KEY=your_openai_key  # Required for running benchmark evaluations
OPENAI_BASE_URL="https://api.openai.com/v1"  # Optional, defaults to OpenAI's API
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üí° Why this is minimal&lt;/strong&gt;: These 3 MCP servers cover the core capabilities needed for research tasks: web search, content extraction, and code execution. All other servers are optional enhancements.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;ü§ñ Summary LLM&lt;/strong&gt;: The &lt;code&gt;SUMMARY_LLM&lt;/code&gt; can be a small model like Qwen3-14B or GPT-5-Nano. The choice has minimal impact on overall performance, use whichever is most convenient for your setup.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;üìä For Benchmark Evaluation&lt;/strong&gt;: If you plan to run benchmark evaluations, you also need &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; (and optionally &lt;code&gt;OPENAI_BASE_URL&lt;/code&gt;) for LLM-as-a-Judge functionality used in evaluation scripts.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;üñºÔ∏è For GAIA Multimodal Tasks&lt;/strong&gt;: GAIA-Val-165 includes tasks with image/audio/video files. Since MiroThinker is a text-only LLM, GPT-4o is used to pre-process these files into text descriptions. The same &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; is used for both this preprocessing and LLM-as-a-Judge.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;üìñ For more details&lt;/strong&gt;: See &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/libs/miroflow-tools/README.md"&gt;MiroFlow Tools README&lt;/a&gt; for complete documentation of all available tools.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details&gt; 
 &lt;summary&gt;üîß Click to expand additional available tools&lt;/summary&gt; 
 &lt;p&gt;The following optional tools are available but were not used in MiroThinker v1.5 and v1.0 evaluation:&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;Server Name&lt;/th&gt; 
    &lt;th align="left"&gt;Type&lt;/th&gt; 
    &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-vqa&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Commercial&lt;/td&gt; 
    &lt;td align="left"&gt;Vision processing using Claude&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-vqa-os&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Open-Source&lt;/td&gt; 
    &lt;td align="left"&gt;Vision processing (open-source alternative)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-transcribe&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Commercial&lt;/td&gt; 
    &lt;td align="left"&gt;Audio transcription using OpenAI&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-transcribe-os&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Open-Source&lt;/td&gt; 
    &lt;td align="left"&gt;Audio transcription using Whisper&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-reasoning&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Commercial&lt;/td&gt; 
    &lt;td align="left"&gt;Reasoning engine using Claude&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-reasoning-os&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Open-Source&lt;/td&gt; 
    &lt;td align="left"&gt;Reasoning engine (open-source alternative)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-reading&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Open-Source&lt;/td&gt; 
    &lt;td align="left"&gt;Document reading using MarkItDown&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-google-search&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Commercial&lt;/td&gt; 
    &lt;td align="left"&gt;Web search using Google + scraping&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-sogou-search&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Commercial&lt;/td&gt; 
    &lt;td align="left"&gt;Web search using Sogou (Chinese)&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;üìñ Local Deployment&lt;/strong&gt;: For instructions on deploying open-source tools (&lt;code&gt;tool-vqa-os&lt;/code&gt;, &lt;code&gt;tool-transcribe-os&lt;/code&gt;, &lt;code&gt;tool-reasoning-os&lt;/code&gt;) locally, see &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/assets/LOCAL-TOOL-DEPLOYMENT.md"&gt;Local Tool Deployment Guide&lt;/a&gt;.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/libs/miroflow-tools/README.md"&gt;MiroFlow Tools README&lt;/a&gt; for complete documentation of all available tools.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h4&gt;Pre-configured Agent Settings&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;apps/miroflow-agent/conf/agent/&lt;/code&gt; directory contains several pre-configured agent settings. Each configuration uses different tools and requires corresponding environment variables in your &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üí° Recommended&lt;/strong&gt;: For MiroThinker v1.5, use &lt;code&gt;mirothinker_v1.5_keep5_max200&lt;/code&gt; (with context management, recommended for most tasks) or &lt;code&gt;mirothinker_v1.5_keep5_max400&lt;/code&gt; (only used for BrowseComp and BrowseComp-ZH). For v1.0, use &lt;code&gt;mirothinker_v1.0_keep5&lt;/code&gt; (with context management). All use minimal configuration with only 3 MCP servers.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Configuration&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;th align="left"&gt;Max Turns&lt;/th&gt; 
   &lt;th align="left"&gt;Context Retention&lt;/th&gt; 
   &lt;th align="left"&gt;Required Environment Variables&lt;/th&gt; 
   &lt;th align="left"&gt;Recommended For&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;mirothinker_v1.5_keep5_max200&lt;/code&gt;&lt;/strong&gt; ‚≠ê&lt;/td&gt; 
   &lt;td align="left"&gt;Single-agent with context management&lt;/td&gt; 
   &lt;td align="left"&gt;200&lt;/td&gt; 
   &lt;td align="left"&gt;Keep 5 most recent&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;SERPER_API_KEY&lt;/code&gt;, &lt;code&gt;SERPER_BASE_URL&lt;/code&gt;, &lt;code&gt;JINA_API_KEY&lt;/code&gt;, &lt;code&gt;JINA_BASE_URL&lt;/code&gt;, &lt;code&gt;E2B_API_KEY&lt;/code&gt;, &lt;code&gt;SUMMARY_LLM_BASE_URL&lt;/code&gt;, &lt;code&gt;SUMMARY_LLM_MODEL_NAME&lt;/code&gt;, &lt;code&gt;SUMMARY_LLM_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;v1.5 (recommended for most tasks)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;mirothinker_v1.5_keep5_max400&lt;/code&gt;&lt;/strong&gt; ‚≠ê&lt;/td&gt; 
   &lt;td align="left"&gt;Single-agent with context management&lt;/td&gt; 
   &lt;td align="left"&gt;400&lt;/td&gt; 
   &lt;td align="left"&gt;Keep 5 most recent&lt;/td&gt; 
   &lt;td align="left"&gt;Same as above&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;v1.5 (for BrowseComp &amp;amp; BrowseComp-ZH)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;mirothinker_v1.5&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Single-agent for MiroThinker v1.5&lt;/td&gt; 
   &lt;td align="left"&gt;600&lt;/td&gt; 
   &lt;td align="left"&gt;Keep all results&lt;/td&gt; 
   &lt;td align="left"&gt;Same as above&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;v1.5&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;mirothinker_v1.0_keep5&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Single-agent with context management&lt;/td&gt; 
   &lt;td align="left"&gt;600&lt;/td&gt; 
   &lt;td align="left"&gt;Keep 5 most recent&lt;/td&gt; 
   &lt;td align="left"&gt;Same as above&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;v1.0&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;mirothinker_v1.0&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Single-agent for MiroThinker v1.0&lt;/td&gt; 
   &lt;td align="left"&gt;600&lt;/td&gt; 
   &lt;td align="left"&gt;Keep all results&lt;/td&gt; 
   &lt;td align="left"&gt;Same as above&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;v1.0&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;details&gt; 
 &lt;summary&gt;üì¶ Click to expand legacy configurations (v0.1/v0.2)&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;Configuration&lt;/th&gt; 
    &lt;th align="left"&gt;Description&lt;/th&gt; 
    &lt;th align="left"&gt;Max Turns&lt;/th&gt; 
    &lt;th align="left"&gt;Context Retention&lt;/th&gt; 
    &lt;th align="left"&gt;Required Environment Variables&lt;/th&gt; 
    &lt;th align="left"&gt;Recommended For&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;multi_agent&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Multi-agent with commercial tools (v0.1/v0.2)&lt;/td&gt; 
    &lt;td align="left"&gt;50&lt;/td&gt; 
    &lt;td align="left"&gt;Keep all results&lt;/td&gt; 
    &lt;td align="left"&gt;&lt;code&gt;E2B_API_KEY&lt;/code&gt;, &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;, &lt;code&gt;ANTHROPIC_BASE_URL&lt;/code&gt;, &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;, &lt;code&gt;OPENAI_BASE_URL&lt;/code&gt;, &lt;code&gt;SERPER_API_KEY&lt;/code&gt;, &lt;code&gt;SERPER_BASE_URL&lt;/code&gt;, &lt;code&gt;JINA_API_KEY&lt;/code&gt;, &lt;code&gt;JINA_BASE_URL&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;v0.1/v0.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;multi_agent_os&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Multi-agent with open-source tools (v0.1/v0.2)&lt;/td&gt; 
    &lt;td align="left"&gt;50&lt;/td&gt; 
    &lt;td align="left"&gt;Keep all results&lt;/td&gt; 
    &lt;td align="left"&gt;&lt;code&gt;E2B_API_KEY&lt;/code&gt;, &lt;code&gt;VISION_API_KEY&lt;/code&gt;, &lt;code&gt;VISION_BASE_URL&lt;/code&gt;, &lt;code&gt;VISION_MODEL_NAME&lt;/code&gt;, &lt;code&gt;WHISPER_API_KEY&lt;/code&gt;, &lt;code&gt;WHISPER_BASE_URL&lt;/code&gt;, &lt;code&gt;WHISPER_MODEL_NAME&lt;/code&gt;, &lt;code&gt;REASONING_API_KEY&lt;/code&gt;, &lt;code&gt;REASONING_BASE_URL&lt;/code&gt;, &lt;code&gt;REASONING_MODEL_NAME&lt;/code&gt;, &lt;code&gt;SERPER_API_KEY&lt;/code&gt;, &lt;code&gt;SERPER_BASE_URL&lt;/code&gt;, &lt;code&gt;JINA_API_KEY&lt;/code&gt;, &lt;code&gt;JINA_BASE_URL&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;v0.1/v0.2&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üí° Note&lt;/strong&gt;: All environment variables are listed in &lt;code&gt;apps/miroflow-agent/.env.example&lt;/code&gt;. Copy it to &lt;code&gt;.env&lt;/code&gt; and fill in the values for the tools you plan to use.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Creating Custom Tool Configurations&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;üîß Click to expand custom tool configuration guide&lt;/summary&gt; 
 &lt;p&gt;You can create your own YAML configuration file to freely combine MCP servers. Here's how:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Create a new YAML file&lt;/strong&gt; in &lt;code&gt;apps/miroflow-agent/conf/agent/&lt;/code&gt;:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;# conf/agent/my_custom_config.yaml
defaults:
  - default
  - _self_

main_agent:
  tools:
    - tool-python                    # Execution environment
    - search_and_scrape_webpage      # Google search
    - jina_scrape_llm_summary        # Web scraping with LLM
    - tool-vqa                       # Vision processing (optional)
    - tool-transcribe                # Audio processing (optional)
    - tool-reasoning                 # Reasoning engine (optional)
    - tool-reading                   # Document reading (optional)
  max_turns: 400  # Maximum number of turns

sub_agents:
  agent-browsing:  # Optional sub-agent
    tools:
      - tool-google-search
      - tool-vqa
      - tool-reading
      - tool-python
    max_turns: 50

keep_tool_result: -1  # Context retention budget: -1 keeps all tool results, or specify K to keep only the K most recent tool responses
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;üí° Context Retention Strategy&lt;/strong&gt;: The &lt;code&gt;keep_tool_result&lt;/code&gt; parameter implements a &lt;strong&gt;recency-based context retention&lt;/strong&gt; strategy. In the standard ReAct paradigm, all tool outputs are retained in the message history, which can lead to inefficient context utilization. Empirically, we observe that the model's subsequent actions depend primarily on recent observations rather than distant ones. This strategy retains only the most recent K tool responses (where K is the &lt;code&gt;keep_tool_result&lt;/code&gt; value) while preserving the complete sequence of thoughts and actions.&lt;/p&gt; 
  &lt;p&gt;&lt;strong&gt;Benefits:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;‚úÖ Preserves the reasoning and action trace&lt;/li&gt; 
   &lt;li&gt;‚úÖ Focuses the model's attention on the most contextually relevant observations&lt;/li&gt; 
   &lt;li&gt;‚úÖ Frees additional context space for extended reasoning and deeper tool-use trajectories&lt;/li&gt; 
   &lt;li&gt;‚úÖ Does not lead to performance degradation while allowing more context space for interactive scaling&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;&lt;strong&gt;Usage:&lt;/strong&gt; Set &lt;code&gt;keep_tool_result: -1&lt;/code&gt; to keep all tool results, or specify a positive integer K (e.g., &lt;code&gt;keep_tool_result: 5&lt;/code&gt;) to keep only the K most recent tool responses.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;&lt;strong&gt;Use your custom configuration&lt;/strong&gt; when running evaluations:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;cd apps/miroflow-agent
uv run main.py llm=qwen-3 agent=my_custom_config llm.base_url=https://your_base_url/v1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="3"&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configure environment variables&lt;/strong&gt; in &lt;code&gt;.env&lt;/code&gt; based on the tools you use.&lt;/p&gt; &lt;p&gt;All available environment variables are listed in &lt;code&gt;apps/miroflow-agent/.env.example&lt;/code&gt;. Copy it to &lt;code&gt;.env&lt;/code&gt; and configure the variables according to your chosen configuration:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd apps/miroflow-agent
cp .env.example .env
# Edit .env with your actual API keys
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;For MiroThinker v1.5&lt;/strong&gt; (&lt;code&gt;mirothinker_v1.5_keep5_max200.yaml&lt;/code&gt;, &lt;code&gt;mirothinker_v1.5_keep5_max400.yaml&lt;/code&gt;, or &lt;code&gt;mirothinker_v1.5.yaml&lt;/code&gt;) and &lt;strong&gt;v1.0&lt;/strong&gt; (&lt;code&gt;mirothinker_v1.0_keep5.yaml&lt;/code&gt; or &lt;code&gt;mirothinker_v1.0.yaml&lt;/code&gt;), see the &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#minimal-configuration-for-mirothinker-v15-and-v10"&gt;Minimal Configuration&lt;/a&gt; section above for the complete configuration example.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For other configurations&lt;/strong&gt;, refer to the &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#pre-configured-agent-settings"&gt;Pre-configured Agent Settings&lt;/a&gt; table above to see which environment variables are required.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;üîë Click to expand optional API keys&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# API for LLM-as-a-Judge (for benchmark testing, required for benchmark evaluation)
OPENAI_API_KEY=your_openai_key
OPENAI_BASE_URL="https://api.openai.com/v1"  # Optional, defaults to OpenAI's API

# API for Open-Source Audio Transcription Tool (for benchmark testing, optional)
WHISPER_MODEL_NAME="openai/whisper-large-v3-turbo"
WHISPER_API_KEY=your_whisper_key
WHISPER_BASE_URL="https://your_whisper_base_url/v1"

# API for Open-Source VQA Tool (for benchmark testing, optional)
VISION_MODEL_NAME="Qwen/Qwen2.5-VL-72B-Instruct"
VISION_API_KEY=your_vision_key
VISION_BASE_URL="https://your_vision_base_url/v1/chat/completions"

# API for Open-Source Reasoning Tool (for benchmark testing, optional)
REASONING_MODEL_NAME="Qwen/Qwen3-235B-A22B-Thinking-2507"
REASONING_API_KEY=your_reasoning_key
REASONING_BASE_URL="https://your_reasoning_base_url/v1/chat/completions"

# API for Claude Sonnet 3.7 as Commercial Tools (optional)
ANTHROPIC_API_KEY=your_anthropic_key

# API for Sogou Search (optional)
TENCENTCLOUD_SECRET_ID=your_tencent_cloud_secret_id
TENCENTCLOUD_SECRET_KEY=your_tencent_cloud_secret_key

# API for Summary LLM (can use small models like Qwen3-14B or GPT-5-Nano)
SUMMARY_LLM_BASE_URL="https://your_summary_llm_base_url/v1/chat/completions"
SUMMARY_LLM_MODEL_NAME=your_summary_llm_model_name  # e.g., "Qwen/Qwen3-14B" or "gpt-5-nano"
SUMMARY_LLM_API_KEY=your_summary_llm_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Serve the MiroThinker Model&lt;/h3&gt; 
&lt;h4&gt;Option 1 (Recommended): Serve with SGLang or vLLM&lt;/h4&gt; 
&lt;p&gt;Use SGLang to serve MiroThinker models at port 61002:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;NUM_GPUS=4
PORT=61002

# Downloading model from HF (v1.5 recommended)
MODEL_PATH=miromind-ai/MiroThinker-v1.5-30B

# Or use v1.0
# MODEL_PATH=miromind-ai/MiroThinker-v1.0-30B

python3 -m sglang.launch_server \
    --model-path $MODEL_PATH \
    --tp $NUM_GPUS \
    --dp 1 \
    --host 0.0.0.0 \
    --port $PORT \
    --trust-remote-code
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üìç Server URL&lt;/strong&gt;: This will start a server at &lt;code&gt;http://0.0.0.0:$PORT&lt;/code&gt;. Use this as your server base URL (e.g., &lt;code&gt;http://0.0.0.0:61002/v1&lt;/code&gt;).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Option 2: Quantized Light-Weight Options&lt;/h4&gt; 
&lt;p&gt;We also provide comprehensive guidance for serving MiroThinker models using CPU-optimized and GPU-accelerated quantization techniques, along with detailed analysis and guidelines for deployment with llama.cpp, Ollama, SGLang, and other inference frameworks.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üìñ Complete Guide&lt;/strong&gt;: See &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/apps/gradio-demo/"&gt;Deployment Documentation&lt;/a&gt; for detailed deployment instructions.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Run Your First Task&lt;/h3&gt; 
&lt;p&gt;After setting up the environment and starting your model server, run &lt;code&gt;main.py&lt;/code&gt; to test with a default question: &lt;em&gt;"What is the title of today's arxiv paper in computer science?"&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd apps/miroflow-agent

# Using MiroThinker models (requires your own model server)
uv run python main.py llm=qwen-3 agent=mirothinker_v1.5_keep5_max200 llm.base_url=http://localhost:61002/v1

# Or using Claude (requires ANTHROPIC_API_KEY in .env)
uv run python main.py llm=claude-3-7 agent=single_agent_keep5

# Or using GPT-5 (requires OPENAI_API_KEY in .env)
uv run python main.py llm=gpt-5 agent=single_agent_keep5
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;To customize your question&lt;/strong&gt;, edit &lt;code&gt;main.py&lt;/code&gt; line 32:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;task_description = "Your custom question here"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The agent will search the web, execute code if needed, and provide an answer with sources.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üìñ More details&lt;/strong&gt;: See &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/apps/miroflow-agent/README.md"&gt;apps/miroflow-agent/README.md&lt;/a&gt; for available configurations and troubleshooting.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;üìä Benchmark Evaluation&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For researchers who want to reproduce our benchmark results or evaluate on standard benchmarks.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Download Benchmark Data&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd MiroThinker  # Back to project root
wget https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/data_20251115_password_protected.zip
unzip data_20251115_password_protected.zip
# Password: pf4*
rm data_20251115_password_protected.zip
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Run Benchmark Evaluation&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For MiroThinker v1.5, use &lt;code&gt;mirothinker_v1.5_keep5_max200&lt;/code&gt; (with context management), &lt;code&gt;mirothinker_v1.5_keep5_max400&lt;/code&gt; (with context management), or &lt;code&gt;mirothinker_v1.5&lt;/code&gt; configurations. For v1.0, use &lt;code&gt;mirothinker_v1.0_keep5&lt;/code&gt; (with context management) or &lt;code&gt;mirothinker_v1.0&lt;/code&gt; configurations.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Available Parameters:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You can customize the evaluation by setting the following environment variables before running the script:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Parameter&lt;/th&gt; 
   &lt;th align="left"&gt;Default&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;LLM_MODEL&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;"MiroThinker-Models"&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Model name identifier&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BASE_URL&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;"https://your-api.com/v1"&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Base URL of your model server&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;NUM_RUNS&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Varies by benchmark&lt;/td&gt; 
   &lt;td align="left"&gt;Number of evaluation runs (3 for most benchmarks, 8 for GAIA/XBench/FutureX/SEAL-0, 32 for AIME2025)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;LLM_PROVIDER&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;"qwen"&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;LLM provider (e.g., &lt;code&gt;qwen&lt;/code&gt;, &lt;code&gt;openai&lt;/code&gt;, &lt;code&gt;anthropic&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;AGENT_SET&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;"mirothinker_v1.5_keep5_max200"&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Agent configuration (e.g., &lt;code&gt;mirothinker_v1.5_keep5_max200&lt;/code&gt;, &lt;code&gt;mirothinker_v1.5_keep5_max400&lt;/code&gt;, &lt;code&gt;mirothinker_v1.0_keep5&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;MAX_CONTEXT_LENGTH&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;262144&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Maximum context length (256K)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;MAX_CONCURRENT&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;10&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Maximum concurrent tasks&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;PASS_AT_K&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Pass@K evaluation metric&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;TEMPERATURE&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;1.0&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Sampling temperature&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;"xxx"&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;API key for the model server&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Example Usage:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Navigate to the miroflow-agent directory first
cd apps/miroflow-agent

# Basic usage with v1.5 (recommended)
NUM_RUNS=8 LLM_MODEL="MiroThinker-v1.5-30B" BASE_URL="https://your-api.com/v1" bash scripts/run_evaluate_multiple_runs_gaia-validation-text-103.sh

# Or with v1.0
# NUM_RUNS=8 LLM_MODEL="MiroThinker-v1.0-30B" BASE_URL="https://your-api.com/v1" bash scripts/run_evaluate_multiple_runs_gaia-validation-text-103.sh

# Customize number of runs and agent configuration (v1.5 with context management)
LLM_MODEL="MiroThinker-v1.5-30B" \
BASE_URL="https://your-api.com/v1" \
NUM_RUNS=8 \
AGENT_SET="mirothinker_v1.5_keep5_max200" \
bash scripts/run_evaluate_multiple_runs_gaia-validation-text-103.sh

# Or with v1.0 configuration (with context management)
# LLM_MODEL="MiroThinker-v1.0-30B" \
# BASE_URL="https://your-api.com/v1" \
# NUM_RUNS=8 \
# AGENT_SET="mirothinker_v1.0_keep5" \
# bash scripts/run_evaluate_multiple_runs_gaia-validation-text-103.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;details open&gt; 
 &lt;summary&gt;üìã Click to expand all benchmark commands&lt;/summary&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;‚ö†Ô∏è Important for MiroThinker v1.5&lt;/strong&gt;: To reproduce our reported results, you must set the correct &lt;code&gt;AGENT_SET&lt;/code&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;BrowseComp &amp;amp; BrowseComp-ZH&lt;/strong&gt;: Use &lt;code&gt;AGENT_SET="mirothinker_v1.5_keep5_max400"&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;All other benchmarks&lt;/strong&gt;: Use &lt;code&gt;AGENT_SET="mirothinker_v1.5_keep5_max200"&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/blockquote&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Navigate to the miroflow-agent directory first
cd apps/miroflow-agent

# HLE
NUM_RUNS=3 LLM_MODEL="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_hle.sh

# HLE-Text-2158
NUM_RUNS=3 LLM_MODEL="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_hle-text-2158.sh

# HLE-Text-500
NUM_RUNS=3 LLM_MODEL="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_hle-text-500.sh

# GAIA-Text-103
NUM_RUNS=8 LLM_MODEL="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_gaia-validation-text-103.sh

# GAIA-Validation (GAIA-Val-165)
NUM_RUNS=8 LLM_MODEL="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_gaia-validation.sh

# BrowseComp-EN (‚ö†Ô∏è use max400)
NUM_RUNS=3 LLM_MODEL="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max400" bash scripts/run_evaluate_multiple_runs_browsecomp.sh

# BrowseComp-ZH (‚ö†Ô∏è use max400)
NUM_RUNS=3 LLM_MODEL="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max400" bash scripts/run_evaluate_multiple_runs_browsecomp_zh.sh

# WebWalkerQA
NUM_RUNS=3 LLM_MODEL="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_webwalkerqa.sh

# XBench-DeepSearch
NUM_RUNS=8 LLM_MODEL="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_xbench_deepsearch.sh

# FRAMES
NUM_RUNS=3 LLM_MODEL="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_frames.sh

# SEAL-0
NUM_RUNS=8 LLM_MODEL="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_seal-0.sh

# FutureX
NUM_RUNS=8 LLM_MODEL="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_futurex.sh

# AIME2025
NUM_RUNS=32 LLM_MODEL="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_aime2025.sh

# DeepSearchQA
NUM_RUNS=3 LLM_MODEL="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_deepsearchqa.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;3. &lt;strong&gt;Monitor evaluation progress&lt;/strong&gt;&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;üìä Click to expand progress monitoring commands&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Navigate to the miroflow-agent directory first
cd apps/miroflow-agent

# For HLE
python benchmarks/check_progress/check_progress_hle.py /path/to/evaluation/logs

# For HLE-Text-2158
python benchmarks/check_progress/check_progress_hle-text-2158.py /path/to/evaluation/logs

# For HLE-Text-500
python benchmarks/check_progress/check_progress_hle-text-500.py /path/to/evaluation/logs

# For BrowseComp-EN
python benchmarks/check_progress/check_progress_browsecomp.py /path/to/evaluation/logs

# For BrowseComp-ZH
python benchmarks/check_progress/check_progress_browsecomp_zh.py /path/to/evaluation/logs

# For GAIA-Validation
python benchmarks/check_progress/check_progress_gaia-validation.py /path/to/evaluation/logs

# For GAIA-Text-103
python benchmarks/check_progress/check_progress_gaia-validation-text-103.py /path/to/evaluation/logs

# For WebWalkerQA
python benchmarks/check_progress/check_progress_webwalkerqa.py /path/to/evaluation/logs

# For Frames
python benchmarks/check_progress/check_progress_frames.py /path/to/evaluation/logs

# For XBench-DeepSearch
python benchmarks/check_progress/check_progress_xbench_deepsearch.py /path/to/evaluation/logs

# For SEAL-0
python benchmarks/check_progress/check_progress_seal-0.py /path/to/evaluation/logs

# For AIME2025
python benchmarks/check_progress/check_progress_aime2025.py /path/to/evaluation/logs

# For DeepSearchQA
python benchmarks/check_progress/check_progress_deepsearchqa.py /path/to/evaluation/logs
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;üî¨ Trace Collection&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;üìã Click to expand trace collection commands&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;cd apps/collect-trace

# Collect Traces for SFT
bash scripts/collect_trace_claude37.sh
bash scripts/collect_trace_gpt5.sh

# Collect Traces for DPO
bash scripts/collect_trace_qwen3.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚ùì FAQ &amp;amp; Troubleshooting&lt;/h2&gt; 
&lt;h3&gt;Common Issues&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;üîß Click to expand troubleshooting guide&lt;/summary&gt; 
 &lt;h4&gt;&lt;strong&gt;Q: Which version should I use?&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; We recommend &lt;strong&gt;MiroThinker v1.5&lt;/strong&gt; ‚≠ê with the minimal configuration:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;v1.5&lt;/strong&gt; ‚≠ê: Latest version with 256K context, world-leading performance. Use config (with context management): 
   &lt;ul&gt; 
    &lt;li&gt;&lt;code&gt;mirothinker_v1.5_keep5_max200&lt;/code&gt; (up to 200 turns, recommended for most tasks)&lt;/li&gt; 
    &lt;li&gt;&lt;code&gt;mirothinker_v1.5_keep5_max400&lt;/code&gt; (up to 400 turns, only used for BrowseComp and BrowseComp-ZH)&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;&lt;strong&gt;Q: How do I get API keys?&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; You need these keys for minimal setup:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;SERPER_API_KEY&lt;/strong&gt;: Get from &lt;a href="https://serper.dev/"&gt;Serper.dev&lt;/a&gt; (Google search API)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;JINA_API_KEY&lt;/strong&gt;: Get from &lt;a href="https://jina.ai/"&gt;Jina.ai&lt;/a&gt; (Web scraping)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;E2B_API_KEY&lt;/strong&gt;: Get from &lt;a href="https://e2b.dev/"&gt;E2B.dev&lt;/a&gt; (Code execution sandbox)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;SUMMARY_LLM_API_KEY&lt;/strong&gt;: Your LLM API credentials (for content summarization). Can be a small model like Qwen3-14B or GPT-5-Nano‚Äîthe choice has minimal impact on performance.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;OPENAI_API_KEY&lt;/strong&gt;: Get from &lt;a href="https://platform.openai.com/"&gt;OpenAI&lt;/a&gt; (Required for benchmark evaluation, used for LLM-as-a-Judge)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;OPENAI_BASE_URL&lt;/strong&gt;: Optional, defaults to &lt;code&gt;https://api.openai.com/v1&lt;/code&gt;. Can be changed to use OpenAI-compatible APIs.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;&lt;strong&gt;Q: Model server connection errors&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Common issues:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Check base URL format&lt;/strong&gt;: Should end with &lt;code&gt;/v1&lt;/code&gt; (e.g., &lt;code&gt;https://your-api.com/v1&lt;/code&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Verify API key&lt;/strong&gt;: Ensure &lt;code&gt;API_KEY&lt;/code&gt; is set correctly in environment or script&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Check server status&lt;/strong&gt;: Make sure your model server is running and accessible&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Network issues&lt;/strong&gt;: Verify firewall/network settings allow connections&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;&lt;strong&gt;Q: Evaluation script fails to run&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Troubleshooting steps:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Check working directory&lt;/strong&gt;: Make sure you're in &lt;code&gt;apps/miroflow-agent&lt;/code&gt; directory&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Verify environment&lt;/strong&gt;: Run &lt;code&gt;uv sync&lt;/code&gt; to ensure dependencies are installed&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Check .env file&lt;/strong&gt;: Ensure all required environment variables are set&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Review logs&lt;/strong&gt;: Check &lt;code&gt;logs/&lt;/code&gt; directory for detailed error messages&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Verify data path&lt;/strong&gt;: Ensure benchmark data is downloaded and in correct location&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h4&gt;&lt;strong&gt;Q: Out of memory errors&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Solutions:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Reduce context length&lt;/strong&gt;: Set &lt;code&gt;MAX_CONTEXT_LENGTH&lt;/code&gt; to a smaller value (e.g., 131072 for 128K)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Use context management with fewer turns&lt;/strong&gt;: 
   &lt;ul&gt; 
    &lt;li&gt;For v1.5: Use &lt;code&gt;mirothinker_v1.5_keep5_max200&lt;/code&gt; or &lt;code&gt;mirothinker_v1.5_keep5_max400&lt;/code&gt; (with context management)&lt;/li&gt; 
    &lt;li&gt;For v1.0: Use &lt;code&gt;mirothinker_v1.0_keep5&lt;/code&gt; (with context management)&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Reduce concurrent tasks&lt;/strong&gt;: Set &lt;code&gt;MAX_CONCURRENT&lt;/code&gt; to a smaller number (e.g., 5)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Use smaller model&lt;/strong&gt;: 
   &lt;ul&gt; 
    &lt;li&gt;For v1.5: Try 30B instead of 235B&lt;/li&gt; 
    &lt;li&gt;For v1.0: Try 8B or 30B instead of 72B&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;&lt;strong&gt;Q: Tool execution errors&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Common fixes:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;E2B errors&lt;/strong&gt;: Verify &lt;code&gt;E2B_API_KEY&lt;/code&gt; is valid and account has credits&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Serper errors&lt;/strong&gt;: Check &lt;code&gt;SERPER_API_KEY&lt;/code&gt; and rate limits&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Jina errors&lt;/strong&gt;: Verify &lt;code&gt;JINA_API_KEY&lt;/code&gt; and &lt;code&gt;JINA_BASE_URL&lt;/code&gt; are correct&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;LLM summarization errors&lt;/strong&gt;: Check &lt;code&gt;SUMMARY_LLM_*&lt;/code&gt; variables and model availability&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;&lt;strong&gt;Q: How to monitor long-running evaluations?&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Use the progress monitoring scripts:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;cd apps/miroflow-agent
python benchmarks/check_progress/check_progress_&amp;lt;benchmark_name&amp;gt;.py /path/to/logs
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;The scripts show completion status, elapsed time, and estimated remaining time.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;Getting Help&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìñ &lt;strong&gt;Documentation&lt;/strong&gt;: Check &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/libs/miroflow-tools/README.md"&gt;MiroFlow Tools README&lt;/a&gt; for tool details&lt;/li&gt; 
 &lt;li&gt;üí¨ &lt;strong&gt;Discord&lt;/strong&gt;: Join our &lt;a href="https://discord.com/invite/GPqEnkzQZd"&gt;Discord community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üêõ &lt;strong&gt;Issues&lt;/strong&gt;: Report bugs on &lt;a href="https://github.com/MiroMindAI/MiroThinker/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìß &lt;strong&gt;Contact&lt;/strong&gt;: Visit &lt;a href="https://miromind.ai/"&gt;our website&lt;/a&gt; for more information&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;üôè Acknowledgments&lt;/h2&gt; 
&lt;p&gt;We extend our sincere gratitude to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üèÜ &lt;strong&gt;Benchmark Contributors&lt;/strong&gt; for the comprehensive evaluation datasets&lt;/li&gt; 
 &lt;li&gt;üåç &lt;strong&gt;Open Source Community&lt;/strong&gt; for the tools and libraries that make this possible&lt;/li&gt; 
 &lt;li&gt;üë• &lt;strong&gt;All Contributors&lt;/strong&gt; who have helped make MiroThinker better&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/MiroMindAI/MiroThinker/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=MiroMindAI/MiroThinker" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;p&gt;Join our community and help us build the future of AI agents!&lt;/p&gt; 
&lt;h3&gt;References&lt;/h3&gt; 
&lt;p&gt;If you find this project useful in your research, please consider citing:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{miromind2025mirothinker,
  title={MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling},
  author={MiroMind Team and Bai, Song and Bing, Lidong and Chen, Carson and Chen, Guanzheng and Chen, Yuntao and Chen, Zhe and Chen, Ziyi and Dai, Jifeng and Dong, Xuan and others},
  journal={arXiv preprint arXiv:2511.11793},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#MiroMindAI/MiroThinker&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=MiroMindAI/MiroThinker&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>