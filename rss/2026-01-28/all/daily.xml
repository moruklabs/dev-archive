<rss version="2.0">
  <channel>
    <title>GitHub All Languages Daily Trending</title>
    <description>Daily Trending of All Languages in GitHub</description>
    <pubDate>Tue, 27 Jan 2026 01:32:20 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>AI4Finance-Foundation/FinRobot</title>
      <link>https://github.com/AI4Finance-Foundation/FinRobot</link>
      <description>&lt;p&gt;FinRobot: An Open-Source AI Agent Platform for Financial Analysis using LLMs ðŸš€ ðŸš€ ðŸš€&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img align="center" width="30%" alt="image" src="https://github.com/AI4Finance-Foundation/FinGPT/assets/31713746/e0371951-1ce1-488e-aa25-0992dafcc139" /&gt; 
&lt;/div&gt; 
&lt;h1&gt;FinRobot: An Open-Source AI Agent Platform for Financial Analysis using Large Language Models&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/AI4Finance-Foundation/FinRobot/master/%5Bhttps://pepy.tech/project/finrobot%5D(https://pepy.tech/project/finrobot)"&gt;&lt;img src="https://static.pepy.tech/badge/finrobot" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/finrobot"&gt;&lt;img src="https://static.pepy.tech/badge/finrobot/week" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/trsr8SXpW5"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join-blue" alt="Join Discord" /&gt;&lt;/a&gt; &lt;a href="https://www.python.org/downloads/release/python-360/"&gt;&lt;img src="https://img.shields.io/badge/python-3.6-blue.svg?sanitize=true" alt="Python 3.8" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/finrobot/"&gt;&lt;img src="https://img.shields.io/pypi/v/finrobot.svg?sanitize=true" alt="PyPI" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/license/AI4Finance-Foundation/finrobot.svg?color=brightgreen" alt="License" /&gt; &lt;img src="https://img.shields.io/github/issues-raw/AI4Finance-Foundation/finrobot?label=Issues" alt="" /&gt; &lt;img src="https://img.shields.io/github/issues-closed-raw/AI4Finance-Foundation/finrobot?label=Closed+Issues" alt="" /&gt; &lt;img src="https://img.shields.io/github/issues-pr-raw/AI4Finance-Foundation/finrobot?label=Open+PRs" alt="" /&gt; &lt;img src="https://img.shields.io/github/issues-pr-closed-raw/AI4Finance-Foundation/finrobot?label=Closed+PRs" alt="" /&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img align="center" src="https://raw.githubusercontent.com/AI4Finance-Foundation/FinRobot/master/figs/logo_white_background.jpg" width="40%" /&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;FinRobot&lt;/strong&gt; is an AI Agent Platform that transcends the scope of FinGPT, representing a comprehensive solution meticulously designed for financial applications. It integrates &lt;strong&gt;a diverse array of AI technologies&lt;/strong&gt;, extending beyond mere language models. This expansive vision highlights the platform's versatility and adaptability, addressing the multifaceted needs of the financial industry.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Concept of AI Agent&lt;/strong&gt;: an AI Agent is an intelligent entity that uses large language models as its brain to perceive its environment, make decisions, and execute actions. Unlike traditional artificial intelligence, AI Agents possess the ability to independently think and utilize tools to progressively achieve given objectives.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://arxiv.org/abs/2405.14767"&gt;Whitepaper of FinRobot&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/trsr8SXpW5"&gt;&lt;img src="https://dcbadge.vercel.app/api/server/trsr8SXpW5" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://api.visitorbadge.io/api/VisitorHit?user=AI4Finance-Foundation&amp;amp;repo=FinRobot&amp;amp;countColor=%23B17A" alt="Visitors" /&gt; &lt;a href="https://discord.gg/trsr8SXpW5"&gt;&lt;img src="https://dcbadge.limes.pink/api/server/trsr8SXpW5?cb=1" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;FinRobot Pro â€” An AI Powered Equity Research Platform: &lt;a href="https://finrobot.ai/"&gt;https://finrobot.ai/&lt;/a&gt;&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img align="center" src="https://github.com/user-attachments/assets/de3b9f9c-50aa-49f0-82c6-3d2b938f4670" width="90%" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;What is FinRobot Pro?&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://finrobot.ai/"&gt;FinRobot Pro&lt;/a&gt; is an AI-powered equity research platform that automates professional stock analysis using Large Language Models (LLMs) and AI Agents.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Automated Report Generation&lt;/strong&gt; â€“ Generate professional equity research reports instantly&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Financial Analysis&lt;/strong&gt; â€“ Deep dive into income statements, balance sheets, and cash flows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Valuation Analysis&lt;/strong&gt; â€“ P/E ratio, EV/EBITDA multiples, and peer comparison&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Risk Assessment&lt;/strong&gt; â€“ Comprehensive investment risk evaluation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FinRobot Ecosystem&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img align="center" src="https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/6b30d9c1-35e5-4d36-a138-7e2769718f62" width="90%" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;The overall framework of FinRobot is organized into four distinct layers, each designed to address specific aspects of financial AI processing and application:&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Financial AI Agents Layer&lt;/strong&gt;: The Financial AI Agents Layer now includes Financial Chain-of-Thought (CoT) prompting, enhancing complex analysis and decision-making capacity. Market Forecasting Agents, Document Analysis Agents, and Trading Strategies Agents utilize CoT to dissect financial challenges into logical steps, aligning their advanced algorithms and domain expertise with the evolving dynamics of financial markets for precise, actionable insights.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Financial LLMs Algorithms Layer&lt;/strong&gt;: The Financial LLMs Algorithms Layer configures and utilizes specially tuned models tailored to specific domains and global market analysis.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLMOps and DataOps Layers&lt;/strong&gt;: The LLMOps layer implements a multi-source integration strategy that selects the most suitable LLMs for specific financial tasks, utilizing a range of state-of-the-art models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-source LLM Foundation Models Layer&lt;/strong&gt;: This foundational layer supports the plug-and-play functionality of various general and specialized LLMs.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;FinRobot: Agent Workflow&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img align="center" src="https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/ff8033be-2326-424a-ac11-17e2c9c4983d" width="60%" /&gt; 
&lt;/div&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Perception&lt;/strong&gt;: This module captures and interprets multimodal financial data from market feeds, news, and economic indicators, using sophisticated techniques to structure the data for thorough analysis.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Brain&lt;/strong&gt;: Acting as the core processing unit, this module perceives data from the Perception module with LLMs and utilizes Financial Chain-of-Thought (CoT) processes to generate structured instructions.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Action&lt;/strong&gt;: This module executes instructions from the Brain module, applying tools to translate analytical insights into actionable outcomes. Actions include trading, portfolio adjustments, generating reports, or sending alerts, thereby actively influencing the financial environment.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;FinRobot: Smart Scheduler&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img align="center" src="https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/06fa0b78-ac53-48d3-8a6e-98d15386327e" width="60%" /&gt; 
&lt;/div&gt; 
&lt;p&gt;The Smart Scheduler is central to ensuring model diversity and optimizing the integration and selection of the most appropriate LLM for each task.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Director Agent&lt;/strong&gt;: This component orchestrates the task assignment process, ensuring that tasks are allocated to agents based on their performance metrics and suitability for specific tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Agent Registration&lt;/strong&gt;: Manages the registration and tracks the availability of agents within the system, facilitating an efficient task allocation process.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Agent Adaptor&lt;/strong&gt;: Tailor agent functionalities to specific tasks, enhancing their performance and integration within the overall system.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Task Manager&lt;/strong&gt;: Manages and stores different general and fine-tuned LLMs-based agents tailored for various financial tasks, updated periodically to ensure relevance and efficacy.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;File Structure&lt;/h2&gt; 
&lt;p&gt;The main folder &lt;strong&gt;finrobot&lt;/strong&gt; has three subfolders &lt;strong&gt;agents, data_source, functional&lt;/strong&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;FinRobot
â”œâ”€â”€ finrobot (main folder)
â”‚   â”œâ”€â”€ agents
â”‚   	â”œâ”€â”€ agent_library.py
â”‚   	â””â”€â”€ workflow.py
â”‚   â”œâ”€â”€ data_source
â”‚   	â”œâ”€â”€ finnhub_utils.py
â”‚   	â”œâ”€â”€ finnlp_utils.py
â”‚   	â”œâ”€â”€ fmp_utils.py
â”‚   	â”œâ”€â”€ sec_utils.py
â”‚   	â””â”€â”€ yfinance_utils.py
â”‚   â”œâ”€â”€ functional
â”‚   	â”œâ”€â”€ analyzer.py
â”‚   	â”œâ”€â”€ charting.py
â”‚   	â”œâ”€â”€ coding.py
â”‚   	â”œâ”€â”€ quantitative.py
â”‚   	â”œâ”€â”€ reportlab.py
â”‚   	â””â”€â”€ text.py
â”‚   â”œâ”€â”€ toolkits.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ configs
â”œâ”€â”€ experiments
â”œâ”€â”€ tutorials_beginner (hands-on tutorial)
â”‚   â”œâ”€â”€ agent_fingpt_forecaster.ipynb
â”‚   â””â”€â”€ agent_annual_report.ipynb 
â”œâ”€â”€ tutorials_advanced (advanced tutorials for potential finrobot developers)
â”‚   â”œâ”€â”€ agent_trade_strategist.ipynb
â”‚   â”œâ”€â”€ agent_fingpt_forecaster.ipynb
â”‚   â”œâ”€â”€ agent_annual_report.ipynb 
â”‚   â”œâ”€â”€ lmm_agent_mplfinance.ipynb
â”‚   â””â”€â”€ lmm_agent_opt_smacross.ipynb
â”œâ”€â”€ setup.py
â”œâ”€â”€ OAI_CONFIG_LIST_sample
â”œâ”€â”€ config_api_keys_sample
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation:&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;1. (Recommended) Create a new virtual environment&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;conda create --name finrobot python=3.10
conda activate finrobot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;2. download the FinRobot repo use terminal or download it manually&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/AI4Finance-Foundation/FinRobot.git
cd FinRobot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;3. install finrobot &amp;amp; dependencies from source or pypi&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;get our latest release from pypi&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U finrobot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or install from this repo directly&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;4. modify OAI_CONFIG_LIST_sample file&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;1) rename OAI_CONFIG_LIST_sample to OAI_CONFIG_LIST
2) remove the four lines of comment within the OAI_CONFIG_LIST file
3) add your own openai api-key &amp;lt;your OpenAI API key here&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;5. modify config_api_keys_sample file&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;1) rename config_api_keys_sample to config_api_keys
2) remove the comment within the config_api_keys file
3) add your own finnhub-api "YOUR_FINNHUB_API_KEY"
4) add your own financialmodelingprep and sec-api keys "YOUR_FMP_API_KEY" and "YOUR_SEC_API_KEY" (for financial report generation)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;6. start navigating the tutorials or the demos below:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# find these notebooks in tutorials
1) agent_annual_report.ipynb
2) agent_fingpt_forecaster.ipynb
3) agent_trade_strategist.ipynb
4) lmm_agent_mplfinance.ipynb
5) lmm_agent_opt_smacross.ipynb
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Demos&lt;/h2&gt; 
&lt;h3&gt;1. Market Forecaster Agent (Predict Stock Movements Direction)&lt;/h3&gt; 
&lt;p&gt;Takes a company's ticker symbol, recent basic financials, and market news as input and predicts its stock movements.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Import&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import autogen
from finrobot.utils import get_current_date, register_keys_from_json
from finrobot.agents.workflow import SingleAssistant
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Config&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Read OpenAI API keys from a JSON file
llm_config = {
    "config_list": autogen.config_list_from_json(
        "../OAI_CONFIG_LIST",
        filter_dict={"model": ["gpt-4-0125-preview"]},
    ),
    "timeout": 120,
    "temperature": 0,
}

# Register FINNHUB API keys
register_keys_from_json("../config_api_keys")
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Run&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;company = "NVDA"

assitant = SingleAssistant(
    "Market_Analyst",
    llm_config,
    # set to "ALWAYS" if you want to chat instead of simply receiving the prediciton
    human_input_mode="NEVER",
)
assitant.chat(
    f"Use all the tools provided to retrieve information available for {company} upon {get_current_date()}. Analyze the positive developments and potential concerns of {company} "
    "with 2-4 most important factors respectively and keep them concise. Most factors should be inferred from company related news. "
    f"Then make a rough prediction (e.g. up/down by 2-3%) of the {company} stock price movement for next week. Provide a summary analysis to support your prediction."
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Result&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center"&gt; 
 &lt;img align="center" src="https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/812ec23a-9cb3-4fad-b716-78533ddcd9dc" width="40%" /&gt; 
 &lt;img align="center" src="https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/9a2f9f48-b0e1-489c-8679-9a4c530f313c" width="41%" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;2. Financial Analyst Agent for Report Writing (Equity Research Report)&lt;/h3&gt; 
&lt;p&gt;Take a company's 10-k form, financial data, and market data as input and output an equity research report&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Import&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
import autogen
from textwrap import dedent
from finrobot.utils import register_keys_from_json
from finrobot.agents.workflow import SingleAssistantShadow
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Config&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;llm_config = {
    "config_list": autogen.config_list_from_json(
        "../OAI_CONFIG_LIST",
        filter_dict={
            "model": ["gpt-4-0125-preview"],
        },
    ),
    "timeout": 120,
    "temperature": 0.5,
}
register_keys_from_json("../config_api_keys")

# Intermediate strategy modules will be saved in this directory
work_dir = "../report"
os.makedirs(work_dir, exist_ok=True)

assistant = SingleAssistantShadow(
    "Expert_Investor",
    llm_config,
    max_consecutive_auto_reply=None,
    human_input_mode="TERMINATE",
)

&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Run&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;company = "Microsoft"
fyear = "2023"

message = dedent(
    f"""
    With the tools you've been provided, write an annual report based on {company}'s {fyear} 10-k report, format it into a pdf.
    Pay attention to the followings:
    - Explicitly explain your working plan before you kick off.
    - Use tools one by one for clarity, especially when asking for instructions. 
    - All your file operations should be done in "{work_dir}". 
    - Display any image in the chat once generated.
    - All the paragraphs should combine between 400 and 450 words, don't generate the pdf until this is explicitly fulfilled.
"""
)

assistant.chat(message, use_cache=True, max_turns=50,
               summary_method="last_msg")
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Result&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center"&gt; 
 &lt;img align="center" src="https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/d2d999e0-dc0e-4196-aca1-218f5fadcc5b" width="60%" /&gt; 
 &lt;img align="center" src="https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/3a21873f-9498-4d73-896b-3740bf6d116d" width="60%" /&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Financial CoT&lt;/strong&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Gather Preliminary Data&lt;/strong&gt;: 10-K report, market data, financial ratios&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Analyze Financial Statements&lt;/strong&gt;: balance sheet, income statement, cash flow&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Company Overview and Performance&lt;/strong&gt;: company description, business highlights, segment analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Risk Assessment&lt;/strong&gt;: assess risks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Financial Performance Visualization&lt;/strong&gt;: plot PE ratio and EPS&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Synthesize Findings into Paragraphs&lt;/strong&gt;: combine all parts into a coherent summary&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Generate PDF Report&lt;/strong&gt;: use tools to generate PDF automatically&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quality Assurance&lt;/strong&gt;: check word counts&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;3. Trade Strategist Agent with multimodal capabilities&lt;/h3&gt; 
&lt;h2&gt;AI Agent Papers&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Stanford University + Microsoft Research] &lt;a href="https://arxiv.org/abs/2401.03568"&gt;Agent AI: Surveying the Horizons of Multimodal Interaction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Stanford University] &lt;a href="https://arxiv.org/abs/2304.03442"&gt;Generative Agents: Interactive Simulacra of Human Behavior&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Fudan NLP Group] &lt;a href="https://arxiv.org/abs/2309.07864"&gt;The Rise and Potential of Large Language Model Based Agents: A Survey&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Fudan NLP Group] &lt;a href="https://github.com/WooooDyy/LLM-Agent-Paper-List"&gt;LLM-Agent-Paper-List&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Tsinghua University] &lt;a href="https://arxiv.org/abs/2312.11970"&gt;Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Renmin University] &lt;a href="https://arxiv.org/pdf/2308.11432.pdf"&gt;A Survey on Large Language Model-based Autonomous Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Nanyang Technological University] &lt;a href="https://arxiv.org/abs/2402.18485"&gt;FinAgent: A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;AI Agent Blogs and Videos&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Medium] &lt;a href="https://medium.com/humansdotai/an-introduction-to-ai-agents-e8c4afd2ee8f"&gt;An Introduction to AI Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Medium] &lt;a href="https://medium.com/@aitrendorbit/unmasking-the-best-character-ai-chatbots-2024-351de43792f4#the-best-character-ai-chatbots"&gt;Unmasking the Best Character AI Chatbots | 2024&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[big-picture] &lt;a href="https://blog.big-picture.com/en/chatgpt-next-level-meet-10-autonomous-ai-agents-auto-gpt-babyagi-agentgpt-microsoft-jarvis-chaosgpt-friends/"&gt;ChatGPT, Next Level: Meet 10 Autonomous AI Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[TowardsDataScience] &lt;a href="https://towardsdatascience.com/navigating-the-world-of-llm-agents-a-beginners-guide-3b8d499db7a9"&gt;Navigating the World of LLM Agents: A Beginnerâ€™s Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[YouTube] &lt;a href="https://www.youtube.com/watch?v=iVbN95ica_k"&gt;Introducing Devin - The "First" AI Agent Software Engineer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;AI Agent Open-Source Framework &amp;amp; Tool&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Significant-Gravitas/AutoGPT"&gt;AutoGPT (163k stars)&lt;/a&gt; is a tool for everyone to use, aiming to democratize AI, making it accessible for everyone to use and build upon.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/langchain-ai/langchain"&gt;LangChain (87.4k stars)&lt;/a&gt; is a framework for developing context-aware applications powered by language models, enabling them to connect to sources of context and rely on the model's reasoning capabilities for responses and actions.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekan/MetaGPT"&gt;MetaGPT (41k stars)&lt;/a&gt; is a multi-agent open-source framework that assigns different roles to GPTs, forming a collaborative software entity to execute complex tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/langgenius/dify"&gt;dify (34.1.7k stars)&lt;/a&gt; is an LLM application development platform. It integrates the concepts of Backend as a Service and LLMOps, covering the core tech stack required for building generative AI-native applications, including a built-in RAG engine&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/autogen"&gt;AutoGen (27.4k stars)&lt;/a&gt; is a framework for developing LLM applications with conversational agents that collaborate to solve tasks. These agents are customizable, support human interaction, and operate in modes combining LLMs, human inputs, and tools.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBMB/ChatDev"&gt;ChatDev (24.1k stars)&lt;/a&gt; is a framework that focuses on developing conversational AI Agents capable of dialogue and question-answering. It provides a range of pre-trained models and interactive interfaces, facilitating the development of customized chat Agents for users.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yoheinakajima/babyagi"&gt;BabyAGI (19.5k stars)&lt;/a&gt; is an AI-powered task management system, dedicated to building AI Agents with preliminary general intelligence.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/joaomdmoura/crewAI"&gt;CrewAI (16k stars)&lt;/a&gt; is a framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TransformerOptimus/SuperAGI"&gt;SuperAGI (14.8k stars)&lt;/a&gt; is a dev-first open-source autonomous AI agent framework enabling developers to build, manage &amp;amp; run useful autonomous agents.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/labring/FastGPT"&gt;FastGPT (14.6k stars)&lt;/a&gt; is a knowledge-based platform built on the LLM, offers out-of-the-box data processing and model invocation capabilities, allows for workflow orchestration through Flow visualization.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBMB/XAgent"&gt;XAgent (7.8k stars)&lt;/a&gt; is an open-source experimental Large Language Model (LLM) driven autonomous agent that can automatically solve various tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dataelement/bisheng"&gt;Bisheng (7.8k stars)&lt;/a&gt; is a leading open-source platform for developing LLM applications.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBMB/XAgent"&gt;Voyager (5.3k stars)&lt;/a&gt; An Open-Ended Embodied Agent with Large Language Models.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/camel-ai/camel"&gt;CAMEL (4.7k stars)&lt;/a&gt; is a framework that offers a comprehensive set of tools and algorithms for building multimodal AI Agents, enabling them to handle various data forms such as text, images, and speech.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/langfuse/langfuse"&gt;Langfuse (4.3k stars)&lt;/a&gt; is a language fusion framework that can integrate the language abilities of multiple AI Agents, enabling them to simultaneously possess multilingual understanding and generation capabilities.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citing FinRobot&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@inproceedings{
zhou2024finrobot,
title={FinRobot: {AI} Agent for Equity Research and Valuation with Large Language Models},
author={Tianyu Zhou and Pinqiao Wang and Yilin Wu and Hongyang Yang},
booktitle={ICAIF 2024: The 1st Workshop on Large Language Models and Generative AI for Finance},
year={2024}
}

@article{yang2024finrobot,
  title={FinRobot: An Open-Source AI Agent Platform for Financial Applications using Large Language Models},
  author={Yang, Hongyang and Zhang, Boyu and Wang, Neng and Guo, Cheng and Zhang, Xiaoli and Lin, Likun and Wang, Junlin and Zhou, Tianyu and Guan, Mao and Zhang, Runjia and others},
  journal={arXiv preprint arXiv:2405.14767},
  year={2024}
}

@inproceedings{han2024enhancing,
  title={Enhancing Investment Analysis: Optimizing AI-Agent Collaboration in Financial Research},
  author={Han, Xuewen and Wang, Neng and Che, Shangkun and Yang, Hongyang and Zhang, Kunpeng and Xu, Sean Xin},
  booktitle={ICAIF 2024: Proceedings of the 5th ACM International Conference on AI in Finance},
  pages={538--546},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: The codes and documents provided herein are released under the Apache-2.0 license. They should not be construed as financial counsel or recommendations for live trading. It is imperative to exercise caution and consult with qualified financial professionals prior to any trading or investment actions.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Blaizzy/mlx-audio</title>
      <link>https://github.com/Blaizzy/mlx-audio</link>
      <description>&lt;p&gt;A text-to-speech (TTS), speech-to-text (STT) and speech-to-speech (STS) library built on Apple's MLX framework, providing efficient speech analysis on Apple Silicon.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MLX-Audio&lt;/h1&gt; 
&lt;p&gt;The best audio processing library built on Apple's MLX framework, providing fast and efficient text-to-speech (TTS), speech-to-text (STT), and speech-to-speech (STS) on Apple Silicon.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fast inference optimized for Apple Silicon (M series chips)&lt;/li&gt; 
 &lt;li&gt;Multiple model architectures for TTS, STT, and STS&lt;/li&gt; 
 &lt;li&gt;Multilingual support across models&lt;/li&gt; 
 &lt;li&gt;Voice customization and cloning capabilities&lt;/li&gt; 
 &lt;li&gt;Adjustable speech speed control&lt;/li&gt; 
 &lt;li&gt;Interactive web interface with 3D audio visualization&lt;/li&gt; 
 &lt;li&gt;OpenAI-compatible REST API&lt;/li&gt; 
 &lt;li&gt;Quantization support (3-bit, 4-bit, 6-bit, 8-bit, and more) for optimized performance&lt;/li&gt; 
 &lt;li&gt;Swift package for iOS/macOS integration&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Using pip&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install mlx-audio
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using uv to install only the command line tools&lt;/h3&gt; 
&lt;p&gt;Latest release from pypi:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv tool install --force mlx-audio --prerelease=allow
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Latest code from github:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv tool install --force git+https://github.com/Blaizzy/mlx-audio.git --prerelease=allow
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;For development or web interface:&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Blaizzy/mlx-audio.git
cd mlx-audio
pip install -e ".[dev]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Command Line&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Basic TTS generation
mlx_audio.tts.generate --model mlx-community/Kokoro-82M-bf16 --text 'Hello, world!' --lang_code a

# With voice selection and speed adjustment
mlx_audio.tts.generate --model mlx-community/Kokoro-82M-bf16 --text 'Hello!' --voice af_heart --speed 1.2 --lang_code a

# Play audio immediately
mlx_audio.tts.generate --model mlx-community/Kokoro-82M-bf16 --text 'Hello!' --play  --lang_code a

# Save to a specific directory
mlx_audio.tts.generate --model mlx-community/Kokoro-82M-bf16 --text 'Hello!' --output_path ./my_audio  --lang_code a
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Python API&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.tts.utils import load_model

# Load model
model = load_model("mlx-community/Kokoro-82M-bf16")

# Generate speech
for result in model.generate("Hello from MLX-Audio!", voice="af_heart"):
    print(f"Generated {result.audio.shape[0]} samples")
    # result.audio contains the waveform as mx.array
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Supported Models&lt;/h2&gt; 
&lt;h3&gt;Text-to-Speech (TTS)&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Languages&lt;/th&gt; 
   &lt;th&gt;Repo&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Kokoro&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Fast, high-quality multilingual TTS&lt;/td&gt; 
   &lt;td&gt;EN, JA, ZH, FR, ES, IT, PT, HI&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Kokoro-82M-bf16"&gt;mlx-community/Kokoro-82M-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen3-TTS&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alibaba's multilingual TTS with voice design&lt;/td&gt; 
   &lt;td&gt;ZH, EN, JA, KO, + more&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-VoiceDesign-bf16"&gt;mlx-community/Qwen3-TTS-12Hz-1.7B-VoiceDesign-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;CSM&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational Speech Model with voice cloning&lt;/td&gt; 
   &lt;td&gt;EN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/csm-1b"&gt;mlx-community/csm-1b&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Dia&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Dialogue-focused TTS&lt;/td&gt; 
   &lt;td&gt;EN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Dia-1.6B-bf16"&gt;mlx-community/Dia-1.6B-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;OuteTTS&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Efficient TTS model&lt;/td&gt; 
   &lt;td&gt;EN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/OuteTTS-0.2-500M"&gt;mlx-community/OuteTTS-0.2-500M&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Spark&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;SparkTTS model&lt;/td&gt; 
   &lt;td&gt;EN, ZH&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/SparkTTS-0.5B-bf16"&gt;mlx-community/SparkTTS-0.5B-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Chatterbox&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Expressive multilingual TTS&lt;/td&gt; 
   &lt;td&gt;EN, ES, FR, DE, IT, PT, PL, TR, RU, NL, CS, AR, ZH, JA, HU, KO&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Chatterbox-bf16"&gt;mlx-community/Chatterbox-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Soprano&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;High-quality TTS&lt;/td&gt; 
   &lt;td&gt;EN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Soprano-bf16"&gt;mlx-community/Soprano-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Speech-to-Text (STT)&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Languages&lt;/th&gt; 
   &lt;th&gt;Repo&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Whisper&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI's robust STT model&lt;/td&gt; 
   &lt;td&gt;99+ languages&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/whisper-large-v3-turbo-asr-fp16"&gt;mlx-community/whisper-large-v3-turbo-asr-fp16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Parakeet&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;NVIDIA's accurate STT&lt;/td&gt; 
   &lt;td&gt;EN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/parakeet-tdt-0.6b-v2"&gt;mlx-community/parakeet-tdt-0.6b-v2&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Voxtral&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Mistral's speech model&lt;/td&gt; 
   &lt;td&gt;Multiple&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Voxtral-Mini-3B-2507-bf16"&gt;mlx-community/Voxtral-Mini-3B-2507-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;VibeVoice-ASR&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Microsoft's 9B ASR with diarization &amp;amp; timestamps&lt;/td&gt; 
   &lt;td&gt;Multiple&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/VibeVoice-ASR-bf16"&gt;mlx-community/VibeVoice-ASR-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Speech-to-Speech (STS)&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
   &lt;th&gt;Repo&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;SAM-Audio&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Text-guided source separation&lt;/td&gt; 
   &lt;td&gt;Extract specific sounds&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/sam-audio-large"&gt;mlx-community/sam-audio-large&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Liquid2.5-Audio&lt;/strong&gt;*&lt;/td&gt; 
   &lt;td&gt;Speech-to-Speech, Text-to-Speech and Speech-to-Text&lt;/td&gt; 
   &lt;td&gt;Speech interactions&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/LFM2.5-Audio-1.5B-8bit"&gt;mlx-community/LFM2.5-Audio-1.5B-8bit&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MossFormer2 SE&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Speech enhancement&lt;/td&gt; 
   &lt;td&gt;Noise removal&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/starkdmi/MossFormer2_SE_48K_MLX"&gt;starkdmi/MossFormer2_SE_48K_MLX&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Model Examples&lt;/h2&gt; 
&lt;h3&gt;Kokoro TTS&lt;/h3&gt; 
&lt;p&gt;Kokoro is a fast, multilingual TTS model with 54 voice presets.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.tts.utils import load_model

model = load_model("mlx-community/Kokoro-82M-bf16")

# Generate with different voices
for result in model.generate(
    text="Welcome to MLX-Audio!",
    voice="af_heart",  # American female
    speed=1.0,
    lang_code="a"  # American English
):
    audio = result.audio
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Available Voices:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;American English: &lt;code&gt;af_heart&lt;/code&gt;, &lt;code&gt;af_bella&lt;/code&gt;, &lt;code&gt;af_nova&lt;/code&gt;, &lt;code&gt;af_sky&lt;/code&gt;, &lt;code&gt;am_adam&lt;/code&gt;, &lt;code&gt;am_echo&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;British English: &lt;code&gt;bf_alice&lt;/code&gt;, &lt;code&gt;bf_emma&lt;/code&gt;, &lt;code&gt;bm_daniel&lt;/code&gt;, &lt;code&gt;bm_george&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;Japanese: &lt;code&gt;jf_alpha&lt;/code&gt;, &lt;code&gt;jm_kumo&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;Chinese: &lt;code&gt;zf_xiaobei&lt;/code&gt;, &lt;code&gt;zm_yunxi&lt;/code&gt;, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Language Codes:&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Code&lt;/th&gt; 
   &lt;th&gt;Language&lt;/th&gt; 
   &lt;th&gt;Note&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;a&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;American English&lt;/td&gt; 
   &lt;td&gt;Default&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;b&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;British English&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;j&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Japanese&lt;/td&gt; 
   &lt;td&gt;Requires &lt;code&gt;pip install misaki[ja]&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;z&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Mandarin Chinese&lt;/td&gt; 
   &lt;td&gt;Requires &lt;code&gt;pip install misaki[zh]&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;e&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Spanish&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;f&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;French&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Qwen3-TTS&lt;/h3&gt; 
&lt;p&gt;Alibaba's state-of-the-art multilingual TTS with voice cloning, emotion control, and voice design capabilities.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.tts.utils import load_model

model = load_model("mlx-community/Qwen3-TTS-12Hz-0.6B-Base-bf16")
results = list(model.generate(
    text="Hello, welcome to MLX-Audio!",
    voice="Chelsie",
    language="English",
))

audio = results[0].audio  # mx.array
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/Blaizzy/mlx-audio/main/mlx_audio/tts/models/qwen3_tts/README.md"&gt;Qwen3-TTS README&lt;/a&gt; for voice cloning, CustomVoice, VoiceDesign, and all available models.&lt;/p&gt; 
&lt;h3&gt;CSM (Voice Cloning)&lt;/h3&gt; 
&lt;p&gt;Clone any voice using a reference audio sample:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mlx_audio.tts.generate \
    --model mlx-community/csm-1b \
    --text "Hello from Sesame." \
    --ref_audio ./reference_voice.wav \
    --play
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Whisper STT&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.stt.generate import generate_transcription

result = generate_transcription(
    model="mlx-community/whisper-large-v3-turbo-asr-fp16",
    audio="audio.wav",
)
print(result.text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;VibeVoice-ASR&lt;/h3&gt; 
&lt;p&gt;Microsoft's 9B parameter speech-to-text model with speaker diarization and timestamps. Supports long-form audio (up to 60 minutes) and outputs structured JSON.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.stt.utils import load

model = load("mlx-community/VibeVoice-ASR-bf16")

# Basic transcription
result = model.generate(audio="meeting.wav", max_tokens=8192, temperature=0.0)
print(result.text)
# [{"Start":0,"End":5.2,"Speaker":0,"Content":"Hello everyone, let's begin."},
#  {"Start":5.5,"End":9.8,"Speaker":1,"Content":"Thanks for joining today."}]

# Access parsed segments
for seg in result.segments:
    print(f"[{seg['start_time']:.1f}-{seg['end_time']:.1f}] Speaker {seg['speaker_id']}: {seg['text']}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Streaming transcription:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Stream tokens as they are generated
for text in model.stream_transcribe(audio="speech.wav", max_tokens=4096):
    print(text, end="", flush=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;With context (hotwords/metadata):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;result = model.generate(
    audio="technical_talk.wav",
    context="MLX, Apple Silicon, PyTorch, Transformer",
    max_tokens=8192,
    temperature=0.0,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;CLI usage:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Basic transcription
python -m mlx_audio.stt.generate \
    --model mlx-community/VibeVoice-ASR-bf16 \
    --audio meeting.wav \
    --output-path output \
    --format json \
    --max-tokens 8192 \
    --verbose

# With context/hotwords
python -m mlx_audio.stt.generate \
    --model mlx-community/VibeVoice-ASR-bf16 \
    --audio technical_talk.wav \
    --output-path output \
    --format json \
    --max-tokens 8192 \
    --context "MLX, Apple Silicon, PyTorch, Transformer" \
    --verbose
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;SAM-Audio (Source Separation)&lt;/h3&gt; 
&lt;p&gt;Separate specific sounds from audio using text prompts:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.sts import SAMAudio, SAMAudioProcessor, save_audio

model = SAMAudio.from_pretrained("mlx-community/sam-audio-large")
processor = SAMAudioProcessor.from_pretrained("mlx-community/sam-audio-large")

batch = processor(
    descriptions=["A person speaking"],
    audios=["mixed_audio.wav"],
)

result = model.separate_long(
    batch.audios,
    descriptions=batch.descriptions,
    anchors=batch.anchor_ids,
    chunk_seconds=10.0,
    overlap_seconds=3.0,
    ode_opt={"method": "midpoint", "step_size": 2/32},
)

save_audio(result.target[0], "voice.wav")
save_audio(result.residual[0], "background.wav")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;MossFormer2 (Speech Enhancement)&lt;/h3&gt; 
&lt;p&gt;Remove noise from speech recordings:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.sts import MossFormer2SEModel, save_audio

model = MossFormer2SEModel.from_pretrained("starkdmi/MossFormer2_SE_48K_MLX")
enhanced = model.enhance("noisy_speech.wav")
save_audio(enhanced, "clean.wav", 48000)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Web Interface &amp;amp; API Server&lt;/h2&gt; 
&lt;p&gt;MLX-Audio includes a modern web interface and OpenAI-compatible API.&lt;/p&gt; 
&lt;h3&gt;Starting the Server&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start API server
mlx_audio.server --host 0.0.0.0 --port 8000

# Start web UI (in another terminal)
cd mlx_audio/ui
npm install &amp;amp;&amp;amp; npm run dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;API Endpoints&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Text-to-Speech&lt;/strong&gt; (OpenAI-compatible):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -X POST http://localhost:8000/v1/audio/speech \
  -H "Content-Type: application/json" \
  -d '{"model": "mlx-community/Kokoro-82M-bf16", "input": "Hello!", "voice": "af_heart"}' \
  --output speech.wav
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Speech-to-Text&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -X POST http://localhost:8000/v1/audio/transcriptions \
  -F "file=@audio.wav" \
  -F "model=mlx-community/whisper-large-v3-turbo-asr-fp16"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quantization&lt;/h2&gt; 
&lt;p&gt;Reduce model size and improve performance with quantization using the convert script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Convert and quantize to 4-bit
python -m mlx_audio.convert \
    --hf-path prince-canuma/Kokoro-82M \
    --mlx-path ./Kokoro-82M-4bit \
    --quantize \
    --q-bits 4 \
    --upload-repo username/Kokoro-82M-4bit (optional: if you want to upload the model to Hugging Face)

# Convert with specific dtype (bfloat16)
python -m mlx_audio.convert \
    --hf-path prince-canuma/Kokoro-82M \
    --mlx-path ./Kokoro-82M-bf16 \
    --dtype bfloat16 \
    --upload-repo username/Kokoro-82M-bf16 (optional: if you want to upload the model to Hugging Face)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Options:&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Flag&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--hf-path&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Source Hugging Face model or local path&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--mlx-path&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Output directory for converted model&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-q, --quantize&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enable quantization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--q-bits&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Bits per weight (4, 6, or 8)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--q-group-size&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Group size for quantization (default: 64)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--dtype&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Weight dtype: &lt;code&gt;float16&lt;/code&gt;, &lt;code&gt;bfloat16&lt;/code&gt;, &lt;code&gt;float32&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--upload-repo&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Upload converted model to HF Hub&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Swift&lt;/h2&gt; 
&lt;p&gt;Looking for Swift/iOS support? Check out &lt;a href="https://github.com/Blaizzy/mlx-audio-swift"&gt;mlx-audio-swift&lt;/a&gt; for on-device TTS using MLX on macOS and iOS.&lt;/p&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+&lt;/li&gt; 
 &lt;li&gt;Apple Silicon Mac (M1/M2/M3/M4)&lt;/li&gt; 
 &lt;li&gt;MLX framework&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ffmpeg&lt;/strong&gt; (required for MP3/FLAC audio encoding)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installing ffmpeg&lt;/h3&gt; 
&lt;p&gt;ffmpeg is required for saving audio in MP3 or FLAC format. Install it using:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# macOS (using Homebrew)
brew install ffmpeg

# Ubuntu/Debian
sudo apt install ffmpeg
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;WAV format works without ffmpeg.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Blaizzy/mlx-audio/main/LICENSE"&gt;MIT License&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{mlx-audio,
  author = {Canuma, Prince},
  title = {MLX Audio},
  year = {2025},
  howpublished = {\url{https://github.com/Blaizzy/mlx-audio}},
  note = {Audio processing library for Apple Silicon with TTS, STT, and STS capabilities.}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ml-explore/mlx"&gt;Apple MLX Team&lt;/a&gt; for the MLX framework&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>block/goose</title>
      <link>https://github.com/block/goose</link>
      <description>&lt;p&gt;an open source, extensible AI agent that goes beyond code suggestions - install, execute, edit, and test with any LLM&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;goose&lt;/h1&gt; 
 &lt;p&gt;&lt;em&gt;a local, extensible, open source AI agent that automates engineering tasks&lt;/em&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://opensource.org/licenses/Apache-2.0"&gt; &lt;img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;a href="https://discord.gg/goose-oss"&gt; &lt;img src="https://img.shields.io/discord/1287729918100246654?logo=discord&amp;amp;logoColor=white&amp;amp;label=Join+Us&amp;amp;color=blueviolet" alt="Discord" /&gt; &lt;/a&gt; &lt;a href="https://github.com/block/goose/actions/workflows/ci.yml"&gt; &lt;img src="https://img.shields.io/github/actions/workflow/status/block/goose/ci.yml?branch=main" alt="CI" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;goose is your on-machine AI agent, capable of automating complex development tasks from start to finish. More than just code suggestions, goose can build entire projects from scratch, write and execute code, debug failures, orchestrate workflows, and interact with external APIs - &lt;em&gt;autonomously&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;Whether you're prototyping an idea, refining existing code, or managing intricate engineering pipelines, goose adapts to your workflow and executes tasks with precision.&lt;/p&gt; 
&lt;p&gt;Designed for maximum flexibility, goose works with any LLM and supports multi-model configuration to optimize performance and cost, seamlessly integrates with MCP servers, and is available as both a desktop app as well as CLI - making it the ultimate AI assistant for developers who want to move faster and focus on innovation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://youtu.be/D-DpDunrbpo"&gt;&lt;img src="https://github.com/user-attachments/assets/ddc71240-3928-41b5-8210-626dfb28af7a" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Quick Links&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://block.github.io/goose/docs/quickstart"&gt;Quickstart&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://block.github.io/goose/docs/getting-started/installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://block.github.io/goose/docs/category/tutorials"&gt;Tutorials&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://block.github.io/goose/docs/category/getting-started"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/block/goose/raw/main/HOWTOAI.md"&gt;Responsible AI-Assisted Coding Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/block/goose/raw/main/GOVERNANCE.md"&gt;Governance&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Need Help?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://block.github.io/goose/docs/troubleshooting/diagnostics-and-reporting"&gt;Diagnostics &amp;amp; Reporting&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://block.github.io/goose/docs/troubleshooting/known-issues"&gt;Known Issues&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;a little goose humor ðŸ¦¢&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Why did the developer choose goose as their AI agent?&lt;/p&gt; 
 &lt;p&gt;Because it always helps them "migrate" their code to production! ðŸš€&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;goose around with us&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/goose-oss"&gt;Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@goose-oss"&gt;YouTube&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.linkedin.com/company/goose-oss"&gt;LinkedIn&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://x.com/goose_oss"&gt;Twitter/X&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://bsky.app/profile/opensource.block.xyz"&gt;Bluesky&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://njump.me/opensource@block.xyz"&gt;Nostr&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>remotion-dev/remotion</title>
      <link>https://github.com/remotion-dev/remotion</link>
      <description>&lt;p&gt;ðŸŽ¥ Make videos programmatically with React&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://github.com/remotion-dev/logo"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/remotion-dev/logo/raw/main/animated-logo-banner-dark.apng" /&gt; 
   &lt;img alt="Animated Remotion Logo" src="https://github.com/remotion-dev/logo/raw/main/animated-logo-banner-light.gif" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://remotion.dev/discord"&gt;&lt;img src="https://img.shields.io/discord/809501355504959528?color=000000&amp;amp;label=Discord&amp;amp;logo=fdgssdf" alt="Discord Shield" /&gt;&lt;/a&gt; &lt;a href="https://www.npmjs.org/package/remotion"&gt;&lt;img src="https://img.shields.io/npm/v/remotion.svg?style=flat&amp;amp;color=black" alt="NPM Version" /&gt;&lt;/a&gt; &lt;a href="https://npmcharts.com/compare/remotion?minimal=true"&gt;&lt;img src="https://img.shields.io/npm/dm/remotion.svg?style=flat&amp;amp;color=black&amp;amp;label=Downloads" alt="NPM Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/remotion-dev/remotion/issues?q=is%3Aopen+label%3A%22%F0%9F%92%8E+Bounty%22+sort%3Aupdated-desc"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Fconsole.algora.io%2Fapi%2Fshields%2Fremotion%2Fbounties%3Fstatus%3Dopen&amp;amp;style=flat&amp;amp;color=black&amp;amp;labelColor=grey&amp;amp;label=Open+Bounties" alt="Open Bounties" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/remotion"&gt;&lt;img src="https://img.shields.io/twitter/follow/remotion?label=Twitter&amp;amp;color=black" alt="Twitter" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Remotion is a framework for &lt;strong&gt;creating videos programmatically using React.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Why create videos in React?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Leverage web technologies&lt;/strong&gt;: Use all of CSS, Canvas, SVG, WebGL, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Leverage programming&lt;/strong&gt;: Use variables, functions, APIs, math and algorithms to create new effects&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Leverage React&lt;/strong&gt;: Reusable components, Powerful composition, Fast Refresh, Package ecosystem&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Created with Remotion&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt; &lt;img style="width: 290px" src="https://pub-646d808d9cb240cea53bedc76dd3cd0c.r2.dev/fireship-quick.gif" /&gt; &lt;p&gt;"This video was made with code" &lt;em&gt;- Fireship&lt;/em&gt; &lt;a href="https://youtu.be/deg8bOoziaE"&gt;Watch&lt;/a&gt; â€¢ &lt;a href="https://github.com/wcandillon/remotion-fireship"&gt;Source&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;img style="width: 240px" src="https://pub-646d808d9cb240cea53bedc76dd3cd0c.r2.dev/unwrapped-2023.gif" /&gt; &lt;p&gt;GitHub Unwrapped - Personalized Year in Review &lt;a href="https://www.githubunwrapped.com"&gt;Try&lt;/a&gt; â€¢ &lt;a href="https://github.com/remotion-dev/github-unwrapped"&gt;Source&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;em&gt;View more in the &lt;a href="https://remotion.dev/showcase"&gt;Remotion Showcase&lt;/a&gt;!&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Get started&lt;/h2&gt; 
&lt;p&gt;If you already have Node.JS installed, type&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;npx create-video@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;to get started. Otherwise, read the &lt;a href="https://www.remotion.dev/docs/"&gt;installation page&lt;/a&gt; in the documentation.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Documentation: &lt;a href="https://www.remotion.dev/docs"&gt;&lt;strong&gt;remotion.dev/docs&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt; API Reference: &lt;a href="https://www.remotion.dev/api"&gt;&lt;strong&gt;remotion.dev/api&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Be aware of that Remotion has a special license and requires obtaining a company license in some cases. Read the &lt;a href="https://raw.githubusercontent.com/remotion-dev/remotion/main/LICENSE.md"&gt;LICENSE&lt;/a&gt; page for more information.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Please read &lt;a href="https://raw.githubusercontent.com/remotion-dev/remotion/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; to learn about contributing to this project.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>business-science/ai-data-science-team</title>
      <link>https://github.com/business-science/ai-data-science-team</link>
      <description>&lt;p&gt;An AI-powered data science team of agents to help you perform common data science tasks 10X faster.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://github.com/business-science/ai-data-science-team"&gt; 
  &lt;picture&gt; 
   &lt;img src="https://raw.githubusercontent.com/business-science/ai-data-science-team/master/img/ai_data_science_logo.png" alt="AI Data Science Team" width="360" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;em&gt;AI Data Science Team + AI Pipeline Studio&lt;/em&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://pypi.python.org/pypi/ai-data-science-team"&gt;&lt;img src="https://img.shields.io/pypi/v/ai-data-science-team.svg?style=for-the-badge" alt="PyPI" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/business-science/ai-data-science-team"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/ai-data-science-team.svg?style=for-the-badge" alt="versions" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/business-science/ai-data-science-team/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/business-science/ai-data-science-team.svg?style=for-the-badge" alt="license" /&gt;&lt;/a&gt; 
 &lt;img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/business-science/ai-data-science-team?style=for-the-badge" /&gt; 
&lt;/div&gt; 
&lt;h1&gt;AI Data Science Team&lt;/h1&gt; 
&lt;p&gt;AI Data Science Team is a Python library of specialized agents for common data science workflows, plus a flagship app: &lt;strong&gt;AI Pipeline Studio&lt;/strong&gt;. The Studio turns your work into a visual, reproducible pipeline, while the AI team handles data loading, cleaning, visualization, and modeling.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Beta. Breaking changes may occur until 0.1.0.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/business-science/ai-data-science-team"&gt;&lt;strong&gt;Please â­ us on GitHub (it takes 2 seconds and means a lot).&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;AI Pipeline Studio (Flagship App)&lt;/h2&gt; 
&lt;p&gt;AI Pipeline Studio is the main example of the AI Data Science Team in action.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/business-science/ai-data-science-team/master/img/apps/ai_pipeline_studio_app.jpg" alt="AI Pipeline Studio" /&gt;&lt;/p&gt; 
&lt;p&gt;Highlights:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Pipeline-first workspace: Visual Editor, Table, Chart, EDA, Code, Model, Predictions, MLflow&lt;/li&gt; 
 &lt;li&gt;Manual + AI steps with lineage and reproducible scripts&lt;/li&gt; 
 &lt;li&gt;Multi-dataset handling and merge workflows&lt;/li&gt; 
 &lt;li&gt;Project saves: metadata-only or full-data&lt;/li&gt; 
 &lt;li&gt;Storage footprint controls and rehydrate workflows&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Run it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;streamlit run apps/ai-pipeline-studio-app/app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Full app docs: &lt;code&gt;apps/ai-pipeline-studio-app/README.md&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;h3&gt;Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+&lt;/li&gt; 
 &lt;li&gt;OpenAI API key (or Ollama for local models)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Install the app and library&lt;/h3&gt; 
&lt;p&gt;Clone the repo and install in editable mode:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Run the AI Pipeline Studio app&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;streamlit run apps/ai-pipeline-studio-app/app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Library Overview&lt;/h2&gt; 
&lt;p&gt;The repository includes both the &lt;strong&gt;AI Pipeline Studio&lt;/strong&gt; app and the underlying &lt;strong&gt;AI Data Science Team&lt;/strong&gt; library. The library provides agent building blocks and multi-agent workflows for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Data loading and inspection&lt;/li&gt; 
 &lt;li&gt;Cleaning, wrangling, and feature engineering&lt;/li&gt; 
 &lt;li&gt;Visualization and EDA&lt;/li&gt; 
 &lt;li&gt;Modeling and evaluation (H2O + MLflow tools)&lt;/li&gt; 
 &lt;li&gt;SQL database interaction&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Agents (Snapshot)&lt;/h3&gt; 
&lt;p&gt;Agent examples live in &lt;code&gt;examples/&lt;/code&gt;. Notable agents:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Data Loader Tools Agent&lt;/li&gt; 
 &lt;li&gt;Data Wrangling Agent&lt;/li&gt; 
 &lt;li&gt;Data Cleaning Agent&lt;/li&gt; 
 &lt;li&gt;Data Visualization Agent&lt;/li&gt; 
 &lt;li&gt;EDA Tools Agent&lt;/li&gt; 
 &lt;li&gt;Feature Engineering Agent&lt;/li&gt; 
 &lt;li&gt;SQL Database Agent&lt;/li&gt; 
 &lt;li&gt;H2O ML Agent&lt;/li&gt; 
 &lt;li&gt;MLflow Tools Agent&lt;/li&gt; 
 &lt;li&gt;Multi-agent workflows (e.g., Pandas Data Analyst, SQL Data Analyst)&lt;/li&gt; 
 &lt;li&gt;Supervisor Agent (oversees other agents)&lt;/li&gt; 
 &lt;li&gt;Custom tools for data science tasks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Apps&lt;/h2&gt; 
&lt;p&gt;See all apps in &lt;code&gt;apps/&lt;/code&gt;. Notable apps:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;AI Pipeline Studio: &lt;code&gt;apps/ai-pipeline-studio-app/&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;EDA Explorer App: &lt;code&gt;apps/exploratory-copilot-app/&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Pandas Data Analyst App: &lt;code&gt;apps/pandas-data-analyst-app/&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Use OpenAI&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from langchain_openai import ChatOpenAI
llm = ChatOpenAI(
    model_name="gpt-4.1-mini",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Use Ollama (Local LLM)&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ollama serve
ollama pull llama3.1:8b
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from langchain_ollama import ChatOllama

llm = ChatOllama(
    model="llama3.1:8b",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Next-Gen AI Agentic Workshop&lt;/h2&gt; 
&lt;p&gt;Want to learn how to build AI agents and AI apps for real data science workflows? Join my nextâ€‘gen AI workshop: &lt;a href="https://learn.business-science.io/ai-register"&gt;https://learn.business-science.io/ai-register&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>VectifyAI/PageIndex</title>
      <link>https://github.com/VectifyAI/PageIndex</link>
      <description>&lt;p&gt;ðŸ“‘ PageIndex: Document Index for Vectorless, Reasoning-based RAG&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://vectify.ai/pageindex" target="_blank"&gt; &lt;img src="https://github.com/user-attachments/assets/46201e72-675b-43bc-bfbd-081cc6b65a1d" alt="PageIndex Banner" /&gt; &lt;/a&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/14736" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14736" alt="VectifyAI%2FPageIndex | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;h1&gt;PageIndex: Vectorless, Reasoning-based RAG&lt;/h1&gt; 
 &lt;p align="center"&gt;&lt;b&gt;Reasoning-based RAG&amp;nbsp; â—¦ &amp;nbsp;No Vector DB&amp;nbsp; â—¦ &amp;nbsp;No Chunking&amp;nbsp; â—¦ &amp;nbsp;Human-like Retrieval&lt;/b&gt;&lt;/p&gt; 
 &lt;h4 align="center"&gt; &lt;a href="https://vectify.ai"&gt;ðŸ  Homepage&lt;/a&gt;&amp;nbsp; â€¢ &amp;nbsp; &lt;a href="https://chat.pageindex.ai"&gt;ðŸ–¥ï¸ Chat Platform&lt;/a&gt;&amp;nbsp; â€¢ &amp;nbsp; &lt;a href="https://pageindex.ai/mcp"&gt;ðŸ”Œ MCP&lt;/a&gt;&amp;nbsp; â€¢ &amp;nbsp; &lt;a href="https://docs.pageindex.ai"&gt;ðŸ“š Docs&lt;/a&gt;&amp;nbsp; â€¢ &amp;nbsp; &lt;a href="https://discord.com/invite/VuXuf29EUj"&gt;ðŸ’¬ Discord&lt;/a&gt;&amp;nbsp; â€¢ &amp;nbsp; &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;âœ‰ï¸ Contact&lt;/a&gt;&amp;nbsp; &lt;/h4&gt; 
&lt;/div&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h3&gt;ðŸ“¢ Latest Updates&lt;/h3&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;ðŸ”¥ Releases:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://chat.pageindex.ai"&gt;&lt;strong&gt;PageIndex Chat&lt;/strong&gt;&lt;/a&gt;: The first human-like document-analysis agent &lt;a href="https://chat.pageindex.ai"&gt;platform&lt;/a&gt; built for professional long documents. Can also be integrated via &lt;a href="https://pageindex.ai/mcp"&gt;MCP&lt;/a&gt; or &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt; (beta).&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;!-- - [**PageIndex Chat API**](https://docs.pageindex.ai/quickstart): An API that brings PageIndex's advanced long-document intelligence directly into your applications and workflows. --&gt; 
 &lt;!-- - [PageIndex MCP](https://pageindex.ai/mcp): Bring PageIndex into Claude, Cursor, or any MCP-enabled agent. Chat with long PDFs in a reasoning-based, human-like way. --&gt; 
 &lt;p&gt;&lt;strong&gt;ðŸ“ Articles:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://pageindex.ai/blog/pageindex-intro"&gt;&lt;strong&gt;PageIndex Framework&lt;/strong&gt;&lt;/a&gt;: Introduces the PageIndex framework â€” an &lt;em&gt;agentic, in-context&lt;/em&gt; &lt;em&gt;tree index&lt;/em&gt; that enables LLMs to perform &lt;em&gt;reasoning-based&lt;/em&gt;, &lt;em&gt;human-like retrieval&lt;/em&gt; over long documents, without vector DB or chunking.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;!-- - [Do We Still Need OCR?](https://pageindex.ai/blog/do-we-need-ocr): Explores how vision-based, reasoning-native RAG challenges the traditional OCR pipeline, and why the future of document AI might be *vectorless* and *vision-based*. --&gt; 
 &lt;p&gt;&lt;strong&gt;ðŸ§ª Cookbooks:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex"&gt;Vectorless RAG&lt;/a&gt;: A minimal, hands-on example of reasoning-based RAG using PageIndex. No vectors, no chunking, and human-like retrieval.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://docs.pageindex.ai/cookbook/vision-rag-pageindex"&gt;Vision-based Vectorless RAG&lt;/a&gt;: OCR-free, vision-only RAG with PageIndex's reasoning-native retrieval workflow that works directly over PDF page images.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h1&gt;ðŸ“‘ Introduction to PageIndex&lt;/h1&gt; 
&lt;p&gt;Are you frustrated with vector database retrieval accuracy for long professional documents? Traditional vector-based RAG relies on semantic &lt;em&gt;similarity&lt;/em&gt; rather than true &lt;em&gt;relevance&lt;/em&gt;. But &lt;strong&gt;similarity â‰  relevance&lt;/strong&gt; â€” what we truly need in retrieval is &lt;strong&gt;relevance&lt;/strong&gt;, and that requires &lt;strong&gt;reasoning&lt;/strong&gt;. When working with professional documents that demand domain expertise and multi-step reasoning, similarity search often falls short.&lt;/p&gt; 
&lt;p&gt;Inspired by AlphaGo, we propose &lt;strong&gt;&lt;a href="https://vectify.ai/pageindex"&gt;PageIndex&lt;/a&gt;&lt;/strong&gt; â€” a &lt;strong&gt;vectorless&lt;/strong&gt;, &lt;strong&gt;reasoning-based RAG&lt;/strong&gt; system that builds a &lt;strong&gt;hierarchical tree index&lt;/strong&gt; from long documents and uses LLMs to &lt;strong&gt;reason&lt;/strong&gt; &lt;em&gt;over that index&lt;/em&gt; for &lt;strong&gt;agentic, context-aware retrieval&lt;/strong&gt;. It simulates how &lt;em&gt;human experts&lt;/em&gt; navigate and extract knowledge from complex documents through &lt;em&gt;tree search&lt;/em&gt;, enabling LLMs to &lt;em&gt;think&lt;/em&gt; and &lt;em&gt;reason&lt;/em&gt; their way to the most relevant document sections. PageIndex performs retrieval in two steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Generate a â€œTable-of-Contentsâ€ &lt;strong&gt;tree structure index&lt;/strong&gt; of documents&lt;/li&gt; 
 &lt;li&gt;Perform reasoning-based retrieval through &lt;strong&gt;tree search&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://pageindex.ai/blog/pageindex-intro" target="_blank" title="The PageIndex Framework"&gt; &lt;img src="https://docs.pageindex.ai/images/cookbook/vectorless-rag.png" width="70%" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h3&gt;ðŸŽ¯ Core Features&lt;/h3&gt; 
&lt;p&gt;Compared to traditional vector-based RAG, &lt;strong&gt;PageIndex&lt;/strong&gt; features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;No Vector DB&lt;/strong&gt;: Uses document structure and LLM reasoning for retrieval, instead of vector similarity search.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No Chunking&lt;/strong&gt;: Documents are organized into natural sections, not artificial chunks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Human-like Retrieval&lt;/strong&gt;: Simulates how human experts navigate and extract knowledge from complex documents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Better Explainability and Traceability&lt;/strong&gt;: Retrieval is based on reasoning â€” traceable and interpretable, with page and section references. No more opaque, approximate vector search (â€œvibe retrievalâ€).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;PageIndex powers a reasoning-based RAG system that achieved &lt;strong&gt;state-of-the-art&lt;/strong&gt; &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt;98.7% accuracy&lt;/a&gt; on FinanceBench, demonstrating superior performance over vector-based RAG solutions in professional document analysis (see our &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;blog post&lt;/a&gt; for details).&lt;/p&gt; 
&lt;h3&gt;ðŸ“ Explore PageIndex&lt;/h3&gt; 
&lt;p&gt;To learn more, please see a detailed introduction of the &lt;a href="https://pageindex.ai/blog/pageindex-intro"&gt;PageIndex framework&lt;/a&gt;. Check out this GitHub repo for open-source code, and the &lt;a href="https://docs.pageindex.ai/cookbook"&gt;cookbooks&lt;/a&gt;, &lt;a href="https://docs.pageindex.ai/tutorials"&gt;tutorials&lt;/a&gt;, and &lt;a href="https://pageindex.ai/blog"&gt;blog&lt;/a&gt; for additional usage guides and examples.&lt;/p&gt; 
&lt;p&gt;The PageIndex service is available as a ChatGPT-style &lt;a href="https://chat.pageindex.ai"&gt;chat platform&lt;/a&gt;, or can be integrated via &lt;a href="https://pageindex.ai/mcp"&gt;MCP&lt;/a&gt; or &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;ðŸ› ï¸ Deployment Options&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Self-host â€” run locally with this open-source repo.&lt;/li&gt; 
 &lt;li&gt;Cloud Service â€” try instantly with our &lt;a href="https://chat.pageindex.ai/"&gt;Chat Platform&lt;/a&gt;, or integrate with &lt;a href="https://pageindex.ai/mcp"&gt;MCP&lt;/a&gt; or &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;Enterprise&lt;/em&gt; â€” private or on-prem deployment. &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;Contact us&lt;/a&gt; or &lt;a href="https://calendly.com/pageindex/meet"&gt;book a demo&lt;/a&gt; for more details.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ðŸ§ª Quick Hands-on&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Try the &lt;a href="https://github.com/VectifyAI/PageIndex/raw/main/cookbook/pageindex_RAG_simple.ipynb"&gt;&lt;strong&gt;Vectorless RAG&lt;/strong&gt;&lt;/a&gt; notebook â€” a &lt;em&gt;minimal&lt;/em&gt;, hands-on example of reasoning-based RAG using PageIndex.&lt;/li&gt; 
 &lt;li&gt;Experiment with &lt;a href="https://github.com/VectifyAI/PageIndex/raw/main/cookbook/vision_RAG_pageindex.ipynb"&gt;&lt;em&gt;Vision-based Vectorless RAG&lt;/em&gt;&lt;/a&gt; â€” no OCR; a minimal, reasoning-native RAG pipeline that works directly over page images.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://colab.research.google.com/github/VectifyAI/PageIndex/blob/main/cookbook/pageindex_RAG_simple.ipynb" target="_blank" rel="noopener"&gt; &lt;img src="https://img.shields.io/badge/Open_In_Colab-Vectorless_RAG-orange?style=for-the-badge&amp;amp;logo=googlecolab" alt="Open in Colab: Vectorless RAG" /&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp; 
 &lt;a href="https://colab.research.google.com/github/VectifyAI/PageIndex/blob/main/cookbook/vision_RAG_pageindex.ipynb" target="_blank" rel="noopener"&gt; &lt;img src="https://img.shields.io/badge/Open_In_Colab-Vision_RAG-orange?style=for-the-badge&amp;amp;logo=googlecolab" alt="Open in Colab: Vision RAG" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;ðŸŒ² PageIndex Tree Structure&lt;/h1&gt; 
&lt;p&gt;PageIndex can transform lengthy PDF documents into a semantic &lt;strong&gt;tree structure&lt;/strong&gt;, similar to a &lt;em&gt;"table of contents"&lt;/em&gt; but optimized for use with Large Language Models (LLMs). It's ideal for: financial reports, regulatory filings, academic textbooks, legal or technical manuals, and any document that exceeds LLM context limits.&lt;/p&gt; 
&lt;p&gt;Below is an example PageIndex tree structure. Also see more example &lt;a href="https://github.com/VectifyAI/PageIndex/tree/main/tests/pdfs"&gt;documents&lt;/a&gt; and generated &lt;a href="https://github.com/VectifyAI/PageIndex/tree/main/tests/results"&gt;tree structures&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsonc"&gt;...
{
  "title": "Financial Stability",
  "node_id": "0006",
  "start_index": 21,
  "end_index": 22,
  "summary": "The Federal Reserve ...",
  "nodes": [
    {
      "title": "Monitoring Financial Vulnerabilities",
      "node_id": "0007",
      "start_index": 22,
      "end_index": 28,
      "summary": "The Federal Reserve's monitoring ..."
    },
    {
      "title": "Domestic and International Cooperation and Coordination",
      "node_id": "0008",
      "start_index": 28,
      "end_index": 31,
      "summary": "In 2023, the Federal Reserve collaborated ..."
    }
  ]
}
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can generate the PageIndex tree structure with this open-source repo, or use our &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;âš™ï¸ Package Usage&lt;/h1&gt; 
&lt;p&gt;You can follow these steps to generate a PageIndex tree from a PDF document.&lt;/p&gt; 
&lt;h3&gt;1. Install dependencies&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip3 install --upgrade -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Set your OpenAI API key&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file in the root directory and add your API key:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CHATGPT_API_KEY=your_openai_key_here
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Run PageIndex on your PDF&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3 run_pageindex.py --pdf_path /path/to/your/document.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Optional parameters&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; You can customize the processing with additional optional arguments: 
 &lt;pre&gt;&lt;code&gt;--model                 OpenAI model to use (default: gpt-4o-2024-11-20)
--toc-check-pages       Pages to check for table of contents (default: 20)
--max-pages-per-node    Max pages per node (default: 10)
--max-tokens-per-node   Max tokens per node (default: 20000)
--if-add-node-id        Add node ID (yes/no, default: yes)
--if-add-node-summary   Add node summary (yes/no, default: yes)
--if-add-doc-description Add doc description (yes/no, default: yes)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Markdown support&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; We also provide markdown support for PageIndex. You can use the `-md_path` flag to generate a tree structure for a markdown file. 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python3 run_pageindex.py --md_path /path/to/your/document.md
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;Note: in this function, we use "#" to determine node heading and their levels. For example, "##" is level 2, "###" is level 3, etc. Make sure your markdown file is formatted correctly. If your Markdown file was converted from a PDF or HTML, we don't recommend using this function, since most existing conversion tools cannot preserve the original hierarchy. Instead, use our &lt;a href="https://pageindex.ai/blog/ocr"&gt;PageIndex OCR&lt;/a&gt;, which is designed to preserve the original hierarchy, to convert the PDF to a markdown file and then use this function.&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;!-- 
# â˜ï¸ Improved Tree Generation with PageIndex OCR

This repo is designed for generating PageIndex tree structure for simple PDFs, but many real-world use cases involve complex PDFs that are hard to parse by classic Python tools. However, extracting high-quality text from PDF documents remains a non-trivial challenge. Most OCR tools only extract page-level content, losing the broader document context and hierarchy.

To address this, we introduced PageIndex OCR â€” the first long-context OCR model designed to preserve the global structure of documents. PageIndex OCR significantly outperforms other leading OCR tools, such as those from Mistral and Contextual AI, in recognizing true hierarchy and semantic relationships across document pages.

- Experience next-level OCR quality with PageIndex OCR at our [Dashboard](https://dash.pageindex.ai/).
- Integrate PageIndex OCR seamlessly into your stack via our [API](https://docs.pageindex.ai/quickstart).

&lt;p align="center"&gt;
  &lt;img src="https://github.com/user-attachments/assets/eb35d8ae-865c-4e60-a33b-ebbd00c41732" width="80%"&gt;
&lt;/p&gt;
--&gt; 
&lt;hr /&gt; 
&lt;h1&gt;ðŸ“ˆ Case Study: PageIndex Leads Finance QA Benchmark&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://vectify.ai/mafin"&gt;Mafin 2.5&lt;/a&gt; is a reasoning-based RAG system for financial document analysis, powered by &lt;strong&gt;PageIndex&lt;/strong&gt;. It achieved a state-of-the-art &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;&lt;strong&gt;98.7% accuracy&lt;/strong&gt;&lt;/a&gt; on the &lt;a href="https://arxiv.org/abs/2311.11944"&gt;FinanceBench&lt;/a&gt; benchmark, significantly outperforming traditional vector-based RAG systems.&lt;/p&gt; 
&lt;p&gt;PageIndex's hierarchical indexing and reasoning-driven retrieval enable precise navigation and extraction of relevant context from complex financial reports, such as SEC filings and earnings disclosures.&lt;/p&gt; 
&lt;p&gt;Explore the full &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt;benchmark results&lt;/a&gt; and our &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;blog post&lt;/a&gt; for detailed comparisons and performance metrics.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt; &lt;img src="https://github.com/user-attachments/assets/571aa074-d803-43c7-80c4-a04254b782a3" width="70%" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;ðŸ§­ Resources&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;ðŸ§ª &lt;a href="https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex"&gt;Cookbooks&lt;/a&gt;: hands-on, runnable examples and advanced use cases.&lt;/li&gt; 
 &lt;li&gt;ðŸ“– &lt;a href="https://docs.pageindex.ai/doc-search"&gt;Tutorials&lt;/a&gt;: practical guides and strategies, including &lt;em&gt;Document Search&lt;/em&gt; and &lt;em&gt;Tree Search&lt;/em&gt;.&lt;/li&gt; 
 &lt;li&gt;ðŸ“ &lt;a href="https://pageindex.ai/blog"&gt;Blog&lt;/a&gt;: technical articles, research insights, and product updates.&lt;/li&gt; 
 &lt;li&gt;ðŸ”Œ &lt;a href="https://pageindex.ai/mcp#quick-setup"&gt;MCP setup&lt;/a&gt; &amp;amp; &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API docs&lt;/a&gt;: integration details and configuration options.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h1&gt;â­ Support Us&lt;/h1&gt; 
&lt;p&gt;Leave us a star ðŸŒŸ if you like our project. Thank you!&lt;/p&gt; 
&lt;p&gt; &lt;img src="https://github.com/user-attachments/assets/eae4ff38-48ae-4a7c-b19f-eab81201d794" width="80%" /&gt; &lt;/p&gt; 
&lt;h3&gt;Connect with Us&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://x.com/PageIndexAI"&gt;&lt;img src="https://img.shields.io/badge/Twitter-000000?style=for-the-badge&amp;amp;logo=x&amp;amp;logoColor=white" alt="Twitter" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://www.linkedin.com/company/vectify-ai/"&gt;&lt;img src="https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&amp;amp;logo=linkedin&amp;amp;logoColor=white" alt="LinkedIn" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://discord.com/invite/VuXuf29EUj"&gt;&lt;img src="https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;&lt;img src="https://img.shields.io/badge/Contact_Us-3B82F6?style=for-the-badge&amp;amp;logo=envelope&amp;amp;logoColor=white" alt="Contact Us" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Â© 2025 &lt;a href="https://vectify.ai"&gt;Vectify AI&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>k4yt3x/video2x</title>
      <link>https://github.com/k4yt3x/video2x</link>
      <description>&lt;p&gt;A machine learning-based video super resolution and frame interpolation framework. Est. Hack the Valley II, 2018.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://github.com/user-attachments/assets/5cd63373-e806-474f-94ec-6e04963bf90f" alt="Video2X: A machine learning-based video super resolution and frame interpolation framework." /&gt; &lt;br /&gt; &lt;img src="https://img.shields.io/github/v/release/k4yt3x/video2x?style=flat-square" /&gt; &lt;img src="https://img.shields.io/github/downloads/k4yt3x/video2x/total?style=flat-square" /&gt; &lt;img src="https://img.shields.io/github/license/k4yt3x/video2x?style=flat-square" /&gt; &lt;img src="https://img.shields.io/github/sponsors/k4yt3x?style=flat-square&amp;amp;link=https%3A%2F%2Fgithub.com%2Fsponsors%2Fk4yt3x" /&gt; &lt;img src="https://img.shields.io/badge/dynamic/json?color=%23e85b46&amp;amp;label=Patreon&amp;amp;query=data.attributes.patron_count&amp;amp;suffix=%20patrons&amp;amp;url=https%3A%2F%2Fwww.patreon.com%2Fapi%2Fcampaigns%2F4507807&amp;amp;style=flat-square" /&gt; &lt;/p&gt; 
&lt;h2&gt;ðŸŒŸ Version 6.0.0&lt;/h2&gt; 
&lt;p&gt;Video2X 6.0.0 highlights:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Complete rewrite of the Video2X project in C/C++.&lt;/li&gt; 
 &lt;li&gt;Faster and more efficient architecture.&lt;/li&gt; 
 &lt;li&gt;Cross-platform support for Windows and Linux.&lt;/li&gt; 
 &lt;li&gt;Vastly improved output quality.&lt;/li&gt; 
 &lt;li&gt;New GUI and installer for easy setup on Windows.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to see more details&lt;/summary&gt; 
 &lt;p&gt;Version 6.0.0 is a complete rewrite of this project in C/C++. It:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;genuinely works this time, with much less hassle compared to the 5.0.0 beta;&lt;/li&gt; 
  &lt;li&gt;is blazing fast, thanks to the new optimized pipeline and the efficiency of C/C++;&lt;/li&gt; 
  &lt;li&gt;is cross-platform, available now for both Windows and Linux;&lt;/li&gt; 
  &lt;li&gt;offers significantly better output quality with Anime4K v4, Real-ESRGAN, Real-CUGAN, and RIFE;&lt;/li&gt; 
  &lt;li&gt;supports two modes: filtering (upscaling) and frame interpolation;&lt;/li&gt; 
  &lt;li&gt;supports Anime4K v4 and all custom MPV-compatible GLSL shaders;&lt;/li&gt; 
  &lt;li&gt;supports Real-ESRGAN, Real-CUGAN, and RIFE (all models) via ncnn and Vulkan;&lt;/li&gt; 
  &lt;li&gt;requires zero additional disk space during processing, just space for the final output.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/9b1cc8a7-2903-4d2c-80a2-8d81f007e45b" alt="6.4.0-screenshot" /&gt;&lt;/p&gt; 
&lt;h2&gt;ðŸ–¥ï¸ Hardware Requirements&lt;/h2&gt; 
&lt;p&gt;Your system must meet the minimum hardware requirements below to run Video2X.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;The precompiled binaries require CPUs with AVX2 support.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Intel&lt;/strong&gt;: Haswell (Q2 2013) or newer&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;AMD&lt;/strong&gt;: Excavator (Q2 2015) or newer&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GPU&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;The GPU must support Vulkan.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;NVIDIA&lt;/strong&gt;: Kepler (GTX 600 series, Q2 2012) or newer&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;AMD&lt;/strong&gt;: GCN 1.0 (Radeon HD 7000 series, Q1 2012) or newer&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Intel&lt;/strong&gt;: HD Graphics 4000 (Q2 2012) or newer&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href="https://docs.video2x.org/installing/windows-qt6.html"&gt;ðŸªŸ Install on Windows&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/k4yt3x/video2x/releases/download/6.4.0/video2x-qt6-windows-amd64-installer.exe"&gt;Download the Latest Windows Installer Executable (6.4.0)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You can download the latest Windows release on the &lt;a href="https://github.com/k4yt3x/video2x/releases/latest"&gt;releases page&lt;/a&gt;. For basic GUI usage, refer to the &lt;a href="https://docs.video2x.org/running/desktop.html"&gt;documentation&lt;/a&gt;. If you're unable to download directly from GitHub, try the &lt;a href="https://files.k4yt3x.com"&gt;mirror site&lt;/a&gt;. The GUI currently supports the following languages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;English (United States)&lt;/li&gt; 
 &lt;li&gt;ç®€ä½“ä¸­æ–‡ï¼ˆä¸­å›½ï¼‰&lt;/li&gt; 
 &lt;li&gt;æ—¥æœ¬èªžï¼ˆæ—¥æœ¬ï¼‰&lt;/li&gt; 
 &lt;li&gt;PortuguÃªs (Portugal)&lt;/li&gt; 
 &lt;li&gt;FranÃ§ais (France)&lt;/li&gt; 
 &lt;li&gt;Deutsch (Deutschland)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href="https://docs.video2x.org/installing/linux.html"&gt;ðŸ§ Install on Linux&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Video2X packages are available for the Linux distros listed below. A universal AppImage is also available for other distros. If you'd like to build it from source code, refer to the &lt;a href="https://raw.githubusercontent.com/k4yt3x/video2x/master/packaging/arch/PKGBUILD"&gt;PKGBUILD&lt;/a&gt; file for a general overview of the required dependencies and commands.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Arch Linux: AUR packages, maintained by &lt;a href="https://github.com/k4yt3x"&gt;@K4YT3X&lt;/a&gt;. 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://aur.archlinux.org/packages/video2x"&gt;aur/video2x&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://aur.archlinux.org/packages/video2x-git"&gt;aur/video2x-git&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://aur.archlinux.org/packages/video2x-qt6"&gt;aur/video2x-qt6&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://aur.archlinux.org/packages/video2x-qt6-git"&gt;aur/video2x-qt6-git&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Arch Linux (Chinese Mainland): archlinuxcn packages, maintained by &lt;a href="https://github.com/Integral-Tech"&gt;@Integral-Tech&lt;/a&gt;. 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/archlinuxcn/repo/tree/master/archlinuxcn/video2x"&gt;archlinuxcn/video2x&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/archlinuxcn/repo/tree/master/archlinuxcn/video2x-git"&gt;archlinuxcn/video2x-git&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/archlinuxcn/repo/tree/master/archlinuxcn/video2x-qt6"&gt;archlinuxcn/video2x-qt6&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/archlinuxcn/repo/tree/master/archlinuxcn/video2x-qt6-git"&gt;archlinuxcn/video2x-qt6-git&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Other distros: &lt;code&gt;Video2X-x86_64.AppImage&lt;/code&gt; on the &lt;a href="https://github.com/k4yt3x/video2x/releases/latest"&gt;releases page&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href="https://docs.video2x.org/running/container.html"&gt;ðŸ“¦ Container Image&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Video2X &lt;a href="https://github.com/k4yt3x/video2x/pkgs/container/video2x"&gt;container images&lt;/a&gt; are available on the GitHub Container Registry for easy deployment on Linux and macOS. If you already have Docker/Podman installed, only one command is needed to start upscaling a video. For more information on how to use Video2X's Docker image, please refer to the &lt;a href="https://docs.video2x.org/running/container.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://colab.research.google.com/drive/1gWEwcA9y57EsxwOjmLNmNMXPsafw0kGo"&gt;ðŸ“” Google Colab&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;You can use Video2X on &lt;a href="https://colab.research.google.com/"&gt;Google Colab&lt;/a&gt; &lt;strong&gt;for free&lt;/strong&gt; if you don't have a powerful GPU of your own. You can borrow a powerful GPU (NVIDIA T4, L4, or A100) on Google's server for free for a maximum of 12 hours per session. &lt;strong&gt;Please use the free resource fairly&lt;/strong&gt; and do not create sessions back-to-back and run upscaling 24/7. This might result in you getting banned. You can get &lt;a href="https://colab.research.google.com/signup/pricing"&gt;Colab Pro/Pro+&lt;/a&gt; if you'd like to use better GPUs and get longer runtimes. Usage instructions are embedded in the &lt;a href="https://colab.research.google.com/drive/1gWEwcA9y57EsxwOjmLNmNMXPsafw0kGo"&gt;Colab Notebook&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://t.me/video2x"&gt;ðŸ’¬ Telegram Discussion Group&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Join our Telegram discussion group to ask any questions you have about Video2X, chat directly with the developers, or discuss super resolution, frame interpolation technologies, or the future of Video2X in general.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://docs.video2x.org/"&gt;ðŸ“– Documentation&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Comprehensive documentation for Video2X is available at &lt;a href="https://docs.video2x.org/"&gt;https://docs.video2x.org/&lt;/a&gt;. It offers detailed instructions on how to &lt;a href="https://docs.video2x.org/building/index.html"&gt;build&lt;/a&gt;, &lt;a href="https://docs.video2x.org/installing/index.html"&gt;install&lt;/a&gt;, &lt;a href="https://docs.video2x.org/running/index.html"&gt;use&lt;/a&gt;, and &lt;a href="https://docs.video2x.org/developing/index.html"&gt;develop&lt;/a&gt; with this program.&lt;/p&gt; 
&lt;h2&gt;ðŸ“½ï¸ Video Demos (Outdated)&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/21986859/49412428-65083280-f73a-11e8-8237-bb34158a545e.png" alt="Spirited Away Demo" /&gt;&lt;br /&gt; &lt;em&gt;Upscale demo: Spirited Away's movie trailer&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Spirited Away&lt;/strong&gt;: &lt;a href="https://youtu.be/mGEfasQl2Zo"&gt;YouTube&lt;/a&gt; | &lt;a href="https://www.bilibili.com/video/BV1V5411471i/"&gt;Bilibili&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;360P to 4K&lt;/li&gt; 
   &lt;li&gt;The &lt;a href="https://www.youtube.com/watch?v=ByXuk9QqQkk"&gt;original video&lt;/a&gt;'s copyright belongs to æ ªå¼ä¼šç¤¾ã‚¹ã‚¿ã‚¸ã‚ªã‚¸ãƒ–ãƒª&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bad Apple!!&lt;/strong&gt;: &lt;a href="https://youtu.be/A81rW_FI3cw"&gt;YouTube&lt;/a&gt; | &lt;a href="https://www.bilibili.com/video/BV16K411K7ue"&gt;Bilibili&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;384P 30 FPS to 4K 120 FPS with waifu2x and DAIN&lt;/li&gt; 
   &lt;li&gt;The &lt;a href="https://www.nicovideo.jp/watch/sm8628149"&gt;original video&lt;/a&gt;'s copyright belongs to ã‚ã«ã‚‰&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;The Pet Girl of Sakurasou&lt;/strong&gt;: &lt;a href="https://youtu.be/M0vDI1HH2_Y"&gt;YouTube&lt;/a&gt; | &lt;a href="https://www.bilibili.com/video/BV14k4y167KP/"&gt;Bilibili&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;240P 29.97 to 1080P 60 FPS with waifu2x and DAIN&lt;/li&gt; 
   &lt;li&gt;The original video's copyright belongs to ASCII Media Works&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Standard Test Clip&lt;/h3&gt; 
&lt;p&gt;The following clip can be used to test if your setup works properly. This is also the standard clip used for running performance benchmarks.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://files.k4yt3x.com/resources/videos/standard-test.mp4"&gt;Standard Test Clip (240P)&lt;/a&gt; 4.54 MiB&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://files.k4yt3x.com/resources/videos/standard-realcugan.mp4"&gt;Real-CUGAN Upscaled Sample (1704P)&lt;/a&gt; 3.5 MiB&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://files.k4yt3x.com/resources/videos/standard-realesrgan.mp4"&gt;Real-ESRGAN Upscaled Sample (1704P)&lt;/a&gt; 3.1 MiB&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://files.k4yt3x.com/resources/videos/standard-waifu2x.mp4"&gt;waifu2x Upscaled Sample (1080P)&lt;/a&gt; 4.54 MiB&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://files.k4yt3x.com/resources/videos/standard-original.mp4"&gt;Ground Truth (1080P)&lt;/a&gt; 22.2 MiB&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The original clip came from the anime "ã•ãã‚‰è˜ã®ãƒšãƒƒãƒˆãªå½¼å¥³."&lt;br /&gt; Copyright of this clip belongs to æ ªå¼ä¼šç¤¾ã‚¢ãƒ‹ãƒ—ãƒ¬ãƒƒã‚¯ã‚¹.&lt;/p&gt; 
&lt;h2&gt;âš–ï¸ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under &lt;a href="https://www.gnu.org/licenses/agpl-3.0.txt"&gt;GNU AGPL version 3&lt;/a&gt;.&lt;br /&gt; Copyright (C) 2018-2025 K4YT3X and &lt;a href="https://github.com/k4yt3x/video2x/graphs/contributors"&gt;contributors&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://www.gnu.org/graphics/agplv3-155x51.png" alt="AGPLv3" /&gt;&lt;/p&gt; 
&lt;p&gt;This project includes or depends on these following projects:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Project&lt;/th&gt; 
   &lt;th&gt;License&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.ffmpeg.org/"&gt;FFmpeg/FFmpeg&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;LGPLv2.1, GPLv2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Tencent/ncnn"&gt;Tencent/ncnn&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;BSD 3-Clause&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/bloc97/Anime4K"&gt;bloc97/Anime4K&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;MIT License&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/nihui/realcugan-ncnn-vulkan"&gt;nihui/realcugan-ncnn-vulkan&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;MIT License&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/nihui/rife-ncnn-vulkan"&gt;nihui/rife-ncnn-vulkan&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;MIT License&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/xinntao/Real-ESRGAN-ncnn-vulkan"&gt;xinntao/Real-ESRGAN-ncnn-vulkan&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;MIT License&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;More licensing information can be found in the &lt;a href="https://raw.githubusercontent.com/k4yt3x/video2x/master/NOTICE"&gt;NOTICE&lt;/a&gt; file.&lt;/p&gt; 
&lt;h2&gt;ðŸŒº Special Thanks&lt;/h2&gt; 
&lt;p&gt;Special thanks to the following individuals for their significant contributions to the project, listed in alphabetical order.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/archiemeng"&gt;@ArchieMeng&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/BrianPetkovsek"&gt;@BrianPetkovsek&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Integral-Tech"&gt;@Integral-Tech&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ddouglas87"&gt;@ddouglas87&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/lhanjian"&gt;@lhanjian&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nihui"&gt;@nihui&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sat3ll"&gt;@sat3ll&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>supermemoryai/supermemory</title>
      <link>https://github.com/supermemoryai/supermemory</link>
      <description>&lt;p&gt;Memory engine and app that is extremely fast, scalable. The Memory API for the AI era.&lt;/p&gt;&lt;hr&gt;&lt;p align="center" style="padding-bottom:20px;padding-top:20px"&gt; 
 &lt;picture&gt; 
  &lt;source srcset="apps/web/public/logo-fullmark.svg" media="(prefers-color-scheme: dark)" /&gt; 
  &lt;source srcset="apps/web/public/logo-light-fullmark.svg" media="(prefers-color-scheme: light)" /&gt; 
  &lt;img src="https://raw.githubusercontent.com/supermemoryai/supermemory/main/apps/web/public/logo-fullmark.svg?sanitize=true" alt="supermemory Logo" width="400" /&gt; 
 &lt;/picture&gt; &lt;br /&gt;&lt;br /&gt; &lt;em&gt;Your AI second brain for saving and organizing everything that matters.&lt;/em&gt; &lt;br /&gt;&lt;br /&gt; &lt;a href="https://app.supermemory.ai" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/Web-App-000000?style=for-the-badge" alt="Web App" /&gt; &lt;/a&gt; &amp;nbsp; &lt;a href="https://chromewebstore.google.com/detail/supermemory/afpgkkipfdpeaflnpoaffkcankadgjfc" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/Chrome-Extension-4285F4?style=for-the-badge&amp;amp;logo=googlechrome&amp;amp;logoColor=white" alt="Chrome Extension" /&gt; &lt;/a&gt; &amp;nbsp; &lt;a href="https://www.raycast.com/supermemory/supermemory" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/Raycast-Extension-FF6363?style=for-the-badge&amp;amp;logo=raycast&amp;amp;logoColor=white" alt="Raycast Extension" /&gt; &lt;/a&gt; &amp;nbsp; &lt;a href="https://supermemory.link/discord" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p style="font-size: 0.9em; color: #666;"&gt; &lt;strong&gt;Building with Supermemory?&lt;/strong&gt; Check out the &lt;a href="https://console.supermemory.ai?utm_source=github&amp;amp;utm_medium=readme&amp;amp;utm_campaign=consumer_app"&gt;Developer Console&lt;/a&gt; and &lt;a href="https://docs.supermemory.ai?utm_source=github&amp;amp;utm_medium=readme&amp;amp;utm_campaign=consumer_app"&gt;Documentation&lt;/a&gt; for API access. &lt;/p&gt; 
&lt;p style="font-size: 0.9em; color: #666;"&gt; &lt;strong&gt;Want to self-host?&lt;/strong&gt; See our &lt;a href="https://supermemory.ai/docs/deployment/self-hosting#self-hosting"&gt;Self-Hosting Guide&lt;/a&gt; for enterprise deployment options. &lt;/p&gt; 
&lt;br /&gt; 
&lt;div align="center" style="padding-bottom:10px;padding-top:10px"&gt; 
 &lt;img src="https://raw.githubusercontent.com/supermemoryai/supermemory/main/apps/web/public/landing-page.jpeg" alt="supermemory" width="100%" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;h3&gt;Core Functionality&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/supermemoryai/supermemory/main/#add-memory"&gt;Add Memories from Any Content&lt;/a&gt;&lt;/strong&gt;: Easily add memories from URLs, PDFs, and plain textâ€”just paste, upload, or link.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/supermemoryai/supermemory/main/#chat-memories"&gt;Chat with Your Memories&lt;/a&gt;&lt;/strong&gt;: Converse with your stored content using natural language chat.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/supermemoryai/supermemory/main/#mcp-integration"&gt;Supermemory MCP Integration&lt;/a&gt;&lt;/strong&gt;: Seamlessly connect with all major AI tools (Claude, Cursor, etc.) via Supermemory MCP.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/supermemoryai/supermemory/main/#browser-extension"&gt;Browser Extension&lt;/a&gt;&lt;/strong&gt;: Save memories directly from your browser with integrations for ChatGPT, Claude, and Twitter/X.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/supermemoryai/supermemory/main/#raycast-extension"&gt;Raycast Extension&lt;/a&gt;&lt;/strong&gt;: Add and search memories directly from Raycast with keyboard shortcuts.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How do I use this?&lt;/h2&gt; 
&lt;p&gt;Go to &lt;a href="https://app.supermemory.ai"&gt;app.supermemory.ai&lt;/a&gt; and sign in with your account&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a id="add-memory"&gt;&lt;/a&gt;Start Adding Memory with your choice of format (Note, Link, File)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center" style="padding-bottom:10px;padding-top:10px"&gt; 
 &lt;img src="https://raw.githubusercontent.com/supermemoryai/supermemory/main/apps/web/public/add-memory.png" alt="supermemory" width="100%" /&gt; 
&lt;/div&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;You can also Connect to your favourite services (Notion, Google Drive, OneDrive)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center" style="padding-bottom:10px;padding-top:10px"&gt; 
 &lt;img src="https://raw.githubusercontent.com/supermemoryai/supermemory/main/apps/web/public/add-connections.png" alt="supermemory" width="100%" /&gt; 
&lt;/div&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;&lt;a id="chat-memories"&gt;&lt;/a&gt;Once Memories are added, you can chat with Supermemory by clicking on "Open Chat" and retrieve info from your saved memories&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center" style="padding-bottom:10px;padding-top:10px"&gt; 
 &lt;img src="https://raw.githubusercontent.com/supermemoryai/supermemory/main/apps/web/public/chat.png" alt="supermemory" width="100%" /&gt; 
&lt;/div&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;&lt;a id="mcp-integration"&gt;&lt;/a&gt;Add MCP to your AI Tools (by clicking on "Connect to your AI" and select the AI tool you are trying to integrate)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center" style="padding-bottom:10px;padding-top:10px"&gt; 
 &lt;img src="https://raw.githubusercontent.com/supermemoryai/supermemory/main/apps/web/public/mcp.png" alt="supermemory" width="100%" /&gt; 
&lt;/div&gt; 
&lt;ol start="5"&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a id="browser-extension"&gt;&lt;/a&gt;&lt;strong&gt;Browser Extension&lt;/strong&gt;: Install the &lt;a href="https://chromewebstore.google.com/detail/supermemory/afpgkkipfdpeaflnpoaffkcankadgjfc"&gt;Chrome/Edge extension&lt;/a&gt; to save memories directly from any webpage, integrate with ChatGPT and Claude conversations, and import from Twitter/X. Right-click on any content or use the extension popup to save memories instantly.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a id="raycast-extension"&gt;&lt;/a&gt;&lt;strong&gt;Raycast Extension&lt;/strong&gt;: Install the &lt;a href="https://www.raycast.com/supermemory/supermemory"&gt;Raycast extension&lt;/a&gt; to add and search memories directly from Raycast. Use the "Add Memory" command to quickly save content, or "Search Memories" to find and retrieve your saved information with keyboard shortcuts.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;Have questions or feedback? We're here to help:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Email: &lt;a href="mailto:support@supermemory.ai"&gt;support@supermemory.ai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Discord: &lt;a href="https://supermemory.link/discord"&gt;Join our Discord server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Documentation: &lt;a href="https://docs.supermemory.ai"&gt;docs.supermemory.ai&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from developers of all skill levels! Whether you're fixing bugs, adding features, or improving documentation, your help makes supermemory better for everyone.&lt;/p&gt; 
&lt;p&gt;For detailed guidelines, development setup, coding standards, and the complete contribution workflow, please see our &lt;a href="https://raw.githubusercontent.com/supermemoryai/supermemory/main/CONTRIBUTING.md"&gt;&lt;strong&gt;Contributing Guide&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Ways to Contribute&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ðŸ› &lt;strong&gt;Bug fixes&lt;/strong&gt; - Help us squash those pesky issues&lt;/li&gt; 
 &lt;li&gt;âœ¨ &lt;strong&gt;New features&lt;/strong&gt; - Add functionality that users will love&lt;/li&gt; 
 &lt;li&gt;ðŸŽ¨ &lt;strong&gt;UI/UX improvements&lt;/strong&gt; - Make the interface more intuitive&lt;/li&gt; 
 &lt;li&gt;âš¡ &lt;strong&gt;Performance optimizations&lt;/strong&gt; - Help us make supermemory faster&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Check out our &lt;a href="https://github.com/supermemoryai/supermemory/issues"&gt;Issues&lt;/a&gt; page for &lt;code&gt;good first issue&lt;/code&gt; and &lt;code&gt;help wanted&lt;/code&gt; labels to get started!&lt;/p&gt; 
&lt;h2&gt;Updates &amp;amp; Roadmap&lt;/h2&gt; 
&lt;p&gt;Stay up to date with the latest improvements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.supermemory.ai/changelog/overview"&gt;Changelog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://x.com/supermemory"&gt;X&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>