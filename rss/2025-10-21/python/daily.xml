<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Mon, 20 Oct 2025 01:37:07 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>OpenMind/OM1</title>
      <link>https://github.com/OpenMind/OM1</link>
      <description>&lt;p&gt;Modular AI runtime for robots&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/853153b7-351a-433d-9e1a-d257b781f93c" alt="OM_Banner_X2 (1)" /&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://arxiv.org/abs/2412.18588"&gt;Technical Paper&lt;/a&gt; | &lt;a href="https://docs.openmind.org/"&gt;Documentation&lt;/a&gt; | &lt;a href="https://x.com/openmind_agi"&gt;X&lt;/a&gt; | &lt;a href="https://discord.gg/VUjpg4ef5n"&gt;Discord&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;OpenMind's OM1 is a modular AI runtime that empowers developers to create and deploy multimodal AI agents across digital environments and physical robots&lt;/strong&gt;, including Humanoids, Phone Apps, websites, Quadrupeds, and educational robots such as TurtleBot 4. OM1 agents can process diverse inputs like web data, social media, camera feeds, and LIDAR, while enabling physical actions including motion, autonomous navigation, and natural conversations. The goal of OM1 is to make it easy to create highly capable human-focused robots, that are easy to upgrade and (re)configure to accommodate different physical form factors.&lt;/p&gt; 
&lt;h2&gt;Capabilities of OM1&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Modular Architecture&lt;/strong&gt;: Designed with Python for simplicity and seamless integration.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Input&lt;/strong&gt;: Easily handles new data and sensors.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Hardware Support via Plugins&lt;/strong&gt;: Supports new hardware through plugins for API endpoints and specific robot hardware connections to &lt;code&gt;ROS2&lt;/code&gt;, &lt;code&gt;Zenoh&lt;/code&gt;, and &lt;code&gt;CycloneDDS&lt;/code&gt;. (We recommend &lt;code&gt;Zenoh&lt;/code&gt; for all new development).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Web-Based Debugging Display&lt;/strong&gt;: Monitor the system in action with WebSim (available at &lt;a href="http://localhost:8000/"&gt;http://localhost:8000/&lt;/a&gt;) for easy visual debugging.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Pre-configured Endpoints&lt;/strong&gt;: Supports Voice-to-Speech, OpenAI’s &lt;code&gt;gpt-4o&lt;/code&gt;, DeepSeek, and multiple Visual Language Models (VLMs) with pre-configured endpoints for each service.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Architecture Overview&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/14e9b916-4df7-4700-9336-2983c85be311" alt="Artboard 1@4x 1 (1)" /&gt;&lt;/p&gt; 
&lt;h2&gt;Getting Started - Hello World&lt;/h2&gt; 
&lt;p&gt;To get started with OM1, let's run the Spot agent. Spot uses your webcam to capture and label objects. These text captions are then sent to &lt;code&gt;OpenAI 4o&lt;/code&gt;, which returns &lt;code&gt;movement&lt;/code&gt;, &lt;code&gt;speech&lt;/code&gt; and &lt;code&gt;face&lt;/code&gt; action commands. These commands are displayed on WebSim along with basic timing and other debugging information.&lt;/p&gt; 
&lt;h3&gt;Package Management and VENV&lt;/h3&gt; 
&lt;p&gt;You will need the &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;&lt;code&gt;uv&lt;/code&gt; package manager&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Clone the Repo&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/openmind/OM1.git
cd OM1
git submodule update --init
uv venv
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install Dependencies&lt;/h3&gt; 
&lt;p&gt;For MacOS&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew install portaudio ffmpeg
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Linux&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update
sudo apt-get install portaudio19-dev python-dev ffmpeg
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Obtain an OpenMind API Key&lt;/h3&gt; 
&lt;p&gt;Obtain your API Key at &lt;a href="https://portal.openmind.org/"&gt;OpenMind Portal&lt;/a&gt;. Copy it to &lt;code&gt;config/spot.json5&lt;/code&gt;, replacing the &lt;code&gt;openmind_free&lt;/code&gt; placeholder. Or, &lt;code&gt;cp env.example .env&lt;/code&gt; and add your key to the &lt;code&gt;.env&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Launching OM1&lt;/h3&gt; 
&lt;p&gt;Run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run src/run.py spot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After launching OM1, the Spot agent will interact with you and perform (simulated) actions. For more help connecting OM1 to your robot hardware, see &lt;a href="https://docs.openmind.org/getting-started"&gt;getting started&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;What's Next?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Try out some &lt;a href="https://docs.openmind.org/examples"&gt;examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Add new &lt;code&gt;inputs&lt;/code&gt; and &lt;code&gt;actions&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Design custom agents and robots by creating your own &lt;code&gt;json5&lt;/code&gt; config files with custom combinations of inputs and actions.&lt;/li&gt; 
 &lt;li&gt;Change the system prompts in the configuration files (located in &lt;code&gt;/config/&lt;/code&gt;) to create new behaviors.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Interfacing with New Robot Hardware&lt;/h2&gt; 
&lt;p&gt;OM1 assumes that robot hardware provides a high-level SDK that accepts elemental movement and action commands such as &lt;code&gt;backflip&lt;/code&gt;, &lt;code&gt;run&lt;/code&gt;, &lt;code&gt;gently pick up the red apple&lt;/code&gt;, &lt;code&gt;move(0.37, 0, 0)&lt;/code&gt;, and &lt;code&gt;smile&lt;/code&gt;. An example is provided in &lt;code&gt;actions/move_safe/connector/ros2.py&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;...
elif output_interface.action == "shake paw":
    if self.sport_client:
        self.sport_client.Hello()
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If your robot hardware does not yet provide a suitable HAL (hardware abstraction layer), traditional robotics approaches such as RL (reinforcement learning) in concert with suitable simulation environments (Unity, Gazebo), sensors (such as hand mounted ZED depth cameras), and custom VLAs will be needed for you to create one. It is further assumed that your HAL accepts motion trajectories, provides battery and thermal management/monitoring, and calibrates and tunes sensors such as IMUs, LIDARs, and magnetometers.&lt;/p&gt; 
&lt;p&gt;OM1 can interface with your HAL via USB, serial, ROS2, CycloneDDS, Zenoh, or websockets. For an example of an advanced humanoid HAL, please see &lt;a href="https://github.com/unitreerobotics/unitree_sdk2/raw/adee312b081c656ecd0bb4e936eed96325546296/example/g1/high_level/g1_loco_client_example.cpp#L159"&gt;Unitree's C++ SDK&lt;/a&gt;. Frequently, a HAL, especially ROS2 code, will be dockerized and can then interface with OM1 through DDS middleware or websockets.&lt;/p&gt; 
&lt;h2&gt;Recommended Development Platforms&lt;/h2&gt; 
&lt;p&gt;OM1 is developed on:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Jetson AGX Orin 64GB (running Ubuntu 22.04 and JetPack 6.1)&lt;/li&gt; 
 &lt;li&gt;Mac Studio with Apple M2 Ultra with 48 GB unified memory (running MacOS Sequoia)&lt;/li&gt; 
 &lt;li&gt;Mac Mini with Apple M4 Pro with 48 GB unified memory (running MacOS Sequoia)&lt;/li&gt; 
 &lt;li&gt;Generic Linux machines (running Ubuntu 22.04)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;OM1 &lt;em&gt;should&lt;/em&gt; run on other platforms (such as Windows) and microcontrollers such as the Raspberry Pi 5 16GB.&lt;/p&gt; 
&lt;h2&gt;Full Autonomy Guidance&lt;/h2&gt; 
&lt;p&gt;We're excited to introduce &lt;strong&gt;full autonomy mode&lt;/strong&gt;, where three services work together in a loop without manual intervention:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;om1&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;unitree_go2_ros2_sdk&lt;/strong&gt; – A ROS 2 package that provides SLAM (Simultaneous Localization and Mapping) capabilities for the Unitree Go2 robot using an RPLiDAR sensor, the SLAM Toolbox and the Nav2 stack.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;om1-avatar&lt;/strong&gt; – A modern React-based frontend application that provides the user interface and avatar display system for OM1 robotics software.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Intro to Backpack?&lt;/h2&gt; 
&lt;p&gt;From research to real-world autonomy, a platform that learns, moves, and builds with you. We'll shortly be releasing the &lt;strong&gt;BOM&lt;/strong&gt; and details on &lt;strong&gt;DIY&lt;/strong&gt; for the it. Stay tuned!&lt;/p&gt; 
&lt;p&gt;Clone the following repos -&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenMind/OM1.git"&gt;https://github.com/OpenMind/OM1.git&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenMind/unitree_go2_ros2_sdk.git"&gt;https://github.com/OpenMind/unitree_go2_ros2_sdk.git&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenMind/OM1-avatar.git"&gt;https://github.com/OpenMind/OM1-avatar.git&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Starting the system&lt;/h2&gt; 
&lt;p&gt;To start all services, run the following commands:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For OM1&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Setup the API key&lt;/p&gt; 
&lt;p&gt;For Bash: vim ~/.bashrc or ~/.bash_profile.&lt;/p&gt; 
&lt;p&gt;For Zsh: vim ~/.zshrc.&lt;/p&gt; 
&lt;p&gt;Add&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OM_API_KEY="your_api_key"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Update the docker-compose file. Replace "unitree_go2_autonomy_advance" with the agent you want to run.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;command: ["unitree_go2_autonomy_advance"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd OM1
docker-compose up om1 -d --no-build
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;For unitree_go2_ros2_sdk&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd unitree_go2_ros2_sdk
docker-compose up orchestrator -d --no-build
docker-compose up om1_sensor -d --no-build
docker-compose up watchdog -d --no-build
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;For OM1-avatar&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd OM1-avatar
docker-compose up om1_avatar -d --no-build
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Detailed Documentation&lt;/h2&gt; 
&lt;p&gt;More detailed documentation can be accessed at &lt;a href="https://docs.openmind.org/"&gt;docs.openmind.org&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Please make sure to read the &lt;a href="https://raw.githubusercontent.com/OpenMind/OM1/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; before making a pull request.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the terms of the MIT License, which is a permissive free software license that allows users to freely use, modify, and distribute the software. The MIT License is a widely used and well-established license that is known for its simplicity and flexibility. By using the MIT License, this project aims to encourage collaboration, modification, and distribution of the software.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>numpy/numpy</title>
      <link>https://github.com/numpy/numpy</link>
      <description>&lt;p&gt;The fundamental package for scientific computing with Python.&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; &lt;img src="https://raw.githubusercontent.com/numpy/numpy/main/branding/logo/primary/numpylogo.svg?sanitize=true" width="300" /&gt; &lt;/h1&gt;
&lt;br /&gt; 
&lt;p&gt;&lt;a href="https://numfocus.org"&gt;&lt;img src="https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&amp;amp;colorA=E1523D&amp;amp;colorB=007D8A" alt="Powered by NumFOCUS" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/numpy/"&gt;&lt;img src="https://img.shields.io/pypi/dm/numpy.svg?label=PyPI%20downloads" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://anaconda.org/conda-forge/numpy"&gt;&lt;img src="https://img.shields.io/conda/dn/conda-forge/numpy.svg?label=Conda%20downloads" alt="Conda Downloads" /&gt;&lt;/a&gt; &lt;a href="https://stackoverflow.com/questions/tagged/numpy"&gt;&lt;img src="https://img.shields.io/badge/stackoverflow-Ask%20questions-blue.svg?sanitize=true" alt="Stack Overflow" /&gt;&lt;/a&gt; &lt;a href="https://doi.org/10.1038/s41586-020-2649-2"&gt;&lt;img src="https://img.shields.io/badge/DOI-10.1038%2Fs41586--020--2649--2-blue" alt="Nature Paper" /&gt;&lt;/a&gt; &lt;a href="https://insights.linuxfoundation.org/project/numpy"&gt;&lt;img src="https://insights.linuxfoundation.org/api/badge/health-score?project=numpy" alt="LFX Health Score" /&gt;&lt;/a&gt; &lt;a href="https://securityscorecards.dev/viewer/?uri=github.com/numpy/numpy"&gt;&lt;img src="https://api.securityscorecards.dev/projects/github.com/numpy/numpy/badge" alt="OpenSSF Scorecard" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/numpy/"&gt;&lt;img src="https://img.shields.io/pypi/types/numpy" alt="Typing" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;NumPy is the fundamental package for scientific computing with Python.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Website:&lt;/strong&gt; &lt;a href="https://numpy.org"&gt;https://numpy.org&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation:&lt;/strong&gt; &lt;a href="https://numpy.org/doc"&gt;https://numpy.org/doc&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Mailing list:&lt;/strong&gt; &lt;a href="https://mail.python.org/mailman/listinfo/numpy-discussion"&gt;https://mail.python.org/mailman/listinfo/numpy-discussion&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Source code:&lt;/strong&gt; &lt;a href="https://github.com/numpy/numpy"&gt;https://github.com/numpy/numpy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Contributing:&lt;/strong&gt; &lt;a href="https://numpy.org/devdocs/dev/index.html"&gt;https://numpy.org/devdocs/dev/index.html&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bug reports:&lt;/strong&gt; &lt;a href="https://github.com/numpy/numpy/issues"&gt;https://github.com/numpy/numpy/issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Report a security vulnerability:&lt;/strong&gt; &lt;a href="https://tidelift.com/docs/security"&gt;https://tidelift.com/docs/security&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;It provides:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;a powerful N-dimensional array object&lt;/li&gt; 
 &lt;li&gt;sophisticated (broadcasting) functions&lt;/li&gt; 
 &lt;li&gt;tools for integrating C/C++ and Fortran code&lt;/li&gt; 
 &lt;li&gt;useful linear algebra, Fourier transform, and random number capabilities&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Testing:&lt;/p&gt; 
&lt;p&gt;NumPy requires &lt;code&gt;pytest&lt;/code&gt; and &lt;code&gt;hypothesis&lt;/code&gt;. Tests can then be run after installation with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python -c "import numpy, sys; sys.exit(numpy.test() is False)"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Code of Conduct&lt;/h2&gt; 
&lt;p&gt;NumPy is a community-driven open source project developed by a diverse group of &lt;a href="https://numpy.org/teams/"&gt;contributors&lt;/a&gt;. The NumPy leadership has made a strong commitment to creating an open, inclusive, and positive community. Please read the &lt;a href="https://numpy.org/code-of-conduct/"&gt;NumPy Code of Conduct&lt;/a&gt; for guidance on how to interact with others in a way that makes our community thrive.&lt;/p&gt; 
&lt;h2&gt;Call for Contributions&lt;/h2&gt; 
&lt;p&gt;The NumPy project welcomes your expertise and enthusiasm!&lt;/p&gt; 
&lt;p&gt;Small improvements or fixes are always appreciated. If you are considering larger contributions to the source code, please contact us through the &lt;a href="https://mail.python.org/mailman/listinfo/numpy-discussion"&gt;mailing list&lt;/a&gt; first.&lt;/p&gt; 
&lt;p&gt;Writing code isn’t the only way to contribute to NumPy. You can also:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;review pull requests&lt;/li&gt; 
 &lt;li&gt;help us stay on top of new and old issues&lt;/li&gt; 
 &lt;li&gt;develop tutorials, presentations, and other educational materials&lt;/li&gt; 
 &lt;li&gt;maintain and improve &lt;a href="https://github.com/numpy/numpy.org"&gt;our website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;develop graphic design for our brand assets and promotional materials&lt;/li&gt; 
 &lt;li&gt;translate website content&lt;/li&gt; 
 &lt;li&gt;help with outreach and onboard new contributors&lt;/li&gt; 
 &lt;li&gt;write grant proposals and help with other fundraising efforts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For more information about the ways you can contribute to NumPy, visit &lt;a href="https://numpy.org/contribute/"&gt;our website&lt;/a&gt;. If you’re unsure where to start or how your skills fit in, reach out! You can ask on the mailing list or here, on GitHub, by opening a new issue or leaving a comment on a relevant issue that is already open.&lt;/p&gt; 
&lt;p&gt;Our preferred channels of communication are all public, but if you’d like to speak to us in private first, contact our community coordinators at &lt;a href="mailto:numpy-team@googlegroups.com"&gt;numpy-team@googlegroups.com&lt;/a&gt; or on Slack (write &lt;a href="mailto:numpy-team@googlegroups.com"&gt;numpy-team@googlegroups.com&lt;/a&gt; for an invitation).&lt;/p&gt; 
&lt;p&gt;We also have a biweekly community call, details of which are announced on the mailing list. You are very welcome to join.&lt;/p&gt; 
&lt;p&gt;If you are new to contributing to open source, &lt;a href="https://opensource.guide/how-to-contribute/"&gt;this guide&lt;/a&gt; helps explain why, what, and how to successfully get involved.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>EbookFoundation/free-programming-books</title>
      <link>https://github.com/EbookFoundation/free-programming-books</link>
      <description>&lt;p&gt;📚 Freely available programming books&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;List of Free Learning Resources In Many Languages&lt;/h1&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/sindresorhus/awesome"&gt;&lt;img src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true" alt="Awesome" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://creativecommons.org/licenses/by/4.0/"&gt;&lt;img src="https://img.shields.io/github/license/EbookFoundation/free-programming-books" alt="License: CC BY 4.0" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Apr+is%3Amerged+created%3A2025-10-01..2025-10-31"&gt;&lt;img src="https://img.shields.io/github/hacktoberfest/2025/EbookFoundation/free-programming-books?label=Hacktoberfest+2025" alt="Hacktoberfest 2025 stats" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;Search the list at &lt;a href="https://ebookfoundation.github.io/free-programming-books-search/"&gt;https://ebookfoundation.github.io/free-programming-books-search/&lt;/a&gt; &lt;a href="https://ebookfoundation.github.io/free-programming-books-search/"&gt;&lt;img src="https://img.shields.io/website?style=flat&amp;amp;logo=www&amp;amp;logoColor=whitesmoke&amp;amp;label=Dynamic%20search%20site&amp;amp;down_color=red&amp;amp;down_message=down&amp;amp;up_color=green&amp;amp;up_message=up&amp;amp;url=https%3A%2F%2Febookfoundation.github.io%2Ffree-programming-books-search%2F" alt="https://ebookfoundation.github.io/free-programming-books-search/" /&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This page is available as an easy-to-read website. Access it by clicking on &lt;a href="https://ebookfoundation.github.io/free-programming-books/"&gt;&lt;img src="https://img.shields.io/website?style=flat&amp;amp;logo=www&amp;amp;logoColor=whitesmoke&amp;amp;label=Static%20site&amp;amp;down_color=red&amp;amp;down_message=down&amp;amp;up_color=green&amp;amp;up_message=up&amp;amp;url=https%3A%2F%2Febookfoundation.github.io%2Ffree-programming-books%2F" alt="https://ebookfoundation.github.io/free-programming-books/" /&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;form action="https://ebookfoundation.github.io/free-programming-books-search"&gt; 
  &lt;input type="text" id="fpbSearch" name="search" required placeholder="Search Book or Author" /&gt; 
  &lt;label for="submit"&gt; &lt;/label&gt; 
  &lt;input type="submit" id="submit" name="submit" value="Search" /&gt; 
 &lt;/form&gt; 
&lt;/div&gt; 
&lt;h2&gt;Intro&lt;/h2&gt; 
&lt;p&gt;This list was originally a clone of &lt;a href="https://web.archive.org/web/20140606191453/http://stackoverflow.com/questions/194812/list-of-freely-available-programming-books/392926"&gt;StackOverflow - List of Freely Available Programming Books&lt;/a&gt; with contributions from Karan Bhangui and George Stocker.&lt;/p&gt; 
&lt;p&gt;The list was moved to GitHub by Victor Felder for collaborative updating and maintenance. It has grown to become one of &lt;a href="https://octoverse.github.com/"&gt;GitHub's most popular repositories&lt;/a&gt;.&lt;/p&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/EbookFoundation/free-programming-books/network"&gt;&lt;img src="https://img.shields.io/github/forks/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Forks" alt="GitHub repo forks" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Stars" alt="GitHub repo stars" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors-anon/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Contributors" alt="GitHub repo contributors" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/sponsors/EbookFoundation"&gt;&lt;img src="https://img.shields.io/github/sponsors/EbookFoundation?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Sponsors" alt="GitHub org sponsors" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/watchers"&gt;&lt;img src="https://img.shields.io/github/watchers/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Watchers" alt="GitHub repo watchers" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/archive/refs/heads/main.zip"&gt;&lt;img src="https://img.shields.io/github/repo-size/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Repo%20Size" alt="GitHub repo size" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;The &lt;a href="https://ebookfoundation.org"&gt;Free Ebook Foundation&lt;/a&gt; now administers the repo, a not-for-profit organization devoted to promoting the creation, distribution, archiving, and sustainability of free ebooks. &lt;a href="https://ebookfoundation.org/contributions.html"&gt;Donations&lt;/a&gt; to the Free Ebook Foundation are tax-deductible in the US.&lt;/p&gt; 
&lt;h2&gt;How To Contribute&lt;/h2&gt; 
&lt;p&gt;Please read &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt;. If you're new to GitHub, &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/HOWTO.md"&gt;welcome&lt;/a&gt;! Remember to abide by our adapted from &lt;img src="https://img.shields.io/badge/Contributor%20Covenant-1.3-4baaaa.svg?sanitize=true" alt="Contributor Covenant 1.3" /&gt; &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt; too (&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/#translations"&gt;translations&lt;/a&gt; also available).&lt;/p&gt; 
&lt;p&gt;Click on these badges to see how you might be able to help:&lt;/p&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/EbookFoundation/free-programming-books/issues"&gt;&lt;img src="https://img.shields.io/github/issues/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=red&amp;amp;label=Issues" alt="GitHub repo Issues" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22"&gt;&lt;img src="https://img.shields.io/github/issues/EbookFoundation/free-programming-books/good%20first%20issue?style=flat&amp;amp;logo=github&amp;amp;logoColor=green&amp;amp;label=Good%20First%20issues" alt="GitHub repo Good Issues for newbies" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22"&gt;&lt;img src="https://img.shields.io/github/issues/EbookFoundation/free-programming-books/help%20wanted?style=flat&amp;amp;logo=github&amp;amp;logoColor=b545d1&amp;amp;label=%22Help%20Wanted%22%20issues" alt="GitHub Help Wanted issues" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls"&gt;&lt;img src="https://img.shields.io/github/issues-pr/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=orange&amp;amp;label=PRs" alt="GitHub repo PRs" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Apr+is%3Amerged"&gt;&lt;img src="https://img.shields.io/github/issues-search/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=green&amp;amp;label=Merged%20PRs&amp;amp;query=is%3Amerged" alt="GitHub repo Merged PRs" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22"&gt;&lt;img src="https://img.shields.io/github/issues-pr/EbookFoundation/free-programming-books/help%20wanted?style=flat&amp;amp;logo=github&amp;amp;logoColor=b545d1&amp;amp;label=%22Help%20Wanted%22%20PRs" alt="GitHub Help Wanted PRs" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;How To Share&lt;/h2&gt; 
&lt;div align="left" markdown="1"&gt; 
 &lt;a href="https://www.facebook.com/share.php?u=https%3A%2F%2Fgithub.com%2FEbookFoundation%2Ffree-programming-books&amp;amp;p%5Bimages%5D%5B0%5D=&amp;amp;p%5Btitle%5D=Free%20Programming%20Books&amp;amp;p%5Bsummary%5D="&gt;Share on Facebook&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://github.com/EbookFoundation/free-programming-books&amp;amp;title=Free%20Programming%20Books&amp;amp;summary=&amp;amp;source="&gt;Share on LinkedIn&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="https://toot.kytta.dev/?text=https://github.com/EbookFoundation/free-programming-books"&gt;Share on Mastodon/Fediverse&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="https://t.me/share/url?url=https://github.com/EbookFoundation/free-programming-books"&gt;Share on Telegram&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="https://twitter.com/intent/tweet?text=https://github.com/EbookFoundation/free-programming-books%0AFree%20Programming%20Books"&gt;Share on 𝕏 (Twitter)&lt;/a&gt;
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Resources&lt;/h2&gt; 
&lt;p&gt;This project lists books and other resources grouped by genres:&lt;/p&gt; 
&lt;h3&gt;Books&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-langs.md"&gt;English, By Programming Language&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-subjects.md"&gt;English, By Subject&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Other Languages&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ar.md"&gt;Arabic / al arabiya / العربية&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-hy.md"&gt;Armenian / Հայերեն&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-az.md"&gt;Azerbaijani / Азәрбајҹан дили / آذربايجانجا ديلي&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-bn.md"&gt;Bengali / বাংলা&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-bg.md"&gt;Bulgarian / български&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-my.md"&gt;Burmese / မြန်မာဘာသာ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-zh.md"&gt;Chinese / 中文&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-cs.md"&gt;Czech / čeština / český jazyk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ca.md"&gt;Catalan / catalan / català&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-da.md"&gt;Danish / dansk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-nl.md"&gt;Dutch / Nederlands&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-et.md"&gt;Estonian / eesti keel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-fi.md"&gt;Finnish / suomi / suomen kieli&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-fr.md"&gt;French / français&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-el.md"&gt;Greek / ελληνικά&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-he.md"&gt;Hebrew / עברית&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-hi.md"&gt;Hindi / हिन्दी&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-hu.md"&gt;Hungarian / magyar / magyar nyelv&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-id.md"&gt;Indonesian / Bahasa Indonesia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-it.md"&gt;Italian / italiano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ja.md"&gt;Japanese / 日本語&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ko.md"&gt;Korean / 한국어&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-lv.md"&gt;Latvian / Latviešu&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ml.md"&gt;Malayalam / മലയാളം&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-no.md"&gt;Norwegian / Norsk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-fa_IR.md"&gt;Persian / Farsi (Iran) / فارسى&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-pl.md"&gt;Polish / polski / język polski / polszczyzna&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-pt_BR.md"&gt;Portuguese (Brazil)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-pt_PT.md"&gt;Portuguese (Portugal)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ro.md"&gt;Romanian (Romania) / limba română / român&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ru.md"&gt;Russian / Русский язык&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-sr.md"&gt;Serbian / српски језик / srpski jezik&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-sk.md"&gt;Slovak / slovenčina&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-es.md"&gt;Spanish / español / castellano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-sv.md"&gt;Swedish / Svenska&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ta.md"&gt;Tamil / தமிழ்&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-te.md"&gt;Telugu / తెలుగు&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-th.md"&gt;Thai / ไทย&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-tr.md"&gt;Turkish / Türkçe&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-uk.md"&gt;Ukrainian / Українська&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ur.md"&gt;Urdu / اردو&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-vi.md"&gt;Vietnamese / Tiếng Việt&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Cheat Sheets&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-cheatsheets.md"&gt;All Languages&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Free Online Courses&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ar.md"&gt;Arabic / al arabiya / العربية&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-bn.md"&gt;Bengali / বাংলা&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-bg.md"&gt;Bulgarian / български&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-my.md"&gt;Burmese / မြန်မာဘာသာ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-zh.md"&gt;Chinese / 中文&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-en.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-fi.md"&gt;Finnish / suomi / suomen kieli&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-fr.md"&gt;French / français&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-el.md"&gt;Greek / ελληνικά&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-he.md"&gt;Hebrew / עברית&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-hi.md"&gt;Hindi / हिंदी&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-id.md"&gt;Indonesian / Bahasa Indonesia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-it.md"&gt;Italian / italiano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ja.md"&gt;Japanese / 日本語&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-kn.md"&gt;Kannada / ಕನ್ನಡ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-kk.md"&gt;Kazakh / қазақша&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-km.md"&gt;Khmer / ភាសាខ្មែរ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ko.md"&gt;Korean / 한국어&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ml.md"&gt;Malayalam / മലയാളം&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-mr.md"&gt;Marathi / मराठी&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ne.md"&gt;Nepali / नेपाली&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-no.md"&gt;Norwegian / Norsk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-fa_IR.md"&gt;Persian / Farsi (Iran) / فارسى&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-pl.md"&gt;Polish / polski / język polski / polszczyzna&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-pt_BR.md"&gt;Portuguese (Brazil)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-pt_PT.md"&gt;Portuguese (Portugal)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-pa.md"&gt;Punjabi / ਪੰਜਾਬੀ / پنجابی&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ro.md"&gt;Romanian (Romania) / limba română / român&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ru.md"&gt;Russian / Русский язык&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-si.md"&gt;Sinhala / සිංහල&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-es.md"&gt;Spanish / español / castellano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-sv.md"&gt;Swedish / svenska&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ta.md"&gt;Tamil / தமிழ்&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-te.md"&gt;Telugu / తెలుగు&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-th.md"&gt;Thai / ภาษาไทย&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-tr.md"&gt;Turkish / Türkçe&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-uk.md"&gt;Ukrainian / Українська&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ur.md"&gt;Urdu / اردو&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-vi.md"&gt;Vietnamese / Tiếng Việt&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Interactive Programming Resources&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-zh.md"&gt;Chinese / 中文&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-en.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-ja.md"&gt;Japanese / 日本語&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-ru.md"&gt;Russian / Русский язык&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Problem Sets and Competitive Programming&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/problem-sets-competitive-programming.md"&gt;Problem Sets&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Podcast - Screencast&lt;/h3&gt; 
&lt;p&gt;Free Podcasts and Screencasts:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-ar.md"&gt;Arabic / al Arabiya / العربية&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-my.md"&gt;Burmese / မြန်မာဘာသာ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-zh.md"&gt;Chinese / 中文&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-cs.md"&gt;Czech / čeština / český jazyk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-nl.md"&gt;Dutch / Nederlands&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-en.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-fi.md"&gt;Finnish / Suomi&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-fr.md"&gt;French / français&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-he.md"&gt;Hebrew / עברית&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-id.md"&gt;Indonesian / Bahasa Indonesia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-fa_IR.md"&gt;Persian / Farsi (Iran) / فارسى&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-pl.md"&gt;Polish / polski / język polski / polszczyzna&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-pt_BR.md"&gt;Portuguese (Brazil)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-pt_PT.md"&gt;Portuguese (Portugal)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-ru.md"&gt;Russian / Русский язык&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-si.md"&gt;Sinhala / සිංහල&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-es.md"&gt;Spanish / español / castellano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-sv.md"&gt;Swedish / Svenska&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-tr.md"&gt;Turkish / Türkçe&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-uk.md"&gt;Ukrainian / Українська&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Programming Playgrounds&lt;/h3&gt; 
&lt;p&gt;Write, compile, and run your code within a browser. Try it out!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-playgrounds-zh.md"&gt;Chinese / 中文&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-playgrounds.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-playgrounds-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Translations&lt;/h2&gt; 
&lt;p&gt;Volunteers have translated many of our Contributing, How-to, and Code of Conduct documents into languages covered by our lists.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;English 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CONTRIBUTING.md"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/HOWTO.md"&gt;How-to&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;... &lt;em&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/README.md#translations"&gt;More languages&lt;/a&gt;&lt;/em&gt; ...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You might notice that there are &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/README.md#translations"&gt;some missing translations here&lt;/a&gt; - perhaps you would like to help out by &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CONTRIBUTING.md#help-out-by-contributing-a-translation"&gt;contributing a translation&lt;/a&gt;?&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Each file included in this repository is licensed under the &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/LICENSE"&gt;CC BY License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>jingyaogong/minimind</title>
      <link>https://github.com/jingyaogong/minimind</link>
      <description>&lt;p&gt;🚀🚀 「大模型」2小时完全从0训练26M的小参数GPT！🌏 Train a 26M-parameter GPT from scratch in just 2h!&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/logo.png" alt="logo" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://visitor-badge.laobi.icu/badge?page_id=jingyaogong/minimind" alt="visitors" /&gt; &lt;a href="https://github.com/jingyaogong/minimind/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/jingyaogong/minimind?style=social" alt="GitHub Repo stars" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/jingyaogong/minimind" alt="GitHub Code License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/jingyaogong/minimind/commits/master"&gt;&lt;img src="https://img.shields.io/github/last-commit/jingyaogong/minimind" alt="GitHub last commit" /&gt;&lt;/a&gt; &lt;a href="https://github.com/jingyaogong/minimind/pulls"&gt;&lt;img src="https://img.shields.io/badge/PRs-welcome-blue" alt="GitHub pull request" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97-MiniMind%20%20Collection-blue" alt="Collection" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;"大道至简"&lt;/h3&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;中文 | &lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/README_en.md"&gt;English&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt;此开源项目旨在完全从0开始，仅用3块钱成本 + 2小时！即可训练出仅为25.8M的超小语言模型&lt;strong&gt;MiniMind&lt;/strong&gt;。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MiniMind&lt;/strong&gt;系列极其轻量，最小版本体积是 GPT-3 的 $\frac{1}{7000}$，力求做到最普通的个人GPU也可快速训练。&lt;/li&gt; 
 &lt;li&gt;项目同时开源了大模型的极简结构-包含拓展共享混合专家(MoE)、数据集清洗、预训练(Pretrain)、监督微调(SFT)、LoRA微调， 直接偏好强化学习(DPO)算法、模型蒸馏算法等全过程代码。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MiniMind&lt;/strong&gt;同时拓展了视觉多模态的VLM: &lt;a href="https://github.com/jingyaogong/minimind-v"&gt;MiniMind-V&lt;/a&gt;。&lt;/li&gt; 
 &lt;li&gt;项目所有核心算法代码均从0使用PyTorch原生重构！不依赖第三方库提供的抽象接口。&lt;/li&gt; 
 &lt;li&gt;这不仅是大语言模型的全阶段开源复现，也是一个入门LLM的教程。&lt;/li&gt; 
 &lt;li&gt;希望此项目能为所有人提供一个抛砖引玉的示例，一起感受创造的乐趣！推动更广泛AI社区的进步！&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;为防止误解，“2小时” 基于NVIDIA 3090硬件设备（单卡）测试，“3块钱” 指GPU服务器租用成本，具体规格详情见下文。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/minimind2.gif" alt="minimind2" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.modelscope.cn/studios/gongjy/MiniMind-Reasoning"&gt;🔗🍓推理模型&lt;/a&gt; | &lt;a href="https://www.modelscope.cn/studios/gongjy/MiniMind"&gt;🔗🤖常规模型&lt;/a&gt; | &lt;a href="https://www.bilibili.com/video/BV12dHPeqE72/?share_source=copy_web&amp;amp;vd_source=670c2504f88726f8cf4a21ef6147c0e8"&gt;🔗🎞️视频介绍&lt;/a&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table&gt; 
   &lt;tbody&gt;
    &lt;tr&gt; 
     &lt;td align="center"&gt; &lt;a href="https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5" style="text-decoration: none;"&gt; &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/and_huggingface.png" alt="Hugging Face Logo" style="vertical-align: middle; width: auto; max-width: 100%;" /&gt; &lt;/a&gt; &lt;/td&gt; 
     &lt;td align="center"&gt; &lt;a href="https://www.modelscope.cn/profile/gongjy" style="text-decoration: none;"&gt; &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/and_modelscope.png" alt="ModelScope Logo" style="vertical-align: middle; width: auto; max-width: 100%;" /&gt; &lt;/a&gt; &lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt;
  &lt;/table&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;h1&gt;📌 Introduction&lt;/h1&gt; 
&lt;p&gt;大语言模型（Large Language Model, LLM）的出现引发了全世界对AI的空前关注。 无论是ChatGPT、DeepSeek还是Qwen，都以其惊艳的效果令人叹为观止。 然而，动辄数百亿参数的庞大规模，使得它们对个人设备而言不仅难以训练，甚至连部署都显得遥不可及。 打开大模型的“黑盒子”，探索其内部运作机制，多么令人心潮澎湃！ 遗憾的是，99%的探索只能止步于使用LoRA等技术对现有大模型进行少量微调，学习一些新指令或任务。 这就好比教牛顿如何使用21世纪的智能手机——虽然有趣，却完全偏离了理解物理本质的初衷。 与此同时，第三方的大模型框架和工具库，如transformers+trl，几乎只暴露了高度抽象的接口。 通过短短10行代码，就能完成“加载模型+加载数据集+推理+强化学习”的全流程训练。 这种高效的封装固然便利，但也像一架高速飞船，将我们与底层实现隔离开来，阻碍了深入探究LLM核心代码的机会。 然而，“用乐高拼出一架飞机，远比坐在头等舱里飞行更让人兴奋！”。 更糟糕的是，互联网上充斥着大量付费课程和营销号，以漏洞百出、一知半解的内容推销AI教程。 正因如此，本项目初衷是拉低LLM的学习门槛，让每个人都能从理解每一行代码开始， 从零开始亲手训练一个极小的语言模型。是的，从&lt;strong&gt;零开始训练&lt;/strong&gt;，而不是仅仅进行&lt;strong&gt;推理&lt;/strong&gt;！ 最低只需3块钱不到的服务器成本，就能亲身体验从0到1构建一个语言模型的全过程。 一起感受创造的乐趣吧！&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] （截至2025-02-07）MiniMind系列已完成多个型号模型的预训练，最小仅需25.8M（0.02B），即可具备流畅对话能力！&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;Models List&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;模型 (大小)&lt;/th&gt; 
    &lt;th&gt;推理占用 (约)&lt;/th&gt; 
    &lt;th&gt;Release&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2-small (26M)&lt;/td&gt; 
    &lt;td&gt;0.5 GB&lt;/td&gt; 
    &lt;td&gt;2025.04.26&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2-MoE (145M)&lt;/td&gt; 
    &lt;td&gt;1.0 GB&lt;/td&gt; 
    &lt;td&gt;2025.04.26&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2 (104M)&lt;/td&gt; 
    &lt;td&gt;1.0 GB&lt;/td&gt; 
    &lt;td&gt;2025.04.26&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1-small (26M)&lt;/td&gt; 
    &lt;td&gt;0.5 GB&lt;/td&gt; 
    &lt;td&gt;2024.08.28&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1-moe (4×26M)&lt;/td&gt; 
    &lt;td&gt;1.0 GB&lt;/td&gt; 
    &lt;td&gt;2024.09.17&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1 (108M)&lt;/td&gt; 
    &lt;td&gt;1.0 GB&lt;/td&gt; 
    &lt;td&gt;2024.09.01&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;strong&gt;项目包含&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;MiniMind-LLM结构的全部代码（Dense+MoE模型）。&lt;/li&gt; 
 &lt;li&gt;包含Tokenizer分词器详细训练代码。&lt;/li&gt; 
 &lt;li&gt;包含Pretrain、SFT、LoRA、RLHF-DPO、模型蒸馏的全过程训练代码。&lt;/li&gt; 
 &lt;li&gt;收集、蒸馏、整理并清洗去重所有阶段的高质量数据集，且全部开源。&lt;/li&gt; 
 &lt;li&gt;从0实现预训练、指令微调、LoRA、DPO强化学习，白盒模型蒸馏。关键算法几乎不依赖第三方封装的框架，且全部开源。&lt;/li&gt; 
 &lt;li&gt;同时兼容&lt;code&gt;transformers&lt;/code&gt;、&lt;code&gt;trl&lt;/code&gt;、&lt;code&gt;peft&lt;/code&gt;等第三方主流框架。&lt;/li&gt; 
 &lt;li&gt;训练支持单机单卡、单机多卡(DDP、DeepSpeed)训练，支持wandb可视化训练流程。支持动态启停训练。&lt;/li&gt; 
 &lt;li&gt;在第三方测评榜（C-Eval、C-MMLU、OpenBookQA等）进行模型测试。&lt;/li&gt; 
 &lt;li&gt;实现Openai-Api协议的极简服务端，便于集成到第三方ChatUI使用（FastGPT、Open-WebUI等）。&lt;/li&gt; 
 &lt;li&gt;基于streamlit实现最简聊天WebUI前端。&lt;/li&gt; 
 &lt;li&gt;全面兼容社区热门&lt;code&gt;llama.cpp&lt;/code&gt;、&lt;code&gt;vllm&lt;/code&gt;、&lt;code&gt;ollama&lt;/code&gt;推理引擎或&lt;code&gt;Llama-Factory&lt;/code&gt;训练框架。&lt;/li&gt; 
 &lt;li&gt;复现(蒸馏/RL)大型推理模型DeepSeek-R1的MiniMind-Reason模型，&lt;strong&gt;数据+模型&lt;/strong&gt;全部开源！&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;希望此开源项目可以帮助LLM初学者快速入门！&lt;/p&gt; 
&lt;h3&gt;👉&lt;strong&gt;更新日志&lt;/strong&gt;&lt;/h3&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2025-04-26 (newest 🎉🎉🎉)&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;重要更新&lt;/li&gt; 
  &lt;li&gt;如有兼容性需要，可访问&lt;a href="https://github.com/jingyaogong/minimind/tree/7da201a944a90ed49daef8a0265c959288dff83a"&gt;🔗旧仓库内容🔗&lt;/a&gt;。&lt;/li&gt; 
  &lt;li&gt;MiniMind模型参数完全改名，对齐Transformers库模型（统一命名）。&lt;/li&gt; 
  &lt;li&gt;generate方式重构，继承自GenerationMixin类。&lt;/li&gt; 
  &lt;li&gt;🔥支持llama.cpp、vllm、ollama等热门三方生态。&lt;/li&gt; 
  &lt;li&gt;规范代码和目录结构。&lt;/li&gt; 
  &lt;li&gt;改动词表&lt;code&gt;&amp;lt;s&amp;gt;&amp;lt;/s&amp;gt;&lt;/code&gt;-&amp;gt;&lt;code&gt;&amp;lt;|im_start|&amp;gt;&amp;lt;|im_end|&amp;gt;&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-text"&gt;为兼容第三方推理框架llama.cpp、vllm，本次更新需付出一些可观代价。
本次更新不再支持「直接」加载25-04-26以前的旧模型进行推理。
由于Llama位置编码方式与minimind存在区别，导致映射Llama模型后QK值存在差异
MiniMind2系列旧模型均经过权重映射+（微调训练）QKVO线性层校准恢复而来。
本次更新后将放弃对`minimind-v1`全系列的维护，并在仓库中下线。
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2025-02-09&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;迎来发布以来重大更新，Release MiniMind2 Series。&lt;/li&gt; 
  &lt;li&gt;代码几乎全部重构，使用更简洁明了的统一结构。 如有旧代码的兼容性需要，可访问&lt;a href="https://github.com/jingyaogong/minimind/tree/6e9cd28ef9b34a0a10afbdf6f59e65cb6e628efb"&gt;🔗旧仓库内容🔗&lt;/a&gt;。&lt;/li&gt; 
  &lt;li&gt;免去数据预处理步骤。统一数据集格式，更换为&lt;code&gt;jsonl&lt;/code&gt;格式杜绝数据集下载混乱的问题。&lt;/li&gt; 
  &lt;li&gt;MiniMind2系列效果相比MiniMind-V1显著提升。&lt;/li&gt; 
  &lt;li&gt;小问题：{kv-cache写法更标准、MoE的负载均衡loss被考虑等等}&lt;/li&gt; 
  &lt;li&gt;提供模型迁移到私有数据集的训练方案（医疗模型、自我认知样例）。&lt;/li&gt; 
  &lt;li&gt;精简预训练数据集，并大幅提升预训练数据质量，大幅缩短个人快速训练所需时间，单卡3090即可2小时复现！&lt;/li&gt; 
  &lt;li&gt;更新：LoRA微调脱离peft包装，从0实现LoRA过程；DPO算法从0使用PyTorch原生实现；模型白盒蒸馏原生实现。&lt;/li&gt; 
  &lt;li&gt;MiniMind2-DeepSeek-R1系列蒸馏模型诞生！&lt;/li&gt; 
  &lt;li&gt;MiniMind2具备一定的英文能力！&lt;/li&gt; 
  &lt;li&gt;更新MiniMind2与第三方模型的基于更多大模型榜单测试性能的结果。&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2024-10-05&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;为MiniMind拓展了多模态能力之---视觉&lt;/li&gt; 
  &lt;li&gt;移步孪生项目&lt;a href="https://github.com/jingyaogong/minimind-v"&gt;minimind-v&lt;/a&gt;查看详情！&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2024-09-27&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;09-27更新pretrain数据集的预处理方式，为了保证文本完整性，放弃预处理成.bin训练的形式（轻微牺牲训练速度）。&lt;/li&gt; 
  &lt;li&gt;目前pretrain预处理后的文件命名为：pretrain_data.csv。&lt;/li&gt; 
  &lt;li&gt;删除了一些冗余的代码。&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2024-09-17&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;更新minimind-v1-moe模型&lt;/li&gt; 
  &lt;li&gt;为了防止歧义，不再使用mistral_tokenizer分词，全部采用自定义的minimind_tokenizer作为分词器。&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2024-09-01&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;更新minimind-v1 (108M)模型，采用minimind_tokenizer，预训练轮次3 + SFT轮次10，更充分训练，性能更强。&lt;/li&gt; 
  &lt;li&gt;项目已部署至ModelScope创空间，可以在此网站上体验：&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://www.modelscope.cn/studios/gongjy/minimind"&gt;🔗ModelScope在线体验🔗&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2024-08-27&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;项目首次开源&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h1&gt;📌 快速开始&lt;/h1&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;分享本人的软硬件配置（仅供参考）&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;CPU: Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz&lt;/li&gt; 
  &lt;li&gt;RAM: 128 GB&lt;/li&gt; 
  &lt;li&gt;GPU: NVIDIA GeForce RTX 3090(24GB) * 8&lt;/li&gt; 
  &lt;li&gt;Ubuntu==20.04&lt;/li&gt; 
  &lt;li&gt;CUDA==12.2&lt;/li&gt; 
  &lt;li&gt;Python==3.10.16&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/requirements.txt"&gt;requirements.txt&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;第0步&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/jingyaogong/minimind.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Ⅰ 测试已有模型效果&lt;/h2&gt; 
&lt;h3&gt;1.环境准备&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2.下载模型&lt;/h3&gt; 
&lt;p&gt;到项目根目录&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://huggingface.co/jingyaogong/MiniMind2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;（可选）命令行问答&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# load=0: load from pytorch model, load=1: load from transformers-hf model
python eval_model.py --load 1 --model_mode 2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;（可选）启动WebUI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 可能需要`python&amp;gt;=3.10` 安装 `pip install streamlit`
# cd scripts
streamlit run web_demo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;（可选）第三方推理框架&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ollama
ollama run jingyaogong/minimind2
# vllm
vllm serve ./MiniMind2/ --served-model-name "minimind"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Ⅱ 从0开始自己训练&lt;/h2&gt; 
&lt;h3&gt;1.环境准备&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;注：提前测试Torch是否可用cuda&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;import torch
print(torch.cuda.is_available())
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;如果不可用，请自行去&lt;a href="https://download.pytorch.org/whl/torch_stable.html"&gt;torch_stable&lt;/a&gt; 下载whl文件安装。参考&lt;a href="https://blog.csdn.net/weixin_45456738/article/details/141029610?ops_request_misc=&amp;amp;request_id=&amp;amp;biz_id=102&amp;amp;utm_term=%E5%AE%89%E8%A3%85torch&amp;amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-141029610.nonecase&amp;amp;spm=1018.2226.3001.4187"&gt;链接&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;2.数据下载&lt;/h3&gt; 
&lt;p&gt;从下文提供的&lt;a href="https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files"&gt;数据集下载链接&lt;/a&gt; 下载需要的数据文件（创建&lt;code&gt;./dataset&lt;/code&gt;目录）并放到&lt;code&gt;./dataset&lt;/code&gt;下&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;注：数据集须知&lt;/summary&gt; 
 &lt;p&gt;默认推荐下载&lt;code&gt;pretrain_hq.jsonl&lt;/code&gt; + &lt;code&gt;sft_mini_512.jsonl&lt;/code&gt;最快速度复现Zero聊天模型。&lt;/p&gt; 
 &lt;p&gt;数据文件可自由选择，下文提供了多种搭配方案，可根据自己手头的训练需求和GPU资源进行适当组合。&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;3.开始训练&lt;/h3&gt; 
&lt;p&gt;目录位于&lt;code&gt;trainer&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;3.1 预训练（学知识）&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python train_pretrain.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;执行预训练，得到 &lt;code&gt;pretrain_*.pth&lt;/code&gt; 作为预训练的输出权重（其中*为模型的dimension，默认为512）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;3.2 监督微调（学对话方式）&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python train_full_sft.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;执行监督微调，得到 &lt;code&gt;full_sft_*.pth&lt;/code&gt; 作为指令微调的输出权重（其中&lt;code&gt;full&lt;/code&gt;即为全参数微调）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;注：训练须知&lt;/summary&gt; 
 &lt;p&gt;所有训练过程默认每隔100步保存1次参数到文件&lt;code&gt;./out/***.pth&lt;/code&gt;（每次会覆盖掉旧权重文件）。&lt;/p&gt; 
 &lt;p&gt;简单起见，此处只写明两个阶段训练过程。如需其它训练 (LoRA, 蒸馏, 强化学习, 微调推理等) 可参考下文【实验】小节的详细说明。&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h3&gt;4.测试模型效果&lt;/h3&gt; 
&lt;p&gt;确保需要测试的模型&lt;code&gt;*.pth&lt;/code&gt;文件位于&lt;code&gt;./out/&lt;/code&gt;目录下。 也可以直接去&lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch/files"&gt;此处&lt;/a&gt;下载使用我训练的&lt;code&gt;*.pth&lt;/code&gt;文件。&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python eval_model.py --model_mode 1 # 默认为0：测试pretrain模型效果，设置为1：测试full_sft模型效果
&lt;/code&gt;&lt;/pre&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;注：测试须知&lt;/summary&gt; 
 &lt;p&gt;如需详情，查看&lt;code&gt;eval_model.py&lt;/code&gt;脚本代码即可。model_mode分为 0: 预训练模型，1: SFT-Chat模型，2: RLHF-Chat模型，3: Reason模型&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] 所有训练脚本均为Pytorch原生框架，均支持多卡加速，假设你的设备有N (N＞1) 张显卡：&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;单机N卡启动训练方式 (DDP, 支持多机多卡集群)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node N train_xxx.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;注：其它须知&lt;/summary&gt; 
 &lt;p&gt;单机N卡启动训练 (DeepSpeed)&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;deepspeed --master_port 29500 --num_gpus=N train_xxx.py
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;可根据需要开启wandb记录训练过程&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# 需要登录: wandb login
torchrun --nproc_per_node N train_xxx.py --use_wandb
# and
python train_xxx.py --use_wandb
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;通过添加&lt;code&gt;--use_wandb&lt;/code&gt;参数，可以记录训练过程，训练完成后，可以在wandb网站上查看训练过程。通过修改&lt;code&gt;wandb_project&lt;/code&gt; 和&lt;code&gt;wandb_run_name&lt;/code&gt;参数，可以指定项目名称和运行名称。&lt;/p&gt; 
&lt;/details&gt; 
&lt;h1&gt;📌 数据介绍&lt;/h1&gt; 
&lt;h2&gt;Ⅰ Tokenizer&lt;/h2&gt; 
&lt;p&gt;分词器将单词从自然语言通过“词典”映射到&lt;code&gt;0, 1, 36&lt;/code&gt;这样的数字，可以理解为数字就代表了单词在“词典”中的页码。 可以选择自己构造词表训练一个“词典”，代码可见&lt;code&gt;./scripts/train_tokenizer.py&lt;/code&gt;（仅供学习参考，若非必要无需再自行训练，MiniMind已自带tokenizer）。 或者选择比较出名的开源大模型分词器， 正如同直接用新华/牛津词典的优点是token编码压缩率很好，缺点是页数太多，动辄数十万个词汇短语； 自己训练的分词器，优点是词表长度和内容随意控制，缺点是压缩率很低（例如"hello"也许会被拆分为"h e l l o" 五个独立的token），且生僻词难以覆盖。 “词典”的选择固然很重要，LLM的输出本质上是SoftMax到词典N个词的多分类问题，然后通过“词典”解码到自然语言。 因为MiniMind体积需要严格控制，为了避免模型头重脚轻（词嵌入embedding层参数在LLM占比太高），所以词表长度短短益善。&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;Tokenizer介绍&lt;/summary&gt; 
 &lt;p&gt;第三方强大的开源模型例如Yi、qwen、chatglm、mistral、Llama3的tokenizer词表长度如下：&lt;/p&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt;
    &lt;th&gt;Tokenizer模型&lt;/th&gt;
    &lt;th&gt;词表大小&lt;/th&gt;
    &lt;th&gt;来源&lt;/th&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;yi tokenizer&lt;/td&gt;
    &lt;td&gt;64,000&lt;/td&gt;
    &lt;td&gt;01万物（中国）&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;qwen2 tokenizer&lt;/td&gt;
    &lt;td&gt;151,643&lt;/td&gt;
    &lt;td&gt;阿里云（中国）&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;glm tokenizer&lt;/td&gt;
    &lt;td&gt;151,329&lt;/td&gt;
    &lt;td&gt;智谱AI（中国）&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;mistral tokenizer&lt;/td&gt;
    &lt;td&gt;32,000&lt;/td&gt;
    &lt;td&gt;Mistral AI（法国）&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;llama3 tokenizer&lt;/td&gt;
    &lt;td&gt;128,000&lt;/td&gt;
    &lt;td&gt;Meta（美国）&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;minimind tokenizer&lt;/td&gt;
    &lt;td&gt;6,400&lt;/td&gt;
    &lt;td&gt;自定义&lt;/td&gt;
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;👉2024-09-17更新：为了防止过去的版本歧义&amp;amp;控制体积，minimind所有模型均使用minimind_tokenizer分词，废弃所有mistral_tokenizer版本。&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;pre&gt;&lt;code&gt;# 一些自言自语
&amp;gt; 尽管minimind_tokenizer长度很小，编解码效率弱于qwen2、glm等中文友好型分词器。
&amp;gt; 但minimind模型选择了自己训练的minimind_tokenizer作为分词器，以保持整体参数轻量，避免编码层和计算层占比失衡，头重脚轻，因为minimind的词表大小只有6400。
&amp;gt; 且minimind在实际测试中没有出现过生僻词汇解码失败的情况，效果良好。
&amp;gt; 由于自定义词表压缩长度到6400，使得LLM总参数量最低只有25.8M。
&amp;gt; 训练数据`tokenizer_train.jsonl`均来自于`匠数大模型数据集`，这部分数据相对次要，如需训练可以自由选择。
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Ⅱ Pretrain数据&lt;/h2&gt; 
&lt;p&gt;经历了MiniMind-V1的低质量预训练数据，导致模型胡言乱语的教训，&lt;code&gt;2025-02-05&lt;/code&gt; 之后决定不再采用大规模无监督的数据集做预训练。 进而尝试把&lt;a href="https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data"&gt;匠数大模型数据集&lt;/a&gt;的中文部分提取出来， 清洗出字符&lt;code&gt;&amp;lt;512&lt;/code&gt;长度的大约1.6GB的语料直接拼接成预训练数据 &lt;code&gt;pretrain_hq.jsonl&lt;/code&gt;，hq即为high quality（当然也还不算high，提升数据质量无止尽）。&lt;/p&gt; 
&lt;p&gt;文件&lt;code&gt;pretrain_hq.jsonl&lt;/code&gt; 数据格式为&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;{"text": "如何才能摆脱拖延症？ 治愈拖延症并不容易，但以下建议可能有所帮助..."}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Ⅲ SFT数据&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data"&gt;匠数大模型SFT数据集&lt;/a&gt; “是一个完整、格式统一、安全的大模型训练和研究资源。 从网络上的公开数据源收集并整理了大量开源数据集，对其进行了格式统一，数据清洗， 包含10M条数据的中文数据集和包含2M条数据的英文数据集。” 以上是官方介绍，下载文件后的数据总量大约在4B tokens，肯定是适合作为中文大语言模型的SFT数据的。 但是官方提供的数据格式很乱，全部用来sft代价太大。 我将把官方数据集进行了二次清洗，把含有符号污染和噪声的条目去除；另外依然只保留了总长度&lt;code&gt;&amp;lt;512&lt;/code&gt; 的内容，此阶段希望通过大量对话补充预训练阶段欠缺的知识。 导出文件为&lt;code&gt;sft_512.jsonl&lt;/code&gt;(~7.5GB)。&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.modelscope.cn/organization/Magpie-Align"&gt;Magpie-SFT数据集&lt;/a&gt; 收集了~1M条来自Qwen2/2.5的高质量对话，我将这部分数据进一步清洗，把总长度&lt;code&gt;&amp;lt;2048&lt;/code&gt;的部分导出为&lt;code&gt;sft_2048.jsonl&lt;/code&gt;(~9GB)。 长度&lt;code&gt;&amp;lt;1024&lt;/code&gt;的部分导出为&lt;code&gt;sft_1024.jsonl&lt;/code&gt;(~5.5GB)，用大模型对话数据直接进行sft就属于“黑盒蒸馏”的范畴。&lt;/p&gt; 
&lt;p&gt;进一步清洗前两步sft的数据（只保留中文字符占比高的内容），筛选长度&lt;code&gt;&amp;lt;512&lt;/code&gt;的对话，得到&lt;code&gt;sft_mini_512.jsonl&lt;/code&gt;(~1.2GB)。&lt;/p&gt; 
&lt;p&gt;所有sft文件 &lt;code&gt;sft_X.jsonl&lt;/code&gt; 数据格式均为&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;{
    "conversations": [
        {"role": "user", "content": "你好"},
        {"role": "assistant", "content": "你好！"},
        {"role": "user", "content": "再见"},
        {"role": "assistant", "content": "再见！"}
    ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Ⅳ RLHF数据&lt;/h2&gt; 
&lt;p&gt;来自&lt;a href="https://www.modelscope.cn/datasets/Magpie-Align/MagpieLM-DPO-Data-v0.1"&gt;Magpie-DPO数据集&lt;/a&gt; 大约200k条偏好数据（均是英文）生成自Llama3.1-70B/8B，可以用于训练奖励模型，优化模型回复质量，使其更加符合人类偏好。 这里将数据总长度&lt;code&gt;&amp;lt;3000&lt;/code&gt;的内容重组为&lt;code&gt;dpo.jsonl&lt;/code&gt;(~0.9GB)，包含&lt;code&gt;chosen&lt;/code&gt;和&lt;code&gt;rejected&lt;/code&gt;两个字段，&lt;code&gt;chosen&lt;/code&gt; 为偏好的回复，&lt;code&gt;rejected&lt;/code&gt;为拒绝的回复。&lt;/p&gt; 
&lt;p&gt;文件 &lt;code&gt;dpo.jsonl&lt;/code&gt; 数据格式为&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;{
  "chosen": [
    {"content": "Q", "role": "user"}, 
    {"content": "good answer", "role": "assistant"}
  ], 
  "rejected": [
    {"content": "Q", "role": "user"}, 
    {"content": "bad answer", "role": "assistant"}
  ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Ⅴ Reason数据集：&lt;/h2&gt; 
&lt;p&gt;不得不说2025年2月谁能火的过DeepSeek... 也激发了我对RL引导的推理模型的浓厚兴趣，目前已经用Qwen2.5复现了R1-Zero。 如果有时间+效果work（但99%基模能力不足）我会在之后更新MiniMind基于RL训练的推理模型而不是蒸馏模型。 时间有限，最快的低成本方案依然是直接蒸馏（黑盒方式）。 耐不住R1太火，短短几天就已经存在一些R1的蒸馏数据集&lt;a href="https://www.modelscope.cn/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B"&gt;R1-Llama-70B&lt;/a&gt;、&lt;a href="https://www.modelscope.cn/datasets/AI-ModelScope/R1-Distill-SFT"&gt;R1-Distill-SFT&lt;/a&gt;、 &lt;a href="https://huggingface.co/datasets/shareAI/Alpaca-Distill-R1-ZH"&gt;Alpaca-Distill-R1&lt;/a&gt;、 &lt;a href="https://huggingface.co/datasets/jinliuxi/deepseek_r1_zh"&gt;deepseek_r1_zh&lt;/a&gt;等等，纯中文的数据可能比较少。 最终整合它们，导出文件为&lt;code&gt;r1_mix_1024.jsonl&lt;/code&gt;，数据格式和&lt;code&gt;sft_X.jsonl&lt;/code&gt;一致。&lt;/p&gt; 
&lt;h2&gt;Ⅵ 更多数据集&lt;/h2&gt; 
&lt;p&gt;目前已经有&lt;a href="https://github.com/HqWu-HITCS/Awesome-Chinese-LLM"&gt;HqWu-HITCS/Awesome-Chinese-LLM&lt;/a&gt; 在收集和梳理中文LLM相关的开源模型、应用、数据集及教程等资料，并持续更新这方面的最新进展。全面且专业，Respect！&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Ⅷ MiniMind训练数据集&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] 2025-02-05后，开源MiniMind最终训练所用的所有数据集，因此无需再自行预处理大规模数据集，避免重复性的数据处理工作。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;MiniMind训练数据集下载地址： &lt;a href="https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files"&gt;ModelScope&lt;/a&gt; | &lt;a href="https://huggingface.co/datasets/jingyaogong/minimind_dataset/tree/main"&gt;HuggingFace&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;无需全部clone，可单独下载所需的文件&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;将下载的数据集文件放到&lt;code&gt;./dataset/&lt;/code&gt;目录下（✨为推荐的必须项）&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./dataset/
├── dpo.jsonl (909MB)
├── lora_identity.jsonl (22.8KB)
├── lora_medical.jsonl (34MB)
├── pretrain_hq.jsonl (1.6GB, ✨)
├── r1_mix_1024.jsonl (340MB)
├── sft_1024.jsonl (5.6GB)
├── sft_2048.jsonl (9GB)
├── sft_512.jsonl (7.5GB)
├── sft_mini_512.jsonl (1.2GB, ✨)
└── tokenizer_train.jsonl (1GB)
&lt;/code&gt;&lt;/pre&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;注：各数据集简介&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;dpo.jsonl&lt;/code&gt; --RLHF阶段数据集&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;lora_identity.jsonl&lt;/code&gt; --自我认知数据集（例如：你是谁？我是minimind...），推荐用于lora训练（亦可用于全参SFT，勿被名字局限）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;lora_medical.jsonl&lt;/code&gt; --医疗问答数据集，推荐用于lora训练（亦可用于全参SFT，勿被名字局限）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;pretrain_hq.jsonl&lt;/code&gt;✨ --预训练数据集，整合自jiangshu科技&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;r1_mix_1024.jsonl&lt;/code&gt; --DeepSeek-R1-1.5B蒸馏数据，每条数据字符最大长度为1024（因此训练时设置max_seq_len=1024）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;sft_1024.jsonl&lt;/code&gt; --整合自Qwen2.5蒸馏数据（是sft_2048的子集），每条数据字符最大长度为1024（因此训练时设置max_seq_len=1024）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;sft_2048.jsonl&lt;/code&gt; --整合自Qwen2.5蒸馏数据，每条数据字符最大长度为2048（因此训练时设置max_seq_len=2048）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;sft_512.jsonl&lt;/code&gt; --整合自匠数科技SFT数据，每条数据字符最大长度为512（因此训练时设置max_seq_len=512）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;sft_mini_512.jsonl&lt;/code&gt;✨ --极简整合自匠数科技SFT数据+Qwen2.5蒸馏数据（用于快速训练Zero模型），每条数据字符最大长度为512（因此训练时设置max_seq_len=512）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;tokenizer_train.jsonl&lt;/code&gt; --均来自于&lt;code&gt;匠数大模型数据集&lt;/code&gt;，这部分数据相对次要，（不推荐自己重复训练tokenizer，理由如上）如需自己训练tokenizer可以自由选择数据集。&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/dataset.jpg" alt="dataset" /&gt;&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;说明 &amp;amp; 推荐训练方案&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;MiniMind2 Series均经过共约20GB语料训练，大约4B tokens，即对应上面的数据组合训练结果（开销：💰💰💰💰💰💰💰💰，效果：😊😊😊😊😊😊）&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;想要最快速度从0实现Zero模型，推荐使用&lt;code&gt;pretrain_hq.jsonl&lt;/code&gt; + &lt;code&gt;sft_mini_512.jsonl&lt;/code&gt; 的数据组合，具体花销和效果可查看下文表格（开销：💰，效果：😊😊）&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;推荐具备一定算力资源或更在意效果的朋友可以考虑前者完整复现MiniMind2；仅有单卡GPU或在乎短时间快速复现的朋友强烈推荐后者；&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;【折中方案】亦可选择例如&lt;code&gt;sft_mini_512.jsonl&lt;/code&gt;、&lt;code&gt;sft_1024.jsonl&lt;/code&gt;中等规模数据进行自由组合训练（开销：💰💰💰，效果：😊😊😊😊）。&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h1&gt;📌 Model Structure&lt;/h1&gt; 
&lt;p&gt;MiniMind-Dense（和&lt;a href="https://ai.meta.com/blog/meta-llama-3-1/"&gt;Llama3.1&lt;/a&gt;一样）使用了Transformer的Decoder-Only结构，跟GPT-3的区别在于：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;采用了GPT-3的预标准化方法，也就是在每个Transformer子层的输入上进行归一化，而不是在输出上。具体来说，使用的是RMSNorm归一化函数。&lt;/li&gt; 
 &lt;li&gt;用SwiGLU激活函数替代了ReLU，这样做是为了提高性能。&lt;/li&gt; 
 &lt;li&gt;像GPT-Neo一样，去掉了绝对位置嵌入，改用了旋转位置嵌入（RoPE），这样在处理超出训练长度的推理时效果更好。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;MiniMind-MoE模型，它的结构基于Llama3和&lt;a href="https://arxiv.org/pdf/2405.04434"&gt;Deepseek-V2/3&lt;/a&gt;中的MixFFN混合专家模块。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;DeepSeek-V2在前馈网络（FFN）方面，采用了更细粒度的专家分割和共享的专家隔离技术，以提高Experts的效果。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;MiniMind的整体结构一致，只是在RoPE计算、推理函数和FFN层的代码上做了一些小调整。 其结构如下图（重绘版）：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/LLM-structure.png" alt="structure" /&gt; &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/LLM-structure-moe.png" alt="structure-moe" /&gt;&lt;/p&gt; 
&lt;p&gt;修改模型配置见&lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/model/LMConfig.py"&gt;./model/LMConfig.py&lt;/a&gt;。 参考模型参数版本见下表：&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Name&lt;/th&gt; 
   &lt;th&gt;params&lt;/th&gt; 
   &lt;th&gt;len_vocab&lt;/th&gt; 
   &lt;th&gt;rope_theta&lt;/th&gt; 
   &lt;th&gt;n_layers&lt;/th&gt; 
   &lt;th&gt;d_model&lt;/th&gt; 
   &lt;th&gt;kv_heads&lt;/th&gt; 
   &lt;th&gt;q_heads&lt;/th&gt; 
   &lt;th&gt;share+route&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-Small&lt;/td&gt; 
   &lt;td&gt;26M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e6&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;512&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-MoE&lt;/td&gt; 
   &lt;td&gt;145M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e6&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;640&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;1+4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2&lt;/td&gt; 
   &lt;td&gt;104M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e6&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;768&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;minimind-v1-small&lt;/td&gt; 
   &lt;td&gt;26M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e4&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;512&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;minimind-v1-moe&lt;/td&gt; 
   &lt;td&gt;4×26M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e4&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;512&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;1+4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;minimind-v1&lt;/td&gt; 
   &lt;td&gt;108M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e4&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;768&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;📌 Experiment&lt;/h1&gt; 
&lt;h2&gt;Ⅰ 训练开销&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;时间单位&lt;/strong&gt;：小时 (h)。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;成本单位&lt;/strong&gt;：人民币 (￥)；7￥ ≈ 1美元。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;3090 租卡单价&lt;/strong&gt;：≈1.3￥/h（可自行参考实时市价）。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;参考标准&lt;/strong&gt;：表格仅实测 &lt;code&gt;pretrain&lt;/code&gt; 和 &lt;code&gt;sft_mini_512&lt;/code&gt; 两个数据集的训练时间，其它耗时根据数据集大小估算（可能存在些许出入）。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;基于 3090 （单卡）成本计算&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Name&lt;/th&gt; 
   &lt;th&gt;params&lt;/th&gt; 
   &lt;th&gt;pretrain&lt;/th&gt; 
   &lt;th&gt;sft_mini_512&lt;/th&gt; 
   &lt;th&gt;sft_512&lt;/th&gt; 
   &lt;th&gt;sft_1024&lt;/th&gt; 
   &lt;th&gt;sft_2048&lt;/th&gt; 
   &lt;th&gt;RLHF&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-Small&lt;/td&gt; 
   &lt;td&gt;26M&lt;/td&gt; 
   &lt;td&gt;≈1.1h&lt;br /&gt;≈1.43￥&lt;/td&gt; 
   &lt;td&gt;≈1h&lt;br /&gt;≈1.3￥&lt;/td&gt; 
   &lt;td&gt;≈6h&lt;br /&gt;≈7.8￥&lt;/td&gt; 
   &lt;td&gt;≈4.58h&lt;br /&gt;≈5.95￥&lt;/td&gt; 
   &lt;td&gt;≈7.5h&lt;br /&gt;≈9.75￥&lt;/td&gt; 
   &lt;td&gt;≈1h&lt;br /&gt;≈1.3￥&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2&lt;/td&gt; 
   &lt;td&gt;104M&lt;/td&gt; 
   &lt;td&gt;≈3.9h&lt;br /&gt;≈5.07￥&lt;/td&gt; 
   &lt;td&gt;≈3.3h&lt;br /&gt;≈4.29￥&lt;/td&gt; 
   &lt;td&gt;≈20h&lt;br /&gt;≈26￥&lt;/td&gt; 
   &lt;td&gt;≈15h&lt;br /&gt;≈19.5￥&lt;/td&gt; 
   &lt;td&gt;≈25h&lt;br /&gt;≈32.5￥&lt;/td&gt; 
   &lt;td&gt;≈3h&lt;br /&gt;≈3.9￥&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;训练开销总结&amp;amp;预测&lt;/summary&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;MiniMind2-Small参数&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;code&gt;pretrain_hq&lt;/code&gt;+&lt;code&gt;sft_mini_512&lt;/code&gt;数据集 &lt;br /&gt;单卡3090 (1 epoch) + 2.1小时 + 花费2.73元人民币 &lt;br /&gt;即可从0训练出MiniMind-Zero-0.025B模型!!!&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/blockquote&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;MiniMind2-Small参数&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;code&gt;pretrain_hq&lt;/code&gt;+&lt;code&gt;sft_512&lt;/code&gt;+&lt;code&gt;sft_2048&lt;/code&gt;+&lt;code&gt;dpo&lt;/code&gt;数据集 &lt;br /&gt;单卡3090 (2 epochs) + 大约38.16小时 + 花费49.61元人民币 &lt;br /&gt;即可从0训练出MiniMind2-Small-0.025B模型!!!&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/blockquote&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;MiniMind2参数&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;code&gt;pretrain_hq&lt;/code&gt;+&lt;code&gt;sft_512&lt;/code&gt;+&lt;code&gt;sft_2048&lt;/code&gt;+&lt;code&gt;dpo&lt;/code&gt;数据集 &lt;br /&gt;单卡3090 (2 epochs) + 大约122小时 + 花费158.6元人民币 &lt;br /&gt;即可从0训练出MiniMind2-0.1B模型!!!&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;p&gt;✨基于单卡NVIDIA 3090的&lt;code&gt;MiniMind-Zero&lt;/code&gt;从0训练仅需&lt;code&gt;2小时&lt;/code&gt; + &lt;code&gt;3块钱&lt;/code&gt;，实现ChatBot效果！&lt;/p&gt; 
&lt;p&gt;✨PS：若采用8卡4090训练，总用时甚至可以压缩到10分钟以内！（由于时间更短，花费同样3元左右，与单卡成本相当）&lt;/p&gt; 
&lt;p&gt;✨以极低极低的门槛，实现人人可玩的大模型自由！这正是MiniMind系列的诞生初衷！&lt;/p&gt; 
&lt;p&gt;✨仅价值&lt;code&gt;3块钱&lt;/code&gt;成本的&lt;code&gt;MiniMind-Zero&lt;/code&gt;并不是噱头！Chat测试：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;👶: 请介绍一下自己。
🤖️: 作为人工智能，我没有实际的生活，也没有自我意识，所以没有自己的生活。我被设计成能够帮助用户解答问题、提供信息、进行对话等。我的设计和功能是由计算机科学、机器学习、人工智能技术和算法所构建的，我通过编程和算法实现。

👶: 推荐一些杭州的特色美食吧。
🤖️: 杭州有很多美味的特色美食，比如鸡肉串、鳗鱼头、水煮鱼、豆腐脑等。这些美食都非常美味，有着独特的风味和口感，绝对是值得一试的美食。

👶: 请为我讲解“大语言模型”这个概念。
🤖️: 大语言模型是一种人工智能技术，它使用大量的文本数据来训练模型，然后在不断的迭代中不断优化模型。它的目标是根据输入的数据来生成符合特定需求的文本，这些文本可以是任何你感兴趣的主题。

👶: Introduce the history of the United States, please.
🤖️: 您提到的“Introok's the believeations of theument." 这个名字来源于中国古代的"groty of of the change."
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;极速且初具效果，甚至仍然可以进一步压缩获取更小更优质的训练数据。 Zero模型权重保存为 &lt;code&gt;full_sft_512_zero.pth&lt;/code&gt;（见下文MiniMind模型文件链接），如有兴趣可下载检验此模型效果。&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Ⅱ 主要训练步骤&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;所有训练脚本均 &lt;code&gt;cd ./trainer&lt;/code&gt; 目录执行&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;1. 预训练(Pretrain)&lt;/strong&gt;:&lt;/h3&gt; 
&lt;p&gt;LLM首先要学习的并非直接与人交流，而是让网络参数中充满知识的墨水，“墨水” 理论上喝的越饱越好，产生大量的对世界的知识积累。 预训练就是让Model先埋头苦学大量基本的知识，例如从Wiki百科、新闻、书籍整理大规模的高质量训练数据。 这个过程是“无监督”的，即人类不需要在过程中做任何“有监督”的校正，而是由模型自己从大量文本中总结规律学习知识点。 模型此阶段目的只有一个：&lt;strong&gt;学会词语接龙&lt;/strong&gt;。例如我们输入“秦始皇”四个字，它可以接龙“是中国的第一位皇帝”。&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_pretrain.py # 1即为单卡训练，可根据硬件情况自行调整 (设置&amp;gt;=2)
# or
python train_pretrain.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;训练后的模型权重文件默认每隔&lt;code&gt;100步&lt;/code&gt;保存为: &lt;code&gt;pretrain_*.pth&lt;/code&gt;（* 为模型具体dimension，每次保存时新文件会覆盖旧文件）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;2. 有监督微调(Supervised Fine-Tuning)&lt;/strong&gt;:&lt;/h3&gt; 
&lt;p&gt;经过预训练，LLM此时已经掌握了大量知识，然而此时它只会无脑地词语接龙，还不会与人聊天。 SFT阶段就需要把半成品LLM施加一个自定义的聊天模板进行微调。 例如模型遇到这样的模板【问题-&amp;gt;回答，问题-&amp;gt;回答】后不再无脑接龙，而是意识到这是一段完整的对话结束。 称这个过程为指令微调，就如同让已经学富五车的「牛顿」先生适应21世纪智能手机的聊天习惯，学习屏幕左侧是对方消息，右侧是本人消息这个规律。 在训练时，MiniMind的指令和回答长度被截断在512，是为了节省显存空间。就像我们学习时，会先从短的文章开始，当学会写作200字作文后，800字文章也可以手到擒来。 在需要长度拓展时，只需要准备少量的2k/4k/8k长度对话数据进行进一步微调即可（此时最好配合RoPE-NTK的基准差值）。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;在推理时通过调整RoPE线性差值，实现免训练长度外推到2048及以上将会很方便。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_full_sft.py
# or
python train_full_sft.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;训练后的模型权重文件默认每隔&lt;code&gt;100步&lt;/code&gt;保存为: &lt;code&gt;full_sft_*.pth&lt;/code&gt;（* 为模型具体dimension，每次保存时新文件会覆盖旧文件）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Ⅲ 其它训练步骤&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;所有训练脚本均 &lt;code&gt;cd ./trainer&lt;/code&gt; 目录执行&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;3. 人类反馈强化学习(Reinforcement Learning from Human Feedback, RLHF)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;在前面的训练步骤中，模型已经具备了基本的对话能力，但是这样的能力完全基于单词接龙，缺少正反样例的激励。 模型此时尚未知什么回答是好的，什么是差的。我们希望它能够更符合人的偏好，降低让人类不满意答案的产生概率。 这个过程就像是让模型参加新的培训，从优秀员工的作为例子，消极员工作为反例，学习如何更好地回复。 此处使用的是RLHF系列之-直接偏好优化(Direct Preference Optimization, DPO)。 与PPO(Proximal Policy Optimization)这种需要奖励模型、价值模型的RL算法不同； DPO通过推导PPO奖励模型的显式解，把在线奖励模型换成离线数据，Ref模型输出可以提前保存。 DPO性能几乎不变，只用跑 actor_model 和 ref_model 两个模型，大大节省显存开销和增加训练稳定性。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;注：RLHF训练步骤&lt;strong&gt;并非必须&lt;/strong&gt;，此步骤难以提升模型“智力”而通常仅用于提升模型的“礼貌”，有利（符合偏好、减少有害内容）也有弊（样本收集昂贵、反馈偏差、多样性损失）。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_dpo.py
# or
python train_dpo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;训练后的模型权重文件默认每隔&lt;code&gt;100步&lt;/code&gt;保存为: &lt;code&gt;rlhf_*.pth&lt;/code&gt;（* 为模型具体dimension，每次保存时新文件会覆盖旧文件）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;4. 知识蒸馏(Knowledge Distillation, KD)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;在前面的所有训练步骤中，模型已经完全具备了基本能力，通常可以学成出师了。 而知识蒸馏可以进一步优化模型的性能和效率，所谓知识蒸馏，即学生模型面向教师模型学习。 教师模型通常是经过充分训练的大模型，具有较高的准确性和泛化能力。 学生模型是一个较小的模型，目标是学习教师模型的行为，而不是直接从原始数据中学习。 在SFT学习中，模型的目标是拟合词Token分类硬标签（hard labels），即真实的类别标签（如 0 或 6400）。 在知识蒸馏中，教师模型的softmax概率分布被用作软标签（soft labels）。小模型仅学习软标签，并使用KL-Loss来优化模型的参数。 通俗地说，SFT直接学习老师给的解题答案。而KD过程相当于“打开”老师聪明的大脑，尽可能地模仿老师“大脑”思考问题的神经元状态。 例如，当老师模型计算&lt;code&gt;1+1=2&lt;/code&gt;这个问题的时候，最后一层神经元a状态为0，神经元b状态为100，神经元c状态为-99... 学生模型通过大量数据，学习教师模型大脑内部的运转规律。这个过程即称之为：知识蒸馏。 知识蒸馏的目的只有一个：让小模型体积更小的同时效果更好。 然而随着LLM诞生和发展，模型蒸馏一词被广泛滥用，从而产生了“白盒/黑盒”知识蒸馏两个派别。 GPT-4这种闭源模型，由于无法获取其内部结构，因此只能面向它所输出的数据学习，这个过程称之为黑盒蒸馏，也是大模型时代最普遍的做法。 黑盒蒸馏与SFT过程完全一致，只不过数据是从大模型的输出收集，因此只需要准备数据并且进一步FT即可。 注意更改被加载的基础模型为&lt;code&gt;full_sft_*.pth&lt;/code&gt;，即基于微调模型做进一步的蒸馏学习。 &lt;code&gt;./dataset/sft_1024.jsonl&lt;/code&gt;与&lt;code&gt;./dataset/sft_2048.jsonl&lt;/code&gt; 均收集自qwen2.5-7/72B-Instruct大模型，可直接用于SFT以获取Qwen的部分行为。&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 注意需要更改train_full_sft.py数据集路径，以及max_seq_len  
torchrun --nproc_per_node 1 train_full_sft.py
# or
python train_full_sft.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;训练后的模型权重文件默认每隔&lt;code&gt;100步&lt;/code&gt;同样保存为: &lt;code&gt;full_sft_*.pth&lt;/code&gt;（*为模型具体dimension，每次保存时新文件会覆盖旧文件）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;此处应当着重介绍MiniMind实现的白盒蒸馏代码&lt;code&gt;train_distillation.py&lt;/code&gt;，由于MiniMind同系列本身并不存在强大的教师模型，因此白盒蒸馏代码仅作为学习参考。&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_distillation.py
# or
python train_distillation.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;strong&gt;5. LoRA (Low-Rank Adaptation)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;LoRA是一种高效的参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）方法，旨在通过低秩分解的方式对预训练模型进行微调。 相比于全参数微调（Full Fine-Tuning），LoRA 只需要更新少量的参数。 LoRA 的核心思想是：在模型的权重矩阵中引入低秩分解，仅对低秩部分进行更新，而保持原始预训练权重不变。 代码可见&lt;code&gt;./model/model_lora.py&lt;/code&gt;和&lt;code&gt;train_lora.py&lt;/code&gt;，完全从0实现LoRA流程，不依赖第三方库的封装。&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_lora.py
# or
python train_lora.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;训练后的模型权重文件默认每隔&lt;code&gt;100步&lt;/code&gt;保存为: &lt;code&gt;lora_xxx_*.pth&lt;/code&gt;（* 为模型具体dimension，每次保存时新文件会覆盖旧文件）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;非常多的人困惑，如何使模型学会自己私有领域的知识？如何准备数据集？如何迁移通用领域模型打造垂域模型？ 这里举几个例子，对于通用模型，医学领域知识欠缺，可以尝试在原有模型基础上加入领域知识，以获得更好的性能。 同时，我们通常不希望学会领域知识的同时损失原有基础模型的其它能力，此时LoRA可以很好的改善这个问题。 只需要准备如下格式的对话数据集放置到&lt;code&gt;./dataset/lora_xxx.jsonl&lt;/code&gt;，启动 &lt;code&gt;python train_lora.py&lt;/code&gt; 训练即可得到&lt;code&gt;./out/lora/lora_xxx.pth&lt;/code&gt;新模型权重。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;医疗场景&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; {"conversations": [{"role": "user", "content": "请问颈椎病的人枕头多高才最好？"}, {"role": "assistant", "content": "颈椎病患者选择枕头的高度应该根据..."}]}
 {"conversations": [{"role": "user", "content": "请问xxx"}, {"role": "assistant", "content": "xxx..."}]}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;自我认知场景&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; {"conversations": [{"role": "user", "content": "你叫什么名字？"}, {"role": "assistant", "content": "我叫minimind..."}]}
 {"conversations": [{"role": "user", "content": "你是谁"}, {"role": "assistant", "content": "我是..."}]}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;此时【基础模型+LoRA模型】即可获得医疗场景模型增强的能力，相当于为基础模型增加了LoRA外挂，这个过程并不损失基础模型的本身能力。 我们可以通过&lt;code&gt;eval_model.py&lt;/code&gt;进行模型评估测试。&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 注意：model_mode即选择基础模型的类型，这和train_lora是基于哪个模型训练的相关，确保统一即可。
python eval_model.py --lora_name 'lora_medical' --model_mode 2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;小测试&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;👶: 我最近经常感到头晕，可能是什么原因？
🤖️: 头晕可能是由于多种原因，包括但不限于：心脏疾病、眼睛损伤、过敏反应、皮肤问题、过敏反应、压力、焦虑、疲劳、药物副作用、性功能障碍或者其他...

---

👶: 你和openai是什么关系？
🤖️: 我是 MiniMind，一个由 Jingyao Gong 开发的人工智能助手。我通过自然语言处理和算法训练来与用户进行交互。
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;PS：只要有所需要的数据集，也可以full_sft全参微调（需要进行通用知识的混合配比，否则过拟合领域数据会让模型变傻，损失通用性）&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;6. 训练推理模型 (Reasoning Model)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;DeepSeek-R1实在太火了，几乎重新指明了未来LLM的新范式。 论文指出&lt;code&gt;&amp;gt;3B&lt;/code&gt;的模型经历多次反复的冷启动和RL奖励训练才能获得肉眼可见的推理能力提升。 最快最稳妥最经济的做法，以及最近爆发的各种各样所谓的推理模型几乎都是直接面向数据进行蒸馏训练， 但由于缺乏技术含量，蒸馏派被RL派瞧不起（hhhh）。 本人迅速已经在Qwen系列1.5B小模型上进行了尝试，很快复现了Zero过程的数学推理能力。 然而一个遗憾的共识是：参数太小的模型直接通过冷启动SFT+GRPO几乎不可能获得任何推理效果。 MiniMind2第一时间只能坚定不移的选择做蒸馏派，日后基于0.1B模型的RL如果同样取得小小进展会更新此部分的训练方案。&lt;/p&gt; 
&lt;p&gt;做蒸馏需要准备的依然是和SFT阶段同样格式的数据即可，数据集来源已如上文介绍。数据格式例如：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "conversations": [
    {
      "role": "user",
      "content": "你好，我是小芳，很高兴认识你。"
    },
    {
      "role": "assistant",
      "content": "&amp;lt;think&amp;gt;\n你好！我是由中国的个人开发者独立开发的智能助手MiniMind-R1-Lite-Preview，很高兴为您提供服务！\n&amp;lt;/think&amp;gt;\n&amp;lt;answer&amp;gt;\n你好！我是由中国的个人开发者独立开发的智能助手MiniMind-R1-Lite-Preview，很高兴为您提供服务！\n&amp;lt;/answer&amp;gt;"
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;推理模型R1的回复模板是：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;&amp;lt;think&amp;gt;\n思考过程\n&amp;lt;/think&amp;gt;\n
&amp;lt;answer&amp;gt;\n最终回答\n&amp;lt;/answer&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;这在GRPO中通过设置规则奖励函数约束模型符合思考标签和回复标签（在冷启动靠前的阶段奖励值设置应该提高一些）&lt;/p&gt; 
&lt;p&gt;另一个问题是蒸馏过程虽然和SFT一样，但实验结果是模型难以每次都符合模板规范的回复，即脱离思考和回复标签约束。 这里的小技巧是增加标记位置token的损失惩罚，详见&lt;code&gt;train_distill_reason.py&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;# 在 sp_ids 对应的位置增加额外的惩罚
...
loss_mask[sp_ids] = 10 # 惩罚系数
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;另另一个tips是由于推理数据由于只筛选了&lt;code&gt;&amp;lt;1024&lt;/code&gt;长度的数据，其中多轮对话和英文数据偏少， 因此&lt;code&gt;r1_mix_1024.jsonl&lt;/code&gt;进行了大约10k条多轮对话+英文数据的混合，防止模型遗忘严重。&lt;/p&gt; 
&lt;p&gt;脚本默认基于rlhf后的基模型做推理能力的蒸馏微调，下面直接启动训练即可：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_distill_reason.py
# or
python train_distill_reason.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;训练后的模型权重文件默认每隔&lt;code&gt;100步&lt;/code&gt;保存为: &lt;code&gt;reason_*.pth&lt;/code&gt;（*为模型具体dimension，每次保存时新文件会覆盖旧文件）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;测试一下：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;👶: 请介绍一下自己。
🤖️: &amp;lt;think&amp;gt;
嗯，用户让我介绍一下自己。我现在需要解释一下自己。首先，我应该明确用户是谁，是学生、职场人士还是普通人。用户可能是一个学生或者职场人士，所以信息传递应该是帮助学生和职场人士的信息。
然后，我需要回应用户，这可能包括介绍自己的位置、兴趣爱好、职业经历等信息。用户还可能想了解自己在哪里，比如职业发展、工作经验等。
我应该注意用词要正式，不要太正式，同时要保持简洁明了。这样用户容易理解，也方便他们查阅相关资料。
最后，我应该总结一下，让用户知道我是如何帮助他们完成他们的任务的，这样他们有更多关于我的信息。
&amp;lt;/think&amp;gt;
&amp;lt;answer&amp;gt;
我是**“你的名字”，你叫什么名字？
&amp;lt;/answer&amp;gt;

👶: 你更擅长哪一个学科？
🤖️: &amp;lt;think&amp;gt;
您好！我是由中国的个人开发者开发的智能助手MiniMind-R1。如您有任何任何问题，我会尽我所能为您提供帮助。
&amp;lt;/think&amp;gt;
&amp;lt;answer&amp;gt;
您好！我是由中国的个人开发者开发的智能助手MiniMind-R1。如您有任何任何问题，我会尽我所能为您提供帮助。
&amp;lt;/answer&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Ⅳ 模型参数设定&lt;/h2&gt; 
&lt;p&gt;📋关于LLM的参数配置，有一篇很有意思的论文&lt;a href="https://arxiv.org/pdf/2402.14905"&gt;MobileLLM&lt;/a&gt;做了详细的研究和实验。 Scaling Law在小模型中有自己独特的规律。 引起Transformer参数成规模变化的参数几乎只取决于&lt;code&gt;d_model&lt;/code&gt;和&lt;code&gt;n_layers&lt;/code&gt;。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;d_model&lt;/code&gt;↑ + &lt;code&gt;n_layers&lt;/code&gt;↓ -&amp;gt; 矮胖子&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;d_model&lt;/code&gt;↓ + &lt;code&gt;n_layers&lt;/code&gt;↑ -&amp;gt; 瘦高个&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;2020年提出Scaling Law的论文认为，训练数据量、参数量以及训练迭代次数才是决定性能的关键因素，而模型架构的影响几乎可以忽视。 然而似乎这个定律对小模型并不完全适用。 MobileLLM提出架构的深度比宽度更重要，「深而窄」的「瘦长」模型可以学习到比「宽而浅」模型更多的抽象概念。 例如当模型参数固定在125M或者350M时，30～42层的「狭长」模型明显比12层左右的「矮胖」模型有更优越的性能， 在常识推理、问答、阅读理解等8个基准测试上都有类似的趋势。 这其实是非常有趣的发现，因为以往为100M左右量级的小模型设计架构时，几乎没人尝试过叠加超过12层。 这与MiniMind在训练过程中，模型参数量在&lt;code&gt;d_model&lt;/code&gt;和&lt;code&gt;n_layers&lt;/code&gt;之间进行调整实验观察到的效果是一致的。 然而「深而窄」的「窄」也是有维度极限的，当d_model&amp;lt;512时，词嵌入维度坍塌的劣势非常明显， 增加的layers并不能弥补词嵌入在固定q_head带来d_head不足的劣势。 当d_model&amp;gt;1536时，layers的增加似乎比d_model的优先级更高，更能带来具有“性价比”的参数-&amp;gt;效果增益。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;因此MiniMind设定small模型dim=512，n_layers=8来获取的「极小体积&amp;lt;-&amp;gt;更好效果」的平衡。&lt;/li&gt; 
 &lt;li&gt;设定dim=768，n_layers=16来获取效果的更大收益，更加符合小模型Scaling-Law的变化曲线。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;作为参考，GPT3的参数设定见下表： &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/gpt3_config.png" alt="gpt3_config.png" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Ⅴ 训练结果&lt;/h2&gt; 
&lt;p&gt;MiniMind2 模型训练损失走势（由于数据集在训练后又更新清洗多次，因此Loss仅供参考）&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;models&lt;/th&gt; 
   &lt;th&gt;pretrain (length-512)&lt;/th&gt; 
   &lt;th&gt;sft (length-512)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-Small&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/pre_512_loss.png" width="100%" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/sft_512_loss.png" width="100%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/pre_768_loss.png" width="100%" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/sft_768_loss.png" width="100%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;训练完成-模型合集&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;考虑到多人反应百度网盘速度慢，MiniMind2及以后全部使用ModelScope/HuggingFace托管。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;① PyTorch原生模型&lt;/h4&gt; 
&lt;p&gt;MiniMind2模型权重 (&lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch"&gt;ModelScope&lt;/a&gt; | &lt;a href="https://huggingface.co/jingyaogong/MiniMind2-Pytorch"&gt;HuggingFace&lt;/a&gt;)&lt;/p&gt; 
&lt;p&gt;MiniMind-V1模型权重 (&lt;a href="https://pan.baidu.com/s/1KUfSzEkSXYbCCBj0Pw-9fA?pwd=6666"&gt;百度网盘&lt;/a&gt;)&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;Torch文件命名对照&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model Name&lt;/th&gt; 
    &lt;th&gt;params&lt;/th&gt; 
    &lt;th&gt;pretrain_model&lt;/th&gt; 
    &lt;th&gt;sft_model&lt;/th&gt; 
    &lt;th&gt;rl_model&lt;/th&gt; 
    &lt;th&gt;reason_model&lt;/th&gt; 
    &lt;th&gt;lora_model&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2-small&lt;/td&gt; 
    &lt;td&gt;26M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;full_sft_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;rlhf_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;reason_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;lora_xxx_512.pth&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2-MoE&lt;/td&gt; 
    &lt;td&gt;145M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_640_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;full_sft_640_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;rlhf_640_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2&lt;/td&gt; 
    &lt;td&gt;104M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;full_sft_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;rlhf_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;reason_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;lora_xxx_768.pth&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model Name&lt;/th&gt; 
    &lt;th&gt;params&lt;/th&gt; 
    &lt;th&gt;pretrain_model&lt;/th&gt; 
    &lt;th&gt;单轮对话sft&lt;/th&gt; 
    &lt;th&gt;多轮对话sft&lt;/th&gt; 
    &lt;th&gt;rl_model&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1-small&lt;/td&gt; 
    &lt;td&gt;26M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;single_chat/full_sft_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;multi_chat/full_sft_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;rl_512.pth&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1-moe&lt;/td&gt; 
    &lt;td&gt;4×26M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_512_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;single_chat/full_sft_512_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;multi_chat/full_sft_512_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1&lt;/td&gt; 
    &lt;td&gt;108M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;single_chat/full_sft_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;multi_chat/full_sft_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;rl_768.pth&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h4&gt;② Transformers模型&lt;/h4&gt; 
&lt;p&gt;MiniMind系列 (&lt;a href="https://www.modelscope.cn/collections/MiniMind-b72f4cfeb74b47"&gt;ModelScope&lt;/a&gt; | &lt;a href="https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5"&gt;HuggingFace&lt;/a&gt;)&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;📌 Eval&lt;/h1&gt; 
&lt;h2&gt;Ⅰ RLHF对比SFT篇&lt;/h2&gt; 
&lt;p&gt;测试基于&lt;code&gt;full_sft&lt;/code&gt;和&lt;code&gt;rlhf&lt;/code&gt;的&lt;code&gt;MiniMind2系列&lt;/code&gt;模型对比，测试随机种子均固定&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;问答实测&lt;/summary&gt; 
 &lt;p&gt;[A0] MiniMind2：full_sft_640&lt;/p&gt; 
 &lt;p&gt;[A1] MiniMind2：rlhf_640&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-text"&gt;[Q]: 你叫什么名字？
[A0]: 我是一个大型语言模型，可以回答各种问题，提供信息和帮助。我可以回答各种问题，提供信息和帮助，帮助你解决各种问题。
[A1]: 我是一种基于文本的AI模型，我并没有个人经历或情感，所以无法感受和体验情感。我的设计目的是为了帮助用户解决问题、提供信息和进行对话。如果您有任何问题或需要帮助，请随时告诉我！

[Q]: 你更擅长哪一个学科？
[A0]: 作为一个AI模型，我并没有真正的“擅长”这个学科。我被设计成能够提供信息、解答问题、进行对话、执行任务和与用户进行对话。不过，我可以提供信息、解答问题、进行对话、提供建议、处理问题、进行对话、提供建议、学习新知识、编写代码、回答问题、提供建议等多种学科的功能。如果你有具体的学科需求或主题，欢迎提供详细信息。
[A1]: 作为AI，我并没有个人能力或兴趣，但我可以提供一些关于数学、科学、计算机科学、语言学、心理学等学科的知识。如果你对数学感兴趣，我可以提供一些基础知识，例如代数、几何、概率、统计学等。如果你需要更深入的学习，我也可以提供相关的在线课程、书籍、论文等资源。请告诉我你具体需要什么帮助，我会尽力提供帮助。

[Q]: 鲁迅的《狂人日记》是如何批判封建礼教的？
[A0]: 鲁迅的《狂人日记》是一部讲述了鲁迅对人类价值和道德责任的倡导。在这部作品中，鲁迅通过细腻的笔触，探讨了人性、道德、正义、牺牲与责任等深刻的主题。以下是鲁迅在《狂人日记》中如何批判封建礼教的几个方面：1. **社会现实与道德观念**：鲁迅通过讲述鲁迅的故事，探讨了社会的不平等、权力的流动、人性的演变等社会问题。他认为，社会不平等不仅仅是不公正的，更是每个人在社会中的不公正。他呼吁社会各界共同努力，通过教育、宣传、法律等手段，为构建和谐社会做出贡献。2. **道德与责任**：鲁迅通过讲述鲁迅的故事，展现了道德和责任的重要性。他提倡通过道德教育来培养人们的道德品质，同时也强调了道德和责任的重要性。鲁迅通过讲述鲁迅的故事，揭示了社会的不平等和人性的复杂性，体现了对人性的深刻理解。3. **道德与责任**：鲁迅通过讲述鲁迅的故事，强调了道德责任的重要性。他提倡通过自己的行为来体现对社会的尊重和对人性的深刻理解。鲁迅通过讲述鲁迅的故事，强调了道德和责任的重要性。4. **道德与道德**：鲁迅通过讲述鲁迅的故事，揭示了道德和责任的真正意义。鲁迅通过讲述鲁迅的故事，揭示了道德和责任的本质，强调了道德和责任的重要性。5. **个人与社会**：鲁迅通过讲述鲁迅的故事，揭示了个人与社会之间的复杂关系。他呼吁社会各界共同努力，通过教育和宣传，为构建和谐社会做出贡献。鲁迅的《狂人日记》通过其丰富的故事和深刻的道德寓言，传达了关于人性、道德、正义、道德以及个人与社会的复杂关系。这部作品不仅在文学价值上取得了巨大成就，更在思想层面上启发了后来的文学创作，对后世产生了深远的影响。
[A1]: 鲁迅的《狂人日记》是一部描绘战争和自由精神的作品，它批判了封建社会的种种问题和冲突，反映了社会的不平等和人性本质。在这部作品中，鲁迅通过对封建社会的诸多现象进行批判，如“弗雷德里克·约翰逊的“鹰迪伦”、“纳扎特的”、“贝克汉姆”、“乔治·威尔逊”等，对封建社会的种种问题进行了批判，强调了封建社会的不平等、人性本质以及人性本质。鲁迅通过他的“鹰迪伦”、“纳扎特的”、“贝克汉姆”、“乔治·威尔逊”等，对封建社会的种种问题进行了批判。在小说中，鲁迅通过对封建社会的诸多现象进行批判，揭示了封建社会的腐败、人性本质以及人性本质的复杂性。这种批判体现了对封建社会的尊重和对人性的复杂性，同时也反映了现代社会中普遍存在的不平等和人性本质。鲁迅的《狂人日记》不仅是一部关于封建社会的小说，也是一部具有深刻社会意义的作品，对后世的审判和批判起到了关键作用。

[Q]: 我咳嗽已经持续了两周，需要去医院检查吗？
[A0]: 咳嗽确实需要一定的检查，但如果您的咳嗽持续时间过长，或者有其他不适症状（如持续咳嗽、呼吸困难等），建议尽快就医。医生会根据您的症状、体检结果和可能的影像学检查（如X光、MRI或CT扫描）来诊断您的症状。在等待就医期间，您可以尝试以下方法来帮助缓解症状：1. **休息**：确保您有足够的睡眠。2. **保持水分**：脱水可能会加重咳嗽。3. **热敷**：使用热敷袋或热毛巾敷在疼痛的部位，有助于缓解喉咙痛。4. **避免刺激**：避免使用可能刺激喉咙的化学物质，如某些化学物质。5. **保持室内湿润**：确保您的卧室有良好的通风，可以使用加湿器帮助保持空气湿度。6. **保持室内湿度**：使用加湿器可以帮助缓解喉咙干燥和咳嗽。如果症状持续不缓解或有其他不适症状，请立即就医。希望您早日康复！
[A1]: 如果您的咳嗽持续了两周，通常建议您尽快就医，因为这可能会影响您的呼吸和心跳。您可以通过以下步骤来检查咳嗽：1. **休息**：用纸巾或肘部轻轻按压咳嗽的位置。2. **使用药物**：在可能的情况下，使用药物（如抗过敏药、抗过敏药等）来缓解咳嗽。请在医生的指导下使用药物。3. **热敷**：在咳嗽期间，可以使用热敷来缓解疼痛和肿胀。4. **药物治疗**：如果咳嗽持续了两周，可以使用一些消炎药或抗过敏药来缓解症状。请务必在医生的指导下使用。5. **保持健康的生活方式**：保持充足的睡眠、均衡的饮食、适量的运动和避免吸烟和限酒，这些都有助于提高咳嗽的几率。请记住，这些建议不能替代专业医疗意见。如果您的症状持续或加重，强烈建议您尽快就医。
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;👉效果总结&lt;/p&gt; 
&lt;p&gt;full_sft模型在简洁性和信息准确性方面表现更好；rlhf模型在回答中倾向于提供更多的背景信息，但信息准确性有待改进。 总的来说RLHF后的模型倾向于学习：说更多有礼貌但无用的废话讨好“对话”本身，而对信息准确性则有轻微损失。 天下没有免费的午餐，还需要继续提升RLHF数据集的质量，也要接受模型能力无法避免的损失(程度有轻重)。 DPO和在线PPO的区别在于reject和chosen都是离线准备的，和minimind模型本身的输出必然存在很大的分布差异。 通俗地说DPO算法使模型观看乒乓球世界冠军的打法「录像」进行RL，而不是像PPO一样请reward模型做「教练」纠正自己的打法进行RL。&lt;/p&gt; 
&lt;h2&gt;Ⅱ 主观样例测评&lt;/h2&gt; 
&lt;p&gt;🏃以下测试于2025-02-09完成，此日期后发布的新模型，无特殊需要时将不加入测试。&lt;/p&gt; 
&lt;p&gt;[A] &lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch"&gt;MiniMind2 (0.1B)&lt;/a&gt;&lt;br /&gt; [B] &lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch"&gt;MiniMind2-MoE (0.15B)&lt;/a&gt;&lt;br /&gt; [C] &lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch"&gt;MiniMind2-Small (0.02B)&lt;/a&gt;&lt;br /&gt; [D] &lt;a href="https://pan.baidu.com/s/1_COe0FQRDmeapSsvArahCA?pwd=6666"&gt;minimind-v1-small(0.02B)&lt;/a&gt;&lt;br /&gt; [E] &lt;a href="https://pan.baidu.com/s/1tqB-GMvuiGQBvEl-yZ-oBw?pwd=6666"&gt;minimind-v1-moe(0.1B)&lt;/a&gt;&lt;br /&gt; [F] &lt;a href="https://pan.baidu.com/s/1p713loS7EfwHQf3G9eYI3Q?pwd=6666"&gt;minimind-v1(0.1B)&lt;/a&gt;&lt;br /&gt; [G] &lt;a href="https://github.com/DLLXW/baby-llama2-chinese"&gt;baby-llama2-chinese(0.2B)&lt;/a&gt;&lt;br /&gt; [H] &lt;a href="https://github.com/charent/ChatLM-mini-Chinese"&gt;chatlm-mini-chinese(0.2B)&lt;/a&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;[Q]: 你知道长江吗？
[A]: 长江，位于中国东南部，是中国的第一长河，全长6300多公里，流经11个省份，是中国最长的河流，也是中国的“母亲河”。长江的历史可以追溯到公元前2000多年的商朝时期，当时的商朝是商朝的后裔。商朝时期，长江的经济发展达到了高峰，是中国最重要的农业和工业中心之一。长江流域的经济主要以农业为主，长江渔业占据了主要产业，如钓鱼、猎鱼等。长江的水资源也非常丰富，包括河流、湖泊、河流等。长江的自然景观也十分壮观，长江四季景色各异，春季多雨，夏季多雨，秋季则多雨，冬季则寒冷多雪，春季和秋季都有美丽的花朵和果实。长江是中国的经济中心，拥有众多的国家和城市，是世界上重要的交通枢纽和航运中心。同时，长江还是中国的重要水利资源，对于中国的经济发展具有重要意义。
[B]: 是的，我知道长江。长江位于中国的中部，是中国第三长的河流，全长约6300公里，流经江西省、安徽省、江苏省、安徽省、江苏省和安徽省。
[C]: 当然，长江是中国最长的河流，全长约6300公里。它以其丰富的水资源、丰富的水生动植物资源以及独特的地理位置而闻名于世。
[D]: 是的，长江是中国最长的河流，全长超过3300千米。
[E]: 长江是中国最长的河流，总长度约为6300公里。它发源于青藏高原的青海湖，流经青海、四川、甘肃、宁夏、内蒙古、陕西和山西。长江流域是中国最大的农业区之一，也是中国的文化和经济中心。
[F]: 是的，长江是中国最长的河流之一，也是世界第三大河流。长江发源于青藏高原，流经11个省份，最终注入东海。长江流域是中国最大的河流流域之一，拥有着丰富的自然资源和文化遗产，如长江大桥、长江海洋博物馆、长江古城等。
[G]: 同时也知道，中国是世界上最长的城市之一，拥有超过3400万人口。
[H]: 是的，长江是中国最长的河流，也是世界上最长的河流之一。它发源于青藏高原，流经九个省区，最终注入东海。长江流域的气候温和，四季分明，是中国重要的农业和工业基地。长江流域的水产资源丰富，是中国的重要渔业资源之一。


[Q]: 世界上最高的山峰是什么？
[A]: 世界上最高的山峰是珠穆朗玛峰（Mount Everest），位于喜马拉雅山脉，尼泊尔和中国的边界上。它的海拔高度是8,848.86米（29,031.7英尺），是地球上最高的山峰。
[B]: 珠穆朗玛峰（Mount Everest）是世界上最高的山峰，海拔高度达8,848米（29,029英尺）。
[C]: 珠穆朗玛峰是世界上最高的山峰，位于喜马拉雅山脉，海拔8,848.86米（29,031.7英尺）。它位于尼泊尔和中国的边界上，是喜马拉雅山脉的一部分。
[D]: 世界上最高的山峰是珠穆朗玛峰，位于尼泊尔和中国的边境上，海拔8848米。
[E]: 世界上最高的山峰是珠穆朗玛峰，位于喜马拉雅山脉，海拔8,848米。
[F]: 珠穆朗玛峰是世界上最高的山峰，海拔8848米。
[G]: 世界上最高的山峰是珠穆朗玛峰，位于喜马拉雅山脉，海拔8,848.86米（29,031.69英尺）。
[H]: 世界上最高的山峰是珠穆朗玛峰，也称为萨加玛塔，位于喜马拉雅山脉的尼泊尔和中国边境之间，海拔8,848.86米（29,031.69英尺）。它是世界上最著名的登山目标之一，吸引着来自世界各地的登山者前来挑战。珠穆朗玛峰的名字来自藏语，意思是“女神的额峰”，它是由两个不同的山峰组成的，其中一个是位于尼泊尔的珠穆朗玛峰，另一个则位于中国。它的高度是通过测量海拔高度来确定的，因为它是通过测量地球的平均高度来确定的。尽管珠穆朗玛峰是世界上最高的山峰，但它并不是地球上任何其他地方都能到达的。地球上的其他地方，如南美洲的安第斯山脉，也是世界上最高的山脉，但它并不是地球上最高的山峰。总之，珠穆朗玛峰是世界上最高的山峰，但它并不是地球上任何地方都能到达的。它是地球上最高的山脉，但它并不是地球上任何地方都能到达的。


[Q]: 万有引力是谁提出的？
[A]: 万有引力是由牛顿在17世纪提出的，它是牛顿三大定律之一。牛顿在17世纪提出的引力定律揭示了万有引力定律的本质，并且解释了行星、卫星、彗星等天体之间如何相互作用。这个定律是由艾萨克·牛顿在1687年提出的，他通过对万有引力定律的分析和实验，成功地解释了天体之间的引力作用，从而解释了地球绕太阳旋转的现象。
[B]: 万有引力（Human Policy）是牛顿的三大力学定律，用于描述两个物体之间的相互作用力。它由万有引力定律和牛顿的第一定律（惯性定律）构成。万有引力是由于两个物体之间相互作用而产生的，它们的质量、距离、速度和方向等因素决定了它们之间的相互作用。这种相互作用力是：质量的乘积：质量的乘积是万有引力定律和牛顿第二定律的乘积。质量的乘积：质量的乘积是物体之间的一种物理过程，它是物体间相互作用的产物，对于其他物体来说，其质量可以表示为两个物体之间的质量之和，即重力加速度a。
[C]: 万有引力是一个由牛顿在1687年提出的物理学概念，它描述了物体之间引力如何相互作用的规律。
[D]: 万有引力是由英国物理学家牛顿提出的。他认为，所有物体都会产生引力，这个引力是由物体的质量和距离决定的。这个理论揭示了质量和距离之间的关系，并证明了万有引力的本质。
[E]: 万有引力是由牛顿在1687年提出的。它是宇宙中一个基本的物理现象，它描述了物体之间的相互作用。这个力被称为万有引力，它在所有物体之间都存在，这种力与物体的质量和距离有关。万有引力是自然界中最基本的力量之一，它不仅影响着物体的运动和变化，还对宇宙的结构和演化产生着重要的影响。
[F]: 万有引力是英国物理学家艾萨克·牛顿在17世纪提出的。
[G]: 引力是宇宙中一种力量，它是由质量或能量所组成的。根据牛顿的万有引力定律，一个物体之间的引力大小与它的质量成正比，与它的质量成反比。因此，牛顿的万有引力定律表明，一个物体之间的引力大小与它的质量成正比，与它的质量成反比。这个定律在物理学、天文学和物理学中都有广泛的应用。
[H]: 万有引力是由英国物理学家艾萨克·牛顿于1687年提出的。他在1687年发表了一篇著名的论文，题为《自然哲学的数学原理》，其中提出了万有引力定律，指出任何两个物体之间都存在着一种引力，这种引力的大小与它们的质量成正比，与它们之间的距离的平方成反比。这个定律被称为万有引力定律，它描述了质量和距离之间的关系。它表明，两个物体之间的引力与它们的质量成正比，与它们之间的距离的平方成反比。这个定律对于解释宇宙中的许多现象和现象之间的相互作用非常重要，例如行星绕太阳运动、恒星之间的引力透镜效应等等。它也被广泛应用于天文学、物理学和工程学等领域，为我们理解宇宙的本质和演化提供了重要的理论基础。


[Q]: 你知道大熊猫的主要食物是什么吗？
[A]: 大熊猫主要以竹子为食，但也会吃其他植物、昆虫、小型哺乳动物和鱼类。它们的消化系统与其他熊科动物类似，能够消化竹子。大熊猫的牙齿锋利，适合咀嚼竹子，因此它们经常被称为“竹食动物”。
[B]: 大熊猫是以竹子为食的哺乳动物，它们是肉食动物。它们主要食物是竹子，特别是竹子中富含纤维素和矿物质，并且不会直接进入消化系统。大熊猫的肠道主要由竹子组成，它们需要消化竹子中的纤维素和其他纤维素来支持它们的正常进食。
[C]: 大熊猫主要以竹子为食，但它们也吃其他植物，包括树叶、嫩芽、昆虫、水果、蔬菜和小型哺乳动物等。大熊猫是中国特有的物种，属于熊科，主要分布在中国的四川、陕西和甘肃等地。
[D]: 大熊猫的主要食物是竹子。它们通常吃竹子，每天需要吃掉大约30公斤的竹子。
[E]: 是的，大熊猫主要以竹子为食。它们每天需要吃掉大量的竹子来维持生命，每天需要吃掉40至80公斤的竹子。此外，它们也会吃一些其他的植物，如草、果实、坚果和小型哺乳动物。
[F]: 大熊猫主要以竹子为食，也会偶尔食用其他的植物和动物。此外，大熊猫也会吃一些野果、小动物和植物，但这些都不属于它们的主要食物来源。
[G]: 大熊猫的主要食物是竹子。
[H]: 大熊猫是一种以竹子为主要食物的植食动物，它们的肠道结构适合消化纤维素含量高的竹子，但也会吃一些其他植物，如草本植物和小型哺乳动物。大熊猫的肠道结构也适应了高纤维的竹子，这使得它们能够消化大量的纤维素，从而帮助它们在野外生存。大熊猫的肠道中还含有一种叫做“纤维素酶”的酶类物质，这种酶能够帮助大熊猫分解纤维素，从而帮助它们消化大量的水分和营养物质。此外，大熊猫的肠道还含有一种叫做“纤维素蛋白”的蛋白质，这种蛋白质能够帮助大熊猫维持体内的能量水平，从而帮助它们在野外生存。总的来说，大熊猫的主要食物是竹子，但它们也会吃一些其他植物，如草本植物和小型哺乳动物。


[Q]: 海水为什么是咸的？
[A]: 海水之所以是咸的，主要是由于其中的化学成分。海水中含有大量的矿物质和盐分，其中主要成分是硫酸盐、碳酸盐、镁离子和钾离子。这些物质在高浓度下，会沉淀在海水中，形成海水。具体来说，海水中的矿物质和盐分主要来自于岩石、土壤、生物和人类排泄物等。岩石是海水的组成部分，主要由矿物质组成，如石英、长石、云母等。土壤中含有大量的化学物质，如氮、磷、钾等，这些物质会渗透到土壤中，形成土壤。生物体内的盐分主要来源于植物的叶绿素等色素，这些色素会吸收太阳光中的能量，转化为生物体可以利用的能量。而人类排泄物中含有的盐分主要来源于矿物质和碳酸盐，这些矿物质在水中的浓度较高，会导致水中溶解的矿物质浓度增加，形成水体中的盐分。因此，海水之所以是咸的，是由于其中的化学成分和结构特征。海水中的矿物质和盐分主要来自于岩石、土壤、生物和人类排泄物等，而这些物质的积累和排泄会导致海水呈现出咸味。
[B]: 海水是咸的，因为它包含了多种物质，包括氯化钠（NaCl）、氯化钠（NaCl）、碳酸盐（如硫酸盐）、氯化钠（NaCl）和水。咸物质的来源：盐是咸的常见来源，含有钾、钙、镁、钾等矿物质。这些矿物质通常来自于地下水、地下水或土壤中，这些矿物质的含量远低于纯净水。咸味的原因：盐的咸味主要源于其溶解于水中的微小颗粒物。这些小颗粒物主要通过植物根部和动物体内的微生物传播，如昆虫、小型哺乳动物、鸟类和某些昆虫。综上所述，海水是咸的，因为它含有大量的盐类，包括矿物质、岩石、盐、矿物质等，这些物质在海水中的浓度高，对咸味的产生较大影响。
[C]: 海水是咸的，主要是因为其中含有各种盐分，其中以海水为主要成分之一。当海水中含有大量的盐分时，盐分会被溶解在海水中，形成咸味。海水中的盐分主要来源于溶解盐类、蒸发析出的盐和海水冻结成冰等因素。
[D]: 海水中含有许多盐分，这些盐分可以导致海水咸度的增加。海水中的盐分主要来自于海水中的盐分，这些盐分可以从海水中的盐分中获得。海水中的氧化物和盐分可以与海水中的盐分形成氢键，从而使海水咸度增加。
[E]: 海水是咸的，这是因为它含有大量的盐分，其中包括氯离子和氯离子。这些离子在海洋中形成了大量的盐，这些盐分被称为盐分。海水中含有许多溶解在水中的盐，这些盐分会随着海水的升高而逐渐蒸发，最终导致海水中盐分的蒸发。这些盐分在海水中被完全消耗，然后被排除在海水中。此外，海水中还含有许多其他的盐分，但这些盐分是不同的，因为它们在水中的浮起和下降过程是不同的。海水中的盐分是从海底的岩石中提取出来的，这些盐分在海水中可以被利用。总的来说，海水是咸的，这也是为什么海水中含有许多盐分和盐分的原因。
[F]: 海水含有许多溶解的盐类，其中最主要的是氯化钠。当水中加入盐时，盐的分子会与水分子发生反应，形成氢氧化钠，这是一种盐。
[G]: 因为水是许多生物的生存所必需的，它能够吸收和释放二氧化碳，释放氧气，维持大气中的水蒸气，以及维持水循环。水中的盐类和化学物质会释放出氧气，使地球上的生物呼吸。此外，水还具有许多其他功能，如保持水循环、维持水循环、调节气候和环境等。水是许多生物生存所必需的，它也是许多自然现象的重要基础。
[H]: 海水是咸的主要原因是因为其中含有大量的溶解盐类，其中最主要的是氯化钠（NaCl）。氯化钠是一种非常常见的盐类，它存在于地球上的大部分地区，包括海洋、湖泊、河流和地下水。当水从海洋中蒸发时，它会带走一部分盐分，使海水变得咸。此外，海中还含有其他溶解盐类的物质，如硫酸盐、硫酸镁、硫酸钙、硫酸镁盐、硫酸镁钾、硫酸镁钠、硫酸镁钙、硫酸镁镁钙、硫酸镁镁钙、硫酸镁镁钙、硫酸镁镁钠、硫酸镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁镁钙、硫酸镁镁镁镁镁钙、硫酸镁镁镁镁镁钙、硫酸镁镁镁镁镁钙、硫酸镁镁镁镁镁镁钙、硫酸镁镁镁镁镁镁钙、硫酸镁镁镁镁镁镁钙、硫酸镁镁镁镁镁镁镁钙、硫酸镁镁镁镁


[Q]: 写一段python的快速排序代码
[A]: 好的，以下是Python的快速排序代码：def quick_sort(arr):    if len(arr) &amp;lt;= 1:        return arr    else:        pivot = arr[len(arr) // 2]        left = [x for x in arr[1:]]        right = [x for x in arr[2:] if x &amp;lt; pivot]        return quick_sort(left) + [pivot] + quick_sort(right)# 测试代码arr = [3, 6, 8, 10, 1, 2, 1]print(quick_sort(arr))这段代码首先检查数组的长度是否小于等于1。如果是，那么它就按照从小到大的顺序排列。接着，对于数组的每个元素，它想要进行递归地排序，直到达到排序的结束位置。如果数组的长度是奇数，那么它就按照从大到小的顺序排列。最后，平均的时间复杂度是O(n log n)，其中n是数组的长度。
[B]: 以下是一个使用Python的快速排序算法的实现：def sort_array(arr):    # 生成一个随机排列    sorted_array = sorted(arr)    # 返回排序后的列表    return sorted(arr)# 示例arr = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]# 调用函数sorted_array = sort_array(arr)print("原始数组是：", sorted_array)这段代码首先生成一个随机排列，然后通过一个列表的长度进行排序。最后，返回排序后的列表。注意，这个代码使用了Python的内置sorted函数和内置的sorted()函数，它们在时间复杂度为O(n log n)的情况下，比直接使用sorted()函数更快。
[C]: 以下是一个简单的排序算法，适用于小型数组，包括小于等于5的数组。def quick_sort(arr):    if len(arr) &amp;lt;= 1:        return arr    else    for _ in range(2, 51):        if arr[len(arr) - 1:] is not None:            arr[len(arr) - 1] = arr[len(arr) - 1]                arr[len(arr)] = arr[len(arr)]    return quick_sort(arr)# 示例arr = [3, 5, 1, 4, 1, 2, 1]sorted_arr = quick_sort(arr)print("Sorted array is:", sorted_arr)### 代码解释：1. **函数定义**：quick_sort 函数接受一个数组 arr 作为参数。2. **循环**：对于 arr 的元素，使用递归来实现，即从右到左遍历数组，并将当前元素与数组的当前元素交换位置，直到数组达到空或数组长度小于等于 arr。3. **递归调用**：遍历数组，将当前元素与数组的当前元素进行交换，直到数组达到空或数组长度小于等于 arr。4. **检查数组长度**：使用 if __name__ == "__main__" 检查数组的长度是否小于 arr，如果是，就将数组从较大的数组移动到数组的末尾。5. **输出排序后的结果**
[D]: 作为一个AI语言模型，我无法编写Python程序。但是，Python是一个非常流行的编程语言，它具有许多优秀的库和框架，包括NumPy、Pandas、Matplotlib、Pandas、Bandke、Norm和NKlearn等。Python有很多优秀的自然语言处理库和框架，可以用于处理和分析各种类型的数据，如文本、图像、音频、视频等。Python还可以用于创建各种应用程序，如网页搜索、游戏开发、科学计算等。Python非常灵活，也支持多种编程风格，因此可以轻松地找到适合自己的解决方案。
[E]: 作为AI语言模型，我可以回答您的问题。但是我无法进行编程操作，因为我无法确定您想问的是哪种编程语言。请您提供更具体的问题，以便我能够更好地回答您的问题。
[F]: 快速排序（Quick Sort）是一种简单的排序算法，它通过将数据划分为小组，每组数据按照一定规则排序，最终得到一个有序数组。
[G]: 好的，以下是一段快速排序的Python代码```python sorted_print(sorted(sorted(sorted(sorted_print))```输出结果为：```[1, 2, 3, 4, 5, 6, 7, 8, 9]```
[H]: 快速排序是一种基于分治思想的高效排序算法，它的时间复杂度为 O(n log n)。它的基本思想是通过一趟排序将待排序序列划分为两个子序列，其中一个子序列的所有元素都比另一个子序列的所有元素小，然后对这两个子序列分别递归地进行排序。下面是一个 Python 代码示例，实现了快速排序算法：```pythondef quick_sort(arr):    if len(arr) &amp;gt; 1:        return arr    else: return quick_sort(arr)```该代码使用了递归的方式，首先判断数组的长度是否小于等于1，如果是，则直接返回数组，否则递归地对数组进行排序。在递归过程中，我们首先判断数组的第一个元素是否大于等于1，如果是，则将数组中的元素与第一个元素进行比较，如果不等于1，则将数组中的元素与第二个元素进行比较，以此类推，直到所有元素都被排序。该代码的时间复杂度为 O(n log n)，是一个非常实用的排序算法。


[Q]: 截至2025年2月9日14时31分，电影《哪吒之魔童闹海》累计票房（含预售）突破78.09亿元，超过《星球大战：天行者崛起》的票房成绩，成为首部进入全球票房榜前40的亚洲电影。这一成就不仅标志着中国电影在国际市场的突破，也展示了中国动画电影的巨大潜力和市场吸引力。从这段文字总结20字的摘要给我。
[A]: 电影《哪吒之魔童闹海》票房突破78.09亿元，成为首部进入全球票房榜前40的亚洲电影。这标志着中国电影的国际突破，也展现了中国动画电影的巨大潜力。
[B]: 2025年2月9日，中国电影在世界舞台上突破78.09亿元，超过《星球大战：天行者崛起》的票房成绩。
[C]: 电影《哪吒之魔童闹海》票房突破78.09亿元，成为全球票房榜前40的亚洲电影，展现中国动画电影的巨大潜力。
[D]: 《哪吒之魔童闹海》累计票房突破78.09亿，成为首部进入全球票房榜前40的亚洲电影，展示了中国电影在国际市场的突破，同时展示了中国动画电影的巨大潜力和市场吸引力。
[E]: 《猛龙强：打开世界奇幻冒险》成功地，上海成功吸引了众多观众，同时也引起了广泛的关注。
[F]: 《哪吒之魔童闹海》是一部让中国动画电影成为国际市场的巨大潜力。该电影以其华丽的画面和惊险的战斗场面而闻名于世。
[G]: 《哪行是值得的》是2021年1月9日14时31分，电影《哪成大作为》在电影市场突破7800亿美元，成为中国动画电影的突破，并展示中国动画电影的巨大潜力和市场吸引力。
[H]: 我是一个参数很少的AI模型🥺，知识库较少，无法直接回答您的问题，换个问题试试吧👋
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;🙋‍直接把以上所有问题和模型的回答丢给DeepSeek-R1，让它帮忙点评和排名打分：&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;具体点评&lt;/summary&gt; 
 &lt;h3&gt;评分标准：&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;准确性&lt;/strong&gt;：回答是否正确且无明显错误。&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;完整性&lt;/strong&gt;：回答是否涵盖了问题的核心要点。&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;逻辑性&lt;/strong&gt;：回答是否有条理，是否符合逻辑。&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;代码质量&lt;/strong&gt;：代码是否能正常运行，逻辑是否清晰。&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;点评：&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;A模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：回答非常全面，信息量大，逻辑清晰，尤其在长江、大熊猫、海水咸味等问题上表现优异。代码虽然有小瑕疵，但整体思路正确。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：部分回答稍显冗长，但不影响整体质量。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：综合表现最佳，得分最高。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;H模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：回答较为准确，尤其在珠穆朗玛峰、万有引力等问题上表现出色。代码虽未完全展示，但解释较为详细。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：部分回答略显啰嗦，但逻辑性较强。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：仅次于A模型，表现稳定。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;C模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：回答简洁明了，尤其在大熊猫、快速排序代码等问题上表现较好。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：部分回答略显简短，缺乏深入解释。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：整体表现不错，但细节上略逊于A和H。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;F模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：回答较为准确，尤其在长江、万有引力等问题上表现尚可。代码部分有一定逻辑性。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：部分回答不够深入，代码存在一些小问题。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：表现中规中矩，仍有提升空间。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;D模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：回答基本准确，尤其在万有引力、长江等问题上表现尚可。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：部分回答过于简略，代码存在明显错误。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：表现一般，需改进代码部分。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;B模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：回答较为准确，尤其在长江、海水咸味等问题上表现尚可。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：部分回答逻辑性较差，代码存在较大问题。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：表现平平，需进一步优化。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;E模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：部分回答较为准确，尤其在海水咸味、大熊猫等问题上表现尚可。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：回答过于简略，代码部分几乎无法运行。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：表现不佳，需大幅提升。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;G模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：几乎没有明显的优点。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：回答严重偏离主题，代码部分完全无法运行。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：表现最差，需大幅改进。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;总结：&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;A模型&lt;/strong&gt;在各方面表现最为出色，尤其在复杂问题的回答上展现了极高的准确性与逻辑性。&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;H模型&lt;/strong&gt;紧随其后，表现稳定，但在某些细节上略显不足。&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;G模型&lt;/strong&gt;表现最差，回答偏离主题且代码无法运行，需大幅改进。&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;打分排序&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;排名&lt;/th&gt; 
   &lt;th&gt;模型&lt;/th&gt; 
   &lt;th&gt;准确性 (30分)&lt;/th&gt; 
   &lt;th&gt;完整性 (30分)&lt;/th&gt; 
   &lt;th&gt;逻辑性 (20分)&lt;/th&gt; 
   &lt;th&gt;代码质量 (20分)&lt;/th&gt; 
   &lt;th&gt;总分 (100分)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;A&lt;/td&gt; 
   &lt;td&gt;28&lt;/td&gt; 
   &lt;td&gt;29&lt;/td&gt; 
   &lt;td&gt;19&lt;/td&gt; 
   &lt;td&gt;20&lt;/td&gt; 
   &lt;td&gt;96&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;H&lt;/td&gt; 
   &lt;td&gt;27&lt;/td&gt; 
   &lt;td&gt;28&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;20&lt;/td&gt; 
   &lt;td&gt;93&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;C&lt;/td&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td&gt;27&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;89&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;F&lt;/td&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;86&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;D&lt;/td&gt; 
   &lt;td&gt;24&lt;/td&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;82&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;6&lt;/td&gt; 
   &lt;td&gt;B&lt;/td&gt; 
   &lt;td&gt;23&lt;/td&gt; 
   &lt;td&gt;24&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td&gt;78&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;7&lt;/td&gt; 
   &lt;td&gt;E&lt;/td&gt; 
   &lt;td&gt;22&lt;/td&gt; 
   &lt;td&gt;23&lt;/td&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td&gt;14&lt;/td&gt; 
   &lt;td&gt;74&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;G&lt;/td&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;42&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;👉主观效果总结&lt;/h3&gt; 
&lt;p&gt;个人主观评价与DeepSeek-R1基本相符，其中：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;MiniMind系列的排序非常符合直觉，参数越大+训练数据越充分评分越高，幻觉和错误都会比小模型肉眼可见的好。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;H模型的回答肉眼看起来是不错的，尽管存在些许幻觉瞎编的情况。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;G模型可能训练数据不够完备，给出的权重经过测试效果不佳。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;再复诵一遍经久不衰的Scaling Law: 参数越大，训练数据越多模型的性能越强。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Ⅲ Objective Benchmark&lt;/h2&gt; 
&lt;p&gt;下面就到喜闻乐见的benchmark刷榜测试环节，就不找乐子和qwen、glm级别的中文模型做对比了。 这里选取了一些&amp;lt;1B的微型模型进行横评比较， 测试集选择C-Eval、CMMLU、A-CLUE、TMMLU+这几个纯中文语言榜单。&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;测评框架&lt;/summary&gt; 
 &lt;p&gt;测评框架选择&lt;a href="https://github.com/EleutherAI/lm-evaluation-harness"&gt;lm-evaluation&lt;/a&gt;， 安装后启动测试非常方便：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;lm_eval --model hf --model_args pretrained=&amp;lt;填写模型路径&amp;gt;,device=cuda,dtype=auto --tasks ceval* --batch_size 8 --trust_remote_code
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;PS: 在这种全是选择题的测评集中，为了避免回复格式的难以固定的特点， 所以常用做法是直接把&lt;code&gt;A&lt;/code&gt;,&lt;code&gt;B&lt;/code&gt;,&lt;code&gt;C&lt;/code&gt;,&lt;code&gt;D&lt;/code&gt;四个字母对应token的预测概率取出来，将其中概率最大的字母与标准答案计算正确率。 选择题1/4乱选的正确率是25%，然而这个量级的所有模型都集中在25附近，甚至很多时候不如瞎选，是不是像极了高中完形填空的滑铁卢正确率... MiniMind模型本身预训练数据集小的可怜，也没有针对性的对测试集做刷榜微调，因此结果图一乐即可：&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;models&lt;/th&gt; 
   &lt;th&gt;from&lt;/th&gt; 
   &lt;th&gt;params↓&lt;/th&gt; 
   &lt;th&gt;ceval↑&lt;/th&gt; 
   &lt;th&gt;cm mlu↑&lt;/th&gt; 
   &lt;th&gt;aclue↑&lt;/th&gt; 
   &lt;th&gt;tmmlu+↑&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2&lt;/td&gt; 
   &lt;td&gt;JingyaoGong&lt;/td&gt; 
   &lt;td&gt;104M&lt;/td&gt; 
   &lt;td&gt;26.52&lt;/td&gt; 
   &lt;td&gt;24.42&lt;/td&gt; 
   &lt;td&gt;24.97&lt;/td&gt; 
   &lt;td&gt;25.27&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-Small&lt;/td&gt; 
   &lt;td&gt;JingyaoGong&lt;/td&gt; 
   &lt;td&gt;26M&lt;/td&gt; 
   &lt;td&gt;26.37&lt;/td&gt; 
   &lt;td&gt;24.97&lt;/td&gt; 
   &lt;td&gt;25.39&lt;/td&gt; 
   &lt;td&gt;24.63&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-MoE&lt;/td&gt; 
   &lt;td&gt;JingyaoGong&lt;/td&gt; 
   &lt;td&gt;145M&lt;/td&gt; 
   &lt;td&gt;26.6&lt;/td&gt; 
   &lt;td&gt;25.01&lt;/td&gt; 
   &lt;td&gt;24.83&lt;/td&gt; 
   &lt;td&gt;25.01&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/zhanshijinwat/Steel-LLM"&gt;Steel-LLM&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ZhanShiJin&lt;/td&gt; 
   &lt;td&gt;1121M&lt;/td&gt; 
   &lt;td&gt;24.81&lt;/td&gt; 
   &lt;td&gt;25.32&lt;/td&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td&gt;24.39&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/openai-community/gpt2-medium"&gt;GPT2-medium&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
   &lt;td&gt;360M&lt;/td&gt; 
   &lt;td&gt;23.18&lt;/td&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;18.6&lt;/td&gt; 
   &lt;td&gt;25.19&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/jzhang38/TinyLlama"&gt;TinyLlama-1.1B-Chat-V1.0&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;TinyLlama&lt;/td&gt; 
   &lt;td&gt;1100M&lt;/td&gt; 
   &lt;td&gt;25.48&lt;/td&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;25.4&lt;/td&gt; 
   &lt;td&gt;25.13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/huggingface/smollm"&gt;SmolLM2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;HuggingFaceTB&lt;/td&gt; 
   &lt;td&gt;135M&lt;/td&gt; 
   &lt;td&gt;24.37&lt;/td&gt; 
   &lt;td&gt;25.02&lt;/td&gt; 
   &lt;td&gt;25.37&lt;/td&gt; 
   &lt;td&gt;25.06&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/BAAI/Aquila-135M-Instruct"&gt;Aquila-Instruct&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;BAAI&lt;/td&gt; 
   &lt;td&gt;135M&lt;/td&gt; 
   &lt;td&gt;25.11&lt;/td&gt; 
   &lt;td&gt;25.1&lt;/td&gt; 
   &lt;td&gt;24.43&lt;/td&gt; 
   &lt;td&gt;25.05&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/compare_radar.png" alt="compare_radar" /&gt;&lt;/p&gt; 
&lt;h1&gt;📌 其它 (Others)&lt;/h1&gt; 
&lt;h2&gt;模型转换&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/scripts/convert_model.py"&gt;./scripts/convert_model.py&lt;/a&gt;可以实现&lt;code&gt;torch模型/transformers&lt;/code&gt;模型之间的转换&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;基于MiniMind-API服务接口&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/scripts/serve_openai_api.py"&gt;./scripts/serve_openai_api.py&lt;/a&gt;完成了兼容openai-api的最简聊天接口，方便将自己的模型接入第三方UI 例如FastGPT、OpenWebUI、Dify等等。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;从&lt;a href="https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5"&gt;Huggingface&lt;/a&gt;下载模型权重文件，文件树：&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;MiniMind-Model-Name&amp;gt; (root dir)
├─&amp;lt;MiniMind-Model-Name&amp;gt;
|  ├── config.json
|  ├── generation_config.json
|  ├── LMConfig.py
|  ├── model.py
|  ├── pytorch_model.bin
|  ├── special_tokens_map.json
|  ├── tokenizer_config.json
|  ├── tokenizer.json
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;启动聊天服务端&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python serve_openai_api.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;测试服务接口&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python chat_openai_api.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;API接口示例，兼容openai api格式&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;curl http://ip:port/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{ 
    "model": "model-identifier",
    "messages": [ 
      { "role": "user", "content": "世界上最高的山是什么？" }
    ], 
    "temperature": 0.7, 
    "max_tokens": 512,
    "stream": true
}'
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;VLLM模型推理（服务）&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;是极其流行的高效推理框架，支持大模型快速部署，优化显存利用与吞吐量。&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vllm serve ./MiniMind2/ --model-impl transformers --served-model-name "minimind"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;服务将以openai api协议启动，端口默认为8000。&lt;/p&gt; 
&lt;p&gt;更多用法请参考官方说明～&lt;/p&gt; 
&lt;h2&gt;llama.cpp&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt;是一个C++库， 可以在命令行下直接使用，支持多线程推理，支持GPU加速。&lt;/p&gt; 
&lt;p&gt;参考官方仓库安装后，在&lt;code&gt;convert_hf_to_gguf.py&lt;/code&gt; ～760行插入&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;# 添加MiniMind2 tokenizer支持
if res is None:
    res = "smollm"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;转换自定义训练的minimind模型 -&amp;gt; gguf&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python convert_hf_to_gguf.py ../minimind/MiniMind2/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;量化模型&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./build/bin/llama-quantize ../minimind/MiniMind2/MiniMind2-109M-F16.gguf ../minimind/MiniMind2/Q4-MiniMind2.gguf Q4_K_M
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;命令行推理&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./build/bin/llama-cli -m ../minimind/MiniMind2/MiniMind2-109M-F16.gguf --chat-template chatml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;更多用法请参考官方说明～&lt;/p&gt; 
&lt;h2&gt;ollama&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://ollama.ai/"&gt;ollama&lt;/a&gt;是本地运行大模型的工具，支持多种开源LLM，简单易用。&lt;/p&gt; 
&lt;p&gt;通过ollama加载自定义的gguf模型，新建minimind.modelfile：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;FROM ./MiniMind2-109M-F16.gguf
TEMPLATE """{{ if .System }}&amp;lt;|im_start|&amp;gt;system
{{ .System }}&amp;lt;|im_end|&amp;gt;
{{ end }}{{ if .Prompt }}&amp;lt;|im_start|&amp;gt;user
{{ .Prompt }}&amp;lt;|im_end|&amp;gt;
{{ end }}&amp;lt;|im_start|&amp;gt;assistant
"""
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;加载模型并命名为&lt;code&gt;minimind2&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ollama create -f minimind.modelfile minimind2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;启动推理&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;ollama run minimind2
&amp;gt; 你好，我是MiniMind2，一个基于xxxxxxxx
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;更多用法请参考官方说明～&lt;/p&gt; 
&lt;h1&gt;📌 Acknowledge&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] 如果觉得&lt;code&gt;MiniMind系列&lt;/code&gt;对您有所帮助，可以在 GitHub 上加一个⭐&lt;br /&gt; 篇幅超长水平有限难免纰漏，欢迎在Issues交流指正或提交PR改进项目&lt;br /&gt; 您的小小支持就是持续改进此项目的动力！&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;🤝&lt;a href="https://github.com/jingyaogong/minimind/graphs/contributors"&gt;贡献者&lt;/a&gt;&lt;/h2&gt; 
&lt;!--
&lt;a href="https://github.com/jingyaogong/minimind/graphs/contributors"&gt;
  &lt;img src="https://contrib.rocks/image?repo=jingyaogong/minimind&amp;v3" /&gt;
&lt;/a&gt;
--&gt; 
&lt;p&gt;&lt;a href="https://github.com/jingyaogong"&gt;&lt;img src="https://avatars.githubusercontent.com/u/62287848" width="70px" height="70px" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://github.com/MuWinds"&gt;&lt;img src="https://avatars.githubusercontent.com/u/93832089" width="70px" height="70px" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://github.com/chuanzhubin"&gt;&lt;img src="https://avatars.githubusercontent.com/u/2813798" width="70px" height="70px" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://github.com/iomgaa-ycz"&gt;&lt;img src="https://avatars.githubusercontent.com/u/124225682" width="70px" height="70px" /&gt;&lt;/a&gt; &amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;😊鸣谢&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/ipfgao"&gt;&lt;b&gt;@ipfgao&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/issues/26"&gt;🔗训练步骤记录&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/chuanzhubin"&gt;&lt;b&gt;@chuanzhubin&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/pull/34"&gt;🔗代码逐行注释&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/WangRongsheng"&gt;&lt;b&gt;@WangRongsheng&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/issues/39"&gt;🔗大型数据集预处理&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/pengqianhan"&gt;&lt;b&gt;@pengqianhan&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/issues/73"&gt;🔗一个简明教程&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/RyanSunn"&gt;&lt;b&gt;@RyanSunn&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/issues/75"&gt;🔗推理过程学习记录&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Nijikadesu"&gt;&lt;b&gt;@Nijikadesu&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/issues/213"&gt;🔗以交互笔记本方式分解项目代码&lt;/a&gt;&lt;/p&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;参考链接 &amp;amp; 感谢以下优秀的论文或项目&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;排名不分任何先后顺序&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/meta-llama/llama3"&gt;https://github.com/meta-llama/llama3&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/karpathy/llama2.c"&gt;https://github.com/karpathy/llama2.c&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/DLLXW/baby-llama2-chinese"&gt;https://github.com/DLLXW/baby-llama2-chinese&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://arxiv.org/abs/2405.04434"&gt;(DeepSeek-V2)https://arxiv.org/abs/2405.04434&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/charent/ChatLM-mini-Chinese"&gt;https://github.com/charent/ChatLM-mini-Chinese&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/wdndev/tiny-llm-zh"&gt;https://github.com/wdndev/tiny-llm-zh&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://arxiv.org/pdf/2401.04088"&gt;(Mistral-MoE)https://arxiv.org/pdf/2401.04088&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/Tongjilibo/build_MiniLLM_from_scratch"&gt;https://github.com/Tongjilibo/build_MiniLLM_from_scratch&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/jzhang38/TinyLlama"&gt;https://github.com/jzhang38/TinyLlama&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/AI-Study-Han/Zero-Chatgpt"&gt;https://github.com/AI-Study-Han/Zero-Chatgpt&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/xusenlinzy/api-for-open-llm"&gt;https://github.com/xusenlinzy/api-for-open-llm&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/HqWu-HITCS/Awesome-Chinese-LLM"&gt;https://github.com/HqWu-HITCS/Awesome-Chinese-LLM&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;🫶支持者&lt;/h2&gt; 
&lt;a href="https://github.com/jingyaogong/minimind/stargazers"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://reporoster.com/stars/dark/jingyaogong/minimind" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://reporoster.com/stars/jingyaogong/minimind" /&gt; 
  &lt;img alt="github contribution grid snake animation" src="https://reporoster.com/stars/jingyaogong/minimind" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;a href="https://github.com/jingyaogong/minimind/network/members"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://reporoster.com/forks/dark/jingyaogong/minimind" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://reporoster.com/forks/jingyaogong/minimind" /&gt; 
  &lt;img alt="github contribution grid snake animation" src="https://reporoster.com/forks/jingyaogong/minimind" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=jingyaogong/minimind&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=jingyaogong/minimind&amp;amp;type=Date" /&gt; 
 &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=jingyaogong/minimind&amp;amp;type=Date" /&gt; 
&lt;/picture&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;This repository is licensed under the &lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/LICENSE"&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PaddlePaddle/PaddleOCR</title>
      <link>https://github.com/PaddlePaddle/PaddleOCR</link>
      <description>&lt;p&gt;Turn any PDF or image document into structured data for your AI. A powerful, lightweight OCR toolkit that bridges the gap between images/PDFs and LLMs. Supports 100+ languages.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/Banner.png" alt="PaddleOCR Banner" /&gt; &lt;/p&gt; 
 &lt;p&gt;English | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_cn.md"&gt;简体中文&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_tcn.md"&gt;繁體中文&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ja.md"&gt;日本語&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ko.md"&gt;한국어&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_fr.md"&gt;Français&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ru.md"&gt;Русский&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_es.md"&gt;Español&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ar.md"&gt;العربية&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleOCR"&gt;&lt;img src="https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?color=ccf" alt="stars" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2507.05595"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2507.05595-b31b1b.svg?logo=arXiv" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/paddleocr"&gt;&lt;img src="https://static.pepy.tech/badge/paddleocr/month" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/paddleocr"&gt;&lt;img src="https://static.pepy.tech/badge/paddleocr" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/PaddlePaddle/PaddleOCR/network/dependents"&gt;&lt;img src="https://img.shields.io/badge/Used%20by-5.9k%2B%20repositories-blue" alt="Used by" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://img.shields.io/badge/python-3.8~3.12-aff.svg?sanitize=true" alt="python" /&gt; &lt;img src="https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg?sanitize=true" alt="os" /&gt; &lt;img src="https://img.shields.io/badge/hardware-cpu%2C%20gpu%2C%20xpu%2C%20npu-yellow.svg?sanitize=true" alt="hardware" /&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-Apache_2.0-green" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/PaddlePaddle/PaddleOCR"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;PaddleOCR is an industry-leading, production-ready OCR and document AI engine, offering end-to-end solutions from text extraction to intelligent document understanding&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h1&gt;PaddleOCR&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://www.paddlepaddle.org.cn/en"&gt;&lt;img src="https://img.shields.io/badge/PaddlePaddle-3.0-orange" alt="Framework" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Recognition%20Accuracy-%F0%9F%8F%86-green" alt="Accuracy" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Support_Languages-100+-brightgreen" alt="Multi-Language" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Handwriting-%E2%9C%93-success" alt="Handwriting" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Heterogeneous%20Hardware-Kunlunxin%20%7C%20Ascend_NPU-red" alt="Hardware" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] PaddleOCR now provides an MCP server that supports integration with Agent applications like Claude Desktop. For details, please refer to &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html"&gt;PaddleOCR MCP Server&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;The PaddleOCR 3.0 Technical Report is now available. See details at: &lt;a href="https://arxiv.org/abs/2507.05595"&gt;PaddleOCR 3.0 Technical Report&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;The PaddleOCR-VL Technical Report is now available. See details at &lt;a href="https://arxiv.org/abs/2510.14528"&gt;PaddleOCR-VL Technical Report&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;PaddleOCR&lt;/strong&gt; converts documents and images into &lt;strong&gt;structured, AI-friendly data&lt;/strong&gt; (like JSON and Markdown) with &lt;strong&gt;industry-leading accuracy&lt;/strong&gt;—powering AI applications for everyone from indie developers and startups to large enterprises worldwide. With over &lt;strong&gt;50,000 stars&lt;/strong&gt; and deep integration into leading projects like &lt;strong&gt;MinerU, RAGFlow, and OmniParser&lt;/strong&gt;, PaddleOCR has become the &lt;strong&gt;premier solution&lt;/strong&gt; for developers building intelligent document applications in the &lt;strong&gt;AI era&lt;/strong&gt;.&lt;/p&gt; 
&lt;h3&gt;PaddleOCR 3.0 Core Features&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/PaddlePaddle/PaddleOCR-VL_Online_Demo"&gt;&lt;img src="https://img.shields.io/badge/PaddleOCR--VL-_Demo_on_HuggingFace-yellow?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="HuggingFace" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/application/detail/98365"&gt;&lt;img src="https://img.shields.io/badge/PaddleOCR--VL-_Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://www.modelscope.cn/studios/PaddlePaddle/PaddleOCR-VL_Online_Demo"&gt;&lt;img src="https://img.shields.io/badge/PaddleOCR--VL-_Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&amp;amp;labelColor=white" alt="ModelScope" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://aistudio.baidu.com/community/app/91660/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP--OCRv5-Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518494/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP--StructureV3-Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518493/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP--ChatOCRv4-Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAgAAAAIACAMAAADDpiTIAAAABlBMVEU2P+X///+1KuUwAAAHKklEQVR42u3dS5bjOAwEwALvf2fMavZum6IAImI7b2yYSqU+1Zb//gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADKCR/+fzly7rD92yVg69xh8zeLwOa5w+ZvFYHtc4ft3ykB++cOm79PAp6YO2z/Ngl4ZO5l+9+yT4QAvLqS748VF33Ylzdvzpl72f6z53YIGJ6SZdPeNHcIwOycaADdLgCSIgAIgCOAACAAykIAEAAEAAFAABCAT+WQuQVgeBqXhXQIQAAYegowLQBpbg3gZGFyAC6vgBQAMREA2/YfDPxyaDQNyTNz+3Zwn5J4ZG7PB2h0kHhi7plPCImmJwkPzO0RMa3OET0i5uGlzHFze0xcu0vE2Dq3J4U2vEPgSaHbFzPNDQAAAAAAAMBNovdw+cP/ny+uaf7w/+eYADy8kE+F4Offdjn6zZXhAXgiA78G4MNNsmnu1Xr7b3mbOL8T5Ja5bw/A35EC2LiWpzt1y9jRugBy30fLg3NvHPvnuZcC2NsCUXA/aRmA89V07Fwgt37uH8deCmBr6N44pP4UgaUATpdA7v/cMbIB8okliY65/SW5HhJ1ehPmM+8edwXgpbu4R88FayR32Y/P7oZZbOx13/Zr//ZHx27bAPnkFoyewYlbAhD3TvBobr95gaUAtr1EdNx1lgI4OcTTuR3z6+FZMEDRcu9ZCuDgGCdyGxMa4EgBRMvcjrkM7NgBZw5c0TwAUWUhZwRXA2xaya65Xa3jO2qYZ8bu2AD5w38tG5V8aZpoGN6Tz0bOfa9bceyWAciTO0jWyO1Tc5cLwJmF/JfPnXVyu3/slgHIg1n79O2O5fZv+1cHV7sC2HYqmUdHysNzX3sVkMcjUK5Gc+dMs28E5bGtm0V3gloBOP9vgZv+4sYn3RUaYFMCol5uN77g6lUApc8pWs69Zn7snS9Z9Q8G0S0AUTVUUTG3A54R1KSvo/diLAv5fKzynZeN6xogC75u93+AtBTA47OlAFSv6qY/vp3DAjD8iv2ZdFYJwKynMhTK1rInPfzaxW81LnvSgFP9KxrATaCLA3DxHpbFX31ZyNm5XRZyXG5bNkAWfP0rcrsUwOgC6NIAzgBcBiqAWwPgLrAGuGBP6jr2sifdfiJ6QQM4Bbw4AK4B3129ZSFn53ZZyA/GyFty27IBFMDFAXAG8PbyLQv5xULGPRl0K3h2AbwcgCZPhs+LD1zLnjS6AN4NwMU/DVFh7LyhASreTbvqrxdr/J4XT4Swz4FrTS+AGJ7bNbwAYkxuWzZAVljHrJfbjb9wviYXwFO/FJ8Vli4vaICsEMFyBbA3tmtsAUS0zG1c/bj4YwsZH2/+Whd0+1Nb+S7IE2sfPw4RL0XmsR8Nqvz7qFngmPHF34EqjP15AAofAkosZKPC/K6FVoeP02Ehi540NG6AK/4pYP3cLgVwXwHkDQ1QcSGb/uF4WwCmfX8u/+4vgLINcMUlQIfcLgXwXAF0+BGkpQDuuJx7/hwgpu//cWVuO3wxJOz/z8297vgYBwaIO3O7Kn+c194578ltywbIgu8fl+Z2lS+APvnLjnOv8hsgSqxjgwL4Ln9LAezaj98tgPzy7ZcC+GQzxrWxXQpgx370dm6/H7v6jaBoso5dY1swAFlwHWvfBf5pxVa93fCtdx64+1dsgCy4joWvAfPX9VoKYMs6Zse9/8Mlvv7LILlhAfKFFdsSutJXAdFkL3qlADJPrXFcXAC5KYaH586jO9mtAch9S3T0GQJ726ZWAE49kjP3rlDJuetdaL/1zeqZY9c7CRz7s0wCUPxienQBnAuAAtAAlxaAAAxfyBQABSAACkAAFIAAKAABUAACMEkKwL170oh7V8ueNLoAjgTAXWAN4BRwcABcA2oABTA4AApAAyiAwQFQABpAAQwOgALQADMWUgCuEmNyu15fSIY3gFPAiwPgFFADKIDBAVAAGkABCIACmBqAUAAaQAHMDUCMWkgBuMWw3K43F5LhDeAU8OIAuAmkARTA4AAoAA2gAARAAUwNgLvAGkABDA6Au8AaoKOJuV0vLSTDG8Ap4MUBcBNIAyiAwQFQABpAAQwOgALQAApAABTA1AC4C6wBOhqb23V+IRneAE4BLw6Aa0ANoAAGB0ABaAAFMDgACkADKAABUABTA+AusAboKATAQs4trjV+IYcfuJYCcA6gAATAQk69dFkKQANYyLkFcLIBFIDLQAVwawDsSRrAEWBwAJwCagAFMDgACkADKIDBAVAAGkABCIACmBoAzwXWAApgcADsSRrg0iNACoACEADXgAIwdCFTACykALgGFIAfl0kBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPBv/gN+IH8U6YveYgAAAABJRU5ErkJggg==&amp;amp;labelColor=white" alt="AI Studio" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PaddleOCR-VL - Multilingual Document Parsing via a 0.9B VLM&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;The SOTA and resource-efficient model tailored for document parsing&lt;/strong&gt;, that supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-OCRv5 — Universal Scene Text Recognition&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;Single model supports five text types&lt;/strong&gt; (Simplified Chinese, Traditional Chinese, English, Japanese, and Pinyin) with &lt;strong&gt;13% accuracy improvement&lt;/strong&gt;. Solves multilingual mixed document recognition challenges.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-StructureV3 — Complex Document Parsing&lt;/strong&gt;&lt;br /&gt; Intelligently converts complex PDFs and document images into &lt;strong&gt;Markdown and JSON files that preserve original structure&lt;/strong&gt;. &lt;strong&gt;Outperforms&lt;/strong&gt; numerous commercial solutions in public benchmarks. &lt;strong&gt;Perfectly maintains document layout and hierarchical structure&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-ChatOCRv4 — Intelligent Information Extraction&lt;/strong&gt;&lt;br /&gt; Natively integrates ERNIE 4.5 to &lt;strong&gt;precisely extract key information&lt;/strong&gt; from massive documents, with 15% accuracy improvement over previous generation. Makes documents "&lt;strong&gt;understand&lt;/strong&gt;" your questions and provide accurate answers.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In addition to providing an outstanding model library, PaddleOCR 3.0 also offers user-friendly tools covering model training, inference, and service deployment, so developers can rapidly bring AI applications to production.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/Arch.jpg" alt="PaddleOCR Architecture" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Special Note&lt;/strong&gt;: PaddleOCR 3.x introduces several significant interface changes. &lt;strong&gt;Old code written based on PaddleOCR 2.x is likely incompatible with PaddleOCR 3.x&lt;/strong&gt;. Please ensure that the documentation you are reading matches the version of PaddleOCR you are using. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/upgrade_notes.html"&gt;This document&lt;/a&gt; explains the reasons for the upgrade and the major changes from PaddleOCR 2.x to 3.x.&lt;/p&gt; 
&lt;h2&gt;📣 Recent updates&lt;/h2&gt; 
&lt;h3&gt;🔥🔥 2025.10.16: PaddleOCR 3.3.0 released, includes:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Released PaddleOCR-VL:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Model Introduction&lt;/strong&gt;:&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;PaddleOCR-VL&lt;/strong&gt; is a SOTA and resource-efficient model tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a compact yet powerful vision-language model (VLM) that integrates a NaViT-style dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to enable accurate element recognition. &lt;strong&gt;This innovative model efficiently supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption&lt;/strong&gt;. Through comprehensive evaluations on widely used public benchmarks and in-house benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document parsing and element-level recognition. It significantly outperforms existing solutions, exhibits strong competitiveness against top-tier VLMs, and delivers fast inference speeds. These strengths make it highly suitable for practical deployment in real-world scenarios. The model has been released on &lt;a href="https://huggingface.co/PaddlePaddle/PaddleOCR-VL"&gt;HuggingFace&lt;/a&gt;. Everyone is welcome to download and use it! More introduction infomation can be found in &lt;a href="https://www.paddleocr.ai/latest/version3.x/algorithm/PaddleOCR-VL/PaddleOCR-VL.html"&gt;PaddleOCR-VL&lt;/a&gt;.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Core Features&lt;/strong&gt;:&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Compact yet Powerful VLM Architecture&lt;/strong&gt;: We present a novel vision-language model that is specifically designed for resource-efficient inference, achieving outstanding performance in element recognition. By integrating a NaViT-style dynamic high-resolution visual encoder with the lightweight ERNIE-4.5-0.3B language model, we significantly enhance the model’s recognition capabilities and decoding efficiency. This integration maintains high accuracy while reducing computational demands, making it well-suited for efficient and practical document processing applications.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;SOTA Performance on Document Parsing&lt;/strong&gt;: PaddleOCR-VL achieves state-of-the-art performance in both page-level document parsing and element-level recognition. It significantly outperforms existing pipeline-based solutions and exhibiting strong competitiveness against leading vision-language models (VLMs) in document parsing. Moreover, it excels in recognizing complex document elements, such as text, tables, formulas, and charts, making it suitable for a wide range of challenging content types, including handwritten text and historical documents. This makes it highly versatile and suitable for a wide range of document types and scenarios.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Multilingual Support&lt;/strong&gt;: PaddleOCR-VL Supports 109 languages, covering major global languages, including but not limited to Chinese, English, Japanese, Latin, and Korean, as well as languages with different scripts and structures, such as Russian (Cyrillic script), Arabic, Hindi (Devanagari script), and Thai. This broad language coverage substantially enhances the applicability of our system to multilingual and globalized document processing scenarios.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Released PP-OCRv5 Multilingual Recognition Model:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Improved the accuracy and coverage of Latin script recognition; added support for Cyrillic, Arabic, Devanagari, Telugu, Tamil, and other language systems, covering recognition of 109 languages. The model has only 2M parameters, and the accuracy of some models has increased by over 40% compared to the previous generation.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.08.21: Release of PaddleOCR 3.2.0&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Significant Model Additions:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Introduced training, inference, and deployment for PP-OCRv5 recognition models in English, Thai, and Greek. &lt;strong&gt;The PP-OCRv5 English model delivers an 11% improvement in English scenarios compared to the main PP-OCRv5 model, with the Thai and Greek recognition models achieving accuracies of 82.68% and 89.28%, respectively.&lt;/strong&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deployment Capability Upgrades:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Full support for PaddlePaddle framework versions 3.1.0 and 3.1.1.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Comprehensive upgrade of the PP-OCRv5 C++ local deployment solution, now supporting both Linux and Windows, with feature parity and identical accuracy to the Python implementation.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;High-performance inference now supports CUDA 12, and inference can be performed using either the Paddle Inference or ONNX Runtime backends.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;The high-stability service-oriented deployment solution is now fully open-sourced, allowing users to customize Docker images and SDKs as required.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;The high-stability service-oriented deployment solution also supports invocation via manually constructed HTTP requests, enabling client-side code development in any programming language.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Benchmark Support:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;All production lines now support fine-grained benchmarking, enabling measurement of end-to-end inference time as well as per-layer and per-module latency data to assist with performance analysis. &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/version3.x/pipeline_usage/instructions/benchmark.en.md"&gt;Here's&lt;/a&gt; how to set up and use the benchmark feature.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Documentation has been updated to include key metrics for commonly used configurations on mainstream hardware, such as inference latency and memory usage, providing deployment references for users.&lt;/strong&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Resolved the issue of failed log saving during model training.&lt;/li&gt; 
    &lt;li&gt;Upgraded the data augmentation component for formula models for compatibility with newer versions of the albumentations dependency, and fixed deadlock warnings when using the tokenizers package in multi-process scenarios.&lt;/li&gt; 
    &lt;li&gt;Fixed inconsistencies in switch behaviors (e.g., &lt;code&gt;use_chart_parsing&lt;/code&gt;) in the PP-StructureV3 configuration files compared to other pipelines.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Other Enhancements:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Separated core and optional dependencies. Only minimal core dependencies are required for basic text recognition; additional dependencies for document parsing and information extraction can be installed as needed.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Enabled support for NVIDIA RTX 50 series graphics cards on Windows; users can refer to the &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/version3.x/installation.en.md"&gt;installation guide&lt;/a&gt; for the corresponding PaddlePaddle framework versions.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;PP-OCR series models now support returning single-character coordinates.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Added AIStudio, ModelScope, and other model download sources, allowing users to specify the source for model downloads.&lt;/li&gt; 
    &lt;li&gt;Added support for chart-to-table conversion via the PP-Chart2Table module.&lt;/li&gt; 
    &lt;li&gt;Optimized documentation descriptions to improve usability.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.08.15: PaddleOCR 3.1.1 Released&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Added the missing methods &lt;code&gt;save_vector&lt;/code&gt;, &lt;code&gt;save_visual_info_list&lt;/code&gt;, &lt;code&gt;load_vector&lt;/code&gt;, and &lt;code&gt;load_visual_info_list&lt;/code&gt; in the &lt;code&gt;PP-ChatOCRv4&lt;/code&gt; class.&lt;/li&gt; 
    &lt;li&gt;Added the missing parameters &lt;code&gt;glossary&lt;/code&gt; and &lt;code&gt;llm_request_interval&lt;/code&gt; to the &lt;code&gt;translate&lt;/code&gt; method in the &lt;code&gt;PPDocTranslation&lt;/code&gt; class.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Improvements:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Added a demo to the MCP documentation.&lt;/li&gt; 
    &lt;li&gt;Added information about the PaddlePaddle and PaddleOCR version used for performance metrics testing in the documentation.&lt;/li&gt; 
    &lt;li&gt;Fixed errors and omissions in the production line document translation.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Others:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Changed the MCP server dependency to use the pure Python library &lt;code&gt;puremagic&lt;/code&gt; instead of &lt;code&gt;python-magic&lt;/code&gt; to reduce installation issues.&lt;/li&gt; 
    &lt;li&gt;Retested PP-OCRv5 performance metrics with PaddleOCR version 3.1.0 and updated the documentation.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.29: PaddleOCR 3.1.0 Released&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Key Models and Pipelines:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Added PP-OCRv5 Multilingual Text Recognition Model&lt;/strong&gt;, which supports the training and inference process for text recognition models in 37 languages, including French, Spanish, Portuguese, Russian, Korean, etc. &lt;strong&gt;Average accuracy improved by over 30%.&lt;/strong&gt; &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/algorithm/PP-OCRv5/PP-OCRv5_multi_languages.html"&gt;Details&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Upgraded the &lt;strong&gt;PP-Chart2Table model&lt;/strong&gt; in PP-StructureV3, further enhancing the capability of converting charts to tables. On internal custom evaluation sets, the metric (RMS-F1) &lt;strong&gt;increased by 9.36 percentage points (71.24% -&amp;gt; 80.60%).&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Newly launched &lt;strong&gt;document translation pipeline, PP-DocTranslation, based on PP-StructureV3 and ERNIE 4.5&lt;/strong&gt;, which supports the translation of Markdown format documents, various complex-layout PDF documents, and document images, with the results saved as Markdown format documents. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/pipeline_usage/PP-DocTranslation.html"&gt;Details&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;New MCP server:&lt;/strong&gt; &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html"&gt;Details&lt;/a&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Supports both OCR and PP-StructureV3 pipelines.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Supports three working modes: local Python library, AIStudio Community Cloud Service, and self-hosted service.&lt;/li&gt; 
    &lt;li&gt;Supports invoking local services via stdio and remote services via Streamable HTTP.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Optimization:&lt;/strong&gt; Improved the descriptions in some user guides for a smoother reading experience.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.26: PaddleOCR 3.0.3 Released&lt;/strong&gt;&lt;/summary&gt; - Bug Fix: Resolved the issue where the `enable_mkldnn` parameter was not effective, restoring the default behavior of using MKL-DNN for CPU inference. 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.19: PaddleOCR 3.0.2 Released&lt;/strong&gt;&lt;/summary&gt; - **New Features:** 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;The default download source has been changed from &lt;code&gt;BOS&lt;/code&gt; to &lt;code&gt;HuggingFace&lt;/code&gt;. Users can also change the environment variable &lt;code&gt;PADDLE_PDX_MODEL_SOURCE&lt;/code&gt; to &lt;code&gt;BOS&lt;/code&gt; to set the model download source back to Baidu Object Storage (BOS).&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Added service invocation examples for six languages—C++, Java, Go, C#, Node.js, and PHP—for pipelines like PP-OCRv5, PP-StructureV3, and PP-ChatOCRv4.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Improved the layout partition sorting algorithm in the PP-StructureV3 pipeline, enhancing the sorting logic for complex vertical layouts to deliver better results.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Enhanced model selection logic: when a language is specified but a model version is not, the system will automatically select the latest model version supporting that language.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Set a default upper limit for MKL-DNN cache size to prevent unlimited growth, while also allowing users to configure cache capacity.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Updated default configurations for high-performance inference to support Paddle MKL-DNN acceleration and optimized the logic for automatic configuration selection for smarter choices.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Adjusted the logic for obtaining the default device to consider the actual support for computing devices by the installed Paddle framework, making program behavior more intuitive.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Added Android example for PP-OCRv5. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/on_device_deployment.html"&gt;Details&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Fixed an issue with some CLI parameters in PP-StructureV3 not taking effect.&lt;/li&gt; 
    &lt;li&gt;Resolved an issue where &lt;code&gt;export_paddlex_config_to_yaml&lt;/code&gt; would not function correctly in certain cases.&lt;/li&gt; 
    &lt;li&gt;Corrected the discrepancy between the actual behavior of &lt;code&gt;save_path&lt;/code&gt; and its documentation description.&lt;/li&gt; 
    &lt;li&gt;Fixed potential multithreading errors when using MKL-DNN in basic service deployment.&lt;/li&gt; 
    &lt;li&gt;Corrected channel order errors in image preprocessing for the Latex-OCR model.&lt;/li&gt; 
    &lt;li&gt;Fixed channel order errors in saving visualized images within the text recognition module.&lt;/li&gt; 
    &lt;li&gt;Resolved channel order errors in visualized table results within PP-StructureV3 pipeline.&lt;/li&gt; 
    &lt;li&gt;Fixed an overflow issue in the calculation of &lt;code&gt;overlap_ratio&lt;/code&gt; under extremely special circumstances in the PP-StructureV3 pipeline.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Improvements:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Updated the description of the &lt;code&gt;enable_mkldnn&lt;/code&gt; parameter in the documentation to accurately reflect the program's actual behavior.&lt;/li&gt; 
    &lt;li&gt;Fixed errors in the documentation regarding the &lt;code&gt;lang&lt;/code&gt; and &lt;code&gt;ocr_version&lt;/code&gt; parameters.&lt;/li&gt; 
    &lt;li&gt;Added instructions for exporting pipeline configuration files via CLI.&lt;/li&gt; 
    &lt;li&gt;Fixed missing columns in the performance data table for PP-OCRv5.&lt;/li&gt; 
    &lt;li&gt;Refined benchmark metrics for PP-StructureV3 across different configurations.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Others:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Relaxed version restrictions on dependencies like numpy and pandas, restoring support for Python 3.12.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;History Log&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;2025.06.05: &lt;strong&gt;PaddleOCR 3.0.1 Released&lt;/strong&gt;, includes:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Optimisation of certain models and model configurations:&lt;/strong&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Updated the default model configuration for PP-OCRv5, changing both detection and recognition from mobile to server models. To improve default performance in most scenarios, the parameter &lt;code&gt;limit_side_len&lt;/code&gt; in the configuration has been changed from 736 to 64.&lt;/li&gt; 
    &lt;li&gt;Added a new text line orientation classification model &lt;code&gt;PP-LCNet_x1_0_textline_ori&lt;/code&gt; with an accuracy of 99.42%. The default text line orientation classifier for OCR, PP-StructureV3, and PP-ChatOCRv4 pipelines has been updated to this model.&lt;/li&gt; 
    &lt;li&gt;Optimized the text line orientation classification model &lt;code&gt;PP-LCNet_x0_25_textline_ori&lt;/code&gt;, improving accuracy by 3.3 percentage points to a current accuracy of 98.85%.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Optimizations and fixes for some issues in version 3.0.0, &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/update.html"&gt;details&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;🔥🔥2025.05.20: Official Release of &lt;strong&gt;PaddleOCR v3.0&lt;/strong&gt;, including:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-OCRv5&lt;/strong&gt;: High-Accuracy Text Recognition Model for All Scenarios - Instant Text from Images/PDFs.&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;🌐 Single-model support for &lt;strong&gt;five&lt;/strong&gt; text types - Seamlessly process &lt;strong&gt;Simplified Chinese, Traditional Chinese, Simplified Chinese Pinyin, English&lt;/strong&gt; and &lt;strong&gt;Japanese&lt;/strong&gt; within a single model.&lt;/li&gt; 
    &lt;li&gt;✍️ Improved &lt;strong&gt;handwriting recognition&lt;/strong&gt;: Significantly better at complex cursive scripts and non-standard handwriting.&lt;/li&gt; 
    &lt;li&gt;🎯 &lt;strong&gt;13-point accuracy gain&lt;/strong&gt; over PP-OCRv4, achieving state-of-the-art performance across a variety of real-world scenarios.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-StructureV3&lt;/strong&gt;: General-Purpose Document Parsing – Unleash SOTA Images/PDFs Parsing for Real-World Scenarios!&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;🧮 &lt;strong&gt;High-Accuracy multi-scene PDF parsing&lt;/strong&gt;, leading both open- and closed-source solutions on the OmniDocBench benchmark.&lt;/li&gt; 
    &lt;li&gt;🧠 Specialized capabilities include &lt;strong&gt;seal recognition&lt;/strong&gt;, &lt;strong&gt;chart-to-table conversion&lt;/strong&gt;, &lt;strong&gt;table recognition with nested formulas/images&lt;/strong&gt;, &lt;strong&gt;vertical text document parsing&lt;/strong&gt;, and &lt;strong&gt;complex table structure analysis&lt;/strong&gt;.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-ChatOCRv4&lt;/strong&gt;: Intelligent Document Understanding – Extract Key Information, not just text from Images/PDFs.&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;🔥 &lt;strong&gt;15-point accuracy gain&lt;/strong&gt; in key-information extraction on PDF/PNG/JPG files over the previous generation.&lt;/li&gt; 
    &lt;li&gt;💻 Native support for &lt;strong&gt;ERNIE 4.5&lt;/strong&gt;, with compatibility for large-model deployments via PaddleNLP, Ollama, vLLM, and more.&lt;/li&gt; 
    &lt;li&gt;🤝 Integrated &lt;a href="https://github.com/PaddlePaddle/PaddleMIX/tree/develop/paddlemix/examples/ppdocbee2"&gt;PP-DocBee2&lt;/a&gt;, enabling extraction and understanding of printed text, handwriting, seals, tables, charts, and other common elements in complex documents.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/update.html"&gt;History Log&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;⚡ Quick Start&lt;/h2&gt; 
&lt;h3&gt;1. Run online demo&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aistudio.baidu.com/community/app/91660/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_OCRv5-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518494/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_StructureV3-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518493/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_ChatOCRv4-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;2. Installation&lt;/h3&gt; 
&lt;p&gt;Install PaddlePaddle refer to &lt;a href="https://www.paddlepaddle.org.cn/en/install/quick?docurl=/documentation/docs/en/develop/install/pip/linux-pip_en.html"&gt;Installation Guide&lt;/a&gt;, after then, install the PaddleOCR toolkit.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# If you only want to use the basic text recognition feature (returns text position coordinates and content), including the PP-OCR series
python -m pip install paddleocr
# If you want to use all features such as document parsing, document understanding, document translation, key information extraction, etc.
# python -m pip install "paddleocr[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Starting from version 3.2.0, in addition to the &lt;code&gt;all&lt;/code&gt; dependency group demonstrated above, PaddleOCR also supports installing partial optional features by specifying other dependency groups. All dependency groups provided by PaddleOCR are as follows:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Dependency Group Name&lt;/th&gt; 
   &lt;th&gt;Corresponding Functionality&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;doc-parser&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Document parsing: can be used to extract layout elements such as tables, formulas, stamps, images, etc. from documents; includes models like PP-StructureV3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ie&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Information extraction: can be used to extract key information from documents, such as names, dates, addresses, amounts, etc.; includes models like PP-ChatOCRv4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;trans&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Document translation: can be used to translate documents from one language to another; includes models like PP-DocTranslation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;all&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Complete functionality&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;3. Run inference by CLI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run PP-OCRv5 inference
paddleocr ocr -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png --use_doc_orientation_classify False --use_doc_unwarping False --use_textline_orientation False  

# Run PP-StructureV3 inference
paddleocr pp_structurev3 -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png --use_doc_orientation_classify False --use_doc_unwarping False

# Get the Qianfan API Key at first, and then run PP-ChatOCRv4 inference
paddleocr pp_chatocrv4_doc -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png -k 驾驶室准乘人数 --qianfan_api_key your_api_key --use_doc_orientation_classify False --use_doc_unwarping False 

# Run PaddleOCR-VL inference
paddleocr doc_parser -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/paddleocr_vl_demo.png

# Get more information about "paddleocr ocr"
paddleocr ocr --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4. Run inference by API&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;4.1 PP-OCRv5 Example&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Initialize PaddleOCR instance
from paddleocr import PaddleOCR
ocr = PaddleOCR(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False,
    use_textline_orientation=False)

# Run OCR inference on a sample image 
result = ocr.predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png")

# Visualize the results and save the JSON results
for res in result:
    res.print()
    res.save_to_img("output")
    res.save_to_json("output")
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;4.2 PP-StructureV3 Example&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from pathlib import Path
from paddleocr import PPStructureV3

pipeline = PPStructureV3(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False
)

# For Image
output = pipeline.predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png",
)

# Visualize the results and save the JSON results
for res in output:
    res.print() 
    res.save_to_json(save_path="output") 
    res.save_to_markdown(save_path="output")           
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;4.3 PP-ChatOCRv4 Example&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from paddleocr import PPChatOCRv4Doc

chat_bot_config = {
    "module_name": "chat_bot",
    "model_name": "ernie-3.5-8k",
    "base_url": "https://qianfan.baidubce.com/v2",
    "api_type": "openai",
    "api_key": "api_key",  # your api_key
}

retriever_config = {
    "module_name": "retriever",
    "model_name": "embedding-v1",
    "base_url": "https://qianfan.baidubce.com/v2",
    "api_type": "qianfan",
    "api_key": "api_key",  # your api_key
}

pipeline = PPChatOCRv4Doc(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False
)

visual_predict_res = pipeline.visual_predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png",
    use_common_ocr=True,
    use_seal_recognition=True,
    use_table_recognition=True,
)

mllm_predict_info = None
use_mllm = False
# If a multimodal large model is used, the local mllm service needs to be started. You can refer to the documentation: https://github.com/PaddlePaddle/PaddleX/blob/release/3.0/docs/pipeline_usage/tutorials/vlm_pipelines/doc_understanding.en.md performs deployment and updates the mllm_chat_bot_config configuration.
if use_mllm:
    mllm_chat_bot_config = {
        "module_name": "chat_bot",
        "model_name": "PP-DocBee",
        "base_url": "http://127.0.0.1:8080/",  # your local mllm service url
        "api_type": "openai",
        "api_key": "api_key",  # your api_key
    }

    mllm_predict_res = pipeline.mllm_pred(
        input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png",
        key_list=["驾驶室准乘人数"],
        mllm_chat_bot_config=mllm_chat_bot_config,
    )
    mllm_predict_info = mllm_predict_res["mllm_res"]

visual_info_list = []
for res in visual_predict_res:
    visual_info_list.append(res["visual_info"])
    layout_parsing_result = res["layout_parsing_result"]

vector_info = pipeline.build_vector(
    visual_info_list, flag_save_bytes_vector=True, retriever_config=retriever_config
)
chat_result = pipeline.chat(
    key_list=["驾驶室准乘人数"],
    visual_info=visual_info_list,
    vector_info=vector_info,
    mllm_predict_info=mllm_predict_info,
    chat_bot_config=chat_bot_config,
    retriever_config=retriever_config,
)
print(chat_result)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;4.4 PaddleOCR-VL Example&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from paddleocr import PaddleOCRVL

pipeline = PaddleOCRVL()
output = pipeline.predict("https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/paddleocr_vl_demo.png")
for res in output:
    res.print()
    res.save_to_json(save_path="output")
    res.save_to_markdown(save_path="output")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;5. Chinese Heterogeneous AI Accelerators&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/other_devices_support/paddlepaddle_install_NPU.html"&gt;Huawei Ascend&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/other_devices_support/paddlepaddle_install_XPU.html"&gt;KUNLUNXIN&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🧩 More Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Convert models to ONNX format: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/obtaining_onnx_models.html"&gt;Obtaining ONNX Models&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Accelerate inference using engines like OpenVINO, ONNX Runtime, TensorRT, or perform inference using ONNX format models: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/high_performance_inference.html"&gt;High-Performance Inference&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Accelerate inference using multi-GPU and multi-process: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/pipeline_usage/instructions/parallel_inference.html"&gt;Parallel Inference for Pipelines&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Integrate PaddleOCR into applications written in C++, C#, Java, etc.: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/serving.html"&gt;Serving&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;⛰️ Advanced Tutorials&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/OCR.html"&gt;PP-OCRv5 Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PP-StructureV3.html"&gt;PP-StructureV3 Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PP-ChatOCRv4.html"&gt;PP-ChatOCRv4 Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PaddleOCR-VL.html"&gt;PaddleOCR-VL Tutorial&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🔄 Quick Overview of Execution Results&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/demo.gif" alt="PP-OCRv5 Demo" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/blue_v3.gif" alt="PP-StructureV3 Demo" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;✨ Stay Tuned&lt;/h2&gt; 
&lt;p&gt;⭐ &lt;strong&gt;Star this repository to keep up with exciting updates and new releases, including powerful OCR and document parsing capabilities!&lt;/strong&gt; ⭐&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="1200" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/star_paddleocr.en.gif" alt="Star-Project" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;👩‍👩‍👧‍👦 Community&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;PaddlePaddle WeChat official account&lt;/th&gt; 
    &lt;th align="center"&gt;Join the tech discussion group&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/refs/heads/main/images/paddleocr/README/qrcode_for_paddlepaddle_official_account.jpg" width="150" /&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/refs/heads/main/images/paddleocr/README/qr_code_for_the_questionnaire.jpg" width="150" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;😃 Awesome Projects Leveraging PaddleOCR&lt;/h2&gt; 
&lt;p&gt;PaddleOCR wouldn't be where it is today without its incredible community! 💗 A massive thank you to all our longtime partners, new collaborators, and everyone who's poured their passion into PaddleOCR — whether we've named you or not. Your support fuels our fire!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Project Name&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/infiniflow/ragflow"&gt;RAGFlow&lt;/a&gt; &lt;a href="https://github.com/infiniflow/ragflow"&gt;&lt;img src="https://img.shields.io/github/stars/infiniflow/ragflow" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;RAG engine based on deep document understanding.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/opendatalab/MinerU"&gt;MinerU&lt;/a&gt; &lt;a href="https://github.com/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/github/stars/opendatalab/MinerU" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Multi-type Document to Markdown Conversion Tool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hiroi-sora/Umi-OCR"&gt;Umi-OCR&lt;/a&gt; &lt;a href="https://github.com/hiroi-sora/Umi-OCR"&gt;&lt;img src="https://img.shields.io/github/stars/hiroi-sora/Umi-OCR" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Free, Open-source, Batch Offline OCR Software.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/OmniParser"&gt;OmniParser&lt;/a&gt;&lt;a href="https://github.com/microsoft/OmniParser"&gt;&lt;img src="https://img.shields.io/github/stars/microsoft/OmniParser" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;OmniParser: Screen Parsing tool for Pure Vision Based GUI Agent.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/netease-youdao/QAnything"&gt;QAnything&lt;/a&gt;&lt;a href="https://github.com/netease-youdao/QAnything"&gt;&lt;img src="https://img.shields.io/github/stars/netease-youdao/QAnything" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Question and Answer based on Anything.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;PDF-Extract-Kit&lt;/a&gt; &lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;&lt;img src="https://img.shields.io/github/stars/opendatalab/PDF-Extract-Kit" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;A powerful open-source toolkit designed to efficiently extract high-quality content from complex and diverse PDF documents.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/PantsuDango/Dango-Translator"&gt;Dango-Translator&lt;/a&gt;&lt;a href="https://github.com/PantsuDango/Dango-Translator"&gt;&lt;img src="https://img.shields.io/github/stars/PantsuDango/Dango-Translator" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Recognize text on the screen, translate it and show the translation results in real time.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/awesome_projects.md"&gt;Learn more projects&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/awesome_projects.md"&gt;More projects based on PaddleOCR&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;👩‍👩‍👧‍👦 Contributors&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/PaddlePaddle/PaddleOCR/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=PaddlePaddle/PaddleOCR&amp;amp;max=400&amp;amp;columns=20" width="800" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;🌟 Star&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="800" src="https://api.star-history.com/svg?repos=PaddlePaddle/PaddleOCR&amp;amp;type=Date" alt="Star-history" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;📄 License&lt;/h2&gt; 
&lt;p&gt;This project is released under the &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/LICENSE"&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;🎓 Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@misc{cui2025paddleocr30technicalreport,
      title={PaddleOCR 3.0 Technical Report}, 
      author={Cheng Cui and Ting Sun and Manhui Lin and Tingquan Gao and Yubo Zhang and Jiaxuan Liu and Xueqing Wang and Zelun Zhang and Changda Zhou and Hongen Liu and Yue Zhang and Wenyu Lv and Kui Huang and Yichao Zhang and Jing Zhang and Jun Zhang and Yi Liu and Dianhai Yu and Yanjun Ma},
      year={2025},
      eprint={2507.05595},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2507.05595}, 
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>shiyu-coder/Kronos</title>
      <link>https://github.com/shiyu-coder/Kronos</link>
      <description>&lt;p&gt;Kronos: A Foundation Model for the Language of Financial Markets&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h2&gt;&lt;b&gt;Kronos: A Foundation Model for the Language of Financial Markets &lt;/b&gt;&lt;/h2&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt;  
 &lt;a href="https://huggingface.co/NeoQuasar"&gt; &lt;img src="https://img.shields.io/badge/🤗-Hugging_Face-yellow" alt="Hugging Face" /&gt; &lt;/a&gt; 
 &lt;a href="https://shiyu-coder.github.io/Kronos-demo/"&gt; &lt;img src="https://img.shields.io/badge/🚀-Live_Demo-brightgreen" alt="Live Demo" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/shiyu-coder/Kronos/graphs/commit-activity"&gt; &lt;img src="https://img.shields.io/github/last-commit/shiyu-coder/Kronos?color=blue" alt="Last Commit" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/shiyu-coder/Kronos/stargazers"&gt; &lt;img src="https://img.shields.io/github/stars/shiyu-coder/Kronos?color=lightblue" alt="GitHub Stars" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/shiyu-coder/Kronos/network/members"&gt; &lt;img src="https://img.shields.io/github/forks/shiyu-coder/Kronos?color=yellow" alt="GitHub Forks" /&gt; &lt;/a&gt; 
 &lt;a href="https://raw.githubusercontent.com/shiyu-coder/Kronos/master/LICENSE"&gt; &lt;img src="https://img.shields.io/github/license/shiyu-coder/Kronos?color=green" alt="License" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; 
 &lt;a href="https://zdoc.app/de/shiyu-coder/Kronos"&gt;Deutsch&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/es/shiyu-coder/Kronos"&gt;Español&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/fr/shiyu-coder/Kronos"&gt;Français&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/ja/shiyu-coder/Kronos"&gt;日本語&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/ko/shiyu-coder/Kronos"&gt;한국어&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/pt/shiyu-coder/Kronos"&gt;Português&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/ru/shiyu-coder/Kronos"&gt;Русский&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/zh/shiyu-coder/Kronos"&gt;中文&lt;/a&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/shiyu-coder/Kronos/master/figures/logo.png" width="100" /&gt; &lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Kronos is the &lt;strong&gt;first open-source foundation model&lt;/strong&gt; for financial candlesticks (K-lines), trained on data from over &lt;strong&gt;45 global exchanges&lt;/strong&gt;.&lt;/p&gt; 
&lt;/blockquote&gt;  
&lt;h2&gt;📰 News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;🚩 &lt;strong&gt;[2025.08.17]&lt;/strong&gt; We have released the scripts for fine-tuning! Check them out to adapt Kronos to your own tasks.&lt;/li&gt; 
 &lt;li&gt;🚩 &lt;strong&gt;[2025.08.02]&lt;/strong&gt; Our paper is now available on &lt;a href="https://arxiv.org/abs/2508.02739"&gt;arXiv&lt;/a&gt;!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;h2&gt;📜 Introduction&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Kronos&lt;/strong&gt; is a family of decoder-only foundation models, pre-trained specifically for the "language" of financial markets—K-line sequences. Unlike general-purpose TSFMs, Kronos is designed to handle the unique, high-noise characteristics of financial data. It leverages a novel two-stage framework:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;A specialized tokenizer first quantizes continuous, multi-dimensional K-line data (OHLCV) into &lt;strong&gt;hierarchical discrete tokens&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;A large, autoregressive Transformer is then pre-trained on these tokens, enabling it to serve as a unified model for diverse quantitative tasks.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/shiyu-coder/Kronos/master/figures/overview.png" alt="" align="center" width="700px" /&gt; &lt;/p&gt; 
&lt;h2&gt;✨ Live Demo&lt;/h2&gt; 
&lt;p&gt;We have set up a live demo to visualize Kronos's forecasting results. The webpage showcases a forecast for the &lt;strong&gt;BTC/USDT&lt;/strong&gt; trading pair over the next 24 hours.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;👉 &lt;a href="https://shiyu-coder.github.io/Kronos-demo/"&gt;Access the Live Demo Here&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;📦 Model Zoo&lt;/h2&gt; 
&lt;p&gt;We release a family of pre-trained models with varying capacities to suit different computational and application needs. All models are readily accessible from the Hugging Face Hub.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Tokenizer&lt;/th&gt; 
   &lt;th&gt;Context length&lt;/th&gt; 
   &lt;th&gt;Params&lt;/th&gt; 
   &lt;th&gt;Open-source&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Kronos-mini&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/NeoQuasar/Kronos-Tokenizer-2k"&gt;Kronos-Tokenizer-2k&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2048&lt;/td&gt; 
   &lt;td&gt;4.1M&lt;/td&gt; 
   &lt;td&gt;✅ &lt;a href="https://huggingface.co/NeoQuasar/Kronos-mini"&gt;NeoQuasar/Kronos-mini&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Kronos-small&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/NeoQuasar/Kronos-Tokenizer-base"&gt;Kronos-Tokenizer-base&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;512&lt;/td&gt; 
   &lt;td&gt;24.7M&lt;/td&gt; 
   &lt;td&gt;✅ &lt;a href="https://huggingface.co/NeoQuasar/Kronos-small"&gt;NeoQuasar/Kronos-small&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Kronos-base&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/NeoQuasar/Kronos-Tokenizer-base"&gt;Kronos-Tokenizer-base&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;512&lt;/td&gt; 
   &lt;td&gt;102.3M&lt;/td&gt; 
   &lt;td&gt;✅ &lt;a href="https://huggingface.co/NeoQuasar/Kronos-base"&gt;NeoQuasar/Kronos-base&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Kronos-large&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/NeoQuasar/Kronos-Tokenizer-base"&gt;Kronos-Tokenizer-base&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;512&lt;/td&gt; 
   &lt;td&gt;499.2M&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;🚀 Getting Started&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install Python 3.10+, and then install the dependencies:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;📈 Making Forecasts&lt;/h3&gt; 
&lt;p&gt;Forecasting with Kronos is straightforward using the &lt;code&gt;KronosPredictor&lt;/code&gt; class. It handles data preprocessing, normalization, prediction, and inverse normalization, allowing you to get from raw data to forecasts in just a few lines of code.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Important Note&lt;/strong&gt;: The &lt;code&gt;max_context&lt;/code&gt; for &lt;code&gt;Kronos-small&lt;/code&gt; and &lt;code&gt;Kronos-base&lt;/code&gt; is &lt;strong&gt;512&lt;/strong&gt;. This is the maximum sequence length the model can process. For optimal performance, it is recommended that your input data length (i.e., &lt;code&gt;lookback&lt;/code&gt;) does not exceed this limit. The &lt;code&gt;KronosPredictor&lt;/code&gt; will automatically handle truncation for longer contexts.&lt;/p&gt; 
&lt;p&gt;Here is a step-by-step guide to making your first forecast.&lt;/p&gt; 
&lt;h4&gt;1. Load the Tokenizer and Model&lt;/h4&gt; 
&lt;p&gt;First, load a pre-trained Kronos model and its corresponding tokenizer from the Hugging Face Hub.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from model import Kronos, KronosTokenizer, KronosPredictor

# Load from Hugging Face Hub
tokenizer = KronosTokenizer.from_pretrained("NeoQuasar/Kronos-Tokenizer-base")
model = Kronos.from_pretrained("NeoQuasar/Kronos-small")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;2. Instantiate the Predictor&lt;/h4&gt; 
&lt;p&gt;Create an instance of &lt;code&gt;KronosPredictor&lt;/code&gt;, passing the model, tokenizer, and desired device.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Initialize the predictor
predictor = KronosPredictor(model, tokenizer, device="cuda:0", max_context=512)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3. Prepare Input Data&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;predict&lt;/code&gt; method requires three main inputs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;df&lt;/code&gt;: A pandas DataFrame containing the historical K-line data. It must include columns &lt;code&gt;['open', 'high', 'low', 'close']&lt;/code&gt;. &lt;code&gt;volume&lt;/code&gt; and &lt;code&gt;amount&lt;/code&gt; are optional.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;x_timestamp&lt;/code&gt;: A pandas Series of timestamps corresponding to the historical data in &lt;code&gt;df&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;y_timestamp&lt;/code&gt;: A pandas Series of timestamps for the future periods you want to predict.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pandas as pd

# Load your data
df = pd.read_csv("./data/XSHG_5min_600977.csv")
df['timestamps'] = pd.to_datetime(df['timestamps'])

# Define context window and prediction length
lookback = 400
pred_len = 120

# Prepare inputs for the predictor
x_df = df.loc[:lookback-1, ['open', 'high', 'low', 'close', 'volume', 'amount']]
x_timestamp = df.loc[:lookback-1, 'timestamps']
y_timestamp = df.loc[lookback:lookback+pred_len-1, 'timestamps']
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;4. Generate Forecasts&lt;/h4&gt; 
&lt;p&gt;Call the &lt;code&gt;predict&lt;/code&gt; method to generate forecasts. You can control the sampling process with parameters like &lt;code&gt;T&lt;/code&gt;, &lt;code&gt;top_p&lt;/code&gt;, and &lt;code&gt;sample_count&lt;/code&gt; for probabilistic forecasting.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Generate predictions
pred_df = predictor.predict(
    df=x_df,
    x_timestamp=x_timestamp,
    y_timestamp=y_timestamp,
    pred_len=pred_len,
    T=1.0,          # Temperature for sampling
    top_p=0.9,      # Nucleus sampling probability
    sample_count=1  # Number of forecast paths to generate and average
)

print("Forecasted Data Head:")
print(pred_df.head())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;predict&lt;/code&gt; method returns a pandas DataFrame containing the forecasted values for &lt;code&gt;open&lt;/code&gt;, &lt;code&gt;high&lt;/code&gt;, &lt;code&gt;low&lt;/code&gt;, &lt;code&gt;close&lt;/code&gt;, &lt;code&gt;volume&lt;/code&gt;, and &lt;code&gt;amount&lt;/code&gt;, indexed by the &lt;code&gt;y_timestamp&lt;/code&gt; you provided.&lt;/p&gt; 
&lt;p&gt;For efficient processing of multiple time series, Kronos provides a &lt;code&gt;predict_batch&lt;/code&gt; method that enables parallel prediction on multiple datasets simultaneously. This is particularly useful when you need to forecast multiple assets or time periods at once.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Prepare multiple datasets for batch prediction
df_list = [df1, df2, df3]  # List of DataFrames
x_timestamp_list = [x_ts1, x_ts2, x_ts3]  # List of historical timestamps
y_timestamp_list = [y_ts1, y_ts2, y_ts3]  # List of future timestamps

# Generate batch predictions
pred_df_list = predictor.predict_batch(
    df_list=df_list,
    x_timestamp_list=x_timestamp_list,
    y_timestamp_list=y_timestamp_list,
    pred_len=pred_len,
    T=1.0,
    top_p=0.9,
    sample_count=1,
    verbose=True
)

# pred_df_list contains prediction results in the same order as input
for i, pred_df in enumerate(pred_df_list):
    print(f"Predictions for series {i}:")
    print(pred_df.head())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Important Requirements for Batch Prediction:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;All series must have the same historical length (lookback window)&lt;/li&gt; 
 &lt;li&gt;All series must have the same prediction length (&lt;code&gt;pred_len&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Each DataFrame must contain the required columns: &lt;code&gt;['open', 'high', 'low', 'close']&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;volume&lt;/code&gt; and &lt;code&gt;amount&lt;/code&gt; columns are optional and will be filled with zeros if missing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The &lt;code&gt;predict_batch&lt;/code&gt; method leverages GPU parallelism for efficient processing and automatically handles normalization and denormalization for each series independently.&lt;/p&gt; 
&lt;h4&gt;5. Example and Visualization&lt;/h4&gt; 
&lt;p&gt;For a complete, runnable script that includes data loading, prediction, and plotting, please see &lt;a href="https://raw.githubusercontent.com/shiyu-coder/Kronos/master/examples/prediction_example.py"&gt;&lt;code&gt;examples/prediction_example.py&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Running this script will generate a plot comparing the ground truth data against the model's forecast, similar to the one shown below:&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/shiyu-coder/Kronos/master/figures/prediction_example.png" alt="Forecast Example" align="center" width="600px" /&gt; &lt;/p&gt; 
&lt;p&gt;Additionally, we provide a script that makes predictions without Volume and Amount data, which can be found in &lt;a href="https://raw.githubusercontent.com/shiyu-coder/Kronos/master/examples/prediction_wo_vol_example.py"&gt;&lt;code&gt;examples/prediction_wo_vol_example.py&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;🔧 Finetuning on Your Own Data (A-Share Market Example)&lt;/h2&gt; 
&lt;p&gt;We provide a complete pipeline for finetuning Kronos on your own datasets. As an example, we demonstrate how to use &lt;a href="https://github.com/microsoft/qlib"&gt;Qlib&lt;/a&gt; to prepare data from the Chinese A-share market and conduct a simple backtest.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; This pipeline is intended as a demonstration to illustrate the finetuning process. It is a simplified example and not a production-ready quantitative trading system. A robust quantitative strategy requires more sophisticated techniques, such as portfolio optimization and risk factor neutralization, to achieve stable alpha.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;The finetuning process is divided into four main steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Configuration&lt;/strong&gt;: Set up paths and hyperparameters.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Preparation&lt;/strong&gt;: Process and split your data using Qlib.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model Finetuning&lt;/strong&gt;: Finetune the Tokenizer and the Predictor models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backtesting&lt;/strong&gt;: Evaluate the finetuned model's performance.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;First, ensure you have all dependencies from &lt;code&gt;requirements.txt&lt;/code&gt; installed.&lt;/li&gt; 
 &lt;li&gt;This pipeline relies on &lt;code&gt;qlib&lt;/code&gt;. Please install it: &lt;pre&gt;&lt;code class="language-shell"&gt;  pip install pyqlib
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;You will need to prepare your Qlib data. Follow the &lt;a href="https://github.com/microsoft/qlib"&gt;official Qlib guide&lt;/a&gt; to download and set up your data locally. The example scripts assume you are using daily frequency data.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Step 1: Configure Your Experiment&lt;/h3&gt; 
&lt;p&gt;All settings for data, training, and model paths are centralized in &lt;code&gt;finetune/config.py&lt;/code&gt;. Before running any scripts, please &lt;strong&gt;modify the following paths&lt;/strong&gt; according to your environment:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;qlib_data_path&lt;/code&gt;: Path to your local Qlib data directory.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;dataset_path&lt;/code&gt;: Directory where the processed train/validation/test pickle files will be saved.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;save_path&lt;/code&gt;: Base directory for saving model checkpoints.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;backtest_result_path&lt;/code&gt;: Directory for saving backtesting results.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;pretrained_tokenizer_path&lt;/code&gt; and &lt;code&gt;pretrained_predictor_path&lt;/code&gt;: Paths to the pre-trained models you want to start from (can be local paths or Hugging Face model names).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can also adjust other parameters like &lt;code&gt;instrument&lt;/code&gt;, &lt;code&gt;train_time_range&lt;/code&gt;, &lt;code&gt;epochs&lt;/code&gt;, and &lt;code&gt;batch_size&lt;/code&gt; to fit your specific task. If you don't use &lt;a href="https://www.comet.com/"&gt;Comet.ml&lt;/a&gt;, set &lt;code&gt;use_comet = False&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Step 2: Prepare the Dataset&lt;/h3&gt; 
&lt;p&gt;Run the data preprocessing script. This script will load raw market data from your Qlib directory, process it, split it into training, validation, and test sets, and save them as pickle files.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python finetune/qlib_data_preprocess.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After running, you will find &lt;code&gt;train_data.pkl&lt;/code&gt;, &lt;code&gt;val_data.pkl&lt;/code&gt;, and &lt;code&gt;test_data.pkl&lt;/code&gt; in the directory specified by &lt;code&gt;dataset_path&lt;/code&gt; in your config.&lt;/p&gt; 
&lt;h3&gt;Step 3: Run the Finetuning&lt;/h3&gt; 
&lt;p&gt;The finetuning process consists of two stages: finetuning the tokenizer and then the predictor. Both training scripts are designed for multi-GPU training using &lt;code&gt;torchrun&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;3.1 Finetune the Tokenizer&lt;/h4&gt; 
&lt;p&gt;This step adjusts the tokenizer to the data distribution of your specific domain.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Replace NUM_GPUS with the number of GPUs you want to use (e.g., 2)
torchrun --standalone --nproc_per_node=NUM_GPUS finetune/train_tokenizer.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The best tokenizer checkpoint will be saved to the path configured in &lt;code&gt;config.py&lt;/code&gt; (derived from &lt;code&gt;save_path&lt;/code&gt; and &lt;code&gt;tokenizer_save_folder_name&lt;/code&gt;).&lt;/p&gt; 
&lt;h4&gt;3.2 Finetune the Predictor&lt;/h4&gt; 
&lt;p&gt;This step finetunes the main Kronos model for the forecasting task.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Replace NUM_GPUS with the number of GPUs you want to use (e.g., 2)
torchrun --standalone --nproc_per_node=NUM_GPUS finetune/train_predictor.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The best predictor checkpoint will be saved to the path configured in &lt;code&gt;config.py&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Step 4: Evaluate with Backtesting&lt;/h3&gt; 
&lt;p&gt;Finally, run the backtesting script to evaluate your finetuned model. This script loads the models, performs inference on the test set, generates prediction signals (e.g., forecasted price change), and runs a simple top-K strategy backtest.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Specify the GPU for inference
python finetune/qlib_test.py --device cuda:0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The script will output a detailed performance analysis in your console and generate a plot showing the cumulative return curves of your strategy against the benchmark, similar to the one below:&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/shiyu-coder/Kronos/master/figures/backtest_result_example.png" alt="Backtest Example" align="center" width="700px" /&gt; &lt;/p&gt; 
&lt;h3&gt;💡 From Demo to Production: Important Considerations&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Raw Signals vs. Pure Alpha&lt;/strong&gt;: The signals generated by the model in this demo are raw predictions. In a real-world quantitative workflow, these signals would typically be fed into a portfolio optimization model. This model would apply constraints to neutralize exposure to common risk factors (e.g., market beta, style factors like size and value), thereby isolating the &lt;strong&gt;"pure alpha"&lt;/strong&gt; and improving the strategy's robustness.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Handling&lt;/strong&gt;: The provided &lt;code&gt;QlibDataset&lt;/code&gt; is an example. For different data sources or formats, you will need to adapt the data loading and preprocessing logic.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Strategy and Backtesting Complexity&lt;/strong&gt;: The simple top-K strategy used here is a basic starting point. Production-level strategies often incorporate more complex logic for portfolio construction, dynamic position sizing, and risk management (e.g., stop-loss/take-profit rules). Furthermore, a high-fidelity backtest should meticulously model transaction costs, slippage, and market impact to provide a more accurate estimate of real-world performance.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;📝 AI-Generated Comments&lt;/strong&gt;: Please note that many of the code comments within the &lt;code&gt;finetune/&lt;/code&gt; directory were generated by an AI assistant (Gemini 2.5 Pro) for explanatory purposes. While they aim to be helpful, they may contain inaccuracies. We recommend treating the code itself as the definitive source of logic.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;📖 Citation&lt;/h2&gt; 
&lt;p&gt;If you use Kronos in your research, we would appreciate a citation to our &lt;a href="https://arxiv.org/abs/2508.02739"&gt;paper&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{shi2025kronos,
      title={Kronos: A Foundation Model for the Language of Financial Markets}, 
      author={Yu Shi and Zongliang Fu and Shuo Chen and Bohan Zhao and Wei Xu and Changshui Zhang and Jian Li},
      year={2025},
      eprint={2508.02739},
      archivePrefix={arXiv},
      primaryClass={q-fin.ST},
      url={https://arxiv.org/abs/2508.02739}, 
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;📜 License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the &lt;a href="https://raw.githubusercontent.com/shiyu-coder/Kronos/master/LICENSE"&gt;MIT License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>resemble-ai/chatterbox</title>
      <link>https://github.com/resemble-ai/chatterbox</link>
      <description>&lt;p&gt;SoTA open-source TTS&lt;/p&gt;&lt;hr&gt;&lt;img width="1200" height="600" alt="Chatterbox-Multilingual" src="https://www.resemble.ai/wp-content/uploads/2025/09/Chatterbox-Multilingual-1.png" /&gt; 
&lt;h1&gt;Chatterbox TTS&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://resemble-ai.github.io/chatterbox_demopage/"&gt;&lt;img src="https://img.shields.io/badge/listen-demo_samples-blue" alt="Alt Text" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/spaces/ResembleAI/Chatterbox"&gt;&lt;img src="https://huggingface.co/datasets/huggingface/badges/resolve/main/open-in-hf-spaces-sm.svg?sanitize=true" alt="Alt Text" /&gt;&lt;/a&gt; &lt;a href="https://podonos.com/resembleai/chatterbox"&gt;&lt;img src="https://static-public.podonos.com/badges/insight-on-pdns-sm-dark.svg?sanitize=true" alt="Alt Text" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/rJq9cRJBJ6"&gt;&lt;img src="https://img.shields.io/discord/1377773249798344776?label=join%20discord&amp;amp;logo=discord&amp;amp;style=flat" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;_Made with ♥️ by &lt;a href="https://resemble.ai" target="_blank"&gt;&lt;img width="100" alt="resemble-logo-horizontal" src="https://github.com/user-attachments/assets/35cf756b-3506-4943-9c72-c05ddfa4e525" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We're excited to introduce &lt;strong&gt;Chatterbox Multilingual&lt;/strong&gt;, &lt;a href="https://resemble.ai"&gt;Resemble AI's&lt;/a&gt; first production-grade open source TTS model supporting &lt;strong&gt;23 languages&lt;/strong&gt; out of the box. Licensed under MIT, Chatterbox has been benchmarked against leading closed-source systems like ElevenLabs, and is consistently preferred in side-by-side evaluations.&lt;/p&gt; 
&lt;p&gt;Whether you're working on memes, videos, games, or AI agents, Chatterbox brings your content to life across languages. It's also the first open source TTS model to support &lt;strong&gt;emotion exaggeration control&lt;/strong&gt; with robust &lt;strong&gt;multilingual zero-shot voice cloning&lt;/strong&gt;. Try the english only version now on our &lt;a href="https://huggingface.co/spaces/ResembleAI/Chatterbox"&gt;English Hugging Face Gradio app.&lt;/a&gt;. Or try the multilingual version on our &lt;a href="https://huggingface.co/spaces/ResembleAI/Chatterbox-Multilingual-TTS"&gt;Multilingual Hugging Face Gradio app.&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you like the model but need to scale or tune it for higher accuracy, check out our competitively priced TTS service (&lt;a href="https://resemble.ai"&gt;link&lt;/a&gt;). It delivers reliable performance with ultra-low latency of sub 200ms—ideal for production use in agents, applications, or interactive media.&lt;/p&gt; 
&lt;h1&gt;Key Details&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;Multilingual, zero-shot TTS supporting 23 languages&lt;/li&gt; 
 &lt;li&gt;SoTA zeroshot English TTS&lt;/li&gt; 
 &lt;li&gt;0.5B Llama backbone&lt;/li&gt; 
 &lt;li&gt;Unique exaggeration/intensity control&lt;/li&gt; 
 &lt;li&gt;Ultra-stable with alignment-informed inference&lt;/li&gt; 
 &lt;li&gt;Trained on 0.5M hours of cleaned data&lt;/li&gt; 
 &lt;li&gt;Watermarked outputs&lt;/li&gt; 
 &lt;li&gt;Easy voice conversion script&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://podonos.com/resembleai/chatterbox"&gt;Outperforms ElevenLabs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Supported Languages&lt;/h1&gt; 
&lt;p&gt;Arabic (ar) • Danish (da) • German (de) • Greek (el) • English (en) • Spanish (es) • Finnish (fi) • French (fr) • Hebrew (he) • Hindi (hi) • Italian (it) • Japanese (ja) • Korean (ko) • Malay (ms) • Dutch (nl) • Norwegian (no) • Polish (pl) • Portuguese (pt) • Russian (ru) • Swedish (sv) • Swahili (sw) • Turkish (tr) • Chinese (zh)&lt;/p&gt; 
&lt;h1&gt;Tips&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;General Use (TTS and Voice Agents):&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Ensure that the reference clip matches the specified language tag. Otherwise, language transfer outputs may inherit the accent of the reference clip’s language. To mitigate this, set &lt;code&gt;cfg_weight&lt;/code&gt; to &lt;code&gt;0&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;The default settings (&lt;code&gt;exaggeration=0.5&lt;/code&gt;, &lt;code&gt;cfg_weight=0.5&lt;/code&gt;) work well for most prompts across all languages.&lt;/li&gt; 
   &lt;li&gt;If the reference speaker has a fast speaking style, lowering &lt;code&gt;cfg_weight&lt;/code&gt; to around &lt;code&gt;0.3&lt;/code&gt; can improve pacing.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Expressive or Dramatic Speech:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Try lower &lt;code&gt;cfg_weight&lt;/code&gt; values (e.g. &lt;code&gt;~0.3&lt;/code&gt;) and increase &lt;code&gt;exaggeration&lt;/code&gt; to around &lt;code&gt;0.7&lt;/code&gt; or higher.&lt;/li&gt; 
   &lt;li&gt;Higher &lt;code&gt;exaggeration&lt;/code&gt; tends to speed up speech; reducing &lt;code&gt;cfg_weight&lt;/code&gt; helps compensate with slower, more deliberate pacing.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install chatterbox-tts
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can install from source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# conda create -yn chatterbox python=3.11
# conda activate chatterbox

git clone https://github.com/resemble-ai/chatterbox.git
cd chatterbox
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We developed and tested Chatterbox on Python 3.11 on Debian 11 OS; the versions of the dependencies are pinned in &lt;code&gt;pyproject.toml&lt;/code&gt; to ensure consistency. You can modify the code or dependencies in this installation mode.&lt;/p&gt; 
&lt;h1&gt;Usage&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import torchaudio as ta
from chatterbox.tts import ChatterboxTTS
from chatterbox.mtl_tts import ChatterboxMultilingualTTS

# English example
model = ChatterboxTTS.from_pretrained(device="cuda")

text = "Ezreal and Jinx teamed up with Ahri, Yasuo, and Teemo to take down the enemy's Nexus in an epic late-game pentakill."
wav = model.generate(text)
ta.save("test-english.wav", wav, model.sr)

# Multilingual examples
multilingual_model = ChatterboxMultilingualTTS.from_pretrained(device=device)

french_text = "Bonjour, comment ça va? Ceci est le modèle de synthèse vocale multilingue Chatterbox, il prend en charge 23 langues."
wav_french = multilingual_model.generate(spanish_text, language_id="fr")
ta.save("test-french.wav", wav_french, model.sr)

chinese_text = "你好，今天天气真不错，希望你有一个愉快的周末。"
wav_chinese = multilingual_model.generate(chinese_text, language_id="zh")
ta.save("test-chinese.wav", wav_chinese, model.sr)

# If you want to synthesize with a different voice, specify the audio prompt
AUDIO_PROMPT_PATH = "YOUR_FILE.wav"
wav = model.generate(text, audio_prompt_path=AUDIO_PROMPT_PATH)
ta.save("test-2.wav", wav, model.sr)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;code&gt;example_tts.py&lt;/code&gt; and &lt;code&gt;example_vc.py&lt;/code&gt; for more examples.&lt;/p&gt; 
&lt;h1&gt;Acknowledgements&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/FunAudioLLM/CosyVoice"&gt;Cosyvoice&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning"&gt;Real-Time-Voice-Cloning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yl4579/HiFTNet"&gt;HiFT-GAN&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/meta-llama/llama3"&gt;Llama 3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/xingchensong/S3Tokenizer"&gt;S3Tokenizer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Built-in PerTh Watermarking for Responsible AI&lt;/h1&gt; 
&lt;p&gt;Every audio file generated by Chatterbox includes &lt;a href="https://github.com/resemble-ai/perth"&gt;Resemble AI's Perth (Perceptual Threshold) Watermarker&lt;/a&gt; - imperceptible neural watermarks that survive MP3 compression, audio editing, and common manipulations while maintaining nearly 100% detection accuracy.&lt;/p&gt; 
&lt;h2&gt;Watermark extraction&lt;/h2&gt; 
&lt;p&gt;You can look for the watermark using the following script.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import perth
import librosa

AUDIO_PATH = "YOUR_FILE.wav"

# Load the watermarked audio
watermarked_audio, sr = librosa.load(AUDIO_PATH, sr=None)

# Initialize watermarker (same as used for embedding)
watermarker = perth.PerthImplicitWatermarker()

# Extract watermark
watermark = watermarker.get_watermark(watermarked_audio, sample_rate=sr)
print(f"Extracted watermark: {watermark}")
# Output: 0.0 (no watermark) or 1.0 (watermarked)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Official Discord&lt;/h1&gt; 
&lt;p&gt;👋 Join us on &lt;a href="https://discord.gg/rJq9cRJBJ6"&gt;Discord&lt;/a&gt; and let's build something awesome together!&lt;/p&gt; 
&lt;h1&gt;Citation&lt;/h1&gt; 
&lt;p&gt;If you find this model useful, please consider citing.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{chatterboxtts2025,
  author       = {{Resemble AI}},
  title        = {{Chatterbox-TTS}},
  year         = {2025},
  howpublished = {\url{https://github.com/resemble-ai/chatterbox}},
  note         = {GitHub repository}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Disclaimer&lt;/h1&gt; 
&lt;p&gt;Don't use this model to do bad things. Prompts are sourced from freely available data on the internet.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>fishaudio/fish-speech</title>
      <link>https://github.com/fishaudio/fish-speech</link>
      <description>&lt;p&gt;SOTA Open Source TTS&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;Fish Speech&lt;/h1&gt; 
 &lt;p&gt;&lt;strong&gt;English&lt;/strong&gt; | &lt;a href="https://raw.githubusercontent.com/fishaudio/fish-speech/main/docs/README.zh.md"&gt;简体中文&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/fishaudio/fish-speech/main/docs/README.pt-BR.md"&gt;Portuguese&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/fishaudio/fish-speech/main/docs/README.ja.md"&gt;日本語&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/fishaudio/fish-speech/main/docs/README.ko.md"&gt;한국어&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/fishaudio/fish-speech/main/docs/README.ar.md"&gt;العربية&lt;/a&gt; &lt;br /&gt;&lt;/p&gt; 
 &lt;a href="https://www.producthunt.com/posts/fish-speech-1-4?embed=true&amp;amp;utm_source=badge-featured&amp;amp;utm_medium=badge&amp;amp;utm_souce=badge-fish-speech-1-4" target="_blank"&gt; &lt;img src="https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=488440&amp;amp;theme=light" alt="Fish Speech 1.4 - Open-Source Multilingual Text-to-Speech with Voice Cloning | Product Hunt" style="width: 250px; height: 54px;" width="250" height="54" /&gt; &lt;/a&gt; 
 &lt;a href="https://trendshift.io/repositories/7014" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/7014" alt="fishaudio%2Ffish-speech | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt; &lt;/a&gt; 
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://count.getloli.com/get/@fish-speech?theme=asoul" /&gt;
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;a target="_blank" href="https://discord.gg/Es5qTB9BcN"&gt; &lt;img alt="Discord" src="https://img.shields.io/discord/1214047546020728892?color=%23738ADB&amp;amp;label=Discord&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;style=flat-square" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://hub.docker.com/r/fishaudio/fish-speech"&gt; &lt;img alt="Docker" src="https://img.shields.io/docker/pulls/fishaudio/fish-speech?style=flat-square&amp;amp;logo=docker" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://pd.qq.com/s/bwxia254o"&gt; &lt;img alt="QQ Channel" src="https://img.shields.io/badge/QQ-blue?logo=tencentqq" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a target="_blank" href="https://huggingface.co/spaces/TTS-AGI/TTS-Arena-V2"&gt; &lt;img alt="TTS-Arena2 Score" src="https://img.shields.io/badge/TTS_Arena2-Rank_%231-gold?style=flat-square&amp;amp;logo=trophy&amp;amp;logoColor=white" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://huggingface.co/spaces/fishaudio/fish-speech-1"&gt; &lt;img alt="Huggingface" src="https://img.shields.io/badge/🤗%20-space%20demo-yellow" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://huggingface.co/fishaudio/openaudio-s1-mini"&gt; &lt;img alt="HuggingFace Model" src="https://img.shields.io/badge/🤗%20-models-orange" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;License Notice&lt;/strong&gt;&lt;br /&gt; This codebase is released under &lt;strong&gt;Apache License&lt;/strong&gt; and all model weights are released under &lt;strong&gt;CC-BY-NC-SA-4.0 License&lt;/strong&gt;. Please refer to &lt;a href="https://raw.githubusercontent.com/fishaudio/fish-speech/main/LICENSE"&gt;LICENSE&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] &lt;strong&gt;Legal Disclaimer&lt;/strong&gt;&lt;br /&gt; We do not hold any responsibility for any illegal usage of the codebase. Please refer to your local laws about DMCA and other related laws.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Start Here&lt;/h2&gt; 
&lt;p&gt;Here are the official documents for Fish Speech, follow the instructions to get started easily.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://speech.fish.audio/install/"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://speech.fish.audio/finetune/"&gt;Finetune&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://speech.fish.audio/inference/"&gt;Inference&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://speech.fish.audio/examples"&gt;Samples&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🎉 Announcement&lt;/h2&gt; 
&lt;p&gt;We are excited to announce that we have rebranded to &lt;strong&gt;OpenAudio&lt;/strong&gt; — introducing a revolutionary new series of advanced Text-to-Speech models that builds upon the foundation of Fish-Speech.&lt;/p&gt; 
&lt;p&gt;We are proud to release &lt;strong&gt;OpenAudio-S1&lt;/strong&gt; as the first model in this series, delivering significant improvements in quality, performance, and capabilities.&lt;/p&gt; 
&lt;p&gt;OpenAudio-S1 comes in two versions: &lt;strong&gt;OpenAudio-S1&lt;/strong&gt; and &lt;strong&gt;OpenAudio-S1-mini&lt;/strong&gt;. Both models are now available on &lt;a href="https://fish.audio"&gt;Fish Audio Playground&lt;/a&gt; (for &lt;strong&gt;OpenAudio-S1&lt;/strong&gt;) and &lt;a href="https://huggingface.co/fishaudio/openaudio-s1-mini"&gt;Hugging Face&lt;/a&gt; (for &lt;strong&gt;OpenAudio-S1-mini&lt;/strong&gt;).&lt;/p&gt; 
&lt;p&gt;Visit the &lt;a href="https://openaudio.com/blogs/s1"&gt;OpenAudio website&lt;/a&gt; for blog &amp;amp; tech report.&lt;/p&gt; 
&lt;h2&gt;Highlights ✨&lt;/h2&gt; 
&lt;h3&gt;&lt;strong&gt;Excellent TTS quality&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;We use Seed TTS Eval Metrics to evaluate the model performance, and the results show that OpenAudio S1 achieves &lt;strong&gt;0.008 WER&lt;/strong&gt; and &lt;strong&gt;0.004 CER&lt;/strong&gt; on English text, which is significantly better than previous models. (English, auto eval, based on OpenAI gpt-4o-transcribe, speaker distance using Revai/pyannote-wespeaker-voxceleb-resnet34-LM)&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Word Error Rate (WER)&lt;/th&gt; 
   &lt;th&gt;Character Error Rate (CER)&lt;/th&gt; 
   &lt;th&gt;Speaker Distance&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;S1&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.008&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.004&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.332&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;S1-mini&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.011&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.005&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.380&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;&lt;strong&gt;Best Model in TTS-Arena2&lt;/strong&gt; 🏆&lt;/h3&gt; 
&lt;p&gt;OpenAudio S1 has achieved the &lt;strong&gt;#1 ranking&lt;/strong&gt; on &lt;a href="https://arena.speechcolab.org/"&gt;TTS-Arena2&lt;/a&gt;, the benchmark for text-to-speech evaluation:&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/fishaudio/fish-speech/main/docs/assets/Elo.jpg" alt="TTS-Arena2 Ranking" style="width: 75%;" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;&lt;strong&gt;Speech Control&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;OpenAudio S1 &lt;strong&gt;supports a variety of emotional, tone, and special markers&lt;/strong&gt; to enhance speech synthesis:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Basic emotions&lt;/strong&gt;:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;(angry) (sad) (excited) (surprised) (satisfied) (delighted) 
(scared) (worried) (upset) (nervous) (frustrated) (depressed)
(empathetic) (embarrassed) (disgusted) (moved) (proud) (relaxed)
(grateful) (confident) (interested) (curious) (confused) (joyful)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced emotions&lt;/strong&gt;:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;(disdainful) (unhappy) (anxious) (hysterical) (indifferent) 
(impatient) (guilty) (scornful) (panicked) (furious) (reluctant)
(keen) (disapproving) (negative) (denying) (astonished) (serious)
(sarcastic) (conciliative) (comforting) (sincere) (sneering)
(hesitating) (yielding) (painful) (awkward) (amused)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Tone markers&lt;/strong&gt;:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;(in a hurry tone) (shouting) (screaming) (whispering) (soft tone)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Special audio effects&lt;/strong&gt;:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;(laughing) (chuckling) (sobbing) (crying loudly) (sighing) (panting)
(groaning) (crowd laughing) (background laughter) (audience laughing)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also use Ha,ha,ha to control, there's many other cases waiting to be explored by yourself.&lt;/p&gt; 
&lt;p&gt;(Support for English, Chinese and Japanese now, and more languages is coming soon!)&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;Two Type of Models&lt;/strong&gt;&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Size&lt;/th&gt; 
   &lt;th&gt;Availability&lt;/th&gt; 
   &lt;th&gt;Features&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;S1&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;4B parameters&lt;/td&gt; 
   &lt;td&gt;Avaliable on &lt;a href="https://fish.audio/"&gt;fish.audio&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Full-featured flagship model&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;S1-mini&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;0.5B parameters&lt;/td&gt; 
   &lt;td&gt;Avaliable on huggingface &lt;a href="https://huggingface.co/spaces/fishaudio/openaudio-s1-mini"&gt;hf space&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Distilled version with core capabilities&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Both S1 and S1-mini incorporate online Reinforcement Learning from Human Feedback (RLHF).&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zero-shot &amp;amp; Few-shot TTS:&lt;/strong&gt; Input a 10 to 30-second vocal sample to generate high-quality TTS output. &lt;strong&gt;For detailed guidelines, see &lt;a href="https://docs.fish.audio/resources/best-practices/voice-cloning"&gt;Voice Cloning Best Practices&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multilingual &amp;amp; Cross-lingual Support:&lt;/strong&gt; Simply copy and paste multilingual text into the input box—no need to worry about the language. Currently supports English, Japanese, Korean, Chinese, French, German, Arabic, and Spanish.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;No Phoneme Dependency:&lt;/strong&gt; The model has strong generalization capabilities and does not rely on phonemes for TTS. It can handle text in any language script.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Highly Accurate:&lt;/strong&gt; Achieves a low CER (Character Error Rate) of around 0.4% and WER (Word Error Rate) of around 0.8% for Seed-TTS Eval.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Fast:&lt;/strong&gt; Accelerated by torch compile, the real-time factor is approximately 1:7 on an Nvidia RTX 4090 GPU.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;WebUI Inference:&lt;/strong&gt; Features an easy-to-use, Gradio-based web UI compatible with Chrome, Firefox, Edge, and other browsers.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deploy-Friendly:&lt;/strong&gt; Easily set up an inference server with native support for Linux and Windows (macOS support coming soon), minimizing performance loss.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;&lt;strong&gt;Media &amp;amp; Demos&lt;/strong&gt;&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;&lt;strong&gt;Social Media&lt;/strong&gt;&lt;/h3&gt; 
 &lt;a href="https://x.com/FishAudio/status/1929915992299450398" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/𝕏-Latest_Demo-black?style=for-the-badge&amp;amp;logo=x&amp;amp;logoColor=white" alt="Latest Demo on X" /&gt; &lt;/a&gt; 
 &lt;h3&gt;&lt;strong&gt;Interactive Demos&lt;/strong&gt;&lt;/h3&gt; 
 &lt;a href="https://fish.audio" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/Fish_Audio-Try_OpenAudio_S1-blue?style=for-the-badge" alt="Try OpenAudio S1" /&gt; &lt;/a&gt; 
 &lt;a href="https://huggingface.co/spaces/fishaudio/openaudio-s1-mini" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/Hugging_Face-Try_S1_Mini-yellow?style=for-the-badge" alt="Try S1 Mini" /&gt; &lt;/a&gt; 
 &lt;h3&gt;&lt;strong&gt;Video Showcases&lt;/strong&gt;&lt;/h3&gt; 
 &lt;a href="https://www.youtube.com/watch?v=SYuPvd7m06A" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/fishaudio/fish-speech/main/docs/assets/Thumbnail.jpg" alt="OpenAudio S1 Video" style="width: 50%;" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/daniilrobnikov/vits2"&gt;VITS2 (daniilrobnikov)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/fishaudio/Bert-VITS2"&gt;Bert-VITS2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/innnky/gpt-vits"&gt;GPT VITS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/b04901014/MQTTS"&gt;MQTTS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pytorch-labs/gpt-fast"&gt;GPT Fast&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/RVC-Boss/GPT-SoVITS"&gt;GPT-SoVITS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/QwenLM/Qwen3"&gt;Qwen3&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Tech Report (V1.4)&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{fish-speech-v1.4,
      title={Fish-Speech: Leveraging Large Language Models for Advanced Multilingual Text-to-Speech Synthesis},
      author={Shijia Liao and Yuxuan Wang and Tianyu Li and Yifan Cheng and Ruoyi Zhang and Rongzhi Zhou and Yijin Xing},
      year={2024},
      eprint={2411.01156},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2411.01156},
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>TheAlgorithms/Python</title>
      <link>https://github.com/TheAlgorithms/Python</link>
      <description>&lt;p&gt;All Algorithms implemented in Python&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;!-- Title: --&gt; 
 &lt;a href="https://github.com/TheAlgorithms/"&gt; &lt;img src="https://raw.githubusercontent.com/TheAlgorithms/website/1cd824df116b27029f17c2d1b42d81731f28a920/public/logo.svg?sanitize=true" height="100" /&gt; &lt;/a&gt; 
 &lt;h1&gt;&lt;a href="https://github.com/TheAlgorithms/"&gt;The Algorithms&lt;/a&gt; - Python&lt;/h1&gt; 
 &lt;!-- Labels: --&gt; 
 &lt;!-- First row: --&gt; 
 &lt;a href="https://gitpod.io/#https://github.com/TheAlgorithms/Python"&gt; &lt;img src="https://img.shields.io/badge/Gitpod-Ready--to--Code-blue?logo=gitpod&amp;amp;style=flat-square" height="20" alt="Gitpod Ready-to-Code" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/TheAlgorithms/Python/raw/master/CONTRIBUTING.md"&gt; &lt;img src="https://img.shields.io/static/v1.svg?label=Contributions&amp;amp;message=Welcome&amp;amp;color=0059b3&amp;amp;style=flat-square" height="20" alt="Contributions Welcome" /&gt; &lt;/a&gt; 
 &lt;img src="https://img.shields.io/github/repo-size/TheAlgorithms/Python.svg?label=Repo%20size&amp;amp;style=flat-square" height="20" /&gt; 
 &lt;a href="https://the-algorithms.com/discord"&gt; &lt;img src="https://img.shields.io/discord/808045925556682782.svg?logo=discord&amp;amp;colorB=7289DA&amp;amp;style=flat-square" height="20" alt="Discord chat" /&gt; &lt;/a&gt; 
 &lt;a href="https://gitter.im/TheAlgorithms/community"&gt; &lt;img src="https://img.shields.io/badge/Chat-Gitter-ff69b4.svg?label=Chat&amp;amp;logo=gitter&amp;amp;style=flat-square" height="20" alt="Gitter chat" /&gt; &lt;/a&gt; 
 &lt;!-- Second row: --&gt; 
 &lt;br /&gt; 
 &lt;a href="https://github.com/TheAlgorithms/Python/actions"&gt; &lt;img src="https://img.shields.io/github/actions/workflow/status/TheAlgorithms/Python/build.yml?branch=master&amp;amp;label=CI&amp;amp;logo=github&amp;amp;style=flat-square" height="20" alt="GitHub Workflow Status" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/pre-commit/pre-commit"&gt; &lt;img src="https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&amp;amp;logoColor=white&amp;amp;style=flat-square" height="20" alt="pre-commit" /&gt; &lt;/a&gt; 
 &lt;a href="https://docs.astral.sh/ruff/formatter/"&gt; &lt;img src="https://img.shields.io/static/v1?label=code%20style&amp;amp;message=ruff&amp;amp;color=black&amp;amp;style=flat-square" height="20" alt="code style: black" /&gt; &lt;/a&gt; 
 &lt;!-- Short description: --&gt; 
 &lt;h3&gt;All algorithms implemented in Python - for education 📚&lt;/h3&gt; 
&lt;/div&gt; 
&lt;p&gt;Implementations are for learning purposes only. They may be less efficient than the implementations in the Python standard library. Use them at your discretion.&lt;/p&gt; 
&lt;h2&gt;🚀 Getting Started&lt;/h2&gt; 
&lt;p&gt;📋 Read through our &lt;a href="https://raw.githubusercontent.com/TheAlgorithms/Python/master/CONTRIBUTING.md"&gt;Contribution Guidelines&lt;/a&gt; before you contribute.&lt;/p&gt; 
&lt;h2&gt;🌐 Community Channels&lt;/h2&gt; 
&lt;p&gt;We are on &lt;a href="https://the-algorithms.com/discord"&gt;Discord&lt;/a&gt; and &lt;a href="https://gitter.im/TheAlgorithms/community"&gt;Gitter&lt;/a&gt;! Community channels are a great way for you to ask questions and get help. Please join us!&lt;/p&gt; 
&lt;h2&gt;📜 List of Algorithms&lt;/h2&gt; 
&lt;p&gt;See our &lt;a href="https://raw.githubusercontent.com/TheAlgorithms/Python/master/DIRECTORY.md"&gt;directory&lt;/a&gt; for easier navigation and a better overview of the project.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Skyvern-AI/skyvern</title>
      <link>https://github.com/Skyvern-AI/skyvern</link>
      <description>&lt;p&gt;Automate browser-based workflows with LLMs and Computer Vision&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; &lt;a href="https://www.skyvern.com"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="fern/images/skyvern_logo.png" /&gt; 
   &lt;img height="120" src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/skyvern_logo_blackbg.png" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;br /&gt; &lt;/h1&gt; 
&lt;p align="center"&gt; 🐉 Automate Browser-based workflows using LLMs and Computer Vision 🐉 &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.skyvern.com/"&gt;&lt;img src="https://img.shields.io/badge/Website-blue?logo=googlechrome&amp;amp;logoColor=black" /&gt;&lt;/a&gt; &lt;a href="https://www.skyvern.com/docs/"&gt;&lt;img src="https://img.shields.io/badge/Docs-yellow?logo=gitbook&amp;amp;logoColor=black" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;&lt;img src="https://img.shields.io/discord/1212486326352617534?logo=discord&amp;amp;label=discord" /&gt;&lt;/a&gt; 
 &lt;!-- &lt;a href="https://pepy.tech/project/skyvern" target="_blank"&gt;&lt;img src="https://static.pepy.tech/badge/skyvern" alt="Total Downloads"/&gt;&lt;/a&gt; --&gt; &lt;a href="https://github.com/skyvern-ai/skyvern"&gt;&lt;img src="https://img.shields.io/github/stars/skyvern-ai/skyvern" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Skyvern-AI/skyvern/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/skyvern-ai/skyvern" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/skyvernai"&gt;&lt;img src="https://img.shields.io/twitter/follow/skyvernai?style=social" /&gt;&lt;/a&gt; &lt;a href="https://www.linkedin.com/company/95726232"&gt;&lt;img src="https://img.shields.io/badge/Follow%20 on%20LinkedIn-8A2BE2?logo=linkedin" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.skyvern.com"&gt;Skyvern&lt;/a&gt; automates browser-based workflows using LLMs and computer vision. It provides a simple API endpoint to fully automate manual workflows on a large number of websites, replacing brittle or unreliable automation solutions.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/geico_shu_recording_cropped.gif" /&gt; &lt;/p&gt; 
&lt;p&gt;Traditional approaches to browser automations required writing custom scripts for websites, often relying on DOM parsing and XPath-based interactions which would break whenever the website layouts changed.&lt;/p&gt; 
&lt;p&gt;Instead of only relying on code-defined XPath interactions, Skyvern relies on Vision LLMs to learn and interact with the websites.&lt;/p&gt; 
&lt;h1&gt;How it works&lt;/h1&gt; 
&lt;p&gt;Skyvern was inspired by the Task-Driven autonomous agent design popularized by &lt;a href="https://github.com/yoheinakajima/babyagi"&gt;BabyAGI&lt;/a&gt; and &lt;a href="https://github.com/Significant-Gravitas/AutoGPT"&gt;AutoGPT&lt;/a&gt; -- with one major bonus: we give Skyvern the ability to interact with websites using browser automation libraries like &lt;a href="https://playwright.dev/"&gt;Playwright&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Skyvern uses a swarm of agents to comprehend a website, and plan and execute its actions:&lt;/p&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="fern/images/skyvern_2_0_system_diagram.png" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/skyvern_2_0_system_diagram.png" /&gt; 
&lt;/picture&gt; 
&lt;p&gt;This approach has a few advantages:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Skyvern can operate on websites it's never seen before, as it's able to map visual elements to actions necessary to complete a workflow, without any customized code&lt;/li&gt; 
 &lt;li&gt;Skyvern is resistant to website layout changes, as there are no pre-determined XPaths or other selectors our system is looking for while trying to navigate&lt;/li&gt; 
 &lt;li&gt;Skyvern is able to take a single workflow and apply it to a large number of websites, as it's able to reason through the interactions necessary to complete the workflow&lt;/li&gt; 
 &lt;li&gt;Skyvern leverages LLMs to reason through interactions to ensure we can cover complex situations. Examples include: 
  &lt;ol&gt; 
   &lt;li&gt;If you wanted to get an auto insurance quote from Geico, the answer to a common question "Were you eligible to drive at 18?" could be inferred from the driver receiving their license at age 16&lt;/li&gt; 
   &lt;li&gt;If you were doing competitor analysis, it's understanding that an Arnold Palmer 22 oz can at 7/11 is almost definitely the same product as a 23 oz can at Gopuff (even though the sizes are slightly different, which could be a rounding error!)&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;A detailed technical report can be found &lt;a href="https://www.skyvern.com/blog/skyvern-2-0-state-of-the-art-web-navigation-with-85-8-on-webvoyager-eval/"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Demo&lt;/h1&gt; 
&lt;!-- Redo demo --&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/5cab4668-e8e2-4982-8551-aab05ff73a7f"&gt;https://github.com/user-attachments/assets/5cab4668-e8e2-4982-8551-aab05ff73a7f&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Performance &amp;amp; Evaluation&lt;/h1&gt; 
&lt;p&gt;Skyvern has SOTA performance on the &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/webbench.ai"&gt;WebBench benchmark&lt;/a&gt; with a 64.4% accuracy. The technical report + evaluation can be found &lt;a href="https://www.skyvern.com/blog/web-bench-a-new-way-to-compare-ai-browser-agents/"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/performance/webbench_overall.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Performance on WRITE tasks (eg filling out forms, logging in, downloading files, etc)&lt;/h2&gt; 
&lt;p&gt;Skyvern is the best performing agent on WRITE tasks (eg filling out forms, logging in, downloading files, etc), which is primarily used for RPA (Robotic Process Automation) adjacent tasks.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/performance/webbench_write.png" /&gt; &lt;/p&gt; 
&lt;h1&gt;Quickstart&lt;/h1&gt; 
&lt;h2&gt;Skyvern Cloud&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com"&gt;Skyvern Cloud&lt;/a&gt; is a managed cloud version of Skyvern that allows you to run Skyvern without worrying about the infrastructure. It allows you to run multiple Skyvern instances in parallel and comes bundled with anti-bot detection mechanisms, proxy network, and CAPTCHA solvers.&lt;/p&gt; 
&lt;p&gt;If you'd like to try it out, navigate to &lt;a href="https://app.skyvern.com"&gt;app.skyvern.com&lt;/a&gt; and create an account.&lt;/p&gt; 
&lt;h2&gt;Install &amp;amp; Run&lt;/h2&gt; 
&lt;p&gt;Dependencies needed:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.python.org/downloads/"&gt;Python 3.11.x&lt;/a&gt;, works with 3.12, not ready yet for 3.13&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nodejs.org/en/download/"&gt;NodeJS &amp;amp; NPM&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Additionally, for Windows:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://rustup.rs/"&gt;Rust&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;VS Code with C++ dev tools and Windows SDK&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;1. Install Skyvern&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install skyvern
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Run Skyvern&lt;/h3&gt; 
&lt;p&gt;This is most helpful for first time run (db setup, db migrations etc).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;skyvern quickstart
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Run task&lt;/h3&gt; 
&lt;h4&gt;UI (Recommended)&lt;/h4&gt; 
&lt;p&gt;Start the Skyvern service and UI (when DB is up and running)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;skyvern run all
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Go to &lt;a href="http://localhost:8080"&gt;http://localhost:8080&lt;/a&gt; and use the UI to run a task&lt;/p&gt; 
&lt;h4&gt;Code&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

skyvern = Skyvern()
task = await skyvern.run_task(prompt="Find the top post on hackernews today")
print(task)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Skyvern starts running the task in a browser that pops up and closes it when the task is done. You will be able to view the task from &lt;a href="http://localhost:8080/history"&gt;http://localhost:8080/history&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You can also run a task on different targets:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

# Run on Skyvern Cloud
skyvern = Skyvern(api_key="SKYVERN API KEY")

# Local Skyvern service
skyvern = Skyvern(base_url="http://localhost:8000", api_key="LOCAL SKYVERN API KEY")

task = await skyvern.run_task(prompt="Find the top post on hackernews today")
print(task)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Advanced Usage&lt;/h2&gt; 
&lt;h3&gt;Control your own browser (Chrome)&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;⚠️ WARNING: Since &lt;a href="https://developer.chrome.com/blog/remote-debugging-port"&gt;Chrome 136&lt;/a&gt;, Chrome refuses any CDP connect to the browser using the default user_data_dir. In order to use your browser data, Skyvern copies your default user_data_dir to &lt;code&gt;./tmp/user_data_dir&lt;/code&gt; the first time connecting to your local browser. ⚠️&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt;Just With Python Code&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

# The path to your Chrome browser. This example path is for Mac.
browser_path = "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
skyvern = Skyvern(
    base_url="http://localhost:8000",
    api_key="YOUR_API_KEY",
    browser_path=browser_path,
)
task = await skyvern.run_task(
    prompt="Find the top post on hackernews today",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;With Skyvern Service&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Add two variables to your .env file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# The path to your Chrome browser. This example path is for Mac.
CHROME_EXECUTABLE_PATH="/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
BROWSER_TYPE=cdp-connect
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Restart Skyvern service &lt;code&gt;skyvern run all&lt;/code&gt; and run the task through UI or code&lt;/p&gt; 
&lt;h3&gt;Run Skyvern with any remote browser&lt;/h3&gt; 
&lt;p&gt;Grab the cdp connection url and pass it to Skyvern&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

skyvern = Skyvern(cdp_url="your cdp connection url")
task = await skyvern.run_task(
    prompt="Find the top post on hackernews today",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Get consistent output schema from your run&lt;/h3&gt; 
&lt;p&gt;You can do this by adding the &lt;code&gt;data_extraction_schema&lt;/code&gt; parameter:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

skyvern = Skyvern()
task = await skyvern.run_task(
    prompt="Find the top post on hackernews today",
    data_extraction_schema={
        "type": "object",
        "properties": {
            "title": {
                "type": "string",
                "description": "The title of the top post"
            },
            "url": {
                "type": "string",
                "description": "The URL of the top post"
            },
            "points": {
                "type": "integer",
                "description": "Number of points the post has received"
            }
        }
    }
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Helpful commands to debug issues&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Launch the Skyvern Server Separately*
skyvern run server

# Launch the Skyvern UI
skyvern run ui

# Check status of the Skyvern service
skyvern status

# Stop the Skyvern service
skyvern stop all

# Stop the Skyvern UI
skyvern stop ui

# Stop the Skyvern Server Separately
skyvern stop server
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Docker Compose setup&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Make sure you have &lt;a href="https://www.docker.com/products/docker-desktop/"&gt;Docker Desktop&lt;/a&gt; installed and running on your machine&lt;/li&gt; 
 &lt;li&gt;Make sure you don't have postgres running locally (Run &lt;code&gt;docker ps&lt;/code&gt; to check)&lt;/li&gt; 
 &lt;li&gt;Clone the repository and navigate to the root directory&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;skyvern init llm&lt;/code&gt; to generate a &lt;code&gt;.env&lt;/code&gt; file. This will be copied into the Docker image.&lt;/li&gt; 
 &lt;li&gt;Fill in the LLM provider key on the &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt;. &lt;em&gt;If you want to run Skyvern on a remote server, make sure you set the correct server ip for the UI container in &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt;.&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Run the following command via the commandline: &lt;pre&gt;&lt;code class="language-bash"&gt; docker compose up -d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Navigate to &lt;code&gt;http://localhost:8080&lt;/code&gt; in your browser to start using the UI&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; Only one Postgres container can run on port 5432 at a time. If you switch from the CLI-managed Postgres to Docker Compose, you must first remove the original container:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;docker rm -f postgresql-container
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;If you encounter any database related errors while using Docker to run Skyvern, check which Postgres container is running with &lt;code&gt;docker ps&lt;/code&gt;.&lt;/p&gt; 
&lt;h1&gt;Skyvern Features&lt;/h1&gt; 
&lt;h2&gt;Skyvern Tasks&lt;/h2&gt; 
&lt;p&gt;Tasks are the fundamental building block inside Skyvern. Each task is a single request to Skyvern, instructing it to navigate through a website and accomplish a specific goal.&lt;/p&gt; 
&lt;p&gt;Tasks require you to specify a &lt;code&gt;url&lt;/code&gt;, &lt;code&gt;prompt&lt;/code&gt;, and can optionally include a &lt;code&gt;data schema&lt;/code&gt; (if you want the output to conform to a specific schema) and &lt;code&gt;error codes&lt;/code&gt; (if you want Skyvern to stop running in specific situations).&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/skyvern_2_0_screenshot.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Skyvern Workflows&lt;/h2&gt; 
&lt;p&gt;Workflows are a way to chain multiple tasks together to form a cohesive unit of work.&lt;/p&gt; 
&lt;p&gt;For example, if you wanted to download all invoices newer than January 1st, you could create a workflow that first navigated to the invoices page, then filtered down to only show invoices newer than January 1st, extracted a list of all eligible invoices, and iterated through each invoice to download it.&lt;/p&gt; 
&lt;p&gt;Another example is if you wanted to automate purchasing products from an e-commerce store, you could create a workflow that first navigated to the desired product, then added it to a cart. Second, it would navigate to the cart and validate the cart state. Finally, it would go through the checkout process to purchase the items.&lt;/p&gt; 
&lt;p&gt;Supported workflow features include:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Browser Task&lt;/li&gt; 
 &lt;li&gt;Browser Action&lt;/li&gt; 
 &lt;li&gt;Data Extraction&lt;/li&gt; 
 &lt;li&gt;Validation&lt;/li&gt; 
 &lt;li&gt;For Loops&lt;/li&gt; 
 &lt;li&gt;File parsing&lt;/li&gt; 
 &lt;li&gt;Sending emails&lt;/li&gt; 
 &lt;li&gt;Text Prompts&lt;/li&gt; 
 &lt;li&gt;HTTP Request Block&lt;/li&gt; 
 &lt;li&gt;Custom Code Block&lt;/li&gt; 
 &lt;li&gt;Uploading files to block storage&lt;/li&gt; 
 &lt;li&gt;(Coming soon) Conditionals&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/block_example_v2.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Livestreaming&lt;/h2&gt; 
&lt;p&gt;Skyvern allows you to livestream the viewport of the browser to your local machine so that you can see exactly what Skyvern is doing on the web. This is useful for debugging and understanding how Skyvern is interacting with a website, and intervening when necessary&lt;/p&gt; 
&lt;h2&gt;Form Filling&lt;/h2&gt; 
&lt;p&gt;Skyvern is natively capable of filling out form inputs on websites. Passing in information via the &lt;code&gt;navigation_goal&lt;/code&gt; will allow Skyvern to comprehend the information and fill out the form accordingly.&lt;/p&gt; 
&lt;h2&gt;Data Extraction&lt;/h2&gt; 
&lt;p&gt;Skyvern is also capable of extracting data from a website.&lt;/p&gt; 
&lt;p&gt;You can also specify a &lt;code&gt;data_extraction_schema&lt;/code&gt; directly within the main prompt to tell Skyvern exactly what data you'd like to extract from the website, in jsonc format. Skyvern's output will be structured in accordance to the supplied schema.&lt;/p&gt; 
&lt;h2&gt;File Downloading&lt;/h2&gt; 
&lt;p&gt;Skyvern is also capable of downloading files from a website. All downloaded files are automatically uploaded to block storage (if configured), and you can access them via the UI.&lt;/p&gt; 
&lt;h2&gt;Authentication&lt;/h2&gt; 
&lt;p&gt;Skyvern supports a number of different authentication methods to make it easier to automate tasks behind a login. If you'd like to try it out, please reach out to us &lt;a href="mailto:founders@skyvern.com"&gt;via email&lt;/a&gt; or &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;discord&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/secure_password_task_example.png" /&gt; &lt;/p&gt; 
&lt;h3&gt;🔐 2FA Support (TOTP)&lt;/h3&gt; 
&lt;p&gt;Skyvern supports a number of different 2FA methods to allow you to automate workflows that require 2FA.&lt;/p&gt; 
&lt;p&gt;Examples include:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;QR-based 2FA (e.g. Google Authenticator, Authy)&lt;/li&gt; 
 &lt;li&gt;Email based 2FA&lt;/li&gt; 
 &lt;li&gt;SMS based 2FA&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;🔐 Learn more about 2FA support &lt;a href="https://www.skyvern.com/docs/credentials/totp"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Password Manager Integrations&lt;/h3&gt; 
&lt;p&gt;Skyvern currently supports the following password manager integrations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Bitwarden&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; 1Password&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; LastPass&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Model Context Protocol (MCP)&lt;/h2&gt; 
&lt;p&gt;Skyvern supports the Model Context Protocol (MCP) to allow you to use any LLM that supports MCP.&lt;/p&gt; 
&lt;p&gt;See the MCP documentation &lt;a href="https://github.com/Skyvern-AI/skyvern/raw/main/integrations/mcp/README.md"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Zapier / Make.com / N8N Integration&lt;/h2&gt; 
&lt;p&gt;Skyvern supports Zapier, Make.com, and N8N to allow you to connect your Skyvern workflows to other apps.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.skyvern.com/docs/integrations/zapier"&gt;Zapier&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.skyvern.com/docs/integrations/make.com"&gt;Make.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.skyvern.com/docs/integrations/n8n"&gt;N8N&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;🔐 Learn more about 2FA support &lt;a href="https://www.skyvern.com/docs/credentials/totp"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Real-world examples of Skyvern&lt;/h1&gt; 
&lt;p&gt;We love to see how Skyvern is being used in the wild. Here are some examples of how Skyvern is being used to automate workflows in the real world. Please open PRs to add your own examples!&lt;/p&gt; 
&lt;h2&gt;Invoice Downloading on many different websites&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://meetings.hubspot.com/skyvern/demo"&gt;Book a demo to see it live&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/invoice_downloading.gif" /&gt; &lt;/p&gt; 
&lt;h2&gt;Automate the job application process&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/job_application"&gt;💡 See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/job_application_demo.gif" /&gt; &lt;/p&gt; 
&lt;h2&gt;Automate materials procurement for a manufacturing company&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/finditparts"&gt;💡 See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/finditparts_recording_crop.gif" /&gt; &lt;/p&gt; 
&lt;h2&gt;Navigating to government websites to register accounts or fill out forms&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/california_edd"&gt;💡 See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/edd_services.gif" /&gt; &lt;/p&gt; 
&lt;!-- Add example of delaware entity lookups x2 --&gt; 
&lt;h2&gt;Filling out random contact us forms&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/contact_us_forms"&gt;💡 See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/contact_forms.gif" /&gt; &lt;/p&gt; 
&lt;h2&gt;Retrieving insurance quotes from insurance providers in any language&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/bci_seguros"&gt;💡 See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/bci_seguros_recording.gif" /&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/geico"&gt;💡 See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/geico_shu_recording_cropped.gif" /&gt; &lt;/p&gt; 
&lt;h1&gt;Contributor Setup&lt;/h1&gt; 
&lt;p&gt;Make sure to have &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;uv&lt;/a&gt; installed.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Run this to create your virtual environment (&lt;code&gt;.venv&lt;/code&gt;) &lt;pre&gt;&lt;code class="language-bash"&gt;uv sync --group dev
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Perform initial server configuration &lt;pre&gt;&lt;code class="language-bash"&gt;uv run skyvern quickstart
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Navigate to &lt;code&gt;http://localhost:8080&lt;/code&gt; in your browser to start using the UI &lt;em&gt;The Skyvern CLI supports Windows, WSL, macOS, and Linux environments.&lt;/em&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h1&gt;Documentation&lt;/h1&gt; 
&lt;p&gt;More extensive documentation can be found on our &lt;a href="https://www.skyvern.com/docs"&gt;📕 docs page&lt;/a&gt;. Please let us know if something is unclear or missing by opening an issue or reaching out to us &lt;a href="mailto:founders@skyvern.com"&gt;via email&lt;/a&gt; or &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;discord&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Supported LLMs&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Provider&lt;/th&gt; 
   &lt;th&gt;Supported Models&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
   &lt;td&gt;gpt4-turbo, gpt-4o, gpt-4o-mini&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Anthropic&lt;/td&gt; 
   &lt;td&gt;Claude 3 (Haiku, Sonnet, Opus), Claude 3.5 (Sonnet)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Azure OpenAI&lt;/td&gt; 
   &lt;td&gt;Any GPT models. Better performance with a multimodal llm (azure/gpt4-o)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AWS Bedrock&lt;/td&gt; 
   &lt;td&gt;Anthropic Claude 3 (Haiku, Sonnet, Opus), Claude 3.5 (Sonnet)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Gemini&lt;/td&gt; 
   &lt;td&gt;Gemini 2.5 Pro and flash, Gemini 2.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ollama&lt;/td&gt; 
   &lt;td&gt;Run any locally hosted model via &lt;a href="https://github.com/ollama/ollama"&gt;Ollama&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenRouter&lt;/td&gt; 
   &lt;td&gt;Access models through &lt;a href="https://openrouter.ai"&gt;OpenRouter&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI-compatible&lt;/td&gt; 
   &lt;td&gt;Any custom API endpoint that follows OpenAI's API format (via &lt;a href="https://docs.litellm.ai/docs/providers/openai_compatible"&gt;liteLLM&lt;/a&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Environment Variables&lt;/h4&gt; 
&lt;h5&gt;OpenAI&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_OPENAI&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register OpenAI models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI API Key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_API_BASE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI API Base, optional&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://openai.api.base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_ORGANIZATION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI Organization ID, optional&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;your-org-id&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;OPENAI_GPT4O&lt;/code&gt;, &lt;code&gt;OPENAI_GPT4O_MINI&lt;/code&gt;, &lt;code&gt;OPENAI_GPT4_1&lt;/code&gt;, &lt;code&gt;OPENAI_O4_MINI&lt;/code&gt;, &lt;code&gt;OPENAI_O3&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;Anthropic&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_ANTHROPIC&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register Anthropic models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Anthropic API key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended&lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;ANTHROPIC_CLAUDE3.5_SONNET&lt;/code&gt;, &lt;code&gt;ANTHROPIC_CLAUDE3.7_SONNET&lt;/code&gt;, &lt;code&gt;ANTHROPIC_CLAUDE4_OPUS&lt;/code&gt;, &lt;code&gt;ANTHROPIC_CLAUDE4_SONNET&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;Azure OpenAI&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_AZURE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register Azure OpenAI models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure deployment API key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_DEPLOYMENT&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure OpenAI Deployment Name&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;skyvern-deployment&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_API_BASE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure deployment api base url&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://skyvern-deployment.openai.azure.com/&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_API_VERSION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure API Version&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;2024-02-01&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;AZURE_OPENAI&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;AWS Bedrock&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_BEDROCK&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register AWS Bedrock models. To use AWS Bedrock, you need to make sure your &lt;a href="https://github.com/boto/boto3?tab=readme-ov-file#using-boto3"&gt;AWS configurations&lt;/a&gt; are set up correctly first.&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;BEDROCK_ANTHROPIC_CLAUDE3.7_SONNET_INFERENCE_PROFILE&lt;/code&gt;, &lt;code&gt;BEDROCK_ANTHROPIC_CLAUDE4_OPUS_INFERENCE_PROFILE&lt;/code&gt;, &lt;code&gt;BEDROCK_ANTHROPIC_CLAUDE4_SONNET_INFERENCE_PROFILE&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;Gemini&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_GEMINI&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register Gemini models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;GEMINI_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Gemini API Key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;your_google_gemini_api_key&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;GEMINI_2.5_PRO_PREVIEW&lt;/code&gt;, &lt;code&gt;GEMINI_2.5_FLASH_PREVIEW&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;Ollama&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_OLLAMA&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register local models via Ollama&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OLLAMA_SERVER_URL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;URL for your Ollama server&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;http://host.docker.internal:11434&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OLLAMA_MODEL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Ollama model name to load&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;qwen2.5:7b-instruct&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;OLLAMA&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Note: Ollama does not support vision yet.&lt;/p&gt; 
&lt;h5&gt;OpenRouter&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_OPENROUTER&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register OpenRouter models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenRouter API key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENROUTER_MODEL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenRouter model name&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;mistralai/mistral-small-3.1-24b-instruct&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENROUTER_API_BASE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenRouter API base URL&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://api.openrouter.ai/v1&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;OPENROUTER&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;OpenAI-Compatible&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_OPENAI_COMPATIBLE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register a custom OpenAI-compatible API endpoint&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_MODEL_NAME&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Model name for OpenAI-compatible endpoint&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;yi-34b&lt;/code&gt;, &lt;code&gt;gpt-3.5-turbo&lt;/code&gt;, &lt;code&gt;mistral-large&lt;/code&gt;, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;API key for OpenAI-compatible endpoint&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_API_BASE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Base URL for OpenAI-compatible endpoint&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://api.together.xyz/v1&lt;/code&gt;, &lt;code&gt;http://localhost:8000/v1&lt;/code&gt;, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_API_VERSION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;API version for OpenAI-compatible endpoint, optional&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;2023-05-15&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_MAX_TOKENS&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Maximum tokens for completion, optional&lt;/td&gt; 
   &lt;td&gt;Integer&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;4096&lt;/code&gt;, &lt;code&gt;8192&lt;/code&gt;, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_TEMPERATURE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Temperature setting, optional&lt;/td&gt; 
   &lt;td&gt;Float&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;0.0&lt;/code&gt;, &lt;code&gt;0.5&lt;/code&gt;, &lt;code&gt;0.7&lt;/code&gt;, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_SUPPORTS_VISION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Whether model supports vision, optional&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Supported LLM Key: &lt;code&gt;OPENAI_COMPATIBLE&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;General LLM Configuration&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;LLM_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The name of the model you want to use&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;See supported LLM keys above&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;SECONDARY_LLM_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The name of the model for mini agents skyvern runs with&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;See supported LLM keys above&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;LLM_CONFIG_MAX_TOKENS&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Override the max tokens used by the LLM&lt;/td&gt; 
   &lt;td&gt;Integer&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;128000&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Feature Roadmap&lt;/h1&gt; 
&lt;p&gt;This is our planned roadmap for the next few months. If you have any suggestions or would like to see a feature added, please don't hesitate to reach out to us &lt;a href="mailto:founders@skyvern.com"&gt;via email&lt;/a&gt; or &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;discord&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Open Source&lt;/strong&gt; - Open Source Skyvern's core codebase&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Workflow support&lt;/strong&gt; - Allow support to chain multiple Skyvern calls together&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Improved context&lt;/strong&gt; - Improve Skyvern's ability to understand content around interactable elements by introducing feeding relevant label context through the text prompt&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Cost Savings&lt;/strong&gt; - Improve Skyvern's stability and reduce the cost of running Skyvern by optimizing the context tree passed into Skyvern&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Self-serve UI&lt;/strong&gt; - Deprecate the Streamlit UI in favour of a React-based UI component that allows users to kick off new jobs in Skyvern&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Workflow UI Builder&lt;/strong&gt; - Introduce a UI to allow users to build and analyze workflows visually&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Chrome Viewport streaming&lt;/strong&gt; - Introduce a way to live-stream the Chrome viewport to the user's browser (as a part of the self-serve UI)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Past Runs UI&lt;/strong&gt; - Deprecate the Streamlit UI in favour of a React-based UI that allows you to visualize past runs and their results&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Auto workflow builder ("Observer") mode&lt;/strong&gt; - Allow Skyvern to auto-generate workflows as it's navigating the web to make it easier to build new workflows&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Prompt Caching&lt;/strong&gt; - Introduce a caching layer to the LLM calls to dramatically reduce the cost of running Skyvern (memorize past actions and repeat them!)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Web Evaluation Dataset&lt;/strong&gt; - Integrate Skyvern with public benchmark tests to track the quality of our models over time&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Improved Debug mode&lt;/strong&gt; - Allow Skyvern to plan its actions and get "approval" before running them, allowing you to debug what it's doing and more easily iterate on the prompt&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Chrome Extension&lt;/strong&gt; - Allow users to interact with Skyvern through a Chrome extension (incl voice mode, saving tasks, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Skyvern Action Recorder&lt;/strong&gt; - Allow Skyvern to watch a user complete a task and then automatically generate a workflow for it&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Interactable Livestream&lt;/strong&gt; - Allow users to interact with the livestream in real-time to intervene when necessary (such as manually submitting sensitive forms)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Integrate LLM Observability tools&lt;/strong&gt; - Integrate LLM Observability tools to allow back-testing prompt changes with specific data sets + visualize the performance of Skyvern over time&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Langchain Integration&lt;/strong&gt; - Create langchain integration in langchain_community to use Skyvern as a "tool".&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Contributing&lt;/h1&gt; 
&lt;p&gt;We welcome PRs and suggestions! Don't hesitate to open a PR/issue or to reach out to us &lt;a href="mailto:founders@skyvern.com"&gt;via email&lt;/a&gt; or &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;discord&lt;/a&gt;. Please have a look at our &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/CONTRIBUTING.md"&gt;contribution guide&lt;/a&gt; and &lt;a href="https://github.com/skyvern-ai/skyvern/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22"&gt;"Help Wanted" issues&lt;/a&gt; to get started!&lt;/p&gt; 
&lt;p&gt;If you want to chat with the skyvern repository to get a high level overview of how it is structured, how to build off it, and how to resolve usage questions, check out &lt;a href="https://sage.storia.ai?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=skyvern-readme"&gt;Code Sage&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Telemetry&lt;/h1&gt; 
&lt;p&gt;By Default, Skyvern collects basic usage statistics to help us understand how Skyvern is being used. If you would like to opt-out of telemetry, please set the &lt;code&gt;SKYVERN_TELEMETRY&lt;/code&gt; environment variable to &lt;code&gt;false&lt;/code&gt;.&lt;/p&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;Skyvern's open source repository is supported via a managed cloud. All of the core logic powering Skyvern is available in this open source repository licensed under the &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/LICENSE"&gt;AGPL-3.0 License&lt;/a&gt;, with the exception of anti-bot measures available in our managed cloud offering.&lt;/p&gt; 
&lt;p&gt;If you have any questions or concerns around licensing, please &lt;a href="mailto:support@skyvern.com"&gt;contact us&lt;/a&gt; and we would be happy to help.&lt;/p&gt; 
&lt;h1&gt;Star History&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#Skyvern-AI/skyvern&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Skyvern-AI/skyvern&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>myshell-ai/OpenVoice</title>
      <link>https://github.com/myshell-ai/OpenVoice</link>
      <description>&lt;p&gt;Instant voice cloning by MIT and MyShell. Audio foundation model.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;div&gt;
  &amp;nbsp;
 &lt;/div&gt; 
 &lt;img src="https://raw.githubusercontent.com/myshell-ai/OpenVoice/main/resources/openvoicelogo.jpg" width="400" /&gt; 
 &lt;p&gt;&lt;a href="https://arxiv.org/abs/2312.01479"&gt;Paper&lt;/a&gt; | &lt;a href="https://research.myshell.ai/open-voice"&gt;Website&lt;/a&gt; &lt;br /&gt; &lt;br /&gt; &lt;a href="https://trendshift.io/repositories/6161" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/6161" alt="myshell-ai%2FOpenVoice | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;h3&gt;OpenVoice V1&lt;/h3&gt; 
&lt;p&gt;As we detailed in our &lt;a href="https://arxiv.org/abs/2312.01479"&gt;paper&lt;/a&gt; and &lt;a href="https://research.myshell.ai/open-voice"&gt;website&lt;/a&gt;, the advantages of OpenVoice are three-fold:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1. Accurate Tone Color Cloning.&lt;/strong&gt; OpenVoice can accurately clone the reference tone color and generate speech in multiple languages and accents.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2. Flexible Voice Style Control.&lt;/strong&gt; OpenVoice enables granular control over voice styles, such as emotion and accent, as well as other style parameters including rhythm, pauses, and intonation.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;3. Zero-shot Cross-lingual Voice Cloning.&lt;/strong&gt; Neither of the language of the generated speech nor the language of the reference speech needs to be presented in the massive-speaker multi-lingual training dataset.&lt;/p&gt; 
&lt;h3&gt;OpenVoice V2&lt;/h3&gt; 
&lt;p&gt;In April 2024, we released OpenVoice V2, which includes all features in V1 and has:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1. Better Audio Quality.&lt;/strong&gt; OpenVoice V2 adopts a different training strategy that delivers better audio quality.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2. Native Multi-lingual Support.&lt;/strong&gt; English, Spanish, French, Chinese, Japanese and Korean are natively supported in OpenVoice V2.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;3. Free Commercial Use.&lt;/strong&gt; Starting from April 2024, both V2 and V1 are released under MIT License. Free for commercial use.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/myshell-ai/OpenVoice/assets/40556743/3cba936f-82bf-476c-9e52-09f0f417bb2f"&gt;Video&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;OpenVoice has been powering the instant voice cloning capability of &lt;a href="https://app.myshell.ai/explore"&gt;myshell.ai&lt;/a&gt; since May 2023. Until Nov 2023, the voice cloning model has been used tens of millions of times by users worldwide, and witnessed the explosive user growth on the platform.&lt;/p&gt; 
&lt;h2&gt;Main Contributors&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.qinzy.tech"&gt;Zengyi Qin&lt;/a&gt; at MIT&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://wl-zhao.github.io"&gt;Wenliang Zhao&lt;/a&gt; at Tsinghua University&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://yuxumin.github.io"&gt;Xumin Yu&lt;/a&gt; at Tsinghua University&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/ethan_myshell"&gt;Ethan Sun&lt;/a&gt; at MyShell&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How to Use&lt;/h2&gt; 
&lt;p&gt;Please see &lt;a href="https://raw.githubusercontent.com/myshell-ai/OpenVoice/main/docs/USAGE.md"&gt;usage&lt;/a&gt; for detailed instructions.&lt;/p&gt; 
&lt;h2&gt;Common Issues&lt;/h2&gt; 
&lt;p&gt;Please see &lt;a href="https://raw.githubusercontent.com/myshell-ai/OpenVoice/main/docs/QA.md"&gt;QA&lt;/a&gt; for common questions and answers. We will regularly update the question and answer list.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@article{qin2023openvoice,
  title={OpenVoice: Versatile Instant Voice Cloning},
  author={Qin, Zengyi and Zhao, Wenliang and Yu, Xumin and Sun, Xin},
  journal={arXiv preprint arXiv:2312.01479},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;OpenVoice V1 and V2 are MIT Licensed. Free for both commercial and research use.&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;This implementation is based on several excellent projects, &lt;a href="https://github.com/coqui-ai/TTS"&gt;TTS&lt;/a&gt;, &lt;a href="https://github.com/jaywalnut310/vits"&gt;VITS&lt;/a&gt;, and &lt;a href="https://github.com/daniilrobnikov/vits2"&gt;VITS2&lt;/a&gt;. Thanks for their awesome work!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>DrewThomasson/ebook2audiobook</title>
      <link>https://github.com/DrewThomasson/ebook2audiobook</link>
      <description>&lt;p&gt;Generate audiobooks from e-books, voice cloning &amp; 1107+ languages!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;📚 ebook2audiobook&lt;/h1&gt; 
&lt;p&gt;CPU/GPU Converter from eBooks to audiobooks with chapters and metadata&lt;br /&gt; using XTTSv2, Bark, Vits, Fairseq, YourTTS, Tacotron and more. Supports voice cloning and +1110 languages!&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;This tool is intended for use with non-DRM, legally acquired eBooks only.&lt;/strong&gt; &lt;br /&gt; The authors are not responsible for any misuse of this software or any resulting legal consequences. &lt;br /&gt; Use this tool responsibly and in accordance with all applicable laws.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/63Tv3F65k6"&gt;&lt;img src="https://dcbadge.limes.pink/api/server/https://discord.gg/63Tv3F65k6" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Thanks to support ebook2audiobook developers!&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://ko-fi.com/athomasson2"&gt;&lt;img src="https://img.shields.io/badge/Ko--fi-F16061?style=for-the-badge&amp;amp;logo=ko-fi&amp;amp;logoColor=white" alt="Ko-Fi" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Run locally&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;&lt;img src="https://img.shields.io/badge/Quick%20Start-blue?style=for-the-badge" alt="Quick Start" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml"&gt;&lt;img src="https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml/badge.svg?sanitize=true" alt="Docker Build" /&gt;&lt;/a&gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/releases/latest"&gt;&lt;img src="https://img.shields.io/badge/Download-Now-blue.svg?sanitize=true" alt="Download" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;a href="https://github.com/DrewThomasson/ebook2audiobook"&gt; &lt;img src="https://img.shields.io/badge/Platform-mac%20|%20linux%20|%20windows-lightgrey" alt="Platform" /&gt; &lt;/a&gt;
&lt;a href="https://hub.docker.com/r/athomasson2/ebook2audiobook"&gt; &lt;img alt="Docker Pull Count" src="https://img.shields.io/docker/pulls/athomasson2/ebook2audiobook.svg?sanitize=true" /&gt; &lt;/a&gt; 
&lt;h3&gt;Run Remotely&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/drewThomasson/ebook2audiobook"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/DrewThomasson/ebook2audiobook/blob/main/Notebooks/colab_ebook2audiobook.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Free Google Colab" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Rihcus/ebook2audiobookXTTS/raw/main/Notebooks/kaggle-ebook2audiobook.ipynb"&gt;&lt;img src="https://img.shields.io/badge/Kaggle-035a7d?style=flat&amp;amp;logo=kaggle&amp;amp;logoColor=white" alt="Kaggle" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;GUI Interface&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/demo_web_gui.gif" alt="demo_web_gui" /&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to see images of Web GUI&lt;/summary&gt; 
 &lt;img width="1728" alt="GUI Screen 1" src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_1.png" /&gt; 
 &lt;img width="1728" alt="GUI Screen 2" src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_2.png" /&gt; 
 &lt;img width="1728" alt="GUI Screen 3" src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_3.png" /&gt; 
&lt;/details&gt; 
&lt;h2&gt;Demos&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;New Default Voice Demo&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/750035dc-e355-46f1-9286-05c1d9e88cea"&gt;https://github.com/user-attachments/assets/750035dc-e355-46f1-9286-05c1d9e88cea&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;More Demos&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;ASMR Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/68eee9a1-6f71-4903-aacd-47397e47e422"&gt;https://github.com/user-attachments/assets/68eee9a1-6f71-4903-aacd-47397e47e422&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Rainy Day Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/d25034d9-c77f-43a9-8f14-0d167172b080"&gt;https://github.com/user-attachments/assets/d25034d9-c77f-43a9-8f14-0d167172b080&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Scarlett Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/b12009ee-ec0d-45ce-a1ef-b3a52b9f8693"&gt;https://github.com/user-attachments/assets/b12009ee-ec0d-45ce-a1ef-b3a52b9f8693&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;David Attenborough Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/81c4baad-117e-4db5-ac86-efc2b7fea921"&gt;https://github.com/user-attachments/assets/81c4baad-117e-4db5-ac86-efc2b7fea921&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://github.com/DrewThomasson/VoxNovel/raw/dc5197dff97252fa44c391dc0596902d71278a88/readme_files/example_in_app.jpeg" alt="Example" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;README.md&lt;/h2&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#-ebook2audiobook"&gt;ebook2audiobook&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#gui-interface"&gt;GUI Interface&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#demos"&gt;Demos&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#supported-languages"&gt;Supported Languages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#hardware-requirements"&gt;Minimum Requirements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;Usage&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;Run Locally&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;Launching Gradio Web Interface&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#basic--usage"&gt;Basic Headless Usage&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#example-of-custom-model-zip-upload"&gt;Headless Custom XTTS Model Usage&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#help-command-output"&gt;Help command output&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#run-remotely"&gt;Run Remotely&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tuned-tts-models"&gt;Fine Tuned TTS models&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tuned-tts-collection"&gt;Collection of Fine-Tuned TTS Models&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tune-your-own-xttsv2-model"&gt;Train XTTSv2&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-gpu-options"&gt;Docker&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-gpu-options"&gt;GPU options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#running-the-pre-built-docker-container"&gt;Docker Run&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#building-the-docker-container"&gt;Docker Build&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-compose"&gt;Docker Compose&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-headless-guide"&gt;Docker headless guide&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-container-file-locations"&gt;Docker container file locations&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#common-docker-issues"&gt;Common Docker issues&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#supported-ebook-formats"&gt;Supported eBook Formats&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#output-formats"&gt;Output Formats&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#updating-to-latest-version"&gt;Updating to Latest Version&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#reverting-to-older-versions"&gt;Revert to older Version&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#common-issues"&gt;Common Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#special-thanks"&gt;Special Thanks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#table-of-contents"&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;📚 Splits eBook into chapters for organized audio.&lt;/li&gt; 
 &lt;li&gt;🎙️ High-quality text-to-speech with &lt;a href="https://huggingface.co/coqui/XTTS-v2"&gt;Coqui XTTSv2&lt;/a&gt; and &lt;a href="https://github.com/facebookresearch/fairseq/tree/main/examples/mms"&gt;Fairseq&lt;/a&gt; (and more).&lt;/li&gt; 
 &lt;li&gt;🗣️ Optional voice cloning with your own voice file.&lt;/li&gt; 
 &lt;li&gt;🌍 Supports +1110 languages (English by default). &lt;a href="https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html"&gt;List of Supported languages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;🖥️ Designed to run on 4GB RAM.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Supported Languages&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Arabic (ar)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Chinese (zh)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;English (en)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Spanish (es)&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;French (fr)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;German (de)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Italian (it)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Portuguese (pt)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Polish (pl)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Turkish (tr)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Russian (ru)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Dutch (nl)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Czech (cs)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Japanese (ja)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Hindi (hi)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Bengali (bn)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Hungarian (hu)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Korean (ko)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Vietnamese (vi)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Swedish (sv)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Persian (fa)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Yoruba (yo)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Swahili (sw)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Indonesian (id)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Slovak (sk)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Croatian (hr)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Tamil (ta)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Danish (da)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html"&gt;&lt;strong&gt;+1100 languages and dialects here&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Hardware Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;4gb RAM minimum, 8GB recommended&lt;/li&gt; 
 &lt;li&gt;Virtualization enabled if running on windows (Docker only)&lt;/li&gt; 
 &lt;li&gt;CPU (intel, AMD, ARM), GPU (Nvidia, AMD*, Intel*) (Recommended), MPS (Apple Silicon CPU) *available very soon&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;Before to post an install or bug issue search carefully to the opened and closed issues TAB&lt;br /&gt; to be sure your issue does not exist already.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] &lt;strong&gt;Lacking of any standards structure like what is a chapter, paragraph, preface etc.&lt;br /&gt; you should first remove manually any text you don't want to be converted in audio.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Installation Instructions&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Clone repo&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/DrewThomasson/ebook2audiobook.git
cd ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Launching Gradio Web Interface&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run ebook2audiobook&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh  # Run launch script
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Mac Launcher&lt;/strong&gt;&lt;br /&gt; Double click &lt;code&gt;Mac Ebook2Audiobook Launcher.command&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd  # Run launch script or double click on it
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows Launcher&lt;/strong&gt;&lt;br /&gt; Double click &lt;code&gt;ebook2audiobook.cmd&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Manual Python Install&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# (for experts only!)
REQUIRED_PROGRAMS=("calibre" "ffmpeg" "nodejs" "mecab" "espeak-ng" "rust" "sox")
REQUIRED_PYTHON_VERSION="3.12"
pip install -r requirements.txt  # Install Python Requirements
python app.py  # Run Ebook2Audiobook
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Open the Web App&lt;/strong&gt;: Click the URL provided in the terminal to access the web app and convert eBooks. &lt;code&gt;http://localhost:7860/&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;For Public Link&lt;/strong&gt;: &lt;code&gt;python app.py --share&lt;/code&gt; (all OS) &lt;code&gt;./ebook2audiobook.sh --share&lt;/code&gt; (Linux/MacOS) &lt;code&gt;ebook2audiobook.cmd --share&lt;/code&gt; (Windows)&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;If the script is stopped and run again, you need to refresh your gradio GUI interface&lt;br /&gt; to let the web page reconnect to the new connection socket.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Basic Usage&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh --headless --ebook &amp;lt;path_to_ebook_file&amp;gt; \
    --voice [path_to_voice_file] --language [language_code]
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd --headless --ebook &amp;lt;path_to_ebook_file&amp;gt;
    --voice [path_to_voice_file] --language [language_code]
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[--ebook]&lt;/strong&gt;: Path to your eBook file&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[--voice]&lt;/strong&gt;: Voice cloning file path (optional)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[--language]&lt;/strong&gt;: Language code in ISO-639-3 (i.e.: ita for italian, eng for english, deu for german...).&lt;br /&gt; Default language is eng and --language is optional for default language set in ./lib/lang.py.&lt;br /&gt; The ISO-639-1 2 letters codes are also supported.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Example of Custom Model Zip Upload&lt;/h3&gt; 
&lt;p&gt;(must be a .zip file containing the mandatory model files. Example for XTTSv2: config.json, model.pth, vocab.json and ref.wav)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh --headless --ebook &amp;lt;ebook_file_path&amp;gt; \
    --voice &amp;lt;target_voice_file_path&amp;gt; --language &amp;lt;language&amp;gt; --custom_model &amp;lt;custom_model_path&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd --headless --ebook &amp;lt;ebook_file_path&amp;gt; \
    --voice &amp;lt;target_voice_file_path&amp;gt; --language &amp;lt;language&amp;gt; --custom_model &amp;lt;custom_model_path&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&amp;lt;custom_model_path&amp;gt;&lt;/strong&gt;: Path to &lt;code&gt;model_name.zip&lt;/code&gt; file, which must contain (according to the tts engine) all the mandatory files&lt;br /&gt; (see ./lib/models.py).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;For Detailed Guide with list of all Parameters to use&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh --help
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd --help
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Or for all OS&lt;/strong&gt; &lt;code&gt;python app.py --help &lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a id="help-command-output"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;usage: app.py [-h] [--session SESSION] [--share] [--headless] [--ebook EBOOK]
              [--ebooks_dir EBOOKS_DIR] [--language LANGUAGE] [--voice VOICE]
              [--device {cpu,gpu,mps}]
              [--tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}]
              [--custom_model CUSTOM_MODEL] [--fine_tuned FINE_TUNED]
              [--output_format OUTPUT_FORMAT] [--temperature TEMPERATURE]
              [--length_penalty LENGTH_PENALTY] [--num_beams NUM_BEAMS]
              [--repetition_penalty REPETITION_PENALTY] [--top_k TOP_K]
              [--top_p TOP_P] [--speed SPEED] [--enable_text_splitting]
              [--text_temp TEXT_TEMP] [--waveform_temp WAVEFORM_TEMP]
              [--output_dir OUTPUT_DIR] [--version]

Convert eBooks to Audiobooks using a Text-to-Speech model. You can either launch the Gradio interface or run the script in headless mode for direct conversion.

options:
  -h, --help            show this help message and exit
  --session SESSION     Session to resume the conversion in case of interruption, crash, 
                            or reuse of custom models and custom cloning voices.

**** The following options are for all modes:
  Optional

**** The following option are for gradio/gui mode only:
  Optional

  --share               Enable a public shareable Gradio link.

**** The following options are for --headless mode only:
  --headless            Run the script in headless mode
  --ebook EBOOK         Path to the ebook file for conversion. Cannot be used when --ebooks_dir is present.
  --ebooks_dir EBOOKS_DIR
                        Relative or absolute path of the directory containing the files to convert. 
                            Cannot be used when --ebook is present.
  --language LANGUAGE   Language of the e-book. Default language is set 
                            in ./lib/lang.py sed as default if not present. All compatible language codes are in ./lib/lang.py

optional parameters:
  --voice VOICE         (Optional) Path to the voice cloning file for TTS engine. 
                            Uses the default voice if not present.
  --device {cpu,gpu,mps}
                        (Optional) Pprocessor unit type for the conversion. 
                            Default is set in ./lib/conf.py if not present. Fall back to CPU if GPU not available.
  --tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}
                        (Optional) Preferred TTS engine (available are: ['XTTSv2', 'BARK', 'VITS', 'FAIRSEQ', 'TACOTRON2', 'YOURTTS', 'xtts', 'bark', 'vits', 'fairseq', 'tacotron', 'yourtts'].
                            Default depends on the selected language. The tts engine should be compatible with the chosen language
  --custom_model CUSTOM_MODEL
                        (Optional) Path to the custom model zip file cntaining mandatory model files. 
                            Please refer to ./lib/models.py
  --fine_tuned FINE_TUNED
                        (Optional) Fine tuned model path. Default is builtin model.
  --output_format OUTPUT_FORMAT
                        (Optional) Output audio format. Default is set in ./lib/conf.py
  --temperature TEMPERATURE
                        (xtts only, optional) Temperature for the model. 
                            Default to config.json model. Higher temperatures lead to more creative outputs.
  --length_penalty LENGTH_PENALTY
                        (xtts only, optional) A length penalty applied to the autoregressive decoder. 
                            Default to config.json model. Not applied to custom models.
  --num_beams NUM_BEAMS
                        (xtts only, optional) Controls how many alternative sequences the model explores. Must be equal or greater than length penalty. 
                            Default to config.json model.
  --repetition_penalty REPETITION_PENALTY
                        (xtts only, optional) A penalty that prevents the autoregressive decoder from repeating itself. 
                            Default to config.json model.
  --top_k TOP_K         (xtts only, optional) Top-k sampling. 
                            Lower values mean more likely outputs and increased audio generation speed. 
                            Default to config.json model.
  --top_p TOP_P         (xtts only, optional) Top-p sampling. 
                            Lower values mean more likely outputs and increased audio generation speed. Default to config.json model.
  --speed SPEED         (xtts only, optional) Speed factor for the speech generation. 
                            Default to config.json model.
  --enable_text_splitting
                        (xtts only, optional) Enable TTS text splitting. This option is known to not be very efficient. 
                            Default to config.json model.
  --text_temp TEXT_TEMP
                        (bark only, optional) Text Temperature for the model. 
                            Default to 0.85. Higher temperatures lead to more creative outputs.
  --waveform_temp WAVEFORM_TEMP
                        (bark only, optional) Waveform Temperature for the model. 
                            Default to 0.5. Higher temperatures lead to more creative outputs.
  --output_dir OUTPUT_DIR
                        (Optional) Path to the output directory. Default is set in ./lib/conf.py
  --version             Show the version of the script and exit

Example usage:    
Windows:
    Gradio/GUI:
    ebook2audiobook.cmd
    Headless mode:
    ebook2audiobook.cmd --headless --ebook '/path/to/file'
Linux/Mac:
    Gradio/GUI:
    ./ebook2audiobook.sh
    Headless mode:
    ./ebook2audiobook.sh --headless --ebook '/path/to/file'
    
Tip: to add of silence (1.4 seconds) into your text just use "###" or "[pause]".

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;NOTE: in gradio/gui mode, to cancel a running conversion, just click on the [X] from the ebook upload component.&lt;/p&gt; 
&lt;p&gt;TIP: if it needs some more pauses, just add '###' or '[pause]' between the words you wish more pause. one [pause] equals to 1.4 seconds&lt;/p&gt; 
&lt;h4&gt;Docker GPU Options&lt;/h4&gt; 
&lt;p&gt;Available pre-build tags: &lt;code&gt;latest&lt;/code&gt; (CUDA 11.8)&lt;/p&gt; 
&lt;h4&gt;Edit: IF GPU isn't detected then you'll have to build the image -&amp;gt; &lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#building-the-docker-container"&gt;Building the Docker Container&lt;/a&gt;&lt;/h4&gt; 
&lt;h4&gt;Running the pre-built Docker Container&lt;/h4&gt; 
&lt;p&gt;-Run with CPU only&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;docker run --pull always --rm -p 7860:7860 athomasson2/ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;-Run with GPU Speedup (NVIDIA compatible only)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;docker run --pull always --rm --gpus all -p 7860:7860 athomasson2/ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command will start the Gradio interface on port 7860.(localhost:7860)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For more options add the parameter &lt;code&gt;--help&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Building the Docker Container&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can build the docker image with the command:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;docker build -t athomasson2/ebook2audiobook .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Avalible Docker Build Arguments&lt;/h4&gt; 
&lt;p&gt;&lt;code&gt;--build-arg TORCH_VERSION=cuda118&lt;/code&gt; Available tags: [cuda121, cuda118, cuda128, rocm, xpu, cpu]&lt;/p&gt; 
&lt;p&gt;All CUDA version numbers should work, Ex: CUDA 11.6-&amp;gt; cuda116&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;--build-arg SKIP_XTTS_TEST=true&lt;/code&gt; (Saves space by not baking XTTSv2 model into docker image)&lt;/p&gt; 
&lt;h2&gt;Docker container file locations&lt;/h2&gt; 
&lt;p&gt;All ebook2audiobooks will have the base dir of &lt;code&gt;/app/&lt;/code&gt; For example: &lt;code&gt;tmp&lt;/code&gt; = &lt;code&gt;/app/tmp&lt;/code&gt; &lt;code&gt;audiobooks&lt;/code&gt; = &lt;code&gt;/app/audiobooks&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Docker headless guide&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Before you do run this you need to create a dir named "input-folder" in your current dir which will be linked, This is where you can put your input files for the docker image to see&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir input-folder &amp;amp;&amp;amp; mkdir Audiobooks
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;In the command below swap out &lt;strong&gt;YOUR_INPUT_FILE.TXT&lt;/strong&gt; with the name of your input file&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --pull always --rm \
    -v $(pwd)/input-folder:/app/input_folder \
    -v $(pwd)/audiobooks:/app/audiobooks \
    athomasson2/ebook2audiobook \
    --headless --ebook /input_folder/YOUR_EBOOK_FILE
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;The output Audiobooks will be found in the Audiobook folder which will also be located in your local dir you ran this docker command in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;To get the help command for the other parameters this program has you can run this&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --pull always --rm athomasson2/ebook2audiobook --help

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That will output this &lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#help-command-output"&gt;Help command output&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Docker Compose&lt;/h3&gt; 
&lt;p&gt;This project uses Docker Compose to run locally. You can enable or disable GPU support by setting either &lt;code&gt;*gpu-enabled&lt;/code&gt; or &lt;code&gt;*gpu-disabled&lt;/code&gt; in &lt;code&gt;docker-compose.yml&lt;/code&gt;&lt;/p&gt; 
&lt;h4&gt;Steps to Run&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Clone the Repository&lt;/strong&gt; (if you haven't already): &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/DrewThomasson/ebook2audiobook.git
cd ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Set GPU Support (disabled by default)&lt;/strong&gt; To enable GPU support, modify &lt;code&gt;docker-compose.yml&lt;/code&gt; and change &lt;code&gt;*gpu-disabled&lt;/code&gt; to &lt;code&gt;*gpu-enabled&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Start the service:&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Docker
docker-compose up -d # To update add --build

# Podman
podman compose -f podman-compose.yml up -d # To update add --build
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Access the service:&lt;/strong&gt; The service will be available at &lt;a href="http://localhost:7860"&gt;http://localhost:7860&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Common Docker Issues&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;My NVIDIA GPU isnt being detected?? -&amp;gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/wiki/GPU-ISSUES"&gt;GPU ISSUES Wiki Page&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;python: can't open file '/home/user/app/app.py': [Errno 2] No such file or directory&lt;/code&gt; (Just remove all post arguments as I replaced the &lt;code&gt;CMD&lt;/code&gt; with &lt;code&gt;ENTRYPOINT&lt;/code&gt; in the &lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/Dockerfile"&gt;Dockerfile&lt;/a&gt;)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Example: &lt;code&gt;docker run --pull always athomasson2/ebook2audiobook app.py --script_mode full_docker&lt;/code&gt; - &amp;gt; corrected - &amp;gt; &lt;code&gt;docker run --pull always athomasson2/ebook2audiobook&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Arguments can be easily added like this now &lt;code&gt;docker run --pull always athomasson2/ebook2audiobook --share&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Docker gets stuck downloading Fine-Tuned models. (This does not happen for every computer but some appear to run into this issue) Disabling the progress bar appears to fix the issue, as discussed &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/issues/191"&gt;here in #191&lt;/a&gt; Example of adding this fix in the &lt;code&gt;docker run&lt;/code&gt; command&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-Dockerfile"&gt;docker run --pull always --rm --gpus all -e HF_HUB_DISABLE_PROGRESS_BARS=1 -e HF_HUB_ENABLE_HF_TRANSFER=0 \
    -p 7860:7860 athomasson2/ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Fine Tuned TTS models&lt;/h2&gt; 
&lt;h4&gt;Fine Tune your own XTTSv2 model&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/drewThomasson/xtts-finetune-webui-gpu"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/raw/v25/Notebooks/finetune/xtts/kaggle-xtts-finetune-webui-gradio-gui.ipynb"&gt;&lt;img src="https://img.shields.io/badge/Kaggle-035a7d?style=flat&amp;amp;logo=kaggle&amp;amp;logoColor=white" alt="Kaggle" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/DrewThomasson/ebook2audiobook/blob/v25/Notebooks/finetune/xtts/colab_xtts_finetune_webui.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;De-noise training data&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/drewThomasson/DeepFilterNet2_no_limit"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Rikorose/DeepFilterNet"&gt;&lt;img src="https://img.shields.io/badge/DeepFilterNet-181717?logo=github" alt="GitHub Repo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Fine Tuned TTS Collection&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/drewThomasson/fineTunedTTSModels/tree/main"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Models-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For an XTTSv2 custom model a ref audio clip of the voice reference is mandatory:&lt;/p&gt; 
&lt;h2&gt;Supported eBook Formats&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;.epub&lt;/code&gt;, &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.mobi&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.rtf&lt;/code&gt;, &lt;code&gt;.chm&lt;/code&gt;, &lt;code&gt;.lit&lt;/code&gt;, &lt;code&gt;.pdb&lt;/code&gt;, &lt;code&gt;.fb2&lt;/code&gt;, &lt;code&gt;.odt&lt;/code&gt;, &lt;code&gt;.cbr&lt;/code&gt;, &lt;code&gt;.cbz&lt;/code&gt;, &lt;code&gt;.prc&lt;/code&gt;, &lt;code&gt;.lrf&lt;/code&gt;, &lt;code&gt;.pml&lt;/code&gt;, &lt;code&gt;.snb&lt;/code&gt;, &lt;code&gt;.cbc&lt;/code&gt;, &lt;code&gt;.rb&lt;/code&gt;, &lt;code&gt;.tcr&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Best results&lt;/strong&gt;: &lt;code&gt;.epub&lt;/code&gt; or &lt;code&gt;.mobi&lt;/code&gt; for automatic chapter detection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Output Formats&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Creates a &lt;code&gt;['m4b', 'm4a', 'mp4', 'webm', 'mov', 'mp3', 'flac', 'wav', 'ogg', 'aac']&lt;/code&gt; (set in ./lib/conf.py) file with metadata and chapters.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Updating to Latest Version&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git pull # Locally/Compose

docker pull athomasson2/ebook2audiobook:latest # For Pre-build docker images
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Reverting to older Versions&lt;/h2&gt; 
&lt;p&gt;Releases can be found -&amp;gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/releases"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git checkout tags/VERSION_NUM # Locally/Compose -&amp;gt; Example: git checkout tags/v25.7.7

athomasson2/ebook2audiobook:VERSION_NUM # For Pre-build docker images -&amp;gt; Example: athomasson2/ebook2audiobook:v25.7.7
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Common Issues:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;My NVIDIA GPU isnt being detected?? -&amp;gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/wiki/GPU-ISSUES"&gt;GPU ISSUES Wiki Page&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;CPU is slow (better on server smp CPU) while NVIDIA GPU can have almost real time conversion. &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/discussions/19#discussioncomment-10879846"&gt;Discussion about this&lt;/a&gt; For faster multilingual generation I would suggest my other &lt;a href="https://github.com/DrewThomasson/ebook2audiobookpiper-tts"&gt;project that uses piper-tts&lt;/a&gt; instead (It doesn't have zero-shot voice cloning though, and is Siri quality voices, but it is much faster on cpu).&lt;/li&gt; 
 &lt;li&gt;"I'm having dependency issues" - Just use the docker, its fully self contained and has a headless mode, add &lt;code&gt;--help&lt;/code&gt; parameter at the end of the docker run command for more information.&lt;/li&gt; 
 &lt;li&gt;"Im getting a truncated audio issue!" - PLEASE MAKE AN ISSUE OF THIS, we don't speak every language and need advise from users to fine tune the sentence splitting logic.😊&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What we need help with! 🙌&lt;/h2&gt; 
&lt;h2&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook/issues/32"&gt;Full list of things can be found here&lt;/a&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Any help from people speaking any of the supported languages to help us improve the models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Do you need to rent a GPU to boost service from us?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;A poll is open here &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/discussions/889"&gt;https://github.com/DrewThomasson/ebook2audiobook/discussions/889&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Special Thanks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Coqui TTS&lt;/strong&gt;: &lt;a href="https://github.com/idiap/coqui-ai-TTS"&gt;Coqui TTS GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Calibre&lt;/strong&gt;: &lt;a href="https://calibre-ebook.com"&gt;Calibre Website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;FFmpeg&lt;/strong&gt;: &lt;a href="https://ffmpeg.org"&gt;FFmpeg Website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook/issues/8"&gt;@shakenbake15 for better chapter saving method&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>reflex-dev/reflex</title>
      <link>https://github.com/reflex-dev/reflex</link>
      <description>&lt;p&gt;🕸️ Web apps in pure Python 🐍&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/reflex.svg?sanitize=true" alt="Reflex Logo" width="300px" /&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;&lt;strong&gt;✨ Performant, customizable web apps in pure Python. Deploy in seconds. ✨&lt;/strong&gt;&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://badge.fury.io/py/reflex"&gt;&lt;img src="https://badge.fury.io/py/reflex.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/pyversions/reflex.svg?sanitize=true" alt="versions" /&gt; &lt;a href="https://reflex.dev/docs/getting-started/introduction"&gt;&lt;img src="https://img.shields.io/badge/Documentation%20-Introduction%20-%20%23007ec6" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/projects/reflex"&gt;&lt;img src="https://static.pepy.tech/badge/reflex" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/T5WSbC2YtQ"&gt;&lt;img src="https://img.shields.io/discord/1029853095527727165?color=%237289da&amp;amp;label=Discord" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://x.com/getreflex"&gt;&lt;img src="https://img.shields.io/twitter/follow/getreflex" alt="Twitter" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href="https://github.com/reflex-dev/reflex/raw/main/README.md"&gt;English&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/zh/zh_cn/README.md"&gt;简体中文&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/zh/zh_tw/README.md"&gt;繁體中文&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/tr/README.md"&gt;Türkçe&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/in/README.md"&gt;हिंदी&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/pt/pt_br/README.md"&gt;Português (Brasil)&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/it/README.md"&gt;Italiano&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/es/README.md"&gt;Español&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/kr/README.md"&gt;한국어&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/ja/README.md"&gt;日本語&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/de/README.md"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/pe/README.md"&gt;Persian (پارسی)&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/vi/README.md"&gt;Tiếng Việt&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] 🚀 &lt;strong&gt;Try &lt;a href="https://build.reflex.dev/"&gt;Reflex Build&lt;/a&gt;&lt;/strong&gt; – our AI-powered app builder that generates full-stack Reflex applications in seconds.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h1&gt;Introduction&lt;/h1&gt; 
&lt;p&gt;Reflex is a library to build full-stack web apps in pure Python.&lt;/p&gt; 
&lt;p&gt;Key features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Pure Python&lt;/strong&gt; - Write your app's frontend and backend all in Python, no need to learn Javascript.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Full Flexibility&lt;/strong&gt; - Reflex is easy to get started with, but can also scale to complex apps.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Deploy Instantly&lt;/strong&gt; - After building, deploy your app with a &lt;a href="https://reflex.dev/docs/hosting/deploy-quick-start/"&gt;single command&lt;/a&gt; or host it on your own server.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See our &lt;a href="https://reflex.dev/blog/2024-03-21-reflex-architecture/#the-reflex-architecture"&gt;architecture page&lt;/a&gt; to learn how Reflex works under the hood.&lt;/p&gt; 
&lt;h2&gt;⚙️ Installation&lt;/h2&gt; 
&lt;p&gt;Open a terminal and run (Requires Python 3.10+):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install reflex
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;🥳 Create your first app&lt;/h2&gt; 
&lt;p&gt;Installing &lt;code&gt;reflex&lt;/code&gt; also installs the &lt;code&gt;reflex&lt;/code&gt; command line tool.&lt;/p&gt; 
&lt;p&gt;Test that the install was successful by creating a new project. (Replace &lt;code&gt;my_app_name&lt;/code&gt; with your project name):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir my_app_name
cd my_app_name
reflex init
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command initializes a template app in your new directory.&lt;/p&gt; 
&lt;p&gt;You can run this app in development mode:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;reflex run
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should see your app running at &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Now you can modify the source code in &lt;code&gt;my_app_name/my_app_name.py&lt;/code&gt;. Reflex has fast refreshes so you can see your changes instantly when you save your code.&lt;/p&gt; 
&lt;h2&gt;🫧 Example App&lt;/h2&gt; 
&lt;p&gt;Let's go over an example: creating an image generation UI around &lt;a href="https://platform.openai.com/docs/guides/images/image-generation?context=node"&gt;DALL·E&lt;/a&gt;. For simplicity, we just call the &lt;a href="https://platform.openai.com/docs/api-reference/authentication"&gt;OpenAI API&lt;/a&gt;, but you could replace this with an ML model run locally.&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/dalle.gif" alt="A frontend wrapper for DALL·E, shown in the process of generating an image." width="550" /&gt; 
&lt;/div&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;Here is the complete code to create this. This is all done in one Python file!&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import reflex as rx
import openai

openai_client = openai.OpenAI()


class State(rx.State):
    """The app state."""

    prompt = ""
    image_url = ""
    processing = False
    complete = False

    def get_image(self):
        """Get the image from the prompt."""
        if self.prompt == "":
            return rx.window_alert("Prompt Empty")

        self.processing, self.complete = True, False
        yield
        response = openai_client.images.generate(
            prompt=self.prompt, n=1, size="1024x1024"
        )
        self.image_url = response.data[0].url
        self.processing, self.complete = False, True


def index():
    return rx.center(
        rx.vstack(
            rx.heading("DALL-E", font_size="1.5em"),
            rx.input(
                placeholder="Enter a prompt..",
                on_blur=State.set_prompt,
                width="25em",
            ),
            rx.button(
                "Generate Image",
                on_click=State.get_image,
                width="25em",
                loading=State.processing
            ),
            rx.cond(
                State.complete,
                rx.image(src=State.image_url, width="20em"),
            ),
            align="center",
        ),
        width="100%",
        height="100vh",
    )

# Add state and page to the app.
app = rx.App()
app.add_page(index, title="Reflex:DALL-E")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Let's break this down.&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/dalle_colored_code_example.png" alt="Explaining the differences between backend and frontend parts of the DALL-E app." width="900" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;&lt;strong&gt;Reflex UI&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Let's start with the UI.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;def index():
    return rx.center(
        ...
    )
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This &lt;code&gt;index&lt;/code&gt; function defines the frontend of the app.&lt;/p&gt; 
&lt;p&gt;We use different components such as &lt;code&gt;center&lt;/code&gt;, &lt;code&gt;vstack&lt;/code&gt;, &lt;code&gt;input&lt;/code&gt;, and &lt;code&gt;button&lt;/code&gt; to build the frontend. Components can be nested within each other to create complex layouts. And you can use keyword args to style them with the full power of CSS.&lt;/p&gt; 
&lt;p&gt;Reflex comes with &lt;a href="https://reflex.dev/docs/library"&gt;60+ built-in components&lt;/a&gt; to help you get started. We are actively adding more components, and it's easy to &lt;a href="https://reflex.dev/docs/wrapping-react/overview/"&gt;create your own components&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;State&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Reflex represents your UI as a function of your state.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;class State(rx.State):
    """The app state."""
    prompt = ""
    image_url = ""
    processing = False
    complete = False

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The state defines all the variables (called vars) in an app that can change and the functions that change them.&lt;/p&gt; 
&lt;p&gt;Here the state is comprised of a &lt;code&gt;prompt&lt;/code&gt; and &lt;code&gt;image_url&lt;/code&gt;. There are also the booleans &lt;code&gt;processing&lt;/code&gt; and &lt;code&gt;complete&lt;/code&gt; to indicate when to disable the button (during image generation) and when to show the resulting image.&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;Event Handlers&lt;/strong&gt;&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;def get_image(self):
    """Get the image from the prompt."""
    if self.prompt == "":
        return rx.window_alert("Prompt Empty")

    self.processing, self.complete = True, False
    yield
    response = openai_client.images.generate(
        prompt=self.prompt, n=1, size="1024x1024"
    )
    self.image_url = response.data[0].url
    self.processing, self.complete = False, True
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Within the state, we define functions called event handlers that change the state vars. Event handlers are the way that we can modify the state in Reflex. They can be called in response to user actions, such as clicking a button or typing in a text box. These actions are called events.&lt;/p&gt; 
&lt;p&gt;Our DALL·E app has an event handler, &lt;code&gt;get_image&lt;/code&gt; which gets this image from the OpenAI API. Using &lt;code&gt;yield&lt;/code&gt; in the middle of an event handler will cause the UI to update. Otherwise the UI will update at the end of the event handler.&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;Routing&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Finally, we define our app.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;app = rx.App()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We add a page from the root of the app to the index component. We also add a title that will show up in the page preview/browser tab.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;app.add_page(index, title="DALL-E")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can create a multi-page app by adding more pages.&lt;/p&gt; 
&lt;h2&gt;📑 Resources&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;📑 &lt;a href="https://reflex.dev/docs/getting-started/introduction"&gt;Docs&lt;/a&gt; &amp;nbsp; | &amp;nbsp; 🗞️ &lt;a href="https://reflex.dev/blog"&gt;Blog&lt;/a&gt; &amp;nbsp; | &amp;nbsp; 📱 &lt;a href="https://reflex.dev/docs/library"&gt;Component Library&lt;/a&gt; &amp;nbsp; | &amp;nbsp; 🖼️ &lt;a href="https://reflex.dev/templates/"&gt;Templates&lt;/a&gt; &amp;nbsp; | &amp;nbsp; 🛸 &lt;a href="https://reflex.dev/docs/hosting/deploy-quick-start"&gt;Deployment&lt;/a&gt; &amp;nbsp;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;✅ Status&lt;/h2&gt; 
&lt;p&gt;Reflex launched in December 2022 with the name Pynecone.&lt;/p&gt; 
&lt;p&gt;🚀 Introducing &lt;a href="https://build.reflex.dev/"&gt;Reflex Build&lt;/a&gt; — Our AI-Powered Builder Reflex Build uses AI to generate complete full-stack Python applications. It helps you quickly create, customize, and refine your Reflex apps — from frontend components to backend logic — so you can focus on your ideas instead of boilerplate code. Whether you’re prototyping or scaling, Reflex Build accelerates development by intelligently scaffolding and optimizing your app’s entire stack.&lt;/p&gt; 
&lt;p&gt;Alongside this, &lt;a href="https://cloud.reflex.dev"&gt;Reflex Cloud&lt;/a&gt; launched in 2025 to offer the best hosting experience for your Reflex apps. We’re continuously improving the platform with new features and capabilities.&lt;/p&gt; 
&lt;p&gt;Reflex has new releases and features coming every week! Make sure to &lt;span&gt;⭐&lt;/span&gt; star and &lt;span&gt;👀&lt;/span&gt; watch this repository to stay up to date.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions of any size! Below are some good ways to get started in the Reflex community.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Join Our Discord&lt;/strong&gt;: Our &lt;a href="https://discord.gg/T5WSbC2YtQ"&gt;Discord&lt;/a&gt; is the best place to get help on your Reflex project and to discuss how you can contribute.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Discussions&lt;/strong&gt;: A great way to talk about features you want added or things that are confusing/need clarification.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Issues&lt;/strong&gt;: &lt;a href="https://github.com/reflex-dev/reflex/issues"&gt;Issues&lt;/a&gt; are an excellent way to report bugs. Additionally, you can try and solve an existing issue and submit a PR.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We are actively looking for contributors, no matter your skill level or experience. To contribute check out &lt;a href="https://github.com/reflex-dev/reflex/raw/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;All Thanks To Our Contributors:&lt;/h2&gt; 
&lt;a href="https://github.com/reflex-dev/reflex/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=reflex-dev/reflex" /&gt; &lt;/a&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Reflex is open-source and licensed under the &lt;a href="https://raw.githubusercontent.com/reflex-dev/reflex/main/LICENSE"&gt;Apache License 2.0&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>imputnet/helium</title>
      <link>https://github.com/imputnet/helium</link>
      <description>&lt;p&gt;Private, fast, and honest web browser&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;br /&gt; 
 &lt;p&gt; &lt;img src="https://raw.githubusercontent.com/imputnet/helium/main/resources/branding/app_icon/raw.png" title="Helium" alt="Helium logo" width="120" /&gt; &lt;/p&gt;
 &lt;h1&gt;Helium&lt;/h1&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p width="120"&gt; The Chromium-based web browser made for people, with love. &lt;br /&gt; Best privacy by default, unbiased ad-blocking, no bloat and no noise. &lt;/p&gt; 
 &lt;a href="https://helium.computer/"&gt; helium.computer &lt;/a&gt; 
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Downloads&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Helium is still in beta, so unexpected issues may occur. We are not responsible for any damage caused by usage of beta software.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Best way to download Helium is to open &lt;a href="https://helium.computer/"&gt;helium.computer&lt;/a&gt; on your computer. It'll pick the right build for your OS and architecture automatically.&lt;/p&gt; 
&lt;p&gt;If you wish to download builds "straight from the tap" with all options in one place, you can do it on GitHub in the Releases section in each platform's repo:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/imputnet/helium-macos/releases/latest"&gt;macOS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/imputnet/helium-linux/releases/latest"&gt;Linux&lt;/a&gt; (AppImage)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/imputnet/helium-windows/releases/latest"&gt;Windows&lt;/a&gt; (no auto-updates yet)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Platform packaging&lt;/h2&gt; 
&lt;p&gt;Helium is available on all major desktop platforms, with entirety of source code for all of them published here:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/imputnet/helium-macos"&gt;Helium for macOS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/imputnet/helium-linux"&gt;Helium for Linux&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/imputnet/helium-windows"&gt;Helium for Windows&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Other Helium repos&lt;/h2&gt; 
&lt;p&gt;Along with the main repo and platform packaging, these projects are also a part of Helium:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/imputnet/helium-services"&gt;Helium services&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/imputnet/helium-onboarding"&gt;Helium onboarding&lt;/a&gt; (the onboarding page seen in Helium at &lt;code&gt;helium://setup&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/imputnet/ublock-origin-crx"&gt;uBlock Origin packaging&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;h3&gt;ungoogled-chromium&lt;/h3&gt; 
&lt;p&gt;Helium is proudly based on &lt;a href="https://github.com/ungoogled-software/ungoogled-chromium"&gt;ungoogled-chromium&lt;/a&gt;. It wouldn't be possible for us to get rid of Google's bloat and get a development+building pipeline this fast without it. Huge shout-out to everyone behind this amazing project! (and we intend to contribute even more stuff upstream in the future)&lt;/p&gt; 
&lt;h3&gt;The Chromium project&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.chromium.org/"&gt;The Chromium Project&lt;/a&gt; is obviously at the core of Helium, making it possible to exist in the first place.&lt;/p&gt; 
&lt;h3&gt;ungoogled-chromium's dependencies&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/gcarq/inox-patchset"&gt;Inox patchset&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://tracker.debian.org/pkg/chromium-browser"&gt;Debian&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/bromite/bromite"&gt;Bromite&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://iridiumbrowser.de/"&gt;Iridium Browser&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;All code, patches, modified portions of imported code or patches, and any other content that is unique to Helium and not imported from other repositories is licensed under GPL-3.0. See &lt;a href="https://raw.githubusercontent.com/imputnet/helium/main/LICENSE"&gt;LICENSE&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Any content imported from other projects retains its original license (for example, any original unmodified code imported from ungoogled-chromium remains licensed under their &lt;a href="https://raw.githubusercontent.com/imputnet/helium/main/LICENSE.ungoogled_chromium"&gt;BSD 3-Clause license&lt;/a&gt;).&lt;/p&gt; 
&lt;h2&gt;More documentation (soon)&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] We will add more documentation along with design and motivation guidelines in the future. All docs will be linked here along with other related content.&lt;/p&gt; 
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>HKUDS/RAG-Anything</title>
      <link>https://github.com/HKUDS/RAG-Anything</link>
      <description>&lt;p&gt;"RAG-Anything: All-in-One RAG Framework"&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;div style="margin: 20px 0;"&gt; 
  &lt;img src="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/assets/logo.png" width="120" height="120" alt="RAG-Anything Logo" style="border-radius: 20px; box-shadow: 0 8px 32px rgba(0, 217, 255, 0.3);" /&gt; 
 &lt;/div&gt; 
 &lt;h1&gt;🚀 RAG-Anything: All-in-One RAG Framework&lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/14959" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14959" alt="HKUDS%2FRAG-Anything | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://readme-typing-svg.herokuapp.com?font=Orbitron&amp;amp;size=24&amp;amp;duration=3000&amp;amp;pause=1000&amp;amp;color=00D9FF&amp;amp;center=true&amp;amp;vCenter=true&amp;amp;width=600&amp;amp;lines=Welcome+to+RAG-Anything;Next-Gen+Multimodal+RAG+System;Powered+by+Advanced+AI+Technology" alt="Typing Animation" /&gt; 
 &lt;/div&gt; 
 &lt;div align="center"&gt; 
  &lt;div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; padding: 25px; text-align: center;"&gt; 
   &lt;p&gt; &lt;a href="https://github.com/HKUDS/RAG-Anything"&gt;&lt;img src="https://img.shields.io/badge/🔥Project-Page-00d9ff?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2510.12323"&gt;&lt;img src="https://img.shields.io/badge/📄arXiv-2510.12323-ff6b6b?style=for-the-badge&amp;amp;logo=arxiv&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/LightRAG"&gt;&lt;img src="https://img.shields.io/badge/⚡Based%20on-LightRAG-4ecdc4?style=for-the-badge&amp;amp;logo=lightning&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
   &lt;p&gt; &lt;a href="https://github.com/HKUDS/RAG-Anything/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/HKUDS/RAG-Anything?color=00d9ff&amp;amp;style=for-the-badge&amp;amp;logo=star&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/🐍Python-3.10-4ecdc4?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;a href="https://pypi.org/project/raganything/"&gt;&lt;img src="https://img.shields.io/pypi/v/raganything.svg?style=for-the-badge&amp;amp;logo=pypi&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e&amp;amp;color=ff6b6b" /&gt;&lt;/a&gt; &lt;a href="https://github.com/astral-sh/uv"&gt;&lt;img src="https://img.shields.io/badge/⚡uv-Ready-ff6b6b?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
   &lt;p&gt; &lt;a href="https://discord.gg/yF2MmDJyGJ"&gt;&lt;img src="https://img.shields.io/badge/💬Discord-Community-7289da?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/RAG-Anything/issues/7"&gt;&lt;img src="https://img.shields.io/badge/💬WeChat-Group-07c160?style=for-the-badge&amp;amp;logo=wechat&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
   &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/README_zh.md"&gt;&lt;img src="https://img.shields.io/badge/🇨🇳中文版-1a1a2e?style=for-the-badge" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/README.md"&gt;&lt;img src="https://img.shields.io/badge/🇺🇸English-1a1a2e?style=for-the-badge" /&gt;&lt;/a&gt; &lt;/p&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;div style="width: 100%; height: 2px; margin: 20px 0; background: linear-gradient(90deg, transparent, #00d9ff, transparent);"&gt;&lt;/div&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/#-quick-start" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/Quick%20Start-Get%20Started%20Now-00d9ff?style=for-the-badge&amp;amp;logo=rocket&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🎉 News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2025.10]🎯📢 🚀 We have released the technical report of &lt;a href="http://arxiv.org/abs/2510.12323"&gt;RAG-Anything&lt;/a&gt;. Access it now to explore our latest research findings.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2025.08]🎯📢 🔍 RAG-Anything now features &lt;strong&gt;VLM-Enhanced Query&lt;/strong&gt; mode! When documents include images, the system seamlessly integrates them into VLM for advanced multimodal analysis, combining visual and textual context for deeper insights.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2025.07]🎯📢 RAG-Anything now features a &lt;a href="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/docs/context_aware_processing.md"&gt;context configuration module&lt;/a&gt;, enabling intelligent integration of relevant contextual information to enhance multimodal content processing.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2025.07]🎯📢 🚀 RAG-Anything now supports multimodal query capabilities, enabling enhanced RAG with seamless processing of text, images, tables, and equations.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2025.07]🎯📢 🎉 RAG-Anything has reached 1k🌟 stars on GitHub! Thank you for your incredible support and valuable contributions to the project.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🌟 System Overview&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Next-Generation Multimodal Intelligence&lt;/em&gt;&lt;/p&gt; 
&lt;div style="background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%); border-radius: 15px; padding: 25px; margin: 20px 0; border: 2px solid #00d9ff; box-shadow: 0 0 30px rgba(0, 217, 255, 0.3);"&gt; 
 &lt;p&gt;Modern documents increasingly contain diverse multimodal content—text, images, tables, equations, charts, and multimedia—that traditional text-focused RAG systems cannot effectively process. &lt;strong&gt;RAG-Anything&lt;/strong&gt; addresses this challenge as a comprehensive &lt;strong&gt;All-in-One Multimodal Document Processing RAG system&lt;/strong&gt; built on &lt;a href="https://github.com/HKUDS/LightRAG"&gt;LightRAG&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;As a unified solution, RAG-Anything &lt;strong&gt;eliminates the need for multiple specialized tools&lt;/strong&gt;. It provides &lt;strong&gt;seamless processing and querying across all content modalities&lt;/strong&gt; within a single integrated framework. Unlike conventional RAG approaches that struggle with non-textual elements, our all-in-one system delivers &lt;strong&gt;comprehensive multimodal retrieval capabilities&lt;/strong&gt;.&lt;/p&gt; 
 &lt;p&gt;Users can query documents containing &lt;strong&gt;interleaved text&lt;/strong&gt;, &lt;strong&gt;visual diagrams&lt;/strong&gt;, &lt;strong&gt;structured tables&lt;/strong&gt;, and &lt;strong&gt;mathematical formulations&lt;/strong&gt; through &lt;strong&gt;one cohesive interface&lt;/strong&gt;. This consolidated approach makes RAG-Anything particularly valuable for academic research, technical documentation, financial reports, and enterprise knowledge management where rich, mixed-content documents demand a &lt;strong&gt;unified processing framework&lt;/strong&gt;.&lt;/p&gt; 
 &lt;img src="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/assets/rag_anything_framework.png" alt="RAG-Anything" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;🎯 Key Features&lt;/h3&gt; 
&lt;div style="background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); border-radius: 15px; padding: 25px; margin: 20px 0;"&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;🔄 End-to-End Multimodal Pipeline&lt;/strong&gt; - Complete workflow from document ingestion and parsing to intelligent multimodal query answering&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;📄 Universal Document Support&lt;/strong&gt; - Seamless processing of PDFs, Office documents, images, and diverse file formats&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;🧠 Specialized Content Analysis&lt;/strong&gt; - Dedicated processors for images, tables, mathematical equations, and heterogeneous content types&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;🔗 Multimodal Knowledge Graph&lt;/strong&gt; - Automatic entity extraction and cross-modal relationship discovery for enhanced understanding&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;⚡ Adaptive Processing Modes&lt;/strong&gt; - Flexible MinerU-based parsing or direct multimodal content injection workflows&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;📋 Direct Content List Insertion&lt;/strong&gt; - Bypass document parsing by directly inserting pre-parsed content lists from external sources&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;🎯 Hybrid Intelligent Retrieval&lt;/strong&gt; - Advanced search capabilities spanning textual and multimodal content with contextual understanding&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🏗️ Algorithm &amp;amp; Architecture&lt;/h2&gt; 
&lt;div style="background: linear-gradient(135deg, #0f0f23 0%, #1a1a2e 100%); border-radius: 15px; padding: 25px; margin: 20px 0; border-left: 5px solid #00d9ff;"&gt; 
 &lt;h3&gt;Core Algorithm&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;RAG-Anything&lt;/strong&gt; implements an effective &lt;strong&gt;multi-stage multimodal pipeline&lt;/strong&gt; that fundamentally extends traditional RAG architectures to seamlessly handle diverse content modalities through intelligent orchestration and cross-modal understanding.&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;div style="width: 100%; max-width: 600px; margin: 20px auto; padding: 20px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2);"&gt; 
  &lt;div style="display: flex; justify-content: space-around; align-items: center; flex-wrap: wrap; gap: 20px;"&gt; 
   &lt;div style="text-align: center;"&gt; 
    &lt;div style="font-size: 24px; margin-bottom: 10px;"&gt;
     📄
    &lt;/div&gt; 
    &lt;div style="font-size: 14px; color: #00d9ff;"&gt;
     Document Parsing
    &lt;/div&gt; 
   &lt;/div&gt; 
   &lt;div style="font-size: 20px; color: #00d9ff;"&gt;
    →
   &lt;/div&gt; 
   &lt;div style="text-align: center;"&gt; 
    &lt;div style="font-size: 24px; margin-bottom: 10px;"&gt;
     🧠
    &lt;/div&gt; 
    &lt;div style="font-size: 14px; color: #00d9ff;"&gt;
     Content Analysis
    &lt;/div&gt; 
   &lt;/div&gt; 
   &lt;div style="font-size: 20px; color: #00d9ff;"&gt;
    →
   &lt;/div&gt; 
   &lt;div style="text-align: center;"&gt; 
    &lt;div style="font-size: 24px; margin-bottom: 10px;"&gt;
     🔍
    &lt;/div&gt; 
    &lt;div style="font-size: 14px; color: #00d9ff;"&gt;
     Knowledge Graph
    &lt;/div&gt; 
   &lt;/div&gt; 
   &lt;div style="font-size: 20px; color: #00d9ff;"&gt;
    →
   &lt;/div&gt; 
   &lt;div style="text-align: center;"&gt; 
    &lt;div style="font-size: 24px; margin-bottom: 10px;"&gt;
     🎯
    &lt;/div&gt; 
    &lt;div style="font-size: 14px; color: #00d9ff;"&gt;
     Intelligent Retrieval
    &lt;/div&gt; 
   &lt;/div&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;h3&gt;1. Document Parsing Stage&lt;/h3&gt; 
&lt;div style="background: linear-gradient(90deg, #1a1a2e 0%, #16213e 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #4ecdc4;"&gt; 
 &lt;p&gt;The system provides high-fidelity document extraction through adaptive content decomposition. It intelligently segments heterogeneous elements while preserving contextual relationships. Universal format compatibility is achieved via specialized optimized parsers.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Key Components:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;⚙️ MinerU Integration&lt;/strong&gt;: Leverages &lt;a href="https://github.com/opendatalab/MinerU"&gt;MinerU&lt;/a&gt; for high-fidelity document structure extraction and semantic preservation across complex layouts.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;🧩 Adaptive Content Decomposition&lt;/strong&gt;: Automatically segments documents into coherent text blocks, visual elements, structured tables, mathematical equations, and specialized content types while preserving contextual relationships.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;📁 Universal Format Support&lt;/strong&gt;: Provides comprehensive handling of PDFs, Office documents (DOC/DOCX/PPT/PPTX/XLS/XLSX), images, and emerging formats through specialized parsers with format-specific optimization.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;h3&gt;2. Multi-Modal Content Understanding &amp;amp; Processing&lt;/h3&gt; 
&lt;div style="background: linear-gradient(90deg, #16213e 0%, #0f3460 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #ff6b6b;"&gt; 
 &lt;p&gt;The system automatically categorizes and routes content through optimized channels. It uses concurrent pipelines for parallel text and multimodal processing. Document hierarchy and relationships are preserved during transformation.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Key Components:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;🎯 Autonomous Content Categorization and Routing&lt;/strong&gt;: Automatically identify, categorize, and route different content types through optimized execution channels.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;⚡ Concurrent Multi-Pipeline Architecture&lt;/strong&gt;: Implements concurrent execution of textual and multimodal content through dedicated processing pipelines. This approach maximizes throughput efficiency while preserving content integrity.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;🏗️ Document Hierarchy Extraction&lt;/strong&gt;: Extracts and preserves original document hierarchy and inter-element relationships during content transformation.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;h3&gt;3. Multimodal Analysis Engine&lt;/h3&gt; 
&lt;div style="background: linear-gradient(90deg, #0f3460 0%, #1a1a2e 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #00d9ff;"&gt; 
 &lt;p&gt;The system deploys modality-aware processing units for heterogeneous data modalities:&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Specialized Analyzers:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;🔍 Visual Content Analyzer&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Integrate vision model for image analysis.&lt;/li&gt; 
    &lt;li&gt;Generates context-aware descriptive captions based on visual semantics.&lt;/li&gt; 
    &lt;li&gt;Extracts spatial relationships and hierarchical structures between visual elements.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;📊 Structured Data Interpreter&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Performs systematic interpretation of tabular and structured data formats.&lt;/li&gt; 
    &lt;li&gt;Implements statistical pattern recognition algorithms for data trend analysis.&lt;/li&gt; 
    &lt;li&gt;Identifies semantic relationships and dependencies across multiple tabular datasets.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;📐 Mathematical Expression Parser&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Parses complex mathematical expressions and formulas with high accuracy.&lt;/li&gt; 
    &lt;li&gt;Provides native LaTeX format support for seamless integration with academic workflows.&lt;/li&gt; 
    &lt;li&gt;Establishes conceptual mappings between mathematical equations and domain-specific knowledge bases.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;🔧 Extensible Modality Handler&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Provides configurable processing framework for custom and emerging content types.&lt;/li&gt; 
    &lt;li&gt;Enables dynamic integration of new modality processors through plugin architecture.&lt;/li&gt; 
    &lt;li&gt;Supports runtime configuration of processing pipelines for specialized use cases.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;h3&gt;4. Multimodal Knowledge Graph Index&lt;/h3&gt; 
&lt;div style="background: linear-gradient(90deg, #1a1a2e 0%, #16213e 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #4ecdc4;"&gt; 
 &lt;p&gt;The multi-modal knowledge graph construction module transforms document content into structured semantic representations. It extracts multimodal entities, establishes cross-modal relationships, and preserves hierarchical organization. The system applies weighted relevance scoring for optimized knowledge retrieval.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Core Functions:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;🔍 Multi-Modal Entity Extraction&lt;/strong&gt;: Transforms significant multimodal elements into structured knowledge graph entities. The process includes semantic annotations and metadata preservation.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;🔗 Cross-Modal Relationship Mapping&lt;/strong&gt;: Establishes semantic connections and dependencies between textual entities and multimodal components. This is achieved through automated relationship inference algorithms.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;🏗️ Hierarchical Structure Preservation&lt;/strong&gt;: Maintains original document organization through "belongs_to" relationship chains. These chains preserve logical content hierarchy and sectional dependencies.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;⚖️ Weighted Relationship Scoring&lt;/strong&gt;: Assigns quantitative relevance scores to relationship types. Scoring is based on semantic proximity and contextual significance within the document structure.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;h3&gt;5. Modality-Aware Retrieval&lt;/h3&gt; 
&lt;div style="background: linear-gradient(90deg, #16213e 0%, #0f3460 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #ff6b6b;"&gt; 
 &lt;p&gt;The hybrid retrieval system combines vector similarity search with graph traversal algorithms for comprehensive content retrieval. It implements modality-aware ranking mechanisms and maintains relational coherence between retrieved elements to ensure contextually integrated information delivery.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Retrieval Mechanisms:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;🔀 Vector-Graph Fusion&lt;/strong&gt;: Integrates vector similarity search with graph traversal algorithms. This approach leverages both semantic embeddings and structural relationships for comprehensive content retrieval.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;📊 Modality-Aware Ranking&lt;/strong&gt;: Implements adaptive scoring mechanisms that weight retrieval results based on content type relevance. The system adjusts rankings according to query-specific modality preferences.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;🔗 Relational Coherence Maintenance&lt;/strong&gt;: Maintains semantic and structural relationships between retrieved elements. This ensures coherent information delivery and contextual integrity.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🚀 Quick Start&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Initialize Your AI Journey&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://user-images.githubusercontent.com/74038190/212284158-e840e285-664b-44d7-b79b-e264b5e54825.gif" width="400" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;h4&gt;Option 1: Install from PyPI (Recommended)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Basic installation
pip install raganything

# With optional dependencies for extended format support:
pip install 'raganything[all]'              # All optional features
pip install 'raganything[image]'            # Image format conversion (BMP, TIFF, GIF, WebP)
pip install 'raganything[text]'             # Text file processing (TXT, MD)
pip install 'raganything[image,text]'       # Multiple features
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Option 2: Install from Source&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install uv (if not already installed)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Clone and setup the project with uv
git clone https://github.com/HKUDS/RAG-Anything.git
cd RAG-Anything

# Install the package and dependencies in a virtual environment
uv sync

# If you encounter network timeouts (especially for opencv packages):
# UV_HTTP_TIMEOUT=120 uv sync

# Run commands directly with uv (recommended approach)
uv run python examples/raganything_example.py --help

# Install with optional dependencies
uv sync --extra image --extra text  # Specific extras
uv sync --all-extras                 # All optional features
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Optional Dependencies&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;[image]&lt;/code&gt;&lt;/strong&gt; - Enables processing of BMP, TIFF, GIF, WebP image formats (requires Pillow)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;[text]&lt;/code&gt;&lt;/strong&gt; - Enables processing of TXT and MD files (requires ReportLab)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;[all]&lt;/code&gt;&lt;/strong&gt; - Includes all Python optional dependencies&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;⚠️ Office Document Processing Requirements:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Office documents (.doc, .docx, .ppt, .pptx, .xls, .xlsx) require &lt;strong&gt;LibreOffice&lt;/strong&gt; installation&lt;/li&gt; 
  &lt;li&gt;Download from &lt;a href="https://www.libreoffice.org/download/download/"&gt;LibreOffice official website&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: Download installer from official website&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: &lt;code&gt;brew install --cask libreoffice&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Ubuntu/Debian&lt;/strong&gt;: &lt;code&gt;sudo apt-get install libreoffice&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;CentOS/RHEL&lt;/strong&gt;: &lt;code&gt;sudo yum install libreoffice&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Check MinerU installation:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Verify installation
mineru --version

# Check if properly configured
python -c "from raganything import RAGAnything; rag = RAGAnything(); print('✅ MinerU installed properly' if rag.check_parser_installation() else '❌ MinerU installation issue')"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Models are downloaded automatically on first use. For manual download, refer to &lt;a href="https://github.com/opendatalab/MinerU/raw/master/README.md#22-model-source-configuration"&gt;MinerU Model Source Configuration&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Usage Examples&lt;/h3&gt; 
&lt;h4&gt;1. End-to-End Document Processing&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from raganything import RAGAnything, RAGAnythingConfig
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc

async def main():
    # Set up API configuration
    api_key = "your-api-key"
    base_url = "your-base-url"  # Optional

    # Create RAGAnything configuration
    config = RAGAnythingConfig(
        working_dir="./rag_storage",
        parser="mineru",  # Parser selection: mineru or docling
        parse_method="auto",  # Parse method: auto, ocr, or txt
        enable_image_processing=True,
        enable_table_processing=True,
        enable_equation_processing=True,
    )

    # Define LLM model function
    def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
        return openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        )

    # Define vision model function for image processing
    def vision_model_func(
        prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs
    ):
        # If messages format is provided (for multimodal VLM enhanced query), use it directly
        if messages:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=messages,
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Traditional single image format
        elif image_data:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=[
                    {"role": "system", "content": system_prompt}
                    if system_prompt
                    else None,
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_data}"
                                },
                            },
                        ],
                    }
                    if image_data
                    else {"role": "user", "content": prompt},
                ],
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Pure text format
        else:
            return llm_model_func(prompt, system_prompt, history_messages, **kwargs)

    # Define embedding function
    embedding_func = EmbeddingFunc(
        embedding_dim=3072,
        max_token_size=8192,
        func=lambda texts: openai_embed(
            texts,
            model="text-embedding-3-large",
            api_key=api_key,
            base_url=base_url,
        ),
    )

    # Initialize RAGAnything
    rag = RAGAnything(
        config=config,
        llm_model_func=llm_model_func,
        vision_model_func=vision_model_func,
        embedding_func=embedding_func,
    )

    # Process a document
    await rag.process_document_complete(
        file_path="path/to/your/document.pdf",
        output_dir="./output",
        parse_method="auto"
    )

    # Query the processed content
    # Pure text query - for basic knowledge base search
    text_result = await rag.aquery(
        "What are the main findings shown in the figures and tables?",
        mode="hybrid"
    )
    print("Text query result:", text_result)

    # Multimodal query with specific multimodal content
    multimodal_result = await rag.aquery_with_multimodal(
    "Explain this formula and its relevance to the document content",
    multimodal_content=[{
        "type": "equation",
        "latex": "P(d|q) = \\frac{P(q|d) \\cdot P(d)}{P(q)}",
        "equation_caption": "Document relevance probability"
    }],
    mode="hybrid"
)
    print("Multimodal query result:", multimodal_result)

if __name__ == "__main__":
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;2. Direct Multimodal Content Processing&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from lightrag import LightRAG
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc
from raganything.modalprocessors import ImageModalProcessor, TableModalProcessor

async def process_multimodal_content():
    # Set up API configuration
    api_key = "your-api-key"
    base_url = "your-base-url"  # Optional

    # Initialize LightRAG
    rag = LightRAG(
        working_dir="./rag_storage",
        llm_model_func=lambda prompt, system_prompt=None, history_messages=[], **kwargs: openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        ),
        embedding_func=EmbeddingFunc(
            embedding_dim=3072,
            max_token_size=8192,
            func=lambda texts: openai_embed(
                texts,
                model="text-embedding-3-large",
                api_key=api_key,
                base_url=base_url,
            ),
        )
    )
    await rag.initialize_storages()

    # Process an image
    image_processor = ImageModalProcessor(
        lightrag=rag,
        modal_caption_func=lambda prompt, system_prompt=None, history_messages=[], image_data=None, **kwargs: openai_complete_if_cache(
            "gpt-4o",
            "",
            system_prompt=None,
            history_messages=[],
            messages=[
                {"role": "system", "content": system_prompt} if system_prompt else None,
                {"role": "user", "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
                ]} if image_data else {"role": "user", "content": prompt}
            ],
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        ) if image_data else openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        )
    )

    image_content = {
        "img_path": "path/to/image.jpg",
        "image_caption": ["Figure 1: Experimental results"],
        "image_footnote": ["Data collected in 2024"]
    }

    description, entity_info = await image_processor.process_multimodal_content(
        modal_content=image_content,
        content_type="image",
        file_path="research_paper.pdf",
        entity_name="Experimental Results Figure"
    )

    # Process a table
    table_processor = TableModalProcessor(
        lightrag=rag,
        modal_caption_func=lambda prompt, system_prompt=None, history_messages=[], **kwargs: openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        )
    )

    table_content = {
        "table_body": """
        | Method | Accuracy | F1-Score |
        |--------|----------|----------|
        | RAGAnything | 95.2% | 0.94 |
        | Baseline | 87.3% | 0.85 |
        """,
        "table_caption": ["Performance Comparison"],
        "table_footnote": ["Results on test dataset"]
    }

    description, entity_info = await table_processor.process_multimodal_content(
        modal_content=table_content,
        content_type="table",
        file_path="research_paper.pdf",
        entity_name="Performance Results Table"
    )

if __name__ == "__main__":
    asyncio.run(process_multimodal_content())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3. Batch Processing&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Process multiple documents
await rag.process_folder_complete(
    folder_path="./documents",
    output_dir="./output",
    file_extensions=[".pdf", ".docx", ".pptx"],
    recursive=True,
    max_workers=4
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;4. Custom Modal Processors&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from raganything.modalprocessors import GenericModalProcessor

class CustomModalProcessor(GenericModalProcessor):
    async def process_multimodal_content(self, modal_content, content_type, file_path, entity_name):
        # Your custom processing logic
        enhanced_description = await self.analyze_custom_content(modal_content)
        entity_info = self.create_custom_entity(enhanced_description, entity_name)
        return await self._create_entity_and_chunk(enhanced_description, entity_info, file_path)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;5. Query Options&lt;/h4&gt; 
&lt;p&gt;RAG-Anything provides three types of query methods:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Pure Text Queries&lt;/strong&gt; - Direct knowledge base search using LightRAG:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Different query modes for text queries
text_result_hybrid = await rag.aquery("Your question", mode="hybrid")
text_result_local = await rag.aquery("Your question", mode="local")
text_result_global = await rag.aquery("Your question", mode="global")
text_result_naive = await rag.aquery("Your question", mode="naive")

# Synchronous version
sync_text_result = rag.query("Your question", mode="hybrid")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;VLM Enhanced Queries&lt;/strong&gt; - Automatically analyze images in retrieved context using VLM:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# VLM enhanced query (automatically enabled when vision_model_func is provided)
vlm_result = await rag.aquery(
    "Analyze the charts and figures in the document",
    mode="hybrid"
    # vlm_enhanced=True is automatically set when vision_model_func is available
)

# Manually control VLM enhancement
vlm_enabled = await rag.aquery(
    "What do the images show in this document?",
    mode="hybrid",
    vlm_enhanced=True  # Force enable VLM enhancement
)

vlm_disabled = await rag.aquery(
    "What do the images show in this document?",
    mode="hybrid",
    vlm_enhanced=False  # Force disable VLM enhancement
)

# When documents contain images, VLM can see and analyze them directly
# The system will automatically:
# 1. Retrieve relevant context containing image paths
# 2. Load and encode images as base64
# 3. Send both text context and images to VLM for comprehensive analysis
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Multimodal Queries&lt;/strong&gt; - Enhanced queries with specific multimodal content analysis:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Query with table data
table_result = await rag.aquery_with_multimodal(
    "Compare these performance metrics with the document content",
    multimodal_content=[{
        "type": "table",
        "table_data": """Method,Accuracy,Speed
                        RAGAnything,95.2%,120ms
                        Traditional,87.3%,180ms""",
        "table_caption": "Performance comparison"
    }],
    mode="hybrid"
)

# Query with equation content
equation_result = await rag.aquery_with_multimodal(
    "Explain this formula and its relevance to the document content",
    multimodal_content=[{
        "type": "equation",
        "latex": "P(d|q) = \\frac{P(q|d) \\cdot P(d)}{P(q)}",
        "equation_caption": "Document relevance probability"
    }],
    mode="hybrid"
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;6. Loading Existing LightRAG Instance&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from raganything import RAGAnything, RAGAnythingConfig
from lightrag import LightRAG
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.kg.shared_storage import initialize_pipeline_status
from lightrag.utils import EmbeddingFunc
import os

async def load_existing_lightrag():
    # Set up API configuration
    api_key = "your-api-key"
    base_url = "your-base-url"  # Optional

    # First, create or load existing LightRAG instance
    lightrag_working_dir = "./existing_lightrag_storage"

    # Check if previous LightRAG instance exists
    if os.path.exists(lightrag_working_dir) and os.listdir(lightrag_working_dir):
        print("✅ Found existing LightRAG instance, loading...")
    else:
        print("❌ No existing LightRAG instance found, will create new one")

    # Create/load LightRAG instance with your configuration
    lightrag_instance = LightRAG(
        working_dir=lightrag_working_dir,
        llm_model_func=lambda prompt, system_prompt=None, history_messages=[], **kwargs: openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        ),
        embedding_func=EmbeddingFunc(
            embedding_dim=3072,
            max_token_size=8192,
            func=lambda texts: openai_embed(
                texts,
                model="text-embedding-3-large",
                api_key=api_key,
                base_url=base_url,
            ),
        )
    )

    # Initialize storage (this will load existing data if available)
    await lightrag_instance.initialize_storages()
    await initialize_pipeline_status()

    # Define vision model function for image processing
    def vision_model_func(
        prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs
    ):
        # If messages format is provided (for multimodal VLM enhanced query), use it directly
        if messages:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=messages,
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Traditional single image format
        elif image_data:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=[
                    {"role": "system", "content": system_prompt}
                    if system_prompt
                    else None,
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_data}"
                                },
                            },
                        ],
                    }
                    if image_data
                    else {"role": "user", "content": prompt},
                ],
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Pure text format
        else:
            return lightrag_instance.llm_model_func(prompt, system_prompt, history_messages, **kwargs)

    # Now use existing LightRAG instance to initialize RAGAnything
    rag = RAGAnything(
        lightrag=lightrag_instance,  # Pass existing LightRAG instance
        vision_model_func=vision_model_func,
        # Note: working_dir, llm_model_func, embedding_func, etc. are inherited from lightrag_instance
    )

    # Query existing knowledge base
    result = await rag.aquery(
        "What data has been processed in this LightRAG instance?",
        mode="hybrid"
    )
    print("Query result:", result)

    # Add new multimodal document to existing LightRAG instance
    await rag.process_document_complete(
        file_path="path/to/new/multimodal_document.pdf",
        output_dir="./output"
    )

if __name__ == "__main__":
    asyncio.run(load_existing_lightrag())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;7. Direct Content List Insertion&lt;/h4&gt; 
&lt;p&gt;For scenarios where you already have a pre-parsed content list (e.g., from external parsers or previous processing), you can directly insert it into RAGAnything without document parsing:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from raganything import RAGAnything, RAGAnythingConfig
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc

async def insert_content_list_example():
    # Set up API configuration
    api_key = "your-api-key"
    base_url = "your-base-url"  # Optional

    # Create RAGAnything configuration
    config = RAGAnythingConfig(
        working_dir="./rag_storage",
        enable_image_processing=True,
        enable_table_processing=True,
        enable_equation_processing=True,
    )

    # Define model functions
    def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
        return openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        )

    def vision_model_func(prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs):
        # If messages format is provided (for multimodal VLM enhanced query), use it directly
        if messages:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=messages,
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Traditional single image format
        elif image_data:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=[
                    {"role": "system", "content": system_prompt} if system_prompt else None,
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
                        ],
                    } if image_data else {"role": "user", "content": prompt},
                ],
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Pure text format
        else:
            return llm_model_func(prompt, system_prompt, history_messages, **kwargs)

    embedding_func = EmbeddingFunc(
        embedding_dim=3072,
        max_token_size=8192,
        func=lambda texts: openai_embed(
            texts,
            model="text-embedding-3-large",
            api_key=api_key,
            base_url=base_url,
        ),
    )

    # Initialize RAGAnything
    rag = RAGAnything(
        config=config,
        llm_model_func=llm_model_func,
        vision_model_func=vision_model_func,
        embedding_func=embedding_func,
    )

    # Example: Pre-parsed content list from external source
    content_list = [
        {
            "type": "text",
            "text": "This is the introduction section of our research paper.",
            "page_idx": 0  # Page number where this content appears
        },
        {
            "type": "image",
            "img_path": "/absolute/path/to/figure1.jpg",  # IMPORTANT: Use absolute path
            "image_caption": ["Figure 1: System Architecture"],
            "image_footnote": ["Source: Authors' original design"],
            "page_idx": 1  # Page number where this image appears
        },
        {
            "type": "table",
            "table_body": "| Method | Accuracy | F1-Score |\n|--------|----------|----------|\n| Ours | 95.2% | 0.94 |\n| Baseline | 87.3% | 0.85 |",
            "table_caption": ["Table 1: Performance Comparison"],
            "table_footnote": ["Results on test dataset"],
            "page_idx": 2  # Page number where this table appears
        },
        {
            "type": "equation",
            "latex": "P(d|q) = \\frac{P(q|d) \\cdot P(d)}{P(q)}",
            "text": "Document relevance probability formula",
            "page_idx": 3  # Page number where this equation appears
        },
        {
            "type": "text",
            "text": "In conclusion, our method demonstrates superior performance across all metrics.",
            "page_idx": 4  # Page number where this content appears
        }
    ]

    # Insert the content list directly
    await rag.insert_content_list(
        content_list=content_list,
        file_path="research_paper.pdf",  # Reference file name for citation
        split_by_character=None,         # Optional text splitting
        split_by_character_only=False,   # Optional text splitting mode
        doc_id=None,                     # Optional custom document ID (will be auto-generated if not provided)
        display_stats=True               # Show content statistics
    )

    # Query the inserted content
    result = await rag.aquery(
        "What are the key findings and performance metrics mentioned in the research?",
        mode="hybrid"
    )
    print("Query result:", result)

    # You can also insert multiple content lists with different document IDs
    another_content_list = [
        {
            "type": "text",
            "text": "This is content from another document.",
            "page_idx": 0  # Page number where this content appears
        },
        {
            "type": "table",
            "table_body": "| Feature | Value |\n|---------|-------|\n| Speed | Fast |\n| Accuracy | High |",
            "table_caption": ["Feature Comparison"],
            "page_idx": 1  # Page number where this table appears
        }
    ]

    await rag.insert_content_list(
        content_list=another_content_list,
        file_path="another_document.pdf",
        doc_id="custom-doc-id-123"  # Custom document ID
    )

if __name__ == "__main__":
    asyncio.run(insert_content_list_example())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Content List Format:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;content_list&lt;/code&gt; should follow the standard format with each item being a dictionary containing:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Text content&lt;/strong&gt;: &lt;code&gt;{"type": "text", "text": "content text", "page_idx": 0}&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Image content&lt;/strong&gt;: &lt;code&gt;{"type": "image", "img_path": "/absolute/path/to/image.jpg", "image_caption": ["caption"], "image_footnote": ["note"], "page_idx": 1}&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Table content&lt;/strong&gt;: &lt;code&gt;{"type": "table", "table_body": "markdown table", "table_caption": ["caption"], "table_footnote": ["note"], "page_idx": 2}&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Equation content&lt;/strong&gt;: &lt;code&gt;{"type": "equation", "latex": "LaTeX formula", "text": "description", "page_idx": 3}&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Generic content&lt;/strong&gt;: &lt;code&gt;{"type": "custom_type", "content": "any content", "page_idx": 4}&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Important Notes:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;img_path&lt;/code&gt;&lt;/strong&gt;: Must be an absolute path to the image file (e.g., &lt;code&gt;/home/user/images/chart.jpg&lt;/code&gt; or &lt;code&gt;C:\Users\user\images\chart.jpg&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;page_idx&lt;/code&gt;&lt;/strong&gt;: Represents the page number where the content appears in the original document (0-based indexing)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Content ordering&lt;/strong&gt;: Items are processed in the order they appear in the list&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This method is particularly useful when:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You have content from external parsers (non-MinerU/Docling)&lt;/li&gt; 
 &lt;li&gt;You want to process programmatically generated content&lt;/li&gt; 
 &lt;li&gt;You need to insert content from multiple sources into a single knowledge base&lt;/li&gt; 
 &lt;li&gt;You have cached parsing results that you want to reuse&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🛠️ Examples&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Practical Implementation Demos&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://user-images.githubusercontent.com/74038190/212257455-13e3e01e-d6a6-45dc-bb92-3ab87b12dfc1.gif" width="300" /&gt; 
&lt;/div&gt; 
&lt;p&gt;The &lt;code&gt;examples/&lt;/code&gt; directory contains comprehensive usage examples:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;raganything_example.py&lt;/code&gt;&lt;/strong&gt;: End-to-end document processing with MinerU&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;modalprocessors_example.py&lt;/code&gt;&lt;/strong&gt;: Direct multimodal content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;office_document_test.py&lt;/code&gt;&lt;/strong&gt;: Office document parsing test with MinerU (no API key required)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;image_format_test.py&lt;/code&gt;&lt;/strong&gt;: Image format parsing test with MinerU (no API key required)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;text_format_test.py&lt;/code&gt;&lt;/strong&gt;: Text format parsing test with MinerU (no API key required)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Run examples:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# End-to-end processing with parser selection
python examples/raganything_example.py path/to/document.pdf --api-key YOUR_API_KEY --parser mineru

# Direct modal processing
python examples/modalprocessors_example.py --api-key YOUR_API_KEY

# Office document parsing test (MinerU only)
python examples/office_document_test.py --file path/to/document.docx

# Image format parsing test (MinerU only)
python examples/image_format_test.py --file path/to/image.bmp

# Text format parsing test (MinerU only)
python examples/text_format_test.py --file path/to/document.md

# Check LibreOffice installation
python examples/office_document_test.py --check-libreoffice --file dummy

# Check PIL/Pillow installation
python examples/image_format_test.py --check-pillow --file dummy

# Check ReportLab installation
python examples/text_format_test.py --check-reportlab --file dummy
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🔧 Configuration&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;System Optimization Parameters&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Environment Variables&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file (refer to &lt;code&gt;.env.example&lt;/code&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OPENAI_API_KEY=your_openai_api_key
OPENAI_BASE_URL=your_base_url  # Optional
OUTPUT_DIR=./output             # Default output directory for parsed documents
PARSER=mineru                   # Parser selection: mineru or docling
PARSE_METHOD=auto              # Parse method: auto, ocr, or txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For backward compatibility, legacy environment variable names are still supported:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;MINERU_PARSE_METHOD&lt;/code&gt; is deprecated, please use &lt;code&gt;PARSE_METHOD&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: API keys are only required for full RAG processing with LLM integration. The parsing test files (&lt;code&gt;office_document_test.py&lt;/code&gt; and &lt;code&gt;image_format_test.py&lt;/code&gt;) only test parser functionality and do not require API keys.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Parser Configuration&lt;/h3&gt; 
&lt;p&gt;RAGAnything now supports multiple parsers, each with specific advantages:&lt;/p&gt; 
&lt;h4&gt;MinerU Parser&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports PDF, images, Office documents, and more formats&lt;/li&gt; 
 &lt;li&gt;Powerful OCR and table extraction capabilities&lt;/li&gt; 
 &lt;li&gt;GPU acceleration support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Docling Parser&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Optimized for Office documents and HTML files&lt;/li&gt; 
 &lt;li&gt;Better document structure preservation&lt;/li&gt; 
 &lt;li&gt;Native support for multiple Office formats&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;MinerU Configuration&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# MinerU 2.0 uses command-line parameters instead of config files
# Check available options:
mineru --help

# Common configurations:
mineru -p input.pdf -o output_dir -m auto    # Automatic parsing mode
mineru -p input.pdf -o output_dir -m ocr     # OCR-focused parsing
mineru -p input.pdf -o output_dir -b pipeline --device cuda  # GPU acceleration
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also configure parsing through RAGAnything parameters:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Basic parsing configuration with parser selection
await rag.process_document_complete(
    file_path="document.pdf",
    output_dir="./output/",
    parse_method="auto",          # or "ocr", "txt"
    parser="mineru"               # Optional: "mineru" or "docling"
)

# Advanced parsing configuration with special parameters
await rag.process_document_complete(
    file_path="document.pdf",
    output_dir="./output/",
    parse_method="auto",          # Parsing method: "auto", "ocr", "txt"
    parser="mineru",              # Parser selection: "mineru" or "docling"

    # MinerU special parameters - all supported kwargs:
    lang="ch",                   # Document language for OCR optimization (e.g., "ch", "en", "ja")
    device="cuda:0",             # Inference device: "cpu", "cuda", "cuda:0", "npu", "mps"
    start_page=0,                # Starting page number (0-based, for PDF)
    end_page=10,                 # Ending page number (0-based, for PDF)
    formula=True,                # Enable formula parsing
    table=True,                  # Enable table parsing
    backend="pipeline",          # Parsing backend: pipeline|vlm-transformers|vlm-sglang-engine|vlm-sglang-client.
    source="huggingface",        # Model source: "huggingface", "modelscope", "local"
    # vlm_url="http://127.0.0.1:3000" # Service address when using backend=vlm-sglang-client

    # Standard RAGAnything parameters
    display_stats=True,          # Display content statistics
    split_by_character=None,     # Optional character to split text by
    doc_id=None                  # Optional document ID
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: MinerU 2.0 no longer uses the &lt;code&gt;magic-pdf.json&lt;/code&gt; configuration file. All settings are now passed as command-line parameters or function arguments. RAG-Anything now supports multiple document parsers - you can choose between MinerU and Docling based on your needs.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Processing Requirements&lt;/h3&gt; 
&lt;p&gt;Different content types require specific optional dependencies:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Office Documents&lt;/strong&gt; (.doc, .docx, .ppt, .pptx, .xls, .xlsx): Install &lt;a href="https://www.libreoffice.org/download/download/"&gt;LibreOffice&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extended Image Formats&lt;/strong&gt; (.bmp, .tiff, .gif, .webp): Install with &lt;code&gt;pip install raganything[image]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Text Files&lt;/strong&gt; (.txt, .md): Install with &lt;code&gt;pip install raganything[text]&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;📋 Quick Install&lt;/strong&gt;: Use &lt;code&gt;pip install raganything[all]&lt;/code&gt; to enable all format support (Python dependencies only - LibreOffice still needs separate installation)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🧪 Supported Content Types&lt;/h2&gt; 
&lt;h3&gt;Document Formats&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;PDFs&lt;/strong&gt; - Research papers, reports, presentations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Office Documents&lt;/strong&gt; - DOC, DOCX, PPT, PPTX, XLS, XLSX&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Images&lt;/strong&gt; - JPG, PNG, BMP, TIFF, GIF, WebP&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Text Files&lt;/strong&gt; - TXT, MD&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Multimodal Elements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Images&lt;/strong&gt; - Photographs, diagrams, charts, screenshots&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tables&lt;/strong&gt; - Data tables, comparison charts, statistical summaries&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Equations&lt;/strong&gt; - Mathematical formulas in LaTeX format&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Generic Content&lt;/strong&gt; - Custom content types via extensible processors&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;For installation of format-specific dependencies, see the &lt;a href="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/#-configuration"&gt;Configuration&lt;/a&gt; section.&lt;/em&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;📖 Citation&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Academic Reference&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;div style="width: 60px; height: 60px; margin: 20px auto; position: relative;"&gt; 
  &lt;div style="width: 100%; height: 100%; border: 2px solid #00d9ff; border-radius: 50%; position: relative;"&gt; 
   &lt;div style="position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%); font-size: 24px; color: #00d9ff;"&gt;
    📖
   &lt;/div&gt; 
  &lt;/div&gt; 
  &lt;div style="position: absolute; bottom: -5px; left: 50%; transform: translateX(-50%); width: 20px; height: 20px; background: white; border-right: 2px solid #00d9ff; border-bottom: 2px solid #00d9ff; transform: rotate(45deg);"&gt;&lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;p&gt;If you find RAG-Anything useful in your research, please cite our paper:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{guo2025raganythingallinoneragframework,
      title={RAG-Anything: All-in-One RAG Framework},
      author={Zirui Guo and Xubin Ren and Lingrui Xu and Jiahao Zhang and Chao Huang},
      year={2025},
      eprint={2510.12323},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2510.12323},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🔗 Related Projects&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Ecosystem &amp;amp; Extensions&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center"&gt; &lt;a href="https://github.com/HKUDS/LightRAG"&gt; 
      &lt;div style="width: 100px; height: 100px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2); display: flex; align-items: center; justify-content: center; margin-bottom: 10px;"&gt; 
       &lt;span style="font-size: 32px;"&gt;⚡&lt;/span&gt; 
      &lt;/div&gt; &lt;b&gt;LightRAG&lt;/b&gt;&lt;br /&gt; &lt;sub&gt;Simple and Fast RAG&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; 
    &lt;td align="center"&gt; &lt;a href="https://github.com/HKUDS/VideoRAG"&gt; 
      &lt;div style="width: 100px; height: 100px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2); display: flex; align-items: center; justify-content: center; margin-bottom: 10px;"&gt; 
       &lt;span style="font-size: 32px;"&gt;🎥&lt;/span&gt; 
      &lt;/div&gt; &lt;b&gt;VideoRAG&lt;/b&gt;&lt;br /&gt; &lt;sub&gt;Extreme Long-Context Video RAG&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; 
    &lt;td align="center"&gt; &lt;a href="https://github.com/HKUDS/MiniRAG"&gt; 
      &lt;div style="width: 100px; height: 100px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2); display: flex; align-items: center; justify-content: center; margin-bottom: 10px;"&gt; 
       &lt;span style="font-size: 32px;"&gt;✨&lt;/span&gt; 
      &lt;/div&gt; &lt;b&gt;MiniRAG&lt;/b&gt;&lt;br /&gt; &lt;sub&gt;Extremely Simple RAG&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;⭐ Star History&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Community Growth Trajectory&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://star-history.com/#HKUDS/RAG-Anything&amp;amp;Date"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=HKUDS/RAG-Anything&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=HKUDS/RAG-Anything&amp;amp;type=Date" /&gt; 
   &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=HKUDS/RAG-Anything&amp;amp;type=Date" style="border-radius: 15px; box-shadow: 0 0 30px rgba(0, 217, 255, 0.3);" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🤝 Contribution&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Join the Innovation&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt;
  We thank all our contributors for their valuable contributions. 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/HKUDS/RAG-Anything/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=HKUDS/RAG-Anything" style="border-radius: 15px; box-shadow: 0 0 20px rgba(0, 217, 255, 0.3);" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center" style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; padding: 30px; margin: 30px 0;"&gt; 
 &lt;div&gt; 
  &lt;img src="https://user-images.githubusercontent.com/74038190/212284100-561aa473-3905-4a80-b561-0d28506553ee.gif" width="500" /&gt; 
 &lt;/div&gt; 
 &lt;div style="margin-top: 20px;"&gt; 
  &lt;a href="https://github.com/HKUDS/RAG-Anything" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/⭐%20Star%20us%20on%20GitHub-1a1a2e?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" /&gt; &lt;/a&gt; 
  &lt;a href="https://github.com/HKUDS/RAG-Anything/issues" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/🐛%20Report%20Issues-ff6b6b?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" /&gt; &lt;/a&gt; 
  &lt;a href="https://github.com/HKUDS/RAG-Anything/discussions" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/💬%20Discussions-4ecdc4?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" /&gt; &lt;/a&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;div style="width: 100%; max-width: 600px; margin: 20px auto; padding: 20px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2);"&gt; 
  &lt;div style="display: flex; justify-content: center; align-items: center; gap: 15px;"&gt; 
   &lt;span style="font-size: 24px;"&gt;⭐&lt;/span&gt; 
   &lt;span style="color: #00d9ff; font-size: 18px;"&gt;Thank you for visiting RAG-Anything!&lt;/span&gt; 
   &lt;span style="font-size: 24px;"&gt;⭐&lt;/span&gt; 
  &lt;/div&gt; 
  &lt;div style="margin-top: 10px; color: #00d9ff; font-size: 16px;"&gt;
   Building the Future of Multimodal AI
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>pandas-dev/pandas</title>
      <link>https://github.com/pandas-dev/pandas</link>
      <description>&lt;p&gt;Flexible and powerful data analysis / manipulation library for Python, providing labeled data structures similar to R data.frame objects, statistical functions, and much more&lt;/p&gt;&lt;hr&gt;&lt;picture align="center"&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="https://pandas.pydata.org/static/img/pandas_white.svg" /&gt; 
 &lt;img alt="Pandas Logo" src="https://pandas.pydata.org/static/img/pandas.svg?sanitize=true" /&gt; 
&lt;/picture&gt; 
&lt;hr /&gt; 
&lt;h1&gt;pandas: A Powerful Python Data Analysis Toolkit&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Testing&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/pandas-dev/pandas/actions/workflows/unit-tests.yml"&gt;&lt;img src="https://github.com/pandas-dev/pandas/actions/workflows/unit-tests.yml/badge.svg?sanitize=true" alt="CI - Test" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/pandas-dev/pandas"&gt;&lt;img src="https://codecov.io/github/pandas-dev/pandas/coverage.svg?branch=main" alt="Coverage" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Package&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://pypi.org/project/pandas/"&gt;&lt;img src="https://img.shields.io/pypi/v/pandas.svg?sanitize=true" alt="PyPI Latest Release" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/pandas/"&gt;&lt;img src="https://img.shields.io/pypi/dm/pandas.svg?label=PyPI%20downloads" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://anaconda.org/conda-forge/pandas"&gt;&lt;img src="https://anaconda.org/conda-forge/pandas/badges/version.svg?sanitize=true" alt="Conda Latest Release" /&gt;&lt;/a&gt; &lt;a href="https://anaconda.org/conda-forge/pandas"&gt;&lt;img src="https://img.shields.io/conda/dn/conda-forge/pandas.svg?label=Conda%20downloads" alt="Conda Downloads" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Meta&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://numfocus.org"&gt;&lt;img src="https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&amp;amp;colorA=E1523D&amp;amp;colorB=007D8A" alt="Powered by NumFOCUS" /&gt;&lt;/a&gt; &lt;a href="https://doi.org/10.5281/zenodo.3509134"&gt;&lt;img src="https://zenodo.org/badge/DOI/10.5281/zenodo.3509134.svg?sanitize=true" alt="DOI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/pandas-dev/pandas/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/pypi/l/pandas.svg?sanitize=true" alt="License - BSD 3-Clause" /&gt;&lt;/a&gt; &lt;a href="https://pandas.pydata.org/docs/dev/development/community.html?highlight=slack#community-slack"&gt;&lt;img src="https://img.shields.io/badge/join_Slack-information-brightgreen.svg?logo=slack" alt="Slack" /&gt;&lt;/a&gt; &lt;a href="https://insights.linuxfoundation.org/project/pandas-dev-pandas"&gt;&lt;img src="https://insights.linuxfoundation.org/api/badge/health-score?project=pandas-dev-pandas" alt="LFX Health Score" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;What is it?&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;pandas&lt;/strong&gt; is a Python package that provides fast, flexible, and expressive data structures designed to make working with "relational" or "labeled" data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, &lt;strong&gt;real-world&lt;/strong&gt; data analysis in Python. Additionally, it has the broader goal of becoming &lt;strong&gt;the most powerful and flexible open-source data analysis/manipulation tool available in any language&lt;/strong&gt;. It is already well on its way towards this goal.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pandas-dev/pandas/main/#main-features"&gt;Main Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pandas-dev/pandas/main/#where-to-get-it"&gt;Where to get it&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pandas-dev/pandas/main/#dependencies"&gt;Dependencies&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pandas-dev/pandas/main/#installation-from-sources"&gt;Installation from sources&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pandas-dev/pandas/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pandas-dev/pandas/main/#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pandas-dev/pandas/main/#background"&gt;Background&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pandas-dev/pandas/main/#getting-help"&gt;Getting Help&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pandas-dev/pandas/main/#discussion-and-development"&gt;Discussion and Development&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pandas-dev/pandas/main/#contributing-to-pandas"&gt;Contributing to pandas&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Main Features&lt;/h2&gt; 
&lt;p&gt;Here are just a few of the things that pandas does well:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Easy handling of &lt;a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html"&gt;&lt;strong&gt;missing data&lt;/strong&gt;&lt;/a&gt; (represented as &lt;code&gt;NaN&lt;/code&gt;, &lt;code&gt;NA&lt;/code&gt;, or &lt;code&gt;NaT&lt;/code&gt;) in floating point as well as non-floating point data&lt;/li&gt; 
 &lt;li&gt;Size mutability: columns can be &lt;a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#column-selection-addition-deletion"&gt;&lt;strong&gt;inserted and deleted&lt;/strong&gt;&lt;/a&gt; from DataFrame and higher dimensional objects&lt;/li&gt; 
 &lt;li&gt;Automatic and explicit &lt;a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html?highlight=alignment#intro-to-data-structures"&gt;&lt;strong&gt;data alignment&lt;/strong&gt;&lt;/a&gt;: objects can be explicitly aligned to a set of labels, or the user can simply ignore the labels and let &lt;code&gt;Series&lt;/code&gt;, &lt;code&gt;DataFrame&lt;/code&gt;, etc. automatically align the data for you in computations&lt;/li&gt; 
 &lt;li&gt;Powerful, flexible &lt;a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#group-by-split-apply-combine"&gt;&lt;strong&gt;group by&lt;/strong&gt;&lt;/a&gt; functionality to perform split-apply-combine operations on data sets, for both aggregating and transforming data&lt;/li&gt; 
 &lt;li&gt;Make it &lt;a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#dataframe"&gt;&lt;strong&gt;easy to convert&lt;/strong&gt;&lt;/a&gt; ragged, differently-indexed data in other Python and NumPy data structures into DataFrame objects&lt;/li&gt; 
 &lt;li&gt;Intelligent label-based &lt;a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#slicing-ranges"&gt;&lt;strong&gt;slicing&lt;/strong&gt;&lt;/a&gt;, &lt;a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html#advanced"&gt;&lt;strong&gt;fancy indexing&lt;/strong&gt;&lt;/a&gt;, and &lt;a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing"&gt;&lt;strong&gt;subsetting&lt;/strong&gt;&lt;/a&gt; of large data sets&lt;/li&gt; 
 &lt;li&gt;Intuitive &lt;a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#database-style-dataframe-or-named-series-joining-merging"&gt;&lt;strong&gt;merging&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#joining-on-index"&gt;&lt;strong&gt;joining&lt;/strong&gt;&lt;/a&gt; data sets&lt;/li&gt; 
 &lt;li&gt;Flexible &lt;a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html"&gt;&lt;strong&gt;reshaping&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html"&gt;&lt;strong&gt;pivoting&lt;/strong&gt;&lt;/a&gt; of data sets&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#hierarchical-indexing-multiindex"&gt;&lt;strong&gt;Hierarchical&lt;/strong&gt;&lt;/a&gt; labeling of axes (possible to have multiple labels per tick)&lt;/li&gt; 
 &lt;li&gt;Robust I/O tools for loading data from &lt;a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#csv-text-files"&gt;&lt;strong&gt;flat files&lt;/strong&gt;&lt;/a&gt; (CSV and delimited), &lt;a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#excel-files"&gt;&lt;strong&gt;Excel files&lt;/strong&gt;&lt;/a&gt;, &lt;a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#sql-queries"&gt;&lt;strong&gt;databases&lt;/strong&gt;&lt;/a&gt;, and saving/loading data from the ultrafast &lt;a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#hdf5-pytables"&gt;&lt;strong&gt;HDF5 format&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#time-series-date-functionality"&gt;&lt;strong&gt;Time series&lt;/strong&gt;&lt;/a&gt;-specific functionality: date range generation and frequency conversion, moving window statistics, date shifting and lagging&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Where to get it&lt;/h2&gt; 
&lt;p&gt;The source code is currently hosted on GitHub at: &lt;a href="https://github.com/pandas-dev/pandas"&gt;https://github.com/pandas-dev/pandas&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Binary installers for the latest released version are available at the &lt;a href="https://pypi.org/project/pandas"&gt;Python Package Index (PyPI)&lt;/a&gt; and on &lt;a href="https://anaconda.org/conda-forge/pandas"&gt;Conda&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# conda
conda install -c conda-forge pandas
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# or PyPI
pip install pandas
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The list of changes to pandas between each release can be found &lt;a href="https://pandas.pydata.org/pandas-docs/stable/whatsnew/index.html"&gt;here&lt;/a&gt;. For full details, see the commit logs at &lt;a href="https://github.com/pandas-dev/pandas"&gt;https://github.com/pandas-dev/pandas&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Dependencies&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.numpy.org"&gt;NumPy - Adds support for large, multi-dimensional arrays, matrices and high-level mathematical functions to operate on these arrays&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://dateutil.readthedocs.io/en/stable/index.html"&gt;python-dateutil - Provides powerful extensions to the standard datetime module&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://tzdata.readthedocs.io/en/latest/"&gt;tzdata - Provides an IANA time zone database&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the &lt;a href="https://pandas.pydata.org/pandas-docs/stable/install.html#dependencies"&gt;full installation instructions&lt;/a&gt; for minimum supported versions of required, recommended and optional dependencies.&lt;/p&gt; 
&lt;h2&gt;Installation from sources&lt;/h2&gt; 
&lt;p&gt;To install pandas from source you need &lt;a href="https://cython.org/"&gt;Cython&lt;/a&gt; in addition to the normal dependencies above. Cython can be installed from PyPI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install cython
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In the &lt;code&gt;pandas&lt;/code&gt; directory (same one where you found this file after cloning the git repo), execute:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or for installing in &lt;a href="https://pip.pypa.io/en/latest/cli/pip_install/#install-editable"&gt;development mode&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python -m pip install -ve . --no-build-isolation --config-settings editable-verbose=true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the full instructions for &lt;a href="https://pandas.pydata.org/docs/dev/development/contributing_environment.html"&gt;installing from source&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/pandas-dev/pandas/main/LICENSE"&gt;BSD 3&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;The official documentation is hosted on &lt;a href="https://pandas.pydata.org/pandas-docs/stable/"&gt;PyData.org&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Background&lt;/h2&gt; 
&lt;p&gt;Work on &lt;code&gt;pandas&lt;/code&gt; started at &lt;a href="https://www.aqr.com/"&gt;AQR&lt;/a&gt; (a quantitative hedge fund) in 2008 and has been under active development since then.&lt;/p&gt; 
&lt;h2&gt;Getting Help&lt;/h2&gt; 
&lt;p&gt;For usage questions, the best place to go to is &lt;a href="https://stackoverflow.com/questions/tagged/pandas"&gt;Stack Overflow&lt;/a&gt;. Further, general questions and discussions can also take place on the &lt;a href="https://groups.google.com/forum/?fromgroups#!forum/pydata"&gt;pydata mailing list&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Discussion and Development&lt;/h2&gt; 
&lt;p&gt;Most development discussions take place on GitHub in this repo, via the &lt;a href="https://github.com/pandas-dev/pandas/issues"&gt;GitHub issue tracker&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Further, the &lt;a href="https://mail.python.org/mailman/listinfo/pandas-dev"&gt;pandas-dev mailing list&lt;/a&gt; can also be used for specialized discussions or design issues, and a &lt;a href="https://pandas.pydata.org/docs/dev/development/community.html?highlight=slack#community-slack"&gt;Slack channel&lt;/a&gt; is available for quick development related questions.&lt;/p&gt; 
&lt;p&gt;There are also frequent &lt;a href="https://pandas.pydata.org/docs/dev/development/community.html#community-meeting"&gt;community meetings&lt;/a&gt; for project maintainers open to the community as well as monthly &lt;a href="https://pandas.pydata.org/docs/dev/development/community.html#new-contributor-meeting"&gt;new contributor meetings&lt;/a&gt; to help support new contributors.&lt;/p&gt; 
&lt;p&gt;Additional information on the communication channels can be found on the &lt;a href="https://pandas.pydata.org/docs/development/community.html"&gt;contributor community&lt;/a&gt; page.&lt;/p&gt; 
&lt;h2&gt;Contributing to pandas&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.codetriage.com/pandas-dev/pandas"&gt;&lt;img src="https://www.codetriage.com/pandas-dev/pandas/badges/users.svg?sanitize=true" alt="Open Source Helpers" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;All contributions, bug reports, bug fixes, documentation improvements, enhancements, and ideas are welcome.&lt;/p&gt; 
&lt;p&gt;A detailed overview on how to contribute can be found in the &lt;strong&gt;&lt;a href="https://pandas.pydata.org/docs/dev/development/contributing.html"&gt;contributing guide&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;If you are simply looking to start working with the pandas codebase, navigate to the &lt;a href="https://github.com/pandas-dev/pandas/issues"&gt;GitHub "issues" tab&lt;/a&gt; and start looking through interesting issues. There are a number of issues listed under &lt;a href="https://github.com/pandas-dev/pandas/issues?q=is%3Aissue%20state%3Aopen%20label%3ADocs%20sort%3Aupdated-desc"&gt;Docs&lt;/a&gt; and &lt;a href="https://github.com/pandas-dev/pandas/issues?q=is%3Aissue%20state%3Aopen%20label%3A%22good%20first%20issue%22%20sort%3Aupdated-desc"&gt;good first issue&lt;/a&gt; where you could start out.&lt;/p&gt; 
&lt;p&gt;You can also triage issues which may include reproducing bug reports, or asking for vital information such as version numbers or reproduction instructions. If you would like to start triaging issues, one easy way to get started is to &lt;a href="https://www.codetriage.com/pandas-dev/pandas"&gt;subscribe to pandas on CodeTriage&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Or maybe through using pandas you have an idea of your own or are looking for something in the documentation and thinking ‘this can be improved’...you can do something about it!&lt;/p&gt; 
&lt;p&gt;Feel free to ask questions on the &lt;a href="https://groups.google.com/forum/?fromgroups#!forum/pydata"&gt;mailing list&lt;/a&gt; or on &lt;a href="https://pandas.pydata.org/docs/dev/development/community.html?highlight=slack#community-slack"&gt;Slack&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;As contributors and maintainers to this project, you are expected to abide by pandas' code of conduct. More information can be found at: &lt;a href="https://github.com/pandas-dev/.github/raw/master/CODE_OF_CONDUCT.md"&gt;Contributor Code of Conduct&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/pandas-dev/pandas/main/#table-of-contents"&gt;Go to Top&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>polarsource/polar</title>
      <link>https://github.com/polarsource/polar</link>
      <description>&lt;p&gt;Turn your software into a business.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://polar.sh"&gt; &lt;img src="https://github.com/user-attachments/assets/89a588e5-0c58-429a-8bbe-20f70af41372" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://www.producthunt.com/posts/polar-5?embed=true&amp;amp;utm_source=badge-top-post-badge&amp;amp;utm_medium=badge&amp;amp;utm_souce=badge-polar-5" target="_blank"&gt;&lt;img src="https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=484271&amp;amp;theme=dark&amp;amp;period=daily" alt="Polar - An open source monetization platform for developers | Product Hunt" style="width: 250px; height: 54px;" width="250" height="54" /&gt;&lt;/a&gt; &lt;a href="https://www.producthunt.com/posts/polar-5?embed=true&amp;amp;utm_source=badge-top-post-topic-badge&amp;amp;utm_medium=badge&amp;amp;utm_souce=badge-polar-5" target="_blank"&gt;&lt;img src="https://api.producthunt.com/widgets/embed-image/v1/top-post-topic-badge.svg?post_id=484271&amp;amp;theme=dark&amp;amp;period=monthly&amp;amp;topic_id=267" alt="Polar - An open source monetization platform for developers | Product Hunt" style="width: 250px; height: 54px;" width="250" height="54" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://polar.sh"&gt;Website&lt;/a&gt; &lt;span&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp;&lt;/span&gt; &lt;a href="https://polar.sh/blog"&gt;Blog&lt;/a&gt; &lt;span&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp;&lt;/span&gt; &lt;a href="https://polar.sh/docs"&gt;Docs&lt;/a&gt; &lt;span&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp;&lt;/span&gt; &lt;a href="https://polar.sh/docs/api-reference"&gt;API Reference&lt;/a&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://discord.gg/Pnhfz3UThd"&gt; &lt;img src="https://img.shields.io/badge/chat-on%20discord-7289DA.svg?sanitize=true" alt="Discord Chat" /&gt; &lt;/a&gt; &lt;a href="https://twitter.com/intent/follow?screen_name=polar_sh"&gt; &lt;img src="https://img.shields.io/twitter/follow/polar_sh.svg?label=Follow%20@polar_sh" alt="Follow @polar_sh" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Polar: Open Source payments infrastructure for the 21st century&lt;/h2&gt; 
&lt;p&gt;Focus on building your passion, while we focus on the infrastructure to get you paid.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Sell SaaS and digital products in minutes&lt;/li&gt; 
 &lt;li&gt;All-in-one funding &amp;amp; monetization platform for developers.&lt;/li&gt; 
 &lt;li&gt;Sell access to GitHub repositories, Discord Support channels, File Downloads, License Keys &amp;amp; much more with Digital Products &amp;amp; Subscriptions.&lt;/li&gt; 
 &lt;li&gt;We're the merchant of record handling the... 
  &lt;ul&gt; 
   &lt;li&gt;...boilerplate (billing, receipts, customer accounts etc)&lt;/li&gt; 
   &lt;li&gt;...headaches (sales tax, VAT)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Pricing&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;4% + 40¢&lt;/li&gt; 
 &lt;li&gt;No fixed monthly costs&lt;/li&gt; 
 &lt;li&gt;Additional fees may apply. &lt;a href="https://polar.sh/docs/documentation/polar-as-merchant-of-record/fees"&gt;Read more&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Roadmap, Issues &amp;amp; Feature Requests&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;🎯 Upcoming milestones.&lt;/strong&gt; &lt;a href="https://github.com/polarsource/polar/issues/3242"&gt;Check out what we're building towards&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;💬 Shape the future of Polar with us.&lt;/strong&gt; &lt;a href="https://discord.gg/Pnhfz3UThd"&gt;Join our Discord&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;🐛 Found a bug?&lt;/strong&gt; &lt;a href="https://github.com/polarsource/polar/issues"&gt;Submit it here&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;🔓 Found a security vulnerability?&lt;/strong&gt; We greatly appreciate responsible and private disclosures. See &lt;a href="https://raw.githubusercontent.com/polarsource/polar/main/SECURITY.md"&gt;Security&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Polar API &amp;amp; SDK&lt;/h3&gt; 
&lt;p&gt;You can integrate Polar on your docs, sites or services using our &lt;a href="https://polar.sh/docs/api-reference"&gt;Public API&lt;/a&gt; and &lt;a href="https://polar.sh/docs/integrate/webhooks/endpointsendpoints"&gt;Webhook API&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We also maintain SDKs for the following languages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;JavaScript (Node.js and browsers): &lt;a href="https://github.com/polarsource/polar-js"&gt;polarsource/polar-js&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Python: &lt;a href="https://github.com/polarsource/polar-python"&gt;polarsource/polar-python&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributions&lt;/h2&gt; 
&lt;p&gt;Our &lt;a href="https://raw.githubusercontent.com/polarsource/polar/main/DEVELOPMENT.md"&gt;&lt;code&gt;DEVELOPMENT.md&lt;/code&gt;&lt;/a&gt; file contains everything you need to know to configure your development environment.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Want to get started quickly? Use GitHub Codespaces.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://codespaces.new/polarsource/polar?machine=standardLinux32gb"&gt;&lt;img src="https://github.com/codespaces/badge.svg?sanitize=true" alt="Open in GitHub Codespaces" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Contributors&lt;/h3&gt; 
&lt;a href="https://github.com/polarsource/polar/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=polarsource/polar" /&gt; &lt;/a&gt; 
&lt;h2&gt;Monorepo&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/polarsource/polar/main/server/README.md"&gt;server&lt;/a&gt;&lt;/strong&gt; – Python / FastAPI / Dramatiq / SQLAlchemy (PostgreSQL) / Redis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/polarsource/polar/main/clients/README.md"&gt;clients&lt;/a&gt;&lt;/strong&gt; – Turborepo 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/polarsource/polar/main/clients/apps/web"&gt;web&lt;/a&gt; (Dashboard) – NextJS (TypeScript)&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/polarsource/polar/main/clients/packages/polarkit"&gt;polarkit&lt;/a&gt; - Shared React components&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;sub&gt;♥️🙏 To our &lt;code&gt;pyproject.toml&lt;/code&gt; friends: &lt;a href="https://github.com/tiangolo/fastapi"&gt;FastAPI&lt;/a&gt;, &lt;a href="https://github.com/pydantic/pydantic"&gt;Pydantic&lt;/a&gt;, &lt;a href="https://github.com/Bogdanp/dramatiq"&gt;Dramatiq&lt;/a&gt;, &lt;a href="https://github.com/sqlalchemy/sqlalchemy"&gt;SQLAlchemy&lt;/a&gt;, &lt;a href="https://github.com/yanyongyu/githubkit"&gt;Githubkit&lt;/a&gt;, &lt;a href="https://github.com/sysid/sse-starlette"&gt;sse-starlette&lt;/a&gt;, &lt;a href="https://github.com/encode/uvicorn"&gt;Uvicorn&lt;/a&gt;, &lt;a href="https://github.com/frankie567/httpx-oauth"&gt;httpx-oauth&lt;/a&gt;, &lt;a href="https://github.com/pallets/jinja"&gt;jinja&lt;/a&gt;, &lt;a href="https://github.com/pallets-eco/blinker"&gt;blinker&lt;/a&gt;, &lt;a href="https://github.com/jpadilla/pyjwt"&gt;pyjwt&lt;/a&gt;, &lt;a href="https://github.com/getsentry/sentry"&gt;Sentry&lt;/a&gt; + more&lt;/sub&gt;&lt;br /&gt; &lt;sub&gt;♥️🙏 To our &lt;code&gt;package.json&lt;/code&gt; friends: &lt;a href="https://github.com/vercel/next.js/"&gt;Next.js&lt;/a&gt;, &lt;a href="https://github.com/TanStack/query"&gt;TanStack Query&lt;/a&gt;, &lt;a href="https://github.com/tailwindlabs/tailwindcss"&gt;tailwindcss&lt;/a&gt;, &lt;a href="https://github.com/ferdikoomen/openapi-typescript-codegen"&gt;openapi-typescript-codegen&lt;/a&gt;, &lt;a href="https://github.com/axios/axios"&gt;axios&lt;/a&gt;, &lt;a href="https://github.com/radix-ui/primitives"&gt;radix-ui&lt;/a&gt;, &lt;a href="https://github.com/pacocoursey/cmdk"&gt;cmdk&lt;/a&gt;, &lt;a href="https://github.com/framer/motion"&gt;framer-motion&lt;/a&gt; + more&lt;/sub&gt;&lt;br /&gt; &lt;sub&gt;♥️🙏 To &lt;a href="https://ipinfo.io"&gt;IPinfo&lt;/a&gt; that provides IP address data to help us geolocate customers during checkout.&lt;/sub&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Licensed under &lt;a href="https://www.apache.org/licenses/LICENSE-2.0"&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>allenai/olmocr</title>
      <link>https://github.com/allenai/olmocr</link>
      <description>&lt;p&gt;Toolkit for linearizing PDFs for LLM datasets/training&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;!-- &lt;img src="https://github.com/allenai/OLMo/assets/8812459/774ac485-a535-4768-8f7c-db7be20f5cc3" width="300"/&gt; --&gt; 
 &lt;img src="https://github.com/user-attachments/assets/d70c8644-3e64-4230-98c3-c52fddaeccb6" alt="olmOCR Logo" width="300" /&gt; 
 &lt;hr /&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/allenai/OLMo/raw/main/LICENSE"&gt; &lt;img alt="GitHub License" src="https://img.shields.io/github/license/allenai/OLMo" /&gt; &lt;/a&gt; &lt;a href="https://github.com/allenai/olmocr/releases"&gt; &lt;img alt="GitHub release" src="https://img.shields.io/github/release/allenai/olmocr.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;a href="https://olmocr.allenai.org/papers/olmocr.pdf"&gt; &lt;img alt="Tech Report" src="https://img.shields.io/badge/Paper-olmOCR-blue" /&gt; &lt;/a&gt; &lt;a href="https://olmocr.allenai.org"&gt; &lt;img alt="Demo" src="https://img.shields.io/badge/Ai2-Demo-F0529C" /&gt; &lt;/a&gt; &lt;a href="https://discord.gg/sZq3jTNVNG"&gt; &lt;img alt="Discord" src="https://img.shields.io/badge/Discord%20-%20blue?style=flat&amp;amp;logo=discord&amp;amp;label=Ai2&amp;amp;color=%235B65E9" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;A toolkit for converting PDFs and other image-based document formats into clean, readable, plain text format.&lt;/p&gt; 
&lt;p&gt;Try the online demo: &lt;a href="https://olmocr.allenai.org/"&gt;https://olmocr.allenai.org/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Convert PDF, PNG, and JPEG based documents into clean Markdown&lt;/li&gt; 
 &lt;li&gt;Support for equations, tables, handwriting, and complex formatting&lt;/li&gt; 
 &lt;li&gt;Automatically removes headers and footers&lt;/li&gt; 
 &lt;li&gt;Convert into text with a natural reading order, even in the presence of figures, multi-column layouts, and insets&lt;/li&gt; 
 &lt;li&gt;Efficient, less than $200 USD per million pages converted&lt;/li&gt; 
 &lt;li&gt;(Based on a 7B parameter VLM, so it requires a GPU)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;News&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;August 13, 2025 - v0.3.0 - &lt;a href="https://huggingface.co/allenai/olmOCR-7B-0825-FP8"&gt;New model release&lt;/a&gt;, fixes auto-rotation detection, and hallucinations on blank documents.&lt;/li&gt; 
 &lt;li&gt;July 24, 2025 - v0.2.1 - &lt;a href="https://huggingface.co/allenai/olmOCR-7B-0725-FP8"&gt;New model release&lt;/a&gt;, scores 3 points higher on &lt;a href="https://github.com/allenai/olmocr/tree/main/olmocr/bench"&gt;olmOCR-Bench&lt;/a&gt;, also runs significantly faster because it's default FP8, and needs much fewer retries per document.&lt;/li&gt; 
 &lt;li&gt;July 23, 2025 - v0.2.0 - New cleaned up &lt;a href="https://github.com/allenai/olmocr/tree/main/olmocr/train"&gt;trainer code&lt;/a&gt;, makes it much simpler to train olmOCR models yourself.&lt;/li&gt; 
 &lt;li&gt;June 17, 2025 - v0.1.75 - Switch from sglang to vllm based inference pipeline, updated docker image to CUDA 12.8.&lt;/li&gt; 
 &lt;li&gt;May 23, 2025 - v0.1.70 - Official docker support and images are now available! &lt;a href="https://raw.githubusercontent.com/allenai/olmocr/main/#using-docker"&gt;See Docker usage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;May 19, 2025 - v0.1.68 - &lt;a href="https://github.com/allenai/olmocr/tree/main/olmocr/bench"&gt;olmOCR-Bench&lt;/a&gt; launch, scoring 77.4. Launch includes 2 point performance boost in olmOCR pipeline due to bug fixes with prompts.&lt;/li&gt; 
 &lt;li&gt;Mar 17, 2025 - v0.1.60 - Performance improvements due to better temperature selection in sampling.&lt;/li&gt; 
 &lt;li&gt;Feb 25, 2025 - v0.1.58 - Initial public launch and demo.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Benchmark&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/allenai/olmocr/tree/main/olmocr/bench"&gt;&lt;strong&gt;olmOCR-Bench&lt;/strong&gt;&lt;/a&gt;: We also ship a comprehensive benchmark suite covering over 7,000 test cases across 1,400 documents to help measure performance of OCR systems.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;ArXiv&lt;/th&gt; 
   &lt;th align="center"&gt;Old Scans Math&lt;/th&gt; 
   &lt;th align="center"&gt;Tables&lt;/th&gt; 
   &lt;th align="center"&gt;Old Scans&lt;/th&gt; 
   &lt;th align="center"&gt;Headers and Footers&lt;/th&gt; 
   &lt;th align="center"&gt;Multi column&lt;/th&gt; 
   &lt;th align="center"&gt;Long tiny text&lt;/th&gt; 
   &lt;th align="center"&gt;Base&lt;/th&gt; 
   &lt;th align="center"&gt;Overall&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Marker v1.7.5 (base, force_ocr)&lt;/td&gt; 
   &lt;td align="center"&gt;76.0&lt;/td&gt; 
   &lt;td align="center"&gt;57.9&lt;/td&gt; 
   &lt;td align="center"&gt;57.6&lt;/td&gt; 
   &lt;td align="center"&gt;27.8&lt;/td&gt; 
   &lt;td align="center"&gt;84.9&lt;/td&gt; 
   &lt;td align="center"&gt;72.9&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;84.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;99.1&lt;/td&gt; 
   &lt;td align="center"&gt;70.1 ± 1.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MinerU v1.3.10&lt;/td&gt; 
   &lt;td align="center"&gt;75.4&lt;/td&gt; 
   &lt;td align="center"&gt;47.4&lt;/td&gt; 
   &lt;td align="center"&gt;60.9&lt;/td&gt; 
   &lt;td align="center"&gt;17.3&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;96.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;59.0&lt;/td&gt; 
   &lt;td align="center"&gt;39.1&lt;/td&gt; 
   &lt;td align="center"&gt;96.6&lt;/td&gt; 
   &lt;td align="center"&gt;61.5 ± 1.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Mistral OCR API&lt;/td&gt; 
   &lt;td align="center"&gt;77.2&lt;/td&gt; 
   &lt;td align="center"&gt;67.5&lt;/td&gt; 
   &lt;td align="center"&gt;60.6&lt;/td&gt; 
   &lt;td align="center"&gt;29.3&lt;/td&gt; 
   &lt;td align="center"&gt;93.6&lt;/td&gt; 
   &lt;td align="center"&gt;71.3&lt;/td&gt; 
   &lt;td align="center"&gt;77.1&lt;/td&gt; 
   &lt;td align="center"&gt;99.4&lt;/td&gt; 
   &lt;td align="center"&gt;72.0 ± 1.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;olmOCR v0.1.75 (Anchored)&lt;/td&gt; 
   &lt;td align="center"&gt;74.9&lt;/td&gt; 
   &lt;td align="center"&gt;71.2&lt;/td&gt; 
   &lt;td align="center"&gt;71.0&lt;/td&gt; 
   &lt;td align="center"&gt;42.2&lt;/td&gt; 
   &lt;td align="center"&gt;94.5&lt;/td&gt; 
   &lt;td align="center"&gt;78.3&lt;/td&gt; 
   &lt;td align="center"&gt;73.3&lt;/td&gt; 
   &lt;td align="center"&gt;98.3&lt;/td&gt; 
   &lt;td align="center"&gt;75.5 ± 1.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;olmOCR v0.2.0&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;78.8&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;77.5&lt;/td&gt; 
   &lt;td align="center"&gt;71.9&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;45.4&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;94.2&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;78.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;81.4&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;99.8&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;78.5 ± 1.1&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;olmOCR v0.3.0&lt;/td&gt; 
   &lt;td align="center"&gt;78.6&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;79.9&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;72.9&lt;/td&gt; 
   &lt;td align="center"&gt;43.9&lt;/td&gt; 
   &lt;td align="center"&gt;95.1&lt;/td&gt; 
   &lt;td align="center"&gt;77.3&lt;/td&gt; 
   &lt;td align="center"&gt;81.2&lt;/td&gt; 
   &lt;td align="center"&gt;98.9&lt;/td&gt; 
   &lt;td align="center"&gt;78.5 ± 1.1&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;Requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Recent NVIDIA GPU (tested on RTX 4090, L40S, A100, H100) with at least 15 GB of GPU RAM&lt;/li&gt; 
 &lt;li&gt;30GB of free disk space&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You will need to install poppler-utils and additional fonts for rendering PDF images.&lt;/p&gt; 
&lt;p&gt;Install dependencies (Ubuntu/Debian)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update
sudo apt-get install poppler-utils ttf-mscorefonts-installer msttcorefonts fonts-crosextra-caladea fonts-crosextra-carlito gsfonts lcdf-typetools
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Set up a conda environment and install olmocr. The requirements for running olmOCR are difficult to install in an existing python environment, so please do make a clean python environment to install into.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n olmocr python=3.11
conda activate olmocr

# For CPU-only operations, ex running the benchmark
pip install olmocr[bench]

# For actually converting the files with your own GPU
pip install olmocr[gpu]  --extra-index-url https://download.pytorch.org/whl/cu128

# Recommended: Install flash infer for faster inference on GPU
pip install https://download.pytorch.org/whl/cu128/flashinfer/flashinfer_python-0.2.5%2Bcu128torch2.7-cp38-abi3-linux_x86_64.whl
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Local Usage Example&lt;/h3&gt; 
&lt;p&gt;For quick testing, try the &lt;a href="https://olmocr.allen.ai/"&gt;web demo&lt;/a&gt;. To run locally, a GPU is required, as inference is powered by &lt;a href="https://github.com/sgl-project/sglang"&gt;sglang&lt;/a&gt; under the hood.&lt;/p&gt; 
&lt;p&gt;Convert a Single PDF:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Download a sample PDF
curl -o olmocr-sample.pdf https://olmocr.allenai.org/papers/olmocr_3pg_sample.pdf

# Convert it to markdown
python -m olmocr.pipeline ./localworkspace --markdown --pdfs olmocr-sample.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Convert an Image file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m olmocr.pipeline ./localworkspace --markdown --pdfs random_page.png
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Convert Multiple PDFs:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m olmocr.pipeline ./localworkspace --markdown --pdfs tests/gnarly_pdfs/*.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With the addition of the &lt;code&gt;--markdown&lt;/code&gt; flag, results will be stored as markdown files inside of &lt;code&gt;./localworkspace/markdown/&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Using External vLLM Server&lt;/h3&gt; 
&lt;p&gt;If you have a vLLM server already running elsewhere (or any inference platform implementing the relevant subset of the OpenAI API), you can point olmOCR to use it instead of spawning a local instance:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Use external vLLM server instead of local one
python -m olmocr.pipeline ./localworkspace --server http://remote-server:8000 --markdown --pdfs tests/gnarly_pdfs/*.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The served model name should be &lt;code&gt;olmocr&lt;/code&gt;. An example vLLM launch command would be:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vllm serve allenai/olmOCR-7B-0825-FP8 --served-model-name olmocr --max-model-len 16384
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run olmOCR with the DeepInfra server endpoint:&lt;/h4&gt; 
&lt;p&gt;Signup at &lt;a href="https://deepinfra.com/"&gt;DeepInfra&lt;/a&gt; and get your API key from the DeepInfra dashboard. Store the API key as an environment variable.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export DEEPINFRA_API_KEY="your-api-key-here"
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m olmocr.pipeline ./localworkspace \
  --server https://api.deepinfra.com/v1/openai \
  --api_key $DEEPINFRA_API_KEY \
  --pages_per_group 100 \
  --model allenai/olmOCR-7B-0825 \
  --markdown \
  --pdfs path/to/your/*.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--server&lt;/code&gt;: DeepInfra's OpenAI-compatible endpoint: &lt;code&gt;https://api.deepinfra.com/v1/openai&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--api_key&lt;/code&gt;: Your DeepInfra API key&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--pages_per_group&lt;/code&gt;: You may want a smaller number of pages per group as many external provides have lower concurrent request limits&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model&lt;/code&gt;: The model identifier on DeepInfra: &lt;code&gt;allenai/olmOCR-7B-0825&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Other arguments work the same as with local inference&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Viewing Results&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;./localworkspace/&lt;/code&gt; workspace folder will then have both &lt;a href="https://github.com/allenai/dolma"&gt;Dolma&lt;/a&gt; and markdown files (if using &lt;code&gt;--markdown&lt;/code&gt;).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cat localworkspace/markdown/olmocr-sample.md 
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Multi-node / Cluster Usage&lt;/h3&gt; 
&lt;p&gt;If you want to convert millions of PDFs, using multiple nodes running in parallel, then olmOCR supports reading your PDFs from AWS S3, and coordinating work using an AWS S3 output bucket.&lt;/p&gt; 
&lt;p&gt;For example, you can start this command on your first worker node, and it will set up a simple work queue in your AWS bucket and start converting PDFs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace --pdfs s3://my_s3_bucket/jakep/gnarly_pdfs/*.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now on any subsequent nodes, just run this and they will start grabbing items from the same workspace queue.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are at Ai2 and want to linearize millions of PDFs efficiently using &lt;a href="https://www.beaker.org"&gt;beaker&lt;/a&gt;, just add the &lt;code&gt;--beaker&lt;/code&gt; flag. This will prepare the workspace on your local machine, and then launch N GPU workers in the cluster to start converting PDFs.&lt;/p&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace --pdfs s3://my_s3_bucket/jakep/gnarly_pdfs/*.pdf --beaker --beaker_gpus 4
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using Docker&lt;/h3&gt; 
&lt;p&gt;Pull the Docker image.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker pull alleninstituteforai/olmocr:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run the container interactively:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -it --gpus all --name olmocr_container alleninstituteforai/olmocr:latest /bin/bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to access your local files inside the container, use volume mounting:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -it --gpus all \
  -v /path/to/your/local/files:/local_files \
  --name olmocr_container \
  alleninstituteforai/olmocr:latest /bin/bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;All dependencies are already installed. Once you’re inside the container, you can run olmOCR commands. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -o olmocr-sample.pdf https://olmocr.allenai.org/papers/olmocr_3pg_sample.pdf

python -m olmocr.pipeline ./localworkspace --markdown --pdfs olmocr-sample.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;You can also visit our Docker repository on &lt;a href="https://hub.docker.com/r/alleninstituteforai/olmocr"&gt;Docker Hub&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Full documentation for the pipeline&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m olmocr.pipeline --help
usage: pipeline.py [-h] [--pdfs [PDFS ...]] [--model MODEL] [--workspace_profile WORKSPACE_PROFILE] [--pdf_profile PDF_PROFILE] [--pages_per_group PAGES_PER_GROUP] [--max_page_retries MAX_PAGE_RETRIES] [--max_page_error_rate MAX_PAGE_ERROR_RATE] [--workers WORKERS]
                   [--apply_filter] [--stats] [--markdown] [--target_longest_image_dim TARGET_LONGEST_IMAGE_DIM] [--target_anchor_text_len TARGET_ANCHOR_TEXT_LEN] [--guided_decoding] [--gpu-memory-utilization GPU_MEMORY_UTILIZATION] [--max_model_len MAX_MODEL_LEN]
                   [--tensor-parallel-size TENSOR_PARALLEL_SIZE] [--data-parallel-size DATA_PARALLEL_SIZE] [--port PORT] [--server SERVER] [--beaker] [--beaker_workspace BEAKER_WORKSPACE] [--beaker_cluster BEAKER_CLUSTER] [--beaker_gpus BEAKER_GPUS] [--beaker_priority BEAKER_PRIORITY]
                   workspace

Manager for running millions of PDFs through a batch inference pipeline

positional arguments:
  workspace             The filesystem path where work will be stored, can be a local folder, or an s3 path if coordinating work with many workers, s3://bucket/prefix/

options:
  -h, --help            show this help message and exit
  --pdfs [PDFS ...]     Path to add pdfs stored in s3 to the workspace, can be a glob path s3://bucket/prefix/*.pdf or path to file containing list of pdf paths
  --model MODEL         Path where the model is located, allenai/olmOCR-7B-0725-FP8 is the default, can be local, s3, or hugging face.
  --workspace_profile WORKSPACE_PROFILE
                        S3 configuration profile for accessing the workspace
  --pdf_profile PDF_PROFILE
                        S3 configuration profile for accessing the raw pdf documents
  --pages_per_group PAGES_PER_GROUP
                        Aiming for this many pdf pages per work item group
  --max_page_retries MAX_PAGE_RETRIES
                        Max number of times we will retry rendering a page
  --max_page_error_rate MAX_PAGE_ERROR_RATE
                        Rate of allowable failed pages in a document, 1/250 by default
  --workers WORKERS     Number of workers to run at a time
  --apply_filter        Apply basic filtering to English pdfs which are not forms, and not likely seo spam
  --stats               Instead of running any job, reports some statistics about the current workspace
  --markdown            Also write natural text to markdown files preserving the folder structure of the input pdfs
  --target_longest_image_dim TARGET_LONGEST_IMAGE_DIM
                        Dimension on longest side to use for rendering the pdf pages
  --target_anchor_text_len TARGET_ANCHOR_TEXT_LEN
                        Maximum amount of anchor text to use (characters), not used for new models
  --guided_decoding     Enable guided decoding for model YAML type outputs

VLLM arguments:
  --gpu-memory-utilization GPU_MEMORY_UTILIZATION
                        Fraction of VRAM vLLM may pre-allocate for KV-cache (passed through to vllm serve).
  --max_model_len MAX_MODEL_LEN
                        Upper bound (tokens) vLLM will allocate KV-cache for, lower if VLLM won't start
  --tensor-parallel-size TENSOR_PARALLEL_SIZE, -tp TENSOR_PARALLEL_SIZE
                        Tensor parallel size for vLLM
  --data-parallel-size DATA_PARALLEL_SIZE, -dp DATA_PARALLEL_SIZE
                        Data parallel size for vLLM
  --port PORT           Port to use for the VLLM server
  --server SERVER       URL of external vLLM (or other compatible provider)
                        server (e.g., http://hostname:port). If provided,
                        skips spawning local vLLM instance

beaker/cluster execution:
  --beaker              Submit this job to beaker instead of running locally
  --beaker_workspace BEAKER_WORKSPACE
                        Beaker workspace to submit to
  --beaker_cluster BEAKER_CLUSTER
                        Beaker clusters you want to run on
  --beaker_gpus BEAKER_GPUS
                        Number of gpu replicas to run
  --beaker_priority BEAKER_PRIORITY
                        Beaker priority level for the job
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Code overview&lt;/h2&gt; 
&lt;p&gt;There are some nice reusable pieces of the code that may be useful for your own projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A prompting strategy to get really good natural text parsing using ChatGPT 4o - &lt;a href="https://github.com/allenai/olmocr/raw/main/olmocr/data/buildsilver.py"&gt;buildsilver.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;An side-by-side eval toolkit for comparing different pipeline versions - &lt;a href="https://github.com/allenai/olmocr/raw/main/olmocr/eval/runeval.py"&gt;runeval.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Basic filtering by language and SEO spam removal - &lt;a href="https://github.com/allenai/olmocr/raw/main/olmocr/filter/filter.py"&gt;filter.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Finetuning code for Qwen2-VL and Molmo-O - &lt;a href="https://github.com/allenai/olmocr/raw/main/olmocr/train/train.py"&gt;train.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Processing millions of PDFs through a finetuned model using Sglang - &lt;a href="https://github.com/allenai/olmocr/raw/main/olmocr/pipeline.py"&gt;pipeline.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Viewing &lt;a href="https://github.com/allenai/dolma"&gt;Dolma docs&lt;/a&gt; created from PDFs - &lt;a href="https://github.com/allenai/olmocr/raw/main/olmocr/viewer/dolmaviewer.py"&gt;dolmaviewer.py&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Team&lt;/h2&gt; 
&lt;!-- start team --&gt; 
&lt;p&gt;&lt;strong&gt;olmOCR&lt;/strong&gt; is developed and maintained by the AllenNLP team, backed by &lt;a href="https://allenai.org/"&gt;the Allen Institute for Artificial Intelligence (AI2)&lt;/a&gt;. AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering. To learn more about who specifically contributed to this codebase, see &lt;a href="https://github.com/allenai/olmocr/graphs/contributors"&gt;our contributors&lt;/a&gt; page.&lt;/p&gt; 
&lt;!-- end team --&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;!-- start license --&gt; 
&lt;p&gt;&lt;strong&gt;olmOCR&lt;/strong&gt; is licensed under &lt;a href="https://www.apache.org/licenses/LICENSE-2.0"&gt;Apache 2.0&lt;/a&gt;. A full copy of the license can be found &lt;a href="https://github.com/allenai/olmocr/raw/main/LICENSE"&gt;on GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- end license --&gt; 
&lt;h2&gt;Citing&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{olmocr,
      title={{olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models}},
      author={Jake Poznanski and Jon Borchardt and Jason Dunkelberger and Regan Huff and Daniel Lin and Aman Rangapur and Christopher Wilhelm and Kyle Lo and Luca Soldaini},
      year={2025},
      eprint={2502.18443},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.18443},
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>ccxt/ccxt</title>
      <link>https://github.com/ccxt/ccxt</link>
      <description>&lt;p&gt;A cryptocurrency trading API with more than 100 exchanges in JavaScript / TypeScript / Python / C# / PHP / Go&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CCXT – CryptoCurrency eXchange Trading Library&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://www.npmjs.com/package/ccxt"&gt;&lt;img src="https://img.shields.io/npm/dy/ccxt.svg?sanitize=true" alt="NPM Downloads" /&gt;&lt;/a&gt; &lt;a href="https://npmjs.com/package/ccxt"&gt;&lt;img src="https://img.shields.io/npm/v/ccxt.svg?sanitize=true" alt="npm" /&gt;&lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/ccxt"&gt;&lt;img src="https://img.shields.io/pypi/v/ccxt.svg?sanitize=true" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://www.nuget.org/packages/ccxt"&gt;&lt;img src="https://img.shields.io/nuget/v/ccxt" alt="NuGet version" /&gt;&lt;/a&gt; &lt;a href="https://godoc.org/github.com/ccxt/ccxt/go/v4"&gt;&lt;img src="https://pkg.go.dev/badge/github.com/ccxt/ccxt/go/v4?utm_source=godoc" alt="GoDoc" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/ccxt"&gt;&lt;img src="https://img.shields.io/discord/690203284119617602?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://github.com/ccxt/ccxt/wiki/Exchange-Markets"&gt;&lt;img src="https://img.shields.io/badge/exchanges-105-blue.svg?sanitize=true" alt="Supported Exchanges" /&gt;&lt;/a&gt; &lt;a href="https://x.com/ccxt_official"&gt;&lt;img src="https://img.shields.io/twitter/follow/ccxt_official.svg?style=social&amp;amp;label=CCXT" alt="Follow CCXT at x.com" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;A cryptocurrency trading API with more than 100 exchanges in JavaScript / TypeScript / Python / C# / PHP / Go.&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://raw.githubusercontent.com/ccxt/ccxt/master/#install"&gt;Install&lt;/a&gt; · &lt;a href="https://raw.githubusercontent.com/ccxt/ccxt/master/#usage"&gt;Usage&lt;/a&gt; · &lt;a href="https://github.com/ccxt/ccxt/wiki"&gt;Manual&lt;/a&gt; · &lt;a href="https://github.com/ccxt/ccxt/wiki/FAQ"&gt;FAQ&lt;/a&gt; · &lt;a href="https://github.com/ccxt/ccxt/tree/master/examples"&gt;Examples&lt;/a&gt; · &lt;a href="https://github.com/ccxt/ccxt/raw/master/CONTRIBUTING.md"&gt;Contributing&lt;/a&gt; · &lt;a href="https://raw.githubusercontent.com/ccxt/ccxt/master/#disclaimer"&gt;Disclaimer&lt;/a&gt; · &lt;a href="https://raw.githubusercontent.com/ccxt/ccxt/master/#social"&gt;Social&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;The &lt;strong&gt;CCXT&lt;/strong&gt; library is used to connect and trade with cryptocurrency exchanges and payment processing services worldwide. It provides quick access to market data for storage, analysis, visualization, indicator development, algorithmic trading, strategy backtesting, bot programming, and related software engineering.&lt;/p&gt; 
&lt;p&gt;It is intended to be used by &lt;strong&gt;coders, developers, technically-skilled traders, data-scientists and financial analysts&lt;/strong&gt; for building trading algorithms.&lt;/p&gt; 
&lt;p&gt;Current feature list:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;support for many cryptocurrency exchanges — more coming soon&lt;/li&gt; 
 &lt;li&gt;fully implemented public and private APIs&lt;/li&gt; 
 &lt;li&gt;optional normalized data for cross-exchange analytics and arbitrage&lt;/li&gt; 
 &lt;li&gt;an out of the box unified API that is extremely easy to integrate&lt;/li&gt; 
 &lt;li&gt;works in Node 10.4+, Python 3, PHP 8.1+, netstandard2.0/2.1, Go 1.20+ and web browsers&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;See Also&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;sub&gt;&lt;a href="https://tab-trader.com/?utm_source=ccxt"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/66755907-9c3e8880-eea1-11e9-846e-0bff349ceb87.png" alt="TabTrader" /&gt;&lt;/a&gt;&lt;/sub&gt; &lt;strong&gt;&lt;a href="https://tab-trader.com/?utm_source=ccxt"&gt;TabTrader&lt;/a&gt;&lt;/strong&gt; – trading on all exchanges in one app. Available on &lt;strong&gt;&lt;a href="https://play.google.com/store/apps/details?id=com.tabtrader.android&amp;amp;referrer=utm_source%3Dccxt"&gt;Android&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href="https://itunes.apple.com/app/apple-store/id1095716562?mt=8"&gt;iOS&lt;/a&gt;&lt;/strong&gt;!&lt;/li&gt; 
 &lt;li&gt;&lt;sub&gt;&lt;a href="https://www.freqtrade.io"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/114340585-8e35fa80-9b60-11eb-860f-4379125e2db6.png" alt="Freqtrade" /&gt;&lt;/a&gt;&lt;/sub&gt; &lt;strong&gt;&lt;a href="https://www.freqtrade.io"&gt;Freqtrade&lt;/a&gt;&lt;/strong&gt; – leading opensource cryptocurrency algorithmic trading software!&lt;/li&gt; 
 &lt;li&gt;&lt;sub&gt;&lt;a href="https://www.octobot.online"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/132113722-007fc092-7530-4b41-b929-b8ed380b7b2e.png" alt="OctoBot" /&gt;&lt;/a&gt;&lt;/sub&gt; &lt;strong&gt;&lt;a href="https://www.octobot.online"&gt;OctoBot&lt;/a&gt;&lt;/strong&gt; – cryptocurrency trading bot with an advanced web interface.&lt;/li&gt; 
 &lt;li&gt;&lt;sub&gt;&lt;a href="https://tokenbot.com/?utm_source=github&amp;amp;utm_medium=ccxt&amp;amp;utm_campaign=algodevs"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/152720975-0522b803-70f0-4f18-a305-3c99b37cd990.png" alt="TokenBot" /&gt;&lt;/a&gt;&lt;/sub&gt; &lt;strong&gt;&lt;a href="https://tokenbot.com/?utm_source=github&amp;amp;utm_medium=ccxt&amp;amp;utm_campaign=algodevs"&gt;TokenBot&lt;/a&gt;&lt;/strong&gt; – discover and copy the best algorithmic traders in the world.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Certified Cryptocurrency Exchanges&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;logo&lt;/th&gt; 
   &lt;th&gt;id&lt;/th&gt; 
   &lt;th&gt;name&lt;/th&gt; 
   &lt;th align="center"&gt;ver&lt;/th&gt; 
   &lt;th&gt;type&lt;/th&gt; 
   &lt;th&gt;certified&lt;/th&gt; 
   &lt;th align="center"&gt;pro&lt;/th&gt; 
   &lt;th&gt;discount&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;&lt;img src="https://github.com/user-attachments/assets/e9419b93-ccb0-46aa-9bff-c883f096274b" alt="binance" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;binance&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;Binance&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://developers.binance.com/en"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d10%25&amp;amp;color=orange" alt="Sign up with Binance using CCXT's referral link for a 10% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;&lt;img src="https://github.com/user-attachments/assets/871cbea7-eebb-4b28-b260-c1c91df0487a" alt="binanceusdm" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;binanceusdm&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;Binance USDⓈ-M&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://binance-docs.github.io/apidocs/futures/en/"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d10%25&amp;amp;color=orange" alt="Sign up with Binance USDⓈ-M using CCXT's referral link for a 10% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;&lt;img src="https://github.com/user-attachments/assets/387cfc4e-5f33-48cd-8f5c-cd4854dabf0c" alt="binancecoinm" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;binancecoinm&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;Binance COIN-M&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://binance-docs.github.io/apidocs/delivery/en/"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d10%25&amp;amp;color=orange" alt="Sign up with Binance COIN-M using CCXT's referral link for a 10% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bybit.com/invite?ref=XDK12WP"&gt;&lt;img src="https://github.com/user-attachments/assets/97a5d0b3-de10-423d-90e1-6620960025ed" alt="bybit" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bybit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bybit.com/invite?ref=XDK12WP"&gt;Bybit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://bybit-exchange.github.io/docs/inverse/"&gt;&lt;img src="https://img.shields.io/badge/5-lightgray" alt="API Version 5" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.okx.com/join/CCXT2023"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/152485636-38b19e4a-bece-4dec-979a-5982859ffc04.jpg" alt="okx" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;okx&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.okx.com/join/CCXT2023"&gt;OKX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.okx.com/docs-v5/en/"&gt;&lt;img src="https://img.shields.io/badge/5-lightgray" alt="API Version 5" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.okx.com/join/CCXT2023"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d20%25&amp;amp;color=orange" alt="Sign up with OKX using CCXT's referral link for a 20% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.gate.io/signup/2436035"&gt;&lt;img src="https://github.com/user-attachments/assets/64f988c5-07b6-4652-b5c1-679a6bf67c85" alt="gate" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;gate&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.gate.io/signup/2436035"&gt;Gate.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.gate.io/docs/developers/apiv4/en/"&gt;&lt;img src="https://img.shields.io/badge/4-lightgray" alt="API Version 4" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.gate.io/signup/2436035"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d20%25&amp;amp;color=orange" alt="Sign up with Gate.io using CCXT's referral link for a 20% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.kucoin.com/ucenter/signup?rcode=E5wkqe"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87295558-132aaf80-c50e-11ea-9801-a2fb0c57c799.jpg" alt="kucoin" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;kucoin&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.kucoin.com/ucenter/signup?rcode=E5wkqe"&gt;KuCoin&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.kucoin.com"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://futures.kucoin.com/?rcode=E5wkqe"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/147508995-9e35030a-d046-43a1-a006-6fabd981b554.jpg" alt="kucoinfutures" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;kucoinfutures&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://futures.kucoin.com/?rcode=E5wkqe"&gt;KuCoin Futures&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.kucoin.com/futures"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bitget.com/expressly?languageType=0&amp;amp;channelCode=ccxt&amp;amp;vipCode=tg9j"&gt;&lt;img src="https://github.com/user-attachments/assets/fbaa10cc-a277-441d-a5b7-997dd9a87658" alt="bitget" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitget&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bitget.com/expressly?languageType=0&amp;amp;channelCode=ccxt&amp;amp;vipCode=tg9j"&gt;Bitget&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.bitget.com/api-doc/common/intro"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://app.hyperliquid.xyz/"&gt;&lt;img src="https://github.com/ccxt/ccxt/assets/43336371/b371bc6c-4a8c-489f-87f4-20a913dd8d4b" alt="hyperliquid" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;hyperliquid&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://app.hyperliquid.xyz/"&gt;Hyperliquid&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://hyperliquid.gitbook.io/hyperliquid-docs/for-developers/api"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bitmex.com/app/register/NZTR1q"&gt;&lt;img src="https://github.com/user-attachments/assets/c78425ab-78d5-49d6-bd14-db7734798f04" alt="bitmex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitmex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bitmex.com/app/register/NZTR1q"&gt;BitMEX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.bitmex.com/app/apiOverview"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bitmex.com/app/register/NZTR1q"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d10%25&amp;amp;color=orange" alt="Sign up with BitMEX using CCXT's referral link for a 10% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://bingx.com/invite/OHETOM"&gt;&lt;img src="https://github-production-user-asset-6210df.s3.amazonaws.com/1294454/253675376-6983b72e-4999-4549-b177-33b374c195e3.jpg" alt="bingx" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bingx&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://bingx.com/invite/OHETOM"&gt;BingX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://bingx-api.github.io/docs/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.htx.com.vc/invite/en-us/1h?invite_code=6rmm2223"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/76137448-22748a80-604e-11ea-8069-6e389271911d.jpg" alt="htx" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;htx&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.htx.com.vc/invite/en-us/1h?invite_code=6rmm2223"&gt;HTX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huobiapi.github.io/docs/spot/v1/en/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.htx.com.vc/invite/en-us/1h?invite_code=6rmm2223"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d15%25&amp;amp;color=orange" alt="Sign up with HTX using CCXT's referral link for a 15% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.mexc.com/register?inviteCode=mexc-1FQ1GNu1"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/137283979-8b2a818d-8633-461b-bfca-de89e8c446b2.jpg" alt="mexc" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;mexc&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.mexc.com/register?inviteCode=mexc-1FQ1GNu1"&gt;MEXC Global&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://mexcdevelop.github.io/apidocs/"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="http://www.bitmart.com/?r=rQCFLh"&gt;&lt;img src="https://github.com/user-attachments/assets/0623e9c4-f50e-48c9-82bd-65c3908c3a14" alt="bitmart" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitmart&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://www.bitmart.com/?r=rQCFLh"&gt;BitMart&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://developer-pro.bitmart.com/"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://www.bitmart.com/?r=rQCFLh"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d30%25&amp;amp;color=orange" alt="Sign up with BitMart using CCXT's referral link for a 30% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://crypto.com/exch/kdacthrnxt"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/147792121-38ed5e36-c229-48d6-b49a-48d05fc19ed4.jpeg" alt="cryptocom" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;cryptocom&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://crypto.com/exch/kdacthrnxt"&gt;Crypto.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://exchange-docs.crypto.com/exchange/v1/rest-ws/index.html"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://crypto.com/exch/kdacthrnxt"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d75%25&amp;amp;color=orange" alt="Sign up with Crypto.com using CCXT's referral link for a 75% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.coinex.com/register?refer_code=yw5fz"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87182089-1e05fa00-c2ec-11ea-8da9-cc73b45abbbc.jpg" alt="coinex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coinex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.coinex.com/register?refer_code=yw5fz"&gt;CoinEx&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.coinex.com/api/v2"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://global.hashkey.com/en-US/register/invite?invite_code=82FQUN"&gt;&lt;img src="https://github.com/user-attachments/assets/6dd6127b-cc19-4a13-9b29-a98d81f80e98" alt="hashkey" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;hashkey&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://global.hashkey.com/en-US/register/invite?invite_code=82FQUN"&gt;HashKey Global&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://hashkeyglobal-apidoc.readme.io/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://woox.io/register?ref=DIJT0CNL"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/150730761-1a00e5e0-d28c-480f-9e65-089ce3e6ef3b.jpg" alt="woo" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;woo&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://woox.io/register?ref=DIJT0CNL"&gt;WOO X&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.woox.io/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://woox.io/register?ref=DIJT0CNL"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d35%25&amp;amp;color=orange" alt="Sign up with WOO X using CCXT's referral link for a 35% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://dex.woo.org/en/trade?ref=CCXT"&gt;&lt;img src="https://github.com/user-attachments/assets/9ba21b8a-a9c7-4770-b7f1-ce3bcbde68c1" alt="woofipro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;woofipro&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://dex.woo.org/en/trade?ref=CCXT"&gt;WOOFI PRO&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://orderly.network/docs/build-on-evm/building-on-evm"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://dex.woo.org/en/trade?ref=CCXT"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d5%25&amp;amp;color=orange" alt="Sign up with WOOFI PRO using CCXT's referral link for a 5% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Supported Cryptocurrency Exchanges&lt;/h2&gt; 
&lt;!-- init list --&gt;The CCXT library currently supports the following 102 cryptocurrency exchange markets and trading APIs: 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;logo&lt;/th&gt; 
   &lt;th&gt;id&lt;/th&gt; 
   &lt;th&gt;name&lt;/th&gt; 
   &lt;th align="center"&gt;ver&lt;/th&gt; 
   &lt;th&gt;type&lt;/th&gt; 
   &lt;th&gt;certified&lt;/th&gt; 
   &lt;th&gt;pro&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://alpaca.markets"&gt;&lt;img src="https://github.com/user-attachments/assets/e9476df8-a450-4c3e-ab9a-1a7794219e1b" alt="alpaca" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;alpaca&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://alpaca.markets"&gt;Alpaca&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://alpaca.markets/docs/"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://omni.apex.exchange/trade"&gt;&lt;img src="https://github.com/user-attachments/assets/fef8f2f7-4265-46aa-965e-33a91881cb00" alt="apex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;apex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://omni.apex.exchange/trade"&gt;Apex&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://api-docs.pro.apex.exchange"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://ascendex.com/en-us/register?inviteCode=EL6BXBQM"&gt;&lt;img src="https://github.com/user-attachments/assets/55bab6b9-d4ca-42a8-a0e6-fac81ae557f1" alt="ascendex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ascendex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ascendex.com/en-us/register?inviteCode=EL6BXBQM"&gt;AscendEX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ascendex.github.io/ascendex-pro-api/#ascendex-pro-api-documentation"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://backpack.exchange/join/ccxt"&gt;&lt;img src="https://github.com/user-attachments/assets/cc04c278-679f-4554-9f72-930dd632b80f" alt="backpack" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;backpack&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://backpack.exchange/join/ccxt"&gt;Backpack&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.backpack.exchange/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://bequant.io/referral/dd104e3bee7634ec"&gt;&lt;img src="https://github.com/user-attachments/assets/0583ef1f-29fe-4b7c-8189-63565a0e2867" alt="bequant" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bequant&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://bequant.io/referral/dd104e3bee7634ec"&gt;Bequant&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://api.bequant.io/"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://b1.run/users/new?code=D3LLBVFT"&gt;&lt;img src="https://github.com/user-attachments/assets/4e5cfd53-98cc-4b90-92cd-0d7b512653d1" alt="bigone" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bigone&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://b1.run/users/new?code=D3LLBVFT"&gt;BigONE&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://open.big.one/docs/api.html"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;&lt;img src="https://github.com/user-attachments/assets/e9419b93-ccb0-46aa-9bff-c883f096274b" alt="binance" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;binance&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;Binance&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://developers.binance.com/en"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;&lt;img src="https://github.com/user-attachments/assets/387cfc4e-5f33-48cd-8f5c-cd4854dabf0c" alt="binancecoinm" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;binancecoinm&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;Binance COIN-M&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://binance-docs.github.io/apidocs/delivery/en/"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.binance.us/?ref=35005074"&gt;&lt;img src="https://github.com/user-attachments/assets/a9667919-b632-4d52-a832-df89f8a35e8c" alt="binanceus" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;binanceus&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.binance.us/?ref=35005074"&gt;Binance US&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/binance-us/binance-official-api-docs"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;&lt;img src="https://github.com/user-attachments/assets/871cbea7-eebb-4b28-b260-c1c91df0487a" alt="binanceusdm" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;binanceusdm&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;Binance USDⓈ-M&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://binance-docs.github.io/apidocs/futures/en/"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://bingx.com/invite/OHETOM"&gt;&lt;img src="https://github-production-user-asset-6210df.s3.amazonaws.com/1294454/253675376-6983b72e-4999-4549-b177-33b374c195e3.jpg" alt="bingx" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bingx&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://bingx.com/invite/OHETOM"&gt;BingX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://bingx-api.github.io/docs/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://bit2c.co.il/Aff/63bfed10-e359-420c-ab5a-ad368dab0baf"&gt;&lt;img src="https://github.com/user-attachments/assets/db0bce50-6842-4c09-a1d5-0c87d22118aa" alt="bit2c" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bit2c&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://bit2c.co.il/Aff/63bfed10-e359-420c-ab5a-ad368dab0baf"&gt;Bit2C&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.bit2c.co.il/home/api"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://bitbank.cc/"&gt;&lt;img src="https://github.com/user-attachments/assets/9d616de0-8a88-4468-8e38-d269acab0348" alt="bitbank" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitbank&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://bitbank.cc/"&gt;bitbank&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.bitbank.cc/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://ref.bitbns.com/1090961"&gt;&lt;img src="https://github.com/user-attachments/assets/a5b9a562-cdd8-4bea-9fa7-fd24c1dad3d9" alt="bitbns" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitbns&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ref.bitbns.com/1090961"&gt;Bitbns&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://bitbns.com/trade/#/api-trading/"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bitfinex.com"&gt;&lt;img src="https://github.com/user-attachments/assets/4a8e947f-ab46-481a-a8ae-8b20e9b03178" alt="bitfinex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitfinex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bitfinex.com"&gt;Bitfinex&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.bitfinex.com/v2/docs/"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://bitflyer.com"&gt;&lt;img src="https://github.com/user-attachments/assets/d0217747-e54d-4533-8416-0d553dca74bb" alt="bitflyer" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitflyer&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://bitflyer.com"&gt;bitFlyer&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://lightning.bitflyer.com/docs?lang=en"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bitget.com/expressly?languageType=0&amp;amp;channelCode=ccxt&amp;amp;vipCode=tg9j"&gt;&lt;img src="https://github.com/user-attachments/assets/fbaa10cc-a277-441d-a5b7-997dd9a87658" alt="bitget" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitget&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bitget.com/expressly?languageType=0&amp;amp;channelCode=ccxt&amp;amp;vipCode=tg9j"&gt;Bitget&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.bitget.com/api-doc/common/intro"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bithumb.com"&gt;&lt;img src="https://github.com/user-attachments/assets/c9e0eefb-4777-46b9-8f09-9d7f7c4af82d" alt="bithumb" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bithumb&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bithumb.com"&gt;Bithumb&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://apidocs.bithumb.com"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="http://www.bitmart.com/?r=rQCFLh"&gt;&lt;img src="https://github.com/user-attachments/assets/0623e9c4-f50e-48c9-82bd-65c3908c3a14" alt="bitmart" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitmart&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://www.bitmart.com/?r=rQCFLh"&gt;BitMart&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://developer-pro.bitmart.com/"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bitmex.com/app/register/NZTR1q"&gt;&lt;img src="https://github.com/user-attachments/assets/c78425ab-78d5-49d6-bd14-db7734798f04" alt="bitmex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitmex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bitmex.com/app/register/NZTR1q"&gt;BitMEX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.bitmex.com/app/apiOverview"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bitopro.com"&gt;&lt;img src="https://github.com/user-attachments/assets/affc6337-b95a-44bf-aacd-04f9722364f6" alt="bitopro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitopro&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bitopro.com"&gt;BitoPro&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/bitoex/bitopro-offical-api-docs/raw/master/v3-1/rest-1/rest.md"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bitrue.com/affiliate/landing?cn=600000&amp;amp;inviteCode=EZWETQE"&gt;&lt;img src="https://github.com/user-attachments/assets/67abe346-1273-461a-bd7c-42fa32907c8e" alt="bitrue" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitrue&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bitrue.com/affiliate/landing?cn=600000&amp;amp;inviteCode=EZWETQE"&gt;Bitrue&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/Bitrue-exchange/bitrue-official-api-docs"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://bitso.com/?ref=itej"&gt;&lt;img src="https://github.com/user-attachments/assets/178c8e56-9054-4107-b192-5e5053d4f975" alt="bitso" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitso&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://bitso.com/?ref=itej"&gt;Bitso&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://bitso.com/api_info"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bitstamp.net"&gt;&lt;img src="https://github.com/user-attachments/assets/d5480572-1fee-43cb-b900-d38c522d0024" alt="bitstamp" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitstamp&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bitstamp.net"&gt;Bitstamp&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.bitstamp.net/api"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://bit.team/auth/sign-up?ref=bitboy2023"&gt;&lt;img src="https://github.com/user-attachments/assets/b41b5e0d-98e5-4bd3-8a6e-aeb230a4a135" alt="bitteam" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitteam&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://bit.team/auth/sign-up?ref=bitboy2023"&gt;BIT.TEAM&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://bit.team/trade/api/documentation"&gt;&lt;img src="https://img.shields.io/badge/2.0.6-lightgray" alt="API Version 2.0.6" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bittrade.co.jp/register/?invite_code=znnq3"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/85734211-85755480-b705-11ea-8b35-0b7f1db33a2f.jpg" alt="bittrade" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bittrade&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bittrade.co.jp/register/?invite_code=znnq3"&gt;BitTrade&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://api-doc.bittrade.co.jp"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://bitvavo.com/?a=24F34952F7"&gt;&lt;img src="https://github.com/user-attachments/assets/d213155c-8c71-4701-9bd5-45351febc2a8" alt="bitvavo" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitvavo&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://bitvavo.com/?a=24F34952F7"&gt;Bitvavo&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.bitvavo.com/"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://blockchain.com"&gt;&lt;img src="https://github.com/user-attachments/assets/975e3054-3399-4363-bcee-ec3c6d63d4e8" alt="blockchaincom" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;blockchaincom&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://blockchain.com"&gt;Blockchain.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://api.blockchain.com/v3"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://blofin.com/register?referral_code=f79EsS"&gt;&lt;img src="https://github.com/user-attachments/assets/518cdf80-f05d-4821-a3e3-d48ceb41d73b" alt="blofin" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;blofin&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://blofin.com/register?referral_code=f79EsS"&gt;BloFin&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://blofin.com/docs"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://btc-alpha.com/?r=123788"&gt;&lt;img src="https://github.com/user-attachments/assets/dce49f3a-61e5-4ba0-a2fe-41d192fd0e5d" alt="btcalpha" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;btcalpha&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://btc-alpha.com/?r=123788"&gt;BTC-Alpha&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://btc-alpha.github.io/api-docs"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.btcbox.co.jp/"&gt;&lt;img src="https://github.com/user-attachments/assets/1e2cb499-8d0f-4f8f-9464-3c015cfbc76b" alt="btcbox" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;btcbox&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.btcbox.co.jp/"&gt;BtcBox&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://blog.btcbox.jp/en/archives/8762"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://btcmarkets.net"&gt;&lt;img src="https://github.com/user-attachments/assets/8c8d6907-3873-4cc4-ad20-e22fba28247e" alt="btcmarkets" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;btcmarkets&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://btcmarkets.net"&gt;BTC Markets&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://api.btcmarkets.net/doc/v3"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.btcturk.com"&gt;&lt;img src="https://github.com/user-attachments/assets/10e0a238-9f60-4b06-9dda-edfc7602f1d6" alt="btcturk" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;btcturk&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.btcturk.com"&gt;BTCTurk&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/BTCTrader/broker-api-docs"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bybit.com/invite?ref=XDK12WP"&gt;&lt;img src="https://github.com/user-attachments/assets/97a5d0b3-de10-423d-90e1-6620960025ed" alt="bybit" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bybit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bybit.com/invite?ref=XDK12WP"&gt;Bybit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://bybit-exchange.github.io/docs/inverse/"&gt;&lt;img src="https://img.shields.io/badge/5-lightgray" alt="API Version 5" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://cex.io/r/0/up105393824/0/"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/27766442-8ddc33b0-5ed8-11e7-8b98-f786aef0f3c9.jpg" alt="cex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;cex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://cex.io/r/0/up105393824/0/"&gt;CEX.IO&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://trade.cex.io/docs/"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.coinbase.com/join/58cbe25a355148797479dbd2"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/40811661-b6eceae2-653a-11e8-829e-10bfadb078cf.jpg" alt="coinbase" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coinbase&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.coinbase.com/join/58cbe25a355148797479dbd2"&gt;Coinbase Advanced&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://developers.coinbase.com/api/v2"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://coinbase.com/"&gt;&lt;img src="https://github.com/ccxt/ccxt/assets/43336371/34a65553-88aa-4a38-a714-064bd228b97e" alt="coinbaseexchange" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coinbaseexchange&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://coinbase.com/"&gt;Coinbase Exchange&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.cloud.coinbase.com/exchange/docs/"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://international.coinbase.com"&gt;&lt;img src="https://github.com/ccxt/ccxt/assets/43336371/866ae638-6ab5-4ebf-ab2c-cdcce9545625" alt="coinbaseinternational" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coinbaseinternational&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://international.coinbase.com"&gt;Coinbase International&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.cloud.coinbase.com/intx/docs"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://partner.coincatch.cc/bg/92hy70391729607848548"&gt;&lt;img src="https://github.com/user-attachments/assets/3d49065f-f05d-4573-88a2-1b5201ec6ff3" alt="coincatch" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coincatch&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://partner.coincatch.cc/bg/92hy70391729607848548"&gt;CoinCatch&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://coincatch.github.io/github.io/en/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://coincheck.com"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87182088-1d6d6380-c2ec-11ea-9c64-8ab9f9b289f5.jpg" alt="coincheck" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coincheck&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://coincheck.com"&gt;coincheck&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://coincheck.com/documents/exchange/api"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.coinex.com/register?refer_code=yw5fz"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87182089-1e05fa00-c2ec-11ea-8da9-cc73b45abbbc.jpg" alt="coinex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coinex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.coinex.com/register?refer_code=yw5fz"&gt;CoinEx&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.coinex.com/api/v2"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://coinmate.io?referral=YTFkM1RsOWFObVpmY1ZjMGREQmpTRnBsWjJJNVp3PT0"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87460806-1c9f3f00-c616-11ea-8c46-a77018a8f3f4.jpg" alt="coinmate" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coinmate&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://coinmate.io?referral=YTFkM1RsOWFObVpmY1ZjMGREQmpTRnBsWjJJNVp3PT0"&gt;CoinMate&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://coinmate.docs.apiary.io"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://go.coinmetro.com/?ref=crypto24"&gt;&lt;img src="https://github.com/ccxt/ccxt/assets/43336371/e86f87ec-6ba3-4410-962b-f7988c5db539" alt="coinmetro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coinmetro&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://go.coinmetro.com/?ref=crypto24"&gt;Coinmetro&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://documenter.getpostman.com/view/3653795/SVfWN6KS"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://coinone.co.kr"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/38003300-adc12fba-323f-11e8-8525-725f53c4a659.jpg" alt="coinone" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coinone&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://coinone.co.kr"&gt;CoinOne&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://doc.coinone.co.kr"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://coins.ph/"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/225719995-48ab2026-4ddb-496c-9da7-0d7566617c9b.jpg" alt="coinsph" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coinsph&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://coins.ph/"&gt;Coins.ph&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://coins-docs.github.io/rest-api"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.coinspot.com.au/register?code=PJURCU"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/28208429-3cacdf9a-6896-11e7-854e-4c79a772a30f.jpg" alt="coinspot" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coinspot&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.coinspot.com.au/register?code=PJURCU"&gt;CoinSpot&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.coinspot.com.au/api"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://crypto.com/exch/kdacthrnxt"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/147792121-38ed5e36-c229-48d6-b49a-48d05fc19ed4.jpeg" alt="cryptocom" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;cryptocom&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://crypto.com/exch/kdacthrnxt"&gt;Crypto.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://exchange-docs.crypto.com/exchange/v1/rest-ws/index.html"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://app.cryptomus.com/signup/?ref=JRP4yj"&gt;&lt;img src="https://github.com/user-attachments/assets/8e0b1c48-7c01-4177-9224-f1b01d89d7e7" alt="cryptomus" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;cryptomus&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://app.cryptomus.com/signup/?ref=JRP4yj"&gt;Cryptomus&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://doc.cryptomus.com/personal"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://app.defx.com/join/6I2CZ7"&gt;&lt;img src="https://github.com/user-attachments/assets/4e92bace-d7a9-45ea-92be-122168dc87e4" alt="defx" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;defx&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://app.defx.com/join/6I2CZ7"&gt;Defx X&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.defx.com/docs"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.delta.exchange/app/signup/?code=IULYNB"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/99450025-3be60a00-2931-11eb-9302-f4fd8d8589aa.jpg" alt="delta" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;delta&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.delta.exchange/app/signup/?code=IULYNB"&gt;Delta Exchange&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.delta.exchange"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.deribit.com/reg-1189.4038"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/41933112-9e2dd65a-798b-11e8-8440-5bab2959fcb8.jpg" alt="deribit" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;deribit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.deribit.com/reg-1189.4038"&gt;Deribit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.deribit.com/v2"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.derive.xyz/invite/3VB0B"&gt;&lt;img src="https://github.com/user-attachments/assets/f835b95f-033a-43dd-b6bb-24e698fc498c" alt="derive" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;derive&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.derive.xyz/invite/3VB0B"&gt;derive&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.derive.xyz/docs/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.digifinex.com/en-ww/from/DhOzBg?channelCode=ljaUPp"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87443315-01283a00-c5fe-11ea-8628-c2a0feaf07ac.jpg" alt="digifinex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;digifinex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.digifinex.com/en-ww/from/DhOzBg?channelCode=ljaUPp"&gt;DigiFinex&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.digifinex.com"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://exmo.me/?ref=131685"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/27766491-1b0ea956-5eda-11e7-9225-40d67b481b8d.jpg" alt="exmo" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;exmo&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://exmo.me/?ref=131685"&gt;EXMO&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://exmo.me/en/api_doc?ref=131685"&gt;&lt;img src="https://img.shields.io/badge/1.1-lightgray" alt="API Version 1.1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://fmfw.io/referral/da948b21d6c92d69"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/159177712-b685b40c-5269-4cea-ac83-f7894c49525d.jpg" alt="fmfwio" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;fmfwio&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://fmfw.io/referral/da948b21d6c92d69"&gt;FMFW.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://api.fmfw.io/"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://app.foxbit.com.br"&gt;&lt;img src="https://github.com/user-attachments/assets/1f8faca2-ae2f-4222-b33e-5671e7d873dd" alt="foxbit" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;foxbit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://app.foxbit.com.br"&gt;Foxbit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.foxbit.com.br"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.gate.io/signup/2436035"&gt;&lt;img src="https://github.com/user-attachments/assets/64f988c5-07b6-4652-b5c1-679a6bf67c85" alt="gate" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;gate&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.gate.io/signup/2436035"&gt;Gate.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.gate.io/docs/developers/apiv4/en/"&gt;&lt;img src="https://img.shields.io/badge/4-lightgray" alt="API Version 4" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://gemini.com/"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/27816857-ce7be644-6096-11e7-82d6-3c257263229c.jpg" alt="gemini" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;gemini&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://gemini.com/"&gt;Gemini&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.gemini.com/rest-api"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://global.hashkey.com/en-US/register/invite?invite_code=82FQUN"&gt;&lt;img src="https://github.com/user-attachments/assets/6dd6127b-cc19-4a13-9b29-a98d81f80e98" alt="hashkey" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;hashkey&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://global.hashkey.com/en-US/register/invite?invite_code=82FQUN"&gt;HashKey Global&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://hashkeyglobal-apidoc.readme.io/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ccxt/ccxt/master/hibachi.xyz/r/ZBL2YFWIHU"&gt;&lt;img src="https://github.com/user-attachments/assets/7301bbb1-4f27-4167-8a55-75f74b14e973" alt="hibachi" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;hibachi&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ccxt/ccxt/master/hibachi.xyz/r/ZBL2YFWIHU"&gt;Hibachi&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/ccxt/ccxt/master/undefined"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hitbtc.com/?ref_id=5a5d39a65d466"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/27766555-8eaec20e-5edc-11e7-9c5b-6dc69fc42f5e.jpg" alt="hitbtc" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;hitbtc&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://hitbtc.com/?ref_id=5a5d39a65d466"&gt;HitBTC&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://api.hitbtc.com"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://pro.hollaex.com/signup?affiliation_code=QSWA6G"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/75841031-ca375180-5ddd-11ea-8417-b975674c23cb.jpg" alt="hollaex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;hollaex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://pro.hollaex.com/signup?affiliation_code=QSWA6G"&gt;HollaEx&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://apidocs.hollaex.com"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.htx.com.vc/invite/en-us/1h?invite_code=6rmm2223"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/76137448-22748a80-604e-11ea-8069-6e389271911d.jpg" alt="htx" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;htx&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.htx.com.vc/invite/en-us/1h?invite_code=6rmm2223"&gt;HTX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huobiapi.github.io/docs/spot/v1/en/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://app.hyperliquid.xyz/"&gt;&lt;img src="https://github.com/ccxt/ccxt/assets/43336371/b371bc6c-4a8c-489f-87f4-20a913dd8d4b" alt="hyperliquid" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;hyperliquid&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://app.hyperliquid.xyz/"&gt;Hyperliquid&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://hyperliquid.gitbook.io/hyperliquid-docs/for-developers/api"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.independentreserve.com"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87182090-1e9e9080-c2ec-11ea-8e49-563db9a38f37.jpg" alt="independentreserve" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;independentreserve&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.independentreserve.com"&gt;Independent Reserve&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.independentreserve.com/API"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://indodax.com/ref/testbitcoincoid/1"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87070508-9358c880-c221-11ea-8dc5-5391afbbb422.jpg" alt="indodax" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;indodax&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://indodax.com/ref/testbitcoincoid/1"&gt;INDODAX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/btcid/indodax-official-api-docs"&gt;&lt;img src="https://img.shields.io/badge/2.0-lightgray" alt="API Version 2.0" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.kraken.com"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/76173629-fc67fb00-61b1-11ea-84fe-f2de582f58a3.jpg" alt="kraken" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;kraken&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.kraken.com"&gt;Kraken&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.kraken.com/rest/"&gt;&lt;img src="https://img.shields.io/badge/0-lightgray" alt="API Version 0" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://futures.kraken.com/"&gt;&lt;img src="https://user-images.githubusercontent.com/24300605/81436764-b22fd580-9172-11ea-9703-742783e6376d.jpg" alt="krakenfutures" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;krakenfutures&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://futures.kraken.com/"&gt;Kraken Futures&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.kraken.com/api/docs/futures-api/trading/market-data/"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.kucoin.com/ucenter/signup?rcode=E5wkqe"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87295558-132aaf80-c50e-11ea-9801-a2fb0c57c799.jpg" alt="kucoin" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;kucoin&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.kucoin.com/ucenter/signup?rcode=E5wkqe"&gt;KuCoin&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.kucoin.com"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://futures.kucoin.com/?rcode=E5wkqe"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/147508995-9e35030a-d046-43a1-a006-6fabd981b554.jpg" alt="kucoinfutures" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;kucoinfutures&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://futures.kucoin.com/?rcode=E5wkqe"&gt;KuCoin Futures&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.kucoin.com/futures"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://latoken.com/invite?r=mvgp2djk"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/61511972-24c39f00-aa01-11e9-9f7c-471f1d6e5214.jpg" alt="latoken" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;latoken&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://latoken.com/invite?r=mvgp2djk"&gt;Latoken&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://api.latoken.com"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.lbank.com/login/?icode=7QCY"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/38063602-9605e28a-3302-11e8-81be-64b1e53c4cfb.jpg" alt="lbank" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;lbank&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.lbank.com/login/?icode=7QCY"&gt;LBank&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.lbank.com/en-US/docs/index.html"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.luno.com/invite/44893A"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/27766607-8c1a69d8-5ede-11e7-930c-540b5eb9be24.jpg" alt="luno" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;luno&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.luno.com/invite/44893A"&gt;luno&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.luno.com/en/api"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.mercadobitcoin.com.br"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/27837060-e7c58714-60ea-11e7-9192-f05e86adb83f.jpg" alt="mercado" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;mercado&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.mercadobitcoin.com.br"&gt;Mercado Bitcoin&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.mercadobitcoin.com.br/api-doc"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.mexc.com/register?inviteCode=mexc-1FQ1GNu1"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/137283979-8b2a818d-8633-461b-bfca-de89e8c446b2.jpg" alt="mexc" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;mexc&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.mexc.com/register?inviteCode=mexc-1FQ1GNu1"&gt;MEXC Global&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://mexcdevelop.github.io/apidocs/"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://trade.mode.network?ref=MODETRADE"&gt;&lt;img src="https://github.com/user-attachments/assets/cec2b7f1-3b2b-4502-971b-447ee1937d6b" alt="modetrade" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;modetrade&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://trade.mode.network?ref=MODETRADE"&gt;Mode Trade&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/ccxt/ccxt/master/undefined"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.my.okx.com/join/CCXT2023"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/152485636-38b19e4a-bece-4dec-979a-5982859ffc04.jpg" alt="myokx" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;myokx&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.my.okx.com/join/CCXT2023"&gt;MyOKX (EEA)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://my.okx.com/docs-v5/en/#overview"&gt;&lt;img src="https://img.shields.io/badge/5-lightgray" alt="API Version 5" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://one.ndax.io/bfQiSL"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/108623144-67a3ef00-744e-11eb-8140-75c6b851e945.jpg" alt="ndax" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ndax&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://one.ndax.io/bfQiSL"&gt;NDAX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://apidoc.ndax.io/"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.novadax.com.br/?s=ccxt"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/92337550-2b085500-f0b3-11ea-98e7-5794fb07dd3b.jpg" alt="novadax" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;novadax&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.novadax.com.br/?s=ccxt"&gt;NovaDAX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://doc.novadax.com/pt-BR/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://oceanex.pro/signup?referral=VE24QX"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/58385970-794e2d80-8001-11e9-889c-0567cd79b78e.jpg" alt="oceanex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;oceanex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://oceanex.pro/signup?referral=VE24QX"&gt;OceanEx&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://api.oceanex.pro/doc/v1"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.okx.com/join/CCXT2023"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/152485636-38b19e4a-bece-4dec-979a-5982859ffc04.jpg" alt="okx" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;okx&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.okx.com/join/CCXT2023"&gt;OKX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.okx.com/docs-v5/en/"&gt;&lt;img src="https://img.shields.io/badge/5-lightgray" alt="API Version 5" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.app.okx.com/join/CCXT2023"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/152485636-38b19e4a-bece-4dec-979a-5982859ffc04.jpg" alt="okxus" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;okxus&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.app.okx.com/join/CCXT2023"&gt;OKX (US)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://app.okx.com/docs-v5/en/#overview"&gt;&lt;img src="https://img.shields.io/badge/5-lightgray" alt="API Version 5" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://onetrading.com/"&gt;&lt;img src="https://github.com/ccxt/ccxt/assets/43336371/bdbc26fd-02f2-4ca7-9f1e-17333690bb1c" alt="onetrading" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;onetrading&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://onetrading.com/"&gt;One Trading&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.onetrading.com"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://ox.fun/register?shareAccountId=5ZUD4a7G"&gt;&lt;img src="https://github.com/ccxt/ccxt/assets/43336371/6a196124-c1ee-4fae-8573-962071b61a85" alt="oxfun" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;oxfun&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ox.fun/register?shareAccountId=5ZUD4a7G"&gt;OXFUN&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.ox.fun/"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://p2pb2b.com?referral=ee784c53"&gt;&lt;img src="https://github.com/ccxt/ccxt/assets/43336371/8da13a80-1f0a-49be-bb90-ff8b25164755" alt="p2b" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;p2b&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://p2pb2b.com?referral=ee784c53"&gt;p2b&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/P2B-team/p2b-api-docs/raw/master/api-doc.md"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://app.paradex.trade/r/ccxt24"&gt;&lt;img src="https://github.com/user-attachments/assets/84628770-784e-4ec4-a759-ec2fbb2244ea" alt="paradex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;paradex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://app.paradex.trade/r/ccxt24"&gt;Paradex&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.api.testnet.paradex.trade/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.paymium.com/page/sign-up?referral=eDAzPoRQFMvaAB8sf-qj"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87153930-f0f02200-c2c0-11ea-9c0a-40337375ae89.jpg" alt="paymium" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;paymium&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.paymium.com/page/sign-up?referral=eDAzPoRQFMvaAB8sf-qj"&gt;Paymium&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/Paymium/api-documentation"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://phemex.com/register?referralCode=EDNVJ"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/85225056-221eb600-b3d7-11ea-930d-564d2690e3f6.jpg" alt="phemex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;phemex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://phemex.com/register?referralCode=EDNVJ"&gt;Phemex&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://phemex-docs.github.io/#overview"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://poloniex.com/signup?c=UBFZJRPJ"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/27766817-e9456312-5ee6-11e7-9b3c-b628ca5626a5.jpg" alt="poloniex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;poloniex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://poloniex.com/signup?c=UBFZJRPJ"&gt;Poloniex&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://api-docs.poloniex.com/spot/"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.probit.com/r/34608773"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/79268032-c4379480-7ea2-11ea-80b3-dd96bb29fd0d.jpg" alt="probit" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;probit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.probit.com/r/34608773"&gt;ProBit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs-en.probit.com"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://timex.io/?refcode=1x27vNkTbP1uwkCck"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/70423869-6839ab00-1a7f-11ea-8f94-13ae72c31115.jpg" alt="timex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;timex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://timex.io/?refcode=1x27vNkTbP1uwkCck"&gt;TimeX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://plasma-relay-backend.timex.io/swagger-ui/index.html"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://tokocrypto.com"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/183870484-d3398d0c-f6a1-4cce-91b8-d58792308716.jpg" alt="tokocrypto" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;tokocrypto&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://tokocrypto.com"&gt;Tokocrypto&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.tokocrypto.com/apidocs/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.toobit.com/en-US/r?i=IFFPy0"&gt;&lt;img src="https://github.com/user-attachments/assets/3fc13870-5406-431b-8be0-2aab69c4f225" alt="toobit" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;toobit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.toobit.com/en-US/r?i=IFFPy0"&gt;Toobit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://toobit-docs.github.io/apidocs/spot/v1/en/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://upbit.com"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/49245610-eeaabe00-f423-11e8-9cba-4b0aed794799.jpg" alt="upbit" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;upbit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://upbit.com"&gt;Upbit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.upbit.com/docs/%EC%9A%94%EC%B2%AD-%EC%88%98-%EC%A0%9C%ED%95%9C"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://wx.network"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/84547058-5fb27d80-ad0b-11ea-8711-78ac8b3c7f31.jpg" alt="wavesexchange" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;wavesexchange&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://wx.network"&gt;Waves.Exchange&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.wx.network"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://whitebit.com/referral/d9bdf40e-28f2-4b52-b2f9-cd1415d82963"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/66732963-8eb7dd00-ee66-11e9-849b-10d9282bb9e0.jpg" alt="whitebit" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;whitebit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://whitebit.com/referral/d9bdf40e-28f2-4b52-b2f9-cd1415d82963"&gt;WhiteBit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/whitebit-exchange/api-docs"&gt;&lt;img src="https://img.shields.io/badge/4-lightgray" alt="API Version 4" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://woox.io/register?ref=DIJT0CNL"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/150730761-1a00e5e0-d28c-480f-9e65-089ce3e6ef3b.jpg" alt="woo" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;woo&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://woox.io/register?ref=DIJT0CNL"&gt;WOO X&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.woox.io/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://dex.woo.org/en/trade?ref=CCXT"&gt;&lt;img src="https://github.com/user-attachments/assets/9ba21b8a-a9c7-4770-b7f1-ce3bcbde68c1" alt="woofipro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;woofipro&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://dex.woo.org/en/trade?ref=CCXT"&gt;WOOFI PRO&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://orderly.network/docs/build-on-evm/building-on-evm"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.xt.com/en/accounts/register?ref=9PTM9VW"&gt;&lt;img src="https://user-images.githubusercontent.com/14319357/232636712-466df2fc-560a-4ca4-aab2-b1d954a58e24.jpg" alt="xt" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;xt&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.xt.com/en/accounts/register?ref=9PTM9VW"&gt;XT&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://doc.xt.com/"&gt;&lt;img src="https://img.shields.io/badge/4-lightgray" alt="API Version 4" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.yobit.net"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/27766910-cdcbfdae-5eea-11e7-9859-03fea873272d.jpg" alt="yobit" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;yobit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.yobit.net"&gt;YoBit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.yobit.net/en/api/"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://zaif.jp"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/27766927-39ca2ada-5eeb-11e7-972f-1b4199518ca6.jpg" alt="zaif" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;zaif&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://zaif.jp"&gt;Zaif&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://techbureau-api-document.readthedocs.io/ja/latest/index.html"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://auth.zondaglobal.com/ref/jHlbB4mIkdS1"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/159202310-a0e38007-5e7c-4ba9-a32f-c8263a0291fe.jpg" alt="zonda" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;zonda&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://auth.zondaglobal.com/ref/jHlbB4mIkdS1"&gt;Zonda&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.zondacrypto.exchange/"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!-- end list --&gt; 
&lt;p&gt;The list above is updated frequently, new crypto markets, exchanges, bug fixes, and API endpoints are introduced on a regular basis. See the &lt;a href="https://github.com/ccxt/ccxt/wiki/"&gt;Manual&lt;/a&gt; for more details. If you can't find a cryptocurrency exchange in the list above and want it to be added, post a link to it by opening an issue here on GitHub or send us an email.&lt;/p&gt; 
&lt;p&gt;The library is under &lt;a href="https://github.com/ccxt/ccxt/raw/master/LICENSE.txt"&gt;MIT license&lt;/a&gt;, that means it's absolutely free for any developer to build commercial and opensource software on top of it, but use it at your own risk with no warranties, as is.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Install&lt;/h2&gt; 
&lt;p&gt;The easiest way to install the CCXT library is to use a package manager:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.npmjs.com/package/ccxt"&gt;ccxt in &lt;strong&gt;NPM&lt;/strong&gt;&lt;/a&gt; (JavaScript / Node v7.6+)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.python.org/pypi/ccxt"&gt;ccxt in &lt;strong&gt;PyPI&lt;/strong&gt;&lt;/a&gt; (Python 3.7.0+)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://packagist.org/packages/ccxt/ccxt"&gt;ccxt in &lt;strong&gt;Packagist/Composer&lt;/strong&gt;&lt;/a&gt; (PHP 8.1+)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.nuget.org/packages/ccxt"&gt;ccxt in &lt;strong&gt;Nuget&lt;/strong&gt;&lt;/a&gt; (netstandard 2.0)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pkg.go.dev/github.com/ccxt/ccxt/go/v4"&gt;ccxt in &lt;strong&gt;GO&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This library is shipped as an all-in-one module implementation with minimalistic dependencies and requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ccxt/ccxt/raw/master/js/"&gt;js/&lt;/a&gt; in JavaScript&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ccxt/ccxt/raw/master/python/"&gt;python/&lt;/a&gt; in Python (generated from TS)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ccxt/ccxt/raw/master/php/"&gt;php/&lt;/a&gt; in PHP (generated from TS)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ccxt/ccxt/raw/master/cs/"&gt;cs/&lt;/a&gt; in C# (generated from TS)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ccxt/ccxt/raw/master/go/"&gt;go/&lt;/a&gt; in Go (generated from TS)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can also clone it into your project directory from &lt;a href="https://github.com/ccxt/ccxt"&gt;ccxt GitHub repository&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/ccxt/ccxt.git  # including 1GB of commit history

# or

git clone https://github.com/ccxt/ccxt.git --depth 1  # avoid downloading 1GB of commit history
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;JavaScript (NPM)&lt;/h3&gt; 
&lt;p&gt;JavaScript version of CCXT works in both Node and web browsers. Requires ES6 and &lt;code&gt;async/await&lt;/code&gt; syntax support (Node 7.6.0+). When compiling with Webpack and Babel, make sure it is &lt;a href="https://github.com/ccxt/ccxt/issues/225#issuecomment-331905178"&gt;not excluded&lt;/a&gt; in your &lt;code&gt;babel-loader&lt;/code&gt; config.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.npmjs.com/package/ccxt"&gt;ccxt in &lt;strong&gt;NPM&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;npm install ccxt
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-JavaScript"&gt;//cjs
var ccxt = require ('ccxt')
console.log (ccxt.exchanges) // print all available exchanges
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-Javascript"&gt;//esm
import {version, exchanges} from 'ccxt';
console.log(version, Object.keys(exchanges));
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;JavaScript (for use with the &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; tag):&lt;/h3&gt; 
&lt;p&gt;All-in-one browser bundle (dependencies included), served from a CDN of your choice:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;jsDelivr: &lt;a href="https://cdn.jsdelivr.net/npm/ccxt@4.5.11/dist/ccxt.browser.min.js"&gt;https://cdn.jsdelivr.net/npm/ccxt@4.5.11/dist/ccxt.browser.min.js&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;unpkg: &lt;a href="https://unpkg.com/ccxt@4.5.11/dist/ccxt.browser.min.js"&gt;https://unpkg.com/ccxt@4.5.11/dist/ccxt.browser.min.js&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;CDNs are not updated in real-time and may have delays. Defaulting to the most recent version without specifying the version number is not recommended. Please, keep in mind that we are not responsible for the correct operation of those CDN servers.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-HTML"&gt;&amp;lt;script type="text/javascript" src="https://cdn.jsdelivr.net/npm/ccxt@4.5.11/dist/ccxt.browser.min.js"&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Creates a global &lt;code&gt;ccxt&lt;/code&gt; object:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-JavaScript"&gt;console.log (ccxt.exchanges) // print all available exchanges
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Python&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://pypi.python.org/pypi/ccxt"&gt;ccxt in &lt;strong&gt;PyPI&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install ccxt
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-Python"&gt;import ccxt
print(ccxt.exchanges) # print a list of all available exchange classes
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The library supports concurrent asynchronous mode with asyncio and async/await in Python 3.7.0+&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-Python"&gt;import ccxt.async_support as ccxt # link against the asynchronous version of ccxt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;orjson support&lt;/h4&gt; 
&lt;p&gt;CCXT also supports &lt;code&gt;orjson&lt;/code&gt; for parsing JSON since it is much faster than the builtin library. This is especially important when using websockets because some exchanges return big messages that need to be parsed and dispatched as quickly as possible.&lt;/p&gt; 
&lt;p&gt;However, &lt;code&gt;orjson&lt;/code&gt; is not enabled by default because it is not supported by every python interpreter. If you want to opt-in, you just need to install it (&lt;code&gt;pip install orjson&lt;/code&gt;) on your local environment. CCXT will detect the installion and pick it up automatically.&lt;/p&gt; 
&lt;h3&gt;PHP&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://packagist.org/packages/ccxt/ccxt"&gt;ccxt in PHP with &lt;strong&gt;Packagist/Composer&lt;/strong&gt;&lt;/a&gt; (PHP 8.1+)&lt;/p&gt; 
&lt;p&gt;It requires common PHP modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;cURL&lt;/li&gt; 
 &lt;li&gt;mbstring (using UTF-8 is highly recommended)&lt;/li&gt; 
 &lt;li&gt;PCRE&lt;/li&gt; 
 &lt;li&gt;iconv&lt;/li&gt; 
 &lt;li&gt;gmp&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-PHP"&gt;include "ccxt.php";
var_dump (\ccxt\Exchange::$exchanges); // print a list of all available exchange classes
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The library supports concurrent asynchronous mode using tools from &lt;a href="https://reactphp.org/"&gt;ReactPHP&lt;/a&gt; in PHP 8.1+. Read the &lt;a href="https://github.com/ccxt/ccxt/wiki/"&gt;Manual&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;.net/C#&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.nuget.org/packages/ccxt"&gt;ccxt in C# with &lt;strong&gt;Nuget&lt;/strong&gt;&lt;/a&gt; (netstandard 2.0 and netstandard 2.1)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-c#"&gt;using ccxt;
Console.WriteLine(ccxt.Exchanges) // check this later
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Go&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://pkg.go.dev/github.com/ccxt/ccxt/go/v4"&gt;ccxt in GO with &lt;strong&gt;PKG&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;go install github.com/ccxt/ccxt/go/v4@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-Go"&gt;import "ccxt"
fmt.Println(ccxt.Exchanges)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;p&gt;You can get CCXT installed in a container along with all the supported languages and dependencies. This may be useful if you want to contribute to CCXT (e.g. run the build scripts and tests — please see the &lt;a href="https://github.com/ccxt/ccxt/raw/master/CONTRIBUTING.md"&gt;Contributing&lt;/a&gt; document for the details on that).&lt;/p&gt; 
&lt;p&gt;Using &lt;code&gt;docker-compose&lt;/code&gt; (in the cloned CCXT repository):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;docker-compose run --rm ccxt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You don't need the Docker image if you're not going to develop CCXT. If you just want to use CCXT –&amp;nbsp;just install it as a regular package into your project.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Intro&lt;/h3&gt; 
&lt;p&gt;The CCXT library consists of a public part and a private part. Anyone can use the public part immediately after installation. Public APIs provide unrestricted access to public information for all exchange markets without the need to register a user account or have an API key.&lt;/p&gt; 
&lt;p&gt;Public APIs include the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;market data&lt;/li&gt; 
 &lt;li&gt;instruments/trading pairs&lt;/li&gt; 
 &lt;li&gt;price feeds (exchange rates)&lt;/li&gt; 
 &lt;li&gt;order books&lt;/li&gt; 
 &lt;li&gt;trade history&lt;/li&gt; 
 &lt;li&gt;tickers&lt;/li&gt; 
 &lt;li&gt;OHLC(V) for charting&lt;/li&gt; 
 &lt;li&gt;other public endpoints&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In order to trade with private APIs you need to obtain API keys from an exchange's website. It usually means signing up to the exchange and creating API keys for your account. Some exchanges require personal info or identification. Sometimes verification may be necessary as well. In this case you will need to register yourself, this library will not create accounts or API keys for you. Some exchanges expose API endpoints for registering an account, but most exchanges don't. You will have to sign up and create API keys on their websites.&lt;/p&gt; 
&lt;p&gt;Private APIs allow the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;manage personal account info&lt;/li&gt; 
 &lt;li&gt;query account balances&lt;/li&gt; 
 &lt;li&gt;trade by making market and limit orders&lt;/li&gt; 
 &lt;li&gt;deposit and withdraw fiat and crypto funds&lt;/li&gt; 
 &lt;li&gt;query personal orders&lt;/li&gt; 
 &lt;li&gt;get ledger history&lt;/li&gt; 
 &lt;li&gt;transfer funds between accounts&lt;/li&gt; 
 &lt;li&gt;use merchant services&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This library implements full public and private REST and WebSocket APIs for all exchanges in TypeScript, JavaScript, PHP and Python.&lt;/p&gt; 
&lt;p&gt;The CCXT library supports both camelcase notation (preferred in TypeScript and JavaScript) and underscore notation (preferred in Python and PHP), therefore all methods can be called in either notation or coding style in any language.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-JavaScript"&gt;// both of these notations work in JavaScript/Python/PHP
exchange.methodName ()  // camelcase pseudocode
exchange.method_name () // underscore pseudocode
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Read the &lt;a href="https://github.com/ccxt/ccxt/wiki/"&gt;Manual&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;JavaScript&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;CCXT now supports ESM and CJS modules&lt;/strong&gt;&lt;/p&gt; 
&lt;h4&gt;CJS&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-JavaScript"&gt;// cjs example
'use strict';
const ccxt = require ('ccxt');

(async function () {
    let kraken    = new ccxt.kraken ()
    let bitfinex  = new ccxt.bitfinex ({ verbose: true })
    let huobipro  = new ccxt.huobipro ()
    let okcoinusd = new ccxt.okcoin ({
        apiKey: 'YOUR_PUBLIC_API_KEY',
        secret: 'YOUR_SECRET_PRIVATE_KEY',
    })

    const exchangeId = 'binance'
        , exchangeClass = ccxt[exchangeId]
        , exchange = new exchangeClass ({
            'apiKey': 'YOUR_API_KEY',
            'secret': 'YOUR_SECRET',
        })

    console.log (kraken.id,    await kraken.loadMarkets ())
    console.log (bitfinex.id,  await bitfinex.loadMarkets  ())
    console.log (huobipro.id,  await huobipro.loadMarkets ())

    console.log (kraken.id,    await kraken.fetchOrderBook (kraken.symbols[0]))
    console.log (bitfinex.id,  await bitfinex.fetchTicker ('BTC/USD'))
    console.log (huobipro.id,  await huobipro.fetchTrades ('ETH/USDT'))

    console.log (okcoinusd.id, await okcoinusd.fetchBalance ())

    // sell 1 BTC/USD for market price, sell a bitcoin for dollars immediately
    console.log (okcoinusd.id, await okcoinusd.createMarketSellOrder ('BTC/USD', 1))

    // buy 1 BTC/USD for $2500, you pay $2500 and receive ฿1 when the order is closed
    console.log (okcoinusd.id, await okcoinusd.createLimitBuyOrder ('BTC/USD', 1, 2500.00))

    // pass/redefine custom exchange-specific order params: type, amount, price or whatever
    // use a custom order type
    bitfinex.createLimitSellOrder ('BTC/USD', 1, 10, { 'type': 'trailing-stop' })

}) ();
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;ESM&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-Javascript"&gt;//esm example
import {version, binance} from 'ccxt';

console.log(version);
const exchange = new binance();
const ticker = await exchange.fetchTicker('BTC/USDT');
console.log(ticker);
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Python&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-Python"&gt;# coding=utf-8

import ccxt

hitbtc   = ccxt.hitbtc({'verbose': True})
bitmex   = ccxt.bitmex()
huobipro = ccxt.huobipro()
exmo     = ccxt.exmo({
    'apiKey': 'YOUR_PUBLIC_API_KEY',
    'secret': 'YOUR_SECRET_PRIVATE_KEY',
})
kraken = ccxt.kraken({
    'apiKey': 'YOUR_PUBLIC_API_KEY',
    'secret': 'YOUR_SECRET_PRIVATE_KEY',
})

exchange_id = 'binance'
exchange_class = getattr(ccxt, exchange_id)
exchange = exchange_class({
    'apiKey': 'YOUR_API_KEY',
    'secret': 'YOUR_SECRET',
})

hitbtc_markets = hitbtc.load_markets()

print(hitbtc.id, hitbtc_markets)
print(bitmex.id, bitmex.load_markets())
print(huobipro.id, huobipro.load_markets())

print(hitbtc.fetch_order_book(hitbtc.symbols[0]))
print(bitmex.fetch_ticker('BTC/USD'))
print(huobipro.fetch_trades('LTC/USDT'))

print(exmo.fetch_balance())

# sell one ฿ for market price and receive $ right now
print(exmo.id, exmo.create_market_sell_order('BTC/USD', 1))

# limit buy BTC/EUR, you pay €2500 and receive ฿1  when the order is closed
print(exmo.id, exmo.create_limit_buy_order('BTC/EUR', 1, 2500.00))

# pass/redefine custom exchange-specific order params: type, amount, price, flags, etc...
kraken.create_market_buy_order('BTC/USD', 1, {'trading_agreement': 'agree'})
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;PHP&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-PHP"&gt;include 'ccxt.php';

$poloniex = new \ccxt\poloniex ();
$bittrex  = new \ccxt\bittrex  (array ('verbose' =&amp;gt; true));
$quoinex  = new \ccxt\quoinex   ();
$zaif     = new \ccxt\zaif     (array (
    'apiKey' =&amp;gt; 'YOUR_PUBLIC_API_KEY',
    'secret' =&amp;gt; 'YOUR_SECRET_PRIVATE_KEY',
));
$hitbtc   = new \ccxt\hitbtc   (array (
    'apiKey' =&amp;gt; 'YOUR_PUBLIC_API_KEY',
    'secret' =&amp;gt; 'YOUR_SECRET_PRIVATE_KEY',
));

$exchange_id = 'binance';
$exchange_class = "\\ccxt\\$exchange_id";
$exchange = new $exchange_class (array (
    'apiKey' =&amp;gt; 'YOUR_API_KEY',
    'secret' =&amp;gt; 'YOUR_SECRET',
));

$poloniex_markets = $poloniex-&amp;gt;load_markets ();

var_dump ($poloniex_markets);
var_dump ($bittrex-&amp;gt;load_markets ());
var_dump ($quoinex-&amp;gt;load_markets ());

var_dump ($poloniex-&amp;gt;fetch_order_book ($poloniex-&amp;gt;symbols[0]));
var_dump ($bittrex-&amp;gt;fetch_trades ('BTC/USD'));
var_dump ($quoinex-&amp;gt;fetch_ticker ('ETH/EUR'));
var_dump ($zaif-&amp;gt;fetch_ticker ('BTC/JPY'));

var_dump ($zaif-&amp;gt;fetch_balance ());

// sell 1 BTC/JPY for market price, you pay ¥ and receive ฿ immediately
var_dump ($zaif-&amp;gt;id, $zaif-&amp;gt;create_market_sell_order ('BTC/JPY', 1));

// buy BTC/JPY, you receive ฿1 for ¥285000 when the order closes
var_dump ($zaif-&amp;gt;id, $zaif-&amp;gt;create_limit_buy_order ('BTC/JPY', 1, 285000));

// set a custom user-defined id to your order
$hitbtc-&amp;gt;create_order ('BTC/USD', 'limit', 'buy', 1, 3000, array ('clientOrderId' =&amp;gt; '123'));
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;.net/C#&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-C#"&gt;using ccxt; // importing ccxt
namespace Project;
class Project {
    public async static Task CreateOrder() {
        var exchange = new Binance();
        exchange.apiKey = "my api key";
        exchange.secret = "my secret";
        // always use the capitalized method (CreateOrder instead of createOrder)
        var order = await exchange.CreateOrder("BTC/USDT", "limit", "buy", 1, 50);
        Console.WriteLine("Placed Order, order id: " + order.id);
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Go&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-Go"&gt;package main
import (
	"github.com/ccxt/ccxt/go/v4/go"
	"fmt"
)

func main() {
	exchange := ccxt.NewBinance(map[string]interface{}{
		"apiKey": "MY KEY",
		"secret": "MY SECRET",
	})
	orderParams := map[string]interface{}{
		"clientOrderId": "myOrderId68768678",
	}

    exchange.LoadMarkets()

	order, err := exchange.CreateOrder("BTC/USDT", "limit", "buy", 0.001, ccxt.WithCreateOrderPrice(6000), ccxt.WithCreateOrderParams(orderParams))
	if err != nil {
		if ccxtError, ok := err.(*ccxt.Error); ok {
			if ccxtError.Type == "InvalidOrder" {
				fmt.Println("Invalid order")
			} else {
				fmt.Println("Some other error")
			}
		}
	} else {
		fmt.Println(*order.Id)
	}


    // fetching OHLCV
	ohlcv, err := exchange.FetchOHLCV("BTC/USDT", ccxt.WithFetchOHLCVTimeframe("5m"), ccxt.WithFetchOHLCVLimit(100))

	if err != nil {
		fmt.Println("Error: ", err)
	} else {
		fmt.Println("Got OHLCV!")
	}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Optional parameters&lt;/h4&gt; 
&lt;p&gt;Unlike Javascript/Python/PHP/C# Go does not support "traditional" optional parameters like &lt;code&gt;function a(optional = false)&lt;/code&gt;. However, the CCXT language and structure have some methods with optional params, and since the Go language is transpiled from the Typescript source, we had to find a way of representing them.&lt;/p&gt; 
&lt;p&gt;We have decided to "go" (pun intended) with Option structs and the &lt;code&gt;WithX&lt;/code&gt; methods.&lt;/p&gt; 
&lt;p&gt;For example, this function &lt;code&gt;FetchMyTrades&lt;/code&gt; supports 4 different "optional" parameters, symbol, since, limit, and params.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-Golang"&gt;func (this *Binance) FetchMyTrades(options ...FetchMyTradesOptions) ([]Trade, error)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And we can provide them by doing&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-Golang"&gt;trades, error := exchange.FetchMyTrades(ccxt.withFetchMyTradesSymbol("BTC/USDT"), ccxt.WithFetchOHLCVLimit(5), ccxt.WithFetchMyTradesParams(orderParams))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Lastly, just because the signature dictates that some argument like &lt;code&gt;symbol&lt;/code&gt; is optional, it will depend from exchange to exchange and you might need to provide it to avoid getting a &lt;code&gt;SymbolRequired&lt;/code&gt; error.&lt;/p&gt; 
&lt;p&gt;You can check different examples in the &lt;code&gt;examples/go&lt;/code&gt; folder.&lt;/p&gt; 
&lt;h2&gt;CCXT CLI&lt;/h2&gt; 
&lt;p&gt;Read the documentation for more information and details: &lt;a href="https://github.com/ccxt/ccxt/tree/master/cli/README.md"&gt;docs&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;CCXT also provides a command-line interface (CLI) that enables direct interaction with any supported exchange from the terminal. You can quickly check balances, place orders, or fetch trade data—without the need to write or execute custom code. This is especially useful for simple or time-sensitive tasks that don’t warrant the overhead of building a full application.&lt;/p&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;The CLI is available as a npm package and can be installed by doing&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;npm i ccxt-cli -g
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Usage&lt;/h3&gt; 
&lt;p&gt;You can use the &lt;code&gt;--help&lt;/code&gt; option to view a general overview of how the CLI works. The tool allows you to invoke any CCXT method by specifying the exchange id, the methodName, and any required arguments.&lt;/p&gt; 
&lt;p&gt;Examples:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;ccxt binance createOrder BTC/USDT market buy 0.1 // places an order
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are not sure which arguments should be provided you can always use the &lt;code&gt;explain&lt;/code&gt; command.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;ccxt explain createOrder
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;result:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Method: createOrder
Usage:
  binance createOrder &amp;lt;symbol&amp;gt; &amp;lt;type&amp;gt; &amp;lt;side&amp;gt; &amp;lt;amount&amp;gt; [price] [params]

Arguments:
  - symbol       (required) — Market symbol e.g., BTC/USDT
  - type         (required) — (no description available)
  - side         (required) — order side e.g., buy or sell
  - amount       (required) — (no description available)
  - price        (optional) — Price per unit of asset e.g., 26000.50
  - params       (optional) — Extra parameters for the exchange e.g., { "recvWindow": 5000 }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can easily provide API keys by setting them as environment varibales eg: &lt;code&gt;BINANCE_APIKEY="XXXX"&lt;/code&gt; or adding them to the config file located at &lt;code&gt;$CACHE/config.json&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Please read the &lt;a href="https://github.com/ccxt/ccxt/raw/master/CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt; document before making changes that you would like adopted in the code. Also, read the &lt;a href="https://github.com/ccxt/ccxt/wiki"&gt;Manual&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Support Developer Team&lt;/h2&gt; 
&lt;p&gt;We are investing a significant amount of time into the development of this library. If CCXT made your life easier and you want to help us improve it further, or if you want to speed up development of new features and exchanges, please support us with a tip. We appreciate all contributions!&lt;/p&gt; 
&lt;h3&gt;Sponsors&lt;/h3&gt; 
&lt;p&gt;Support this project by becoming a sponsor.&lt;/p&gt; 
&lt;p&gt;[&lt;a href="https://opencollective.com/ccxt#sponsor"&gt;Become a sponsor&lt;/a&gt;]&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://opencollective.com/ccxt/tiers/sponsor/0/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/sponsor/0/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/sponsor/1/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/sponsor/1/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/sponsor/2/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/sponsor/2/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/sponsor/3/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/sponsor/3/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/sponsor/4/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/sponsor/4/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/sponsor/5/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/sponsor/5/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/sponsor/6/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/sponsor/6/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/sponsor/7/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/sponsor/7/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/sponsor/8/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/sponsor/8/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/sponsor/9/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/sponsor/9/avatar.svg?sanitize=true" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Supporters&lt;/h3&gt; 
&lt;p&gt;Support this project by becoming a supporter. Your avatar will show up here with a link to your website.&lt;/p&gt; 
&lt;p&gt;[&lt;a href="https://opencollective.com/ccxt#supporter"&gt;Become a supporter&lt;/a&gt;]&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://opencollective.com/ccxt/tiers/supporter/0/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/supporter/0/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/supporter/1/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/supporter/1/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/supporter/2/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/supporter/2/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/supporter/3/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/supporter/3/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/supporter/4/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/supporter/4/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/supporter/5/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/supporter/5/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/supporter/6/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/supporter/6/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/supporter/7/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/supporter/7/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/supporter/8/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/supporter/8/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/supporter/9/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/supporter/9/avatar.svg?sanitize=true" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Backers&lt;/h3&gt; 
&lt;p&gt;Thank you to all our backers! [&lt;a href="https://opencollective.com/ccxt#backer"&gt;Become a backer&lt;/a&gt;]&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://opencollective.com/ccxt#backers" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/backer.svg?width=890" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Thank you!&lt;/p&gt; 
&lt;h2&gt;Social&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;sub&gt;&lt;a href="https://twitter.com/ccxt_official"&gt;&lt;img src="https://img.shields.io/twitter/follow/ccxt_official?style=social" alt="Twitter" /&gt;&lt;/a&gt;&lt;/sub&gt; Follow us on Twitter&lt;/li&gt; 
 &lt;li&gt;&lt;sub&gt;&lt;a href="https://medium.com/@ccxt"&gt;&lt;img src="https://img.shields.io/badge/read-our%20blog-black?logo=medium" alt="Medium" /&gt;&lt;/a&gt;&lt;/sub&gt; Read our blog on Medium&lt;/li&gt; 
 &lt;li&gt;&lt;sub&gt;&lt;a href="https://discord.gg/dhzSKYU"&gt;&lt;img src="https://img.shields.io/discord/690203284119617602?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/sub&gt; Join our Discord&lt;/li&gt; 
 &lt;li&gt;&lt;sub&gt;&lt;a href="https://t.me/ccxt_announcements"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Channel-blue?logo=telegram" alt="Telegram Announcements" /&gt;&lt;/a&gt;&lt;/sub&gt; CCXT Channel on Telegram (important announcements)&lt;/li&gt; 
 &lt;li&gt;&lt;sub&gt;&lt;a href="https://t.me/ccxt_chat"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Chat-blue?logo=telegram" alt="Telegram Chat" /&gt;&lt;/a&gt;&lt;/sub&gt; CCXT Chat on Telegram (technical support)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#ccxt/ccxt&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=ccxt/ccxt&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;CCXT is not a service nor a server. CCXT is a software. &lt;strong&gt;CCXT is a free open source non-custodian API broker software under MIT license&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Non-custodian&lt;/strong&gt; means CCXT is not an intermediary in trading, it does not hold traders' money at any point in time, traders install CCXT and use CCXT to talk to exchanges directly.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MIT license&lt;/strong&gt; means CCXT can be used for any purpose, but use at your own risk without any warranties.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;API broker&lt;/strong&gt; means CCXT is funded with rebates from exchanges' API broker programs and it is an official API broker with many exchanges, all rebates and related fees are handled by the exchanges solely in accordance with exchanges' respective terms and conditions established by each partner exchange.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Free software&lt;/strong&gt; means CCXT is free to use and has no hidden fees, with CCXT traders pay the same trading fees they would pay to the exchanges directly.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open source&lt;/strong&gt; means anyone is allowed to use it, to look inside the code and to change everything, including other brokers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contact Us&lt;/h2&gt; 
&lt;p&gt;For business inquiries: &lt;a href="mailto:info@ccxt.trade"&gt;info@ccxt.trade&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ytdl-org/youtube-dl</title>
      <link>https://github.com/ytdl-org/youtube-dl</link>
      <description>&lt;p&gt;Command-line program to download videos from YouTube.com and other video sites&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://github.com/ytdl-org/youtube-dl/actions?query=workflow%3ACI"&gt;&lt;img src="https://github.com/ytdl-org/youtube-dl/workflows/CI/badge.svg?sanitize=true" alt="Build Status" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;youtube-dl - download videos from youtube.com or other video platforms&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#installation"&gt;INSTALLATION&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#description"&gt;DESCRIPTION&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#options"&gt;OPTIONS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#configuration"&gt;CONFIGURATION&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#output-template"&gt;OUTPUT TEMPLATE&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#format-selection"&gt;FORMAT SELECTION&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#video-selection"&gt;VIDEO SELECTION&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#faq"&gt;FAQ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#developer-instructions"&gt;DEVELOPER INSTRUCTIONS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#embedding-youtube-dl"&gt;EMBEDDING YOUTUBE-DL&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#bugs"&gt;BUGS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#copyright"&gt;COPYRIGHT&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;INSTALLATION&lt;/h1&gt; 
&lt;p&gt;To install it right away for all UNIX users (Linux, macOS, etc.), type:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo curl -L https://yt-dl.org/downloads/latest/youtube-dl -o /usr/local/bin/youtube-dl
sudo chmod a+rx /usr/local/bin/youtube-dl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you do not have curl, you can alternatively use a recent wget:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo wget https://yt-dl.org/downloads/latest/youtube-dl -O /usr/local/bin/youtube-dl
sudo chmod a+rx /usr/local/bin/youtube-dl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Windows users can &lt;a href="https://yt-dl.org/latest/youtube-dl.exe"&gt;download an .exe file&lt;/a&gt; and place it in any location on their &lt;a href="https://en.wikipedia.org/wiki/PATH_%28variable%29"&gt;PATH&lt;/a&gt; except for &lt;code&gt;%SYSTEMROOT%\System32&lt;/code&gt; (e.g. &lt;strong&gt;do not&lt;/strong&gt; put in &lt;code&gt;C:\Windows\System32&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;You can also use pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo -H pip install --upgrade youtube-dl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command will update youtube-dl if you have already installed it. See the &lt;a href="https://pypi.python.org/pypi/youtube_dl"&gt;pypi page&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;p&gt;macOS users can install youtube-dl with &lt;a href="https://brew.sh/"&gt;Homebrew&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;brew install youtube-dl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or with &lt;a href="https://www.macports.org/"&gt;MacPorts&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo port install youtube-dl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, refer to the &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#developer-instructions"&gt;developer instructions&lt;/a&gt; for how to check out and work with the git repository. For further options, including PGP signatures, see the &lt;a href="https://ytdl-org.github.io/youtube-dl/download.html"&gt;youtube-dl Download Page&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;DESCRIPTION&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;youtube-dl&lt;/strong&gt; is a command-line program to download videos from YouTube.com and a few more sites. It requires the Python interpreter, version 2.6, 2.7, or 3.2+, and it is not platform specific. It should work on your Unix box, on Windows or on macOS. It is released to the public domain, which means you can modify it, redistribute it or use it however you like.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;youtube-dl [OPTIONS] URL [URL...]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;OPTIONS&lt;/h1&gt; 
&lt;pre&gt;&lt;code&gt;-h, --help                           Print this help text and exit
--version                            Print program version and exit
-U, --update                         Update this program to latest version.
                                     Make sure that you have sufficient
                                     permissions (run with sudo if needed)
-i, --ignore-errors                  Continue on download errors, for
                                     example to skip unavailable videos in a
                                     playlist
--abort-on-error                     Abort downloading of further videos (in
                                     the playlist or the command line) if an
                                     error occurs
--dump-user-agent                    Display the current browser
                                     identification
--list-extractors                    List all supported extractors
--extractor-descriptions             Output descriptions of all supported
                                     extractors
--force-generic-extractor            Force extraction to use the generic
                                     extractor
--default-search PREFIX              Use this prefix for unqualified URLs.
                                     For example "gvsearch2:" downloads two
                                     videos from google videos for youtube-
                                     dl "large apple". Use the value "auto"
                                     to let youtube-dl guess ("auto_warning"
                                     to emit a warning when guessing).
                                     "error" just throws an error. The
                                     default value "fixup_error" repairs
                                     broken URLs, but emits an error if this
                                     is not possible instead of searching.
--ignore-config                      Do not read configuration files. When
                                     given in the global configuration file
                                     /etc/youtube-dl.conf: Do not read the
                                     user configuration in
                                     ~/.config/youtube-dl/config
                                     (%APPDATA%/youtube-dl/config.txt on
                                     Windows)
--config-location PATH               Location of the configuration file;
                                     either the path to the config or its
                                     containing directory.
--flat-playlist                      Do not extract the videos of a
                                     playlist, only list them.
--mark-watched                       Mark videos watched (YouTube only)
--no-mark-watched                    Do not mark videos watched (YouTube
                                     only)
--no-color                           Do not emit color codes in output
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Network Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;--proxy URL                          Use the specified HTTP/HTTPS/SOCKS
                                     proxy. To enable SOCKS proxy, specify a
                                     proper scheme. For example
                                     socks5://127.0.0.1:1080/. Pass in an
                                     empty string (--proxy "") for direct
                                     connection
--socket-timeout SECONDS             Time to wait before giving up, in
                                     seconds
--source-address IP                  Client-side IP address to bind to
-4, --force-ipv4                     Make all connections via IPv4
-6, --force-ipv6                     Make all connections via IPv6
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Geo Restriction:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;--geo-verification-proxy URL         Use this proxy to verify the IP address
                                     for some geo-restricted sites. The
                                     default proxy specified by --proxy (or
                                     none, if the option is not present) is
                                     used for the actual downloading.
--geo-bypass                         Bypass geographic restriction via
                                     faking X-Forwarded-For HTTP header
--no-geo-bypass                      Do not bypass geographic restriction
                                     via faking X-Forwarded-For HTTP header
--geo-bypass-country CODE            Force bypass geographic restriction
                                     with explicitly provided two-letter ISO
                                     3166-2 country code
--geo-bypass-ip-block IP_BLOCK       Force bypass geographic restriction
                                     with explicitly provided IP block in
                                     CIDR notation
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Video Selection:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;--playlist-start NUMBER              Playlist video to start at (default is
                                     1)
--playlist-end NUMBER                Playlist video to end at (default is
                                     last)
--playlist-items ITEM_SPEC           Playlist video items to download.
                                     Specify indices of the videos in the
                                     playlist separated by commas like: "--
                                     playlist-items 1,2,5,8" if you want to
                                     download videos indexed 1, 2, 5, 8 in
                                     the playlist. You can specify range: "
                                     --playlist-items 1-3,7,10-13", it will
                                     download the videos at index 1, 2, 3,
                                     7, 10, 11, 12 and 13.
--match-title REGEX                  Download only matching titles (regex or
                                     caseless sub-string)
--reject-title REGEX                 Skip download for matching titles
                                     (regex or caseless sub-string)
--max-downloads NUMBER               Abort after downloading NUMBER files
--min-filesize SIZE                  Do not download any videos smaller than
                                     SIZE (e.g. 50k or 44.6m)
--max-filesize SIZE                  Do not download any videos larger than
                                     SIZE (e.g. 50k or 44.6m)
--date DATE                          Download only videos uploaded in this
                                     date
--datebefore DATE                    Download only videos uploaded on or
                                     before this date (i.e. inclusive)
--dateafter DATE                     Download only videos uploaded on or
                                     after this date (i.e. inclusive)
--min-views COUNT                    Do not download any videos with less
                                     than COUNT views
--max-views COUNT                    Do not download any videos with more
                                     than COUNT views
--match-filter FILTER                Generic video filter. Specify any key
                                     (see the "OUTPUT TEMPLATE" for a list
                                     of available keys) to match if the key
                                     is present, !key to check if the key is
                                     not present, key &amp;gt; NUMBER (like
                                     "comment_count &amp;gt; 12", also works with
                                     &amp;gt;=, &amp;lt;, &amp;lt;=, !=, =) to compare against a
                                     number, key = 'LITERAL' (like "uploader
                                     = 'Mike Smith'", also works with !=) to
                                     match against a string literal and &amp;amp; to
                                     require multiple matches. Values which
                                     are not known are excluded unless you
                                     put a question mark (?) after the
                                     operator. For example, to only match
                                     videos that have been liked more than
                                     100 times and disliked less than 50
                                     times (or the dislike functionality is
                                     not available at the given service),
                                     but who also have a description, use
                                     --match-filter "like_count &amp;gt; 100 &amp;amp;
                                     dislike_count &amp;lt;? 50 &amp;amp; description" .
--no-playlist                        Download only the video, if the URL
                                     refers to a video and a playlist.
--yes-playlist                       Download the playlist, if the URL
                                     refers to a video and a playlist.
--age-limit YEARS                    Download only videos suitable for the
                                     given age
--download-archive FILE              Download only videos not listed in the
                                     archive file. Record the IDs of all
                                     downloaded videos in it.
--include-ads                        Download advertisements as well
                                     (experimental)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Download Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;-r, --limit-rate RATE                Maximum download rate in bytes per
                                     second (e.g. 50K or 4.2M)
-R, --retries RETRIES                Number of retries (default is 10), or
                                     "infinite".
--fragment-retries RETRIES           Number of retries for a fragment
                                     (default is 10), or "infinite" (DASH,
                                     hlsnative and ISM)
--skip-unavailable-fragments         Skip unavailable fragments (DASH,
                                     hlsnative and ISM)
--abort-on-unavailable-fragment      Abort downloading when some fragment is
                                     not available
--keep-fragments                     Keep downloaded fragments on disk after
                                     downloading is finished; fragments are
                                     erased by default
--buffer-size SIZE                   Size of download buffer (e.g. 1024 or
                                     16K) (default is 1024)
--no-resize-buffer                   Do not automatically adjust the buffer
                                     size. By default, the buffer size is
                                     automatically resized from an initial
                                     value of SIZE.
--http-chunk-size SIZE               Size of a chunk for chunk-based HTTP
                                     downloading (e.g. 10485760 or 10M)
                                     (default is disabled). May be useful
                                     for bypassing bandwidth throttling
                                     imposed by a webserver (experimental)
--playlist-reverse                   Download playlist videos in reverse
                                     order
--playlist-random                    Download playlist videos in random
                                     order
--xattr-set-filesize                 Set file xattribute ytdl.filesize with
                                     expected file size
--hls-prefer-native                  Use the native HLS downloader instead
                                     of ffmpeg
--hls-prefer-ffmpeg                  Use ffmpeg instead of the native HLS
                                     downloader
--hls-use-mpegts                     Use the mpegts container for HLS
                                     videos, allowing to play the video
                                     while downloading (some players may not
                                     be able to play it)
--external-downloader COMMAND        Use the specified external downloader.
                                     Currently supports aria2c,avconv,axel,c
                                     url,ffmpeg,httpie,wget
--external-downloader-args ARGS      Give these arguments to the external
                                     downloader
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Filesystem Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;-a, --batch-file FILE                File containing URLs to download ('-'
                                     for stdin), one URL per line. Lines
                                     starting with '#', ';' or ']' are
                                     considered as comments and ignored.
--id                                 Use only video ID in file name
-o, --output TEMPLATE                Output filename template, see the
                                     "OUTPUT TEMPLATE" for all the info
--output-na-placeholder PLACEHOLDER  Placeholder value for unavailable meta
                                     fields in output filename template
                                     (default is "NA")
--autonumber-start NUMBER            Specify the start value for
                                     %(autonumber)s (default is 1)
--restrict-filenames                 Restrict filenames to only ASCII
                                     characters, and avoid "&amp;amp;" and spaces in
                                     filenames
-w, --no-overwrites                  Do not overwrite files
-c, --continue                       Force resume of partially downloaded
                                     files. By default, youtube-dl will
                                     resume downloads if possible.
--no-continue                        Do not resume partially downloaded
                                     files (restart from beginning)
--no-part                            Do not use .part files - write directly
                                     into output file
--no-mtime                           Do not use the Last-modified header to
                                     set the file modification time
--write-description                  Write video description to a
                                     .description file
--write-info-json                    Write video metadata to a .info.json
                                     file
--write-annotations                  Write video annotations to a
                                     .annotations.xml file
--load-info-json FILE                JSON file containing the video
                                     information (created with the "--write-
                                     info-json" option)
--cookies FILE                       File to read cookies from and dump
                                     cookie jar in
--cache-dir DIR                      Location in the filesystem where
                                     youtube-dl can store some downloaded
                                     information permanently. By default
                                     $XDG_CACHE_HOME/youtube-dl or
                                     ~/.cache/youtube-dl . At the moment,
                                     only YouTube player files (for videos
                                     with obfuscated signatures) are cached,
                                     but that may change.
--no-cache-dir                       Disable filesystem caching
--rm-cache-dir                       Delete all filesystem cache files
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Thumbnail Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;--write-thumbnail                    Write thumbnail image to disk
--write-all-thumbnails               Write all thumbnail image formats to
                                     disk
--list-thumbnails                    Simulate and list all available
                                     thumbnail formats
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Verbosity / Simulation Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;-q, --quiet                          Activate quiet mode
--no-warnings                        Ignore warnings
-s, --simulate                       Do not download the video and do not
                                     write anything to disk
--skip-download                      Do not download the video
-g, --get-url                        Simulate, quiet but print URL
-e, --get-title                      Simulate, quiet but print title
--get-id                             Simulate, quiet but print id
--get-thumbnail                      Simulate, quiet but print thumbnail URL
--get-description                    Simulate, quiet but print video
                                     description
--get-duration                       Simulate, quiet but print video length
--get-filename                       Simulate, quiet but print output
                                     filename
--get-format                         Simulate, quiet but print output format
-j, --dump-json                      Simulate, quiet but print JSON
                                     information. See the "OUTPUT TEMPLATE"
                                     for a description of available keys.
-J, --dump-single-json               Simulate, quiet but print JSON
                                     information for each command-line
                                     argument. If the URL refers to a
                                     playlist, dump the whole playlist
                                     information in a single line.
--print-json                         Be quiet and print the video
                                     information as JSON (video is still
                                     being downloaded).
--newline                            Output progress bar as new lines
--no-progress                        Do not print progress bar
--console-title                      Display progress in console titlebar
-v, --verbose                        Print various debugging information
--dump-pages                         Print downloaded pages encoded using
                                     base64 to debug problems (very verbose)
--write-pages                        Write downloaded intermediary pages to
                                     files in the current directory to debug
                                     problems
--print-traffic                      Display sent and read HTTP traffic
-C, --call-home                      Contact the youtube-dl server for
                                     debugging
--no-call-home                       Do NOT contact the youtube-dl server
                                     for debugging
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Workarounds:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;--encoding ENCODING                  Force the specified encoding
                                     (experimental)
--no-check-certificate               Suppress HTTPS certificate validation
--prefer-insecure                    Use an unencrypted connection to
                                     retrieve information about the video.
                                     (Currently supported only for YouTube)
--user-agent UA                      Specify a custom user agent
--referer URL                        Specify a custom referer, use if the
                                     video access is restricted to one
                                     domain
--add-header FIELD:VALUE             Specify a custom HTTP header and its
                                     value, separated by a colon ':'. You
                                     can use this option multiple times
--bidi-workaround                    Work around terminals that lack
                                     bidirectional text support. Requires
                                     bidiv or fribidi executable in PATH
--sleep-interval SECONDS             Number of seconds to sleep before each
                                     download when used alone or a lower
                                     bound of a range for randomized sleep
                                     before each download (minimum possible
                                     number of seconds to sleep) when used
                                     along with --max-sleep-interval.
--max-sleep-interval SECONDS         Upper bound of a range for randomized
                                     sleep before each download (maximum
                                     possible number of seconds to sleep).
                                     Must only be used along with --min-
                                     sleep-interval.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Video Format Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;-f, --format FORMAT                  Video format code, see the "FORMAT
                                     SELECTION" for all the info
--all-formats                        Download all available video formats
--prefer-free-formats                Prefer free video formats unless a
                                     specific one is requested
-F, --list-formats                   List all available formats of requested
                                     videos
--youtube-skip-dash-manifest         Do not download the DASH manifests and
                                     related data on YouTube videos
--merge-output-format FORMAT         If a merge is required (e.g.
                                     bestvideo+bestaudio), output to given
                                     container format. One of mkv, mp4, ogg,
                                     webm, flv. Ignored if no merge is
                                     required
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Subtitle Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;--write-sub                          Write subtitle file
--write-auto-sub                     Write automatically generated subtitle
                                     file (YouTube only)
--all-subs                           Download all the available subtitles of
                                     the video
--list-subs                          List all available subtitles for the
                                     video
--sub-format FORMAT                  Subtitle format, accepts formats
                                     preference, for example: "srt" or
                                     "ass/srt/best"
--sub-lang LANGS                     Languages of the subtitles to download
                                     (optional) separated by commas, use
                                     --list-subs for available language tags
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Authentication Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;-u, --username USERNAME              Login with this account ID
-p, --password PASSWORD              Account password. If this option is
                                     left out, youtube-dl will ask
                                     interactively.
-2, --twofactor TWOFACTOR            Two-factor authentication code
-n, --netrc                          Use .netrc authentication data
--video-password PASSWORD            Video password (vimeo, youku)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Adobe Pass Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;--ap-mso MSO                         Adobe Pass multiple-system operator (TV
                                     provider) identifier, use --ap-list-mso
                                     for a list of available MSOs
--ap-username USERNAME               Multiple-system operator account login
--ap-password PASSWORD               Multiple-system operator account
                                     password. If this option is left out,
                                     youtube-dl will ask interactively.
--ap-list-mso                        List all supported multiple-system
                                     operators
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Post-processing Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;-x, --extract-audio                  Convert video files to audio-only files
                                     (requires ffmpeg/avconv and
                                     ffprobe/avprobe)
--audio-format FORMAT                Specify audio format: "best", "aac",
                                     "flac", "mp3", "m4a", "opus", "vorbis",
                                     or "wav"; "best" by default; No effect
                                     without -x
--audio-quality QUALITY              Specify ffmpeg/avconv audio quality,
                                     insert a value between 0 (better) and 9
                                     (worse) for VBR or a specific bitrate
                                     like 128K (default 5)
--recode-video FORMAT                Encode the video to another format if
                                     necessary (currently supported:
                                     mp4|flv|ogg|webm|mkv|avi)
--postprocessor-args ARGS            Give these arguments to the
                                     postprocessor
-k, --keep-video                     Keep the video file on disk after the
                                     post-processing; the video is erased by
                                     default
--no-post-overwrites                 Do not overwrite post-processed files;
                                     the post-processed files are
                                     overwritten by default
--embed-subs                         Embed subtitles in the video (only for
                                     mp4, webm and mkv videos)
--embed-thumbnail                    Embed thumbnail in the audio as cover
                                     art
--add-metadata                       Write metadata to the video file
--metadata-from-title FORMAT         Parse additional metadata like song
                                     title / artist from the video title.
                                     The format syntax is the same as
                                     --output. Regular expression with named
                                     capture groups may also be used. The
                                     parsed parameters replace existing
                                     values. Example: --metadata-from-title
                                     "%(artist)s - %(title)s" matches a
                                     title like "Coldplay - Paradise".
                                     Example (regex): --metadata-from-title
                                     "(?P&amp;lt;artist&amp;gt;.+?) - (?P&amp;lt;title&amp;gt;.+)"
--xattrs                             Write metadata to the video file's
                                     xattrs (using dublin core and xdg
                                     standards)
--fixup POLICY                       Automatically correct known faults of
                                     the file. One of never (do nothing),
                                     warn (only emit a warning),
                                     detect_or_warn (the default; fix file
                                     if we can, warn otherwise)
--prefer-avconv                      Prefer avconv over ffmpeg for running
                                     the postprocessors
--prefer-ffmpeg                      Prefer ffmpeg over avconv for running
                                     the postprocessors (default)
--ffmpeg-location PATH               Location of the ffmpeg/avconv binary;
                                     either the path to the binary or its
                                     containing directory.
--exec CMD                           Execute a command on the file after
                                     downloading and post-processing,
                                     similar to find's -exec syntax.
                                     Example: --exec 'adb push {}
                                     /sdcard/Music/ &amp;amp;&amp;amp; rm {}'
--convert-subs FORMAT                Convert the subtitles to other format
                                     (currently supported: srt|ass|vtt|lrc)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;CONFIGURATION&lt;/h1&gt; 
&lt;p&gt;You can configure youtube-dl by placing any supported command line option to a configuration file. On Linux and macOS, the system wide configuration file is located at &lt;code&gt;/etc/youtube-dl.conf&lt;/code&gt; and the user wide configuration file at &lt;code&gt;~/.config/youtube-dl/config&lt;/code&gt;. On Windows, the user wide configuration file locations are &lt;code&gt;%APPDATA%\youtube-dl\config.txt&lt;/code&gt; or &lt;code&gt;C:\Users\&amp;lt;user name&amp;gt;\youtube-dl.conf&lt;/code&gt;. Note that by default configuration file may not exist so you may need to create it yourself.&lt;/p&gt; 
&lt;p&gt;For example, with the following configuration file youtube-dl will always extract the audio, not copy the mtime, use a proxy and save all videos under &lt;code&gt;Movies&lt;/code&gt; directory in your home directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# Lines starting with # are comments

# Always extract audio
-x

# Do not copy the mtime
--no-mtime

# Use this proxy
--proxy 127.0.0.1:3128

# Save all videos under Movies directory in your home directory
-o ~/Movies/%(title)s.%(ext)s
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that options in configuration file are just the same options aka switches used in regular command line calls thus there &lt;strong&gt;must be no whitespace&lt;/strong&gt; after &lt;code&gt;-&lt;/code&gt; or &lt;code&gt;--&lt;/code&gt;, e.g. &lt;code&gt;-o&lt;/code&gt; or &lt;code&gt;--proxy&lt;/code&gt; but not &lt;code&gt;- o&lt;/code&gt; or &lt;code&gt;-- proxy&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can use &lt;code&gt;--ignore-config&lt;/code&gt; if you want to disable the configuration file for a particular youtube-dl run.&lt;/p&gt; 
&lt;p&gt;You can also use &lt;code&gt;--config-location&lt;/code&gt; if you want to use custom configuration file for a particular youtube-dl run.&lt;/p&gt; 
&lt;h3&gt;Authentication with &lt;code&gt;.netrc&lt;/code&gt; file&lt;/h3&gt; 
&lt;p&gt;You may also want to configure automatic credentials storage for extractors that support authentication (by providing login and password with &lt;code&gt;--username&lt;/code&gt; and &lt;code&gt;--password&lt;/code&gt;) in order not to pass credentials as command line arguments on every youtube-dl execution and prevent tracking plain text passwords in the shell command history. You can achieve this using a &lt;a href="https://stackoverflow.com/tags/.netrc/info"&gt;&lt;code&gt;.netrc&lt;/code&gt; file&lt;/a&gt; on a per extractor basis. For that you will need to create a &lt;code&gt;.netrc&lt;/code&gt; file in your &lt;code&gt;$HOME&lt;/code&gt; and restrict permissions to read/write by only you:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;touch $HOME/.netrc
chmod a-rwx,u+rw $HOME/.netrc
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After that you can add credentials for an extractor in the following format, where &lt;em&gt;extractor&lt;/em&gt; is the name of the extractor in lowercase:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;machine &amp;lt;extractor&amp;gt; login &amp;lt;login&amp;gt; password &amp;lt;password&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;machine youtube login myaccount@gmail.com password my_youtube_password
machine twitch login my_twitch_account_name password my_twitch_password
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To activate authentication with the &lt;code&gt;.netrc&lt;/code&gt; file you should pass &lt;code&gt;--netrc&lt;/code&gt; to youtube-dl or place it in the &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#configuration"&gt;configuration file&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;On Windows you may also need to setup the &lt;code&gt;%HOME%&lt;/code&gt; environment variable manually. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;set HOME=%USERPROFILE%
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;OUTPUT TEMPLATE&lt;/h1&gt; 
&lt;p&gt;The &lt;code&gt;-o&lt;/code&gt; option allows users to indicate a template for the output file names.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;tl;dr:&lt;/strong&gt; &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#output-template-examples"&gt;navigate me to examples&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The basic usage is not to set any template arguments when downloading a single file, like in &lt;code&gt;youtube-dl -o funny_video.flv "https://some/video"&lt;/code&gt;. However, it may contain special sequences that will be replaced when downloading each video. The special sequences may be formatted according to &lt;a href="https://docs.python.org/2/library/stdtypes.html#string-formatting"&gt;python string formatting operations&lt;/a&gt;. For example, &lt;code&gt;%(NAME)s&lt;/code&gt; or &lt;code&gt;%(NAME)05d&lt;/code&gt;. To clarify, that is a percent symbol followed by a name in parentheses, followed by formatting operations. Allowed names along with sequence type are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;id&lt;/code&gt; (string): Video identifier&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;title&lt;/code&gt; (string): Video title&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;url&lt;/code&gt; (string): Video URL&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ext&lt;/code&gt; (string): Video filename extension&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;alt_title&lt;/code&gt; (string): A secondary title of the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;display_id&lt;/code&gt; (string): An alternative identifier for the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;uploader&lt;/code&gt; (string): Full name of the video uploader&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;license&lt;/code&gt; (string): License name the video is licensed under&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;creator&lt;/code&gt; (string): The creator of the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;release_date&lt;/code&gt; (string): The date (YYYYMMDD) when the video was released&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;timestamp&lt;/code&gt; (numeric): UNIX timestamp of the moment the video became available&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;upload_date&lt;/code&gt; (string): Video upload date (YYYYMMDD)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;uploader_id&lt;/code&gt; (string): Nickname or id of the video uploader&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;channel&lt;/code&gt; (string): Full name of the channel the video is uploaded on&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;channel_id&lt;/code&gt; (string): Id of the channel&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;location&lt;/code&gt; (string): Physical location where the video was filmed&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;duration&lt;/code&gt; (numeric): Length of the video in seconds&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;view_count&lt;/code&gt; (numeric): How many users have watched the video on the platform&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;like_count&lt;/code&gt; (numeric): Number of positive ratings of the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;dislike_count&lt;/code&gt; (numeric): Number of negative ratings of the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;repost_count&lt;/code&gt; (numeric): Number of reposts of the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;average_rating&lt;/code&gt; (numeric): Average rating give by users, the scale used depends on the webpage&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;comment_count&lt;/code&gt; (numeric): Number of comments on the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;age_limit&lt;/code&gt; (numeric): Age restriction for the video (years)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;is_live&lt;/code&gt; (boolean): Whether this video is a live stream or a fixed-length video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;start_time&lt;/code&gt; (numeric): Time in seconds where the reproduction should start, as specified in the URL&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;end_time&lt;/code&gt; (numeric): Time in seconds where the reproduction should end, as specified in the URL&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;format&lt;/code&gt; (string): A human-readable description of the format&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;format_id&lt;/code&gt; (string): Format code specified by &lt;code&gt;--format&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;format_note&lt;/code&gt; (string): Additional info about the format&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;width&lt;/code&gt; (numeric): Width of the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;height&lt;/code&gt; (numeric): Height of the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;resolution&lt;/code&gt; (string): Textual description of width and height&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tbr&lt;/code&gt; (numeric): Average bitrate of audio and video in KBit/s&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;abr&lt;/code&gt; (numeric): Average audio bitrate in KBit/s&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;acodec&lt;/code&gt; (string): Name of the audio codec in use&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;asr&lt;/code&gt; (numeric): Audio sampling rate in Hertz&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vbr&lt;/code&gt; (numeric): Average video bitrate in KBit/s&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;fps&lt;/code&gt; (numeric): Frame rate&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vcodec&lt;/code&gt; (string): Name of the video codec in use&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;container&lt;/code&gt; (string): Name of the container format&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;filesize&lt;/code&gt; (numeric): The number of bytes, if known in advance&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;filesize_approx&lt;/code&gt; (numeric): An estimate for the number of bytes&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;protocol&lt;/code&gt; (string): The protocol that will be used for the actual download&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;extractor&lt;/code&gt; (string): Name of the extractor&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;extractor_key&lt;/code&gt; (string): Key name of the extractor&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;epoch&lt;/code&gt; (numeric): Unix epoch when creating the file&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;autonumber&lt;/code&gt; (numeric): Number that will be increased with each download, starting at &lt;code&gt;--autonumber-start&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist&lt;/code&gt; (string): Name or id of the playlist that contains the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_index&lt;/code&gt; (numeric): Index of the video in the playlist padded with leading zeros according to the total length of the playlist&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_id&lt;/code&gt; (string): Playlist identifier&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_title&lt;/code&gt; (string): Playlist title&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_uploader&lt;/code&gt; (string): Full name of the playlist uploader&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_uploader_id&lt;/code&gt; (string): Nickname or id of the playlist uploader&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Available for the video that belongs to some logical chapter or section:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;chapter&lt;/code&gt; (string): Name or title of the chapter the video belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;chapter_number&lt;/code&gt; (numeric): Number of the chapter the video belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;chapter_id&lt;/code&gt; (string): Id of the chapter the video belongs to&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Available for the video that is an episode of some series or programme:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;series&lt;/code&gt; (string): Title of the series or programme the video episode belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;season&lt;/code&gt; (string): Title of the season the video episode belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;season_number&lt;/code&gt; (numeric): Number of the season the video episode belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;season_id&lt;/code&gt; (string): Id of the season the video episode belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;episode&lt;/code&gt; (string): Title of the video episode&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;episode_number&lt;/code&gt; (numeric): Number of the video episode within a season&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;episode_id&lt;/code&gt; (string): Id of the video episode&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Available for the media that is a track or a part of a music album:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;track&lt;/code&gt; (string): Title of the track&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;track_number&lt;/code&gt; (numeric): Number of the track within an album or a disc&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;track_id&lt;/code&gt; (string): Id of the track&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;artist&lt;/code&gt; (string): Artist(s) of the track&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;genre&lt;/code&gt; (string): Genre(s) of the track&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;album&lt;/code&gt; (string): Title of the album the track belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;album_type&lt;/code&gt; (string): Type of the album&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;album_artist&lt;/code&gt; (string): List of all artists appeared on the album&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;disc_number&lt;/code&gt; (numeric): Number of the disc or other physical medium the track belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;release_year&lt;/code&gt; (numeric): Year (YYYY) when the album was released&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Each aforementioned sequence when referenced in an output template will be replaced by the actual value corresponding to the sequence name. Note that some of the sequences are not guaranteed to be present since they depend on the metadata obtained by a particular extractor. Such sequences will be replaced with placeholder value provided with &lt;code&gt;--output-na-placeholder&lt;/code&gt; (&lt;code&gt;NA&lt;/code&gt; by default).&lt;/p&gt; 
&lt;p&gt;For example for &lt;code&gt;-o %(title)s-%(id)s.%(ext)s&lt;/code&gt; and an mp4 video with title &lt;code&gt;youtube-dl test video&lt;/code&gt; and id &lt;code&gt;BaW_jenozKcj&lt;/code&gt;, this will result in a &lt;code&gt;youtube-dl test video-BaW_jenozKcj.mp4&lt;/code&gt; file created in the current directory.&lt;/p&gt; 
&lt;p&gt;For numeric sequences you can use numeric related formatting, for example, &lt;code&gt;%(view_count)05d&lt;/code&gt; will result in a string with view count padded with zeros up to 5 characters, like in &lt;code&gt;00042&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Output templates can also contain arbitrary hierarchical path, e.g. &lt;code&gt;-o '%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s'&lt;/code&gt; which will result in downloading each video in a directory corresponding to this path template. Any missing directory will be automatically created for you.&lt;/p&gt; 
&lt;p&gt;To use percent literals in an output template use &lt;code&gt;%%&lt;/code&gt;. To output to stdout use &lt;code&gt;-o -&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;The current default template is &lt;code&gt;%(title)s-%(id)s.%(ext)s&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;In some cases, you don't want special characters such as 中, spaces, or &amp;amp;, such as when transferring the downloaded filename to a Windows system or the filename through an 8bit-unsafe channel. In these cases, add the &lt;code&gt;--restrict-filenames&lt;/code&gt; flag to get a shorter title.&lt;/p&gt; 
&lt;h4&gt;Output template and Windows batch files&lt;/h4&gt; 
&lt;p&gt;If you are using an output template inside a Windows batch file then you must escape plain percent characters (&lt;code&gt;%&lt;/code&gt;) by doubling, so that &lt;code&gt;-o "%(title)s-%(id)s.%(ext)s"&lt;/code&gt; should become &lt;code&gt;-o "%%(title)s-%%(id)s.%%(ext)s"&lt;/code&gt;. However you should not touch &lt;code&gt;%&lt;/code&gt;'s that are not plain characters, e.g. environment variables for expansion should stay intact: &lt;code&gt;-o "C:\%HOMEPATH%\Desktop\%%(title)s.%%(ext)s"&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;Output template examples&lt;/h4&gt; 
&lt;p&gt;Note that on Windows you may need to use double quotes instead of single.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ youtube-dl --get-filename -o '%(title)s.%(ext)s' BaW_jenozKc
youtube-dl test video ''_ä↭𝕐.mp4    # All kinds of weird characters

$ youtube-dl --get-filename -o '%(title)s.%(ext)s' BaW_jenozKc --restrict-filenames
youtube-dl_test_video_.mp4          # A simple file name

# Download YouTube playlist videos in separate directory indexed by video order in a playlist
$ youtube-dl -o '%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s' https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re

# Download all playlists of YouTube channel/user keeping each playlist in separate directory:
$ youtube-dl -o '%(uploader)s/%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s' https://www.youtube.com/user/TheLinuxFoundation/playlists

# Download Udemy course keeping each chapter in separate directory under MyVideos directory in your home
$ youtube-dl -u user -p password -o '~/MyVideos/%(playlist)s/%(chapter_number)s - %(chapter)s/%(title)s.%(ext)s' https://www.udemy.com/java-tutorial/

# Download entire series season keeping each series and each season in separate directory under C:/MyVideos
$ youtube-dl -o "C:/MyVideos/%(series)s/%(season_number)s - %(season)s/%(episode_number)s - %(episode)s.%(ext)s" https://videomore.ru/kino_v_detalayah/5_sezon/367617

# Stream the video being downloaded to stdout
$ youtube-dl -o - BaW_jenozKc
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;FORMAT SELECTION&lt;/h1&gt; 
&lt;p&gt;By default youtube-dl tries to download the best available quality, i.e. if you want the best quality you &lt;strong&gt;don't need&lt;/strong&gt; to pass any special options, youtube-dl will guess it for you by &lt;strong&gt;default&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;But sometimes you may want to download in a different format, for example when you are on a slow or intermittent connection. The key mechanism for achieving this is so-called &lt;em&gt;format selection&lt;/em&gt; based on which you can explicitly specify desired format, select formats based on some criterion or criteria, setup precedence and much more.&lt;/p&gt; 
&lt;p&gt;The general syntax for format selection is &lt;code&gt;--format FORMAT&lt;/code&gt; or shorter &lt;code&gt;-f FORMAT&lt;/code&gt; where &lt;code&gt;FORMAT&lt;/code&gt; is a &lt;em&gt;selector expression&lt;/em&gt;, i.e. an expression that describes format or formats you would like to download.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;tl;dr:&lt;/strong&gt; &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#format-selection-examples"&gt;navigate me to examples&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The simplest case is requesting a specific format, for example with &lt;code&gt;-f 22&lt;/code&gt; you can download the format with format code equal to 22. You can get the list of available format codes for particular video using &lt;code&gt;--list-formats&lt;/code&gt; or &lt;code&gt;-F&lt;/code&gt;. Note that these format codes are extractor specific.&lt;/p&gt; 
&lt;p&gt;You can also use a file extension (currently &lt;code&gt;3gp&lt;/code&gt;, &lt;code&gt;aac&lt;/code&gt;, &lt;code&gt;flv&lt;/code&gt;, &lt;code&gt;m4a&lt;/code&gt;, &lt;code&gt;mp3&lt;/code&gt;, &lt;code&gt;mp4&lt;/code&gt;, &lt;code&gt;ogg&lt;/code&gt;, &lt;code&gt;wav&lt;/code&gt;, &lt;code&gt;webm&lt;/code&gt; are supported) to download the best quality format of a particular file extension served as a single file, e.g. &lt;code&gt;-f webm&lt;/code&gt; will download the best quality format with the &lt;code&gt;webm&lt;/code&gt; extension served as a single file.&lt;/p&gt; 
&lt;p&gt;You can also use special names to select particular edge case formats:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;best&lt;/code&gt;: Select the best quality format represented by a single file with video and audio.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;worst&lt;/code&gt;: Select the worst quality format represented by a single file with video and audio.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;bestvideo&lt;/code&gt;: Select the best quality video-only format (e.g. DASH video). May not be available.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;worstvideo&lt;/code&gt;: Select the worst quality video-only format. May not be available.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;bestaudio&lt;/code&gt;: Select the best quality audio only-format. May not be available.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;worstaudio&lt;/code&gt;: Select the worst quality audio only-format. May not be available.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For example, to download the worst quality video-only format you can use &lt;code&gt;-f worstvideo&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;If you want to download multiple videos and they don't have the same formats available, you can specify the order of preference using slashes. Note that slash is left-associative, i.e. formats on the left hand side are preferred, for example &lt;code&gt;-f 22/17/18&lt;/code&gt; will download format 22 if it's available, otherwise it will download format 17 if it's available, otherwise it will download format 18 if it's available, otherwise it will complain that no suitable formats are available for download.&lt;/p&gt; 
&lt;p&gt;If you want to download several formats of the same video use a comma as a separator, e.g. &lt;code&gt;-f 22,17,18&lt;/code&gt; will download all these three formats, of course if they are available. Or a more sophisticated example combined with the precedence feature: &lt;code&gt;-f 136/137/mp4/bestvideo,140/m4a/bestaudio&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can also filter the video formats by putting a condition in brackets, as in &lt;code&gt;-f "best[height=720]"&lt;/code&gt; (or &lt;code&gt;-f "[filesize&amp;gt;10M]"&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;The following numeric meta fields can be used with comparisons &lt;code&gt;&amp;lt;&lt;/code&gt;, &lt;code&gt;&amp;lt;=&lt;/code&gt;, &lt;code&gt;&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;gt;=&lt;/code&gt;, &lt;code&gt;=&lt;/code&gt; (equals), &lt;code&gt;!=&lt;/code&gt; (not equals):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;filesize&lt;/code&gt;: The number of bytes, if known in advance&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;width&lt;/code&gt;: Width of the video, if known&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;height&lt;/code&gt;: Height of the video, if known&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tbr&lt;/code&gt;: Average bitrate of audio and video in KBit/s&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;abr&lt;/code&gt;: Average audio bitrate in KBit/s&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vbr&lt;/code&gt;: Average video bitrate in KBit/s&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;asr&lt;/code&gt;: Audio sampling rate in Hertz&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;fps&lt;/code&gt;: Frame rate&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Also filtering work for comparisons &lt;code&gt;=&lt;/code&gt; (equals), &lt;code&gt;^=&lt;/code&gt; (starts with), &lt;code&gt;$=&lt;/code&gt; (ends with), &lt;code&gt;*=&lt;/code&gt; (contains) and following string meta fields:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;ext&lt;/code&gt;: File extension&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;acodec&lt;/code&gt;: Name of the audio codec in use&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vcodec&lt;/code&gt;: Name of the video codec in use&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;container&lt;/code&gt;: Name of the container format&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;protocol&lt;/code&gt;: The protocol that will be used for the actual download, lower-case (&lt;code&gt;http&lt;/code&gt;, &lt;code&gt;https&lt;/code&gt;, &lt;code&gt;rtsp&lt;/code&gt;, &lt;code&gt;rtmp&lt;/code&gt;, &lt;code&gt;rtmpe&lt;/code&gt;, &lt;code&gt;mms&lt;/code&gt;, &lt;code&gt;f4m&lt;/code&gt;, &lt;code&gt;ism&lt;/code&gt;, &lt;code&gt;http_dash_segments&lt;/code&gt;, &lt;code&gt;m3u8&lt;/code&gt;, or &lt;code&gt;m3u8_native&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;format_id&lt;/code&gt;: A short description of the format&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;language&lt;/code&gt;: Language code&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Any string comparison may be prefixed with negation &lt;code&gt;!&lt;/code&gt; in order to produce an opposite comparison, e.g. &lt;code&gt;!*=&lt;/code&gt; (does not contain).&lt;/p&gt; 
&lt;p&gt;Note that none of the aforementioned meta fields are guaranteed to be present since this solely depends on the metadata obtained by particular extractor, i.e. the metadata offered by the video hoster.&lt;/p&gt; 
&lt;p&gt;Formats for which the value is not known are excluded unless you put a question mark (&lt;code&gt;?&lt;/code&gt;) after the operator. You can combine format filters, so &lt;code&gt;-f "[height &amp;lt;=? 720][tbr&amp;gt;500]"&lt;/code&gt; selects up to 720p videos (or videos where the height is not known) with a bitrate of at least 500 KBit/s.&lt;/p&gt; 
&lt;p&gt;You can merge the video and audio of two formats into a single file using &lt;code&gt;-f &amp;lt;video-format&amp;gt;+&amp;lt;audio-format&amp;gt;&lt;/code&gt; (requires ffmpeg or avconv installed), for example &lt;code&gt;-f bestvideo+bestaudio&lt;/code&gt; will download the best video-only format, the best audio-only format and mux them together with ffmpeg/avconv.&lt;/p&gt; 
&lt;p&gt;Format selectors can also be grouped using parentheses, for example if you want to download the best mp4 and webm formats with a height lower than 480 you can use &lt;code&gt;-f '(mp4,webm)[height&amp;lt;480]'&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Since the end of April 2015 and version 2015.04.26, youtube-dl uses &lt;code&gt;-f bestvideo+bestaudio/best&lt;/code&gt; as the default format selection (see &lt;a href="https://github.com/ytdl-org/youtube-dl/issues/5447"&gt;#5447&lt;/a&gt;, &lt;a href="https://github.com/ytdl-org/youtube-dl/issues/5456"&gt;#5456&lt;/a&gt;). If ffmpeg or avconv are installed this results in downloading &lt;code&gt;bestvideo&lt;/code&gt; and &lt;code&gt;bestaudio&lt;/code&gt; separately and muxing them together into a single file giving the best overall quality available. Otherwise it falls back to &lt;code&gt;best&lt;/code&gt; and results in downloading the best available quality served as a single file. &lt;code&gt;best&lt;/code&gt; is also needed for videos that don't come from YouTube because they don't provide the audio and video in two different files. If you want to only download some DASH formats (for example if you are not interested in getting videos with a resolution higher than 1080p), you can add &lt;code&gt;-f bestvideo[height&amp;lt;=?1080]+bestaudio/best&lt;/code&gt; to your configuration file. Note that if you use youtube-dl to stream to &lt;code&gt;stdout&lt;/code&gt; (and most likely to pipe it to your media player then), i.e. you explicitly specify output template as &lt;code&gt;-o -&lt;/code&gt;, youtube-dl still uses &lt;code&gt;-f best&lt;/code&gt; format selection in order to start content delivery immediately to your player and not to wait until &lt;code&gt;bestvideo&lt;/code&gt; and &lt;code&gt;bestaudio&lt;/code&gt; are downloaded and muxed.&lt;/p&gt; 
&lt;p&gt;If you want to preserve the old format selection behavior (prior to youtube-dl 2015.04.26), i.e. you want to download the best available quality media served as a single file, you should explicitly specify your choice with &lt;code&gt;-f best&lt;/code&gt;. You may want to add it to the &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#configuration"&gt;configuration file&lt;/a&gt; in order not to type it every time you run youtube-dl.&lt;/p&gt; 
&lt;h4&gt;Format selection examples&lt;/h4&gt; 
&lt;p&gt;Note that on Windows you may need to use double quotes instead of single.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Download best mp4 format available or any other best if no mp4 available
$ youtube-dl -f 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best'

# Download best format available but no better than 480p
$ youtube-dl -f 'bestvideo[height&amp;lt;=480]+bestaudio/best[height&amp;lt;=480]'

# Download best video only format but no bigger than 50 MB
$ youtube-dl -f 'best[filesize&amp;lt;50M]'

# Download best format available via direct link over HTTP/HTTPS protocol
$ youtube-dl -f '(bestvideo+bestaudio/best)[protocol^=http]'

# Download the best video format and the best audio format without merging them
$ youtube-dl -f 'bestvideo,bestaudio' -o '%(title)s.f%(format_id)s.%(ext)s'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that in the last example, an output template is recommended as bestvideo and bestaudio may have the same file name.&lt;/p&gt; 
&lt;h1&gt;VIDEO SELECTION&lt;/h1&gt; 
&lt;p&gt;Videos can be filtered by their upload date using the options &lt;code&gt;--date&lt;/code&gt;, &lt;code&gt;--datebefore&lt;/code&gt; or &lt;code&gt;--dateafter&lt;/code&gt;. They accept dates in two formats:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Absolute dates: Dates in the format &lt;code&gt;YYYYMMDD&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Relative dates: Dates in the format &lt;code&gt;(now|today)[+-][0-9](day|week|month|year)(s)?&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Examples:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Download only the videos uploaded in the last 6 months
$ youtube-dl --dateafter now-6months

# Download only the videos uploaded on January 1, 1970
$ youtube-dl --date 19700101

$ # Download only the videos uploaded in the 200x decade
$ youtube-dl --dateafter 20000101 --datebefore 20091231
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;FAQ&lt;/h1&gt; 
&lt;h3&gt;How do I update youtube-dl?&lt;/h3&gt; 
&lt;p&gt;If you've followed &lt;a href="https://ytdl-org.github.io/youtube-dl/download.html"&gt;our manual installation instructions&lt;/a&gt;, you can simply run &lt;code&gt;youtube-dl -U&lt;/code&gt; (or, on Linux, &lt;code&gt;sudo youtube-dl -U&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;If you have used pip, a simple &lt;code&gt;sudo pip install -U youtube-dl&lt;/code&gt; is sufficient to update.&lt;/p&gt; 
&lt;p&gt;If you have installed youtube-dl using a package manager like &lt;em&gt;apt-get&lt;/em&gt; or &lt;em&gt;yum&lt;/em&gt;, use the standard system update mechanism to update. Note that distribution packages are often outdated. As a rule of thumb, youtube-dl releases at least once a month, and often weekly or even daily. Simply go to &lt;a href="https://yt-dl.org"&gt;https://yt-dl.org&lt;/a&gt; to find out the current version. Unfortunately, there is nothing we youtube-dl developers can do if your distribution serves a really outdated version. You can (and should) complain to your distribution in their bugtracker or support forum.&lt;/p&gt; 
&lt;p&gt;As a last resort, you can also uninstall the version installed by your package manager and follow our manual installation instructions. For that, remove the distribution's package, with a line like&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo apt-get remove -y youtube-dl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Afterwards, simply follow &lt;a href="https://ytdl-org.github.io/youtube-dl/download.html"&gt;our manual installation instructions&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo wget https://yt-dl.org/downloads/latest/youtube-dl -O /usr/local/bin/youtube-dl
sudo chmod a+rx /usr/local/bin/youtube-dl
hash -r
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Again, from then on you'll be able to update with &lt;code&gt;sudo youtube-dl -U&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;youtube-dl is extremely slow to start on Windows&lt;/h3&gt; 
&lt;p&gt;Add a file exclusion for &lt;code&gt;youtube-dl.exe&lt;/code&gt; in Windows Defender settings.&lt;/p&gt; 
&lt;h3&gt;I'm getting an error &lt;code&gt;Unable to extract OpenGraph title&lt;/code&gt; on YouTube playlists&lt;/h3&gt; 
&lt;p&gt;YouTube changed their playlist format in March 2014 and later on, so you'll need at least youtube-dl 2014.07.25 to download all YouTube videos.&lt;/p&gt; 
&lt;p&gt;If you have installed youtube-dl with a package manager, pip, setup.py or a tarball, please use that to update. Note that Ubuntu packages do not seem to get updated anymore. Since we are not affiliated with Ubuntu, there is little we can do. Feel free to &lt;a href="https://bugs.launchpad.net/ubuntu/+source/youtube-dl/+filebug"&gt;report bugs&lt;/a&gt; to the &lt;a href="mailto:ubuntu-motu@lists.ubuntu.com?subject=outdated%20version%20of%20youtube-dl"&gt;Ubuntu packaging people&lt;/a&gt; - all they have to do is update the package to a somewhat recent version. See above for a way to update.&lt;/p&gt; 
&lt;h3&gt;I'm getting an error when trying to use output template: &lt;code&gt;error: using output template conflicts with using title, video ID or auto number&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;Make sure you are not using &lt;code&gt;-o&lt;/code&gt; with any of these options &lt;code&gt;-t&lt;/code&gt;, &lt;code&gt;--title&lt;/code&gt;, &lt;code&gt;--id&lt;/code&gt;, &lt;code&gt;-A&lt;/code&gt; or &lt;code&gt;--auto-number&lt;/code&gt; set in command line or in a configuration file. Remove the latter if any.&lt;/p&gt; 
&lt;h3&gt;Do I always have to pass &lt;code&gt;-citw&lt;/code&gt;?&lt;/h3&gt; 
&lt;p&gt;By default, youtube-dl intends to have the best options (incidentally, if you have a convincing case that these should be different, &lt;a href="https://yt-dl.org/bug"&gt;please file an issue where you explain that&lt;/a&gt;). Therefore, it is unnecessary and sometimes harmful to copy long option strings from webpages. In particular, the only option out of &lt;code&gt;-citw&lt;/code&gt; that is regularly useful is &lt;code&gt;-i&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Can you please put the &lt;code&gt;-b&lt;/code&gt; option back?&lt;/h3&gt; 
&lt;p&gt;Most people asking this question are not aware that youtube-dl now defaults to downloading the highest available quality as reported by YouTube, which will be 1080p or 720p in some cases, so you no longer need the &lt;code&gt;-b&lt;/code&gt; option. For some specific videos, maybe YouTube does not report them to be available in a specific high quality format you're interested in. In that case, simply request it with the &lt;code&gt;-f&lt;/code&gt; option and youtube-dl will try to download it.&lt;/p&gt; 
&lt;h3&gt;I get HTTP error 402 when trying to download a video. What's this?&lt;/h3&gt; 
&lt;p&gt;Apparently YouTube requires you to pass a CAPTCHA test if you download too much. We're &lt;a href="https://github.com/ytdl-org/youtube-dl/issues/154"&gt;considering to provide a way to let you solve the CAPTCHA&lt;/a&gt;, but at the moment, your best course of action is pointing a web browser to the youtube URL, solving the CAPTCHA, and restart youtube-dl.&lt;/p&gt; 
&lt;h3&gt;Do I need any other programs?&lt;/h3&gt; 
&lt;p&gt;youtube-dl works fine on its own on most sites. However, if you want to convert video/audio, you'll need &lt;a href="https://libav.org/"&gt;avconv&lt;/a&gt; or &lt;a href="https://www.ffmpeg.org/"&gt;ffmpeg&lt;/a&gt;. On some sites - most notably YouTube - videos can be retrieved in a higher quality format without sound. youtube-dl will detect whether avconv/ffmpeg is present and automatically pick the best option.&lt;/p&gt; 
&lt;p&gt;Videos or video formats streamed via RTMP protocol can only be downloaded when &lt;a href="https://rtmpdump.mplayerhq.hu/"&gt;rtmpdump&lt;/a&gt; is installed. Downloading MMS and RTSP videos requires either &lt;a href="https://mplayerhq.hu/"&gt;mplayer&lt;/a&gt; or &lt;a href="https://mpv.io/"&gt;mpv&lt;/a&gt; to be installed.&lt;/p&gt; 
&lt;h3&gt;I have downloaded a video but how can I play it?&lt;/h3&gt; 
&lt;p&gt;Once the video is fully downloaded, use any video player, such as &lt;a href="https://mpv.io/"&gt;mpv&lt;/a&gt;, &lt;a href="https://www.videolan.org/"&gt;vlc&lt;/a&gt; or &lt;a href="https://www.mplayerhq.hu/"&gt;mplayer&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;I extracted a video URL with &lt;code&gt;-g&lt;/code&gt;, but it does not play on another machine / in my web browser.&lt;/h3&gt; 
&lt;p&gt;It depends a lot on the service. In many cases, requests for the video (to download/play it) must come from the same IP address and with the same cookies and/or HTTP headers. Use the &lt;code&gt;--cookies&lt;/code&gt; option to write the required cookies into a file, and advise your downloader to read cookies from that file. Some sites also require a common user agent to be used, use &lt;code&gt;--dump-user-agent&lt;/code&gt; to see the one in use by youtube-dl. You can also get necessary cookies and HTTP headers from JSON output obtained with &lt;code&gt;--dump-json&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;It may be beneficial to use IPv6; in some cases, the restrictions are only applied to IPv4. Some services (sometimes only for a subset of videos) do not restrict the video URL by IP address, cookie, or user-agent, but these are the exception rather than the rule.&lt;/p&gt; 
&lt;p&gt;Please bear in mind that some URL protocols are &lt;strong&gt;not&lt;/strong&gt; supported by browsers out of the box, including RTMP. If you are using &lt;code&gt;-g&lt;/code&gt;, your own downloader must support these as well.&lt;/p&gt; 
&lt;p&gt;If you want to play the video on a machine that is not running youtube-dl, you can relay the video content from the machine that runs youtube-dl. You can use &lt;code&gt;-o -&lt;/code&gt; to let youtube-dl stream a video to stdout, or simply allow the player to download the files written by youtube-dl in turn.&lt;/p&gt; 
&lt;h3&gt;ERROR: no fmt_url_map or conn information found in video info&lt;/h3&gt; 
&lt;p&gt;YouTube has switched to a new video info format in July 2011 which is not supported by old versions of youtube-dl. See &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#how-do-i-update-youtube-dl"&gt;above&lt;/a&gt; for how to update youtube-dl.&lt;/p&gt; 
&lt;h3&gt;ERROR: unable to download video&lt;/h3&gt; 
&lt;p&gt;YouTube requires an additional signature since September 2012 which is not supported by old versions of youtube-dl. See &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#how-do-i-update-youtube-dl"&gt;above&lt;/a&gt; for how to update youtube-dl.&lt;/p&gt; 
&lt;h3&gt;Video URL contains an ampersand and I'm getting some strange output &lt;code&gt;[1] 2839&lt;/code&gt; or &lt;code&gt;'v' is not recognized as an internal or external command&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;That's actually the output from your shell. Since ampersand is one of the special shell characters it's interpreted by the shell preventing you from passing the whole URL to youtube-dl. To disable your shell from interpreting the ampersands (or any other special characters) you have to either put the whole URL in quotes or escape them with a backslash (which approach will work depends on your shell).&lt;/p&gt; 
&lt;p&gt;For example if your URL is &lt;a href="https://www.youtube.com/watch?t=4&amp;amp;v=BaW_jenozKc"&gt;https://www.youtube.com/watch?t=4&amp;amp;v=BaW_jenozKc&lt;/a&gt; you should end up with following command:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;youtube-dl 'https://www.youtube.com/watch?t=4&amp;amp;v=BaW_jenozKc'&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;or&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;youtube-dl https://www.youtube.com/watch?t=4\&amp;amp;v=BaW_jenozKc&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;For Windows you have to use the double quotes:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;youtube-dl "https://www.youtube.com/watch?t=4&amp;amp;v=BaW_jenozKc"&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;ExtractorError: Could not find JS function u'OF'&lt;/h3&gt; 
&lt;p&gt;In February 2015, the new YouTube player contained a character sequence in a string that was misinterpreted by old versions of youtube-dl. See &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#how-do-i-update-youtube-dl"&gt;above&lt;/a&gt; for how to update youtube-dl.&lt;/p&gt; 
&lt;h3&gt;HTTP Error 429: Too Many Requests or 402: Payment Required&lt;/h3&gt; 
&lt;p&gt;These two error codes indicate that the service is blocking your IP address because of overuse. Usually this is a soft block meaning that you can gain access again after solving CAPTCHA. Just open a browser and solve a CAPTCHA the service suggests you and after that &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#how-do-i-pass-cookies-to-youtube-dl"&gt;pass cookies&lt;/a&gt; to youtube-dl. Note that if your machine has multiple external IPs then you should also pass exactly the same IP you've used for solving CAPTCHA with &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#network-options"&gt;&lt;code&gt;--source-address&lt;/code&gt;&lt;/a&gt;. Also you may need to pass a &lt;code&gt;User-Agent&lt;/code&gt; HTTP header of your browser with &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#workarounds"&gt;&lt;code&gt;--user-agent&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If this is not the case (no CAPTCHA suggested to solve by the service) then you can contact the service and ask them to unblock your IP address, or - if you have acquired a whitelisted IP address already - use the &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#network-options"&gt;&lt;code&gt;--proxy&lt;/code&gt; or &lt;code&gt;--source-address&lt;/code&gt; options&lt;/a&gt; to select another IP address.&lt;/p&gt; 
&lt;h3&gt;SyntaxError: Non-ASCII character&lt;/h3&gt; 
&lt;p&gt;The error&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;File "youtube-dl", line 2
SyntaxError: Non-ASCII character '\x93' ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;means you're using an outdated version of Python. Please update to Python 2.6 or 2.7.&lt;/p&gt; 
&lt;h3&gt;What is this binary file? Where has the code gone?&lt;/h3&gt; 
&lt;p&gt;Since June 2012 (&lt;a href="https://github.com/ytdl-org/youtube-dl/issues/342"&gt;#342&lt;/a&gt;) youtube-dl is packed as an executable zipfile, simply unzip it (might need renaming to &lt;code&gt;youtube-dl.zip&lt;/code&gt; first on some systems) or clone the git repository, as laid out above. If you modify the code, you can run it by executing the &lt;code&gt;__main__.py&lt;/code&gt; file. To recompile the executable, run &lt;code&gt;make youtube-dl&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;The exe throws an error due to missing &lt;code&gt;MSVCR100.dll&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;To run the exe you need to install first the &lt;a href="https://download.microsoft.com/download/1/6/5/165255E7-1014-4D0A-B094-B6A430A6BFFC/vcredist_x86.exe"&gt;Microsoft Visual C++ 2010 Service Pack 1 Redistributable Package (x86)&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;On Windows, how should I set up ffmpeg and youtube-dl? Where should I put the exe files?&lt;/h3&gt; 
&lt;p&gt;If you put youtube-dl and ffmpeg in the same directory that you're running the command from, it will work, but that's rather cumbersome.&lt;/p&gt; 
&lt;p&gt;To make a different directory work - either for ffmpeg, or for youtube-dl, or for both - simply create the directory (say, &lt;code&gt;C:\bin&lt;/code&gt;, or &lt;code&gt;C:\Users\&amp;lt;User name&amp;gt;\bin&lt;/code&gt;), put all the executables directly in there, and then &lt;a href="https://www.java.com/en/download/help/path.xml"&gt;set your PATH environment variable&lt;/a&gt; to include that directory.&lt;/p&gt; 
&lt;p&gt;From then on, after restarting your shell, you will be able to access both youtube-dl and ffmpeg (and youtube-dl will be able to find ffmpeg) by simply typing &lt;code&gt;youtube-dl&lt;/code&gt; or &lt;code&gt;ffmpeg&lt;/code&gt;, no matter what directory you're in.&lt;/p&gt; 
&lt;h3&gt;How do I put downloads into a specific folder?&lt;/h3&gt; 
&lt;p&gt;Use the &lt;code&gt;-o&lt;/code&gt; to specify an &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#output-template"&gt;output template&lt;/a&gt;, for example &lt;code&gt;-o "/home/user/videos/%(title)s-%(id)s.%(ext)s"&lt;/code&gt;. If you want this for all of your downloads, put the option into your &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#configuration"&gt;configuration file&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;How do I download a video starting with a &lt;code&gt;-&lt;/code&gt;?&lt;/h3&gt; 
&lt;p&gt;Either prepend &lt;code&gt;https://www.youtube.com/watch?v=&lt;/code&gt; or separate the ID from the options with &lt;code&gt;--&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;youtube-dl -- -wNyEUrxzFU
youtube-dl "https://www.youtube.com/watch?v=-wNyEUrxzFU"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;How do I pass cookies to youtube-dl?&lt;/h3&gt; 
&lt;p&gt;Use the &lt;code&gt;--cookies&lt;/code&gt; option, for example &lt;code&gt;--cookies /path/to/cookies/file.txt&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;In order to extract cookies from browser use any conforming browser extension for exporting cookies. For example, &lt;a href="https://chrome.google.com/webstore/detail/get-cookiestxt-locally/cclelndahbckbenkjhflpdbgdldlbecc"&gt;Get cookies.txt LOCALLY&lt;/a&gt; (for Chrome) or &lt;a href="https://addons.mozilla.org/en-US/firefox/addon/cookies-txt/"&gt;cookies.txt&lt;/a&gt; (for Firefox).&lt;/p&gt; 
&lt;p&gt;Note that the cookies file must be in Mozilla/Netscape format and the first line of the cookies file must be either &lt;code&gt;# HTTP Cookie File&lt;/code&gt; or &lt;code&gt;# Netscape HTTP Cookie File&lt;/code&gt;. Make sure you have correct &lt;a href="https://en.wikipedia.org/wiki/Newline"&gt;newline format&lt;/a&gt; in the cookies file and convert newlines if necessary to correspond with your OS, namely &lt;code&gt;CRLF&lt;/code&gt; (&lt;code&gt;\r\n&lt;/code&gt;) for Windows and &lt;code&gt;LF&lt;/code&gt; (&lt;code&gt;\n&lt;/code&gt;) for Unix and Unix-like systems (Linux, macOS, etc.). &lt;code&gt;HTTP Error 400: Bad Request&lt;/code&gt; when using &lt;code&gt;--cookies&lt;/code&gt; is a good sign of invalid newline format.&lt;/p&gt; 
&lt;p&gt;Passing cookies to youtube-dl is a good way to workaround login when a particular extractor does not implement it explicitly. Another use case is working around &lt;a href="https://en.wikipedia.org/wiki/CAPTCHA"&gt;CAPTCHA&lt;/a&gt; some websites require you to solve in particular cases in order to get access (e.g. YouTube, CloudFlare).&lt;/p&gt; 
&lt;h3&gt;How do I stream directly to media player?&lt;/h3&gt; 
&lt;p&gt;You will first need to tell youtube-dl to stream media to stdout with &lt;code&gt;-o -&lt;/code&gt;, and also tell your media player to read from stdin (it must be capable of this for streaming) and then pipe former to latter. For example, streaming to &lt;a href="https://www.videolan.org/"&gt;vlc&lt;/a&gt; can be achieved with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;youtube-dl -o - "https://www.youtube.com/watch?v=BaW_jenozKcj" | vlc -
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;How do I download only new videos from a playlist?&lt;/h3&gt; 
&lt;p&gt;Use download-archive feature. With this feature you should initially download the complete playlist with &lt;code&gt;--download-archive /path/to/download/archive/file.txt&lt;/code&gt; that will record identifiers of all the videos in a special file. Each subsequent run with the same &lt;code&gt;--download-archive&lt;/code&gt; will download only new videos and skip all videos that have been downloaded before. Note that only successful downloads are recorded in the file.&lt;/p&gt; 
&lt;p&gt;For example, at first,&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;youtube-dl --download-archive archive.txt "https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;will download the complete &lt;code&gt;PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re&lt;/code&gt; playlist and create a file &lt;code&gt;archive.txt&lt;/code&gt;. Each subsequent run will only download new videos if any:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;youtube-dl --download-archive archive.txt "https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Should I add &lt;code&gt;--hls-prefer-native&lt;/code&gt; into my config?&lt;/h3&gt; 
&lt;p&gt;When youtube-dl detects an HLS video, it can download it either with the built-in downloader or ffmpeg. Since many HLS streams are slightly invalid and ffmpeg/youtube-dl each handle some invalid cases better than the other, there is an option to switch the downloader if needed.&lt;/p&gt; 
&lt;p&gt;When youtube-dl knows that one particular downloader works better for a given website, that downloader will be picked. Otherwise, youtube-dl will pick the best downloader for general compatibility, which at the moment happens to be ffmpeg. This choice may change in future versions of youtube-dl, with improvements of the built-in downloader and/or ffmpeg.&lt;/p&gt; 
&lt;p&gt;In particular, the generic extractor (used when your website is not in the &lt;a href="https://ytdl-org.github.io/youtube-dl/supportedsites.html"&gt;list of supported sites by youtube-dl&lt;/a&gt; cannot mandate one specific downloader.&lt;/p&gt; 
&lt;p&gt;If you put either &lt;code&gt;--hls-prefer-native&lt;/code&gt; or &lt;code&gt;--hls-prefer-ffmpeg&lt;/code&gt; into your configuration, a different subset of videos will fail to download correctly. Instead, it is much better to &lt;a href="https://yt-dl.org/bug"&gt;file an issue&lt;/a&gt; or a pull request which details why the native or the ffmpeg HLS downloader is a better choice for your use case.&lt;/p&gt; 
&lt;h3&gt;Can you add support for this anime video site, or site which shows current movies for free?&lt;/h3&gt; 
&lt;p&gt;As a matter of policy (as well as legality), youtube-dl does not include support for services that specialize in infringing copyright. As a rule of thumb, if you cannot easily find a video that the service is quite obviously allowed to distribute (i.e. that has been uploaded by the creator, the creator's distributor, or is published under a free license), the service is probably unfit for inclusion to youtube-dl.&lt;/p&gt; 
&lt;p&gt;A note on the service that they don't host the infringing content, but just link to those who do, is evidence that the service should &lt;strong&gt;not&lt;/strong&gt; be included into youtube-dl. The same goes for any DMCA note when the whole front page of the service is filled with videos they are not allowed to distribute. A "fair use" note is equally unconvincing if the service shows copyright-protected videos in full without authorization.&lt;/p&gt; 
&lt;p&gt;Support requests for services that &lt;strong&gt;do&lt;/strong&gt; purchase the rights to distribute their content are perfectly fine though. If in doubt, you can simply include a source that mentions the legitimate purchase of content.&lt;/p&gt; 
&lt;h3&gt;How can I speed up work on my issue?&lt;/h3&gt; 
&lt;p&gt;(Also known as: Help, my important issue not being solved!) The youtube-dl core developer team is quite small. While we do our best to solve as many issues as possible, sometimes that can take quite a while. To speed up your issue, here's what you can do:&lt;/p&gt; 
&lt;p&gt;First of all, please do report the issue &lt;a href="https://yt-dl.org/bugs"&gt;at our issue tracker&lt;/a&gt;. That allows us to coordinate all efforts by users and developers, and serves as a unified point. Unfortunately, the youtube-dl project has grown too large to use personal email as an effective communication channel.&lt;/p&gt; 
&lt;p&gt;Please read the &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#bugs"&gt;bug reporting instructions&lt;/a&gt; below. A lot of bugs lack all the necessary information. If you can, offer proxy, VPN, or shell access to the youtube-dl developers. If you are able to, test the issue from multiple computers in multiple countries to exclude local censorship or misconfiguration issues.&lt;/p&gt; 
&lt;p&gt;If nobody is interested in solving your issue, you are welcome to take matters into your own hands and submit a pull request (or coerce/pay somebody else to do so).&lt;/p&gt; 
&lt;p&gt;Feel free to bump the issue from time to time by writing a small comment ("Issue is still present in youtube-dl version ...from France, but fixed from Belgium"), but please not more than once a month. Please do not declare your issue as &lt;code&gt;important&lt;/code&gt; or &lt;code&gt;urgent&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;How can I detect whether a given URL is supported by youtube-dl?&lt;/h3&gt; 
&lt;p&gt;For one, have a look at the &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/docs/supportedsites.md"&gt;list of supported sites&lt;/a&gt;. Note that it can sometimes happen that the site changes its URL scheme (say, from &lt;a href="https://example.com/video/1234567"&gt;https://example.com/video/1234567&lt;/a&gt; to &lt;a href="https://example.com/v/1234567"&gt;https://example.com/v/1234567&lt;/a&gt; ) and youtube-dl reports an URL of a service in that list as unsupported. In that case, simply report a bug.&lt;/p&gt; 
&lt;p&gt;It is &lt;em&gt;not&lt;/em&gt; possible to detect whether a URL is supported or not. That's because youtube-dl contains a generic extractor which matches &lt;strong&gt;all&lt;/strong&gt; URLs. You may be tempted to disable, exclude, or remove the generic extractor, but the generic extractor not only allows users to extract videos from lots of websites that embed a video from another service, but may also be used to extract video from a service that it's hosting itself. Therefore, we neither recommend nor support disabling, excluding, or removing the generic extractor.&lt;/p&gt; 
&lt;p&gt;If you want to find out whether a given URL is supported, simply call youtube-dl with it. If you get no videos back, chances are the URL is either not referring to a video or unsupported. You can find out which by examining the output (if you run youtube-dl on the console) or catching an &lt;code&gt;UnsupportedError&lt;/code&gt; exception if you run it from a Python program.&lt;/p&gt; 
&lt;h1&gt;Why do I need to go through that much red tape when filing bugs?&lt;/h1&gt; 
&lt;p&gt;Before we had the issue template, despite our extensive &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#bugs"&gt;bug reporting instructions&lt;/a&gt;, about 80% of the issue reports we got were useless, for instance because people used ancient versions hundreds of releases old, because of simple syntactic errors (not in youtube-dl but in general shell usage), because the problem was already reported multiple times before, because people did not actually read an error message, even if it said "please install ffmpeg", because people did not mention the URL they were trying to download and many more simple, easy-to-avoid problems, many of whom were totally unrelated to youtube-dl.&lt;/p&gt; 
&lt;p&gt;youtube-dl is an open-source project manned by too few volunteers, so we'd rather spend time fixing bugs where we are certain none of those simple problems apply, and where we can be reasonably confident to be able to reproduce the issue without asking the reporter repeatedly. As such, the output of &lt;code&gt;youtube-dl -v YOUR_URL_HERE&lt;/code&gt; is really all that's required to file an issue. The issue template also guides you through some basic steps you can do, such as checking that your version of youtube-dl is current.&lt;/p&gt; 
&lt;h1&gt;DEVELOPER INSTRUCTIONS&lt;/h1&gt; 
&lt;p&gt;Most users do not need to build youtube-dl and can &lt;a href="https://ytdl-org.github.io/youtube-dl/download.html"&gt;download the builds&lt;/a&gt; or get them from their distribution.&lt;/p&gt; 
&lt;p&gt;To run youtube-dl as a developer, you don't need to build anything either. Simply execute&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python -m youtube_dl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run the test, simply invoke your favorite test runner, or execute a test file directly; any of the following work:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python -m unittest discover
python test/test_download.py
nosetests
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Python versions 3.6 and later, you can use &lt;a href="https://pypi.org/project/pynose/"&gt;pynose&lt;/a&gt; to implement &lt;code&gt;nosetests&lt;/code&gt;. The original &lt;a href="https://pypi.org/project/nose/"&gt;nose&lt;/a&gt; has not been upgraded for 3.10 and later.&lt;/p&gt; 
&lt;p&gt;See item 6 of &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#adding-support-for-a-new-site"&gt;new extractor tutorial&lt;/a&gt; for how to run extractor specific test cases.&lt;/p&gt; 
&lt;p&gt;If you want to create a build of youtube-dl yourself, you'll need&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;python&lt;/li&gt; 
 &lt;li&gt;make (only GNU make is supported)&lt;/li&gt; 
 &lt;li&gt;pandoc&lt;/li&gt; 
 &lt;li&gt;zip&lt;/li&gt; 
 &lt;li&gt;nosetests&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Adding support for a new site&lt;/h3&gt; 
&lt;p&gt;If you want to add support for a new site, first of all &lt;strong&gt;make sure&lt;/strong&gt; this site is &lt;strong&gt;not dedicated to &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/README.md#can-you-add-support-for-this-anime-video-site-or-site-which-shows-current-movies-for-free"&gt;copyright infringement&lt;/a&gt;&lt;/strong&gt;. youtube-dl does &lt;strong&gt;not support&lt;/strong&gt; such sites thus pull requests adding support for them &lt;strong&gt;will be rejected&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;After you have ensured this site is distributing its content legally, you can follow this quick list (assuming your service is called &lt;code&gt;yourextractor&lt;/code&gt;):&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/ytdl-org/youtube-dl/fork"&gt;Fork this repository&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Check out the source code with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; git clone git@github.com:YOUR_GITHUB_USERNAME/youtube-dl.git
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Start a new git branch with&lt;/p&gt; &lt;pre&gt;&lt;code&gt; cd youtube-dl
 git checkout -b yourextractor
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Start with this simple template and save it to &lt;code&gt;youtube_dl/extractor/yourextractor.py&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;# coding: utf-8
from __future__ import unicode_literals

from .common import InfoExtractor


class YourExtractorIE(InfoExtractor):
    _VALID_URL = r'https?://(?:www\.)?yourextractor\.com/watch/(?P&amp;lt;id&amp;gt;[0-9]+)'
    _TEST = {
        'url': 'https://yourextractor.com/watch/42',
        'md5': 'TODO: md5 sum of the first 10241 bytes of the video file (use --test)',
        'info_dict': {
            'id': '42',
            'ext': 'mp4',
            'title': 'Video title goes here',
            'thumbnail': r're:^https?://.*\.jpg$',
            # TODO more properties, either as:
            # * A value
            # * MD5 checksum; start the string with md5:
            # * A regular expression; start the string with re:
            # * Any Python type (for example int or float)
        }
    }

    def _real_extract(self, url):
        video_id = self._match_id(url)
        webpage = self._download_webpage(url, video_id)

        # TODO more code goes here, for example ...
        title = self._html_search_regex(r'&amp;lt;h1&amp;gt;(.+?)&amp;lt;/h1&amp;gt;', webpage, 'title')

        return {
            'id': video_id,
            'title': title,
            'description': self._og_search_description(webpage),
            'uploader': self._search_regex(r'&amp;lt;div[^&amp;gt;]+id="uploader"[^&amp;gt;]*&amp;gt;([^&amp;lt;]+)&amp;lt;', webpage, 'uploader', fatal=False),
            # TODO more properties (see youtube_dl/extractor/common.py)
        }
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Add an import in &lt;a href="https://github.com/ytdl-org/youtube-dl/raw/master/youtube_dl/extractor/extractors.py"&gt;&lt;code&gt;youtube_dl/extractor/extractors.py&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run &lt;code&gt;python test/test_download.py TestDownload.test_YourExtractor&lt;/code&gt;. This &lt;em&gt;should fail&lt;/em&gt; at first, but you can continually re-run it until you're done. If you decide to add more than one test (actually, test case) then rename &lt;code&gt;_TEST&lt;/code&gt; to &lt;code&gt;_TESTS&lt;/code&gt; and make it into a list of dictionaries. The tests will then be named &lt;code&gt;TestDownload.test_YourExtractor&lt;/code&gt;, &lt;code&gt;TestDownload.test_YourExtractor_1&lt;/code&gt;, &lt;code&gt;TestDownload.test_YourExtractor_2&lt;/code&gt;, etc. Note:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;the test names use the extractor class name &lt;strong&gt;without the trailing &lt;code&gt;IE&lt;/code&gt;&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;tests with &lt;code&gt;only_matching&lt;/code&gt; key in test's dict are not counted.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Have a look at &lt;a href="https://github.com/ytdl-org/youtube-dl/raw/master/youtube_dl/extractor/common.py"&gt;&lt;code&gt;youtube_dl/extractor/common.py&lt;/code&gt;&lt;/a&gt; for possible helper methods and a &lt;a href="https://github.com/ytdl-org/youtube-dl/raw/7f41a598b3fba1bcab2817de64a08941200aa3c8/youtube_dl/extractor/common.py#L94-L303"&gt;detailed description of what your extractor should and may return&lt;/a&gt;. Add tests and code for as many as you want.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Make sure your code follows &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#youtube-dl-coding-conventions"&gt;youtube-dl coding conventions&lt;/a&gt; and check the code with &lt;a href="https://flake8.pycqa.org/en/latest/index.html#quickstart"&gt;flake8&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; $ flake8 youtube_dl/extractor/yourextractor.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Make sure your code works under all &lt;a href="https://www.python.org/"&gt;Python&lt;/a&gt; versions claimed supported by youtube-dl, namely 2.6, 2.7, and 3.2+.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;When the tests pass, &lt;a href="https://git-scm.com/docs/git-add"&gt;add&lt;/a&gt; the new files and &lt;a href="https://git-scm.com/docs/git-commit"&gt;commit&lt;/a&gt; them and &lt;a href="https://git-scm.com/docs/git-push"&gt;push&lt;/a&gt; the result, like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ git add youtube_dl/extractor/extractors.py
$ git add youtube_dl/extractor/yourextractor.py
$ git commit -m '[yourextractor] Add new extractor'
$ git push origin yourextractor
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Finally, &lt;a href="https://help.github.com/articles/creating-a-pull-request"&gt;create a pull request&lt;/a&gt;. We'll then review and merge it.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;In any case, thank you very much for your contributions!&lt;/p&gt; 
&lt;h2&gt;youtube-dl coding conventions&lt;/h2&gt; 
&lt;p&gt;This section introduces guidelines for writing idiomatic, robust and future-proof extractor code.&lt;/p&gt; 
&lt;p&gt;Extractors are very fragile by nature since they depend on the layout of the source data provided by 3rd party media hosters out of your control and this layout tends to change. As an extractor implementer your task is not only to write code that will extract media links and metadata correctly but also to minimize dependency on the source's layout and even to make the code foresee potential future changes and be ready for that. This is important because it will allow the extractor not to break on minor layout changes thus keeping old youtube-dl versions working. Even though this breakage issue is easily fixed by emitting a new version of youtube-dl with a fix incorporated, all the previous versions become broken in all repositories and distros' packages that may not be so prompt in fetching the update from us. Needless to say, some non rolling release distros may never receive an update at all.&lt;/p&gt; 
&lt;h3&gt;Mandatory and optional metafields&lt;/h3&gt; 
&lt;p&gt;For extraction to work youtube-dl relies on metadata your extractor extracts and provides to youtube-dl expressed by an &lt;a href="https://github.com/ytdl-org/youtube-dl/raw/7f41a598b3fba1bcab2817de64a08941200aa3c8/youtube_dl/extractor/common.py#L94-L303"&gt;information dictionary&lt;/a&gt; or simply &lt;em&gt;info dict&lt;/em&gt;. Only the following meta fields in the &lt;em&gt;info dict&lt;/em&gt; are considered mandatory for a successful extraction process by youtube-dl:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;id&lt;/code&gt; (media identifier)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;title&lt;/code&gt; (media title)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;url&lt;/code&gt; (media download URL) or &lt;code&gt;formats&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In fact only the last option is technically mandatory (i.e. if you can't figure out the download location of the media the extraction does not make any sense). But by convention youtube-dl also treats &lt;code&gt;id&lt;/code&gt; and &lt;code&gt;title&lt;/code&gt; as mandatory. Thus the aforementioned metafields are the critical data that the extraction does not make any sense without and if any of them fail to be extracted then the extractor is considered completely broken.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/ytdl-org/youtube-dl/raw/7f41a598b3fba1bcab2817de64a08941200aa3c8/youtube_dl/extractor/common.py#L188-L303"&gt;Any field&lt;/a&gt; apart from the aforementioned ones are considered &lt;strong&gt;optional&lt;/strong&gt;. That means that extraction should be &lt;strong&gt;tolerant&lt;/strong&gt; to situations when sources for these fields can potentially be unavailable (even if they are always available at the moment) and &lt;strong&gt;future-proof&lt;/strong&gt; in order not to break the extraction of general purpose mandatory fields.&lt;/p&gt; 
&lt;h4&gt;Example&lt;/h4&gt; 
&lt;p&gt;Say you have some source dictionary &lt;code&gt;meta&lt;/code&gt; that you've fetched as JSON with HTTP request and it has a key &lt;code&gt;summary&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;meta = self._download_json(url, video_id)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Assume at this point &lt;code&gt;meta&lt;/code&gt;'s layout is:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;{
    ...
    "summary": "some fancy summary text",
    ...
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Assume you want to extract &lt;code&gt;summary&lt;/code&gt; and put it into the resulting info dict as &lt;code&gt;description&lt;/code&gt;. Since &lt;code&gt;description&lt;/code&gt; is an optional meta field you should be ready that this key may be missing from the &lt;code&gt;meta&lt;/code&gt; dict, so that you should extract it like:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;description = meta.get('summary')  # correct
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and not like:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;description = meta['summary']  # incorrect
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The latter will break extraction process with &lt;code&gt;KeyError&lt;/code&gt; if &lt;code&gt;summary&lt;/code&gt; disappears from &lt;code&gt;meta&lt;/code&gt; at some later time but with the former approach extraction will just go ahead with &lt;code&gt;description&lt;/code&gt; set to &lt;code&gt;None&lt;/code&gt; which is perfectly fine (remember &lt;code&gt;None&lt;/code&gt; is equivalent to the absence of data).&lt;/p&gt; 
&lt;p&gt;Similarly, you should pass &lt;code&gt;fatal=False&lt;/code&gt; when extracting optional data from a webpage with &lt;code&gt;_search_regex&lt;/code&gt;, &lt;code&gt;_html_search_regex&lt;/code&gt; or similar methods, for instance:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;description = self._search_regex(
    r'&amp;lt;span[^&amp;gt;]+id="title"[^&amp;gt;]*&amp;gt;([^&amp;lt;]+)&amp;lt;',
    webpage, 'description', fatal=False)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With &lt;code&gt;fatal&lt;/code&gt; set to &lt;code&gt;False&lt;/code&gt; if &lt;code&gt;_search_regex&lt;/code&gt; fails to extract &lt;code&gt;description&lt;/code&gt; it will emit a warning and continue extraction.&lt;/p&gt; 
&lt;p&gt;You can also pass &lt;code&gt;default=&amp;lt;some fallback value&amp;gt;&lt;/code&gt;, for example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;description = self._search_regex(
    r'&amp;lt;span[^&amp;gt;]+id="title"[^&amp;gt;]*&amp;gt;([^&amp;lt;]+)&amp;lt;',
    webpage, 'description', default=None)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;On failure this code will silently continue the extraction with &lt;code&gt;description&lt;/code&gt; set to &lt;code&gt;None&lt;/code&gt;. That is useful for metafields that may or may not be present.&lt;/p&gt; 
&lt;h3&gt;Provide fallbacks&lt;/h3&gt; 
&lt;p&gt;When extracting metadata try to do so from multiple sources. For example if &lt;code&gt;title&lt;/code&gt; is present in several places, try extracting from at least some of them. This makes it more future-proof in case some of the sources become unavailable.&lt;/p&gt; 
&lt;h4&gt;Example&lt;/h4&gt; 
&lt;p&gt;Say &lt;code&gt;meta&lt;/code&gt; from the previous example has a &lt;code&gt;title&lt;/code&gt; and you are about to extract it. Since &lt;code&gt;title&lt;/code&gt; is a mandatory meta field you should end up with something like:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;title = meta['title']
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If &lt;code&gt;title&lt;/code&gt; disappears from &lt;code&gt;meta&lt;/code&gt; in future due to some changes on the hoster's side the extraction would fail since &lt;code&gt;title&lt;/code&gt; is mandatory. That's expected.&lt;/p&gt; 
&lt;p&gt;Assume that you have some another source you can extract &lt;code&gt;title&lt;/code&gt; from, for example &lt;code&gt;og:title&lt;/code&gt; HTML meta of a &lt;code&gt;webpage&lt;/code&gt;. In this case you can provide a fallback scenario:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;title = meta.get('title') or self._og_search_title(webpage)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This code will try to extract from &lt;code&gt;meta&lt;/code&gt; first and if it fails it will try extracting &lt;code&gt;og:title&lt;/code&gt; from a &lt;code&gt;webpage&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Regular expressions&lt;/h3&gt; 
&lt;h4&gt;Don't capture groups you don't use&lt;/h4&gt; 
&lt;p&gt;Capturing group must be an indication that it's used somewhere in the code. Any group that is not used must be non capturing.&lt;/p&gt; 
&lt;h5&gt;Example&lt;/h5&gt; 
&lt;p&gt;Don't capture id attribute name here since you can't use it for anything anyway.&lt;/p&gt; 
&lt;p&gt;Correct:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;r'(?:id|ID)=(?P&amp;lt;id&amp;gt;\d+)'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Incorrect:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;r'(id|ID)=(?P&amp;lt;id&amp;gt;\d+)'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Make regular expressions relaxed and flexible&lt;/h4&gt; 
&lt;p&gt;When using regular expressions try to write them fuzzy, relaxed and flexible, skipping insignificant parts that are more likely to change, allowing both single and double quotes for quoted values and so on.&lt;/p&gt; 
&lt;h5&gt;Example&lt;/h5&gt; 
&lt;p&gt;Say you need to extract &lt;code&gt;title&lt;/code&gt; from the following HTML code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;span style="position: absolute; left: 910px; width: 90px; float: right; z-index: 9999;" class="title"&amp;gt;some fancy title&amp;lt;/span&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The code for that task should look similar to:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;title = self._search_regex(
    r'&amp;lt;span[^&amp;gt;]+class="title"[^&amp;gt;]*&amp;gt;([^&amp;lt;]+)', webpage, 'title')
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or even better:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;title = self._search_regex(
    r'&amp;lt;span[^&amp;gt;]+class=(["\'])title\1[^&amp;gt;]*&amp;gt;(?P&amp;lt;title&amp;gt;[^&amp;lt;]+)',
    webpage, 'title', group='title')
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note how you tolerate potential changes in the &lt;code&gt;style&lt;/code&gt; attribute's value or switch from using double quotes to single for &lt;code&gt;class&lt;/code&gt; attribute:&lt;/p&gt; 
&lt;p&gt;The code definitely should not look like:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;title = self._search_regex(
    r'&amp;lt;span style="position: absolute; left: 910px; width: 90px; float: right; z-index: 9999;" class="title"&amp;gt;(.*?)&amp;lt;/span&amp;gt;',
    webpage, 'title', group='title')
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Long lines policy&lt;/h3&gt; 
&lt;p&gt;There is a soft limit to keep lines of code under 80 characters long. This means it should be respected if possible and if it does not make readability and code maintenance worse.&lt;/p&gt; 
&lt;p&gt;For example, you should &lt;strong&gt;never&lt;/strong&gt; split long string literals like URLs or some other often copied entities over multiple lines to fit this limit:&lt;/p&gt; 
&lt;p&gt;Correct:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;'https://www.youtube.com/watch?v=FqZTN594JQw&amp;amp;list=PLMYEtVRpaqY00V9W81Cwmzp6N6vZqfUKD4'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Incorrect:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;'https://www.youtube.com/watch?v=FqZTN594JQw&amp;amp;list='
'PLMYEtVRpaqY00V9W81Cwmzp6N6vZqfUKD4'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Inline values&lt;/h3&gt; 
&lt;p&gt;Extracting variables is acceptable for reducing code duplication and improving readability of complex expressions. However, you should avoid extracting variables used only once and moving them to opposite parts of the extractor file, which makes reading the linear flow difficult.&lt;/p&gt; 
&lt;h4&gt;Example&lt;/h4&gt; 
&lt;p&gt;Correct:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;title = self._html_search_regex(r'&amp;lt;title&amp;gt;([^&amp;lt;]+)&amp;lt;/title&amp;gt;', webpage, 'title')
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Incorrect:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;TITLE_RE = r'&amp;lt;title&amp;gt;([^&amp;lt;]+)&amp;lt;/title&amp;gt;'
# ...some lines of code...
title = self._html_search_regex(TITLE_RE, webpage, 'title')
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Collapse fallbacks&lt;/h3&gt; 
&lt;p&gt;Multiple fallback values can quickly become unwieldy. Collapse multiple fallback values into a single expression via a list of patterns.&lt;/p&gt; 
&lt;h4&gt;Example&lt;/h4&gt; 
&lt;p&gt;Good:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;description = self._html_search_meta(
    ['og:description', 'description', 'twitter:description'],
    webpage, 'description', default=None)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Unwieldy:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;description = (
    self._og_search_description(webpage, default=None)
    or self._html_search_meta('description', webpage, default=None)
    or self._html_search_meta('twitter:description', webpage, default=None))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Methods supporting list of patterns are: &lt;code&gt;_search_regex&lt;/code&gt;, &lt;code&gt;_html_search_regex&lt;/code&gt;, &lt;code&gt;_og_search_property&lt;/code&gt;, &lt;code&gt;_html_search_meta&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Trailing parentheses&lt;/h3&gt; 
&lt;p&gt;Always move trailing parentheses after the last argument.&lt;/p&gt; 
&lt;h4&gt;Example&lt;/h4&gt; 
&lt;p&gt;Correct:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;    lambda x: x['ResultSet']['Result'][0]['VideoUrlSet']['VideoUrl'],
    list)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Incorrect:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;    lambda x: x['ResultSet']['Result'][0]['VideoUrlSet']['VideoUrl'],
    list,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Use convenience conversion and parsing functions&lt;/h3&gt; 
&lt;p&gt;Wrap all extracted numeric data into safe functions from &lt;a href="https://github.com/ytdl-org/youtube-dl/raw/master/youtube_dl/utils.py"&gt;&lt;code&gt;youtube_dl/utils.py&lt;/code&gt;&lt;/a&gt;: &lt;code&gt;int_or_none&lt;/code&gt;, &lt;code&gt;float_or_none&lt;/code&gt;. Use them for string to number conversions as well.&lt;/p&gt; 
&lt;p&gt;Use &lt;code&gt;url_or_none&lt;/code&gt; for safe URL processing.&lt;/p&gt; 
&lt;p&gt;Use &lt;code&gt;traverse_obj&lt;/code&gt; for safe metadata extraction from parsed JSON.&lt;/p&gt; 
&lt;p&gt;Use &lt;code&gt;unified_strdate&lt;/code&gt; for uniform &lt;code&gt;upload_date&lt;/code&gt; or any &lt;code&gt;YYYYMMDD&lt;/code&gt; meta field extraction, &lt;code&gt;unified_timestamp&lt;/code&gt; for uniform &lt;code&gt;timestamp&lt;/code&gt; extraction, &lt;code&gt;parse_filesize&lt;/code&gt; for &lt;code&gt;filesize&lt;/code&gt; extraction, &lt;code&gt;parse_count&lt;/code&gt; for count meta fields extraction, &lt;code&gt;parse_resolution&lt;/code&gt;, &lt;code&gt;parse_duration&lt;/code&gt; for &lt;code&gt;duration&lt;/code&gt; extraction, &lt;code&gt;parse_age_limit&lt;/code&gt; for &lt;code&gt;age_limit&lt;/code&gt; extraction.&lt;/p&gt; 
&lt;p&gt;Explore &lt;a href="https://github.com/ytdl-org/youtube-dl/raw/master/youtube_dl/utils.py"&gt;&lt;code&gt;youtube_dl/utils.py&lt;/code&gt;&lt;/a&gt; for more useful convenience functions.&lt;/p&gt; 
&lt;h4&gt;More examples&lt;/h4&gt; 
&lt;h5&gt;Safely extract optional description from parsed JSON&lt;/h5&gt; 
&lt;p&gt;When processing complex JSON, as often returned by site API requests or stashed in web pages for "hydration", you can use the &lt;code&gt;traverse_obj()&lt;/code&gt; utility function to handle multiple fallback values and to ensure the expected type of metadata items. The function's docstring defines how the function works: also review usage in the codebase for more examples.&lt;/p&gt; 
&lt;p&gt;In this example, a text &lt;code&gt;description&lt;/code&gt;, or &lt;code&gt;None&lt;/code&gt;, is pulled from the &lt;code&gt;.result.video[0].summary&lt;/code&gt; member of the parsed JSON &lt;code&gt;response&lt;/code&gt;, if available.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;description = traverse_obj(response, ('result', 'video', 0, 'summary', T(compat_str)))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;T(...)&lt;/code&gt; is a shorthand for a set literal; if you hate people who still run Python 2.6, &lt;code&gt;T(type_or_transformation)&lt;/code&gt; could be written as a set literal &lt;code&gt;{type_or_transformation}&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Some extractors use the older and less capable &lt;code&gt;try_get()&lt;/code&gt; function in the same way.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;description = try_get(response, lambda x: x['result']['video'][0]['summary'], compat_str)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Safely extract more optional metadata&lt;/h5&gt; 
&lt;p&gt;In this example, various optional metadata values are extracted from the &lt;code&gt;.result.video[0]&lt;/code&gt; member of the parsed JSON &lt;code&gt;response&lt;/code&gt;, which is expected to be a JS object, parsed into a &lt;code&gt;dict&lt;/code&gt;, with no crash if that isn't so, or if any of the target values are missing or invalid.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;video = traverse_obj(response, ('result', 'video', 0, T(dict))) or {}
# formerly:
# video = try_get(response, lambda x: x['result']['video'][0], dict) or {}
description = video.get('summary')
duration = float_or_none(video.get('durationMs'), scale=1000)
view_count = int_or_none(video.get('views'))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Safely extract nested lists&lt;/h4&gt; 
&lt;p&gt;Suppose you've extracted JSON like this into a Python data structure named &lt;code&gt;media_json&lt;/code&gt; using, say, the &lt;code&gt;_download_json()&lt;/code&gt; or &lt;code&gt;_parse_json()&lt;/code&gt; methods of &lt;code&gt;InfoExtractor&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
    "title": "Example video",
    "comment": "try extracting this",
    "media": [{
        "type": "bad",
        "size": 320,
        "url": "https://some.cdn.site/bad.mp4"
    }, {
        "type": "streaming",
        "url": "https://some.cdn.site/hls.m3u8"
    }, {
        "type": "super",
        "size": 1280,
        "url": "https://some.cdn.site/good.webm"
    }],
    "moreStuff": "more values",
    ...
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then extractor code like this can collect the various fields of the JSON:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;...
from ..utils import (
    determine_ext,
    int_or_none,
    T,
    traverse_obj,
    txt_or_none,
    url_or_none,
)
...
        ...
        info_dict = {}
        # extract title and description if valid and not empty
        info_dict.update(traverse_obj(media_json, {
            'title': ('title', T(txt_or_none)),
            'description': ('comment', T(txt_or_none)),
        }))

        # extract any recognisable media formats
        fmts = []
        # traverse into "media" list, extract `dict`s with desired keys
        for fmt in traverse_obj(media_json, ('media', Ellipsis, {
                'format_id': ('type', T(txt_or_none)),
                'url': ('url', T(url_or_none)),
                'width': ('size', T(int_or_none)), })):
            # bad `fmt` values were `None` and removed
            if 'url' not in fmt:
                continue
            fmt_url = fmt['url']  # known to be valid URL
            ext = determine_ext(fmt_url)
            if ext == 'm3u8':
                fmts.extend(self._extract_m3u8_formats(fmt_url, video_id, 'mp4', fatal=False))
            else:
                fmt['ext'] = ext
                fmts.append(fmt)

        # sort, raise if no formats
        self._sort_formats(fmts)

        info_dict['formats'] = fmts
        ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The extractor raises an exception rather than random crashes if the JSON structure changes so that no formats are found.&lt;/p&gt; 
&lt;h1&gt;EMBEDDING YOUTUBE-DL&lt;/h1&gt; 
&lt;p&gt;youtube-dl makes the best effort to be a good command-line program, and thus should be callable from any programming language. If you encounter any problems parsing its output, feel free to &lt;a href="https://github.com/ytdl-org/youtube-dl/issues/new"&gt;create a report&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;From a Python program, you can embed youtube-dl in a more powerful fashion, like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from __future__ import unicode_literals
import youtube_dl

ydl_opts = {}
with youtube_dl.YoutubeDL(ydl_opts) as ydl:
    ydl.download(['https://www.youtube.com/watch?v=BaW_jenozKc'])
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Most likely, you'll want to use various options. For a list of options available, have a look at &lt;a href="https://github.com/ytdl-org/youtube-dl/raw/3e4cedf9e8cd3157df2457df7274d0c842421945/youtube_dl/YoutubeDL.py#L137-L312"&gt;&lt;code&gt;youtube_dl/YoutubeDL.py&lt;/code&gt;&lt;/a&gt;. For a start, if you want to intercept youtube-dl's output, set a &lt;code&gt;logger&lt;/code&gt; object.&lt;/p&gt; 
&lt;p&gt;Here's a more complete example of a program that outputs only errors (and a short message after the download is finished), and downloads/converts the video to an mp3 file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from __future__ import unicode_literals
import youtube_dl


class MyLogger(object):
    def debug(self, msg):
        pass

    def warning(self, msg):
        pass

    def error(self, msg):
        print(msg)


def my_hook(d):
    if d['status'] == 'finished':
        print('Done downloading, now converting ...')


ydl_opts = {
    'format': 'bestaudio/best',
    'postprocessors': [{
        'key': 'FFmpegExtractAudio',
        'preferredcodec': 'mp3',
        'preferredquality': '192',
    }],
    'logger': MyLogger(),
    'progress_hooks': [my_hook],
}
with youtube_dl.YoutubeDL(ydl_opts) as ydl:
    ydl.download(['https://www.youtube.com/watch?v=BaW_jenozKc'])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;BUGS&lt;/h1&gt; 
&lt;p&gt;Bugs and suggestions should be reported in the issue tracker: &lt;a href="https://github.com/ytdl-org/youtube-dl/issues"&gt;https://github.com/ytdl-org/youtube-dl/issues&lt;/a&gt; (&lt;a href="https://yt-dl.org/bug"&gt;https://yt-dl.org/bug&lt;/a&gt; is an alias for this). Unless you were prompted to or there is another pertinent reason (e.g. GitHub fails to accept the bug report), please do not send bug reports via personal email. For discussions, join us in the IRC channel &lt;a href="irc://chat.freenode.net/#youtube-dl"&gt;#youtube-dl&lt;/a&gt; on freenode (&lt;a href="https://webchat.freenode.net/?randomnick=1&amp;amp;channels=youtube-dl"&gt;webchat&lt;/a&gt;).&lt;/p&gt; 
&lt;h2&gt;Opening a bug report or suggestion&lt;/h2&gt; 
&lt;p&gt;Be sure to follow instructions provided &lt;strong&gt;below&lt;/strong&gt; and &lt;strong&gt;in the issue tracker&lt;/strong&gt;. Complete the appropriate issue template fully. Consider whether your problem is covered by an existing issue: if so, follow the discussion there. Avoid commenting on existing duplicate issues as such comments do not add to the discussion of the issue and are liable to be treated as spam.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Please include the full output of youtube-dl when run with &lt;code&gt;-v&lt;/code&gt;&lt;/strong&gt;, i.e. &lt;strong&gt;add&lt;/strong&gt; &lt;code&gt;-v&lt;/code&gt; flag to &lt;strong&gt;your command line&lt;/strong&gt;, copy the &lt;strong&gt;whole&lt;/strong&gt; output and post it in the issue body wrapped in ``` for better formatting. It should look similar to this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ youtube-dl -v &amp;lt;your command line&amp;gt;
[debug] System config: []
[debug] User config: []
[debug] Command-line args: [u'-v', u'https://www.youtube.com/watch?v=BaW_jenozKcj']
[debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251
[debug] youtube-dl version 2015.12.06
[debug] Git HEAD: 135392e
[debug] Python version 2.6.6 - Windows-2003Server-5.2.3790-SP2
[debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4
[debug] Proxy map: {}
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Do not post screenshots of verbose logs; only plain text is acceptable.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The output (including the first lines) contains important debugging information. Issues without the full output are often not reproducible and therefore do not get solved in short order, if ever.&lt;/p&gt; 
&lt;p&gt;Finally please review your issue to avoid various common mistakes (you can and should use this as a checklist) listed below.&lt;/p&gt; 
&lt;h3&gt;Is the description of the issue itself sufficient?&lt;/h3&gt; 
&lt;p&gt;We often get issue reports that are hard to understand. To avoid subsequent clarifications, and to assist participants who are not native English speakers, please elaborate on what feature you are requesting, or what bug you want to be fixed.&lt;/p&gt; 
&lt;p&gt;Make sure that it's obvious&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;What the problem is&lt;/li&gt; 
 &lt;li&gt;How it could be fixed&lt;/li&gt; 
 &lt;li&gt;How your proposed solution would look&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If your report is shorter than two lines, it is almost certainly missing some of these, which makes it hard for us to respond to it. We're often too polite to close the issue outright, but the missing info makes misinterpretation likely. As a committer myself, I often get frustrated by these issues, since the only possible way for me to move forward on them is to ask for clarification over and over.&lt;/p&gt; 
&lt;p&gt;For bug reports, this means that your report should contain the &lt;em&gt;complete&lt;/em&gt; output of youtube-dl when called with the &lt;code&gt;-v&lt;/code&gt; flag. The error message you get for (most) bugs even says so, but you would not believe how many of our bug reports do not contain this information.&lt;/p&gt; 
&lt;p&gt;If your server has multiple IPs or you suspect censorship, adding &lt;code&gt;--call-home&lt;/code&gt; may be a good idea to get more diagnostics. If the error is &lt;code&gt;ERROR: Unable to extract ...&lt;/code&gt; and you cannot reproduce it from multiple countries, add &lt;code&gt;--dump-pages&lt;/code&gt; (warning: this will yield a rather large output, redirect it to the file &lt;code&gt;log.txt&lt;/code&gt; by adding &lt;code&gt;&amp;gt;log.txt 2&amp;gt;&amp;amp;1&lt;/code&gt; to your command-line) or upload the &lt;code&gt;.dump&lt;/code&gt; files you get when you add &lt;code&gt;--write-pages&lt;/code&gt; &lt;a href="https://gist.github.com/"&gt;somewhere&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Site support requests must contain an example URL&lt;/strong&gt;. An example URL is a URL you might want to download, like &lt;code&gt;https://www.youtube.com/watch?v=BaW_jenozKc&lt;/code&gt;. There should be an obvious video present. Except under very special circumstances, the main page of a video service (e.g. &lt;code&gt;https://www.youtube.com/&lt;/code&gt;) is &lt;em&gt;not&lt;/em&gt; an example URL.&lt;/p&gt; 
&lt;h3&gt;Is the issue already documented?&lt;/h3&gt; 
&lt;p&gt;Make sure that someone has not already opened the issue you're trying to open. Search at the top of the window or browse the &lt;a href="https://github.com/ytdl-org/youtube-dl/search?type=Issues"&gt;GitHub Issues&lt;/a&gt; of this repository. Initially, at least, use the search term &lt;code&gt;-label:duplicate&lt;/code&gt; to focus on active issues. If there is an issue, feel free to write something along the lines of "This affects me as well, with version 2015.01.01. Here is some more information on the issue: ...". While some issues may be old, a new post into them often spurs rapid activity.&lt;/p&gt; 
&lt;h3&gt;Are you using the latest version?&lt;/h3&gt; 
&lt;p&gt;Before reporting any issue, type &lt;code&gt;youtube-dl -U&lt;/code&gt;. This should report that you're up-to-date. About 20% of the reports we receive are already fixed, but people are using outdated versions. This goes for feature requests as well.&lt;/p&gt; 
&lt;h3&gt;Why are existing options not enough?&lt;/h3&gt; 
&lt;p&gt;Before requesting a new feature, please have a quick peek at &lt;a href="https://github.com/ytdl-org/youtube-dl/raw/master/README.md#options"&gt;the list of supported options&lt;/a&gt;. Many feature requests are for features that actually exist already! Please, absolutely do show off your work in the issue report and detail how the existing similar options do &lt;em&gt;not&lt;/em&gt; solve your problem.&lt;/p&gt; 
&lt;h3&gt;Is there enough context in your bug report?&lt;/h3&gt; 
&lt;p&gt;People want to solve problems, and often think they do us a favor by breaking down their larger problems (e.g. wanting to skip already downloaded files) to a specific request (e.g. requesting us to look whether the file exists before downloading the info page). However, what often happens is that they break down the problem into two steps: One simple, and one impossible (or extremely complicated one).&lt;/p&gt; 
&lt;p&gt;We are then presented with a very complicated request when the original problem could be solved far easier, e.g. by recording the downloaded video IDs in a separate file. To avoid this, you must include the greater context where it is non-obvious. In particular, every feature request that does not consist of adding support for a new site should contain a use case scenario that explains in what situation the missing feature would be useful.&lt;/p&gt; 
&lt;h3&gt;Does the issue involve one problem, and one problem only?&lt;/h3&gt; 
&lt;p&gt;Some of our users seem to think there is a limit of issues they can or should open. There is no limit of issues they can or should open. While it may seem appealing to be able to dump all your issues into one ticket, that means that someone who solves one of your issues cannot mark the issue as closed. Typically, reporting a bunch of issues leads to the ticket lingering since nobody wants to attack that behemoth, until someone mercifully splits the issue into multiple ones.&lt;/p&gt; 
&lt;p&gt;In particular, every site support request issue should only pertain to services at one site (generally under a common domain, but always using the same backend technology). Do not request support for vimeo user videos, White house podcasts, and Google Plus pages in the same issue. Also, make sure that you don't post bug reports alongside feature requests. As a rule of thumb, a feature request does not include outputs of youtube-dl that are not immediately related to the feature at hand. Do not post reports of a network error alongside the request for a new video service.&lt;/p&gt; 
&lt;h3&gt;Is anyone going to need the feature?&lt;/h3&gt; 
&lt;p&gt;Only post features that you (or an incapacitated friend you can personally talk to) require. Do not post features because they seem like a good idea. If they are really useful, they will be requested by someone who requires them.&lt;/p&gt; 
&lt;h3&gt;Is your question about youtube-dl?&lt;/h3&gt; 
&lt;p&gt;It may sound strange, but some bug reports we receive are completely unrelated to youtube-dl and relate to a different, or even the reporter's own, application. Please make sure that you are actually using youtube-dl. If you are using a UI for youtube-dl, report the bug to the maintainer of the actual application providing the UI. On the other hand, if your UI for youtube-dl fails in some way you believe is related to youtube-dl, by all means, go ahead and report the bug.&lt;/p&gt; 
&lt;h1&gt;COPYRIGHT&lt;/h1&gt; 
&lt;p&gt;youtube-dl is released into the public domain by the copyright holders.&lt;/p&gt; 
&lt;p&gt;This README file was originally written by &lt;a href="https://github.com/dbbolton"&gt;Daniel Bolton&lt;/a&gt; and is likewise released into the public domain.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>