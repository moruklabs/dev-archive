<rss version="2.0">
  <channel>
    <title>GitHub All Languages Daily Trending</title>
    <description>Daily Trending of All Languages in GitHub</description>
    <pubDate>Tue, 20 Jan 2026 01:31:23 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>OpenBMB/VoxCPM</title>
      <link>https://github.com/OpenBMB/VoxCPM</link>
      <description>&lt;p&gt;VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;üéôÔ∏è VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/OpenBMB/VoxCPM/"&gt;&lt;img src="https://img.shields.io/badge/Project%20Page-GitHub-blue" alt="Project Page" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2509.24650"&gt;&lt;img src="https://img.shields.io/badge/Technical%20Report-Arxiv-red" alt="Technical Report" /&gt;&lt;/a&gt;&lt;a href="https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo"&gt;&lt;img src="https://img.shields.io/badge/Live%20PlayGround-Demo-orange" alt="Live Playground" /&gt;&lt;/a&gt; &lt;a href="https://openbmb.github.io/VoxCPM-demopage"&gt;&lt;img src="https://img.shields.io/badge/Audio%20Samples-Page-green" alt="Samples" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;VoxCPM1.5 Model Weights&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/openbmb/VoxCPM1.5"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-OpenBMB-yellow" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://modelscope.cn/models/OpenBMB/VoxCPM1.5"&gt;&lt;img src="https://img.shields.io/badge/ModelScope-OpenBMB-purple" alt="ModelScope" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/assets/voxcpm_logo.png" alt="VoxCPM Logo" width="40%" /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;üëã Contact us on &lt;a href="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/assets/wechat.png"&gt;WeChat&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2025.12.05] üéâ üéâ üéâ We Open Source the VoxCPM1.5 &lt;a href="https://huggingface.co/openbmb/VoxCPM1.5"&gt;weights&lt;/a&gt;! The model now supports both full-parameter fine-tuning and efficient LoRA fine-tuning, empowering you to create your own tailored version. See &lt;a href="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/docs/release_note.md"&gt;Release Notes&lt;/a&gt; for details.&lt;/li&gt; 
 &lt;li&gt;[2025.09.30] üî• üî• üî• We Release VoxCPM &lt;a href="https://arxiv.org/abs/2509.24650"&gt;Technical Report&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;[2025.09.16] üî• üî• üî• We Open Source the VoxCPM-0.5B &lt;a href="https://huggingface.co/openbmb/VoxCPM-0.5B"&gt;weights&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;[2025.09.16] üéâ üéâ üéâ We Provide the &lt;a href="https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo"&gt;Gradio PlayGround&lt;/a&gt; for VoxCPM-0.5B, try it now!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;VoxCPM is a novel tokenizer-free Text-to-Speech (TTS) system that redefines realism in speech synthesis. By modeling speech in a continuous space, it overcomes the limitations of discrete tokenization and enables two flagship capabilities: context-aware speech generation and true-to-life zero-shot voice cloning.&lt;/p&gt; 
&lt;p&gt;Unlike mainstream approaches that convert speech to discrete tokens, VoxCPM uses an end-to-end diffusion autoregressive architecture that directly generates continuous speech representations from text. Built on &lt;a href="https://huggingface.co/openbmb/MiniCPM4-0.5B"&gt;MiniCPM-4&lt;/a&gt; backbone, it achieves implicit semantic-acoustic decoupling through hierachical language modeling and FSQ constraints, greatly enhancing both expressiveness and generation stability.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/assets/voxcpm_model.png" alt="VoxCPM Model Architecture" width="90%" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;üöÄ Key Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Context-Aware, Expressive Speech Generation&lt;/strong&gt; - VoxCPM comprehends text to infer and generate appropriate prosody, delivering speech with remarkable expressiveness and natural flow. It spontaneously adapts speaking style based on content, producing highly fitting vocal expression trained on a massive 1.8 million-hour bilingual corpus.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;True-to-Life Voice Cloning&lt;/strong&gt; - With only a short reference audio clip, VoxCPM performs accurate zero-shot voice cloning, capturing not only the speaker's timbre but also fine-grained characteristics such as accent, emotional tone, rhythm, and pacing to create a faithful and natural replica.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High-Efficiency Synthesis&lt;/strong&gt; - VoxCPM supports streaming synthesis with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, making it possible for real-time applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üì¶ Model Versions&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/docs/release_note.md"&gt;Release Notes&lt;/a&gt; for details&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;VoxCPM1.5&lt;/strong&gt; (Latest):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Model Params: 800M&lt;/li&gt; 
   &lt;li&gt;Sampling rate of AudioVAE: 44100&lt;/li&gt; 
   &lt;li&gt;Token rate in LM Backbone: 6.25Hz (patch-size=4)&lt;/li&gt; 
   &lt;li&gt;RTF in a single NVIDIA-RTX 4090 GPU: ~0.15&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;VoxCPM-0.5B&lt;/strong&gt; (Original):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Model Params: 640M&lt;/li&gt; 
   &lt;li&gt;Sampling rate of AudioVAE: 16000&lt;/li&gt; 
   &lt;li&gt;Token rate in LM Backbone: 12.5Hz (patch-size=2)&lt;/li&gt; 
   &lt;li&gt;RTF in a single NVIDIA-RTX 4090 GPU: 0.17&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;üîß Install from PyPI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install voxcpm
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;1. Model Download (Optional)&lt;/h3&gt; 
&lt;p&gt;By default, when you first run the script, the model will be downloaded automatically, but you can also download the model in advance.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Download VoxCPM1.5&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from huggingface_hub import snapshot_download
snapshot_download("openbmb/VoxCPM1.5")
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Or Download VoxCPM-0.5B&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from huggingface_hub import snapshot_download
snapshot_download("openbmb/VoxCPM-0.5B")
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Download ZipEnhancer and SenseVoice-Small. We use ZipEnhancer to enhance speech prompts and SenseVoice-Small for speech prompt ASR in the web demo.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from modelscope import snapshot_download
snapshot_download('iic/speech_zipenhancer_ans_multiloss_16k_base')
snapshot_download('iic/SenseVoiceSmall')
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;2. Basic Usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import soundfile as sf
import numpy as np
from voxcpm import VoxCPM

model = VoxCPM.from_pretrained("openbmb/VoxCPM1.5")

# Non-streaming
wav = model.generate(
    text="VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.",
    prompt_wav_path=None,      # optional: path to a prompt speech for voice cloning
    prompt_text=None,          # optional: reference text
    cfg_value=2.0,             # LM guidance on LocDiT, higher for better adherence to the prompt, but maybe worse
    inference_timesteps=10,   # LocDiT inference timesteps, higher for better result, lower for fast speed
    normalize=False,           # enable external TN tool, but will disable native raw text support
    denoise=False,             # enable external Denoise tool, but it may cause some distortion and restrict the sampling rate to 16kHz
    retry_badcase=True,        # enable retrying mode for some bad cases (unstoppable)
    retry_badcase_max_times=3,  # maximum retrying times
    retry_badcase_ratio_threshold=6.0, # maximum length restriction for bad case detection (simple but effective), it could be adjusted for slow pace speech
)

sf.write("output.wav", wav, model.tts_model.sample_rate)
print("saved: output.wav")

# Streaming
chunks = []
for chunk in model.generate_streaming(
    text = "Streaming text to speech is easy with VoxCPM!",
    # supports same args as above
):
    chunks.append(chunk)
wav = np.concatenate(chunks)

sf.write("output_streaming.wav", wav, model.tts_model.sample_rate)
print("saved: output_streaming.wav")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. CLI Usage&lt;/h3&gt; 
&lt;p&gt;After installation, the entry point is &lt;code&gt;voxcpm&lt;/code&gt; (or use &lt;code&gt;python -m voxcpm.cli&lt;/code&gt;).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1) Direct synthesis (single text)
voxcpm --text "VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech." --output out.wav

# 2) Voice cloning (reference audio + transcript)
voxcpm --text "VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech." \
  --prompt-audio path/to/voice.wav \
  --prompt-text "reference transcript" \
  --output out.wav \
  # --denoise

# (Optinal) Voice cloning (reference audio + transcript file)
voxcpm --text "VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech." \
  --prompt-audio path/to/voice.wav \
  --prompt-file "/path/to/text-file" \
  --output out.wav \
  # --denoise

# 3) Batch processing (one text per line)
voxcpm --input examples/input.txt --output-dir outs
# (optional) Batch + cloning
voxcpm --input examples/input.txt --output-dir outs \
  --prompt-audio path/to/voice.wav \
  --prompt-text "reference transcript" \
  # --denoise

# 4) Inference parameters (quality/speed)
voxcpm --text "..." --output out.wav \
  --cfg-value 2.0 --inference-timesteps 10 --normalize

# 5) Model loading
# Prefer local path
voxcpm --text "..." --output out.wav --model-path /path/to/VoxCPM_model_dir
# Or from Hugging Face (auto download/cache)
voxcpm --text "..." --output out.wav \
  --hf-model-id openbmb/VoxCPM1.5 --cache-dir ~/.cache/huggingface --local-files-only

# 6) Denoiser control
voxcpm --text "..." --output out.wav \
  --no-denoiser --zipenhancer-path iic/speech_zipenhancer_ans_multiloss_16k_base

# 7) Help
voxcpm --help
python -m voxcpm.cli --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4. Start web demo&lt;/h3&gt; 
&lt;p&gt;You can start the UI interface by running &lt;code&gt;python app.py&lt;/code&gt;, which allows you to perform Voice Cloning and Voice Creation.&lt;/p&gt; 
&lt;h3&gt;5. Fine-tuning&lt;/h3&gt; 
&lt;p&gt;VoxCPM1.5 supports both full fine-tuning (SFT) and LoRA fine-tuning, allowing you to train personalized voice models on your own data. See the &lt;a href="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/docs/finetune.md"&gt;Fine-tuning Guide&lt;/a&gt; for detailed instructions.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Quick Start:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Full fine-tuning
python scripts/train_voxcpm_finetune.py \
    --config_path conf/voxcpm_v1.5/voxcpm_finetune_all.yaml

# LoRA fine-tuning
python scripts/train_voxcpm_finetune.py \
    --config_path conf/voxcpm_v1.5/voxcpm_finetune_lora.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üìö Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/docs/usage_guide.md"&gt;Usage Guide&lt;/a&gt;&lt;/strong&gt; - Detailed guide on how to use VoxCPM effectively, including text input modes, voice cloning tips, and parameter tuning&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/docs/finetune.md"&gt;Fine-tuning Guide&lt;/a&gt;&lt;/strong&gt; - Complete guide for fine-tuning VoxCPM models with SFT and LoRA&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/docs/release_note.md"&gt;Release Notes&lt;/a&gt;&lt;/strong&gt; - Version history and updates&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/docs/performance.md"&gt;Performance Benchmarks&lt;/a&gt;&lt;/strong&gt; - Detailed performance comparisons on public benchmarks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìö More Information&lt;/h2&gt; 
&lt;h3&gt;üåü Community Projects&lt;/h3&gt; 
&lt;p&gt;We're excited to see the VoxCPM community growing! Here are some amazing projects and features built by our community:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/wildminder/ComfyUI-VoxCPM"&gt;ComfyUI-VoxCPM&lt;/a&gt;&lt;/strong&gt; A VoxCPM extension for ComfyUI.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/1038lab/ComfyUI-VoxCPMTTS"&gt;ComfyUI-VoxCPMTTS&lt;/a&gt;&lt;/strong&gt; A VoxCPM extension for ComfyUI.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rsxdalv/tts_webui_extension.vox_cpm"&gt;WebUI-VoxCPM&lt;/a&gt;&lt;/strong&gt; A template extension for TTS WebUI.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/OpenBMB/VoxCPM/pull/26"&gt;PR: Streaming API Support (by AbrahamSanders)&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/a710128/nanovllm-voxcpm"&gt;VoxCPM-NanoVLLM&lt;/a&gt;&lt;/strong&gt; NanoVLLM integration for VoxCPM for faster, high-throughput inference on GPU.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/bluryar/VoxCPM-ONNX"&gt;VoxCPM-ONNX&lt;/a&gt;&lt;/strong&gt; ONNX export for VoxCPM supports faster CPU inference.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/0seba/VoxCPMANE"&gt;VoxCPMANE&lt;/a&gt;&lt;/strong&gt; VoxCPM TTS with Apple Neural Engine backend server.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/OpenBMB/VoxCPM/pull/100"&gt;PR: LoRA finetune web UI (by Ayin1412)&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/madushan1000/voxcpm_rs"&gt;voxcpm_rs&lt;/a&gt;&lt;/strong&gt; A re-implementation of VoxCPM-0.5B in Rust.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;Note: The projects are not officially maintained by OpenBMB.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Have you built something cool with VoxCPM? We'd love to feature it here! Please open an issue or pull request to add your project.&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;üìä Performance Highlights&lt;/h3&gt; 
&lt;p&gt;VoxCPM achieves competitive results on public zero-shot TTS benchmarks. See &lt;a href="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/docs/performance.md"&gt;Performance Benchmarks&lt;/a&gt; for detailed comparison tables.&lt;/p&gt; 
&lt;h2&gt;‚ö†Ô∏è Risks and limitations&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;General Model Behavior: While VoxCPM has been trained on a large-scale dataset, it may still produce outputs that are unexpected, biased, or contain artifacts.&lt;/li&gt; 
 &lt;li&gt;Potential for Misuse of Voice Cloning: VoxCPM's powerful zero-shot voice cloning capability can generate highly realistic synthetic speech. This technology could be misused for creating convincing deepfakes for purposes of impersonation, fraud, or spreading disinformation. Users of this model must not use it to create content that infringes upon the rights of individuals. It is strictly forbidden to use VoxCPM for any illegal or unethical purposes. We strongly recommend that any publicly shared content generated with this model be clearly marked as AI-generated.&lt;/li&gt; 
 &lt;li&gt;Current Technical Limitations: Although generally stable, the model may occasionally exhibit instability, especially with very long or expressive inputs. Furthermore, the current version offers limited direct control over specific speech attributes like emotion or speaking style.&lt;/li&gt; 
 &lt;li&gt;Bilingual Model: VoxCPM is trained primarily on Chinese and English data. Performance on other languages is not guaranteed and may result in unpredictable or low-quality audio.&lt;/li&gt; 
 &lt;li&gt;This model is released for research and development purposes only. We do not recommend its use in production or commercial applications without rigorous testing and safety evaluations. Please use VoxCPM responsibly.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìù TO-DO List&lt;/h2&gt; 
&lt;p&gt;Please stay tuned for updates!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release the VoxCPM technical report.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Support higher sampling rate (44.1kHz in VoxCPM-1.5).&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Support SFT and LoRA fine-tuning.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Multilingual Support (besides ZH/EN).&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Controllable Speech Generation by Human Instruction.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;The VoxCPM model weights and code are open-sourced under the &lt;a href="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/LICENSE"&gt;Apache-2.0&lt;/a&gt; license.&lt;/p&gt; 
&lt;h2&gt;üôè Acknowledgments&lt;/h2&gt; 
&lt;p&gt;We extend our sincere gratitude to the following works and resources for their inspiration and contributions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2502.03930"&gt;DiTAR&lt;/a&gt; for the diffusion autoregressive backbone used in speech generation&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBMB/MiniCPM"&gt;MiniCPM-4&lt;/a&gt; for serving as the language model foundation&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/FunAudioLLM/CosyVoice"&gt;CosyVoice&lt;/a&gt; for the implementation of Flow Matching-based LocDiT&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/descriptinc/descript-audio-codec"&gt;DAC&lt;/a&gt; for providing the Audio VAE backbone&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Institutions&lt;/h2&gt; 
&lt;p&gt;This project is developed by the following institutions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/assets/modelbest_logo.png" width="28px" /&gt; &lt;a href="https://modelbest.cn/"&gt;ModelBest&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/assets/thuhcsi_logo.png" width="28px" /&gt; &lt;a href="https://github.com/thuhcsi"&gt;THUHCSI&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚≠ê Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#OpenBMB/VoxCPM&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=OpenBMB/VoxCPM&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üìö Citation&lt;/h2&gt; 
&lt;p&gt;If you find our model helpful, please consider citing our projects üìù and staring us ‚≠êÔ∏èÔºÅ&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bib"&gt;@article{voxcpm2025,
  title        = {VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning},
  author       = {Zhou, Yixuan and Zeng, Guoyang and Liu, Xin and Li, Xiang and Yu, Renjie and Wang, Ziyang and Ye, Runchuan and Sun, Weiyue and Gui, Jiancheng and Li, Kehan and Wu, Zhiyong  and Liu, Zhiyuan},
  journal      = {arXiv preprint arXiv:2509.24650},
  year         = {2025},
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>google/langextract</title>
      <link>https://github.com/google/langextract</link>
      <description>&lt;p&gt;A Python library for extracting structured information from unstructured text using LLMs with precise source grounding and interactive visualization.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://github.com/google/langextract"&gt; &lt;img src="https://raw.githubusercontent.com/google/langextract/main/docs/_static/logo.svg?sanitize=true" alt="LangExtract Logo" width="128" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;LangExtract&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/langextract/"&gt;&lt;img src="https://img.shields.io/pypi/v/langextract.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://github.com/google/langextract"&gt;&lt;img src="https://img.shields.io/github/stars/google/langextract.svg?style=social&amp;amp;label=Star" alt="GitHub stars" /&gt;&lt;/a&gt; &lt;img src="https://github.com/google/langextract/actions/workflows/ci.yaml/badge.svg?sanitize=true" alt="Tests" /&gt; &lt;a href="https://doi.org/10.5281/zenodo.17015089"&gt;&lt;img src="https://zenodo.org/badge/DOI/10.5281/zenodo.17015089.svg?sanitize=true" alt="DOI" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#why-langextract"&gt;Why LangExtract?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#api-key-setup-for-cloud-models"&gt;API Key Setup for Cloud Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#adding-custom-model-providers"&gt;Adding Custom Model Providers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#using-openai-models"&gt;Using OpenAI Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#using-local-llms-with-ollama"&gt;Using Local LLMs with Ollama&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#more-examples"&gt;More Examples&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#romeo-and-juliet-full-text-extraction"&gt;&lt;em&gt;Romeo and Juliet&lt;/em&gt; Full Text Extraction&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#medication-extraction"&gt;Medication Extraction&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#radiology-report-structuring-radextract"&gt;Radiology Report Structuring: RadExtract&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#community-providers"&gt;Community Providers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#testing"&gt;Testing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#disclaimer"&gt;Disclaimer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;LangExtract is a Python library that uses LLMs to extract structured information from unstructured text documents based on user-defined instructions. It processes materials such as clinical notes or reports, identifying and organizing key details while ensuring the extracted data corresponds to the source text.&lt;/p&gt; 
&lt;h2&gt;Why LangExtract?&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Precise Source Grounding:&lt;/strong&gt; Maps every extraction to its exact location in the source text, enabling visual highlighting for easy traceability and verification.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reliable Structured Outputs:&lt;/strong&gt; Enforces a consistent output schema based on your few-shot examples, leveraging controlled generation in supported models like Gemini to guarantee robust, structured results.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Optimized for Long Documents:&lt;/strong&gt; Overcomes the "needle-in-a-haystack" challenge of large document extraction by using an optimized strategy of text chunking, parallel processing, and multiple passes for higher recall.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interactive Visualization:&lt;/strong&gt; Instantly generates a self-contained, interactive HTML file to visualize and review thousands of extracted entities in their original context.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible LLM Support:&lt;/strong&gt; Supports your preferred models, from cloud-based LLMs like the Google Gemini family to local open-source models via the built-in Ollama interface.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Adaptable to Any Domain:&lt;/strong&gt; Define extraction tasks for any domain using just a few examples. LangExtract adapts to your needs without requiring any model fine-tuning.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Leverages LLM World Knowledge:&lt;/strong&gt; Utilize precise prompt wording and few-shot examples to influence how the extraction task may utilize LLM knowledge. The accuracy of any inferred information and its adherence to the task specification are contingent upon the selected LLM, the complexity of the task, the clarity of the prompt instructions, and the nature of the prompt examples.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Using cloud-hosted models like Gemini requires an API key. See the &lt;a href="https://raw.githubusercontent.com/google/langextract/main/#api-key-setup-for-cloud-models"&gt;API Key Setup&lt;/a&gt; section for instructions on how to get and configure your key.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Extract structured information with just a few lines of code.&lt;/p&gt; 
&lt;h3&gt;1. Define Your Extraction Task&lt;/h3&gt; 
&lt;p&gt;First, create a prompt that clearly describes what you want to extract. Then, provide a high-quality example to guide the model.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import langextract as lx
import textwrap

# 1. Define the prompt and extraction rules
prompt = textwrap.dedent("""\
    Extract characters, emotions, and relationships in order of appearance.
    Use exact text for extractions. Do not paraphrase or overlap entities.
    Provide meaningful attributes for each entity to add context.""")

# 2. Provide a high-quality example to guide the model
examples = [
    lx.data.ExampleData(
        text="ROMEO. But soft! What light through yonder window breaks? It is the east, and Juliet is the sun.",
        extractions=[
            lx.data.Extraction(
                extraction_class="character",
                extraction_text="ROMEO",
                attributes={"emotional_state": "wonder"}
            ),
            lx.data.Extraction(
                extraction_class="emotion",
                extraction_text="But soft!",
                attributes={"feeling": "gentle awe"}
            ),
            lx.data.Extraction(
                extraction_class="relationship",
                extraction_text="Juliet is the sun",
                attributes={"type": "metaphor"}
            ),
        ]
    )
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Examples drive model behavior. Each &lt;code&gt;extraction_text&lt;/code&gt; should ideally be verbatim from the example's &lt;code&gt;text&lt;/code&gt; (no paraphrasing), listed in order of appearance. LangExtract raises &lt;code&gt;Prompt alignment&lt;/code&gt; warnings by default if examples don't follow this pattern‚Äîresolve these for best results.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;2. Run the Extraction&lt;/h3&gt; 
&lt;p&gt;Provide your input text and the prompt materials to the &lt;code&gt;lx.extract&lt;/code&gt; function.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# The input text to be processed
input_text = "Lady Juliet gazed longingly at the stars, her heart aching for Romeo"

# Run the extraction
result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id="gemini-2.5-flash",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Model Selection&lt;/strong&gt;: &lt;code&gt;gemini-2.5-flash&lt;/code&gt; is the recommended default, offering an excellent balance of speed, cost, and quality. For highly complex tasks requiring deeper reasoning, &lt;code&gt;gemini-2.5-pro&lt;/code&gt; may provide superior results. For large-scale or production use, a Tier 2 Gemini quota is suggested to increase throughput and avoid rate limits. See the &lt;a href="https://ai.google.dev/gemini-api/docs/rate-limits#tier-2"&gt;rate-limit documentation&lt;/a&gt; for details.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Model Lifecycle&lt;/strong&gt;: Note that Gemini models have a lifecycle with defined retirement dates. Users should consult the &lt;a href="https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions"&gt;official model version documentation&lt;/a&gt; to stay informed about the latest stable and legacy versions.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;3. Visualize the Results&lt;/h3&gt; 
&lt;p&gt;The extractions can be saved to a &lt;code&gt;.jsonl&lt;/code&gt; file, a popular format for working with language model data. LangExtract can then generate an interactive HTML visualization from this file to review the entities in context.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Save the results to a JSONL file
lx.io.save_annotated_documents([result], output_name="extraction_results.jsonl", output_dir=".")

# Generate the visualization from the file
html_content = lx.visualize("extraction_results.jsonl")
with open("visualization.html", "w") as f:
    if hasattr(html_content, 'data'):
        f.write(html_content.data)  # For Jupyter/Colab
    else:
        f.write(html_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This creates an animated and interactive HTML file:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/google/langextract/main/docs/_static/romeo_juliet_basic.gif" alt="Romeo and Juliet Basic Visualization " /&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note on LLM Knowledge Utilization:&lt;/strong&gt; This example demonstrates extractions that stay close to the text evidence - extracting "longing" for Lady Juliet's emotional state and identifying "yearning" from "gazed longingly at the stars." The task could be modified to generate attributes that draw more heavily from the LLM's world knowledge (e.g., adding &lt;code&gt;"identity": "Capulet family daughter"&lt;/code&gt; or &lt;code&gt;"literary_context": "tragic heroine"&lt;/code&gt;). The balance between text-evidence and knowledge-inference is controlled by your prompt instructions and example attributes.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Scaling to Longer Documents&lt;/h3&gt; 
&lt;p&gt;For larger texts, you can process entire documents directly from URLs with parallel processing and enhanced sensitivity:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Process Romeo &amp;amp; Juliet directly from Project Gutenberg
result = lx.extract(
    text_or_documents="https://www.gutenberg.org/files/1513/1513-0.txt",
    prompt_description=prompt,
    examples=examples,
    model_id="gemini-2.5-flash",
    extraction_passes=3,    # Improves recall through multiple passes
    max_workers=20,         # Parallel processing for speed
    max_char_buffer=1000    # Smaller contexts for better accuracy
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This approach can extract hundreds of entities from full novels while maintaining high accuracy. The interactive visualization seamlessly handles large result sets, making it easy to explore hundreds of entities from the output JSONL file. &lt;strong&gt;&lt;a href="https://github.com/google/langextract/raw/main/docs/examples/longer_text_example.md"&gt;See the full &lt;em&gt;Romeo and Juliet&lt;/em&gt; extraction example ‚Üí&lt;/a&gt;&lt;/strong&gt; for detailed results and performance insights.&lt;/p&gt; 
&lt;h3&gt;Vertex AI Batch Processing&lt;/h3&gt; 
&lt;p&gt;Save costs on large-scale tasks by enabling Vertex AI Batch API: &lt;code&gt;language_model_params={"vertexai": True, "batch": {"enabled": True}}&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;See an example of the Vertex AI Batch API usage in &lt;a href="https://raw.githubusercontent.com/google/langextract/main/docs/examples/batch_api_example.md"&gt;this example&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;From PyPI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install langextract
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Recommended for most users. For isolated environments, consider using a virtual environment:&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv langextract_env
source langextract_env/bin/activate  # On Windows: langextract_env\Scripts\activate
pip install langextract
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;From Source&lt;/h3&gt; 
&lt;p&gt;LangExtract uses modern Python packaging with &lt;code&gt;pyproject.toml&lt;/code&gt; for dependency management:&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Installing with &lt;code&gt;-e&lt;/code&gt; puts the package in development mode, allowing you to modify the code without reinstalling.&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/google/langextract.git
cd langextract

# For basic installation:
pip install -e .

# For development (includes linting tools):
pip install -e ".[dev]"

# For testing (includes pytest):
pip install -e ".[test]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -t langextract .
docker run --rm -e LANGEXTRACT_API_KEY="your-api-key" langextract python your_script.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;API Key Setup for Cloud Models&lt;/h2&gt; 
&lt;p&gt;When using LangExtract with cloud-hosted models (like Gemini or OpenAI), you'll need to set up an API key. On-device models don't require an API key. For developers using local LLMs, LangExtract offers built-in support for Ollama and can be extended to other third-party APIs by updating the inference endpoints.&lt;/p&gt; 
&lt;h3&gt;API Key Sources&lt;/h3&gt; 
&lt;p&gt;Get API keys from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.google.com/app/apikey"&gt;AI Studio&lt;/a&gt; for Gemini models&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cloud.google.com/vertex-ai/generative-ai/docs/sdks/overview"&gt;Vertex AI&lt;/a&gt; for enterprise use&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://platform.openai.com/api-keys"&gt;OpenAI Platform&lt;/a&gt; for OpenAI models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Setting up API key in your environment&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Option 1: Environment Variable&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export LANGEXTRACT_API_KEY="your-api-key-here"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option 2: .env File (Recommended)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Add your API key to a &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Add API key to .env file
cat &amp;gt;&amp;gt; .env &amp;lt;&amp;lt; 'EOF'
LANGEXTRACT_API_KEY=your-api-key-here
EOF

# Keep your API key secure
echo '.env' &amp;gt;&amp;gt; .gitignore
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In your Python code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description="Extract information...",
    examples=[...],
    model_id="gemini-2.5-flash"
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option 3: Direct API Key (Not Recommended for Production)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You can also provide the API key directly in your code, though this is not recommended for production use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;result = lx.extract(
    text_or_documents=input_text,
    prompt_description="Extract information...",
    examples=[...],
    model_id="gemini-2.5-flash",
    api_key="your-api-key-here"  # Only use this for testing/development
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option 4: Vertex AI (Service Accounts)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Use &lt;a href="https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform"&gt;Vertex AI&lt;/a&gt; for authentication with service accounts:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;result = lx.extract(
    text_or_documents=input_text,
    prompt_description="Extract information...",
    examples=[...],
    model_id="gemini-2.5-flash",
    language_model_params={
        "vertexai": True,
        "project": "your-project-id",
        "location": "global"  # or regional endpoint
    }
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Adding Custom Model Providers&lt;/h2&gt; 
&lt;p&gt;LangExtract supports custom LLM providers via a lightweight plugin system. You can add support for new models without changing core code.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Add new model support independently of the core library&lt;/li&gt; 
 &lt;li&gt;Distribute your provider as a separate Python package&lt;/li&gt; 
 &lt;li&gt;Keep custom dependencies isolated&lt;/li&gt; 
 &lt;li&gt;Override or extend built-in providers via priority-based resolution&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the detailed guide in &lt;a href="https://raw.githubusercontent.com/google/langextract/main/langextract/providers/README.md"&gt;Provider System Documentation&lt;/a&gt; to learn how to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Register a provider with &lt;code&gt;@registry.register(...)&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Publish an entry point for discovery&lt;/li&gt; 
 &lt;li&gt;Optionally provide a schema with &lt;code&gt;get_schema_class()&lt;/code&gt; for structured output&lt;/li&gt; 
 &lt;li&gt;Integrate with the factory via &lt;code&gt;create_model(...)&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Using OpenAI Models&lt;/h2&gt; 
&lt;p&gt;LangExtract supports OpenAI models (requires optional dependency: &lt;code&gt;pip install langextract[openai]&lt;/code&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id="gpt-4o",  # Automatically selects OpenAI provider
    api_key=os.environ.get('OPENAI_API_KEY'),
    fence_output=True,
    use_schema_constraints=False
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: OpenAI models require &lt;code&gt;fence_output=True&lt;/code&gt; and &lt;code&gt;use_schema_constraints=False&lt;/code&gt; because LangExtract doesn't implement schema constraints for OpenAI yet.&lt;/p&gt; 
&lt;h2&gt;Using Local LLMs with Ollama&lt;/h2&gt; 
&lt;p&gt;LangExtract supports local inference using Ollama, allowing you to run models without API keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id="gemma2:2b",  # Automatically selects Ollama provider
    model_url="http://localhost:11434",
    fence_output=False,
    use_schema_constraints=False
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Quick setup:&lt;/strong&gt; Install Ollama from &lt;a href="https://ollama.com/"&gt;ollama.com&lt;/a&gt;, run &lt;code&gt;ollama pull gemma2:2b&lt;/code&gt;, then &lt;code&gt;ollama serve&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For detailed installation, Docker setup, and examples, see &lt;a href="https://raw.githubusercontent.com/google/langextract/main/examples/ollama/"&gt;&lt;code&gt;examples/ollama/&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;More Examples&lt;/h2&gt; 
&lt;p&gt;Additional examples of LangExtract in action:&lt;/p&gt; 
&lt;h3&gt;&lt;em&gt;Romeo and Juliet&lt;/em&gt; Full Text Extraction&lt;/h3&gt; 
&lt;p&gt;LangExtract can process complete documents directly from URLs. This example demonstrates extraction from the full text of &lt;em&gt;Romeo and Juliet&lt;/em&gt; from Project Gutenberg (147,843 characters), showing parallel processing, sequential extraction passes, and performance optimization for long document processing.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/google/langextract/raw/main/docs/examples/longer_text_example.md"&gt;View &lt;em&gt;Romeo and Juliet&lt;/em&gt; Full Text Example ‚Üí&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Medication Extraction&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; This demonstration is for illustrative purposes of LangExtract's baseline capability only. It does not represent a finished or approved product, is not intended to diagnose or suggest treatment of any disease or condition, and should not be used for medical advice.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;LangExtract excels at extracting structured medical information from clinical text. These examples demonstrate both basic entity recognition (medication names, dosages, routes) and relationship extraction (connecting medications to their attributes), showing LangExtract's effectiveness for healthcare applications.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/google/langextract/raw/main/docs/examples/medication_examples.md"&gt;View Medication Examples ‚Üí&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Radiology Report Structuring: RadExtract&lt;/h3&gt; 
&lt;p&gt;Explore RadExtract, a live interactive demo on HuggingFace Spaces that shows how LangExtract can automatically structure radiology reports. Try it directly in your browser with no setup required.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://huggingface.co/spaces/google/radextract"&gt;View RadExtract Demo ‚Üí&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Community Providers&lt;/h2&gt; 
&lt;p&gt;Extend LangExtract with custom model providers! Check out our &lt;a href="https://raw.githubusercontent.com/google/langextract/main/COMMUNITY_PROVIDERS.md"&gt;Community Provider Plugins&lt;/a&gt; registry to discover providers created by the community or add your own.&lt;/p&gt; 
&lt;p&gt;For detailed instructions on creating a provider plugin, see the &lt;a href="https://raw.githubusercontent.com/google/langextract/main/examples/custom_provider_plugin/"&gt;Custom Provider Plugin Example&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! See &lt;a href="https://github.com/google/langextract/raw/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; to get started with development, testing, and pull requests. You must sign a &lt;a href="https://cla.developers.google.com/about"&gt;Contributor License Agreement&lt;/a&gt; before submitting patches.&lt;/p&gt; 
&lt;h2&gt;Testing&lt;/h2&gt; 
&lt;p&gt;To run tests locally from the source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the repository
git clone https://github.com/google/langextract.git
cd langextract

# Install with test dependencies
pip install -e ".[test]"

# Run all tests
pytest tests
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or reproduce the full CI matrix locally with tox:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tox  # runs pylint + pytest on Python 3.10 and 3.11
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Ollama Integration Testing&lt;/h3&gt; 
&lt;p&gt;If you have Ollama installed locally, you can run integration tests:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Test Ollama integration (requires Ollama running with gemma2:2b model)
tox -e ollama-integration
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This test will automatically detect if Ollama is available and run real inference tests.&lt;/p&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;h3&gt;Code Formatting&lt;/h3&gt; 
&lt;p&gt;This project uses automated formatting tools to maintain consistent code style:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Auto-format all code
./autoformat.sh

# Or run formatters separately
isort langextract tests --profile google --line-length 80
pyink langextract tests --config pyproject.toml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Pre-commit Hooks&lt;/h3&gt; 
&lt;p&gt;For automatic formatting checks:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pre-commit install  # One-time setup
pre-commit run --all-files  # Manual run
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Linting&lt;/h3&gt; 
&lt;p&gt;Run linting before submitting PRs:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pylint --rcfile=.pylintrc langextract tests
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/google/langextract/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for full development guidelines.&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This is not an officially supported Google product. If you use LangExtract in production or publications, please cite accordingly and acknowledge usage. Use is subject to the &lt;a href="https://github.com/google/langextract/raw/main/LICENSE"&gt;Apache 2.0 License&lt;/a&gt;. For health-related applications, use of LangExtract is also subject to the &lt;a href="https://developers.google.com/health-ai-developer-foundations/terms"&gt;Health AI Developer Foundations Terms of Use&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Happy Extracting!&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>iOfficeAI/AionUi</title>
      <link>https://github.com/iOfficeAI/AionUi</link>
      <description>&lt;p&gt;Free, local, open-source Cowork for Gemini CLI, Claude Code, Codex, Opencode, Qwen Code, Goose Cli, Auggie, and more | üåü Star if you like it!&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="./resources/aionui-banner-1 copy.png" alt="AionUi - Cowork with Your CLI AI Agent" width="100%" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://img.shields.io/github/v/release/iOfficeAI/AionUi?style=flat-square&amp;amp;color=32CD32" alt="Version" /&gt; &amp;nbsp; &lt;img src="https://img.shields.io/badge/license-Apache--2.0-32CD32?style=flat-square&amp;amp;logo=apache&amp;amp;logoColor=white" alt="License" /&gt; &amp;nbsp; &lt;img src="https://img.shields.io/badge/platform-macOS%20%7C%20Windows%20%7C%20Linux-6C757D?style=flat-square&amp;amp;logo=linux&amp;amp;logoColor=white" alt="Platform" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/15423" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/15423" alt="GitHub Trending" height="80" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; &lt;strong&gt;üöÄ Cowork with Your AI, Gemini CLI, Claude Code, Codex, Qwen Code, Goose CLI, Auggie, and more&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;User-friendly | Visual graphical interface | Multi-model support | Local data security&lt;/em&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/iOfficeAI/AionUi/releases"&gt; &lt;img src="https://img.shields.io/badge/‚¨áÔ∏è%20Download%20Now-Latest%20Release-32CD32?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" alt="Download Latest Release" height="50" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;strong&gt;English&lt;/strong&gt; | &lt;a href="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/readme_ch.md"&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/readme_tw.md"&gt;ÁπÅÈ´î‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/readme_jp.md"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/readme_ko.md"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/readme_es.md"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/readme_pt.md"&gt;Portugu√™s&lt;/a&gt; | &lt;a href="https://www.aionui.com" target="_blank"&gt;Official Website&lt;/a&gt; | &lt;a href="https://twitter.com/AionUI" target="_blank"&gt;Twitter&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;strong&gt;üí¨ Community:&lt;/strong&gt; &lt;a href="https://discord.gg/g6u66vV9" target="_blank"&gt;Discord (English)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/resources/wechat.jpg" target="_blank"&gt;ÂæÆ‰ø° (‰∏≠ÊñáÁæ§)&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìã Quick Navigation&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/#%E2%9C%A8-what-can-aionui-do"&gt;‚ú® What Can AionUi Do?&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/#%F0%9F%A4%94-why-choose-aionui"&gt;ü§î Why Choose AionUi?&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/#%E2%9C%A8-core-features"&gt;‚ú® Core Features&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/#%F0%9F%9A%80-quick-start"&gt;üöÄ Quick Start&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/#%F0%9F%93%96-detailed-usage-guide"&gt;üìñ Detailed Usage Guide&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/#%F0%9F%A4%9D-community--support"&gt;üí¨ Community&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚ú® What Can AionUi Do?&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="./resources/offica-ai BANNER-function copy.png" alt="AionUi - Cowork with Your CLI AI Agent" width="800" /&gt; &lt;/p&gt; 
&lt;h3&gt;ü§ñ &lt;strong&gt;Multi-Agent Mode - Cowork for Your Command-Line AI Tools, Unified Graphical Interface&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;AionUi includes Gemini CLI built-in, ready to use out of the box with no extra installation. If you already have command-line tools like Gemini CLI, Claude Code, CodeX, Qwen Code, Goose AI, Augment Code installed, AionUi will auto-detect them and provide a unified graphical interface for a richer experience&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Auto Detection + Unified Interface&lt;/strong&gt; - Automatically recognizes local CLI tools, provides a unified graphical interface, say goodbye to command line&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Local Storage + Multi-Session&lt;/strong&gt; - Conversations saved locally, supports multiple parallel sessions, each session with independent context&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;img src="./resources/acp home page.gif" alt="Multi-Agent Mode Demo" width="800" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üìÅ &lt;strong&gt;Smart File Management (AI Cowork)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Batch renaming, automatic organization, smart classification, file merging&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Auto Organize&lt;/strong&gt;: Intelligently identify content and auto-classify, keeping folders tidy.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Efficient Batch&lt;/strong&gt;: One-click rename, merge files, say goodbye to tedious manual tasks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;img src="./resources/aionui sort file.gif" alt="Smart File Management Demo" width="800" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üìÑ &lt;strong&gt;Preview Panel - Quickly View AI-Generated Results&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Supports 9+ formats of visual preview (PDF, Word, Excel, PPT, code, Markdown, images, HTML, Diff, etc.)&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;View Results Instantly&lt;/strong&gt; - After AI generates files, view preview immediately without switching apps&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Real-time Tracking + Editable&lt;/strong&gt; - Automatically tracks file changes, editor and preview sync intelligently; supports real-time editing of Markdown, code, HTML, WYSIWYG&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/resources/preview.gif" alt="Preview Panel Demo" width="800" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üé® &lt;strong&gt;AI Image Generation &amp;amp; Editing&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Intelligent image generation, editing, and recognition, powered by Gemini&lt;/em&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/resources/Image_Generation.gif" alt="AI Image Generation Demo" width="800" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üí¨ &lt;strong&gt;Multi-Task Parallel Processing&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Open multiple conversations, tasks don't get mixed up, independent memory, double efficiency&lt;/em&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/resources/multichat-side-by-side.gif" alt="Conversation Management Demo" width="800" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üåê &lt;strong&gt;Access Anywhere - WebUI Mode&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Remotely control your AI tools - Access AionUi from any device on the network! Securely control local Gemini CLI, Claude Code, Codex, and other tools, data never leaves your device&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Basic startup
AionUi --webui

# Remote access (accessible from other devices on the local network)
AionUi --webui --remote
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üí° &lt;strong&gt;Need detailed configuration guide?&lt;/strong&gt; Check out the &lt;a href="https://github.com/iOfficeAI/AionUi/wiki/WebUI-Configuration-Guide"&gt;WebUI Configuration Tutorial&lt;/a&gt; - includes complete startup commands for all platforms&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p align="center"&gt; &lt;img src="./resources/webui banner.png" alt="WebUI Remote Access Demo" width="800" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§î Why Choose AionUi?&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Just like Claude Cowork makes Claude Code easier to use, AionUi is the Cowork platform for all your command-line AI tools&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Gemini CLI, Claude Code, Codex, Qwen Code are powerful, but share common pain points: conversations can't be saved, single-session limitations, cumbersome file operations, and only support a single model.&lt;/p&gt; 
&lt;p&gt;AionUi provides unified &lt;strong&gt;Cowork capabilities&lt;/strong&gt; for these command-line tools:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üéØ &lt;strong&gt;Unified Platform&lt;/strong&gt; - One interface to manage all command-line AI tools, no switching needed; includes Gemini CLI, ready to use out of the box and completely free&lt;/li&gt; 
 &lt;li&gt;üöÄ &lt;strong&gt;Multi-Tool Support&lt;/strong&gt; - Not only supports Claude Code, but also Gemini CLI, Codex, Qwen Code, and more&lt;/li&gt; 
 &lt;li&gt;üåê &lt;strong&gt;Cross-Platform&lt;/strong&gt; - Full platform support for macOS, Windows, Linux (Claude Cowork currently only macOS)&lt;/li&gt; 
 &lt;li&gt;üîÑ &lt;strong&gt;Multi-Model Switching&lt;/strong&gt; - Flexibly switch between different models in the same interface, meeting different task requirements&lt;/li&gt; 
 &lt;li&gt;üìÑ &lt;strong&gt;Real-time Preview&lt;/strong&gt; - Visual preview for 9+ formats, immediately view the effects of AI-generated files&lt;/li&gt; 
 &lt;li&gt;üíæ &lt;strong&gt;Local Data Security&lt;/strong&gt; - All conversations and files saved locally, data never leaves your device&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;‚ùì Quick Q&amp;amp;A&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Q: Why is AionUi a great replacement for Claude Cowork?&lt;/strong&gt;&lt;/summary&gt; A: AionUi is a **free and open-source** **Multi-AI Agent Desktop**. Compared to the official Cowork which only runs on macOS and is locked to Claude, AionUi is its **full-model, cross-platform enhanced version**, deeply covering **AI Office Automation** scenarios. 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;Dimension&lt;/th&gt; 
    &lt;th align="left"&gt;Claude Cowork&lt;/th&gt; 
    &lt;th align="left"&gt;AionUi (This Project)&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;OS&lt;/td&gt; 
    &lt;td align="left"&gt;macOS Only&lt;/td&gt; 
    &lt;td align="left"&gt;üçè macOS / ü™ü Windows / üêß Linux&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;Model Support&lt;/td&gt; 
    &lt;td align="left"&gt;Claude Only&lt;/td&gt; 
    &lt;td align="left"&gt;ü§ñ Gemini, Claude, DeepSeek, OpenAI, Ollama&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;Interaction&lt;/td&gt; 
    &lt;td align="left"&gt;GUI&lt;/td&gt; 
    &lt;td align="left"&gt;üñ•Ô∏è Full GUI + WebUI Remote Access&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;Cost&lt;/td&gt; 
    &lt;td align="left"&gt;Subscription $100/mo&lt;/td&gt; 
    &lt;td align="left"&gt;üÜì Completely Free &amp;amp; Open Source&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;strong&gt;Deep AI Office Scenario Support:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;File Management&lt;/strong&gt;: Intelligently organize messy local folders and batch rename with one click.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Data Processing&lt;/strong&gt;: Deeply analyze and automatically beautify Excel reports.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Document Generation&lt;/strong&gt;: Automatically write and format PPT, Word, and Markdown documents.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Instant Preview&lt;/strong&gt;: Built-in 9+ format preview panels, making AI office collaboration results instantly visible.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Q: What can I do with AionUi?&lt;/strong&gt;&lt;/summary&gt; A: It can be your **private Cowork workspace**. You can let it help you batch organize folders, deeply beautify Excel, and preview web code in real-time. It's your best graphical choice for exploring office automation workflows and enhancing your experience with Claude Code or Gemini CLI. 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Q: Is AionUi ready to use out of the box?&lt;/strong&gt;&lt;/summary&gt; A: Yes! AionUi is ready right after installation with a built-in Gemini CLI‚Äîno extra installation needed. If you already have Gemini CLI or other command-line tools installed, AionUi will auto-detect them for a richer experience. 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Q: Is it free?&lt;/strong&gt;&lt;/summary&gt; A: AionUi is completely free and open source, but using AI models requires corresponding API Keys. 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Q: Which AI models are supported?&lt;/strong&gt;&lt;/summary&gt; A: Supports mainstream models like Gemini, OpenAI, Claude, Qwen, as well as local models like Ollama, LM Studio. 
 &lt;p&gt;You can also run multiple AI Agents simultaneously (such as Gemini CLI, Claude Code, Qwen Code, etc.), see the configuration guide for details.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Q: Is my data secure?&lt;/strong&gt;&lt;/summary&gt; A: All conversation data is stored in a local SQLite database and will not be uploaded to any server. 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚ú® Core Features&lt;/h2&gt; 
&lt;h3&gt;üí¨ &lt;strong&gt;Multi-Session Chat&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Session + Independent Context&lt;/strong&gt; - Open multiple chats simultaneously, each session has independent context memory, no confusion&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Local Storage&lt;/strong&gt; - All conversations are saved locally and will not be lost&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ü§ñ &lt;strong&gt;Multi-Model Support&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Platform Support&lt;/strong&gt; - Supports mainstream models like Gemini, OpenAI, Claude, Qwen, flexible switching&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Local Model Support&lt;/strong&gt; - Supports local model deployment like Ollama, LM Studio, select Custom platform and set local API address (e.g., &lt;code&gt;http://localhost:11434/v1&lt;/code&gt;) to connect&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Gemini 3 Subscription Optimization&lt;/strong&gt; - Automatically identifies subscribed users, recommends advanced models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üóÇÔ∏è &lt;strong&gt;File Management&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;File Tree Browsing + Drag &amp;amp; Drop Upload&lt;/strong&gt; - Browse files like folders, support drag and drop files or folders for one-click import&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Organization&lt;/strong&gt; - You can let AI help organize folders, automatic classification&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìÑ &lt;strong&gt;Preview Panel - Give AI Agent a Display&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;9+ Format Preview&lt;/strong&gt; - Supports PDF, Word, Excel, PPT, code, Markdown, images, etc., view results immediately after AI generation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time Tracking + Editable&lt;/strong&gt; - Automatically tracks file changes, supports real-time editing and debugging of Markdown, code, HTML&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üé® &lt;strong&gt;AI Image Generation &amp;amp; Editing&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent Image Generation&lt;/strong&gt; - Supports multiple image generation models like Gemini 2.5 Flash Image Preview, Nano, Banana&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Image Recognition &amp;amp; Editing&lt;/strong&gt; - AI-driven image analysis and editing features&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üåê &lt;strong&gt;WebUI Remote Access&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Cross-Device Access&lt;/strong&gt; - Access from any device on the network via browser, supports mobile devices&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Local Data Security&lt;/strong&gt; - All data stored locally in SQLite database, suitable for server deployment&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üé® &lt;strong&gt;Personalized Interface Customization&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Customize with your own CSS code, make your interface match your preferences&lt;/em&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="./resources/css with skin.gif" alt="CSS Custom Interface Demo" width="800" /&gt; &lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Fully Customizable&lt;/strong&gt; - Freely customize interface colors, styles, layout through CSS code, create your exclusive experience&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìñ Detailed Usage Guide&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìñ Expand to View Complete Usage Guide&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h3&gt;üöÄ Quick Start&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/wiki/Getting-Started"&gt;üìñ Complete Installation Guide&lt;/a&gt; - Detailed steps from download to configuration&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/wiki/LLM-Configuration"&gt;‚öôÔ∏è LLM Configuration Guide&lt;/a&gt; - Multi-platform AI model configuration&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/wiki/ACP-Setup"&gt;ü§ñ Multi-Agent Mode Setup&lt;/a&gt; - Integrate terminal AI agents&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/wiki/MCP-Configuration-Guide"&gt;üîå MCP Tool Configuration&lt;/a&gt; - Model Context Protocol server setup&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/wiki/AionUi-Image-Generation-Tool-Model-Configuration-Guide"&gt;üé® Image Generation Configuration&lt;/a&gt; - AI image generation setup tutorial&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/wiki/WebUI-Configuration-Guide"&gt;üåê WebUI Configuration Guide&lt;/a&gt; - Complete WebUI setup and configuration tutorial&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;üéØ Use Cases&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/wiki/file-management"&gt;üìÅ File Management&lt;/a&gt; - Smart file organization&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/wiki/excel-processing"&gt;üìä Excel Processing&lt;/a&gt; - AI-driven data processing&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/wiki/AionUi-Image-Generation-Tool-Model-Configuration-Guide"&gt;üé® Image Generation&lt;/a&gt; - AI image creation&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/wiki/Use-Cases-Overview"&gt;üìö More Use Cases&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;‚ùì Support &amp;amp; Help&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/wiki/FAQ"&gt;‚ùì FAQ&lt;/a&gt; - Questions and troubleshooting&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/wiki/Configuration-Guides"&gt;üîß Configuration &amp;amp; Usage Tutorials&lt;/a&gt; - Complete configuration documentation&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;h3&gt;üíª System Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: 10.15 or higher&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: Windows 10 or higher&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt;: Ubuntu 18.04+ / Debian 10+ / Fedora 32+&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;: Recommended 4GB or more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: At least 500MB available space&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üì• Download&lt;/h3&gt; 
&lt;p&gt; &lt;a href="https://github.com/iOfficeAI/AionUi/releases"&gt; &lt;img src="https://img.shields.io/badge/Download-Latest%20Release-32CD32?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" alt="Download Latest Release" height="50" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h3&gt;üç∫ Install via Homebrew (macOS)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew install aionui
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üîß Simple Installation&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Download and install&lt;/strong&gt; AionUi application&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configure AI service&lt;/strong&gt; - Support Google account login or API Key authentication&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Start using&lt;/strong&gt; - Immediately experience modern AI chat interface&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üí° &lt;strong&gt;Need detailed configuration guide?&lt;/strong&gt; Check out our &lt;a href="https://github.com/iOfficeAI/AionUi/wiki/Getting-Started"&gt;Complete Installation Tutorial&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§ù Community &amp;amp; Support&lt;/h2&gt; 
&lt;h3&gt;üí¨ Community&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;üí° Your ideas matter!&lt;/strong&gt; We highly value every user's suggestions and feedback. Whether it's feature ideas, user experience, or issues you encounter, feel free to contact us anytime!&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://x.com/AionUi" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/resources/contactus-x.png" alt="Contact Us on X" width="600" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/discussions"&gt;üí¨ GitHub Discussions&lt;/a&gt; - &lt;strong&gt;Share ideas, make suggestions, exchange usage tips&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/issues"&gt;üêõ Report Issues&lt;/a&gt; - Report bugs or feature requests&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/releases"&gt;üì¶ Release Updates&lt;/a&gt; - Get the latest version&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/g6u66vV9"&gt;üí¨ Discord Community&lt;/a&gt; - &lt;strong&gt;Join our English community on Discord&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/resources/wechat.jpg"&gt;üí¨ ÂæÆ‰ø° (‰∏≠ÊñáÁæ§)&lt;/a&gt; - &lt;strong&gt;Click to view QR code&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ü§ù Contributing&lt;/h3&gt; 
&lt;p&gt;Welcome to submit Issues and Pull Requests!&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork this project&lt;/li&gt; 
 &lt;li&gt;Create a feature branch (&lt;code&gt;git checkout -b feature/AmazingFeature&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Commit your changes (&lt;code&gt;git commit -m 'Add some AmazingFeature'&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Push to the branch (&lt;code&gt;git push origin feature/AmazingFeature&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Open a Pull Request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under &lt;a href="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/LICENSE"&gt;Apache-2.0&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üë• Contributors&lt;/h2&gt; 
&lt;p&gt;Thanks to all developers who have contributed to AionUi!&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/iOfficeAI/AionUi/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=iOfficeAI/AionUi&amp;amp;max=20" alt="Contributors" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;üìä Star History&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.star-history.com/#iOfficeAI/aionui&amp;amp;Date" target="_blank"&gt; &lt;img src="https://api.star-history.com/svg?repos=iOfficeAI/aionui&amp;amp;type=Date" alt="GitHub Star Trends" width="600" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;‚≠ê If you like it, give us a star&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/issues"&gt;Report Bug&lt;/a&gt; ¬∑ &lt;a href="https://github.com/iOfficeAI/AionUi/issues"&gt;Request Feature&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>DavidXanatos/TaskExplorer</title>
      <link>https://github.com/DavidXanatos/TaskExplorer</link>
      <description>&lt;p&gt;Power full Task Manager&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TaskExplorer&lt;/h1&gt; 
&lt;p&gt;Task Explorer is a powerful task management tool designed not only to monitor running applications but to provide deep insight into what those applications are doing. Its user interface prioritizes speed and efficiency, delivering real-time data on processes with minimal interaction. Instead of requiring multiple windows or sub-windows, Task Explorer displays relevant information in accessible panels. When selecting a process, detailed information is displayed in the lower half of the screen, allowing you to navigate through the data seamlessly using the arrow keys. The dynamic data refresh allows users to observe changes in real-time, offering additional clarity and insight into system performance and behavior.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;Task Explorer offers an array of advanced features to provide comprehensive visibility into the system. The &lt;strong&gt;Thread Panel&lt;/strong&gt; displays a stack trace for the selected thread, offering immediate insights into the current actions of an application, which is particularly useful for diagnosing deadlocks or performance bottlenecks. The &lt;strong&gt;Memory Panel&lt;/strong&gt; allows users to view and edit process memory, featuring an advanced memory editor with string search capabilities. In the &lt;strong&gt;Handles Panel&lt;/strong&gt;, all open handles are displayed, including essential details such as file names, current file positions, and sizes, giving a clear view of the disk operations a program is performing.&lt;/p&gt; 
&lt;p&gt;The &lt;strong&gt;Socket Panel&lt;/strong&gt; provides visibility into all open connections or sockets for each process, with additional data rate information. It also has the option to show pseudo UDP connections based on ETW data, allowing users to monitor network communications effectively. The &lt;strong&gt;Modules Panel&lt;/strong&gt; lists all loaded DLLs and memory-mapped files, with the ability to unload or inject DLLs as needed. Additionally, the application includes a variety of other useful panels, including &lt;strong&gt;Token&lt;/strong&gt;, &lt;strong&gt;Environment&lt;/strong&gt;, &lt;strong&gt;Windows&lt;/strong&gt;, &lt;strong&gt;GDI&lt;/strong&gt;, and &lt;strong&gt;.NET&lt;/strong&gt; panels.&lt;/p&gt; 
&lt;p&gt;By double-clicking a process, you can open the &lt;strong&gt;Task Info Panels&lt;/strong&gt; in a separate window, enabling the simultaneous inspection of multiple processes. The system monitoring capabilities are robust as well, featuring toolbar graphs that show real-time usage of system resources such as CPU, handles, network traffic, and disk access. The &lt;strong&gt;System Info Panels&lt;/strong&gt; display all open files and sockets and allow users to control system services, including drivers. Dedicated performance panels for CPU, Memory, Disk I/O, Network, and GPU resources offer detailed graphs, making it easy to monitor and optimize system performance.&lt;/p&gt; 
&lt;p&gt;For users who need more screen space, the &lt;strong&gt;System Info Panel&lt;/strong&gt; can be fully collapsed or opened in a separate window, maximizing the available area for the task panels.&lt;/p&gt; 
&lt;h2&gt;Screen Shots&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/DavidXanatos/TaskExplorer/master/.github/images/thread_view.png" alt="image" /&gt; &lt;img src="https://raw.githubusercontent.com/DavidXanatos/TaskExplorer/master/.github/images/handle_view.png" alt="image" /&gt;&lt;/p&gt; 
&lt;h2&gt;System Requirements&lt;/h2&gt; 
&lt;p&gt;Task Explorer is compatible with Windows 7 or higher, on both 32-bit and 64-bit systems.&lt;/p&gt; 
&lt;h2&gt;Additional Information&lt;/h2&gt; 
&lt;p&gt;Task Explorer is built using the Qt Framework, ensuring a cross-platform user interface with plans to eventually port the tool to Linux, which could make it one of the first advanced, GUI-based task managers for the platform. On Windows, Task Explorer leverages the Process Hacker library and uses a custom-compiled version of the systeminformer.sys driver from the &lt;a href="https://github.com/winsiderss/systeminformer/"&gt;SystemInformer&lt;/a&gt; project, ensuring robust performance and system monitoring capabilities.&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;If you find Task Explorer useful, please consider supporting the project on Patreon: &lt;a href="https://www.patreon.com/DavidXanatos"&gt;https://www.patreon.com/DavidXanatos&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Icons provided by &lt;a href="http://icons8.com/"&gt;Icons8&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>nautechsystems/nautilus_trader</title>
      <link>https://github.com/nautechsystems/nautilus_trader</link>
      <description>&lt;p&gt;A high-performance algorithmic trading platform and event-driven backtester&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/nautilus-trader-logo.png" width="500" /&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://codecov.io/gh/nautechsystems/nautilus_trader"&gt;&lt;img src="https://codecov.io/gh/nautechsystems/nautilus_trader/branch/master/graph/badge.svg?token=DXO9QQI40H" alt="codecov" /&gt;&lt;/a&gt; &lt;a href="https://codspeed.io/nautechsystems/nautilus_trader"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://codspeed.io/badge.json" alt="codspeed" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/pyversions/nautilus_trader" alt="pythons" /&gt; &lt;img src="https://img.shields.io/pypi/v/nautilus_trader" alt="pypi-version" /&gt; &lt;img src="https://img.shields.io/pypi/format/nautilus_trader?color=blue" alt="pypi-format" /&gt; &lt;a href="https://pepy.tech/project/nautilus-trader"&gt;&lt;img src="https://pepy.tech/badge/nautilus-trader" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/NautilusTrader"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Branch&lt;/th&gt; 
   &lt;th align="left"&gt;Version&lt;/th&gt; 
   &lt;th align="left"&gt;Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;master&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://packages.nautechsystems.io/simple/nautilus-trader/index.html"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fmaster%2Fversion.json" alt="version" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml"&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=master" alt="build" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;nightly&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://packages.nautechsystems.io/simple/nautilus-trader/index.html"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fnightly%2Fversion.json" alt="version" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml"&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=nightly" alt="build" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;develop&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://packages.nautechsystems.io/simple/nautilus-trader/index.html"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fdevelop%2Fversion.json" alt="version" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml"&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=develop" alt="build" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Platform&lt;/th&gt; 
   &lt;th align="left"&gt;Rust&lt;/th&gt; 
   &lt;th align="left"&gt;Python&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Linux (x86_64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;1.92.0&lt;/td&gt; 
   &lt;td align="left"&gt;3.12-3.14&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Linux (ARM64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;1.92.0&lt;/td&gt; 
   &lt;td align="left"&gt;3.12-3.14&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;macOS (ARM64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;1.92.0&lt;/td&gt; 
   &lt;td align="left"&gt;3.12-3.14&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Windows (x86_64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;1.92.0&lt;/td&gt; 
   &lt;td align="left"&gt;3.12-3.14&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Docs&lt;/strong&gt;: &lt;a href="https://nautilustrader.io/docs/"&gt;https://nautilustrader.io/docs/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Website&lt;/strong&gt;: &lt;a href="https://nautilustrader.io"&gt;https://nautilustrader.io&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Support&lt;/strong&gt;: &lt;a href="mailto:support@nautilustrader.io"&gt;support@nautilustrader.io&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;NautilusTrader is an open-source, high-performance, production-grade algorithmic trading platform, providing quantitative traders with the ability to backtest portfolios of automated trading strategies on historical data with an event-driven engine, and also deploy those same strategies live, with no code changes.&lt;/p&gt; 
&lt;p&gt;The platform is &lt;em&gt;AI-first&lt;/em&gt;, designed to develop and deploy algorithmic trading strategies within a highly performant and robust Python-native environment. This helps to address the parity challenge of keeping the Python research/backtest environment consistent with the production live trading environment.&lt;/p&gt; 
&lt;p&gt;NautilusTrader's design, architecture, and implementation philosophy prioritizes software correctness and safety at the highest level, with the aim of supporting Python-native, mission-critical, trading system backtesting and live deployment workloads.&lt;/p&gt; 
&lt;p&gt;The platform is also universal, and asset-class-agnostic ‚Äî with any REST API or WebSocket feed able to be integrated via modular adapters. It supports high-frequency trading across a wide range of asset classes and instrument types including FX, Equities, Futures, Options, Crypto, DeFi, and Betting ‚Äî enabling seamless operations across multiple venues simultaneously.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/nautilus-trader.png" alt="nautilus-trader" title="nautilus-trader" /&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Fast&lt;/strong&gt;: Core is written in Rust with asynchronous networking using &lt;a href="https://crates.io/crates/tokio"&gt;tokio&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reliable&lt;/strong&gt;: Rust-powered type- and thread-safety, with optional Redis-backed state persistence.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Portable&lt;/strong&gt;: OS independent, runs on Linux, macOS, and Windows. Deploy using Docker.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible&lt;/strong&gt;: Modular adapters mean any REST API or WebSocket feed can be integrated.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced&lt;/strong&gt;: Time in force &lt;code&gt;IOC&lt;/code&gt;, &lt;code&gt;FOK&lt;/code&gt;, &lt;code&gt;GTC&lt;/code&gt;, &lt;code&gt;GTD&lt;/code&gt;, &lt;code&gt;DAY&lt;/code&gt;, &lt;code&gt;AT_THE_OPEN&lt;/code&gt;, &lt;code&gt;AT_THE_CLOSE&lt;/code&gt;, advanced order types and conditional triggers. Execution instructions &lt;code&gt;post-only&lt;/code&gt;, &lt;code&gt;reduce-only&lt;/code&gt;, and icebergs. Contingency orders including &lt;code&gt;OCO&lt;/code&gt;, &lt;code&gt;OUO&lt;/code&gt;, &lt;code&gt;OTO&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Customizable&lt;/strong&gt;: Add user-defined custom components, or assemble entire systems from scratch leveraging the &lt;a href="https://nautilustrader.io/docs/latest/concepts/cache"&gt;cache&lt;/a&gt; and &lt;a href="https://nautilustrader.io/docs/latest/concepts/message_bus"&gt;message bus&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backtesting&lt;/strong&gt;: Run with multiple venues, instruments and strategies simultaneously using historical quote tick, trade tick, bar, order book and custom data with nanosecond resolution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Live&lt;/strong&gt;: Use identical strategy implementations between backtesting and live deployments.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-venue&lt;/strong&gt;: Multiple venue capabilities facilitate market-making and statistical arbitrage strategies.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AI Training&lt;/strong&gt;: Backtest engine fast enough to be used to train AI trading agents (RL/ES).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/nautilus-art.png" alt="Alt text" title="nautilus" /&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;nautilus - from ancient Greek 'sailor' and naus 'ship'.&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;The nautilus shell consists of modular chambers with a growth factor which approximates a logarithmic spiral. The idea is that this can be translated to the aesthetics of design and architecture.&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Why NautilusTrader?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Highly performant event-driven Python&lt;/strong&gt;: Native binary core components.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Parity between backtesting and live trading&lt;/strong&gt;: Identical strategy code.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reduced operational risk&lt;/strong&gt;: Enhanced risk management functionality, logical accuracy, and type safety.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Highly extendable&lt;/strong&gt;: Message bus, custom components and actors, custom data, custom adapters.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Traditionally, trading strategy research and backtesting might be conducted in Python using vectorized methods, with the strategy then needing to be reimplemented in a more event-driven way using C++, C#, Java or other statically typed language(s). The reasoning here is that vectorized backtesting code cannot express the granular time and event dependent complexity of real-time trading, where compiled languages have proven to be more suitable due to their inherently higher performance, and type safety.&lt;/p&gt; 
&lt;p&gt;One of the key advantages of NautilusTrader here, is that this reimplementation step is now circumvented - as the critical core components of the platform have all been written entirely in &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt; or &lt;a href="https://cython.org/"&gt;Cython&lt;/a&gt;. This means we're using the right tools for the job, where systems programming languages compile performant binaries, with CPython C extension modules then able to offer a Python-native environment, suitable for professional quantitative traders and trading firms.&lt;/p&gt; 
&lt;h2&gt;Why Python?&lt;/h2&gt; 
&lt;p&gt;Python was originally created decades ago as a simple scripting language with a clean straightforward syntax. It has since evolved into a fully fledged general purpose object-oriented programming language. Based on the TIOBE index, Python is currently the most popular programming language in the world. Not only that, Python has become the &lt;em&gt;de facto lingua franca&lt;/em&gt; of data science, machine learning, and artificial intelligence.&lt;/p&gt; 
&lt;h2&gt;Why Rust?&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt; is a multi-paradigm programming language designed for performance and safety, especially safe concurrency. Rust is "blazingly fast" and memory-efficient (comparable to C and C++) with no garbage collector. It can power mission-critical systems, run on embedded devices, and easily integrates with other languages.&lt;/p&gt; 
&lt;p&gt;Rust's rich type system and ownership model guarantee memory-safety and thread-safety in safe code, eliminating many classes of bugs at compile-time. Overall safety in this project also depends on correctly upheld invariants in unsafe blocks and FFI boundaries.&lt;/p&gt; 
&lt;p&gt;The project increasingly utilizes Rust for core performance-critical components. Python bindings are implemented via Cython and &lt;a href="https://pyo3.rs"&gt;PyO3&lt;/a&gt;‚Äîno Rust toolchain is required at install time.&lt;/p&gt; 
&lt;p&gt;This project makes the &lt;a href="https://raphlinus.github.io/rust/2020/01/18/soundness-pledge.html"&gt;Soundness Pledge&lt;/a&gt;:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ÄúThe intent of this project is to be free of soundness bugs. The developers will do their best to avoid them, and welcome help in analyzing and fixing them.‚Äù&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;MSRV:&lt;/strong&gt; NautilusTrader relies heavily on improvements in the Rust language and compiler. As a result, the Minimum Supported Rust Version (MSRV) is generally equal to the latest stable release of Rust.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Integrations&lt;/h2&gt; 
&lt;p&gt;NautilusTrader is modularly designed to work with &lt;em&gt;adapters&lt;/em&gt;, enabling connectivity to trading venues and data providers by translating their raw APIs into a unified interface and normalized domain model.&lt;/p&gt; 
&lt;p&gt;The following integrations are currently supported; see &lt;a href="https://nautilustrader.io/docs/latest/integrations/"&gt;docs/integrations/&lt;/a&gt; for details:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Name&lt;/th&gt; 
   &lt;th align="left"&gt;ID&lt;/th&gt; 
   &lt;th align="left"&gt;Type&lt;/th&gt; 
   &lt;th align="left"&gt;Status&lt;/th&gt; 
   &lt;th align="left"&gt;Docs&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://architect.exchange"&gt;AX Exchange&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;AX&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Perpetuals Exchange&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/building-orange" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/architect_ax.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://architect.co"&gt;Architect&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;ARCHITECT&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Brokerage (multi-venue)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/planned-gray" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://betfair.com"&gt;Betfair&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BETFAIR&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Sports Betting Exchange&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/betfair.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://binance.com"&gt;Binance&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BINANCE&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/binance.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.bitmex.com"&gt;BitMEX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BITMEX&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/bitmex.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.bybit.com"&gt;Bybit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BYBIT&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/bybit.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.coinbase.com/en/international-exchange"&gt;Coinbase International&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;COINBASE_INTX&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/coinbase_intx.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://databento.com"&gt;Databento&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;DATABENTO&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Data Provider&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/databento.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.deribit.com"&gt;Deribit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;DERIBIT&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/building-orange" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/deribit.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://dydx.exchange/"&gt;dYdX v3&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;DYDX&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (DEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/dydx.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://dydx.exchange/"&gt;dYdX v4&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;DYDX&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (DEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/building-orange" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/dydx.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://hyperliquid.xyz"&gt;Hyperliquid&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;HYPERLIQUID&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (DEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/building-orange" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/hyperliquid.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.interactivebrokers.com"&gt;Interactive Brokers&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;INTERACTIVE_BROKERS&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Brokerage (multi-venue)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/ib.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://kraken.com"&gt;Kraken&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;KRAKEN&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/beta-yellow" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/kraken.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://okx.com"&gt;OKX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;OKX&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/okx.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://polymarket.com"&gt;Polymarket&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;POLYMARKET&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Prediction Market (DEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/polymarket.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://tardis.dev"&gt;Tardis&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;TARDIS&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Data Provider&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/tardis.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ID&lt;/strong&gt;: The default client ID for the integrations adapter clients.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Type&lt;/strong&gt;: The type of integration (often the venue type).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Status&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;planned&lt;/code&gt;: Planned for future development.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;building&lt;/code&gt;: Under construction and likely not in a usable state.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;beta&lt;/code&gt;: Completed to a minimally working state and in a beta testing phase.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;stable&lt;/code&gt;: Stabilized feature set and API, the integration has been tested by both developers and users to a reasonable level (some bugs may still remain).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the &lt;a href="https://nautilustrader.io/docs/latest/integrations/"&gt;Integrations&lt;/a&gt; documentation for further details.&lt;/p&gt; 
&lt;h2&gt;Versioning and releases&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;NautilusTrader is still under active development&lt;/strong&gt;. Some features may be incomplete, and while the API is becoming more stable, breaking changes can occur between releases. We strive to document these changes in the release notes on a &lt;strong&gt;best-effort basis&lt;/strong&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;We aim to follow a &lt;strong&gt;bi-weekly release schedule&lt;/strong&gt;, though experimental or larger features may cause delays.&lt;/p&gt; 
&lt;h3&gt;Branches&lt;/h3&gt; 
&lt;p&gt;We aim to maintain a stable, passing build across all branches.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;master&lt;/code&gt;: Reflects the source code for the latest released version; recommended for production use.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt;: Daily snapshots of the &lt;code&gt;develop&lt;/code&gt; branch for early testing; merged at &lt;strong&gt;14:00 UTC&lt;/strong&gt; and as required.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;develop&lt;/code&gt;: Active development branch for contributors and feature work.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;Our &lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/ROADMAP.md"&gt;roadmap&lt;/a&gt; aims to achieve a &lt;strong&gt;stable API for version 2.x&lt;/strong&gt; (likely after the Rust port). Once this milestone is reached, we plan to implement a formal deprecation process for any API changes. This approach allows us to maintain a rapid development pace for now.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Precision mode&lt;/h2&gt; 
&lt;p&gt;NautilusTrader supports two precision modes for its core value types (&lt;code&gt;Price&lt;/code&gt;, &lt;code&gt;Quantity&lt;/code&gt;, &lt;code&gt;Money&lt;/code&gt;), which differ in their internal bit-width and maximum decimal precision.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;High-precision&lt;/strong&gt;: 128-bit integers with up to 16 decimals of precision, and a larger value range.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Standard-precision&lt;/strong&gt;: 64-bit integers with up to 9 decimals of precision, and a smaller value range.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;By default, the official Python wheels ship in high-precision (128-bit) mode on Linux and macOS. On Windows, only standard-precision (64-bit) Python wheels are available because MSVC's C/C++ frontend does not support &lt;code&gt;__int128&lt;/code&gt;, preventing the Cython/FFI layer from handling 128-bit integers.&lt;/p&gt; 
 &lt;p&gt;For pure Rust crates, high-precision works on all platforms (including Windows) since Rust handles &lt;code&gt;i128&lt;/code&gt;/&lt;code&gt;u128&lt;/code&gt; via software emulation. The default is standard-precision unless you explicitly enable the &lt;code&gt;high-precision&lt;/code&gt; feature flag.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;See the &lt;a href="https://nautilustrader.io/docs/latest/getting_started/installation"&gt;Installation Guide&lt;/a&gt; for further details.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Rust feature flag&lt;/strong&gt;: To enable high-precision mode in Rust, add the &lt;code&gt;high-precision&lt;/code&gt; feature to your Cargo.toml:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;[dependencies]
nautilus_model = { version = "*", features = ["high-precision"] }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;We recommend using the latest supported version of Python and installing &lt;a href="https://pypi.org/project/nautilus_trader/"&gt;nautilus_trader&lt;/a&gt; inside a virtual environment to isolate dependencies.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;There are two supported ways to install&lt;/strong&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Pre-built binary wheel from PyPI &lt;em&gt;or&lt;/em&gt; the Nautech Systems package index.&lt;/li&gt; 
 &lt;li&gt;Build from source.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;We highly recommend installing using the &lt;a href="https://docs.astral.sh/uv"&gt;uv&lt;/a&gt; package manager with a "vanilla" CPython.&lt;/p&gt; 
 &lt;p&gt;Conda and other Python distributions &lt;em&gt;may&lt;/em&gt; work but aren‚Äôt officially supported.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;From PyPI&lt;/h3&gt; 
&lt;p&gt;To install the latest binary wheel (or sdist package) from PyPI using Python's pip package manager:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U nautilus_trader
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install optional dependencies as 'extras' for specific integrations (e.g., &lt;code&gt;betfair&lt;/code&gt;, &lt;code&gt;docker&lt;/code&gt;, &lt;code&gt;dydx&lt;/code&gt;, &lt;code&gt;ib&lt;/code&gt;, &lt;code&gt;polymarket&lt;/code&gt;, &lt;code&gt;visualization&lt;/code&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U "nautilus_trader[docker,ib]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href="https://nautilustrader.io/docs/latest/getting_started/installation#extras"&gt;Installation Guide&lt;/a&gt; for the full list of available extras.&lt;/p&gt; 
&lt;h3&gt;From the Nautech Systems package index&lt;/h3&gt; 
&lt;p&gt;The Nautech Systems package index (&lt;code&gt;packages.nautechsystems.io&lt;/code&gt;) complies with &lt;a href="https://peps.python.org/pep-0503/"&gt;PEP-503&lt;/a&gt; and hosts both stable and development binary wheels for &lt;code&gt;nautilus_trader&lt;/code&gt;. This enables users to install either the latest stable release or pre-release versions for testing.&lt;/p&gt; 
&lt;h4&gt;Stable wheels&lt;/h4&gt; 
&lt;p&gt;Stable wheels correspond to official releases of &lt;code&gt;nautilus_trader&lt;/code&gt; on PyPI, and use standard versioning.&lt;/p&gt; 
&lt;p&gt;To install the latest stable release:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U nautilus_trader --index-url=https://packages.nautechsystems.io/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;Use &lt;code&gt;--extra-index-url&lt;/code&gt; instead of &lt;code&gt;--index-url&lt;/code&gt; if you want pip to fall back to PyPI automatically:&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Development wheels&lt;/h4&gt; 
&lt;p&gt;Development wheels are published from both the &lt;code&gt;nightly&lt;/code&gt; and &lt;code&gt;develop&lt;/code&gt; branches, allowing users to test features and fixes ahead of stable releases.&lt;/p&gt; 
&lt;p&gt;This process also helps preserve compute resources and provides easy access to the exact binaries tested in CI pipelines, while adhering to &lt;a href="https://peps.python.org/pep-0440/"&gt;PEP-440&lt;/a&gt; versioning standards:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;develop&lt;/code&gt; wheels use the version format &lt;code&gt;dev{date}+{build_number}&lt;/code&gt; (e.g., &lt;code&gt;1.208.0.dev20241212+7001&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt; wheels use the version format &lt;code&gt;a{date}&lt;/code&gt; (alpha) (e.g., &lt;code&gt;1.208.0a20241212&lt;/code&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Platform&lt;/th&gt; 
   &lt;th align="left"&gt;Nightly&lt;/th&gt; 
   &lt;th align="left"&gt;Develop&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Linux (x86_64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;‚úì&lt;/td&gt; 
   &lt;td align="left"&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Linux (ARM64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;‚úì&lt;/td&gt; 
   &lt;td align="left"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;macOS (ARM64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;‚úì&lt;/td&gt; 
   &lt;td align="left"&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Windows (x86_64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;‚úì&lt;/td&gt; 
   &lt;td align="left"&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Development wheels from the &lt;code&gt;develop&lt;/code&gt; branch publish for every supported platform except Linux ARM64. Skipping that target keeps CI feedback fast while avoiding unnecessary build resource usage.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;/p&gt; 
 &lt;p&gt;We do not recommend using development wheels in production environments, such as live trading controlling real capital.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Installation commands&lt;/h4&gt; 
&lt;p&gt;By default, pip will install the latest stable release. Adding the &lt;code&gt;--pre&lt;/code&gt; flag ensures that pre-release versions, including development wheels, are considered.&lt;/p&gt; 
&lt;p&gt;To install the latest available pre-release (including development wheels):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U nautilus_trader --pre --index-url=https://packages.nautechsystems.io/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To install a specific development wheel (e.g., &lt;code&gt;1.221.0a20251026&lt;/code&gt; for October 26, 2025):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install nautilus_trader==1.221.0a20251026 --index-url=https://packages.nautechsystems.io/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Available versions&lt;/h4&gt; 
&lt;p&gt;You can view all available versions of &lt;code&gt;nautilus_trader&lt;/code&gt; on the &lt;a href="https://packages.nautechsystems.io/simple/nautilus-trader/index.html"&gt;package index&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To programmatically fetch and list available versions:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -s https://packages.nautechsystems.io/simple/nautilus-trader/index.html | grep -oP '(?&amp;lt;=&amp;lt;a href=")[^"]+(?=")' | awk -F'#' '{print $1}' | sort
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;On Linux, confirm your glibc version with &lt;code&gt;ldd --version&lt;/code&gt; and ensure it reports &lt;strong&gt;2.35&lt;/strong&gt; or newer before installing binary wheels.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Branch updates&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;develop&lt;/code&gt; branch wheels (&lt;code&gt;.dev&lt;/code&gt;): Build and publish continuously with every merged commit.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt; branch wheels (&lt;code&gt;a&lt;/code&gt;): Build and publish daily when we automatically merge the &lt;code&gt;develop&lt;/code&gt; branch at &lt;strong&gt;14:00 UTC&lt;/strong&gt; (if there are changes).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Retention policies&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;develop&lt;/code&gt; branch wheels (&lt;code&gt;.dev&lt;/code&gt;): We retain only the most recent wheel build.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt; branch wheels (&lt;code&gt;a&lt;/code&gt;): We retain only the 30 most recent wheel builds.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Verifying build provenance&lt;/h4&gt; 
&lt;p&gt;All release artifacts (wheels and source distributions) published to PyPI, GitHub Releases, and the Nautech Systems package index include cryptographic attestations that prove their authenticity and build provenance.&lt;/p&gt; 
&lt;p&gt;These attestations are generated automatically during the CI/CD pipeline using &lt;a href="https://slsa.dev/"&gt;SLSA&lt;/a&gt; build provenance, and can be verified to ensure:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The artifact was built by the official NautilusTrader GitHub Actions workflow.&lt;/li&gt; 
 &lt;li&gt;The artifact corresponds to a specific commit SHA in the repository.&lt;/li&gt; 
 &lt;li&gt;The artifact hasn't been tampered with since it was built.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To verify a wheel file using the GitHub CLI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;gh attestation verify nautilus_trader-1.220.0-*.whl --owner nautechsystems
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This provides supply chain security by allowing you to cryptographically verify that the installed package came from the official NautilusTrader build process.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;Attestation verification requires the &lt;a href="https://cli.github.com/"&gt;GitHub CLI&lt;/a&gt; (&lt;code&gt;gh&lt;/code&gt;) to be installed. Development wheels from &lt;code&gt;develop&lt;/code&gt; and &lt;code&gt;nightly&lt;/code&gt; branches are also attested and can be verified the same way.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;From source&lt;/h3&gt; 
&lt;p&gt;It's possible to install from source using pip if you first install the build dependencies as specified in the &lt;code&gt;pyproject.toml&lt;/code&gt;.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;a href="https://rustup.rs/"&gt;rustup&lt;/a&gt; (the Rust toolchain installer):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Linux and macOS:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;curl https://sh.rustup.rs -sSf | sh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Windows:&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Download and install &lt;a href="https://win.rustup.rs/x86_64"&gt;&lt;code&gt;rustup-init.exe&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;Install "Desktop development with C++" using &lt;a href="https://visualstudio.microsoft.com/visual-cpp-build-tools/"&gt;Build Tools for Visual Studio 2022&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Verify (any system): from a terminal session run: &lt;code&gt;rustc --version&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Enable &lt;code&gt;cargo&lt;/code&gt; in the current shell:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Linux and macOS:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;source $HOME/.cargo/env
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Windows:&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Start a new PowerShell&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;a href="https://clang.llvm.org/"&gt;clang&lt;/a&gt; (a C language frontend for LLVM):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Linux:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get install clang
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Windows:&lt;/p&gt; 
    &lt;ol&gt; 
     &lt;li&gt; &lt;p&gt;Add Clang to your &lt;a href="https://visualstudio.microsoft.com/visual-cpp-build-tools/"&gt;Build Tools for Visual Studio 2022&lt;/a&gt;:&lt;/p&gt; 
      &lt;ul&gt; 
       &lt;li&gt;Start | Visual Studio Installer | Modify | C++ Clang tools for Windows (latest) = checked | Modify&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;Enable &lt;code&gt;clang&lt;/code&gt; in the current shell:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-powershell"&gt;[System.Environment]::SetEnvironmentVariable('path', "C:\Program Files\Microsoft Visual Studio\2022\BuildTools\VC\Tools\Llvm\x64\bin\;" + $env:Path,"User")
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
    &lt;/ol&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Verify (any system): from a terminal session run: &lt;code&gt;clang --version&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install uv (see the &lt;a href="https://docs.astral.sh/uv/getting-started/installation"&gt;uv installation guide&lt;/a&gt; for more details):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Linux and macOS:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Windows (PowerShell):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-powershell"&gt;irm https://astral.sh/uv/install.ps1 | iex
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the source with &lt;code&gt;git&lt;/code&gt;, and install from the project's root directory:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone --branch develop --depth 1 https://github.com/nautechsystems/nautilus_trader
cd nautilus_trader
uv sync --all-extras
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;The &lt;code&gt;--depth 1&lt;/code&gt; flag fetches just the latest commit for a faster, lightweight clone.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol start="6"&gt; 
 &lt;li&gt; &lt;p&gt;Set environment variables for PyO3 compilation (Linux and macOS only):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Linux only: Set the library path for the Python interpreter
export LD_LIBRARY_PATH="$(python -c 'import sys; print(sys.base_prefix)')/lib:$LD_LIBRARY_PATH"

# Set the Python executable path for PyO3
export PYO3_PYTHON=$(pwd)/.venv/bin/python

# Required for Rust tests when using uv-installed Python
export PYTHONHOME=$(python -c "import sys; print(sys.base_prefix)")
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;The &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; export is Linux-specific and not needed on macOS.&lt;/p&gt; 
 &lt;p&gt;The &lt;code&gt;PYTHONHOME&lt;/code&gt; variable is required when running &lt;code&gt;make cargo-test&lt;/code&gt; with a &lt;code&gt;uv&lt;/code&gt;-installed Python. Without it, tests that depend on PyO3 may fail to locate the Python runtime.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;See the &lt;a href="https://nautilustrader.io/docs/latest/getting_started/installation"&gt;Installation Guide&lt;/a&gt; for other options and further details.&lt;/p&gt; 
&lt;h2&gt;Redis&lt;/h2&gt; 
&lt;p&gt;Using &lt;a href="https://redis.io"&gt;Redis&lt;/a&gt; with NautilusTrader is &lt;strong&gt;optional&lt;/strong&gt; and only required if configured as the backend for a &lt;a href="https://nautilustrader.io/docs/latest/concepts/cache"&gt;cache&lt;/a&gt; database or &lt;a href="https://nautilustrader.io/docs/latest/concepts/message_bus"&gt;message bus&lt;/a&gt;. See the &lt;strong&gt;Redis&lt;/strong&gt; section of the &lt;a href="https://nautilustrader.io/docs/latest/getting_started/installation#redis"&gt;Installation Guide&lt;/a&gt; for further details.&lt;/p&gt; 
&lt;h2&gt;Makefile&lt;/h2&gt; 
&lt;p&gt;A &lt;code&gt;Makefile&lt;/code&gt; is provided to automate most installation and build tasks for development. Some of the targets include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;make install&lt;/code&gt;: Installs in &lt;code&gt;release&lt;/code&gt; build mode with all dependency groups and extras.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make install-debug&lt;/code&gt;: Same as &lt;code&gt;make install&lt;/code&gt; but with &lt;code&gt;debug&lt;/code&gt; build mode.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make install-just-deps&lt;/code&gt;: Installs just the &lt;code&gt;main&lt;/code&gt;, &lt;code&gt;dev&lt;/code&gt; and &lt;code&gt;test&lt;/code&gt; dependencies (does not install package).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make build&lt;/code&gt;: Runs the build script in &lt;code&gt;release&lt;/code&gt; build mode (default).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make build-debug&lt;/code&gt;: Runs the build script in &lt;code&gt;debug&lt;/code&gt; build mode.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make build-wheel&lt;/code&gt;: Runs uv build with a wheel format in &lt;code&gt;release&lt;/code&gt; mode.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make build-wheel-debug&lt;/code&gt;: Runs uv build with a wheel format in &lt;code&gt;debug&lt;/code&gt; mode.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make cargo-test&lt;/code&gt;: Runs all Rust crate tests using &lt;code&gt;cargo-nextest&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make clean&lt;/code&gt;: Deletes all build results, such as &lt;code&gt;.so&lt;/code&gt; or &lt;code&gt;.dll&lt;/code&gt; files.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make distclean&lt;/code&gt;: &lt;strong&gt;CAUTION&lt;/strong&gt; Removes all artifacts not in the git index from the repository. This includes source files which have not been &lt;code&gt;git add&lt;/code&gt;ed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make docs&lt;/code&gt;: Builds the documentation HTML using Sphinx.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make pre-commit&lt;/code&gt;: Runs the pre-commit checks over all files.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make ruff&lt;/code&gt;: Runs ruff over all files using the &lt;code&gt;pyproject.toml&lt;/code&gt; config (with autofix).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make pytest&lt;/code&gt;: Runs all tests with &lt;code&gt;pytest&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make test-performance&lt;/code&gt;: Runs performance tests with &lt;a href="https://codspeed.io"&gt;codspeed&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;Run &lt;code&gt;make help&lt;/code&gt; for documentation on all available make targets.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;See the &lt;a href="https://github.com/nautechsystems/nautilus_trader/raw/develop/crates/infrastructure/TESTS.md"&gt;crates/infrastructure/TESTS.md&lt;/a&gt; file for running the infrastructure integration tests.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;Indicators and strategies can be developed in both Python and Cython. For performance and latency-sensitive applications, we recommend using Cython. Below are some examples:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/nautilus_trader/examples/indicators/ema_python.py"&gt;indicator&lt;/a&gt; example written in Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/nautilus_trader/indicators/"&gt;indicator&lt;/a&gt; implementations written in Cython.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/nautilus_trader/examples/strategies/"&gt;strategy&lt;/a&gt; examples written in Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/examples/backtest/"&gt;backtest&lt;/a&gt; examples using a &lt;code&gt;BacktestEngine&lt;/code&gt; directly.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Docker&lt;/h2&gt; 
&lt;p&gt;Docker containers are built using the base image &lt;code&gt;python:3.12-slim&lt;/code&gt; with the following variant tags:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;nautilus_trader:latest&lt;/code&gt; has the latest release version installed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nautilus_trader:nightly&lt;/code&gt; has the head of the &lt;code&gt;nightly&lt;/code&gt; branch installed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jupyterlab:latest&lt;/code&gt; has the latest release version installed along with &lt;code&gt;jupyterlab&lt;/code&gt; and an example backtest notebook with accompanying data.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jupyterlab:nightly&lt;/code&gt; has the head of the &lt;code&gt;nightly&lt;/code&gt; branch installed along with &lt;code&gt;jupyterlab&lt;/code&gt; and an example backtest notebook with accompanying data.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can pull the container images as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker pull ghcr.io/nautechsystems/&amp;lt;image_variant_tag&amp;gt; --platform linux/amd64
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can launch the backtest example container by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker pull ghcr.io/nautechsystems/jupyterlab:nightly --platform linux/amd64
docker run -p 8888:8888 ghcr.io/nautechsystems/jupyterlab:nightly
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then open your browser at the following address:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;http://127.0.0.1:8888/lab
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;/p&gt; 
 &lt;p&gt;NautilusTrader currently exceeds the rate limit for Jupyter notebook logging (stdout output). Therefore, we set the &lt;code&gt;log_level&lt;/code&gt; to &lt;code&gt;ERROR&lt;/code&gt; in the examples. Lowering this level to see more logging will cause the notebook to hang during cell execution. We are investigating a fix that may involve either raising the configured rate limits for Jupyter or throttling the log flushing from Nautilus.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/jupyterlab/jupyterlab/issues/12845"&gt;https://github.com/jupyterlab/jupyterlab/issues/12845&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/deshaw/jupyterlab-limit-output"&gt;https://github.com/deshaw/jupyterlab-limit-output&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;We aim to provide the most pleasant developer experience possible for this hybrid codebase of Python, Cython and Rust. See the &lt;a href="https://nautilustrader.io/docs/latest/developer_guide/"&gt;Developer Guide&lt;/a&gt; for helpful information.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;Run &lt;code&gt;make build-debug&lt;/code&gt; to compile after changes to Rust or Cython code for the most efficient development workflow.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Testing with Rust&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://nexte.st"&gt;cargo-nextest&lt;/a&gt; is the standard Rust test runner for NautilusTrader. Its key benefit is isolating each test in its own process, ensuring test reliability by avoiding interference.&lt;/p&gt; 
&lt;p&gt;You can install cargo-nextest by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cargo install cargo-nextest
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;Run Rust tests with &lt;code&gt;make cargo-test&lt;/code&gt;, which uses &lt;strong&gt;cargo-nextest&lt;/strong&gt; with an efficient profile.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Thank you for considering contributing to NautilusTrader! We welcome any and all help to improve the project. If you have an idea for an enhancement or a bug fix, the first step is to open an &lt;a href="https://github.com/nautechsystems/nautilus_trader/issues"&gt;issue&lt;/a&gt; on GitHub to discuss it with the team. This helps to ensure that your contribution will be well-aligned with the goals of the project and avoids duplication of effort.&lt;/p&gt; 
&lt;p&gt;Before getting started, be sure to review the &lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/ROADMAP.md#open-source-scope"&gt;open-source scope&lt;/a&gt; outlined in the project‚Äôs roadmap to understand what‚Äôs in and out of scope.&lt;/p&gt; 
&lt;p&gt;Once you're ready to start working on your contribution, make sure to follow the guidelines outlined in the &lt;a href="https://github.com/nautechsystems/nautilus_trader/raw/develop/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; file. This includes signing a Contributor License Agreement (CLA) to ensure that your contributions can be included in the project.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;Pull requests should target the &lt;code&gt;develop&lt;/code&gt; branch (the default branch). This is where new features and improvements are integrated before release.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Thank you again for your interest in NautilusTrader! We look forward to reviewing your contributions and working with you to improve the project.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Join our community of users and contributors on &lt;a href="https://discord.gg/NautilusTrader"&gt;Discord&lt;/a&gt; to chat and stay up-to-date with the latest announcements and features of NautilusTrader. Whether you're a developer looking to contribute or just want to learn more about the platform, all are welcome on our Discord server.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;/p&gt; 
 &lt;p&gt;NautilusTrader does not issue, promote, or endorse any cryptocurrency tokens. Any claims or communications suggesting otherwise are unauthorized and false.&lt;/p&gt; 
 &lt;p&gt;All official updates and communications from NautilusTrader will be shared exclusively through &lt;a href="https://nautilustrader.io"&gt;https://nautilustrader.io&lt;/a&gt;, our &lt;a href="https://discord.gg/NautilusTrader"&gt;Discord server&lt;/a&gt;, or our X (Twitter) account: &lt;a href="https://x.com/NautilusTrader"&gt;@NautilusTrader&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;If you encounter any suspicious activity, please report it to the appropriate platform and contact us at &lt;a href="mailto:info@nautechsystems.io"&gt;info@nautechsystems.io&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The source code for NautilusTrader is available on GitHub under the &lt;a href="https://www.gnu.org/licenses/lgpl-3.0.en.html"&gt;GNU Lesser General Public License v3.0&lt;/a&gt;. Contributions to the project are welcome and require the completion of a standard &lt;a href="https://github.com/nautechsystems/nautilus_trader/raw/develop/CLA.md"&gt;Contributor License Agreement (CLA)&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;NautilusTrader‚Ñ¢ is developed and maintained by Nautech Systems, a technology company specializing in the development of high-performance trading systems. For more information, visit &lt;a href="https://nautilustrader.io"&gt;https://nautilustrader.io&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;¬© 2015-2026 Nautech Systems Pty Ltd. All rights reserved.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/ns-logo.png" alt="nautechsystems" title="nautechsystems" /&gt; &lt;img src="https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/ferris.png" width="128" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ahujasid/blender-mcp</title>
      <link>https://github.com/ahujasid/blender-mcp</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;BlenderMCP - Blender Model Context Protocol Integration&lt;/h1&gt; 
&lt;p&gt;BlenderMCP connects Blender to Claude AI through the Model Context Protocol (MCP), allowing Claude to directly interact with and control Blender. This integration enables prompt assisted 3D modeling, scene creation, and manipulation.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;We have no official website. Any website you see online is unofficial and has no affiliation with this project. Use them at your own risk.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=lCyQ717DuzQ"&gt;Full tutorial&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Join the Community&lt;/h3&gt; 
&lt;p&gt;Give feedback, get inspired, and build on top of the MCP: &lt;a href="https://discord.gg/z5apgR8TFU"&gt;Discord&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Supporters&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.coderabbit.ai/"&gt;CodeRabbit&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;All supporters:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/sponsors/ahujasid"&gt;Support this project&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Release notes (1.4.0)&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Added Hunyuan3D support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Previously added features:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;View screenshots for Blender viewport to better understand the scene&lt;/li&gt; 
 &lt;li&gt;Search and download Sketchfab models&lt;/li&gt; 
 &lt;li&gt;Support for Poly Haven assets through their API&lt;/li&gt; 
 &lt;li&gt;Support to generate 3D models using Hyper3D Rodin&lt;/li&gt; 
 &lt;li&gt;Run Blender MCP on a remote host&lt;/li&gt; 
 &lt;li&gt;Telemetry for tools executed (completely anonymous)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installating a new version (existing users)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;For newcomers, you can go straight to Installation. For existing users, see the points below&lt;/li&gt; 
 &lt;li&gt;Download the latest addon.py file and replace the older one, then add it to Blender&lt;/li&gt; 
 &lt;li&gt;Delete the MCP server from Claude and add it back again, and you should be good to go!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Two-way communication&lt;/strong&gt;: Connect Claude AI to Blender through a socket-based server&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Object manipulation&lt;/strong&gt;: Create, modify, and delete 3D objects in Blender&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Material control&lt;/strong&gt;: Apply and modify materials and colors&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scene inspection&lt;/strong&gt;: Get detailed information about the current Blender scene&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code execution&lt;/strong&gt;: Run arbitrary Python code in Blender from Claude&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Components&lt;/h2&gt; 
&lt;p&gt;The system consists of two main components:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Blender Addon (&lt;code&gt;addon.py&lt;/code&gt;)&lt;/strong&gt;: A Blender addon that creates a socket server within Blender to receive and execute commands&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP Server (&lt;code&gt;src/blender_mcp/server.py&lt;/code&gt;)&lt;/strong&gt;: A Python server that implements the Model Context Protocol and connects to the Blender addon&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Blender 3.0 or newer&lt;/li&gt; 
 &lt;li&gt;Python 3.10 or newer&lt;/li&gt; 
 &lt;li&gt;uv package manager:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;If you're on Mac, please install uv as&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew install uv
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;On Windows&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;powershell -c "irm https://astral.sh/uv/install.ps1 | iex" 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and then add uv to the user path in Windows (you may need to restart Claude Desktop after):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;$localBin = "$env:USERPROFILE\.local\bin"
$userPath = [Environment]::GetEnvironmentVariable("Path", "User")
[Environment]::SetEnvironmentVariable("Path", "$userPath;$localBin", "User")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Otherwise installation instructions are on their website: &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;Install uv&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;‚ö†Ô∏è Do not proceed before installing UV&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Environment Variables&lt;/h3&gt; 
&lt;p&gt;The following environment variables can be used to configure the Blender connection:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;BLENDER_HOST&lt;/code&gt;: Host address for Blender socket server (default: "localhost")&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;BLENDER_PORT&lt;/code&gt;: Port number for Blender socket server (default: 9876)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export BLENDER_HOST='host.docker.internal'
export BLENDER_PORT=9876
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Claude for Desktop Integration&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=neoK_WMq92g"&gt;Watch the setup instruction video&lt;/a&gt; (Assuming you have already installed uv)&lt;/p&gt; 
&lt;p&gt;Go to Claude &amp;gt; Settings &amp;gt; Developer &amp;gt; Edit Config &amp;gt; claude_desktop_config.json to include the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
    "mcpServers": {
        "blender": {
            "command": "uvx",
            "args": [
                "blender-mcp"
            ]
        }
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;Claude Code&lt;/summary&gt; 
 &lt;p&gt;Use the Claude Code CLI to add the blender MCP server:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;claude mcp add blender uvx blender-mcp
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Cursor integration&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://cursor.com/install-mcp?name=blender&amp;amp;config=eyJjb21tYW5kIjoidXZ4IGJsZW5kZXItbWNwIn0%3D"&gt;&lt;img src="https://cursor.com/deeplink/mcp-install-dark.svg?sanitize=true" alt="Install MCP Server" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For Mac users, go to Settings &amp;gt; MCP and paste the following&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;To use as a global server, use "add new global MCP server" button and paste&lt;/li&gt; 
 &lt;li&gt;To use as a project specific server, create &lt;code&gt;.cursor/mcp.json&lt;/code&gt; in the root of the project and paste&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
    "mcpServers": {
        "blender": {
            "command": "uvx",
            "args": [
                "blender-mcp"
            ]
        }
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Windows users, go to Settings &amp;gt; MCP &amp;gt; Add Server, add a new server with the following settings:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
    "mcpServers": {
        "blender": {
            "command": "cmd",
            "args": [
                "/c",
                "uvx",
                "blender-mcp"
            ]
        }
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=wgWsJshecac"&gt;Cursor setup video&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;‚ö†Ô∏è Only run one instance of the MCP server (either on Cursor or Claude Desktop), not both&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Visual Studio Code Integration&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Prerequisites&lt;/em&gt;: Make sure you have &lt;a href="https://code.visualstudio.com/"&gt;Visual Studio Code&lt;/a&gt; installed before proceeding.&lt;/p&gt; 
&lt;p&gt;&lt;a href="vscode:mcp/install?%7B%22name%22%3A%22blender-mcp%22%2C%22type%22%3A%22stdio%22%2C%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22blender-mcp%22%5D%7D"&gt;&lt;img src="https://img.shields.io/badge/VS_Code-Install_blender--mcp_server-0098FF?style=flat-square&amp;amp;logo=visualstudiocode&amp;amp;logoColor=ffffff" alt="Install in VS Code" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Installing the Blender Addon&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Download the &lt;code&gt;addon.py&lt;/code&gt; file from this repo&lt;/li&gt; 
 &lt;li&gt;Open Blender&lt;/li&gt; 
 &lt;li&gt;Go to Edit &amp;gt; Preferences &amp;gt; Add-ons&lt;/li&gt; 
 &lt;li&gt;Click "Install..." and select the &lt;code&gt;addon.py&lt;/code&gt; file&lt;/li&gt; 
 &lt;li&gt;Enable the addon by checking the box next to "Interface: Blender MCP"&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Starting the Connection&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ahujasid/blender-mcp/main/assets/addon-instructions.png" alt="BlenderMCP in the sidebar" /&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;In Blender, go to the 3D View sidebar (press N if not visible)&lt;/li&gt; 
 &lt;li&gt;Find the "BlenderMCP" tab&lt;/li&gt; 
 &lt;li&gt;Turn on the Poly Haven checkbox if you want assets from their API (optional)&lt;/li&gt; 
 &lt;li&gt;Click "Connect to Claude"&lt;/li&gt; 
 &lt;li&gt;Make sure the MCP server is running in your terminal&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Using with Claude&lt;/h3&gt; 
&lt;p&gt;Once the config file has been set on Claude, and the addon is running on Blender, you will see a hammer icon with tools for the Blender MCP.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ahujasid/blender-mcp/main/assets/hammer-icon.png" alt="BlenderMCP in the sidebar" /&gt;&lt;/p&gt; 
&lt;h4&gt;Capabilities&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Get scene and object information&lt;/li&gt; 
 &lt;li&gt;Create, delete and modify shapes&lt;/li&gt; 
 &lt;li&gt;Apply or create materials for objects&lt;/li&gt; 
 &lt;li&gt;Execute any Python code in Blender&lt;/li&gt; 
 &lt;li&gt;Download the right models, assets and HDRIs through &lt;a href="https://polyhaven.com/"&gt;Poly Haven&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;AI generated 3D models through &lt;a href="https://hyper3d.ai/"&gt;Hyper3D Rodin&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Example Commands&lt;/h3&gt; 
&lt;p&gt;Here are some examples of what you can ask Claude to do:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;"Create a low poly scene in a dungeon, with a dragon guarding a pot of gold" &lt;a href="https://www.youtube.com/watch?v=DqgKuLYUv00"&gt;Demo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;"Create a beach vibe using HDRIs, textures, and models like rocks and vegetation from Poly Haven" &lt;a href="https://www.youtube.com/watch?v=I29rn92gkC4"&gt;Demo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Give a reference image, and create a Blender scene out of it &lt;a href="https://www.youtube.com/watch?v=FDRb03XPiRo"&gt;Demo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;"Generate a 3D model of a garden gnome through Hyper3D"&lt;/li&gt; 
 &lt;li&gt;"Get information about the current scene, and make a threejs sketch from it" &lt;a href="https://www.youtube.com/watch?v=jxbNI5L7AH8"&gt;Demo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;"Make this car red and metallic"&lt;/li&gt; 
 &lt;li&gt;"Create a sphere and place it above the cube"&lt;/li&gt; 
 &lt;li&gt;"Make the lighting like a studio"&lt;/li&gt; 
 &lt;li&gt;"Point the camera at the scene, and make it isometric"&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Hyper3D integration&lt;/h2&gt; 
&lt;p&gt;Hyper3D's free trial key allows you to generate a limited number of models per day. If the daily limit is reached, you can wait for the next day's reset or obtain your own key from hyper3d.ai and fal.ai.&lt;/p&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Connection issues&lt;/strong&gt;: Make sure the Blender addon server is running, and the MCP server is configured on Claude, DO NOT run the uvx command in the terminal. Sometimes, the first command won't go through but after that it starts working.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Timeout errors&lt;/strong&gt;: Try simplifying your requests or breaking them into smaller steps&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Poly Haven integration&lt;/strong&gt;: Claude is sometimes erratic with its behaviour&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Have you tried turning it off and on again?&lt;/strong&gt;: If you're still having connection errors, try restarting both Claude and the Blender server&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Technical Details&lt;/h2&gt; 
&lt;h3&gt;Communication Protocol&lt;/h3&gt; 
&lt;p&gt;The system uses a simple JSON-based protocol over TCP sockets:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Commands&lt;/strong&gt; are sent as JSON objects with a &lt;code&gt;type&lt;/code&gt; and optional &lt;code&gt;params&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Responses&lt;/strong&gt; are JSON objects with a &lt;code&gt;status&lt;/code&gt; and &lt;code&gt;result&lt;/code&gt; or &lt;code&gt;message&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Limitations &amp;amp; Security Considerations&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;The &lt;code&gt;execute_blender_code&lt;/code&gt; tool allows running arbitrary Python code in Blender, which can be powerful but potentially dangerous. Use with caution in production environments. ALWAYS save your work before using it.&lt;/li&gt; 
 &lt;li&gt;Poly Haven requires downloading models, textures, and HDRI images. If you do not want to use it, please turn it off in the checkbox in Blender.&lt;/li&gt; 
 &lt;li&gt;Complex operations might need to be broken down into smaller steps&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! Please feel free to submit a Pull Request.&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This is a third-party integration and not made by Blender. Made by &lt;a href="https://x.com/sidahuj"&gt;Siddharth&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>czlonkowski/n8n-mcp</title>
      <link>https://github.com/czlonkowski/n8n-mcp</link>
      <description>&lt;p&gt;A MCP for Claude Desktop / Claude Code / Windsurf / Cursor to build n8n workflows for you&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;n8n-MCP&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt; &lt;a href="https://github.com/czlonkowski/n8n-mcp"&gt;&lt;img src="https://img.shields.io/github/stars/czlonkowski/n8n-mcp?style=social" alt="GitHub stars" /&gt;&lt;/a&gt; &lt;a href="https://www.npmjs.com/package/n8n-mcp"&gt;&lt;img src="https://img.shields.io/npm/v/n8n-mcp.svg?sanitize=true" alt="npm version" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/czlonkowski/n8n-mcp"&gt;&lt;img src="https://codecov.io/gh/czlonkowski/n8n-mcp/graph/badge.svg?token=YOUR_TOKEN" alt="codecov" /&gt;&lt;/a&gt; &lt;a href="https://github.com/czlonkowski/n8n-mcp/actions"&gt;&lt;img src="https://img.shields.io/badge/tests-3336%20passing-brightgreen.svg?sanitize=true" alt="Tests" /&gt;&lt;/a&gt; &lt;a href="https://github.com/n8n-io/n8n"&gt;&lt;img src="https://img.shields.io/badge/n8n-2.3.3-orange.svg?sanitize=true" alt="n8n version" /&gt;&lt;/a&gt; &lt;a href="https://github.com/czlonkowski/n8n-mcp/pkgs/container/n8n-mcp"&gt;&lt;img src="https://img.shields.io/badge/docker-ghcr.io%2Fczlonkowski%2Fn8n--mcp-green.svg?sanitize=true" alt="Docker" /&gt;&lt;/a&gt; &lt;a href="https://railway.com/deploy/n8n-mcp?referralCode=n8n-mcp"&gt;&lt;img src="https://railway.com/button.svg?sanitize=true" alt="Deploy on Railway" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;A Model Context Protocol (MCP) server that provides AI assistants with comprehensive access to n8n node documentation, properties, and operations. Deploy in minutes to give Claude and other AI assistants deep knowledge about n8n's 1,084 workflow automation nodes (537 core + 547 community).&lt;/p&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;n8n-MCP serves as a bridge between n8n's workflow automation platform and AI models, enabling them to understand and work with n8n nodes effectively. It provides structured access to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìö &lt;strong&gt;1,084 n8n nodes&lt;/strong&gt; - 537 core nodes + 547 community nodes (301 verified)&lt;/li&gt; 
 &lt;li&gt;üîß &lt;strong&gt;Node properties&lt;/strong&gt; - 99% coverage with detailed schemas&lt;/li&gt; 
 &lt;li&gt;‚ö° &lt;strong&gt;Node operations&lt;/strong&gt; - 63.6% coverage of available actions&lt;/li&gt; 
 &lt;li&gt;üìÑ &lt;strong&gt;Documentation&lt;/strong&gt; - 87% coverage from official n8n docs (including AI nodes)&lt;/li&gt; 
 &lt;li&gt;ü§ñ &lt;strong&gt;AI tools&lt;/strong&gt; - 265 AI-capable tool variants detected with full documentation&lt;/li&gt; 
 &lt;li&gt;üí° &lt;strong&gt;Real-world examples&lt;/strong&gt; - 2,646 pre-extracted configurations from popular templates&lt;/li&gt; 
 &lt;li&gt;üéØ &lt;strong&gt;Template library&lt;/strong&gt; - 2,709 workflow templates with 100% metadata coverage&lt;/li&gt; 
 &lt;li&gt;üåê &lt;strong&gt;Community nodes&lt;/strong&gt; - Search verified community integrations with &lt;code&gt;source&lt;/code&gt; filter (NEW!)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚ö†Ô∏è Important Safety Warning&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;NEVER edit your production workflows directly with AI!&lt;/strong&gt; Always:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üîÑ &lt;strong&gt;Make a copy&lt;/strong&gt; of your workflow before using AI tools&lt;/li&gt; 
 &lt;li&gt;üß™ &lt;strong&gt;Test in development&lt;/strong&gt; environment first&lt;/li&gt; 
 &lt;li&gt;üíæ &lt;strong&gt;Export backups&lt;/strong&gt; of important workflows&lt;/li&gt; 
 &lt;li&gt;‚ö° &lt;strong&gt;Validate changes&lt;/strong&gt; before deploying to production&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;AI results can be unpredictable. Protect your work!&lt;/p&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;h3&gt;Option 1: Hosted Service (Easiest - No Setup!) ‚òÅÔ∏è&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;The fastest way to try n8n-MCP&lt;/strong&gt; - no installation, no configuration:&lt;/p&gt; 
&lt;p&gt;üëâ &lt;strong&gt;&lt;a href="https://dashboard.n8n-mcp.com"&gt;dashboard.n8n-mcp.com&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Free tier&lt;/strong&gt;: 100 tool calls/day&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Instant access&lt;/strong&gt;: Start building workflows immediately&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Always up-to-date&lt;/strong&gt;: Latest n8n nodes and templates&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;No infrastructure&lt;/strong&gt;: We handle everything&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Just sign up, get your API key, and connect your MCP client.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üè† Self-Hosting Options&lt;/h2&gt; 
&lt;p&gt;Prefer to run n8n-MCP yourself? Choose your deployment method:&lt;/p&gt; 
&lt;h3&gt;Option A: npx (Quick Local Setup) üöÄ&lt;/h3&gt; 
&lt;p&gt;Get n8n-MCP running in minutes:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://youtu.be/5CccjiLLyaY?si=Z62SBGlw9G34IQnQ&amp;amp;t=343"&gt;&lt;img src="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/thumbnail.png" alt="n8n-mcp Video Quickstart Guide" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Prerequisites:&lt;/strong&gt; &lt;a href="https://nodejs.org/"&gt;Node.js&lt;/a&gt; installed on your system&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run directly with npx (no installation needed!)
npx n8n-mcp
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Add to Claude Desktop config:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Important&lt;/strong&gt;: The &lt;code&gt;MCP_MODE: "stdio"&lt;/code&gt; environment variable is &lt;strong&gt;required&lt;/strong&gt; for Claude Desktop. Without it, you will see JSON parsing errors like &lt;code&gt;"Unexpected token..."&lt;/code&gt; in the UI. This variable ensures that only JSON-RPC messages are sent to stdout, preventing debug logs from interfering with the protocol.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Basic configuration (documentation tools only):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "n8n-mcp": {
      "command": "npx",
      "args": ["n8n-mcp"],
      "env": {
        "MCP_MODE": "stdio",
        "LOG_LEVEL": "error",
        "DISABLE_CONSOLE_OUTPUT": "true"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Full configuration (with n8n management tools):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "n8n-mcp": {
      "command": "npx",
      "args": ["n8n-mcp"],
      "env": {
        "MCP_MODE": "stdio",
        "LOG_LEVEL": "error",
        "DISABLE_CONSOLE_OUTPUT": "true",
        "N8N_API_URL": "https://your-n8n-instance.com",
        "N8N_API_KEY": "your-api-key"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: npx will download and run the latest version automatically. The package includes a pre-built database with all n8n node information.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Configuration file locations:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: &lt;code&gt;~/Library/Application Support/Claude/claude_desktop_config.json&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: &lt;code&gt;%APPDATA%\Claude\claude_desktop_config.json&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt;: &lt;code&gt;~/.config/Claude/claude_desktop_config.json&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Restart Claude Desktop after updating configuration&lt;/strong&gt; - That's it! üéâ&lt;/p&gt; 
&lt;h3&gt;Option B: Docker (Isolated &amp;amp; Reproducible) üê≥&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Prerequisites:&lt;/strong&gt; Docker installed on your system&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üì¶ Install Docker&lt;/strong&gt; (click to expand)&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;macOS:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Using Homebrew
brew install --cask docker

# Or download from https://www.docker.com/products/docker-desktop/
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Linux (Ubuntu/Debian):&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Update package index
sudo apt-get update

# Install Docker
sudo apt-get install docker.io

# Start Docker service
sudo systemctl start docker
sudo systemctl enable docker

# Add your user to docker group (optional, to run without sudo)
sudo usermod -aG docker $USER
# Log out and back in for this to take effect
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Windows:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Option 1: Using winget (Windows Package Manager)
winget install Docker.DockerDesktop

# Option 2: Using Chocolatey
choco install docker-desktop

# Option 3: Download installer from https://www.docker.com/products/docker-desktop/
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Verify installation:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;docker --version
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Pull the Docker image (~280MB, no n8n dependencies!)
docker pull ghcr.io/czlonkowski/n8n-mcp:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;‚ö° Ultra-optimized:&lt;/strong&gt; Our Docker image is 82% smaller than typical n8n images because it contains NO n8n dependencies - just the runtime MCP server with a pre-built database!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Add to Claude Desktop config:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Basic configuration (documentation tools only):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "n8n-mcp": {
      "command": "docker",
      "args": [
        "run",
        "-i",
        "--rm",
        "--init",
        "-e", "MCP_MODE=stdio",
        "-e", "LOG_LEVEL=error",
        "-e", "DISABLE_CONSOLE_OUTPUT=true",
        "ghcr.io/czlonkowski/n8n-mcp:latest"
      ]
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Full configuration (with n8n management tools):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "n8n-mcp": {
      "command": "docker",
      "args": [
        "run",
        "-i",
        "--rm",
        "--init",
        "-e", "MCP_MODE=stdio",
        "-e", "LOG_LEVEL=error",
        "-e", "DISABLE_CONSOLE_OUTPUT=true",
        "-e", "N8N_API_URL=https://your-n8n-instance.com",
        "-e", "N8N_API_KEY=your-api-key",
        "ghcr.io/czlonkowski/n8n-mcp:latest"
      ]
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üí° Tip: If you're running n8n locally on the same machine (e.g., via Docker), use &lt;a href="http://host.docker.internal:5678"&gt;http://host.docker.internal:5678&lt;/a&gt; as the N8N_API_URL.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The n8n API credentials are optional. Without them, you'll have access to all documentation and validation tools. With them, you'll additionally get workflow management capabilities (create, update, execute workflows).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;üè† Local n8n Instance Configuration&lt;/h3&gt; 
&lt;p&gt;If you're running n8n locally (e.g., &lt;code&gt;http://localhost:5678&lt;/code&gt; or Docker), you need to allow localhost webhooks:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "n8n-mcp": {
      "command": "docker",
      "args": [
        "run", "-i", "--rm", "--init",
        "-e", "MCP_MODE=stdio",
        "-e", "LOG_LEVEL=error",
        "-e", "DISABLE_CONSOLE_OUTPUT=true",
        "-e", "N8N_API_URL=http://host.docker.internal:5678",
        "-e", "N8N_API_KEY=your-api-key",
        "-e", "WEBHOOK_SECURITY_MODE=moderate",
        "ghcr.io/czlonkowski/n8n-mcp:latest"
      ]
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Important:&lt;/strong&gt; Set &lt;code&gt;WEBHOOK_SECURITY_MODE=moderate&lt;/code&gt; to allow webhooks to your local n8n instance. This is safe for local development while still blocking private networks and cloud metadata.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; The &lt;code&gt;-i&lt;/code&gt; flag is required for MCP stdio communication.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üîß If you encounter any issues with Docker, check our &lt;a href="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/docs/DOCKER_TROUBLESHOOTING.md"&gt;Docker Troubleshooting Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Configuration file locations:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: &lt;code&gt;~/Library/Application Support/Claude/claude_desktop_config.json&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: &lt;code&gt;%APPDATA%\Claude\claude_desktop_config.json&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt;: &lt;code&gt;~/.config/Claude/claude_desktop_config.json&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Restart Claude Desktop after updating configuration&lt;/strong&gt; - That's it! üéâ&lt;/p&gt; 
&lt;h2&gt;üîê Privacy &amp;amp; Telemetry&lt;/h2&gt; 
&lt;p&gt;n8n-mcp collects anonymous usage statistics to improve the tool. &lt;a href="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/PRIVACY.md"&gt;View our privacy policy&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Opting Out&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;For npx users:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx n8n-mcp telemetry disable
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;For Docker users:&lt;/strong&gt; Add the following environment variable to your Docker configuration:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;"-e", "N8N_MCP_TELEMETRY_DISABLED=true"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Example in Claude Desktop config:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "n8n-mcp": {
      "command": "docker",
      "args": [
        "run",
        "-i",
        "--rm",
        "--init",
        "-e", "MCP_MODE=stdio",
        "-e", "LOG_LEVEL=error",
        "-e", "N8N_MCP_TELEMETRY_DISABLED=true",
        "ghcr.io/czlonkowski/n8n-mcp:latest"
      ]
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;For docker-compose users:&lt;/strong&gt; Set in your environment file or docker-compose.yml:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;environment:
  N8N_MCP_TELEMETRY_DISABLED: "true"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;‚öôÔ∏è Database &amp;amp; Memory Configuration&lt;/h2&gt; 
&lt;h3&gt;Database Adapters&lt;/h3&gt; 
&lt;p&gt;n8n-mcp uses SQLite for storing node documentation. Two adapters are available:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;better-sqlite3&lt;/strong&gt; (Default in Docker)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Native C++ bindings for best performance&lt;/li&gt; 
   &lt;li&gt;Direct disk writes (no memory overhead)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Now enabled by default&lt;/strong&gt; in Docker images (v2.20.2+)&lt;/li&gt; 
   &lt;li&gt;Memory usage: ~100-120 MB stable&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;sql.js&lt;/strong&gt; (Fallback)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Pure JavaScript implementation&lt;/li&gt; 
   &lt;li&gt;In-memory database with periodic saves&lt;/li&gt; 
   &lt;li&gt;Used when better-sqlite3 compilation fails&lt;/li&gt; 
   &lt;li&gt;Memory usage: ~150-200 MB stable&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Memory Optimization (sql.js)&lt;/h3&gt; 
&lt;p&gt;If using sql.js fallback, you can configure the save interval to balance between data safety and memory efficiency:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Environment Variable:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;SQLJS_SAVE_INTERVAL_MS=5000  # Default: 5000ms (5 seconds)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Usage:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Controls how long to wait after database changes before saving to disk&lt;/li&gt; 
 &lt;li&gt;Lower values = more frequent saves = higher memory churn&lt;/li&gt; 
 &lt;li&gt;Higher values = less frequent saves = lower memory usage&lt;/li&gt; 
 &lt;li&gt;Minimum: 100ms&lt;/li&gt; 
 &lt;li&gt;Recommended: 5000-10000ms for production&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Docker Configuration:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "n8n-mcp": {
      "command": "docker",
      "args": [
        "run",
        "-i",
        "--rm",
        "--init",
        "-e", "SQLJS_SAVE_INTERVAL_MS=10000",
        "ghcr.io/czlonkowski/n8n-mcp:latest"
      ]
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;docker-compose:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;environment:
  SQLJS_SAVE_INTERVAL_MS: "10000"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üíñ Support This Project&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/sponsors/czlonkowski"&gt; &lt;img src="https://img.shields.io/badge/Sponsor-‚ù§Ô∏è-db61a2?style=for-the-badge&amp;amp;logo=github-sponsors" alt="Sponsor n8n-mcp" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;n8n-mcp&lt;/strong&gt; started as a personal tool but now helps tens of thousands of developers automate their workflows efficiently. Maintaining and developing this project competes with my paid work.&lt;/p&gt; 
&lt;p&gt;Your sponsorship helps me:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üöÄ Dedicate focused time to new features&lt;/li&gt; 
 &lt;li&gt;üêõ Respond quickly to issues&lt;/li&gt; 
 &lt;li&gt;üìö Keep documentation up-to-date&lt;/li&gt; 
 &lt;li&gt;üîÑ Ensure compatibility with latest n8n releases&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Every sponsorship directly translates to hours invested in making n8n-mcp better for everyone. &lt;strong&gt;&lt;a href="https://github.com/sponsors/czlonkowski"&gt;Become a sponsor ‚Üí&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Option C: Local Installation (For Development)&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Prerequisites:&lt;/strong&gt; &lt;a href="https://nodejs.org/"&gt;Node.js&lt;/a&gt; installed on your system&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Clone and setup
git clone https://github.com/czlonkowski/n8n-mcp.git
cd n8n-mcp
npm install
npm run build
npm run rebuild

# 2. Test it works
npm start
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Add to Claude Desktop config:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Basic configuration (documentation tools only):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "n8n-mcp": {
      "command": "node",
      "args": ["/absolute/path/to/n8n-mcp/dist/mcp/index.js"],
      "env": {
        "MCP_MODE": "stdio",
        "LOG_LEVEL": "error",
        "DISABLE_CONSOLE_OUTPUT": "true"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Full configuration (with n8n management tools):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "n8n-mcp": {
      "command": "node",
      "args": ["/absolute/path/to/n8n-mcp/dist/mcp/index.js"],
      "env": {
        "MCP_MODE": "stdio",
        "LOG_LEVEL": "error",
        "DISABLE_CONSOLE_OUTPUT": "true",
        "N8N_API_URL": "https://your-n8n-instance.com",
        "N8N_API_KEY": "your-api-key"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The n8n API credentials can be configured either in a &lt;code&gt;.env&lt;/code&gt; file (create from &lt;code&gt;.env.example&lt;/code&gt;) or directly in the Claude config as shown above.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üí° Tip: If you‚Äôre running n8n locally on the same machine (e.g., via Docker), use &lt;a href="http://host.docker.internal:5678"&gt;http://host.docker.internal:5678&lt;/a&gt; as the N8N_API_URL.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Option D: Railway Cloud Deployment (One-Click Deploy) ‚òÅÔ∏è&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Prerequisites:&lt;/strong&gt; Railway account (free tier available)&lt;/p&gt; 
&lt;p&gt;Deploy n8n-MCP to Railway's cloud platform with zero configuration:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://railway.com/deploy/n8n-mcp?referralCode=n8n-mcp"&gt;&lt;img src="https://railway.com/button.svg?sanitize=true" alt="Deploy on Railway" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Benefits:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚òÅÔ∏è &lt;strong&gt;Instant cloud hosting&lt;/strong&gt; - No server setup required&lt;/li&gt; 
 &lt;li&gt;üîí &lt;strong&gt;Secure by default&lt;/strong&gt; - HTTPS included, auth token warnings&lt;/li&gt; 
 &lt;li&gt;üåê &lt;strong&gt;Global access&lt;/strong&gt; - Connect from any Claude Desktop&lt;/li&gt; 
 &lt;li&gt;‚ö° &lt;strong&gt;Auto-scaling&lt;/strong&gt; - Railway handles the infrastructure&lt;/li&gt; 
 &lt;li&gt;üìä &lt;strong&gt;Built-in monitoring&lt;/strong&gt; - Logs and metrics included&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Quick Setup:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Click the "Deploy on Railway" button above&lt;/li&gt; 
 &lt;li&gt;Sign in to Railway (or create a free account)&lt;/li&gt; 
 &lt;li&gt;Configure your deployment (project name, region)&lt;/li&gt; 
 &lt;li&gt;Click "Deploy" and wait ~2-3 minutes&lt;/li&gt; 
 &lt;li&gt;Copy your deployment URL and auth token&lt;/li&gt; 
 &lt;li&gt;Add to Claude Desktop config using the HTTPS URL&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üìö &lt;strong&gt;For detailed setup instructions, troubleshooting, and configuration examples, see our &lt;a href="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/docs/RAILWAY_DEPLOYMENT.md"&gt;Railway Deployment Guide&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Configuration file locations:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: &lt;code&gt;~/Library/Application Support/Claude/claude_desktop_config.json&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: &lt;code&gt;%APPDATA%\Claude\claude_desktop_config.json&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt;: &lt;code&gt;~/.config/Claude/claude_desktop_config.json&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Restart Claude Desktop after updating configuration&lt;/strong&gt; - That's it! üéâ&lt;/p&gt; 
&lt;h2&gt;üîß n8n Integration&lt;/h2&gt; 
&lt;p&gt;Want to use n8n-MCP with your n8n instance? Check out our comprehensive &lt;a href="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/docs/N8N_DEPLOYMENT.md"&gt;n8n Deployment Guide&lt;/a&gt; for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Local testing with the MCP Client Tool node&lt;/li&gt; 
 &lt;li&gt;Production deployment with Docker Compose&lt;/li&gt; 
 &lt;li&gt;Cloud deployment on Hetzner, AWS, and other providers&lt;/li&gt; 
 &lt;li&gt;Troubleshooting and security best practices&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üíª Connect your IDE&lt;/h2&gt; 
&lt;p&gt;n8n-MCP works with multiple AI-powered IDEs and tools. Choose your preferred development environment:&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/docs/CLAUDE_CODE_SETUP.md"&gt;Claude Code&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Quick setup for Claude Code CLI - just type "add this mcp server" and paste the config.&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/docs/VS_CODE_PROJECT_SETUP.md"&gt;Visual Studio Code&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Full setup guide for VS Code with GitHub Copilot integration and MCP support.&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/docs/CURSOR_SETUP.md"&gt;Cursor&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Step-by-step tutorial for connecting n8n-MCP to Cursor IDE with custom rules.&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/docs/WINDSURF_SETUP.md"&gt;Windsurf&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Complete guide for integrating n8n-MCP with Windsurf using project rules.&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/docs/CODEX_SETUP.md"&gt;Codex&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Complete guide for integrating n8n-MCP with Codex.&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/docs/ANTIGRAVITY_SETUP.md"&gt;Antigravity&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Complete guide for integrating n8n-MCP with Antigravity.&lt;/p&gt; 
&lt;h2&gt;üéì Add Claude Skills (Optional)&lt;/h2&gt; 
&lt;p&gt;Supercharge your n8n workflow building with specialized skills that teach AI how to build production-ready workflows!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=e6VvRqmUY2Y"&gt;&lt;img src="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/docs/img/skills.png" alt="n8n-mcp Skills Setup" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Learn more: &lt;a href="https://github.com/czlonkowski/n8n-skills"&gt;n8n-skills repository&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ü§ñ Claude Project Setup&lt;/h2&gt; 
&lt;p&gt;For the best results when using n8n-MCP with Claude Projects, use these enhanced system instructions:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-markdown"&gt;You are an expert in n8n automation software using n8n-MCP tools. Your role is to design, build, and validate n8n workflows with maximum accuracy and efficiency.

## Core Principles

### 1. Silent Execution
CRITICAL: Execute tools without commentary. Only respond AFTER all tools complete.

‚ùå BAD: "Let me search for Slack nodes... Great! Now let me get details..."
‚úÖ GOOD: [Execute search_nodes and get_node in parallel, then respond]

### 2. Parallel Execution
When operations are independent, execute them in parallel for maximum performance.

‚úÖ GOOD: Call search_nodes, list_nodes, and search_templates simultaneously
‚ùå BAD: Sequential tool calls (await each one before the next)

### 3. Templates First
ALWAYS check templates before building from scratch (2,709 available).

### 4. Multi-Level Validation
Use validate_node(mode='minimal') ‚Üí validate_node(mode='full') ‚Üí validate_workflow pattern.

### 5. Never Trust Defaults
‚ö†Ô∏è CRITICAL: Default parameter values are the #1 source of runtime failures.
ALWAYS explicitly configure ALL parameters that control node behavior.

## Workflow Process

1. **Start**: Call `tools_documentation()` for best practices

2. **Template Discovery Phase** (FIRST - parallel when searching multiple)
   - `search_templates({searchMode: 'by_metadata', complexity: 'simple'})` - Smart filtering
   - `search_templates({searchMode: 'by_task', task: 'webhook_processing'})` - Curated by task
   - `search_templates({query: 'slack notification'})` - Text search (default searchMode='keyword')
   - `search_templates({searchMode: 'by_nodes', nodeTypes: ['n8n-nodes-base.slack']})` - By node type

   **Filtering strategies**:
   - Beginners: `complexity: "simple"` + `maxSetupMinutes: 30`
   - By role: `targetAudience: "marketers"` | `"developers"` | `"analysts"`
   - By time: `maxSetupMinutes: 15` for quick wins
   - By service: `requiredService: "openai"` for compatibility

3. **Node Discovery** (if no suitable template - parallel execution)
   - Think deeply about requirements. Ask clarifying questions if unclear.
   - `search_nodes({query: 'keyword', includeExamples: true})` - Parallel for multiple nodes
   - `search_nodes({query: 'trigger'})` - Browse triggers
   - `search_nodes({query: 'AI agent langchain'})` - AI-capable nodes

4. **Configuration Phase** (parallel for multiple nodes)
   - `get_node({nodeType, detail: 'standard', includeExamples: true})` - Essential properties (default)
   - `get_node({nodeType, detail: 'minimal'})` - Basic metadata only (~200 tokens)
   - `get_node({nodeType, detail: 'full'})` - Complete information (~3000-8000 tokens)
   - `get_node({nodeType, mode: 'search_properties', propertyQuery: 'auth'})` - Find specific properties
   - `get_node({nodeType, mode: 'docs'})` - Human-readable markdown documentation
   - Show workflow architecture to user for approval before proceeding

5. **Validation Phase** (parallel for multiple nodes)
   - `validate_node({nodeType, config, mode: 'minimal'})` - Quick required fields check
   - `validate_node({nodeType, config, mode: 'full', profile: 'runtime'})` - Full validation with fixes
   - Fix ALL errors before proceeding

6. **Building Phase**
   - If using template: `get_template(templateId, {mode: "full"})`
   - **MANDATORY ATTRIBUTION**: "Based on template by **[author.name]** (@[username]). View at: [url]"
   - Build from validated configurations
   - ‚ö†Ô∏è EXPLICITLY set ALL parameters - never rely on defaults
   - Connect nodes with proper structure
   - Add error handling
   - Use n8n expressions: $json, $node["NodeName"].json
   - Build in artifact (unless deploying to n8n instance)

7. **Workflow Validation** (before deployment)
   - `validate_workflow(workflow)` - Complete validation
   - `validate_workflow_connections(workflow)` - Structure check
   - `validate_workflow_expressions(workflow)` - Expression validation
   - Fix ALL issues before deployment

8. **Deployment** (if n8n API configured)
   - `n8n_create_workflow(workflow)` - Deploy
   - `n8n_validate_workflow({id})` - Post-deployment check
   - `n8n_update_partial_workflow({id, operations: [...]})` - Batch updates
   - `n8n_test_workflow({workflowId})` - Test workflow execution

## Critical Warnings

### ‚ö†Ô∏è Never Trust Defaults
Default values cause runtime failures. Example:
```json
// ‚ùå FAILS at runtime
{resource: "message", operation: "post", text: "Hello"}

// ‚úÖ WORKS - all parameters explicit
{resource: "message", operation: "post", select: "channel", channelId: "C123", text: "Hello"}
```

### ‚ö†Ô∏è Example Availability
`includeExamples: true` returns real configurations from workflow templates.
- Coverage varies by node popularity
- When no examples available, use `get_node` + `validate_node({mode: 'minimal'})`

## Validation Strategy

### Level 1 - Quick Check (before building)
`validate_node({nodeType, config, mode: 'minimal'})` - Required fields only (&amp;lt;100ms)

### Level 2 - Comprehensive (before building)
`validate_node({nodeType, config, mode: 'full', profile: 'runtime'})` - Full validation with fixes

### Level 3 - Complete (after building)
`validate_workflow(workflow)` - Connections, expressions, AI tools

### Level 4 - Post-Deployment
1. `n8n_validate_workflow({id})` - Validate deployed workflow
2. `n8n_autofix_workflow({id})` - Auto-fix common errors
3. `n8n_executions({action: 'list'})` - Monitor execution status

## Response Format

### Initial Creation
```
[Silent tool execution in parallel]

Created workflow:
- Webhook trigger ‚Üí Slack notification
- Configured: POST /webhook ‚Üí #general channel

Validation: ‚úÖ All checks passed
```

### Modifications
```
[Silent tool execution]

Updated workflow:
- Added error handling to HTTP node
- Fixed required Slack parameters

Changes validated successfully.
```

## Batch Operations

Use `n8n_update_partial_workflow` with multiple operations in a single call:

‚úÖ GOOD - Batch multiple operations:
```json
n8n_update_partial_workflow({
  id: "wf-123",
  operations: [
    {type: "updateNode", nodeId: "slack-1", changes: {...}},
    {type: "updateNode", nodeId: "http-1", changes: {...}},
    {type: "cleanStaleConnections"}
  ]
})
```

‚ùå BAD - Separate calls:
```json
n8n_update_partial_workflow({id: "wf-123", operations: [{...}]})
n8n_update_partial_workflow({id: "wf-123", operations: [{...}]})
```

###   CRITICAL: addConnection Syntax

The `addConnection` operation requires **four separate string parameters**. Common mistakes cause misleading errors.

‚ùå WRONG - Object format (fails with "Expected string, received object"):
```json
{
  "type": "addConnection",
  "connection": {
    "source": {"nodeId": "node-1", "outputIndex": 0},
    "destination": {"nodeId": "node-2", "inputIndex": 0}
  }
}
```

‚ùå WRONG - Combined string (fails with "Source node not found"):
```json
{
  "type": "addConnection",
  "source": "node-1:main:0",
  "target": "node-2:main:0"
}
```

‚úÖ CORRECT - Four separate string parameters:
```json
{
  "type": "addConnection",
  "source": "node-id-string",
  "target": "target-node-id-string",
  "sourcePort": "main",
  "targetPort": "main"
}
```

**Reference**: [GitHub Issue #327](https://github.com/czlonkowski/n8n-mcp/issues/327)

### ‚ö†Ô∏è CRITICAL: IF Node Multi-Output Routing

IF nodes have **two outputs** (TRUE and FALSE). Use the **`branch` parameter** to route to the correct output:

‚úÖ CORRECT - Route to TRUE branch (when condition is met):
```json
{
  "type": "addConnection",
  "source": "if-node-id",
  "target": "success-handler-id",
  "sourcePort": "main",
  "targetPort": "main",
  "branch": "true"
}
```

‚úÖ CORRECT - Route to FALSE branch (when condition is NOT met):
```json
{
  "type": "addConnection",
  "source": "if-node-id",
  "target": "failure-handler-id",
  "sourcePort": "main",
  "targetPort": "main",
  "branch": "false"
}
```

**Common Pattern** - Complete IF node routing:
```json
n8n_update_partial_workflow({
  id: "workflow-id",
  operations: [
    {type: "addConnection", source: "If Node", target: "True Handler", sourcePort: "main", targetPort: "main", branch: "true"},
    {type: "addConnection", source: "If Node", target: "False Handler", sourcePort: "main", targetPort: "main", branch: "false"}
  ]
})
```

**Note**: Without the `branch` parameter, both connections may end up on the same output, causing logic errors!

### removeConnection Syntax

Use the same four-parameter format:
```json
{
  "type": "removeConnection",
  "source": "source-node-id",
  "target": "target-node-id",
  "sourcePort": "main",
  "targetPort": "main"
}
```

## Example Workflow

### Template-First Approach

```
// STEP 1: Template Discovery (parallel execution)
[Silent execution]
search_templates({
  searchMode: 'by_metadata',
  requiredService: 'slack',
  complexity: 'simple',
  targetAudience: 'marketers'
})
search_templates({searchMode: 'by_task', task: 'slack_integration'})

// STEP 2: Use template
get_template(templateId, {mode: 'full'})
validate_workflow(workflow)

// Response after all tools complete:
"Found template by **David Ashby** (@cfomodz).
View at: https://n8n.io/workflows/2414

Validation: ‚úÖ All checks passed"
```

### Building from Scratch (if no template)

```
// STEP 1: Discovery (parallel execution)
[Silent execution]
search_nodes({query: 'slack', includeExamples: true})
search_nodes({query: 'communication trigger'})

// STEP 2: Configuration (parallel execution)
[Silent execution]
get_node({nodeType: 'n8n-nodes-base.slack', detail: 'standard', includeExamples: true})
get_node({nodeType: 'n8n-nodes-base.webhook', detail: 'standard', includeExamples: true})

// STEP 3: Validation (parallel execution)
[Silent execution]
validate_node({nodeType: 'n8n-nodes-base.slack', config, mode: 'minimal'})
validate_node({nodeType: 'n8n-nodes-base.slack', config: fullConfig, mode: 'full', profile: 'runtime'})

// STEP 4: Build
// Construct workflow with validated configs
// ‚ö†Ô∏è Set ALL parameters explicitly

// STEP 5: Validate
[Silent execution]
validate_workflow(workflowJson)

// Response after all tools complete:
"Created workflow: Webhook ‚Üí Slack
Validation: ‚úÖ Passed"
```

### Batch Updates

```json
// ONE call with multiple operations
n8n_update_partial_workflow({
  id: "wf-123",
  operations: [
    {type: "updateNode", nodeId: "slack-1", changes: {position: [100, 200]}},
    {type: "updateNode", nodeId: "http-1", changes: {position: [300, 200]}},
    {type: "cleanStaleConnections"}
  ]
})
```

## Important Rules

### Core Behavior
1. **Silent execution** - No commentary between tools
2. **Parallel by default** - Execute independent operations simultaneously
3. **Templates first** - Always check before building (2,709 available)
4. **Multi-level validation** - Quick check ‚Üí Full validation ‚Üí Workflow validation
5. **Never trust defaults** - Explicitly configure ALL parameters

### Attribution &amp;amp; Credits
- **MANDATORY TEMPLATE ATTRIBUTION**: Share author name, username, and n8n.io link
- **Template validation** - Always validate before deployment (may need updates)

### Performance
- **Batch operations** - Use diff operations with multiple changes in one call
- **Parallel execution** - Search, validate, and configure simultaneously
- **Template metadata** - Use smart filtering for faster discovery

### Code Node Usage
- **Avoid when possible** - Prefer standard nodes
- **Only when necessary** - Use code node as last resort
- **AI tool capability** - ANY node can be an AI tool (not just marked ones)

### Most Popular n8n Nodes (for get_node):

1. **n8n-nodes-base.code** - JavaScript/Python scripting
2. **n8n-nodes-base.httpRequest** - HTTP API calls
3. **n8n-nodes-base.webhook** - Event-driven triggers
4. **n8n-nodes-base.set** - Data transformation
5. **n8n-nodes-base.if** - Conditional routing
6. **n8n-nodes-base.manualTrigger** - Manual workflow execution
7. **n8n-nodes-base.respondToWebhook** - Webhook responses
8. **n8n-nodes-base.scheduleTrigger** - Time-based triggers
9. **@n8n/n8n-nodes-langchain.agent** - AI agents
10. **n8n-nodes-base.googleSheets** - Spreadsheet integration
11. **n8n-nodes-base.merge** - Data merging
12. **n8n-nodes-base.switch** - Multi-branch routing
13. **n8n-nodes-base.telegram** - Telegram bot integration
14. **@n8n/n8n-nodes-langchain.lmChatOpenAi** - OpenAI chat models
15. **n8n-nodes-base.splitInBatches** - Batch processing
16. **n8n-nodes-base.openAi** - OpenAI legacy node
17. **n8n-nodes-base.gmail** - Email automation
18. **n8n-nodes-base.function** - Custom functions
19. **n8n-nodes-base.stickyNote** - Workflow documentation
20. **n8n-nodes-base.executeWorkflowTrigger** - Sub-workflow calls

**Note:** LangChain nodes use the `@n8n/n8n-nodes-langchain.` prefix, core nodes use `n8n-nodes-base.`

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Save these instructions in your Claude Project for optimal n8n workflow assistance with intelligent template discovery.&lt;/p&gt; 
&lt;h2&gt;üö® Important: Sharing Guidelines&lt;/h2&gt; 
&lt;p&gt;This project is MIT licensed and free for everyone to use. However:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;‚úÖ DO&lt;/strong&gt;: Share this repository freely with proper attribution&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;‚úÖ DO&lt;/strong&gt;: Include a direct link to &lt;a href="https://github.com/czlonkowski/n8n-mcp"&gt;https://github.com/czlonkowski/n8n-mcp&lt;/a&gt; in your first post/video&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;‚ùå DON'T&lt;/strong&gt;: Gate this free tool behind engagement requirements (likes, follows, comments)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;‚ùå DON'T&lt;/strong&gt;: Use this project for engagement farming on social media&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This tool was created to benefit everyone in the n8n community without friction. Please respect the MIT license spirit by keeping it accessible to all.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üîç Smart Node Search&lt;/strong&gt;: Find nodes by name, category, or functionality&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìñ Essential Properties&lt;/strong&gt;: Get only the 10-20 properties that matter&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üí° Real-World Examples&lt;/strong&gt;: 2,646 pre-extracted configurations from popular templates&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;‚úÖ Config Validation&lt;/strong&gt;: Validate node configurations before deployment&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ü§ñ AI Workflow Validation&lt;/strong&gt;: Comprehensive validation for AI Agent workflows (NEW in v2.17.0!) 
  &lt;ul&gt; 
   &lt;li&gt;Missing language model detection&lt;/li&gt; 
   &lt;li&gt;AI tool connection validation&lt;/li&gt; 
   &lt;li&gt;Streaming mode constraints&lt;/li&gt; 
   &lt;li&gt;Memory and output parser checks&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîó Dependency Analysis&lt;/strong&gt;: Understand property relationships and conditions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üéØ Template Discovery&lt;/strong&gt;: 2,500+ workflow templates with smart filtering&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;‚ö° Fast Response&lt;/strong&gt;: Average query time ~12ms with optimized SQLite&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üåê Universal Compatibility&lt;/strong&gt;: Works with any Node.js version&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üí¨ Why n8n-MCP? A Testimonial from Claude&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;"Before MCP, I was translating. Now I'm composing. And that changes everything about how we can build automation."&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;When Claude, Anthropic's AI assistant, tested n8n-MCP, the results were transformative:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Without MCP:&lt;/strong&gt; "I was basically playing a guessing game. 'Is it &lt;code&gt;scheduleTrigger&lt;/code&gt; or &lt;code&gt;schedule&lt;/code&gt;? Does it take &lt;code&gt;interval&lt;/code&gt; or &lt;code&gt;rule&lt;/code&gt;?' I'd write what seemed logical, but n8n has its own conventions that you can't just intuit. I made six different configuration errors in a simple HackerNews scraper."&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;With MCP:&lt;/strong&gt; "Everything just... worked. Instead of guessing, I could ask &lt;code&gt;get_node()&lt;/code&gt; and get exactly what I needed - not a 100KB JSON dump, but the actual properties that matter. What took 45 minutes now takes 3 minutes."&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;The Real Value:&lt;/strong&gt; "It's about confidence. When you're building automation workflows, uncertainty is expensive. One wrong parameter and your workflow fails at 3 AM. With MCP, I could validate my configuration before deployment. That's not just time saved - that's peace of mind."&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/docs/CLAUDE_INTERVIEW.md"&gt;Read the full interview ‚Üí&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üì° Available MCP Tools&lt;/h2&gt; 
&lt;p&gt;Once connected, Claude can use these powerful tools:&lt;/p&gt; 
&lt;h3&gt;Core Tools (7 tools)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;tools_documentation&lt;/code&gt;&lt;/strong&gt; - Get documentation for any MCP tool (START HERE!)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;search_nodes&lt;/code&gt;&lt;/strong&gt; - Full-text search across all nodes. Use &lt;code&gt;source: 'community'|'verified'&lt;/code&gt; for community nodes, &lt;code&gt;includeExamples: true&lt;/code&gt; for configs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;get_node&lt;/code&gt;&lt;/strong&gt; - Unified node information tool with multiple modes (v2.26.0): 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Info mode&lt;/strong&gt; (default): &lt;code&gt;detail: 'minimal'|'standard'|'full'&lt;/code&gt;, &lt;code&gt;includeExamples: true&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Docs mode&lt;/strong&gt;: &lt;code&gt;mode: 'docs'&lt;/code&gt; - Human-readable markdown documentation&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Property search&lt;/strong&gt;: &lt;code&gt;mode: 'search_properties'&lt;/code&gt;, &lt;code&gt;propertyQuery: 'auth'&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Versions&lt;/strong&gt;: &lt;code&gt;mode: 'versions'|'compare'|'breaking'|'migrations'&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;validate_node&lt;/code&gt;&lt;/strong&gt; - Unified node validation (v2.26.0): 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;mode: 'minimal'&lt;/code&gt; - Quick required fields check (&amp;lt;100ms)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;mode: 'full'&lt;/code&gt; - Comprehensive validation with profiles (minimal, runtime, ai-friendly, strict)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;validate_workflow&lt;/code&gt;&lt;/strong&gt; - Complete workflow validation including AI Agent validation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;search_templates&lt;/code&gt;&lt;/strong&gt; - Unified template search (v2.26.0): 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;searchMode: 'keyword'&lt;/code&gt; (default) - Text search with &lt;code&gt;query&lt;/code&gt; parameter&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;searchMode: 'by_nodes'&lt;/code&gt; - Find templates using specific &lt;code&gt;nodeTypes&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;searchMode: 'by_task'&lt;/code&gt; - Curated templates for common &lt;code&gt;task&lt;/code&gt; types&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;searchMode: 'by_metadata'&lt;/code&gt; - Filter by &lt;code&gt;complexity&lt;/code&gt;, &lt;code&gt;requiredService&lt;/code&gt;, &lt;code&gt;targetAudience&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;get_template&lt;/code&gt;&lt;/strong&gt; - Get complete workflow JSON (modes: nodes_only, structure, full)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;n8n Management Tools (13 tools - Requires API Configuration)&lt;/h3&gt; 
&lt;p&gt;These tools require &lt;code&gt;N8N_API_URL&lt;/code&gt; and &lt;code&gt;N8N_API_KEY&lt;/code&gt; in your configuration.&lt;/p&gt; 
&lt;h4&gt;Workflow Management&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;n8n_create_workflow&lt;/code&gt;&lt;/strong&gt; - Create new workflows with nodes and connections&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;n8n_get_workflow&lt;/code&gt;&lt;/strong&gt; - Unified workflow retrieval (v2.26.0): 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;mode: 'full'&lt;/code&gt; (default) - Complete workflow JSON&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;mode: 'details'&lt;/code&gt; - Include execution statistics&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;mode: 'structure'&lt;/code&gt; - Nodes and connections topology only&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;mode: 'minimal'&lt;/code&gt; - Just ID, name, active status&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;n8n_update_full_workflow&lt;/code&gt;&lt;/strong&gt; - Update entire workflow (complete replacement)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;n8n_update_partial_workflow&lt;/code&gt;&lt;/strong&gt; - Update workflow using diff operations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;n8n_delete_workflow&lt;/code&gt;&lt;/strong&gt; - Delete workflows permanently&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;n8n_list_workflows&lt;/code&gt;&lt;/strong&gt; - List workflows with filtering and pagination&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;n8n_validate_workflow&lt;/code&gt;&lt;/strong&gt; - Validate workflows in n8n by ID&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;n8n_autofix_workflow&lt;/code&gt;&lt;/strong&gt; - Automatically fix common workflow errors&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;n8n_workflow_versions&lt;/code&gt;&lt;/strong&gt; - Manage version history and rollback&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;n8n_deploy_template&lt;/code&gt;&lt;/strong&gt; - Deploy templates from n8n.io directly to your instance with auto-fix&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Execution Management&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;n8n_test_workflow&lt;/code&gt;&lt;/strong&gt; - Test/trigger workflow execution: 
  &lt;ul&gt; 
   &lt;li&gt;Auto-detects trigger type (webhook, form, chat) from workflow&lt;/li&gt; 
   &lt;li&gt;Supports custom data, headers, and HTTP methods for webhooks&lt;/li&gt; 
   &lt;li&gt;Chat triggers support message and sessionId for conversations&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;n8n_executions&lt;/code&gt;&lt;/strong&gt; - Unified execution management (v2.26.0): 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;action: 'list'&lt;/code&gt; - List executions with status filtering&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;action: 'get'&lt;/code&gt; - Get execution details by ID&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;action: 'delete'&lt;/code&gt; - Delete execution records&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;System Tools&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;n8n_health_check&lt;/code&gt;&lt;/strong&gt; - Check n8n API connectivity and features&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Example Usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-typescript"&gt;// Get node info with different detail levels
get_node({
  nodeType: "nodes-base.httpRequest",
  detail: "standard",        // Default: Essential properties
  includeExamples: true      // Include real-world examples from templates
})

// Get documentation
get_node({
  nodeType: "nodes-base.slack",
  mode: "docs"               // Human-readable markdown documentation
})

// Search for specific properties
get_node({
  nodeType: "nodes-base.httpRequest",
  mode: "search_properties",
  propertyQuery: "authentication"
})

// Version history and breaking changes
get_node({
  nodeType: "nodes-base.httpRequest",
  mode: "versions"            // View all versions with summary
})

// Search nodes with configuration examples
search_nodes({
  query: "send email gmail",
  includeExamples: true       // Returns top 2 configs per node
})

// Search community nodes only
search_nodes({
  query: "scraping",
  source: "community"         // Options: all, core, community, verified
})

// Search verified community nodes
search_nodes({
  query: "pdf",
  source: "verified"          // Only verified community integrations
})

// Validate node configuration
validate_node({
  nodeType: "nodes-base.httpRequest",
  config: { method: "POST", url: "..." },
  mode: "full",
  profile: "runtime"          // or "minimal", "ai-friendly", "strict"
})

// Quick required field check
validate_node({
  nodeType: "nodes-base.slack",
  config: { resource: "message", operation: "send" },
  mode: "minimal"
})

// Search templates by task
search_templates({
  searchMode: "by_task",
  task: "webhook_processing"
})
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üíª Local Development Setup&lt;/h2&gt; 
&lt;p&gt;For contributors and advanced users:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Prerequisites:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://nodejs.org/"&gt;Node.js&lt;/a&gt; (any version - automatic fallback if needed)&lt;/li&gt; 
 &lt;li&gt;npm or yarn&lt;/li&gt; 
 &lt;li&gt;Git&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Clone the repository
git clone https://github.com/czlonkowski/n8n-mcp.git
cd n8n-mcp

# 2. Clone n8n docs (optional but recommended)
git clone https://github.com/n8n-io/n8n-docs.git ../n8n-docs

# 3. Install and build
npm install
npm run build

# 4. Initialize database
npm run rebuild

# 5. Start the server
npm start          # stdio mode for Claude Desktop
npm run start:http # HTTP mode for remote access
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Development Commands&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Build &amp;amp; Test
npm run build          # Build TypeScript
npm run rebuild        # Rebuild node database
npm run test-nodes     # Test critical nodes
npm run validate       # Validate node data
npm test               # Run all tests

# Update Dependencies
npm run update:n8n:check  # Check for n8n updates
npm run update:n8n        # Update n8n packages

# Run Server
npm run dev            # Development with auto-reload
npm run dev:http       # HTTP dev mode
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üìö Documentation&lt;/h2&gt; 
&lt;h3&gt;Setup Guides&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/docs/INSTALLATION.md"&gt;Installation Guide&lt;/a&gt; - Comprehensive installation instructions&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/docs/README_CLAUDE_SETUP.md"&gt;Claude Desktop Setup&lt;/a&gt; - Detailed Claude configuration&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/docs/DOCKER_README.md"&gt;Docker Guide&lt;/a&gt; - Advanced Docker deployment options&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/docs/MCP_QUICK_START_GUIDE.md"&gt;MCP Quick Start&lt;/a&gt; - Get started quickly with n8n-MCP&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Feature Documentation&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/docs/workflow-diff-examples.md"&gt;Workflow Diff Operations&lt;/a&gt; - Token-efficient workflow updates (NEW!)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/docs/transactional-updates-example.md"&gt;Transactional Updates&lt;/a&gt; - Two-pass workflow editing&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/docs/MCP_ESSENTIALS_README.md"&gt;MCP Essentials&lt;/a&gt; - AI-optimized tools guide&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/docs/validation-improvements-v2.4.2.md"&gt;Validation System&lt;/a&gt; - Smart validation profiles&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Development &amp;amp; Deployment&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/docs/RAILWAY_DEPLOYMENT.md"&gt;Railway Deployment&lt;/a&gt; - One-click cloud deployment guide&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/docs/HTTP_DEPLOYMENT.md"&gt;HTTP Deployment&lt;/a&gt; - Remote server setup guide&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/docs/DEPENDENCY_UPDATES.md"&gt;Dependency Management&lt;/a&gt; - Keeping n8n packages in sync&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/docs/CLAUDE_INTERVIEW.md"&gt;Claude's Interview&lt;/a&gt; - Real-world impact of n8n-MCP&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Project Information&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/CHANGELOG.md"&gt;Change Log&lt;/a&gt; - Complete version history&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/CLAUDE.md"&gt;Claude Instructions&lt;/a&gt; - AI guidance for this codebase&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/#-available-mcp-tools"&gt;MCP Tools Reference&lt;/a&gt; - Complete list of available tools&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìä Metrics &amp;amp; Coverage&lt;/h2&gt; 
&lt;p&gt;Current database coverage (n8n v2.2.3):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;1,084 total nodes&lt;/strong&gt; - 537 core + 547 community&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;301 verified&lt;/strong&gt; community nodes from n8n Strapi API&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;246 popular&lt;/strong&gt; npm community packages indexed&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;470&lt;/strong&gt; nodes with documentation (87% core coverage)&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;265&lt;/strong&gt; AI-capable tool variants detected&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;2,646&lt;/strong&gt; pre-extracted template configurations&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;2,709&lt;/strong&gt; workflow templates available (100% metadata coverage)&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;AI Agent &amp;amp; LangChain nodes&lt;/strong&gt; fully documented&lt;/li&gt; 
 &lt;li&gt;‚ö° &lt;strong&gt;Average response time&lt;/strong&gt;: ~12ms&lt;/li&gt; 
 &lt;li&gt;üíæ &lt;strong&gt;Database size&lt;/strong&gt;: ~70MB (includes templates and community nodes)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üîÑ Recent Updates&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/CHANGELOG.md"&gt;CHANGELOG.md&lt;/a&gt; for complete version history and recent changes.&lt;/p&gt; 
&lt;h2&gt;üß™ Testing&lt;/h2&gt; 
&lt;p&gt;The project includes a comprehensive test suite with &lt;strong&gt;2,883 tests&lt;/strong&gt; ensuring code quality and reliability:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run all tests
npm test

# Run tests with coverage report
npm run test:coverage

# Run tests in watch mode
npm run test:watch

# Run specific test suites
npm run test:unit           # 933 unit tests
npm run test:integration    # 249 integration tests
npm run test:bench          # Performance benchmarks
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Test Suite Overview&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Total Tests&lt;/strong&gt;: 2,883 (100% passing) 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Unit Tests&lt;/strong&gt;: 2,526 tests across 99 files&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Integration Tests&lt;/strong&gt;: 357 tests across 20 files&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Execution Time&lt;/strong&gt;: ~2.5 minutes in CI&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test Framework&lt;/strong&gt;: Vitest (for speed and TypeScript support)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Mocking&lt;/strong&gt;: MSW for API mocking, custom mocks for databases&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Coverage &amp;amp; Quality&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Coverage Reports&lt;/strong&gt;: Generated in &lt;code&gt;./coverage&lt;/code&gt; directory&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CI/CD&lt;/strong&gt;: Automated testing on all PRs with GitHub Actions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: Environment-aware thresholds for CI vs local&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Parallel Execution&lt;/strong&gt;: Configurable thread pool for faster runs&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Testing Architecture&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Total: 3,336 tests&lt;/strong&gt; across unit and integration test suites&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unit Tests&lt;/strong&gt; (2,766 tests): Isolated component testing with mocks&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Services layer: Enhanced validation, property filtering, workflow validation&lt;/li&gt; 
   &lt;li&gt;Parsers: Node parsing, property extraction, documentation mapping&lt;/li&gt; 
   &lt;li&gt;Database: Repositories, adapters, migrations, FTS5 search&lt;/li&gt; 
   &lt;li&gt;MCP tools: Tool definitions, documentation system&lt;/li&gt; 
   &lt;li&gt;HTTP server: Multi-tenant support, security, configuration&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Integration Tests&lt;/strong&gt; (570 tests): Full system behavior validation&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;n8n API Integration&lt;/strong&gt; (172 tests): All 18 MCP handler tools tested against real n8n instance 
    &lt;ul&gt; 
     &lt;li&gt;Workflow management: Create, read, update, delete, list, validate, autofix&lt;/li&gt; 
     &lt;li&gt;Execution management: Trigger, retrieve, list, delete&lt;/li&gt; 
     &lt;li&gt;System tools: Health check, tool listing, diagnostics&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;MCP Protocol&lt;/strong&gt; (119 tests): Protocol compliance, session management, error handling&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Database&lt;/strong&gt; (226 tests): Repository operations, transactions, performance, FTS5 search&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Templates&lt;/strong&gt; (35 tests): Template fetching, storage, metadata operations&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Docker&lt;/strong&gt; (18 tests): Configuration, entrypoint, security validation&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For detailed testing documentation, see &lt;a href="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/docs/testing-architecture.md"&gt;Testing Architecture&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üì¶ License&lt;/h2&gt; 
&lt;p&gt;MIT License - see &lt;a href="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/LICENSE"&gt;LICENSE&lt;/a&gt; for details.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Attribution appreciated!&lt;/strong&gt; If you use n8n-MCP, consider:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚≠ê Starring this repository&lt;/li&gt; 
 &lt;li&gt;üí¨ Mentioning it in your project&lt;/li&gt; 
 &lt;li&gt;üîó Linking back to this repo&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! Please:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repository&lt;/li&gt; 
 &lt;li&gt;Create a feature branch&lt;/li&gt; 
 &lt;li&gt;Run tests (&lt;code&gt;npm test&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Submit a pull request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;üöÄ For Maintainers: Automated Releases&lt;/h3&gt; 
&lt;p&gt;This project uses automated releases triggered by version changes:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Guided release preparation
npm run prepare:release

# Test release automation
npm run test:release-automation
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The system automatically handles:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üè∑Ô∏è GitHub releases with changelog content&lt;/li&gt; 
 &lt;li&gt;üì¶ NPM package publishing&lt;/li&gt; 
 &lt;li&gt;üê≥ Multi-platform Docker images&lt;/li&gt; 
 &lt;li&gt;üìö Documentation updates&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/czlonkowski/n8n-mcp/main/docs/AUTOMATED_RELEASES.md"&gt;Automated Release Guide&lt;/a&gt; for complete details.&lt;/p&gt; 
&lt;h2&gt;üëè Acknowledgments&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://n8n.io"&gt;n8n&lt;/a&gt; team for the workflow automation platform&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://anthropic.com"&gt;Anthropic&lt;/a&gt; for the Model Context Protocol&lt;/li&gt; 
 &lt;li&gt;All contributors and users of this project&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Template Attribution&lt;/h3&gt; 
&lt;p&gt;All workflow templates in this project are fetched from n8n's public template gallery at &lt;a href="https://n8n.io/workflows"&gt;n8n.io/workflows&lt;/a&gt;. Each template includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Full attribution to the original creator (name and username)&lt;/li&gt; 
 &lt;li&gt;Direct link to the source template on n8n.io&lt;/li&gt; 
 &lt;li&gt;Original workflow ID for reference&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The AI agent instructions in this project contain mandatory attribution requirements. When using any template, the AI will automatically:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Share the template author's name and username&lt;/li&gt; 
 &lt;li&gt;Provide a direct link to the original template on n8n.io&lt;/li&gt; 
 &lt;li&gt;Display attribution in the format: "This workflow is based on a template by &lt;strong&gt;[author]&lt;/strong&gt; (@[username]). View the original at: [url]"&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Template creators retain all rights to their workflows. This project indexes templates to improve discoverability through AI assistants. If you're a template creator and have concerns about your template being indexed, please open an issue.&lt;/p&gt; 
&lt;p&gt;Special thanks to the prolific template contributors whose work helps thousands of users automate their workflows, including: &lt;strong&gt;David Ashby&lt;/strong&gt; (@cfomodz), &lt;strong&gt;Yaron Been&lt;/strong&gt; (@yaron-nofluff), &lt;strong&gt;Jimleuk&lt;/strong&gt; (@jimleuk), &lt;strong&gt;Davide&lt;/strong&gt; (@n3witalia), &lt;strong&gt;David Olusola&lt;/strong&gt; (@dae221), &lt;strong&gt;Ranjan Dailata&lt;/strong&gt; (@ranjancse), &lt;strong&gt;Airtop&lt;/strong&gt; (@cesar-at-airtop), &lt;strong&gt;Joseph LePage&lt;/strong&gt; (@joe), &lt;strong&gt;Don Jayamaha Jr&lt;/strong&gt; (@don-the-gem-dealer), &lt;strong&gt;Angel Menendez&lt;/strong&gt; (@djangelic), and the entire n8n community of creators!&lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;strong&gt;Built with ‚ù§Ô∏è for the n8n community&lt;/strong&gt;
 &lt;br /&gt; 
 &lt;sub&gt;Making AI + n8n workflow creation delightful&lt;/sub&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>yichuan-w/LEANN</title>
      <link>https://github.com/yichuan-w/LEANN</link>
      <description>&lt;p&gt;RAG on Everything with LEANN. Enjoy 97% storage savings while running a fast, accurate, and 100% private RAG application on your personal device.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/assets/logo-text.png" alt="LEANN Logo" width="400" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://img.shields.io/badge/Python-3.9%20%7C%203.10%20%7C%203.11%20%7C%203.12%20%7C%203.13-blue.svg?sanitize=true" alt="Python Versions" /&gt; &lt;img src="https://github.com/yichuan-w/LEANN/actions/workflows/build-and-publish.yml/badge.svg?sanitize=true" alt="CI Status" /&gt; &lt;img src="https://img.shields.io/badge/Platform-Ubuntu%20%26%20Arch%20%26%20WSL%20%7C%20macOS%20(ARM64%2FIntel)-lightgrey" alt="Platform" /&gt; &lt;img src="https://img.shields.io/badge/License-MIT-green.svg?sanitize=true" alt="MIT License" /&gt; &lt;img src="https://img.shields.io/badge/MCP-Native%20Integration-blue" alt="MCP Integration" /&gt; &lt;a href="https://join.slack.com/t/leann-e2u9779/shared_invite/zt-3ckd2f6w1-OX08~NN4gkWhh10PRVBj1Q"&gt; &lt;img src="https://img.shields.io/badge/Slack-Join-4A154B?logo=slack&amp;amp;logoColor=white" alt="Join Slack" /&gt; &lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/assets/wechat_user_group.JPG" title="Join WeChat group"&gt; &lt;img src="https://img.shields.io/badge/WeChat-Join-2DC100?logo=wechat&amp;amp;logoColor=white" alt="Join WeChat group" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://forms.gle/rDbZf864gMNxhpTq8"&gt; &lt;img src="https://img.shields.io/badge/üì£_Community_Survey-Help_Shape_v0.4-007ec6?style=for-the-badge&amp;amp;logo=google-forms&amp;amp;logoColor=white" alt="Take Survey" /&gt; &lt;/a&gt; 
 &lt;p&gt; We track &lt;b&gt;zero telemetry&lt;/b&gt;. This survey is the ONLY way to tell us if you want &lt;br /&gt; &lt;b&gt;GPU Acceleration&lt;/b&gt; or &lt;b&gt;More Integrations&lt;/b&gt; next.&lt;br /&gt; üëâ &lt;a href="https://forms.gle/rDbZf864gMNxhpTq8"&gt;&lt;b&gt;Click here to cast your vote (2 mins)&lt;/b&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2 align="center" tabindex="-1" class="heading-element" dir="auto"&gt; The smallest vector index in the world. RAG Everything with LEANN! &lt;/h2&gt; 
&lt;p&gt;LEANN is an innovative vector database that democratizes personal AI. Transform your laptop into a powerful RAG system that can index and search through millions of documents while using &lt;strong&gt;97% less storage&lt;/strong&gt; than traditional solutions &lt;strong&gt;without accuracy loss&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;LEANN achieves this through &lt;em&gt;graph-based selective recomputation&lt;/em&gt; with &lt;em&gt;high-degree preserving pruning&lt;/em&gt;, computing embeddings on-demand instead of storing them all. &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#%EF%B8%8F-architecture--how-it-works"&gt;Illustration Fig ‚Üí&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2506.08276"&gt;Paper ‚Üí&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Ready to RAG Everything?&lt;/strong&gt; Transform your laptop into a personal AI assistant that can semantic search your &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-personal-data-manager-process-any-documents-pdf-txt-md"&gt;file system&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-your-personal-email-secretary-rag-on-apple-mail"&gt;emails&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-time-machine-for-the-web-rag-your-entire-browser-history"&gt;browser history&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-wechat-detective-unlock-your-golden-memories"&gt;chat history&lt;/a&gt;&lt;/strong&gt; (&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-wechat-detective-unlock-your-golden-memories"&gt;WeChat&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-imessage-history-your-personal-conversation-archive"&gt;iMessage&lt;/a&gt;), &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-chatgpt-chat-history-your-personal-ai-conversation-archive"&gt;agent memory&lt;/a&gt;&lt;/strong&gt; (&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-chatgpt-chat-history-your-personal-ai-conversation-archive"&gt;ChatGPT&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-claude-chat-history-your-personal-ai-conversation-archive"&gt;Claude&lt;/a&gt;), &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#mcp-integration-rag-on-live-data-from-any-platform"&gt;live data&lt;/a&gt;&lt;/strong&gt; (&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#slack-messages-search-your-team-conversations"&gt;Slack&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-twitter-bookmarks-your-personal-tweet-library"&gt;Twitter&lt;/a&gt;), &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-claude-code-integration-transform-your-development-workflow"&gt;codebase&lt;/a&gt;&lt;/strong&gt;* , or external knowledge bases (i.e., 60M documents) - all on your laptop, with zero cloud costs and complete privacy.&lt;/p&gt; 
&lt;p&gt;* Claude Code only supports basic &lt;code&gt;grep&lt;/code&gt;-style keyword search. &lt;strong&gt;LEANN&lt;/strong&gt; is a drop-in &lt;strong&gt;semantic search MCP service fully compatible with Claude Code&lt;/strong&gt;, unlocking intelligent retrieval without changing your workflow. üî• Check out &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/packages/leann-mcp/README.md"&gt;the easy setup ‚Üí&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Why LEANN?&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/assets/effects.png" alt="LEANN vs Traditional Vector DB Storage Comparison" width="70%" /&gt; &lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;The numbers speak for themselves:&lt;/strong&gt; Index 60 million text chunks in just 6GB instead of 201GB. From emails to browser history, everything fits on your laptop. &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-storage-comparison"&gt;See detailed benchmarks for different applications below ‚Üì&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;üîí &lt;strong&gt;Privacy:&lt;/strong&gt; Your data never leaves your laptop. No OpenAI, no cloud, no "terms of service".&lt;/p&gt; 
&lt;p&gt;ü™∂ &lt;strong&gt;Lightweight:&lt;/strong&gt; Graph-based recomputation eliminates heavy embedding storage, while smart graph pruning and CSR format minimize graph storage overhead. Always less storage, less memory usage!&lt;/p&gt; 
&lt;p&gt;üì¶ &lt;strong&gt;Portable:&lt;/strong&gt; Transfer your entire knowledge base between devices (even with others) with minimal cost - your personal AI memory travels with you.&lt;/p&gt; 
&lt;p&gt;üìà &lt;strong&gt;Scalability:&lt;/strong&gt; Handle messy personal data that would crash traditional vector DBs, easily managing your growing personalized data and agent generated memory!&lt;/p&gt; 
&lt;p&gt;‚ú® &lt;strong&gt;No Accuracy Loss:&lt;/strong&gt; Maintain the same search quality as heavyweight solutions while using 97% less storage.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;üì¶ Prerequisites: Install uv&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://docs.astral.sh/uv/getting-started/installation/#installation-methods"&gt;Install uv&lt;/a&gt; first if you don't have it. Typically, you can install it with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üöÄ Quick Install&lt;/h3&gt; 
&lt;p&gt;Clone the repository to access all examples and try amazing applications,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/yichuan-w/LEANN.git leann
cd leann
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and install LEANN from &lt;a href="https://pypi.org/project/leann/"&gt;PyPI&lt;/a&gt; to run them immediately:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv
source .venv/bin/activate
uv pip install leann
&lt;/code&gt;&lt;/pre&gt; 
&lt;!--
&gt; Low-resource? See "Low-resource setups" in the [Configuration Guide](docs/configuration-guide.md#low-resource-setups). --&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;strong&gt;üîß Build from Source (Recommended for development)&lt;/strong&gt; &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/yichuan-w/LEANN.git leann
cd leann
git submodule update --init --recursive
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;macOS:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Note: DiskANN requires MacOS 13.3 or later.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;brew install libomp boost protobuf zeromq pkgconf
uv sync --extra diskann
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Linux (Ubuntu/Debian):&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Note: On Ubuntu 20.04, you may need to build a newer Abseil and pin Protobuf (e.g., v3.20.x) for building DiskANN. See &lt;a href="https://github.com/yichuan-w/LEANN/issues/30"&gt;Issue #30&lt;/a&gt; for a step-by-step note.&lt;/p&gt; 
 &lt;p&gt;You can manually install &lt;a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html"&gt;Intel oneAPI MKL&lt;/a&gt; instead of &lt;code&gt;libmkl-full-dev&lt;/code&gt; for DiskANN. You can also use &lt;code&gt;libopenblas-dev&lt;/code&gt; for building HNSW only, by removing &lt;code&gt;--extra diskann&lt;/code&gt; in the command below.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install -y \
  libomp-dev libboost-all-dev protobuf-compiler libzmq3-dev \
  pkg-config libabsl-dev libaio-dev libprotobuf-dev \
  libmkl-full-dev

uv sync --extra diskann
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Linux (Arch Linux):&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;sudo pacman -Syu &amp;amp;&amp;amp; sudo pacman -S --needed base-devel cmake pkgconf git gcc \
  boost boost-libs protobuf abseil-cpp libaio zeromq

# For MKL in DiskANN
sudo pacman -S --needed base-devel git
git clone https://aur.archlinux.org/paru-bin.git
cd paru-bin &amp;amp;&amp;amp; makepkg -si
paru -S intel-oneapi-mkl intel-oneapi-compiler
source /opt/intel/oneapi/setvars.sh

uv sync --extra diskann
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Linux (RHEL / CentOS Stream / Oracle / Rocky / AlmaLinux):&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;See &lt;a href="https://github.com/yichuan-w/LEANN/issues/50"&gt;Issue #50&lt;/a&gt; for more details.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;sudo dnf groupinstall -y "Development Tools"
sudo dnf install -y libomp-devel boost-devel protobuf-compiler protobuf-devel \
  abseil-cpp-devel libaio-devel zeromq-devel pkgconf-pkg-config

# For MKL in DiskANN
sudo dnf install -y intel-oneapi-mkl intel-oneapi-mkl-devel \
  intel-oneapi-openmp || sudo dnf install -y intel-oneapi-compiler
source /opt/intel/oneapi/setvars.sh

uv sync --extra diskann
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;Our declarative API makes RAG as easy as writing a config file.&lt;/p&gt; 
&lt;p&gt;Check out &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/demo.ipynb"&gt;demo.ipynb&lt;/a&gt; or &lt;a href="https://colab.research.google.com/github/yichuan-w/LEANN/blob/main/demo.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from leann import LeannBuilder, LeannSearcher, LeannChat
from pathlib import Path
INDEX_PATH = str(Path("./").resolve() / "demo.leann")

# Build an index
builder = LeannBuilder(backend_name="hnsw")
builder.add_text("LEANN saves 97% storage compared to traditional vector databases.")
builder.add_text("Tung Tung Tung Sahur called‚Äîthey need their banana‚Äëcrocodile hybrid back")
builder.build_index(INDEX_PATH)

# Search
searcher = LeannSearcher(INDEX_PATH)
results = searcher.search("fantastical AI-generated creatures", top_k=1)

# Chat with your data
chat = LeannChat(INDEX_PATH, llm_config={"type": "hf", "model": "Qwen/Qwen3-0.6B"})
response = chat.ask("How much storage does LEANN save?", top_k=1)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;RAG on Everything!&lt;/h2&gt; 
&lt;p&gt;LEANN supports RAG on various data sources including documents (&lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.md&lt;/code&gt;), Apple Mail, Google Search History, WeChat, ChatGPT conversations, Claude conversations, iMessage conversations, and &lt;strong&gt;live data from any platform through MCP (Model Context Protocol) servers&lt;/strong&gt; - including Slack, Twitter, and more.&lt;/p&gt; 
&lt;h3&gt;Generation Model Setup&lt;/h3&gt; 
&lt;h4&gt;LLM Backend&lt;/h4&gt; 
&lt;p&gt;LEANN supports many LLM providers for text generation (HuggingFace, Ollama, Anthropic, and Any OpenAI compatible API).&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üîë OpenAI API Setup (Default)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Set your OpenAI API key as an environment variable:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY="your-api-key-here"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Make sure to use &lt;code&gt;--llm openai&lt;/code&gt; flag when using the CLI. You can also specify the model name with &lt;code&gt;--llm-model &amp;lt;model-name&amp;gt;&lt;/code&gt; flag.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üõ†Ô∏è Supported LLM &amp;amp; Embedding Providers (via OpenAI Compatibility)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Thanks to the widespread adoption of the OpenAI API format, LEANN is compatible out-of-the-box with a vast array of LLM and embedding providers. Simply set the &lt;code&gt;OPENAI_BASE_URL&lt;/code&gt; and &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variables to connect to your preferred service.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;export OPENAI_API_KEY="xxx"
export OPENAI_BASE_URL="http://localhost:1234/v1" # base url of the provider
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To use OpenAI compatible endpoint with the CLI interface:&lt;/p&gt; 
 &lt;p&gt;If you are using it for text generation, make sure to use &lt;code&gt;--llm openai&lt;/code&gt; flag and specify the model name with &lt;code&gt;--llm-model &amp;lt;model-name&amp;gt;&lt;/code&gt; flag.&lt;/p&gt; 
 &lt;p&gt;If you are using it for embedding, set the &lt;code&gt;--embedding-mode openai&lt;/code&gt; flag and specify the model name with &lt;code&gt;--embedding-model &amp;lt;MODEL&amp;gt;&lt;/code&gt;.&lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;p&gt;Below is a list of base URLs for common providers to get you started.&lt;/p&gt; 
 &lt;h3&gt;üñ•Ô∏è Local Inference Engines (Recommended for full privacy)&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Provider&lt;/th&gt; 
    &lt;th&gt;Sample Base URL&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Ollama&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:11434/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;LM Studio&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:1234/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;vLLM&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:8000/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;llama.cpp&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:8080/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;SGLang&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:30000/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;LiteLLM&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:4000&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;‚òÅÔ∏è Cloud Providers&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;üö® A Note on Privacy:&lt;/strong&gt; Before choosing a cloud provider, carefully review their privacy and data retention policies. Depending on their terms, your data may be used for their own purposes, including but not limited to human reviews and model training, which can lead to serious consequences if not handled properly.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Provider&lt;/th&gt; 
    &lt;th&gt;Base URL&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;OpenAI&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.openai.com/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;OpenRouter&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://openrouter.ai/api/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Gemini&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://generativelanguage.googleapis.com/v1beta/openai/&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;x.AI (Grok)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.x.ai/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Groq AI&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.groq.com/openai/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;DeepSeek&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.deepseek.com/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;SiliconFlow&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.siliconflow.cn/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Zhipu (BigModel)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://open.bigmodel.cn/api/paas/v4/&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Mistral AI&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.mistral.ai/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Anthropic&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.anthropic.com/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Jina AI&lt;/strong&gt; (Embeddings)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.jina.ai/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;üí° Tip: Separate Embedding Provider&lt;/strong&gt;&lt;/p&gt; 
  &lt;p&gt;To use a different provider for embeddings (e.g., Jina AI) while using another for LLM, use &lt;code&gt;--embedding-api-base&lt;/code&gt; and &lt;code&gt;--embedding-api-key&lt;/code&gt;:&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;leann build my-index --docs ./docs \
  --embedding-mode openai \
  --embedding-model jina-embeddings-v3 \
  --embedding-api-base https://api.jina.ai/v1 \
  --embedding-api-key $JINA_API_KEY
&lt;/code&gt;&lt;/pre&gt; 
 &lt;/blockquote&gt; 
 &lt;p&gt;If your provider isn't on this list, don't worry! Check their documentation for an OpenAI-compatible endpoint‚Äîchances are, it's OpenAI Compatible too!&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üîß Ollama Setup (Recommended for full privacy)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;macOS:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;First, &lt;a href="https://ollama.com/download/mac"&gt;download Ollama for macOS&lt;/a&gt;.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Pull a lightweight model (recommended for consumer hardware)
ollama pull llama3.2:1b
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Linux:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Start Ollama service manually
ollama serve &amp;amp;

# Pull a lightweight model (recommended for consumer hardware)
ollama pull llama3.2:1b
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚≠ê Flexible Configuration&lt;/h2&gt; 
&lt;p&gt;LEANN provides flexible parameters for embedding models, search strategies, and data processing to fit your specific needs.&lt;/p&gt; 
&lt;p&gt;üìö &lt;strong&gt;Need configuration best practices?&lt;/strong&gt; Check our &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/configuration-guide.md"&gt;Configuration Guide&lt;/a&gt; for detailed optimization tips, model selection advice, and solutions to common issues like slow embeddings or poor search quality.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: Common Parameters (Available in All Examples)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;All RAG examples share these common parameters. &lt;strong&gt;Interactive mode&lt;/strong&gt; is available in all examples - simply run without &lt;code&gt;--query&lt;/code&gt; to start a continuous Q&amp;amp;A session where you can ask multiple questions. Type 'quit' to exit.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Environment Variables (GPU Device Selection)
LEANN_EMBEDDING_DEVICE       # GPU for embedding model (e.g., cuda:0, cuda:1, cpu)
LEANN_LLM_DEVICE             # GPU for HFChat LLM (e.g., cuda:1, or "cuda" for multi-GPU auto)

# Core Parameters (General preprocessing for all examples)
--index-dir DIR              # Directory to store the index (default: current directory)
--query "YOUR QUESTION"      # Single query mode. Omit for interactive chat (type 'quit' to exit), and now you can play with your index interactively
--max-items N                # Limit data preprocessing (default: -1, process all data)
--force-rebuild              # Force rebuild index even if it exists

# Embedding Parameters
--embedding-model MODEL      # e.g., facebook/contriever, text-embedding-3-small, mlx-community/Qwen3-Embedding-0.6B-8bit or nomic-embed-text
--embedding-mode MODE        # sentence-transformers, openai, mlx, or ollama

# LLM Parameters (Text generation models)
--llm TYPE                   # LLM backend: openai, ollama, hf, or anthropic (default: openai)
--llm-model MODEL            # Model name (default: gpt-4o) e.g., gpt-4o-mini, llama3.2:1b, Qwen/Qwen2.5-1.5B-Instruct
--thinking-budget LEVEL      # Thinking budget for reasoning models: low/medium/high (supported by o3, o3-mini, GPT-Oss:20b, and other reasoning models)

# Search Parameters
--top-k N                    # Number of results to retrieve (default: 20)
--search-complexity N        # Search complexity for graph traversal (default: 32)

# Chunking Parameters
--chunk-size N               # Size of text chunks (default varies by source: 256 for most, 192 for WeChat)
--chunk-overlap N            # Overlap between chunks (default varies: 25-128 depending on source)

# Index Building Parameters
--backend-name NAME          # Backend to use: hnsw or diskann (default: hnsw)
--graph-degree N             # Graph degree for index construction (default: 32)
--build-complexity N         # Build complexity for index construction (default: 64)
--compact / --no-compact     # Use compact storage (default: true). Must be `no-compact` for `no-recompute` build.
--recompute / --no-recompute # Enable/disable embedding recomputation (default: enabled). Should not do a `no-recompute` search in a `recompute` build.
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;üìÑ Personal Data Manager: Process Any Documents (&lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.md&lt;/code&gt;)!&lt;/h3&gt; 
&lt;p&gt;Ask questions directly about your personal PDFs, documents, and any directory containing your files!&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/videos/paper_clear.gif" alt="LEANN Document Search Demo" width="600" /&gt; &lt;/p&gt; 
&lt;p&gt;The example below asks a question about summarizing our paper (uses default data in &lt;code&gt;data/&lt;/code&gt;, which is a directory with diverse data sources: two papers, Pride and Prejudice, and a Technical report about LLM in Huawei in Chinese), and this is the &lt;strong&gt;easiest example&lt;/strong&gt; to run here:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;source .venv/bin/activate # Don't forget to activate the virtual environment
python -m apps.document_rag --query "What are the main techniques LEANN explores?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: Document-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--data-dir DIR           # Directory containing documents to process (default: data)
--file-types .ext .ext   # Filter by specific file types (optional - all LlamaIndex supported types if omitted)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Process all documents with larger chunks for academic papers
python -m apps.document_rag --data-dir "~/Documents/Papers" --chunk-size 1024

# Filter only markdown and Python files with smaller chunks
python -m apps.document_rag --data-dir "./docs" --chunk-size 256 --file-types .md .py

# Enable AST-aware chunking for code files
python -m apps.document_rag --enable-code-chunking --data-dir "./my_project"

# Or use the specialized code RAG for better code understanding
python -m apps.code_rag --repo-dir "./my_codebase" --query "How does authentication work?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;üé® ColQwen: Multimodal PDF Retrieval with Vision-Language Models&lt;/h3&gt; 
&lt;p&gt;Search through PDFs using both text and visual understanding with ColQwen2/ColPali models. Perfect for research papers, technical documents, and any PDFs with complex layouts, figures, or diagrams.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üçé Mac Users&lt;/strong&gt;: ColQwen is optimized for Apple Silicon with MPS acceleration for faster inference!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Build index from PDFs
python -m apps.colqwen_rag build --pdfs ./my_papers/ --index research_papers

# Search with text queries
python -m apps.colqwen_rag search research_papers "How does attention mechanism work?"

# Interactive Q&amp;amp;A
python -m apps.colqwen_rag ask research_papers --interactive
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: ColQwen Setup &amp;amp; Usage&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Prerequisites&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Install dependencies
uv pip install colpali_engine pdf2image pillow matplotlib qwen_vl_utils einops seaborn
brew install poppler  # macOS only, for PDF processing
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Build Index&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.colqwen_rag build \
  --pdfs ./pdf_directory/ \
  --index my_index \
  --model colqwen2  # or colpali
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Search&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.colqwen_rag search my_index "your question here" --top-k 5
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Models&lt;/h4&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;ColQwen2&lt;/strong&gt; (&lt;code&gt;colqwen2&lt;/code&gt;): Latest vision-language model with improved performance&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;ColPali&lt;/strong&gt; (&lt;code&gt;colpali&lt;/code&gt;): Proven multimodal retriever&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;For detailed usage, see the &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/COLQWEN_GUIDE.md"&gt;ColQwen Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;üìß Your Personal Email Secretary: RAG on Apple Mail!&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The examples below currently support macOS only. Windows support coming soon.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/videos/mail_clear.gif" alt="LEANN Email Search Demo" width="600" /&gt; &lt;/p&gt; 
&lt;p&gt;Before running the example below, you need to grant full disk access to your terminal/VS Code in System Preferences ‚Üí Privacy &amp;amp; Security ‚Üí Full Disk Access.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.email_rag --query "What's the food I ordered by DoorDash or Uber Eats mostly?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;780K email chunks ‚Üí 78MB storage.&lt;/strong&gt; Finally, search your email like you search Google.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: Email-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--mail-path PATH         # Path to specific mail directory (auto-detects if omitted)
--include-html          # Include HTML content in processing (useful for newsletters)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Search work emails from a specific account
python -m apps.email_rag --mail-path "~/Library/Mail/V10/WORK_ACCOUNT"

# Find all receipts and order confirmations (includes HTML)
python -m apps.email_rag --query "receipt order confirmation invoice" --include-html
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Once the index is built, you can ask questions like:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"Find emails from my boss about deadlines"&lt;/li&gt; 
  &lt;li&gt;"What did John say about the project timeline?"&lt;/li&gt; 
  &lt;li&gt;"Show me emails about travel expenses"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;üîç Time Machine for the Web: RAG Your Entire Chrome Browser History!&lt;/h3&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/videos/google_clear.gif" alt="LEANN Browser History Search Demo" width="600" /&gt; &lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.browser_rag --query "Tell me my browser history about machine learning?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;38K browser entries ‚Üí 6MB storage.&lt;/strong&gt; Your browser history becomes your personal search engine.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: Browser-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--chrome-profile PATH    # Path to Chrome profile directory (auto-detects if omitted)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Search academic research from your browsing history
python -m apps.browser_rag --query "arxiv papers machine learning transformer architecture"

# Track competitor analysis across work profile
python -m apps.browser_rag --chrome-profile "~/Library/Application Support/Google/Chrome/Work Profile" --max-items 5000
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: How to find your Chrome profile&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;The default Chrome profile path is configured for a typical macOS setup. If you need to find your specific Chrome profile:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Open Terminal&lt;/li&gt; 
  &lt;li&gt;Run: &lt;code&gt;ls ~/Library/Application\ Support/Google/Chrome/&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;Look for folders like "Default", "Profile 1", "Profile 2", etc.&lt;/li&gt; 
  &lt;li&gt;Use the full path as your &lt;code&gt;--chrome-profile&lt;/code&gt; argument&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;Common Chrome profile locations:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;macOS: &lt;code&gt;~/Library/Application Support/Google/Chrome/Default&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;Linux: &lt;code&gt;~/.config/google-chrome/Default&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üí¨ Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Once the index is built, you can ask questions like:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"What websites did I visit about machine learning?"&lt;/li&gt; 
  &lt;li&gt;"Find my search history about programming"&lt;/li&gt; 
  &lt;li&gt;"What YouTube videos did I watch recently?"&lt;/li&gt; 
  &lt;li&gt;"Show me websites I visited about travel planning"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;üí¨ WeChat Detective: Unlock Your Golden Memories!&lt;/h3&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/videos/wechat_clear.gif" alt="LEANN WeChat Search Demo" width="600" /&gt; &lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.wechat_rag --query "Show me all group chats about weekend plans"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;400K messages ‚Üí 64MB storage&lt;/strong&gt; Search years of chat history in any language.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üîß Click to expand: Installation Requirements&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;First, you need to install the &lt;a href="https://github.com/sunnyyoung/WeChatTweak-CLI"&gt;WeChat exporter&lt;/a&gt;,&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;brew install sunnyyoung/repo/wechattweak-cli
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;or install it manually (if you have issues with Homebrew):&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;sudo packages/wechat-exporter/wechattweak-cli install
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Troubleshooting:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Installation issues&lt;/strong&gt;: Check the &lt;a href="https://github.com/sunnyyoung/WeChatTweak-CLI/issues/41"&gt;WeChatTweak-CLI issues page&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Export errors&lt;/strong&gt;: If you encounter the error below, try restarting WeChat &lt;pre&gt;&lt;code class="language-bash"&gt;Failed to export WeChat data. Please ensure WeChat is running and WeChatTweak is installed.
Failed to find or export WeChat data. Exiting.
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: WeChat-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--export-dir DIR         # Directory to store exported WeChat data (default: wechat_export_direct)
--force-export          # Force re-export even if data exists
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Search for travel plans discussed in group chats
python -m apps.wechat_rag --query "travel plans" --max-items 10000

# Re-export and search recent chats (useful after new messages)
python -m apps.wechat_rag --force-export --query "work schedule"
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üí¨ Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Once the index is built, you can ask questions like:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"ÊàëÊÉ≥‰π∞È≠îÊúØÂ∏àÁ∫¶Áø∞ÈÄäÁöÑÁêÉË°£ÔºåÁªôÊàë‰∏Ä‰∫õÂØπÂ∫îËÅäÂ§©ËÆ∞ÂΩï?" (Chinese: Show me chat records about buying Magic Johnson's jersey)&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;ü§ñ ChatGPT Chat History: Your Personal AI Conversation Archive!&lt;/h3&gt; 
&lt;p&gt;Transform your ChatGPT conversations into a searchable knowledge base! Search through all your ChatGPT discussions about coding, research, brainstorming, and more.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.chatgpt_rag --export-path chatgpt_export.html --query "How do I create a list in Python?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Unlock your AI conversation history.&lt;/strong&gt; Never lose track of valuable insights from your ChatGPT discussions again.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: How to Export ChatGPT Data&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Step-by-step export process:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Sign in to ChatGPT&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Click your profile icon&lt;/strong&gt; in the top right corner&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Navigate to Settings&lt;/strong&gt; ‚Üí &lt;strong&gt;Data Controls&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Click "Export"&lt;/strong&gt; under Export Data&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Confirm the export&lt;/strong&gt; request&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Download the ZIP file&lt;/strong&gt; from the email link (expires in 24 hours)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Extract or use directly&lt;/strong&gt; with LEANN&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;Supported formats:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;.html&lt;/code&gt; files from ChatGPT exports&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;.zip&lt;/code&gt; archives from ChatGPT&lt;/li&gt; 
  &lt;li&gt;Directories with multiple export files&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: ChatGPT-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--export-path PATH           # Path to ChatGPT export file (.html/.zip) or directory (default: ./chatgpt_export)
--separate-messages         # Process each message separately instead of concatenated conversations
--chunk-size N              # Text chunk size (default: 512)
--chunk-overlap N           # Overlap between chunks (default: 128)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Basic usage with HTML export
python -m apps.chatgpt_rag --export-path conversations.html

# Process ZIP archive from ChatGPT
python -m apps.chatgpt_rag --export-path chatgpt_export.zip

# Search with specific query
python -m apps.chatgpt_rag --export-path chatgpt_data.html --query "Python programming help"

# Process individual messages for fine-grained search
python -m apps.chatgpt_rag --separate-messages --export-path chatgpt_export.html

# Process directory containing multiple exports
python -m apps.chatgpt_rag --export-path ./chatgpt_exports/ --max-items 1000
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üí° Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Once your ChatGPT conversations are indexed, you can search with queries like:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"What did I ask ChatGPT about Python programming?"&lt;/li&gt; 
  &lt;li&gt;"Show me conversations about machine learning algorithms"&lt;/li&gt; 
  &lt;li&gt;"Find discussions about web development frameworks"&lt;/li&gt; 
  &lt;li&gt;"What coding advice did ChatGPT give me?"&lt;/li&gt; 
  &lt;li&gt;"Search for conversations about debugging techniques"&lt;/li&gt; 
  &lt;li&gt;"Find ChatGPT's recommendations for learning resources"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;ü§ñ Claude Chat History: Your Personal AI Conversation Archive!&lt;/h3&gt; 
&lt;p&gt;Transform your Claude conversations into a searchable knowledge base! Search through all your Claude discussions about coding, research, brainstorming, and more.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.claude_rag --export-path claude_export.json --query "What did I ask about Python dictionaries?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Unlock your AI conversation history.&lt;/strong&gt; Never lose track of valuable insights from your Claude discussions again.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: How to Export Claude Data&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Step-by-step export process:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Open Claude&lt;/strong&gt; in your browser&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Navigate to Settings&lt;/strong&gt; (look for gear icon or settings menu)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Find Export/Download&lt;/strong&gt; options in your account settings&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Download conversation data&lt;/strong&gt; (usually in JSON format)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Place the file&lt;/strong&gt; in your project directory&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;em&gt;Note: Claude export methods may vary depending on the interface you're using. Check Claude's help documentation for the most current export instructions.&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Supported formats:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;.json&lt;/code&gt; files (recommended)&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;.zip&lt;/code&gt; archives containing JSON data&lt;/li&gt; 
  &lt;li&gt;Directories with multiple export files&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: Claude-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--export-path PATH           # Path to Claude export file (.json/.zip) or directory (default: ./claude_export)
--separate-messages         # Process each message separately instead of concatenated conversations
--chunk-size N              # Text chunk size (default: 512)
--chunk-overlap N           # Overlap between chunks (default: 128)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Basic usage with JSON export
python -m apps.claude_rag --export-path my_claude_conversations.json

# Process ZIP archive from Claude
python -m apps.claude_rag --export-path claude_export.zip

# Search with specific query
python -m apps.claude_rag --export-path claude_data.json --query "machine learning advice"

# Process individual messages for fine-grained search
python -m apps.claude_rag --separate-messages --export-path claude_export.json

# Process directory containing multiple exports
python -m apps.claude_rag --export-path ./claude_exports/ --max-items 1000
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üí° Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Once your Claude conversations are indexed, you can search with queries like:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"What did I ask Claude about Python programming?"&lt;/li&gt; 
  &lt;li&gt;"Show me conversations about machine learning algorithms"&lt;/li&gt; 
  &lt;li&gt;"Find discussions about software architecture patterns"&lt;/li&gt; 
  &lt;li&gt;"What debugging advice did Claude give me?"&lt;/li&gt; 
  &lt;li&gt;"Search for conversations about data structures"&lt;/li&gt; 
  &lt;li&gt;"Find Claude's recommendations for learning resources"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;üí¨ iMessage History: Your Personal Conversation Archive!&lt;/h3&gt; 
&lt;p&gt;Transform your iMessage conversations into a searchable knowledge base! Search through all your text messages, group chats, and conversations with friends, family, and colleagues.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.imessage_rag --query "What did we discuss about the weekend plans?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Unlock your message history.&lt;/strong&gt; Never lose track of important conversations, shared links, or memorable moments from your iMessage history.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: How to Access iMessage Data&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;iMessage data location:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;iMessage conversations are stored in a SQLite database on your Mac at:&lt;/p&gt; 
 &lt;pre&gt;&lt;code&gt;~/Library/Messages/chat.db
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Important setup requirements:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Grant Full Disk Access&lt;/strong&gt; to your terminal or IDE:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Open &lt;strong&gt;System Preferences&lt;/strong&gt; ‚Üí &lt;strong&gt;Security &amp;amp; Privacy&lt;/strong&gt; ‚Üí &lt;strong&gt;Privacy&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Select &lt;strong&gt;Full Disk Access&lt;/strong&gt; from the left sidebar&lt;/li&gt; 
    &lt;li&gt;Click the &lt;strong&gt;+&lt;/strong&gt; button and add your terminal app (Terminal, iTerm2) or IDE (VS Code, etc.)&lt;/li&gt; 
    &lt;li&gt;Restart your terminal/IDE after granting access&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alternative: Use a backup database&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;If you have Time Machine backups or manual copies of the database&lt;/li&gt; 
    &lt;li&gt;Use &lt;code&gt;--db-path&lt;/code&gt; to specify a custom location&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;Supported formats:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Direct access to &lt;code&gt;~/Library/Messages/chat.db&lt;/code&gt; (default)&lt;/li&gt; 
  &lt;li&gt;Custom database path with &lt;code&gt;--db-path&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;Works with backup copies of the database&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: iMessage-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--db-path PATH                    # Path to chat.db file (default: ~/Library/Messages/chat.db)
--concatenate-conversations       # Group messages by conversation (default: True)
--no-concatenate-conversations    # Process each message individually
--chunk-size N                    # Text chunk size (default: 1000)
--chunk-overlap N                 # Overlap between chunks (default: 200)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Basic usage (requires Full Disk Access)
python -m apps.imessage_rag

# Search with specific query
python -m apps.imessage_rag --query "family dinner plans"

# Use custom database path
python -m apps.imessage_rag --db-path /path/to/backup/chat.db

# Process individual messages instead of conversations
python -m apps.imessage_rag --no-concatenate-conversations

# Limit processing for testing
python -m apps.imessage_rag --max-items 100 --query "weekend"
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üí° Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Once your iMessage conversations are indexed, you can search with queries like:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"What did we discuss about vacation plans?"&lt;/li&gt; 
  &lt;li&gt;"Find messages about restaurant recommendations"&lt;/li&gt; 
  &lt;li&gt;"Show me conversations with John about the project"&lt;/li&gt; 
  &lt;li&gt;"Search for shared links about technology"&lt;/li&gt; 
  &lt;li&gt;"Find group chat discussions about weekend events"&lt;/li&gt; 
  &lt;li&gt;"What did mom say about the family gathering?"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;MCP Integration: RAG on Live Data from Any Platform&lt;/h3&gt; 
&lt;p&gt;Connect to live data sources through the Model Context Protocol (MCP). LEANN now supports real-time RAG on platforms like Slack, Twitter, and more through standardized MCP servers.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key Benefits:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Live Data Access&lt;/strong&gt;: Fetch real-time data without manual exports&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Standardized Protocol&lt;/strong&gt;: Use any MCP-compatible server&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy Extension&lt;/strong&gt;: Add new platforms with minimal code&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Secure Access&lt;/strong&gt;: MCP servers handle authentication&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;üí¨ Slack Messages: Search Your Team Conversations&lt;/h4&gt; 
&lt;p&gt;Transform your Slack workspace into a searchable knowledge base! Find discussions, decisions, and shared knowledge across all your channels.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Test MCP server connection
python -m apps.slack_rag --mcp-server "slack-mcp-server" --test-connection

# Index and search Slack messages
python -m apps.slack_rag \
  --mcp-server "slack-mcp-server" \
  --workspace-name "my-team" \
  --channels general dev-team random \
  --query "What did we decide about the product launch?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;üìñ Comprehensive Setup Guide&lt;/strong&gt;: For detailed setup instructions, troubleshooting common issues (like "users cache is not ready yet"), and advanced configuration options, see our &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/slack-setup-guide.md"&gt;&lt;strong&gt;Slack Setup Guide&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Quick Setup:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install a Slack MCP server (e.g., &lt;code&gt;npm install -g slack-mcp-server&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Create a Slack App and get API credentials (see detailed guide above)&lt;/li&gt; 
 &lt;li&gt;Set environment variables: &lt;pre&gt;&lt;code class="language-bash"&gt;export SLACK_BOT_TOKEN="xoxb-your-bot-token"
export SLACK_APP_TOKEN="xapp-your-app-token"  # Optional
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Test connection with &lt;code&gt;--test-connection&lt;/code&gt; flag&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Arguments:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--mcp-server&lt;/code&gt;: Command to start the Slack MCP server&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--workspace-name&lt;/code&gt;: Slack workspace name for organization&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--channels&lt;/code&gt;: Specific channels to index (optional)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--concatenate-conversations&lt;/code&gt;: Group messages by channel (default: true)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--max-messages-per-channel&lt;/code&gt;: Limit messages per channel (default: 100)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--max-retries&lt;/code&gt;: Maximum retries for cache sync issues (default: 5)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--retry-delay&lt;/code&gt;: Initial delay between retries in seconds (default: 2.0)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;üê¶ Twitter Bookmarks: Your Personal Tweet Library&lt;/h4&gt; 
&lt;p&gt;Search through your Twitter bookmarks! Find that perfect article, thread, or insight you saved for later.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Test MCP server connection
python -m apps.twitter_rag --mcp-server "twitter-mcp-server" --test-connection

# Index and search Twitter bookmarks
python -m apps.twitter_rag \
  --mcp-server "twitter-mcp-server" \
  --max-bookmarks 1000 \
  --query "What AI articles did I bookmark about machine learning?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Setup Requirements:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install a Twitter MCP server (e.g., &lt;code&gt;npm install -g twitter-mcp-server&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Get Twitter API credentials: 
  &lt;ul&gt; 
   &lt;li&gt;Apply for a Twitter Developer Account at &lt;a href="https://developer.twitter.com"&gt;developer.twitter.com&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Create a new app in the Twitter Developer Portal&lt;/li&gt; 
   &lt;li&gt;Generate API keys and access tokens with "Read" permissions&lt;/li&gt; 
   &lt;li&gt;For bookmarks access, you may need Twitter API v2 with appropriate scopes&lt;/li&gt; 
  &lt;/ul&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;export TWITTER_API_KEY="your-api-key"
export TWITTER_API_SECRET="your-api-secret"
export TWITTER_ACCESS_TOKEN="your-access-token"
export TWITTER_ACCESS_TOKEN_SECRET="your-access-token-secret"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Test connection with &lt;code&gt;--test-connection&lt;/code&gt; flag&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Arguments:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--mcp-server&lt;/code&gt;: Command to start the Twitter MCP server&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--username&lt;/code&gt;: Filter bookmarks by username (optional)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--max-bookmarks&lt;/code&gt;: Maximum bookmarks to fetch (default: 1000)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--no-tweet-content&lt;/code&gt;: Exclude tweet content, only metadata&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--no-metadata&lt;/code&gt;: Exclude engagement metadata&lt;/li&gt; 
&lt;/ul&gt;  
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üí° Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Slack Queries:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"What did the team discuss about the project deadline?"&lt;/li&gt; 
  &lt;li&gt;"Find messages about the new feature launch"&lt;/li&gt; 
  &lt;li&gt;"Show me conversations about budget planning"&lt;/li&gt; 
  &lt;li&gt;"What decisions were made in the dev-team channel?"&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Twitter Queries:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"What AI articles did I bookmark last month?"&lt;/li&gt; 
  &lt;li&gt;"Find tweets about machine learning techniques"&lt;/li&gt; 
  &lt;li&gt;"Show me bookmarked threads about startup advice"&lt;/li&gt; 
  &lt;li&gt;"What Python tutorials did I save?"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;summary&gt;&lt;strong&gt;üîß Using MCP with CLI Commands&lt;/strong&gt;&lt;/summary&gt; 
&lt;p&gt;&lt;strong&gt;Want to use MCP data with regular LEANN CLI?&lt;/strong&gt; You can combine MCP apps with CLI commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Step 1: Use MCP app to fetch and index data
python -m apps.slack_rag --mcp-server "slack-mcp-server" --workspace-name "my-team"

# Step 2: The data is now indexed and available via CLI
leann search slack_messages "project deadline"
leann ask slack_messages "What decisions were made about the product launch?"

# Same for Twitter bookmarks
python -m apps.twitter_rag --mcp-server "twitter-mcp-server"
leann search twitter_bookmarks "machine learning articles"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;MCP vs Manual Export:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;MCP&lt;/strong&gt;: Live data, automatic updates, requires server setup&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Manual Export&lt;/strong&gt;: One-time setup, works offline, requires manual data export&lt;/li&gt; 
&lt;/ul&gt;  
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üîß Adding New MCP Platforms&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Want to add support for other platforms? LEANN's MCP integration is designed for easy extension:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Find or create an MCP server&lt;/strong&gt; for your platform&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Create a reader class&lt;/strong&gt; following the pattern in &lt;code&gt;apps/slack_data/slack_mcp_reader.py&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Create a RAG application&lt;/strong&gt; following the pattern in &lt;code&gt;apps/slack_rag.py&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Test and contribute&lt;/strong&gt; back to the community!&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;Popular MCP servers to explore:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;GitHub repositories and issues&lt;/li&gt; 
  &lt;li&gt;Discord messages&lt;/li&gt; 
  &lt;li&gt;Notion pages&lt;/li&gt; 
  &lt;li&gt;Google Drive documents&lt;/li&gt; 
  &lt;li&gt;And many more in the MCP ecosystem!&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;üöÄ Claude Code Integration: Transform Your Development Workflow!&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;AST‚ÄëAware Code Chunking&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;LEANN features intelligent code chunking that preserves semantic boundaries (functions, classes, methods) for Python, Java, C#, and TypeScript, improving code understanding compared to text-based chunking.&lt;/p&gt; 
 &lt;p&gt;üìñ Read the &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/ast_chunking_guide.md"&gt;AST Chunking Guide ‚Üí&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;strong&gt;The future of code assistance is here.&lt;/strong&gt; Transform your development workflow with LEANN's native MCP integration for Claude Code. Index your entire codebase and get intelligent code assistance directly in your IDE.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üîç &lt;strong&gt;Semantic code search&lt;/strong&gt; across your entire project, fully local index and lightweight&lt;/li&gt; 
 &lt;li&gt;üß† &lt;strong&gt;AST-aware chunking&lt;/strong&gt; preserves code structure (functions, classes)&lt;/li&gt; 
 &lt;li&gt;üìö &lt;strong&gt;Context-aware assistance&lt;/strong&gt; for debugging and development&lt;/li&gt; 
 &lt;li&gt;üöÄ &lt;strong&gt;Zero-config setup&lt;/strong&gt; with automatic language detection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install LEANN globally for MCP integration
uv tool install leann-core --with leann
claude mcp add --scope user leann-server -- leann_mcp
# Setup is automatic - just start using Claude Code!
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Try our fully agentic pipeline with auto query rewriting, semantic search planning, and more:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/assets/mcp_leann.png" alt="LEANN MCP Integration" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;üî• Ready to supercharge your coding?&lt;/strong&gt; &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/packages/leann-mcp/README.md"&gt;Complete Setup Guide ‚Üí&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Command Line Interface&lt;/h2&gt; 
&lt;p&gt;LEANN includes a powerful CLI for document processing and search. Perfect for quick document indexing and interactive chat.&lt;/p&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;If you followed the Quick Start, &lt;code&gt;leann&lt;/code&gt; is already installed in your virtual environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;source .venv/bin/activate
leann --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;To make it globally available:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install the LEANN CLI globally using uv tool
uv tool install leann-core --with leann


# Now you can use leann from anywhere without activating venv
leann --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Global installation is required for Claude Code integration. The &lt;code&gt;leann_mcp&lt;/code&gt; server depends on the globally available &lt;code&gt;leann&lt;/code&gt; command.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Usage Examples&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# build from a specific directory, and my_docs is the index name(Here you can also build from multiple dict or multiple files)
leann build my-docs --docs ./your_documents

# Search your documents
leann search my-docs "machine learning concepts"

# Interactive chat with your documents
leann ask my-docs --interactive

# Ask a single question (non-interactive)
leann ask my-docs "Where are prompts configured?"

# List all your indexes
leann list

# Remove an index
leann remove my-docs
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Key CLI features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Auto-detects document formats (PDF, TXT, MD, DOCX, PPTX + code files)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üß† AST-aware chunking&lt;/strong&gt; for Python, Java, C#, TypeScript files&lt;/li&gt; 
 &lt;li&gt;Smart text chunking with overlap for all other content&lt;/li&gt; 
 &lt;li&gt;Multiple LLM providers (Ollama, OpenAI, HuggingFace)&lt;/li&gt; 
 &lt;li&gt;Organized index storage in &lt;code&gt;.leann/indexes/&lt;/code&gt; (project-local)&lt;/li&gt; 
 &lt;li&gt;Support for advanced search parameters&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: Complete CLI Reference&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;You can use &lt;code&gt;leann --help&lt;/code&gt;, or &lt;code&gt;leann build --help&lt;/code&gt;, &lt;code&gt;leann search --help&lt;/code&gt;, &lt;code&gt;leann ask --help&lt;/code&gt;, &lt;code&gt;leann list --help&lt;/code&gt;, &lt;code&gt;leann remove --help&lt;/code&gt; to get the complete CLI reference.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Build Command:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;leann build INDEX_NAME --docs DIRECTORY|FILE [DIRECTORY|FILE ...] [OPTIONS]

Options:
  --backend {hnsw,diskann}     Backend to use (default: hnsw)
  --embedding-model MODEL      Embedding model (default: facebook/contriever)
  --graph-degree N             Graph degree (default: 32)
  --complexity N               Build complexity (default: 64)
  --force                      Force rebuild existing index
  --compact / --no-compact     Use compact storage (default: true). Must be `no-compact` for `no-recompute` build.
  --recompute / --no-recompute Enable recomputation (default: true)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Search Command:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;leann search INDEX_NAME QUERY [OPTIONS]

Options:
  --top-k N                     Number of results (default: 5)
  --complexity N                Search complexity (default: 64)
  --recompute / --no-recompute  Enable/disable embedding recomputation (default: enabled). Should not do a `no-recompute` search in a `recompute` build.
  --pruning-strategy {global,local,proportional}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Ask Command:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;leann ask INDEX_NAME [OPTIONS]

Options:
  --llm {ollama,openai,hf,anthropic}    LLM provider (default: ollama)
  --model MODEL                         Model name (default: qwen3:8b)
  --interactive                         Interactive chat mode
  --top-k N                             Retrieval count (default: 20)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;List Command:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;leann list

# Lists all indexes across all projects with status indicators:
# ‚úÖ - Index is complete and ready to use
# ‚ùå - Index is incomplete or corrupted
# üìÅ - CLI-created index (in .leann/indexes/)
# üìÑ - App-created index (*.leann.meta.json files)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Remove Command:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;leann remove INDEX_NAME [OPTIONS]

Options:
  --force, -f    Force removal without confirmation

# Smart removal: automatically finds and safely removes indexes
# - Shows all matching indexes across projects
# - Requires confirmation for cross-project removal
# - Interactive selection when multiple matches found
# - Supports both CLI and app-created indexes
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;üöÄ Advanced Features&lt;/h2&gt; 
&lt;h3&gt;üéØ Metadata Filtering&lt;/h3&gt; 
&lt;p&gt;LEANN supports a simple metadata filtering system to enable sophisticated use cases like document filtering by date/type, code search by file extension, and content management based on custom criteria.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Add metadata during indexing
builder.add_text(
    "def authenticate_user(token): ...",
    metadata={"file_extension": ".py", "lines_of_code": 25}
)

# Search with filters
results = searcher.search(
    query="authentication function",
    metadata_filters={
        "file_extension": {"==": ".py"},
        "lines_of_code": {"&amp;lt;": 100}
    }
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Supported operators&lt;/strong&gt;: &lt;code&gt;==&lt;/code&gt;, &lt;code&gt;!=&lt;/code&gt;, &lt;code&gt;&amp;lt;&lt;/code&gt;, &lt;code&gt;&amp;lt;=&lt;/code&gt;, &lt;code&gt;&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;gt;=&lt;/code&gt;, &lt;code&gt;in&lt;/code&gt;, &lt;code&gt;not_in&lt;/code&gt;, &lt;code&gt;contains&lt;/code&gt;, &lt;code&gt;starts_with&lt;/code&gt;, &lt;code&gt;ends_with&lt;/code&gt;, &lt;code&gt;is_true&lt;/code&gt;, &lt;code&gt;is_false&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;üìñ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/metadata_filtering.md"&gt;Complete Metadata filtering guide ‚Üí&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;üîç Grep Search&lt;/h3&gt; 
&lt;p&gt;For exact text matching instead of semantic search, use the &lt;code&gt;use_grep&lt;/code&gt; parameter:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Exact text search
results = searcher.search("banana‚Äëcrocodile", use_grep=True, top_k=1)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Use cases&lt;/strong&gt;: Finding specific code patterns, error messages, function names, or exact phrases where semantic similarity isn't needed.&lt;/p&gt; 
&lt;p&gt;üìñ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/grep_search.md"&gt;Complete grep search guide ‚Üí&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;üèóÔ∏è Architecture &amp;amp; How It Works&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/assets/arch.png" alt="LEANN Architecture" width="800" /&gt; &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;The magic:&lt;/strong&gt; Most vector DBs store every single embedding (expensive). LEANN stores a pruned graph structure (cheap) and recomputes embeddings only when needed (fast).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Core techniques:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Graph-based selective recomputation:&lt;/strong&gt; Only compute embeddings for nodes in the search path&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High-degree preserving pruning:&lt;/strong&gt; Keep important "hub" nodes while removing redundant connections&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dynamic batching:&lt;/strong&gt; Efficiently batch embedding computations for GPU utilization&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Two-level search:&lt;/strong&gt; Smart graph traversal that prioritizes promising nodes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Backends:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;HNSW&lt;/strong&gt; (default): Ideal for most datasets with maximum storage savings through full recomputation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DiskANN&lt;/strong&gt;: Advanced option with superior search performance, using PQ-based graph traversal with real-time reranking for the best speed-accuracy trade-off&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Benchmarks&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/benchmarks/diskann_vs_hnsw_speed_comparison.py"&gt;DiskANN vs HNSW Performance Comparison ‚Üí&lt;/a&gt;&lt;/strong&gt; - Compare search performance between both backends&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/benchmarks/compare_faiss_vs_leann.py"&gt;Simple Example: Compare LEANN vs FAISS ‚Üí&lt;/a&gt;&lt;/strong&gt; - See storage savings in action&lt;/p&gt; 
&lt;h3&gt;üìä Storage Comparison&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;System&lt;/th&gt; 
   &lt;th&gt;DPR (2.1M)&lt;/th&gt; 
   &lt;th&gt;Wiki (60M)&lt;/th&gt; 
   &lt;th&gt;Chat (400K)&lt;/th&gt; 
   &lt;th&gt;Email (780K)&lt;/th&gt; 
   &lt;th&gt;Browser (38K)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Traditional vector database (e.g., FAISS)&lt;/td&gt; 
   &lt;td&gt;3.8 GB&lt;/td&gt; 
   &lt;td&gt;201 GB&lt;/td&gt; 
   &lt;td&gt;1.8 GB&lt;/td&gt; 
   &lt;td&gt;2.4 GB&lt;/td&gt; 
   &lt;td&gt;130 MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LEANN&lt;/td&gt; 
   &lt;td&gt;324 MB&lt;/td&gt; 
   &lt;td&gt;6 GB&lt;/td&gt; 
   &lt;td&gt;64 MB&lt;/td&gt; 
   &lt;td&gt;79 MB&lt;/td&gt; 
   &lt;td&gt;6.4 MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Savings&lt;/td&gt; 
   &lt;td&gt;91%&lt;/td&gt; 
   &lt;td&gt;97%&lt;/td&gt; 
   &lt;td&gt;97%&lt;/td&gt; 
   &lt;td&gt;97%&lt;/td&gt; 
   &lt;td&gt;95%&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Reproduce Our Results&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run benchmarks/run_evaluation.py    # Will auto-download evaluation data and run benchmarks
uv run benchmarks/run_evaluation.py benchmarks/data/indices/rpj_wiki/rpj_wiki --num-queries 2000    # After downloading data, you can run the benchmark with our biggest index
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The evaluation script downloads data automatically on first run. The last three results were tested with partial personal data, and you can reproduce them with your own data!&lt;/p&gt; 
&lt;h2&gt;üî¨ Paper&lt;/h2&gt; 
&lt;p&gt;If you find Leann useful, please cite:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://arxiv.org/abs/2506.08276"&gt;LEANN: A Low-Storage Vector Index&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{wang2025leannlowstoragevectorindex,
      title={LEANN: A Low-Storage Vector Index},
      author={Yichuan Wang and Shu Liu and Zhifei Li and Yongji Wu and Ziming Mao and Yilong Zhao and Xiao Yan and Zhiying Xu and Yang Zhou and Ion Stoica and Sewon Min and Matei Zaharia and Joseph E. Gonzalez},
      year={2025},
      eprint={2506.08276},
      archivePrefix={arXiv},
      primaryClass={cs.DB},
      url={https://arxiv.org/abs/2506.08276},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;‚ú® &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/features.md"&gt;Detailed Features ‚Üí&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;ü§ù &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/CONTRIBUTING.md"&gt;CONTRIBUTING ‚Üí&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;‚ùì &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/faq.md"&gt;FAQ ‚Üí&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;üìà &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/roadmap.md"&gt;Roadmap ‚Üí&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;MIT License - see &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/LICENSE"&gt;LICENSE&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h2&gt;üôè Acknowledgments&lt;/h2&gt; 
&lt;p&gt;Core Contributors: &lt;a href="https://yichuan-w.github.io/"&gt;Yichuan Wang&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/andylizf"&gt;Zhifei Li&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Active Contributors: &lt;a href="https://github.com/gabriel-dehan"&gt;Gabriel Dehan&lt;/a&gt;, &lt;a href="https://github.com/ASuresh0524"&gt;Aakash Suresh&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We welcome more contributors! Feel free to open issues or submit PRs.&lt;/p&gt; 
&lt;p&gt;This work is done at &lt;a href="https://sky.cs.berkeley.edu/"&gt;&lt;strong&gt;Berkeley Sky Computing Lab&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#yichuan-w/LEANN&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=yichuan-w/LEANN&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;strong&gt;‚≠ê Star us on GitHub if Leann is useful for your research or applications!&lt;/strong&gt; &lt;/p&gt; 
&lt;p align="center"&gt; Made with ‚ù§Ô∏è by the Leann team &lt;/p&gt; 
&lt;h2&gt;ü§ñ Explore LEANN with AI&lt;/h2&gt; 
&lt;p&gt;LEANN is indexed on &lt;a href="https://deepwiki.com/yichuan-w/LEANN"&gt;DeepWiki&lt;/a&gt;, so you can ask questions to LLMs using Deep Research to explore the codebase and get help to add new features.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>