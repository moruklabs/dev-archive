<rss version="2.0">
  <channel>
    <title>GitHub Python Weekly Trending</title>
    <description>Weekly Trending of Python in GitHub</description>
    <pubDate>Fri, 09 Jan 2026 01:45:37 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>3b1b/manim</title>
      <link>https://github.com/3b1b/manim</link>
      <description>&lt;p&gt;Animation engine for explanatory math videos&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://github.com/3b1b/manim"&gt; &lt;img src="https://raw.githubusercontent.com/3b1b/manim/master/logo/cropped.png" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/manimgl/"&gt;&lt;img src="https://img.shields.io/pypi/v/manimgl?logo=pypi" alt="pypi version" /&gt;&lt;/a&gt; &lt;a href="http://choosealicense.com/licenses/mit/"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?style=flat" alt="MIT License" /&gt;&lt;/a&gt; &lt;a href="https://www.reddit.com/r/manim/"&gt;&lt;img src="https://img.shields.io/reddit/subreddit-subscribers/manim.svg?color=ff4301&amp;amp;label=reddit&amp;amp;logo=reddit" alt="Manim Subreddit" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/bYCyhM9Kz2"&gt;&lt;img src="https://img.shields.io/discord/581738731934056449.svg?label=discord&amp;amp;logo=discord" alt="Manim Discord" /&gt;&lt;/a&gt; &lt;a href="https://3b1b.github.io/manim/"&gt;&lt;img src="https://github.com/3b1b/manim/workflows/docs/badge.svg?sanitize=true" alt="docs" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Manim is an engine for precise programmatic animations, designed for creating explanatory math videos.&lt;/p&gt; 
&lt;p&gt;Note, there are two versions of manim. This repository began as a personal project by the author of &lt;a href="https://www.3blue1brown.com/"&gt;3Blue1Brown&lt;/a&gt; for the purpose of animating those videos, with video-specific code available &lt;a href="https://github.com/3b1b/videos"&gt;here&lt;/a&gt;. In 2020 a group of developers forked it into what is now the &lt;a href="https://github.com/ManimCommunity/manim/"&gt;community edition&lt;/a&gt;, with a goal of being more stable, better tested, quicker to respond to community contributions, and all around friendlier to get started with. See &lt;a href="https://docs.manim.community/en/stable/faq/installation.html#different-versions"&gt;this page&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Warning] &lt;strong&gt;WARNING:&lt;/strong&gt; These instructions are for ManimGL &lt;em&gt;only&lt;/em&gt;. Trying to use these instructions to install &lt;a href="https://github.com/ManimCommunity/manim"&gt;Manim Community/manim&lt;/a&gt; or instructions there to install this version will cause problems. You should first decide which version you wish to install, then only follow the instructions for your desired version.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] &lt;strong&gt;Note&lt;/strong&gt;: To install manim directly through pip, please pay attention to the name of the installed package. This repository is ManimGL of 3b1b. The package name is &lt;code&gt;manimgl&lt;/code&gt; instead of &lt;code&gt;manim&lt;/code&gt; or &lt;code&gt;manimlib&lt;/code&gt;. Please use &lt;code&gt;pip install manimgl&lt;/code&gt; to install the version in this repository.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Manim runs on Python 3.7 or higher.&lt;/p&gt; 
&lt;p&gt;System requirements are &lt;a href="https://ffmpeg.org/"&gt;FFmpeg&lt;/a&gt;, &lt;a href="https://www.opengl.org/"&gt;OpenGL&lt;/a&gt; and &lt;a href="https://www.latex-project.org"&gt;LaTeX&lt;/a&gt; (optional, if you want to use LaTeX). For Linux, &lt;a href="https://pango.org"&gt;Pango&lt;/a&gt; along with its development headers are required. See instruction &lt;a href="https://github.com/ManimCommunity/ManimPango#building"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Directly&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Install manimgl
pip install manimgl

# Try it out
manimgl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more options, take a look at the &lt;a href="https://raw.githubusercontent.com/3b1b/manim/master/#using-manim"&gt;Using manim&lt;/a&gt; sections further below.&lt;/p&gt; 
&lt;p&gt;If you want to hack on manimlib itself, clone this repository and in that directory execute:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Install manimgl
pip install -e .

# Try it out
manimgl example_scenes.py OpeningManimExample
# or
manim-render example_scenes.py OpeningManimExample
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Directly (Windows)&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://www.wikihow.com/Install-FFmpeg-on-Windows"&gt;Install FFmpeg&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Install a LaTeX distribution. &lt;a href="https://miktex.org/download"&gt;MiKTeX&lt;/a&gt; is recommended.&lt;/li&gt; 
 &lt;li&gt;Install the remaining Python packages. &lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/3b1b/manim.git
cd manim
pip install -e .
manimgl example_scenes.py OpeningManimExample
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Mac OSX&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install FFmpeg, LaTeX in terminal using homebrew.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;brew install ffmpeg mactex
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If you are using an ARM-based processor, install Cairo.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;arch -arm64 brew install pkg-config cairo
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install latest version of manim using these command.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/3b1b/manim.git
cd manim
pip install -e .
manimgl example_scenes.py OpeningManimExample (make sure to add manimgl to path first.)
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Anaconda Install&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install LaTeX as above.&lt;/li&gt; 
 &lt;li&gt;Create a conda environment using &lt;code&gt;conda create -n manim python=3.8&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Activate the environment using &lt;code&gt;conda activate manim&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Install manimgl using &lt;code&gt;pip install -e .&lt;/code&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Using manim&lt;/h2&gt; 
&lt;p&gt;Try running the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;manimgl example_scenes.py OpeningManimExample
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This should pop up a window playing a simple scene.&lt;/p&gt; 
&lt;p&gt;Look through the &lt;a href="https://3b1b.github.io/manim/getting_started/example_scenes.html"&gt;example scenes&lt;/a&gt; to see examples of the library's syntax, animation types and object types. In the &lt;a href="https://github.com/3b1b/videos"&gt;3b1b/videos&lt;/a&gt; repo, you can see all the code for 3blue1brown videos, though code from older videos may not be compatible with the most recent version of manim. The readme of that repo also outlines some details for how to set up a more interactive workflow, as shown in &lt;a href="https://www.youtube.com/watch?v=rbu7Zu5X1zI"&gt;this manim demo video&lt;/a&gt; for example.&lt;/p&gt; 
&lt;p&gt;When running in the CLI, some useful flags include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-w&lt;/code&gt; to write the scene to a file&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-o&lt;/code&gt; to write the scene to a file and open the result&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-s&lt;/code&gt; to skip to the end and just show the final frame. 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;-so&lt;/code&gt; will save the final frame to an image and show it&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-n &amp;lt;number&amp;gt;&lt;/code&gt; to skip ahead to the &lt;code&gt;n&lt;/code&gt;'th animation of a scene.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-f&lt;/code&gt; to make the playback window fullscreen&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Take a look at custom_config.yml for further configuration. To add your customization, you can either edit this file, or add another file by the same name "custom_config.yml" to whatever directory you are running manim from. For example &lt;a href="https://github.com/3b1b/videos/raw/master/custom_config.yml"&gt;this is the one&lt;/a&gt; for 3blue1brown videos. There you can specify where videos should be output to, where manim should look for image files and sounds you want to read in, and other defaults regarding style and video quality.&lt;/p&gt; 
&lt;h3&gt;Documentation&lt;/h3&gt; 
&lt;p&gt;Documentation is in progress at &lt;a href="https://3b1b.github.io/manim/"&gt;3b1b.github.io/manim&lt;/a&gt;. And there is also a Chinese version maintained by &lt;a href="https://manim.org.cn"&gt;&lt;strong&gt;@manim-kindergarten&lt;/strong&gt;&lt;/a&gt;: &lt;a href="https://docs.manim.org.cn/"&gt;docs.manim.org.cn&lt;/a&gt; (in Chinese).&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/manim-kindergarten/"&gt;manim-kindergarten&lt;/a&gt; wrote and collected some useful extra classes and some codes of videos in &lt;a href="https://github.com/manim-kindergarten/manim_sandbox"&gt;manim_sandbox repo&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Is always welcome. As mentioned above, the &lt;a href="https://github.com/ManimCommunity/manim"&gt;community edition&lt;/a&gt; has the most active ecosystem for contributions, with testing and continuous integration, but pull requests are welcome here too. Please explain the motivation for a given change and examples of its effect.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project falls under the MIT license.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>HunxByts/GhostTrack</title>
      <link>https://github.com/HunxByts/GhostTrack</link>
      <description>&lt;p&gt;Useful tool to track location or mobile number&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GhostTrack&lt;/h1&gt; 
&lt;p&gt;Useful tool to track location or mobile number, so this tool can be called osint or also information gathering&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/raw/main/asset/bn.png" /&gt; 
&lt;p&gt;New update : &lt;code&gt;Version 2.2&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Instalation on Linux (deb)&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;sudo apt-get install git
sudo apt-get install python3
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Instalation on Termux&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;pkg install git
pkg install python3
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Usage Tool&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/HunxByts/GhostTrack.git
cd GhostTrack
pip3 install -r requirements.txt
python3 GhostTR.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Display on the menu &lt;code&gt;IP Tracker&lt;/code&gt;&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/blob/main/asset/ip.png " /&gt; 
&lt;p&gt;on the IP Track menu, you can combo with the seeker tool to get the target IP&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;span&gt;âš¡&lt;/span&gt; Install Seeker :&lt;/summary&gt; - 
 &lt;strong&gt;&lt;a href="https://github.com/thewhiteh4t/seeker"&gt;Get Seeker&lt;/a&gt;&lt;/strong&gt; 
&lt;/details&gt; 
&lt;p&gt;Display on the menu &lt;code&gt;Phone Tracker&lt;/code&gt;&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/raw/main/asset/phone.png" /&gt; 
&lt;p&gt;on this menu you can search for information from the target phone number&lt;/p&gt; 
&lt;p&gt;Display on the menu &lt;code&gt;Username Tracker&lt;/code&gt;&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/raw/main/asset/User.png" /&gt; on this menu you can search for information from the target username on social media 
&lt;details&gt; 
 &lt;summary&gt;&lt;span&gt;âš¡&lt;/span&gt; Author :&lt;/summary&gt; - 
 &lt;strong&gt;&lt;a href="https://github.com/HunxByts"&gt;HunxByts&lt;/a&gt;&lt;/strong&gt; 
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>QwenLM/Qwen-Image</title>
      <link>https://github.com/QwenLM/Qwen-Image</link>
      <description>&lt;p&gt;Qwen-Image is a powerful image generation foundation model capable of complex text rendering and precise image editing.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/qwen_image_logo.png" width="400" /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align="center"&gt;&amp;nbsp;&amp;nbsp;ğŸ’œ &lt;a href="https://chat.qwen.ai/"&gt;Qwen Chat&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ğŸ¤— &lt;a href="https://huggingface.co/Qwen/Qwen-Image"&gt;HuggingFace(T2I)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ğŸ¤— &lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit-2511"&gt;HuggingFace(Edit)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ğŸ¤– &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image"&gt;ModelScope-T2I&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ğŸ¤– &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image-Edit-2511"&gt;ModelScope-Edit&lt;/a&gt;&amp;nbsp;&amp;nbsp;| &amp;nbsp;&amp;nbsp; ğŸ“‘ &lt;a href="https://arxiv.org/abs/2508.02324"&gt;Tech Report&lt;/a&gt; &amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; ğŸ“‘ &lt;a href="https://qwenlm.github.io/blog/qwen-image/"&gt;Blog(T2I)&lt;/a&gt; &amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; ğŸ“‘ &lt;a href="https://qwenlm.github.io/blog/qwen-image-edit-2511/"&gt;Blog(Edit)&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;br /&gt; ğŸ–¥ï¸ &lt;a href="https://huggingface.co/spaces/Qwen/Qwen-Image"&gt;T2I Demo&lt;/a&gt;&amp;nbsp;&amp;nbsp; | ğŸ–¥ï¸ &lt;a href="https://huggingface.co/spaces/Qwen/Qwen-Image-Edit-2511"&gt;Edit Demo&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ğŸ’¬ &lt;a href="https://github.com/QwenLM/Qwen-Image/raw/main/assets/wechat.png"&gt;WeChat (å¾®ä¿¡)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ğŸ«¨ &lt;a href="https://discord.gg/CV4E9rpNSD"&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/merge3.jpg" width="1024" /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;We are thrilled to release &lt;strong&gt;Qwen-Image&lt;/strong&gt;, a 20B MMDiT image foundation model that achieves significant advances in &lt;strong&gt;complex text rendering&lt;/strong&gt; and &lt;strong&gt;precise image editing&lt;/strong&gt;. Experiments show strong general capabilities in both image generation and editing, with exceptional performance in text rendering, especially for Chinese.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/bench.png#center" alt="" /&gt;&lt;/p&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.31: We released Qwen-Image-2512 weights! Check at &lt;a href="https://huggingface.co/Qwen/Qwen-Image-2512"&gt;Huggingface&lt;/a&gt; and &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image-2512"&gt;ModelScope&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.31: We released Qwen-Image-2512! Check our &lt;a href="https://qwen.ai/blog?id=qwen-image-2512"&gt;Blog&lt;/a&gt; for more details! ğŸš€ Our December upgrade to Qwen-Image, just in time for the New Year.&lt;/p&gt; &lt;p&gt;âœ¨ Whatâ€™s new: â€¢ More realistic humans â€” dramatically reduced â€œAI look,â€ richer facial &amp;amp; age details â€¢ Finer natural textures â€” sharper landscapes, water, fur, and materials â€¢ Stronger text rendering â€” better layout, higher accuracy in textâ€“image composition&lt;/p&gt; &lt;p&gt;ğŸ† Tested in 10,000+ blind rounds on AI Arena, Qwen-Image-2512 ranks as the strongest open-source image model, while staying competitive with closed-source systems. &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/arena.png#center" alt="" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.31: &lt;a href="https://github.com/ModelTC/Qwen-Image-Lightning"&gt;Qwen-Image-Lightning&lt;/a&gt;, developed by &lt;a href="https://github.com/ModelTC/LightX2V"&gt;Lightx2v&lt;/a&gt;, provides &lt;a href="https://huggingface.co/lightx2v/Qwen-Image-2512-Lightning"&gt;Day 0 acceleration support for Qwen-Image-2512&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.31:vLLM-Omni supports high performance Qwen-Image-2512 inference from Day-0, with long sequence parallelism, cache acceleration and fast kernels, please check &lt;a href="https://github.com/vllm-project/vllm-omni/tree/main/examples/offline_inference/text_to_image"&gt;here&lt;/a&gt; for details.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.23: We released Qwen-Image-Edit-2511 weights! Check at &lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit-2511"&gt;Huggingface&lt;/a&gt; and &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image-Edit-2511"&gt;ModelScope&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.23: We released Qwen-Image-Edit-2511! Check our &lt;a href="https://qwen.ai/blog?id=qwen-image-edit-2511"&gt;Blog&lt;/a&gt; for more details!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.23: &lt;strong&gt;&lt;a href="https://github.com/ModelTC/LightX2V/"&gt;LightX2V&lt;/a&gt;&lt;/strong&gt; delivers Day 0 acceleration for Qwen-Image-Edit-2511, with native support for a wide range of hardware, including &lt;strong&gt;NVIDIA, Hygon, Metax, Ascend, and Cambricon&lt;/strong&gt;. By combining &lt;strong&gt;&lt;a href="https://github.com/ModelTC/Qwen-Image-Lightning"&gt;diffusion distillation&lt;/a&gt;&lt;/strong&gt; with cutting-edge inference optimizations, LightX2V achieves a &lt;strong&gt;25x reduction in DiT NFEs&lt;/strong&gt; and &lt;strong&gt;an order-of-magnitude 42.55x overall speedup&lt;/strong&gt;, enabling real-time image editing across diverse AI accelerators.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.23: &lt;strong&gt;vLLM-Omni&lt;/strong&gt; supports high performance &lt;code&gt;Qwen-Image-Edit-2511&lt;/code&gt;, &lt;code&gt;Qwen-Image-Layered&lt;/code&gt; inference from Day-0, with long sequence parallelism, cache acceleration and fast kernels, please check &lt;a href="https://github.com/vllm-project/vllm-omni/tree/main/examples/offline_inference/image_to_image"&gt;here&lt;/a&gt; for details.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.23: &lt;strong&gt;SGLang-Diffusion&lt;/strong&gt; provides day-0 support for Qwen-Image models. To play with &lt;code&gt;Qwen-Image-Edit-2511&lt;/code&gt; in SGlang, please check community supports section for details.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.19: We released Qwen-Image-Layered weights! Check at &lt;a href="https://huggingface.co/Qwen/Qwen-Image-Layered"&gt;Huggingface&lt;/a&gt; and &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image-Layered"&gt;ModelScope&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.19: We released Qwen-Image-Layered! Check our &lt;a href="https://qwenlm.github.io/blog/qwen-image-layered"&gt;Blog&lt;/a&gt; for more details!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.18: We released our &lt;a href="https://arxiv.org/abs/2512.15603"&gt;Research Paper&lt;/a&gt; on Arxiv!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.11.11: &lt;strong&gt;&lt;a href="https://t2i-corebench.github.io/"&gt;T2I-CoreBench&lt;/a&gt;&lt;/strong&gt; offers a comprehensive and complex evaluation of T2I models in real-world scenarios. On this benchmark, Qwen-Image achieves state-of-the-art performance under real-world complexities in both composition and reasoning T2I tasks, surpassing other open-source models and showing comparable results to closed-source ones.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.11.07: LeMiCa is a diffusion model inference acceleration solution developed by China Unicom Data Science and Artificial Intelligence Research Institute. By leveraging cache-based techniques and global denoising path optimization, LeMiCa provides efficient inference support for Qwen-Image, achieving nearly 3x lossless acceleration while maintaining visual consistency and quality. For more details, please visit the homepage: &lt;a href="https://unicomai.github.io/LeMiCa/"&gt;https://unicomai.github.io/LeMiCa/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.09.22: This September, we are pleased to introduce Qwen-Image-Edit-2509, the monthly iteration of Qwen-Image-Edit. To experience the latest model, please visit &lt;a href="https://qwen.ai"&gt;Qwen Chat&lt;/a&gt; and select the "Image Editing" feature. Compared with Qwen-Image-Edit released in August, the main improvements of Qwen-Image-Edit-2509 include:&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.19: We have observed performance misalignments of Qwen-Image-Edit. To ensure optimal results, please update to the latest diffusers commit. Improvements are expected, especially in identity preservation and instruction following.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.18: Weâ€™re excited to announce the open-sourcing of Qwen-Image-Edit! ğŸ‰ Try it out in your local environment with the quick start guide below, or head over to &lt;a href="https://chat.qwen.ai/"&gt;Qwen Chat&lt;/a&gt; or &lt;a href="https://huggingface.co/spaces/Qwen/Qwen-Image-Edit"&gt;Huggingface Demo&lt;/a&gt; to experience the online demo right away! If you enjoy our work, please show your support by giving our repository a star. Your encouragement means a lot to us!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.09: Qwen-Image now supports a variety of LoRA models, such as MajicBeauty LoRA, enabling the generation of highly realistic beauty images. Check out the available weights on &lt;a href="https://modelscope.cn/models/merjic/majicbeauty-qwen1/summary"&gt;ModelScope&lt;/a&gt;. &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/magicbeauty.png#center" alt="" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.05: Qwen-Image is now natively supported in ComfyUI, see &lt;a href="https://blog.comfy.org/p/qwen-image-in-comfyui-new-era-of"&gt;Qwen-Image in ComfyUI: New Era of Text Generation in Images!&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.05: Qwen-Image is now on Qwen Chat. Click &lt;a href="https://chat.qwen.ai/"&gt;Qwen Chat&lt;/a&gt; and choose "Image Generation".&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.05: We released our &lt;a href="https://arxiv.org/abs/2508.02324"&gt;Technical Report&lt;/a&gt; on Arxiv!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.04: We released Qwen-Image weights! Check at &lt;a href="https://huggingface.co/Qwen/Qwen-Image"&gt;Huggingface&lt;/a&gt; and &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image"&gt;ModelScope&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.04: We released Qwen-Image! Check our &lt;a href="https://qwenlm.github.io/blog/qwen-image"&gt;Blog&lt;/a&gt; for more details!&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Due to heavy traffic, if you'd like to experience our demo online, we also recommend visiting DashScope, WaveSpeed, and LibLib. Please find the links below in the community support.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Make sure your transformers&amp;gt;=4.51.3 (Supporting Qwen2.5-VL)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install the latest version of diffusers&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/huggingface/diffusers
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Qwen-Image-2512 (for Text to Image generation, better character realism/texture quality)&lt;/h3&gt; 
&lt;p&gt;We recommand use the latest prompt enhancing tools for Qwen-Image-2512, please check &lt;code&gt;src/examples/tools/prompt_utils_2512.py&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from diffusers import QwenImagePipeline
import torch
# Load the pipeline
if torch.cuda.is_available():
    torch_dtype = torch.bfloat16
    device = "cuda"
else:
    torch_dtype = torch.float32
    device = "cpu"

pipe = QwenImagePipeline.from_pretrained("Qwen/Qwen-Image-2512", torch_dtype=torch_dtype).to(device)

# Generate image
prompt = '''A 20-year-old East Asian girl with delicate, charming features and large, bright brown eyesâ€”expressive and lively, with a cheerful or subtly smiling expression. Her naturally wavy long hair is either loose or tied in twin ponytails. She has fair skin and light makeup accentuating her youthful freshness. She wears a modern, cute dress or relaxed outfit in bright, soft colorsâ€”lightweight fabric, minimalist cut. She stands indoors at an anime convention, surrounded by banners, posters, or stalls. Lighting is typical indoor illuminationâ€”no staged lightingâ€”and the image resembles a casual iPhone snapshot: unpretentious composition, yet brimming with vivid, fresh, youthful charm.'''

negative_prompt = "ä½åˆ†è¾¨ç‡ï¼Œä½ç”»è´¨ï¼Œè‚¢ä½“ç•¸å½¢ï¼Œæ‰‹æŒ‡ç•¸å½¢ï¼Œç”»é¢è¿‡é¥±å’Œï¼Œèœ¡åƒæ„Ÿï¼Œäººè„¸æ— ç»†èŠ‚ï¼Œè¿‡åº¦å…‰æ»‘ï¼Œç”»é¢å…·æœ‰AIæ„Ÿã€‚æ„å›¾æ··ä¹±ã€‚æ–‡å­—æ¨¡ç³Šï¼Œæ‰­æ›²ã€‚"


# Generate with different aspect ratios
aspect_ratios = {
    "1:1": (1328, 1328),
    "16:9": (1664, 928),
    "9:16": (928, 1664),
    "4:3": (1472, 1104),
    "3:4": (1104, 1472),
    "3:2": (1584, 1056),
    "2:3": (1056, 1584),
}

width, height = aspect_ratios["16:9"]

image = pipe(
    prompt=prompt,
    negative_prompt=negative_prompt,
    width=width,
    height=height,
    num_inference_steps=50,
    true_cfg_scale=4.0,
    generator=torch.Generator(device="cuda").manual_seed(42)
).images[0]

image.save("example.png")

&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Qwen-Image-Edit-2511 (for Image Editing, Multiple Image Support and Improved Consistency)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
import torch
from PIL import Image
from diffusers import QwenImageEditPlusPipeline
from io import BytesIO
import requests

pipeline = QwenImageEditPlusPipeline.from_pretrained("Qwen/Qwen-Image-Edit-2511", torch_dtype=torch.bfloat16)
print("pipeline loaded")

pipeline.to('cuda')
pipeline.set_progress_bar_config(disable=None)
image1 = Image.open(BytesIO(requests.get("https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen-Image/edit2511/edit2511input.png").content))
prompt = "è¿™ä¸ªå¥³ç”Ÿçœ‹ç€é¢å‰çš„ç”µè§†å±å¹•ï¼Œå±å¹•ä¸Šé¢å†™ç€â€œé˜¿é‡Œå·´å·´â€"
inputs = {
    "image": [image1],
    "prompt": prompt,
    "generator": torch.manual_seed(0),
    "true_cfg_scale": 4.0,
    "negative_prompt": " ",
    "num_inference_steps": 40,
    "guidance_scale": 1.0,
    "num_images_per_prompt": 1,
}
with torch.inference_mode():
    output = pipeline(**inputs)
    output_image = output.images[0]
    output_image.save("output_image_edit_2511.png")
    print("image saved at", os.path.abspath("output_image_edit_2511.png"))
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt; Previous Version &lt;/summary&gt; 
 &lt;h3&gt;Qwen-Image (for Text-to-Image)&lt;/h3&gt; 
 &lt;p&gt;The following contains a code snippet illustrating how to use the model to generate images based on text prompts:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from diffusers import DiffusionPipeline
import torch

model_name = "Qwen/Qwen-Image"

# Load the pipeline
if torch.cuda.is_available():
    torch_dtype = torch.bfloat16
    device = "cuda"
else:
    torch_dtype = torch.float32
    device = "cpu"

pipe = DiffusionPipeline.from_pretrained(model_name, torch_dtype=torch_dtype).to(device)

positive_magic = {
    "en": ", Ultra HD, 4K, cinematic composition.", # for english prompt
    "zh": ", è¶…æ¸…ï¼Œ4Kï¼Œç”µå½±çº§æ„å›¾." # for chinese prompt
}

# Generate image
prompt = '''A coffee shop entrance features a chalkboard sign reading "Qwen Coffee ğŸ˜Š $2 per cup," with a neon light beside it displaying "é€šä¹‰åƒé—®". Next to it hangs a poster showing a beautiful Chinese woman, and beneath the poster is written "Ï€â‰ˆ3.1415926-53589793-23846264-33832795-02384197".'''

negative_prompt = " " # Recommended if you don't use a negative prompt.


# Generate with different aspect ratios
aspect_ratios = {
    "1:1": (1328, 1328),
    "16:9": (1664, 928),
    "9:16": (928, 1664),
    "4:3": (1472, 1104),
    "3:4": (1104, 1472),
    "3:2": (1584, 1056),
    "2:3": (1056, 1584),
}

width, height = aspect_ratios["16:9"]

image = pipe(
    prompt=prompt + positive_magic["en"],
    negative_prompt=negative_prompt,
    width=width,
    height=height,
    num_inference_steps=50,
    true_cfg_scale=4.0,
    generator=torch.Generator(device="cuda").manual_seed(42)
).images[0]

image.save("example.png")
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Qwen-Image-Edit (for Image Editing, Only Support Single Image Input)&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;[!NOTE] Qwen-Image-Edit-2509 has better consistency than Qwen-Image-Edit; it is recommended to use Qwen-Image-Edit-2509 directlyï¼Œfor both single image input and multiple image inputs.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import os
from PIL import Image
import torch

from diffusers import QwenImageEditPipeline

pipeline = QwenImageEditPipeline.from_pretrained("Qwen/Qwen-Image-Edit")
print("pipeline loaded")
pipeline.to(torch.bfloat16)
pipeline.to("cuda")
pipeline.set_progress_bar_config(disable=None)

image = Image.open("./input.png").convert("RGB")
prompt = "Change the rabbit's color to purple, with a flash light background."


inputs = {
    "image": image,
    "prompt": prompt,
    "generator": torch.manual_seed(0),
    "true_cfg_scale": 4.0,
    "negative_prompt": " ",
    "num_inference_steps": 50,
}

with torch.inference_mode():
    output = pipeline(**inputs)
    output_image = output.images[0]
    output_image.save("output_image_edit.png")
    print("image saved at", os.path.abspath("output_image_edit.png"))
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;[!NOTE] We have observed that editing results may become unstable if prompt rewriting is not used. Therefore, we strongly recommend applying prompt rewriting to improve the stability of editing tasks. For reference, please see our official &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen-Image/main/src/examples/tools/prompt_utils.py"&gt;demo script&lt;/a&gt; or Advanced Usage below, which includes example system prompts. Qwen-Image-Edit is actively evolving with ongoing development. Stay tuned for future enhancements!&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h3&gt;Qwen-Image-Edit-2509 (for Image Editing, Multiple Image Support and Improved Consistency)&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import os
import torch
from PIL import Image
from diffusers import QwenImageEditPlusPipeline
from io import BytesIO
import requests

pipeline = QwenImageEditPlusPipeline.from_pretrained("Qwen/Qwen-Image-Edit-2509", torch_dtype=torch.bfloat16)
print("pipeline loaded")

pipeline.to('cuda')
pipeline.set_progress_bar_config(disable=None)
image1 = Image.open(BytesIO(requests.get("https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/edit2509_1.jpg").content))
image2 = Image.open(BytesIO(requests.get("https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/edit2509_2.jpg").content))
prompt = "The magician bear is on the left, the alchemist bear is on the right, facing each other in the central park square."
inputs = {
    "image": [image1, image2],
    "prompt": prompt,
    "generator": torch.manual_seed(0),
    "true_cfg_scale": 4.0,
    "negative_prompt": " ",
    "num_inference_steps": 40,
    "guidance_scale": 1.0,
    "num_images_per_prompt": 1,
}
with torch.inference_mode():
    output = pipeline(**inputs)
    output_image = output.images[0]
    output_image.save("output_image_edit_plus.png")
    print("image saved at", os.path.abspath("output_image_edit_plus.png"))
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Advanced Usage&lt;/h3&gt; 
&lt;h4&gt;Prompt Enhance for Text-to-Image&lt;/h4&gt; 
&lt;p&gt;For enhanced prompt optimization and multi-language support, we recommend using our official Prompt Enhancement Tool powered by Qwen-Plus .&lt;/p&gt; 
&lt;p&gt;You can integrate it directly into your code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from tools.prompt_utils import rewrite
prompt = rewrite(prompt)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, run the example script from the command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd src
DASHSCOPE_API_KEY=sk-xxxxxxxxxxxxxxxxxxxx python examples/generate_w_prompt_enhance.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Prompt Enhance for Image Edit&lt;/h4&gt; 
&lt;p&gt;For enhanced stability, we recommend using our official Prompt Enhancement Tool powered by Qwen-VL-Max.&lt;/p&gt; 
&lt;p&gt;You can integrate it directly into your code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from tools.prompt_utils import polish_edit_prompt
prompt = polish_edit_prompt(prompt, pil_image)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Deploy Qwen-Image&lt;/h2&gt; 
&lt;p&gt;Qwen-Image supports Multi-GPU API Server for local deployment:&lt;/p&gt; 
&lt;h3&gt;Multi-GPU API Server Pipeline &amp;amp; Usage&lt;/h3&gt; 
&lt;p&gt;The Multi-GPU API Server will start a Gradio-based web interface with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Multi-GPU parallel processing&lt;/li&gt; 
 &lt;li&gt;Queue management for high concurrency&lt;/li&gt; 
 &lt;li&gt;Automatic prompt optimization&lt;/li&gt; 
 &lt;li&gt;Support for multiple aspect ratios&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Configuration via environment variables:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export NUM_GPUS_TO_USE=4          # Number of GPUs to use
export TASK_QUEUE_SIZE=100        # Task queue size
export TASK_TIMEOUT=300           # Task timeout in seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start the gradio demo server, api key for prompt enhance
cd src
DASHSCOPE_API_KEY=sk-xxxxxxxxxxxxxxxxx python examples/demo.py 
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Showcase&lt;/h2&gt; 
&lt;p&gt;For previous showcases, click the following links:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen-Image/main/Qwen-Image.md"&gt;Qwen-Image&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen-Image/main/Qwen-Image-Edit.md"&gt;Qwen-Image-Edit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen-Image/main/Qwen-Image-Edit-2509.md"&gt;Qwen-Image-Edit-2509&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Showcase of Qwen-Image-2512&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Enhanced Huamn Realism&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;In Qwen-Image-2512, human depiction has been substantially refined. Compared to the August release, Qwen-Image-2512 adds significantly richer facial details and better environmental context. For example:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A Chinese female college student, around 20 years old, with a very short haircut that conveys a gentle, artistic vibe. Her hair naturally falls to partially cover her cheeks, projecting a tomboyish yet charming demeanor. She has cool-toned fair skin and delicate features, with a slightly shy yet subtly confident expressionâ€”her mouth crooked in a playful, youthful smirk. She wears an off-shoulder top, revealing one shoulder, with a well-proportioned figure. The image is framed as a close-up selfie: she dominates the foreground, while the background clearly shows her dormitoryâ€”a neatly made bed with white linens on the top bunk, a tidy study desk with organized stationery, and wooden cabinets and drawers. The photo is captured on a smartphone under soft, even ambient lighting, with natural tones, high clarity, and a bright, lively atmosphere full of youthful, everyday energy.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%871.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;For the same prompt, Qwen-Image-2512 yields notably more lifelike facial features, and background objectsâ€”e.g., the desk, stationery, and beddingâ€”are rendered with significantly greater clarity than in Qwen-Image.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A 20-year-old East Asian girl with delicate, charming features and large, bright brown eyesâ€”expressive and lively, with a cheerful or subtly smiling expression. Her naturally wavy long hair is either loose or tied in twin ponytails. She has fair skin and light makeup accentuating her youthful freshness. She wears a modern, cute dress or relaxed outfit in bright, soft colorsâ€”lightweight fabric, minimalist cut. She stands indoors at an anime convention, surrounded by banners, posters, or stalls. Lighting is typical indoor illuminationâ€”no staged lightingâ€”and the image resembles a casual iPhone snapshot: unpretentious composition, yet brimming with vivid, fresh, youthful charm.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%872.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;Here, hair strands serve as a key differentiator: Qwen-Imageâ€™s August version tends to blur them together, losing fine detail, whereas Qwen-Image-2512 renders individual strands with precision, resulting in a more natural and realistic appearance.&lt;/p&gt; 
&lt;p&gt;Another case:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;An East Asian teenage boy, aged 15â€“18, with soft, fluffy black short hair and refined facial contours. His large, warm brown eyes sparkle with energy. His fair skin and sunny, open smile convey an approachable, friendly demeanorâ€”no makeup or blemishes. He wears a blue-and-white summer uniform shirt, slightly unbuttoned, made of thin breathable fabric, with black headphones hanging around his neck. His hands are in his pockets, body leaning slightly forward in a relaxed pose, as if engaged in conversation. Behind him lies a summer school playground: lush green grass and a red rubber track in the foreground, blurred school buildings in the distance, a clear blue sky with fluffy white clouds. The bright, airy lighting evokes a joyful, carefree adolescent atmosphere.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%873.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;In this example, Qwen-Image-2512 better adheres to semantic instructionsâ€”for instance, the prompt specifies â€œbody leaning slightly forward,â€ and Qwen-Image-2512 accurately captures this posture, unlike its predecessor.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;An elderly Chinese couple in their 70s in a clean, organized home kitchen. The woman has a kind face and a warm smile, wearing a patterned apron; the man stands behind her, also smiling, as they both gaze at a steaming pot of buns on the stove. The kitchen is bright and tidy, exuding warmth and harmony. The scene is captured with a wide-angle lens to fully show the subjects and their surroundings.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%874.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;This comparison starkly highlights the gap between the August and December models. The original Qwen-Image struggles to accurately render aged facial features (e.g., wrinkles), resulting in an artificial â€œAI look.â€ In contrast, Qwen-Image-2512 precisely captures age cues, dramatically boosting realism.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Finer Natural Detail&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Qwen-Image-2512â€™s enhanced detail rendering extends beyond humansâ€”to landscapes, wildlife, and more. For instance:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A turquoise river winds through a lush canyon. Thick moss and dense ferns blanket the rocky walls; multiple waterfalls cascade from above, enveloped in mist. At noon, sunlight filters through the dense canopy, dappling the river surface with shimmering light. The atmosphere is humid and fresh, pulsing with primal jungle vitality. No humans, text, or artificial traces present.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%875.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;Side-by-side, Qwen-Image-2512 exhibits superior fidelity in water flow, foliage, and waterfall mistâ€”and renders richer gradation in greens. Another example (wave rendering):&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;At dawn, a thin mist veils the sea. An ancient stone lighthouse stands at the cliffâ€™s edge, its beacon faintly visible through the fog. Black rocks are pounded by waves, sending up bursts of white spray. The sky glows in soft blue-purple hues under cool, hazy lightâ€”evoking solitude and solemn grandeur.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%876.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;Fur detail is another highlightâ€”here, a golden retriever portrait:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;An ultra-realistic close-up of a golden retriever outdoors under soft daylight. Hair is exquisitely detailed: strands distinct, color transitioning naturally from warm gold to light cream, light glinting delicately at the tips; a gentle breeze adds subtle volume. Undercoat is soft and dense; guard hairs are long and well-defined, with visible layering. Eyes are moist, expressive; nose is slightly damp with fine specular highlights. Background is softly blurred to emphasize the dogâ€™s tangible texture and vivid expression.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%877.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;Similarly, texture quality improves in depictions of rugged wildlifeâ€”for example, a male argali sheep:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A male argali stands atop a barren, rocky mountainside. Its coarse, dense grey-brown coat covers a powerful, muscular body. Most striking are its massive, thick, outward-spiraling hornsâ€”a symbol of wild strength. Its gaze is alert and sharp. The background reveals steep alpine terrain: jagged peaks, sparse low vegetation, and abundant sunlightâ€”conveying the harsh yet majestic wilderness and the animalâ€™s resilient vitality.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%878.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Improved Text Rendering&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Qwen-Image-2512 further elevates text renderingâ€”already a strength of the originalâ€”by improving accuracy, layout, and multimodal integration.&lt;/p&gt; 
&lt;p&gt;For instance, this prompt requests a complete PPT slide illustrating Qwen-Imageâ€™s development roadmap (generation and editing tracks):&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;è¿™æ˜¯ä¸€å¼ ç°ä»£é£æ ¼çš„ç§‘æŠ€æ„Ÿå¹»ç¯ç‰‡ï¼Œæ•´ä½“é‡‡ç”¨æ·±è“è‰²æ¸å˜èƒŒæ™¯ã€‚æ ‡é¢˜æ˜¯â€œQwen-Imageå‘å±•å†ç¨‹â€ã€‚ä¸‹æ–¹ä¸€æ¡æ°´å¹³å»¶ä¼¸çš„å‘å…‰æ—¶é—´è½´ï¼Œè½´çº¿ä¸­é—´å†™ç€â€œç”Ÿå›¾è·¯çº¿â€ã€‚ç”±å·¦ä¾§æ·¡è“è‰²æ¸å˜ä¸ºå³ä¾§æ·±ç´«è‰²ï¼Œå¹¶ä»¥ç²¾è‡´çš„ç®­å¤´æ”¶å°¾ã€‚æ—¶é—´è½´ä¸Šæ¯ä¸ªèŠ‚ç‚¹é€šè¿‡è™šçº¿è¿æ¥è‡³ä¸‹æ–¹é†’ç›®çš„è“è‰²åœ†è§’çŸ©å½¢æ—¥æœŸæ ‡ç­¾ï¼Œæ ‡ç­¾å†…ä¸ºæ¸…æ™°ç™½è‰²å­—ä½“ï¼Œä»å·¦å‘å³ä¾æ¬¡å†™ç€ï¼šâ€œ2025å¹´5æœˆ6æ—¥ Qwen-Image é¡¹ç›®å¯åŠ¨â€â€œ2025å¹´8æœˆ4æ—¥ Qwen-Image å¼€æºå‘å¸ƒâ€â€œ2025å¹´12æœˆ31æ—¥ Qwen-Image-2512 å¼€æºå‘å¸ƒâ€ ï¼ˆå‘¨å›´å…‰æ™•æ˜¾è‘—ï¼‰åœ¨ä¸‹æ–¹ä¸€æ¡æ°´å¹³å»¶ä¼¸çš„å‘å…‰æ—¶é—´è½´ï¼Œè½´çº¿ä¸­é—´å†™ç€â€œç¼–è¾‘è·¯çº¿â€ã€‚ç”±å·¦ä¾§æ·¡è“è‰²æ¸å˜ä¸ºå³ä¾§æ·±ç´«è‰²ï¼Œå¹¶ä»¥ç²¾è‡´çš„ç®­å¤´æ”¶å°¾ã€‚æ—¶é—´è½´ä¸Šæ¯ä¸ªèŠ‚ç‚¹é€šè¿‡è™šçº¿è¿æ¥è‡³ä¸‹æ–¹é†’ç›®çš„è“è‰²åœ†è§’çŸ©å½¢æ—¥æœŸæ ‡ç­¾ï¼Œæ ‡ç­¾å†…ä¸ºæ¸…æ™°ç™½è‰²å­—ä½“ï¼Œä»å·¦å‘å³ä¾æ¬¡å†™ç€ï¼šâ€œ2025å¹´8æœˆ18æ—¥ Qwen-Image-Edit å¼€æºå‘å¸ƒâ€â€œ2025å¹´9æœˆ22æ—¥ Qwen-Image-Edit-2509 å¼€æºå‘å¸ƒâ€â€œ2025å¹´12æœˆ19æ—¥ Qwen-Image-Layered å¼€æºå‘å¸ƒâ€â€œ2025å¹´12æœˆ23æ—¥ Qwen-Image-Edit-2511 å¼€æºå‘å¸ƒâ€&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%879.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;We can even generate a before-and-after comparison slide to highlight the leap from â€œAI-blurryâ€ to â€œphotorealisticâ€:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;è¿™æ˜¯ä¸€å¼ ç°ä»£é£æ ¼çš„ç§‘æŠ€æ„Ÿå¹»ç¯ç‰‡ï¼Œæ•´ä½“é‡‡ç”¨æ·±è“è‰²æ¸å˜èƒŒæ™¯ã€‚é¡¶éƒ¨ä¸­å¤®ä¸ºç™½è‰²æ— è¡¬çº¿ç²—ä½“å¤§å­—æ ‡é¢˜â€œQwen-Image-2512é‡ç£…å‘å¸ƒâ€ã€‚ç”»é¢ä¸»ä½“ä¸ºæ¨ªå‘å¯¹æ¯”å›¾ï¼Œè§†è§‰ç„¦ç‚¹é›†ä¸­äºä¸­é—´çš„å‡çº§å¯¹æ¯”åŒºåŸŸã€‚å·¦ä¾§ä¸ºé¢éƒ¨å…‰æ»‘æ²¡æœ‰ä»»ä½•ç»†èŠ‚çš„å¥³æ€§äººåƒï¼Œè´¨æ„Ÿå·®ï¼›å³ä¾§ä¸ºé«˜åº¦å†™å®çš„å¹´è½»å¥³æ€§è‚–åƒï¼Œçš®è‚¤å‘ˆç°çœŸå®æ¯›å­”çº¹ç†ä¸ç»†å¾®å…‰å½±å˜åŒ–ï¼Œå‘ä¸æ ¹æ ¹åˆ†æ˜ï¼Œçœ¼çœ¸é€äº®ï¼Œè¡¨æƒ…è‡ªç„¶ï¼Œæ•´ä½“è´¨æ„Ÿæ¥è¿‘å†™å®æ‘„å½±ã€‚ä¸¤å›¾åƒä¹‹é—´ä»¥ä¸€ä¸ªç»¿è‰²æµçº¿å‹ç®­å¤´é“¾æ¥ã€‚é€ å‹ç§‘æŠ€æ„Ÿåè¶³ï¼Œä¸­éƒ¨æ ‡æ³¨â€œ2512è´¨æ„Ÿå‡çº§â€ï¼Œä½¿ç”¨ç™½è‰²åŠ ç²—å­—ä½“ï¼Œå±…ä¸­æ˜¾ç¤ºã€‚ç®­å¤´ä¸¤ä¾§æœ‰å¾®å¼±å…‰æ™•æ•ˆæœï¼Œå¢å¼ºåŠ¨æ€æ„Ÿã€‚åœ¨å›¾åƒä¸‹æ–¹ï¼Œä»¥ç™½è‰²æ–‡å­—å‘ˆç°ä¸‰è¡Œè¯´æ˜ï¼šâ€œâ— æ›´çœŸå®çš„äººç‰©è´¨æ„Ÿã€‚å¤§å¹…åº¦é™ä½äº†ç”Ÿæˆå›¾ç‰‡çš„AIæ„Ÿï¼Œæå‡äº†å›¾åƒçœŸå®æ€§ â— æ›´ç»†è…»çš„è‡ªç„¶çº¹ç†ã€‚å¤§å¹…åº¦æå‡äº†ç”Ÿæˆå›¾ç‰‡çš„çº¹ç†ç»†èŠ‚ã€‚é£æ™¯å›¾ï¼ŒåŠ¨ç‰©æ¯›å‘åˆ»ç”»æ›´ç»†è…»ã€‚â— æ›´å¤æ‚çš„æ–‡å­—æ¸²æŸ“ã€‚å¤§å¹…æå‡äº†æ–‡å­—æ¸²æŸ“çš„è´¨é‡ã€‚å›¾æ–‡æ··åˆæ¸²æŸ“æ›´å‡†ç¡®ï¼Œæ’ç‰ˆæ›´å¥½â€&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%8710.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;A more complex infographic example:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;è¿™æ˜¯ä¸€å¹…ä¸“ä¸šçº§å·¥ä¸šæŠ€æœ¯ä¿¡æ¯å›¾è¡¨ï¼Œæ•´ä½“é‡‡ç”¨æ·±è“è‰²ç§‘æŠ€æ„ŸèƒŒæ™¯ï¼Œå…‰çº¿å‡åŒ€æŸ”å’Œï¼Œè¥é€ å‡ºå†·é™ã€ç²¾å‡†çš„ç°ä»£å·¥ä¸šæ°›å›´ã€‚ç”»é¢åˆ†ä¸ºå·¦å³ä¸¤å¤§æ¿å—ï¼Œå¸ƒå±€æ¸…æ™°ï¼Œè§†è§‰å±‚æ¬¡åˆ†æ˜ã€‚å·¦ä¾§æ¿å—æ ‡é¢˜ä¸ºâ€œå®é™…å‘ç”Ÿçš„ç°è±¡â€ï¼Œä»¥æµ…è“è‰²åœ†è§’çŸ©å½¢æ¡†çªå‡ºæ˜¾ç¤ºï¼Œå†…éƒ¨æ’åˆ—ä¸‰ä¸ªæ·±è“è‰²æŒ‰é’®å¼æ¡ç›®ï¼Œç¬¬ä¸€ä¸ªæ¡ç›®å±•ç¤ºä¸€å †æ£•è‰²ç²‰æœ«çŠ¶åŸæ–™ä¸Šæ»´è½æ°´æ»´çš„å›¾æ ‡ï¼Œæ–‡å­—ä¸ºâ€œå›¢èš/ç»“å—â€ï¼Œåé¢é…æœ‰ç»¿è‰²å¯¹é’©ï¼›ç¬¬äºŒä¸ªæ¡ç›®ä¸ºä¸€ä¸ªè£…æœ‰è“è‰²æ¶²ä½“å¹¶å†’å‡ºæ°”æ³¡çš„é”¥å½¢ç“¶ï¼Œæ–‡å­—ä¸ºâ€œäº§ç”Ÿæ°”æ³¡/ç¼ºé™·â€ï¼Œåé¢é…æœ‰ç»¿è‰²å¯¹é’©ï¼›ç¬¬ä¸‰ä¸ªæ¡ç›®ä¸ºä¸¤ä¸ªç”Ÿé”ˆçš„é½¿è½®ï¼Œæ–‡å­—ä¸ºâ€œè®¾å¤‡è…èš€/å‚¬åŒ–å‰‚å¤±æ´»â€ï¼Œåé¢é…æœ‰ç»¿è‰²å¯¹é’©ã€‚å³ä¾§æ¿å—æ ‡é¢˜ä¸ºâ€œã€ä¸ä¼šã€‘å‘ç”Ÿçš„ç°è±¡â€ï¼Œä½¿ç”¨ç±³é»„è‰²åœ†è§’çŸ©å½¢æ¡†å‘ˆç°ï¼Œå†…éƒ¨å››ä¸ªæ¡ç›®å‡ç½®äºæ·±ç°è‰²èƒŒæ™¯æ–¹æ¡†ä¸­ã€‚å›¾æ ‡åˆ†åˆ«ä¸ºï¼šä¸€ç»„ç²¾å¯†å•®åˆçš„é‡‘å±é½¿è½®ï¼Œæ–‡å­—ä¸ºâ€œååº”æ•ˆç‡ã€æ˜¾è‘—æé«˜ã€‘â€ï¼Œä¸Šæ–¹è¦†ç›–é†’ç›®çš„çº¢è‰²å‰å·ï¼›ä¸€æ†æ•´é½æ’åˆ—çš„é‡‘å±ç®¡æï¼Œæ–‡å­—ä¸ºâ€œæˆå“å†…éƒ¨ã€ç»å¯¹æ— æ°”æ³¡/å­”éš™ã€‘â€ï¼Œä¸Šæ–¹è¦†ç›–é†’ç›®çš„çº¢è‰²å‰å·ï¼›ä¸€æ¡åšå›ºçš„é‡‘å±é“¾æ¡æ­£åœ¨æ‰¿å—æ‹‰åŠ›ï¼Œæ–‡å­—ä¸ºâ€œææ–™å¼ºåº¦ä¸è€ä¹…æ€§ã€å¾—åˆ°å¢å¼ºã€‘â€ï¼Œä¸Šæ–¹è¦†ç›–é†’ç›®çš„çº¢è‰²å‰å·ï¼›ä¸€å †è…èš€çš„æ‰³æ‰‹ï¼Œæ–‡å­—ä¸ºâ€œåŠ å·¥è¿‡ç¨‹ã€é›¶è…èš€/é›¶å‰¯ååº”é£é™©ã€‘â€ï¼Œä¸Šæ–¹è¦†ç›–é†’ç›®çš„çº¢è‰²å‰å·ã€‚åº•éƒ¨ä¸­å¤®æœ‰ä¸€è¡Œå°å­—æ³¨é‡Šï¼šâ€œæ³¨ï¼šæ°´åˆ†çš„å­˜åœ¨é€šå¸¸ä¼šå¯¼è‡´è´Ÿé¢æˆ–å¹²æ‰°æ€§çš„ç»“æœï¼Œè€Œéç†æƒ³æˆ–å¢å¼ºçš„çŠ¶æ€â€ï¼Œå­—ä½“ä¸ºç™½è‰²ï¼Œæ¸…æ™°å¯è¯»ã€‚æ•´ä½“é£æ ¼ç°ä»£ç®€çº¦ï¼Œé…è‰²å¯¹æ¯”å¼ºçƒˆï¼Œå›¾å½¢ç¬¦å·å‡†ç¡®ä¼ è¾¾æŠ€æœ¯é€»è¾‘ï¼Œé€‚åˆç”¨äºå·¥ä¸šåŸ¹è®­æˆ–ç§‘æ™®æ¼”ç¤ºåœºæ™¯ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%8711.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;Or even a full educational poster:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;è¿™æ˜¯ä¸€å¹…ç”±åäºŒä¸ªåˆ†æ ¼ç»„æˆçš„3Ã—4ç½‘æ ¼å¸ƒå±€çš„å†™å®æ‘„å½±ä½œå“ï¼Œæ•´ä½“å‘ˆç°â€œå¥åº·çš„ä¸€å¤©â€ä¸»é¢˜ï¼Œç”»é¢é£æ ¼ç®€æ´æ¸…æ™°ï¼Œæ¯ä¸€åˆ†æ ¼ç‹¬ç«‹æˆæ™¯åˆç»Ÿä¸€äºç”Ÿæ´»èŠ‚å¥çš„å™äº‹è„‰ç»œã€‚ç¬¬ä¸€è¡Œåˆ†åˆ«æ˜¯â€œ06:00 æ™¨è·‘å”¤é†’èº«ä½“â€ï¼šé¢éƒ¨ç‰¹å†™ï¼Œä¸€ä½å¥³æ€§èº«ç©¿ç°è‰²è¿åŠ¨å¥—è£…ï¼ŒèƒŒæ™¯æ˜¯åˆå‡çš„æœé˜³ä¸è‘±éƒç»¿æ ‘ï¼›â€œ06:30 åŠ¨æ€æ‹‰ä¼¸æ¿€æ´»å…³èŠ‚â€ï¼šå¥³æ€§èº«ç€ç‘œä¼½æœåœ¨é˜³å°åšæ™¨é—´æ‹‰ä¼¸ï¼Œèº«ä½“èˆ’å±•ï¼ŒèƒŒæ™¯ä¸ºæ·¡ç²‰è‰²å¤©ç©ºä¸è¿œå±±è½®å»“ï¼›â€œ07:30 å‡è¡¡è¥å…»æ—©é¤â€ï¼šæ¡Œä¸Šæ‘†æ”¾å…¨éº¦é¢åŒ…ã€ç‰›æ²¹æœå’Œä¸€æ¯æ©™æ±ï¼Œå¥³æ€§å¾®ç¬‘ç€å‡†å¤‡ç”¨é¤ï¼›â€œ08:00 è¡¥æ°´æ¶¦ç‡¥â€ï¼šé€æ˜ç»ç’ƒæ°´æ¯ä¸­æµ®æœ‰æŸ æª¬ç‰‡ï¼Œå¥³æ€§æ‰‹æŒæ°´æ¯è½»å•œï¼Œé˜³å…‰ä»å·¦ä¾§æ–œç…§å…¥å®¤ï¼Œæ¯å£æ°´ç æ»‘è½ï¼›ç¬¬äºŒè¡Œåˆ†åˆ«æ˜¯ï¼šâ€œ09:00 ä¸“æ³¨é«˜æ•ˆå·¥ä½œâ€ï¼šå¥³æ€§ä¸“æ³¨æ•²å‡»é”®ç›˜ï¼Œå±å¹•æ˜¾ç¤ºç®€æ´ç•Œé¢ï¼Œèº«æ—æ”¾æœ‰ä¸€æ¯å’–å•¡ä¸ä¸€ç›†ç»¿æ¤ï¼›â€œ12:00 é™å¿ƒé˜…è¯»æ—¶å…‰â€ï¼šå¥³æ€§ååœ¨ä¹¦æ¡Œå‰ç¿»é˜…çº¸è´¨ä¹¦ç±ï¼Œå°ç¯æ•£å‘æš–å…‰ï¼Œä¹¦é¡µæ³›é»„ï¼Œæ—æ”¾åŠæ¯çº¢èŒ¶ï¼›â€œ12:30 åˆåè½»æ¾æ¼«æ­¥â€ï¼šå¥³æ€§åœ¨æ—è«é“ä¸Šæ¼«æ­¥ï¼Œè„¸éƒ¨ç‰¹å†™ï¼›â€œ15:00 èŒ¶é¦™ä¼´åˆåâ€ï¼šå¥³æ€§ç«¯ç€éª¨ç“·èŒ¶æ¯ç«™åœ¨çª—è¾¹ï¼Œçª—å¤–æ˜¯åŸå¸‚è¡—æ™¯ä¸é£˜åŠ¨äº‘æœµï¼ŒèŒ¶é¦™è¢…è¢…ï¼›ç¬¬ä¸‰è¡Œåˆ†åˆ«æ˜¯ï¼šâ€œ18:00 è¿åŠ¨é‡Šæ”¾å‹åŠ›â€ï¼šå¥èº«æˆ¿å†…ï¼Œå¥³æ€§æ­£åœ¨ç»ƒä¹ ç‘œä¼½ï¼›â€œ19:00 ç¾å‘³æ™šé¤â€ï¼šå¥³æ€§åœ¨å¼€æ”¾å¼å¨æˆ¿ä¸­åˆ‡èœï¼Œç §æ¿ä¸Šæœ‰ç•ªèŒ„ä¸é’æ¤’ï¼Œé”…ä¸­çƒ­æ°”å‡è…¾ï¼Œç¯å…‰æ¸©æš–ï¼›â€œ21:00 å†¥æƒ³åŠ©çœ â€ï¼šå¥³æ€§ç›˜è…¿ååœ¨æŸ”è½¯åœ°æ¯¯ä¸Šå†¥æƒ³ï¼ŒåŒæ‰‹è½»æ”¾è†ä¸Šï¼Œé—­ç›®å®é™ï¼›â€œ21:30 è¿›å…¥ç¡çœ â€ï¼šå¥³æ€§èººåœ¨åºŠä¸Šä¼‘æ¯ã€‚æ•´ä½“é‡‡ç”¨è‡ªç„¶å…‰çº¿ä¸ºä¸»ï¼Œè‰²è°ƒä»¥æš–ç™½ä¸ç±³ç°ä¸ºåŸºè°ƒï¼Œå…‰å½±å±‚æ¬¡åˆ†æ˜ï¼Œç”»é¢å……æ»¡æ¸©é¦¨çš„ç”Ÿæ´»æ°”æ¯ä¸è§„å¾‹çš„èŠ‚å¥æ„Ÿã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%8712.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;These are the core enhancements in this update. We hope you enjoy using Qwen-Image-2512!&lt;/p&gt; 
&lt;h3&gt;Showcase of Qwen-Image-Edit-2511&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Qwen-Image-Edit-2511 Enhances Character Consistency&lt;/strong&gt; In Qwen-Image-Edit-2511, character consistency has been significantly improved. The model can perform imaginative edits based on an input portrait while preserving the identity and visual characteristics of the subject.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%871.JPG#center" alt="" /&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%872.JPG#center" alt="" /&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%873.JPG#center" alt="" /&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%874.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Improved Multi-Person Consistency&lt;/strong&gt; While Qwen-Image-Edit-2509 already improved consistency for single-subject editing, Qwen-Image-Edit-2511 further enhances consistency in multi-person group photosâ€”enabling high-fidelity fusion of two separate person images into a coherent group shot: &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%875.JPG#center" alt="" /&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%876.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Built-in Support for Community-Created LoRAs&lt;/strong&gt; Since Qwen-Image-Editâ€™s release, the community has developed many creative and high-quality LoRAsâ€”greatly expanding its expressive potential. Qwen-Image-Edit-2511 integrates selected popular LoRAs directly into the base model, unlocking their effects without extra tuning.&lt;/p&gt; 
&lt;p&gt;For example, Lighting Enhancement LoRA Realistic lighting control is now achievable out-of-the-box: &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%877.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%878.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;Another example, generating new viewpoints can now be done directly with the base model:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%879.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%8710.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Industrial Design Applications&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Weâ€™ve paid special attention to practical engineering scenariosâ€”for instance, batch industrial product design:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%8711.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%8712.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;â€¦and material replacement for industrial components: &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%8713.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%8714.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Enhanced Geometric Reasoning&lt;/strong&gt; Qwen-Image-Edit-2511 introduces stronger geometric reasoning capabilityâ€”e.g., directly generating auxiliary construction lines for design or annotation purposes:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%8715.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%8716.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;h2&gt;AI Arena&lt;/h2&gt; 
&lt;p&gt;To comprehensively evaluate the general image generation capabilities of Qwen-Image and objectively compare it with state-of-the-art closed-source APIs, we introduce &lt;a href="https://aiarena.alibaba-inc.com"&gt;AI Arena&lt;/a&gt;, an open benchmarking platform built on the Elo rating system. AI Arena provides a fair, transparent, and dynamic environment for model evaluation.&lt;/p&gt; 
&lt;p&gt;In each round, two imagesâ€”generated by randomly selected models from the same promptâ€”are anonymously presented to users for pairwise comparison. Users vote for the better image, and the results are used to update both personal and global leaderboards via the Elo algorithm, enabling developers, researchers, and the public to assess model performance in a robust and data-driven way. AI Arena is now publicly available, welcoming everyone to participate in model evaluations.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/figure_aiarena_website.png" alt="AI Arena" /&gt;&lt;/p&gt; 
&lt;p&gt;The latest leaderboard rankings can be viewed at &lt;a href="https://aiarena.alibaba-inc.com/corpora/arena/leaderboard?arenaType=text2image"&gt;AI Arena Learboard&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you wish to deploy your model on AI Arena and participate in the evaluation, please contact &lt;a href="mailto:weiyue.wy@alibaba-inc.com"&gt;weiyue.wy@alibaba-inc.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Community Support&lt;/h2&gt; 
&lt;h3&gt;Huggingface&lt;/h3&gt; 
&lt;p&gt;Diffusers has supported Qwen-Image since day 0. Support for LoRA and finetuning workflows is currently in development and will be available soon.&lt;/p&gt; 
&lt;h3&gt;ModelScope&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/modelscope/DiffSynth-Studio"&gt;DiffSynth-Studio&lt;/a&gt;&lt;/strong&gt; provides comprehensive support for Qwen-Image, including low-GPU-memory layer-by-layer offload (inference within 4GB VRAM), FP8 quantization, LoRA / full training.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/modelscope/DiffSynth-Engine"&gt;DiffSynth-Engine&lt;/a&gt;&lt;/strong&gt; delivers advanced optimizations for Qwen-Image inference and deployment, including FBCache-based acceleration, classifier-free guidance (CFG) parallel, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://www.modelscope.cn/aigc"&gt;ModelScope AIGC Central&lt;/a&gt;&lt;/strong&gt; provides hands-on experiences on Qwen Image, including: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://www.modelscope.cn/aigc/imageGeneration"&gt;Image Generation&lt;/a&gt;: Generate high fidelity images using the Qwen Image model.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.modelscope.cn/aigc/modelTraining"&gt;LoRA Training&lt;/a&gt;: Easily train Qwen Image LoRAs for personalized concepts.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;SGLang&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;SGLang-Diffusion&lt;/strong&gt; provides day-0 support for Qwen-Image models. To play with &lt;code&gt;Qwen-Image-Edit-2511&lt;/code&gt;, use the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sglang generate --model-path Qwen/Qwen-Image-Edit-2511 --prompt "make the girl in Figure 1 dance with the capybara in Figure 2."  --image-path "https://github.com/lm-sys/lm-sys.github.io/releases/download/test/TI2I_Qwen_Image_Edit_Input.jpg" "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/edit2509_2.jpg"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The output should be like &lt;img src="https://github.com/lm-sys/lm-sys.github.io/releases/download/test/SGLang_Diffusion_Qwen_Image_Edit_2511_example_output.jpg" alt="" /&gt;&lt;/p&gt; 
&lt;h3&gt;WaveSpeedAI&lt;/h3&gt; 
&lt;p&gt;WaveSpeed has deployed Qwen-Image on their platform from day 0, visit their &lt;a href="https://wavespeed.ai/models/wavespeed-ai/qwen-image/text-to-image"&gt;model page&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;LiblibAI&lt;/h3&gt; 
&lt;p&gt;LiblibAI offers native support for Qwen-Image from day 0. Visit their &lt;a href="https://www.liblib.art/modelinfo/c62a103bd98a4246a2334e2d952f7b21?from=sd&amp;amp;versionUuid=75e0be0c93b34dd8baeec9c968013e0c"&gt;community&lt;/a&gt; page for more details and discussions.&lt;/p&gt; 
&lt;h3&gt;Inference Acceleration Method: cache-dit&lt;/h3&gt; 
&lt;p&gt;cache-dit offers cache acceleration support for Qwen-Image with DBCache, TaylorSeer and Cache CFG. Visit their &lt;a href="https://github.com/vipshop/cache-dit/raw/main/examples/pipeline/run_qwen_image.py"&gt;example&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;License Agreement&lt;/h2&gt; 
&lt;p&gt;Qwen-Image is licensed under Apache 2.0.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;We kindly encourage citation of our work if you find it useful.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{wu2025qwenimagetechnicalreport,
      title={Qwen-Image Technical Report}, 
      author={Chenfei Wu and Jiahao Li and Jingren Zhou and Junyang Lin and Kaiyuan Gao and Kun Yan and Sheng-ming Yin and Shuai Bai and Xiao Xu and Yilei Chen and Yuxiang Chen and Zecheng Tang and Zekai Zhang and Zhengyi Wang and An Yang and Bowen Yu and Chen Cheng and Dayiheng Liu and Deqing Li and Hang Zhang and Hao Meng and Hu Wei and Jingyuan Ni and Kai Chen and Kuan Cao and Liang Peng and Lin Qu and Minggang Wu and Peng Wang and Shuting Yu and Tingkun Wen and Wensen Feng and Xiaoxiao Xu and Yi Wang and Yichang Zhang and Yongqiang Zhu and Yujia Wu and Yuxuan Cai and Zenan Liu},
      year={2025},
      eprint={2508.02324},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2508.02324}, 
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contact and Join Us&lt;/h2&gt; 
&lt;p&gt;If you'd like to get in touch with our research team, we'd love to hear from you! Join our &lt;a href="https://discord.gg/z3GAxXZ9Ce"&gt;Discord&lt;/a&gt; or scan the QR code to connect via our &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen-Image/main/assets/wechat.png"&gt;WeChat groups&lt;/a&gt; â€” we're always open to discussion and collaboration.&lt;/p&gt; 
&lt;p&gt;If you have questions about this repository, feedback to share, or want to contribute directly, we welcome your issues and pull requests on GitHub. Your contributions help make Qwen-Image better for everyone.&lt;/p&gt; 
&lt;p&gt;If you're passionate about fundamental research, we're hiring full-time employees (FTEs) and research interns. Don't wait â€” reach out to us at &lt;a href="mailto:fulai.hr@alibaba-inc.com"&gt;fulai.hr@alibaba-inc.com&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#QwenLM/Qwen-Image&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=QwenLM/Qwen-Image&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Shubhamsaboo/awesome-llm-apps</title>
      <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
      <description>&lt;p&gt;Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="http://www.theunwindai.com"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/unwind_black.png" width="900px" alt="Unwind AI" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.linkedin.com/in/shubhamsaboo/"&gt; &lt;img src="https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;amp;style=flat-square" alt="LinkedIn" /&gt; &lt;/a&gt; &lt;a href="https://twitter.com/Saboo_Shubham_"&gt; &lt;img src="https://img.shields.io/twitter/follow/Shubham_Saboo" alt="Twitter" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=de"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=es"&gt;EspaÃ±ol&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=fr"&gt;franÃ§ais&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ja"&gt;æ—¥æœ¬èª&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ko"&gt;í•œêµ­ì–´&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=pt"&gt;PortuguÃªs&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ru"&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=zh"&gt;ä¸­æ–‡&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;ğŸŒŸ Awesome LLM Apps&lt;/h1&gt; 
&lt;p&gt;A curated collection of &lt;strong&gt;Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.&lt;/strong&gt; This repository features LLM apps that use models from &lt;img src="https://cdn.simpleicons.org/openai" alt="openai logo" width="25" height="15" /&gt;&lt;strong&gt;OpenAI&lt;/strong&gt; , &lt;img src="https://cdn.simpleicons.org/anthropic" alt="anthropic logo" width="25" height="15" /&gt;&lt;strong&gt;Anthropic&lt;/strong&gt;, &lt;img src="https://cdn.simpleicons.org/googlegemini" alt="google logo" width="25" height="18" /&gt;&lt;strong&gt;Google&lt;/strong&gt;, &lt;img src="https://cdn.simpleicons.org/x" alt="X logo" width="25" height="15" /&gt;&lt;strong&gt;xAI&lt;/strong&gt; and open-source models like &lt;img src="https://cdn.simpleicons.org/alibabacloud" alt="alibaba logo" width="25" height="15" /&gt;&lt;strong&gt;Qwen&lt;/strong&gt; or &lt;img src="https://cdn.simpleicons.org/meta" alt="meta logo" width="25" height="15" /&gt;&lt;strong&gt;Llama&lt;/strong&gt; that you can run locally on your computer.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/9876" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/9876" alt="Shubhamsaboo%2Fawesome-llm-apps | Trendshift" style="width: 250px; height: 55px;" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;ğŸ¤” Why Awesome LLM Apps?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ’¡ Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.&lt;/li&gt; 
 &lt;li&gt;ğŸ”¥ Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp;amp; RAG.&lt;/li&gt; 
 &lt;li&gt;ğŸ“ Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ™ Thanks to our sponsors&lt;/h2&gt; 
&lt;table align="center" cellpadding="16" cellspacing="12"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt; &lt;a href="https://tsdb.co/shubham-gh" target="_blank" rel="noopener" title="Tiger Data"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsors/tigerdata.png" alt="Tiger Data" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://tsdb.co/shubham-gh" target="_blank" rel="noopener" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Tiger Data MCP &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://github.com/speechmatics/speechmatics-academy" target="_blank" rel="noopener" title="Speechmatics"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsors/speechmatics.png" alt="Speechmatics" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/speechmatics/speechmatics-academy" target="_blank" rel="noopener" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Speechmatics &lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt; &lt;a href="https://okara.ai/?utm_source=oss&amp;amp;utm_medium=sponsorship&amp;amp;utm_campaign=awesome-llm-apps" title="Okara"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsors/okara.png" alt="Okara" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://okara.ai/?utm_source=oss&amp;amp;utm_medium=sponsorship&amp;amp;utm_campaign=awesome-llm-apps" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Okara AI &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://sponsorunwindai.com/" title="Become a Sponsor"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsor_awesome_llm_apps.png" alt="Become a Sponsor" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://sponsorunwindai.com/" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Become a Sponsor &lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;ğŸ“‚ Featured AI Projects&lt;/h2&gt; 
&lt;h3&gt;AI Agents&lt;/h3&gt; 
&lt;h3&gt;ğŸŒ± Starter AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_blog_to_podcast_agent/"&gt;ğŸ™ï¸ AI Blog to Podcast Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_breakup_recovery_agent/"&gt;â¤ï¸â€ğŸ©¹ AI Breakup Recovery Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_data_analysis_agent/"&gt;ğŸ“Š AI Data Analysis Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_medical_imaging_agent/"&gt;ğŸ©» AI Medical Imaging Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_meme_generator_agent_browseruse/"&gt;ğŸ˜‚ AI Meme Generator Agent (Browser)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_music_generator_agent/"&gt;ğŸµ AI Music Generator Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_travel_agent/"&gt;ğŸ›« AI Travel Agent (Local &amp;amp; Cloud)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/gemini_multimodal_agent_demo/"&gt;âœ¨ Gemini Multimodal Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/mixture_of_agents/"&gt;ğŸ”„ Mixture of Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/xai_finance_agent/"&gt;ğŸ“Š xAI Finance Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/opeani_research_agent/"&gt;ğŸ” OpenAI Research Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/web_scrapping_ai_agent/"&gt;ğŸ•¸ï¸ Web Scraping AI Agent (Local &amp;amp; Cloud SDK)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸš€ Advanced AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_home_renovation_agent"&gt;ğŸšï¸ ğŸŒ AI Home Renovation Agent with Nano Banana Pro&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_deep_research_agent/"&gt;ğŸ” AI Deep Research Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_vc_due_diligence_agent_team"&gt;ğŸ“Š AI VC Due Diligence Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/research_agent_gemini_interaction_api"&gt;ğŸ”¬ AI Research Planner &amp;amp; Executor (Google Interactions API)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_consultant_agent"&gt;ğŸ¤ AI Consultant Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_system_architect_r1/"&gt;ğŸ—ï¸ AI System Architect Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/"&gt;ğŸ’° AI Financial Coach Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_movie_production_agent/"&gt;ğŸ¬ AI Movie Production Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_investment_agent/"&gt;ğŸ“ˆ AI Investment Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/"&gt;ğŸ‹ï¸â€â™‚ï¸ AI Health &amp;amp; Fitness Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/product_launch_intelligence_agent"&gt;ğŸš€ AI Product Launch Intelligence Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_journalist_agent/"&gt;ğŸ—ï¸ AI Journalist Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/"&gt;ğŸ§  AI Mental Wellbeing Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_meeting_agent/"&gt;ğŸ“‘ AI Meeting Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_Self-Evolving_agent/"&gt;ğŸ§¬ AI Self-Evolving Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_sales_intelligence_agent_team"&gt;ğŸ‘¨ğŸ»â€ğŸ’¼ AI Sales Intelligence Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_news_and_podcast_agents/"&gt;ğŸ§ AI Social Media News and Podcast Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ® Autonomous Game Playing Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/"&gt;ğŸ® AI 3D Pygame Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/"&gt;â™œ AI Chess Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/"&gt;ğŸ² AI Tic-Tac-Toe Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ¤ Multi-agent Teams&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/"&gt;ğŸ§² AI Competitor Intelligence Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/"&gt;ğŸ’² AI Finance Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/"&gt;ğŸ¨ AI Game Design Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/"&gt;ğŸ‘¨â€âš–ï¸ AI Legal Agent Team (Cloud &amp;amp; Local)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/"&gt;ğŸ’¼ AI Recruitment Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_real_estate_agent_team"&gt;ğŸ  AI Real Estate Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/"&gt;ğŸ‘¨â€ğŸ’¼ AI Services Agency (CrewAI)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/"&gt;ğŸ‘¨â€ğŸ« AI Teaching Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/"&gt;ğŸ’» Multimodal Coding Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/"&gt;âœ¨ Multimodal Design Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_uiux_feedback_agent_team/"&gt;ğŸ¨ ğŸŒ Multimodal UI/UX Feedback Agent Team with Nano Banana&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_travel_planner_agent_team/"&gt;ğŸŒ AI Travel Planner Agent Team&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ—£ï¸ Voice AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/ai_audio_tour_agent/"&gt;ğŸ—£ï¸ AI Audio Tour Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/customer_support_voice_agent/"&gt;ğŸ“ Customer Support Voice Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/voice_rag_openaisdk/"&gt;ğŸ”Š Voice RAG Agent (OpenAI SDK)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/akshayaggarwal99/jarvis-ai-assistant"&gt;ğŸ™ï¸ OpenSource Voice Dictation Agent (like Wispr Flow&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;img src="https://cdn.simpleicons.org/modelcontextprotocol" alt="mcp logo" width="25" height="20" /&gt; MCP AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/browser_mcp_agent/"&gt;â™¾ï¸ Browser MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/github_mcp_agent/"&gt;ğŸ™ GitHub MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/notion_mcp_agent"&gt;ğŸ“‘ Notion MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/ai_travel_planner_mcp_agent_team"&gt;ğŸŒ AI Travel Planner MCP Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ“€ RAG (Retrieval Augmented Generation)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/agentic_rag_embedding_gemma"&gt;ğŸ”¥ Agentic RAG with Embedding Gemma&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/agentic_rag_with_reasoning/"&gt;ğŸ§ Agentic RAG with Reasoning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/ai_blog_search/"&gt;ğŸ“° AI Blog Search (RAG)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/autonomous_rag/"&gt;ğŸ” Autonomous RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/contextualai_rag_agent/"&gt;ğŸ”„ Contextual AI RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/corrective_rag/"&gt;ğŸ”„ Corrective RAG (CRAG)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/deepseek_local_rag_agent/"&gt;ğŸ‹ Deepseek Local RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/gemini_agentic_rag/"&gt;ğŸ¤” Gemini Agentic RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/hybrid_search_rag/"&gt;ğŸ‘€ Hybrid Search RAG (Cloud)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/llama3.1_local_rag/"&gt;ğŸ”„ Llama 3.1 Local RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/local_hybrid_search_rag/"&gt;ğŸ–¥ï¸ Local Hybrid Search RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/local_rag_agent/"&gt;ğŸ¦™ Local RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag-as-a-service/"&gt;ğŸ§© RAG-as-a-Service&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_agent_cohere/"&gt;âœ¨ RAG Agent with Cohere&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_chain/"&gt;â›“ï¸ Basic RAG Chain&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_database_routing/"&gt;ğŸ“  RAG with Database Routing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/vision_rag/"&gt;ğŸ–¼ï¸ Vision RAG&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ’¾ LLM Apps with Memory Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory/"&gt;ğŸ’¾ AI ArXiv Agent with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/ai_travel_agent_memory/"&gt;ğŸ›©ï¸ AI Travel Agent with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/llama3_stateful_chat/"&gt;ğŸ’¬ Llama3 Stateful Chat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/llm_app_personalized_memory/"&gt;ğŸ“ LLM App with Personalized Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/local_chatgpt_with_memory/"&gt;ğŸ—„ï¸ Local ChatGPT Clone with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/multi_llm_memory/"&gt;ğŸ§  Multi-LLM Application with Shared Memory&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ’¬ Chat with X Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_github/"&gt;ğŸ’¬ Chat with GitHub (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_gmail/"&gt;ğŸ“¨ Chat with Gmail&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_pdf/"&gt;ğŸ“„ Chat with PDF (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_research_papers/"&gt;ğŸ“š Chat with Research Papers (ArXiv) (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_substack/"&gt;ğŸ“ Chat with Substack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_youtube_videos/"&gt;ğŸ“½ï¸ Chat with YouTube Videos&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ¯ LLM Optimization Tools&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_optimization_tools/toonify_token_optimization/"&gt;ğŸ¯ Toonify Token Optimization&lt;/a&gt; - Reduce LLM API costs by 30-60% using TOON format&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ”§ LLM Fine-tuning Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;img src="https://cdn.simpleicons.org/google" alt="google logo" width="20" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_finetuning_tutorials/gemma3_finetuning/"&gt;Gemma 3 Fine-tuning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;img src="https://cdn.simpleicons.org/meta" alt="meta logo" width="25" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_finetuning_tutorials/llama3.2_finetuning/"&gt;Llama 3.2 Fine-tuning&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ§‘â€ğŸ« AI Agent Framework Crash Course&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://cdn.simpleicons.org/google" alt="google logo" width="25" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/ai_agent_framework_crash_course/google_adk_crash_course/"&gt;Google ADK Crash Course&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Starter agent; modelâ€‘agnostic (OpenAI, Claude)&lt;/li&gt; 
 &lt;li&gt;Structured outputs (Pydantic)&lt;/li&gt; 
 &lt;li&gt;Tools: builtâ€‘in, function, thirdâ€‘party, MCP tools&lt;/li&gt; 
 &lt;li&gt;Memory; callbacks; Plugins&lt;/li&gt; 
 &lt;li&gt;Simple multiâ€‘agent; Multiâ€‘agent patterns&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://cdn.simpleicons.org/openai" alt="openai logo" width="25" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/ai_agent_framework_crash_course/openai_sdk_crash_course/"&gt;OpenAI Agents SDK Crash Course&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Starter agent; function calling; structured outputs&lt;/li&gt; 
 &lt;li&gt;Tools: builtâ€‘in, function, thirdâ€‘party integrations&lt;/li&gt; 
 &lt;li&gt;Memory; callbacks; evaluation&lt;/li&gt; 
 &lt;li&gt;Multiâ€‘agent patterns; agent handoffs&lt;/li&gt; 
 &lt;li&gt;Swarm orchestration; routing logic&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸš€ Getting Started&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the repository&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Navigate to the desired project directory&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd awesome-llm-apps/starter_ai_agents/ai_travel_agent
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install the required dependencies&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Follow the project-specific instructions&lt;/strong&gt; in each project's &lt;code&gt;README.md&lt;/code&gt; file to set up and run the app.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;&lt;img src="https://cdn.simpleicons.org/github" alt="github logo" width="25" height="20" /&gt; Thank You, Community, for the Support! ğŸ™&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;ğŸŒŸ &lt;strong&gt;Donâ€™t miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Aider-AI/aider</title>
      <link>https://github.com/Aider-AI/aider</link>
      <description>&lt;p&gt;aider is AI pair programming in your terminal&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://aider.chat/"&gt;&lt;img src="https://aider.chat/assets/logo.svg?sanitize=true" alt="Aider Logo" width="300" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h1 align="center"&gt; AI Pair Programming in Your Terminal &lt;/h1&gt; 
&lt;p align="center"&gt; Aider lets you pair program with LLMs to start a new project or build on your existing codebase. &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://aider.chat/assets/screencast.svg?sanitize=true" alt="aider screencast" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;!--[[[cog
from scripts.homepage import get_badges_md
text = get_badges_md()
cog.out(text)
]]]--&gt; &lt;a href="https://github.com/Aider-AI/aider/stargazers"&gt;&lt;img alt="GitHub Stars" title="Total number of GitHub stars the Aider project has received" src="https://img.shields.io/github/stars/Aider-AI/aider?style=flat-square&amp;amp;logo=github&amp;amp;color=f1c40f&amp;amp;labelColor=555555" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/aider-chat/"&gt;&lt;img alt="PyPI Downloads" title="Total number of installations via pip from PyPI" src="https://img.shields.io/badge/ğŸ“¦%20Installs-4.1M-2ecc71?style=flat-square&amp;amp;labelColor=555555" /&gt;&lt;/a&gt; &lt;img alt="Tokens per week" title="Number of tokens processed weekly by Aider users" src="https://img.shields.io/badge/ğŸ“ˆ%20Tokens%2Fweek-15B-3498db?style=flat-square&amp;amp;labelColor=555555" /&gt; &lt;a href="https://openrouter.ai/#options-menu"&gt;&lt;img alt="OpenRouter Ranking" title="Aider's ranking among applications on the OpenRouter platform" src="https://img.shields.io/badge/ğŸ†%20OpenRouter-Top%2020-9b59b6?style=flat-square&amp;amp;labelColor=555555" /&gt;&lt;/a&gt; &lt;a href="https://aider.chat/HISTORY.html"&gt;&lt;img alt="Singularity" title="Percentage of the new code in Aider's last release written by Aider itself" src="https://img.shields.io/badge/ğŸ”„%20Singularity-88%25-e74c3c?style=flat-square&amp;amp;labelColor=555555" /&gt;&lt;/a&gt; 
 &lt;!--[[[end]]]--&gt; &lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;h3&gt;&lt;a href="https://aider.chat/docs/llms.html"&gt;Cloud and local LLMs&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aider.chat/docs/llms.html"&gt;&lt;img src="https://aider.chat/assets/icons/brain.svg?sanitize=true" width="32" height="32" align="left" valign="middle" style="margin-right:10px" /&gt;&lt;/a&gt; Aider works best with Claude 3.7 Sonnet, DeepSeek R1 &amp;amp; Chat V3, OpenAI o1, o3-mini &amp;amp; GPT-4o, but can connect to almost any LLM, including local models.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h3&gt;&lt;a href="https://aider.chat/docs/repomap.html"&gt;Maps your codebase&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aider.chat/docs/repomap.html"&gt;&lt;img src="https://aider.chat/assets/icons/map-outline.svg?sanitize=true" width="32" height="32" align="left" valign="middle" style="margin-right:10px" /&gt;&lt;/a&gt; Aider makes a map of your entire codebase, which helps it work well in larger projects.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h3&gt;&lt;a href="https://aider.chat/docs/languages.html"&gt;100+ code languages&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aider.chat/docs/languages.html"&gt;&lt;img src="https://aider.chat/assets/icons/code-tags.svg?sanitize=true" width="32" height="32" align="left" valign="middle" style="margin-right:10px" /&gt;&lt;/a&gt; Aider works with most popular programming languages: python, javascript, rust, ruby, go, cpp, php, html, css, and dozens more.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h3&gt;&lt;a href="https://aider.chat/docs/git.html"&gt;Git integration&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aider.chat/docs/git.html"&gt;&lt;img src="https://aider.chat/assets/icons/source-branch.svg?sanitize=true" width="32" height="32" align="left" valign="middle" style="margin-right:10px" /&gt;&lt;/a&gt; Aider automatically commits changes with sensible commit messages. Use familiar git tools to easily diff, manage and undo AI changes.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h3&gt;&lt;a href="https://aider.chat/docs/usage/watch.html"&gt;Use in your IDE&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aider.chat/docs/usage/watch.html"&gt;&lt;img src="https://aider.chat/assets/icons/monitor.svg?sanitize=true" width="32" height="32" align="left" valign="middle" style="margin-right:10px" /&gt;&lt;/a&gt; Use aider from within your favorite IDE or editor. Ask for changes by adding comments to your code and aider will get to work.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h3&gt;&lt;a href="https://aider.chat/docs/usage/images-urls.html"&gt;Images &amp;amp; web pages&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aider.chat/docs/usage/images-urls.html"&gt;&lt;img src="https://aider.chat/assets/icons/image-multiple.svg?sanitize=true" width="32" height="32" align="left" valign="middle" style="margin-right:10px" /&gt;&lt;/a&gt; Add images and web pages to the chat to provide visual context, screenshots, reference docs, etc.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h3&gt;&lt;a href="https://aider.chat/docs/usage/voice.html"&gt;Voice-to-code&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aider.chat/docs/usage/voice.html"&gt;&lt;img src="https://aider.chat/assets/icons/microphone.svg?sanitize=true" width="32" height="32" align="left" valign="middle" style="margin-right:10px" /&gt;&lt;/a&gt; Speak with aider about your code! Request new features, test cases or bug fixes using your voice and let aider implement the changes.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h3&gt;&lt;a href="https://aider.chat/docs/usage/lint-test.html"&gt;Linting &amp;amp; testing&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aider.chat/docs/usage/lint-test.html"&gt;&lt;img src="https://aider.chat/assets/icons/check-all.svg?sanitize=true" width="32" height="32" align="left" valign="middle" style="margin-right:10px" /&gt;&lt;/a&gt; Automatically lint and test your code every time aider makes changes. Aider can fix problems detected by your linters and test suites.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h3&gt;&lt;a href="https://aider.chat/docs/usage/copypaste.html"&gt;Copy/paste to web chat&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aider.chat/docs/usage/copypaste.html"&gt;&lt;img src="https://aider.chat/assets/icons/content-copy.svg?sanitize=true" width="32" height="32" align="left" valign="middle" style="margin-right:10px" /&gt;&lt;/a&gt; Work with any LLM via its web chat interface. Aider streamlines copy/pasting code context and edits back and forth with a browser.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m pip install aider-install
aider-install

# Change directory into your codebase
cd /to/your/project

# DeepSeek
aider --model deepseek --api-key deepseek=&amp;lt;key&amp;gt;

# Claude 3.7 Sonnet
aider --model sonnet --api-key anthropic=&amp;lt;key&amp;gt;

# o3-mini
aider --model o3-mini --api-key openai=&amp;lt;key&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href="https://aider.chat/docs/install.html"&gt;installation instructions&lt;/a&gt; and &lt;a href="https://aider.chat/docs/usage.html"&gt;usage documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;More Information&lt;/h2&gt; 
&lt;h3&gt;Documentation&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aider.chat/docs/install.html"&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aider.chat/docs/usage.html"&gt;Usage Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aider.chat/docs/usage/tutorials.html"&gt;Tutorial Videos&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aider.chat/docs/llms.html"&gt;Connecting to LLMs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aider.chat/docs/config.html"&gt;Configuration Options&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aider.chat/docs/troubleshooting.html"&gt;Troubleshooting&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aider.chat/docs/faq.html"&gt;FAQ&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Community &amp;amp; Resources&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aider.chat/docs/leaderboards/"&gt;LLM Leaderboards&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Aider-AI/aider"&gt;GitHub Repository&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/Y7X7bhMQFV"&gt;Discord Community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aider.chat/HISTORY.html"&gt;Release notes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aider.chat/blog/"&gt;Blog&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Kind Words From Users&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;em&gt;"My life has changed... Aider... It's going to rock your world."&lt;/em&gt; â€” &lt;a href="https://x.com/esrtweet/status/1910809356381413593"&gt;Eric S. Raymond on X&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"The best free open source AI coding assistant."&lt;/em&gt; â€” &lt;a href="https://youtu.be/YALpX8oOn78"&gt;IndyDevDan on YouTube&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"The best AI coding assistant so far."&lt;/em&gt; â€” &lt;a href="https://www.youtube.com/watch?v=df8afeb1FY8"&gt;Matthew Berman on YouTube&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Aider ... has easily quadrupled my coding productivity."&lt;/em&gt; â€” &lt;a href="https://news.ycombinator.com/item?id=36212100"&gt;SOLAR_FIELDS on Hacker News&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"It's a cool workflow... Aider's ergonomics are perfect for me."&lt;/em&gt; â€” &lt;a href="https://news.ycombinator.com/item?id=38185326"&gt;qup on Hacker News&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"It's really like having your senior developer live right in your Git repo - truly amazing!"&lt;/em&gt; â€” &lt;a href="https://github.com/Aider-AI/aider/issues/124"&gt;rappster on GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"What an amazing tool. It's incredible."&lt;/em&gt; â€” &lt;a href="https://github.com/Aider-AI/aider/issues/6#issue-1722897858"&gt;valyagolev on GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Aider is such an astounding thing!"&lt;/em&gt; â€” &lt;a href="https://github.com/Aider-AI/aider/issues/82#issuecomment-1631876700"&gt;cgrothaus on GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"It was WAY faster than I would be getting off the ground and making the first few working versions."&lt;/em&gt; â€” &lt;a href="https://twitter.com/d_feldman/status/1662295077387923456"&gt;Daniel Feldman on X&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"THANK YOU for Aider! It really feels like a glimpse into the future of coding."&lt;/em&gt; â€” &lt;a href="https://news.ycombinator.com/item?id=38205643"&gt;derwiki on Hacker News&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"It's just amazing. It is freeing me to do things I felt were out my comfort zone before."&lt;/em&gt; â€” &lt;a href="https://discord.com/channels/1131200896827654144/1174002618058678323/1174084556257775656"&gt;Dougie on Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"This project is stellar."&lt;/em&gt; â€” &lt;a href="https://github.com/Aider-AI/aider/issues/112#issuecomment-1637429008"&gt;funkytaco on GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Amazing project, definitely the best AI coding assistant I've used."&lt;/em&gt; â€” &lt;a href="https://github.com/Aider-AI/aider/issues/84"&gt;joshuavial on GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"I absolutely love using Aider ... It makes software development feel so much lighter as an experience."&lt;/em&gt; â€” &lt;a href="https://discord.com/channels/1131200896827654144/1133421607499595858/1229689636012691468"&gt;principalideal0 on Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"I have been recovering from ... surgeries ... aider ... has allowed me to continue productivity."&lt;/em&gt; â€” &lt;a href="https://www.reddit.com/r/OpenAI/s/nmNwkHy1zG"&gt;codeninja on Reddit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"I am an aider addict. I'm getting so much more work done, but in less time."&lt;/em&gt; â€” &lt;a href="https://discord.com/channels/1131200896827654144/1131200896827654149/1135913253483069470"&gt;dandandan on Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Aider... blows everything else out of the water hands down, there's no competition whatsoever."&lt;/em&gt; â€” &lt;a href="https://discord.com/channels/1131200896827654144/1131200896827654149/1178736602797846548"&gt;SystemSculpt on Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Aider is amazing, coupled with Sonnet 3.5 it's quite mind blowing."&lt;/em&gt; â€” &lt;a href="https://discord.com/channels/1131200896827654144/1133060684540813372/1262374225298198548"&gt;Josh Dingus on Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Hands down, this is the best AI coding assistant tool so far."&lt;/em&gt; â€” &lt;a href="https://www.youtube.com/watch?v=MPYFPvxfGZs"&gt;IndyDevDan on YouTube&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"[Aider] changed my daily coding workflows. It's mind-blowing how ...(it)... can change your life."&lt;/em&gt; â€” &lt;a href="https://discord.com/channels/1131200896827654144/1131200896827654149/1258453375620747264"&gt;maledorak on Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Best agent for actual dev work in existing codebases."&lt;/em&gt; â€” &lt;a href="https://twitter.com/NickADobos/status/1690408967963652097?s=20"&gt;Nick Dobos on X&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"One of my favorite pieces of software. Blazing trails on new paradigms!"&lt;/em&gt; â€” &lt;a href="https://x.com/chris65536/status/1905053299251798432"&gt;Chris Wall on X&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Aider has been revolutionary for me and my work."&lt;/em&gt; â€” &lt;a href="https://x.com/starryhopeblog/status/1904985812137132056"&gt;Starry Hope on X&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Try aider! One of the best ways to vibe code."&lt;/em&gt; â€” &lt;a href="https://x.com/Chris65536/status/1905053418961391929"&gt;Chris Wall on X&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Freaking love Aider."&lt;/em&gt; â€” &lt;a href="https://news.ycombinator.com/item?id=44035015"&gt;hztar on Hacker News&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Aider is hands down the best. And it's free and opensource."&lt;/em&gt; â€” &lt;a href="https://www.reddit.com/r/ChatGPTCoding/comments/1ik16y6/whats_your_take_on_aider/mbip39n/"&gt;AriyaSavakaLurker on Reddit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Aider is also my best friend."&lt;/em&gt; â€” &lt;a href="https://www.reddit.com/r/ChatGPTCoding/comments/1heuvuo/aider_vs_cline_vs_windsurf_vs_cursor/m27dcnb/"&gt;jzn21 on Reddit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Try Aider, it's worth it."&lt;/em&gt; â€” &lt;a href="https://www.reddit.com/r/ChatGPTCoding/comments/1heuvuo/aider_vs_cline_vs_windsurf_vs_cursor/m27cp99/"&gt;jorgejhms on Reddit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"I like aider :)"&lt;/em&gt; â€” &lt;a href="https://x.com/ccui42/status/1904965344999145698"&gt;Chenwei Cui on X&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Aider is the precision tool of LLM code gen... Minimal, thoughtful and capable of surgical changes ... while keeping the developer in control."&lt;/em&gt; â€” &lt;a href="https://x.com/rsweetland/status/1904963807237259586"&gt;Reilly Sweetland on X&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Cannot believe aider vibe coded a 650 LOC feature across service and cli today in 1 shot."&lt;/em&gt; - &lt;a href="https://discord.com/channels/1131200896827654144/1131200896827654149/1355675042259796101"&gt;autopoietist on Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Oh no the secret is out! Yes, Aider is the best coding tool around. I highly, highly recommend it to anyone."&lt;/em&gt; â€” &lt;a href="https://x.com/jodavaho/status/1911154899057795218"&gt;Joshua D Vander Hook on X&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"thanks to aider, i have started and finished three personal projects within the last two days"&lt;/em&gt; â€” &lt;a href="https://x.com/anitaheeder/status/1908338609645904160"&gt;joseph stalzyn on X&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Been using aider as my daily driver for over a year ... I absolutely love the tool, like beyond words."&lt;/em&gt; â€” &lt;a href="https://discord.com/channels/1131200896827654144/1273248471394291754/1356727448372252783"&gt;koleok on Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Aider ... is the tool to benchmark against."&lt;/em&gt; â€” &lt;a href="https://news.ycombinator.com/item?id=43930201"&gt;BeetleB on Hacker News&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"aider is really cool"&lt;/em&gt; â€” &lt;a href="https://x.com/yacineMTB/status/1911224442430124387"&gt;kache on X&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>SYSTRAN/faster-whisper</title>
      <link>https://github.com/SYSTRAN/faster-whisper</link>
      <description>&lt;p&gt;Faster Whisper transcription with CTranslate2&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://github.com/SYSTRAN/faster-whisper/actions?query=workflow%3ACI"&gt;&lt;img src="https://github.com/SYSTRAN/faster-whisper/workflows/CI/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/faster-whisper"&gt;&lt;img src="https://badge.fury.io/py/faster-whisper.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Faster Whisper transcription with CTranslate2&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;faster-whisper&lt;/strong&gt; is a reimplementation of OpenAI's Whisper model using &lt;a href="https://github.com/OpenNMT/CTranslate2/"&gt;CTranslate2&lt;/a&gt;, which is a fast inference engine for Transformer models.&lt;/p&gt; 
&lt;p&gt;This implementation is up to 4 times faster than &lt;a href="https://github.com/openai/whisper"&gt;openai/whisper&lt;/a&gt; for the same accuracy while using less memory. The efficiency can be further improved with 8-bit quantization on both CPU and GPU.&lt;/p&gt; 
&lt;h2&gt;Benchmark&lt;/h2&gt; 
&lt;h3&gt;Whisper&lt;/h3&gt; 
&lt;p&gt;For reference, here's the time and memory usage that are required to transcribe &lt;a href="https://www.youtube.com/watch?v=0u7tTptBo9I"&gt;&lt;strong&gt;13 minutes&lt;/strong&gt;&lt;/a&gt; of audio using different implementations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/openai/whisper"&gt;openai/whisper&lt;/a&gt;@&lt;a href="https://github.com/openai/whisper/tree/v20240930"&gt;v20240930&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ggerganov/whisper.cpp"&gt;whisper.cpp&lt;/a&gt;@&lt;a href="https://github.com/ggerganov/whisper.cpp/tree/v1.7.2"&gt;v1.7.2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/transformers"&gt;transformers&lt;/a&gt;@&lt;a href="https://github.com/huggingface/transformers/tree/v4.46.3"&gt;v4.46.3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/SYSTRAN/faster-whisper"&gt;faster-whisper&lt;/a&gt;@&lt;a href="https://github.com/SYSTRAN/faster-whisper/tree/v1.1.0"&gt;v1.1.0&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Large-v2 model on GPU&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Implementation&lt;/th&gt; 
   &lt;th&gt;Precision&lt;/th&gt; 
   &lt;th&gt;Beam size&lt;/th&gt; 
   &lt;th&gt;Time&lt;/th&gt; 
   &lt;th&gt;VRAM Usage&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;openai/whisper&lt;/td&gt; 
   &lt;td&gt;fp16&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;2m23s&lt;/td&gt; 
   &lt;td&gt;4708MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;whisper.cpp (Flash Attention)&lt;/td&gt; 
   &lt;td&gt;fp16&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;1m05s&lt;/td&gt; 
   &lt;td&gt;4127MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;transformers (SDPA)[^1]&lt;/td&gt; 
   &lt;td&gt;fp16&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;1m52s&lt;/td&gt; 
   &lt;td&gt;4960MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;faster-whisper&lt;/td&gt; 
   &lt;td&gt;fp16&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;1m03s&lt;/td&gt; 
   &lt;td&gt;4525MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;faster-whisper (&lt;code&gt;batch_size=8&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;fp16&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;17s&lt;/td&gt; 
   &lt;td&gt;6090MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;faster-whisper&lt;/td&gt; 
   &lt;td&gt;int8&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;59s&lt;/td&gt; 
   &lt;td&gt;2926MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;faster-whisper (&lt;code&gt;batch_size=8&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;int8&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;16s&lt;/td&gt; 
   &lt;td&gt;4500MB&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;distil-whisper-large-v3 model on GPU&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Implementation&lt;/th&gt; 
   &lt;th&gt;Precision&lt;/th&gt; 
   &lt;th&gt;Beam size&lt;/th&gt; 
   &lt;th&gt;Time&lt;/th&gt; 
   &lt;th&gt;YT Commons WER&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;transformers (SDPA) (&lt;code&gt;batch_size=16&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;fp16&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;46m12s&lt;/td&gt; 
   &lt;td&gt;14.801&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;faster-whisper (&lt;code&gt;batch_size=16&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;fp16&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;25m50s&lt;/td&gt; 
   &lt;td&gt;13.527&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;em&gt;GPU Benchmarks are Executed with CUDA 12.4 on a NVIDIA RTX 3070 Ti 8GB.&lt;/em&gt; [^1]: transformers OOM for any batch size &amp;gt; 1&lt;/p&gt; 
&lt;h3&gt;Small model on CPU&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Implementation&lt;/th&gt; 
   &lt;th&gt;Precision&lt;/th&gt; 
   &lt;th&gt;Beam size&lt;/th&gt; 
   &lt;th&gt;Time&lt;/th&gt; 
   &lt;th&gt;RAM Usage&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;openai/whisper&lt;/td&gt; 
   &lt;td&gt;fp32&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;6m58s&lt;/td&gt; 
   &lt;td&gt;2335MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;whisper.cpp&lt;/td&gt; 
   &lt;td&gt;fp32&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;2m05s&lt;/td&gt; 
   &lt;td&gt;1049MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;whisper.cpp (OpenVINO)&lt;/td&gt; 
   &lt;td&gt;fp32&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;1m45s&lt;/td&gt; 
   &lt;td&gt;1642MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;faster-whisper&lt;/td&gt; 
   &lt;td&gt;fp32&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;2m37s&lt;/td&gt; 
   &lt;td&gt;2257MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;faster-whisper (&lt;code&gt;batch_size=8&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;fp32&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;1m06s&lt;/td&gt; 
   &lt;td&gt;4230MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;faster-whisper&lt;/td&gt; 
   &lt;td&gt;int8&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;1m42s&lt;/td&gt; 
   &lt;td&gt;1477MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;faster-whisper (&lt;code&gt;batch_size=8&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;int8&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;51s&lt;/td&gt; 
   &lt;td&gt;3608MB&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;em&gt;Executed with 8 threads on an Intel Core i7-12700K.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.9 or greater&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Unlike openai-whisper, FFmpeg does &lt;strong&gt;not&lt;/strong&gt; need to be installed on the system. The audio is decoded with the Python library &lt;a href="https://github.com/PyAV-Org/PyAV"&gt;PyAV&lt;/a&gt; which bundles the FFmpeg libraries in its package.&lt;/p&gt; 
&lt;h3&gt;GPU&lt;/h3&gt; 
&lt;p&gt;GPU execution requires the following NVIDIA libraries to be installed:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://developer.nvidia.com/cublas"&gt;cuBLAS for CUDA 12&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://developer.nvidia.com/cudnn"&gt;cuDNN 9 for CUDA 12&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The latest versions of &lt;code&gt;ctranslate2&lt;/code&gt; only support CUDA 12 and cuDNN 9. For CUDA 11 and cuDNN 8, the current workaround is downgrading to the &lt;code&gt;3.24.0&lt;/code&gt; version of &lt;code&gt;ctranslate2&lt;/code&gt;, for CUDA 12 and cuDNN 8, downgrade to the &lt;code&gt;4.4.0&lt;/code&gt; version of &lt;code&gt;ctranslate2&lt;/code&gt;, (This can be done with &lt;code&gt;pip install --force-reinstall ctranslate2==4.4.0&lt;/code&gt; or specifying the version in a &lt;code&gt;requirements.txt&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;There are multiple ways to install the NVIDIA libraries mentioned above. The recommended way is described in the official NVIDIA documentation, but we also suggest other installation methods below.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Other installation methods (click to expand)&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For all these methods below, keep in mind the above note regarding CUDA versions. Depending on your setup, you may need to install the &lt;em&gt;CUDA 11&lt;/em&gt; versions of libraries that correspond to the CUDA 12 libraries listed in the instructions below.&lt;/p&gt; 
 &lt;h4&gt;Use Docker&lt;/h4&gt; 
 &lt;p&gt;The libraries (cuBLAS, cuDNN) are installed in this official NVIDIA CUDA Docker images: &lt;code&gt;nvidia/cuda:12.3.2-cudnn9-runtime-ubuntu22.04&lt;/code&gt;.&lt;/p&gt; 
 &lt;h4&gt;Install with &lt;code&gt;pip&lt;/code&gt; (Linux only)&lt;/h4&gt; 
 &lt;p&gt;On Linux these libraries can be installed with &lt;code&gt;pip&lt;/code&gt;. Note that &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; must be set before launching Python.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install nvidia-cublas-cu12 nvidia-cudnn-cu12==9.*

export LD_LIBRARY_PATH=`python3 -c 'import os; import nvidia.cublas.lib; import nvidia.cudnn.lib; print(os.path.dirname(nvidia.cublas.lib.__file__) + ":" + os.path.dirname(nvidia.cudnn.lib.__file__))'`
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Download the libraries from Purfview's repository (Windows &amp;amp; Linux)&lt;/h4&gt; 
 &lt;p&gt;Purfview's &lt;a href="https://github.com/Purfview/whisper-standalone-win"&gt;whisper-standalone-win&lt;/a&gt; provides the required NVIDIA libraries for Windows &amp;amp; Linux in a &lt;a href="https://github.com/Purfview/whisper-standalone-win/releases/tag/libs"&gt;single archive&lt;/a&gt;. Decompress the archive and place the libraries in a directory included in the &lt;code&gt;PATH&lt;/code&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;The module can be installed from &lt;a href="https://pypi.org/project/faster-whisper/"&gt;PyPI&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install faster-whisper
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;Other installation methods (click to expand)&lt;/summary&gt; 
 &lt;h3&gt;Install the master branch&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install --force-reinstall "faster-whisper @ https://github.com/SYSTRAN/faster-whisper/archive/refs/heads/master.tar.gz"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Install a specific commit&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install --force-reinstall "faster-whisper @ https://github.com/SYSTRAN/faster-whisper/archive/a4f1cc8f11433e454c3934442b5e1a4ed5e865c3.tar.gz"
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Faster-whisper&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from faster_whisper import WhisperModel

model_size = "large-v3"

# Run on GPU with FP16
model = WhisperModel(model_size, device="cuda", compute_type="float16")

# or run on GPU with INT8
# model = WhisperModel(model_size, device="cuda", compute_type="int8_float16")
# or run on CPU with INT8
# model = WhisperModel(model_size, device="cpu", compute_type="int8")

segments, info = model.transcribe("audio.mp3", beam_size=5)

print("Detected language '%s' with probability %f" % (info.language, info.language_probability))

for segment in segments:
    print("[%.2fs -&amp;gt; %.2fs] %s" % (segment.start, segment.end, segment.text))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Warning:&lt;/strong&gt; &lt;code&gt;segments&lt;/code&gt; is a &lt;em&gt;generator&lt;/em&gt; so the transcription only starts when you iterate over it. The transcription can be run to completion by gathering the segments in a list or a &lt;code&gt;for&lt;/code&gt; loop:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;segments, _ = model.transcribe("audio.mp3")
segments = list(segments)  # The transcription will actually run here.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Batched Transcription&lt;/h3&gt; 
&lt;p&gt;The following code snippet illustrates how to run batched transcription on an example audio file. &lt;code&gt;BatchedInferencePipeline.transcribe&lt;/code&gt; is a drop-in replacement for &lt;code&gt;WhisperModel.transcribe&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from faster_whisper import WhisperModel, BatchedInferencePipeline

model = WhisperModel("turbo", device="cuda", compute_type="float16")
batched_model = BatchedInferencePipeline(model=model)
segments, info = batched_model.transcribe("audio.mp3", batch_size=16)

for segment in segments:
    print("[%.2fs -&amp;gt; %.2fs] %s" % (segment.start, segment.end, segment.text))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Faster Distil-Whisper&lt;/h3&gt; 
&lt;p&gt;The Distil-Whisper checkpoints are compatible with the Faster-Whisper package. In particular, the latest &lt;a href="https://huggingface.co/distil-whisper/distil-large-v3"&gt;distil-large-v3&lt;/a&gt; checkpoint is intrinsically designed to work with the Faster-Whisper transcription algorithm. The following code snippet demonstrates how to run inference with distil-large-v3 on a specified audio file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from faster_whisper import WhisperModel

model_size = "distil-large-v3"

model = WhisperModel(model_size, device="cuda", compute_type="float16")
segments, info = model.transcribe("audio.mp3", beam_size=5, language="en", condition_on_previous_text=False)

for segment in segments:
    print("[%.2fs -&amp;gt; %.2fs] %s" % (segment.start, segment.end, segment.text))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more information about the distil-large-v3 model, refer to the original &lt;a href="https://huggingface.co/distil-whisper/distil-large-v3"&gt;model card&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Word-level timestamps&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;segments, _ = model.transcribe("audio.mp3", word_timestamps=True)

for segment in segments:
    for word in segment.words:
        print("[%.2fs -&amp;gt; %.2fs] %s" % (word.start, word.end, word.word))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;VAD filter&lt;/h3&gt; 
&lt;p&gt;The library integrates the &lt;a href="https://github.com/snakers4/silero-vad"&gt;Silero VAD&lt;/a&gt; model to filter out parts of the audio without speech:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;segments, _ = model.transcribe("audio.mp3", vad_filter=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The default behavior is conservative and only removes silence longer than 2 seconds. See the available VAD parameters and default values in the &lt;a href="https://github.com/SYSTRAN/faster-whisper/raw/master/faster_whisper/vad.py"&gt;source code&lt;/a&gt;. They can be customized with the dictionary argument &lt;code&gt;vad_parameters&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;segments, _ = model.transcribe(
    "audio.mp3",
    vad_filter=True,
    vad_parameters=dict(min_silence_duration_ms=500),
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Vad filter is enabled by default for batched transcription.&lt;/p&gt; 
&lt;h3&gt;Logging&lt;/h3&gt; 
&lt;p&gt;The library logging level can be configured like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import logging

logging.basicConfig()
logging.getLogger("faster_whisper").setLevel(logging.DEBUG)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Going further&lt;/h3&gt; 
&lt;p&gt;See more model and transcription options in the &lt;a href="https://github.com/SYSTRAN/faster-whisper/raw/master/faster_whisper/transcribe.py"&gt;&lt;code&gt;WhisperModel&lt;/code&gt;&lt;/a&gt; class implementation.&lt;/p&gt; 
&lt;h2&gt;Community integrations&lt;/h2&gt; 
&lt;p&gt;Here is a non exhaustive list of open-source projects using faster-whisper. Feel free to add your project to the list!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/speaches-ai/speaches"&gt;speaches&lt;/a&gt; is an OpenAI compatible server using &lt;code&gt;faster-whisper&lt;/code&gt;. It's easily deployable with Docker, works with OpenAI SDKs/CLI, supports streaming, and live transcription.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/m-bain/whisperX"&gt;WhisperX&lt;/a&gt; is an award-winning Python library that offers speaker diarization and accurate word-level timestamps using wav2vec2 alignment&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Softcatala/whisper-ctranslate2"&gt;whisper-ctranslate2&lt;/a&gt; is a command line client based on faster-whisper and compatible with the original client from openai/whisper.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/MahmoudAshraf97/whisper-diarization"&gt;whisper-diarize&lt;/a&gt; is a speaker diarization tool that is based on faster-whisper and NVIDIA NeMo.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Purfview/whisper-standalone-win"&gt;whisper-standalone-win&lt;/a&gt; Standalone CLI executables of faster-whisper for Windows, Linux &amp;amp; macOS.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hedrergudene/asr-sd-pipeline"&gt;asr-sd-pipeline&lt;/a&gt; provides a scalable, modular, end to end multi-speaker speech to text solution implemented using AzureML pipelines.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/zh-plus/Open-Lyrics"&gt;Open-Lyrics&lt;/a&gt; is a Python library that transcribes voice files using faster-whisper, and translates/polishes the resulting text into &lt;code&gt;.lrc&lt;/code&gt; files in the desired language using OpenAI-GPT.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekodour/wscribe"&gt;wscribe&lt;/a&gt; is a flexible transcript generation tool supporting faster-whisper, it can export word level transcript and the exported transcript then can be edited with &lt;a href="https://github.com/geekodour/wscribe-editor"&gt;wscribe-editor&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/BANDAS-Center/aTrain"&gt;aTrain&lt;/a&gt; is a graphical user interface implementation of faster-whisper developed at the BANDAS-Center at the University of Graz for transcription and diarization in Windows (&lt;a href="https://apps.microsoft.com/detail/atrain/9N15Q44SZNS2"&gt;Windows Store App&lt;/a&gt;) and Linux.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ufal/whisper_streaming"&gt;Whisper-Streaming&lt;/a&gt; implements real-time mode for offline Whisper-like speech-to-text models with faster-whisper as the most recommended back-end. It implements a streaming policy with self-adaptive latency based on the actual source complexity, and demonstrates the state of the art.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/collabora/WhisperLive"&gt;WhisperLive&lt;/a&gt; is a nearly-live implementation of OpenAI's Whisper which uses faster-whisper as the backend to transcribe audio in real-time.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/BBC-Esq/ctranslate2-faster-whisper-transcriber"&gt;Faster-Whisper-Transcriber&lt;/a&gt; is a simple but reliable voice transcriber that provides a user-friendly interface.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/softcatala/open-dubbing"&gt;Open-dubbing&lt;/a&gt; is open dubbing is an AI dubbing system which uses machine learning models to automatically translate and synchronize audio dialogue into different languages.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/heimoshuiyu/whisper-fastapi"&gt;Whisper-FastAPI&lt;/a&gt; whisper-fastapi is a very simple script that provides an API backend compatible with OpenAI, HomeAssistant, and Konele (Android voice typing) formats.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Model conversion&lt;/h2&gt; 
&lt;p&gt;When loading a model from its size such as &lt;code&gt;WhisperModel("large-v3")&lt;/code&gt;, the corresponding CTranslate2 model is automatically downloaded from the &lt;a href="https://huggingface.co/Systran"&gt;Hugging Face Hub&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We also provide a script to convert any Whisper models compatible with the Transformers library. They could be the original OpenAI models or user fine-tuned models.&lt;/p&gt; 
&lt;p&gt;For example the command below converts the &lt;a href="https://huggingface.co/openai/whisper-large-v3"&gt;original "large-v3" Whisper model&lt;/a&gt; and saves the weights in FP16:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install transformers[torch]&amp;gt;=4.23

ct2-transformers-converter --model openai/whisper-large-v3 --output_dir whisper-large-v3-ct2
--copy_files tokenizer.json preprocessor_config.json --quantization float16
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;The option &lt;code&gt;--model&lt;/code&gt; accepts a model name on the Hub or a path to a model directory.&lt;/li&gt; 
 &lt;li&gt;If the option &lt;code&gt;--copy_files tokenizer.json&lt;/code&gt; is not used, the tokenizer configuration is automatically downloaded when the model is loaded later.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Models can also be converted from the code. See the &lt;a href="https://opennmt.net/CTranslate2/python/ctranslate2.converters.TransformersConverter.html"&gt;conversion API&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Load a converted model&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Directly load the model from a local directory:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;model = faster_whisper.WhisperModel("whisper-large-v3-ct2")
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/docs/transformers/model_sharing#upload-with-the-web-interface"&gt;Upload your model to the Hugging Face Hub&lt;/a&gt; and load it from its name:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;model = faster_whisper.WhisperModel("username/whisper-large-v3-ct2")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Comparing performance against other implementations&lt;/h2&gt; 
&lt;p&gt;If you are comparing the performance against other Whisper implementations, you should make sure to run the comparison with similar settings. In particular:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Verify that the same transcription options are used, especially the same beam size. For example in openai/whisper, &lt;code&gt;model.transcribe&lt;/code&gt; uses a default beam size of 1 but here we use a default beam size of 5.&lt;/li&gt; 
 &lt;li&gt;Transcription speed is closely affected by the number of words in the transcript, so ensure that other implementations have a similar WER (Word Error Rate) to this one.&lt;/li&gt; 
 &lt;li&gt;When running on CPU, make sure to set the same number of threads. Many frameworks will read the environment variable &lt;code&gt;OMP_NUM_THREADS&lt;/code&gt;, which can be set when running your script:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OMP_NUM_THREADS=4 python3 my_script.py
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>OpenHands/OpenHands</title>
      <link>https://github.com/OpenHands/OpenHands</link>
      <description>&lt;p&gt;ğŸ™Œ OpenHands: AI-Driven Development&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenHands/docs/main/openhands/static/img/logo.png" alt="Logo" width="200" /&gt; 
 &lt;h1 align="center" style="border-bottom: none"&gt;OpenHands: AI-Driven Development&lt;/h1&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/OpenHands/OpenHands/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/LICENSE-MIT-20B2AA?style=for-the-badge" alt="MIT License" /&gt;&lt;/a&gt; 
 &lt;a href="https://docs.google.com/spreadsheets/d/1wOUdFCMyY6Nt0AIqF705KN4JKOWgeI4wUGUP60krXXs/edit?gid=811504672#gid=811504672"&gt;&lt;img src="https://img.shields.io/badge/SWEBench-77.6-00cc00?logoColor=FFE165&amp;amp;style=for-the-badge" alt="Benchmark Score" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://docs.openhands.dev/sdk"&gt;&lt;img src="https://img.shields.io/badge/Documentation-000?logo=googledocs&amp;amp;logoColor=FFE165&amp;amp;style=for-the-badge" alt="Check out the documentation" /&gt;&lt;/a&gt; 
 &lt;a href="https://arxiv.org/abs/2511.03690"&gt;&lt;img src="https://img.shields.io/badge/Paper-000?logoColor=FFE165&amp;amp;logo=arxiv&amp;amp;style=for-the-badge" alt="Tech Report" /&gt;&lt;/a&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; 
 &lt;p&gt;&lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=de"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=es"&gt;EspaÃ±ol&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=fr"&gt;franÃ§ais&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=ja"&gt;æ—¥æœ¬èª&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=ko"&gt;í•œêµ­ì–´&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=pt"&gt;PortuguÃªs&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=ru"&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=zh"&gt;ä¸­æ–‡&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;ğŸ™Œ&amp;nbsp;Welcome to OpenHands, a &lt;a href="https://raw.githubusercontent.com/OpenHands/OpenHands/main/COMMUNITY.md"&gt;community&lt;/a&gt; focused on AI-driven development. Weâ€™d love for you to &lt;a href="https://dub.sh/openhands"&gt;join us on Slack&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;There are a few ways to work with OpenHands:&lt;/p&gt; 
&lt;h3&gt;OpenHands Software Agent SDK&lt;/h3&gt; 
&lt;p&gt;The SDK is a composable Python library that contains all of our agentic tech. It's the engine that powers everything else below.&lt;/p&gt; 
&lt;p&gt;Define agents in code, then run them locally, or scale to 1000s of agents in the cloud.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.openhands.dev/sdk"&gt;Check out the docs&lt;/a&gt; or &lt;a href="https://github.com/OpenHands/software-agent-sdk/"&gt;view the source&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;OpenHands CLI&lt;/h3&gt; 
&lt;p&gt;The CLI is the easiest way to start using OpenHands. The experience will be familiar to anyone who has worked with e.g. Claude Code or Codex. You can power it with Claude, GPT, or any other LLM.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.openhands.dev/openhands/usage/run-openhands/cli-mode"&gt;Check out the docs&lt;/a&gt; or &lt;a href="https://github.com/OpenHands/OpenHands-CLI"&gt;view the source&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;OpenHands Local GUI&lt;/h3&gt; 
&lt;p&gt;Use the Local GUI for running agents on your laptop. It comes with a REST API and a single-page React application. The experience will be familiar to anyone who has used Devin or Jules.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.openhands.dev/openhands/usage/run-openhands/local-setup"&gt;Check out the docs&lt;/a&gt; or view the source in this repo.&lt;/p&gt; 
&lt;h3&gt;OpenHands Cloud&lt;/h3&gt; 
&lt;p&gt;This is a deployment of OpenHands GUI, running on hosted infrastructure.&lt;/p&gt; 
&lt;p&gt;You can try it with a free $10 credit by &lt;a href="https://app.all-hands.dev"&gt;signing in with your GitHub or GitLab account&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;OpenHands Cloud comes with source-available features and integrations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Integrations with Slack, Jira, and Linear&lt;/li&gt; 
 &lt;li&gt;Multi-user support&lt;/li&gt; 
 &lt;li&gt;RBAC and permissions&lt;/li&gt; 
 &lt;li&gt;Collaboration features (e.g., conversation sharing)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;OpenHands Enterprise&lt;/h3&gt; 
&lt;p&gt;Large enterprises can work with us to self-host OpenHands Cloud in their own VPC, via Kubernetes. OpenHands Enterprise can also work with the CLI and SDK above.&lt;/p&gt; 
&lt;p&gt;OpenHands Enterprise is source-available--you can see all the source code here in the enterprise/ directory, but you'll need to purchase a license if you want to run it for more than one month.&lt;/p&gt; 
&lt;p&gt;Enterprise contracts also come with extended support and access to our research team.&lt;/p&gt; 
&lt;p&gt;Learn more at &lt;a href="https://openhands.dev/enterprise"&gt;openhands.dev/enterprise&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Everything Else&lt;/h3&gt; 
&lt;p&gt;Check out our &lt;a href="https://github.com/orgs/openhands/projects/1"&gt;Product Roadmap&lt;/a&gt;, and feel free to &lt;a href="https://github.com/OpenHands/OpenHands/issues"&gt;open up an issue&lt;/a&gt; if there's something you'd like to see!&lt;/p&gt; 
&lt;p&gt;You might also be interested in our &lt;a href="https://github.com/OpenHands/benchmarks"&gt;evaluation infrastructure&lt;/a&gt;, our &lt;a href="https://github.com/OpenHands/openhands-chrome-extension/"&gt;chrome extension&lt;/a&gt;, or our &lt;a href="https://github.com/OpenHands/ToM-SWE"&gt;Theory-of-Mind module&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;All our work is available under the MIT license, except for the &lt;code&gt;enterprise/&lt;/code&gt; directory in this repository (see the &lt;a href="https://raw.githubusercontent.com/OpenHands/OpenHands/main/enterprise/LICENSE"&gt;enterprise license&lt;/a&gt; for details). The core &lt;code&gt;openhands&lt;/code&gt; and &lt;code&gt;agent-server&lt;/code&gt; Docker images are fully MIT-licensed as well.&lt;/p&gt; 
&lt;p&gt;If you need help with anything, or just want to chat, &lt;a href="https://dub.sh/openhands"&gt;come find us on Slack&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>pathwaycom/pathway</title>
      <link>https://github.com/pathwaycom/pathway</link>
      <description>&lt;p&gt;Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://pathway.com/"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://pathway.com/logo-light.svg" /&gt; 
   &lt;img src="https://pathway.com/logo-dark.svg?sanitize=true" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
 &lt;br /&gt;
 &lt;br /&gt; 
 &lt;a href="https://trendshift.io/repositories/10388" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/10388" alt="pathwaycom%2Fpathway | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
 &lt;br /&gt;
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml"&gt; &lt;img src="https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml/badge.svg?sanitize=true" alt="ubuntu" /&gt; &lt;br /&gt; &lt;/a&gt;&lt;a href="https://github.com/pathwaycom/pathway/actions/workflows/release.yml"&gt; &lt;img src="https://github.com/pathwaycom/pathway/actions/workflows/release.yml/badge.svg?sanitize=true" alt="Last release" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/pathway"&gt;&lt;img src="https://badge.fury.io/py/pathway.svg?sanitize=true" alt="PyPI version" height="18" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/pathway"&gt;&lt;img src="https://static.pepy.tech/badge/pathway" alt="PyPI downloads" height="18" /&gt;&lt;/a&gt; &lt;a href="https://github.com/pathwaycom/pathway/raw/main/LICENSE.txt"&gt; &lt;img src="https://img.shields.io/badge/license-BSL-green" alt="License: BSL" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://discord.gg/pathway"&gt; &lt;img src="https://img.shields.io/discord/1042405378304004156?logo=discord" alt="chat on Discord" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/intent/follow?screen_name=pathway_com"&gt; &lt;img src="https://img.shields.io/twitter/follow/pathwaycom" alt="follow on Twitter" /&gt;&lt;/a&gt; &lt;a href="https://linkedin.com/company/pathway"&gt; &lt;img src="https://img.shields.io/badge/pathway-0077B5?style=social&amp;amp;logo=linkedin" alt="follow on LinkedIn" /&gt;&lt;/a&gt; &lt;a href="https://github.com/dylanhogg/awesome-python/raw/main/README.md"&gt; &lt;img src="https://awesome.re/badge.svg?sanitize=true" alt="Awesome Python" /&gt;&lt;/a&gt; &lt;a href="https://gurubase.io/g/pathway"&gt; &lt;img src="https://img.shields.io/badge/Gurubase-Ask%20Pathway%20Guru-006BFF" alt="Pathway Guru" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://raw.githubusercontent.com/pathwaycom/pathway/main/#getting-started"&gt;Getting Started&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/pathwaycom/pathway/main/#deployment"&gt;Deployment&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/pathwaycom/pathway/main/#resources"&gt;Documentation and Support&lt;/a&gt; | &lt;a href="https://pathway.com/blog/"&gt;Blog&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/pathwaycom/pathway/main/#license"&gt;License&lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;Pathway&lt;a id="pathway"&gt; Live Data Framework&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pathway.com"&gt;Pathway&lt;/a&gt; is a Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.&lt;/p&gt; 
&lt;p&gt;Pathway comes with an &lt;strong&gt;easy-to-use Python API&lt;/strong&gt;, allowing you to seamlessly integrate your favorite Python ML libraries. Pathway code is versatile and robust: &lt;strong&gt;you can use it in both development and production environments, handling both batch and streaming data effectively&lt;/strong&gt;. The same code can be used for local development, CI/CD tests, running batch jobs, handling stream replays, and processing data streams.&lt;/p&gt; 
&lt;p&gt;Pathway is powered by a &lt;strong&gt;scalable Rust engine&lt;/strong&gt; based on Differential Dataflow and performs incremental computation. Your Pathway code, despite being written in Python, is run by the Rust engine, enabling multithreading, multiprocessing, and distributed computations. All the pipeline is kept in memory and can be easily deployed with &lt;strong&gt;Docker and Kubernetes&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;You can install Pathway with pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install -U pathway
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For any questions, you will find the community and team behind the project &lt;a href="https://discord.com/invite/pathway"&gt;on Discord&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Use-cases and templates&lt;/h2&gt; 
&lt;p&gt;Ready to see what Pathway can do?&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://pathway.com/developers/templates"&gt;Try one of our easy-to-run examples&lt;/a&gt;!&lt;/p&gt; 
&lt;p&gt;Available in both notebook and docker formats, these ready-to-launch examples can be launched in just a few clicks. Pick one and start your hands-on experience with Pathway today!&lt;/p&gt; 
&lt;h3&gt;Event processing and real-time analytics pipelines&lt;/h3&gt; 
&lt;p&gt;With its unified engine for batch and streaming and its full Python compatibility, Pathway makes data processing as easy as possible. It's the ideal solution for a wide range of data processing pipelines, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/kafka-etl"&gt;Showcase: Real-time ETL.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/realtime-log-monitoring"&gt;Showcase: Event-driven pipelines with alerting.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/linear_regression_with_kafka"&gt;Showcase: Realtime analytics.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/user-guide/connecting-to-data/switch-from-batch-to-streaming"&gt;Docs: Switch from batch to streaming.&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;AI Pipelines&lt;/h3&gt; 
&lt;p&gt;Pathway provides dedicated LLM tooling to build live LLM and RAG pipelines. Wrappers for most common LLM services and utilities are included, making working with LLMs and RAGs pipelines incredibly easy. Check out our &lt;a href="https://pathway.com/developers/user-guide/llm-xpack/overview"&gt;LLM xpack documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Don't hesitate to try one of our runnable examples featuring LLM tooling. You can find such examples &lt;a href="https://pathway.com/developers/user-guide/llm-xpack/llm-examples"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/unstructured-to-structured"&gt;Template: Unstructured data to SQL on-the-fly.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/private-rag-ollama-mistral"&gt;Template: Private RAG with Ollama and Mistral AI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/adaptive-rag"&gt;Template: Adaptive RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/multimodal-rag"&gt;Template: Multimodal RAG with gpt-4o&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;A wide range of connectors&lt;/strong&gt;: Pathway comes with connectors that connect to external data sources such as Kafka, GDrive, PostgreSQL, or SharePoint. Its Airbyte connector allows you to connect to more than 300 different data sources. If the connector you want is not available, you can build your own custom connector using Pathway Python connector.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Stateless and stateful transformations&lt;/strong&gt;: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Persistence&lt;/strong&gt;: Pathway provides persistence to save the state of the computation. This allows you to restart your pipeline after an update or a crash. Your pipelines are in good hands with Pathway!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Consistency&lt;/strong&gt;: Pathway handles the time for you, making sure all your computations are consistent. In particular, Pathway manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system. The free version of Pathway gives the "at least once" consistency while the enterprise version provides the "exactly once" consistency.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scalable Rust engine&lt;/strong&gt;: with Pathway Rust engine, you are free from the usual limits imposed by Python. You can easily do multithreading, multiprocessing, and distributed computations.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLM helpers&lt;/strong&gt;: Pathway provides an LLM extension with all the utilities to integrate LLMs with your data pipelines (LLM wrappers, parsers, embedders, splitters), including an in-memory real-time Vector Index, and integrations with LLamaIndex and LangChain. You can quickly build and deploy RAG applications with your live documents.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting started&lt;a id="getting-started"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;a id="installation"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Pathway requires Python 3.10 or above.&lt;/p&gt; 
&lt;p&gt;You can install the current release of Pathway using &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ pip install -U pathway
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;âš ï¸ Pathway is available on MacOS and Linux. Users of other systems should run Pathway on a Virtual Machine.&lt;/p&gt; 
&lt;h3&gt;Example: computing the sum of positive values in real time.&lt;a id="example"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pathway as pw

# Define the schema of your data (Optional)
class InputSchema(pw.Schema):
  value: int

# Connect to your data using connectors
input_table = pw.io.csv.read(
  "./input/",
  schema=InputSchema
)

#Define your operations on the data
filtered_table = input_table.filter(input_table.value&amp;gt;=0)
result_table = filtered_table.reduce(
  sum_value = pw.reducers.sum(filtered_table.value)
)

# Load your results to external systems
pw.io.jsonlines.write(result_table, "output.jsonl")

# Run the computation
pw.run()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run Pathway &lt;a href="https://colab.research.google.com/drive/1aBIJ2HCng-YEUOMrr0qtj0NeZMEyRz55?usp=sharing"&gt;in Google Colab&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can find more examples &lt;a href="https://github.com/pathwaycom/pathway/tree/main/examples"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Deployment&lt;a id="deployment"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h3&gt;Locally&lt;a id="running-pathway-locally"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;To use Pathway, you only need to import it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pathway as pw
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now, you can easily create your processing pipeline, and let Pathway handle the updates. Once your pipeline is created, you can launch the computation on streaming data with a one-line command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;pw.run()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can then run your Pathway project (say, &lt;code&gt;main.py&lt;/code&gt;) just like a normal Python script: &lt;code&gt;$ python main.py&lt;/code&gt;. Pathway comes with a monitoring dashboard that allows you to keep track of the number of messages sent by each connector and the latency of the system. The dashboard also includes log messages.&lt;/p&gt; 
&lt;img src="https://d14l3brkh44201.cloudfront.net/pathway-dashboard.png" width="1326" alt="Pathway dashboard" /&gt; 
&lt;p&gt;Alternatively, you can use the pathway'ish version:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ pathway spawn python main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Pathway natively supports multithreading. To launch your application with 3 threads, you can do as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ pathway spawn --threads 3 python main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To jumpstart a Pathway project, you can use our &lt;a href="https://github.com/pathwaycom/cookiecutter-pathway"&gt;cookiecutter template&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Docker&lt;a id="docker"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;You can easily run Pathway using docker.&lt;/p&gt; 
&lt;h4&gt;Pathway image&lt;/h4&gt; 
&lt;p&gt;You can use the &lt;a href="https://hub.docker.com/r/pathwaycom/pathway"&gt;Pathway docker image&lt;/a&gt;, using a Dockerfile:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-dockerfile"&gt;FROM pathwaycom/pathway:latest

WORKDIR /app

COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD [ "python", "./your-script.py" ]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can then build and run the Docker image:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;docker build -t my-pathway-app .
docker run -it --rm --name my-pathway-app my-pathway-app
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run a single Python script&lt;/h4&gt; 
&lt;p&gt;When dealing with single-file projects, creating a full-fledged &lt;code&gt;Dockerfile&lt;/code&gt; might seem unnecessary. In such scenarios, you can execute a Python script directly using the Pathway Docker image. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;docker run -it --rm --name my-pathway-app -v "$PWD":/app pathwaycom/pathway:latest python my-pathway-app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Python docker image&lt;/h4&gt; 
&lt;p&gt;You can also use a standard Python image and install Pathway using pip with a Dockerfile:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-dockerfile"&gt;FROM --platform=linux/x86_64 python:3.10

RUN pip install -U pathway
COPY ./pathway-script.py pathway-script.py

CMD ["python", "-u", "pathway-script.py"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Kubernetes and cloud&lt;a id="k8s"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Docker containers are ideally suited for deployment on the cloud with Kubernetes. If you want to scale your Pathway application, you may be interested in our Pathway for Enterprise. Pathway for Enterprise is specially tailored towards end-to-end data processing and real time intelligent analytics. It scales using distributed computing on the cloud and supports distributed Kubernetes deployment, with external persistence setup.&lt;/p&gt; 
&lt;p&gt;You can easily deploy Pathway using services like Render: see &lt;a href="https://pathway.com/developers/user-guide/deployment/render-deploy/"&gt;how to deploy Pathway in a few clicks&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you are interested, don't hesitate to &lt;a href="mailto:contact@pathway.com"&gt;contact us&lt;/a&gt; to learn more.&lt;/p&gt; 
&lt;h2&gt;Performance&lt;a id="performance"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Pathway is made to outperform state-of-the-art technologies designed for streaming and batch data processing tasks, including: Flink, Spark, and Kafka Streaming. It also makes it possible to implement a lot of algorithms/UDF's in streaming mode which are not readily supported by other streaming frameworks (especially: temporal joins, iterative graph algorithms, machine learning routines).&lt;/p&gt; 
&lt;p&gt;If you are curious, here are &lt;a href="https://github.com/pathwaycom/pathway-benchmarks"&gt;some benchmarks to play with&lt;/a&gt;.&lt;/p&gt; 
&lt;img src="https://github.com/pathwaycom/pathway-benchmarks/raw/main/images/bm-wordcount-lineplot.png" width="1326" alt="WordCount Graph" /&gt; 
&lt;h2&gt;Documentation and Support&lt;a id="resources"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;The entire documentation of Pathway is available at &lt;a href="https://pathway.com/developers/user-guide/introduction/welcome"&gt;pathway.com/developers/&lt;/a&gt;, including the &lt;a href="https://pathway.com/developers/api-docs/pathway"&gt;API Docs&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you have any question, don't hesitate to &lt;a href="https://github.com/pathwaycom/pathway/issues"&gt;open an issue on GitHub&lt;/a&gt;, join us on &lt;a href="https://discord.com/invite/pathway"&gt;Discord&lt;/a&gt;, or send us an email at &lt;a href="mailto:contact@pathway.com"&gt;contact@pathway.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ¤ Featured Collaborations &amp;amp; Integrations&lt;/h2&gt; 
&lt;p&gt;We build cutting-edge data processing pipelines and co-promote solutions that push the boundaries of what's possible with Python and streaming data. Meet the people helping us shape the future of data engineering.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Project&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://databento.com/blog/option-greeks"&gt;Databento&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;A simpler, faster way to get market data.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://docs.langchain.com/oss/python/integrations/vectorstores/pathway"&gt;LangChain&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;LangChain is the platform for agent engineering.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://developers.llamaindex.ai/python/examples/retrievers/pathway_retriever/"&gt;LlamaIndex&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;The developer-trusted framework for building context-aware AI agents.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.min.io/"&gt;MinIO&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;MinIO is a high-performance, S3 compatible object store, open sourced under GNU AGPLv3 license.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleOCR"&gt;PaddleOCR&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;PaddleOCR is an industry-leading, production-ready OCR and document AI engine, offering end-to-end solutions from text extraction to intelligent document understanding.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.redpanda.com/blog/replace-kafka-redpanda-data-analysis-streaming"&gt;Redpanda&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Build, operate, and govern streaming and AI applications without the complexity of Kafka.&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;License&lt;a id="license"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Pathway is distributed on a &lt;a href="https://github.com/pathwaycom/pathway/raw/main/LICENSE.txt"&gt;BSL 1.1 License&lt;/a&gt; which allows for unlimited non-commercial use, as well as use of the Pathway package &lt;a href="https://pathway.com/license/"&gt;for most commercial purposes&lt;/a&gt;, free of charge. Code in this repository automatically converts to Open Source (Apache 2.0 License) after 4 years. Some &lt;a href="https://github.com/pathwaycom"&gt;public repos&lt;/a&gt; which are complementary to this one (examples, libraries, connectors, etc.) are licensed as Open Source, under the MIT license.&lt;/p&gt; 
&lt;h2&gt;Contribution guidelines&lt;a id="contribution-guidelines"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;If you develop a library or connector which you would like to integrate with this repo, we suggest releasing it first as a separate repo on a MIT/Apache 2.0 license.&lt;/p&gt; 
&lt;p&gt;For all concerns regarding core Pathway functionalities, Issues are encouraged. For further information, don't hesitate to engage with Pathway's &lt;a href="https://discord.gg/pathway"&gt;Discord community&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/BitNet</title>
      <link>https://github.com/microsoft/BitNet</link>
      <description>&lt;p&gt;Official inference framework for 1-bit LLMs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;bitnet.cpp&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/version-1.0-blue" alt="version" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/header_model_release.png" alt="BitNet Model on Hugging Face" width="800" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Try it out via this &lt;a href="https://bitnet-demo.azurewebsites.net/"&gt;demo&lt;/a&gt;, or build and run it on your own &lt;a href="https://github.com/microsoft/BitNet?tab=readme-ov-file#build-from-source"&gt;CPU&lt;/a&gt; or &lt;a href="https://github.com/microsoft/BitNet/raw/main/gpu/README.md"&gt;GPU&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;bitnet.cpp is the official inference framework for 1-bit LLMs (e.g., BitNet b1.58). It offers a suite of optimized kernels, that support &lt;strong&gt;fast&lt;/strong&gt; and &lt;strong&gt;lossless&lt;/strong&gt; inference of 1.58-bit models on CPU and GPU (NPU support will coming next).&lt;/p&gt; 
&lt;p&gt;The first release of bitnet.cpp is to support inference on CPUs. bitnet.cpp achieves speedups of &lt;strong&gt;1.37x&lt;/strong&gt; to &lt;strong&gt;5.07x&lt;/strong&gt; on ARM CPUs, with larger models experiencing greater performance gains. Additionally, it reduces energy consumption by &lt;strong&gt;55.4%&lt;/strong&gt; to &lt;strong&gt;70.0%&lt;/strong&gt;, further boosting overall efficiency. On x86 CPUs, speedups range from &lt;strong&gt;2.37x&lt;/strong&gt; to &lt;strong&gt;6.17x&lt;/strong&gt; with energy reductions between &lt;strong&gt;71.9%&lt;/strong&gt; to &lt;strong&gt;82.2%&lt;/strong&gt;. Furthermore, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU, achieving speeds comparable to human reading (5-7 tokens per second), significantly enhancing the potential for running LLMs on local devices. Please refer to the &lt;a href="https://arxiv.org/abs/2410.16144"&gt;technical report&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/m2_performance.jpg" alt="m2_performance" width="800" /&gt; 
&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/intel_performance.jpg" alt="m2_performance" width="800" /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The tested models are dummy setups used in a research context to demonstrate the inference performance of bitnet.cpp.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;p&gt;A demo of bitnet.cpp running a BitNet b1.58 3B model on Apple M2:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1"&gt;https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What's New:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;05/20/2025 &lt;a href="https://github.com/microsoft/BitNet/raw/main/gpu/README.md"&gt;BitNet Official GPU inference kernel&lt;/a&gt; &lt;img src="https://img.shields.io/badge/NEW-red" alt="NEW" /&gt;&lt;/li&gt; 
 &lt;li&gt;04/14/2025 &lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;BitNet Official 2B Parameter Model on Hugging Face&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;02/18/2025 &lt;a href="https://arxiv.org/abs/2502.11880"&gt;Bitnet.cpp: Efficient Edge Inference for Ternary LLMs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;11/08/2024 &lt;a href="https://arxiv.org/abs/2411.04965"&gt;BitNet a4.8: 4-bit Activations for 1-bit LLMs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/21/2024 &lt;a href="https://arxiv.org/abs/2410.16144"&gt;1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/17/2024 bitnet.cpp 1.0 released.&lt;/li&gt; 
 &lt;li&gt;03/21/2024 &lt;a href="https://github.com/microsoft/unilm/raw/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf"&gt;The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;02/27/2024 &lt;a href="https://arxiv.org/abs/2402.17764"&gt;The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/17/2023 &lt;a href="https://arxiv.org/abs/2310.11453"&gt;BitNet: Scaling 1-bit Transformers for Large Language Models&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;This project is based on the &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt; framework. We would like to thank all the authors for their contributions to the open-source community. Also, bitnet.cpp's kernels are built on top of the Lookup Table methodologies pioneered in &lt;a href="https://github.com/microsoft/T-MAC/"&gt;T-MAC&lt;/a&gt;. For inference of general low-bit LLMs beyond ternary models, we recommend using T-MAC.&lt;/p&gt; 
&lt;h2&gt;Official Models&lt;/h2&gt; 
&lt;table&gt;  
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th rowspan="2"&gt;Model&lt;/th&gt; 
   &lt;th rowspan="2"&gt;Parameters&lt;/th&gt; 
   &lt;th rowspan="2"&gt;CPU&lt;/th&gt; 
   &lt;th colspan="3"&gt;Kernel&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;I2_S&lt;/th&gt; 
   &lt;th&gt;TL1&lt;/th&gt; 
   &lt;th&gt;TL2&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;BitNet-b1.58-2B-4T&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;2.4B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Supported Models&lt;/h2&gt; 
&lt;p&gt;â—ï¸&lt;strong&gt;We use existing 1-bit LLMs available on &lt;a href="https://huggingface.co/"&gt;Hugging Face&lt;/a&gt; to demonstrate the inference capabilities of bitnet.cpp. We hope the release of bitnet.cpp will inspire the development of 1-bit LLMs in large-scale settings in terms of model size and training tokens.&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt;  
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th rowspan="2"&gt;Model&lt;/th&gt; 
   &lt;th rowspan="2"&gt;Parameters&lt;/th&gt; 
   &lt;th rowspan="2"&gt;CPU&lt;/th&gt; 
   &lt;th colspan="3"&gt;Kernel&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;I2_S&lt;/th&gt; 
   &lt;th&gt;TL1&lt;/th&gt; 
   &lt;th&gt;TL2&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/1bitLLM/bitnet_b1_58-large"&gt;bitnet_b1_58-large&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;0.7B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/1bitLLM/bitnet_b1_58-3B"&gt;bitnet_b1_58-3B&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;3.3B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens"&gt;Llama3-8B-1.58-100B-tokens&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;8.0B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026"&gt;Falcon3 Family&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;1B-10B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/collections/tiiuae/falcon-edge-series-6804fd13344d6d8a8fa71130"&gt;Falcon-E Family&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;1B-3B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;python&amp;gt;=3.9&lt;/li&gt; 
 &lt;li&gt;cmake&amp;gt;=3.22&lt;/li&gt; 
 &lt;li&gt;clang&amp;gt;=18 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;For Windows users, install &lt;a href="https://visualstudio.microsoft.com/downloads/"&gt;Visual Studio 2022&lt;/a&gt;. In the installer, toggle on at least the following options(this also automatically installs the required additional tools like CMake):&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Desktop-development with C++&lt;/li&gt; 
     &lt;li&gt;C++-CMake Tools for Windows&lt;/li&gt; 
     &lt;li&gt;Git for Windows&lt;/li&gt; 
     &lt;li&gt;C++-Clang Compiler for Windows&lt;/li&gt; 
     &lt;li&gt;MS-Build Support for LLVM-Toolset (clang)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;For Debian/Ubuntu users, you can download with &lt;a href="https://apt.llvm.org/"&gt;Automatic installation script&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash -c "$(wget -O - https://apt.llvm.org/llvm.sh)"&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;conda (highly recommend)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Build from source&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] If you are using Windows, please remember to always use a Developer Command Prompt / PowerShell for VS2022 for the following commands. Please refer to the FAQs below if you see any issues.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone the repo&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone --recursive https://github.com/microsoft/BitNet.git
cd BitNet
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install the dependencies&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# (Recommended) Create a new conda environment
conda create -n bitnet-cpp python=3.9
conda activate bitnet-cpp

pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Build the project&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Manually download the model and run with local path
huggingface-cli download microsoft/BitNet-b1.58-2B-4T-gguf --local-dir models/BitNet-b1.58-2B-4T
python setup_env.py -md models/BitNet-b1.58-2B-4T -q i2_s

&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;
usage: setup_env.py [-h] [--hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}] [--model-dir MODEL_DIR] [--log-dir LOG_DIR] [--quant-type {i2_s,tl1}] [--quant-embd]
                    [--use-pretuned]

Setup the environment for running inference

optional arguments:
  -h, --help            show this help message and exit
  --hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}, -hr {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}
                        Model used for inference
  --model-dir MODEL_DIR, -md MODEL_DIR
                        Directory to save/load the model
  --log-dir LOG_DIR, -ld LOG_DIR
                        Directory to save the logging info
  --quant-type {i2_s,tl1}, -q {i2_s,tl1}
                        Quantization type
  --quant-embd          Quantize the embeddings to f16
  --use-pretuned, -p    Use the pretuned kernel parameters
&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Basic usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run inference with the quantized model
python run_inference.py -m models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf -p "You are a helpful assistant" -cnv
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;
usage: run_inference.py [-h] [-m MODEL] [-n N_PREDICT] -p PROMPT [-t THREADS] [-c CTX_SIZE] [-temp TEMPERATURE] [-cnv]

Run inference

optional arguments:
  -h, --help            show this help message and exit
  -m MODEL, --model MODEL
                        Path to model file
  -n N_PREDICT, --n-predict N_PREDICT
                        Number of tokens to predict when generating text
  -p PROMPT, --prompt PROMPT
                        Prompt to generate text from
  -t THREADS, --threads THREADS
                        Number of threads to use
  -c CTX_SIZE, --ctx-size CTX_SIZE
                        Size of the prompt context
  -temp TEMPERATURE, --temperature TEMPERATURE
                        Temperature, a hyperparameter that controls the randomness of the generated text
  -cnv, --conversation  Whether to enable chat mode or not (for instruct models.)
                        (When this option is turned on, the prompt specified by -p will be used as the system prompt.)
&lt;/pre&gt; 
&lt;h3&gt;Benchmark&lt;/h3&gt; 
&lt;p&gt;We provide scripts to run the inference benchmark providing a model.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;usage: e2e_benchmark.py -m MODEL [-n N_TOKEN] [-p N_PROMPT] [-t THREADS]  
   
Setup the environment for running the inference  
   
required arguments:  
  -m MODEL, --model MODEL  
                        Path to the model file. 
   
optional arguments:  
  -h, --help  
                        Show this help message and exit. 
  -n N_TOKEN, --n-token N_TOKEN  
                        Number of generated tokens. 
  -p N_PROMPT, --n-prompt N_PROMPT  
                        Prompt to generate text from. 
  -t THREADS, --threads THREADS  
                        Number of threads to use. 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Here's a brief explanation of each argument:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-m&lt;/code&gt;, &lt;code&gt;--model&lt;/code&gt;: The path to the model file. This is a required argument that must be provided when running the script.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-n&lt;/code&gt;, &lt;code&gt;--n-token&lt;/code&gt;: The number of tokens to generate during the inference. It is an optional argument with a default value of 128.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-p&lt;/code&gt;, &lt;code&gt;--n-prompt&lt;/code&gt;: The number of prompt tokens to use for generating text. This is an optional argument with a default value of 512.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-t&lt;/code&gt;, &lt;code&gt;--threads&lt;/code&gt;: The number of threads to use for running the inference. It is an optional argument with a default value of 2.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-h&lt;/code&gt;, &lt;code&gt;--help&lt;/code&gt;: Show the help message and exit. Use this argument to display usage information.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python utils/e2e_benchmark.py -m /path/to/model -n 200 -p 256 -t 4  
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command would run the inference benchmark using the model located at &lt;code&gt;/path/to/model&lt;/code&gt;, generating 200 tokens from a 256 token prompt, utilizing 4 threads.&lt;/p&gt; 
&lt;p&gt;For the model layout that do not supported by any public model, we provide scripts to generate a dummy model with the given model layout, and run the benchmark on your machine:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python utils/generate-dummy-bitnet-model.py models/bitnet_b1_58-large --outfile models/dummy-bitnet-125m.tl1.gguf --outtype tl1 --model-size 125M

# Run benchmark with the generated model, use -m to specify the model path, -p to specify the prompt processed, -n to specify the number of token to generate
python utils/e2e_benchmark.py -m models/dummy-bitnet-125m.tl1.gguf -p 512 -n 128
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Convert from &lt;code&gt;.safetensors&lt;/code&gt; Checkpoints&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Prepare the .safetensors model file
huggingface-cli download microsoft/bitnet-b1.58-2B-4T-bf16 --local-dir ./models/bitnet-b1.58-2B-4T-bf16

# Convert to gguf model
python ./utils/convert-helper-bitnet.py ./models/bitnet-b1.58-2B-4T-bf16
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;FAQ (Frequently Asked Questions)ğŸ“Œ&lt;/h3&gt; 
&lt;h4&gt;Q1: The build dies with errors building llama.cpp due to issues with std::chrono in log.cpp?&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; This is an issue introduced in recent version of llama.cpp. Please refer to this &lt;a href="https://github.com/tinglou/llama.cpp/commit/4e3db1e3d78cc1bcd22bcb3af54bd2a4628dd323"&gt;commit&lt;/a&gt; in the &lt;a href="https://github.com/abetlen/llama-cpp-python/issues/1942"&gt;discussion&lt;/a&gt; to fix this issue.&lt;/p&gt; 
&lt;h4&gt;Q2: How to build with clang in conda environment on windows?&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Before building the project, verify your clang installation and access to Visual Studio tools by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;clang -v
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command checks that you are using the correct version of clang and that the Visual Studio tools are available. If you see an error message such as:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;'clang' is not recognized as an internal or external command, operable program or batch file.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It indicates that your command line window is not properly initialized for Visual Studio tools.&lt;/p&gt; 
&lt;p&gt;â€¢ If you are using Command Prompt, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;"C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\VsDevCmd.bat" -startdir=none -arch=x64 -host_arch=x64
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;â€¢ If you are using Windows PowerShell, run the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Import-Module "C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\Microsoft.VisualStudio.DevShell.dll" Enter-VsDevShell 3f0e31ad -SkipAutomaticLocation -DevCmdArguments "-arch=x64 -host_arch=x64"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These steps will initialize your environment and allow you to use the correct Visual Studio tools.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>timescale/pg-aiguide</title>
      <link>https://github.com/timescale/pg-aiguide</link>
      <description>&lt;p&gt;MCP server and Claude plugin for Postgres skills and documentation. Helps AI coding tools generate better PostgreSQL code.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;pg-aiguide&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;AI-optimized PostgreSQL expertise for coding assistants&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;pg-aiguide helps AI coding tools write dramatically better PostgreSQL code. It provides:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Semantic search&lt;/strong&gt; across the official PostgreSQL manual (version-aware)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AI-optimized â€œskillsâ€&lt;/strong&gt; â€” curated, opinionated Postgres best practices used automatically by AI agents&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extension ecosystem docs&lt;/strong&gt;, starting with TimescaleDB, with more coming soon&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Use it either as:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;a &lt;strong&gt;public MCP server&lt;/strong&gt; that can be used with any AI coding agent, or&lt;/li&gt; 
 &lt;li&gt;a &lt;strong&gt;Claude Code plugin&lt;/strong&gt; optimized for use with Claude's native skill support.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;â­ Why pg-aiguide?&lt;/h2&gt; 
&lt;p&gt;AI coding tools often generate Postgres code that is:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;outdated&lt;/li&gt; 
 &lt;li&gt;missing constraints and indexes&lt;/li&gt; 
 &lt;li&gt;unaware of modern PG features&lt;/li&gt; 
 &lt;li&gt;inconsistent with real-world best practices&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;pg-aiguide fixes that by giving AI agents deep, versioned PostgreSQL knowledge and proven patterns.&lt;/p&gt; 
&lt;h3&gt;See the difference&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/5a426381-09b5-4635-9050-f55422253a3d"&gt;https://github.com/user-attachments/assets/5a426381-09b5-4635-9050-f55422253a3d&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Video Transcript &lt;/summary&gt; 
 &lt;p&gt;Prompt given to Claude Code:&lt;/p&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;Please describe the schema you would create for an e-commerce website two times, first with the tiger mcp server disabled, then with the tiger mcp server enabled. For each time, write the schema to its own file in the current working directory. Then compare the two files and let me know which approach generated the better schema, using both qualitative and quantitative reasons. For this example, only use standard Postgres.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;p&gt;Result (summarized):&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;4Ã— more constraints&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;55% more indexes&lt;/strong&gt; (including partial/expression indexes)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;PG17-recommended patterns&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Modern features&lt;/strong&gt; (&lt;code&gt;GENERATED ALWAYS AS IDENTITY&lt;/code&gt;, &lt;code&gt;NULLS NOT DISTINCT&lt;/code&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Cleaner naming &amp;amp; documentation&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Conclusion: &lt;em&gt;pg-aiguide produces more robust, performant, maintainable schemas.&lt;/em&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸš€ Quickstart&lt;/h2&gt; 
&lt;p&gt;pg-aiguide is available as a &lt;strong&gt;public MCP server&lt;/strong&gt;:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://mcp.tigerdata.com/docs"&gt;https://mcp.tigerdata.com/docs&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Manual MCP configuration using JSON&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "pg-aiguide": {
      "url": "https://mcp.tigerdata.com/docs"
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;Or it can be used as a &lt;strong&gt;Claude Code Plugin&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;claude plugin marketplace add timescale/pg-aiguide
claude plugin install pg@aiguide
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install by environment&lt;/h3&gt; 
&lt;h4&gt;One-click installs&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://cursor.com/en/install-mcp?name=pg-aiguide&amp;amp;config=eyJuYW1lIjoicGctYWlndWlkZSIsInR5cGUiOiJodHRwIiwidXJsIjoiaHR0cHM6Ly9tY3AudGlnZXJkYXRhLmNvbS9kb2NzIn0="&gt;&lt;img src="https://img.shields.io/badge/Install_in-Cursor-000000?style=flat-square&amp;amp;logoColor=white" alt="Install in Cursor" /&gt;&lt;/a&gt; &lt;a href="https://vscode.dev/redirect/mcp/install?name=pg-aiguide&amp;amp;config=%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Fmcp.tigerdata.com%2Fdocs%22%7D"&gt;&lt;img src="https://img.shields.io/badge/Install_in-VS_Code-0098FF?style=flat-square&amp;amp;logo=visualstudiocode&amp;amp;logoColor=white" alt="Install in VS Code" /&gt;&lt;/a&gt; &lt;a href="https://insiders.vscode.dev/redirect/mcp/install?name=pg-aiguide&amp;amp;config=%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Fmcp.tigerdata.com%2Fdocs%22%7D&amp;amp;quality=insiders"&gt;&lt;img src="https://img.shields.io/badge/Install_in-VS_Code_Insiders-24bfa5?style=flat-square&amp;amp;logo=visualstudiocode&amp;amp;logoColor=white" alt="Install in VS Code Insiders" /&gt;&lt;/a&gt; &lt;a href="https://vs-open.link/mcp-install?%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Fmcp.tigerdata.com%2Fdocs%22%7D"&gt;&lt;img src="https://img.shields.io/badge/Install_in-Visual_Studio-C16FDE?style=flat-square&amp;amp;logo=visualstudio&amp;amp;logoColor=white" alt="Install in Visual Studio" /&gt;&lt;/a&gt; &lt;a href="https://block.github.io/goose/extension?cmd=&amp;amp;arg=&amp;amp;id=pg-aiguide&amp;amp;name=pg-aiguide&amp;amp;description=MCP%20Server%20for%20pg-aiguide"&gt;&lt;img src="https://block.github.io/goose/img/extension-install-dark.svg?sanitize=true" alt="Install in Goose" /&gt;&lt;/a&gt; &lt;a href="https://lmstudio.ai/install-mcp?name=pg-aiguide&amp;amp;config=eyJuYW1lIjoicGctYWlndWlkZSIsInR5cGUiOiJodHRwIiwidXJsIjoiaHR0cHM6Ly9tY3AudGlnZXJkYXRhLmNvbS9kb2NzIn0="&gt;&lt;img src="https://files.lmstudio.ai/deeplink/mcp-install-light.svg?sanitize=true" alt="Add MCP Server pg-aiguide to LM Studio" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Claude Code&lt;/summary&gt; 
 &lt;p&gt;This repo serves as a claude code marketplace plugin. To install, run:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;claude plugin marketplace add timescale/pg-aiguide
claude plugin install pg@aiguide
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;This plugin uses the skills available in the &lt;code&gt;skills&lt;/code&gt; directory as well as our publicly available MCP server endpoint hosted by TigerData for searching PostgreSQL documentation.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; Codex &lt;/summary&gt; 
 &lt;p&gt;Run the following to add the MCP server to codex:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;codex mcp add --url "https://mcp.tigerdata.com/docs" pg-aiguide
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; Cursor &lt;/summary&gt; 
 &lt;p&gt;One-click install:&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://cursor.com/en-US/install-mcp?name=pg-aiguide&amp;amp;config=eyJ1cmwiOiJodHRwczovL21jcC50aWdlcmRhdGEuY29tL2RvY3MifQ%3D%3D"&gt;&lt;img src="https://cursor.com/deeplink/mcp-install-dark.svg?sanitize=true" alt="Install MCP Server" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;Or add the following to &lt;code&gt;.cursor/mcp.json&lt;/code&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "pg-aiguide": {
      "url": "https://mcp.tigerdata.com/docs"
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; Gemini CLI &lt;/summary&gt; 
 &lt;p&gt;Run the following to add the MCP server to Gemini CLI:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;gemini mcp add -s user pg-aiguide "https://mcp.tigerdata.com/docs" -t http
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; Visual Studio &lt;/summary&gt; 
 &lt;p&gt;Click the button to install:&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://vs-open.link/mcp-install?%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Fmcp.tigerdata.com%2Fdocs%22%7D"&gt;&lt;img src="https://img.shields.io/badge/Install_in-Visual_Studio-C16FDE?style=flat-square&amp;amp;logo=visualstudio&amp;amp;logoColor=white" alt="Install in Visual Studio" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; VS Code &lt;/summary&gt; 
 &lt;p&gt;Click the button to install:&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://vscode.dev/redirect/mcp/install?name=pg-aiguide&amp;amp;config=%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Fmcp.tigerdata.com%2Fdocs%22%7D"&gt;&lt;img src="https://img.shields.io/badge/Install_in-VS_Code-0098FF?style=flat-square&amp;amp;logo=visualstudiocode&amp;amp;logoColor=white" alt="Install in VS Code" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;Alternatively, run the following to add the MCP server to VS Code:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;code --add-mcp '{"name":"pg-aiguide","type":"http","url":"https://mcp.tigerdata.com/docs"}'
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; VS Code Insiders &lt;/summary&gt; 
 &lt;p&gt;Click the button to install:&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://insiders.vscode.dev/redirect/mcp/install?name=pg-aiguide&amp;amp;config=%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Fmcp.tigerdata.com%2Fdocs%22%7D&amp;amp;quality=insiders"&gt;&lt;img src="https://img.shields.io/badge/Install_in-VS_Code_Insiders-24bfa5?style=flat-square&amp;amp;logo=visualstudiocode&amp;amp;logoColor=white" alt="Install in VS Code Insiders" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;Alternatively, run the following to add the MCP server to VS Code Insiders:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;code-insiders --add-mcp '{"name":"pg-aiguide","type":"http","url":"https://mcp.tigerdata.com/docs"}'
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; Windsurf &lt;/summary&gt; 
 &lt;p&gt;Add the following to &lt;code&gt;~/.codeium/windsurf/mcp_config.json&lt;/code&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "pg-aiguide": {
      "serverUrl": "https://mcp.tigerdata.com/docs"
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;ğŸ’¡ Your First Prompt&lt;/h3&gt; 
&lt;p&gt;Once installed, pg-aiguide can answer Postgres questions or design schemas.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Simple schema example prompt&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Create a Postgres table schema for storing usernames and unique email addresses.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Complex schema example prompt&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;You are a senior software engineer. You are given a task to generate a Postgres schema for an IoT device company. The devices collect environmental data on a factory floor. The data includes temperature, humidity, pressure, as the main data points as well as other measurements that vary from device to device. Each device has a unique id and a human-readable name. We want to record the time the data was collected as well. Analysis for recent data includes finding outliers and anomalies based on measurements, as well as analyzing the data of particular devices for ad-hoc analysis. Historical data analysis includes analyzing the history of data for one device or getting statistics for all devices over long periods of time.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;h3&gt;Semantic Search (MCP Tools)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/timescale/pg-aiguide/main/API.md#semantic_search_postgres_docs"&gt;&lt;strong&gt;&lt;code&gt;semantic_search_postgres_docs&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt; Performs semantic search over the official PostgreSQL manual, with results scoped to a specific Postgres version.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/timescale/pg-aiguide/main/API.md#semantic_search_tiger_docs"&gt;&lt;strong&gt;&lt;code&gt;semantic_search_tiger_docs&lt;/code&gt;&lt;/strong&gt; &lt;/a&gt; Searches Tiger Dataâ€™s documentation corpus, including TimescaleDB and future ecosystem extensions.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Skills (AI-Optimized Best Practices)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/timescale/pg-aiguide/main/API.md#view_skill"&gt;&lt;code&gt;view_skill&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; Exposes curated, opinionated PostgreSQL best-practice skills used automatically by AI coding assistants.&lt;/p&gt; &lt;p&gt;These skills provide guidance on:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Schema design&lt;/li&gt; 
   &lt;li&gt;Indexing strategies&lt;/li&gt; 
   &lt;li&gt;Data types&lt;/li&gt; 
   &lt;li&gt;Data integrity and constraints&lt;/li&gt; 
   &lt;li&gt;Naming conventions&lt;/li&gt; 
   &lt;li&gt;Performance tuning&lt;/li&gt; 
   &lt;li&gt;Modern PostgreSQL features&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ”Œ Ecosystem Documentation&lt;/h2&gt; 
&lt;p&gt;Supported today:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;TimescaleDB&lt;/strong&gt; (docs + skills)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Coming soon:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;pgvector&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;PostGIS&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We welcome contributions for additional extensions and tools.&lt;/p&gt; 
&lt;h2&gt;ğŸ›  Development&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/timescale/pg-aiguide/main/DEVELOPMENT.md"&gt;DEVELOPMENT.md&lt;/a&gt; for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;running the MCP server locally&lt;/li&gt; 
 &lt;li&gt;adding new skills&lt;/li&gt; 
 &lt;li&gt;adding new docs&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ¤ Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;new Postgres best-practice skills&lt;/li&gt; 
 &lt;li&gt;additional documentation corpora&lt;/li&gt; 
 &lt;li&gt;search quality improvements&lt;/li&gt; 
 &lt;li&gt;bug reports and feature ideas&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; 
&lt;p&gt;Apache 2.0&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenBB-finance/OpenBB</title>
      <link>https://github.com/OpenBB-finance/OpenBB</link>
      <description>&lt;p&gt;Financial data platform for analysts, quants and AI agents.&lt;/p&gt;&lt;hr&gt;&lt;br /&gt; 
&lt;img src="https://github.com/OpenBB-finance/OpenBB/raw/develop/images/odp-light.svg?raw=true#gh-light-mode-only" alt="Open Data Platform by OpenBB logo" width="600" /&gt; 
&lt;img src="https://github.com/OpenBB-finance/OpenBB/raw/develop/images/odp-dark.svg?raw=true#gh-dark-mode-only" alt="Open Data Platform by OpenBB logo" width="600" /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href="https://x.com/openbb_finance"&gt;&lt;img src="https://img.shields.io/twitter/url/https/twitter.com/openbb_finance.svg?style=social&amp;amp;label=Follow%20%40openbb_finance" alt="Twitter" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/xPHTuHCmuV"&gt;&lt;img src="https://img.shields.io/discord/831165782750789672" alt="Discord Shield" /&gt;&lt;/a&gt; &lt;a href="https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/OpenBB-finance/OpenBB"&gt;&lt;img src="https://img.shields.io/static/v1?label=Dev%20Containers&amp;amp;message=Open&amp;amp;color=blue&amp;amp;logo=visualstudiocode" alt="Open in Dev Containers" /&gt;&lt;/a&gt; &lt;a href="https://codespaces.new/OpenBB-finance/OpenBB"&gt; &lt;img src="https://github.com/codespaces/badge.svg?sanitize=true" height="20" /&gt; &lt;/a&gt; &lt;a target="_blank" href="https://colab.research.google.com/github/OpenBB-finance/OpenBB/blob/develop/examples/googleColab.ipynb"&gt; &lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt; &lt;/a&gt; &lt;a href="https://pypi.org/project/openbb/"&gt;&lt;img src="https://img.shields.io/pypi/v/openbb?color=blue&amp;amp;label=PyPI%20Package" alt="PyPI" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Open Data Platform by OpenBB (ODP) is the open-source toolset that helps data engineers integrate proprietary, licensed, and public data sources into downstream applications like AI copilots and research dashboards.&lt;/p&gt; 
&lt;p&gt;ODP operates as the "connect once, consume everywhere" infrastructure layer that consolidates and exposes data to multiple surfaces at once: Python environments for quants, OpenBB Workspace and Excel for analysts, MCP servers for AI agents, and REST APIs for other applications.&lt;/p&gt; 
&lt;a href="https://pro.openbb.co"&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://openbb-cms.directus.app/assets/70b971ef-7a7e-486e-b5ae-1cc602f2162c.png" alt="Logo" width="1000" /&gt; 
 &lt;/div&gt; &lt;/a&gt; 
&lt;p&gt;Get started with: &lt;code&gt;pip install openbb&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openbb import obb
output = obb.equity.price.historical("AAPL")
df = output.to_dataframe()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Data integrations available can be found here: &lt;a href="https://docs.openbb.co/python/reference"&gt;https://docs.openbb.co/python/reference&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;OpenBB Workspace&lt;/h2&gt; 
&lt;p&gt;While the Open Data Platform provides the open-source data integration foundation, &lt;strong&gt;OpenBB Workspace&lt;/strong&gt; offers the enterprise UI for analysts to visualize datasets and leverage AI agents. The platform's "connect once, consume everywhere" architecture enables seamless integration between the two.&lt;/p&gt; 
&lt;p&gt;You can find OpenBB Workspace at &lt;a href="https://pro.openbb.co"&gt;https://pro.openbb.co&lt;/a&gt;. &lt;a href="https://pro.openbb.co"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;a href="https://pro.openbb.co"&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://openbb-cms.directus.app/assets/f69b6aaf-0821-4bc8-a43c-715e03a924ef.png" alt="Logo" width="1000" /&gt; 
 &lt;/div&gt; &lt;/a&gt; 
&lt;p&gt;Data integration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can learn more about adding data to the OpenBB workspace from the &lt;a href="https://docs.openbb.co/workspace"&gt;docs&lt;/a&gt; or &lt;a href="https://github.com/OpenBB-finance/backends-for-openbb"&gt;this open source repository&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;AI Agents integration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can learn more about adding AI agents to the OpenBB workspace from &lt;a href="https://github.com/OpenBB-finance/agents-for-openbb"&gt;this open source repository&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Integrating Open Data Platform to the OpenBB Workspace&lt;/h3&gt; 
&lt;p&gt;Connect this library to the OpenBB Workspace with a few simple commands, in a Python (3.9.21 - 3.12) environment.&lt;/p&gt; 
&lt;h4&gt;Run an ODP backend&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install the packages.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install "openbb[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Start the API server over localhost.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;openbb-api
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will launch a FastAPI server, via Uvicorn, at &lt;code&gt;127.0.0.1:6900&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can check that it works by going to &lt;a href="http://127.0.0.1:6900"&gt;http://127.0.0.1:6900&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Integrate the ODP Backend to OpenBB Workspace&lt;/h4&gt; 
&lt;p&gt;Sign-in to the &lt;a href="https://pro.openbb.co/"&gt;OpenBB Workspace&lt;/a&gt;, and follow the following steps:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/75cffb4a-5e95-470a-b9d0-6ffd4067e069" alt="CleanShot 2025-05-17 at 09 51 56@2x" /&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Go to the "Apps" tab&lt;/li&gt; 
 &lt;li&gt;Click on "Connect backend"&lt;/li&gt; 
 &lt;li&gt;Fill in the form with: Name: Open Data Platform URL: &lt;a href="http://127.0.0.1:6900"&gt;http://127.0.0.1:6900&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Click on "Test". You should get a "Test successful" with the number of apps found.&lt;/li&gt; 
 &lt;li&gt;Click on "Add".&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;That's it.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;!-- TABLE OF CONTENTS --&gt; 
&lt;details closed="closed"&gt; 
 &lt;summary&gt;&lt;h2 style="display: inline-block"&gt;Table of Contents&lt;/h2&gt;&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#1-installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#2-contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#3-license"&gt;License&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#4-disclaimer"&gt;Disclaimer&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#5-contacts"&gt;Contacts&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#6-star-history"&gt;Star History&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#7-contributors"&gt;Contributors&lt;/a&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h2&gt;1. Installation&lt;/h2&gt; 
&lt;p&gt;The ODP Python Package can be installed from &lt;a href="https://pypi.org/project/openbb/"&gt;PyPI package&lt;/a&gt; by running &lt;code&gt;pip install openbb&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;or by cloning the repository directly with &lt;code&gt;git clone https://github.com/OpenBB-finance/OpenBB.git&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Please find more about the installation process, in the &lt;a href="https://docs.openbb.co/python/installation"&gt;OpenBB Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;ODP CLI installation&lt;/h3&gt; 
&lt;p&gt;The ODP CLI is a command-line interface that allows you to access the ODP directly from your command line.&lt;/p&gt; 
&lt;p&gt;It can be installed by running &lt;code&gt;pip install openbb-cli&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;or by cloning the repository directly with &lt;code&gt;git clone https://github.com/OpenBB-finance/OpenBB.git&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Please find more about the installation process in the &lt;a href="https://docs.openbb.co/cli/installation"&gt;OpenBB Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;2. Contributing&lt;/h2&gt; 
&lt;p&gt;There are three main ways of contributing to this project. (Hopefully you have starred the project by now â­ï¸)&lt;/p&gt; 
&lt;h3&gt;Become a Contributor&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;More information on our &lt;a href="https://docs.openbb.co/python/developer"&gt;Developer Documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Create a GitHub ticket&lt;/h3&gt; 
&lt;p&gt;Before creating a ticket make sure the one you are creating doesn't exist already &lt;a href="https://github.com/OpenBB-finance/OpenBB/issues"&gt;among the existing issues&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=bug&amp;amp;template=bug_report.md&amp;amp;title=%5BBug%5D"&gt;Report bug&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=enhancement&amp;amp;template=enhancement.md&amp;amp;title=%5BIMPROVE%5D"&gt;Suggest improvement&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=new+feature&amp;amp;template=feature_request.md&amp;amp;title=%5BFR%5D"&gt;Request a feature&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Provide feedback&lt;/h3&gt; 
&lt;p&gt;We are most active on &lt;a href="https://openbb.co/discord"&gt;our Discord&lt;/a&gt;, but feel free to reach out to us in any of &lt;a href="https://openbb.co/links"&gt;our social media&lt;/a&gt; for feedback.&lt;/p&gt; 
&lt;h2&gt;3. License&lt;/h2&gt; 
&lt;p&gt;Distributed under the AGPLv3 License. See &lt;a href="https://github.com/OpenBB-finance/OpenBB/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;4. Disclaimer&lt;/h2&gt; 
&lt;p&gt;Trading in financial instruments involves high risks including the risk of losing some, or all, of your investment amount, and may not be suitable for all investors.&lt;/p&gt; 
&lt;p&gt;Before deciding to trade in a financial instrument you should be fully informed of the risks and costs associated with trading the financial markets, carefully consider your investment objectives, level of experience, and risk appetite, and seek professional advice where needed.&lt;/p&gt; 
&lt;p&gt;The data contained in the Open Data Platform is not necessarily accurate.&lt;/p&gt; 
&lt;p&gt;OpenBB and any provider of the data contained in this website will not accept liability for any loss or damage as a result of your trading, or your reliance on the information displayed.&lt;/p&gt; 
&lt;p&gt;All names, logos, and brands of third parties that may be referenced in our sites, products or documentation are trademarks of their respective owners. Unless otherwise specified, OpenBB and its products and services are not endorsed by, sponsored by, or affiliated with these third parties.&lt;/p&gt; 
&lt;p&gt;Our use of these names, logos, and brands is for identification purposes only, and does not imply any such endorsement, sponsorship, or affiliation.&lt;/p&gt; 
&lt;h2&gt;5. Contacts&lt;/h2&gt; 
&lt;p&gt;If you have any questions about the platform or anything OpenBB, feel free to email us at &lt;code&gt;support@openbb.co&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;If you want to say hi, or are interested in partnering with us, feel free to reach us at &lt;code&gt;hello@openbb.co&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Any of our social media platforms: &lt;a href="https://openbb.co/links"&gt;openbb.co/links&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;6. Star History&lt;/h2&gt; 
&lt;p&gt;This is a proxy of our growth and that we are just getting started.&lt;/p&gt; 
&lt;p&gt;But for more metrics important to us check &lt;a href="https://openbb.co/open"&gt;openbb.co/open&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;amp;type=Date&amp;amp;theme=dark"&gt;&lt;img src="https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;amp;type=Date&amp;amp;theme=dark" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;7. Contributors&lt;/h2&gt; 
&lt;p&gt;OpenBB wouldn't be OpenBB without you. If we are going to disrupt financial industry, every contribution counts. Thank you for being part of this journey.&lt;/p&gt; 
&lt;a href="https://github.com/OpenBB-finance/OpenBB/graphs/contributors"&gt; &lt;img src="https://contributors-img.web.app/image?repo=OpenBB-finance/OpenBB" width="800" /&gt; &lt;/a&gt; 
&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; 
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</description>
    </item>
    
    <item>
      <title>camel-ai/owl</title>
      <link>https://github.com/camel-ai/owl</link>
      <description>&lt;p&gt;ğŸ¦‰ OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; ğŸ¦‰ OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation &lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://camel-ai.github.io/camel/index.html"&gt;&lt;img src="https://img.shields.io/badge/Documentation-EB3ECC" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://discord.camel-ai.org/"&gt;&lt;img src="https://img.shields.io/discord/1082486657678311454?logo=discord&amp;amp;labelColor=%20%235462eb&amp;amp;logoColor=%20%23f5f5f5&amp;amp;color=%20%235462eb" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://x.com/CamelAIOrg"&gt;&lt;img src="https://img.shields.io/twitter/follow/CamelAIOrg?style=social" alt="X" /&gt;&lt;/a&gt; &lt;a href="https://www.reddit.com/r/CamelAI/"&gt;&lt;img src="https://img.shields.io/reddit/subreddit-subscribers/CamelAI?style=plastic&amp;amp;logo=reddit&amp;amp;label=r%2FCAMEL&amp;amp;labelColor=white" alt="Reddit" /&gt;&lt;/a&gt; &lt;a href="https://ghli.org/camel/wechat.png"&gt;&lt;img src="https://img.shields.io/badge/WeChat-CamelAIOrg-brightgreen?logo=wechat&amp;amp;logoColor=white" alt="Wechat" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/assets/qr_code.jpg"&gt;&lt;img src="https://img.shields.io/badge/WeChat-OWLProject-brightgreen?logo=wechat&amp;amp;logoColor=white" alt="Wechat" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/camel-ai"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-CAMEL--AI-ffc107?color=ffc107&amp;amp;logoColor=white" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://github.com/camel-ai/owl/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/camel-ai/owl?label=stars&amp;amp;logo=github&amp;amp;color=brightgreen" alt="Star" /&gt;&lt;/a&gt; &lt;a href="https://github.com/camel-ai/owl/raw/main/licenses/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true" alt="Package License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center" style="background-color: #e3f2fd; padding: 20px; border-radius: 15px; border: 3px solid #1976d2; margin: 25px 0;"&gt; 
 &lt;h2 style="color: #1976d2; margin: 0 0 15px 0; font-size: 1.8em;"&gt; ğŸš€ &lt;b&gt;Introducing Eigent: The World's First Multi-Agent Workforce Desktop Application&lt;/b&gt; ğŸš€ &lt;/h2&gt; 
 &lt;p style="font-size: 1.2em; margin: 10px 0; line-height: 1.6;"&gt; &lt;b&gt;Eigent&lt;/b&gt; empowers you to build, manage, and deploy a custom AI workforce that can turn your most complex workflows into automated tasks. &lt;/p&gt; 
 &lt;p style="font-size: 1.1em; margin: 15px 0;"&gt; âœ¨ &lt;b&gt;100% Open Source&lt;/b&gt; â€¢ ğŸ”§ &lt;b&gt;Fully Customizable&lt;/b&gt; â€¢ ğŸ”’ &lt;b&gt;Privacy-First&lt;/b&gt; â€¢ âš¡ &lt;b&gt;Parallel Execution&lt;/b&gt; &lt;/p&gt; 
 &lt;p style="font-size: 1em; margin: 15px 0; font-style: italic;"&gt; Built on CAMEL-AI's acclaimed open-source project, Eigent introduces a Multi-Agent Workforce that boosts productivity through parallel execution, customization, and privacy protection. &lt;/p&gt; 
 &lt;div style="margin-top: 20px;"&gt; 
  &lt;a href="https://github.com/eigent-ai/eigent" style="background-color: #d81b60; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; font-weight: bold; margin: 0 5px;"&gt;ğŸ”— Visit Eigent Repo&lt;/a&gt; 
  &lt;a href="https://www.eigent.ai/" style="background-color: #1976d2; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; font-weight: bold; margin: 0 5px;"&gt;Learn More&lt;/a&gt; 
  &lt;a href="https://www.eigent.ai/download" style="background-color: #43a047; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; font-weight: bold; margin: 0 5px;"&gt;Get Started&lt;/a&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;h4 align="center"&gt; &lt;p&gt;&lt;a href="https://github.com/camel-ai/owl/tree/main/README_zh.md"&gt;ä¸­æ–‡é˜…è¯»&lt;/a&gt; | &lt;a href="https://github.com/camel-ai/owl#community"&gt;Community&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#%EF%B8%8F-installation"&gt;Installation&lt;/a&gt; | &lt;a href="https://github.com/camel-ai/owl/tree/main/owl"&gt;Examples&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2505.23885"&gt;Paper&lt;/a&gt; |&lt;/p&gt; 
  &lt;!-- [Technical Report](https://hypnotic-mind-6bd.notion.site/OWL-Optimized-Workforce-Learning-for-General-Multi-Agent-Assistance-in-Real-World-Task-Automation-1d4004aeb21380158749c7f84b20643f) | --&gt; &lt;p&gt;&lt;a href="https://github.com/camel-ai/owl#citation"&gt;Citation&lt;/a&gt; | &lt;a href="https://github.com/camel-ai/owl/graphs/contributors"&gt;Contributing&lt;/a&gt; | &lt;a href="https://www.camel-ai.org/"&gt;CAMEL-AI&lt;/a&gt;&lt;/p&gt; &lt;/h4&gt; 
 &lt;div align="center" style="background-color: #f0f7ff; padding: 10px; border-radius: 5px; margin: 15px 0;"&gt; 
  &lt;h3 style="color: #1e88e5; margin: 0;"&gt; ğŸ† OWL achieves &lt;span style="color: #d81b60; font-weight: bold; font-size: 1.2em;"&gt;69.09&lt;/span&gt; average score on GAIA benchmark and ranks &lt;span style="color: #d81b60; font-weight: bold; font-size: 1.2em;"&gt;ğŸ…ï¸ #1&lt;/span&gt; among open-source frameworks! ğŸ† &lt;/h3&gt; 
 &lt;/div&gt; 
 &lt;div align="center"&gt; 
  &lt;p&gt;ğŸ¦‰ OWL is a cutting-edge framework for multi-agent collaboration that pushes the boundaries of task automation, built on top of the &lt;a href="https://github.com/camel-ai/camel"&gt;CAMEL-AI Framework&lt;/a&gt;.&lt;/p&gt; 
  &lt;!-- OWL achieves **58.18** average score on [GAIA](https://huggingface.co/spaces/gaia-benchmark/leaderboard) benchmark and ranks ğŸ…ï¸ #1 among open-source frameworks. --&gt; 
  &lt;p&gt;Our vision is to revolutionize how AI agents collaborate to solve real-world tasks. By leveraging dynamic agent interactions, OWL enables more natural, efficient, and robust task automation across diverse domains.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/camel-ai/owl/main/assets/owl_architecture.png" alt="" /&gt;&lt;/p&gt; 
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;!-- # Key Features --&gt; 
&lt;h1&gt;ğŸ“‹ Table of Contents&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-table-of-contents"&gt;ğŸ“‹ Table of Contents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-eigent-multi-agent-workforce-desktop-application"&gt;ğŸš€ Eigent: Multi-Agent Workforce Desktop Application&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-news"&gt;ğŸ”¥ News&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-demo-video"&gt;ğŸ¬ Demo Video&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#%EF%B8%8F-core-features"&gt;âœ¨ï¸ Core Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#%EF%B8%8F-installation"&gt;ğŸ› ï¸ Installation&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#prerequisites"&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#install-python"&gt;Install Python&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#installation-options"&gt;&lt;strong&gt;Installation Options&lt;/strong&gt;&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#option-1-using-uv-recommended"&gt;Option 1: Using uv (Recommended)&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#option-2-using-venv-and-pip"&gt;Option 2: Using venv and pip&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#option-3-using-conda"&gt;Option 3: Using conda&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#option-4-using-docker"&gt;Option 4: Using Docker&lt;/a&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#using-pre-built-image-recommended"&gt;&lt;strong&gt;Using Pre-built Image (Recommended)&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#building-image-locally"&gt;&lt;strong&gt;Building Image Locally&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#using-convenience-scripts"&gt;&lt;strong&gt;Using Convenience Scripts&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#setup-environment-variables"&gt;&lt;strong&gt;Setup Environment Variables&lt;/strong&gt;&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#setting-environment-variables-directly"&gt;Setting Environment Variables Directly&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#alternative-using-a-env-file"&gt;Alternative: Using a &lt;code&gt;.env&lt;/code&gt; File&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#mcp-desktop-commander-setup"&gt;&lt;strong&gt;MCP Desktop Commander Setup&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-quick-start"&gt;ğŸš€ Quick Start&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#basic-usage"&gt;Basic Usage&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#running-with-different-models"&gt;Running with Different Models&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#model-requirements"&gt;Model Requirements&lt;/a&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#supported-models"&gt;Supported Models&lt;/a&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#example-tasks"&gt;Example Tasks&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-toolkits-and-capabilities"&gt;ğŸ§° Toolkits and Capabilities&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#model-context-protocol-mcp"&gt;Model Context Protocol (MCP)&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#install-nodejs"&gt;&lt;strong&gt;Install Node.js&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#windows"&gt;Windows&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#linux"&gt;Linux&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#mac"&gt;Mac&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#install-playwright-mcp-service"&gt;&lt;strong&gt;Install Playwright MCP Service&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#available-toolkits"&gt;Available Toolkits&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#available-toolkits-1"&gt;Available Toolkits&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#multimodal-toolkits-require-multimodal-model-capabilities"&gt;Multimodal Toolkits (Require multimodal model capabilities)&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#text-based-toolkits"&gt;Text-Based Toolkits&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#customizing-your-configuration"&gt;Customizing Your Configuration&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-web-interface"&gt;ğŸŒ Web Interface&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#starting-the-web-ui"&gt;Starting the Web UI&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-experiments"&gt;ğŸ§ª Experiments&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#%EF%B8%8F-future-plans"&gt;â±ï¸ Future Plans&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-license"&gt;ğŸ“„ License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-contributing"&gt;ğŸ¤ Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-community"&gt;ğŸ”¥ Community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-faq"&gt;â“ FAQ&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#general-questions"&gt;General Questions&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#experiment-questions"&gt;Experiment Questions&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-exploring-camel-dependency"&gt;ğŸ“š Exploring CAMEL Dependency&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#accessing-camel-source-code"&gt;Accessing CAMEL Source Code&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#%EF%B8%8F-cite"&gt;ğŸ–Šï¸ Cite&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-star-history"&gt;â­ Star History&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;ğŸš€ Eigent: Multi-Agent Workforce Desktop Application&lt;/h1&gt; 
&lt;div align="center" style="background-color: #f5f5f5; padding: 20px; border-radius: 10px; margin: 20px 0;"&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/eigent-ai/eigent"&gt;Eigent&lt;/a&gt;&lt;/strong&gt; is revolutionizing the way we work with AI agents. As the world's first Multi-Agent Workforce desktop application, Eigent transforms complex workflows into automated, intelligent processes.&lt;/p&gt; 
 &lt;h3&gt;Why Eigent?&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;ğŸ¤– Multi-Agent Collaboration&lt;/strong&gt;: Deploy multiple specialized AI agents that work together seamlessly&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;ğŸš€ Parallel Execution&lt;/strong&gt;: Boost productivity with agents that can work on multiple tasks simultaneously&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;ğŸ¨ Full Customization&lt;/strong&gt;: Build and configure your AI workforce to match your specific needs&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;ğŸ”’ Privacy-First Design&lt;/strong&gt;: Your data stays on your machine - no cloud dependencies required&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;ğŸ’¯ 100% Open Source&lt;/strong&gt;: Complete transparency and community-driven development&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;Key Capabilities&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Build Custom Workflows&lt;/strong&gt;: Design complex multi-step processes that agents can execute autonomously&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Manage AI Teams&lt;/strong&gt;: Orchestrate multiple agents with different specializations working in concert&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Deploy Instantly&lt;/strong&gt;: From idea to execution in minutes, not hours&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Monitor Progress&lt;/strong&gt;: Real-time visibility into agent activities and task completion&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;Use Cases&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ğŸ“Š &lt;strong&gt;Data Analysis&lt;/strong&gt;: Automate complex data processing and analysis workflows&lt;/li&gt; 
  &lt;li&gt;ğŸ” &lt;strong&gt;Research&lt;/strong&gt;: Deploy agents to gather, synthesize, and report on information&lt;/li&gt; 
  &lt;li&gt;ğŸ’» &lt;strong&gt;Development&lt;/strong&gt;: Accelerate coding tasks with AI-powered development teams&lt;/li&gt; 
  &lt;li&gt;ğŸ“ &lt;strong&gt;Content Creation&lt;/strong&gt;: Generate, edit, and optimize content at scale&lt;/li&gt; 
  &lt;li&gt;ğŸ¤ &lt;strong&gt;Business Automation&lt;/strong&gt;: Transform repetitive business processes into automated workflows&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;Get Started with Eigent&lt;/h3&gt; 
 &lt;p&gt;Eigent is built on top of the OWL framework, leveraging CAMEL-AI's powerful multi-agent capabilities.&lt;/p&gt; 
 &lt;p&gt;ğŸ”— &lt;strong&gt;&lt;a href="https://github.com/eigent-ai/eigent"&gt;Visit the Eigent Repository&lt;/a&gt;&lt;/strong&gt; to explore the codebase, contribute, or learn more about building your own AI workforce.&lt;/p&gt; 
 &lt;p&gt;Follow our &lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#%EF%B8%8F-installation"&gt;installation guide&lt;/a&gt; to start building your own AI workforce today!&lt;/p&gt; 
&lt;/div&gt; 
&lt;h1&gt;ğŸ”¥ News&lt;/h1&gt; 
&lt;div align="center" style="background-color: #e8f5e9; padding: 15px; border-radius: 10px; border: 2px solid #4caf50; margin: 20px 0;"&gt; 
 &lt;h3 style="color: #2e7d32; margin: 0; font-size: 1.3em;"&gt; ğŸ§© &lt;b&gt;NEW: COMMUNITY AGENT CHALLENGES!&lt;/b&gt; ğŸ§© &lt;/h3&gt; 
 &lt;p style="font-size: 1.1em; margin: 10px 0;"&gt; Showcase your creativity by designing unique challenges for AI agents! &lt;br /&gt; Join our community and see your innovative ideas tackled by cutting-edge AI. &lt;/p&gt; 
 &lt;p&gt; &lt;a href="https://github.com/camel-ai/owl/raw/main/community_challenges.md" style="background-color: #2e7d32; color: white; padding: 8px 15px; text-decoration: none; border-radius: 5px; font-weight: bold;"&gt;View &amp;amp; Submit Challenges&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;!-- &lt;div style="background-color: #e3f2fd; padding: 12px; border-radius: 8px; border-left: 4px solid #1e88e5; margin: 10px 0;"&gt;
  &lt;h4 style="color: #1e88e5; margin: 0 0 8px 0;"&gt;
    ğŸ‰ Latest Major Update - March 15, 2025
  &lt;/h4&gt;
  &lt;p style="margin: 0;"&gt;
    &lt;b&gt;Significant Improvements:&lt;/b&gt;
    &lt;ul style="margin: 5px 0 0 0; padding-left: 20px;"&gt;
      &lt;li&gt;Restructured web-based UI architecture for enhanced stability ğŸ—ï¸&lt;/li&gt;
      &lt;li&gt;Optimized OWL Agent execution mechanisms for better performance ğŸš€&lt;/li&gt;
    &lt;/ul&gt;
    &lt;i&gt;Try it now and experience the improved performance in your automation tasks!&lt;/i&gt;
  &lt;/p&gt;
&lt;/div&gt; --&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.09.22]&lt;/strong&gt;: Exicited to announce that OWL has been accepted by NeurIPS 2025!ğŸš€ Check the latest paper &lt;a href="https://arxiv.org/abs/2505.23885"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.07.21]&lt;/strong&gt;: We open-sourced the training dataset and model checkpoints of OWL project. Training code coming soon. &lt;a href="https://huggingface.co/collections/camel-ai/optimized-workforce-learning-682ef4ab498befb9426e6e27"&gt;huggingface link&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.05.27]&lt;/strong&gt;: We released the technical report of OWL, including more details on the workforce (framework) and optimized workforce learning (training methodology). &lt;a href="https://arxiv.org/abs/2505.23885"&gt;paper&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.05.18]&lt;/strong&gt;: We open-sourced an initial version for replicating workforce experiment on GAIA &lt;a href="https://github.com/camel-ai/owl/tree/gaia69"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.04.18]&lt;/strong&gt;: We uploaded OWL's new GAIA benchmark score of &lt;strong&gt;69.09%&lt;/strong&gt;, ranking #1 among open-source frameworks. Check the technical report &lt;a href="https://hypnotic-mind-6bd.notion.site/OWL-Optimized-Workforce-Learning-for-General-Multi-Agent-Assistance-in-Real-World-Task-Automation-1d4004aeb21380158749c7f84b20643f"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.27]&lt;/strong&gt;: Integrate SearxNGToolkit performing web searches using SearxNG search engine.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.26]&lt;/strong&gt;: Enhanced Browser Toolkit with multi-browser support for "chrome", "msedge", and "chromium" channels.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.25]&lt;/strong&gt;: Supported Gemini 2.5 Pro, added example run code&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.21]&lt;/strong&gt;: Integrated OpenRouter model platform, fix bug with Gemini tool calling.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.20]&lt;/strong&gt;: Accept header in MCP Toolkit, support automatic playwright installation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.16]&lt;/strong&gt;: Support Bing search, Baidu search.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.12]&lt;/strong&gt;: Added Bocha search in SearchToolkit, integrated Volcano Engine model platform, and enhanced Azure and OpenAI Compatible models with structured output and tool calling.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.11]&lt;/strong&gt;: We added MCPToolkit, FileWriteToolkit, and TerminalToolkit to enhance OWL agents with MCP tool calling, file writing capabilities, and terminal command execution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.09]&lt;/strong&gt;: We added a web-based user interface that makes it easier to interact with the system.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.07]&lt;/strong&gt;: We open-sourced the codebase of the ğŸ¦‰ OWL project.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.03]&lt;/strong&gt;: OWL achieved the #1 position among open-source frameworks on the GAIA benchmark with a score of 58.18.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;ğŸ¬ Demo Video&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/2a2a825d-39ea-45c5-9ba1-f9d58efbc372"&gt;https://github.com/user-attachments/assets/2a2a825d-39ea-45c5-9ba1-f9d58efbc372&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://private-user-images.githubusercontent.com/55657767/420212194-e813fc05-136a-485f-8df3-f10d9b4e63ec.mp4"&gt;https://private-user-images.githubusercontent.com/55657767/420212194-e813fc05-136a-485f-8df3-f10d9b4e63ec.mp4&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This video demonstrates how to install OWL locally and showcases its capabilities as a cutting-edge framework for multi-agent collaboration: &lt;a href="https://www.youtube.com/watch?v=8XlqVyAZOr8"&gt;https://www.youtube.com/watch?v=8XlqVyAZOr8&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;âœ¨ï¸ Core Features&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Online Search&lt;/strong&gt;: Support for multiple search engines (including Wikipedia, Google, DuckDuckGo, Baidu, Bocha, etc.) for real-time information retrieval and knowledge acquisition.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multimodal Processing&lt;/strong&gt;: Support for handling internet or local videos, images, and audio data.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Browser Automation&lt;/strong&gt;: Utilize the Playwright framework for simulating browser interactions, including scrolling, clicking, input handling, downloading, navigation, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Document Parsing&lt;/strong&gt;: Extract content from Word, Excel, PDF, and PowerPoint files, converting them into text or Markdown format.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code Execution&lt;/strong&gt;: Write and execute Python code using interpreter.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Built-in Toolkits&lt;/strong&gt;: Access to a comprehensive set of built-in toolkits including: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt;: A universal protocol layer that standardizes AI model interactions with various tools and data sources&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Core Toolkits&lt;/strong&gt;: ArxivToolkit, AudioAnalysisToolkit, CodeExecutionToolkit, DalleToolkit, DataCommonsToolkit, ExcelToolkit, GitHubToolkit, GoogleMapsToolkit, GoogleScholarToolkit, ImageAnalysisToolkit, MathToolkit, NetworkXToolkit, NotionToolkit, OpenAPIToolkit, RedditToolkit, SearchToolkit, SemanticScholarToolkit, SymPyToolkit, VideoAnalysisToolkit, WeatherToolkit, BrowserToolkit, and many more for specialized tasks&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;ğŸ› ï¸ Installation&lt;/h1&gt; 
&lt;h2&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;/h2&gt; 
&lt;h3&gt;Install Python&lt;/h3&gt; 
&lt;p&gt;Before installing OWL, ensure you have Python installed (version 3.10, 3.11, or 3.12 is supported):&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note for GAIA Benchmark Users&lt;/strong&gt;: When running the GAIA benchmark evaluation, please use the &lt;code&gt;gaia58.18&lt;/code&gt; branch which includes a customized version of the CAMEL framework in the &lt;code&gt;owl/camel&lt;/code&gt; directory. This version contains enhanced toolkits with improved stability specifically optimized for the GAIA benchmark compared to the standard CAMEL installation.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Check if Python is installed
python --version

# If not installed, download and install from https://www.python.org/downloads/
# For macOS users with Homebrew:
brew install python@3.10

# For Ubuntu/Debian:
sudo apt update
sudo apt install python3.10 python3.10-venv python3-pip
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;strong&gt;Installation Options&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;OWL supports multiple installation methods to fit your workflow preferences.&lt;/p&gt; 
&lt;h3&gt;Option 1: Using uv (Recommended)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone github repo
git clone https://github.com/camel-ai/owl.git

# Change directory into project directory
cd owl

# Install uv if you don't have it already
pip install uv

# Create a virtual environment and install dependencies
uv venv .venv --python=3.10

# Activate the virtual environment
# For macOS/Linux
source .venv/bin/activate
# For Windows
.venv\Scripts\activate

# Install CAMEL with all dependencies
uv pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Option 2: Using venv and pip&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone github repo
git clone https://github.com/camel-ai/owl.git

# Change directory into project directory
cd owl

# Create a virtual environment
# For Python 3.10 (also works with 3.11, 3.12)
python3.10 -m venv .venv

# Activate the virtual environment
# For macOS/Linux
source .venv/bin/activate
# For Windows
.venv\Scripts\activate

# Install from requirements.txt
pip install -r requirements.txt --use-pep517
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Option 3: Using conda&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone github repo
git clone https://github.com/camel-ai/owl.git

# Change directory into project directory
cd owl

# Create a conda environment
conda create -n owl python=3.10

# Activate the conda environment
conda activate owl

# Option 1: Install as a package (recommended)
pip install -e .

# Option 2: Install from requirements.txt
pip install -r requirements.txt --use-pep517
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Option 4: Using Docker&lt;/h3&gt; 
&lt;h4&gt;&lt;strong&gt;Using Pre-built Image (Recommended)&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# This option downloads a ready-to-use image from Docker Hub
# Fastest and recommended for most users
docker compose up -d

# Run OWL inside the container
docker compose exec owl bash
cd .. &amp;amp;&amp;amp; source .venv/bin/activate
playwright install-deps
xvfb-python examples/run.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;&lt;strong&gt;Building Image Locally&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# For users who need to customize the Docker image or cannot access Docker Hub:
# 1. Open docker-compose.yml
# 2. Comment out the "image: mugglejinx/owl:latest" line
# 3. Uncomment the "build:" section and its nested properties
# 4. Then run:
docker compose up -d --build

# Run OWL inside the container
docker compose exec owl bash
cd .. &amp;amp;&amp;amp; source .venv/bin/activate
playwright install-deps
xvfb-python examples/run.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;&lt;strong&gt;Using Convenience Scripts&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Navigate to container directory
cd .container

# Make the script executable and build the Docker image
chmod +x build_docker.sh
./build_docker.sh

# Run OWL with your question
./run_in_docker.sh "your question"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;strong&gt;Setup Environment Variables&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;OWL requires various API keys to interact with different services.&lt;/p&gt; 
&lt;h3&gt;Setting Environment Variables Directly&lt;/h3&gt; 
&lt;p&gt;You can set environment variables directly in your terminal:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;macOS/Linux (Bash/Zsh)&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY="your-openai-api-key-here"
# Add other required API keys as needed
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows (Command Prompt)&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-batch"&gt;set OPENAI_API_KEY=your-openai-api-key-here
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows (PowerShell)&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-powershell"&gt;$env:OPENAI_API_KEY = "your-openai-api-key-here"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Environment variables set directly in the terminal will only persist for the current session.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Alternative: Using a &lt;code&gt;.env&lt;/code&gt; File&lt;/h3&gt; 
&lt;p&gt;If you prefer using a &lt;code&gt;.env&lt;/code&gt; file instead, you can:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Copy and Rename the Template&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# For macOS/Linux
cd owl
cp .env_template .env

# For Windows
cd owl
copy .env_template .env
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Alternatively, you can manually create a new file named &lt;code&gt;.env&lt;/code&gt; in the owl directory and copy the contents from &lt;code&gt;.env_template&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configure Your API Keys&lt;/strong&gt;: Open the &lt;code&gt;.env&lt;/code&gt; file in your preferred text editor and insert your API keys in the corresponding fields.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: For the minimal example (&lt;code&gt;examples/run_mini.py&lt;/code&gt;), you only need to configure the LLM API key (e.g., &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;MCP Desktop Commander Setup&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;If using MCP Desktop Commander within Docker, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx -y @wonderwhy-er/desktop-commander setup --force-file-protocol
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more detailed Docker usage instructions, including cross-platform support, optimized configurations, and troubleshooting, please refer to &lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/.container/DOCKER_README_en.md"&gt;DOCKER_README.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;ğŸš€ Quick Start&lt;/h1&gt; 
&lt;h2&gt;Basic Usage&lt;/h2&gt; 
&lt;p&gt;After installation and setting up your environment variables, you can start using OWL right away:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python examples/run.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Running with Different Models&lt;/h2&gt; 
&lt;h3&gt;Model Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tool Calling&lt;/strong&gt;: OWL requires models with robust tool calling capabilities to interact with various toolkits. Models must be able to understand tool descriptions, generate appropriate tool calls, and process tool outputs.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multimodal Understanding&lt;/strong&gt;: For tasks involving web interaction, image analysis, or video processing, models with multimodal capabilities are required to interpret visual content and context.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Supported Models&lt;/h4&gt; 
&lt;p&gt;For information on configuring AI models, please refer to our &lt;a href="https://docs.camel-ai.org/key_modules/models.html#supported-model-platforms-in-camel"&gt;CAMEL models documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: For optimal performance, we strongly recommend using OpenAI models (GPT-4 or later versions). Our experiments show that other models may result in significantly lower performance on complex tasks and benchmarks, especially those requiring advanced multi-modal understanding and tool use.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;OWL supports various LLM backends, though capabilities may vary depending on the model's tool calling and multimodal abilities. You can use the following scripts to run with different models:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run with Claude model
python examples/run_claude.py

# Run with Qwen model
python examples/run_qwen_zh.py

# Run with Deepseek model
python examples/run_deepseek_zh.py

# Run with other OpenAI-compatible models
python examples/run_openai_compatible_model.py

# Run with Gemini model
python examples/run_gemini.py

# Run with Azure OpenAI
python examples/run_azure_openai.py

# Run with Ollama
python examples/run_ollama.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For a simpler version that only requires an LLM API key, you can try our minimal example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python examples/run_mini.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can run OWL agent with your own task by modifying the &lt;code&gt;examples/run.py&lt;/code&gt; script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Define your own task
task = "Task description here."

society = construct_society(question)
answer, chat_history, token_count = run_society(society)

print(f"\033[94mAnswer: {answer}\033[0m")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For uploading files, simply provide the file path along with your question:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Task with a local file (e.g., file path: `tmp/example.docx`)
task = "What is in the given DOCX file? Here is the file path: tmp/example.docx"

society = construct_society(question)
answer, chat_history, token_count = run_society(society)
print(f"\033[94mAnswer: {answer}\033[0m")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;OWL will then automatically invoke document-related tools to process the file and extract the answer.&lt;/p&gt; 
&lt;h3&gt;Example Tasks&lt;/h3&gt; 
&lt;p&gt;Here are some tasks you can try with OWL:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;"Find the latest stock price for Apple Inc."&lt;/li&gt; 
 &lt;li&gt;"Analyze the sentiment of recent tweets about climate change"&lt;/li&gt; 
 &lt;li&gt;"Help me debug this Python code: [your code here]"&lt;/li&gt; 
 &lt;li&gt;"Summarize the main points from this research paper: [paper URL]"&lt;/li&gt; 
 &lt;li&gt;"Create a data visualization for this dataset: [dataset path]"&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;ğŸ§° Toolkits and Capabilities&lt;/h1&gt; 
&lt;h2&gt;Model Context Protocol (MCP)&lt;/h2&gt; 
&lt;p&gt;OWL's MCP integration provides a standardized way for AI models to interact with various tools and data sources:&lt;/p&gt; 
&lt;p&gt;Before using MCP, you need to install Node.js first.&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;Install Node.js&lt;/strong&gt;&lt;/h3&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;p&gt;Download the official installer: &lt;a href="https://nodejs.org/en"&gt;Node.js&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Check "Add to PATH" option during installation.&lt;/p&gt; 
&lt;h3&gt;Linux&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt update
sudo apt install nodejs npm -y
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Mac&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew install node
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;strong&gt;Install Playwright MCP Service&lt;/strong&gt;&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm install -g @executeautomation/playwright-mcp-server
npx playwright install-deps
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Try our comprehensive MCP examples:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;examples/run_mcp.py&lt;/code&gt; - Basic MCP functionality demonstration (local call, requires dependencies)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;examples/run_mcp_sse.py&lt;/code&gt; - Example using the SSE protocol (Use remote services, no dependencies)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Available Toolkits&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Effective use of toolkits requires models with strong tool calling capabilities. For multimodal toolkits (Web, Image, Video), models must also have multimodal understanding abilities.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;OWL supports various toolkits that can be customized by modifying the &lt;code&gt;tools&lt;/code&gt; list in your script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Configure toolkits
tools = [
    *BrowserToolkit(headless=False).get_tools(),  # Browser automation
    *VideoAnalysisToolkit(model=models["video"]).get_tools(),
    *AudioAnalysisToolkit().get_tools(),  # Requires OpenAI Key
    *CodeExecutionToolkit(sandbox="subprocess").get_tools(),
    *ImageAnalysisToolkit(model=models["image"]).get_tools(),
    SearchToolkit().search_duckduckgo,
    SearchToolkit().search_google,  # Comment out if unavailable
    SearchToolkit().search_wiki,
    SearchToolkit().search_bocha,
    SearchToolkit().search_baidu,
    *ExcelToolkit().get_tools(),
    *DocumentProcessingToolkit(model=models["document"]).get_tools(),
    *FileWriteToolkit(output_dir="./").get_tools(),
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Available Toolkits&lt;/h2&gt; 
&lt;p&gt;Key toolkits include:&lt;/p&gt; 
&lt;h3&gt;Multimodal Toolkits (Require multimodal model capabilities)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;BrowserToolkit&lt;/strong&gt;: Browser automation for web interaction and navigation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;VideoAnalysisToolkit&lt;/strong&gt;: Video processing and content analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ImageAnalysisToolkit&lt;/strong&gt;: Image analysis and interpretation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Text-Based Toolkits&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;AudioAnalysisToolkit&lt;/strong&gt;: Audio processing (requires OpenAI API)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CodeExecutionToolkit&lt;/strong&gt;: Python code execution and evaluation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SearchToolkit&lt;/strong&gt;: Web searches (Google, DuckDuckGo, Wikipedia)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DocumentProcessingToolkit&lt;/strong&gt;: Document parsing (PDF, DOCX, etc.)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Additional specialized toolkits: ArxivToolkit, GitHubToolkit, GoogleMapsToolkit, MathToolkit, NetworkXToolkit, NotionToolkit, RedditToolkit, WeatherToolkit, and more. For a complete list, see the &lt;a href="https://docs.camel-ai.org/key_modules/tools.html#built-in-toolkits"&gt;CAMEL toolkits documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Customizing Your Configuration&lt;/h2&gt; 
&lt;p&gt;To customize available tools:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# 1. Import toolkits
from camel.toolkits import BrowserToolkit, SearchToolkit, CodeExecutionToolkit

# 2. Configure tools list
tools = [
    *BrowserToolkit(headless=True).get_tools(),
    SearchToolkit().search_wiki,
    *CodeExecutionToolkit(sandbox="subprocess").get_tools(),
]

# 3. Pass to assistant agent
assistant_agent_kwargs = {"model": models["assistant"], "tools": tools}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Selecting only necessary toolkits optimizes performance and reduces resource usage.&lt;/p&gt; 
&lt;h1&gt;ğŸŒ Web Interface&lt;/h1&gt; 
&lt;div align="center" style="background-color: #f0f7ff; padding: 15px; border-radius: 10px; border: 2px solid #1e88e5; margin: 20px 0;"&gt; 
 &lt;h3 style="color: #1e88e5; margin: 0;"&gt; ğŸš€ Enhanced Web Interface Now Available! &lt;/h3&gt; 
 &lt;p style="margin: 10px 0;"&gt; Experience improved system stability and optimized performance with our latest update. Start exploring the power of OWL through our user-friendly interface! &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Starting the Web UI&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start the Chinese version
python owl/webapp_zh.py

# Start the English version
python owl/webapp.py

# Start the Japanese version
python owl/webapp_jp.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Easy Model Selection&lt;/strong&gt;: Choose between different models (OpenAI, Qwen, DeepSeek, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Environment Variable Management&lt;/strong&gt;: Configure your API keys and other settings directly from the UI&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interactive Chat Interface&lt;/strong&gt;: Communicate with OWL agents through a user-friendly interface&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Task History&lt;/strong&gt;: View the history and results of your interactions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The web interface is built using Gradio and runs locally on your machine. No data is sent to external servers beyond what's required for the model API calls you configure.&lt;/p&gt; 
&lt;h1&gt;ğŸ§ª Experiments&lt;/h1&gt; 
&lt;p&gt;To reproduce OWL's GAIA benchmark score: Furthermore, to ensure optimal performance on the GAIA benchmark, please note that our &lt;code&gt;gaia69&lt;/code&gt; branch includes a customized version of the CAMEL framework in the &lt;code&gt;owl/camel&lt;/code&gt; directory. This version contains enhanced toolkits with improved stability for gaia benchmark compared to the standard CAMEL installation.&lt;/p&gt; 
&lt;p&gt;When running the benchmark evaluation:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Switch to the &lt;code&gt;gaia69&lt;/code&gt; branch:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git checkout gaia69
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the evaluation script:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python run_gaia_workforce_claude.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This will execute the same configuration that achieved our top-ranking performance on the GAIA benchmark.&lt;/p&gt; 
&lt;h1&gt;â±ï¸ Future Plans&lt;/h1&gt; 
&lt;p&gt;We're continuously working to improve OWL. Here's what's on our roadmap:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Write a technical blog post detailing our exploration and insights in multi-agent collaboration in real-world tasks&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Enhance the toolkit ecosystem with more specialized tools for domain-specific tasks&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Develop more sophisticated agent interaction patterns and communication protocols&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Improve performance on complex multi-step reasoning tasks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;ğŸ“„ License&lt;/h1&gt; 
&lt;p&gt;The source code is licensed under Apache 2.0.&lt;/p&gt; 
&lt;h1&gt;ğŸ¤ Contributing&lt;/h1&gt; 
&lt;p&gt;We welcome contributions from the community! Here's how you can help:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Read our &lt;a href="https://github.com/camel-ai/camel/raw/master/CONTRIBUTING.md"&gt;Contribution Guidelines&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Check &lt;a href="https://github.com/camel-ai/camel/issues"&gt;open issues&lt;/a&gt; or create new ones&lt;/li&gt; 
 &lt;li&gt;Submit pull requests with your improvements&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Current Issues Open for Contribution:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/camel-ai/camel/issues/1915"&gt;#1915&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/camel-ai/camel/issues/2190"&gt;#2190&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/camel-ai/camel/issues/2165"&gt;#2165&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/camel-ai/camel/issues/2121"&gt;#2121&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/camel-ai/camel/issues/1908"&gt;#1908&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/camel-ai/camel/issues/1538"&gt;#1538&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/camel-ai/camel/issues/1481"&gt;#1481&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To take on an issue, simply leave a comment stating your interest.&lt;/p&gt; 
&lt;h1&gt;ğŸ”¥ Community&lt;/h1&gt; 
&lt;p&gt;Join us (&lt;a href="https://discord.camel-ai.org/"&gt;&lt;em&gt;Discord&lt;/em&gt;&lt;/a&gt; or &lt;a href="https://ghli.org/camel/wechat.png"&gt;&lt;em&gt;WeChat&lt;/em&gt;&lt;/a&gt;) in pushing the boundaries of finding the scaling laws of agents.&lt;/p&gt; 
&lt;p&gt;Join us for further discussions!&lt;/p&gt; 
&lt;!-- ![](./assets/community.png) --&gt; 
&lt;img src="https://raw.githubusercontent.com/camel-ai/owl/main/assets/community_code.jpeg" width="50%" /&gt; 
&lt;h1&gt;â“ FAQ&lt;/h1&gt; 
&lt;h2&gt;General Questions&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Q: Why don't I see Chrome running locally after starting the example script?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;A: If OWL determines that a task can be completed using non-browser tools (such as search or code execution), the browser will not be launched. The browser window will only appear when OWL determines that browser-based interaction is necessary.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Q: Which Python version should I use?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;A: OWL supports Python 3.10, 3.11, and 3.12.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Q: How can I contribute to the project?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;A: See our &lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-contributing"&gt;Contributing&lt;/a&gt; section for details on how to get involved. We welcome contributions of all kinds, from code improvements to documentation updates.&lt;/p&gt; 
&lt;h2&gt;Experiment Questions&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Q: Which CAMEL version should I use for replicate the role playing result?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;A: We provide a modified version of CAMEL (owl/camel) in the gaia58.18 branch. Please make sure you use this CAMEL version for your experiments.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Q: Why are my experiment results lower than the reported numbers?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;A: Since the GAIA benchmark evaluates LLM agents in a realistic world, it introduces a significant amount of randomness. Based on user feedback, one of the most common issues for replication is, for example, agents being blocked on certain webpages due to network reasons. We have uploaded a keywords matching script to help quickly filter out these errors &lt;a href="https://github.com/camel-ai/owl/raw/gaia58.18/owl/filter_failed_cases.py"&gt;here&lt;/a&gt;. You can also check this &lt;a href="https://hypnotic-mind-6bd.notion.site/OWL-Optimized-Workforce-Learning-for-General-Multi-Agent-Assistance-in-Real-World-Task-Automation-1d4004aeb21380158749c7f84b20643f?pvs=74"&gt;technical report&lt;/a&gt; for more details when evaluating LLM agents in realistic open-world environments.&lt;/p&gt; 
&lt;h1&gt;ğŸ“š Exploring CAMEL Dependency&lt;/h1&gt; 
&lt;p&gt;OWL is built on top of the &lt;a href="https://github.com/camel-ai/camel"&gt;CAMEL&lt;/a&gt; Framework, here's how you can explore the CAMEL source code and understand how it works with OWL:&lt;/p&gt; 
&lt;h2&gt;Accessing CAMEL Source Code&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the CAMEL repository
git clone https://github.com/camel-ai/camel.git
cd camel
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;ğŸ–Šï¸ Cite&lt;/h1&gt; 
&lt;p&gt;If you find this repo useful, please cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{hu2025owl,
      title={OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation}, 
      author={Mengkang Hu and Yuhang Zhou and Wendong Fan and Yuzhou Nie and Bowei Xia and Tao Sun and Ziyu Ye and Zhaoxuan Jin and Yingru Li and Qiguang Chen and Zeyu Zhang and Yifeng Wang and Qianshuo Ye and Bernard Ghanem and Ping Luo and Guohao Li},
      year={2025},
      eprint={2505.23885},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2505.23885}, 
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;â­ Star History&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#camel-ai/owl&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=camel-ai/owl&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>mealie-recipes/mealie</title>
      <link>https://github.com/mealie-recipes/mealie</link>
      <description>&lt;p&gt;Mealie is a self hosted recipe manager and meal planner with a RestAPI backend and a reactive frontend application built in Vue for a pleasant user experience for the whole family. Easily add recipes into your database by providing the url and mealie will automatically import the relevant data or add a family recipe with the UI editor&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://github.com/mealie-recipes/mealie/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/mealie-recipes/mealie?style=flat-square&amp;amp;label=latest%20release" alt="Latest Release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/mealie-recipes/mealie/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/mealie-recipes/mealie.svg?style=flat-square" alt="Contributors" /&gt;&lt;/a&gt; &lt;a href="https://github.com/mealie-recipes/mealie/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/mealie-recipes/mealie.svg?style=flat-square" alt="Stargazers" /&gt;&lt;/a&gt; &lt;a href="https://github.com/mealie-recipes/mealie/issues"&gt;&lt;img src="https://img.shields.io/github/issues/mealie-recipes/mealie.svg?style=flat-square" alt="Issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/mealie-recipes/mealie/raw/mealie-next/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/mealie-recipes/mealie.svg?style=flat-square" alt="AGPL License" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/hkotel/mealie"&gt;&lt;img src="https://img.shields.io/docker/pulls/hkotel/mealie?style=flat-square" alt="Docker Pulls" /&gt;&lt;/a&gt; &lt;a href="https://github.com/mealie-recipes/mealie/pkgs/container/mealie"&gt;&lt;img src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fipitio.github.io%2Fbackage%2Fmealie-recipes%2Fmealie%2Fmealie.json&amp;amp;query=%24.downloads&amp;amp;style=flat-square&amp;amp;label=ghcr%20pulls" alt="GHCR Pulls" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- PROJECT LOGO --&gt; 
&lt;br /&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/mealie-recipes/mealie"&gt; 
  &lt;svg style="width:100px;height:100px" viewbox="0 0 24 24"&gt; 
   &lt;path fill="currentColor" d="M8.1,13.34L3.91,9.16C2.35,7.59 2.35,5.06 3.91,3.5L10.93,10.5L8.1,13.34M13.41,13L20.29,19.88L18.88,21.29L12,14.41L5.12,21.29L3.71,19.88L13.36,10.22L13.16,10C12.38,9.23 12.38,7.97 13.16,7.19L17.5,2.82L18.43,3.74L15.19,7L16.15,7.94L19.39,4.69L20.31,5.61L17.06,8.85L18,9.81L21.26,6.56L22.18,7.5L17.81,11.84C17.03,12.62 15.77,12.62 15,11.84L14.78,11.64L13.41,13Z" /&gt; 
  &lt;/svg&gt; &lt;/a&gt; &lt;/p&gt;
&lt;h3 align="center"&gt;Mealie&lt;/h3&gt; 
&lt;p align="center"&gt; A Place For All Your Recipes &lt;br /&gt; &lt;a href="https://docs.mealie.io/"&gt;&lt;strong&gt;Explore the docs Â»&lt;/strong&gt;&lt;/a&gt; &lt;a href="https://github.com/mealie-recipes/mealie"&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://demo.mealie.io/"&gt;View Demo&lt;/a&gt; Â· &lt;a href="https://github.com/mealie-recipes/mealie/issues"&gt;Report Bug&lt;/a&gt; Â· &lt;a href="https://github.com/mealie-recipes/mealie/pkgs/container/mealie"&gt;GitHub Container Registry&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.mealie.io"&gt;&lt;img src="https://raw.githubusercontent.com/mealie-recipes/mealie/mealie-next/docs/docs/assets/img/home_screenshot.png" alt="Product Name Screen Shot" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;About The Project&lt;/h1&gt; 
&lt;p&gt;Mealie is a self hosted recipe manager, meal planner and shopping list with a RestAPI backend and a reactive frontend built in Vue for a pleasant user experience for the whole family. Easily add recipes into your database by providing the URL and Mealie will automatically import the relevant data, or add a family recipe with the UI editor. Mealie also provides an API for interactions from 3rd party applications.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/QuStdQGSGK"&gt;Remember to join the Discord&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.mealie.io/"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Recipe imports: Create recipes, by &lt;strong&gt;importing from a URL&lt;/strong&gt; or entering data manually&lt;/li&gt; 
 &lt;li&gt;Meal Planner: Use the &lt;strong&gt;Meal Planner&lt;/strong&gt; to plan your what you'll cook for the next week&lt;/li&gt; 
 &lt;li&gt;Shopping List: Put the necessary ingredients on your &lt;strong&gt;Shopping List&lt;/strong&gt;, organised into sections of your local supermarket&lt;/li&gt; 
 &lt;li&gt;Cookbooks: Group recipes into &lt;strong&gt;Cookbooks&lt;/strong&gt; based on your own criteria&lt;/li&gt; 
 &lt;li&gt;Docker: Easy &lt;strong&gt;Docker&lt;/strong&gt; deployment&lt;/li&gt; 
 &lt;li&gt;Localisation: &lt;strong&gt;Translations&lt;/strong&gt; for 35+ languages&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- CONTRIBUTING --&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are what make the open source community such an amazing place to learn, inspire, and create. Any contributions you make are &lt;strong&gt;greatly appreciated&lt;/strong&gt;. If you're going to be working on the code-base, you'll want to use the nightly documentation to ensure you get the latest information.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;See the &lt;a href="https://nightly.mealie.io/contributors/developers-guide/code-contributions/"&gt;Contributors Guide&lt;/a&gt; for help getting started.&lt;/li&gt; 
 &lt;li&gt;We use &lt;a href="https://code.visualstudio.com/docs/remote/containers"&gt;VSCode Dev Containers&lt;/a&gt; to make it easy for contributors to get started!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you are not a coder, you can still contribute financially. Financial contributions help me prioritize working on this project over others and helps me know that there is a real demand for project development.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.buymeacoffee.com/haykot" target="_blank"&gt;&lt;img src="https://cdn.buymeacoffee.com/buttons/v2/default-green.png" alt="Buy Me A Coffee" style="height: 30px !important;width: 107px !important;" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Translations&lt;/h3&gt; 
&lt;p&gt;Translations can be a great way for &lt;strong&gt;non-coders&lt;/strong&gt; to contribute to the project. We use &lt;a href="https://crowdin.com/project/mealie"&gt;Crowdin&lt;/a&gt; to allow several contributors to work on translating Mealie. You can simply help by voting for your preferred translations, or even by completely translating Mealie into a new language.&lt;/p&gt; 
&lt;p&gt;For more information, check out the translation page on the &lt;a href="https://nightly.mealie.io/contributors/translating/"&gt;contributor's guide&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- LICENSE --&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Distributed under the AGPL License. See &lt;code&gt;LICENSE&lt;/code&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;p&gt;Huge thanks to all the sponsors of this project on &lt;a href="https://github.com/sponsors/hay-kot"&gt;Github Sponsors&lt;/a&gt; and Buy Me a Coffee. Without you, this project would surely not be possible.&lt;/p&gt; 
&lt;p&gt;Thanks to Depot for providing build instances for our Docker image builds.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://depot.dev?utm_source=Mealie"&gt;&lt;img src="https://depot.dev/badges/built-with-depot.svg?sanitize=true" alt="Built with Depot" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; 
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</description>
    </item>
    
  </channel>
</rss>