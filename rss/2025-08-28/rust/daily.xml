<rss version="2.0">
  <channel>
    <title>GitHub Rust Daily Trending</title>
    <description>Daily Trending of Rust in GitHub</description>
    <pubDate>Wed, 27 Aug 2025 01:41:04 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>iced-rs/iced</title>
      <link>https://github.com/iced-rs/iced</link>
      <description>&lt;p&gt;A cross-platform GUI library for Rust, inspired by Elm&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/iced-rs/iced/master/docs/logo.svg?sanitize=true" width="140px" /&gt; 
 &lt;h1&gt;Iced&lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://docs.rs/iced/"&gt;&lt;img src="https://docs.rs/iced/badge.svg?sanitize=true" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://crates.io/crates/iced"&gt;&lt;img src="https://img.shields.io/crates/v/iced.svg?sanitize=true" alt="Crates.io" /&gt;&lt;/a&gt; &lt;a href="https://github.com/iced-rs/iced/raw/master/LICENSE"&gt;&lt;img src="https://img.shields.io/crates/l/iced.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://crates.io/crates/iced"&gt;&lt;img src="https://img.shields.io/crates/d/iced.svg?sanitize=true" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/iced-rs/iced/actions"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/iced-rs/iced/test.yml?branch=master&amp;amp;event=push&amp;amp;label=test" alt="Test Status" /&gt;&lt;/a&gt; &lt;a href="https://discourse.iced.rs/"&gt;&lt;img src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fdiscourse.iced.rs%2Fsite%2Fstatistics.json&amp;amp;query=%24.users_count&amp;amp;suffix=%20users&amp;amp;label=discourse&amp;amp;color=5e7ce2" alt="Discourse" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/3xZJ65GAhd"&gt;&lt;img src="https://img.shields.io/discord/628993209984614400?label=&amp;amp;labelColor=6A7EC2&amp;amp;logo=discord&amp;amp;logoColor=ffffff&amp;amp;color=7389D8" alt="Discord Server" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;A cross-platform GUI library for Rust focused on simplicity and type-safety. Inspired by &lt;a href="https://elm-lang.org/"&gt;Elm&lt;/a&gt;.&lt;/p&gt; 
 &lt;a href="https://github.com/squidowl/halloy"&gt; &lt;img src="https://iced.rs/showcase/halloy.gif" width="460px" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/hecrj/icebreaker"&gt; &lt;img src="https://iced.rs/showcase/icebreaker.gif" width="360px" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Simple, easy-to-use, batteries-included API&lt;/li&gt; 
 &lt;li&gt;Type-safe, reactive programming model&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/iced-rs/iced/master/docs/images/todos_desktop.jpg"&gt;Cross-platform support&lt;/a&gt; (Windows, macOS, Linux, and the Web)&lt;/li&gt; 
 &lt;li&gt;Responsive layout&lt;/li&gt; 
 &lt;li&gt;Built-in widgets (including &lt;a href="https://iced.rs/examples/text_input.mp4"&gt;text inputs&lt;/a&gt;, &lt;a href="https://iced.rs/examples/scrollable.mp4"&gt;scrollables&lt;/a&gt;, and more!)&lt;/li&gt; 
 &lt;li&gt;Custom widget support (create your own!)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://iced.rs/examples/debug.mp4"&gt;Debug overlay with performance metrics&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;First-class support for async actions (use futures!)&lt;/li&gt; 
 &lt;li&gt;Modular ecosystem split into reusable parts: 
  &lt;ul&gt; 
   &lt;li&gt;A &lt;a href="https://raw.githubusercontent.com/iced-rs/iced/master/runtime/"&gt;renderer-agnostic native runtime&lt;/a&gt; enabling integration with existing systems&lt;/li&gt; 
   &lt;li&gt;Two built-in renderers leveraging &lt;a href="https://github.com/gfx-rs/wgpu"&gt;&lt;code&gt;wgpu&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://github.com/RazrFalcon/tiny-skia"&gt;&lt;code&gt;tiny-skia&lt;/code&gt;&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/iced-rs/iced/master/wgpu/"&gt;&lt;code&gt;iced_wgpu&lt;/code&gt;&lt;/a&gt; supporting Vulkan, Metal and DX12&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/iced-rs/iced/master/tiny_skia/"&gt;&lt;code&gt;iced_tiny_skia&lt;/code&gt;&lt;/a&gt; offering a software alternative as a fallback&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;A &lt;a href="https://raw.githubusercontent.com/iced-rs/iced/master/winit/"&gt;windowing shell&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Iced is currently experimental software.&lt;/strong&gt; &lt;a href="https://raw.githubusercontent.com/iced-rs/iced/master/ROADMAP.md"&gt;Take a look at the roadmap&lt;/a&gt; and &lt;a href="https://github.com/iced-rs/iced/issues"&gt;check out the issues&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;Inspired by &lt;a href="https://guide.elm-lang.org/architecture/"&gt;The Elm Architecture&lt;/a&gt;, Iced expects you to split user interfaces into four different concepts:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;State&lt;/strong&gt; — the state of your application&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Messages&lt;/strong&gt; — user interactions or meaningful events that you care about&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;View logic&lt;/strong&gt; — a way to display your &lt;strong&gt;state&lt;/strong&gt; as widgets that may produce &lt;strong&gt;messages&lt;/strong&gt; on user interaction&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Update logic&lt;/strong&gt; — a way to react to &lt;strong&gt;messages&lt;/strong&gt; and update your &lt;strong&gt;state&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We can build something to see how this works! Let's say we want a simple counter that can be incremented and decremented using two buttons.&lt;/p&gt; 
&lt;p&gt;We start by modelling the &lt;strong&gt;state&lt;/strong&gt; of our application:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;#[derive(Default)]
struct Counter {
    value: i32,
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Next, we need to define the possible user interactions of our counter: the button presses. These interactions are our &lt;strong&gt;messages&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;#[derive(Debug, Clone, Copy)]
pub enum Message {
    Increment,
    Decrement,
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now, let's show the actual counter by putting it all together in our &lt;strong&gt;view logic&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;use iced::widget::{button, column, text, Column};

impl Counter {
    pub fn view(&amp;amp;self) -&amp;gt; Column&amp;lt;Message&amp;gt; {
        // We use a column: a simple vertical layout
        column![
            // The increment button. We tell it to produce an
            // `Increment` message when pressed
            button("+").on_press(Message::Increment),

            // We show the value of the counter here
            text(self.value).size(50),

            // The decrement button. We tell it to produce a
            // `Decrement` message when pressed
            button("-").on_press(Message::Decrement),
        ]
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Finally, we need to be able to react to any produced &lt;strong&gt;messages&lt;/strong&gt; and change our &lt;strong&gt;state&lt;/strong&gt; accordingly in our &lt;strong&gt;update logic&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;impl Counter {
    // ...

    pub fn update(&amp;amp;mut self, message: Message) {
        match message {
            Message::Increment =&amp;gt; {
                self.value += 1;
            }
            Message::Decrement =&amp;gt; {
                self.value -= 1;
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And that's everything! We just wrote a whole user interface. Let's run it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;fn main() -&amp;gt; iced::Result {
    iced::run("A cool counter", Counter::update, Counter::view)
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Iced will automatically:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Take the result of our &lt;strong&gt;view logic&lt;/strong&gt; and layout its widgets.&lt;/li&gt; 
 &lt;li&gt;Process events from our system and produce &lt;strong&gt;messages&lt;/strong&gt; for our &lt;strong&gt;update logic&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Draw the resulting user interface.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Read the &lt;a href="https://book.iced.rs/"&gt;book&lt;/a&gt;, the &lt;a href="https://docs.rs/iced/"&gt;documentation&lt;/a&gt;, and the &lt;a href="https://github.com/iced-rs/iced/tree/master/examples#examples"&gt;examples&lt;/a&gt; to learn more!&lt;/p&gt; 
&lt;h2&gt;Implementation details&lt;/h2&gt; 
&lt;p&gt;Iced was originally born as an attempt at bringing the simplicity of &lt;a href="https://elm-lang.org/"&gt;Elm&lt;/a&gt; and &lt;a href="https://guide.elm-lang.org/architecture/"&gt;The Elm Architecture&lt;/a&gt; into &lt;a href="https://github.com/hecrj/coffee"&gt;Coffee&lt;/a&gt;, a 2D game library I am working on.&lt;/p&gt; 
&lt;p&gt;The core of the library was implemented during May 2019 in &lt;a href="https://github.com/hecrj/coffee/pull/35"&gt;this pull request&lt;/a&gt;. &lt;a href="https://github.com/iced-rs/iced/tree/0.1.0-alpha"&gt;The first alpha version&lt;/a&gt; was eventually released as &lt;a href="https://www.reddit.com/r/rust/comments/czzjnv/iced_a_rendereragnostic_gui_library_focused_on/"&gt;a renderer-agnostic GUI library&lt;/a&gt;. The library did not provide a renderer and implemented the current &lt;a href="https://raw.githubusercontent.com/iced-rs/iced/master/examples/README.md#tour"&gt;tour example&lt;/a&gt; on top of &lt;a href="https://github.com/ggez/ggez"&gt;&lt;code&gt;ggez&lt;/code&gt;&lt;/a&gt;, a game library.&lt;/p&gt; 
&lt;p&gt;Since then, the focus has shifted towards providing a batteries-included, end-user-oriented GUI library, while keeping the ecosystem modular.&lt;/p&gt; 
&lt;h2&gt;Contributing / Feedback&lt;/h2&gt; 
&lt;p&gt;If you want to contribute, please read our &lt;a href="https://github.com/iced-rs/iced/raw/master/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;Feedback is also welcome! You can create a new topic in &lt;a href="https://discourse.iced.rs/"&gt;our Discourse forum&lt;/a&gt; or come chat to &lt;a href="https://discord.gg/3xZJ65GAhd"&gt;our Discord server&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;p&gt;The development of Iced is sponsored by the &lt;a href="https://cryptowat.ch/charts"&gt;Cryptowatch&lt;/a&gt; team at &lt;a href="https://kraken.com/"&gt;Kraken.com&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>moghtech/komodo</title>
      <link>https://github.com/moghtech/komodo</link>
      <description>&lt;p&gt;🦎 a tool to build and deploy software on many servers 🦎&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Komodo 🦎&lt;/h1&gt; 
&lt;p&gt;A tool to build and deploy software across many servers.&lt;/p&gt; 
&lt;p&gt;🦎 &lt;a href="https://komo.do"&gt;See the docs&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;🦎 &lt;a href="https://demo.komo.do"&gt;Try the Demo&lt;/a&gt; - Login: &lt;code&gt;demo&lt;/code&gt; : &lt;code&gt;demo&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;🦎 &lt;a href="https://build.komo.do"&gt;See the Build Server&lt;/a&gt; - Login: &lt;code&gt;komodo&lt;/code&gt; : &lt;code&gt;komodo&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;🦎 &lt;a href="https://discord.gg/DRqE8Fvg5c"&gt;Join the Discord&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;About&lt;/h2&gt; 
&lt;p&gt;The Komodo dragon is the largest living member of the &lt;a href="https://en.wikipedia.org/wiki/Monitor_lizard"&gt;&lt;em&gt;Monitor&lt;/em&gt; family of lizards&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;There is no limit to the number of servers you can connect, and there will never be. There is no limit to what API you can use for automation, and there never will be. No "business edition" here.&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;Warning. This is open source software (GPL-V3), and while we make a best effort to ensure releases are stable and bug-free, there are no warranties. Use at your own risk.&lt;/p&gt; 
&lt;h2&gt;Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/moghtech/komodo/raw/main/scripts/readme.md"&gt;periphery setup&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/moghtech/komodo/raw/main/roadmap.md"&gt;roadmap&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Screenshots&lt;/h2&gt; 
&lt;h3&gt;Light Theme&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Light-Dashboard.png" alt="Dashboard" /&gt; &lt;img src="https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Light-Stack.png" alt="Stack" /&gt; &lt;img src="https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Light-Compose.png" alt="Compose" /&gt; &lt;img src="https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Light-Env.png" alt="Env" /&gt; &lt;img src="https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Light-Sync.png" alt="Sync" /&gt; &lt;img src="https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Light-Update.png" alt="Update" /&gt; &lt;img src="https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Light-Stats.png" alt="Stats" /&gt; &lt;img src="https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Light-Export.png" alt="Export" /&gt;&lt;/p&gt; 
&lt;h3&gt;Dark Theme&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Dark-Dashboard.png" alt="Dashboard" /&gt; &lt;img src="https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Dark-Stack.png" alt="Stack" /&gt; &lt;img src="https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Dark-Compose.png" alt="Compose" /&gt; &lt;img src="https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Dark-Env.png" alt="Env" /&gt; &lt;img src="https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Dark-Sync.png" alt="Sync" /&gt; &lt;img src="https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Dark-Update.png" alt="Update" /&gt; &lt;img src="https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Dark-Stats.png" alt="Stats" /&gt; &lt;img src="https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Dark-Export.png" alt="Export" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>tracel-ai/burn</title>
      <link>https://github.com/tracel-ai/burn</link>
      <description>&lt;p&gt;Burn is a next generation Deep Learning Framework that doesn't compromise on flexibility, efficiency and portability.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/tracel-ai/burn/main/assets/logo-burn-neutral.webp" width="350px" /&gt; 
 &lt;p&gt;&lt;a href="https://discord.gg/uPEBbYYDB6"&gt;&lt;img src="https://img.shields.io/discord/1038839012602941528.svg?color=7289da&amp;amp;&amp;amp;logo=discord" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://crates.io/crates/burn"&gt;&lt;img src="https://img.shields.io/crates/v/burn.svg?sanitize=true" alt="Current Crates.io Version" /&gt;&lt;/a&gt; &lt;a href="https://crates.io/crates/burn"&gt;&lt;img src="https://img.shields.io/crates/msrv/burn" alt="Minimum Supported Rust Version" /&gt;&lt;/a&gt; &lt;a href="https://burn.dev/docs/burn"&gt;&lt;img src="https://img.shields.io/badge/docs-latest-blue" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://github.com/tracel-ai/burn/actions/workflows/test.yml"&gt;&lt;img src="https://github.com/tracel-ai/burn/actions/workflows/test.yml/badge.svg?sanitize=true" alt="Test Status" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/#license"&gt;&lt;img src="https://shields.io/badge/license-MIT%2FApache--2.0-blue" alt="license" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/tracel-ai/burn"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.runblaze.dev"&gt;&lt;img src="https://www.runblaze.dev/ci-blaze-powered.png" width="125px" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;p&gt;&lt;strong&gt;Burn is a next generation Deep Learning Framework that doesn't compromise on &lt;br /&gt; flexibility, efficiency and portability.&lt;/strong&gt;&lt;/p&gt; 
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;div align="left"&gt; 
 &lt;h2&gt;Performance&lt;/h2&gt; 
 &lt;div align="left"&gt; 
  &lt;img align="right" src="https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-blazingly-fast.png" height="96px" /&gt; 
  &lt;p&gt;Because we believe the goal of a deep learning framework is to convert computation into useful intelligence, we have made performance a core pillar of Burn. We strive to achieve top efficiency by leveraging multiple optimization techniques described below.&lt;/p&gt; 
  &lt;p&gt;&lt;strong&gt;Click on each section for more details&lt;/strong&gt; 👇&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;br /&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Automatic kernel fusion 💥 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;Using Burn means having your models optimized on any backend. When possible, we provide a way to automatically and dynamically create custom kernels that minimize data relocation between different memory spaces, extremely useful when moving memory is the bottleneck.&lt;/p&gt; 
  &lt;p&gt;As an example, you could write your own GELU activation function with the high level tensor api (see Rust code snippet below).&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-rust"&gt;fn gelu_custom&amp;lt;B: Backend, const D: usize&amp;gt;(x: Tensor&amp;lt;B, D&amp;gt;) -&amp;gt; Tensor&amp;lt;B, D&amp;gt; {
    let x = x.clone() * ((x / SQRT_2).erf() + 1);
    x / 2
}
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;Then, at runtime, a custom low-level kernel will be automatically created for your specific implementation and will rival a handcrafted GPU implementation. The kernel consists of about 60 lines of WGSL &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/%22https://www.w3.org/TR/WGSL/https://www.w3.org/TR/WGSL/%22"&gt;WebGPU Shading Language&lt;/a&gt;, an extremely verbose lower level shader language you probably don't want to program your deep learning models in!&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Asynchronous execution ❤️‍🔥 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;For &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/#backends"&gt;first-party backends&lt;/a&gt;, an asynchronous execution style is used, which allows to perform various optimizations, such as the previously mentioned automatic kernel fusion.&lt;/p&gt; 
  &lt;p&gt;Asynchronous execution also ensures that the normal execution of the framework does not block the model computations, which implies that the framework overhead won't impact the speed of execution significantly. Conversely, the intense computations in the model do not interfere with the responsiveness of the framework. For more information about our asynchronous backends, see &lt;a href="https://burn.dev/blog/creating-high-performance-asynchronous-backends-with-burn-compute"&gt;this blog post&lt;/a&gt;.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Thread-safe building blocks 🦞 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;Burn emphasizes thread safety by leveraging the &lt;a href="https://doc.rust-lang.org/book/ch04-00-understanding-ownership.html"&gt;ownership system of Rust&lt;/a&gt;. With Burn, each module is the owner of its weights. It is therefore possible to send a module to another thread for computing the gradients, then send the gradients to the main thread that can aggregate them, and &lt;em&gt;voilà&lt;/em&gt;, you get multi-device training.&lt;/p&gt; 
  &lt;p&gt;This is a very different approach from what PyTorch does, where backpropagation actually mutates the &lt;em&gt;grad&lt;/em&gt; attribute of each tensor parameter. This is not a thread-safe operation and therefore requires lower level synchronization primitives, see &lt;a href="https://pytorch.org/docs/stable/distributed.html"&gt;distributed training&lt;/a&gt; for reference. Note that this is still very fast, but not compatible across different backends and quite hard to implement.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Intelligent memory management 🦀 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;One of the main roles of a deep learning framework is to reduce the amount of memory necessary to run models. The naive way of handling memory is that each tensor has its own memory space, which is allocated when the tensor is created then deallocated as the tensor gets out of scope. However, allocating and deallocating data is very costly, so a memory pool is often required to achieve good throughput. Burn offers an infrastructure that allows for easily creating and selecting memory management strategies for backends. For more details on memory management in Burn, see &lt;a href="https://burn.dev/blog/creating-high-performance-asynchronous-backends-with-burn-compute"&gt;this blog post&lt;/a&gt;.&lt;/p&gt; 
  &lt;p&gt;Another very important memory optimization of Burn is that we keep track of when a tensor can be mutated in-place just by using the ownership system well. Even though it is a rather small memory optimization on its own, it adds up considerably when training or running inference with larger models and contributes to reduce the memory usage even more. For more information, see &lt;a href="https://burn.dev/blog/burn-rusty-approach-to-tensor-handling"&gt;this blog post about tensor handling&lt;/a&gt;.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Automatic kernel selection 🎯 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;A good deep learning framework should ensure that models run smoothly on all hardware. However, not all hardware share the same behavior in terms of execution speed. For instance, a matrix multiplication kernel can be launched with many different parameters, which are highly sensitive to the size of the matrices and the hardware. Using the wrong configuration could reduce the speed of execution by a large factor (10 times or even more in extreme cases), so choosing the right kernels becomes a priority.&lt;/p&gt; 
  &lt;p&gt;With our home-made backends, we run benchmarks automatically and choose the best configuration for the current hardware and matrix sizes with a reasonable caching strategy.&lt;/p&gt; 
  &lt;p&gt;This adds a small overhead by increasing the warmup execution time, but stabilizes quickly after a few forward and backward passes, saving lots of time in the long run. Note that this feature isn't mandatory, and can be disabled when cold starts are a priority over optimized throughput.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Hardware specific features 🔥 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;It is no secret that deep learning is mostly relying on matrix multiplication as its core operation, since this is how fully-connected neural networks are modeled.&lt;/p&gt; 
  &lt;p&gt;More and more, hardware manufacturers optimize their chips specifically for matrix multiplication workloads. For instance, Nvidia has its &lt;em&gt;Tensor Cores&lt;/em&gt; and today most cellphones have AI specialized chips. As of this moment, we support Tensor Cores with our LibTorch, Candle, CUDA, Metal and WGPU/SPIR-V backends, but not other accelerators yet. We hope &lt;a href="https://github.com/gpuweb/gpuweb/issues/4195"&gt;this issue&lt;/a&gt; gets resolved at some point to bring support to our WGPU backend.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Custom Backend Extension 🎒 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;Burn aims to be the most flexible deep learning framework. While it's crucial to maintain compatibility with a wide variety of backends, Burn also provides the ability to extend the functionalities of a backend implementation to suit your personal modeling requirements.&lt;/p&gt; 
  &lt;p&gt;This versatility is advantageous in numerous ways, such as supporting custom operations like flash attention or manually writing your own kernel for a specific backend to enhance performance. See &lt;a href="https://burn.dev/books/burn/advanced/backend-extension/index.html"&gt;this section&lt;/a&gt; in the Burn Book 🔥 for more details.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;h2&gt;Backend&lt;/h2&gt; 
 &lt;div align="left"&gt; 
  &lt;img align="right" src="https://raw.githubusercontent.com/tracel-ai/burn/main/assets/backend-chip.png" height="96px" /&gt; 
  &lt;p&gt;Burn strives to be as fast as possible on as many hardwares as possible, with robust implementations. We believe this flexibility is crucial for modern needs where you may train your models in the cloud, then deploy on customer hardwares, which vary from user to user.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;Supported Backends&lt;/strong&gt;&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Backend&lt;/th&gt; 
    &lt;th&gt;Devices&lt;/th&gt; 
    &lt;th&gt;Class&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CUDA&lt;/td&gt; 
    &lt;td&gt;NVIDIA GPUs&lt;/td&gt; 
    &lt;td&gt;First-Party&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;ROCm&lt;/td&gt; 
    &lt;td&gt;AMD GPUs&lt;/td&gt; 
    &lt;td&gt;First-Party&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Metal&lt;/td&gt; 
    &lt;td&gt;Apple GPUs&lt;/td&gt; 
    &lt;td&gt;First-Party&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Vulkan&lt;/td&gt; 
    &lt;td&gt;Most GPUs on Linux &amp;amp; Windows&lt;/td&gt; 
    &lt;td&gt;First-Party&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Wgpu&lt;/td&gt; 
    &lt;td&gt;Most GPUs&lt;/td&gt; 
    &lt;td&gt;First-Party&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;NdArray&lt;/td&gt; 
    &lt;td&gt;Most CPUs&lt;/td&gt; 
    &lt;td&gt;Third-Party&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;LibTorch&lt;/td&gt; 
    &lt;td&gt;Most GPUs &amp;amp; CPUs&lt;/td&gt; 
    &lt;td&gt;Third-Party&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Candle&lt;/td&gt; 
    &lt;td&gt;Nvidia, Apple GPUs &amp;amp; CPUs&lt;/td&gt; 
    &lt;td&gt;Third-Party&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;br /&gt; 
 &lt;p&gt;Compared to other frameworks, Burn has a very different approach to supporting many backends. By design, most code is generic over the Backend trait, which allows us to build Burn with swappable backends. This makes composing backend possible, augmenting them with additional functionalities such as autodifferentiation and automatic kernel fusion.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Autodiff: Backend decorator that brings backpropagation to any backend 🔄 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;Contrary to the aforementioned backends, Autodiff is actually a backend &lt;em&gt;decorator&lt;/em&gt;. This means that it cannot exist by itself; it must encapsulate another backend.&lt;/p&gt; 
  &lt;p&gt;The simple act of wrapping a base backend with Autodiff transparently equips it with autodifferentiation support, making it possible to call backward on your model.&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-rust"&gt;use burn::backend::{Autodiff, Wgpu};
use burn::tensor::{Distribution, Tensor};

fn main() {
    type Backend = Autodiff&amp;lt;Wgpu&amp;gt;;

    let device = Default::default();

    let x: Tensor&amp;lt;Backend, 2&amp;gt; = Tensor::random([32, 32], Distribution::Default, &amp;amp;device);
    let y: Tensor&amp;lt;Backend, 2&amp;gt; = Tensor::random([32, 32], Distribution::Default, &amp;amp;device).require_grad();

    let tmp = x.clone() + y.clone();
    let tmp = tmp.matmul(x);
    let tmp = tmp.exp();

    let grads = tmp.backward();
    let y_grad = y.grad(&amp;amp;grads).unwrap();
    println!("{y_grad}");
}
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;Of note, it is impossible to make the mistake of calling backward on a model that runs on a backend that does not support autodiff (for inference), as this method is only offered by an Autodiff backend.&lt;/p&gt; 
  &lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/crates/burn-autodiff/README.md"&gt;Autodiff Backend README&lt;/a&gt; for more details.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Fusion: Backend decorator that brings kernel fusion to all first-party backends &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;This backend decorator enhances a backend with kernel fusion, provided that the inner backend supports it. Note that you can compose this backend with other backend decorators such as Autodiff. All first-party accelerated backends (like WGPU and CUDA) use Fusion by default (&lt;code&gt;burn/fusion&lt;/code&gt; feature flag), so you typically don't need to apply it manually.&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-rust"&gt;#[cfg(not(feature = "fusion"))]
pub type Cuda&amp;lt;F = f32, I = i32&amp;gt; = CubeBackend&amp;lt;CudaRuntime, F, I, u8&amp;gt;;

#[cfg(feature = "fusion")]
pub type Cuda&amp;lt;F = f32, I = i32&amp;gt; = burn_fusion::Fusion&amp;lt;CubeBackend&amp;lt;CudaRuntime, F, I, u8&amp;gt;&amp;gt;;
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;Of note, we plan to implement automatic gradient checkpointing based on compute bound and memory bound operations, which will work gracefully with the fusion backend to make your code run even faster during training, see &lt;a href="https://github.com/tracel-ai/burn/issues/936"&gt;this issue&lt;/a&gt;.&lt;/p&gt; 
  &lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/crates/burn-fusion/README.md"&gt;Fusion Backend README&lt;/a&gt; for more details.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Router (Beta): Backend decorator that composes multiple backends into a single one &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;That backend simplifies hardware operability, if for instance you want to execute some operations on the CPU and other operations on the GPU.&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-rust"&gt;use burn::tensor::{Distribution, Tensor};
use burn::backend::{
    NdArray, Router, Wgpu, ndarray::NdArrayDevice, router::duo::MultiDevice, wgpu::WgpuDevice,
};

fn main() {
    type Backend = Router&amp;lt;(Wgpu, NdArray)&amp;gt;;

    let device_0 = MultiDevice::B1(WgpuDevice::DiscreteGpu(0));
    let device_1 = MultiDevice::B2(NdArrayDevice::Cpu);

    let tensor_gpu =
        Tensor::&amp;lt;Backend, 2&amp;gt;::random([3, 3], burn::tensor::Distribution::Default, &amp;amp;device_0);
    let tensor_cpu =
        Tensor::&amp;lt;Backend, 2&amp;gt;::random([3, 3], burn::tensor::Distribution::Default, &amp;amp;device_1);
}

&lt;/code&gt;&lt;/pre&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Remote (Beta): Backend decorator for remote backend execution, useful for distributed computations &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;That backend has two parts, one client and one server. The client sends tensor operations over the network to a remote compute backend. You can use any first-party backend as server in a single line of code:&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-rust"&gt;fn main_server() {
    // Start a server on port 3000.
    burn::server::start::&amp;lt;burn::backend::Cuda&amp;gt;(Default::default(), 3000);
}

fn main_client() {
    // Create a client that communicate with the server on port 3000.
    use burn::backend::{Autodiff, RemoteBackend};

    type Backend = Autodiff&amp;lt;RemoteDevice&amp;gt;;

    let device = RemoteDevice::new("ws://localhost:3000");
    let tensor_gpu =
        Tensor::&amp;lt;Backend, 2&amp;gt;::random([3, 3], Distribution::Default, &amp;amp;device);
}

&lt;/code&gt;&lt;/pre&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;h2&gt;Training &amp;amp; Inference&lt;/h2&gt; 
 &lt;div align="left"&gt; 
  &lt;img align="right" src="https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-wall.png" height="96px" /&gt; 
  &lt;p&gt;The whole deep learning workflow is made easy with Burn, as you can monitor your training progress with an ergonomic dashboard, and run inference everywhere from embedded devices to large GPU clusters.&lt;/p&gt; 
  &lt;p&gt;Burn was built from the ground up with training and inference in mind. It's also worth noting how Burn, in comparison to frameworks like PyTorch, simplifies the transition from training to deployment, eliminating the need for code changes.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div align="center"&gt; 
  &lt;br /&gt; 
  &lt;a href="https://www.youtube.com/watch?v=N9RM5CQbNQc" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/tracel-ai/burn/main/assets/burn-train-tui.png" alt="Burn Train TUI" width="75%" /&gt; &lt;/a&gt; 
 &lt;/div&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;Click on the following sections to expand 👇&lt;/strong&gt;&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Training Dashboard 📈 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;As you can see in the previous video (click on the picture!), a new terminal UI dashboard based on the &lt;a href="https://github.com/ratatui-org/ratatui"&gt;Ratatui&lt;/a&gt; crate allows users to follow their training with ease without having to connect to any external application.&lt;/p&gt; 
  &lt;p&gt;You can visualize your training and validation metrics updating in real-time and analyze the lifelong progression or recent history of any registered metrics using only the arrow keys. Break from the training loop without crashing, allowing potential checkpoints to be fully written or important pieces of code to complete without interruption 🛡&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; ONNX Support 🐫 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;ONNX (Open Neural Network Exchange) is an open-standard format that exports both the architecture and the weights of a deep learning model.&lt;/p&gt; 
  &lt;p&gt;Burn supports the importation of models that follow the ONNX standard so you can easily port a model you have written in another framework like TensorFlow or PyTorch to Burn to benefit from all the advantages our framework offers.&lt;/p&gt; 
  &lt;p&gt;Our ONNX support is further described in &lt;a href="https://burn.dev/books/burn/import/onnx-model.html"&gt;this section of the Burn Book 🔥&lt;/a&gt;.&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This crate is in active development and currently supports a &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/crates/burn-import/SUPPORTED-ONNX-OPS.md"&gt;limited set of ONNX operators&lt;/a&gt;.&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Importing PyTorch or Safetensors Models 🚚 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;You can load weights from PyTorch or Safetensors formats directly into your Burn-defined models. This makes it easy to reuse existing models while benefiting from Burn's performance and deployment features.&lt;/p&gt; 
  &lt;p&gt;Learn more:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://burn.dev/books/burn/import/pytorch-model.html"&gt;Import pre-trained PyTorch models into Burn&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://burn.dev/books/burn/import/safetensors-model.html"&gt;Load models from Safetensors format&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Inference in the Browser 🌐 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;Several of our backends can run in WebAssembly environments: Candle and NdArray for CPU execution, and WGPU for GPU acceleration via WebGPU. This means that you can run inference directly within a browser. We provide several examples of this:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/mnist-inference-web"&gt;MNIST&lt;/a&gt; where you can draw digits and a small convnet tries to find which one it is! 2️⃣ 7️⃣ 😰&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/image-classification-web"&gt;Image Classification&lt;/a&gt; where you can upload images and classify them! 🌄&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Embedded: &lt;i&gt;no_std&lt;/i&gt; support ⚙️ &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;Burn's core components support &lt;a href="https://docs.rust-embedded.org/book/intro/no-std.html"&gt;no_std&lt;/a&gt;. This means it can run in bare metal environment such as embedded devices without an operating system.&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;As of now, only the NdArray backend can be used in a &lt;em&gt;no_std&lt;/em&gt; environment.&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;h3&gt;Benchmarks&lt;/h3&gt; 
 &lt;p&gt;To evaluate performance across different backends and track improvements over time, we provide a dedicated benchmarking suite.&lt;/p&gt; 
 &lt;p&gt;Run and compare benchmarks using &lt;a href="https://github.com/tracel-ai/burn-bench"&gt;burn-bench&lt;/a&gt;.&lt;/p&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;⚠️ &lt;strong&gt;Warning&lt;/strong&gt; When using one of the &lt;code&gt;wgpu&lt;/code&gt; backends, you may encounter compilation errors related to recursive type evaluation. This is due to complex type nesting within the &lt;code&gt;wgpu&lt;/code&gt; dependency chain. To resolve this issue, add the following line at the top of your &lt;code&gt;main.rs&lt;/code&gt; or &lt;code&gt;lib.rs&lt;/code&gt; file:&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-rust"&gt;#![recursion_limit = "256"]
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;The default recursion limit (128) is often just below the required depth (typically 130-150) due to deeply nested associated types and trait bounds.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h2&gt;Getting Started&lt;/h2&gt; 
 &lt;div align="left"&gt; 
  &lt;img align="right" src="https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-walking.png" height="96px" /&gt; 
  &lt;p&gt;Just heard of Burn? You are at the right place! Just continue reading this section and we hope you can get on board really quickly.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;details&gt; 
  &lt;summary&gt; The Burn Book 🔥 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;To begin working effectively with Burn, it is crucial to understand its key components and philosophy. This is why we highly recommend new users to read the first sections of &lt;a href="https://burn.dev/books/burn/"&gt;The Burn Book 🔥&lt;/a&gt;. It provides detailed examples and explanations covering every facet of the framework, including building blocks like tensors, modules, and optimizers, all the way to advanced usage, like coding your own GPU kernels.&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;The project is constantly evolving, and we try as much as possible to keep the book up to date with new additions. However, we might miss some details sometimes, so if you see something weird, let us know! We also gladly accept Pull Requests 😄&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Examples 🙏 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;Let's start with a code snippet that shows how intuitive the framework is to use! In the following, we declare a neural network module with some parameters along with its forward pass.&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-rust"&gt;use burn::nn;
use burn::module::Module;
use burn::tensor::backend::Backend;

#[derive(Module, Debug)]
pub struct PositionWiseFeedForward&amp;lt;B: Backend&amp;gt; {
    linear_inner: nn::Linear&amp;lt;B&amp;gt;,
    linear_outer: nn::Linear&amp;lt;B&amp;gt;,
    dropout: nn::Dropout,
    gelu: nn::Gelu,
}

impl&amp;lt;B: Backend&amp;gt; PositionWiseFeedForward&amp;lt;B&amp;gt; {
    pub fn forward&amp;lt;const D: usize&amp;gt;(&amp;amp;self, input: Tensor&amp;lt;B, D&amp;gt;) -&amp;gt; Tensor&amp;lt;B, D&amp;gt; {
        let x = self.linear_inner.forward(input);
        let x = self.gelu.forward(x);
        let x = self.dropout.forward(x);

        self.linear_outer.forward(x)
    }
}
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;We have a somewhat large amount of &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples"&gt;examples&lt;/a&gt; in the repository that shows how to use the framework in different scenarios.&lt;/p&gt; 
  &lt;p&gt;Following &lt;a href="https://burn.dev/books/burn/"&gt;the book&lt;/a&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/guide"&gt;Basic Workflow&lt;/a&gt; : Creates a custom CNN &lt;code&gt;Module&lt;/code&gt; to train on the MNIST dataset and use for inference.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/custom-training-loop"&gt;Custom Training Loop&lt;/a&gt; : Implements a basic training loop instead of using the &lt;code&gt;Learner&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/custom-wgpu-kernel"&gt;Custom WGPU Kernel&lt;/a&gt; : Learn how to create your own custom operation with the WGPU backend.&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;Additional examples:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/custom-csv-dataset"&gt;Custom CSV Dataset&lt;/a&gt; : Implements a dataset to parse CSV data for a regression task.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/simple-regression"&gt;Regression&lt;/a&gt; : Trains a simple MLP on the California Housing dataset to predict the median house value for a district.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/custom-image-dataset"&gt;Custom Image Dataset&lt;/a&gt; : Trains a simple CNN on custom image dataset following a simple folder structure.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/custom-renderer"&gt;Custom Renderer&lt;/a&gt; : Implements a custom renderer to display the &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/building-blocks/learner.md"&gt;&lt;code&gt;Learner&lt;/code&gt;&lt;/a&gt; progress.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/image-classification-web"&gt;Image Classification Web&lt;/a&gt; : Image classification web browser demo using Burn, WGPU and WebAssembly.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/mnist-inference-web"&gt;MNIST Inference on Web&lt;/a&gt; : An interactive MNIST inference demo in the browser. The demo is available &lt;a href="https://burn.dev/demo/"&gt;online&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/mnist"&gt;MNIST Training&lt;/a&gt; : Demonstrates how to train a custom &lt;code&gt;Module&lt;/code&gt; (MLP) with the &lt;code&gt;Learner&lt;/code&gt; configured to log metrics and keep training checkpoints.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/named-tensor"&gt;Named Tensor&lt;/a&gt; : Performs operations with the experimental &lt;code&gt;NamedTensor&lt;/code&gt; feature.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/onnx-inference"&gt;ONNX Import Inference&lt;/a&gt; : Imports an ONNX model pre-trained on MNIST to perform inference on a sample image with Burn.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/import-model-weights"&gt;PyTorch Import Inference&lt;/a&gt; : Imports a PyTorch model pre-trained on MNIST to perform inference on a sample image with Burn.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/text-classification"&gt;Text Classification&lt;/a&gt; : Trains a text classification transformer model on the AG News or DbPedia dataset. The trained model can then be used to classify a text sample.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/text-generation"&gt;Text Generation&lt;/a&gt; : Trains a text generation transformer model on the DbPedia dataset.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/wgan"&gt;Wasserstein GAN MNIST&lt;/a&gt; : Trains a WGAN model to generate new handwritten digits based on MNIST.&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;For more practical insights, you can clone the repository and run any of them directly on your computer!&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Pre-trained Models 🤖 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;We keep an updated and curated list of models and examples built with Burn, see the &lt;a href="https://github.com/tracel-ai/models"&gt;tracel-ai/models repository&lt;/a&gt; for more details.&lt;/p&gt; 
  &lt;p&gt;Don't see the model you want? Don't hesitate to open an issue, and we may prioritize it. Built a model using Burn and want to share it? You can also open a Pull Request and add your model under the community section!&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Why use Rust for Deep Learning? 🦀 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;Deep Learning is a special form of software where you need very high level abstractions as well as extremely fast execution time. Rust is the perfect candidate for that use case since it provides zero-cost abstractions to easily create neural network modules, and fine-grained control over memory to optimize every detail.&lt;/p&gt; 
  &lt;p&gt;It's important that a framework be easy to use at a high level so that its users can focus on innovating in the AI field. However, since running models relies so heavily on computations, performance can't be neglected.&lt;/p&gt; 
  &lt;p&gt;To this day, the mainstream solution to this problem has been to offer APIs in Python, but rely on bindings to low-level languages such as C/C++. This reduces portability, increases complexity and creates frictions between researchers and engineers. We feel like Rust's approach to abstractions makes it versatile enough to tackle this two languages dichotomy.&lt;/p&gt; 
  &lt;p&gt;Rust also comes with the Cargo package manager, which makes it incredibly easy to build, test, and deploy from any environment, which is usually a pain in Python.&lt;/p&gt; 
  &lt;p&gt;Although Rust has the reputation of being a difficult language at first, we strongly believe it leads to more reliable, bug-free solutions built faster (after some practice 😅)!&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;Deprecation Note&lt;/strong&gt;&lt;br /&gt;Since &lt;code&gt;0.14.0&lt;/code&gt;, the internal structure for tensor data has changed. The previous &lt;code&gt;Data&lt;/code&gt; struct was deprecated and officially removed since &lt;code&gt;0.17.0&lt;/code&gt; in favor of the new &lt;code&gt;TensorData&lt;/code&gt; struct, which allows for more flexibility by storing the underlying data as bytes and keeping the data type as a field. If you are using &lt;code&gt;Data&lt;/code&gt; in your code, make sure to switch to &lt;code&gt;TensorData&lt;/code&gt;.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;!-- &gt;
&gt; In the event that you are trying to load a model record saved in a previous version, make sure to
&gt; enable the `record-backward-compat` feature using a previous version of burn (&lt;=0.16.0). Otherwise,
&gt; the record won't be deserialized correctly and you will get an error message (which will also point
&gt; you to the backward compatible feature flag). The backward compatibility was maintained for
&gt; deserialization (loading), so as soon as you have saved the record again it will be saved according
&gt; to the new structure and you will be able to upgrade to this version. Please note that binary formats
&gt; are not backward compatible. Thus, you will need to load your record in a previous version and save it
&gt; to another of the self-describing record formats before using a compatible version (as described) with the
&gt; `record-backward-compat` feature flag. --&gt; 
 &lt;details id="deprecation"&gt; 
  &lt;summary&gt; Loading Model Records From Previous Versions ⚠️ &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;In the event that you are trying to load a model record saved in a version older than &lt;code&gt;0.14.0&lt;/code&gt;, make sure to use a compatible version (&lt;code&gt;0.14&lt;/code&gt;, &lt;code&gt;0.15&lt;/code&gt; or &lt;code&gt;0.16&lt;/code&gt;) with the &lt;code&gt;record-backward-compat&lt;/code&gt; feature flag.&lt;/p&gt; 
  &lt;pre&gt;&lt;code&gt;features = [..., "record-backward-compat"]
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;Otherwise, the record won't be deserialized correctly and you will get an error message. This error will also point you to the backward compatible feature flag.&lt;/p&gt; 
  &lt;p&gt;The backward compatibility was maintained for deserialization when loading records. Therefore, as soon as you have saved the record again it will be saved according to the new structure and you can upgrade back to the current version&lt;/p&gt; 
  &lt;p&gt;Please note that binary formats are not backward compatible. Thus, you will need to load your record in a previous version and save it in any of the other self-describing record format (e.g., using the &lt;code&gt;NamedMpkFileRecorder&lt;/code&gt;) before using a compatible version (as described) with the &lt;code&gt;record-backward-compat&lt;/code&gt; feature flag.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;h2&gt;Community&lt;/h2&gt; 
 &lt;div align="left"&gt; 
  &lt;img align="right" src="https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-community.png" height="96px" /&gt; 
  &lt;p&gt;If you are excited about the project, don't hesitate to join our &lt;a href="https://discord.gg/uPEBbYYDB6"&gt;Discord&lt;/a&gt;! We try to be as welcoming as possible to everybody from any background. You can ask your questions and share what you built with the community!&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;Contributing&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Before contributing, please take a moment to review our &lt;a href="https://github.com/tracel-ai/burn/tree/main/CODE-OF-CONDUCT.md"&gt;code of conduct&lt;/a&gt;. It's also highly recommended to read the &lt;a href="https://github.com/tracel-ai/burn/tree/main/contributor-book/src/project-architecture"&gt;architecture overview&lt;/a&gt;, which explains some of our architectural decisions. Refer to our &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/CONTRIBUTING.md"&gt;contributing guide&lt;/a&gt; for more details.&lt;/p&gt; 
 &lt;h2&gt;Status&lt;/h2&gt; 
 &lt;p&gt;Burn is currently in active development, and there will be breaking changes. While any resulting issues are likely to be easy to fix, there are no guarantees at this stage.&lt;/p&gt; 
 &lt;h2&gt;License&lt;/h2&gt; 
 &lt;p&gt;Burn is distributed under the terms of both the MIT license and the Apache License (Version 2.0). See &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/LICENSE-APACHE"&gt;LICENSE-APACHE&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/LICENSE-MIT"&gt;LICENSE-MIT&lt;/a&gt; for details. Opening a pull request is assumed to signal agreement with these licensing terms.&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>sigp/lighthouse</title>
      <link>https://github.com/sigp/lighthouse</link>
      <description>&lt;p&gt;Ethereum consensus client in Rust&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Lighthouse: Ethereum consensus client&lt;/h1&gt; 
&lt;p&gt;An open-source Ethereum consensus client, written in Rust and maintained by Sigma Prime.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://lighthouse-book.sigmaprime.io"&gt;&lt;img src="https://img.shields.io/badge/user--docs-unstable-informational" alt="Book Status" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/cyAszAh"&gt;&lt;img src="https://img.shields.io/badge/chat-discord-%237289da" alt="Chat Badge" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://lighthouse-book.sigmaprime.io"&gt;Documentation&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://i.postimg.cc/hjdTGKPd/photo-2020-10-23-09-52-16.jpg" alt="Banner" /&gt;&lt;/p&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;Lighthouse is:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Ready for use on Ethereum consensus mainnet.&lt;/li&gt; 
 &lt;li&gt;Fully open-source, licensed under Apache 2.0.&lt;/li&gt; 
 &lt;li&gt;Security-focused. Fuzzing techniques have been continuously applied and several external security reviews have been performed.&lt;/li&gt; 
 &lt;li&gt;Built in &lt;a href="https://www.rust-lang.org"&gt;Rust&lt;/a&gt;, a modern language providing unique safety guarantees and excellent performance (comparable to C++).&lt;/li&gt; 
 &lt;li&gt;Funded by various organisations, including Sigma Prime, the Ethereum Foundation, Consensys, the Decentralization Foundation and private individuals.&lt;/li&gt; 
 &lt;li&gt;Actively involved in the specification and security analysis of the Ethereum proof-of-stake consensus specification.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Staking Deposit Contract&lt;/h2&gt; 
&lt;p&gt;The Lighthouse team acknowledges &lt;a href="https://etherscan.io/address/0x00000000219ab540356cbb839cbe05303d7705fa"&gt;&lt;code&gt;0x00000000219ab540356cBB839Cbe05303d7705Fa&lt;/code&gt;&lt;/a&gt; as the canonical staking deposit contract address.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://lighthouse-book.sigmaprime.io"&gt;Lighthouse Book&lt;/a&gt; contains information for users and developers.&lt;/p&gt; 
&lt;p&gt;The Lighthouse team maintains a blog at &lt;a href="https://lighthouse-blog.sigmaprime.io"&gt;https://blog.sigmaprime.io/tag/lighthouse&lt;/a&gt; which contains periodic progress updates, roadmap insights and interesting findings.&lt;/p&gt; 
&lt;h2&gt;Branches&lt;/h2&gt; 
&lt;p&gt;Lighthouse maintains two permanent branches:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sigp/lighthouse/tree/stable"&gt;&lt;code&gt;stable&lt;/code&gt;&lt;/a&gt;: Always points to the latest stable release. 
  &lt;ul&gt; 
   &lt;li&gt;This is ideal for most users.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sigp/lighthouse/tree/unstable"&gt;&lt;code&gt;unstable&lt;/code&gt;&lt;/a&gt;: Used for development, contains the latest PRs. 
  &lt;ul&gt; 
   &lt;li&gt;Developers should base their PRs on this branch.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Lighthouse welcomes contributors.&lt;/p&gt; 
&lt;p&gt;If you are looking to contribute, please head to the &lt;a href="https://lighthouse-book.sigmaprime.io/contributing.html"&gt;Contributing&lt;/a&gt; section of the Lighthouse book.&lt;/p&gt; 
&lt;h2&gt;Contact&lt;/h2&gt; 
&lt;p&gt;The best place for discussion is the &lt;a href="https://discord.gg/cyAszAh"&gt;Lighthouse Discord server&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Sign up to the &lt;a href="https://eepurl.com/dh9Lvb"&gt;Lighthouse Development Updates&lt;/a&gt; mailing list for email notifications about releases, network status and other important information.&lt;/p&gt; 
&lt;p&gt;Encrypt sensitive messages using our &lt;a href="https://keybase.io/sigp/pgp_keys.asc?fingerprint=15e66d941f697e28f49381f426416dc3f30674b0"&gt;PGP key&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Donations&lt;/h2&gt; 
&lt;p&gt;Lighthouse is an open-source project and a public good. Funding public goods is hard and we're grateful for the donations we receive from the community via:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://gitcoin.co/grants/25/lighthouse-ethereum-20-client"&gt;Gitcoin Grants&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Ethereum address: &lt;code&gt;0x25c4a76E7d118705e7Ea2e9b7d8C59930d8aCD3b&lt;/code&gt; (donation.sigmaprime.eth).&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>rerun-io/rerun</title>
      <link>https://github.com/rerun-io/rerun</link>
      <description>&lt;p&gt;Visualize streams of multimodal data. Free, fast, easy to use, and simple to integrate. Built in Rust.&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; &lt;a href="https://www.rerun.io/"&gt; &lt;img alt="banner" src="https://user-images.githubusercontent.com/1148717/218142418-1d320929-6b7a-486e-8277-fbeef2432529.png" /&gt; &lt;/a&gt; &lt;/h1&gt; 
&lt;h1 align="center"&gt; &lt;a href="https://pypi.org/project/rerun-sdk/"&gt; &lt;img alt="PyPi" src="https://img.shields.io/pypi/v/rerun-sdk.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;a href="https://crates.io/crates/rerun"&gt; &lt;img alt="crates.io" src="https://img.shields.io/crates/v/rerun.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;a href="https://github.com/rerun-io/rerun/raw/main/LICENSE-MIT"&gt; &lt;img alt="MIT" src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;a href="https://github.com/rerun-io/rerun/raw/main/LICENSE-APACHE"&gt; &lt;img alt="Apache" src="https://img.shields.io/badge/license-Apache-blue.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;a href="https://discord.gg/Gcm8BbTaAj"&gt; &lt;img alt="Rerun Discord" src="https://img.shields.io/discord/1062300748202921994?label=Rerun%20Discord" /&gt; &lt;/a&gt; &lt;/h1&gt; 
&lt;h1&gt;Time-aware multimodal data stack and visualizations&lt;/h1&gt; 
&lt;p&gt;Rerun is building the multimodal data stack to model, ingest, store, query and view robotics-style data. It's used in areas like robotics, spatial and embodied AI, generative media, industrial processing, simulation, security, and health.&lt;/p&gt; 
&lt;p&gt;Rerun is easy to use! Use the Rerun SDK (available for C++, Python and Rust) to log data like images, tensors, point clouds, and text. Logs are streamed to the Rerun Viewer for live visualization or to file for later use. You can also query the logged data through &lt;a href="https://rerun.io/docs/howto/dataframe-api"&gt;our dataframe API&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/rerun-io/rerun/main/#getting-started"&gt;Get started&lt;/a&gt; in minutes – no account needed.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.rerun.io/viewer"&gt;Run the Rerun Viewer in your browser&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.rerun.io/docs/getting-started/what-is-rerun"&gt;Read about what Rerun is and who it is for&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;A short taste&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import rerun as rr  # pip install rerun-sdk

rr.init("rerun_example_app")

rr.spawn()  # Spawn a child process with a viewer and connect
# rr.save("recording.rrd")  # Stream all logs to disk
# rr.connect_grpc()  # Connect to a remote viewer

# Associate subsequent data with 42 on the “frame” timeline
rr.set_time("frame", sequence=42)

# Log colored 3D points to the entity at `path/to/points`
rr.log("path/to/points", rr.Points3D(positions, colors=colors))
…
&lt;/code&gt;&lt;/pre&gt; 
&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;img src="https://static.rerun.io/opf_screenshot/bee51040cba93c0bae62ef6c57fa703704012a41/full.png" alt="" /&gt; 
  &lt;source media="(max-width: 480px)" srcset="https://static.rerun.io/opf_screenshot/bee51040cba93c0bae62ef6c57fa703704012a41/480w.png" /&gt; 
  &lt;source media="(max-width: 768px)" srcset="https://static.rerun.io/opf_screenshot/bee51040cba93c0bae62ef6c57fa703704012a41/768w.png" /&gt; 
  &lt;source media="(max-width: 1024px)" srcset="https://static.rerun.io/opf_screenshot/bee51040cba93c0bae62ef6c57fa703704012a41/1024w.png" /&gt; 
  &lt;source media="(max-width: 1200px)" srcset="https://static.rerun.io/opf_screenshot/bee51040cba93c0bae62ef6c57fa703704012a41/1200w.png" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.rerun.io/docs/getting-started/quick-start/cpp"&gt;&lt;strong&gt;C++&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.rerun.io/docs/getting-started/quick-start/python"&gt;&lt;strong&gt;Python&lt;/strong&gt;&lt;/a&gt;: &lt;code&gt;pip install rerun-sdk&lt;/code&gt; or on &lt;a href="https://github.com/conda-forge/rerun-sdk-feedstock"&gt;&lt;code&gt;conda&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.rerun.io/docs/getting-started/quick-start/rust"&gt;&lt;strong&gt;Rust&lt;/strong&gt;&lt;/a&gt;: &lt;code&gt;cargo add rerun&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installing the Rerun Viewer binary&lt;/h3&gt; 
&lt;p&gt;To stream log data over the network or load our &lt;code&gt;.rrd&lt;/code&gt; data files you also need the &lt;code&gt;rerun&lt;/code&gt; binary. It can be installed with &lt;code&gt;pip install rerun-sdk&lt;/code&gt; or with &lt;code&gt;cargo install rerun-cli --locked --features nasm&lt;/code&gt; (see note below). Note that only the Python SDK comes bundled with the Viewer whereas C++ &amp;amp; Rust always rely on a separate install.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: the &lt;code&gt;nasm&lt;/code&gt; Cargo feature requires the &lt;a href="https://github.com/netwide-assembler/nasm"&gt;&lt;code&gt;nasm&lt;/code&gt;&lt;/a&gt; CLI to be installed and available in your path. Alternatively, you may skip enabling this feature, but this may result in inferior video decoding performance.&lt;/p&gt; 
&lt;p&gt;You should now be able to run &lt;code&gt;rerun --help&lt;/code&gt; in any terminal.&lt;/p&gt; 
&lt;h3&gt;Documentation&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📚 &lt;a href="http://rerun.io/docs"&gt;High-level docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;⏃ &lt;a href="https://www.rerun.io/docs/reference/types"&gt;Loggable Types&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;⚙️ &lt;a href="http://rerun.io/examples"&gt;Examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;📖 &lt;a href="https://raw.githubusercontent.com/rerun-io/rerun/main/docs/snippets/INDEX.md"&gt;Code snippets&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;🌊 &lt;a href="https://ref.rerun.io/docs/cpp"&gt;C++ API docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;🐍 &lt;a href="https://ref.rerun.io/docs/python"&gt;Python API docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;🦀 &lt;a href="https://docs.rs/rerun/"&gt;Rust API docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;⁉️ &lt;a href="https://www.rerun.io/docs/getting-started/troubleshooting"&gt;Troubleshooting&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Status&lt;/h2&gt; 
&lt;p&gt;We are in active development. There are many features we want to add, and the API is still evolving. &lt;em&gt;Expect breaking changes!&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Some shortcomings:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/rerun-io/rerun/issues/7115"&gt;The viewer slows down when there are too many entities&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/rerun-io/rerun/issues/1611"&gt;We don't support transparency yet&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;The data you want to visualize must fit in RAM 
  &lt;ul&gt; 
   &lt;li&gt;See &lt;a href="https://www.rerun.io/docs/howto/limit-ram"&gt;https://www.rerun.io/docs/howto/limit-ram&lt;/a&gt; for how to bound memory use.&lt;/li&gt; 
   &lt;li&gt;We plan on having a disk-based data store some time in the future.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/rerun-io/rerun/issues/1136"&gt;Multi-million point clouds can be slow&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What is Rerun for?&lt;/h2&gt; 
&lt;p&gt;Rerun is built to help you understand and improve complex processes that include rich multimodal data, like 2D, 3D, text, time series, tensors, etc. It is used in many industries, including robotics, simulation, computer vision, or anything that involves a lot of sensors or other signals that evolve over time.&lt;/p&gt; 
&lt;h3&gt;Example use case&lt;/h3&gt; 
&lt;p&gt;Say you're building a vacuum cleaning robot and it keeps running into walls. Why is it doing that? You need some tool to debug it, but a normal debugger isn't gonna be helpful. Similarly, just logging text won't be very helpful either. The robot may log "Going through doorway" but that won't explain why it thinks the wall is a door.&lt;/p&gt; 
&lt;p&gt;What you need is a visual and temporal debugger, that can log all the different representations of the world the robots holds in its little head, such as:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;RGB camera feed&lt;/li&gt; 
 &lt;li&gt;depth images&lt;/li&gt; 
 &lt;li&gt;lidar scan&lt;/li&gt; 
 &lt;li&gt;segmentation image (how the robot interprets what it sees)&lt;/li&gt; 
 &lt;li&gt;its 3D map of the apartment&lt;/li&gt; 
 &lt;li&gt;all the objects the robot has detected (or thinks it has detected), as 3D shapes in the 3D map&lt;/li&gt; 
 &lt;li&gt;its confidence in its prediction&lt;/li&gt; 
 &lt;li&gt;etc&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You also want to see how all these streams of data evolve over time so you can go back and pinpoint exactly what went wrong, when and why.&lt;/p&gt; 
&lt;p&gt;Maybe it turns out that a glare from the sun hit one of the sensors in the wrong way, confusing the segmentation network leading to bad object detection. Or maybe it was a bug in the lidar scanning code. Or maybe the robot thought it was somewhere else in the apartment, because its odometry was broken. Or it could be one of a thousand other things. Rerun will help you find out!&lt;/p&gt; 
&lt;p&gt;But seeing the world from the point of the view of the robot is not just for debugging - it will also give you ideas on how to improve the algorithms, new test cases to set up, or datasets to collect. It will also let you explain the brains of the robot to your colleagues, boss, and customers. And so on. Seeing is believing, and an image is worth a thousand words, and multimodal temporal logging is worth a thousand images :)&lt;/p&gt; 
&lt;p&gt;While seeing and understanding your data is core to making progress in robotics, there is one more thing: You can also use the data you collected for visualization to create new datasets for training and evaluating the models and algorithms that run on your robot. Rerun provides query APIs to make it easy to extract clean datasets from your recording for exactly that purpose.&lt;/p&gt; 
&lt;p&gt;Of course, Rerun is useful for much more than just robots. Any time you have any form of sensors, or 2D or 3D state evolving over time, Rerun is a great tool.&lt;/p&gt; 
&lt;h2&gt;Business model&lt;/h2&gt; 
&lt;p&gt;Rerun uses an open-core model. Everything in this repository will stay open source and free (both as in beer and as in freedom).&lt;/p&gt; 
&lt;p&gt;We are also building a commercial data platform. Right now that is only available for a few select design partners. &lt;a href="https://rerun.io/pricing"&gt;Click here if you're interested&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The Rerun open source project targets the needs of individual developers. The commercial product targets the needs specific to teams that build and run computer vision and robotics products.&lt;/p&gt; 
&lt;h2&gt;How to cite Rerun&lt;/h2&gt; 
&lt;p&gt;When using Rerun in your research, please cite it to acknowledge its contribution to your work. This can be done by including a reference to Rerun in the software or methods section of your paper.&lt;/p&gt; 
&lt;p&gt;Suggested citation format:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@software{RerunSDK,
  title = {Rerun: A Visualization SDK for Multimodal Data},
  author = {{Rerun Development Team}},
  url = {https://www.rerun.io},
  version = {insert version number},
  date = {insert date of usage},
  year = {2024},
  publisher = {{Rerun Technologies AB}},
  address = {Online},
  note = {Available from https://www.rerun.io/ and https://github.com/rerun-io/rerun}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please replace "insert version number" with the version of Rerun you used and "insert date of usage" with the date(s) you used the tool in your research. This citation format helps ensure that Rerun's development team receives appropriate credit for their work and facilitates the tool's discovery by other researchers.&lt;/p&gt; 
&lt;h1&gt;Development&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rerun-io/rerun/main/ARCHITECTURE.md"&gt;&lt;code&gt;ARCHITECTURE.md&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rerun-io/rerun/main/CODE_OF_CONDUCT.md"&gt;&lt;code&gt;CODE_OF_CONDUCT.md&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rerun-io/rerun/main/CODE_STYLE.md"&gt;&lt;code&gt;CODE_STYLE.md&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rerun-io/rerun/main/CONTRIBUTING.md"&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rerun-io/rerun/main/BUILD.md"&gt;&lt;code&gt;BUILD.md&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rerun-io/rerun/main/rerun_py/README.md"&gt;&lt;code&gt;rerun_py/README.md&lt;/code&gt;&lt;/a&gt; - instructions for Python SDK&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rerun-io/rerun/main/rerun_cpp/README.md"&gt;&lt;code&gt;rerun_cpp/README.md&lt;/code&gt;&lt;/a&gt; - instructions for C++ SDK&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installing a pre-release Python SDK&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Download the correct &lt;code&gt;.whl&lt;/code&gt; from &lt;a href="https://github.com/rerun-io/rerun/releases"&gt;GitHub Releases&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;pip install rerun_sdk&amp;lt;…&amp;gt;.whl&lt;/code&gt; (replace &lt;code&gt;&amp;lt;…&amp;gt;&lt;/code&gt; with the actual filename)&lt;/li&gt; 
 &lt;li&gt;Test it: &lt;code&gt;rerun --version&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt;</description>
    </item>
    
    <item>
      <title>warp-tech/warpgate</title>
      <link>https://github.com/warp-tech/warpgate</link>
      <description>&lt;p&gt;Fully transparent SSH, HTTPS, MySQL and Postgres bastion/PAM that doesn't need additional client-side software&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://github.com/user-attachments/assets/89be835b-ff96-46df-94c7-ae2d176615e3" /&gt; &lt;/p&gt; 
&lt;br /&gt; 
&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset=".github/readme/brand-dark.svg" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="warpgate-web/public/assets/brand.svg" /&gt; 
  &lt;img alt="Shows a black logo in light color mode and a white one in dark color mode." src="https://raw.githubusercontent.com/warp-tech/warpgate/main/.github/readme/brand-dark.svg?sanitize=true" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;br /&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/warp-tech/warpgate/releases/latest"&gt;&lt;img alt="GitHub All Releases" src="https://img.shields.io/github/downloads/warp-tech/warpgate/total.svg?label=DOWNLOADS&amp;amp;logo=github&amp;amp;style=for-the-badge&amp;amp;color=8f8" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://nightly.link/warp-tech/warpgate/workflows/build/main"&gt;&lt;img src="https://shields.io/badge/-Nightly%20Builds-fa5?logo=hackthebox&amp;amp;logoColor=444&amp;amp;style=for-the-badge" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://discord.gg/Vn7BjmzhtF"&gt;&lt;img alt="Discord" src="https://img.shields.io/discord/1280890060195233934?style=for-the-badge&amp;amp;color=acc&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;label=Discord" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://ko-fi.com/J3J8KWTF"&gt; &lt;img src="https://cdn.ko-fi.com/cdn/kofi3.png?v=2" width="150" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Warpgate is a smart &amp;amp; fully transparent SSH, HTTPS, MySQL and PostgreSQL bastion host that doesn't require a client app or an SSH wrapper.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Set it up in your DMZ, add user accounts and easily assign them to specific hosts and URLs within the network.&lt;/li&gt; 
 &lt;li&gt;Warpgate will record every session for you to view (live) and replay later through a built-in admin web UI.&lt;/li&gt; 
 &lt;li&gt;Not a jump host - forwards connection straight to the target in a way that's fully transparent to the client.&lt;/li&gt; 
 &lt;li&gt;Native 2FA and SSO support (TOTP &amp;amp; OpenID Connect)&lt;/li&gt; 
 &lt;li&gt;Single binary with no dependencies.&lt;/li&gt; 
 &lt;li&gt;Written in 100% safe Rust.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting started &amp;amp; downloads&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;See the &lt;a href="https://warpgate.null.page/getting-started/"&gt;Getting started&lt;/a&gt; docs page (or &lt;a href="https://warpgate.null.page/getting-started-on-docker/"&gt;Getting started on Docker&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/warp-tech/warpgate/releases"&gt;Release / beta binaries&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nightly.link/warp-tech/warpgate/workflows/build/main"&gt;Nightly builds&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;center&gt; 
 &lt;img width="783" alt="image" src="https://user-images.githubusercontent.com/161476/162640762-a91a2816-48c0-44d9-8b03-5b1e2cb42d51.png" /&gt; 
&lt;/center&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt; &lt;img src="https://github.com/user-attachments/assets/c9a6a372-198e-4f46-ab86-8c420dc24bca" /&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;img src="https://github.com/user-attachments/assets/a2166426-e865-4aba-9600-520954bcfe7f" /&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;img src="https://github.com/user-attachments/assets/366a5afb-aa86-4902-9080-eb2f40bf162c" /&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Reporting security issues&lt;/h2&gt; 
&lt;p&gt;Please use GitHub's &lt;a href="https://github.com/warp-tech/warpgate/security/policy"&gt;vulnerability reporting system&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Project Status&lt;/h2&gt; 
&lt;p&gt;The project is ready for production.&lt;/p&gt; 
&lt;h2&gt;How it works&lt;/h2&gt; 
&lt;p&gt;Warpgate is a service that you deploy on the bastion/DMZ host, which will accept SSH, HTTPS, MySQL and PostgreSQL connections and provide an (optional) web admin UI.&lt;/p&gt; 
&lt;p&gt;Run &lt;code&gt;warpgate setup&lt;/code&gt; to interactively generate a config file, including port bindings. See &lt;a href="https://warpgate.null.page/getting-started/"&gt;Getting started&lt;/a&gt; for details.&lt;/p&gt; 
&lt;p&gt;It receives connections with specifically formatted credentials, authenticates the user locally, connects to the target itself, and then connects both parties together while (optionally) recording the session.&lt;/p&gt; 
&lt;p&gt;When connecting through HTTPS, Warpgate presents a selection of available targets, and will then proxy all traffic in a session to the selected target. You can switch between targets at any time.&lt;/p&gt; 
&lt;p&gt;You manage the target and user lists and assign them to each other through the admin UI, and the session history is stored in an SQLite database (default: in &lt;code&gt;/var/lib/warpgate&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;You can also use the admin web interface to view the live session list, review session recordings, logs and more.&lt;/p&gt; 
&lt;h2&gt;Contributing / building from source&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;You'll need Rust, NodeJS and NPM&lt;/li&gt; 
 &lt;li&gt;Clone the repo&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/casey/just"&gt;Just&lt;/a&gt; is used to run tasks - install it: &lt;code&gt;cargo install just&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install the admin UI deps: &lt;code&gt;just npm&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Build the frontend: &lt;code&gt;just npm run build&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Build Warpgate: &lt;code&gt;cargo build&lt;/code&gt; (optionally &lt;code&gt;--release&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The binary is in &lt;code&gt;target/{debug|release}&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Tech stack&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Rust 🦀 
  &lt;ul&gt; 
   &lt;li&gt;HTTP: &lt;code&gt;poem-web&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Database: SQLite via &lt;code&gt;sea-orm&lt;/code&gt; + &lt;code&gt;sqlx&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;SSH: &lt;code&gt;russh&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Typescript 
  &lt;ul&gt; 
   &lt;li&gt;Svelte&lt;/li&gt; 
   &lt;li&gt;Bootstrap&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Backend API&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Warpgate admin and user facing APIs use autogenerated OpenAPI schemas and SDKs. To update the SDKs after changing the query/response structures, run &lt;code&gt;just openapi-all&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributors ✨&lt;/h2&gt; 
&lt;p&gt;Thanks goes to these wonderful people (&lt;a href="https://allcontributors.org/docs/en/emoji-key"&gt;emoji key&lt;/a&gt;):&lt;/p&gt; 
&lt;!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --&gt; 
&lt;!-- prettier-ignore-start --&gt; 
&lt;!-- markdownlint-disable --&gt; 
&lt;table&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/Eugeny"&gt;&lt;img src="https://avatars.githubusercontent.com/u/161476?v=4?s=100" width="100px;" alt="Eugeny" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Eugeny&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/Eugeny/warpgate/commits?author=Eugeny" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://the-empire.systems/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/18178614?v=4?s=100" width="100px;" alt="Spencer Heywood" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Spencer Heywood&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/Eugeny/warpgate/commits?author=heywoodlh" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/apiening"&gt;&lt;img src="https://avatars.githubusercontent.com/u/2064875?v=4?s=100" width="100px;" alt="Andreas Piening" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Andreas Piening&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/Eugeny/warpgate/commits?author=apiening" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/Gurkengewuerz"&gt;&lt;img src="https://avatars.githubusercontent.com/u/10966337?v=4?s=100" width="100px;" alt="Niklas" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Niklas&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/Eugeny/warpgate/commits?author=Gurkengewuerz" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/notnooblord"&gt;&lt;img src="https://avatars.githubusercontent.com/u/11678665?v=4?s=100" width="100px;" alt="Nooblord" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Nooblord&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/Eugeny/warpgate/commits?author=notnooblord" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://shea.nz/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/51303984?v=4?s=100" width="100px;" alt="Shea Smith" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Shea Smith&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/Eugeny/warpgate/commits?author=SheaSmith" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/samtoxie"&gt;&lt;img src="https://avatars.githubusercontent.com/u/7732658?v=4?s=100" width="100px;" alt="samtoxie" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;samtoxie&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/Eugeny/warpgate/commits?author=samtoxie" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!-- markdownlint-restore --&gt; 
&lt;!-- prettier-ignore-end --&gt; 
&lt;!-- ALL-CONTRIBUTORS-LIST:END --&gt; 
&lt;p&gt;This project follows the &lt;a href="https://github.com/all-contributors/all-contributors"&gt;all-contributors&lt;/a&gt; specification. Contributions of any kind welcome!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>nautechsystems/nautilus_trader</title>
      <link>https://github.com/nautechsystems/nautilus_trader</link>
      <description>&lt;p&gt;A high-performance algorithmic trading platform and event-driven backtester&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/nautilus-trader-logo.png" width="500" /&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://codecov.io/gh/nautechsystems/nautilus_trader"&gt;&lt;img src="https://codecov.io/gh/nautechsystems/nautilus_trader/branch/master/graph/badge.svg?token=DXO9QQI40H" alt="codecov" /&gt;&lt;/a&gt; &lt;a href="https://codspeed.io/nautechsystems/nautilus_trader"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://codspeed.io/badge.json" alt="codspeed" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/pyversions/nautilus_trader" alt="pythons" /&gt; &lt;img src="https://img.shields.io/pypi/v/nautilus_trader" alt="pypi-version" /&gt; &lt;img src="https://img.shields.io/pypi/format/nautilus_trader?color=blue" alt="pypi-format" /&gt; &lt;a href="https://pepy.tech/project/nautilus-trader"&gt;&lt;img src="https://pepy.tech/badge/nautilus-trader" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/NautilusTrader"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Branch&lt;/th&gt; 
   &lt;th align="left"&gt;Version&lt;/th&gt; 
   &lt;th align="left"&gt;Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;master&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://packages.nautechsystems.io/simple/nautilus-trader/index.html"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fmaster%2Fversion.json" alt="version" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml"&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=nightly" alt="build" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;nightly&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://packages.nautechsystems.io/simple/nautilus-trader/index.html"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fnightly%2Fversion.json" alt="version" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml"&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=nightly" alt="build" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;develop&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://packages.nautechsystems.io/simple/nautilus-trader/index.html"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fdevelop%2Fversion.json" alt="version" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml"&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=develop" alt="build" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Platform&lt;/th&gt; 
   &lt;th align="left"&gt;Rust&lt;/th&gt; 
   &lt;th align="left"&gt;Python&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Linux (x86_64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;1.89.0&lt;/td&gt; 
   &lt;td align="left"&gt;3.11-3.13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Linux (ARM64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;1.89.0&lt;/td&gt; 
   &lt;td align="left"&gt;3.11-3.13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;macOS (ARM64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;1.89.0&lt;/td&gt; 
   &lt;td align="left"&gt;3.11-3.13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Windows (x86_64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;1.89.0&lt;/td&gt; 
   &lt;td align="left"&gt;3.11-3.13&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Docs&lt;/strong&gt;: &lt;a href="https://nautilustrader.io/docs/"&gt;https://nautilustrader.io/docs/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Website&lt;/strong&gt;: &lt;a href="https://nautilustrader.io"&gt;https://nautilustrader.io&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Support&lt;/strong&gt;: &lt;a href="mailto:support@nautilustrader.io"&gt;support@nautilustrader.io&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;NautilusTrader is an open-source, high-performance, production-grade algorithmic trading platform, providing quantitative traders with the ability to backtest portfolios of automated trading strategies on historical data with an event-driven engine, and also deploy those same strategies live, with no code changes.&lt;/p&gt; 
&lt;p&gt;The platform is &lt;em&gt;AI-first&lt;/em&gt;, designed to develop and deploy algorithmic trading strategies within a highly performant and robust Python-native environment. This helps to address the parity challenge of keeping the Python research/backtest environment consistent with the production live trading environment.&lt;/p&gt; 
&lt;p&gt;NautilusTrader's design, architecture, and implementation philosophy prioritizes software correctness and safety at the highest level, with the aim of supporting Python-native, mission-critical, trading system backtesting and live deployment workloads.&lt;/p&gt; 
&lt;p&gt;The platform is also universal, and asset-class-agnostic — with any REST API or WebSocket feed able to be integrated via modular adapters. It supports high-frequency trading across a wide range of asset classes and instrument types including FX, Equities, Futures, Options, Crypto, DeFi, and Betting, enabling seamless operations across multiple venues simultaneously.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/nautilus-trader.png" alt="nautilus-trader" title="nautilus-trader" /&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Fast&lt;/strong&gt;: Core is written in Rust with asynchronous networking using &lt;a href="https://crates.io/crates/tokio"&gt;tokio&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reliable&lt;/strong&gt;: Rust-powered type- and thread-safety, with optional Redis-backed state persistence.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Portable&lt;/strong&gt;: OS independent, runs on Linux, macOS, and Windows. Deploy using Docker.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible&lt;/strong&gt;: Modular adapters mean any REST API or WebSocket feed can be integrated.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced&lt;/strong&gt;: Time in force &lt;code&gt;IOC&lt;/code&gt;, &lt;code&gt;FOK&lt;/code&gt;, &lt;code&gt;GTC&lt;/code&gt;, &lt;code&gt;GTD&lt;/code&gt;, &lt;code&gt;DAY&lt;/code&gt;, &lt;code&gt;AT_THE_OPEN&lt;/code&gt;, &lt;code&gt;AT_THE_CLOSE&lt;/code&gt;, advanced order types and conditional triggers. Execution instructions &lt;code&gt;post-only&lt;/code&gt;, &lt;code&gt;reduce-only&lt;/code&gt;, and icebergs. Contingency orders including &lt;code&gt;OCO&lt;/code&gt;, &lt;code&gt;OUO&lt;/code&gt;, &lt;code&gt;OTO&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Customizable&lt;/strong&gt;: Add user-defined custom components, or assemble entire systems from scratch leveraging the &lt;a href="https://nautilustrader.io/docs/latest/concepts/cache"&gt;cache&lt;/a&gt; and &lt;a href="https://nautilustrader.io/docs/latest/concepts/message_bus"&gt;message bus&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backtesting&lt;/strong&gt;: Run with multiple venues, instruments and strategies simultaneously using historical quote tick, trade tick, bar, order book and custom data with nanosecond resolution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Live&lt;/strong&gt;: Use identical strategy implementations between backtesting and live deployments.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-venue&lt;/strong&gt;: Multiple venue capabilities facilitate market-making and statistical arbitrage strategies.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AI Training&lt;/strong&gt;: Backtest engine fast enough to be used to train AI trading agents (RL/ES).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/nautilus-art.png" alt="Alt text" title="nautilus" /&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;nautilus - from ancient Greek 'sailor' and naus 'ship'.&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;The nautilus shell consists of modular chambers with a growth factor which approximates a logarithmic spiral. The idea is that this can be translated to the aesthetics of design and architecture.&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Why NautilusTrader?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Highly performant event-driven Python&lt;/strong&gt;: Native binary core components.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Parity between backtesting and live trading&lt;/strong&gt;: Identical strategy code.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reduced operational risk&lt;/strong&gt;: Enhanced risk management functionality, logical accuracy, and type safety.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Highly extendable&lt;/strong&gt;: Message bus, custom components and actors, custom data, custom adapters.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Traditionally, trading strategy research and backtesting might be conducted in Python using vectorized methods, with the strategy then needing to be reimplemented in a more event-driven way using C++, C#, Java or other statically typed language(s). The reasoning here is that vectorized backtesting code cannot express the granular time and event dependent complexity of real-time trading, where compiled languages have proven to be more suitable due to their inherently higher performance, and type safety.&lt;/p&gt; 
&lt;p&gt;One of the key advantages of NautilusTrader here, is that this reimplementation step is now circumvented - as the critical core components of the platform have all been written entirely in &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt; or &lt;a href="https://cython.org/"&gt;Cython&lt;/a&gt;. This means we're using the right tools for the job, where systems programming languages compile performant binaries, with CPython C extension modules then able to offer a Python-native environment, suitable for professional quantitative traders and trading firms.&lt;/p&gt; 
&lt;h2&gt;Why Python?&lt;/h2&gt; 
&lt;p&gt;Python was originally created decades ago as a simple scripting language with a clean straightforward syntax. It has since evolved into a fully fledged general purpose object-oriented programming language. Based on the TIOBE index, Python is currently the most popular programming language in the world. Not only that, Python has become the &lt;em&gt;de facto lingua franca&lt;/em&gt; of data science, machine learning, and artificial intelligence.&lt;/p&gt; 
&lt;p&gt;developer/user communities. However, Python has performance and typing limitations for large-scale, latency-sensitive systems. Cython addresses many of these issues by introducing static typing into Python's rich ecosystem of libraries and communities.&lt;/p&gt; 
&lt;h2&gt;Why Rust?&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt; is a multi-paradigm programming language designed for performance and safety, especially safe concurrency. Rust is "blazingly fast" and memory-efficient (comparable to C and C++) with no garbage collector. It can power mission-critical systems, run on embedded devices, and easily integrates with other languages.&lt;/p&gt; 
&lt;p&gt;Rust’s rich type system and ownership model guarantees memory-safety and thread-safety deterministically — eliminating many classes of bugs at compile-time.&lt;/p&gt; 
&lt;p&gt;The project increasingly utilizes Rust for core performance-critical components. Python bindings are implemented via Cython and &lt;a href="https://pyo3.rs"&gt;PyO3&lt;/a&gt;—no Rust toolchain is required at install time.&lt;/p&gt; 
&lt;p&gt;This project makes the &lt;a href="https://raphlinus.github.io/rust/2020/01/18/soundness-pledge.html"&gt;Soundness Pledge&lt;/a&gt;:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;“The intent of this project is to be free of soundness bugs. The developers will do their best to avoid them, and welcome help in analyzing and fixing them.”&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;MSRV:&lt;/strong&gt; NautilusTrader relies heavily on improvements in the Rust language and compiler. As a result, the Minimum Supported Rust Version (MSRV) is generally equal to the latest stable release of Rust.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Integrations&lt;/h2&gt; 
&lt;p&gt;NautilusTrader is modularly designed to work with &lt;em&gt;adapters&lt;/em&gt;, enabling connectivity to trading venues and data providers by translating their raw APIs into a unified interface and normalized domain model.&lt;/p&gt; 
&lt;p&gt;The following integrations are currently supported; see &lt;a href="https://nautilustrader.io/docs/latest/integrations/"&gt;docs/integrations/&lt;/a&gt; for details:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Name&lt;/th&gt; 
   &lt;th align="left"&gt;ID&lt;/th&gt; 
   &lt;th align="left"&gt;Type&lt;/th&gt; 
   &lt;th align="left"&gt;Status&lt;/th&gt; 
   &lt;th align="left"&gt;Docs&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://betfair.com"&gt;Betfair&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BETFAIR&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Sports Betting Exchange&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/betfair.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://binance.com"&gt;Binance&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BINANCE&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/binance.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://binance.us"&gt;Binance US&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BINANCE&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/binance.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.binance.com/en/futures"&gt;Binance Futures&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BINANCE&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/binance.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.bitmex.com"&gt;BitMEX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BITMEX&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/building-orange" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/bitmex.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.bybit.com"&gt;Bybit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BYBIT&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/bybit.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.coinbase.com/en/international-exchange"&gt;Coinbase International&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;COINBASE_INTX&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/coinbase_intx.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://databento.com"&gt;Databento&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;DATABENTO&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Data Provider&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/databento.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://dydx.exchange/"&gt;dYdX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;DYDX&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (DEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/dydx.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://hyperliquid.xyz"&gt;Hyperliquid&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;HYPERLIQUID&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (DEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/building-orange" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/hyperliquid.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.interactivebrokers.com"&gt;Interactive Brokers&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;INTERACTIVE_BROKERS&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Brokerage (multi-venue)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/ib.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://okx.com"&gt;OKX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;OKX&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/beta-yellow" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/okx.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://polymarket.com"&gt;Polymarket&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;POLYMARKET&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Prediction Market (DEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/polymarket.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://tardis.dev"&gt;Tardis&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;TARDIS&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Data Provider&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/tardis.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ID&lt;/strong&gt;: The default client ID for the integrations adapter clients.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Type&lt;/strong&gt;: The type of integration (often the venue type).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Status&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;building&lt;/code&gt;: Under construction and likely not in a usable state.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;beta&lt;/code&gt;: Completed to a minimally working state and in a beta testing phase.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;stable&lt;/code&gt;: Stabilized feature set and API, the integration has been tested by both developers and users to a reasonable level (some bugs may still remain).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the &lt;a href="https://nautilustrader.io/docs/latest/integrations/index.html"&gt;Integrations&lt;/a&gt; documentation for further details.&lt;/p&gt; 
&lt;h2&gt;Versioning and releases&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;NautilusTrader is still under active development&lt;/strong&gt;. Some features may be incomplete, and while the API is becoming more stable, breaking changes can occur between releases. We strive to document these changes in the release notes on a &lt;strong&gt;best-effort basis&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;We aim to follow a &lt;strong&gt;bi-weekly release schedule&lt;/strong&gt;, though experimental or larger features may cause delays.&lt;/p&gt; 
&lt;h3&gt;Branches&lt;/h3&gt; 
&lt;p&gt;We aim to maintain a stable, passing build across all branches.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;master&lt;/code&gt;: Reflects the source code for the latest released version; recommended for production use.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt;: Daily snapshots of the &lt;code&gt;develop&lt;/code&gt; branch for early testing; merged at &lt;strong&gt;14:00 UTC&lt;/strong&gt; or on demand.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;develop&lt;/code&gt;: Active development branch for contributors and feature work.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;Our &lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/ROADMAP.md"&gt;roadmap&lt;/a&gt; aims to achieve a &lt;strong&gt;stable API for version 2.x&lt;/strong&gt; (likely after the Rust port). Once this milestone is reached, we plan to implement a formal deprecation process for any API changes. This approach allows us to maintain a rapid development pace for now.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Precision mode&lt;/h2&gt; 
&lt;p&gt;NautilusTrader supports two precision modes for its core value types (&lt;code&gt;Price&lt;/code&gt;, &lt;code&gt;Quantity&lt;/code&gt;, &lt;code&gt;Money&lt;/code&gt;), which differ in their internal bit-width and maximum decimal precision.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;High-precision&lt;/strong&gt;: 128-bit integers with up to 16 decimals of precision, and a larger value range.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Standard-precision&lt;/strong&gt;: 64-bit integers with up to 9 decimals of precision, and a smaller value range.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;By default, the official Python wheels &lt;strong&gt;ship&lt;/strong&gt; in high-precision (128-bit) mode on Linux and macOS. On Windows, only standard-precision (64-bit) is available due to the lack of native 128-bit integer support. For the Rust crates, the default is standard-precision unless you explicitly enable the &lt;code&gt;high-precision&lt;/code&gt; feature flag.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;See the &lt;a href="https://nautilustrader.io/docs/latest/getting_started/installation"&gt;Installation Guide&lt;/a&gt; for further details.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Rust feature flag&lt;/strong&gt;: To enable high-precision mode in Rust, add the &lt;code&gt;high-precision&lt;/code&gt; feature to your Cargo.toml:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;[dependencies]
nautilus_model = { version = "*", features = ["high-precision"] }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;We recommend using the latest supported version of Python and installing &lt;a href="https://pypi.org/project/nautilus_trader/"&gt;nautilus_trader&lt;/a&gt; inside a virtual environment to isolate dependencies.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;There are two supported ways to install&lt;/strong&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Pre-built binary wheel from PyPI &lt;em&gt;or&lt;/em&gt; the Nautech Systems package index.&lt;/li&gt; 
 &lt;li&gt;Build from source.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;We highly recommend installing using the &lt;a href="https://docs.astral.sh/uv"&gt;uv&lt;/a&gt; package manager with a "vanilla" CPython.&lt;/p&gt; 
 &lt;p&gt;Conda and other Python distributions &lt;em&gt;may&lt;/em&gt; work but aren’t officially supported.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;From PyPI&lt;/h3&gt; 
&lt;p&gt;To install the latest binary wheel (or sdist package) from PyPI using Python's pip package manager:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U nautilus_trader
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;From the Nautech Systems package index&lt;/h3&gt; 
&lt;p&gt;The Nautech Systems package index (&lt;code&gt;packages.nautechsystems.io&lt;/code&gt;) complies with &lt;a href="https://peps.python.org/pep-0503/"&gt;PEP-503&lt;/a&gt; and hosts both stable and development binary wheels for &lt;code&gt;nautilus_trader&lt;/code&gt;. This enables users to install either the latest stable release or pre-release versions for testing.&lt;/p&gt; 
&lt;h4&gt;Stable wheels&lt;/h4&gt; 
&lt;p&gt;Stable wheels correspond to official releases of &lt;code&gt;nautilus_trader&lt;/code&gt; on PyPI, and use standard versioning.&lt;/p&gt; 
&lt;p&gt;To install the latest stable release:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U nautilus_trader --index-url=https://packages.nautechsystems.io/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Development wheels&lt;/h4&gt; 
&lt;p&gt;Development wheels are published from both the &lt;code&gt;nightly&lt;/code&gt; and &lt;code&gt;develop&lt;/code&gt; branches, allowing users to test features and fixes ahead of stable releases.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Wheels from the &lt;code&gt;develop&lt;/code&gt; branch are only built for the Linux x86_64 platform to save time and compute resources, while &lt;code&gt;nightly&lt;/code&gt; wheels support additional platforms as shown below.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Platform&lt;/th&gt; 
   &lt;th align="left"&gt;Nightly&lt;/th&gt; 
   &lt;th align="left"&gt;Develop&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Linux (x86_64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;✓&lt;/td&gt; 
   &lt;td align="left"&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Linux (ARM64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;✓&lt;/td&gt; 
   &lt;td align="left"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;macOS (ARM64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;✓&lt;/td&gt; 
   &lt;td align="left"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Windows (x86_64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;✓&lt;/td&gt; 
   &lt;td align="left"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;This process also helps preserve compute resources and ensures easy access to the exact binaries tested in CI pipelines, while adhering to &lt;a href="https://peps.python.org/pep-0440/"&gt;PEP-440&lt;/a&gt; versioning standards:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;develop&lt;/code&gt; wheels use the version format &lt;code&gt;dev{date}+{build_number}&lt;/code&gt; (e.g., &lt;code&gt;1.208.0.dev20241212+7001&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt; wheels use the version format &lt;code&gt;a{date}&lt;/code&gt; (alpha) (e.g., &lt;code&gt;1.208.0a20241212&lt;/code&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;/p&gt; 
 &lt;p&gt;We do not recommend using development wheels in production environments, such as live trading controlling real capital.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Installation commands&lt;/h4&gt; 
&lt;p&gt;By default, pip will install the latest stable release. Adding the &lt;code&gt;--pre&lt;/code&gt; flag ensures that pre-release versions, including development wheels, are considered.&lt;/p&gt; 
&lt;p&gt;To install the latest available pre-release (including development wheels):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U nautilus_trader --pre --index-url=https://packages.nautechsystems.io/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To install a specific development wheel (e.g., &lt;code&gt;1.208.0a20241212&lt;/code&gt; for December 12, 2024):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install nautilus_trader==1.208.0a20241212 --index-url=https://packages.nautechsystems.io/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Available versions&lt;/h4&gt; 
&lt;p&gt;You can view all available versions of &lt;code&gt;nautilus_trader&lt;/code&gt; on the &lt;a href="https://packages.nautechsystems.io/simple/nautilus-trader/index.html"&gt;package index&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To programmatically fetch and list available versions:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -s https://packages.nautechsystems.io/simple/nautilus-trader/index.html | grep -oP '(?&amp;lt;=&amp;lt;a href=")[^"]+(?=")' | awk -F'#' '{print $1}' | sort
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Branch updates&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;develop&lt;/code&gt; branch wheels (&lt;code&gt;.dev&lt;/code&gt;): Build and publish continuously with every merged commit.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt; branch wheels (&lt;code&gt;a&lt;/code&gt;): Build and publish daily when we automatically merge the &lt;code&gt;develop&lt;/code&gt; branch at &lt;strong&gt;14:00 UTC&lt;/strong&gt; (if there are changes).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Retention policies&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;develop&lt;/code&gt; branch wheels (&lt;code&gt;.dev&lt;/code&gt;): We retain only the most recent wheel build.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt; branch wheels (&lt;code&gt;a&lt;/code&gt;): We retain only the 30 most recent wheel builds.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;From Source&lt;/h3&gt; 
&lt;p&gt;It's possible to install from source using pip if you first install the build dependencies as specified in the &lt;code&gt;pyproject.toml&lt;/code&gt;.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;a href="https://rustup.rs/"&gt;rustup&lt;/a&gt; (the Rust toolchain installer):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Linux and macOS:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;curl https://sh.rustup.rs -sSf | sh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Windows:&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Download and install &lt;a href="https://win.rustup.rs/x86_64"&gt;&lt;code&gt;rustup-init.exe&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;Install "Desktop development with C++" with &lt;a href="https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=BuildTools&amp;amp;rel=16"&gt;Build Tools for Visual Studio 2019&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Verify (any system): from a terminal session run: &lt;code&gt;rustc --version&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Enable &lt;code&gt;cargo&lt;/code&gt; in the current shell:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Linux and macOS:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;source $HOME/.cargo/env
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Windows:&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Start a new PowerShell&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;a href="https://clang.llvm.org/"&gt;clang&lt;/a&gt; (a C language frontend for LLVM):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Linux:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get install clang
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Windows:&lt;/p&gt; 
    &lt;ol&gt; 
     &lt;li&gt; &lt;p&gt;Add Clang to your &lt;a href="https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=BuildTools&amp;amp;rel=16"&gt;Build Tools for Visual Studio 2019&lt;/a&gt;:&lt;/p&gt; 
      &lt;ul&gt; 
       &lt;li&gt;Start | Visual Studio Installer | Modify | C++ Clang tools for Windows (12.0.0 - x64…) = checked | Modify&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;Enable &lt;code&gt;clang&lt;/code&gt; in the current shell:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-powershell"&gt;[System.Environment]::SetEnvironmentVariable('path', "C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\Llvm\x64\bin\;" + $env:Path,"User")
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
    &lt;/ol&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Verify (any system): from a terminal session run: &lt;code&gt;clang --version&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install uv (see the &lt;a href="https://docs.astral.sh/uv/getting-started/installation"&gt;uv installation guide&lt;/a&gt; for more details):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the source with &lt;code&gt;git&lt;/code&gt;, and install from the project's root directory:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone --branch develop --depth 1 https://github.com/nautechsystems/nautilus_trader
cd nautilus_trader
uv sync --all-extras
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;The &lt;code&gt;--depth 1&lt;/code&gt; flag fetches just the latest commit for a faster, lightweight clone.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol start="6"&gt; 
 &lt;li&gt; &lt;p&gt;Set environment variables for PyO3 compilation (Linux and macOS only):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Set the library path for the Python interpreter (in this case Python 3.13.4)
export LD_LIBRARY_PATH="$HOME/.local/share/uv/python/cpython-3.13.4-linux-x86_64-gnu/lib:$LD_LIBRARY_PATH"

# Set the Python executable path for PyO3
export PYO3_PYTHON=$(pwd)/.venv/bin/python
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;Adjust the Python version and architecture in the &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; to match your system. Use &lt;code&gt;uv python list&lt;/code&gt; to find the exact path for your Python installation.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;See the &lt;a href="https://nautilustrader.io/docs/latest/getting_started/installation"&gt;Installation Guide&lt;/a&gt; for other options and further details.&lt;/p&gt; 
&lt;h2&gt;Redis&lt;/h2&gt; 
&lt;p&gt;Using &lt;a href="https://redis.io"&gt;Redis&lt;/a&gt; with NautilusTrader is &lt;strong&gt;optional&lt;/strong&gt; and only required if configured as the backend for a &lt;a href="https://nautilustrader.io/docs/latest/concepts/cache"&gt;cache&lt;/a&gt; database or &lt;a href="https://nautilustrader.io/docs/latest/concepts/message_bus"&gt;message bus&lt;/a&gt;. See the &lt;strong&gt;Redis&lt;/strong&gt; section of the &lt;a href="https://nautilustrader.io/docs/latest/getting_started/installation#redis"&gt;Installation Guide&lt;/a&gt; for further details.&lt;/p&gt; 
&lt;h2&gt;Makefile&lt;/h2&gt; 
&lt;p&gt;A &lt;code&gt;Makefile&lt;/code&gt; is provided to automate most installation and build tasks for development. Some of the targets include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;make install&lt;/code&gt;: Installs in &lt;code&gt;release&lt;/code&gt; build mode with all dependency groups and extras.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make install-debug&lt;/code&gt;: Same as &lt;code&gt;make install&lt;/code&gt; but with &lt;code&gt;debug&lt;/code&gt; build mode.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make install-just-deps&lt;/code&gt;: Installs just the &lt;code&gt;main&lt;/code&gt;, &lt;code&gt;dev&lt;/code&gt; and &lt;code&gt;test&lt;/code&gt; dependencies (does not install package).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make build&lt;/code&gt;: Runs the build script in &lt;code&gt;release&lt;/code&gt; build mode (default).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make build-debug&lt;/code&gt;: Runs the build script in &lt;code&gt;debug&lt;/code&gt; build mode.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make build-wheel&lt;/code&gt;: Runs uv build with a wheel format in &lt;code&gt;release&lt;/code&gt; mode.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make build-wheel-debug&lt;/code&gt;: Runs uv build with a wheel format in &lt;code&gt;debug&lt;/code&gt; mode.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make cargo-test&lt;/code&gt;: Runs all Rust crate tests using &lt;code&gt;cargo-nextest&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make clean&lt;/code&gt;: Deletes all build results, such as &lt;code&gt;.so&lt;/code&gt; or &lt;code&gt;.dll&lt;/code&gt; files.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make distclean&lt;/code&gt;: &lt;strong&gt;CAUTION&lt;/strong&gt; Removes all artifacts not in the git index from the repository. This includes source files which have not been &lt;code&gt;git add&lt;/code&gt;ed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make docs&lt;/code&gt;: Builds the documentation HTML using Sphinx.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make pre-commit&lt;/code&gt;: Runs the pre-commit checks over all files.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make ruff&lt;/code&gt;: Runs ruff over all files using the &lt;code&gt;pyproject.toml&lt;/code&gt; config (with autofix).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make pytest&lt;/code&gt;: Runs all tests with &lt;code&gt;pytest&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make test-performance&lt;/code&gt;: Runs performance tests with &lt;a href="https://codspeed.io"&gt;codspeed&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;Run &lt;code&gt;make help&lt;/code&gt; for documentation on all available make targets.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;See the &lt;a href="https://github.com/nautechsystems/nautilus_trader/raw/develop/crates/infrastructure/TESTS.md"&gt;crates/infrastructure/TESTS.md&lt;/a&gt; file for running the infrastructure integration tests.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;Indicators and strategies can be developed in both Python and Cython. For performance and latency-sensitive applications, we recommend using Cython. Below are some examples:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/nautilus_trader/examples/indicators/ema_python.py"&gt;indicator&lt;/a&gt; example written in Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/nautilus_trader/indicators/"&gt;indicator&lt;/a&gt; examples written in Cython.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/nautilus_trader/examples/strategies/"&gt;strategy&lt;/a&gt; examples written in Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/examples/backtest/"&gt;backtest&lt;/a&gt; examples using a &lt;code&gt;BacktestEngine&lt;/code&gt; directly.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Docker&lt;/h2&gt; 
&lt;p&gt;Docker containers are built using the base image &lt;code&gt;python:3.12-slim&lt;/code&gt; with the following variant tags:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;nautilus_trader:latest&lt;/code&gt; has the latest release version installed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nautilus_trader:nightly&lt;/code&gt; has the head of the &lt;code&gt;nightly&lt;/code&gt; branch installed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jupyterlab:latest&lt;/code&gt; has the latest release version installed along with &lt;code&gt;jupyterlab&lt;/code&gt; and an example backtest notebook with accompanying data.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jupyterlab:nightly&lt;/code&gt; has the head of the &lt;code&gt;nightly&lt;/code&gt; branch installed along with &lt;code&gt;jupyterlab&lt;/code&gt; and an example backtest notebook with accompanying data.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can pull the container images as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker pull ghcr.io/nautechsystems/&amp;lt;image_variant_tag&amp;gt; --platform linux/amd64
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can launch the backtest example container by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker pull ghcr.io/nautechsystems/jupyterlab:nightly --platform linux/amd64
docker run -p 8888:8888 ghcr.io/nautechsystems/jupyterlab:nightly
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then open your browser at the following address:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;http://127.0.0.1:8888/lab
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;/p&gt; 
 &lt;p&gt;NautilusTrader currently exceeds the rate limit for Jupyter notebook logging (stdout output). Therefore, we set the &lt;code&gt;log_level&lt;/code&gt; to &lt;code&gt;ERROR&lt;/code&gt; in the examples. Lowering this level to see more logging will cause the notebook to hang during cell execution. We are investigating a fix that may involve either raising the configured rate limits for Jupyter or throttling the log flushing from Nautilus.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/jupyterlab/jupyterlab/issues/12845"&gt;https://github.com/jupyterlab/jupyterlab/issues/12845&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/deshaw/jupyterlab-limit-output"&gt;https://github.com/deshaw/jupyterlab-limit-output&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;We aim to provide the most pleasant developer experience possible for this hybrid codebase of Python, Cython and Rust. See the &lt;a href="https://nautilustrader.io/docs/latest/developer_guide/index.html"&gt;Developer Guide&lt;/a&gt; for helpful information.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;Run &lt;code&gt;make build-debug&lt;/code&gt; to compile after changes to Rust or Cython code for the most efficient development workflow.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Testing with Rust&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://nexte.st"&gt;cargo-nextest&lt;/a&gt; is the standard Rust test runner for NautilusTrader. Its key benefit is isolating each test in its own process, ensuring test reliability by avoiding interference.&lt;/p&gt; 
&lt;p&gt;You can install cargo-nextest by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cargo install cargo-nextest
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;Run Rust tests with &lt;code&gt;make cargo-test&lt;/code&gt;, which uses &lt;strong&gt;cargo-nextest&lt;/strong&gt; with an efficient profile.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Thank you for considering contributing to NautilusTrader! We welcome any and all help to improve the project. If you have an idea for an enhancement or a bug fix, the first step is to open an &lt;a href="https://github.com/nautechsystems/nautilus_trader/issues"&gt;issue&lt;/a&gt; on GitHub to discuss it with the team. This helps to ensure that your contribution will be well-aligned with the goals of the project and avoids duplication of effort.&lt;/p&gt; 
&lt;p&gt;Before getting started, be sure to review the &lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/ROADMAP.md#open-source-scope"&gt;open-source scope&lt;/a&gt; outlined in the project’s roadmap to understand what’s in and out of scope.&lt;/p&gt; 
&lt;p&gt;Once you're ready to start working on your contribution, make sure to follow the guidelines outlined in the &lt;a href="https://github.com/nautechsystems/nautilus_trader/raw/develop/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; file. This includes signing a Contributor License Agreement (CLA) to ensure that your contributions can be included in the project.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;Pull requests should target the &lt;code&gt;develop&lt;/code&gt; branch (the default branch). This is where new features and improvements are integrated before release.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Thank you again for your interest in NautilusTrader! We look forward to reviewing your contributions and working with you to improve the project.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Join our community of users and contributors on &lt;a href="https://discord.gg/NautilusTrader"&gt;Discord&lt;/a&gt; to chat and stay up-to-date with the latest announcements and features of NautilusTrader. Whether you're a developer looking to contribute or just want to learn more about the platform, all are welcome on our Discord server.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;/p&gt; 
 &lt;p&gt;NautilusTrader does not issue, promote, or endorse any cryptocurrency tokens. Any claims or communications suggesting otherwise are unauthorized and false.&lt;/p&gt; 
 &lt;p&gt;All official updates and communications from NautilusTrader will be shared exclusively through &lt;a href="https://nautilustrader.io"&gt;https://nautilustrader.io&lt;/a&gt;, our &lt;a href="https://discord.gg/NautilusTrader"&gt;Discord server&lt;/a&gt;, or our X (Twitter) account: &lt;a href="https://x.com/NautilusTrader"&gt;@NautilusTrader&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;If you encounter any suspicious activity, please report it to the appropriate platform and contact us at &lt;a href="mailto:info@nautechsystems.io"&gt;info@nautechsystems.io&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The source code for NautilusTrader is available on GitHub under the &lt;a href="https://www.gnu.org/licenses/lgpl-3.0.en.html"&gt;GNU Lesser General Public License v3.0&lt;/a&gt;. Contributions to the project are welcome and require the completion of a standard &lt;a href="https://github.com/nautechsystems/nautilus_trader/raw/develop/CLA.md"&gt;Contributor License Agreement (CLA)&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;NautilusTrader™ is developed and maintained by Nautech Systems, a technology company specializing in the development of high-performance trading systems. For more information, visit &lt;a href="https://nautilustrader.io"&gt;https://nautilustrader.io&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;© 2015-2025 Nautech Systems Pty Ltd. All rights reserved.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/ns-logo.png" alt="nautechsystems" title="nautechsystems" /&gt; &lt;img src="https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/ferris.png" width="128" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>openobserve/openobserve</title>
      <link>https://github.com/openobserve/openobserve</link>
      <description>&lt;p&gt;🚀 10x easier, 🚀 140x lower storage cost, 🚀 high performance, 🚀 petabyte scale - Elasticsearch/Splunk/Datadog alternative for 🚀 (logs, metrics, traces, RUM, Error tracking, Session replay).&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://openobserve.ai"&gt;&lt;img src="https://openobserve.ai/img/logo/o2-logo-readme.svg?sanitize=true" alt="OpenObserve" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;em&gt;🚀 10x easier, 🚀 140x lower storage cost, 🚀 high performance, 🚀 petabyte scale - Elasticsearch/Splunk/Datadog alternative for 🚀 (logs, metrics, traces).&lt;/em&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/openobserve/openobserve" target="_blank"&gt; &lt;img src="https://img.shields.io/github/last-commit/openobserve/openobserve" alt="Last Commit" /&gt; &lt;/a&gt; &lt;a href="https://github.com/openobserve/openobserve/stargazers" target="_blank"&gt; &lt;img src="https://img.shields.io/github/stars/openobserve/openobserve" alt="GitHub Stars" /&gt; &lt;/a&gt; &lt;a href="https://github.com/openobserve/openobserve/issues" target="_blank"&gt; &lt;img src="https://img.shields.io/github/issues/openobserve/openobserve" alt="GitHub Issues" /&gt; &lt;/a&gt; &lt;a href="https://github.com/openobserve/openobserve/graphs/contributors" target="_blank"&gt; &lt;img src="https://img.shields.io/github/contributors/openobserve/openobserve" alt="Contributors" /&gt; &lt;/a&gt; &lt;a href="https://github.com/openobserve/openobserve/releases" target="_blank"&gt; &lt;img src="https://img.shields.io/github/v/release/openobserve/openobserve" alt="GitHub Release" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;OpenObserve (O2 for short) is a cloud-native observability platform built specifically for logs, metrics, traces, analytics, RUM (Real User Monitoring - Performance, Errors, Session Replay) designed to work at petabyte scale.&lt;/p&gt; 
&lt;p&gt;It is straightforward and easy to operate, in contrast to Elasticsearch, which requires understanding and tuning numerous settings. Get OpenObserve up and running in under 2 minutes.&lt;/p&gt; 
&lt;p&gt;OpenObserve serves as a seamless replacement for Elasticsearch for users who ingest data using APIs and perform searches. OpenObserve comes with its own user interface, eliminating the need for separate installation.&lt;/p&gt; 
&lt;p&gt;You can reduce your log storage costs by ~140x compared to Elasticsearch by using OpenObserve. Below, we present the results from pushing logs from our production Kubernetes cluster to both Elasticsearch and OpenObserve using Fluent Bit.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/openobserve/openobserve/main/screenshots/zo_vs_es.png" alt="OpenObserve Vs Elasticsearch" /&gt;&lt;/p&gt; 
&lt;h2&gt;🎥 Introduction Video&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=4VwuC1tpRP4"&gt;&lt;img src="https://raw.githubusercontent.com/openobserve/openobserve/main/screenshots/o2_intro.webp" alt="OpenObserve Introduction" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;🌟 Features:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Logs, Metrics, Traces&lt;/strong&gt;: Comprehensive support for various data types.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;OpenTelemetry Support&lt;/strong&gt;: Full compatibility with OTLP for logs, metrics, and traces.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real User Monitoring (RUM)&lt;/strong&gt;: Includes performance tracking, error logging, and session replay.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dashboards, Reports, Alerts&lt;/strong&gt;: Features over 18 different chart types for comprehensive data visualization for on-the-fly analysis and reporting along with alerting.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Pipelines&lt;/strong&gt;: Enrich, redact, reduce, normalize data on the fly. Stream processing for logs to metrics and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced Embedded GUI&lt;/strong&gt;: Intuitive and user-friendly interface.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SQL and PromQL Support&lt;/strong&gt;: Query logs and traces with SQL, and metrics with SQL and PromQL.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Single Binary or HA Installation&lt;/strong&gt;: Install using a single binary for small deployments or in HA mode for large deployments.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Versatile Storage Options&lt;/strong&gt;: Supports local disk, S3, MinIO, GCS, Azure Blob Storage.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High Availability and Clustering&lt;/strong&gt;: Ensures reliable and scalable performance.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dynamic Schema&lt;/strong&gt;: Adapts to your data structure seamlessly.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Built-in Authentication&lt;/strong&gt;: Secure and ready to use.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ease of Operation&lt;/strong&gt;: Designed for simplicity and efficiency.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Seamless Upgrades&lt;/strong&gt;: Hassle-free updates.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multilingual UI&lt;/strong&gt;: Supports 11 languages, including English, Spanish, German, French, Chinese, and more.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For a full list of features, check the &lt;a href="https://openobserve.ai/docs/#project-status-features-and-roadmap"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;⚡️ Quick start&lt;/h2&gt; 
&lt;h3&gt;🐳 Docker:&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d \
      --name openobserve \
      -v $PWD/data:/data \
      -p 5080:5080 \
      -e ZO_ROOT_USER_EMAIL="root@example.com" \
      -e ZO_ROOT_USER_PASSWORD="Complexpass#123" \
      public.ecr.aws/zinclabs/openobserve:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;🐙 Docker Compose:&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;services:
  openobserve:
    image: public.ecr.aws/zinclabs/openobserve:latest
    restart: unless-stopped
    environment:
      ZO_ROOT_USER_EMAIL: "root@example.com"
      ZO_ROOT_USER_PASSWORD: "Complexpass#123"
    ports:
      - "5080:5080"
    volumes:
      - data:/data
volumes:
  data:
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For other ways to quickly install OpenObserve or use OpenObserve cloud, check &lt;a href="https://openobserve.ai/docs/quickstart"&gt;quickstart documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For installing OpenObserve in HA mode, check &lt;a href="https://openobserve.ai/docs/ha_deployment/"&gt;HA deployment documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- ## Enterprise Vs Open source Vs Cloud edition

OpenObserve is available in three different editions:


| Feature | Open Source (Self hosted) | Enterprise (Self hosted) | Cloud |
| --- | --- | --- | --- | 
| Logs | ✅ | ✅ | ✅ |
| Metrics | ✅ | ✅ | ✅ |
| Traces | ✅ | ✅ | ✅ |
| RUM | ✅ | ✅ | ✅ |
| Alerts | ✅ | ✅ | ✅ |
| Dashboards | ✅ | ✅ | ✅ |
| Reports | ✅ | ✅ | ✅ |
| VRL functions | ✅ | ✅ | ✅ |
| Pipelines | ✅ | ✅ | ✅ |
| High Availability | ✅ | ✅ | ✅ |
| Multitenancy (Organizations) | ✅ | ✅ | ✅ |
| Dynamic schema and schema evolution | ✅ | ✅ | ✅ |
| Advanced multilingual GUI | ✅ | ✅ | ✅ |
| Single Sign On | ❌ | ✅ | ✅ |
| Role Based Access Control (RBAC) | ❌ | ✅ | ✅ |
| Federated search / Super cluster | ❌ | ✅ | ❌ |
| Query management | ❌ | ✅ | ❌ |
| Workload management (QoS) | ❌ | ✅ | ❌ |
| Audit trail | ❌ | ✅ | ❌ |
| Ability to influence roadmap | ❌ | ✅ | ✅ on enterprise plan |
| License | AGPL | Enterprise | Cloud |
| Support | Community | Enterprise | Cloud |
| Cost | Free | If self hosted, free for up to 200 GB/Day data ingested &lt;br&gt; Paid thereafter  | Free 200 GB/Month data ingested &lt;br&gt; Paid thereafter | --&gt; 
&lt;h2&gt;📷 Screenshots&lt;/h2&gt; 
&lt;h3&gt;Home&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/openobserve/openobserve/main/screenshots/zo_home.png" alt="Home" /&gt;&lt;/p&gt; 
&lt;h3&gt;Logs&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/openobserve/openobserve/main/screenshots/logs.png" alt="Logs" /&gt;&lt;/p&gt; 
&lt;h3&gt;Traces (OpenTelemetry)&lt;/h3&gt; 
&lt;p&gt;Trace details page &lt;img src="https://raw.githubusercontent.com/openobserve/openobserve/main/screenshots/traces.png" alt="Traces using OpenTelemetry" /&gt;&lt;/p&gt; 
&lt;p&gt;Golden metrics based on traces &lt;img src="https://raw.githubusercontent.com/openobserve/openobserve/main/screenshots/traces-overall.png" alt="Traces golden metrics" /&gt;&lt;/p&gt; 
&lt;h3&gt;Visualizations and Dashboards&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/openobserve/openobserve/main/screenshots/dashboard.png" alt="Dashboard" /&gt; &lt;img src="https://raw.githubusercontent.com/openobserve/openobserve/main/screenshots/dashboard2.png" alt="Dashboard" /&gt; &lt;img src="https://raw.githubusercontent.com/openobserve/openobserve/main/screenshots/create-panel.png" alt="Create panel" /&gt; &lt;img src="https://raw.githubusercontent.com/openobserve/openobserve/main/screenshots/map.png" alt="Map" /&gt;&lt;/p&gt; 
&lt;h3&gt;Front end monitoring&lt;/h3&gt; 
&lt;p&gt;Performance analytics &lt;img src="https://raw.githubusercontent.com/openobserve/openobserve/main/screenshots/performance.png" alt="Performance" /&gt;&lt;/p&gt; 
&lt;p&gt;Session replay &lt;img src="https://raw.githubusercontent.com/openobserve/openobserve/main/screenshots/session-replay.png" alt="Session replay" /&gt;&lt;/p&gt; 
&lt;p&gt;Error tracking &lt;img src="https://raw.githubusercontent.com/openobserve/openobserve/main/screenshots/error-tracking.png" alt="Error tracking" /&gt;&lt;/p&gt; 
&lt;h3&gt;Alerts&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/openobserve/openobserve/main/screenshots/alerts.png" alt="Alerts" /&gt;&lt;/p&gt; 
&lt;h3&gt;Streams&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/openobserve/openobserve/main/screenshots/streams.png" alt="Streams" /&gt;&lt;/p&gt; 
&lt;h3&gt;Ingestion&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/openobserve/openobserve/main/screenshots/ingestion1.png" alt="Ingestion" /&gt;&lt;/p&gt; 
&lt;h3&gt;Pipeline&lt;/h3&gt; 
&lt;p&gt;Pipeline &lt;img src="https://raw.githubusercontent.com/openobserve/openobserve/main/screenshots/pipeline.png" alt="Pipeline" /&gt;&lt;/p&gt; 
&lt;p&gt;Function &lt;img src="https://raw.githubusercontent.com/openobserve/openobserve/main/screenshots/function.png" alt="Function" /&gt;&lt;/p&gt; 
&lt;h3&gt;IAM&lt;/h3&gt; 
&lt;p&gt;SSO (Single Sign On) &lt;img src="https://raw.githubusercontent.com/openobserve/openobserve/main/screenshots/sso.png" alt="SSO" /&gt;&lt;/p&gt; 
&lt;p&gt;RBAC (Role Based Access Control) &lt;img src="https://raw.githubusercontent.com/openobserve/openobserve/main/screenshots/iam_rbac.png" alt="RBAC" /&gt;&lt;/p&gt; 
&lt;h3&gt;SBOM&lt;/h3&gt; 
&lt;p&gt;Software Bill of Materials for OpenObserve&lt;/p&gt; 
&lt;h4&gt;Rust&lt;/h4&gt; 
&lt;p&gt;SBOM can be found &lt;a href="https://raw.githubusercontent.com/openobserve/openobserve/main/openobserve.cdx.xml"&gt;here&lt;/a&gt;. You can analyze it using &lt;a href="https://dependencytrack.org/"&gt;dependency track&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In order to generate the SBOM, you can use the following commands:&lt;/p&gt; 
&lt;p&gt;Install cargo-cyclonedx:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cargo install cargo-cyclonedx
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Generate the SBOM:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cargo-cyclonedx cyclonedx
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;JavaScript&lt;/h4&gt; 
&lt;p&gt;SBOM can be found &lt;a href="https://raw.githubusercontent.com/openobserve/openobserve/main/web/sbom.json"&gt;here&lt;/a&gt;. You can analyze it using &lt;a href="https://dependencytrack.org/"&gt;dependency track&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In order to generate the SBOM, you can use the following commands:&lt;/p&gt; 
&lt;p&gt;Install cyclonedx-npm:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm install --global @cyclonedx/cyclonedx-npm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Generate the SBOM:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd web
cyclonedx-npm &amp;gt; sbom.json         
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;⚖️ License&lt;/h2&gt; 
&lt;p&gt;OpenObserve is licensed under the AGPL-3.0 license. For more details, see the &lt;a href="https://github.com/openobserve/openobserve/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;🌍 Community&lt;/h2&gt; 
&lt;h3&gt;🔗 Join OpenObserve community on Slack&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://short.openobserve.ai/community"&gt;&lt;img src="https://raw.githubusercontent.com/openobserve/openobserve/main/screenshots/slack.png" alt="Slack" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Easiest way to get support is to join the &lt;a href="https://short.openobserve.ai/community"&gt;Slack channel&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;📱 Join OpenObserve community on WeChat&lt;/h3&gt; 
&lt;img src="https://raw.githubusercontent.com/openobserve/openobserve/main/screenshots/wechat_qr.jpg" width="300" /&gt;</description>
    </item>
    
    <item>
      <title>mozilla/sccache</title>
      <link>https://github.com/mozilla/sccache</link>
      <description>&lt;p&gt;Sccache is a ccache-like tool. It is used as a compiler wrapper and avoids compilation when possible. Sccache has the capability to utilize caching in remote storage environments, including various cloud storage options, or alternatively, in local storage.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://github.com/mozilla/sccache/actions?query=workflow%3Aci"&gt;&lt;img src="https://github.com/mozilla/sccache/workflows/ci/badge.svg?sanitize=true" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://crates.io/crates/sccache"&gt;&lt;img src="https://img.shields.io/crates/v/sccache.svg?sanitize=true" alt="Crates.io" /&gt;&lt;/a&gt; &lt;a href="https://chat.mozilla.org/#/room/#sccache:mozilla.org"&gt;&lt;img src="https://img.shields.io/matrix/sccache:mozilla.org" alt="Matrix" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/crates/l/sccache" alt="Crates.io" /&gt; &lt;a href="https://deps.rs/repo/github/mozilla/sccache"&gt;&lt;img src="https://deps.rs/repo/github/mozilla/sccache/status.svg?sanitize=true" alt="dependency status" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://codecov.io/gh/mozilla/sccache"&gt;&lt;img src="https://codecov.io/gh/mozilla/sccache/branch/main/graph/badge.svg?sanitize=true" alt="CodeCov" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;sccache - Shared Compilation Cache&lt;/h1&gt; 
&lt;p&gt;sccache is a &lt;a href="https://ccache.dev/"&gt;ccache&lt;/a&gt;-like compiler caching tool. It is used as a compiler wrapper and avoids compilation when possible, storing cached results either on &lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/docs/Local.md"&gt;local disk&lt;/a&gt; or in one of &lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/#storage-options"&gt;several cloud storage backends&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;sccache includes support for caching the compilation of C/C++ code, &lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/docs/Rust.md"&gt;Rust&lt;/a&gt;, as well as NVIDIA's CUDA using &lt;a href="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html"&gt;nvcc&lt;/a&gt;, and &lt;a href="https://llvm.org/docs/CompileCudaWithLLVM.html"&gt;clang&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;sccache also provides &lt;a href="https://github.com/icecc/icecream"&gt;icecream&lt;/a&gt;-style distributed compilation (automatic packaging of local toolchains) for all supported compilers (including Rust). The distributed compilation system includes several security features that icecream lacks such as authentication, transport layer encryption, and sandboxed compiler execution on build servers. See &lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/docs/DistributedQuickstart.md"&gt;the distributed quickstart&lt;/a&gt; guide for more information.&lt;/p&gt; 
&lt;p&gt;sccache is also available as a &lt;a href="https://github.com/marketplace/actions/sccache-action"&gt;GitHub Actions&lt;/a&gt; to facilitate the deployment using GitHub Actions cache.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;Table of Contents (ToC)&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/#usage"&gt;Usage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/#build-requirements"&gt;Build Requirements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/#build"&gt;Build&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/#separating-caches-between-invocations"&gt;Separating caches between invocations&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/#overwriting-the-cache"&gt;Overwriting the cache&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/#debugging"&gt;Debugging&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/#interaction-with-gnu-make-jobserver"&gt;Interaction with GNU &lt;code&gt;make&lt;/code&gt; jobserver&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/#known-caveats"&gt;Known Caveats&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/#storage-options"&gt;Storage Options&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/docs/Local.md"&gt;Local&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/docs/S3.md"&gt;S3&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/docs/S3.md#R2"&gt;R2&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/docs/Redis.md"&gt;Redis&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/docs/Memcached.md"&gt;Memcached&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/docs/Gcs.md"&gt;Google Cloud Storage&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/docs/Azure.md"&gt;Azure&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/docs/GHA.md"&gt;GitHub Actions&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/docs/Webdav.md"&gt;WebDAV (Ccache/Bazel/Gradle compatible)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/docs/OSS.md"&gt;Alibaba OSS&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;There are prebuilt x86-64 binaries available for Windows, Linux (a portable binary compiled against musl), and macOS &lt;a href="https://github.com/mozilla/sccache/releases/latest"&gt;on the releases page&lt;/a&gt;. Several package managers also include sccache packages, you can install the latest release from source using cargo, or build directly from a source checkout.&lt;/p&gt; 
&lt;h3&gt;macOS&lt;/h3&gt; 
&lt;p&gt;On macOS sccache can be installed via &lt;a href="https://brew.sh/"&gt;Homebrew&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew install sccache
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or via &lt;a href="https://www.macports.org/"&gt;MacPorts&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo port install sccache
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;p&gt;On Windows, sccache can be installed via &lt;a href="https://scoop.sh/"&gt;scoop&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;scoop install sccache
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Via cargo&lt;/h3&gt; 
&lt;p&gt;If you have a Rust toolchain installed you can install sccache using cargo. &lt;strong&gt;Note that this will compile sccache from source which is fairly resource-intensive. For CI purposes you should use prebuilt binary packages.&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cargo install sccache --locked
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;Running sccache is like running ccache: prefix your compilation commands with it, like so:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sccache gcc -o foo.o -c foo.c
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to use sccache for caching Rust builds you can define &lt;code&gt;build.rustc-wrapper&lt;/code&gt; in the &lt;a href="https://doc.rust-lang.org/cargo/reference/config.html"&gt;cargo configuration file&lt;/a&gt;. For example, you can set it globally in &lt;code&gt;$HOME/.cargo/config.toml&lt;/code&gt; by adding:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;[build]
rustc-wrapper = "/path/to/sccache"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that you need to use cargo 1.40 or newer for this to work.&lt;/p&gt; 
&lt;p&gt;Alternatively you can use the environment variable &lt;code&gt;RUSTC_WRAPPER&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export RUSTC_WRAPPER=/path/to/sccache
cargo build
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;sccache supports gcc, clang, MSVC, rustc, &lt;a href="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html"&gt;NVCC&lt;/a&gt;, &lt;a href="https://docs.nvidia.com/hpc-sdk//compilers/hpc-compilers-user-guide/index.html"&gt;NVC++&lt;/a&gt;, and &lt;a href="https://www.windriver.com/products/development-tools/#diab_compiler"&gt;Wind River's diab compiler&lt;/a&gt;. Both gcc and msvc support Response Files, read more about their implementation &lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/docs/ResponseFiles.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you don't &lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/#storage-options"&gt;specify otherwise&lt;/a&gt;, sccache will use a local disk cache.&lt;/p&gt; 
&lt;p&gt;sccache works using a client-server model, where the server runs locally on the same machine as the client. The client-server model allows the server to be more efficient by keeping some state in memory. The sccache command will spawn a server process if one is not already running, or you can run &lt;code&gt;sccache --start-server&lt;/code&gt; to start the background server process without performing any compilation.&lt;/p&gt; 
&lt;p&gt;By default sccache server will listen on &lt;code&gt;127.0.0.1:4226&lt;/code&gt;, you can specify environment variable &lt;code&gt;SCCACHE_SERVER_PORT&lt;/code&gt; to use a different port or &lt;code&gt;SCCACHE_SERVER_UDS&lt;/code&gt; to listen on unix domain socket. Abstract unix socket is also supported as long as the path is escaped following the &lt;a href="https://doc.rust-lang.org/std/ascii/fn.escape_default.html"&gt;format&lt;/a&gt;. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;% env SCCACHE_SERVER_UDS=$HOME/sccache.sock sccache --start-server # unix socket
% env SCCACHE_SERVER_UDS=\\x00sccache.sock sccache --start-server # abstract unix socket
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can run &lt;code&gt;sccache --stop-server&lt;/code&gt; to terminate the server. It will also terminate after (by default) 10 minutes of inactivity.&lt;/p&gt; 
&lt;p&gt;Running &lt;code&gt;sccache --show-stats&lt;/code&gt; will print a summary of cache statistics.&lt;/p&gt; 
&lt;p&gt;Some notes about using &lt;code&gt;sccache&lt;/code&gt; with &lt;a href="https://jenkins.io"&gt;Jenkins&lt;/a&gt; are &lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/docs/Jenkins.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To use sccache with cmake, provide the following command line arguments to cmake 3.4 or newer:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;-DCMAKE_C_COMPILER_LAUNCHER=sccache
-DCMAKE_CXX_COMPILER_LAUNCHER=sccache
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The process for using sccache with MSVC and cmake, depends on which version of cmake you're using. &lt;strong&gt;For versions of cmake 3.24 and earlier&lt;/strong&gt;, to generate PDB files for debugging with MSVC, you can use the &lt;a href="https://docs.microsoft.com/en-us/cpp/build/reference/z7-zi-zi-debug-information-format?view=msvc-160"&gt;&lt;code&gt;/Z7&lt;/code&gt; option&lt;/a&gt;. Alternatively, the &lt;code&gt;/Zi&lt;/code&gt; option together with &lt;code&gt;/Fd&lt;/code&gt; can work if &lt;code&gt;/Fd&lt;/code&gt; names a different PDB file name for each object file created. Note that CMake sets &lt;code&gt;/Zi&lt;/code&gt; by default, so if you use CMake, you can use &lt;code&gt;/Z7&lt;/code&gt; by adding code like this in your CMakeLists.txt:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-cmake"&gt;if(CMAKE_BUILD_TYPE STREQUAL "Debug")
  string(REPLACE "/Zi" "/Z7" CMAKE_CXX_FLAGS_DEBUG "${CMAKE_CXX_FLAGS_DEBUG}")
  string(REPLACE "/Zi" "/Z7" CMAKE_C_FLAGS_DEBUG "${CMAKE_C_FLAGS_DEBUG}")
elseif(CMAKE_BUILD_TYPE STREQUAL "Release")
  string(REPLACE "/Zi" "/Z7" CMAKE_CXX_FLAGS_RELEASE "${CMAKE_CXX_FLAGS_RELEASE}")
  string(REPLACE "/Zi" "/Z7" CMAKE_C_FLAGS_RELEASE "${CMAKE_C_FLAGS_RELEASE}")
elseif(CMAKE_BUILD_TYPE STREQUAL "RelWithDebInfo")
  string(REPLACE "/Zi" "/Z7" CMAKE_CXX_FLAGS_RELWITHDEBINFO "${CMAKE_CXX_FLAGS_RELWITHDEBINFO}")
  string(REPLACE "/Zi" "/Z7" CMAKE_C_FLAGS_RELWITHDEBINFO "${CMAKE_C_FLAGS_RELWITHDEBINFO}")
endif()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;By default, sccache will fail your build if it fails to successfully communicate with its associated server. To have sccache instead gracefully failover to the local compiler without stopping, set the environment variable &lt;code&gt;SCCACHE_IGNORE_SERVER_IO_ERROR=1&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;For versions of cmake 3.25 and later&lt;/strong&gt;, to compile with MSVC, you have to use the new &lt;code&gt;CMAKE_MSVC_DEBUG_INFORMATION_FORMAT&lt;/code&gt; option, meant to configure the &lt;code&gt;-Z7&lt;/code&gt; flag. Additionally, you must set the cmake policy number 0141 to the NEW setting:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-cmake"&gt;set(CMAKE_MSVC_DEBUG_INFORMATION_FORMAT Embedded)
cmake_policy(SET CMP0141 NEW)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Example configuration where we automatically look for &lt;code&gt;sccache&lt;/code&gt; in the &lt;code&gt;PATH&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-cmake"&gt;find_program(SCCACHE sccache REQUIRED)

set(CMAKE_C_COMPILER_LAUNCHER ${SCCACHE})
set(CMAKE_CXX_COMPILER_LAUNCHER ${SCCACHE})
set(CMAKE_MSVC_DEBUG_INFORMATION_FORMAT Embedded)
cmake_policy(SET CMP0141 NEW)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, if configuring cmake with MSVC on the command line, assuming that sccache is on the default search path:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cmake -DCMAKE_C_COMPILER_LAUNCHER=sccache -DCMAKE_CXX_COMPILER_LAUNCHER=sccache -DCMAKE_MSVC_DEBUG_INFORMATION_FORMAT=Embedded -DCMAKE_POLICY_CMP0141=NEW [...]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And you can build code as usual without any additional flags in the command line, which is useful for IDEs.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Build Requirements&lt;/h2&gt; 
&lt;p&gt;sccache is a &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt; program. Building it requires &lt;code&gt;cargo&lt;/code&gt; (and thus&lt;code&gt;rustc&lt;/code&gt;). sccache currently requires &lt;strong&gt;Rust 1.75.0&lt;/strong&gt;. We recommend you install Rust via &lt;a href="https://rustup.rs/"&gt;Rustup&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Build&lt;/h2&gt; 
&lt;p&gt;If you are building sccache for non-development purposes make sure you use &lt;code&gt;cargo build --release&lt;/code&gt; to get optimized binaries:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cargo build --release [--no-default-features --features=s3|redis|gcs|memcached|azure|gha|webdav|oss]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The list of features can be found in the &lt;code&gt;Cargo.toml&lt;/code&gt; file, &lt;code&gt;[features]&lt;/code&gt; section.&lt;/p&gt; 
&lt;p&gt;By default, &lt;code&gt;sccache&lt;/code&gt; builds with support for all storage backends, but individual backends may be disabled by resetting the list of features and enabling all the other backends. Refer the &lt;a href="http://doc.crates.io/manifest.html#the-features-section"&gt;Cargo Documentation&lt;/a&gt; for details on how to select features with Cargo.&lt;/p&gt; 
&lt;h3&gt;Building portable binaries&lt;/h3&gt; 
&lt;p&gt;When building with the &lt;code&gt;dist-server&lt;/code&gt; feature, &lt;code&gt;sccache&lt;/code&gt; will depend on OpenSSL, which can be an annoyance if you want to distribute portable binaries. It is possible to statically link against OpenSSL using the &lt;code&gt;openssl/vendored&lt;/code&gt; feature.&lt;/p&gt; 
&lt;h4&gt;Linux&lt;/h4&gt; 
&lt;p&gt;Build with &lt;code&gt;cargo&lt;/code&gt; and use &lt;code&gt;ldd&lt;/code&gt; to check that the resulting binary does not depend on OpenSSL anymore.&lt;/p&gt; 
&lt;h4&gt;macOS&lt;/h4&gt; 
&lt;p&gt;Build with &lt;code&gt;cargo&lt;/code&gt; and use &lt;code&gt;otool -L&lt;/code&gt; to check that the resulting binary does not depend on OpenSSL anymore.&lt;/p&gt; 
&lt;h4&gt;Windows&lt;/h4&gt; 
&lt;p&gt;On Windows, the binary might also depend on a few MSVC CRT DLLs that are not available on older Windows versions.&lt;/p&gt; 
&lt;p&gt;It is possible to statically link against the CRT using a &lt;code&gt;.cargo/config.toml&lt;/code&gt; file with the following contents.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;[target.x86_64-pc-windows-msvc]
rustflags = ["-Ctarget-feature=+crt-static"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Build with &lt;code&gt;cargo&lt;/code&gt; and use &lt;code&gt;dumpbin /dependents&lt;/code&gt; to check that the resulting binary does not depend on MSVC CRT DLLs anymore.&lt;/p&gt; 
&lt;p&gt;When statically linking with OpenSSL, you will need Perl available in your &lt;code&gt;$PATH&lt;/code&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Separating caches between invocations&lt;/h2&gt; 
&lt;p&gt;In situations where several different compilation invocations should not reuse the cached results from each other, one can set &lt;code&gt;SCCACHE_C_CUSTOM_CACHE_BUSTER&lt;/code&gt; to a unique value that'll be mixed into the hash. &lt;code&gt;MACOSX_DEPLOYMENT_TARGET&lt;/code&gt; and &lt;code&gt;IPHONEOS_DEPLOYMENT_TARGET&lt;/code&gt; variables already exhibit such reuse-suppression behaviour. There are currently no such variables for compiling Rust.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Overwriting the cache&lt;/h2&gt; 
&lt;p&gt;In situations where the cache contains broken build artifacts, it can be necessary to overwrite the contents in the cache. That can be achieved by setting the &lt;code&gt;SCCACHE_RECACHE&lt;/code&gt; environment variable.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Debugging&lt;/h2&gt; 
&lt;p&gt;You can set the &lt;code&gt;SCCACHE_ERROR_LOG&lt;/code&gt; environment variable to a path and set &lt;code&gt;SCCACHE_LOG&lt;/code&gt; to get the server process to redirect its logging there (including the output of unhandled panics, since the server sets &lt;code&gt;RUST_BACKTRACE=1&lt;/code&gt; internally).&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;SCCACHE_ERROR_LOG=/tmp/sccache_log.txt SCCACHE_LOG=debug sccache
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also set these environment variables for your build system, for example&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;SCCACHE_ERROR_LOG=/tmp/sccache_log.txt SCCACHE_LOG=debug cmake --build /path/to/cmake/build/directory
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, if you are compiling locally, you can run the server manually in foreground mode by running &lt;code&gt;SCCACHE_START_SERVER=1 SCCACHE_NO_DAEMON=1 sccache&lt;/code&gt;, and send logging to stderr by setting the &lt;a href="https://docs.rs/env_logger/0.7.1/env_logger/#enabling-logging"&gt;&lt;code&gt;SCCACHE_LOG&lt;/code&gt; environment variable&lt;/a&gt; for example. This method is not suitable for CI services because you need to compile in another shell at the same time.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;SCCACHE_LOG=debug SCCACHE_START_SERVER=1 SCCACHE_NO_DAEMON=1 sccache
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Interaction with GNU &lt;code&gt;make&lt;/code&gt; jobserver&lt;/h2&gt; 
&lt;p&gt;sccache provides support for a &lt;a href="https://www.gnu.org/software/make/manual/html_node/Job-Slots.html"&gt;GNU make jobserver&lt;/a&gt;. When the server is started from a process that provides a jobserver, sccache will use that jobserver and provide it to any processes it spawns. (If you are running sccache from a GNU make recipe, you will need to prefix the command with &lt;code&gt;+&lt;/code&gt; to get this behavior.) If the sccache server is started without a jobserver present it will create its own with the number of slots equal to the number of available CPU cores.&lt;/p&gt; 
&lt;p&gt;This is most useful when using sccache for Rust compilation, as rustc supports using a jobserver for parallel codegen, so this ensures that rustc will not overwhelm the system with codegen tasks. Cargo implements its own jobserver (&lt;a href="https://doc.rust-lang.org/stable/cargo/reference/environment-variables.html#environment-variables-cargo-sets-for-build-scripts"&gt;see the information on &lt;code&gt;NUM_JOBS&lt;/code&gt; in the cargo documentation&lt;/a&gt;) for rustc to use, so using sccache for Rust compilation in cargo via &lt;code&gt;RUSTC_WRAPPER&lt;/code&gt; should do the right thing automatically.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Known Caveats&lt;/h2&gt; 
&lt;h3&gt;General&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Absolute paths to files must match to get a cache hit. This means that even if you are using a shared cache, everyone will have to build at the same absolute path (i.e. not in &lt;code&gt;$HOME&lt;/code&gt;) in order to benefit each other. In Rust this includes the source for third party crates which are stored in &lt;code&gt;$HOME/.cargo/registry/cache&lt;/code&gt; by default.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Rust&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Crates that invoke the system linker cannot be cached. This includes &lt;code&gt;bin&lt;/code&gt;, &lt;code&gt;dylib&lt;/code&gt;, &lt;code&gt;cdylib&lt;/code&gt;, and &lt;code&gt;proc-macro&lt;/code&gt; crates. You may be able to improve compilation time of large &lt;code&gt;bin&lt;/code&gt; crates by converting them to a &lt;code&gt;lib&lt;/code&gt; crate with a thin &lt;code&gt;bin&lt;/code&gt; wrapper.&lt;/li&gt; 
 &lt;li&gt;Incrementally compiled crates cannot be cached. By default, in the debug profile Cargo will use incremental compilation for workspace members and path dependencies. &lt;a href="https://doc.rust-lang.org/cargo/reference/profiles.html#incremental"&gt;You can disable incremental compilation.&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/docs/Rust.md"&gt;More details on Rust caveats&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Symbolic links&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Symbolic links to sccache won't work. Use hardlinks: &lt;code&gt;ln sccache /usr/local/bin/cc&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;User Agent&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Requests sent to your storage option of choice will have a user agent header indicating the current sccache version, e.g. &lt;code&gt;sccache/0.8.2&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Storage Options&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/docs/Local.md"&gt;Local&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/docs/S3.md"&gt;S3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/docs/S3.md#R2"&gt;R2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/docs/Redis.md"&gt;Redis&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/docs/Memcached.md"&gt;Memcached&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/docs/Gcs.md"&gt;Google Cloud Storage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/docs/Azure.md"&gt;Azure&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/docs/GHA.md"&gt;GitHub Actions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/docs/Webdav.md"&gt;WebDAV (Ccache/Bazel/Gradle compatible)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mozilla/sccache/main/docs/OSS.md"&gt;Alibaba OSS&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>sxyazi/yazi</title>
      <link>https://github.com/sxyazi/yazi</link>
      <description>&lt;p&gt;💥 Blazing fast terminal file manager written in Rust, based on async I/O.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;sup&gt;Special thanks to:&lt;/sup&gt; 
 &lt;br /&gt;
 &lt;br /&gt; 
 &lt;a href="https://www.warp.dev/?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=yazi" target="_blank"&gt; &lt;img alt="Warp sponsorship" width="300" src="https://github.com/user-attachments/assets/c7f141e7-9751-407d-bb0e-d6f2c487b34f" /&gt; &lt;br /&gt; &lt;b&gt;Warp, the intelligent terminal&lt;/b&gt; &lt;br /&gt; &lt;sup&gt;Yazi's AI-powered terminal of choice!&lt;br /&gt;Available for macOS, Linux and Windows&lt;/sup&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2&gt;Yazi - ⚡️ Blazing Fast Terminal File Manager&lt;/h2&gt; 
&lt;p&gt;Yazi (means "duck") is a terminal file manager written in Rust, based on non-blocking async I/O. It aims to provide an efficient, user-friendly, and customizable file management experience.&lt;/p&gt; 
&lt;p&gt;💡 A new article explaining its internal workings: &lt;a href="https://yazi-rs.github.io/blog/why-is-yazi-fast"&gt;Why is Yazi Fast?&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🚀 &lt;strong&gt;Full Asynchronous Support&lt;/strong&gt;: All I/O operations are asynchronous, CPU tasks are spread across multiple threads, making the most of available resources.&lt;/li&gt; 
 &lt;li&gt;💪 &lt;strong&gt;Powerful Async Task Scheduling and Management&lt;/strong&gt;: Provides real-time progress updates, task cancellation, and internal task priority assignment.&lt;/li&gt; 
 &lt;li&gt;🖼️ &lt;strong&gt;Built-in Support for Multiple Image Protocols&lt;/strong&gt;: Also integrated with Überzug++ and Chafa, covering almost all terminals.&lt;/li&gt; 
 &lt;li&gt;🌟 &lt;strong&gt;Built-in Code Highlighting and Image Decoding&lt;/strong&gt;: Combined with the pre-loading mechanism, greatly accelerates image and normal file loading.&lt;/li&gt; 
 &lt;li&gt;🔌 &lt;strong&gt;Concurrent Plugin System&lt;/strong&gt;: UI plugins (rewriting most of the UI), functional plugins, custom previewer/preloader/spotter/fetcher; Just some pieces of Lua.&lt;/li&gt; 
 &lt;li&gt;📡 &lt;strong&gt;Data Distribution Service&lt;/strong&gt;: Built on a client-server architecture (no additional server process required), integrated with a Lua-based publish-subscribe model, achieving cross-instance communication and state persistence.&lt;/li&gt; 
 &lt;li&gt;📦 &lt;strong&gt;Package Manager&lt;/strong&gt;: Install plugins and themes with one command, keeping them up-to-date, or pin them to a specific version.&lt;/li&gt; 
 &lt;li&gt;🧰 Integration with ripgrep, fd, fzf, zoxide&lt;/li&gt; 
 &lt;li&gt;💫 Vim-like input/pick/confirm/which/notify component, auto-completion for cd paths&lt;/li&gt; 
 &lt;li&gt;🏷️ Multi-Tab Support, Cross-directory selection, Scrollable Preview (for videos, PDFs, archives, code, directories, etc.)&lt;/li&gt; 
 &lt;li&gt;🔄 Bulk Renaming, Archive Extraction, Visual Mode, File Chooser, &lt;a href="https://github.com/yazi-rs/plugins/tree/main/git.yazi"&gt;Git Integration&lt;/a&gt;, &lt;a href="https://github.com/yazi-rs/plugins/tree/main/mount.yazi"&gt;Mount Manager&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;🎨 Theme System, Mouse Support, Trash Bin, Custom Layouts, Virtual Filesystem, CSI u, OSC 52&lt;/li&gt; 
 &lt;li&gt;... and more!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://github.com/sxyazi/yazi/assets/17523360/92ff23fa-0cd5-4f04-b387-894c12265cc7"&gt;https://github.com/sxyazi/yazi/assets/17523360/92ff23fa-0cd5-4f04-b387-894c12265cc7&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Project status&lt;/h2&gt; 
&lt;p&gt;Public beta, can be used as a daily driver.&lt;/p&gt; 
&lt;p&gt;Yazi is currently in heavy development, expect breaking changes.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Usage: &lt;a href="https://yazi-rs.github.io/docs/installation"&gt;https://yazi-rs.github.io/docs/installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Features: &lt;a href="https://yazi-rs.github.io/features"&gt;https://yazi-rs.github.io/features&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Discussion&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Discord Server (English mainly): &lt;a href="https://discord.gg/qfADduSdJu"&gt;https://discord.gg/qfADduSdJu&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Telegram Group (Chinese mainly): &lt;a href="https://t.me/yazi_rs"&gt;https://t.me/yazi_rs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Image Preview&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Platform&lt;/th&gt; 
   &lt;th&gt;Protocol&lt;/th&gt; 
   &lt;th&gt;Support&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/kovidgoyal/kitty"&gt;kitty&lt;/a&gt; (&amp;gt;= 0.28.0)&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://sw.kovidgoyal.net/kitty/graphics-protocol/#unicode-placeholders"&gt;Kitty unicode placeholders&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅ Built-in&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://iterm2.com"&gt;iTerm2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://iterm2.com/documentation-images.html"&gt;Inline images protocol&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅ Built-in&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/wez/wezterm"&gt;WezTerm&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://iterm2.com/documentation-images.html"&gt;Inline images protocol&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅ Built-in&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://invent.kde.org/utilities/konsole"&gt;Konsole&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/sxyazi/yazi/raw/main/yazi-adapter/src/drivers/kgp_old.rs"&gt;Kitty old protocol&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅ Built-in&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://codeberg.org/dnkl/foot"&gt;foot&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.vt100.net/docs/vt3xx-gp/chapter14.html"&gt;Sixel graphics format&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅ Built-in&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ghostty-org/ghostty"&gt;Ghostty&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://sw.kovidgoyal.net/kitty/graphics-protocol/#unicode-placeholders"&gt;Kitty unicode placeholders&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅ Built-in&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/terminal"&gt;Windows Terminal&lt;/a&gt; (&amp;gt;= v1.22.10352.0)&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.vt100.net/docs/vt3xx-gp/chapter14.html"&gt;Sixel graphics format&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅ Built-in&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/bakkeby/st-flexipatch"&gt;st with Sixel patch&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.vt100.net/docs/vt3xx-gp/chapter14.html"&gt;Sixel graphics format&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅ Built-in&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.warp.dev"&gt;Warp&lt;/a&gt; (macOS/Linux only)&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://iterm2.com/documentation-images.html"&gt;Inline images protocol&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅ Built-in&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Eugeny/tabby"&gt;Tabby&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://iterm2.com/documentation-images.html"&gt;Inline images protocol&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅ Built-in&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/vscode"&gt;VSCode&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://iterm2.com/documentation-images.html"&gt;Inline images protocol&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅ Built-in&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/raphamorim/rio"&gt;Rio&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://iterm2.com/documentation-images.html"&gt;Inline images protocol&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅ Built-in&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://gitlab.gnome.org/raggesilver/blackbox"&gt;Black Box&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.vt100.net/docs/vt3xx-gp/chapter14.html"&gt;Sixel graphics format&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅ Built-in&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/vercel/hyper"&gt;Hyper&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://iterm2.com/documentation-images.html"&gt;Inline images protocol&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅ Built-in&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ismail-yilmaz/Bobcat"&gt;Bobcat&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://iterm2.com/documentation-images.html"&gt;Inline images protocol&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅ Built-in&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;X11 / Wayland&lt;/td&gt; 
   &lt;td&gt;Window system protocol&lt;/td&gt; 
   &lt;td&gt;☑️ &lt;a href="https://github.com/jstkdng/ueberzugpp"&gt;Überzug++&lt;/a&gt; required&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fallback&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://en.wikipedia.org/wiki/ASCII_art"&gt;ASCII art (Unicode block)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;☑️ &lt;a href="https://hpjansson.org/chafa/"&gt;Chafa&lt;/a&gt; required&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;See &lt;a href="https://yazi-rs.github.io/docs/image-preview"&gt;https://yazi-rs.github.io/docs/image-preview&lt;/a&gt; for details.&lt;/p&gt; 
&lt;!-- Protocols --&gt; 
&lt;!-- Dependencies --&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Yazi is MIT-licensed. For more information check the &lt;a href="https://raw.githubusercontent.com/sxyazi/yazi/main/LICENSE"&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>openai/codex</title>
      <link>https://github.com/openai/codex</link>
      <description>&lt;p&gt;Lightweight coding agent that runs in your terminal&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt;OpenAI Codex CLI&lt;/h1&gt; 
&lt;p align="center"&gt;&lt;code&gt;npm i -g @openai/codex&lt;/code&gt;&lt;br /&gt;or &lt;code&gt;brew install codex&lt;/code&gt;&lt;/p&gt; 
&lt;p align="center"&gt;&lt;strong&gt;Codex CLI&lt;/strong&gt; is a coding agent from OpenAI that runs locally on your computer.&lt;br /&gt;If you are looking for the &lt;em&gt;cloud-based agent&lt;/em&gt; from OpenAI, &lt;strong&gt;Codex Web&lt;/strong&gt;, see &lt;a href="https://chatgpt.com/codex"&gt;chatgpt.com/codex&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/openai/codex/main/.github/codex-cli-splash.png" alt="Codex CLI splash" width="50%" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Table of contents&lt;/strong&gt;&lt;/summary&gt; 
 &lt;!-- Begin ToC --&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#quickstart"&gt;Quickstart&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#installing-and-running-codex-cli"&gt;Installing and running Codex CLI&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#using-codex-with-your-chatgpt-plan"&gt;Using Codex with your ChatGPT plan&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#connecting-on-a-headless-machine"&gt;Connecting on a "Headless" Machine&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#authenticate-locally-and-copy-your-credentials-to-the-headless-machine"&gt;Authenticate locally and copy your credentials to the "headless" machine&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#connecting-through-vps-or-remote"&gt;Connecting through VPS or remote&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#usage-based-billing-alternative-use-an-openai-api-key"&gt;Usage-based billing alternative: Use an OpenAI API key&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#forcing-a-specific-auth-method-advanced"&gt;Forcing a specific auth method (advanced)&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#choosing-codexs-level-of-autonomy"&gt;Choosing Codex's level of autonomy&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#1-readwrite"&gt;&lt;strong&gt;1. Read/write&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#2-read-only"&gt;&lt;strong&gt;2. Read-only&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#3-advanced-configuration"&gt;&lt;strong&gt;3. Advanced configuration&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#can-i-run-without-any-approvals"&gt;Can I run without ANY approvals?&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#fine-tuning-in-configtoml"&gt;Fine-tuning in &lt;code&gt;config.toml&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#example-prompts"&gt;Example prompts&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#running-with-a-prompt-as-input"&gt;Running with a prompt as input&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#using-open-source-models"&gt;Using Open Source Models&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#platform-sandboxing-details"&gt;Platform sandboxing details&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#experimental-technology-disclaimer"&gt;Experimental technology disclaimer&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#system-requirements"&gt;System requirements&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#cli-reference"&gt;CLI reference&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#memory--project-docs"&gt;Memory &amp;amp; project docs&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#non-interactive--ci-mode"&gt;Non-interactive / CI mode&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#model-context-protocol-mcp"&gt;Model Context Protocol (MCP)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#tracing--verbose-logging"&gt;Tracing / verbose logging&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#dotslash"&gt;DotSlash&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#configuration"&gt;Configuration&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#faq"&gt;FAQ&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#zero-data-retention-zdr-usage"&gt;Zero data retention (ZDR) usage&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#codex-open-source-fund"&gt;Codex open source fund&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#contributing"&gt;Contributing&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#development-workflow"&gt;Development workflow&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#writing-high-impact-code-changes"&gt;Writing high-impact code changes&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#opening-a-pull-request"&gt;Opening a pull request&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#review-process"&gt;Review process&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#community-values"&gt;Community values&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#getting-help"&gt;Getting help&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#contributor-license-agreement-cla"&gt;Contributor license agreement (CLA)&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#quick-fixes"&gt;Quick fixes&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#releasing-codex"&gt;Releasing &lt;code&gt;codex&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#security--responsible-ai"&gt;Security &amp;amp; responsible AI&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;!-- End ToC --&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;h3&gt;Installing and running Codex CLI&lt;/h3&gt; 
&lt;p&gt;Install globally with your preferred package manager:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;npm install -g @openai/codex  # Alternatively: `brew install codex`
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then simply run &lt;code&gt;codex&lt;/code&gt; to get started:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;codex
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;You can also go to the &lt;a href="https://github.com/openai/codex/releases/latest"&gt;latest GitHub Release&lt;/a&gt; and download the appropriate binary for your platform.&lt;/summary&gt; 
 &lt;p&gt;Each GitHub Release contains many executables, but in practice, you likely want one of these:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;macOS 
   &lt;ul&gt; 
    &lt;li&gt;Apple Silicon/arm64: &lt;code&gt;codex-aarch64-apple-darwin.tar.gz&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;x86_64 (older Mac hardware): &lt;code&gt;codex-x86_64-apple-darwin.tar.gz&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Linux 
   &lt;ul&gt; 
    &lt;li&gt;x86_64: &lt;code&gt;codex-x86_64-unknown-linux-musl.tar.gz&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;arm64: &lt;code&gt;codex-aarch64-unknown-linux-musl.tar.gz&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Each archive contains a single entry with the platform baked into the name (e.g., &lt;code&gt;codex-x86_64-unknown-linux-musl&lt;/code&gt;), so you likely want to rename it to &lt;code&gt;codex&lt;/code&gt; after extracting it.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;Using Codex with your ChatGPT plan&lt;/h3&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/openai/codex/main/.github/codex-cli-login.png" alt="Codex CLI login" width="50%" /&gt; &lt;/p&gt; 
&lt;p&gt;Run &lt;code&gt;codex&lt;/code&gt; and select &lt;strong&gt;Sign in with ChatGPT&lt;/strong&gt;. You'll need a Plus, Pro, or Team ChatGPT account, and will get access to our latest models, including &lt;code&gt;gpt-5&lt;/code&gt;, at no extra cost to your plan. (Enterprise is coming soon.)&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Important: If you've used the Codex CLI before, follow these steps to migrate from usage-based billing with your API key:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Update the CLI and ensure &lt;code&gt;codex --version&lt;/code&gt; is &lt;code&gt;0.20.0&lt;/code&gt; or later&lt;/li&gt; 
  &lt;li&gt;Delete &lt;code&gt;~/.codex/auth.json&lt;/code&gt; (this should be &lt;code&gt;C:\Users\USERNAME\.codex\auth.json&lt;/code&gt; on Windows)&lt;/li&gt; 
  &lt;li&gt;Run &lt;code&gt;codex login&lt;/code&gt; again&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;If you encounter problems with the login flow, please comment on &lt;a href="https://github.com/openai/codex/issues/1243"&gt;this issue&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Connecting on a "Headless" Machine&lt;/h3&gt; 
&lt;p&gt;Today, the login process entails running a server on &lt;code&gt;localhost:1455&lt;/code&gt;. If you are on a "headless" server, such as a Docker container or are &lt;code&gt;ssh&lt;/code&gt;'d into a remote machine, loading &lt;code&gt;localhost:1455&lt;/code&gt; in the browser on your local machine will not automatically connect to the webserver running on the &lt;em&gt;headless&lt;/em&gt; machine, so you must use one of the following workarounds:&lt;/p&gt; 
&lt;h4&gt;Authenticate locally and copy your credentials to the "headless" machine&lt;/h4&gt; 
&lt;p&gt;The easiest solution is likely to run through the &lt;code&gt;codex login&lt;/code&gt; process on your local machine such that &lt;code&gt;localhost:1455&lt;/code&gt; &lt;em&gt;is&lt;/em&gt; accessible in your web browser. When you complete the authentication process, an &lt;code&gt;auth.json&lt;/code&gt; file should be available at &lt;code&gt;$CODEX_HOME/auth.json&lt;/code&gt; (on Mac/Linux, &lt;code&gt;$CODEX_HOME&lt;/code&gt; defaults to &lt;code&gt;~/.codex&lt;/code&gt; whereas on Windows, it defaults to &lt;code&gt;%USERPROFILE%\.codex&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;Because the &lt;code&gt;auth.json&lt;/code&gt; file is not tied to a specific host, once you complete the authentication flow locally, you can copy the &lt;code&gt;$CODEX_HOME/auth.json&lt;/code&gt; file to the headless machine and then &lt;code&gt;codex&lt;/code&gt; should "just work" on that machine. Note to copy a file to a Docker container, you can do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# substitute MY_CONTAINER with the name or id of your Docker container:
CONTAINER_HOME=$(docker exec MY_CONTAINER printenv HOME)
docker exec MY_CONTAINER mkdir -p "$CONTAINER_HOME/.codex"
docker cp auth.json MY_CONTAINER:"$CONTAINER_HOME/.codex/auth.json"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;whereas if you are &lt;code&gt;ssh&lt;/code&gt;'d into a remote machine, you likely want to use &lt;a href="https://en.wikipedia.org/wiki/Secure_copy_protocol"&gt;&lt;code&gt;scp&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ssh user@remote 'mkdir -p ~/.codex'
scp ~/.codex/auth.json user@remote:~/.codex/auth.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or try this one-liner:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ssh user@remote 'mkdir -p ~/.codex &amp;amp;&amp;amp; cat &amp;gt; ~/.codex/auth.json' &amp;lt; ~/.codex/auth.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Connecting through VPS or remote&lt;/h4&gt; 
&lt;p&gt;If you run Codex on a remote machine (VPS/server) without a local browser, the login helper starts a server on &lt;code&gt;localhost:1455&lt;/code&gt; on the remote host. To complete login in your local browser, forward that port to your machine before starting the login flow:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# From your local machine
ssh -L 1455:localhost:1455 &amp;lt;user&amp;gt;@&amp;lt;remote-host&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, in that SSH session, run &lt;code&gt;codex&lt;/code&gt; and select "Sign in with ChatGPT". When prompted, open the printed URL (it will be &lt;code&gt;http://localhost:1455/...&lt;/code&gt;) in your local browser. The traffic will be tunneled to the remote server.&lt;/p&gt; 
&lt;h3&gt;Usage-based billing alternative: Use an OpenAI API key&lt;/h3&gt; 
&lt;p&gt;If you prefer to pay-as-you-go, you can still authenticate with your OpenAI API key by setting it as an environment variable:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;export OPENAI_API_KEY="your-api-key-here"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Notes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;This command only sets the key for your current terminal session, which we recommend. To set it for all future sessions, you can also add the &lt;code&gt;export&lt;/code&gt; line to your shell's configuration file (e.g., &lt;code&gt;~/.zshrc&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;If you have signed in with ChatGPT, Codex will default to using your ChatGPT credits. If you wish to use your API key, use the &lt;code&gt;/logout&lt;/code&gt; command to clear your ChatGPT authentication.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Forcing a specific auth method (advanced)&lt;/h4&gt; 
&lt;p&gt;You can explicitly choose which authentication Codex should prefer when both are available.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;To always use your API key (even when ChatGPT auth exists), set:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;# ~/.codex/config.toml
preferred_auth_method = "apikey"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or override ad-hoc via CLI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;codex --config preferred_auth_method="apikey"
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;To prefer ChatGPT auth (default), set:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;# ~/.codex/config.toml
preferred_auth_method = "chatgpt"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Notes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;When &lt;code&gt;preferred_auth_method = "apikey"&lt;/code&gt; and an API key is available, the login screen is skipped.&lt;/li&gt; 
 &lt;li&gt;When &lt;code&gt;preferred_auth_method = "chatgpt"&lt;/code&gt; (default), Codex prefers ChatGPT auth if present; if only an API key is present, it will use the API key. Certain account types may also require API-key mode.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Choosing Codex's level of autonomy&lt;/h3&gt; 
&lt;p&gt;We always recommend running Codex in its default sandbox that gives you strong guardrails around what the agent can do. The default sandbox prevents it from editing files outside its workspace, or from accessing the network.&lt;/p&gt; 
&lt;p&gt;When you launch Codex in a new folder, it detects whether the folder is version controlled and recommends one of two levels of autonomy:&lt;/p&gt; 
&lt;h4&gt;&lt;strong&gt;1. Read/write&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Codex can run commands and write files in the workspace without approval.&lt;/li&gt; 
 &lt;li&gt;To write files in other folders, access network, update git or perform other actions protected by the sandbox, Codex will need your permission.&lt;/li&gt; 
 &lt;li&gt;By default, the workspace includes the current directory, as well as temporary directories like &lt;code&gt;/tmp&lt;/code&gt;. You can see what directories are in the workspace with the &lt;code&gt;/status&lt;/code&gt; command. See the docs for how to customize this behavior.&lt;/li&gt; 
 &lt;li&gt;Advanced: You can manually specify this configuration by running &lt;code&gt;codex --sandbox workspace-write --ask-for-approval on-request&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;This is the recommended default for version-controlled folders.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;strong&gt;2. Read-only&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Codex can run read-only commands without approval.&lt;/li&gt; 
 &lt;li&gt;To edit files, access network, or perform other actions protected by the sandbox, Codex will need your permission.&lt;/li&gt; 
 &lt;li&gt;Advanced: You can manually specify this configuration by running &lt;code&gt;codex --sandbox read-only --ask-for-approval on-request&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;This is the recommended default non-version-controlled folders.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;strong&gt;3. Advanced configuration&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;Codex gives you fine-grained control over the sandbox with the &lt;code&gt;--sandbox&lt;/code&gt; option, and over when it requests approval with the &lt;code&gt;--ask-for-approval&lt;/code&gt; option. Run &lt;code&gt;codex help&lt;/code&gt; for more on these options.&lt;/p&gt; 
&lt;h4&gt;Can I run without ANY approvals?&lt;/h4&gt; 
&lt;p&gt;Yes, run codex non-interactively with &lt;code&gt;--ask-for-approval never&lt;/code&gt;. This option works with all &lt;code&gt;--sandbox&lt;/code&gt; options, so you still have full control over Codex's level of autonomy. It will make its best attempt with whatever contrainsts you provide. For example:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use &lt;code&gt;codex --ask-for-approval never --sandbox read-only&lt;/code&gt; when you are running many agents to answer questions in parallel in the same workspace.&lt;/li&gt; 
 &lt;li&gt;Use &lt;code&gt;codex --ask-for-approval never --sandbox workspace-write&lt;/code&gt; when you want the agent to non-interactively take time to produce the best outcome, with strong guardrails around its behavior.&lt;/li&gt; 
 &lt;li&gt;Use &lt;code&gt;codex --ask-for-approval never --sandbox danger-full-access&lt;/code&gt; to dangerously give the agent full autonomy. Because this disables important safety mechanisms, we recommend against using this unless running Codex in an isolated environment.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Fine-tuning in &lt;code&gt;config.toml&lt;/code&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;# approval mode
approval_policy = "untrusted"
sandbox_mode    = "read-only"

# full-auto mode
approval_policy = "on-request"
sandbox_mode    = "workspace-write"

# Optional: allow network in workspace-write mode
[sandbox_workspace_write]
network_access = true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also save presets as &lt;strong&gt;profiles&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;[profiles.full_auto]
approval_policy = "on-request"
sandbox_mode    = "workspace-write"

[profiles.readonly_quiet]
approval_policy = "never"
sandbox_mode    = "read-only"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Example prompts&lt;/h3&gt; 
&lt;p&gt;Below are a few bite-size examples you can copy-paste. Replace the text in quotes with your own task. See the &lt;a href="https://github.com/openai/codex/raw/main/codex-cli/examples/prompting_guide.md"&gt;prompting guide&lt;/a&gt; for more tips and usage patterns.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;✨&lt;/th&gt; 
   &lt;th&gt;What you type&lt;/th&gt; 
   &lt;th&gt;What happens&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex "Refactor the Dashboard component to React Hooks"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Codex rewrites the class component, runs &lt;code&gt;npm test&lt;/code&gt;, and shows the diff.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex "Generate SQL migrations for adding a users table"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Infers your ORM, creates migration files, and runs them in a sandboxed DB.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex "Write unit tests for utils/date.ts"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Generates tests, executes them, and iterates until they pass.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex "Bulk-rename *.jpeg -&amp;gt; *.jpg with git mv"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Safely renames files and updates imports/usages.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex "Explain what this regex does: ^(?=.*[A-Z]).{8,}$"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Outputs a step-by-step human explanation.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;6&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex "Carefully review this repo, and propose 3 high impact well-scoped PRs"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Suggests impactful PRs in the current codebase.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;7&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex "Look for vulnerabilities and create a security review report"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Finds and explains security bugs.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Running with a prompt as input&lt;/h2&gt; 
&lt;p&gt;You can also run Codex CLI with a prompt as input:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;codex "explain this codebase to me"
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;codex --full-auto "create the fanciest todo-list app"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That's it - Codex will scaffold a file, run it inside a sandbox, install any missing dependencies, and show you the live result. Approve the changes and they'll be committed to your working directory.&lt;/p&gt; 
&lt;h2&gt;Using Open Source Models&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Use &lt;code&gt;--profile&lt;/code&gt; to use other models&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Codex also allows you to use other providers that support the OpenAI Chat Completions (or Responses) API.&lt;/p&gt; 
 &lt;p&gt;To do so, you must first define custom &lt;a href="https://raw.githubusercontent.com/openai/codex/main/config.md#model_providers"&gt;providers&lt;/a&gt; in &lt;code&gt;~/.codex/config.toml&lt;/code&gt;. For example, the provider for a standard Ollama setup would be defined as follows:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-toml"&gt;[model_providers.ollama]
name = "Ollama"
base_url = "http://localhost:11434/v1"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;The &lt;code&gt;base_url&lt;/code&gt; will have &lt;code&gt;/chat/completions&lt;/code&gt; appended to it to build the full URL for the request.&lt;/p&gt; 
 &lt;p&gt;For providers that also require an &lt;code&gt;Authorization&lt;/code&gt; header of the form &lt;code&gt;Bearer: SECRET&lt;/code&gt;, an &lt;code&gt;env_key&lt;/code&gt; can be specified, which indicates the environment variable to read to use as the value of &lt;code&gt;SECRET&lt;/code&gt; when making a request:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-toml"&gt;[model_providers.openrouter]
name = "OpenRouter"
base_url = "https://openrouter.ai/api/v1"
env_key = "OPENROUTER_API_KEY"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Providers that speak the Responses API are also supported by adding &lt;code&gt;wire_api = "responses"&lt;/code&gt; as part of the definition. Accessing OpenAI models via Azure is an example of such a provider, though it also requires specifying additional &lt;code&gt;query_params&lt;/code&gt; that need to be appended to the request URL:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-toml"&gt;[model_providers.azure]
name = "Azure"
# Make sure you set the appropriate subdomain for this URL.
base_url = "https://YOUR_PROJECT_NAME.openai.azure.com/openai"
env_key = "AZURE_OPENAI_API_KEY"  # Or "OPENAI_API_KEY", whichever you use.
# Newer versions appear to support the responses API, see https://github.com/openai/codex/pull/1321
query_params = { api-version = "2025-04-01-preview" }
wire_api = "responses"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Once you have defined a provider you wish to use, you can configure it as your default provider as follows:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-toml"&gt;model_provider = "azure"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;[!TIP] If you find yourself experimenting with a variety of models and providers, then you likely want to invest in defining a &lt;em&gt;profile&lt;/em&gt; for each configuration like so:&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;pre&gt;&lt;code class="language-toml"&gt;[profiles.o3]
model_provider = "azure"
model = "o3"

[profiles.mistral]
model_provider = "ollama"
model = "mistral"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;This way, you can specify one command-line argument (.e.g., &lt;code&gt;--profile o3&lt;/code&gt;, &lt;code&gt;--profile mistral&lt;/code&gt;) to override multiple settings together.&lt;/p&gt; 
&lt;/details&gt; 
&lt;p&gt;Codex can run fully locally against an OpenAI-compatible OSS host (like Ollama) using the &lt;code&gt;--oss&lt;/code&gt; flag:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Interactive UI: 
  &lt;ul&gt; 
   &lt;li&gt;codex --oss&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Non-interactive (programmatic) mode: 
  &lt;ul&gt; 
   &lt;li&gt;echo "Refactor utils" | codex exec --oss&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Model selection when using &lt;code&gt;--oss&lt;/code&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you omit &lt;code&gt;-m/--model&lt;/code&gt;, Codex defaults to -m gpt-oss:20b and will verify it exists locally (downloading if needed).&lt;/li&gt; 
 &lt;li&gt;To pick a different size, pass one of: 
  &lt;ul&gt; 
   &lt;li&gt;-m "gpt-oss:20b"&lt;/li&gt; 
   &lt;li&gt;-m "gpt-oss:120b"&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Point Codex at your own OSS host:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;By default, &lt;code&gt;--oss&lt;/code&gt; talks to &lt;a href="http://localhost:11434/v1"&gt;http://localhost:11434/v1&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;To use a different host, set one of these environment variables before running Codex: 
  &lt;ul&gt; 
   &lt;li&gt;CODEX_OSS_BASE_URL, for example: 
    &lt;ul&gt; 
     &lt;li&gt;CODEX_OSS_BASE_URL="&lt;a href="http://my-ollama.example.com:11434/v1"&gt;http://my-ollama.example.com:11434/v1&lt;/a&gt;" codex --oss -m gpt-oss:20b&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;or CODEX_OSS_PORT (when the host is localhost): 
    &lt;ul&gt; 
     &lt;li&gt;CODEX_OSS_PORT=11434 codex --oss&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Advanced: you can persist this in your config instead of environment variables by overriding the built-in &lt;code&gt;oss&lt;/code&gt; provider in &lt;code&gt;~/.codex/config.toml&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;[model_providers.oss]
name = "Open Source"
base_url = "http://my-ollama.example.com:11434/v1"
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Platform sandboxing details&lt;/h3&gt; 
&lt;p&gt;By default, Codex CLI runs code and shell commands inside a restricted sandbox to protect your system.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Not all tool calls are sandboxed. Specifically, &lt;strong&gt;trusted Model Context Protocol (MCP) tool calls&lt;/strong&gt; are executed outside of the sandbox.&lt;br /&gt; This is intentional: MCP tools are explicitly configured and trusted by you, and they often need to connect to &lt;strong&gt;external applications or services&lt;/strong&gt; (e.g. issue trackers, databases, messaging systems).&lt;br /&gt; Running them outside the sandbox allows Codex to integrate with these external systems without being blocked by sandbox restrictions.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;The mechanism Codex uses to implement the sandbox policy depends on your OS:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;macOS 12+&lt;/strong&gt; uses &lt;strong&gt;Apple Seatbelt&lt;/strong&gt; and runs commands using &lt;code&gt;sandbox-exec&lt;/code&gt; with a profile (&lt;code&gt;-p&lt;/code&gt;) that corresponds to the &lt;code&gt;--sandbox&lt;/code&gt; that was specified.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt; uses a combination of Landlock/seccomp APIs to enforce the &lt;code&gt;sandbox&lt;/code&gt; configuration.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Note that when running Linux in a containerized environment such as Docker, sandboxing may not work if the host/container configuration does not support the necessary Landlock/seccomp APIs. In such cases, we recommend configuring your Docker container so that it provides the sandbox guarantees you are looking for and then running &lt;code&gt;codex&lt;/code&gt; with &lt;code&gt;--sandbox danger-full-access&lt;/code&gt; (or, more simply, the &lt;code&gt;--dangerously-bypass-approvals-and-sandbox&lt;/code&gt; flag) within your container.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Experimental technology disclaimer&lt;/h2&gt; 
&lt;p&gt;Codex CLI is an experimental project under active development. It is not yet stable, may contain bugs, incomplete features, or undergo breaking changes. We're building it in the open with the community and welcome:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Bug reports&lt;/li&gt; 
 &lt;li&gt;Feature requests&lt;/li&gt; 
 &lt;li&gt;Pull requests&lt;/li&gt; 
 &lt;li&gt;Good vibes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Help us improve by filing issues or submitting PRs (see the section below for how to contribute)!&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;System requirements&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Requirement&lt;/th&gt; 
   &lt;th&gt;Details&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Operating systems&lt;/td&gt; 
   &lt;td&gt;macOS 12+, Ubuntu 20.04+/Debian 10+, or Windows 11 &lt;strong&gt;via WSL2&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Git (optional, recommended)&lt;/td&gt; 
   &lt;td&gt;2.23+ for built-in PR helpers&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;RAM&lt;/td&gt; 
   &lt;td&gt;4-GB minimum (8-GB recommended)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;CLI reference&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Command&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
   &lt;th&gt;Example&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;codex&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Interactive TUI&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;codex "..."&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Initial prompt for interactive TUI&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex "fix lint errors"&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;codex exec "..."&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Non-interactive "automation mode"&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex exec "explain utils.ts"&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Key flags: &lt;code&gt;--model/-m&lt;/code&gt;, &lt;code&gt;--ask-for-approval/-a&lt;/code&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Memory &amp;amp; project docs&lt;/h2&gt; 
&lt;p&gt;You can give Codex extra instructions and guidance using &lt;code&gt;AGENTS.md&lt;/code&gt; files. Codex looks for &lt;code&gt;AGENTS.md&lt;/code&gt; files in the following places, and merges them top-down:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;code&gt;~/.codex/AGENTS.md&lt;/code&gt; - personal global guidance&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;AGENTS.md&lt;/code&gt; at repo root - shared project notes&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;AGENTS.md&lt;/code&gt; in the current working directory - sub-folder/feature specifics&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Non-interactive / CI mode&lt;/h2&gt; 
&lt;p&gt;Run Codex head-less in pipelines. Example GitHub Action step:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;- name: Update changelog via Codex
  run: |
    npm install -g @openai/codex
    export OPENAI_API_KEY="${{ secrets.OPENAI_KEY }}"
    codex exec --full-auto "update CHANGELOG for next release"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Model Context Protocol (MCP)&lt;/h2&gt; 
&lt;p&gt;The Codex CLI can be configured to leverage MCP servers by defining an &lt;a href="https://raw.githubusercontent.com/openai/codex/main/codex-rs/config.md#mcp_servers"&gt;&lt;code&gt;mcp_servers&lt;/code&gt;&lt;/a&gt; section in &lt;code&gt;~/.codex/config.toml&lt;/code&gt;. It is intended to mirror how tools such as Claude and Cursor define &lt;code&gt;mcpServers&lt;/code&gt; in their respective JSON config files, though the Codex format is slightly different since it uses TOML rather than JSON, e.g.:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;# IMPORTANT: the top-level key is `mcp_servers` rather than `mcpServers`.
[mcp_servers.server-name]
command = "npx"
args = ["-y", "mcp-server"]
env = { "API_KEY" = "value" }
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] It is somewhat experimental, but the Codex CLI can also be run as an MCP &lt;em&gt;server&lt;/em&gt; via &lt;code&gt;codex mcp&lt;/code&gt;. If you launch it with an MCP client such as &lt;code&gt;npx @modelcontextprotocol/inspector codex mcp&lt;/code&gt; and send it a &lt;code&gt;tools/list&lt;/code&gt; request, you will see that there is only one tool, &lt;code&gt;codex&lt;/code&gt;, that accepts a grab-bag of inputs, including a catch-all &lt;code&gt;config&lt;/code&gt; map for anything you might want to override. Feel free to play around with it and provide feedback via GitHub issues.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Tracing / verbose logging&lt;/h2&gt; 
&lt;p&gt;Because Codex is written in Rust, it honors the &lt;code&gt;RUST_LOG&lt;/code&gt; environment variable to configure its logging behavior.&lt;/p&gt; 
&lt;p&gt;The TUI defaults to &lt;code&gt;RUST_LOG=codex_core=info,codex_tui=info&lt;/code&gt; and log messages are written to &lt;code&gt;~/.codex/log/codex-tui.log&lt;/code&gt;, so you can leave the following running in a separate terminal to monitor log messages as they are written:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;tail -F ~/.codex/log/codex-tui.log
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;By comparison, the non-interactive mode (&lt;code&gt;codex exec&lt;/code&gt;) defaults to &lt;code&gt;RUST_LOG=error&lt;/code&gt;, but messages are printed inline, so there is no need to monitor a separate file.&lt;/p&gt; 
&lt;p&gt;See the Rust documentation on &lt;a href="https://docs.rs/env_logger/latest/env_logger/#enabling-logging"&gt;&lt;code&gt;RUST_LOG&lt;/code&gt;&lt;/a&gt; for more information on the configuration options.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;DotSlash&lt;/h3&gt; 
&lt;p&gt;The GitHub Release also contains a &lt;a href="https://dotslash-cli.com/"&gt;DotSlash&lt;/a&gt; file for the Codex CLI named &lt;code&gt;codex&lt;/code&gt;. Using a DotSlash file makes it possible to make a lightweight commit to source control to ensure all contributors use the same version of an executable, regardless of what platform they use for development.&lt;/p&gt;  
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Build from source&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the repository and navigate to the root of the Cargo workspace.
git clone https://github.com/openai/codex.git
cd codex/codex-rs

# Install the Rust toolchain, if necessary.
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
source "$HOME/.cargo/env"
rustup component add rustfmt
rustup component add clippy

# Build Codex.
cargo build

# Launch the TUI with a sample prompt.
cargo run --bin codex -- "explain this codebase to me"

# After making changes, ensure the code is clean.
cargo fmt -- --config imports_granularity=Item
cargo clippy --tests

# Run the tests.
cargo test
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Configuration&lt;/h2&gt; 
&lt;p&gt;Codex supports a rich set of configuration options documented in &lt;a href="https://raw.githubusercontent.com/openai/codex/main/codex-rs/config.md"&gt;&lt;code&gt;codex-rs/config.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;By default, Codex loads its configuration from &lt;code&gt;~/.codex/config.toml&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Though &lt;code&gt;--config&lt;/code&gt; can be used to set/override ad-hoc config values for individual invocations of &lt;code&gt;codex&lt;/code&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;OpenAI released a model called Codex in 2021 - is this related?&lt;/summary&gt; 
 &lt;p&gt;In 2021, OpenAI released Codex, an AI system designed to generate code from natural language prompts. That original Codex model was deprecated as of March 2023 and is separate from the CLI tool.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Which models are supported?&lt;/summary&gt; 
 &lt;p&gt;Any model available with &lt;a href="https://platform.openai.com/docs/api-reference/responses"&gt;Responses API&lt;/a&gt;. The default is &lt;code&gt;o4-mini&lt;/code&gt;, but pass &lt;code&gt;--model gpt-4.1&lt;/code&gt; or set &lt;code&gt;model: gpt-4.1&lt;/code&gt; in your config file to override.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Why does &lt;code&gt;o3&lt;/code&gt; or &lt;code&gt;o4-mini&lt;/code&gt; not work for me?&lt;/summary&gt; 
 &lt;p&gt;It's possible that your &lt;a href="https://help.openai.com/en/articles/10910291-api-organization-verification"&gt;API account needs to be verified&lt;/a&gt; in order to start streaming responses and seeing chain of thought summaries from the API. If you're still running into issues, please let us know!&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How do I stop Codex from editing my files?&lt;/summary&gt; 
 &lt;p&gt;Codex runs model-generated commands in a sandbox. If a proposed command or file change doesn't look right, you can simply type &lt;strong&gt;n&lt;/strong&gt; to deny the command or give the model feedback.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Does it work on Windows?&lt;/summary&gt; 
 &lt;p&gt;Not directly. It requires &lt;a href="https://learn.microsoft.com/en-us/windows/wsl/install"&gt;Windows Subsystem for Linux (WSL2)&lt;/a&gt; - Codex has been tested on macOS and Linux with Node 22.&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Zero data retention (ZDR) usage&lt;/h2&gt; 
&lt;p&gt;Codex CLI &lt;strong&gt;does&lt;/strong&gt; support OpenAI organizations with &lt;a href="https://platform.openai.com/docs/guides/your-data#zero-data-retention"&gt;Zero Data Retention (ZDR)&lt;/a&gt; enabled. If your OpenAI organization has Zero Data Retention enabled and you still encounter errors such as:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;OpenAI rejected the request. Error details: Status: 400, Code: unsupported_parameter, Type: invalid_request_error, Message: 400 Previous response cannot be used for this organization due to Zero Data Retention.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Ensure you are running &lt;code&gt;codex&lt;/code&gt; with &lt;code&gt;--config disable_response_storage=true&lt;/code&gt; or add this line to &lt;code&gt;~/.codex/config.toml&lt;/code&gt; to avoid specifying the command line option each time:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;disable_response_storage = true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/openai/codex/main/codex-rs/config.md#disable_response_storage"&gt;the configuration documentation on &lt;code&gt;disable_response_storage&lt;/code&gt;&lt;/a&gt; for details.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Codex open source fund&lt;/h2&gt; 
&lt;p&gt;We're excited to launch a &lt;strong&gt;$1 million initiative&lt;/strong&gt; supporting open source projects that use Codex CLI and other OpenAI models.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Grants are awarded up to &lt;strong&gt;$25,000&lt;/strong&gt; API credits.&lt;/li&gt; 
 &lt;li&gt;Applications are reviewed &lt;strong&gt;on a rolling basis&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Interested? &lt;a href="https://openai.com/form/codex-open-source-fund/"&gt;Apply here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project is under active development and the code will likely change pretty significantly.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;At the moment, we only plan to prioritize reviewing external contributions for bugs or security fixes.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;If you want to add a new feature or change the behavior of an existing one, please open an issue proposing the feature and get approval from an OpenAI team member before spending time building it.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;New contributions that don't go through this process may be closed&lt;/strong&gt; if they aren't aligned with our current roadmap or conflict with other priorities/upcoming features.&lt;/p&gt; 
&lt;h3&gt;Development workflow&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create a &lt;em&gt;topic branch&lt;/em&gt; from &lt;code&gt;main&lt;/code&gt; - e.g. &lt;code&gt;feat/interactive-prompt&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Keep your changes focused. Multiple unrelated fixes should be opened as separate PRs.&lt;/li&gt; 
 &lt;li&gt;Following the &lt;a href="https://raw.githubusercontent.com/openai/codex/main/#development-workflow"&gt;development setup&lt;/a&gt; instructions above, ensure your change is free of lint warnings and test failures.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Writing high-impact code changes&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Start with an issue.&lt;/strong&gt; Open a new one or comment on an existing discussion so we can agree on the solution before code is written.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Add or update tests.&lt;/strong&gt; Every new feature or bug-fix should come with test coverage that fails before your change and passes afterwards. 100% coverage is not required, but aim for meaningful assertions.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Document behaviour.&lt;/strong&gt; If your change affects user-facing behaviour, update the README, inline help (&lt;code&gt;codex --help&lt;/code&gt;), or relevant example projects.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Keep commits atomic.&lt;/strong&gt; Each commit should compile and the tests should pass. This makes reviews and potential rollbacks easier.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Opening a pull request&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fill in the PR template (or include similar information) - &lt;strong&gt;What? Why? How?&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Run &lt;strong&gt;all&lt;/strong&gt; checks locally (&lt;code&gt;cargo test &amp;amp;&amp;amp; cargo clippy --tests &amp;amp;&amp;amp; cargo fmt -- --config imports_granularity=Item&lt;/code&gt;). CI failures that could have been caught locally slow down the process.&lt;/li&gt; 
 &lt;li&gt;Make sure your branch is up-to-date with &lt;code&gt;main&lt;/code&gt; and that you have resolved merge conflicts.&lt;/li&gt; 
 &lt;li&gt;Mark the PR as &lt;strong&gt;Ready for review&lt;/strong&gt; only when you believe it is in a merge-able state.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Review process&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;One maintainer will be assigned as a primary reviewer.&lt;/li&gt; 
 &lt;li&gt;If your PR adds a new feature that was not previously discussed and approved, we may choose to close your PR (see &lt;a href="https://raw.githubusercontent.com/openai/codex/main/#contributing"&gt;Contributing&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;We may ask for changes - please do not take this personally. We value the work, but we also value consistency and long-term maintainability.&lt;/li&gt; 
 &lt;li&gt;When there is consensus that the PR meets the bar, a maintainer will squash-and-merge.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Community values&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Be kind and inclusive.&lt;/strong&gt; Treat others with respect; we follow the &lt;a href="https://www.contributor-covenant.org/"&gt;Contributor Covenant&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Assume good intent.&lt;/strong&gt; Written communication is hard - err on the side of generosity.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Teach &amp;amp; learn.&lt;/strong&gt; If you spot something confusing, open an issue or PR with improvements.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Getting help&lt;/h3&gt; 
&lt;p&gt;If you run into problems setting up the project, would like feedback on an idea, or just want to say &lt;em&gt;hi&lt;/em&gt; - please open a Discussion or jump into the relevant issue. We are happy to help.&lt;/p&gt; 
&lt;p&gt;Together we can make Codex CLI an incredible tool. &lt;strong&gt;Happy hacking!&lt;/strong&gt; &lt;span&gt;🚀&lt;/span&gt;&lt;/p&gt; 
&lt;h3&gt;Contributor license agreement (CLA)&lt;/h3&gt; 
&lt;p&gt;All contributors &lt;strong&gt;must&lt;/strong&gt; accept the CLA. The process is lightweight:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Open your pull request.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Paste the following comment (or reply &lt;code&gt;recheck&lt;/code&gt; if you've signed before):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-text"&gt;I have read the CLA Document and I hereby sign the CLA
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The CLA-Assistant bot records your signature in the repo and marks the status check as passed.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;No special Git commands, email attachments, or commit footers required.&lt;/p&gt; 
&lt;h4&gt;Quick fixes&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Scenario&lt;/th&gt; 
   &lt;th&gt;Command&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Amend last commit&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;git commit --amend -s --no-edit &amp;amp;&amp;amp; git push -f&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The &lt;strong&gt;DCO check&lt;/strong&gt; blocks merges until every commit in the PR carries the footer (with squash this is just the one).&lt;/p&gt; 
&lt;h3&gt;Releasing &lt;code&gt;codex&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;For admins only.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Make sure you are on &lt;code&gt;main&lt;/code&gt; and have no local changes. Then run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;VERSION=0.2.0  # Can also be 0.2.0-alpha.1 or any valid Rust version.
./codex-rs/scripts/create_github_release.sh "$VERSION"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will make a local commit on top of &lt;code&gt;main&lt;/code&gt; with &lt;code&gt;version&lt;/code&gt; set to &lt;code&gt;$VERSION&lt;/code&gt; in &lt;code&gt;codex-rs/Cargo.toml&lt;/code&gt; (note that on &lt;code&gt;main&lt;/code&gt;, we leave the version as &lt;code&gt;version = "0.0.0"&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;This will push the commit using the tag &lt;code&gt;rust-v${VERSION}&lt;/code&gt;, which in turn kicks off &lt;a href="https://raw.githubusercontent.com/openai/codex/main/.github/workflows/rust-release.yml"&gt;the release workflow&lt;/a&gt;. This will create a new GitHub Release named &lt;code&gt;$VERSION&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;If everything looks good in the generated GitHub Release, uncheck the &lt;strong&gt;pre-release&lt;/strong&gt; box so it is the latest release.&lt;/p&gt; 
&lt;p&gt;Create a PR to update &lt;a href="https://github.com/Homebrew/homebrew-core/raw/main/Formula/c/codex.rb"&gt;&lt;code&gt;Formula/c/codex.rb&lt;/code&gt;&lt;/a&gt; on Homebrew.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Security &amp;amp; responsible AI&lt;/h2&gt; 
&lt;p&gt;Have you discovered a vulnerability or have concerns about model output? Please e-mail &lt;strong&gt;&lt;a href="mailto:security@openai.com"&gt;security@openai.com&lt;/a&gt;&lt;/strong&gt; and we will respond promptly.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This repository is licensed under the &lt;a href="https://raw.githubusercontent.com/openai/codex/main/LICENSE"&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>foundry-rs/foundry</title>
      <link>https://github.com/foundry-rs/foundry</link>
      <description>&lt;p&gt;Foundry is a blazing fast, portable and modular toolkit for Ethereum application development written in Rust.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/foundry-rs/foundry/master/.github/assets/banner.png" alt="Foundry banner" /&gt; 
 &lt;p&gt;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/foundry-rs/foundry/actions"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/foundry-rs/foundry/test.yml?branch=master" alt="Github Actions" /&gt;&lt;/a&gt; &lt;a href="https://t.me/foundry_rs"&gt;&lt;img src="https://img.shields.io/endpoint?color=neon&amp;amp;logo=telegram&amp;amp;label=chat&amp;amp;style=flat-square&amp;amp;url=https%3A%2F%2Ftg.sumanjay.workers.dev%2Ffoundry_rs" alt="Telegram Chat" /&gt;&lt;/a&gt; &lt;a href="https://t.me/foundry_support"&gt;&lt;img src="https://img.shields.io/endpoint?color=neon&amp;amp;logo=telegram&amp;amp;label=support&amp;amp;style=flat-square&amp;amp;url=https%3A%2F%2Ftg.sumanjay.workers.dev%2Ffoundry_support" alt="Telegram Support" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/Foundry-grey?style=flat&amp;amp;logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAElElEQVR4nH1VUUhUaRg9984YdzBpkqR0Z210rIESIXSabEbcHgydrpNRRj00kWaztj0U1MOW0MOIbD300IvLMqBpMTGYxdoqyoRNDUESBDWwUuPugCSSsTM7u0Oj1/+efdiMcmnP2/fDd77D4f/OB6xCa2urQZbllVICYGtqanK1tLS4AdgAyAAgyzJaW1sNq/ulT4twOGw4fPiwAGDp7Ow8VV1d7bVarRWxWCw/k8mgsbExm0wmZ+Lx+M/Xr1//CcAsSVmSJH01McLhsAEAnE5nx+Tk5B/xeJxOp5N9fX2sqqqixWLhnTt36HA4GIvFGI1GU3V1df5Pe/9D1t7eHkgkEuzo6GBPT49WWloq7Ha7fujQITocDu7atUs3m83i6tWr2okTJ/jixQuePn265zPScDhskGUZe/fubXv8+DFv3rypbdiwQaxbt46RSIT79u3j0NAQb926RVVVOT4+TqvVyvz8fD0YDC5NTk6ysbHxlCRJ/5KSlAAURyKRTFNTkwAg7t69S5/Px76+Pq7GyMgI9+/fz9HRUQIQO3bsEKOjo38DsJCUJADw+/0BVVW7otHo8ps3b4yvXr3CxMQETCYTTCYTNE0DAOTl5SGXy0FRFOzZswdmsxkVFRXLNTU1xmg0+kNvb+/3AGAcGBiI7969Wwcg6urq+OTJE967d49btmzh9PT0R3WJRIKBQIDBYJBTU1NsaGggAGGz2fTe3t5fAeQZAWwuLi4uP3nypOT1emEwGFBeXo7a2losLCygoaEB/f39MJlMCIVCkCQJBw8ehNVqhcfjQXNzs1RSUiKtX7++DEAZqqqq3KFQiABYUFDAM2fOkCQXFxdJkvfv32dhYSG9Xi+vXbvG2dnZj4oDgQCLioqoKAqHhobodDq/Mc7NzUklJSUIBoOw2WzYtm0blpeXsWbNGkxMTODp06doa2vD4OAgNm7cCIvFApLQdR3nzp3Dzp078fLlSxQVFeHdu3cAgIpHjx69/zBUX5k+MDBAt9vNY8eOsbu7m6lUigcOHKDL5WImkyHJz9TGYrEcALsMIPn69esZTdMIgM+ePUNXVxdu376NsrIyuN1uXLp0CWazGcPDw3C5XFBVFWfPnkVNTQ18Pp+ezWY5MzPzO4DfAABHjhzpJslUKqVdvHiR4+PjbG9vZy6XI0kuLS0xmUxSCEGS9Pv9LC0tpdFoZGVlpSaEoM/nuwIAKx/7q5GRkb9CoZBQVVWcP3+ez58/J0mm02kODg7ywoULjMViTKfTtNvtXLt2LTdt2qTncrnlsbGxLICvSUqfrl5HJBLh1NTUkhBCJ8mFhQX29/dTVVUWFBTwwYMH1HWdly9fpqIoeiKRWJqfn2d1dXWnLMuf7zMAHD16tGd+fn7FZy2bzYrKykodAAFQVVV9cXFRkNTevn3Lubk5trS0XPnfxHE4HN8ODw+nV/yanp6mx+Ohx+P5aIMQgmNjY3/W1tZ+t5rsSwG7+fjx4/76+vrm7du32woLC00AkE6n38fj8ZmHDx/+cuPGjR8BJL8YsCtYdQIMALYqilKvKEo9APuHty+egH8A3GfFDJXmxmMAAAAASUVORK5CYII%3D&amp;amp;link=https%3A%2F%2Fbook.getfoundry.sh%2F" alt="Foundry" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://getfoundry.sh/getting-started/installation"&gt;Install&lt;/a&gt;&lt;/strong&gt; | &lt;a href="https://getfoundry.sh"&gt;Docs&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/docs/dev/README.md"&gt;Developer Guidelines&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/CONTRIBUTING.md"&gt;Contributing&lt;/a&gt; | &lt;a href="https://foundry-rs.github.io/foundry"&gt;Crate Docs&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Foundry is a blazing fast, portable and modular toolkit for Ethereum application development written in Rust.&lt;/h3&gt; 
&lt;p&gt;Foundry consists of:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/#forge"&gt;&lt;strong&gt;Forge&lt;/strong&gt;&lt;/a&gt;: Build, test, fuzz, debug and deploy &lt;a href="https://soliditylang.org/"&gt;Solidity&lt;/a&gt; contracts, like Hardhat, Brownie, Ape.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/#cast"&gt;&lt;strong&gt;Cast&lt;/strong&gt;&lt;/a&gt;: A Swiss Army knife for interacting with EVM smart contracts, sending transactions and getting chain data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/#anvil"&gt;&lt;strong&gt;Anvil&lt;/strong&gt;&lt;/a&gt;: Fast local Ethereum development node, akin to Hardhat Network, Tenderly.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/#chisel"&gt;&lt;strong&gt;Chisel&lt;/strong&gt;&lt;/a&gt;: Fast, utilitarian, and verbose Solidity REPL.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Need help getting started with Foundry? Read the &lt;a href="https://getfoundry.sh"&gt;📖 Foundry Docs&lt;/a&gt;!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/foundry-rs/foundry/master/.github/assets/demo.gif" alt="Demo" /&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;High-Performance Compilation&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Fast and Flexible&lt;/strong&gt;: Automatically detects and installs the required Solidity compiler version.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Solidity and Vyper Support&lt;/strong&gt;: Fully supports both Solidity and Vyper out-of-the-box.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Incremental Compilation&lt;/strong&gt;: Re-compiles only changed files, saving time.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Parallelized Pipeline&lt;/strong&gt;: Leverages multi-core systems for ultra-fast builds.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Broad Compatibility&lt;/strong&gt;: Supports non-standard directory structures, including &lt;a href="https://twitter.com/gakonst/status/1461289225337421829"&gt;Hardhat repos&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Advanced Testing&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;No Context Switching&lt;/strong&gt;: Write tests directly in Solidity.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Fuzz Testing&lt;/strong&gt;: Quickly identify edge cases with input shrinking and counter-example generation.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Invariant Testing&lt;/strong&gt;: Ensure complex system properties hold across a wide range of inputs.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Debugging Made Easy&lt;/strong&gt;: Use &lt;a href="https://github.com/foundry-rs/forge-std"&gt;forge-std&lt;/a&gt;'s &lt;code&gt;console.sol&lt;/code&gt; for flexible debug logging.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Interactive Debugger&lt;/strong&gt;: Step through your Solidity code with Foundry's interactive debugger, making it easy to pinpoint issues.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Powerful Runtime Features&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;RPC Forking&lt;/strong&gt;: Fast and efficient remote RPC forking backed by &lt;a href="https://github.com/alloy-rs/alloy"&gt;Alloy&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Lightweight &amp;amp; Portable&lt;/strong&gt;: No dependency on Nix or other package managers for installation.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Streamlined CI/CD&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Optimized CI&lt;/strong&gt;: Accelerate builds, run tests and execute scripts using &lt;a href="https://github.com/foundry-rs/foundry-toolchain"&gt;Foundry's GitHub action&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Getting started is very easy:&lt;/p&gt; 
&lt;p&gt;Install &lt;code&gt;foundryup&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;curl -L https://foundry.paradigm.xyz | bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Next, run &lt;code&gt;foundryup&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;It will automatically install the latest version of the precompiled binaries: &lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/#forge"&gt;&lt;code&gt;forge&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/#cast"&gt;&lt;code&gt;cast&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/#anvil"&gt;&lt;code&gt;anvil&lt;/code&gt;&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/#chisel"&gt;&lt;code&gt;chisel&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;foundryup
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Done!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;For additional details see the &lt;a href="https://getfoundry.sh/getting-started/installation"&gt;installation guide&lt;/a&gt; in the &lt;a href="https://getfoundry.sh"&gt;Foundry Docs&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you're experiencing any issues while installing, check out &lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/#getting-help"&gt;Getting Help&lt;/a&gt; and the &lt;a href="https://getfoundry.sh/faq"&gt;FAQ&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;How Fast?&lt;/h2&gt; 
&lt;p&gt;Forge is quite fast at both compiling (leveraging &lt;code&gt;solc&lt;/code&gt; with &lt;a href="https://github.com/foundry-rs/compilers"&gt;foundry-compilers&lt;/a&gt;) and testing.&lt;/p&gt; 
&lt;p&gt;See the benchmarks below. Older benchmarks against &lt;a href="https://github.com/dapphub/dapptools"&gt;DappTools&lt;/a&gt; can be found in the &lt;a href="https://www.paradigm.xyz/2022/03/foundry-02#blazing-fast-compilation--testing"&gt;v0.2.0 announcement post&lt;/a&gt; and in the &lt;a href="https://github.com/mds1/convex-shutdown-simulation"&gt;Convex Shutdown Simulation&lt;/a&gt; repository.&lt;/p&gt; 
&lt;h3&gt;Testing Benchmarks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Project&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://github.com/foundry-rs/foundry/releases/tag/nightly-59f354c179f4e7f6d7292acb3d068815c79286d1"&gt;Forge 1.0&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://github.com/foundry-rs/foundry/releases/tag/nightly-5b7e4cb3c882b28f3c32ba580de27ce7381f415a"&gt;Forge 0.2&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;DappTools&lt;/th&gt; 
   &lt;th&gt;Speedup&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Vectorized/solady"&gt;vectorized/solady&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Unit / Fuzz&lt;/td&gt; 
   &lt;td&gt;0.9s&lt;/td&gt; 
   &lt;td&gt;2.3s&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;2.6x&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/morpho-org/morpho-blue"&gt;morpho-org/morpho-blue&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Invariant&lt;/td&gt; 
   &lt;td&gt;0.7s&lt;/td&gt; 
   &lt;td&gt;1m43s&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;147.1x&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/morpho-org/morpho-blue"&gt;morpho-org/morpho-blue-oracles&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Integration (Cold)&lt;/td&gt; 
   &lt;td&gt;6.1s&lt;/td&gt; 
   &lt;td&gt;6.3s&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;1.04x&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/morpho-org/morpho-blue"&gt;morpho-org/morpho-blue-oracles&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Integration (Cached)&lt;/td&gt; 
   &lt;td&gt;0.6s&lt;/td&gt; 
   &lt;td&gt;0.9s&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;1.50x&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/transmissions11/solmate/"&gt;transmissions11/solmate&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Unit / Fuzz&lt;/td&gt; 
   &lt;td&gt;2.7s&lt;/td&gt; 
   &lt;td&gt;2.8s&lt;/td&gt; 
   &lt;td&gt;6m34s&lt;/td&gt; 
   &lt;td&gt;1.03x / 140.0x&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/reflexer-labs/geb"&gt;reflexer-labs/geb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Unit / Fuzz&lt;/td&gt; 
   &lt;td&gt;0.2s&lt;/td&gt; 
   &lt;td&gt;0.4s&lt;/td&gt; 
   &lt;td&gt;23s&lt;/td&gt; 
   &lt;td&gt;2.0x / 57.5x&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;em&gt;In the above benchmarks, compilation was always skipped&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Takeaway: Forge dramatically outperforms the competition, delivering blazing-fast execution speeds while continuously expanding its robust feature set.&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Compilation Benchmarks&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset=".github/assets/build_benchmark_solady_dark.png" width="600px" /&gt; 
  &lt;img src="https://raw.githubusercontent.com/foundry-rs/foundry/master/.github/assets/build_benchmark_solady_light.png" width="600px" /&gt; 
 &lt;/picture&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset=".github/assets/build_benchmark_openzeppelin_dark.png" width="600px" /&gt; 
  &lt;img src="https://raw.githubusercontent.com/foundry-rs/foundry/master/.github/assets/build_benchmark_openzeppelin_light.png" width="600px" /&gt; 
 &lt;/picture&gt; 
 &lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Takeaway: Forge compilation is consistently faster than Hardhat by a factor of &lt;code&gt;2.1x&lt;/code&gt; to &lt;code&gt;5.2x&lt;/code&gt;, depending on the amount of caching involved.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Forge&lt;/h2&gt; 
&lt;p&gt;Forge helps you build, test, fuzz, debug and deploy Solidity contracts.&lt;/p&gt; 
&lt;p&gt;The best way to understand Forge is to simply try it (in less than 30 seconds!).&lt;/p&gt; 
&lt;p&gt;First, let's initialize a new &lt;code&gt;counter&lt;/code&gt; example repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;forge init counter
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Next &lt;code&gt;cd&lt;/code&gt; into &lt;code&gt;counter&lt;/code&gt; and build :&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;forge build
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;[⠊] Compiling...
[⠔] Compiling 27 files with Solc 0.8.28
[⠒] Solc 0.8.28 finished in 452.13ms
Compiler run successful!
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Let's &lt;a href="https://getfoundry.sh/forge/tests#tests"&gt;test&lt;/a&gt; our contracts:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;forge test
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;[⠊] Compiling...
No files changed, compilation skipped

Ran 2 tests for test/Counter.t.sol:CounterTest
[PASS] testFuzz_SetNumber(uint256) (runs: 256, μ: 31121, ~: 31277)
[PASS] test_Increment() (gas: 31293)
Suite result: ok. 2 passed; 0 failed; 0 skipped; finished in 5.35ms (4.86ms CPU time)

Ran 1 test suite in 5.91ms (5.35ms CPU time): 2 tests passed, 0 failed, 0 skipped (2 total tests)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Finally, let's run our deployment script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;forge script script/Counter.s.sol
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;[⠊] Compiling...
No files changed, compilation skipped
Script ran successfully.
Gas used: 109037

If you wish to simulate on-chain transactions pass a RPC URL.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run &lt;code&gt;forge --help&lt;/code&gt; to explore the full list of available subcommands and their usage.&lt;/p&gt; 
&lt;p&gt;More documentation can be found in the &lt;a href="https://getfoundry.sh/forge/overview"&gt;forge&lt;/a&gt; section of the Foundry Docs.&lt;/p&gt; 
&lt;h2&gt;Cast&lt;/h2&gt; 
&lt;p&gt;Cast is a Swiss Army knife for interacting with Ethereum applications from the command line.&lt;/p&gt; 
&lt;p&gt;Here are a few examples of what you can do:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Check the latest block on Ethereum Mainnet&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cast block-number --rpc-url https://eth.merkle.io
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Check the Ether balance of &lt;code&gt;vitalik.eth&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cast balance vitalik.eth --ether --rpc-url https://eth.merkle.io
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Replay and trace a transaction&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cast run 0x9c32042f5e997e27e67f82583839548eb19dc78c4769ad6218657c17f2a5ed31 --rpc-url https://eth.merkle.io
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Optionally, pass &lt;code&gt;--etherscan-api-key &amp;lt;API_KEY&amp;gt;&lt;/code&gt; to decode transaction traces using verified source maps, providing more detailed and human-readable information.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Run &lt;code&gt;cast --help&lt;/code&gt; to explore the full list of available subcommands and their usage.&lt;/p&gt; 
&lt;p&gt;More documentation can be found in the &lt;a href="https://getfoundry.sh/cast/overview"&gt;cast&lt;/a&gt; section of the Foundry Docs.&lt;/p&gt; 
&lt;h2&gt;Anvil&lt;/h2&gt; 
&lt;p&gt;Anvil is a fast local Ethereum development node.&lt;/p&gt; 
&lt;p&gt;Let's fork Ethereum mainnet at the latest block:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;anvil --fork-url https://eth.merkle.io
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can use those same &lt;code&gt;cast&lt;/code&gt; subcommands against your &lt;code&gt;anvil&lt;/code&gt; instance:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cast block-number
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;Run &lt;code&gt;anvil --help&lt;/code&gt; to explore the full list of available features and their usage.&lt;/p&gt; 
&lt;p&gt;More documentation can be found in the &lt;a href="https://getfoundry.sh/anvil/overview"&gt;anvil&lt;/a&gt; section of the Foundry Docs.&lt;/p&gt; 
&lt;h2&gt;Chisel&lt;/h2&gt; 
&lt;p&gt;Chisel is a fast, utilitarian, and verbose Solidity REPL.&lt;/p&gt; 
&lt;p&gt;To use Chisel, simply type &lt;code&gt;chisel&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;chisel
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;From here, start writing Solidity code! Chisel will offer verbose feedback on each input.&lt;/p&gt; 
&lt;p&gt;Create a variable &lt;code&gt;a&lt;/code&gt; and query it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;➜ uint256 a = 123;
➜ a
Type: uint256
├ Hex: 0x7b
├ Hex (full word): 0x000000000000000000000000000000000000000000000000000000000000007b
└ Decimal: 123
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Finally, run &lt;code&gt;!source&lt;/code&gt; to see &lt;code&gt;a&lt;/code&gt; was applied:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-solidity"&gt;// SPDX-License-Identifier: UNLICENSED
pragma solidity ^0.8.28;

import {Vm} from "forge-std/Vm.sol";

contract REPL {
    Vm internal constant vm = Vm(address(uint160(uint256(keccak256("hevm cheat code")))));

    /// @notice REPL contract entry point
    function run() public {
        uint256 a = 123;
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;Run &lt;code&gt;chisel --help&lt;/code&gt; to explore the full list of available features and their usage.&lt;/p&gt; 
&lt;p&gt;More documentation can be found in the &lt;a href="https://getfoundry.sh/chisel/overview"&gt;chisel&lt;/a&gt; section of the Foundry Docs.&lt;/p&gt; 
&lt;h2&gt;Configuration&lt;/h2&gt; 
&lt;p&gt;Foundry is highly configurable, allowing you to tailor it to your needs. Configuration is managed via a file called &lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/crates/config"&gt;&lt;code&gt;foundry.toml&lt;/code&gt;&lt;/a&gt; located in the root of your project or any parent directory. For a full list of configuration options, refer to the &lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/crates/config/README.md#all-options"&gt;config package documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Profiles and Namespaces&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Configuration can be organized into &lt;strong&gt;profiles&lt;/strong&gt;, which are arbitrarily namespaced for flexibility.&lt;/li&gt; 
 &lt;li&gt;The default profile is named &lt;code&gt;default&lt;/code&gt;. Learn more in the &lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/crates/config/README.md#default-profile"&gt;Default Profile section&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;To select a different profile, set the &lt;code&gt;FOUNDRY_PROFILE&lt;/code&gt; environment variable.&lt;/li&gt; 
 &lt;li&gt;Override specific settings using environment variables prefixed with &lt;code&gt;FOUNDRY_&lt;/code&gt; (e.g., &lt;code&gt;FOUNDRY_SRC&lt;/code&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;You can find additional &lt;a href="https://getfoundry.sh/config/overview"&gt;setup and configurations guides&lt;/a&gt; in the &lt;a href="https://getfoundry.sh"&gt;Foundry Docs&lt;/a&gt; and in the &lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/crates/config/README.md"&gt;config crate&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://getfoundry.sh/config/overview"&gt;Configuring with &lt;code&gt;foundry.toml&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://getfoundry.sh/config/vscode.html"&gt;Setting up VSCode&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://getfoundry.sh/config/shell-autocompletion.html"&gt;Shell autocompletions&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See our &lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting Help&lt;/h2&gt; 
&lt;p&gt;First, see if the answer to your question can be found in the &lt;a href="https://getfoundry.sh"&gt;Foundry Docs&lt;/a&gt;, or in the relevant crate.&lt;/p&gt; 
&lt;p&gt;If the answer is not there:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Join the &lt;a href="https://t.me/foundry_support"&gt;support Telegram&lt;/a&gt; to get help, or&lt;/li&gt; 
 &lt;li&gt;Open a &lt;a href="https://github.com/foundry-rs/foundry/discussions/new"&gt;discussion&lt;/a&gt; with your question, or&lt;/li&gt; 
 &lt;li&gt;Open an issue with &lt;a href="https://github.com/foundry-rs/foundry/issues/new"&gt;the bug&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you want to contribute, or follow along with contributor discussion, you can use our &lt;a href="https://t.me/foundry_rs"&gt;main telegram&lt;/a&gt; to chat with us about the development of Foundry!&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Licensed under either of &lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/LICENSE-APACHE"&gt;Apache License&lt;/a&gt;, Version 2.0 or &lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/LICENSE-MIT"&gt;MIT License&lt;/a&gt; at your option.&lt;/p&gt; 
&lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in these crates by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Foundry is a clean-room rewrite of the testing framework &lt;a href="https://github.com/dapphub/dapptools"&gt;DappTools&lt;/a&gt;. None of this would have been possible without the DappHub team's work over the years.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/mattsse_"&gt;Matthias Seitz&lt;/a&gt;: Created &lt;a href="https://github.com/gakonst/ethers-rs/tree/master/ethers-solc/"&gt;ethers-solc&lt;/a&gt; (now &lt;a href="https://github.com/foundry-rs/compilers"&gt;foundry-compilers&lt;/a&gt;) which is the backbone of our compilation pipeline, as well as countless contributions to ethers, in particular the &lt;code&gt;abigen&lt;/code&gt; macros.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/rohitnarurkar"&gt;Rohit Narurkar&lt;/a&gt;: Created the Rust Solidity version manager &lt;a href="https://github.com/roynalnaruto/svm-rs"&gt;svm-rs&lt;/a&gt; which we use to auto-detect and manage multiple Solidity versions.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/brockjelmore"&gt;Brock Elmore&lt;/a&gt;: For extending the VM's cheatcodes and implementing &lt;a href="https://github.com/foundry-rs/foundry/pull/192"&gt;structured call tracing&lt;/a&gt;, a critical feature for debugging smart contract calls.&lt;/li&gt; 
 &lt;li&gt;All the other &lt;a href="https://github.com/foundry-rs/foundry/graphs/contributors"&gt;contributors&lt;/a&gt; to the &lt;a href="https://github.com/gakonst/ethers-rs"&gt;ethers-rs&lt;/a&gt;, &lt;a href="https://github.com/alloy-rs/alloy"&gt;alloy&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/foundry-rs/foundry"&gt;foundry&lt;/a&gt; repositories and chatrooms.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>eythaann/Seelen-UI</title>
      <link>https://github.com/eythaann/Seelen-UI</link>
      <description>&lt;p&gt;The Fully Customizable Desktop Environment for Windows 10/11.&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; &lt;img src="https://raw.githubusercontent.com/eythaann/Seelen-UI/master/documentation/images/logo.svg?sanitize=true" width="44" align="top" alt="Seelen UI Logo" /&gt; Seelen UI &lt;/h1&gt; 
&lt;h2 align="center"&gt; Fully Customizable Desktop Environment for Windows &lt;br /&gt; Available in 70+ Languages &lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/eythaann/seelen-ui/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/eythaann/seelen-ui.svg?sanitize=true" alt="Contributors" /&gt;&lt;/a&gt; &lt;a href="https://github.com/eythaann/seelen-ui/commits/main"&gt;&lt;img src="https://img.shields.io/github/last-commit/eythaann/seelen-ui.svg?sanitize=true" alt="Last Commit" /&gt;&lt;/a&gt; &lt;a href="https://github.com/eythaann/seelen-ui/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/eythaann/seelen-ui.svg?sanitize=true" alt="Version" /&gt;&lt;/a&gt; &lt;a href="https://github.com/eythaann/seelen-ui/releases"&gt;&lt;img src="https://img.shields.io/github/downloads/eythaann/seelen-ui/total.svg?sanitize=true" alt="Downloads" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;img src="https://raw.githubusercontent.com/eythaann/Seelen-UI/master/documentation/images/preview.png" width="100%" alt="Screenshot of Seelen UI desktop showing a customized desktop environment" /&gt; 
&lt;table align="center"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center" width="33%"&gt; &lt;a href="https://apps.microsoft.com/detail/Seelen%20UI/9p67c2d4t9fb?mode=full" target="_blank" rel="noopener noreferrer" aria-label="Download Seelen UI from Microsoft Store"&gt; &lt;img src="https://get.microsoft.com/images/en-us%20dark.svg?sanitize=true" width="100%" alt="Download Seelen UI from Microsoft Store" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="center" width="33%"&gt; &lt;a href="https://discord.gg/ABfASx5ZAJ" target="_blank" rel="noopener noreferrer" aria-label="Join the Seelen UI Discord community"&gt; &lt;img src="https://raw.githubusercontent.com/eythaann/Seelen-UI/master/documentation/images/discord-alt.png" width="100%" alt="Join the Seelen UI Discord community" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="center" width="33%"&gt; &lt;a href="https://www.digitalocean.com/?refcode=955c7335abf5&amp;amp;utm_campaign=Referral_Invite&amp;amp;utm_medium=Referral_Program&amp;amp;utm_source=badge" target="_blank" rel="noopener noreferrer" aria-label="DigitalOcean Referral Badge"&gt; &lt;img src="https://web-platforms.sfo2.cdn.digitaloceanspaces.com/WWW/Badge%201.svg?sanitize=true" width="100%" alt="DigitalOcean Referral Badge" /&gt; &lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://seelen.io/apps/seelen-ui"&gt;Seelen UI&lt;/a&gt; is a tool designed to enhance your Windows desktop experience with a focus on customization and productivity. It integrates smoothly into your system, providing a range of features that allow you to personalize your desktop and optimize your workflow.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Be Creative&lt;/strong&gt;: Seelen UI lets you tailor your desktop to fit your style and needs. You can adjust menus, widgets, icons, and other elements to create a personalized and visually appealing desktop environment.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/eythaann/Seelen-UI/master/documentation/images/theme_preview.png" alt="Seelen UI Custom Theme" /&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Enhance Your Productivity&lt;/strong&gt;: Seelen UI helps you organize your desktop efficiently. With a Tiling Windows Manager, windows automatically arrange themselves to support multitasking, making your work more streamlined.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/eythaann/Seelen-UI/master/documentation/images/twm_preview.png" alt="Seelen UI Tiling Window Manager" /&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Enjoy your music&lt;/strong&gt;: With an integrated media module that's compatible with most music players, Seelen UI allows you to enjoy your music seamlessly. You can pause, resume, and skip tracks at any time without the need to open additional windows.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/eythaann/Seelen-UI/master/documentation/images/media_module_preview.png" alt="Seelen UI Media Module" /&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Be faster!&lt;/strong&gt;: With an app launcher inspired by Rofi, Seelen UI provides a simple and intuitive way to quickly access your applications and execute commands.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/eythaann/Seelen-UI/master/documentation/images/app_launcher_preview.png" alt="Seelen UI App Launcher" /&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;User-Friendly Configuration&lt;/strong&gt;: Seelen UI offers an intuitive interface for easy customization. Adjust settings such as themes, taskbar layouts, icons, etc. With just a few clicks.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/eythaann/Seelen-UI/master/documentation/images/settings_preview.png" alt="Seelen UI Settings" /&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!CAUTION] Seelen UI requires the WebView runtime to be installed. On Windows 11, it comes pre-installed with the system. However, on Windows 10, the WebView runtime is included with the &lt;code&gt;setup.exe&lt;/code&gt; installer. Additionally, Microsoft Edge is necessary to function correctly. Some users may have modified their system and removed Edge, so please ensure both Edge and the WebView runtime are installed on your system.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] On fresh installations of Windows, the app might show a white or dark screen. You only need to update your Windows through Windows Update and restart your PC.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;You can choose from different installation options based on your preference:&lt;/p&gt; 
&lt;h3&gt;Microsoft Store &lt;em&gt;(recommended)&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;Download the latest version from the &lt;a href="https://www.microsoft.com/store/productId/9P67C2D4T9FB?ocid=pdpshare"&gt;Store&lt;/a&gt; page. This is the recommended option because you will receive updates and a secure version of the program.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/em&gt;: It may take around 1 to 3 business days for changes to be reflected in the Microsoft Store, as updates are approved by real people in the store.&lt;/p&gt; 
&lt;h3&gt;Winget&lt;/h3&gt; 
&lt;p&gt;Install the latest version using:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-pwsh"&gt;winget install --id Seelen.SeelenUI
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This option also uses the signed &lt;code&gt;.msix&lt;/code&gt; package and ensures you have the latest secure version. Similar to the Microsoft Store, it may take around 1 to 3 business days for changes to be reflected in Winget, as updates are approved by real people in the &lt;code&gt;winget-pkg&lt;/code&gt; project.&lt;/p&gt; 
&lt;h3&gt;.msix Installer&lt;/h3&gt; 
&lt;p&gt;Download the &lt;code&gt;.msix&lt;/code&gt; installer from the &lt;a href="https://github.com/eythaann/seelen-ui/releases"&gt;Releases&lt;/a&gt; page. This package is signed, ensuring a secure installation. This is the same option as the Microsoft Store but is a portable installer.&lt;/p&gt; 
&lt;h3&gt;.exe Installer&lt;/h3&gt; 
&lt;p&gt;Download the latest version from the &lt;a href="https://github.com/eythaann/seelen-ui/releases"&gt;Releases&lt;/a&gt; page and run the &lt;code&gt;setup.exe&lt;/code&gt; installer. This option is less recommended as the installer is not signed, which may cause it to be flagged as a potential threat by some antivirus programs. The &lt;code&gt;setup.exe&lt;/code&gt; is updated more quickly than the Microsoft Store or Winget versions and also it receives notifications updates on new release.&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;Once installed or extracted, simply open the program. The easy-to-use and intuitive GUI will guide you through the configuration process. Customize your desktop environment effortlessly.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;For in-depth details on various aspects of Seelen UI, explore the following documents:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eythaann/Seelen-UI/master/documentation/languages.md"&gt;Languages&lt;/a&gt; - Information regarding translations.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eythaann/Seelen-UI/master/documentation/toolbar.md"&gt;Toolbar&lt;/a&gt; - Details about customizing and using the toolbar.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://seelen.io/blog/seelen-ui-theme-tutorial"&gt;Themes&lt;/a&gt; - Guidance on creating and applying themes.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eythaann/Seelen-UI/master/documentation/window_manager.md"&gt;Window Manager&lt;/a&gt; - Instructions on configuring the window manager.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eythaann/Seelen-UI/master/documentation/project.md"&gt;Project&lt;/a&gt; - General information about the project and its structure.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Upcoming Features&lt;/h2&gt; 
&lt;p&gt;I’m excited to share some upcoming features for Seelen UI! Here’s a glimpse of what’s planned for the future:&lt;/p&gt; 
&lt;h3&gt;&lt;del&gt;App Launcher&lt;/del&gt; ✅&lt;/h3&gt; 
&lt;p&gt;I’m planning to develop an app launcher inspired by &lt;a href="https://github.com/davatorium/rofi"&gt;Rofi&lt;/a&gt; on Linux. This feature will provide a sleek and highly customizable way to quickly access your applications.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/adi1090x/files/master/rofi/previews/colorful/main.gif" alt="App Launcher Preview" /&gt; &lt;em&gt;Image courtesy of &lt;a href="https://github.com/dctxmei/rofi-themes"&gt;rofi-themes&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Customizable Popup Widgets&lt;/h3&gt; 
&lt;p&gt;I aim to introduce a set of fully customizable popup widgets, similar to the features available in &lt;a href="https://github.com/elkowar/eww"&gt;EWW&lt;/a&gt;. These widgets will be highly configurable and adaptable to your needs, providing an enhanced and interactive way to manage your desktop environment.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/adi1090x/widgets/main/previews/dashboard.png" alt="Customizable Widgets Preview" /&gt; &lt;em&gt;Image courtesy of &lt;a href="https://github.com/adi1090x/widgets"&gt;adi1090x&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Custom Alt + Tab (Task Switching)&lt;/h3&gt; 
&lt;p&gt;An upgraded Alt + Tab system for task switching is on the horizon. This will offer a more visually appealing and functional experience, allowing for smoother transitions between open applications and windows.&lt;/p&gt; 
&lt;h3&gt;Custom Virtual Desktops Viewer and Animations&lt;/h3&gt; 
&lt;p&gt;I’m also working on a custom virtual desktops viewer and dynamic animations to improve navigation between different workspaces. This will provide a more intuitive and immersive multitasking experience.&lt;/p&gt; 
&lt;p&gt;Stay tuned for more updates as I develop these features. I appreciate your support and enthusiasm!&lt;/p&gt; 
&lt;p&gt;Happy customizing!&lt;/p&gt; 
&lt;p&gt;The Seelen UI Team&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Read the &lt;a href="https://raw.githubusercontent.com/eythaann/Seelen-UI/master/CONTRIBUTING"&gt;Contribution Guidelines&lt;/a&gt; to get started with terms.&lt;/li&gt; 
 &lt;li&gt;Read the &lt;a href="https://raw.githubusercontent.com/eythaann/Seelen-UI/master/documentation/project.md"&gt;Project Documentation&lt;/a&gt; to understand the project structure and how to use it.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/eythaann/Seelen-UI/master/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;Contact&lt;/h2&gt; 
&lt;p&gt;For inquiries and support, please contact me on &lt;a href="https://discord.gg/ABfASx5ZAJ"&gt;Discord&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;See you later&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;                   .      .&amp;amp;     _,x&amp;amp;"``
                    &amp;amp; .   &amp;amp;'  ;.&amp;amp;&amp;amp;'
              &amp;amp;.  . &amp;amp;.&amp;amp;     .0&amp;amp;&amp;amp;&amp;amp;;&amp;amp;""`
         .    '&amp;amp;  &amp;amp;.&amp;amp;&amp;amp;&amp;amp;  .&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;'
       .&amp;amp;         ;&amp;amp;&amp;amp;&amp;amp; &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;'
      &amp;amp;&amp;amp;          &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;     &amp;amp;&amp;amp;&amp;amp;
     0&amp;amp;    .     &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;""
    &amp;amp;&amp;amp;   .0     &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;
   0&amp;amp;&amp;amp; .&amp;amp;'     &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;
  :&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;    . &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp; 
  0&amp;amp;&amp;amp;&amp;amp;&amp;amp;    &amp;amp; &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;
  &amp;amp;&amp;amp;&amp;amp;&amp;amp;'   &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;               .&amp;amp;&amp;amp;&amp;amp;x&amp;amp;
  &amp;amp;&amp;amp;&amp;amp;&amp;amp;   :&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;0.&amp;amp;'        , .&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;;.
  &amp;amp;&amp;amp;&amp;amp;&amp;amp;.  &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;        .&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;'               .
  0&amp;amp;&amp;amp;&amp;amp;&amp;amp;  &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;       ,&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;                &amp;amp;
  :&amp;amp;&amp;amp;&amp;amp;&amp;amp;; &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;0       ,;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;             ;  .0
   0&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;0     ,;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;             &amp;amp;  &amp;amp;;
    0&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;0   :',;".&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;".&amp;amp;             &amp;amp;&amp;amp; &amp;amp;0
     0&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;0  ',;',&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;" ,&amp;amp;'             &amp;amp;&amp;amp;&amp;amp;&amp;amp;0
      0&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;0 ,x&amp;amp;&amp;amp;&amp;amp;&amp;amp;" .&amp;amp;&amp;amp;&amp;amp;              &amp;amp;&amp;amp;&amp;amp;&amp;amp;0
        0&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp; .&amp;amp;&amp;amp;&amp;amp;&amp;amp;"'''"&amp;amp;&amp;amp;"&amp;amp;&amp;amp;            &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;0
         0&amp;amp;&amp;amp; .&amp;amp;&amp;amp;;``       `&amp;amp;: :&amp;amp;         &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;0
            &amp;amp;"' &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;   &amp;amp;"&amp;amp; &amp;amp;"&amp;amp;   &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;0
              0&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;0
                 0&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;0         Seelen
                      0&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;0
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;📌 &lt;strong&gt;Official Website&lt;/strong&gt;: &lt;a href="https://seelen.io"&gt;https://seelen.io&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Seelen Inc © 2025 - All rights reserved&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>facebookincubator/below</title>
      <link>https://github.com/facebookincubator/below</link>
      <description>&lt;p&gt;A time traveling resource monitor for modern Linux systems&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="300" src="https://github.com/facebookincubator/below/raw/main/img/below_logo_horizontal.png" align="center" alt="Below" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a href="https://matrix.to/#/#below:matrix.org"&gt; &lt;img alt="Matrix chat" src="https://img.shields.io/matrix/below:matrix.org" /&gt; &lt;/a&gt; &lt;a href="https://github.com/facebookincubator/below/actions?query=workflow%3ACI+branch%3Amain+"&gt; &lt;img alt="CI" src="https://github.com/facebookincubator/below/workflows/CI/badge.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;code&gt;below&lt;/code&gt; is an interactive tool to view and record historical system data. It has support for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;information regarding hardware resource utilization&lt;/li&gt; 
 &lt;li&gt;viewing the cgroup hierarchy&lt;/li&gt; 
 &lt;li&gt;cgroup and process information&lt;/li&gt; 
 &lt;li&gt;pressure stall information (PSI)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;record&lt;/code&gt; mode to record system data&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;replay&lt;/code&gt; mode to replay historical system data&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;live&lt;/code&gt; mode to view live system data&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;dump&lt;/code&gt; subcommand to report script-friendly information (eg JSON, CSV, OpenMetrics, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;snapshot&lt;/code&gt; subcommand to create a replayable snapshot file of historical system data&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;below does &lt;strong&gt;not&lt;/strong&gt; have support for cgroup1.&lt;/p&gt; 
&lt;p&gt;The name "below" stems from the fact that the below developers rejected many of &lt;a href="https://linux.die.net/man/1/atop"&gt;atop&lt;/a&gt;'s design and style decisions.&lt;/p&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;a href="https://asciinema.org/a/355506"&gt; &lt;img src="https://asciinema.org/a/355506.svg?sanitize=true" width="500" /&gt; &lt;/a&gt; 
&lt;h2&gt;Installing&lt;/h2&gt; 
&lt;h3&gt;Fedora&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;below&lt;/code&gt; is packaged in Fedora as of Fedora 34, and can be installed with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;sudo dnf install below
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Optionally, the systemd service for persistent data collection can also be enabled with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;sudo systemctl enable --now below
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Alpine Linux&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;below&lt;/code&gt; is packaged in Alpine Linux - it's available in (upcoming) v3.17+ and Edge. It can be installed with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;sudo apk add below
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Optionally, the OpenRC service for persistent data collection can also be enabled with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;sudo rc-service below start
sudo rc-update add below
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Gentoo Linux&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;below&lt;/code&gt; is available in the &lt;a href="https://packages.gentoo.org/packages/sys-process/below"&gt;&lt;code&gt;sys-process/below&lt;/code&gt;&lt;/a&gt; package and can be installed with &lt;code&gt;emerge&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;sudo emerge sys-process/below
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installing from source&lt;/h2&gt; 
&lt;p&gt;First, install dependencies listed in &lt;a href="https://raw.githubusercontent.com/facebookincubator/below/main/docs/building.md"&gt;building.md&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;$ cargo install below
$ below --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For convenience, we also provide a Dockerfile and &lt;a href="https://hub.docker.com/r/below/below"&gt;pre-built images&lt;/a&gt; on Docker Hub. See &lt;a href="https://raw.githubusercontent.com/facebookincubator/below/main/docs/docker.md"&gt;docker.md&lt;/a&gt; for how to use them.&lt;/p&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;Live view of system:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;$ sudo below live
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run recording daemon:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;$ sudo cp ~/.cargo/bin/below /bin/below  # if using cargo-install
$ sudo cp etc/below.service /etc/systemd/system
$ sudo systemctl daemon-reload
$ sudo systemctl start below
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Replay historical data:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;$ below replay -t "3m ago"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Integration with Prometheus/Grafana&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;below&lt;/code&gt; has basic support for Prometheus/Grafana through the &lt;code&gt;dump&lt;/code&gt; interface.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/facebookincubator/below/main/contrib/grafana"&gt;contrib/grafana/&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Comparison with alternative tools&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/facebookincubator/below/main/docs/comparison.md"&gt;comparison.md&lt;/a&gt; for a feature comparison with alternative tools.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/facebookincubator/below/main/CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt; file for how to help out.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/facebookincubator/below/main/LICENSE"&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/edit</title>
      <link>https://github.com/microsoft/edit</link>
      <description>&lt;p&gt;We all edit.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src="https://raw.githubusercontent.com/microsoft/edit/main/assets/edit.svg?sanitize=true" alt="Application Icon for Edit" /&gt; Edit&lt;/h1&gt; 
&lt;p&gt;A simple editor for simple needs.&lt;/p&gt; 
&lt;p&gt;This editor pays homage to the classic &lt;a href="https://en.wikipedia.org/wiki/MS-DOS_Editor"&gt;MS-DOS Editor&lt;/a&gt;, but with a modern interface and input controls similar to VS Code. The goal is to provide an accessible editor that even users largely unfamiliar with terminals can easily use.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/microsoft/edit/main/assets/edit_hero_image.png" alt="Screenshot of Edit with the About dialog in the foreground" /&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://repology.org/project/microsoft-edit/versions"&gt;&lt;img src="https://repology.org/badge/vertical-allrepos/microsoft-edit.svg?exclude_unsupported=1" alt="Packaging status" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You can also download binaries from &lt;a href="https://github.com/microsoft/edit/releases/latest"&gt;our Releases page&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;p&gt;You can install the latest version with WinGet:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;winget install Microsoft.Edit
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Build Instructions&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.rust-lang.org/tools/install"&gt;Install Rust&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Install the nightly toolchain: &lt;code&gt;rustup install nightly&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Alternatively, set the environment variable &lt;code&gt;RUSTC_BOOTSTRAP=1&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Clone the repository&lt;/li&gt; 
 &lt;li&gt;For a release build, run: &lt;code&gt;cargo build --config .cargo/release.toml --release&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Build Configuration&lt;/h3&gt; 
&lt;p&gt;During compilation you can set various environment variables to configure the build. The following table lists the available configuration options:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Environment variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;EDIT_CFG_ICU*&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;See &lt;a href="https://raw.githubusercontent.com/microsoft/edit/main/#icu-library-name-soname"&gt;ICU library name (SONAME)&lt;/a&gt; for details.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;EDIT_CFG_LANGUAGES&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A comma-separated list of languages to include in the build. See &lt;a href="https://raw.githubusercontent.com/microsoft/edit/main/i18n/edit.toml"&gt;i18n/edit.toml&lt;/a&gt; for available languages.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Notes to Package Maintainers&lt;/h2&gt; 
&lt;h3&gt;Package Naming&lt;/h3&gt; 
&lt;p&gt;The canonical executable name is "edit" and the alternative name is "msedit". We're aware of the potential conflict of "edit" with existing commands and recommend alternatively naming packages and executables "msedit". Names such as "ms-edit" should be avoided. Assigning an "edit" alias is recommended, if possible.&lt;/p&gt; 
&lt;h3&gt;ICU library name (SONAME)&lt;/h3&gt; 
&lt;p&gt;This project &lt;em&gt;optionally&lt;/em&gt; depends on the ICU library for its Search and Replace functionality. By default, the project will look for a SONAME without version suffix:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Windows: &lt;code&gt;icuuc.dll&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;macOS: &lt;code&gt;libicuuc.dylib&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;UNIX, and other OS: &lt;code&gt;libicuuc.so&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If your installation uses a different SONAME, please set the following environment variable at build time:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;EDIT_CFG_ICUUC_SONAME&lt;/code&gt;: For instance, &lt;code&gt;libicuuc.so.76&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;EDIT_CFG_ICUI18N_SONAME&lt;/code&gt;: For instance, &lt;code&gt;libicui18n.so.76&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Additionally, this project assumes that the ICU exports are exported without &lt;code&gt;_&lt;/code&gt; prefix and without version suffix, such as &lt;code&gt;u_errorName&lt;/code&gt;. If your installation uses versioned exports, please set:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;EDIT_CFG_ICU_CPP_EXPORTS&lt;/code&gt;: If set to &lt;code&gt;true&lt;/code&gt;, it'll look for C++ symbols such as &lt;code&gt;_u_errorName&lt;/code&gt;. Enabled by default on macOS.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;EDIT_CFG_ICU_RENAMING_VERSION&lt;/code&gt;: If set to a version number, such as &lt;code&gt;76&lt;/code&gt;, it'll look for symbols such as &lt;code&gt;u_errorName_76&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Finally, you can set the following environment variables:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;EDIT_CFG_ICU_RENAMING_AUTO_DETECT&lt;/code&gt;: If set to &lt;code&gt;true&lt;/code&gt;, the executable will try to detect the &lt;code&gt;EDIT_CFG_ICU_RENAMING_VERSION&lt;/code&gt; value at runtime. The way it does this is not officially supported by ICU and as such is not recommended to be relied upon. Enabled by default on UNIX (excluding macOS) if no other options are set.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To test your settings, run &lt;code&gt;cargo test&lt;/code&gt; again but with the &lt;code&gt;--ignored&lt;/code&gt; flag. For instance:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cargo test -- --ignored
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>dandavison/delta</title>
      <link>https://github.com/dandavison/delta</link>
      <description>&lt;p&gt;A syntax-highlighting pager for git, diff, grep, and blame output&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img width="400px" src="https://user-images.githubusercontent.com/52205/147996902-9829bd3f-cd33-466e-833e-49a6f3ebd623.png" alt="image" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/dandavison/delta/actions"&gt; &lt;img src="https://github.com/dandavison/delta/workflows/Continuous%20Integration/badge.svg?sanitize=true" alt="CI" /&gt; &lt;/a&gt; &lt;a href="https://coveralls.io/github/dandavison/delta?branch=main"&gt; &lt;img src="https://coveralls.io/repos/github/dandavison/delta/badge.svg?branch=main" alt="Coverage Status" /&gt; &lt;/a&gt; &lt;a href="https://gitter.im/dandavison-delta/community?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge"&gt; &lt;img src="https://badges.gitter.im/dandavison-delta/community.svg?sanitize=true" alt="Gitter" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;Get Started&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://dandavison.github.io/delta/installation.html"&gt;Install it&lt;/a&gt; (the package is called "git-delta" in most package managers, but the executable is just &lt;code&gt;delta&lt;/code&gt;) and add this to your &lt;code&gt;~/.gitconfig&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-gitconfig"&gt;[core]
    pager = delta

[interactive]
    diffFilter = delta --color-only

[delta]
    navigate = true  # use n and N to move between diff sections
    dark = true      # or light = true, or omit for auto-detection

[merge]
    conflictStyle = zdiff3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git config --global core.pager delta
git config --global interactive.diffFilter 'delta --color-only'
git config --global delta.navigate true
git config --global merge.conflictStyle zdiff3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Delta has many features and is very customizable; please see &lt;code&gt;delta -h&lt;/code&gt; (short help) or &lt;code&gt;delta --help&lt;/code&gt; (full manual), or the &lt;a href="https://dandavison.github.io/delta/"&gt;online user manual&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Language syntax highlighting with the same syntax-highlighting themes as &lt;a href="https://github.com/sharkdp/bat#readme"&gt;bat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Word-level diff highlighting using a Levenshtein edit inference algorithm&lt;/li&gt; 
 &lt;li&gt;Side-by-side view with line-wrapping&lt;/li&gt; 
 &lt;li&gt;Line numbering&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;n&lt;/code&gt; and &lt;code&gt;N&lt;/code&gt; keybindings to move between files in large diffs, and between diffs in &lt;code&gt;log -p&lt;/code&gt; views (&lt;code&gt;--navigate&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Improved merge conflict display&lt;/li&gt; 
 &lt;li&gt;Improved &lt;code&gt;git blame&lt;/code&gt; display (syntax highlighting; &lt;code&gt;--hyperlinks&lt;/code&gt; formats commits as links to hosting provider etc. Supported hosting providers are: GitHub, GitLab, SourceHut, Codeberg)&lt;/li&gt; 
 &lt;li&gt;Syntax-highlights grep output from &lt;code&gt;rg&lt;/code&gt;, &lt;code&gt;git grep&lt;/code&gt;, &lt;code&gt;grep&lt;/code&gt;, etc&lt;/li&gt; 
 &lt;li&gt;Support for Git's &lt;code&gt;--color-moved&lt;/code&gt; feature.&lt;/li&gt; 
 &lt;li&gt;Code can be copied directly from the diff (&lt;code&gt;-/+&lt;/code&gt; markers are removed by default).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;diff-highlight&lt;/code&gt; and &lt;code&gt;diff-so-fancy&lt;/code&gt; emulation modes&lt;/li&gt; 
 &lt;li&gt;Commit hashes can be formatted as terminal &lt;a href="https://gist.github.com/egmontkob/eb114294efbcd5adb1944c9f3cb5feda"&gt;hyperlinks&lt;/a&gt; to the hosting provider page (&lt;code&gt;--hyperlinks&lt;/code&gt;). File paths can also be formatted as hyperlinks for opening in your OS.&lt;/li&gt; 
 &lt;li&gt;Stylable box/line decorations to draw attention to commit, file and hunk header sections.&lt;/li&gt; 
 &lt;li&gt;Style strings (foreground color, background color, font attributes) are supported for &amp;gt;20 stylable elements, using the same color/style language as git&lt;/li&gt; 
 &lt;li&gt;Handles traditional unified diff output in addition to git output&lt;/li&gt; 
 &lt;li&gt;Automatic detection of light/dark terminal background&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;A syntax-highlighting pager for git, diff, and grep output&lt;/h2&gt; 
&lt;p&gt;Code evolves, and we all spend time studying diffs. Delta aims to make this both efficient and enjoyable: it allows you to make extensive changes to the layout and styling of diffs, as well as allowing you to stay arbitrarily close to the default git/diff output.&lt;/p&gt; 
&lt;table align="center"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt; &lt;img width="400px" src="https://user-images.githubusercontent.com/52205/86275526-76792100-bba1-11ea-9e78-6be9baa80b29.png" alt="image" /&gt; &lt;br /&gt; &lt;p align="center"&gt;&lt;sub&gt;delta with &lt;code&gt;line-numbers&lt;/code&gt; activated&lt;/sub&gt;&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;table align="center"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt; &lt;img width="800px" src="https://user-images.githubusercontent.com/52205/87230973-412eb900-c381-11ea-8aec-cc200290bd1b.png" alt="image" /&gt; &lt;br /&gt; &lt;p align="center"&gt;&lt;sub&gt;delta with &lt;code&gt;side-by-side&lt;/code&gt; and &lt;code&gt;line-numbers&lt;/code&gt; activated&lt;/sub&gt;&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;p&gt;Here's what &lt;code&gt;git show&lt;/code&gt; can look like with git configured to use delta:&lt;/p&gt; 
&lt;br /&gt; 
&lt;table align="center"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt; &lt;img width="500px" style="border: 1px solid black" src="https://user-images.githubusercontent.com/52205/81058545-a5725f80-8e9c-11ea-912e-d21954586a44.png" alt="image" /&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;img width="500px" style="border: 1px solid black" src="https://user-images.githubusercontent.com/52205/81058911-6abcf700-8e9d-11ea-93be-e212824ec03d.png" alt="image" /&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; "Dracula" theme &lt;/td&gt; 
   &lt;td&gt; "GitHub" theme &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;h3&gt;Syntax-highlighting themes&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;All the syntax-highlighting color themes that are available with &lt;a href="https://github.com/sharkdp/bat/"&gt;bat&lt;/a&gt; are available with delta:&lt;/strong&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt; &lt;img width="400px" style="border: 1px solid black" src="https://user-images.githubusercontent.com/52205/149431273-e3ad049d-771e-4186-869d-0e57967958a6.png" alt="image" /&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;img width="400px" style="border: 1px solid black" src="https://user-images.githubusercontent.com/52205/149431419-48836001-2afc-4fd0-97ad-561a69b71db7.png" alt="image" /&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; &lt;code&gt;delta --show-syntax-themes --dark&lt;/code&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;code&gt;delta --show-syntax-themes --light&lt;/code&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;br /&gt; 
&lt;h3&gt;Side-by-side view&lt;/h3&gt; 
&lt;p&gt;[&lt;a href="https://dandavison.github.io/delta/side-by-side-view.html"&gt;User manual&lt;/a&gt;]&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-gitconfig"&gt;[delta]
    side-by-side = true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;By default, side-by-side view has line-numbers activated, and has syntax highlighting in both the left and right panels: [&lt;a href="https://raw.githubusercontent.com/dandavison/delta/main/#side-by-side-view-1"&gt;config&lt;/a&gt;]&lt;/p&gt; 
&lt;table&gt;
 &lt;tbody&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;img width="800px" src="https://user-images.githubusercontent.com/52205/87230973-412eb900-c381-11ea-8aec-cc200290bd1b.png" alt="image" /&gt;&lt;/td&gt;
  &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;p&gt;Side-by-side view wraps long lines automatically:&lt;/p&gt; 
&lt;table&gt;
 &lt;tbody&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;img width="600px" src="https://user-images.githubusercontent.com/52205/139064537-f8479504-16d3-429a-b4f6-d0122438adaa.png" alt="image" /&gt;&lt;/td&gt;
  &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Line numbers&lt;/h3&gt; 
&lt;p&gt;[&lt;a href="https://dandavison.github.io/delta/line-numbers.html"&gt;User manual&lt;/a&gt;]&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-gitconfig"&gt;[delta]
    line-numbers = true
&lt;/code&gt;&lt;/pre&gt; 
&lt;table&gt;
 &lt;tbody&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;img width="400px" src="https://user-images.githubusercontent.com/52205/86275526-76792100-bba1-11ea-9e78-6be9baa80b29.png" alt="image" /&gt;&lt;/td&gt;
  &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Merge conflicts&lt;/h3&gt; 
&lt;p&gt;[&lt;a href="https://dandavison.github.io/delta/merge-conflicts.html"&gt;User manual&lt;/a&gt;]&lt;/p&gt; 
&lt;table&gt;
 &lt;tbody&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;img width="500px" src="https://user-images.githubusercontent.com/52205/144783121-bb549100-69d8-41b8-ac62-1704f1f7b43e.png" alt="image" /&gt;&lt;/td&gt;
  &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Git blame&lt;/h3&gt; 
&lt;p&gt;[&lt;a href="https://dandavison.github.io/delta/git-blame.html"&gt;User manual&lt;/a&gt;]&lt;/p&gt; 
&lt;table&gt;
 &lt;tbody&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;img width="600px" src="https://user-images.githubusercontent.com/52205/141891376-1fdb87dc-1d9c-4ad6-9d72-eeb19a8aeb0b.png" alt="image" /&gt;&lt;/td&gt;
  &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Ripgrep, git grep&lt;/h3&gt; 
&lt;p&gt;[&lt;a href="https://dandavison.github.io/delta/grep.html"&gt;User manual&lt;/a&gt;]&lt;/p&gt; 
&lt;table&gt;
 &lt;tbody&gt;
  &lt;tr&gt;
   &lt;td&gt; &lt;img width="600px" alt="image" src="https://github.com/dandavison/open-in-editor/assets/52205/d203d380-5acb-4296-aeb9-e38c73d6c27f" /&gt; &lt;/td&gt;
  &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Installation and usage&lt;/h3&gt; 
&lt;p&gt;Please see the &lt;a href="https://dandavison.github.io/delta/"&gt;user manual&lt;/a&gt; and &lt;code&gt;delta --help&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Maintainers&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dandavison"&gt;@dandavison&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/th1000s"&gt;@th1000s&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>atuinsh/atuin</title>
      <link>https://github.com/atuinsh/atuin</link>
      <description>&lt;p&gt;✨ Magical shell history&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/atuinsh/atuin/assets/53315310/13216a1d-1ac0-4c99-b0eb-d88290fe0efd" /&gt; 
  &lt;img alt="Text changing depending on mode. Light: 'So light!' Dark: 'So dark!'" src="https://github.com/atuinsh/atuin/assets/53315310/08bc86d4-a781-4aaa-8d7e-478ae6bcd129" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;em&gt;magical shell history&lt;/em&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/atuinsh/atuin/actions?query=workflow%3ARust"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/atuinsh/atuin/rust.yml?style=flat-square" /&gt;&lt;/a&gt; &lt;a href="https://crates.io/crates/atuin"&gt;&lt;img src="https://img.shields.io/crates/v/atuin.svg?style=flat-square" /&gt;&lt;/a&gt; &lt;a href="https://crates.io/crates/atuin"&gt;&lt;img src="https://img.shields.io/crates/d/atuin.svg?style=flat-square" /&gt;&lt;/a&gt; &lt;a href="https://github.com/atuinsh/atuin/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/crates/l/atuin.svg?style=flat-square" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/Fq8bJSKPHh"&gt;&lt;img src="https://img.shields.io/discord/954121165239115808" /&gt;&lt;/a&gt; &lt;a rel="me" href="https://hachyderm.io/@atuin"&gt;&lt;img src="https://img.shields.io/mastodon/follow/109944632283122560?domain=https%3A%2F%2Fhachyderm.io&amp;amp;style=social" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/atuinsh"&gt;&lt;img src="https://img.shields.io/twitter/follow/atuinsh?style=social" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/atuinsh/atuin/main/README.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/atuinsh/atuin/main/docs/zh-CN/README.md"&gt;简体中文&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Atuin replaces your existing shell history with a SQLite database, and records additional context for your commands. Additionally, it provides optional and &lt;em&gt;fully encrypted&lt;/em&gt; synchronisation of your history between machines, via an Atuin server.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/atuinsh/atuin/main/demo.gif" alt="animated" width="80%" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;em&gt;exit code, duration, time and command shown&lt;/em&gt; &lt;/p&gt; 
&lt;p&gt;As well as the search UI, it can do things like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# search for all successful `make` commands, recorded after 3pm yesterday
atuin search --exit 0 --after "yesterday 3pm" make
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You may use either the server I host, or host your own! Or just don't use sync at all. As all history sync is encrypted, I couldn't access your data even if I wanted to. And I &lt;strong&gt;really&lt;/strong&gt; don't want to.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;rebind &lt;code&gt;ctrl-r&lt;/code&gt; and &lt;code&gt;up&lt;/code&gt; (configurable) to a full screen history search UI&lt;/li&gt; 
 &lt;li&gt;store shell history in a sqlite database&lt;/li&gt; 
 &lt;li&gt;back up and sync &lt;strong&gt;encrypted&lt;/strong&gt; shell history&lt;/li&gt; 
 &lt;li&gt;the same history across terminals, across sessions, and across machines&lt;/li&gt; 
 &lt;li&gt;log exit code, cwd, hostname, session, command duration, etc&lt;/li&gt; 
 &lt;li&gt;calculate statistics such as "most used command"&lt;/li&gt; 
 &lt;li&gt;old history file is not replaced&lt;/li&gt; 
 &lt;li&gt;quick-jump to previous items with &lt;kbd&gt;Alt-&amp;lt;num&amp;gt;&lt;/kbd&gt;&lt;/li&gt; 
 &lt;li&gt;switch filter modes via ctrl-r; search history just from the current session, directory, or globally&lt;/li&gt; 
 &lt;li&gt;enter to execute a command, tab to edit&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/atuinsh/atuin/main/#quickstart"&gt;Quickstart&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.atuin.sh/guide/installation/"&gt;Install&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.atuin.sh/guide/sync/"&gt;Setting up sync&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.atuin.sh/guide/import/"&gt;Import history&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.atuin.sh/guide/basic-usage/"&gt;Basic usage&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Supported Shells&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;zsh&lt;/li&gt; 
 &lt;li&gt;bash&lt;/li&gt; 
 &lt;li&gt;fish&lt;/li&gt; 
 &lt;li&gt;nushell&lt;/li&gt; 
 &lt;li&gt;xonsh&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;h3&gt;Forum&lt;/h3&gt; 
&lt;p&gt;Atuin has a community forum, please ask here for help and support: &lt;a href="https://forum.atuin.sh/"&gt;https://forum.atuin.sh/&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Discord&lt;/h3&gt; 
&lt;p&gt;Atuin also has a community Discord, available &lt;a href="https://discord.gg/jR3tfchVvW"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Quickstart&lt;/h1&gt; 
&lt;p&gt;This will sign you up for the Atuin Cloud sync server. Everything is end-to-end encrypted, so your secrets are safe!&lt;/p&gt; 
&lt;p&gt;Read more in the &lt;a href="https://docs.atuin.sh"&gt;docs&lt;/a&gt; for an offline setup, self hosted server, and more.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;curl --proto '=https' --tlsv1.2 -LsSf https://setup.atuin.sh | sh

atuin register -u &amp;lt;USERNAME&amp;gt; -e &amp;lt;EMAIL&amp;gt;
atuin import auto
atuin sync
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then restart your shell!&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;For Bash users&lt;/strong&gt;: The above sets up &lt;code&gt;bash-preexec&lt;/code&gt; for necessary hooks, but &lt;code&gt;bash-preexec&lt;/code&gt; has limitations. For details, please see the &lt;a href="https://docs.atuin.sh/guide/installation/#installing-the-shell-plugin"&gt;Bash&lt;/a&gt; section of the shell plugin documentation.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;Security&lt;/h1&gt; 
&lt;p&gt;If you find any security issues, we'd appreciate it if you could alert &lt;a href="mailto:ellie@atuin.sh"&gt;ellie@atuin.sh&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Contributors&lt;/h1&gt; 
&lt;a href="https://github.com/atuinsh/atuin/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=atuinsh/atuin&amp;amp;max=300" /&gt; &lt;/a&gt; 
&lt;p&gt;Made with &lt;a href="https://contrib.rocks"&gt;contrib.rocks&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>lancedb/lance</title>
      <link>https://github.com/lancedb/lance</link>
      <description>&lt;p&gt;Modern columnar data format for ML and LLMs implemented in Rust. Convert from parquet in 2 lines of code for 100x faster random access, vector index, and data versioning. Compatible with Pandas, DuckDB, Polars, Pyarrow, and PyTorch with more integrations coming..&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p align="center"&gt; &lt;img width="257" alt="Lance Logo" src="https://user-images.githubusercontent.com/917119/199353423-d3e202f7-0269-411d-8ff2-e747e419e492.png" /&gt; &lt;/p&gt;
 &lt;p&gt;&lt;strong&gt;Modern columnar data format for ML. Convert from Parquet in 2-lines of code for 100x faster random access, zero-cost schema evolution, rich secondary indices, versioning, and more.&lt;br /&gt;&lt;/strong&gt; &lt;strong&gt;Compatible with Pandas, DuckDB, Polars, Pyarrow, and Ray with more integrations on the way.&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://lancedb.github.io/lance/"&gt;Documentation&lt;/a&gt; • &lt;a href="https://blog.lancedb.com/"&gt;Blog&lt;/a&gt; • &lt;a href="https://discord.gg/zMM32dvNtd"&gt;Discord&lt;/a&gt; • &lt;a href="https://x.com/lancedb"&gt;X&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/lancedb/lance/actions/workflows/rust.yml"&gt;&lt;img src="https://github.com/lancedb/lance/actions/workflows/rust.yml/badge.svg?sanitize=true" alt="CI Badge" /&gt;&lt;/a&gt; &lt;a href="https://lancedb.github.io/lance/"&gt;&lt;img src="https://img.shields.io/badge/docs-passing-brightgreen" alt="Docs Badge" /&gt;&lt;/a&gt; &lt;a href="https://crates.io/crates/lance"&gt;&lt;img src="https://img.shields.io/crates/v/lance.svg?sanitize=true" alt="crates.io badge" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/pylance/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/pylance" alt="Python versions badge" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;Lance is a modern columnar data format that is optimized for ML workflows and datasets. Lance is perfect for:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Building search engines and feature stores.&lt;/li&gt; 
 &lt;li&gt;Large-scale ML training requiring high performance IO and shuffles.&lt;/li&gt; 
 &lt;li&gt;Storing, querying, and inspecting deeply nested data for robotics or large blobs like images, point clouds, and more.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The key features of Lance include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;High-performance random access:&lt;/strong&gt; 100x faster than Parquet without sacrificing scan performance.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Vector search:&lt;/strong&gt; find nearest neighbors in milliseconds and combine OLAP-queries with vector search.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zero-copy, automatic versioning:&lt;/strong&gt; manage versions of your data without needing extra infrastructure.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Ecosystem integrations:&lt;/strong&gt; Apache Arrow, Pandas, Polars, DuckDB, Ray, Spark and more on the way.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Lance is in active development and we welcome contributions. Please see our &lt;a href="https://lancedb.github.io/lance/community/contributing"&gt;contributing guide&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Installation&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install pylance
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To install a preview release:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install --pre --extra-index-url https://pypi.fury.io/lancedb/ pylance
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Preview releases are released more often than full releases and contain the latest features and bug fixes. They receive the same level of testing as full releases. We guarantee they will remain published and available for download for at least 6 months. When you want to pin to a specific version, prefer a stable release.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Converting to Lance&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import lance

import pandas as pd
import pyarrow as pa
import pyarrow.dataset

df = pd.DataFrame({"a": [5], "b": [10]})
uri = "/tmp/test.parquet"
tbl = pa.Table.from_pandas(df)
pa.dataset.write_dataset(tbl, uri, format='parquet')

parquet = pa.dataset.dataset(uri, format='parquet')
lance.write_dataset(parquet, "/tmp/test.lance")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Reading Lance data&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;dataset = lance.dataset("/tmp/test.lance")
assert isinstance(dataset, pa.dataset.Dataset)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Pandas&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;df = dataset.to_table().to_pandas()
df
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;DuckDB&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import duckdb

# If this segfaults, make sure you have duckdb v0.7+ installed
duckdb.query("SELECT * FROM dataset LIMIT 10").to_df()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Vector search&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Download the sift1m subset&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;wget ftp://ftp.irisa.fr/local/texmex/corpus/sift.tar.gz
tar -xzf sift.tar.gz
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Convert it to Lance&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import lance
from lance.vector import vec_to_table
import numpy as np
import struct

nvecs = 1000000
ndims = 128
with open("sift/sift_base.fvecs", mode="rb") as fobj:
    buf = fobj.read()
    data = np.array(struct.unpack("&amp;lt;128000000f", buf[4 : 4 + 4 * nvecs * ndims])).reshape((nvecs, ndims))
    dd = dict(zip(range(nvecs), data))

table = vec_to_table(dd)
uri = "vec_data.lance"
sift1m = lance.write_dataset(table, uri, max_rows_per_group=8192, max_rows_per_file=1024*1024)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Build the index&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;sift1m.create_index("vector",
                    index_type="IVF_PQ",
                    num_partitions=256,  # IVF
                    num_sub_vectors=16)  # PQ
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Search the dataset&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Get top 10 similar vectors
import duckdb

dataset = lance.dataset(uri)

# Sample 100 query vectors. If this segfaults, make sure you have duckdb v0.7+ installed
sample = duckdb.query("SELECT vector FROM dataset USING SAMPLE 100").to_df()
query_vectors = np.array([np.array(x) for x in sample.vector])

# Get nearest neighbors for all of them
rs = [dataset.to_table(nearest={"column": "vector", "k": 10, "q": q})
      for q in query_vectors]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Directory structure&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Directory&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/lancedb/lance/main/rust"&gt;rust&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Core Rust implementation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/lancedb/lance/main/python"&gt;python&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Python bindings (PyO3)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/lancedb/lance/main/java"&gt;java&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Java bindings (JNI)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/lancedb/lance/main/docs"&gt;docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Documentation source&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;What makes Lance different&lt;/h2&gt; 
&lt;p&gt;Here we will highlight a few aspects of Lance’s design. For more details, see the full &lt;a href="https://lancedb.github.io/lance/format"&gt;Lance design document&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Vector index&lt;/strong&gt;: Vector index for similarity search over embedding space. Support both CPUs (&lt;code&gt;x86_64&lt;/code&gt; and &lt;code&gt;arm&lt;/code&gt;) and GPU (&lt;code&gt;Nvidia (cuda)&lt;/code&gt; and &lt;code&gt;Apple Silicon (mps)&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Encodings&lt;/strong&gt;: To achieve both fast columnar scan and sub-linear point queries, Lance uses custom encodings and layouts.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Nested fields&lt;/strong&gt;: Lance stores each subfield as a separate column to support efficient filters like “find images where detected objects include cats”.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Versioning&lt;/strong&gt;: A Manifest can be used to record snapshots. Currently we support creating new versions automatically via appends, overwrites, and index creation.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Fast updates&lt;/strong&gt; (ROADMAP): Updates will be supported via write-ahead logs.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Rich secondary indices&lt;/strong&gt;: Support &lt;code&gt;BTree&lt;/code&gt;, &lt;code&gt;Bitmap&lt;/code&gt;, &lt;code&gt;Full text search&lt;/code&gt;, &lt;code&gt;Label list&lt;/code&gt;, &lt;code&gt;NGrams&lt;/code&gt;, and more.&lt;/p&gt; 
&lt;h2&gt;Benchmarks&lt;/h2&gt; 
&lt;h3&gt;Vector search&lt;/h3&gt; 
&lt;p&gt;We used the SIFT dataset to benchmark our results with 1M vectors of 128D&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;For 100 randomly sampled query vectors, we get &amp;lt;1ms average response time (on a 2023 m2 MacBook Air)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/lancedb/lance/main/docs/src/images/avg_latency.png" alt="avg_latency.png" /&gt;&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;ANNs are always a trade-off between recall and performance&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/lancedb/lance/main/docs/src/images/recall_vs_latency.png" alt="avg_latency.png" /&gt;&lt;/p&gt; 
&lt;h3&gt;Vs. parquet&lt;/h3&gt; 
&lt;p&gt;We create a Lance dataset using the Oxford Pet dataset to do some preliminary performance testing of Lance as compared to Parquet and raw image/XMLs. For analytics queries, Lance is 50-100x better than reading the raw metadata. For batched random access, Lance is 100x better than both parquet and raw files.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/lancedb/lance/main/docs/src/images/lance_perf.png" alt="" /&gt;&lt;/p&gt; 
&lt;h2&gt;Why are you building yet another data format?!&lt;/h2&gt; 
&lt;p&gt;The machine learning development cycle involves the steps:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-mermaid"&gt;graph LR
    A[Collection] --&amp;gt; B[Exploration];
    B --&amp;gt; C[Analytics];
    C --&amp;gt; D[Feature Engineer];
    D --&amp;gt; E[Training];
    E --&amp;gt; F[Evaluation];
    F --&amp;gt; C;
    E --&amp;gt; G[Deployment];
    G --&amp;gt; H[Monitoring];
    H --&amp;gt; A;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;People use different data representations to varying stages for the performance or limited by the tooling available. Academia mainly uses XML / JSON for annotations and zipped images/sensors data for deep learning, which is difficult to integrate into data infrastructure and slow to train over cloud storage. While industry uses data lakes (Parquet-based techniques, i.e., Delta Lake, Iceberg) or data warehouses (AWS Redshift or Google BigQuery) to collect and analyze data, they have to convert the data into training-friendly formats, such as &lt;a href="https://github.com/eto-ai/rikai"&gt;Rikai&lt;/a&gt;/&lt;a href="https://github.com/uber/petastorm"&gt;Petastorm&lt;/a&gt; or &lt;a href="https://www.tensorflow.org/tutorials/load_data/tfrecord"&gt;TFRecord&lt;/a&gt;. Multiple single-purpose data transforms, as well as syncing copies between cloud storage to local training instances have become a common practice.&lt;/p&gt; 
&lt;p&gt;While each of the existing data formats excels at the workload it was originally designed for, we need a new data format tailored for multistage ML development cycles to reduce and data silos.&lt;/p&gt; 
&lt;p&gt;A comparison of different data formats in each stage of ML development cycle.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;Lance&lt;/th&gt; 
   &lt;th&gt;Parquet &amp;amp; ORC&lt;/th&gt; 
   &lt;th&gt;JSON &amp;amp; XML&lt;/th&gt; 
   &lt;th&gt;TFRecord&lt;/th&gt; 
   &lt;th&gt;Database&lt;/th&gt; 
   &lt;th&gt;Warehouse&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Analytics&lt;/td&gt; 
   &lt;td&gt;Fast&lt;/td&gt; 
   &lt;td&gt;Fast&lt;/td&gt; 
   &lt;td&gt;Slow&lt;/td&gt; 
   &lt;td&gt;Slow&lt;/td&gt; 
   &lt;td&gt;Decent&lt;/td&gt; 
   &lt;td&gt;Fast&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Feature Engineering&lt;/td&gt; 
   &lt;td&gt;Fast&lt;/td&gt; 
   &lt;td&gt;Fast&lt;/td&gt; 
   &lt;td&gt;Decent&lt;/td&gt; 
   &lt;td&gt;Slow&lt;/td&gt; 
   &lt;td&gt;Decent&lt;/td&gt; 
   &lt;td&gt;Good&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Training&lt;/td&gt; 
   &lt;td&gt;Fast&lt;/td&gt; 
   &lt;td&gt;Decent&lt;/td&gt; 
   &lt;td&gt;Slow&lt;/td&gt; 
   &lt;td&gt;Fast&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Exploration&lt;/td&gt; 
   &lt;td&gt;Fast&lt;/td&gt; 
   &lt;td&gt;Slow&lt;/td&gt; 
   &lt;td&gt;Fast&lt;/td&gt; 
   &lt;td&gt;Slow&lt;/td&gt; 
   &lt;td&gt;Fast&lt;/td&gt; 
   &lt;td&gt;Decent&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Infra Support&lt;/td&gt; 
   &lt;td&gt;Rich&lt;/td&gt; 
   &lt;td&gt;Rich&lt;/td&gt; 
   &lt;td&gt;Decent&lt;/td&gt; 
   &lt;td&gt;Limited&lt;/td&gt; 
   &lt;td&gt;Rich&lt;/td&gt; 
   &lt;td&gt;Rich&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Community Highlights&lt;/h2&gt; 
&lt;p&gt;Lance is currently used in production by:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/lancedb/lancedb"&gt;LanceDB&lt;/a&gt;, a serverless, low-latency vector database for ML applications&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.lancedb.com/enterprise/introduction"&gt;LanceDB Enterprise&lt;/a&gt;, hyperscale LanceDB with enterprise SLA.&lt;/li&gt; 
 &lt;li&gt;Leading multimodal Gen AI companies for training over petabyte-scale multimodal data.&lt;/li&gt; 
 &lt;li&gt;Self-driving car company for large-scale storage, retrieval and processing of multi-modal data.&lt;/li&gt; 
 &lt;li&gt;E-commerce company for billion-scale+ vector personalized search.&lt;/li&gt; 
 &lt;li&gt;and more.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Presentations, Blogs and Talks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://blog.lancedb.com/designing-a-table-format-for-ml-workloads/"&gt;Designing a Table Format for ML Workloads&lt;/a&gt;, Feb 2025.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=xmTFEzAh8ho"&gt;Transforming Multimodal Data Management with LanceDB, Ray Summit&lt;/a&gt;, Oct 2024.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://blog.lancedb.com/lance-v2/"&gt;Lance v2: A columnar container format for modern data&lt;/a&gt;, Apr 2024.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://drive.google.com/file/d/1Orh9rK0Mpj9zN_gnQF1eJJFpAc6lStGm/view?usp=drive_link"&gt;Lance Deep Dive&lt;/a&gt;. July 2023.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.google.com/presentation/d/1a4nAiQAkPDBtOfXFpPg7lbeDAxcNDVKgoUkw3cUs2rE/edit#slide=id.p"&gt;Lance: A New Columnar Data Format&lt;/a&gt;, &lt;a href="https://www.scipy2022.scipy.org/posters"&gt;Scipy 2022, Austin, TX&lt;/a&gt;. July, 2022.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>redlib-org/redlib</title>
      <link>https://github.com/redlib-org/redlib</link>
      <description>&lt;p&gt;Private front-end for Reddit&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Redlib&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;An alternative private front-end to Reddit, with its origins in &lt;a href="https://github.com/libreddit/libreddit"&gt;Libreddit&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://i.ibb.co/18vrdxk/redlib-rust.png" alt="screenshot" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;10-second pitch:&lt;/strong&gt; Redlib is a private front-end like &lt;a href="https://github.com/iv-org/invidious"&gt;Invidious&lt;/a&gt; but for Reddit. Browse the coldest takes of &lt;a href="https://farside.link/redlib/r/unpopularopinion"&gt;r/unpopularopinion&lt;/a&gt; without being &lt;a href="https://raw.githubusercontent.com/redlib-org/redlib/main/#reddit"&gt;tracked&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🚀 Fast: written in Rust for blazing-fast speeds and memory safety&lt;/li&gt; 
 &lt;li&gt;☁️ Light: no JavaScript, no ads, no tracking, no bloat&lt;/li&gt; 
 &lt;li&gt;🕵 Private: all requests are proxied through the server, including media&lt;/li&gt; 
 &lt;li&gt;🔒 Secure: strong &lt;a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/CSP"&gt;Content Security Policy&lt;/a&gt; prevents browser requests to Reddit&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/redlib-org/redlib/main/#redlib"&gt;Redlib&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/redlib-org/redlib/main/#instances"&gt;Instances&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/redlib-org/redlib/main/#about"&gt;About&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/redlib-org/redlib/main/#built-with"&gt;Built with&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/redlib-org/redlib/main/#how-is-it-different-from-other-reddit-front-ends"&gt;How is it different from other Reddit front ends?&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/redlib-org/redlib/main/#teddit"&gt;Teddit&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/redlib-org/redlib/main/#libreddit"&gt;Libreddit&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/redlib-org/redlib/main/#comparison"&gt;Comparison&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/redlib-org/redlib/main/#speed"&gt;Speed&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/redlib-org/redlib/main/#privacy"&gt;Privacy&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/redlib-org/redlib/main/#reddit"&gt;Reddit&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/redlib-org/redlib/main/#redlib-1"&gt;Redlib&lt;/a&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/redlib-org/redlib/main/#server"&gt;Server&lt;/a&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/redlib-org/redlib/main/#deployment"&gt;Deployment&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/redlib-org/redlib/main/#docker"&gt;Docker&lt;/a&gt;&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/redlib-org/redlib/main/#docker-compose"&gt;Docker Compose&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/redlib-org/redlib/main/#docker-cli"&gt;Docker CLI&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Podman&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Quadlets&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/redlib-org/redlib/main/#binary"&gt;Binary&lt;/a&gt;&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/redlib-org/redlib/main/#running-as-a-systemd-service"&gt;Running as a systemd service&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/redlib-org/redlib/main/#building-from-source"&gt;Building from source&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/redlib-org/redlib/main/#replit-heroku-glitch"&gt;Replit/Heroku/Glitch&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/redlib-org/redlib/main/#launchd-macos"&gt;launchd (macOS)&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/redlib-org/redlib/main/#configuration"&gt;Configuration&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/redlib-org/redlib/main/#instance-settings"&gt;Instance settings&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/redlib-org/redlib/main/#default-user-settings"&gt;Default user settings&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h1&gt;Instances&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] 🔗 &lt;strong&gt;Want to automatically redirect Reddit links to Redlib? Use &lt;a href="https://github.com/libredirect/libredirect"&gt;LibRedirect&lt;/a&gt; or &lt;a href="https://github.com/SimonBrazell/privacy-redirect"&gt;Privacy Redirect&lt;/a&gt;!&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;An up-to-date table of instances is available in &lt;a href="https://github.com/redlib-org/redlib-instances/raw/main/instances.md"&gt;Markdown&lt;/a&gt; and &lt;a href="https://github.com/redlib-org/redlib-instances/raw/main/instances.json"&gt;machine-readable JSON&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Both files are part of the &lt;a href="https://github.com/redlib-org/redlib-instances"&gt;redlib-instances&lt;/a&gt; repository. To contribute your &lt;a href="https://raw.githubusercontent.com/redlib-org/redlib/main/#deployment"&gt;self-hosted instance&lt;/a&gt; to the list, see the &lt;a href="https://github.com/redlib-org/redlib-instances/raw/main/README.md"&gt;redlib-instances README&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For information on instance uptime, see the &lt;a href="https://stats.uptimerobot.com/mpmqAs1G2Q"&gt;Uptime Robot status page&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;About&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Find Redlib on 💬 &lt;a href="https://matrix.to/#/#redlib:matrix.org"&gt;Matrix&lt;/a&gt;, 🐋 &lt;a href="https://quay.io/repository/redlib/redlib"&gt;Quay.io&lt;/a&gt;, &lt;img alt="octocat" src="https://github.githubassets.com/images/icons/emoji/octocat.png?v8" /&gt;) &lt;a href="https://github.com/redlib-org/redlib"&gt;GitHub&lt;/a&gt;, and 🦊 &lt;a href="https://gitlab.com/redlib/redlib"&gt;GitLab&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Redlib hopes to provide an easier way to browse Reddit, without the ads, trackers, and bloat. Redlib was inspired by other alternative front-ends to popular services such as &lt;a href="https://github.com/iv-org/invidious"&gt;Invidious&lt;/a&gt; for YouTube, &lt;a href="https://github.com/zedeus/nitter"&gt;Nitter&lt;/a&gt; for Twitter, and &lt;a href="https://sr.ht/~cadence/bibliogram/"&gt;Bibliogram&lt;/a&gt; for Instagram.&lt;/p&gt; 
&lt;p&gt;Redlib currently implements most of Reddit's (signed-out) functionalities but still lacks &lt;a href="https://github.com/redlib-org/redlib/issues"&gt;a few features&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Built with&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt; - Programming language&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hyperium/hyper"&gt;Hyper&lt;/a&gt; - HTTP server and client&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/rinja-rs/rinja"&gt;Rinja&lt;/a&gt; - Templating engine&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/rustls/rustls"&gt;Rustls&lt;/a&gt; - TLS library&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How is it different from other Reddit front ends?&lt;/h2&gt; 
&lt;h3&gt;Teddit&lt;/h3&gt; 
&lt;p&gt;Teddit is another awesome open source project designed to provide an alternative frontend to Reddit. There is no connection between the two, and you're welcome to use whichever one you favor. Competition fosters innovation and Teddit's release has motivated me to build Redlib into an even more polished product.&lt;/p&gt; 
&lt;p&gt;If you are looking to compare, the biggest differences I have noticed are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Redlib is themed around Reddit's redesign whereas Teddit appears to stick much closer to Reddit's old design. This may suit some users better as design is always subjective.&lt;/li&gt; 
 &lt;li&gt;Redlib is written in &lt;a href="https://www.rust-lang.org"&gt;Rust&lt;/a&gt; for speed and memory safety. It uses &lt;a href="https://hyper.rs"&gt;Hyper&lt;/a&gt;, a speedy and lightweight HTTP server/client implementation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Libreddit&lt;/h3&gt; 
&lt;p&gt;While originating as a fork of Libreddit, the name "Redlib" was adopted to avoid legal issues, as Reddit only allows the use of their name if structured as "XYZ For Reddit".&lt;/p&gt; 
&lt;p&gt;Several technical improvements have also been made, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;OAuth token spoofing&lt;/strong&gt;: To circumvent rate limits imposed by Reddit, OAuth token spoofing is used to mimick the most common iOS and Android clients. While spoofing both iOS and Android clients was explored, only the Android client was chosen due to content restrictions when using an anonymous iOS client.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Token refreshing&lt;/strong&gt;: The authentication token is refreshed every 24 hours, emulating the behavior of the official Android app.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;HTTP header mimicking&lt;/strong&gt;: Efforts are made to send along as many of the official app's headers as possible to reduce the likelihood of Reddit's crackdown on Redlib's requests.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h1&gt;Comparison&lt;/h1&gt; 
&lt;p&gt;This section outlines how Redlib compares to Reddit in terms of speed and privacy.&lt;/p&gt; 
&lt;h2&gt;Speed&lt;/h2&gt; 
&lt;p&gt;Last tested on January 12, 2024.&lt;/p&gt; 
&lt;p&gt;Results from Google PageSpeed Insights (&lt;a href="https://pagespeed.web.dev/report?url=https%3A%2F%2Fredlib.matthew.science%2F"&gt;Redlib Report&lt;/a&gt;, &lt;a href="https://pagespeed.web.dev/report?url=https://www.reddit.com"&gt;Reddit Report&lt;/a&gt;).&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Performance metric&lt;/th&gt; 
   &lt;th&gt;Redlib&lt;/th&gt; 
   &lt;th&gt;Reddit&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Speed Index&lt;/td&gt; 
   &lt;td&gt;0.6s&lt;/td&gt; 
   &lt;td&gt;1.9s&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Performance Score&lt;/td&gt; 
   &lt;td&gt;100%&lt;/td&gt; 
   &lt;td&gt;64%&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Time to Interactive&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;2.8s&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;12.4s&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Privacy&lt;/h2&gt; 
&lt;h3&gt;Reddit&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Logging:&lt;/strong&gt; According to Reddit's &lt;a href="https://www.redditinc.com/policies/privacy-policy"&gt;privacy policy&lt;/a&gt;, they "may [automatically] log information" including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;IP address&lt;/li&gt; 
 &lt;li&gt;User-agent string&lt;/li&gt; 
 &lt;li&gt;Browser type&lt;/li&gt; 
 &lt;li&gt;Operating system&lt;/li&gt; 
 &lt;li&gt;Referral URLs&lt;/li&gt; 
 &lt;li&gt;Device information (e.g., device IDs)&lt;/li&gt; 
 &lt;li&gt;Device settings&lt;/li&gt; 
 &lt;li&gt;Pages visited&lt;/li&gt; 
 &lt;li&gt;Links clicked&lt;/li&gt; 
 &lt;li&gt;The requested URL&lt;/li&gt; 
 &lt;li&gt;Search terms&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Location:&lt;/strong&gt; The same privacy policy goes on to describe that location data may be collected through the use of:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;GPS (consensual)&lt;/li&gt; 
 &lt;li&gt;Bluetooth (consensual)&lt;/li&gt; 
 &lt;li&gt;Content associated with a location (consensual)&lt;/li&gt; 
 &lt;li&gt;Your IP Address&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Cookies:&lt;/strong&gt; Reddit's &lt;a href="https://www.redditinc.com/policies/cookies"&gt;cookie notice&lt;/a&gt; documents the array of cookies used by Reddit including/regarding:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Authentication&lt;/li&gt; 
 &lt;li&gt;Functionality&lt;/li&gt; 
 &lt;li&gt;Analytics and Performance&lt;/li&gt; 
 &lt;li&gt;Advertising&lt;/li&gt; 
 &lt;li&gt;Third-Party Cookies&lt;/li&gt; 
 &lt;li&gt;Third-Party Site&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Redlib&lt;/h3&gt; 
&lt;p&gt;For transparency, I hope to describe all the ways Redlib handles user privacy.&lt;/p&gt; 
&lt;h4&gt;Server&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Logging:&lt;/strong&gt; In production (when running the binary, hosting with docker, or using the official instances), Redlib logs nothing. When debugging (running from source without &lt;code&gt;--release&lt;/code&gt;), Redlib logs post IDs fetched to aid with troubleshooting.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Cookies:&lt;/strong&gt; Redlib uses optional cookies to store any configured settings in the settings menu. These are not cross-site cookies and the cookies hold no personal data.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h1&gt;Deployment&lt;/h1&gt; 
&lt;p&gt;This section covers multiple ways of deploying Redlib. Using &lt;a href="https://raw.githubusercontent.com/redlib-org/redlib/main/#docker"&gt;Docker&lt;/a&gt; is recommended for production.&lt;/p&gt; 
&lt;p&gt;For configuration options, see the &lt;a href="https://raw.githubusercontent.com/redlib-org/redlib/main/#Configuration"&gt;Configuration section&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Docker&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.docker.com"&gt;Docker&lt;/a&gt; lets you run containerized applications. Containers are loosely isolated environments that are lightweight and contain everything needed to run the application, so there's no need to rely on what's installed on the host.&lt;/p&gt; 
&lt;p&gt;Container images for Redlib are available at &lt;a href="https://quay.io/repository/redlib/redlib"&gt;quay.io&lt;/a&gt;, with support for &lt;code&gt;amd64&lt;/code&gt;, &lt;code&gt;arm64&lt;/code&gt;, and &lt;code&gt;armv7&lt;/code&gt; platforms.&lt;/p&gt; 
&lt;h3&gt;Docker Compose&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] These instructions assume the &lt;a href="https://docs.docker.com/compose/migrate/#what-are-the-differences-between-compose-v1-and-compose-v2"&gt;Compose plugin&lt;/a&gt; has already been installed. If not, follow these &lt;a href="https://docs.docker.com/compose/install"&gt;instructions on the Docker Docs&lt;/a&gt; for how to do so.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Copy &lt;code&gt;compose.yaml&lt;/code&gt; and modify any relevant values (for example, the ports Redlib should listen on).&lt;/p&gt; 
&lt;p&gt;Start Redlib in detached mode (running in the background):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker compose up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Stream logs from the Redlib container:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker logs -f redlib
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker CLI&lt;/h3&gt; 
&lt;p&gt;Deploy Redlib:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker pull quay.io/redlib/redlib:latest
docker run -d --name redlib -p 8080:8080 quay.io/redlib/redlib:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Deploy using a different port on the host (in this case, port 80):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker pull quay.io/redlib/redlib:latest
docker run -d --name redlib -p 80:8080 quay.io/redlib/redlib:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you're using a reverse proxy in front of Redlib, prefix the port numbers with &lt;code&gt;127.0.0.1&lt;/code&gt; so that Redlib only listens on the host port &lt;strong&gt;locally&lt;/strong&gt;. For example, if the host port for Redlib is &lt;code&gt;8080&lt;/code&gt;, specify &lt;code&gt;127.0.0.1:8080:8080&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Stream logs from the Redlib container:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker logs -f redlib
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Podman&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://podman.io/"&gt;Podman&lt;/a&gt; lets you run containerized applications in a rootless fashion. Containers are loosely isolated environments that are lightweight and contain everything needed to run the application, so there's no need to rely on what's installed on the host.&lt;/p&gt; 
&lt;p&gt;Container images for Redlib are available at &lt;a href="https://quay.io/repository/redlib/redlib"&gt;quay.io&lt;/a&gt;, with support for &lt;code&gt;amd64&lt;/code&gt;, &lt;code&gt;arm64&lt;/code&gt;, and &lt;code&gt;armv7&lt;/code&gt; platforms.&lt;/p&gt; 
&lt;h3&gt;Quadlets&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] These instructions assume that you are on a systemd based distro with &lt;a href="https://podman.io/"&gt;podman&lt;/a&gt;. If not, follow these &lt;a href="https://podman.io/docs/installation"&gt;instructions on podman's website&lt;/a&gt; for how to do so. It also assumes you have used &lt;code&gt;loginctl enable-linger &amp;lt;username&amp;gt;&lt;/code&gt; to enable the service to start for your user without logging in.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Copy the &lt;code&gt;redlib.container&lt;/code&gt; and &lt;code&gt;.env.example&lt;/code&gt; files to &lt;code&gt;.config/containers/systemd/&lt;/code&gt; and modify any relevant values (for example, the ports Redlib should listen on, renaming the .env file and editing its values, etc.).&lt;/p&gt; 
&lt;p&gt;To start Redlib either reboot or follow the instructions below:&lt;/p&gt; 
&lt;p&gt;Notify systemd of the new files&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;systemctl --user daemon-reload
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Start the newly generated service file&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;systemctl --user start redlib.service
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can check the status of your container by using the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;systemctl --user status redlib.service
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Binary&lt;/h2&gt; 
&lt;p&gt;If you're on Linux, you can grab a binary from &lt;a href="https://github.com/redlib-org/redlib/releases/latest"&gt;the newest release&lt;/a&gt; from GitHub.&lt;/p&gt; 
&lt;p&gt;Download the binary using &lt;a href="https://www.gnu.org/software/wget/"&gt;Wget&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;wget https://github.com/redlib-org/redlib/releases/download/v0.31.0/redlib
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Make the binary executable and change its ownership to &lt;code&gt;root&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo chmod +x redlib &amp;amp;&amp;amp; sudo chown root:root redlib
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Copy the binary to &lt;code&gt;/usr/bin&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo cp ./redlib /usr/bin/redlib
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Deploy Redlib to &lt;code&gt;0.0.0.0:8080&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;redlib
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] If you're proxying Redlib through NGINX (see &lt;a href="https://github.com/libreddit/libreddit/issues/122#issuecomment-782226853"&gt;issue #122&lt;/a&gt;), add&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-nginx"&gt;proxy_http_version 1.1;
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;to your NGINX configuration file above your &lt;code&gt;proxy_pass&lt;/code&gt; line.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Running as a systemd service&lt;/h3&gt; 
&lt;p&gt;You can use the systemd service available in &lt;code&gt;contrib/redlib.service&lt;/code&gt; (install it on &lt;code&gt;/etc/systemd/system/redlib.service&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;That service can be optionally configured in terms of environment variables by creating a file in &lt;code&gt;/etc/redlib.conf&lt;/code&gt;. Use the &lt;code&gt;contrib/redlib.conf&lt;/code&gt; as a template. You can also add the &lt;code&gt;REDLIB_DEFAULT__{X}&lt;/code&gt; settings explained above.&lt;/p&gt; 
&lt;p&gt;When "Proxying using NGINX" where the proxy is on the same machine, you should guarantee nginx waits for this service to start. Edit &lt;code&gt;/etc/systemd/system/redlib.service.d/reverse-proxy.conf&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-conf"&gt;[Unit]
Before=nginx.service
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Building from source&lt;/h2&gt; 
&lt;p&gt;To deploy Redlib with changes not yet included in the latest release, you can build the application from source.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/redlib-org/redlib &amp;amp;&amp;amp; cd redlib
cargo run
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Replit/Heroku&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] These are free hosting options, but they are &lt;em&gt;not&lt;/em&gt; private and will monitor server usage to prevent abuse. If you need a free and easy setup, this method may work best for you.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://repl.it/github/redlib-org/redlib"&gt;&lt;img src="https://repl.it/badge/github/redlib-org/redlib" alt="Run on Repl.it" height="32" /&gt;&lt;/a&gt; &lt;a href="https://heroku.com/deploy?template=https://github.com/redlib-org/redlib"&gt;&lt;img src="https://www.herokucdn.com/deploy/button.svg?sanitize=true" alt="Deploy" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;launchd (macOS)&lt;/h2&gt; 
&lt;p&gt;If you are on macOS, you can use the &lt;a href="https://en.wikipedia.org/wiki/Launchd"&gt;launchd&lt;/a&gt; service available in &lt;code&gt;contrib/redlib.plist&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Install it with &lt;code&gt;cp contrib/redlib.plist ~/Library/LaunchAgents/&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Load and start it with &lt;code&gt;launchctl load ~/Library/LaunchAgents/redlib.plist&lt;/code&gt;.&lt;/p&gt; 
&lt;!-- ## Cargo

Make sure Rust stable is installed along with `cargo`, Rust's package manager.

```bash
cargo install libreddit
``` --&gt; 
&lt;!-- ## AUR

For ArchLinux users, Redlib is available from the AUR as [`libreddit-git`](https://aur.archlinux.org/packages/libreddit-git).

```bash
yay -S libreddit-git
```
## NetBSD/pkgsrc

For NetBSD users, Redlib is available from the official repositories.

```bash
pkgin install libreddit
```

Or, if you prefer to build from source

```bash
cd /usr/pkgsrc/libreddit
make install
``` --&gt; 
&lt;hr /&gt; 
&lt;h1&gt;Configuration&lt;/h1&gt; 
&lt;p&gt;You can configure Redlib further using environment variables. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;REDLIB_DEFAULT_SHOW_NSFW=on redlib
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;REDLIB_DEFAULT_WIDE=on REDLIB_DEFAULT_THEME=dark redlib -r
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also configure Redlib with a configuration file named &lt;code&gt;redlib.toml&lt;/code&gt;. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;REDLIB_DEFAULT_WIDE = "on"
REDLIB_DEFAULT_USE_HLS = "on"
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] If you're deploying Redlib using the &lt;strong&gt;Docker CLI or Docker Compose&lt;/strong&gt;, environment variables can be defined in a &lt;a href="https://docs.docker.com/compose/environment-variables/set-environment-variables/"&gt;&lt;code&gt;.env&lt;/code&gt; file&lt;/a&gt;, allowing you to centralize and manage configuration in one place.&lt;/p&gt; 
 &lt;p&gt;To configure Redlib using a &lt;code&gt;.env&lt;/code&gt; file, copy the &lt;code&gt;.env.example&lt;/code&gt; file to &lt;code&gt;.env&lt;/code&gt; and edit it accordingly.&lt;/p&gt; 
 &lt;p&gt;If using the Docker CLI, add &lt;code&gt; --env-file .env&lt;/code&gt; to the command that runs Redlib. For example:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d --name redlib -p 8080:8080 --env-file .env quay.io/redlib/redlib:latest
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;If using Docker Compose, no changes are needed as the &lt;code&gt;.env&lt;/code&gt; file is already referenced in &lt;code&gt;compose.yaml&lt;/code&gt; via the &lt;code&gt;env_file: .env&lt;/code&gt; line.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Command Line Flags&lt;/h2&gt; 
&lt;p&gt;Redlib supports the following command line flags:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-4&lt;/code&gt;, &lt;code&gt;--ipv4-only&lt;/code&gt;: Listen on IPv4 only.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-6&lt;/code&gt;, &lt;code&gt;--ipv6-only&lt;/code&gt;: Listen on IPv6 only.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-r&lt;/code&gt;, &lt;code&gt;--redirect-https&lt;/code&gt;: Redirect all HTTP requests to HTTPS (no longer functional).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-a&lt;/code&gt;, &lt;code&gt;--address &amp;lt;ADDRESS&amp;gt;&lt;/code&gt;: Sets address to listen on. Default is &lt;code&gt;[::]&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-p&lt;/code&gt;, &lt;code&gt;--port &amp;lt;PORT&amp;gt;&lt;/code&gt;: Port to listen on. Default is &lt;code&gt;8080&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-H&lt;/code&gt;, &lt;code&gt;--hsts &amp;lt;EXPIRE_TIME&amp;gt;&lt;/code&gt;: HSTS header to tell browsers that this site should only be accessed over HTTPS. Default is &lt;code&gt;604800&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Instance settings&lt;/h2&gt; 
&lt;p&gt;Assign a default value for each instance-specific setting by passing environment variables to Redlib in the format &lt;code&gt;REDLIB_{X}&lt;/code&gt;. Replace &lt;code&gt;{X}&lt;/code&gt; with the setting name (see list below) in capital letters.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Name&lt;/th&gt; 
   &lt;th&gt;Possible values&lt;/th&gt; 
   &lt;th&gt;Default value&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;SFW_ONLY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;["on", "off"]&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;off&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enables SFW-only mode for the instance, i.e. all NSFW content is filtered.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;BANNER&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;(empty)&lt;/td&gt; 
   &lt;td&gt;Allows the server to set a banner to be displayed. Currently this is displayed on the instance info page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ROBOTS_DISABLE_INDEXING&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;["on", "off"]&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;off&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disables indexing of the instance by search engines.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PUSHSHIFT_FRONTEND&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;undelete.pullpush.io&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Allows the server to set the Pushshift frontend to be used with "removed" links.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PORT&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Integer 0-65535&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;8080&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The &lt;strong&gt;internal&lt;/strong&gt; port Redlib listens on.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_RSS&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;["on", "off"]&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;off&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enables RSS feed generation.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;FULL_URL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;(empty)&lt;/td&gt; 
   &lt;td&gt;Allows for proper URLs (for now, only needed by RSS)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Default user settings&lt;/h2&gt; 
&lt;p&gt;Assign a default value for each user-modifiable setting by passing environment variables to Redlib in the format &lt;code&gt;REDLIB_DEFAULT_{Y}&lt;/code&gt;. Replace &lt;code&gt;{Y}&lt;/code&gt; with the setting name (see list below) in capital letters.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Name&lt;/th&gt; 
   &lt;th&gt;Possible values&lt;/th&gt; 
   &lt;th&gt;Default value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;THEME&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;["system", "light", "dark", "black", "dracula", "nord", "laserwave", "violet", "gold", "rosebox", "gruvboxdark", "gruvboxlight", "tokyoNight", "icebergDark", "doomone", "libredditBlack", "libredditDark", "libredditLight"]&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;system&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;FRONT_PAGE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;["default", "popular", "all"]&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;default&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;LAYOUT&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;["card", "clean", "compact"]&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;card&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;WIDE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;["on", "off"]&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;off&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;POST_SORT&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;["hot", "new", "top", "rising", "controversial"]&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;hot&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;COMMENT_SORT&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;["confidence", "top", "new", "controversial", "old"]&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;confidence&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;BLUR_SPOILER&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;["on", "off"]&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;off&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;SHOW_NSFW&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;["on", "off"]&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;off&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;BLUR_NSFW&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;["on", "off"]&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;off&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;USE_HLS&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;["on", "off"]&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;off&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;HIDE_HLS_NOTIFICATION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;["on", "off"]&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;off&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AUTOPLAY_VIDEOS&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;["on", "off"]&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;off&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;SUBSCRIPTIONS&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;+&lt;/code&gt;-delimited list of subreddits (&lt;code&gt;sub1+sub2+sub3+...&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;&lt;em&gt;(none)&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;HIDE_AWARDS&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;["on", "off"]&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;off&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;DISABLE_VISIT_REDDIT_CONFIRMATION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;["on", "off"]&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;off&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;HIDE_SCORE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;["on", "off"]&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;off&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;HIDE_SIDEBAR_AND_SUMMARY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;["on", "off"]&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;off&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;FIXED_NAVBAR&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;["on", "off"]&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;on&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;REMOVE_DEFAULT_FEEDS&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;["on", "off"]&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;off&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>YaLTeR/niri</title>
      <link>https://github.com/YaLTeR/niri</link>
      <description>&lt;p&gt;A scrollable-tiling Wayland compositor.&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt;niri&lt;/h1&gt; 
&lt;p align="center"&gt;A scrollable-tiling Wayland compositor.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://matrix.to/#/#niri:matrix.org"&gt;&lt;img alt="Matrix" src="https://img.shields.io/badge/matrix-%23niri-blue?logo=matrix" /&gt;&lt;/a&gt; &lt;a href="https://github.com/YaLTeR/niri/raw/main/LICENSE"&gt;&lt;img alt="GitHub License" src="https://img.shields.io/github/license/YaLTeR/niri" /&gt;&lt;/a&gt; &lt;a href="https://github.com/YaLTeR/niri/releases"&gt;&lt;img alt="GitHub Release" src="https://img.shields.io/github/v/release/YaLTeR/niri?logo=github" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://yalter.github.io/niri/Getting-Started.html"&gt;Getting Started&lt;/a&gt; | &lt;a href="https://yalter.github.io/niri/Configuration%3A-Introduction.html"&gt;Configuration&lt;/a&gt; | &lt;a href="https://github.com/YaLTeR/niri/discussions/325"&gt;Setup&amp;nbsp;Showcase&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/535e6530-2f44-4b84-a883-1240a3eee6e9" alt="niri with a few windows open" /&gt;&lt;/p&gt; 
&lt;h2&gt;About&lt;/h2&gt; 
&lt;p&gt;Windows are arranged in columns on an infinite strip going to the right. Opening a new window never causes existing windows to resize.&lt;/p&gt; 
&lt;p&gt;Every monitor has its own separate window strip. Windows can never "overflow" onto an adjacent monitor.&lt;/p&gt; 
&lt;p&gt;Workspaces are dynamic and arranged vertically. Every monitor has an independent set of workspaces, and there's always one empty workspace present all the way down.&lt;/p&gt; 
&lt;p&gt;The workspace arrangement is preserved across disconnecting and connecting monitors where it makes sense. When a monitor disconnects, its workspaces will move to another monitor, but upon reconnection they will move back to the original monitor.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Built from the ground up for scrollable tiling&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://yalter.github.io/niri/Workspaces.html"&gt;Dynamic workspaces&lt;/a&gt; like in GNOME&lt;/li&gt; 
 &lt;li&gt;An &lt;a href="https://github.com/user-attachments/assets/379a5d1f-acdb-4c11-b36c-e85fd91f0995"&gt;Overview&lt;/a&gt; that zooms out workspaces and windows&lt;/li&gt; 
 &lt;li&gt;Built-in screenshot UI&lt;/li&gt; 
 &lt;li&gt;Monitor and window screencasting through xdg-desktop-portal-gnome 
  &lt;ul&gt; 
   &lt;li&gt;You can &lt;a href="https://yalter.github.io/niri/Configuration%3A-Window-Rules.html#block-out-from"&gt;block out&lt;/a&gt; sensitive windows from screencasts&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://yalter.github.io/niri/Screencasting.html#dynamic-screencast-target"&gt;Dynamic cast target&lt;/a&gt; that can change what it shows on the go&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/YaLTeR/niri/assets/1794388/946a910e-9bec-4cd1-a923-4a9421707515"&gt;Touchpad&lt;/a&gt; and &lt;a href="https://github.com/YaLTeR/niri/assets/1794388/8464e65d-4bf2-44fa-8c8e-5883355bd000"&gt;mouse&lt;/a&gt; gestures&lt;/li&gt; 
 &lt;li&gt;Group windows into &lt;a href="https://yalter.github.io/niri/Tabs.html"&gt;tabs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Configurable layout: gaps, borders, struts, window sizes&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://yalter.github.io/niri/Configuration%3A-Layout.html#gradients"&gt;Gradient borders&lt;/a&gt; with Oklab and Oklch support&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/YaLTeR/niri/assets/1794388/ce178da2-af9e-4c51-876f-8709c241d95e"&gt;Animations&lt;/a&gt; with support for &lt;a href="https://github.com/YaLTeR/niri/assets/1794388/27a238d6-0a22-4692-b794-30dc7a626fad"&gt;custom shaders&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Live-reloading config&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Video Demo&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/YaLTeR/niri/assets/1794388/bce834b0-f205-434e-a027-b373495f9729"&gt;https://github.com/YaLTeR/niri/assets/1794388/bce834b0-f205-434e-a027-b373495f9729&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Also check out this video from Brodie Robertson that showcases a lot of the niri functionality: &lt;a href="https://youtu.be/DeYx2exm04M"&gt;Niri Is My New Favorite Wayland Compositor&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Status&lt;/h2&gt; 
&lt;p&gt;Niri is stable for day-to-day use and does most things expected of a Wayland compositor. Many people are daily-driving niri, and are happy to help in our &lt;a href="https://matrix.to/#/#niri:matrix.org"&gt;Matrix channel&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Give it a try! Follow the instructions on the &lt;a href="https://yalter.github.io/niri/Getting-Started.html"&gt;Getting Started&lt;/a&gt; page. Have your &lt;a href="https://github.com/Alexays/Waybar"&gt;waybar&lt;/a&gt;s and &lt;a href="https://codeberg.org/dnkl/fuzzel"&gt;fuzzel&lt;/a&gt;s ready: niri is not a complete desktop environment. Also check out &lt;a href="https://github.com/Vortriz/awesome-niri"&gt;awesome-niri&lt;/a&gt;, a list of niri-related links and projects.&lt;/p&gt; 
&lt;p&gt;Here are some points you may have questions about:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-monitor&lt;/strong&gt;: yes, a core part of the design from the very start. Mixed DPI works.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Fractional scaling&lt;/strong&gt;: yes, plus all niri UI stays pixel-perfect.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;NVIDIA&lt;/strong&gt;: seems to work fine.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Floating windows&lt;/strong&gt;: yes, starting from niri 25.01.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Input devices&lt;/strong&gt;: niri supports tablets, touchpads, and touchscreens. You can map the tablet to a specific monitor, or use &lt;a href="https://opentabletdriver.net/"&gt;OpenTabletDriver&lt;/a&gt;. We have touchpad gestures, but no touchscreen gestures yet.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Wlr protocols&lt;/strong&gt;: yes, we have most of the important ones like layer-shell, gamma-control, screencopy. You can check on &lt;a href="https://wayland.app"&gt;wayland.app&lt;/a&gt; at the bottom of each protocol's page.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: while I run niri on beefy machines, I try to stay conscious of performance. I've seen someone use it fine on an Eee&amp;nbsp;PC&amp;nbsp;900 from&amp;nbsp;2008, of all things.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Xwayland&lt;/strong&gt;: no built-in support, but xwayland-satellite is &lt;a href="https://yalter.github.io/niri/Xwayland.html#using-xwayland-satellite"&gt;easy to set up&lt;/a&gt; and works very well. 
  &lt;ul&gt; 
   &lt;li&gt;Steam and games, including Proton: work perfectly through xwayland-satellite.&lt;/li&gt; 
   &lt;li&gt;JetBrains IDEs, Ghidra: work well through xwayland-satellite.&lt;/li&gt; 
   &lt;li&gt;Discord and other Electron apps: work well through xwayland-satellite.&lt;/li&gt; 
   &lt;li&gt;Chromium and VSCode: work perfectly natively on Wayland with the right flags.&lt;/li&gt; 
   &lt;li&gt;X11 apps that want to position windows or bars at specific screen coordinates: won't work well; you can run them in a nested compositor like &lt;a href="https://yalter.github.io/niri/Xwayland.html#using-the-labwc-wayland-compositor"&gt;labwc&lt;/a&gt; or &lt;a href="https://yalter.github.io/niri/Xwayland.html#directly-running-xwayland-in-rootful-mode"&gt;rootful Xwayland&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;Display scaling (integer or fractional) keeps X11 apps crisp, but you need the latest xwayland-satellite. For games, you can run them in &lt;a href="https://github.com/ValveSoftware/gamescope"&gt;gamescope&lt;/a&gt; at native resolution, even with display scaling.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Media&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://youtu.be/Kmz8ODolnDg?list=PLRdS-n5seLRqrmWDQY4KDqtRMfIwU0U3T"&gt;niri: Making a Wayland compositor in Rust&lt;/a&gt; · &lt;em&gt;December 2024&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;My talk from the 2024 Moscow RustCon about niri, and how I do randomized property testing and profiling, and measure input latency. The talk is in Russian, but I prepared full English subtitles that you can find in YouTube's subtitle language selector.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.trommelspeicher.de/podcast/special_the_developer_behind_niri"&gt;An interview with Ivan, the developer behind Niri&lt;/a&gt; · &lt;em&gt;June 2025&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;An interview by a German tech podcast Das Triumvirat (in English). We talk about niri development and history, and my experience building and maintaining niri.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://lwn.net/Articles/1025866/"&gt;A tour of the niri scrolling-tiling Wayland compositor&lt;/a&gt; · &lt;em&gt;July 2025&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;An LWN article with a nice overview and introduction to niri.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;If you'd like to help with niri, there are plenty of both coding- and non-coding-related ways to do so. See &lt;a href="https://github.com/YaLTeR/niri/raw/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for an overview.&lt;/p&gt; 
&lt;h2&gt;Inspiration&lt;/h2&gt; 
&lt;p&gt;Niri is heavily inspired by &lt;a href="https://github.com/paperwm/PaperWM"&gt;PaperWM&lt;/a&gt; which implements scrollable tiling on top of GNOME Shell.&lt;/p&gt; 
&lt;p&gt;One of the reasons that prompted me to try writing my own compositor is being able to properly separate the monitors. Being a GNOME Shell extension, PaperWM has to work against Shell's global window coordinate space to prevent windows from overflowing.&lt;/p&gt; 
&lt;h2&gt;Tile Scrollably Elsewhere&lt;/h2&gt; 
&lt;p&gt;Here are some other projects which implement a similar workflow:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/paperwm/PaperWM"&gt;PaperWM&lt;/a&gt;: scrollable tiling on top of GNOME Shell.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/peterfajdiga/karousel"&gt;karousel&lt;/a&gt;: scrollable tiling on top of KDE.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dawsers/scroll"&gt;scroll&lt;/a&gt; and &lt;a href="https://spwhitton.name/tech/code/papersway/"&gt;papersway&lt;/a&gt;: scrollable tiling on top of sway/i3.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hyprwm/hyprland-plugins/tree/main/hyprscrolling"&gt;hyprscrolling&lt;/a&gt; and &lt;a href="https://gitlab.com/magus/hyprslidr"&gt;hyprslidr&lt;/a&gt;: scrollable tiling on top of Hyprland.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mogenson/PaperWM.spoon"&gt;PaperWM.spoon&lt;/a&gt;: scrollable tiling on top of macOS.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contact&lt;/h2&gt; 
&lt;p&gt;Our main communication channel is a Matrix chat, feel free to join and ask a question: &lt;a href="https://matrix.to/#/#niri:matrix.org"&gt;https://matrix.to/#/#niri:matrix.org&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We also have a community Discord server: &lt;a href="https://discord.gg/vT8Sfjy7sx"&gt;https://discord.gg/vT8Sfjy7sx&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>qdrant/qdrant</title>
      <link>https://github.com/qdrant/qdrant</link>
      <description>&lt;p&gt;Qdrant - High-performance, massive-scale Vector Database and Vector Search Engine for the next generation of AI. Also available in the cloud https://cloud.qdrant.io/&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/qdrant/qdrant/raw/master/docs/logo-dark.svg" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://github.com/qdrant/qdrant/raw/master/docs/logo-light.svg" /&gt; 
  &lt;img height="100" alt="Qdrant" src="https://github.com/qdrant/qdrant/raw/master/docs/logo.svg?sanitize=true" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;b&gt;Vector Search Engine for the next generation of AI applications&lt;/b&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/qdrant/qdrant/actions/workflows/rust.yml"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/qdrant/qdrant/rust.yml?style=flat-square" alt="Tests status" /&gt;&lt;/a&gt; &lt;a href="https://api.qdrant.tech/"&gt;&lt;img src="https://img.shields.io/badge/Docs-OpenAPI%203.0-success?style=flat-square" alt="OpenAPI Docs" /&gt;&lt;/a&gt; &lt;a href="https://github.com/qdrant/qdrant/raw/master/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/qdrant/qdrant?style=flat-square" alt="Apache 2.0 License" /&gt;&lt;/a&gt; &lt;a href="https://qdrant.to/discord"&gt;&lt;img src="https://img.shields.io/discord/907569970500743200?logo=Discord&amp;amp;style=flat-square&amp;amp;color=7289da" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://qdrant.to/roadmap"&gt;&lt;img src="https://img.shields.io/badge/Roadmap-2025-bc1439.svg?style=flat-square" alt="Roadmap 2025" /&gt;&lt;/a&gt; &lt;a href="https://cloud.qdrant.io/"&gt;&lt;img src="https://img.shields.io/badge/Qdrant-Cloud-24386C.svg?logo=cloud&amp;amp;style=flat-square" alt="Qdrant Cloud" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Qdrant&lt;/strong&gt; (read: &lt;em&gt;quadrant&lt;/em&gt;) is a vector similarity search engine and vector database. It provides a production-ready service with a convenient API to store, search, and manage points—vectors with an additional payload Qdrant is tailored to extended filtering support. It makes it useful for all sorts of neural-network or semantic-based matching, faceted search, and other applications.&lt;/p&gt; 
&lt;p&gt;Qdrant is written in Rust 🦀, which makes it fast and reliable even under high load. See &lt;a href="https://qdrant.tech/benchmarks/"&gt;benchmarks&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!&lt;/p&gt; 
&lt;p&gt;Qdrant is also available as a fully managed &lt;strong&gt;&lt;a href="https://cloud.qdrant.io/"&gt;Qdrant Cloud&lt;/a&gt;&lt;/strong&gt; ⛅ including a &lt;strong&gt;free tier&lt;/strong&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/qdrant/qdrant/master/docs/QUICK_START.md"&gt;Quick Start&lt;/a&gt; • &lt;a href="https://raw.githubusercontent.com/qdrant/qdrant/master/#clients"&gt;Client Libraries&lt;/a&gt; • &lt;a href="https://raw.githubusercontent.com/qdrant/qdrant/master/#demo-projects"&gt;Demo Projects&lt;/a&gt; • &lt;a href="https://raw.githubusercontent.com/qdrant/qdrant/master/#integrations"&gt;Integrations&lt;/a&gt; • &lt;a href="https://raw.githubusercontent.com/qdrant/qdrant/master/#contacts"&gt;Contact&lt;/a&gt; &lt;/strong&gt; &lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Python&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;pip install qdrant-client
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The python client offers a convenient way to start with Qdrant locally:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from qdrant_client import QdrantClient
qdrant = QdrantClient(":memory:") # Create in-memory Qdrant instance, for testing, CI/CD
# OR
client = QdrantClient(path="path/to/db")  # Persists changes to disk, fast prototyping
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Client-Server&lt;/h3&gt; 
&lt;p&gt;To experience the full power of Qdrant locally, run the container with this command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -p 6333:6333 qdrant/qdrant
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now you can connect to this with any client, including Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;qdrant = QdrantClient("http://localhost:6333") # Connect to existing Qdrant instance
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Before deploying Qdrant to production, be sure to read our &lt;a href="https://qdrant.tech/documentation/guides/installation/"&gt;installation&lt;/a&gt; and &lt;a href="https://qdrant.tech/documentation/guides/security/"&gt;security&lt;/a&gt; guides.&lt;/p&gt; 
&lt;h3&gt;Clients&lt;/h3&gt; 
&lt;p&gt;Qdrant offers the following client libraries to help you integrate it into your application stack with ease:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Official: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/qdrant/go-client"&gt;Go client&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/qdrant/rust-client"&gt;Rust client&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/qdrant/qdrant-js"&gt;JavaScript/TypeScript client&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/qdrant/qdrant-client"&gt;Python client&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/qdrant/qdrant-dotnet"&gt;.NET/C# client&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/qdrant/java-client"&gt;Java client&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Community: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://hexdocs.pm/qdrant/readme.html"&gt;Elixir&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/hkulekci/qdrant-php"&gt;PHP&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/andreibondarev/qdrant-ruby"&gt;Ruby&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/metaloom/qdrant-java-client"&gt;Java&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Where do I go from here?&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/qdrant/qdrant/master/docs/QUICK_START.md"&gt;Quick Start Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;End to End &lt;a href="https://colab.research.google.com/drive/1Bz8RSVHwnNDaNtDwotfPj0w7AYzsdXZ-?usp=sharing"&gt;Colab Notebook&lt;/a&gt; demo with SentenceBERT and Qdrant&lt;/li&gt; 
 &lt;li&gt;Detailed &lt;a href="https://qdrant.tech/documentation/"&gt;Documentation&lt;/a&gt; are great starting points&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://qdrant.to/qdrant-tutorial"&gt;Step-by-Step Tutorial&lt;/a&gt; to create your first neural network project with Qdrant&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Demo Projects &lt;a href="https://replit.com/@qdrant"&gt;&lt;img align="right" src="https://replit.com/badge/github/qdrant/qdrant" alt="Run on Repl.it" /&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h3&gt;Discover Semantic Text Search 🔍&lt;/h3&gt; 
&lt;p&gt;Unlock the power of semantic embeddings with Qdrant, transcending keyword-based search to find meaningful connections in short texts. Deploy a neural search in minutes using a pre-trained neural network, and experience the future of text search. &lt;a href="https://qdrant.to/semantic-search-demo"&gt;Try it online!&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Explore Similar Image Search - Food Discovery 🍕&lt;/h3&gt; 
&lt;p&gt;There's more to discovery than text search, especially when it comes to food. People often choose meals based on appearance rather than descriptions and ingredients. Let Qdrant help your users find their next delicious meal using visual search, even if they don't know the dish's name. &lt;a href="https://qdrant.to/food-discovery"&gt;Check it out!&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Master Extreme Classification - E-commerce Product Categorization 📺&lt;/h3&gt; 
&lt;p&gt;Enter the cutting-edge realm of extreme classification, an emerging machine learning field tackling multi-class and multi-label problems with millions of labels. Harness the potential of similarity learning models, and see how a pre-trained transformer model and Qdrant can revolutionize e-commerce product categorization. &lt;a href="https://qdrant.to/extreme-classification-demo"&gt;Play with it online!&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; More solutions &lt;/summary&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td width="30%"&gt; &lt;img src="https://qdrant.tech/content/images/text_search.png" /&gt; &lt;/td&gt; 
    &lt;td width="30%"&gt; &lt;img src="https://qdrant.tech/content/images/image_search.png" /&gt; &lt;/td&gt; 
    &lt;td width="30%"&gt; &lt;img src="https://qdrant.tech/content/images/recommendations.png" /&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt; Semantic Text Search &lt;/td&gt; 
    &lt;td&gt; Similar Image Search &lt;/td&gt; 
    &lt;td&gt; Recommendations &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td&gt; &lt;img width="300px" src="https://qdrant.tech/content/images/chat_bots.png" /&gt; &lt;/td&gt; 
    &lt;td&gt; &lt;img width="300px" src="https://qdrant.tech/content/images/matching_engines.png" /&gt; &lt;/td&gt; 
    &lt;td&gt; &lt;img width="300px" src="https://qdrant.tech/content/images/anomalies_detection.png" /&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt; Chat Bots &lt;/td&gt; 
    &lt;td&gt; Matching Engines &lt;/td&gt; 
    &lt;td&gt; Anomaly Detection &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h2&gt;API&lt;/h2&gt; 
&lt;h3&gt;REST&lt;/h3&gt; 
&lt;p&gt;Online OpenAPI 3.0 documentation is available &lt;a href="https://api.qdrant.tech/"&gt;here&lt;/a&gt;. OpenAPI makes it easy to generate a client for virtually any framework or programming language.&lt;/p&gt; 
&lt;p&gt;You can also download raw OpenAPI &lt;a href="https://github.com/qdrant/qdrant/raw/master/docs/redoc/master/openapi.json"&gt;definitions&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;gRPC&lt;/h3&gt; 
&lt;p&gt;For faster production-tier searches, Qdrant also provides a gRPC interface. You can find gRPC documentation &lt;a href="https://qdrant.tech/documentation/interfaces/#grpc-interface"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;h3&gt;Filtering and Payload&lt;/h3&gt; 
&lt;p&gt;Qdrant can attach any JSON payloads to vectors, allowing for both the storage and filtering of data based on the values in these payloads. Payload supports a wide range of data types and query conditions, including keyword matching, full-text filtering, numerical ranges, geo-locations, and more.&lt;/p&gt; 
&lt;p&gt;Filtering conditions can be combined in various ways, including &lt;code&gt;should&lt;/code&gt;, &lt;code&gt;must&lt;/code&gt;, and &lt;code&gt;must_not&lt;/code&gt; clauses, ensuring that you can implement any desired business logic on top of similarity matching.&lt;/p&gt; 
&lt;h3&gt;Hybrid Search with Sparse Vectors&lt;/h3&gt; 
&lt;p&gt;To address the limitations of vector embeddings when searching for specific keywords, Qdrant introduces support for sparse vectors in addition to the regular dense ones.&lt;/p&gt; 
&lt;p&gt;Sparse vectors can be viewed as an generalization of BM25 or TF-IDF ranking. They enable you to harness the capabilities of transformer-based neural networks to weigh individual tokens effectively.&lt;/p&gt; 
&lt;h3&gt;Vector Quantization and On-Disk Storage&lt;/h3&gt; 
&lt;p&gt;Qdrant provides multiple options to make vector search cheaper and more resource-efficient. Built-in vector quantization reduces RAM usage by up to 97% and dynamically manages the trade-off between search speed and precision.&lt;/p&gt; 
&lt;h3&gt;Distributed Deployment&lt;/h3&gt; 
&lt;p&gt;Qdrant offers comprehensive horizontal scaling support through two key mechanisms:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Size expansion via sharding and throughput enhancement via replication&lt;/li&gt; 
 &lt;li&gt;Zero-downtime rolling updates and seamless dynamic scaling of the collections&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Highlighted Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Query Planning and Payload Indexes&lt;/strong&gt; - leverages stored payload information to optimize query execution strategy.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SIMD Hardware Acceleration&lt;/strong&gt; - utilizes modern CPU x86-x64 and Neon architectures to deliver better performance.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Async I/O&lt;/strong&gt; - uses &lt;code&gt;io_uring&lt;/code&gt; to maximize disk throughput utilization even on a network-attached storage.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Write-Ahead Logging&lt;/strong&gt; - ensures data persistence with update confirmation, even during power outages.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Integrations&lt;/h1&gt; 
&lt;p&gt;Examples and/or documentation of Qdrant integrations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.cohere.com/docs/qdrant-and-cohere"&gt;Cohere&lt;/a&gt; (&lt;a href="https://qdrant.tech/articles/qa-with-cohere-and-qdrant/"&gt;blogpost on building a QA app with Cohere and Qdrant&lt;/a&gt;) - Use Cohere embeddings with Qdrant&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.docarray.org/user_guide/storing/index_qdrant/"&gt;DocArray&lt;/a&gt; - Use Qdrant as a document store in DocArray&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://haystack.deepset.ai/integrations/qdrant-document-store"&gt;Haystack&lt;/a&gt; - Use Qdrant as a document store with Haystack (&lt;a href="https://haystack.deepset.ai/blog/qdrant-integration"&gt;blogpost&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://python.langchain.com/docs/integrations/providers/qdrant/"&gt;LangChain&lt;/a&gt; (&lt;a href="https://qdrant.tech/articles/langchain-integration/"&gt;blogpost&lt;/a&gt;) - Use Qdrant as a memory backend for LangChain.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://gpt-index.readthedocs.io/en/latest/examples/vector_stores/QdrantIndexDemo.html"&gt;LlamaIndex&lt;/a&gt; - Use Qdrant as a Vector Store with LlamaIndex.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/openai/chatgpt-retrieval-plugin/raw/main/docs/providers/qdrant/setup.md"&gt;OpenAI - ChatGPT retrieval plugin&lt;/a&gt; - Use Qdrant as a memory backend for ChatGPT&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://devblogs.microsoft.com/semantic-kernel/the-power-of-persistent-memory-with-semantic-kernel-and-qdrant-vector-database/"&gt;Microsoft Semantic Kernel&lt;/a&gt; - Use Qdrant as persistent memory with Semantic Kernel&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contacts&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Have questions? Join our &lt;a href="https://qdrant.to/discord"&gt;Discord channel&lt;/a&gt; or mention &lt;a href="https://qdrant.to/twitter"&gt;@qdrant_engine on Twitter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Want to stay in touch with latest releases? Subscribe to our &lt;a href="https://qdrant.tech/subscribe/"&gt;Newsletters&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Looking for a managed cloud? Check &lt;a href="https://qdrant.tech/pricing/"&gt;pricing&lt;/a&gt;, need something personalised? We're at &lt;a href="mailto:info@qdrant.tech"&gt;info@qdrant.tech&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Qdrant is licensed under the Apache License, Version 2.0. View a copy of the &lt;a href="https://github.com/qdrant/qdrant/raw/master/LICENSE"&gt;License file&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>