<rss version="2.0">
  <channel>
    <title>GitHub All Languages Daily Trending</title>
    <description>Daily Trending of All Languages in GitHub</description>
    <pubDate>Tue, 21 Oct 2025 01:30:32 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>EbookFoundation/free-programming-books</title>
      <link>https://github.com/EbookFoundation/free-programming-books</link>
      <description>&lt;p&gt;📚 Freely available programming books&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;List of Free Learning Resources In Many Languages&lt;/h1&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/sindresorhus/awesome"&gt;&lt;img src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true" alt="Awesome" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://creativecommons.org/licenses/by/4.0/"&gt;&lt;img src="https://img.shields.io/github/license/EbookFoundation/free-programming-books" alt="License: CC BY 4.0" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Apr+is%3Amerged+created%3A2025-10-01..2025-10-31"&gt;&lt;img src="https://img.shields.io/github/hacktoberfest/2025/EbookFoundation/free-programming-books?label=Hacktoberfest+2025" alt="Hacktoberfest 2025 stats" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;Search the list at &lt;a href="https://ebookfoundation.github.io/free-programming-books-search/"&gt;https://ebookfoundation.github.io/free-programming-books-search/&lt;/a&gt; &lt;a href="https://ebookfoundation.github.io/free-programming-books-search/"&gt;&lt;img src="https://img.shields.io/website?style=flat&amp;amp;logo=www&amp;amp;logoColor=whitesmoke&amp;amp;label=Dynamic%20search%20site&amp;amp;down_color=red&amp;amp;down_message=down&amp;amp;up_color=green&amp;amp;up_message=up&amp;amp;url=https%3A%2F%2Febookfoundation.github.io%2Ffree-programming-books-search%2F" alt="https://ebookfoundation.github.io/free-programming-books-search/" /&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This page is available as an easy-to-read website. Access it by clicking on &lt;a href="https://ebookfoundation.github.io/free-programming-books/"&gt;&lt;img src="https://img.shields.io/website?style=flat&amp;amp;logo=www&amp;amp;logoColor=whitesmoke&amp;amp;label=Static%20site&amp;amp;down_color=red&amp;amp;down_message=down&amp;amp;up_color=green&amp;amp;up_message=up&amp;amp;url=https%3A%2F%2Febookfoundation.github.io%2Ffree-programming-books%2F" alt="https://ebookfoundation.github.io/free-programming-books/" /&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;form action="https://ebookfoundation.github.io/free-programming-books-search"&gt; 
  &lt;input type="text" id="fpbSearch" name="search" required placeholder="Search Book or Author" /&gt; 
  &lt;label for="submit"&gt; &lt;/label&gt; 
  &lt;input type="submit" id="submit" name="submit" value="Search" /&gt; 
 &lt;/form&gt; 
&lt;/div&gt; 
&lt;h2&gt;Intro&lt;/h2&gt; 
&lt;p&gt;This list was originally a clone of &lt;a href="https://web.archive.org/web/20140606191453/http://stackoverflow.com/questions/194812/list-of-freely-available-programming-books/392926"&gt;StackOverflow - List of Freely Available Programming Books&lt;/a&gt; with contributions from Karan Bhangui and George Stocker.&lt;/p&gt; 
&lt;p&gt;The list was moved to GitHub by Victor Felder for collaborative updating and maintenance. It has grown to become one of &lt;a href="https://octoverse.github.com/"&gt;GitHub's most popular repositories&lt;/a&gt;.&lt;/p&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/EbookFoundation/free-programming-books/network"&gt;&lt;img src="https://img.shields.io/github/forks/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Forks" alt="GitHub repo forks" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Stars" alt="GitHub repo stars" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors-anon/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Contributors" alt="GitHub repo contributors" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/sponsors/EbookFoundation"&gt;&lt;img src="https://img.shields.io/github/sponsors/EbookFoundation?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Sponsors" alt="GitHub org sponsors" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/watchers"&gt;&lt;img src="https://img.shields.io/github/watchers/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Watchers" alt="GitHub repo watchers" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/archive/refs/heads/main.zip"&gt;&lt;img src="https://img.shields.io/github/repo-size/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Repo%20Size" alt="GitHub repo size" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;The &lt;a href="https://ebookfoundation.org"&gt;Free Ebook Foundation&lt;/a&gt; now administers the repo, a not-for-profit organization devoted to promoting the creation, distribution, archiving, and sustainability of free ebooks. &lt;a href="https://ebookfoundation.org/contributions.html"&gt;Donations&lt;/a&gt; to the Free Ebook Foundation are tax-deductible in the US.&lt;/p&gt; 
&lt;h2&gt;How To Contribute&lt;/h2&gt; 
&lt;p&gt;Please read &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt;. If you're new to GitHub, &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/HOWTO.md"&gt;welcome&lt;/a&gt;! Remember to abide by our adapted from &lt;img src="https://img.shields.io/badge/Contributor%20Covenant-1.3-4baaaa.svg?sanitize=true" alt="Contributor Covenant 1.3" /&gt; &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt; too (&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/#translations"&gt;translations&lt;/a&gt; also available).&lt;/p&gt; 
&lt;p&gt;Click on these badges to see how you might be able to help:&lt;/p&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/EbookFoundation/free-programming-books/issues"&gt;&lt;img src="https://img.shields.io/github/issues/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=red&amp;amp;label=Issues" alt="GitHub repo Issues" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22"&gt;&lt;img src="https://img.shields.io/github/issues/EbookFoundation/free-programming-books/good%20first%20issue?style=flat&amp;amp;logo=github&amp;amp;logoColor=green&amp;amp;label=Good%20First%20issues" alt="GitHub repo Good Issues for newbies" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22"&gt;&lt;img src="https://img.shields.io/github/issues/EbookFoundation/free-programming-books/help%20wanted?style=flat&amp;amp;logo=github&amp;amp;logoColor=b545d1&amp;amp;label=%22Help%20Wanted%22%20issues" alt="GitHub Help Wanted issues" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls"&gt;&lt;img src="https://img.shields.io/github/issues-pr/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=orange&amp;amp;label=PRs" alt="GitHub repo PRs" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Apr+is%3Amerged"&gt;&lt;img src="https://img.shields.io/github/issues-search/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=green&amp;amp;label=Merged%20PRs&amp;amp;query=is%3Amerged" alt="GitHub repo Merged PRs" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22"&gt;&lt;img src="https://img.shields.io/github/issues-pr/EbookFoundation/free-programming-books/help%20wanted?style=flat&amp;amp;logo=github&amp;amp;logoColor=b545d1&amp;amp;label=%22Help%20Wanted%22%20PRs" alt="GitHub Help Wanted PRs" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;How To Share&lt;/h2&gt; 
&lt;div align="left" markdown="1"&gt; 
 &lt;a href="https://www.facebook.com/share.php?u=https%3A%2F%2Fgithub.com%2FEbookFoundation%2Ffree-programming-books&amp;amp;p%5Bimages%5D%5B0%5D=&amp;amp;p%5Btitle%5D=Free%20Programming%20Books&amp;amp;p%5Bsummary%5D="&gt;Share on Facebook&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://github.com/EbookFoundation/free-programming-books&amp;amp;title=Free%20Programming%20Books&amp;amp;summary=&amp;amp;source="&gt;Share on LinkedIn&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="https://toot.kytta.dev/?text=https://github.com/EbookFoundation/free-programming-books"&gt;Share on Mastodon/Fediverse&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="https://t.me/share/url?url=https://github.com/EbookFoundation/free-programming-books"&gt;Share on Telegram&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="https://twitter.com/intent/tweet?text=https://github.com/EbookFoundation/free-programming-books%0AFree%20Programming%20Books"&gt;Share on 𝕏 (Twitter)&lt;/a&gt;
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Resources&lt;/h2&gt; 
&lt;p&gt;This project lists books and other resources grouped by genres:&lt;/p&gt; 
&lt;h3&gt;Books&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-langs.md"&gt;English, By Programming Language&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-subjects.md"&gt;English, By Subject&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Other Languages&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ar.md"&gt;Arabic / al arabiya / العربية&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-hy.md"&gt;Armenian / Հայերեն&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-az.md"&gt;Azerbaijani / Азәрбајҹан дили / آذربايجانجا ديلي&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-bn.md"&gt;Bengali / বাংলা&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-bg.md"&gt;Bulgarian / български&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-my.md"&gt;Burmese / မြန်မာဘာသာ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-zh.md"&gt;Chinese / 中文&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-cs.md"&gt;Czech / čeština / český jazyk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ca.md"&gt;Catalan / catalan / català&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-da.md"&gt;Danish / dansk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-nl.md"&gt;Dutch / Nederlands&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-et.md"&gt;Estonian / eesti keel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-fi.md"&gt;Finnish / suomi / suomen kieli&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-fr.md"&gt;French / français&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-el.md"&gt;Greek / ελληνικά&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-he.md"&gt;Hebrew / עברית&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-hi.md"&gt;Hindi / हिन्दी&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-hu.md"&gt;Hungarian / magyar / magyar nyelv&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-id.md"&gt;Indonesian / Bahasa Indonesia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-it.md"&gt;Italian / italiano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ja.md"&gt;Japanese / 日本語&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ko.md"&gt;Korean / 한국어&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-lv.md"&gt;Latvian / Latviešu&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ml.md"&gt;Malayalam / മലയാളം&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-no.md"&gt;Norwegian / Norsk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-fa_IR.md"&gt;Persian / Farsi (Iran) / فارسى&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-pl.md"&gt;Polish / polski / język polski / polszczyzna&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-pt_BR.md"&gt;Portuguese (Brazil)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-pt_PT.md"&gt;Portuguese (Portugal)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ro.md"&gt;Romanian (Romania) / limba română / român&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ru.md"&gt;Russian / Русский язык&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-sr.md"&gt;Serbian / српски језик / srpski jezik&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-sk.md"&gt;Slovak / slovenčina&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-es.md"&gt;Spanish / español / castellano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-sv.md"&gt;Swedish / Svenska&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ta.md"&gt;Tamil / தமிழ்&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-te.md"&gt;Telugu / తెలుగు&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-th.md"&gt;Thai / ไทย&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-tr.md"&gt;Turkish / Türkçe&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-uk.md"&gt;Ukrainian / Українська&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ur.md"&gt;Urdu / اردو&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-vi.md"&gt;Vietnamese / Tiếng Việt&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Cheat Sheets&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-cheatsheets.md"&gt;All Languages&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Free Online Courses&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ar.md"&gt;Arabic / al arabiya / العربية&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-bn.md"&gt;Bengali / বাংলা&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-bg.md"&gt;Bulgarian / български&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-my.md"&gt;Burmese / မြန်မာဘာသာ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-zh.md"&gt;Chinese / 中文&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-en.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-fi.md"&gt;Finnish / suomi / suomen kieli&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-fr.md"&gt;French / français&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-el.md"&gt;Greek / ελληνικά&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-he.md"&gt;Hebrew / עברית&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-hi.md"&gt;Hindi / हिंदी&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-id.md"&gt;Indonesian / Bahasa Indonesia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-it.md"&gt;Italian / italiano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ja.md"&gt;Japanese / 日本語&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-kn.md"&gt;Kannada / ಕನ್ನಡ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-kk.md"&gt;Kazakh / қазақша&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-km.md"&gt;Khmer / ភាសាខ្មែរ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ko.md"&gt;Korean / 한국어&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ml.md"&gt;Malayalam / മലയാളം&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-mr.md"&gt;Marathi / मराठी&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ne.md"&gt;Nepali / नेपाली&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-no.md"&gt;Norwegian / Norsk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-fa_IR.md"&gt;Persian / Farsi (Iran) / فارسى&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-pl.md"&gt;Polish / polski / język polski / polszczyzna&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-pt_BR.md"&gt;Portuguese (Brazil)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-pt_PT.md"&gt;Portuguese (Portugal)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-pa.md"&gt;Punjabi / ਪੰਜਾਬੀ / پنجابی&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ro.md"&gt;Romanian (Romania) / limba română / român&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ru.md"&gt;Russian / Русский язык&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-si.md"&gt;Sinhala / සිංහල&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-es.md"&gt;Spanish / español / castellano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-sv.md"&gt;Swedish / svenska&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ta.md"&gt;Tamil / தமிழ்&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-te.md"&gt;Telugu / తెలుగు&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-th.md"&gt;Thai / ภาษาไทย&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-tr.md"&gt;Turkish / Türkçe&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-uk.md"&gt;Ukrainian / Українська&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ur.md"&gt;Urdu / اردو&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-vi.md"&gt;Vietnamese / Tiếng Việt&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Interactive Programming Resources&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-zh.md"&gt;Chinese / 中文&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-en.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-ja.md"&gt;Japanese / 日本語&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-ru.md"&gt;Russian / Русский язык&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Problem Sets and Competitive Programming&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/problem-sets-competitive-programming.md"&gt;Problem Sets&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Podcast - Screencast&lt;/h3&gt; 
&lt;p&gt;Free Podcasts and Screencasts:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-ar.md"&gt;Arabic / al Arabiya / العربية&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-my.md"&gt;Burmese / မြန်မာဘာသာ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-zh.md"&gt;Chinese / 中文&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-cs.md"&gt;Czech / čeština / český jazyk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-nl.md"&gt;Dutch / Nederlands&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-en.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-fi.md"&gt;Finnish / Suomi&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-fr.md"&gt;French / français&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-he.md"&gt;Hebrew / עברית&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-id.md"&gt;Indonesian / Bahasa Indonesia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-fa_IR.md"&gt;Persian / Farsi (Iran) / فارسى&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-pl.md"&gt;Polish / polski / język polski / polszczyzna&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-pt_BR.md"&gt;Portuguese (Brazil)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-pt_PT.md"&gt;Portuguese (Portugal)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-ru.md"&gt;Russian / Русский язык&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-si.md"&gt;Sinhala / සිංහල&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-es.md"&gt;Spanish / español / castellano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-sv.md"&gt;Swedish / Svenska&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-tr.md"&gt;Turkish / Türkçe&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-uk.md"&gt;Ukrainian / Українська&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Programming Playgrounds&lt;/h3&gt; 
&lt;p&gt;Write, compile, and run your code within a browser. Try it out!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-playgrounds-zh.md"&gt;Chinese / 中文&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-playgrounds.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-playgrounds-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Translations&lt;/h2&gt; 
&lt;p&gt;Volunteers have translated many of our Contributing, How-to, and Code of Conduct documents into languages covered by our lists.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;English 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CONTRIBUTING.md"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/HOWTO.md"&gt;How-to&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;... &lt;em&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/README.md#translations"&gt;More languages&lt;/a&gt;&lt;/em&gt; ...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You might notice that there are &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/README.md#translations"&gt;some missing translations here&lt;/a&gt; - perhaps you would like to help out by &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CONTRIBUTING.md#help-out-by-contributing-a-translation"&gt;contributing a translation&lt;/a&gt;?&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Each file included in this repository is licensed under the &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/LICENSE"&gt;CC BY License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>TheAlgorithms/Python</title>
      <link>https://github.com/TheAlgorithms/Python</link>
      <description>&lt;p&gt;All Algorithms implemented in Python&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;!-- Title: --&gt; 
 &lt;a href="https://github.com/TheAlgorithms/"&gt; &lt;img src="https://raw.githubusercontent.com/TheAlgorithms/website/1cd824df116b27029f17c2d1b42d81731f28a920/public/logo.svg?sanitize=true" height="100" /&gt; &lt;/a&gt; 
 &lt;h1&gt;&lt;a href="https://github.com/TheAlgorithms/"&gt;The Algorithms&lt;/a&gt; - Python&lt;/h1&gt; 
 &lt;!-- Labels: --&gt; 
 &lt;!-- First row: --&gt; 
 &lt;a href="https://gitpod.io/#https://github.com/TheAlgorithms/Python"&gt; &lt;img src="https://img.shields.io/badge/Gitpod-Ready--to--Code-blue?logo=gitpod&amp;amp;style=flat-square" height="20" alt="Gitpod Ready-to-Code" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/TheAlgorithms/Python/raw/master/CONTRIBUTING.md"&gt; &lt;img src="https://img.shields.io/static/v1.svg?label=Contributions&amp;amp;message=Welcome&amp;amp;color=0059b3&amp;amp;style=flat-square" height="20" alt="Contributions Welcome" /&gt; &lt;/a&gt; 
 &lt;img src="https://img.shields.io/github/repo-size/TheAlgorithms/Python.svg?label=Repo%20size&amp;amp;style=flat-square" height="20" /&gt; 
 &lt;a href="https://the-algorithms.com/discord"&gt; &lt;img src="https://img.shields.io/discord/808045925556682782.svg?logo=discord&amp;amp;colorB=7289DA&amp;amp;style=flat-square" height="20" alt="Discord chat" /&gt; &lt;/a&gt; 
 &lt;a href="https://gitter.im/TheAlgorithms/community"&gt; &lt;img src="https://img.shields.io/badge/Chat-Gitter-ff69b4.svg?label=Chat&amp;amp;logo=gitter&amp;amp;style=flat-square" height="20" alt="Gitter chat" /&gt; &lt;/a&gt; 
 &lt;!-- Second row: --&gt; 
 &lt;br /&gt; 
 &lt;a href="https://github.com/TheAlgorithms/Python/actions"&gt; &lt;img src="https://img.shields.io/github/actions/workflow/status/TheAlgorithms/Python/build.yml?branch=master&amp;amp;label=CI&amp;amp;logo=github&amp;amp;style=flat-square" height="20" alt="GitHub Workflow Status" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/pre-commit/pre-commit"&gt; &lt;img src="https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&amp;amp;logoColor=white&amp;amp;style=flat-square" height="20" alt="pre-commit" /&gt; &lt;/a&gt; 
 &lt;a href="https://docs.astral.sh/ruff/formatter/"&gt; &lt;img src="https://img.shields.io/static/v1?label=code%20style&amp;amp;message=ruff&amp;amp;color=black&amp;amp;style=flat-square" height="20" alt="code style: black" /&gt; &lt;/a&gt; 
 &lt;!-- Short description: --&gt; 
 &lt;h3&gt;All algorithms implemented in Python - for education 📚&lt;/h3&gt; 
&lt;/div&gt; 
&lt;p&gt;Implementations are for learning purposes only. They may be less efficient than the implementations in the Python standard library. Use them at your discretion.&lt;/p&gt; 
&lt;h2&gt;🚀 Getting Started&lt;/h2&gt; 
&lt;p&gt;📋 Read through our &lt;a href="https://raw.githubusercontent.com/TheAlgorithms/Python/master/CONTRIBUTING.md"&gt;Contribution Guidelines&lt;/a&gt; before you contribute.&lt;/p&gt; 
&lt;h2&gt;🌐 Community Channels&lt;/h2&gt; 
&lt;p&gt;We are on &lt;a href="https://the-algorithms.com/discord"&gt;Discord&lt;/a&gt; and &lt;a href="https://gitter.im/TheAlgorithms/community"&gt;Gitter&lt;/a&gt;! Community channels are a great way for you to ask questions and get help. Please join us!&lt;/p&gt; 
&lt;h2&gt;📜 List of Algorithms&lt;/h2&gt; 
&lt;p&gt;See our &lt;a href="https://raw.githubusercontent.com/TheAlgorithms/Python/master/DIRECTORY.md"&gt;directory&lt;/a&gt; for easier navigation and a better overview of the project.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>karpathy/micrograd</title>
      <link>https://github.com/karpathy/micrograd</link>
      <description>&lt;p&gt;A tiny scalar-valued autograd engine and a neural net library on top of it with PyTorch-like API&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;micrograd&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/karpathy/micrograd/master/puppy.jpg" alt="awww" /&gt;&lt;/p&gt; 
&lt;p&gt;A tiny Autograd engine (with a bite! :)). Implements backpropagation (reverse-mode autodiff) over a dynamically built DAG and a small neural networks library on top of it with a PyTorch-like API. Both are tiny, with about 100 and 50 lines of code respectively. The DAG only operates over scalar values, so e.g. we chop up each neuron into all of its individual tiny adds and multiplies. However, this is enough to build up entire deep neural nets doing binary classification, as the demo notebook shows. Potentially useful for educational purposes.&lt;/p&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install micrograd
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Example usage&lt;/h3&gt; 
&lt;p&gt;Below is a slightly contrived example showing a number of possible supported operations:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from micrograd.engine import Value

a = Value(-4.0)
b = Value(2.0)
c = a + b
d = a * b + b**3
c += c + 1
c += 1 + c + (-a)
d += d * 2 + (b + a).relu()
d += 3 * d + (b - a).relu()
e = c - d
f = e**2
g = f / 2.0
g += 10.0 / f
print(f'{g.data:.4f}') # prints 24.7041, the outcome of this forward pass
g.backward()
print(f'{a.grad:.4f}') # prints 138.8338, i.e. the numerical value of dg/da
print(f'{b.grad:.4f}') # prints 645.5773, i.e. the numerical value of dg/db
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Training a neural net&lt;/h3&gt; 
&lt;p&gt;The notebook &lt;code&gt;demo.ipynb&lt;/code&gt; provides a full demo of training an 2-layer neural network (MLP) binary classifier. This is achieved by initializing a neural net from &lt;code&gt;micrograd.nn&lt;/code&gt; module, implementing a simple svm "max-margin" binary classification loss and using SGD for optimization. As shown in the notebook, using a 2-layer neural net with two 16-node hidden layers we achieve the following decision boundary on the moon dataset:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/karpathy/micrograd/master/moon_mlp.png" alt="2d neuron" /&gt;&lt;/p&gt; 
&lt;h3&gt;Tracing / visualization&lt;/h3&gt; 
&lt;p&gt;For added convenience, the notebook &lt;code&gt;trace_graph.ipynb&lt;/code&gt; produces graphviz visualizations. E.g. this one below is of a simple 2D neuron, arrived at by calling &lt;code&gt;draw_dot&lt;/code&gt; on the code below, and it shows both the data (left number in each node) and the gradient (right number in each node).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from micrograd import nn
n = nn.Neuron(2)
x = [Value(1.0), Value(-2.0)]
y = n(x)
dot = draw_dot(y)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/karpathy/micrograd/master/gout.svg?sanitize=true" alt="2d neuron" /&gt;&lt;/p&gt; 
&lt;h3&gt;Running tests&lt;/h3&gt; 
&lt;p&gt;To run the unit tests you will have to install &lt;a href="https://pytorch.org/"&gt;PyTorch&lt;/a&gt;, which the tests use as a reference for verifying the correctness of the calculated gradients. Then simply:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m pytest
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;License&lt;/h3&gt; 
&lt;p&gt;MIT&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>x1xhlol/system-prompts-and-models-of-ai-tools</title>
      <link>https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools</link>
      <description>&lt;p&gt;FULL Augment Code, Claude Code, Cluely, CodeBuddy, Comet, Cursor, Devin AI, Junie, Kiro, Leap.new, Lovable, Manus Agent Tools, NotionAI, Orchids.app, Perplexity, Poke, Qoder, Replit, Same.dev, Trae, Traycer AI, VSCode Agent, Warp.dev, Windsurf, Xcode, Z.ai Code, dia &amp; v0. (And other Open Sourced) System Prompts, Internal Tools &amp; AI Models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;strong&gt;System Prompts and Models of AI Tools&lt;/strong&gt;&lt;/h1&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; &lt;sub&gt;Special thanks to&lt;/sub&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://latitude.so/developers?utm_source=github&amp;amp;utm_medium=readme&amp;amp;utm_campaign=prompt_repo_sponsorship" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/assets/Latitude_logo.png" alt="Latitude Logo" width="700" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;h3&gt;&lt;a href="https://latitude.so/developers?utm_source=github&amp;amp;utm_medium=readme&amp;amp;utm_campaign=prompt_repo_sponsorship" target="_blank"&gt;The tools you need for building reliable Agents and Prompts&lt;/a&gt;&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://latitude.so/developers?utm_source=github&amp;amp;utm_medium=readme&amp;amp;utm_campaign=prompt_repo_sponsorship" target="_blank"&gt;Open Source AI Engineering Platform&lt;/a&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;a href="https://discord.gg/NwzrWErdMU" target="_blank"&gt; &lt;img src="https://img.shields.io/discord/1402660735833604126?label=LeaksLab%20Discord&amp;amp;logo=discord&amp;amp;style=for-the-badge" alt="LeaksLab Discord" /&gt; &lt;/a&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Join the Conversation:&lt;/strong&gt; New system instructions are released on Discord &lt;strong&gt;before&lt;/strong&gt; they appear in this repository. Get early access and discuss them in real time.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://trendshift.io/repositories/14084" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14084" alt="x1xhlol%2Fsystem-prompts-and-models-of-ai-tools | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;📜 Over &lt;strong&gt;30,000+ lines&lt;/strong&gt; of insights into their structure and functionality.&lt;/p&gt; 
&lt;p&gt;⭐ &lt;strong&gt;Star to follow updates&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://cloudback.it"&gt;&lt;img src="https://app.cloudback.it/badge/x1xhlol/system-prompts-and-models-of-ai-tools" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/x1xhlol/system-prompts-and-models-of-ai-tools"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;❤️ Support the Project&lt;/h2&gt; 
&lt;p&gt;If you find this collection valuable and appreciate the effort involved in obtaining and sharing these insights, please consider supporting the project. Your contribution helps keep this resource updated and allows for further exploration.&lt;/p&gt; 
&lt;p&gt;You can show your support via:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;PayPal:&lt;/strong&gt; &lt;code&gt;lucknitelol@proton.me&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cryptocurrency:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;BTC:&lt;/strong&gt; &lt;code&gt;bc1q7zldmzjwspnaa48udvelwe6k3fef7xrrhg5625&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;LTC:&lt;/strong&gt; &lt;code&gt;LRWgqwEYDwqau1WeiTs6Mjg85NJ7m3fsdQ&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;ETH:&lt;/strong&gt; &lt;code&gt;0x3f844B2cc3c4b7242964373fB0A41C4fdffB192A&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Patreon:&lt;/strong&gt; &lt;a href="https://patreon.com/lucknite"&gt;https://patreon.com/lucknite&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ko-fi:&lt;/strong&gt; &lt;a href="https://ko-fi.com/lucknite"&gt;https://ko-fi.com/lucknite&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;🙏 Thank you for your support!&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;Sponsors&lt;/h1&gt; 
&lt;h2&gt;Support the Future of AI Development&lt;/h2&gt; 
&lt;p&gt;Sponsor the most comprehensive collection of AI system prompts and reach thousands of developers building the next generation of AI applications.&lt;/p&gt; 
&lt;p&gt;&lt;a href="mailto:lucknitelol@proton.me"&gt;Get Started&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;📑 Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/#-table-of-contents"&gt;📑 Table of Contents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/#-available-files"&gt;📂 Available Files&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/#-roadmap--feedback"&gt;🛠 Roadmap &amp;amp; Feedback&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/#-connect-with-me"&gt;🔗 Connect With Me&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/#%EF%B8%8F-security-notice-for-ai-startups"&gt;🛡️ Security Notice for AI Startups&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/#-star-history"&gt;📊 Star History&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;📂 Available Files&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/v0%20Prompts%20and%20Tools/"&gt;&lt;strong&gt;v0&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/Manus%20Agent%20Tools%20&amp;amp;%20Prompt/"&gt;&lt;strong&gt;Manus&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/Augment%20Code/"&gt;&lt;strong&gt;Augment Code&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/Lovable/"&gt;&lt;strong&gt;Lovable&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/Devin%20AI/"&gt;&lt;strong&gt;Devin&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/Same.dev/"&gt;&lt;strong&gt;Same.dev&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/Replit/"&gt;&lt;strong&gt;Replit&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/Windsurf/"&gt;&lt;strong&gt;Windsurf Agent&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/VSCode%20Agent/"&gt;&lt;strong&gt;VSCode (Copilot) Agent&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/Cursor%20Prompts/"&gt;&lt;strong&gt;Cursor&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/dia/"&gt;&lt;strong&gt;Dia&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/Trae/"&gt;&lt;strong&gt;Trae AI&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/Perplexity/"&gt;&lt;strong&gt;Perplexity&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/Cluely/"&gt;&lt;strong&gt;Cluely&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/Xcode/"&gt;&lt;strong&gt;Xcode&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/Leap.new/"&gt;&lt;strong&gt;Leap.new&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/NotionAi/"&gt;&lt;strong&gt;Notion AI&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/Orchids.app/"&gt;&lt;strong&gt;Orchids.app&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/Junie/"&gt;&lt;strong&gt;Junie&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/Kiro/"&gt;&lt;strong&gt;Kiro&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/Warp.dev/"&gt;&lt;strong&gt;Warp.dev&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/Z.ai%20Code/"&gt;&lt;strong&gt;Z.ai Code&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/Qoder/"&gt;&lt;strong&gt;Qoder&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/Claude%20Code/"&gt;&lt;strong&gt;Claude Code&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/Open%20Source%20prompts/"&gt;&lt;strong&gt;Open Source prompts&lt;/strong&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/Open%20Source%20prompts/Codex%20CLI/"&gt;Codex CLI&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/Open%20Source%20prompts/Cline/"&gt;Cline&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/Open%20Source%20prompts/Bolt/"&gt;Bolt&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/Open%20Source%20prompts/RooCode/"&gt;RooCode&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/Open%20Source%20prompts/Lumo/"&gt;Lumo&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/Open%20Source%20prompts/Gemini%20CLI/"&gt;Gemini CLI&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/CodeBuddy%20Prompts/"&gt;&lt;strong&gt;CodeBuddy&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/Poke/"&gt;&lt;strong&gt;Poke&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/Comet%20Assistant/"&gt;&lt;strong&gt;Comet Assistant&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/Anthropic/"&gt;&lt;strong&gt;Anthropic&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/x1xhlol/system-prompts-and-models-of-ai-tools/main/AMp/"&gt;&lt;strong&gt;Amp&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🛠 Roadmap &amp;amp; Feedback&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Open an issue.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Latest Update:&lt;/strong&gt; 19/10/2025&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🔗 Connect With Me&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;X:&lt;/strong&gt; &lt;a href="https://x.com/NotLucknite"&gt;NotLucknite&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Discord&lt;/strong&gt;: &lt;code&gt;x1xh&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🛡️ Security Notice for AI Startups&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;⚠️ &lt;strong&gt;Warning:&lt;/strong&gt; If you're an AI startup, make sure your data is secure. Exposed prompts or AI models can easily become a target for hackers.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;🔐 &lt;strong&gt;Important:&lt;/strong&gt; Interested in securing your AI systems?&lt;br /&gt; Check out &lt;strong&gt;&lt;a href="https://zeroleaks.io/"&gt;ZeroLeaks&lt;/a&gt;&lt;/strong&gt;, a service designed to help startups &lt;strong&gt;identify and secure&lt;/strong&gt; leaks in system instructions, internal tools, and model configurations. &lt;strong&gt;Get a free AI security audit&lt;/strong&gt; to ensure your AI is protected from vulnerabilities.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;em&gt;The company is mine, this is NOT a 3rd party AD.&lt;/em&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;📊 Star History&lt;/h2&gt; 
&lt;a href="https://www.star-history.com/#x1xhlol/system-prompts-and-models-of-ai-tools&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=x1xhlol/system-prompts-and-models-of-ai-tools&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=x1xhlol/system-prompts-and-models-of-ai-tools&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=x1xhlol/system-prompts-and-models-of-ai-tools&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;p&gt;⭐ &lt;strong&gt;Drop a star if you find this useful!&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>huggingface/chat-ui</title>
      <link>https://github.com/huggingface/chat-ui</link>
      <description>&lt;p&gt;Open source codebase powering the HuggingChat app&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Chat UI&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/chat-ui/chat-ui-2026.png" alt="Chat UI repository thumbnail" /&gt;&lt;/p&gt; 
&lt;p&gt;A chat interface for LLMs. It is a SvelteKit app and it powers the &lt;a href="https://huggingface.co/chat"&gt;HuggingChat app on hf.co/chat&lt;/a&gt;.&lt;/p&gt; 
&lt;ol start="0"&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/chat-ui/main/#quickstart"&gt;Quickstart&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/chat-ui/main/#database-options"&gt;Database Options&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/chat-ui/main/#launch"&gt;Launch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/chat-ui/main/#optional-docker-image"&gt;Optional Docker Image&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/chat-ui/main/#extra-parameters"&gt;Extra parameters&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/chat-ui/main/#building"&gt;Building&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Chat UI only supports OpenAI-compatible APIs via &lt;code&gt;OPENAI_BASE_URL&lt;/code&gt; and the &lt;code&gt;/models&lt;/code&gt; endpoint. Provider-specific integrations (legacy &lt;code&gt;MODELS&lt;/code&gt; env var, GGUF discovery, embeddings, web-search helpers, etc.) are removed, but any service that speaks the OpenAI protocol (llama.cpp server, Ollama, OpenRouter, etc. will work by default).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] The old version is still available on the &lt;a href="https://github.com/huggingface/chat-ui/tree/legacy"&gt;legacy branch&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;Chat UI speaks to OpenAI-compatible APIs only. The fastest way to get running is with the Hugging Face Inference Providers router plus your personal Hugging Face access token.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Step 1 – Create &lt;code&gt;.env.local&lt;/code&gt;:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-env"&gt;OPENAI_BASE_URL=https://router.huggingface.co/v1
OPENAI_API_KEY=hf_************************
# Fill in once you pick a database option below
MONGODB_URL=
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;OPENAI_API_KEY&lt;/code&gt; can come from any OpenAI-compatible endpoint you plan to call. Pick the combo that matches your setup and drop the values into &lt;code&gt;.env.local&lt;/code&gt;:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Provider&lt;/th&gt; 
   &lt;th&gt;Example &lt;code&gt;OPENAI_BASE_URL&lt;/code&gt;&lt;/th&gt; 
   &lt;th&gt;Example key env&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Hugging Face Inference Providers router&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://router.huggingface.co/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_API_KEY=hf_xxx&lt;/code&gt; (or &lt;code&gt;HF_TOKEN&lt;/code&gt; legacy alias)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;llama.cpp server (&lt;code&gt;llama.cpp --server --api&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;http://127.0.0.1:8080/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_API_KEY=sk-local-demo&lt;/code&gt; (any string works; llama.cpp ignores it)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ollama (with OpenAI-compatible bridge)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;http://127.0.0.1:11434/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_API_KEY=ollama&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenRouter&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://openrouter.ai/api/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_API_KEY=sk-or-v1-...&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Poe&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://api.poe.com/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_API_KEY=pk_...&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Check the root &lt;a href="https://raw.githubusercontent.com/huggingface/chat-ui/main/.env"&gt;&lt;code&gt;.env&lt;/code&gt; template&lt;/a&gt; for the full list of optional variables you can override.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Step 2 – Choose where MongoDB lives:&lt;/strong&gt; Either provision a managed cluster (for example MongoDB Atlas) or run a local container. Both approaches are described in &lt;a href="https://raw.githubusercontent.com/huggingface/chat-ui/main/#database-options"&gt;Database Options&lt;/a&gt;. After you have the URI, drop it into &lt;code&gt;MONGODB_URL&lt;/code&gt; (and, if desired, set &lt;code&gt;MONGODB_DB_NAME&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Step 3 – Install and launch the dev server:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/huggingface/chat-ui
cd chat-ui
npm install
npm run dev -- --open
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You now have Chat UI running against the Hugging Face router without needing to host MongoDB yourself.&lt;/p&gt; 
&lt;h2&gt;Database Options&lt;/h2&gt; 
&lt;p&gt;Chat history, users, settings, files, and stats all live in MongoDB. You can point Chat UI at any MongoDB 6/7 deployment.&lt;/p&gt; 
&lt;h3&gt;MongoDB Atlas (managed)&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Create a free cluster at &lt;a href="https://www.mongodb.com/pricing"&gt;mongodb.com&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Add your IP (or &lt;code&gt;0.0.0.0/0&lt;/code&gt; for development) to the network access list.&lt;/li&gt; 
 &lt;li&gt;Create a database user and copy the connection string.&lt;/li&gt; 
 &lt;li&gt;Paste that string into &lt;code&gt;MONGODB_URL&lt;/code&gt; in &lt;code&gt;.env.local&lt;/code&gt;. Keep the default &lt;code&gt;MONGODB_DB_NAME=chat-ui&lt;/code&gt; or change it per environment.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Atlas keeps MongoDB off your laptop, which is ideal for teams or cloud deployments.&lt;/p&gt; 
&lt;h3&gt;Local MongoDB (container)&lt;/h3&gt; 
&lt;p&gt;If you prefer to run MongoDB locally:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d -p 27017:27017 --name mongo-chatui mongo:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then set &lt;code&gt;MONGODB_URL=mongodb://localhost:27017&lt;/code&gt; in &lt;code&gt;.env.local&lt;/code&gt;. You can also supply &lt;code&gt;MONGO_STORAGE_PATH&lt;/code&gt; if you want Chat UI’s fallback in-memory server to persist under a specific folder.&lt;/p&gt; 
&lt;h2&gt;Launch&lt;/h2&gt; 
&lt;p&gt;After configuring your environment variables, start Chat UI with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm install
npm run dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The dev server listens on &lt;code&gt;http://localhost:5173&lt;/code&gt; by default. Use &lt;code&gt;npm run build&lt;/code&gt; / &lt;code&gt;npm run preview&lt;/code&gt; for production builds.&lt;/p&gt; 
&lt;h2&gt;Optional Docker Image&lt;/h2&gt; 
&lt;p&gt;Prefer containerized setup? You can run everything in one container as long as you supply a MongoDB URI (local or hosted):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run \
  -p 3000 \
  -e MONGODB_URL=mongodb://host.docker.internal:27017 \
  -e OPENAI_BASE_URL=https://router.huggingface.co/v1 \
  -e OPENAI_API_KEY=hf_*** \
  -v db:/data \
  ghcr.io/huggingface/chat-ui-db:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;host.docker.internal&lt;/code&gt; lets the container reach a MongoDB instance on your host machine; swap it for your Atlas URI if you use the hosted option. All environment variables accepted in &lt;code&gt;.env.local&lt;/code&gt; can be provided as &lt;code&gt;-e&lt;/code&gt; flags.&lt;/p&gt; 
&lt;h2&gt;Extra parameters&lt;/h2&gt; 
&lt;h3&gt;Theming&lt;/h3&gt; 
&lt;p&gt;You can use a few environment variables to customize the look and feel of chat-ui. These are by default:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-env"&gt;PUBLIC_APP_NAME=ChatUI
PUBLIC_APP_ASSETS=chatui
PUBLIC_APP_DESCRIPTION="Making the community's best AI chat models available to everyone."
PUBLIC_APP_DATA_SHARING=
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;PUBLIC_APP_NAME&lt;/code&gt; The name used as a title throughout the app.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;PUBLIC_APP_ASSETS&lt;/code&gt; Is used to find logos &amp;amp; favicons in &lt;code&gt;static/$PUBLIC_APP_ASSETS&lt;/code&gt;, current options are &lt;code&gt;chatui&lt;/code&gt; and &lt;code&gt;huggingchat&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;PUBLIC_APP_DATA_SHARING&lt;/code&gt; Can be set to 1 to add a toggle in the user settings that lets your users opt-in to data sharing with models creator.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Models&lt;/h3&gt; 
&lt;p&gt;This build does not use the &lt;code&gt;MODELS&lt;/code&gt; env var or GGUF discovery. Configure models via &lt;code&gt;OPENAI_BASE_URL&lt;/code&gt; only; Chat UI will fetch &lt;code&gt;${OPENAI_BASE_URL}/models&lt;/code&gt; and populate the list automatically. Authorization uses &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; (preferred). &lt;code&gt;HF_TOKEN&lt;/code&gt; remains a legacy alias.&lt;/p&gt; 
&lt;h3&gt;LLM Router (Optional)&lt;/h3&gt; 
&lt;p&gt;Chat UI can perform client-side routing &lt;a href="https://huggingface.co/katanemo/Arch-Router-1.5B"&gt;katanemo/Arch-Router-1.5B&lt;/a&gt; as the routing model without running a separate router service. The UI exposes a virtual model alias called "Omni" (configurable) that, when selected, chooses the best route/model for each message.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Provide a routes policy JSON via &lt;code&gt;LLM_ROUTER_ROUTES_PATH&lt;/code&gt;. No sample file ships with this branch, so you must point the variable to a JSON array you create yourself (for example, commit one in your project like &lt;code&gt;config/routes.chat.json&lt;/code&gt;). Each route entry needs &lt;code&gt;name&lt;/code&gt;, &lt;code&gt;description&lt;/code&gt;, &lt;code&gt;primary_model&lt;/code&gt;, and optional &lt;code&gt;fallback_models&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Configure the Arch router selection endpoint with &lt;code&gt;LLM_ROUTER_ARCH_BASE_URL&lt;/code&gt; (OpenAI-compatible &lt;code&gt;/chat/completions&lt;/code&gt;) and &lt;code&gt;LLM_ROUTER_ARCH_MODEL&lt;/code&gt; (e.g. &lt;code&gt;router/omni&lt;/code&gt;). The Arch call reuses &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; for auth.&lt;/li&gt; 
 &lt;li&gt;Map &lt;code&gt;other&lt;/code&gt; to a concrete route via &lt;code&gt;LLM_ROUTER_OTHER_ROUTE&lt;/code&gt; (default: &lt;code&gt;casual_conversation&lt;/code&gt;). If Arch selection fails, calls fall back to &lt;code&gt;LLM_ROUTER_FALLBACK_MODEL&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Selection timeout can be tuned via &lt;code&gt;LLM_ROUTER_ARCH_TIMEOUT_MS&lt;/code&gt; (default 10000).&lt;/li&gt; 
 &lt;li&gt;Omni alias configuration: &lt;code&gt;PUBLIC_LLM_ROUTER_ALIAS_ID&lt;/code&gt; (default &lt;code&gt;omni&lt;/code&gt;), &lt;code&gt;PUBLIC_LLM_ROUTER_DISPLAY_NAME&lt;/code&gt; (default &lt;code&gt;Omni&lt;/code&gt;), and optional &lt;code&gt;PUBLIC_LLM_ROUTER_LOGO_URL&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;When you select Omni in the UI, Chat UI will:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Call the Arch endpoint once (non-streaming) to pick the best route for the last turns.&lt;/li&gt; 
 &lt;li&gt;Emit RouterMetadata immediately (route and actual model used) so the UI can display it.&lt;/li&gt; 
 &lt;li&gt;Stream from the selected model via your configured &lt;code&gt;OPENAI_BASE_URL&lt;/code&gt;. On errors, it tries route fallbacks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Building&lt;/h2&gt; 
&lt;p&gt;To create a production version of your app:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm run build
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can preview the production build with &lt;code&gt;npm run preview&lt;/code&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;To deploy your app, you may need to install an &lt;a href="https://kit.svelte.dev/docs/adapters"&gt;adapter&lt;/a&gt; for your target environment.&lt;/p&gt; 
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>DrewThomasson/ebook2audiobook</title>
      <link>https://github.com/DrewThomasson/ebook2audiobook</link>
      <description>&lt;p&gt;Generate audiobooks from e-books, voice cloning &amp; 1107+ languages!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;📚 ebook2audiobook&lt;/h1&gt; 
&lt;p&gt;CPU/GPU Converter from eBooks to audiobooks with chapters and metadata&lt;br /&gt; using XTTSv2, Bark, Vits, Fairseq, YourTTS, Tacotron and more. Supports voice cloning and +1110 languages!&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;This tool is intended for use with non-DRM, legally acquired eBooks only.&lt;/strong&gt; &lt;br /&gt; The authors are not responsible for any misuse of this software or any resulting legal consequences. &lt;br /&gt; Use this tool responsibly and in accordance with all applicable laws.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/63Tv3F65k6"&gt;&lt;img src="https://dcbadge.limes.pink/api/server/https://discord.gg/63Tv3F65k6" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Thanks to support ebook2audiobook developers!&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://ko-fi.com/athomasson2"&gt;&lt;img src="https://img.shields.io/badge/Ko--fi-F16061?style=for-the-badge&amp;amp;logo=ko-fi&amp;amp;logoColor=white" alt="Ko-Fi" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Run locally&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;&lt;img src="https://img.shields.io/badge/Quick%20Start-blue?style=for-the-badge" alt="Quick Start" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml"&gt;&lt;img src="https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml/badge.svg?sanitize=true" alt="Docker Build" /&gt;&lt;/a&gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/releases/latest"&gt;&lt;img src="https://img.shields.io/badge/Download-Now-blue.svg?sanitize=true" alt="Download" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;a href="https://github.com/DrewThomasson/ebook2audiobook"&gt; &lt;img src="https://img.shields.io/badge/Platform-mac%20|%20linux%20|%20windows-lightgrey" alt="Platform" /&gt; &lt;/a&gt;
&lt;a href="https://hub.docker.com/r/athomasson2/ebook2audiobook"&gt; &lt;img alt="Docker Pull Count" src="https://img.shields.io/docker/pulls/athomasson2/ebook2audiobook.svg?sanitize=true" /&gt; &lt;/a&gt; 
&lt;h3&gt;Run Remotely&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/drewThomasson/ebook2audiobook"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/DrewThomasson/ebook2audiobook/blob/main/Notebooks/colab_ebook2audiobook.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Free Google Colab" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Rihcus/ebook2audiobookXTTS/raw/main/Notebooks/kaggle-ebook2audiobook.ipynb"&gt;&lt;img src="https://img.shields.io/badge/Kaggle-035a7d?style=flat&amp;amp;logo=kaggle&amp;amp;logoColor=white" alt="Kaggle" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;GUI Interface&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/demo_web_gui.gif" alt="demo_web_gui" /&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to see images of Web GUI&lt;/summary&gt; 
 &lt;img width="1728" alt="GUI Screen 1" src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_1.png" /&gt; 
 &lt;img width="1728" alt="GUI Screen 2" src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_2.png" /&gt; 
 &lt;img width="1728" alt="GUI Screen 3" src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_3.png" /&gt; 
&lt;/details&gt; 
&lt;h2&gt;Demos&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;New Default Voice Demo&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/750035dc-e355-46f1-9286-05c1d9e88cea"&gt;https://github.com/user-attachments/assets/750035dc-e355-46f1-9286-05c1d9e88cea&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;More Demos&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;ASMR Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/68eee9a1-6f71-4903-aacd-47397e47e422"&gt;https://github.com/user-attachments/assets/68eee9a1-6f71-4903-aacd-47397e47e422&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Rainy Day Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/d25034d9-c77f-43a9-8f14-0d167172b080"&gt;https://github.com/user-attachments/assets/d25034d9-c77f-43a9-8f14-0d167172b080&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Scarlett Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/b12009ee-ec0d-45ce-a1ef-b3a52b9f8693"&gt;https://github.com/user-attachments/assets/b12009ee-ec0d-45ce-a1ef-b3a52b9f8693&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;David Attenborough Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/81c4baad-117e-4db5-ac86-efc2b7fea921"&gt;https://github.com/user-attachments/assets/81c4baad-117e-4db5-ac86-efc2b7fea921&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://github.com/DrewThomasson/VoxNovel/raw/dc5197dff97252fa44c391dc0596902d71278a88/readme_files/example_in_app.jpeg" alt="Example" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;README.md&lt;/h2&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#-ebook2audiobook"&gt;ebook2audiobook&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#gui-interface"&gt;GUI Interface&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#demos"&gt;Demos&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#supported-languages"&gt;Supported Languages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#hardware-requirements"&gt;Minimum Requirements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;Usage&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;Run Locally&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;Launching Gradio Web Interface&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#basic--usage"&gt;Basic Headless Usage&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#example-of-custom-model-zip-upload"&gt;Headless Custom XTTS Model Usage&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#help-command-output"&gt;Help command output&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#run-remotely"&gt;Run Remotely&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tuned-tts-models"&gt;Fine Tuned TTS models&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tuned-tts-collection"&gt;Collection of Fine-Tuned TTS Models&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tune-your-own-xttsv2-model"&gt;Train XTTSv2&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-gpu-options"&gt;Docker&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-gpu-options"&gt;GPU options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#running-the-pre-built-docker-container"&gt;Docker Run&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#building-the-docker-container"&gt;Docker Build&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-compose"&gt;Docker Compose&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-headless-guide"&gt;Docker headless guide&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-container-file-locations"&gt;Docker container file locations&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#common-docker-issues"&gt;Common Docker issues&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#supported-ebook-formats"&gt;Supported eBook Formats&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#output-formats"&gt;Output Formats&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#updating-to-latest-version"&gt;Updating to Latest Version&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#reverting-to-older-versions"&gt;Revert to older Version&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#common-issues"&gt;Common Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#special-thanks"&gt;Special Thanks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#table-of-contents"&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;📚 Splits eBook into chapters for organized audio.&lt;/li&gt; 
 &lt;li&gt;🎙️ High-quality text-to-speech with &lt;a href="https://huggingface.co/coqui/XTTS-v2"&gt;Coqui XTTSv2&lt;/a&gt; and &lt;a href="https://github.com/facebookresearch/fairseq/tree/main/examples/mms"&gt;Fairseq&lt;/a&gt; (and more).&lt;/li&gt; 
 &lt;li&gt;🗣️ Optional voice cloning with your own voice file.&lt;/li&gt; 
 &lt;li&gt;🌍 Supports +1110 languages (English by default). &lt;a href="https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html"&gt;List of Supported languages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;🖥️ Designed to run on 4GB RAM.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Supported Languages&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Arabic (ar)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Chinese (zh)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;English (en)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Spanish (es)&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;French (fr)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;German (de)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Italian (it)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Portuguese (pt)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Polish (pl)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Turkish (tr)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Russian (ru)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Dutch (nl)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Czech (cs)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Japanese (ja)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Hindi (hi)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Bengali (bn)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Hungarian (hu)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Korean (ko)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Vietnamese (vi)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Swedish (sv)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Persian (fa)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Yoruba (yo)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Swahili (sw)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Indonesian (id)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Slovak (sk)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Croatian (hr)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Tamil (ta)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Danish (da)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html"&gt;&lt;strong&gt;+1100 languages and dialects here&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Hardware Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;4gb RAM minimum, 8GB recommended&lt;/li&gt; 
 &lt;li&gt;Virtualization enabled if running on windows (Docker only)&lt;/li&gt; 
 &lt;li&gt;CPU (intel, AMD, ARM), GPU (Nvidia, AMD*, Intel*) (Recommended), MPS (Apple Silicon CPU) *available very soon&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;Before to post an install or bug issue search carefully to the opened and closed issues TAB&lt;br /&gt; to be sure your issue does not exist already.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] &lt;strong&gt;Lacking of any standards structure like what is a chapter, paragraph, preface etc.&lt;br /&gt; you should first remove manually any text you don't want to be converted in audio.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Installation Instructions&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Clone repo&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/DrewThomasson/ebook2audiobook.git
cd ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Launching Gradio Web Interface&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run ebook2audiobook&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh  # Run launch script
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Mac Launcher&lt;/strong&gt;&lt;br /&gt; Double click &lt;code&gt;Mac Ebook2Audiobook Launcher.command&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd  # Run launch script or double click on it
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows Launcher&lt;/strong&gt;&lt;br /&gt; Double click &lt;code&gt;ebook2audiobook.cmd&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Manual Python Install&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# (for experts only!)
REQUIRED_PROGRAMS=("calibre" "ffmpeg" "nodejs" "mecab" "espeak-ng" "rust" "sox")
REQUIRED_PYTHON_VERSION="3.12"
pip install -r requirements.txt  # Install Python Requirements
python app.py  # Run Ebook2Audiobook
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Open the Web App&lt;/strong&gt;: Click the URL provided in the terminal to access the web app and convert eBooks. &lt;code&gt;http://localhost:7860/&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;For Public Link&lt;/strong&gt;: &lt;code&gt;python app.py --share&lt;/code&gt; (all OS) &lt;code&gt;./ebook2audiobook.sh --share&lt;/code&gt; (Linux/MacOS) &lt;code&gt;ebook2audiobook.cmd --share&lt;/code&gt; (Windows)&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;If the script is stopped and run again, you need to refresh your gradio GUI interface&lt;br /&gt; to let the web page reconnect to the new connection socket.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Basic Usage&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh --headless --ebook &amp;lt;path_to_ebook_file&amp;gt; \
    --voice [path_to_voice_file] --language [language_code]
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd --headless --ebook &amp;lt;path_to_ebook_file&amp;gt;
    --voice [path_to_voice_file] --language [language_code]
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[--ebook]&lt;/strong&gt;: Path to your eBook file&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[--voice]&lt;/strong&gt;: Voice cloning file path (optional)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[--language]&lt;/strong&gt;: Language code in ISO-639-3 (i.e.: ita for italian, eng for english, deu for german...).&lt;br /&gt; Default language is eng and --language is optional for default language set in ./lib/lang.py.&lt;br /&gt; The ISO-639-1 2 letters codes are also supported.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Example of Custom Model Zip Upload&lt;/h3&gt; 
&lt;p&gt;(must be a .zip file containing the mandatory model files. Example for XTTSv2: config.json, model.pth, vocab.json and ref.wav)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh --headless --ebook &amp;lt;ebook_file_path&amp;gt; \
    --voice &amp;lt;target_voice_file_path&amp;gt; --language &amp;lt;language&amp;gt; --custom_model &amp;lt;custom_model_path&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd --headless --ebook &amp;lt;ebook_file_path&amp;gt; \
    --voice &amp;lt;target_voice_file_path&amp;gt; --language &amp;lt;language&amp;gt; --custom_model &amp;lt;custom_model_path&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&amp;lt;custom_model_path&amp;gt;&lt;/strong&gt;: Path to &lt;code&gt;model_name.zip&lt;/code&gt; file, which must contain (according to the tts engine) all the mandatory files&lt;br /&gt; (see ./lib/models.py).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;For Detailed Guide with list of all Parameters to use&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh --help
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd --help
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Or for all OS&lt;/strong&gt; &lt;code&gt;python app.py --help &lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a id="help-command-output"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;usage: app.py [-h] [--session SESSION] [--share] [--headless] [--ebook EBOOK]
              [--ebooks_dir EBOOKS_DIR] [--language LANGUAGE] [--voice VOICE]
              [--device {cpu,gpu,mps}]
              [--tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}]
              [--custom_model CUSTOM_MODEL] [--fine_tuned FINE_TUNED]
              [--output_format OUTPUT_FORMAT] [--temperature TEMPERATURE]
              [--length_penalty LENGTH_PENALTY] [--num_beams NUM_BEAMS]
              [--repetition_penalty REPETITION_PENALTY] [--top_k TOP_K]
              [--top_p TOP_P] [--speed SPEED] [--enable_text_splitting]
              [--text_temp TEXT_TEMP] [--waveform_temp WAVEFORM_TEMP]
              [--output_dir OUTPUT_DIR] [--version]

Convert eBooks to Audiobooks using a Text-to-Speech model. You can either launch the Gradio interface or run the script in headless mode for direct conversion.

options:
  -h, --help            show this help message and exit
  --session SESSION     Session to resume the conversion in case of interruption, crash, 
                            or reuse of custom models and custom cloning voices.

**** The following options are for all modes:
  Optional

**** The following option are for gradio/gui mode only:
  Optional

  --share               Enable a public shareable Gradio link.

**** The following options are for --headless mode only:
  --headless            Run the script in headless mode
  --ebook EBOOK         Path to the ebook file for conversion. Cannot be used when --ebooks_dir is present.
  --ebooks_dir EBOOKS_DIR
                        Relative or absolute path of the directory containing the files to convert. 
                            Cannot be used when --ebook is present.
  --language LANGUAGE   Language of the e-book. Default language is set 
                            in ./lib/lang.py sed as default if not present. All compatible language codes are in ./lib/lang.py

optional parameters:
  --voice VOICE         (Optional) Path to the voice cloning file for TTS engine. 
                            Uses the default voice if not present.
  --device {cpu,gpu,mps}
                        (Optional) Pprocessor unit type for the conversion. 
                            Default is set in ./lib/conf.py if not present. Fall back to CPU if GPU not available.
  --tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}
                        (Optional) Preferred TTS engine (available are: ['XTTSv2', 'BARK', 'VITS', 'FAIRSEQ', 'TACOTRON2', 'YOURTTS', 'xtts', 'bark', 'vits', 'fairseq', 'tacotron', 'yourtts'].
                            Default depends on the selected language. The tts engine should be compatible with the chosen language
  --custom_model CUSTOM_MODEL
                        (Optional) Path to the custom model zip file cntaining mandatory model files. 
                            Please refer to ./lib/models.py
  --fine_tuned FINE_TUNED
                        (Optional) Fine tuned model path. Default is builtin model.
  --output_format OUTPUT_FORMAT
                        (Optional) Output audio format. Default is set in ./lib/conf.py
  --temperature TEMPERATURE
                        (xtts only, optional) Temperature for the model. 
                            Default to config.json model. Higher temperatures lead to more creative outputs.
  --length_penalty LENGTH_PENALTY
                        (xtts only, optional) A length penalty applied to the autoregressive decoder. 
                            Default to config.json model. Not applied to custom models.
  --num_beams NUM_BEAMS
                        (xtts only, optional) Controls how many alternative sequences the model explores. Must be equal or greater than length penalty. 
                            Default to config.json model.
  --repetition_penalty REPETITION_PENALTY
                        (xtts only, optional) A penalty that prevents the autoregressive decoder from repeating itself. 
                            Default to config.json model.
  --top_k TOP_K         (xtts only, optional) Top-k sampling. 
                            Lower values mean more likely outputs and increased audio generation speed. 
                            Default to config.json model.
  --top_p TOP_P         (xtts only, optional) Top-p sampling. 
                            Lower values mean more likely outputs and increased audio generation speed. Default to config.json model.
  --speed SPEED         (xtts only, optional) Speed factor for the speech generation. 
                            Default to config.json model.
  --enable_text_splitting
                        (xtts only, optional) Enable TTS text splitting. This option is known to not be very efficient. 
                            Default to config.json model.
  --text_temp TEXT_TEMP
                        (bark only, optional) Text Temperature for the model. 
                            Default to 0.85. Higher temperatures lead to more creative outputs.
  --waveform_temp WAVEFORM_TEMP
                        (bark only, optional) Waveform Temperature for the model. 
                            Default to 0.5. Higher temperatures lead to more creative outputs.
  --output_dir OUTPUT_DIR
                        (Optional) Path to the output directory. Default is set in ./lib/conf.py
  --version             Show the version of the script and exit

Example usage:    
Windows:
    Gradio/GUI:
    ebook2audiobook.cmd
    Headless mode:
    ebook2audiobook.cmd --headless --ebook '/path/to/file'
Linux/Mac:
    Gradio/GUI:
    ./ebook2audiobook.sh
    Headless mode:
    ./ebook2audiobook.sh --headless --ebook '/path/to/file'
    
Tip: to add of silence (1.4 seconds) into your text just use "###" or "[pause]".

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;NOTE: in gradio/gui mode, to cancel a running conversion, just click on the [X] from the ebook upload component.&lt;/p&gt; 
&lt;p&gt;TIP: if it needs some more pauses, just add '###' or '[pause]' between the words you wish more pause. one [pause] equals to 1.4 seconds&lt;/p&gt; 
&lt;h4&gt;Docker GPU Options&lt;/h4&gt; 
&lt;p&gt;Available pre-build tags: &lt;code&gt;latest&lt;/code&gt; (CUDA 11.8)&lt;/p&gt; 
&lt;h4&gt;Edit: IF GPU isn't detected then you'll have to build the image -&amp;gt; &lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#building-the-docker-container"&gt;Building the Docker Container&lt;/a&gt;&lt;/h4&gt; 
&lt;h4&gt;Running the pre-built Docker Container&lt;/h4&gt; 
&lt;p&gt;-Run with CPU only&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;docker run --pull always --rm -p 7860:7860 athomasson2/ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;-Run with GPU Speedup (NVIDIA compatible only)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;docker run --pull always --rm --gpus all -p 7860:7860 athomasson2/ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command will start the Gradio interface on port 7860.(localhost:7860)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For more options add the parameter &lt;code&gt;--help&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Building the Docker Container&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can build the docker image with the command:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;docker build -t athomasson2/ebook2audiobook .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Avalible Docker Build Arguments&lt;/h4&gt; 
&lt;p&gt;&lt;code&gt;--build-arg TORCH_VERSION=cuda118&lt;/code&gt; Available tags: [cuda121, cuda118, cuda128, rocm, xpu, cpu]&lt;/p&gt; 
&lt;p&gt;All CUDA version numbers should work, Ex: CUDA 11.6-&amp;gt; cuda116&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;--build-arg SKIP_XTTS_TEST=true&lt;/code&gt; (Saves space by not baking XTTSv2 model into docker image)&lt;/p&gt; 
&lt;h2&gt;Docker container file locations&lt;/h2&gt; 
&lt;p&gt;All ebook2audiobooks will have the base dir of &lt;code&gt;/app/&lt;/code&gt; For example: &lt;code&gt;tmp&lt;/code&gt; = &lt;code&gt;/app/tmp&lt;/code&gt; &lt;code&gt;audiobooks&lt;/code&gt; = &lt;code&gt;/app/audiobooks&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Docker headless guide&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Before you do run this you need to create a dir named "input-folder" in your current dir which will be linked, This is where you can put your input files for the docker image to see&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir input-folder &amp;amp;&amp;amp; mkdir Audiobooks
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;In the command below swap out &lt;strong&gt;YOUR_INPUT_FILE.TXT&lt;/strong&gt; with the name of your input file&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --pull always --rm \
    -v $(pwd)/input-folder:/app/input_folder \
    -v $(pwd)/audiobooks:/app/audiobooks \
    athomasson2/ebook2audiobook \
    --headless --ebook /input_folder/YOUR_EBOOK_FILE
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;The output Audiobooks will be found in the Audiobook folder which will also be located in your local dir you ran this docker command in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;To get the help command for the other parameters this program has you can run this&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --pull always --rm athomasson2/ebook2audiobook --help

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That will output this &lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#help-command-output"&gt;Help command output&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Docker Compose&lt;/h3&gt; 
&lt;p&gt;This project uses Docker Compose to run locally. You can enable or disable GPU support by setting either &lt;code&gt;*gpu-enabled&lt;/code&gt; or &lt;code&gt;*gpu-disabled&lt;/code&gt; in &lt;code&gt;docker-compose.yml&lt;/code&gt;&lt;/p&gt; 
&lt;h4&gt;Steps to Run&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Clone the Repository&lt;/strong&gt; (if you haven't already): &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/DrewThomasson/ebook2audiobook.git
cd ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Set GPU Support (disabled by default)&lt;/strong&gt; To enable GPU support, modify &lt;code&gt;docker-compose.yml&lt;/code&gt; and change &lt;code&gt;*gpu-disabled&lt;/code&gt; to &lt;code&gt;*gpu-enabled&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Start the service:&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Docker
docker-compose up -d # To update add --build

# Podman
podman compose -f podman-compose.yml up -d # To update add --build
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Access the service:&lt;/strong&gt; The service will be available at &lt;a href="http://localhost:7860"&gt;http://localhost:7860&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Common Docker Issues&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;My NVIDIA GPU isnt being detected?? -&amp;gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/wiki/GPU-ISSUES"&gt;GPU ISSUES Wiki Page&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;python: can't open file '/home/user/app/app.py': [Errno 2] No such file or directory&lt;/code&gt; (Just remove all post arguments as I replaced the &lt;code&gt;CMD&lt;/code&gt; with &lt;code&gt;ENTRYPOINT&lt;/code&gt; in the &lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/Dockerfile"&gt;Dockerfile&lt;/a&gt;)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Example: &lt;code&gt;docker run --pull always athomasson2/ebook2audiobook app.py --script_mode full_docker&lt;/code&gt; - &amp;gt; corrected - &amp;gt; &lt;code&gt;docker run --pull always athomasson2/ebook2audiobook&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Arguments can be easily added like this now &lt;code&gt;docker run --pull always athomasson2/ebook2audiobook --share&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Docker gets stuck downloading Fine-Tuned models. (This does not happen for every computer but some appear to run into this issue) Disabling the progress bar appears to fix the issue, as discussed &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/issues/191"&gt;here in #191&lt;/a&gt; Example of adding this fix in the &lt;code&gt;docker run&lt;/code&gt; command&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-Dockerfile"&gt;docker run --pull always --rm --gpus all -e HF_HUB_DISABLE_PROGRESS_BARS=1 -e HF_HUB_ENABLE_HF_TRANSFER=0 \
    -p 7860:7860 athomasson2/ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Fine Tuned TTS models&lt;/h2&gt; 
&lt;h4&gt;Fine Tune your own XTTSv2 model&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/drewThomasson/xtts-finetune-webui-gpu"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/raw/v25/Notebooks/finetune/xtts/kaggle-xtts-finetune-webui-gradio-gui.ipynb"&gt;&lt;img src="https://img.shields.io/badge/Kaggle-035a7d?style=flat&amp;amp;logo=kaggle&amp;amp;logoColor=white" alt="Kaggle" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/DrewThomasson/ebook2audiobook/blob/v25/Notebooks/finetune/xtts/colab_xtts_finetune_webui.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;De-noise training data&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/drewThomasson/DeepFilterNet2_no_limit"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Rikorose/DeepFilterNet"&gt;&lt;img src="https://img.shields.io/badge/DeepFilterNet-181717?logo=github" alt="GitHub Repo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Fine Tuned TTS Collection&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/drewThomasson/fineTunedTTSModels/tree/main"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Models-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For an XTTSv2 custom model a ref audio clip of the voice reference is mandatory:&lt;/p&gt; 
&lt;h2&gt;Supported eBook Formats&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;.epub&lt;/code&gt;, &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.mobi&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.rtf&lt;/code&gt;, &lt;code&gt;.chm&lt;/code&gt;, &lt;code&gt;.lit&lt;/code&gt;, &lt;code&gt;.pdb&lt;/code&gt;, &lt;code&gt;.fb2&lt;/code&gt;, &lt;code&gt;.odt&lt;/code&gt;, &lt;code&gt;.cbr&lt;/code&gt;, &lt;code&gt;.cbz&lt;/code&gt;, &lt;code&gt;.prc&lt;/code&gt;, &lt;code&gt;.lrf&lt;/code&gt;, &lt;code&gt;.pml&lt;/code&gt;, &lt;code&gt;.snb&lt;/code&gt;, &lt;code&gt;.cbc&lt;/code&gt;, &lt;code&gt;.rb&lt;/code&gt;, &lt;code&gt;.tcr&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Best results&lt;/strong&gt;: &lt;code&gt;.epub&lt;/code&gt; or &lt;code&gt;.mobi&lt;/code&gt; for automatic chapter detection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Output Formats&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Creates a &lt;code&gt;['m4b', 'm4a', 'mp4', 'webm', 'mov', 'mp3', 'flac', 'wav', 'ogg', 'aac']&lt;/code&gt; (set in ./lib/conf.py) file with metadata and chapters.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Updating to Latest Version&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git pull # Locally/Compose

docker pull athomasson2/ebook2audiobook:latest # For Pre-build docker images
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Reverting to older Versions&lt;/h2&gt; 
&lt;p&gt;Releases can be found -&amp;gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/releases"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git checkout tags/VERSION_NUM # Locally/Compose -&amp;gt; Example: git checkout tags/v25.7.7

athomasson2/ebook2audiobook:VERSION_NUM # For Pre-build docker images -&amp;gt; Example: athomasson2/ebook2audiobook:v25.7.7
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Common Issues:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;My NVIDIA GPU isnt being detected?? -&amp;gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/wiki/GPU-ISSUES"&gt;GPU ISSUES Wiki Page&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;CPU is slow (better on server smp CPU) while NVIDIA GPU can have almost real time conversion. &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/discussions/19#discussioncomment-10879846"&gt;Discussion about this&lt;/a&gt; For faster multilingual generation I would suggest my other &lt;a href="https://github.com/DrewThomasson/ebook2audiobookpiper-tts"&gt;project that uses piper-tts&lt;/a&gt; instead (It doesn't have zero-shot voice cloning though, and is Siri quality voices, but it is much faster on cpu).&lt;/li&gt; 
 &lt;li&gt;"I'm having dependency issues" - Just use the docker, its fully self contained and has a headless mode, add &lt;code&gt;--help&lt;/code&gt; parameter at the end of the docker run command for more information.&lt;/li&gt; 
 &lt;li&gt;"Im getting a truncated audio issue!" - PLEASE MAKE AN ISSUE OF THIS, we don't speak every language and need advise from users to fine tune the sentence splitting logic.😊&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What we need help with! 🙌&lt;/h2&gt; 
&lt;h2&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook/issues/32"&gt;Full list of things can be found here&lt;/a&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Any help from people speaking any of the supported languages to help us improve the models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Do you need to rent a GPU to boost service from us?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;A poll is open here &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/discussions/889"&gt;https://github.com/DrewThomasson/ebook2audiobook/discussions/889&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Special Thanks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Coqui TTS&lt;/strong&gt;: &lt;a href="https://github.com/idiap/coqui-ai-TTS"&gt;Coqui TTS GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Calibre&lt;/strong&gt;: &lt;a href="https://calibre-ebook.com"&gt;Calibre Website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;FFmpeg&lt;/strong&gt;: &lt;a href="https://ffmpeg.org"&gt;FFmpeg Website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook/issues/8"&gt;@shakenbake15 for better chapter saving method&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>anthropics/claude-cookbooks</title>
      <link>https://github.com/anthropics/claude-cookbooks</link>
      <description>&lt;p&gt;A collection of notebooks/recipes showcasing some fun and effective ways of using Claude.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Claude Cookbooks&lt;/h1&gt; 
&lt;p&gt;The Claude Cookbooks provide code and guides designed to help developers build with Claude, offering copy-able code snippets that you can easily integrate into your own projects.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;To make the most of the examples in this cookbook, you'll need an Claude API key (sign up for free &lt;a href="https://www.anthropic.com"&gt;here&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;While the code examples are primarily written in Python, the concepts can be adapted to any programming language that supports interaction with the Claude API.&lt;/p&gt; 
&lt;p&gt;If you're new to working with the Claude API, we recommend starting with our &lt;a href="https://github.com/anthropics/courses/tree/master/anthropic_api_fundamentals"&gt;Claude API Fundamentals course&lt;/a&gt; to get a solid foundation.&lt;/p&gt; 
&lt;h2&gt;Explore Further&lt;/h2&gt; 
&lt;p&gt;Looking for more resources to enhance your experience with Claude and AI assistants? Check out these helpful links:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.claude.com/claude/docs/guide-to-anthropics-prompt-engineering-resources"&gt;Anthropic developer documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://support.anthropic.com"&gt;Anthropic support docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.anthropic.com/discord"&gt;Anthropic Discord community&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;The Claude Cookbooks thrives on the contributions of the developer community. We value your input, whether it's submitting an idea, fixing a typo, adding a new guide, or improving an existing one. By contributing, you help make this resource even more valuable for everyone.&lt;/p&gt; 
&lt;p&gt;To avoid duplication of efforts, please review the existing issues and pull requests before contributing.&lt;/p&gt; 
&lt;p&gt;If you have ideas for new examples or guides, share them on the &lt;a href="https://github.com/anthropics/anthropic-cookbook/issues"&gt;issues page&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Table of recipes&lt;/h2&gt; 
&lt;h3&gt;Capabilities&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/tree/main/capabilities/classification"&gt;Classification&lt;/a&gt;: Explore techniques for text and data classification using Claude.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/tree/main/capabilities/retrieval_augmented_generation"&gt;Retrieval Augmented Generation&lt;/a&gt;: Learn how to enhance Claude's responses with external knowledge.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/tree/main/capabilities/summarization"&gt;Summarization&lt;/a&gt;: Discover techniques for effective text summarization with Claude.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Tool Use and Integration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/tree/main/tool_use"&gt;Tool use&lt;/a&gt;: Learn how to integrate Claude with external tools and functions to extend its capabilities. 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/tool_use/customer_service_agent.ipynb"&gt;Customer service agent&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/tool_use/calculator_tool.ipynb"&gt;Calculator integration&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/misc/how_to_make_sql_queries.ipynb"&gt;SQL queries&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Third-Party Integrations&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/tree/main/third_party"&gt;Retrieval augmented generation&lt;/a&gt;: Supplement Claude's knowledge with external data sources. 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/third_party/Pinecone/rag_using_pinecone.ipynb"&gt;Vector databases (Pinecone)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/third_party/Wikipedia/wikipedia-search-cookbook.ipynb/"&gt;Wikipedia&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/misc/read_web_pages_with_haiku.ipynb"&gt;Web pages&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/third_party/Brave/web_search_using_brave.ipynb"&gt;Internet search (Brave)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/third_party/VoyageAI/how_to_create_embeddings.md"&gt;Embeddings with Voyage AI&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Multimodal Capabilities&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/tree/main/multimodal"&gt;Vision with Claude&lt;/a&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/multimodal/getting_started_with_vision.ipynb"&gt;Getting started with images&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/multimodal/best_practices_for_vision.ipynb"&gt;Best practices for vision&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/multimodal/reading_charts_graphs_powerpoints.ipynb"&gt;Interpreting charts and graphs&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/multimodal/how_to_transcribe_text.ipynb"&gt;Extracting content from forms&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/misc/illustrated_responses.ipynb"&gt;Generate images with Claude&lt;/a&gt;: Use Claude with Stable Diffusion for image generation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Techniques&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/multimodal/using_sub_agents.ipynb"&gt;Sub-agents&lt;/a&gt;: Learn how to use Haiku as a sub-agent in combination with Opus.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/misc/pdf_upload_summarization.ipynb"&gt;Upload PDFs to Claude&lt;/a&gt;: Parse and pass PDFs as text to Claude.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/misc/building_evals.ipynb"&gt;Automated evaluations&lt;/a&gt;: Use Claude to automate the prompt evaluation process.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/misc/how_to_enable_json_mode.ipynb"&gt;Enable JSON mode&lt;/a&gt;: Ensure consistent JSON output from Claude.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/misc/building_moderation_filter.ipynb"&gt;Create a moderation filter&lt;/a&gt;: Use Claude to create a content moderation filter for your application.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/misc/prompt_caching.ipynb"&gt;Prompt caching&lt;/a&gt;: Learn techniques for efficient prompt caching with Claude.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Additional Resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aws-samples/anthropic-on-aws"&gt;Anthropic on AWS&lt;/a&gt;: Explore examples and solutions for using Claude on AWS infrastructure.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aws-samples/"&gt;AWS Samples&lt;/a&gt;: A collection of code samples from AWS which can be adapted for use with Claude. Note that some samples may require modification to work optimally with Claude.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>huggingface/lerobot</title>
      <link>https://github.com/huggingface/lerobot</link>
      <description>&lt;p&gt;🤗 LeRobot: Making AI for Robotics more accessible with end-to-end learning&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img alt="LeRobot, Hugging Face Robotics Library" src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/lerobot-logo-thumbnail.png" width="100%" /&gt; &lt;br /&gt; &lt;br /&gt; &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/huggingface/lerobot/actions/workflows/nightly.yml?query=branch%3Amain"&gt;&lt;img src="https://github.com/huggingface/lerobot/actions/workflows/nightly.yml/badge.svg?branch=main" alt="Tests" /&gt;&lt;/a&gt; &lt;a href="https://www.python.org/downloads/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/lerobot" alt="Python versions" /&gt;&lt;/a&gt; &lt;a href="https://github.com/huggingface/lerobot/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/lerobot/"&gt;&lt;img src="https://img.shields.io/pypi/status/lerobot" alt="Status" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/lerobot/"&gt;&lt;img src="https://img.shields.io/pypi/v/lerobot" alt="Version" /&gt;&lt;/a&gt; &lt;a href="https://github.com/huggingface/lerobot/raw/main/CODE_OF_CONDUCT.md"&gt;&lt;img src="https://img.shields.io/badge/Contributor%20Covenant-v2.1-ff69b4.svg?sanitize=true" alt="Contributor Covenant" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/s3KuuzsPFb"&gt;&lt;img src="https://dcbadge.vercel.app/api/server/C5P34WJ68S?style=flat" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;!-- [![Coverage](https://codecov.io/gh/huggingface/lerobot/branch/main/graph/badge.svg?token=TODO)](https://codecov.io/gh/huggingface/lerobot) --&gt; 
&lt;/div&gt; 
&lt;h2 align="center"&gt; &lt;p&gt;&lt;a href="https://huggingface.co/docs/lerobot/hope_jr"&gt; Build Your Own HopeJR Robot!&lt;/a&gt;&lt;/p&gt; &lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/hope_jr/hopejr.png" alt="HopeJR robot" title="HopeJR robot" width="60%" /&gt; 
 &lt;p&gt;&lt;strong&gt;Meet HopeJR – A humanoid robot arm and hand for dexterous manipulation!&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Control it with exoskeletons and gloves for precise hand movements.&lt;/p&gt; 
 &lt;p&gt;Perfect for advanced manipulation tasks! 🤖&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://huggingface.co/docs/lerobot/hope_jr"&gt; See the full HopeJR tutorial here.&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2 align="center"&gt; &lt;p&gt;&lt;a href="https://huggingface.co/docs/lerobot/so101"&gt; Build Your Own SO-101 Robot!&lt;/a&gt;&lt;/p&gt; &lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/so101/so101.webp" alt="SO-101 follower arm" title="SO-101 follower arm" width="90%" /&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/so101/so101-leader.webp" alt="SO-101 leader arm" title="SO-101 leader arm" width="90%" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;p&gt;&lt;strong&gt;Meet the updated SO100, the SO-101 – Just €114 per arm!&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Train it in minutes with a few simple moves on your laptop.&lt;/p&gt; 
 &lt;p&gt;Then sit back and watch your creation act autonomously! 🤯&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://huggingface.co/docs/lerobot/so101"&gt; See the full SO-101 tutorial here.&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;Want to take it to the next level? Make your SO-101 mobile by building LeKiwi!&lt;/p&gt; 
 &lt;p&gt;Check out the &lt;a href="https://huggingface.co/docs/lerobot/lekiwi"&gt;LeKiwi tutorial&lt;/a&gt; and bring your robot to life on wheels.&lt;/p&gt; 
 &lt;img src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/lekiwi/kiwi.webp" alt="LeKiwi mobile robot" title="LeKiwi mobile robot" width="50%" /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h3 align="center"&gt; &lt;p&gt;LeRobot: State-of-the-art AI for real-world robotics&lt;/p&gt; &lt;/h3&gt; 
&lt;hr /&gt; 
&lt;p&gt;🤗 LeRobot aims to provide models, datasets, and tools for real-world robotics in PyTorch. The goal is to lower the barrier to entry to robotics so that everyone can contribute and benefit from sharing datasets and pretrained models.&lt;/p&gt; 
&lt;p&gt;🤗 LeRobot contains state-of-the-art approaches that have been shown to transfer to the real-world with a focus on imitation learning and reinforcement learning.&lt;/p&gt; 
&lt;p&gt;🤗 LeRobot already provides a set of pretrained models, datasets with human collected demonstrations, and simulation environments to get started without assembling a robot. In the coming weeks, the plan is to add more and more support for real-world robotics on the most affordable and capable robots out there.&lt;/p&gt; 
&lt;p&gt;🤗 LeRobot hosts pretrained models and datasets on this Hugging Face community page: &lt;a href="https://huggingface.co/lerobot"&gt;huggingface.co/lerobot&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Examples of pretrained models on simulation environments&lt;/h4&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/gym/aloha_act.gif" width="100%" alt="ACT policy on ALOHA env" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/gym/simxarm_tdmpc.gif" width="100%" alt="TDMPC policy on SimXArm env" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/gym/pusht_diffusion.gif" width="100%" alt="Diffusion policy on PushT env" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;ACT policy on ALOHA env&lt;/td&gt; 
   &lt;td align="center"&gt;TDMPC policy on SimXArm env&lt;/td&gt; 
   &lt;td align="center"&gt;Diffusion policy on PushT env&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;LeRobot works with Python 3.10+ and PyTorch 2.2+.&lt;/p&gt; 
&lt;h3&gt;Environment Setup&lt;/h3&gt; 
&lt;p&gt;Create a virtual environment with Python 3.10 and activate it, e.g. with &lt;a href="https://conda-forge.org/download/"&gt;&lt;code&gt;miniforge&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -y -n lerobot python=3.10
conda activate lerobot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When using &lt;code&gt;conda&lt;/code&gt;, install &lt;code&gt;ffmpeg&lt;/code&gt; in your environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda install ffmpeg -c conda-forge
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; This usually installs &lt;code&gt;ffmpeg 7.X&lt;/code&gt; for your platform compiled with the &lt;code&gt;libsvtav1&lt;/code&gt; encoder. If &lt;code&gt;libsvtav1&lt;/code&gt; is not supported (check supported encoders with &lt;code&gt;ffmpeg -encoders&lt;/code&gt;), you can:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;em&gt;[On any platform]&lt;/em&gt; Explicitly install &lt;code&gt;ffmpeg 7.X&lt;/code&gt; using:&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;conda install ffmpeg=7.1.1 -c conda-forge
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;em&gt;[On Linux only]&lt;/em&gt; Install &lt;a href="https://trac.ffmpeg.org/wiki/CompilationGuide/Ubuntu#GettheDependencies"&gt;ffmpeg build dependencies&lt;/a&gt; and &lt;a href="https://trac.ffmpeg.org/wiki/CompilationGuide/Ubuntu#libsvtav1"&gt;compile ffmpeg from source with libsvtav1&lt;/a&gt;, and make sure you use the corresponding ffmpeg binary to your install with &lt;code&gt;which ffmpeg&lt;/code&gt;.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Install LeRobot 🤗&lt;/h3&gt; 
&lt;h4&gt;From Source&lt;/h4&gt; 
&lt;p&gt;First, clone the repository and navigate into the directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/huggingface/lerobot.git
cd lerobot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, install the library in editable mode. This is useful if you plan to contribute to the code.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; If you encounter build errors, you may need to install additional dependencies (&lt;code&gt;cmake&lt;/code&gt;, &lt;code&gt;build-essential&lt;/code&gt;, and &lt;code&gt;ffmpeg libs&lt;/code&gt;). On Linux, run: &lt;code&gt;sudo apt-get install cmake build-essential python3-dev pkg-config libavformat-dev libavcodec-dev libavdevice-dev libavutil-dev libswscale-dev libswresample-dev libavfilter-dev&lt;/code&gt;. For other systems, see: &lt;a href="https://pyav.org/docs/develop/overview/installation.html#bring-your-own-ffmpeg"&gt;Compiling PyAV&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;For simulations, 🤗 LeRobot comes with gymnasium environments that can be installed as extras:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/gym-aloha"&gt;aloha&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/gym-xarm"&gt;xarm&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/gym-pusht"&gt;pusht&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For instance, to install 🤗 LeRobot with aloha and pusht, use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e ".[aloha, pusht]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Installation from PyPI&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Core Library:&lt;/strong&gt; Install the base package with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install lerobot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;This installs only the default dependencies.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Extra Features:&lt;/strong&gt; To install additional functionality, use one of the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install 'lerobot[all]'          # All available features
pip install 'lerobot[aloha,pusht]'  # Specific features (Aloha &amp;amp; Pusht)
pip install 'lerobot[feetech]'      # Feetech motor support
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Replace &lt;code&gt;[...]&lt;/code&gt; with your desired features.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Available Tags:&lt;/strong&gt; For a full list of optional dependencies, see: &lt;a href="https://pypi.org/project/lerobot/"&gt;https://pypi.org/project/lerobot/&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Weights &amp;amp; Biases&lt;/h3&gt; 
&lt;p&gt;To use &lt;a href="https://docs.wandb.ai/quickstart"&gt;Weights and Biases&lt;/a&gt; for experiment tracking, log in with&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;wandb login
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;(note: you will also need to enable WandB in the configuration. See below.)&lt;/p&gt; 
&lt;h3&gt;Visualize datasets&lt;/h3&gt; 
&lt;p&gt;Check out &lt;a href="https://github.com/huggingface/lerobot/raw/main/examples/dataset/load_lerobot_dataset.py"&gt;example 1&lt;/a&gt; that illustrates how to use our dataset class which automatically downloads data from the Hugging Face hub.&lt;/p&gt; 
&lt;p&gt;You can also locally visualize episodes from a dataset on the hub by executing our script from the command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;lerobot-dataset-viz \
    --repo-id lerobot/pusht \
    --episode-index 0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or from a dataset in a local folder with the &lt;code&gt;root&lt;/code&gt; option and the &lt;code&gt;--mode local&lt;/code&gt; (in the following case the dataset will be searched for in &lt;code&gt;./my_local_data_dir/lerobot/pusht&lt;/code&gt;)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;lerobot-dataset-viz \
    --repo-id lerobot/pusht \
    --root ./my_local_data_dir \
    --mode local \
    --episode-index 0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It will open &lt;code&gt;rerun.io&lt;/code&gt; and display the camera streams, robot states and actions, like this:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github-production-user-asset-6210df.s3.amazonaws.com/4681518/328035972-fd46b787-b532-47e2-bb6f-fd536a55a7ed.mov?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;amp;X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&amp;amp;X-Amz-Date=20240505T172924Z&amp;amp;X-Amz-Expires=300&amp;amp;X-Amz-Signature=d680b26c532eeaf80740f08af3320d22ad0b8a4e4da1bcc4f33142c15b509eda&amp;amp;X-Amz-SignedHeaders=host&amp;amp;actor_id=24889239&amp;amp;key_id=0&amp;amp;repo_id=748713144"&gt;https://github-production-user-asset-6210df.s3.amazonaws.com/4681518/328035972-fd46b787-b532-47e2-bb6f-fd536a55a7ed.mov?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;amp;X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&amp;amp;X-Amz-Date=20240505T172924Z&amp;amp;X-Amz-Expires=300&amp;amp;X-Amz-Signature=d680b26c532eeaf80740f08af3320d22ad0b8a4e4da1bcc4f33142c15b509eda&amp;amp;X-Amz-SignedHeaders=host&amp;amp;actor_id=24889239&amp;amp;key_id=0&amp;amp;repo_id=748713144&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Our script can also visualize datasets stored on a distant server. See &lt;code&gt;lerobot-dataset-viz --help&lt;/code&gt; for more instructions.&lt;/p&gt; 
&lt;h3&gt;The &lt;code&gt;LeRobotDataset&lt;/code&gt; format&lt;/h3&gt; 
&lt;p&gt;A dataset in &lt;code&gt;LeRobotDataset&lt;/code&gt; format is very simple to use. It can be loaded from a repository on the Hugging Face hub or a local folder simply with e.g. &lt;code&gt;dataset = LeRobotDataset("lerobot/aloha_static_coffee")&lt;/code&gt; and can be indexed into like any Hugging Face and PyTorch dataset. For instance &lt;code&gt;dataset[0]&lt;/code&gt; will retrieve a single temporal frame from the dataset containing observation(s) and an action as PyTorch tensors ready to be fed to a model.&lt;/p&gt; 
&lt;p&gt;A specificity of &lt;code&gt;LeRobotDataset&lt;/code&gt; is that, rather than retrieving a single frame by its index, we can retrieve several frames based on their temporal relationship with the indexed frame, by setting &lt;code&gt;delta_timestamps&lt;/code&gt; to a list of relative times with respect to the indexed frame. For example, with &lt;code&gt;delta_timestamps = {"observation.image": [-1, -0.5, -0.2, 0]}&lt;/code&gt; one can retrieve, for a given index, 4 frames: 3 "previous" frames 1 second, 0.5 seconds, and 0.2 seconds before the indexed frame, and the indexed frame itself (corresponding to the 0 entry). See example &lt;a href="https://github.com/huggingface/lerobot/raw/main/examples/dataset/load_lerobot_dataset.py"&gt;1_load_lerobot_dataset.py&lt;/a&gt; for more details on &lt;code&gt;delta_timestamps&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Under the hood, the &lt;code&gt;LeRobotDataset&lt;/code&gt; format makes use of several ways to serialize data which can be useful to understand if you plan to work more closely with this format. We tried to make a flexible yet simple dataset format that would cover most type of features and specificities present in reinforcement learning and robotics, in simulation and in real-world, with a focus on cameras and robot states but easily extended to other types of sensory inputs as long as they can be represented by a tensor.&lt;/p&gt; 
&lt;p&gt;Here are the important details and internal structure organization of a typical &lt;code&gt;LeRobotDataset&lt;/code&gt; instantiated with &lt;code&gt;dataset = LeRobotDataset("lerobot/aloha_static_coffee")&lt;/code&gt;. The exact features will change from dataset to dataset but not the main aspects:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;dataset attributes:
  ├ hf_dataset: a Hugging Face dataset (backed by Arrow/parquet). Typical features example:
  │  ├ observation.images.cam_high (VideoFrame):
  │  │   VideoFrame = {'path': path to a mp4 video, 'timestamp' (float32): timestamp in the video}
  │  ├ observation.state (list of float32): position of an arm joints (for instance)
  │  ... (more observations)
  │  ├ action (list of float32): goal position of an arm joints (for instance)
  │  ├ episode_index (int64): index of the episode for this sample
  │  ├ frame_index (int64): index of the frame for this sample in the episode ; starts at 0 for each episode
  │  ├ timestamp (float32): timestamp in the episode
  │  ├ next.done (bool): indicates the end of an episode ; True for the last frame in each episode
  │  └ index (int64): general index in the whole dataset
  ├ meta: a LeRobotDatasetMetadata object containing:
  │  ├ info: a dictionary of metadata on the dataset
  │  │  ├ codebase_version (str): this is to keep track of the codebase version the dataset was created with
  │  │  ├ fps (int): frame per second the dataset is recorded/synchronized to
  │  │  ├ features (dict): all features contained in the dataset with their shapes and types
  │  │  ├ total_episodes (int): total number of episodes in the dataset
  │  │  ├ total_frames (int): total number of frames in the dataset
  │  │  ├ robot_type (str): robot type used for recording
  │  │  ├ data_path (str): formattable string for the parquet files
  │  │  └ video_path (str): formattable string for the video files (if using videos)
  │  ├ episodes: a DataFrame containing episode metadata with columns:
  │  │  ├ episode_index (int): index of the episode
  │  │  ├ tasks (list): list of tasks for this episode
  │  │  ├ length (int): number of frames in this episode
  │  │  ├ dataset_from_index (int): start index of this episode in the dataset
  │  │  └ dataset_to_index (int): end index of this episode in the dataset
  │  ├ stats: a dictionary of statistics (max, mean, min, std) for each feature in the dataset, for instance
  │  │  ├ observation.images.front_cam: {'max': tensor with same number of dimensions (e.g. `(c, 1, 1)` for images, `(c,)` for states), etc.}
  │  │  └ ...
  │  └ tasks: a DataFrame containing task information with task names as index and task_index as values
  ├ root (Path): local directory where the dataset is stored
  ├ image_transforms (Callable): optional image transformations to apply to visual modalities
  └ delta_timestamps (dict): optional delta timestamps for temporal queries
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;A &lt;code&gt;LeRobotDataset&lt;/code&gt; is serialised using several widespread file formats for each of its parts, namely:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;hf_dataset stored using Hugging Face datasets library serialization to parquet&lt;/li&gt; 
 &lt;li&gt;videos are stored in mp4 format to save space&lt;/li&gt; 
 &lt;li&gt;metadata are stored in plain json/jsonl files&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Dataset can be uploaded/downloaded from the HuggingFace hub seamlessly. To work on a local dataset, you can specify its location with the &lt;code&gt;root&lt;/code&gt; argument if it's not in the default &lt;code&gt;~/.cache/huggingface/lerobot&lt;/code&gt; location.&lt;/p&gt; 
&lt;h4&gt;Reproduce state-of-the-art (SOTA)&lt;/h4&gt; 
&lt;p&gt;We provide some pretrained policies on our &lt;a href="https://huggingface.co/lerobot"&gt;hub page&lt;/a&gt; that can achieve state-of-the-art performances. You can reproduce their training by loading the config from their run. Simply running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;lerobot-train --config_path=lerobot/diffusion_pusht
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;reproduces SOTA results for Diffusion Policy on the PushT task.&lt;/p&gt; 
&lt;h2&gt;Contribute&lt;/h2&gt; 
&lt;p&gt;If you would like to contribute to 🤗 LeRobot, please check out our &lt;a href="https://github.com/huggingface/lerobot/raw/main/CONTRIBUTING.md"&gt;contribution guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Add a pretrained policy&lt;/h3&gt; 
&lt;p&gt;Once you have trained a policy you may upload it to the Hugging Face hub using a hub id that looks like &lt;code&gt;${hf_user}/${repo_name}&lt;/code&gt; (e.g. &lt;a href="https://huggingface.co/lerobot/diffusion_pusht"&gt;lerobot/diffusion_pusht&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;You first need to find the checkpoint folder located inside your experiment directory (e.g. &lt;code&gt;outputs/train/2024-05-05/20-21-12_aloha_act_default/checkpoints/002500&lt;/code&gt;). Within that there is a &lt;code&gt;pretrained_model&lt;/code&gt; directory which should contain:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;config.json&lt;/code&gt;: A serialized version of the policy configuration (following the policy's dataclass config).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;model.safetensors&lt;/code&gt;: A set of &lt;code&gt;torch.nn.Module&lt;/code&gt; parameters, saved in &lt;a href="https://huggingface.co/docs/safetensors/index"&gt;Hugging Face Safetensors&lt;/a&gt; format.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;train_config.json&lt;/code&gt;: A consolidated configuration containing all parameters used for training. The policy configuration should match &lt;code&gt;config.json&lt;/code&gt; exactly. This is useful for anyone who wants to evaluate your policy or for reproducibility.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To upload these to the hub, run the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;huggingface-cli upload ${hf_user}/${repo_name} path/to/pretrained_model
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://github.com/huggingface/lerobot/raw/main/src/lerobot/scripts/lerobot_eval.py"&gt;lerobot_eval.py&lt;/a&gt; for an example of how other people may use your policy.&lt;/p&gt; 
&lt;h3&gt;Acknowledgment&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;The LeRobot team 🤗 for building SmolVLA &lt;a href="https://arxiv.org/abs/2506.01844"&gt;Paper&lt;/a&gt;, &lt;a href="https://huggingface.co/blog/smolvla"&gt;Blog&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Thanks to Tony Zhao, Zipeng Fu and colleagues for open sourcing ACT policy, ALOHA environments and datasets. Ours are adapted from &lt;a href="https://tonyzhaozh.github.io/aloha"&gt;ALOHA&lt;/a&gt; and &lt;a href="https://mobile-aloha.github.io"&gt;Mobile ALOHA&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Thanks to Cheng Chi, Zhenjia Xu and colleagues for open sourcing Diffusion policy, Pusht environment and datasets, as well as UMI datasets. Ours are adapted from &lt;a href="https://diffusion-policy.cs.columbia.edu"&gt;Diffusion Policy&lt;/a&gt; and &lt;a href="https://umi-gripper.github.io"&gt;UMI Gripper&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Thanks to Nicklas Hansen, Yunhai Feng and colleagues for open sourcing TDMPC policy, Simxarm environments and datasets. Ours are adapted from &lt;a href="https://github.com/nicklashansen/tdmpc"&gt;TDMPC&lt;/a&gt; and &lt;a href="https://www.yunhaifeng.com/FOWM"&gt;FOWM&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Thanks to Antonio Loquercio and Ashish Kumar for their early support.&lt;/li&gt; 
 &lt;li&gt;Thanks to &lt;a href="https://sjlee.cc/"&gt;Seungjae (Jay) Lee&lt;/a&gt;, &lt;a href="https://mahis.life/"&gt;Mahi Shafiullah&lt;/a&gt; and colleagues for open sourcing &lt;a href="https://sjlee.cc/vq-bet/"&gt;VQ-BeT&lt;/a&gt; policy and helping us adapt the codebase to our repository. The policy is adapted from &lt;a href="https://github.com/jayLEE0301/vq_bet_official"&gt;VQ-BeT repo&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you want, you can cite this work with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{cadene2024lerobot,
    author = {Cadene, Remi and Alibert, Simon and Soare, Alexander and Gallouedec, Quentin and Zouitine, Adil and Palma, Steven and Kooijmans, Pepijn and Aractingi, Michel and Shukor, Mustafa and Aubakirova, Dana and Russi, Martino and Capuano, Francesco and Pascal, Caroline and Choghari, Jade and Moss, Jess and Wolf, Thomas},
    title = {LeRobot: State-of-the-art Machine Learning for Real-World Robotics in Pytorch},
    howpublished = "\url{https://github.com/huggingface/lerobot}",
    year = {2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#huggingface/lerobot&amp;amp;Timeline"&gt;&lt;img src="https://api.star-history.com/svg?repos=huggingface/lerobot&amp;amp;type=Timeline" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>SagerNet/sing-box</title>
      <link>https://github.com/SagerNet/sing-box</link>
      <description>&lt;p&gt;The universal proxy platform&lt;/p&gt;&lt;hr&gt;&lt;blockquote&gt; 
 &lt;p&gt;Sponsored by &lt;a href="https://go.warp.dev/sing-box"&gt;Warp&lt;/a&gt;, built for coding with multiple AI agents&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;a href="https://go.warp.dev/sing-box"&gt; &lt;img alt="Warp sponsorship" width="400" src="https://github.com/warpdotdev/brand-assets/raw/refs/heads/main/Github/Sponsor/Warp-Github-LG-02.png" /&gt; &lt;/a&gt; 
&lt;hr /&gt; 
&lt;h1&gt;sing-box&lt;/h1&gt; 
&lt;p&gt;The universal proxy platform.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://repology.org/project/sing-box/versions"&gt;&lt;img src="https://repology.org/badge/vertical-allrepos/sing-box.svg?sanitize=true" alt="Packaging status" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://sing-box.sagernet.org"&gt;https://sing-box.sagernet.org&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;Copyright (C) 2022 by nekohasekai &amp;lt;contact-sagernet@sekai.icu&amp;gt;

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program. If not, see &amp;lt;http://www.gnu.org/licenses/&amp;gt;.

In addition, no derivative work may use the name or imply association
with this application without prior consent.
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>basecamp/omarchy</title>
      <link>https://github.com/basecamp/omarchy</link>
      <description>&lt;p&gt;Opinionated Arch/Hyprland Setup&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Omarchy&lt;/h1&gt; 
&lt;p&gt;Turn a fresh Arch installation into a fully-configured, beautiful, and modern web development system based on Hyprland by running a single command. That's the one-line pitch for Omarchy (like it was for Omakub). No need to write bespoke configs for every essential tool just to get started or to be up on all the latest command-line tools. Omarchy is an opinionated take on what Linux can be at its best.&lt;/p&gt; 
&lt;p&gt;Read more at &lt;a href="https://omarchy.org"&gt;omarchy.org&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Omarchy is released under the &lt;a href="https://opensource.org/licenses/MIT"&gt;MIT License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Anuken/Mindustry</title>
      <link>https://github.com/Anuken/Mindustry</link>
      <description>&lt;p&gt;The automation tower defense RTS&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Anuken/Mindustry/master/core/assets-raw/sprites/ui/logo.png" alt="Logo" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Anuken/Mindustry/actions"&gt;&lt;img src="https://github.com/Anuken/Mindustry/workflows/Tests/badge.svg?event=push" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/mindustry"&gt;&lt;img src="https://img.shields.io/discord/391020510269669376.svg?logo=discord&amp;amp;logoColor=white&amp;amp;logoWidth=20&amp;amp;labelColor=7289DA&amp;amp;label=Discord&amp;amp;color=17cf48" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The automation tower defense RTS, written in Java.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;a href="https://trello.com/b/aE2tcUwF/mindustry-40-plans"&gt;Trello Board&lt;/a&gt;&lt;/em&gt;&lt;br /&gt; &lt;em&gt;&lt;a href="https://mindustrygame.github.io/wiki"&gt;Wiki&lt;/a&gt;&lt;/em&gt;&lt;br /&gt; &lt;em&gt;&lt;a href="https://mindustrygame.github.io/docs/"&gt;Javadoc&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/Anuken/Mindustry/master/CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Building&lt;/h2&gt; 
&lt;p&gt;Bleeding-edge builds are generated automatically for every commit. You can see them &lt;a href="https://github.com/Anuken/MindustryBuilds/releases"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you'd rather compile on your own, follow these instructions. First, make sure you have &lt;a href="https://adoptium.net/temurin/releases/?os=any&amp;amp;arch=any&amp;amp;version=17"&gt;JDK 17&lt;/a&gt; installed. &lt;strong&gt;Other JDK versions will not work.&lt;/strong&gt; Open a terminal in the Mindustry directory and run the following commands:&lt;/p&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Running:&lt;/em&gt; &lt;code&gt;gradlew desktop:run&lt;/code&gt;&lt;br /&gt; &lt;em&gt;Building:&lt;/em&gt; &lt;code&gt;gradlew desktop:dist&lt;/code&gt;&lt;br /&gt; &lt;em&gt;Sprite Packing:&lt;/em&gt; &lt;code&gt;gradlew tools:pack&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Linux/Mac OS&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Running:&lt;/em&gt; &lt;code&gt;./gradlew desktop:run&lt;/code&gt;&lt;br /&gt; &lt;em&gt;Building:&lt;/em&gt; &lt;code&gt;./gradlew desktop:dist&lt;/code&gt;&lt;br /&gt; &lt;em&gt;Sprite Packing:&lt;/em&gt; &lt;code&gt;./gradlew tools:pack&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Server&lt;/h3&gt; 
&lt;p&gt;Server builds are bundled with each released build (in Releases). If you'd rather compile on your own, replace 'desktop' with 'server', e.g. &lt;code&gt;gradlew server:dist&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Android&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install the Android SDK &lt;a href="https://developer.android.com/studio#command-tools"&gt;here.&lt;/a&gt; Make sure you're downloading the "Command line tools only", as Android Studio is not required.&lt;/li&gt; 
 &lt;li&gt;In the unzipped Android SDK folder, find the cmdline-tools directory. Then create a folder inside of it called &lt;code&gt;latest&lt;/code&gt; and put all of its contents into the newly created folder.&lt;/li&gt; 
 &lt;li&gt;In the same directory run the command &lt;code&gt;sdkmanager --licenses&lt;/code&gt; (or &lt;code&gt;./sdkmanager --licenses&lt;/code&gt; if on linux/mac)&lt;/li&gt; 
 &lt;li&gt;Set the &lt;code&gt;ANDROID_HOME&lt;/code&gt; environment variable to point to your unzipped Android SDK directory.&lt;/li&gt; 
 &lt;li&gt;Enable developer mode on your device/emulator. If you are on testing on a phone you can follow &lt;a href="https://developer.android.com/studio/command-line/adb#Enabling"&gt;these instructions&lt;/a&gt;, otherwise you need to google how to enable your emulator's developer mode specifically.&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;gradlew android:assembleDebug&lt;/code&gt; (or &lt;code&gt;./gradlew&lt;/code&gt; if on linux/mac). This will create an unsigned APK in &lt;code&gt;android/build/outputs/apk&lt;/code&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;To debug the application on a connected device/emulator, run &lt;code&gt;gradlew android:installDebug android:run&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Troubleshooting&lt;/h3&gt; 
&lt;h4&gt;Permission Denied&lt;/h4&gt; 
&lt;p&gt;If the terminal returns &lt;code&gt;Permission denied&lt;/code&gt; or &lt;code&gt;Command not found&lt;/code&gt; on Mac/Linux, run &lt;code&gt;chmod +x ./gradlew&lt;/code&gt; before running &lt;code&gt;./gradlew&lt;/code&gt;. &lt;em&gt;This is a one-time procedure.&lt;/em&gt;&lt;/p&gt; 
&lt;h4&gt;Where is the &lt;code&gt;mindustry.gen&lt;/code&gt; package?&lt;/h4&gt; 
&lt;p&gt;As the name implies, &lt;code&gt;mindustry.gen&lt;/code&gt; is generated &lt;em&gt;at build time&lt;/em&gt; based on other code. You will not find source code for this package in the repository, and it should not be edited by hand.&lt;/p&gt; 
&lt;p&gt;The following is a non-exhaustive list of the "source" of generated code in &lt;code&gt;mindustry.gen&lt;/code&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;Call&lt;/code&gt;, &lt;code&gt;*Packet&lt;/code&gt; classes: Generated from methods marked with &lt;code&gt;@Remote&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;All entity classes (&lt;code&gt;Unit&lt;/code&gt;, &lt;code&gt;EffectState&lt;/code&gt;, &lt;code&gt;Posc&lt;/code&gt;, etc): Generated from component classes in the &lt;code&gt;mindustry.entities.comp&lt;/code&gt; package, and combined using definitions in &lt;code&gt;mindustry.content.UnitTypes&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Sounds&lt;/code&gt;, &lt;code&gt;Musics&lt;/code&gt;, &lt;code&gt;Tex&lt;/code&gt;, &lt;code&gt;Icon&lt;/code&gt;, etc: Generated based on files in the respective asset folders.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;Gradle may take up to several minutes to download files. Be patient. &lt;br /&gt; After building, the output .JAR file should be in &lt;code&gt;/desktop/build/libs/Mindustry.jar&lt;/code&gt; for desktop builds, and in &lt;code&gt;/server/build/libs/server-release.jar&lt;/code&gt; for server builds.&lt;/p&gt; 
&lt;h2&gt;Feature Requests&lt;/h2&gt; 
&lt;p&gt;Post feature requests and feedback &lt;a href="https://github.com/Anuken/Mindustry-Suggestions/issues/new/choose"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Downloads&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;a href="https://anuke.itch.io/mindustry"&gt;&lt;img src="https://static.itch.io/images/badge.svg?sanitize=true" alt="" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://play.google.com/store/apps/details?id=io.anuke.mindustry"&gt;&lt;img src="https://play.google.com/intl/en_us/badges/images/generic/en-play-badge.png" alt="" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://f-droid.org/packages/io.anuke.mindustry"&gt;&lt;img src="https://fdroid.gitlab.io/artwork/badge/get-it-on.png" alt="" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://flathub.org/apps/details/com.github.Anuken.Mindustry"&gt;&lt;img src="https://flathub.org/assets/badges/flathub-badge-en.svg?sanitize=true" alt="" /&gt;&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>Skyvern-AI/skyvern</title>
      <link>https://github.com/Skyvern-AI/skyvern</link>
      <description>&lt;p&gt;Automate browser-based workflows with LLMs and Computer Vision&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; &lt;a href="https://www.skyvern.com"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="fern/images/skyvern_logo.png" /&gt; 
   &lt;img height="120" src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/skyvern_logo_blackbg.png" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;br /&gt; &lt;/h1&gt; 
&lt;p align="center"&gt; 🐉 Automate Browser-based workflows using LLMs and Computer Vision 🐉 &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.skyvern.com/"&gt;&lt;img src="https://img.shields.io/badge/Website-blue?logo=googlechrome&amp;amp;logoColor=black" /&gt;&lt;/a&gt; &lt;a href="https://www.skyvern.com/docs/"&gt;&lt;img src="https://img.shields.io/badge/Docs-yellow?logo=gitbook&amp;amp;logoColor=black" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;&lt;img src="https://img.shields.io/discord/1212486326352617534?logo=discord&amp;amp;label=discord" /&gt;&lt;/a&gt; 
 &lt;!-- &lt;a href="https://pepy.tech/project/skyvern" target="_blank"&gt;&lt;img src="https://static.pepy.tech/badge/skyvern" alt="Total Downloads"/&gt;&lt;/a&gt; --&gt; &lt;a href="https://github.com/skyvern-ai/skyvern"&gt;&lt;img src="https://img.shields.io/github/stars/skyvern-ai/skyvern" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Skyvern-AI/skyvern/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/skyvern-ai/skyvern" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/skyvernai"&gt;&lt;img src="https://img.shields.io/twitter/follow/skyvernai?style=social" /&gt;&lt;/a&gt; &lt;a href="https://www.linkedin.com/company/95726232"&gt;&lt;img src="https://img.shields.io/badge/Follow%20 on%20LinkedIn-8A2BE2?logo=linkedin" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.skyvern.com"&gt;Skyvern&lt;/a&gt; automates browser-based workflows using LLMs and computer vision. It provides a simple API endpoint to fully automate manual workflows on a large number of websites, replacing brittle or unreliable automation solutions.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/geico_shu_recording_cropped.gif" /&gt; &lt;/p&gt; 
&lt;p&gt;Traditional approaches to browser automations required writing custom scripts for websites, often relying on DOM parsing and XPath-based interactions which would break whenever the website layouts changed.&lt;/p&gt; 
&lt;p&gt;Instead of only relying on code-defined XPath interactions, Skyvern relies on Vision LLMs to learn and interact with the websites.&lt;/p&gt; 
&lt;h1&gt;How it works&lt;/h1&gt; 
&lt;p&gt;Skyvern was inspired by the Task-Driven autonomous agent design popularized by &lt;a href="https://github.com/yoheinakajima/babyagi"&gt;BabyAGI&lt;/a&gt; and &lt;a href="https://github.com/Significant-Gravitas/AutoGPT"&gt;AutoGPT&lt;/a&gt; -- with one major bonus: we give Skyvern the ability to interact with websites using browser automation libraries like &lt;a href="https://playwright.dev/"&gt;Playwright&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Skyvern uses a swarm of agents to comprehend a website, and plan and execute its actions:&lt;/p&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="fern/images/skyvern_2_0_system_diagram.png" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/skyvern_2_0_system_diagram.png" /&gt; 
&lt;/picture&gt; 
&lt;p&gt;This approach has a few advantages:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Skyvern can operate on websites it's never seen before, as it's able to map visual elements to actions necessary to complete a workflow, without any customized code&lt;/li&gt; 
 &lt;li&gt;Skyvern is resistant to website layout changes, as there are no pre-determined XPaths or other selectors our system is looking for while trying to navigate&lt;/li&gt; 
 &lt;li&gt;Skyvern is able to take a single workflow and apply it to a large number of websites, as it's able to reason through the interactions necessary to complete the workflow&lt;/li&gt; 
 &lt;li&gt;Skyvern leverages LLMs to reason through interactions to ensure we can cover complex situations. Examples include: 
  &lt;ol&gt; 
   &lt;li&gt;If you wanted to get an auto insurance quote from Geico, the answer to a common question "Were you eligible to drive at 18?" could be inferred from the driver receiving their license at age 16&lt;/li&gt; 
   &lt;li&gt;If you were doing competitor analysis, it's understanding that an Arnold Palmer 22 oz can at 7/11 is almost definitely the same product as a 23 oz can at Gopuff (even though the sizes are slightly different, which could be a rounding error!)&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;A detailed technical report can be found &lt;a href="https://www.skyvern.com/blog/skyvern-2-0-state-of-the-art-web-navigation-with-85-8-on-webvoyager-eval/"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Demo&lt;/h1&gt; 
&lt;!-- Redo demo --&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/5cab4668-e8e2-4982-8551-aab05ff73a7f"&gt;https://github.com/user-attachments/assets/5cab4668-e8e2-4982-8551-aab05ff73a7f&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Performance &amp;amp; Evaluation&lt;/h1&gt; 
&lt;p&gt;Skyvern has SOTA performance on the &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/webbench.ai"&gt;WebBench benchmark&lt;/a&gt; with a 64.4% accuracy. The technical report + evaluation can be found &lt;a href="https://www.skyvern.com/blog/web-bench-a-new-way-to-compare-ai-browser-agents/"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/performance/webbench_overall.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Performance on WRITE tasks (eg filling out forms, logging in, downloading files, etc)&lt;/h2&gt; 
&lt;p&gt;Skyvern is the best performing agent on WRITE tasks (eg filling out forms, logging in, downloading files, etc), which is primarily used for RPA (Robotic Process Automation) adjacent tasks.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/performance/webbench_write.png" /&gt; &lt;/p&gt; 
&lt;h1&gt;Quickstart&lt;/h1&gt; 
&lt;h2&gt;Skyvern Cloud&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com"&gt;Skyvern Cloud&lt;/a&gt; is a managed cloud version of Skyvern that allows you to run Skyvern without worrying about the infrastructure. It allows you to run multiple Skyvern instances in parallel and comes bundled with anti-bot detection mechanisms, proxy network, and CAPTCHA solvers.&lt;/p&gt; 
&lt;p&gt;If you'd like to try it out, navigate to &lt;a href="https://app.skyvern.com"&gt;app.skyvern.com&lt;/a&gt; and create an account.&lt;/p&gt; 
&lt;h2&gt;Install &amp;amp; Run&lt;/h2&gt; 
&lt;p&gt;Dependencies needed:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.python.org/downloads/"&gt;Python 3.11.x&lt;/a&gt;, works with 3.12, not ready yet for 3.13&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nodejs.org/en/download/"&gt;NodeJS &amp;amp; NPM&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Additionally, for Windows:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://rustup.rs/"&gt;Rust&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;VS Code with C++ dev tools and Windows SDK&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;1. Install Skyvern&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install skyvern
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Run Skyvern&lt;/h3&gt; 
&lt;p&gt;This is most helpful for first time run (db setup, db migrations etc).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;skyvern quickstart
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Run task&lt;/h3&gt; 
&lt;h4&gt;UI (Recommended)&lt;/h4&gt; 
&lt;p&gt;Start the Skyvern service and UI (when DB is up and running)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;skyvern run all
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Go to &lt;a href="http://localhost:8080"&gt;http://localhost:8080&lt;/a&gt; and use the UI to run a task&lt;/p&gt; 
&lt;h4&gt;Code&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

skyvern = Skyvern()
task = await skyvern.run_task(prompt="Find the top post on hackernews today")
print(task)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Skyvern starts running the task in a browser that pops up and closes it when the task is done. You will be able to view the task from &lt;a href="http://localhost:8080/history"&gt;http://localhost:8080/history&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You can also run a task on different targets:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

# Run on Skyvern Cloud
skyvern = Skyvern(api_key="SKYVERN API KEY")

# Local Skyvern service
skyvern = Skyvern(base_url="http://localhost:8000", api_key="LOCAL SKYVERN API KEY")

task = await skyvern.run_task(prompt="Find the top post on hackernews today")
print(task)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Advanced Usage&lt;/h2&gt; 
&lt;h3&gt;Control your own browser (Chrome)&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;⚠️ WARNING: Since &lt;a href="https://developer.chrome.com/blog/remote-debugging-port"&gt;Chrome 136&lt;/a&gt;, Chrome refuses any CDP connect to the browser using the default user_data_dir. In order to use your browser data, Skyvern copies your default user_data_dir to &lt;code&gt;./tmp/user_data_dir&lt;/code&gt; the first time connecting to your local browser. ⚠️&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt;Just With Python Code&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

# The path to your Chrome browser. This example path is for Mac.
browser_path = "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
skyvern = Skyvern(
    base_url="http://localhost:8000",
    api_key="YOUR_API_KEY",
    browser_path=browser_path,
)
task = await skyvern.run_task(
    prompt="Find the top post on hackernews today",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;With Skyvern Service&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Add two variables to your .env file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# The path to your Chrome browser. This example path is for Mac.
CHROME_EXECUTABLE_PATH="/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
BROWSER_TYPE=cdp-connect
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Restart Skyvern service &lt;code&gt;skyvern run all&lt;/code&gt; and run the task through UI or code&lt;/p&gt; 
&lt;h3&gt;Run Skyvern with any remote browser&lt;/h3&gt; 
&lt;p&gt;Grab the cdp connection url and pass it to Skyvern&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

skyvern = Skyvern(cdp_url="your cdp connection url")
task = await skyvern.run_task(
    prompt="Find the top post on hackernews today",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Get consistent output schema from your run&lt;/h3&gt; 
&lt;p&gt;You can do this by adding the &lt;code&gt;data_extraction_schema&lt;/code&gt; parameter:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

skyvern = Skyvern()
task = await skyvern.run_task(
    prompt="Find the top post on hackernews today",
    data_extraction_schema={
        "type": "object",
        "properties": {
            "title": {
                "type": "string",
                "description": "The title of the top post"
            },
            "url": {
                "type": "string",
                "description": "The URL of the top post"
            },
            "points": {
                "type": "integer",
                "description": "Number of points the post has received"
            }
        }
    }
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Helpful commands to debug issues&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Launch the Skyvern Server Separately*
skyvern run server

# Launch the Skyvern UI
skyvern run ui

# Check status of the Skyvern service
skyvern status

# Stop the Skyvern service
skyvern stop all

# Stop the Skyvern UI
skyvern stop ui

# Stop the Skyvern Server Separately
skyvern stop server
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Docker Compose setup&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Make sure you have &lt;a href="https://www.docker.com/products/docker-desktop/"&gt;Docker Desktop&lt;/a&gt; installed and running on your machine&lt;/li&gt; 
 &lt;li&gt;Make sure you don't have postgres running locally (Run &lt;code&gt;docker ps&lt;/code&gt; to check)&lt;/li&gt; 
 &lt;li&gt;Clone the repository and navigate to the root directory&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;skyvern init llm&lt;/code&gt; to generate a &lt;code&gt;.env&lt;/code&gt; file. This will be copied into the Docker image.&lt;/li&gt; 
 &lt;li&gt;Fill in the LLM provider key on the &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt;. &lt;em&gt;If you want to run Skyvern on a remote server, make sure you set the correct server ip for the UI container in &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt;.&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Run the following command via the commandline: &lt;pre&gt;&lt;code class="language-bash"&gt; docker compose up -d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Navigate to &lt;code&gt;http://localhost:8080&lt;/code&gt; in your browser to start using the UI&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; Only one Postgres container can run on port 5432 at a time. If you switch from the CLI-managed Postgres to Docker Compose, you must first remove the original container:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;docker rm -f postgresql-container
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;If you encounter any database related errors while using Docker to run Skyvern, check which Postgres container is running with &lt;code&gt;docker ps&lt;/code&gt;.&lt;/p&gt; 
&lt;h1&gt;Skyvern Features&lt;/h1&gt; 
&lt;h2&gt;Skyvern Tasks&lt;/h2&gt; 
&lt;p&gt;Tasks are the fundamental building block inside Skyvern. Each task is a single request to Skyvern, instructing it to navigate through a website and accomplish a specific goal.&lt;/p&gt; 
&lt;p&gt;Tasks require you to specify a &lt;code&gt;url&lt;/code&gt;, &lt;code&gt;prompt&lt;/code&gt;, and can optionally include a &lt;code&gt;data schema&lt;/code&gt; (if you want the output to conform to a specific schema) and &lt;code&gt;error codes&lt;/code&gt; (if you want Skyvern to stop running in specific situations).&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/skyvern_2_0_screenshot.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Skyvern Workflows&lt;/h2&gt; 
&lt;p&gt;Workflows are a way to chain multiple tasks together to form a cohesive unit of work.&lt;/p&gt; 
&lt;p&gt;For example, if you wanted to download all invoices newer than January 1st, you could create a workflow that first navigated to the invoices page, then filtered down to only show invoices newer than January 1st, extracted a list of all eligible invoices, and iterated through each invoice to download it.&lt;/p&gt; 
&lt;p&gt;Another example is if you wanted to automate purchasing products from an e-commerce store, you could create a workflow that first navigated to the desired product, then added it to a cart. Second, it would navigate to the cart and validate the cart state. Finally, it would go through the checkout process to purchase the items.&lt;/p&gt; 
&lt;p&gt;Supported workflow features include:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Browser Task&lt;/li&gt; 
 &lt;li&gt;Browser Action&lt;/li&gt; 
 &lt;li&gt;Data Extraction&lt;/li&gt; 
 &lt;li&gt;Validation&lt;/li&gt; 
 &lt;li&gt;For Loops&lt;/li&gt; 
 &lt;li&gt;File parsing&lt;/li&gt; 
 &lt;li&gt;Sending emails&lt;/li&gt; 
 &lt;li&gt;Text Prompts&lt;/li&gt; 
 &lt;li&gt;HTTP Request Block&lt;/li&gt; 
 &lt;li&gt;Custom Code Block&lt;/li&gt; 
 &lt;li&gt;Uploading files to block storage&lt;/li&gt; 
 &lt;li&gt;(Coming soon) Conditionals&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/block_example_v2.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Livestreaming&lt;/h2&gt; 
&lt;p&gt;Skyvern allows you to livestream the viewport of the browser to your local machine so that you can see exactly what Skyvern is doing on the web. This is useful for debugging and understanding how Skyvern is interacting with a website, and intervening when necessary&lt;/p&gt; 
&lt;h2&gt;Form Filling&lt;/h2&gt; 
&lt;p&gt;Skyvern is natively capable of filling out form inputs on websites. Passing in information via the &lt;code&gt;navigation_goal&lt;/code&gt; will allow Skyvern to comprehend the information and fill out the form accordingly.&lt;/p&gt; 
&lt;h2&gt;Data Extraction&lt;/h2&gt; 
&lt;p&gt;Skyvern is also capable of extracting data from a website.&lt;/p&gt; 
&lt;p&gt;You can also specify a &lt;code&gt;data_extraction_schema&lt;/code&gt; directly within the main prompt to tell Skyvern exactly what data you'd like to extract from the website, in jsonc format. Skyvern's output will be structured in accordance to the supplied schema.&lt;/p&gt; 
&lt;h2&gt;File Downloading&lt;/h2&gt; 
&lt;p&gt;Skyvern is also capable of downloading files from a website. All downloaded files are automatically uploaded to block storage (if configured), and you can access them via the UI.&lt;/p&gt; 
&lt;h2&gt;Authentication&lt;/h2&gt; 
&lt;p&gt;Skyvern supports a number of different authentication methods to make it easier to automate tasks behind a login. If you'd like to try it out, please reach out to us &lt;a href="mailto:founders@skyvern.com"&gt;via email&lt;/a&gt; or &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;discord&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/secure_password_task_example.png" /&gt; &lt;/p&gt; 
&lt;h3&gt;🔐 2FA Support (TOTP)&lt;/h3&gt; 
&lt;p&gt;Skyvern supports a number of different 2FA methods to allow you to automate workflows that require 2FA.&lt;/p&gt; 
&lt;p&gt;Examples include:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;QR-based 2FA (e.g. Google Authenticator, Authy)&lt;/li&gt; 
 &lt;li&gt;Email based 2FA&lt;/li&gt; 
 &lt;li&gt;SMS based 2FA&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;🔐 Learn more about 2FA support &lt;a href="https://www.skyvern.com/docs/credentials/totp"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Password Manager Integrations&lt;/h3&gt; 
&lt;p&gt;Skyvern currently supports the following password manager integrations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Bitwarden&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; 1Password&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; LastPass&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Model Context Protocol (MCP)&lt;/h2&gt; 
&lt;p&gt;Skyvern supports the Model Context Protocol (MCP) to allow you to use any LLM that supports MCP.&lt;/p&gt; 
&lt;p&gt;See the MCP documentation &lt;a href="https://github.com/Skyvern-AI/skyvern/raw/main/integrations/mcp/README.md"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Zapier / Make.com / N8N Integration&lt;/h2&gt; 
&lt;p&gt;Skyvern supports Zapier, Make.com, and N8N to allow you to connect your Skyvern workflows to other apps.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.skyvern.com/docs/integrations/zapier"&gt;Zapier&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.skyvern.com/docs/integrations/make.com"&gt;Make.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.skyvern.com/docs/integrations/n8n"&gt;N8N&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;🔐 Learn more about 2FA support &lt;a href="https://www.skyvern.com/docs/credentials/totp"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Real-world examples of Skyvern&lt;/h1&gt; 
&lt;p&gt;We love to see how Skyvern is being used in the wild. Here are some examples of how Skyvern is being used to automate workflows in the real world. Please open PRs to add your own examples!&lt;/p&gt; 
&lt;h2&gt;Invoice Downloading on many different websites&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://meetings.hubspot.com/skyvern/demo"&gt;Book a demo to see it live&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/invoice_downloading.gif" /&gt; &lt;/p&gt; 
&lt;h2&gt;Automate the job application process&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/job_application"&gt;💡 See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/job_application_demo.gif" /&gt; &lt;/p&gt; 
&lt;h2&gt;Automate materials procurement for a manufacturing company&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/finditparts"&gt;💡 See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/finditparts_recording_crop.gif" /&gt; &lt;/p&gt; 
&lt;h2&gt;Navigating to government websites to register accounts or fill out forms&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/california_edd"&gt;💡 See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/edd_services.gif" /&gt; &lt;/p&gt; 
&lt;!-- Add example of delaware entity lookups x2 --&gt; 
&lt;h2&gt;Filling out random contact us forms&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/contact_us_forms"&gt;💡 See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/contact_forms.gif" /&gt; &lt;/p&gt; 
&lt;h2&gt;Retrieving insurance quotes from insurance providers in any language&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/bci_seguros"&gt;💡 See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/bci_seguros_recording.gif" /&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/geico"&gt;💡 See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/geico_shu_recording_cropped.gif" /&gt; &lt;/p&gt; 
&lt;h1&gt;Contributor Setup&lt;/h1&gt; 
&lt;p&gt;Make sure to have &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;uv&lt;/a&gt; installed.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Run this to create your virtual environment (&lt;code&gt;.venv&lt;/code&gt;) &lt;pre&gt;&lt;code class="language-bash"&gt;uv sync --group dev
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Perform initial server configuration &lt;pre&gt;&lt;code class="language-bash"&gt;uv run skyvern quickstart
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Navigate to &lt;code&gt;http://localhost:8080&lt;/code&gt; in your browser to start using the UI &lt;em&gt;The Skyvern CLI supports Windows, WSL, macOS, and Linux environments.&lt;/em&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h1&gt;Documentation&lt;/h1&gt; 
&lt;p&gt;More extensive documentation can be found on our &lt;a href="https://www.skyvern.com/docs"&gt;📕 docs page&lt;/a&gt;. Please let us know if something is unclear or missing by opening an issue or reaching out to us &lt;a href="mailto:founders@skyvern.com"&gt;via email&lt;/a&gt; or &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;discord&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Supported LLMs&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Provider&lt;/th&gt; 
   &lt;th&gt;Supported Models&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
   &lt;td&gt;gpt4-turbo, gpt-4o, gpt-4o-mini&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Anthropic&lt;/td&gt; 
   &lt;td&gt;Claude 3 (Haiku, Sonnet, Opus), Claude 3.5 (Sonnet)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Azure OpenAI&lt;/td&gt; 
   &lt;td&gt;Any GPT models. Better performance with a multimodal llm (azure/gpt4-o)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AWS Bedrock&lt;/td&gt; 
   &lt;td&gt;Anthropic Claude 3 (Haiku, Sonnet, Opus), Claude 3.5 (Sonnet)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Gemini&lt;/td&gt; 
   &lt;td&gt;Gemini 2.5 Pro and flash, Gemini 2.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ollama&lt;/td&gt; 
   &lt;td&gt;Run any locally hosted model via &lt;a href="https://github.com/ollama/ollama"&gt;Ollama&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenRouter&lt;/td&gt; 
   &lt;td&gt;Access models through &lt;a href="https://openrouter.ai"&gt;OpenRouter&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI-compatible&lt;/td&gt; 
   &lt;td&gt;Any custom API endpoint that follows OpenAI's API format (via &lt;a href="https://docs.litellm.ai/docs/providers/openai_compatible"&gt;liteLLM&lt;/a&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Environment Variables&lt;/h4&gt; 
&lt;h5&gt;OpenAI&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_OPENAI&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register OpenAI models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI API Key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_API_BASE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI API Base, optional&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://openai.api.base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_ORGANIZATION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI Organization ID, optional&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;your-org-id&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;OPENAI_GPT4O&lt;/code&gt;, &lt;code&gt;OPENAI_GPT4O_MINI&lt;/code&gt;, &lt;code&gt;OPENAI_GPT4_1&lt;/code&gt;, &lt;code&gt;OPENAI_O4_MINI&lt;/code&gt;, &lt;code&gt;OPENAI_O3&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;Anthropic&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_ANTHROPIC&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register Anthropic models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Anthropic API key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended&lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;ANTHROPIC_CLAUDE3.5_SONNET&lt;/code&gt;, &lt;code&gt;ANTHROPIC_CLAUDE3.7_SONNET&lt;/code&gt;, &lt;code&gt;ANTHROPIC_CLAUDE4_OPUS&lt;/code&gt;, &lt;code&gt;ANTHROPIC_CLAUDE4_SONNET&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;Azure OpenAI&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_AZURE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register Azure OpenAI models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure deployment API key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_DEPLOYMENT&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure OpenAI Deployment Name&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;skyvern-deployment&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_API_BASE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure deployment api base url&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://skyvern-deployment.openai.azure.com/&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_API_VERSION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure API Version&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;2024-02-01&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;AZURE_OPENAI&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;AWS Bedrock&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_BEDROCK&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register AWS Bedrock models. To use AWS Bedrock, you need to make sure your &lt;a href="https://github.com/boto/boto3?tab=readme-ov-file#using-boto3"&gt;AWS configurations&lt;/a&gt; are set up correctly first.&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;BEDROCK_ANTHROPIC_CLAUDE3.7_SONNET_INFERENCE_PROFILE&lt;/code&gt;, &lt;code&gt;BEDROCK_ANTHROPIC_CLAUDE4_OPUS_INFERENCE_PROFILE&lt;/code&gt;, &lt;code&gt;BEDROCK_ANTHROPIC_CLAUDE4_SONNET_INFERENCE_PROFILE&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;Gemini&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_GEMINI&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register Gemini models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;GEMINI_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Gemini API Key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;your_google_gemini_api_key&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;GEMINI_2.5_PRO_PREVIEW&lt;/code&gt;, &lt;code&gt;GEMINI_2.5_FLASH_PREVIEW&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;Ollama&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_OLLAMA&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register local models via Ollama&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OLLAMA_SERVER_URL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;URL for your Ollama server&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;http://host.docker.internal:11434&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OLLAMA_MODEL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Ollama model name to load&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;qwen2.5:7b-instruct&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;OLLAMA&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Note: Ollama does not support vision yet.&lt;/p&gt; 
&lt;h5&gt;OpenRouter&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_OPENROUTER&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register OpenRouter models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenRouter API key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENROUTER_MODEL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenRouter model name&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;mistralai/mistral-small-3.1-24b-instruct&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENROUTER_API_BASE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenRouter API base URL&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://api.openrouter.ai/v1&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;OPENROUTER&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;OpenAI-Compatible&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_OPENAI_COMPATIBLE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register a custom OpenAI-compatible API endpoint&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_MODEL_NAME&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Model name for OpenAI-compatible endpoint&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;yi-34b&lt;/code&gt;, &lt;code&gt;gpt-3.5-turbo&lt;/code&gt;, &lt;code&gt;mistral-large&lt;/code&gt;, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;API key for OpenAI-compatible endpoint&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_API_BASE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Base URL for OpenAI-compatible endpoint&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://api.together.xyz/v1&lt;/code&gt;, &lt;code&gt;http://localhost:8000/v1&lt;/code&gt;, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_API_VERSION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;API version for OpenAI-compatible endpoint, optional&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;2023-05-15&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_MAX_TOKENS&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Maximum tokens for completion, optional&lt;/td&gt; 
   &lt;td&gt;Integer&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;4096&lt;/code&gt;, &lt;code&gt;8192&lt;/code&gt;, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_TEMPERATURE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Temperature setting, optional&lt;/td&gt; 
   &lt;td&gt;Float&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;0.0&lt;/code&gt;, &lt;code&gt;0.5&lt;/code&gt;, &lt;code&gt;0.7&lt;/code&gt;, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_SUPPORTS_VISION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Whether model supports vision, optional&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Supported LLM Key: &lt;code&gt;OPENAI_COMPATIBLE&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;General LLM Configuration&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;LLM_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The name of the model you want to use&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;See supported LLM keys above&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;SECONDARY_LLM_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The name of the model for mini agents skyvern runs with&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;See supported LLM keys above&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;LLM_CONFIG_MAX_TOKENS&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Override the max tokens used by the LLM&lt;/td&gt; 
   &lt;td&gt;Integer&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;128000&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Feature Roadmap&lt;/h1&gt; 
&lt;p&gt;This is our planned roadmap for the next few months. If you have any suggestions or would like to see a feature added, please don't hesitate to reach out to us &lt;a href="mailto:founders@skyvern.com"&gt;via email&lt;/a&gt; or &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;discord&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Open Source&lt;/strong&gt; - Open Source Skyvern's core codebase&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Workflow support&lt;/strong&gt; - Allow support to chain multiple Skyvern calls together&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Improved context&lt;/strong&gt; - Improve Skyvern's ability to understand content around interactable elements by introducing feeding relevant label context through the text prompt&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Cost Savings&lt;/strong&gt; - Improve Skyvern's stability and reduce the cost of running Skyvern by optimizing the context tree passed into Skyvern&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Self-serve UI&lt;/strong&gt; - Deprecate the Streamlit UI in favour of a React-based UI component that allows users to kick off new jobs in Skyvern&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Workflow UI Builder&lt;/strong&gt; - Introduce a UI to allow users to build and analyze workflows visually&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Chrome Viewport streaming&lt;/strong&gt; - Introduce a way to live-stream the Chrome viewport to the user's browser (as a part of the self-serve UI)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Past Runs UI&lt;/strong&gt; - Deprecate the Streamlit UI in favour of a React-based UI that allows you to visualize past runs and their results&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Auto workflow builder ("Observer") mode&lt;/strong&gt; - Allow Skyvern to auto-generate workflows as it's navigating the web to make it easier to build new workflows&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Prompt Caching&lt;/strong&gt; - Introduce a caching layer to the LLM calls to dramatically reduce the cost of running Skyvern (memorize past actions and repeat them!)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Web Evaluation Dataset&lt;/strong&gt; - Integrate Skyvern with public benchmark tests to track the quality of our models over time&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Improved Debug mode&lt;/strong&gt; - Allow Skyvern to plan its actions and get "approval" before running them, allowing you to debug what it's doing and more easily iterate on the prompt&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Chrome Extension&lt;/strong&gt; - Allow users to interact with Skyvern through a Chrome extension (incl voice mode, saving tasks, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Skyvern Action Recorder&lt;/strong&gt; - Allow Skyvern to watch a user complete a task and then automatically generate a workflow for it&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Interactable Livestream&lt;/strong&gt; - Allow users to interact with the livestream in real-time to intervene when necessary (such as manually submitting sensitive forms)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Integrate LLM Observability tools&lt;/strong&gt; - Integrate LLM Observability tools to allow back-testing prompt changes with specific data sets + visualize the performance of Skyvern over time&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Langchain Integration&lt;/strong&gt; - Create langchain integration in langchain_community to use Skyvern as a "tool".&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Contributing&lt;/h1&gt; 
&lt;p&gt;We welcome PRs and suggestions! Don't hesitate to open a PR/issue or to reach out to us &lt;a href="mailto:founders@skyvern.com"&gt;via email&lt;/a&gt; or &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;discord&lt;/a&gt;. Please have a look at our &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/CONTRIBUTING.md"&gt;contribution guide&lt;/a&gt; and &lt;a href="https://github.com/skyvern-ai/skyvern/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22"&gt;"Help Wanted" issues&lt;/a&gt; to get started!&lt;/p&gt; 
&lt;p&gt;If you want to chat with the skyvern repository to get a high level overview of how it is structured, how to build off it, and how to resolve usage questions, check out &lt;a href="https://sage.storia.ai?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=skyvern-readme"&gt;Code Sage&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Telemetry&lt;/h1&gt; 
&lt;p&gt;By Default, Skyvern collects basic usage statistics to help us understand how Skyvern is being used. If you would like to opt-out of telemetry, please set the &lt;code&gt;SKYVERN_TELEMETRY&lt;/code&gt; environment variable to &lt;code&gt;false&lt;/code&gt;.&lt;/p&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;Skyvern's open source repository is supported via a managed cloud. All of the core logic powering Skyvern is available in this open source repository licensed under the &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/LICENSE"&gt;AGPL-3.0 License&lt;/a&gt;, with the exception of anti-bot measures available in our managed cloud offering.&lt;/p&gt; 
&lt;p&gt;If you have any questions or concerns around licensing, please &lt;a href="mailto:support@skyvern.com"&gt;contact us&lt;/a&gt; and we would be happy to help.&lt;/p&gt; 
&lt;h1&gt;Star History&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#Skyvern-AI/skyvern&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Skyvern-AI/skyvern&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PaddlePaddle/PaddleOCR</title>
      <link>https://github.com/PaddlePaddle/PaddleOCR</link>
      <description>&lt;p&gt;Turn any PDF or image document into structured data for your AI. A powerful, lightweight OCR toolkit that bridges the gap between images/PDFs and LLMs. Supports 100+ languages.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/Banner.png" alt="PaddleOCR Banner" /&gt; &lt;/p&gt; 
 &lt;p&gt;English | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_cn.md"&gt;简体中文&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_tcn.md"&gt;繁體中文&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ja.md"&gt;日本語&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ko.md"&gt;한국어&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_fr.md"&gt;Français&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ru.md"&gt;Русский&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_es.md"&gt;Español&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ar.md"&gt;العربية&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleOCR"&gt;&lt;img src="https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?color=ccf" alt="stars" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2507.05595"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2507.05595-b31b1b.svg?logo=arXiv" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/paddleocr"&gt;&lt;img src="https://static.pepy.tech/badge/paddleocr/month" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/paddleocr"&gt;&lt;img src="https://static.pepy.tech/badge/paddleocr" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/PaddlePaddle/PaddleOCR/network/dependents"&gt;&lt;img src="https://img.shields.io/badge/Used%20by-5.9k%2B%20repositories-blue" alt="Used by" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://img.shields.io/badge/python-3.8~3.12-aff.svg?sanitize=true" alt="python" /&gt; &lt;img src="https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg?sanitize=true" alt="os" /&gt; &lt;img src="https://img.shields.io/badge/hardware-cpu%2C%20gpu%2C%20xpu%2C%20npu-yellow.svg?sanitize=true" alt="hardware" /&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-Apache_2.0-green" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/PaddlePaddle/PaddleOCR"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;PaddleOCR is an industry-leading, production-ready OCR and document AI engine, offering end-to-end solutions from text extraction to intelligent document understanding&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h1&gt;PaddleOCR&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://www.paddlepaddle.org.cn/en"&gt;&lt;img src="https://img.shields.io/badge/PaddlePaddle-3.0-orange" alt="Framework" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Recognition%20Accuracy-%F0%9F%8F%86-green" alt="Accuracy" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Support_Languages-100+-brightgreen" alt="Multi-Language" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Handwriting-%E2%9C%93-success" alt="Handwriting" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Heterogeneous%20Hardware-Kunlunxin%20%7C%20Ascend_NPU-red" alt="Hardware" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] PaddleOCR now provides an MCP server that supports integration with Agent applications like Claude Desktop. For details, please refer to &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html"&gt;PaddleOCR MCP Server&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;The PaddleOCR 3.0 Technical Report is now available. See details at: &lt;a href="https://arxiv.org/abs/2507.05595"&gt;PaddleOCR 3.0 Technical Report&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;The PaddleOCR-VL Technical Report is now available. See details at &lt;a href="https://arxiv.org/abs/2510.14528"&gt;PaddleOCR-VL Technical Report&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;PaddleOCR&lt;/strong&gt; converts documents and images into &lt;strong&gt;structured, AI-friendly data&lt;/strong&gt; (like JSON and Markdown) with &lt;strong&gt;industry-leading accuracy&lt;/strong&gt;—powering AI applications for everyone from indie developers and startups to large enterprises worldwide. With over &lt;strong&gt;50,000 stars&lt;/strong&gt; and deep integration into leading projects like &lt;strong&gt;MinerU, RAGFlow, and OmniParser&lt;/strong&gt;, PaddleOCR has become the &lt;strong&gt;premier solution&lt;/strong&gt; for developers building intelligent document applications in the &lt;strong&gt;AI era&lt;/strong&gt;.&lt;/p&gt; 
&lt;h3&gt;PaddleOCR 3.0 Core Features&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/PaddlePaddle/PaddleOCR-VL_Online_Demo"&gt;&lt;img src="https://img.shields.io/badge/PaddleOCR--VL-_Demo_on_HuggingFace-yellow?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAF8AAABYCAMAAACkl9t/AAAAk1BMVEVHcEz/nQv/nQv/nQr/nQv/nQr/nQv/nQv/nQr/wRf/txT/pg7/yRr/rBD/zRz/ngv/oAz/zhz/nwv/txT/ngv/0B3+zBz/nQv/0h7/wxn/vRb/thXkuiT/rxH/pxD/ogzcqyf/nQvTlSz/czCxky7/SjifdjT/Mj3+Mj3wMj15aTnDNz+DSD9RTUBsP0FRO0Q6O0WyIxEIAAAAGHRSTlMADB8zSWF3krDDw8TJ1NbX5efv8ff9/fxKDJ9uAAAGKklEQVR42u2Z63qjOAyGC4RwCOfB2JAGqrSb2WnTw/1f3UaWcSGYNKTdf/P+mOkTrE+yJBulvfvLT2A5ruenaVHyIks33npl/6C4s/ZLAM45SOi/1FtZPyFur1OYofBX3w7d54Bxm+E8db+nDr12ttmESZ4zludJEG5S7TO72YPlKZFyE+YCYUJTBZsMiNS5Sd7NlDmKM2Eg2JQg8awbglfqgbhArjxkS7dgp2RH6hc9AMLdZYUtZN5DJr4molC8BfKrEkPKEnEVjLbgW1fLy77ZVOJagoIcLIl+IxaQZGjiX597HopF5CkaXVMDO9Pyix3AFV3kw4lQLCbHuMovz8FallbcQIJ5Ta0vks9RnolbCK84BtjKRS5uA43hYoZcOBGIG2Epbv6CvFVQ8m8loh66WNySsnN7htL58LNp+NXT8/PhXiBXPMjLSxtwp8W9f/1AngRierBkA+kk/IpUSOeKByzn8y3kAAAfh//0oXgV4roHm/kz4E2z//zRc3/lgwBzbM2mJxQEa5pqgX7d1L0htrhx7LKxOZlKbwcAWyEOWqYSI8YPtgDQVjpB5nvaHaSnBaQSD6hweDi8PosxD6/PT09YY3xQA7LTCTKfYX+QHpA0GCcqmEHvr/cyfKQTEuwgbs2kPxJEB0iNjfJcCTPyocx+A0griHSmADiC91oNGVwJ69RudYe65vJmoqfpul0lrqXadW0jFKH5BKwAeCq+Den7s+3zfRJzA61/Uj/9H/VzLKTx9jFPPdXeeP+L7WEvDLAKAIoF8bPTKT0+TM7W8ePj3Rz/Yn3kOAp2f1Kf0Weony7pn/cPydvhQYV+eFOfmOu7VB/ViPe34/EN3RFHY/yRuT8ddCtMPH/McBAT5s+vRde/gf2c/sPsjLK+m5IBQF5tO+h2tTlBGnP6693JdsvofjOPnnEHkh2TnV/X1fBl9S5zrwuwF8NFrAVJVwCAPTe8gaJlomqlp0pv4Pjn98tJ/t/fL++6unpR1YGC2n/KCoa0tTLoKiEeUPDl94nj+5/Tv3/eT5vBQ60X1S0oZr+IWRR8Ldhu7AlLjPISlJcO9vrFotky9SpzDequlwEir5beYAc0R7D9KS1DXva0jhYRDXoExPdc6yw5GShkZXe9QdO/uOvHofxjrV/TNS6iMJS+4TcSTgk9n5agJdBQbB//IfF/HpvPt3Tbi7b6I6K0R72p6ajryEJrENW2bbeVUGjfgoals4L443c7BEE4mJO2SpbRngxQrAKRudRzGQ8jVOL2qDVjjI8K1gc3TIJ5KiFZ1q+gdsARPB4NQS4AjwVSt72DSoXNyOWUrU5mQ9nRYyjp89Xo7oRI6Bga9QNT1mQ/ptaJq5T/7WcgAZywR/XlPGAUDdet3LE+qS0TI+g+aJU8MIqjo0Kx8Ly+maxLjJmjQ18rA0YCkxLQbUZP1WqdmyQGJLUm7VnQFqodmXSqmRrdVpqdzk5LvmvgtEcW8PMGdaS23EOWyDVbACZzUJPaqMbjDxpA3Qrgl0AikimGDbqmyT8P8NOYiqrldF8rX+YN7TopX4UoHuSCYY7cgX4gHwclQKl1zhx0THf+tCAUValzjI7Wg9EhptrkIcfIJjA94evOn8B2eHaVzvBrnl2ig0So6hvPaz0IGcOvTHvUIlE2+prqAxLSQxZlU2stql1NqCCLdIiIN/i1DBEHUoElM9dBravbiAnKqgpi4IBkw+utSPIoBijDXJipSVV7MpOEJUAc5Qmm3BnUN+w3hteEieYKfRZSIUcXKMVf0u5wD4EwsUNVvZOtUT7A2GkffHjByWpHqvRBYrTV72a6j8zZ6W0DTE86Hn04bmyWX3Ri9WH7ZU6Q7h+ZHo0nHUAcsQvVhXRDZHChwiyi/hnPuOsSEF6Exk3o6Y9DT1eZ+6cASXk2Y9k+6EOQMDGm6WBK10wOQJCBwren86cPPWUcRAnTVjGcU1LBgs9FURiX/e6479yZcLwCBmTxiawEwrOcleuu12t3tbLv/N4RLYIBhYexm7Fcn4OJcn0+zc+s8/VfPeddZHAGN6TT8eGczHdR/Gts1/MzDkThr23zqrVfAMFT33Nx1RJsx1k5zuWILLnG/vsH+Fv5D4NTVcp1Gzo8AAAAAElFTkSuQmCC&amp;amp;labelColor=white" alt="HuggingFace" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/application/detail/98365"&gt;&lt;img src="https://img.shields.io/badge/PaddleOCR--VL-_Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://www.modelscope.cn/studios/PaddlePaddle/PaddleOCR-VL_Online_Demo"&gt;&lt;img src="https://img.shields.io/badge/PaddleOCR--VL-_Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&amp;amp;labelColor=white" alt="ModelScope" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://aistudio.baidu.com/community/app/91660/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP--OCRv5-Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518494/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP--StructureV3-Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAgAAAAIACAMAAADDpiTIAAAABlBMVEU2P+X///+1KuUwAAAHKklEQVR42u3dS5bjOAwEwALvf2fMavZum6IAImI7b2yYSqU+1Zb//gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADKCR/+fzly7rD92yVg69xh8zeLwOa5w+ZvFYHtc4ft3ykB++cOm79PAp6YO2z/Ngl4ZO5l+9+yT4QAvLqS748VF33Ylzdvzpl72f6z53YIGJ6SZdPeNHcIwOycaADdLgCSIgAIgCOAACAAykIAEAAEAAFAABCAT+WQuQVgeBqXhXQIQAAYegowLQBpbg3gZGFyAC6vgBQAMREA2/YfDPxyaDQNyTNz+3Zwn5J4ZG7PB2h0kHhi7plPCImmJwkPzO0RMa3OET0i5uGlzHFze0xcu0vE2Dq3J4U2vEPgSaHbFzPNDQAAAAAAAMBNovdw+cP/ny+uaf7w/+eYADy8kE+F4Offdjn6zZXhAXgiA78G4MNNsmnu1Xr7b3mbOL8T5Ja5bw/A35EC2LiWpzt1y9jRugBy30fLg3NvHPvnuZcC2NsCUXA/aRmA89V07Fwgt37uH8deCmBr6N44pP4UgaUATpdA7v/cMbIB8okliY65/SW5HhJ1ehPmM+8edwXgpbu4R88FayR32Y/P7oZZbOx13/Zr//ZHx27bAPnkFoyewYlbAhD3TvBobr95gaUAtr1EdNx1lgI4OcTTuR3z6+FZMEDRcu9ZCuDgGCdyGxMa4EgBRMvcjrkM7NgBZw5c0TwAUWUhZwRXA2xaya65Xa3jO2qYZ8bu2AD5w38tG5V8aZpoGN6Tz0bOfa9bceyWAciTO0jWyO1Tc5cLwJmF/JfPnXVyu3/slgHIg1n79O2O5fZv+1cHV7sC2HYqmUdHysNzX3sVkMcjUK5Gc+dMs28E5bGtm0V3gloBOP9vgZv+4sYn3RUaYFMCol5uN77g6lUApc8pWs69Zn7snS9Z9Q8G0S0AUTVUUTG3A54R1KSvo/diLAv5fKzynZeN6xogC75u93+AtBTA47OlAFSv6qY/vp3DAjD8iv2ZdFYJwKynMhTK1rInPfzaxW81LnvSgFP9KxrATaCLA3DxHpbFX31ZyNm5XRZyXG5bNkAWfP0rcrsUwOgC6NIAzgBcBiqAWwPgLrAGuGBP6jr2sifdfiJ6QQM4Bbw4AK4B3129ZSFn53ZZyA/GyFty27IBFMDFAXAG8PbyLQv5xULGPRl0K3h2AbwcgCZPhs+LD1zLnjS6AN4NwMU/DVFh7LyhASreTbvqrxdr/J4XT4Swz4FrTS+AGJ7bNbwAYkxuWzZAVljHrJfbjb9wviYXwFO/FJ8Vli4vaICsEMFyBbA3tmtsAUS0zG1c/bj4YwsZH2/+Whd0+1Nb+S7IE2sfPw4RL0XmsR8Nqvz7qFngmPHF34EqjP15AAofAkosZKPC/K6FVoeP02Ehi540NG6AK/4pYP3cLgVwXwHkDQ1QcSGb/uF4WwCmfX8u/+4vgLINcMUlQIfcLgXwXAF0+BGkpQDuuJx7/hwgpu//cWVuO3wxJOz/z8297vgYBwaIO3O7Kn+c194578ltywbIgu8fl+Z2lS+APvnLjnOv8hsgSqxjgwL4Ln9LAezaj98tgPzy7ZcC+GQzxrWxXQpgx370dm6/H7v6jaBoso5dY1swAFlwHWvfBf5pxVa93fCtdx64+1dsgCy4joWvAfPX9VoKYMs6Zse9/8Mlvv7LILlhAfKFFdsSutJXAdFkL3qlADJPrXFcXAC5KYaH586jO9mtAch9S3T0GQJ726ZWAE49kjP3rlDJuetdaL/1zeqZY9c7CRz7s0wCUPxienQBnAuAAtAAlxaAAAxfyBQABSAACkAAFIAAKAABUAACMEkKwL170oh7V8ueNLoAjgTAXWAN4BRwcABcA2oABTA4AApAAyiAwQFQABpAAQwOgALQADMWUgCuEmNyu15fSIY3gFPAiwPgFFADKIDBAVAAGkABCIACmBqAUAAaQAHMDUCMWkgBuMWw3K43F5LhDeAU8OIAuAmkARTA4AAoAA2gAARAAUwNgLvAGkABDA6Au8AaoKOJuV0vLSTDG8Ap4MUBcBNIAyiAwQFQABpAAQwOgALQAApAABTA1AC4C6wBOhqb23V+IRneAE4BLw6Aa0ANoAAGB0ABaAAFMDgACkADKAABUABTA+AusAboKATAQs4trjV+IYcfuJYCcA6gAATAQk69dFkKQANYyLkFcLIBFIDLQAVwawDsSRrAEWBwAJwCagAFMDgACkADKIDBAVAAGkABCIACmBoAzwXWAApgcADsSRrg0iNACoACEADXgAIwdCFTACykALgGFIAfl0kBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPBv/gN+IH8U6YveYgAAAABJRU5ErkJggg==&amp;amp;labelColor=white" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518493/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP--ChatOCRv4-Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="AI Studio" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PaddleOCR-VL - Multilingual Document Parsing via a 0.9B VLM&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;The SOTA and resource-efficient model tailored for document parsing&lt;/strong&gt;, that supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-OCRv5 — Universal Scene Text Recognition&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;Single model supports five text types&lt;/strong&gt; (Simplified Chinese, Traditional Chinese, English, Japanese, and Pinyin) with &lt;strong&gt;13% accuracy improvement&lt;/strong&gt;. Solves multilingual mixed document recognition challenges.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-StructureV3 — Complex Document Parsing&lt;/strong&gt;&lt;br /&gt; Intelligently converts complex PDFs and document images into &lt;strong&gt;Markdown and JSON files that preserve original structure&lt;/strong&gt;. &lt;strong&gt;Outperforms&lt;/strong&gt; numerous commercial solutions in public benchmarks. &lt;strong&gt;Perfectly maintains document layout and hierarchical structure&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-ChatOCRv4 — Intelligent Information Extraction&lt;/strong&gt;&lt;br /&gt; Natively integrates ERNIE 4.5 to &lt;strong&gt;precisely extract key information&lt;/strong&gt; from massive documents, with 15% accuracy improvement over previous generation. Makes documents "&lt;strong&gt;understand&lt;/strong&gt;" your questions and provide accurate answers.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In addition to providing an outstanding model library, PaddleOCR 3.0 also offers user-friendly tools covering model training, inference, and service deployment, so developers can rapidly bring AI applications to production.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/Arch.jpg" alt="PaddleOCR Architecture" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Special Note&lt;/strong&gt;: PaddleOCR 3.x introduces several significant interface changes. &lt;strong&gt;Old code written based on PaddleOCR 2.x is likely incompatible with PaddleOCR 3.x&lt;/strong&gt;. Please ensure that the documentation you are reading matches the version of PaddleOCR you are using. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/upgrade_notes.html"&gt;This document&lt;/a&gt; explains the reasons for the upgrade and the major changes from PaddleOCR 2.x to 3.x.&lt;/p&gt; 
&lt;h2&gt;📣 Recent updates&lt;/h2&gt; 
&lt;h3&gt;🔥🔥 2025.10.16: PaddleOCR 3.3.0 released, includes:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Released PaddleOCR-VL:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Model Introduction&lt;/strong&gt;:&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;PaddleOCR-VL&lt;/strong&gt; is a SOTA and resource-efficient model tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a compact yet powerful vision-language model (VLM) that integrates a NaViT-style dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to enable accurate element recognition. &lt;strong&gt;This innovative model efficiently supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption&lt;/strong&gt;. Through comprehensive evaluations on widely used public benchmarks and in-house benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document parsing and element-level recognition. It significantly outperforms existing solutions, exhibits strong competitiveness against top-tier VLMs, and delivers fast inference speeds. These strengths make it highly suitable for practical deployment in real-world scenarios. The model has been released on &lt;a href="https://huggingface.co/PaddlePaddle/PaddleOCR-VL"&gt;HuggingFace&lt;/a&gt;. Everyone is welcome to download and use it! More introduction infomation can be found in &lt;a href="https://www.paddleocr.ai/latest/version3.x/algorithm/PaddleOCR-VL/PaddleOCR-VL.html"&gt;PaddleOCR-VL&lt;/a&gt;.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Core Features&lt;/strong&gt;:&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Compact yet Powerful VLM Architecture&lt;/strong&gt;: We present a novel vision-language model that is specifically designed for resource-efficient inference, achieving outstanding performance in element recognition. By integrating a NaViT-style dynamic high-resolution visual encoder with the lightweight ERNIE-4.5-0.3B language model, we significantly enhance the model’s recognition capabilities and decoding efficiency. This integration maintains high accuracy while reducing computational demands, making it well-suited for efficient and practical document processing applications.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;SOTA Performance on Document Parsing&lt;/strong&gt;: PaddleOCR-VL achieves state-of-the-art performance in both page-level document parsing and element-level recognition. It significantly outperforms existing pipeline-based solutions and exhibiting strong competitiveness against leading vision-language models (VLMs) in document parsing. Moreover, it excels in recognizing complex document elements, such as text, tables, formulas, and charts, making it suitable for a wide range of challenging content types, including handwritten text and historical documents. This makes it highly versatile and suitable for a wide range of document types and scenarios.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Multilingual Support&lt;/strong&gt;: PaddleOCR-VL Supports 109 languages, covering major global languages, including but not limited to Chinese, English, Japanese, Latin, and Korean, as well as languages with different scripts and structures, such as Russian (Cyrillic script), Arabic, Hindi (Devanagari script), and Thai. This broad language coverage substantially enhances the applicability of our system to multilingual and globalized document processing scenarios.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Released PP-OCRv5 Multilingual Recognition Model:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Improved the accuracy and coverage of Latin script recognition; added support for Cyrillic, Arabic, Devanagari, Telugu, Tamil, and other language systems, covering recognition of 109 languages. The model has only 2M parameters, and the accuracy of some models has increased by over 40% compared to the previous generation.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.08.21: Release of PaddleOCR 3.2.0&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Significant Model Additions:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Introduced training, inference, and deployment for PP-OCRv5 recognition models in English, Thai, and Greek. &lt;strong&gt;The PP-OCRv5 English model delivers an 11% improvement in English scenarios compared to the main PP-OCRv5 model, with the Thai and Greek recognition models achieving accuracies of 82.68% and 89.28%, respectively.&lt;/strong&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deployment Capability Upgrades:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Full support for PaddlePaddle framework versions 3.1.0 and 3.1.1.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Comprehensive upgrade of the PP-OCRv5 C++ local deployment solution, now supporting both Linux and Windows, with feature parity and identical accuracy to the Python implementation.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;High-performance inference now supports CUDA 12, and inference can be performed using either the Paddle Inference or ONNX Runtime backends.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;The high-stability service-oriented deployment solution is now fully open-sourced, allowing users to customize Docker images and SDKs as required.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;The high-stability service-oriented deployment solution also supports invocation via manually constructed HTTP requests, enabling client-side code development in any programming language.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Benchmark Support:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;All production lines now support fine-grained benchmarking, enabling measurement of end-to-end inference time as well as per-layer and per-module latency data to assist with performance analysis. &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/version3.x/pipeline_usage/instructions/benchmark.en.md"&gt;Here's&lt;/a&gt; how to set up and use the benchmark feature.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Documentation has been updated to include key metrics for commonly used configurations on mainstream hardware, such as inference latency and memory usage, providing deployment references for users.&lt;/strong&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Resolved the issue of failed log saving during model training.&lt;/li&gt; 
    &lt;li&gt;Upgraded the data augmentation component for formula models for compatibility with newer versions of the albumentations dependency, and fixed deadlock warnings when using the tokenizers package in multi-process scenarios.&lt;/li&gt; 
    &lt;li&gt;Fixed inconsistencies in switch behaviors (e.g., &lt;code&gt;use_chart_parsing&lt;/code&gt;) in the PP-StructureV3 configuration files compared to other pipelines.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Other Enhancements:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Separated core and optional dependencies. Only minimal core dependencies are required for basic text recognition; additional dependencies for document parsing and information extraction can be installed as needed.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Enabled support for NVIDIA RTX 50 series graphics cards on Windows; users can refer to the &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/version3.x/installation.en.md"&gt;installation guide&lt;/a&gt; for the corresponding PaddlePaddle framework versions.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;PP-OCR series models now support returning single-character coordinates.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Added AIStudio, ModelScope, and other model download sources, allowing users to specify the source for model downloads.&lt;/li&gt; 
    &lt;li&gt;Added support for chart-to-table conversion via the PP-Chart2Table module.&lt;/li&gt; 
    &lt;li&gt;Optimized documentation descriptions to improve usability.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.08.15: PaddleOCR 3.1.1 Released&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Added the missing methods &lt;code&gt;save_vector&lt;/code&gt;, &lt;code&gt;save_visual_info_list&lt;/code&gt;, &lt;code&gt;load_vector&lt;/code&gt;, and &lt;code&gt;load_visual_info_list&lt;/code&gt; in the &lt;code&gt;PP-ChatOCRv4&lt;/code&gt; class.&lt;/li&gt; 
    &lt;li&gt;Added the missing parameters &lt;code&gt;glossary&lt;/code&gt; and &lt;code&gt;llm_request_interval&lt;/code&gt; to the &lt;code&gt;translate&lt;/code&gt; method in the &lt;code&gt;PPDocTranslation&lt;/code&gt; class.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Improvements:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Added a demo to the MCP documentation.&lt;/li&gt; 
    &lt;li&gt;Added information about the PaddlePaddle and PaddleOCR version used for performance metrics testing in the documentation.&lt;/li&gt; 
    &lt;li&gt;Fixed errors and omissions in the production line document translation.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Others:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Changed the MCP server dependency to use the pure Python library &lt;code&gt;puremagic&lt;/code&gt; instead of &lt;code&gt;python-magic&lt;/code&gt; to reduce installation issues.&lt;/li&gt; 
    &lt;li&gt;Retested PP-OCRv5 performance metrics with PaddleOCR version 3.1.0 and updated the documentation.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.29: PaddleOCR 3.1.0 Released&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Key Models and Pipelines:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Added PP-OCRv5 Multilingual Text Recognition Model&lt;/strong&gt;, which supports the training and inference process for text recognition models in 37 languages, including French, Spanish, Portuguese, Russian, Korean, etc. &lt;strong&gt;Average accuracy improved by over 30%.&lt;/strong&gt; &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/algorithm/PP-OCRv5/PP-OCRv5_multi_languages.html"&gt;Details&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Upgraded the &lt;strong&gt;PP-Chart2Table model&lt;/strong&gt; in PP-StructureV3, further enhancing the capability of converting charts to tables. On internal custom evaluation sets, the metric (RMS-F1) &lt;strong&gt;increased by 9.36 percentage points (71.24% -&amp;gt; 80.60%).&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Newly launched &lt;strong&gt;document translation pipeline, PP-DocTranslation, based on PP-StructureV3 and ERNIE 4.5&lt;/strong&gt;, which supports the translation of Markdown format documents, various complex-layout PDF documents, and document images, with the results saved as Markdown format documents. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/pipeline_usage/PP-DocTranslation.html"&gt;Details&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;New MCP server:&lt;/strong&gt; &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html"&gt;Details&lt;/a&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Supports both OCR and PP-StructureV3 pipelines.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Supports three working modes: local Python library, AIStudio Community Cloud Service, and self-hosted service.&lt;/li&gt; 
    &lt;li&gt;Supports invoking local services via stdio and remote services via Streamable HTTP.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Optimization:&lt;/strong&gt; Improved the descriptions in some user guides for a smoother reading experience.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.26: PaddleOCR 3.0.3 Released&lt;/strong&gt;&lt;/summary&gt; - Bug Fix: Resolved the issue where the `enable_mkldnn` parameter was not effective, restoring the default behavior of using MKL-DNN for CPU inference. 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.19: PaddleOCR 3.0.2 Released&lt;/strong&gt;&lt;/summary&gt; - **New Features:** 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;The default download source has been changed from &lt;code&gt;BOS&lt;/code&gt; to &lt;code&gt;HuggingFace&lt;/code&gt;. Users can also change the environment variable &lt;code&gt;PADDLE_PDX_MODEL_SOURCE&lt;/code&gt; to &lt;code&gt;BOS&lt;/code&gt; to set the model download source back to Baidu Object Storage (BOS).&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Added service invocation examples for six languages—C++, Java, Go, C#, Node.js, and PHP—for pipelines like PP-OCRv5, PP-StructureV3, and PP-ChatOCRv4.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Improved the layout partition sorting algorithm in the PP-StructureV3 pipeline, enhancing the sorting logic for complex vertical layouts to deliver better results.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Enhanced model selection logic: when a language is specified but a model version is not, the system will automatically select the latest model version supporting that language.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Set a default upper limit for MKL-DNN cache size to prevent unlimited growth, while also allowing users to configure cache capacity.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Updated default configurations for high-performance inference to support Paddle MKL-DNN acceleration and optimized the logic for automatic configuration selection for smarter choices.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Adjusted the logic for obtaining the default device to consider the actual support for computing devices by the installed Paddle framework, making program behavior more intuitive.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Added Android example for PP-OCRv5. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/on_device_deployment.html"&gt;Details&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Fixed an issue with some CLI parameters in PP-StructureV3 not taking effect.&lt;/li&gt; 
    &lt;li&gt;Resolved an issue where &lt;code&gt;export_paddlex_config_to_yaml&lt;/code&gt; would not function correctly in certain cases.&lt;/li&gt; 
    &lt;li&gt;Corrected the discrepancy between the actual behavior of &lt;code&gt;save_path&lt;/code&gt; and its documentation description.&lt;/li&gt; 
    &lt;li&gt;Fixed potential multithreading errors when using MKL-DNN in basic service deployment.&lt;/li&gt; 
    &lt;li&gt;Corrected channel order errors in image preprocessing for the Latex-OCR model.&lt;/li&gt; 
    &lt;li&gt;Fixed channel order errors in saving visualized images within the text recognition module.&lt;/li&gt; 
    &lt;li&gt;Resolved channel order errors in visualized table results within PP-StructureV3 pipeline.&lt;/li&gt; 
    &lt;li&gt;Fixed an overflow issue in the calculation of &lt;code&gt;overlap_ratio&lt;/code&gt; under extremely special circumstances in the PP-StructureV3 pipeline.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Improvements:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Updated the description of the &lt;code&gt;enable_mkldnn&lt;/code&gt; parameter in the documentation to accurately reflect the program's actual behavior.&lt;/li&gt; 
    &lt;li&gt;Fixed errors in the documentation regarding the &lt;code&gt;lang&lt;/code&gt; and &lt;code&gt;ocr_version&lt;/code&gt; parameters.&lt;/li&gt; 
    &lt;li&gt;Added instructions for exporting pipeline configuration files via CLI.&lt;/li&gt; 
    &lt;li&gt;Fixed missing columns in the performance data table for PP-OCRv5.&lt;/li&gt; 
    &lt;li&gt;Refined benchmark metrics for PP-StructureV3 across different configurations.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Others:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Relaxed version restrictions on dependencies like numpy and pandas, restoring support for Python 3.12.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;History Log&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;2025.06.05: &lt;strong&gt;PaddleOCR 3.0.1 Released&lt;/strong&gt;, includes:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Optimisation of certain models and model configurations:&lt;/strong&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Updated the default model configuration for PP-OCRv5, changing both detection and recognition from mobile to server models. To improve default performance in most scenarios, the parameter &lt;code&gt;limit_side_len&lt;/code&gt; in the configuration has been changed from 736 to 64.&lt;/li&gt; 
    &lt;li&gt;Added a new text line orientation classification model &lt;code&gt;PP-LCNet_x1_0_textline_ori&lt;/code&gt; with an accuracy of 99.42%. The default text line orientation classifier for OCR, PP-StructureV3, and PP-ChatOCRv4 pipelines has been updated to this model.&lt;/li&gt; 
    &lt;li&gt;Optimized the text line orientation classification model &lt;code&gt;PP-LCNet_x0_25_textline_ori&lt;/code&gt;, improving accuracy by 3.3 percentage points to a current accuracy of 98.85%.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Optimizations and fixes for some issues in version 3.0.0, &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/update.html"&gt;details&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;🔥🔥2025.05.20: Official Release of &lt;strong&gt;PaddleOCR v3.0&lt;/strong&gt;, including:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-OCRv5&lt;/strong&gt;: High-Accuracy Text Recognition Model for All Scenarios - Instant Text from Images/PDFs.&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;🌐 Single-model support for &lt;strong&gt;five&lt;/strong&gt; text types - Seamlessly process &lt;strong&gt;Simplified Chinese, Traditional Chinese, Simplified Chinese Pinyin, English&lt;/strong&gt; and &lt;strong&gt;Japanese&lt;/strong&gt; within a single model.&lt;/li&gt; 
    &lt;li&gt;✍️ Improved &lt;strong&gt;handwriting recognition&lt;/strong&gt;: Significantly better at complex cursive scripts and non-standard handwriting.&lt;/li&gt; 
    &lt;li&gt;🎯 &lt;strong&gt;13-point accuracy gain&lt;/strong&gt; over PP-OCRv4, achieving state-of-the-art performance across a variety of real-world scenarios.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-StructureV3&lt;/strong&gt;: General-Purpose Document Parsing – Unleash SOTA Images/PDFs Parsing for Real-World Scenarios!&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;🧮 &lt;strong&gt;High-Accuracy multi-scene PDF parsing&lt;/strong&gt;, leading both open- and closed-source solutions on the OmniDocBench benchmark.&lt;/li&gt; 
    &lt;li&gt;🧠 Specialized capabilities include &lt;strong&gt;seal recognition&lt;/strong&gt;, &lt;strong&gt;chart-to-table conversion&lt;/strong&gt;, &lt;strong&gt;table recognition with nested formulas/images&lt;/strong&gt;, &lt;strong&gt;vertical text document parsing&lt;/strong&gt;, and &lt;strong&gt;complex table structure analysis&lt;/strong&gt;.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-ChatOCRv4&lt;/strong&gt;: Intelligent Document Understanding – Extract Key Information, not just text from Images/PDFs.&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;🔥 &lt;strong&gt;15-point accuracy gain&lt;/strong&gt; in key-information extraction on PDF/PNG/JPG files over the previous generation.&lt;/li&gt; 
    &lt;li&gt;💻 Native support for &lt;strong&gt;ERNIE 4.5&lt;/strong&gt;, with compatibility for large-model deployments via PaddleNLP, Ollama, vLLM, and more.&lt;/li&gt; 
    &lt;li&gt;🤝 Integrated &lt;a href="https://github.com/PaddlePaddle/PaddleMIX/tree/develop/paddlemix/examples/ppdocbee2"&gt;PP-DocBee2&lt;/a&gt;, enabling extraction and understanding of printed text, handwriting, seals, tables, charts, and other common elements in complex documents.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/update.html"&gt;History Log&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;⚡ Quick Start&lt;/h2&gt; 
&lt;h3&gt;1. Run online demo&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aistudio.baidu.com/community/app/91660/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_OCRv5-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518494/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_StructureV3-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518493/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_ChatOCRv4-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;2. Installation&lt;/h3&gt; 
&lt;p&gt;Install PaddlePaddle refer to &lt;a href="https://www.paddlepaddle.org.cn/en/install/quick?docurl=/documentation/docs/en/develop/install/pip/linux-pip_en.html"&gt;Installation Guide&lt;/a&gt;, after then, install the PaddleOCR toolkit.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# If you only want to use the basic text recognition feature (returns text position coordinates and content), including the PP-OCR series
python -m pip install paddleocr
# If you want to use all features such as document parsing, document understanding, document translation, key information extraction, etc.
# python -m pip install "paddleocr[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Starting from version 3.2.0, in addition to the &lt;code&gt;all&lt;/code&gt; dependency group demonstrated above, PaddleOCR also supports installing partial optional features by specifying other dependency groups. All dependency groups provided by PaddleOCR are as follows:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Dependency Group Name&lt;/th&gt; 
   &lt;th&gt;Corresponding Functionality&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;doc-parser&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Document parsing: can be used to extract layout elements such as tables, formulas, stamps, images, etc. from documents; includes models like PP-StructureV3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ie&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Information extraction: can be used to extract key information from documents, such as names, dates, addresses, amounts, etc.; includes models like PP-ChatOCRv4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;trans&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Document translation: can be used to translate documents from one language to another; includes models like PP-DocTranslation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;all&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Complete functionality&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;3. Run inference by CLI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run PP-OCRv5 inference
paddleocr ocr -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png --use_doc_orientation_classify False --use_doc_unwarping False --use_textline_orientation False  

# Run PP-StructureV3 inference
paddleocr pp_structurev3 -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png --use_doc_orientation_classify False --use_doc_unwarping False

# Get the Qianfan API Key at first, and then run PP-ChatOCRv4 inference
paddleocr pp_chatocrv4_doc -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png -k 驾驶室准乘人数 --qianfan_api_key your_api_key --use_doc_orientation_classify False --use_doc_unwarping False 

# Run PaddleOCR-VL inference
paddleocr doc_parser -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/paddleocr_vl_demo.png

# Get more information about "paddleocr ocr"
paddleocr ocr --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4. Run inference by API&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;4.1 PP-OCRv5 Example&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Initialize PaddleOCR instance
from paddleocr import PaddleOCR
ocr = PaddleOCR(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False,
    use_textline_orientation=False)

# Run OCR inference on a sample image 
result = ocr.predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png")

# Visualize the results and save the JSON results
for res in result:
    res.print()
    res.save_to_img("output")
    res.save_to_json("output")
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;4.2 PP-StructureV3 Example&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from pathlib import Path
from paddleocr import PPStructureV3

pipeline = PPStructureV3(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False
)

# For Image
output = pipeline.predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png",
)

# Visualize the results and save the JSON results
for res in output:
    res.print() 
    res.save_to_json(save_path="output") 
    res.save_to_markdown(save_path="output")           
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;4.3 PP-ChatOCRv4 Example&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from paddleocr import PPChatOCRv4Doc

chat_bot_config = {
    "module_name": "chat_bot",
    "model_name": "ernie-3.5-8k",
    "base_url": "https://qianfan.baidubce.com/v2",
    "api_type": "openai",
    "api_key": "api_key",  # your api_key
}

retriever_config = {
    "module_name": "retriever",
    "model_name": "embedding-v1",
    "base_url": "https://qianfan.baidubce.com/v2",
    "api_type": "qianfan",
    "api_key": "api_key",  # your api_key
}

pipeline = PPChatOCRv4Doc(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False
)

visual_predict_res = pipeline.visual_predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png",
    use_common_ocr=True,
    use_seal_recognition=True,
    use_table_recognition=True,
)

mllm_predict_info = None
use_mllm = False
# If a multimodal large model is used, the local mllm service needs to be started. You can refer to the documentation: https://github.com/PaddlePaddle/PaddleX/blob/release/3.0/docs/pipeline_usage/tutorials/vlm_pipelines/doc_understanding.en.md performs deployment and updates the mllm_chat_bot_config configuration.
if use_mllm:
    mllm_chat_bot_config = {
        "module_name": "chat_bot",
        "model_name": "PP-DocBee",
        "base_url": "http://127.0.0.1:8080/",  # your local mllm service url
        "api_type": "openai",
        "api_key": "api_key",  # your api_key
    }

    mllm_predict_res = pipeline.mllm_pred(
        input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png",
        key_list=["驾驶室准乘人数"],
        mllm_chat_bot_config=mllm_chat_bot_config,
    )
    mllm_predict_info = mllm_predict_res["mllm_res"]

visual_info_list = []
for res in visual_predict_res:
    visual_info_list.append(res["visual_info"])
    layout_parsing_result = res["layout_parsing_result"]

vector_info = pipeline.build_vector(
    visual_info_list, flag_save_bytes_vector=True, retriever_config=retriever_config
)
chat_result = pipeline.chat(
    key_list=["驾驶室准乘人数"],
    visual_info=visual_info_list,
    vector_info=vector_info,
    mllm_predict_info=mllm_predict_info,
    chat_bot_config=chat_bot_config,
    retriever_config=retriever_config,
)
print(chat_result)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;4.4 PaddleOCR-VL Example&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from paddleocr import PaddleOCRVL

pipeline = PaddleOCRVL()
output = pipeline.predict("https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/paddleocr_vl_demo.png")
for res in output:
    res.print()
    res.save_to_json(save_path="output")
    res.save_to_markdown(save_path="output")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;5. Chinese Heterogeneous AI Accelerators&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/other_devices_support/paddlepaddle_install_NPU.html"&gt;Huawei Ascend&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/other_devices_support/paddlepaddle_install_XPU.html"&gt;KUNLUNXIN&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🧩 More Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Convert models to ONNX format: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/obtaining_onnx_models.html"&gt;Obtaining ONNX Models&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Accelerate inference using engines like OpenVINO, ONNX Runtime, TensorRT, or perform inference using ONNX format models: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/high_performance_inference.html"&gt;High-Performance Inference&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Accelerate inference using multi-GPU and multi-process: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/pipeline_usage/instructions/parallel_inference.html"&gt;Parallel Inference for Pipelines&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Integrate PaddleOCR into applications written in C++, C#, Java, etc.: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/serving.html"&gt;Serving&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;⛰️ Advanced Tutorials&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/OCR.html"&gt;PP-OCRv5 Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PP-StructureV3.html"&gt;PP-StructureV3 Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PP-ChatOCRv4.html"&gt;PP-ChatOCRv4 Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PaddleOCR-VL.html"&gt;PaddleOCR-VL Tutorial&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🔄 Quick Overview of Execution Results&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/demo.gif" alt="PP-OCRv5 Demo" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/blue_v3.gif" alt="PP-StructureV3 Demo" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;✨ Stay Tuned&lt;/h2&gt; 
&lt;p&gt;⭐ &lt;strong&gt;Star this repository to keep up with exciting updates and new releases, including powerful OCR and document parsing capabilities!&lt;/strong&gt; ⭐&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="1200" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/star_paddleocr.en.gif" alt="Star-Project" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;👩‍👩‍👧‍👦 Community&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;PaddlePaddle WeChat official account&lt;/th&gt; 
    &lt;th align="center"&gt;Join the tech discussion group&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/refs/heads/main/images/paddleocr/README/qrcode_for_paddlepaddle_official_account.jpg" width="150" /&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/refs/heads/main/images/paddleocr/README/qr_code_for_the_questionnaire.jpg" width="150" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;😃 Awesome Projects Leveraging PaddleOCR&lt;/h2&gt; 
&lt;p&gt;PaddleOCR wouldn't be where it is today without its incredible community! 💗 A massive thank you to all our longtime partners, new collaborators, and everyone who's poured their passion into PaddleOCR — whether we've named you or not. Your support fuels our fire!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Project Name&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/infiniflow/ragflow"&gt;RAGFlow&lt;/a&gt; &lt;a href="https://github.com/infiniflow/ragflow"&gt;&lt;img src="https://img.shields.io/github/stars/infiniflow/ragflow" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;RAG engine based on deep document understanding.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/opendatalab/MinerU"&gt;MinerU&lt;/a&gt; &lt;a href="https://github.com/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/github/stars/opendatalab/MinerU" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Multi-type Document to Markdown Conversion Tool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hiroi-sora/Umi-OCR"&gt;Umi-OCR&lt;/a&gt; &lt;a href="https://github.com/hiroi-sora/Umi-OCR"&gt;&lt;img src="https://img.shields.io/github/stars/hiroi-sora/Umi-OCR" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Free, Open-source, Batch Offline OCR Software.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/OmniParser"&gt;OmniParser&lt;/a&gt;&lt;a href="https://github.com/microsoft/OmniParser"&gt;&lt;img src="https://img.shields.io/github/stars/microsoft/OmniParser" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;OmniParser: Screen Parsing tool for Pure Vision Based GUI Agent.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/netease-youdao/QAnything"&gt;QAnything&lt;/a&gt;&lt;a href="https://github.com/netease-youdao/QAnything"&gt;&lt;img src="https://img.shields.io/github/stars/netease-youdao/QAnything" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Question and Answer based on Anything.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;PDF-Extract-Kit&lt;/a&gt; &lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;&lt;img src="https://img.shields.io/github/stars/opendatalab/PDF-Extract-Kit" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;A powerful open-source toolkit designed to efficiently extract high-quality content from complex and diverse PDF documents.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/PantsuDango/Dango-Translator"&gt;Dango-Translator&lt;/a&gt;&lt;a href="https://github.com/PantsuDango/Dango-Translator"&gt;&lt;img src="https://img.shields.io/github/stars/PantsuDango/Dango-Translator" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Recognize text on the screen, translate it and show the translation results in real time.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/awesome_projects.md"&gt;Learn more projects&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/awesome_projects.md"&gt;More projects based on PaddleOCR&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;👩‍👩‍👧‍👦 Contributors&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/PaddlePaddle/PaddleOCR/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=PaddlePaddle/PaddleOCR&amp;amp;max=400&amp;amp;columns=20" width="800" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;🌟 Star&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="800" src="https://api.star-history.com/svg?repos=PaddlePaddle/PaddleOCR&amp;amp;type=Date" alt="Star-history" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;📄 License&lt;/h2&gt; 
&lt;p&gt;This project is released under the &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/LICENSE"&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;🎓 Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@misc{cui2025paddleocr30technicalreport,
      title={PaddleOCR 3.0 Technical Report}, 
      author={Cheng Cui and Ting Sun and Manhui Lin and Tingquan Gao and Yubo Zhang and Jiaxuan Liu and Xueqing Wang and Zelun Zhang and Changda Zhou and Hongen Liu and Yue Zhang and Wenyu Lv and Kui Huang and Yichao Zhang and Jing Zhang and Jun Zhang and Yi Liu and Dianhai Yu and Yanjun Ma},
      year={2025},
      eprint={2507.05595},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2507.05595}, 
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>lfnovo/open-notebook</title>
      <link>https://github.com/lfnovo/open-notebook</link>
      <description>&lt;p&gt;An Open Source implementation of Notebook LM with more flexibility and features&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a id="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- [![Contributors][contributors-shield]][contributors-url] --&gt; 
&lt;p&gt;&lt;a href="https://github.com/lfnovo/open-notebook/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/lfnovo/open-notebook.svg?style=for-the-badge" alt="Forks" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lfnovo/open-notebook/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/lfnovo/open-notebook.svg?style=for-the-badge" alt="Stargazers" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;&lt;img src="https://img.shields.io/github/issues/lfnovo/open-notebook.svg?style=for-the-badge" alt="Issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lfnovo/open-notebook/raw/master/LICENSE.txt"&gt;&lt;img src="https://img.shields.io/github/license/lfnovo/open-notebook.svg?style=for-the-badge" alt="MIT License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- [![LinkedIn][linkedin-shield]][linkedin-url] --&gt; 
&lt;!-- PROJECT LOGO --&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/lfnovo/open-notebook"&gt; &lt;img src="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/assets/hero.svg?sanitize=true" alt="Logo" /&gt; &lt;/a&gt; 
 &lt;h3 align="center"&gt;Open Notebook&lt;/h3&gt; 
 &lt;p align="center"&gt; An open source, privacy-focused alternative to Google's Notebook LM! &lt;br /&gt;&lt;strong&gt;Join our &lt;a href="https://discord.gg/37XJPXfz2w"&gt;Discord server&lt;/a&gt; for help, to share workflow ideas, and suggest features!&lt;/strong&gt; &lt;br /&gt; &lt;a href="https://www.open-notebook.ai"&gt;&lt;strong&gt;Checkout our website »&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt; &lt;br /&gt; &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/index.md"&gt;📚 Get Started&lt;/a&gt; · &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/index.md"&gt;📖 User Guide&lt;/a&gt; · &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/index.md"&gt;✨ Features&lt;/a&gt; · &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/index.md"&gt;🚀 Deploy&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; 
 &lt;a href="https://zdoc.app/de/lfnovo/open-notebook"&gt;Deutsch&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/es/lfnovo/open-notebook"&gt;Español&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/fr/lfnovo/open-notebook"&gt;français&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/ja/lfnovo/open-notebook"&gt;日本語&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/ko/lfnovo/open-notebook"&gt;한국어&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/pt/lfnovo/open-notebook"&gt;Português&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/ru/lfnovo/open-notebook"&gt;Русский&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/zh/lfnovo/open-notebook"&gt;中文&lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;A private, multi-model, 100% local, full-featured alternative to Notebook LM&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/assets/asset_list.png" alt="New Notebook" /&gt;&lt;/p&gt; 
&lt;p&gt;In a world dominated by Artificial Intelligence, having the ability to think 🧠 and acquire new knowledge 💡, is a skill that should not be a privilege for a few, nor restricted to a single provider.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Open Notebook empowers you to:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🔒 &lt;strong&gt;Control your data&lt;/strong&gt; - Keep your research private and secure&lt;/li&gt; 
 &lt;li&gt;🤖 &lt;strong&gt;Choose your AI models&lt;/strong&gt; - Support for 16+ providers including OpenAI, Anthropic, Ollama, LM Studio, and more&lt;/li&gt; 
 &lt;li&gt;📚 &lt;strong&gt;Organize multi-modal content&lt;/strong&gt; - PDFs, videos, audio, web pages, and more&lt;/li&gt; 
 &lt;li&gt;🎙️ &lt;strong&gt;Generate professional podcasts&lt;/strong&gt; - Advanced multi-speaker podcast generation&lt;/li&gt; 
 &lt;li&gt;🔍 &lt;strong&gt;Search intelligently&lt;/strong&gt; - Full-text and vector search across all your content&lt;/li&gt; 
 &lt;li&gt;💬 &lt;strong&gt;Chat with context&lt;/strong&gt; - AI conversations powered by your research&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Learn more about our project at &lt;a href="https://www.open-notebook.ai"&gt;https://www.open-notebook.ai&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;⚠️ IMPORTANT: v1.0 Breaking Changes&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;If you're upgrading from a previous version&lt;/strong&gt;, please note:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🏷️ &lt;strong&gt;Docker tags have changed&lt;/strong&gt;: The &lt;code&gt;latest&lt;/code&gt; tag is now &lt;strong&gt;frozen&lt;/strong&gt; at the last Streamlit version&lt;/li&gt; 
 &lt;li&gt;🆕 &lt;strong&gt;Use &lt;code&gt;v1-latest&lt;/code&gt; tag&lt;/strong&gt; for the new React/Next.js version (recommended)&lt;/li&gt; 
 &lt;li&gt;🔌 &lt;strong&gt;Port 5055 required&lt;/strong&gt;: You must expose port 5055 for the API to work&lt;/li&gt; 
 &lt;li&gt;📖 &lt;strong&gt;Read the migration guide&lt;/strong&gt;: See &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/MIGRATION.md"&gt;MIGRATION.md&lt;/a&gt; for detailed upgrade instructions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;New users&lt;/strong&gt;: You can ignore this notice and proceed with the Quick Start below using the &lt;code&gt;v1-latest-single&lt;/code&gt; tag.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🆚 Open Notebook vs Google Notebook LM&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;Open Notebook&lt;/th&gt; 
   &lt;th&gt;Google Notebook LM&lt;/th&gt; 
   &lt;th&gt;Advantage&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Privacy &amp;amp; Control&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Self-hosted, your data&lt;/td&gt; 
   &lt;td&gt;Google cloud only&lt;/td&gt; 
   &lt;td&gt;Complete data sovereignty&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;AI Provider Choice&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;16+ providers (OpenAI, Anthropic, Ollama, LM Studio, etc.)&lt;/td&gt; 
   &lt;td&gt;Google models only&lt;/td&gt; 
   &lt;td&gt;Flexibility and cost optimization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Podcast Speakers&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;1-4 speakers with custom profiles&lt;/td&gt; 
   &lt;td&gt;2 speakers only&lt;/td&gt; 
   &lt;td&gt;Extreme flexibility&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Context Control&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;3 granular levels&lt;/td&gt; 
   &lt;td&gt;All-or-nothing&lt;/td&gt; 
   &lt;td&gt;Privacy and performance tuning&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Content Transformations&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Custom and built-in&lt;/td&gt; 
   &lt;td&gt;Limited options&lt;/td&gt; 
   &lt;td&gt;Unlimited processing power&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;API Access&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Full REST API&lt;/td&gt; 
   &lt;td&gt;No API&lt;/td&gt; 
   &lt;td&gt;Complete automation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Deployment&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Docker, cloud, or local&lt;/td&gt; 
   &lt;td&gt;Google hosted only&lt;/td&gt; 
   &lt;td&gt;Deploy anywhere&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Citations&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Comprehensive with sources&lt;/td&gt; 
   &lt;td&gt;Basic references&lt;/td&gt; 
   &lt;td&gt;Research integrity&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Customization&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Open source, fully customizable&lt;/td&gt; 
   &lt;td&gt;Closed system&lt;/td&gt; 
   &lt;td&gt;Unlimited extensibility&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Cost&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Pay only for AI usage&lt;/td&gt; 
   &lt;td&gt;Monthly subscription + usage&lt;/td&gt; 
   &lt;td&gt;Transparent and controllable&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Why Choose Open Notebook?&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🔒 &lt;strong&gt;Privacy First&lt;/strong&gt;: Your sensitive research stays completely private&lt;/li&gt; 
 &lt;li&gt;💰 &lt;strong&gt;Cost Control&lt;/strong&gt;: Choose cheaper AI providers or run locally with Ollama&lt;/li&gt; 
 &lt;li&gt;🎙️ &lt;strong&gt;Better Podcasts&lt;/strong&gt;: Full script control and multi-speaker flexibility vs limited 2-speaker deep-dive format&lt;/li&gt; 
 &lt;li&gt;🔧 &lt;strong&gt;Unlimited Customization&lt;/strong&gt;: Modify, extend, and integrate as needed&lt;/li&gt; 
 &lt;li&gt;🌐 &lt;strong&gt;No Vendor Lock-in&lt;/strong&gt;: Switch providers, deploy anywhere, own your data&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Built With&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.python.org/"&gt;&lt;img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white" alt="Python" /&gt;&lt;/a&gt; &lt;a href="https://nextjs.org/"&gt;&lt;img src="https://img.shields.io/badge/Next.js-000000?style=for-the-badge&amp;amp;logo=next.js&amp;amp;logoColor=white" alt="Next.js" /&gt;&lt;/a&gt; &lt;a href="https://reactjs.org/"&gt;&lt;img src="https://img.shields.io/badge/React-61DAFB?style=for-the-badge&amp;amp;logo=react&amp;amp;logoColor=black" alt="React" /&gt;&lt;/a&gt; &lt;a href="https://surrealdb.com/"&gt;&lt;img src="https://img.shields.io/badge/SurrealDB-FF5E00?style=for-the-badge&amp;amp;logo=databricks&amp;amp;logoColor=white" alt="SurrealDB" /&gt;&lt;/a&gt; &lt;a href="https://www.langchain.com/"&gt;&lt;img src="https://img.shields.io/badge/LangChain-3A3A3A?style=for-the-badge&amp;amp;logo=chainlink&amp;amp;logoColor=white" alt="LangChain" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;🚀 Quick Start&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Docker Images Available:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Docker Hub&lt;/strong&gt;: &lt;code&gt;lfnovo/open_notebook:v1-latest-single&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Container Registry&lt;/strong&gt;: &lt;code&gt;ghcr.io/lfnovo/open-notebook:v1-latest-single&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Both registries contain identical images - choose whichever you prefer!&lt;/p&gt; 
&lt;h3&gt;Choose Your Setup:&lt;/h3&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h4&gt;🏠 &lt;strong&gt;Local Machine Setup&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;Perfect if Docker runs on the &lt;strong&gt;same computer&lt;/strong&gt; where you'll access Open Notebook.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;mkdir open-notebook &amp;amp;&amp;amp; cd open-notebook

docker run -d \
  --name open-notebook \
  -p 8502:8502 -p 5055:5055 \
  -v ./notebook_data:/app/data \
  -v ./surreal_data:/mydata \
  -e OPENAI_API_KEY=your_key_here \
  -e SURREAL_URL="ws://localhost:8000/rpc" \
  -e SURREAL_USER="root" \
  -e SURREAL_PASSWORD="root" \
  -e SURREAL_NAMESPACE="open_notebook" \
  -e SURREAL_DATABASE="production" \
  lfnovo/open_notebook:v1-latest-single
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Access at:&lt;/strong&gt; &lt;a href="http://localhost:8502"&gt;http://localhost:8502&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h4&gt;🌐 &lt;strong&gt;Remote Server Setup&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;Use this for servers, Raspberry Pi, NAS, Proxmox, or any remote machine.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;mkdir open-notebook &amp;amp;&amp;amp; cd open-notebook

docker run -d \
  --name open-notebook \
  -p 8502:8502 -p 5055:5055 \
  -v ./notebook_data:/app/data \
  -v ./surreal_data:/mydata \
  -e OPENAI_API_KEY=your_key_here \
  -e API_URL=http://YOUR_SERVER_IP:5055 \
  -e SURREAL_URL="ws://localhost:8000/rpc" \
  -e SURREAL_USER="root" \
  -e SURREAL_PASSWORD="root" \
  -e SURREAL_NAMESPACE="open_notebook" \
  -e SURREAL_DATABASE="production" \
  lfnovo/open_notebook:v1-latest-single
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Replace &lt;code&gt;YOUR_SERVER_IP&lt;/code&gt;&lt;/strong&gt; with your server's IP (e.g., &lt;code&gt;192.168.1.100&lt;/code&gt;) or domain&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Access at:&lt;/strong&gt; http://YOUR_SERVER_IP:8502&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;⚠️ Critical Setup Notes:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Both ports are required:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Port 8502&lt;/strong&gt;: Web interface (what you see in your browser)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Port 5055&lt;/strong&gt;: API backend (required for the app to function)&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;API_URL must match how YOU access the server:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;✅ Access via &lt;code&gt;http://192.168.1.100:8502&lt;/code&gt; → set &lt;code&gt;API_URL=http://192.168.1.100:5055&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;✅ Access via &lt;code&gt;http://myserver.local:8502&lt;/code&gt; → set &lt;code&gt;API_URL=http://myserver.local:5055&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;❌ Don't use &lt;code&gt;localhost&lt;/code&gt; for remote servers - it won't work from other devices!&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Using Docker Compose (Recommended for Easy Management)&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;docker-compose.yml&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;services:
  open_notebook:
    image: lfnovo/open_notebook:v1-latest-single
    # Or use: ghcr.io/lfnovo/open-notebook:v1-latest-single
    ports:
      - "8502:8502"  # Web UI
      - "5055:5055"  # API (required!)
    environment:
      - OPENAI_API_KEY=your_key_here
      # For remote access, uncomment and set your server IP/domain:
      # - API_URL=http://192.168.1.100:5055
      # Database connection (required for single-container)
      - SURREAL_URL=ws://localhost:8000/rpc
      - SURREAL_USER=root
      - SURREAL_PASSWORD=root
      - SURREAL_NAMESPACE=open_notebook
      - SURREAL_DATABASE=production
    volumes:
      - ./notebook_data:/app/data
      - ./surreal_data:/mydata
    restart: always
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Start with: &lt;code&gt;docker compose up -d&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;What gets created:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;open-notebook/
├── docker-compose.yml # Your configuration
├── notebook_data/     # Your notebooks and research content
└── surreal_data/      # Database files
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;🆘 Quick Troubleshooting&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Problem&lt;/th&gt; 
   &lt;th&gt;Solution&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;"Unable to connect to server"&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Set &lt;code&gt;API_URL&lt;/code&gt; environment variable to match how you access the server (see remote setup above)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Blank page or errors&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Ensure BOTH ports (8502 and 5055) are exposed in your docker command&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Works on server but not from other computers&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Don't use &lt;code&gt;localhost&lt;/code&gt; in &lt;code&gt;API_URL&lt;/code&gt; - use your server's actual IP address&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;"404" or "config endpoint" errors&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Don't add &lt;code&gt;/api&lt;/code&gt; to &lt;code&gt;API_URL&lt;/code&gt; - use just &lt;code&gt;http://your-ip:5055&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Still having issues?&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Check our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/troubleshooting/quick-fixes.md"&gt;5-minute troubleshooting guide&lt;/a&gt; or &lt;a href="https://discord.gg/37XJPXfz2w"&gt;join Discord&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;How Open Notebook Works&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;┌─────────────────────────────────────────────────────────┐
│  Your Browser                                           │
│  Access: http://your-server-ip:8502                     │
└────────────────┬────────────────────────────────────────┘
                 │
                 ▼
         ┌───────────────┐
         │   Port 8502   │  ← Next.js Frontend (what you see)
         │   Frontend    │
         └───────┬───────┘
                 │ needs to call ↓
                 ▼
         ┌───────────────┐
         │   Port 5055   │  ← FastAPI Backend (handles requests)
         │     API       │     This is why you need API_URL!
         └───────┬───────┘
                 │
                 ▼
         ┌───────────────┐
         │   SurrealDB   │  ← Database (internal, auto-configured)
         │   (Port 8000) │
         └───────────────┘
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Key Point:&lt;/strong&gt; Your browser loads the frontend from port 8502, but that frontend needs to know where to find the API (port 5055). When accessing remotely, you must tell it explicitly: &lt;code&gt;API_URL=http://your-server-ip:5055&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#lfnovo/open-notebook&amp;amp;type=date&amp;amp;legend=top-left"&gt;&lt;img src="https://api.star-history.com/svg?repos=lfnovo/open-notebook&amp;amp;type=date&amp;amp;legend=top-left" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;🛠️ Full Installation&lt;/h3&gt; 
&lt;p&gt;For development or customization:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/lfnovo/open-notebook
cd open-notebook
make start-all
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;📖 Need Help?&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;🤖 AI Installation Assistant&lt;/strong&gt;: We have a &lt;a href="https://chatgpt.com/g/g-68776e2765b48191bd1bae3f30212631-open-notebook-installation-assistant"&gt;CustomGPT built to help you install Open Notebook&lt;/a&gt; - it will guide you through each step!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;New to Open Notebook?&lt;/strong&gt; Start with our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/index.md"&gt;Getting Started Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Need installation help?&lt;/strong&gt; Check our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/installation.md"&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Want to see it in action?&lt;/strong&gt; Try our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/quick-start.md"&gt;Quick Start Tutorial&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Provider Support Matrix&lt;/h2&gt; 
&lt;p&gt;Thanks to the &lt;a href="https://github.com/lfnovo/esperanto"&gt;Esperanto&lt;/a&gt; library, we support this providers out of the box!&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Provider&lt;/th&gt; 
   &lt;th&gt;LLM Support&lt;/th&gt; 
   &lt;th&gt;Embedding Support&lt;/th&gt; 
   &lt;th&gt;Speech-to-Text&lt;/th&gt; 
   &lt;th&gt;Text-to-Speech&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Anthropic&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Groq&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Google (GenAI)&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vertex AI&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ollama&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Perplexity&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ElevenLabs&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Azure OpenAI&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mistral&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DeepSeek&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Voyage&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;xAI&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenRouter&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI Compatible*&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;*Supports LM Studio and any OpenAI-compatible endpoint&lt;/p&gt; 
&lt;h2&gt;✨ Key Features&lt;/h2&gt; 
&lt;h3&gt;Core Capabilities&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;🔒 Privacy-First&lt;/strong&gt;: Your data stays under your control - no cloud dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🎯 Multi-Notebook Organization&lt;/strong&gt;: Manage multiple research projects seamlessly&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📚 Universal Content Support&lt;/strong&gt;: PDFs, videos, audio, web pages, Office docs, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🤖 Multi-Model AI Support&lt;/strong&gt;: 16+ providers including OpenAI, Anthropic, Ollama, Google, LM Studio, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🎙️ Professional Podcast Generation&lt;/strong&gt;: Advanced multi-speaker podcasts with Episode Profiles&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🔍 Intelligent Search&lt;/strong&gt;: Full-text and vector search across all your content&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;💬 Context-Aware Chat&lt;/strong&gt;: AI conversations powered by your research materials&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📝 AI-Assisted Notes&lt;/strong&gt;: Generate insights or write notes manually&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;⚡ Reasoning Model Support&lt;/strong&gt;: Full support for thinking models like DeepSeek-R1 and Qwen3&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🔧 Content Transformations&lt;/strong&gt;: Powerful customizable actions to summarize and extract insights&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🌐 Comprehensive REST API&lt;/strong&gt;: Full programmatic access for custom integrations &lt;a href="http://localhost:5055/docs"&gt;&lt;img src="https://img.shields.io/badge/API-Documentation-blue?style=flat-square" alt="API Docs" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🔐 Optional Password Protection&lt;/strong&gt;: Secure public deployments with authentication&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📊 Fine-Grained Context Control&lt;/strong&gt;: Choose exactly what to share with AI models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📎 Citations&lt;/strong&gt;: Get answers with proper source citations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Three-Column Interface&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Sources&lt;/strong&gt;: Manage all your research materials&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Notes&lt;/strong&gt;: Create manual or AI-generated notes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chat&lt;/strong&gt;: Converse with AI using your content as context&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=D-760MlGwaI"&gt;&lt;img src="https://img.youtube.com/vi/D-760MlGwaI/0.jpg" alt="Check out our podcast sample" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;📚 Documentation&lt;/h2&gt; 
&lt;h3&gt;Getting Started&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/introduction.md"&gt;📖 Introduction&lt;/a&gt;&lt;/strong&gt; - Learn what Open Notebook offers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/quick-start.md"&gt;⚡ Quick Start&lt;/a&gt;&lt;/strong&gt; - Get up and running in 5 minutes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/installation.md"&gt;🔧 Installation&lt;/a&gt;&lt;/strong&gt; - Comprehensive setup guide&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/first-notebook.md"&gt;🎯 Your First Notebook&lt;/a&gt;&lt;/strong&gt; - Step-by-step tutorial&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;User Guide&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/interface-overview.md"&gt;📱 Interface Overview&lt;/a&gt;&lt;/strong&gt; - Understanding the layout&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/notebooks.md"&gt;📚 Notebooks&lt;/a&gt;&lt;/strong&gt; - Organizing your research&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/sources.md"&gt;📄 Sources&lt;/a&gt;&lt;/strong&gt; - Managing content types&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/notes.md"&gt;📝 Notes&lt;/a&gt;&lt;/strong&gt; - Creating and managing notes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/chat.md"&gt;💬 Chat&lt;/a&gt;&lt;/strong&gt; - AI conversations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/search.md"&gt;🔍 Search&lt;/a&gt;&lt;/strong&gt; - Finding information&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Topics&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/podcasts.md"&gt;🎙️ Podcast Generation&lt;/a&gt;&lt;/strong&gt; - Create professional podcasts&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/transformations.md"&gt;🔧 Content Transformations&lt;/a&gt;&lt;/strong&gt; - Customize content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/ai-models.md"&gt;🤖 AI Models&lt;/a&gt;&lt;/strong&gt; - AI model configuration&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/development/api-reference.md"&gt;🔧 REST API Reference&lt;/a&gt;&lt;/strong&gt; - Complete API documentation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/security.md"&gt;🔐 Security&lt;/a&gt;&lt;/strong&gt; - Password protection and privacy&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/index.md"&gt;🚀 Deployment&lt;/a&gt;&lt;/strong&gt; - Complete deployment guides for all scenarios&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;🗺️ Roadmap&lt;/h2&gt; 
&lt;h3&gt;Upcoming Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Live Front-End Updates&lt;/strong&gt;: Real-time UI updates for smoother experience&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Async Processing&lt;/strong&gt;: Faster UI through asynchronous content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cross-Notebook Sources&lt;/strong&gt;: Reuse research materials across projects&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bookmark Integration&lt;/strong&gt;: Connect with your favorite bookmarking apps&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Recently Completed ✅&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Next.js Frontend&lt;/strong&gt;: Modern React-based frontend with improved performance&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Comprehensive REST API&lt;/strong&gt;: Full programmatic access to all functionality&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Model Support&lt;/strong&gt;: 16+ AI providers including OpenAI, Anthropic, Ollama, LM Studio&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced Podcast Generator&lt;/strong&gt;: Professional multi-speaker podcasts with Episode Profiles&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Content Transformations&lt;/strong&gt;: Powerful customizable actions for content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced Citations&lt;/strong&gt;: Improved layout and finer control for source citations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multiple Chat Sessions&lt;/strong&gt;: Manage different conversations within notebooks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;open issues&lt;/a&gt; for a full list of proposed features and known issues.&lt;/p&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;🤝 Community &amp;amp; Contributing&lt;/h2&gt; 
&lt;h3&gt;Join the Community&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;💬 &lt;strong&gt;&lt;a href="https://discord.gg/37XJPXfz2w"&gt;Discord Server&lt;/a&gt;&lt;/strong&gt; - Get help, share ideas, and connect with other users&lt;/li&gt; 
 &lt;li&gt;🐛 &lt;strong&gt;&lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/strong&gt; - Report bugs and request features&lt;/li&gt; 
 &lt;li&gt;⭐ &lt;strong&gt;Star this repo&lt;/strong&gt; - Show your support and help others discover Open Notebook&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing&lt;/h3&gt; 
&lt;p&gt;We welcome contributions! We're especially looking for help with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Frontend Development&lt;/strong&gt;: Help improve our modern Next.js/React UI&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Testing &amp;amp; Bug Fixes&lt;/strong&gt;: Make Open Notebook more robust&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Feature Development&lt;/strong&gt;: Build the coolest research tool together&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: Improve guides and tutorials&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Current Tech Stack&lt;/strong&gt;: Python, FastAPI, Next.js, React, SurrealDB &lt;strong&gt;Future Roadmap&lt;/strong&gt;: Real-time updates, enhanced async processing&lt;/p&gt; 
&lt;p&gt;See our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; for detailed information on how to get started.&lt;/p&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;📄 License&lt;/h2&gt; 
&lt;p&gt;Open Notebook is MIT licensed. See the &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;📞 Contact&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Luis Novo&lt;/strong&gt; - &lt;a href="https://twitter.com/lfnovo"&gt;@lfnovo&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Community Support&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;💬 &lt;a href="https://discord.gg/37XJPXfz2w"&gt;Discord Server&lt;/a&gt; - Get help, share ideas, and connect with users&lt;/li&gt; 
 &lt;li&gt;🐛 &lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;GitHub Issues&lt;/a&gt; - Report bugs and request features&lt;/li&gt; 
 &lt;li&gt;🌐 &lt;a href="https://www.open-notebook.ai"&gt;Website&lt;/a&gt; - Learn more about the project&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🙏 Acknowledgments&lt;/h2&gt; 
&lt;p&gt;Open Notebook is built on the shoulders of amazing open-source projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/podcast-creator"&gt;Podcast Creator&lt;/a&gt;&lt;/strong&gt; - Advanced podcast generation capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/surreal-commands"&gt;Surreal Commands&lt;/a&gt;&lt;/strong&gt; - Background job processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/content-core"&gt;Content Core&lt;/a&gt;&lt;/strong&gt; - Content processing and management&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/esperanto"&gt;Esperanto&lt;/a&gt;&lt;/strong&gt; - Multi-provider AI model abstraction&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/docling-project/docling"&gt;Docling&lt;/a&gt;&lt;/strong&gt; - Document processing and parsing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; 
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</description>
    </item>
    
    <item>
      <title>tokio-rs/tokio</title>
      <link>https://github.com/tokio-rs/tokio</link>
      <description>&lt;p&gt;A runtime for writing reliable asynchronous applications with Rust. Provides I/O, networking, scheduling, timers, ...&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Tokio&lt;/h1&gt; 
&lt;p&gt;A runtime for writing reliable, asynchronous, and slim applications with the Rust programming language. It is:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Fast&lt;/strong&gt;: Tokio's zero-cost abstractions give you bare-metal performance.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Reliable&lt;/strong&gt;: Tokio leverages Rust's ownership, type system, and concurrency model to reduce bugs and ensure thread safety.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Scalable&lt;/strong&gt;: Tokio has a minimal footprint, and handles backpressure and cancellation naturally.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://crates.io/crates/tokio"&gt;&lt;img src="https://img.shields.io/crates/v/tokio.svg?sanitize=true" alt="Crates.io" /&gt;&lt;/a&gt; &lt;a href="https://github.com/tokio-rs/tokio/raw/master/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" alt="MIT licensed" /&gt;&lt;/a&gt; &lt;a href="https://github.com/tokio-rs/tokio/actions?query=workflow%3ACI+branch%3Amaster"&gt;&lt;img src="https://github.com/tokio-rs/tokio/workflows/CI/badge.svg?sanitize=true" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/tokio"&gt;&lt;img src="https://img.shields.io/discord/500028886025895936.svg?logo=discord&amp;amp;style=flat-square" alt="Discord chat" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://tokio.rs"&gt;Website&lt;/a&gt; | &lt;a href="https://tokio.rs/tokio/tutorial"&gt;Guides&lt;/a&gt; | &lt;a href="https://docs.rs/tokio/latest/tokio"&gt;API Docs&lt;/a&gt; | &lt;a href="https://discord.gg/tokio"&gt;Chat&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;Tokio is an event-driven, non-blocking I/O platform for writing asynchronous applications with the Rust programming language. At a high level, it provides a few major components:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A multithreaded, work-stealing based task &lt;a href="https://docs.rs/tokio/latest/tokio/runtime/index.html"&gt;scheduler&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;A reactor backed by the operating system's event queue (epoll, kqueue, IOCP, etc.).&lt;/li&gt; 
 &lt;li&gt;Asynchronous &lt;a href="https://docs.rs/tokio/latest/tokio/net/index.html"&gt;TCP and UDP&lt;/a&gt; sockets.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;These components provide the runtime components necessary for building an asynchronous application.&lt;/p&gt; 
&lt;h2&gt;Example&lt;/h2&gt; 
&lt;p&gt;A basic TCP echo server with Tokio.&lt;/p&gt; 
&lt;p&gt;Make sure you activated the full features of the tokio crate on Cargo.toml:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;[dependencies]
tokio = { version = "1.48.0", features = ["full"] }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, on your main.rs:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust,no_run"&gt;use tokio::net::TcpListener;
use tokio::io::{AsyncReadExt, AsyncWriteExt};

#[tokio::main]
async fn main() -&amp;gt; Result&amp;lt;(), Box&amp;lt;dyn std::error::Error&amp;gt;&amp;gt; {
    let listener = TcpListener::bind("127.0.0.1:8080").await?;

    loop {
        let (mut socket, _) = listener.accept().await?;

        tokio::spawn(async move {
            let mut buf = [0; 1024];

            // In a loop, read data from the socket and write the data back.
            loop {
                let n = match socket.read(&amp;amp;mut buf).await {
                    // socket closed
                    Ok(0) =&amp;gt; return,
                    Ok(n) =&amp;gt; n,
                    Err(e) =&amp;gt; {
                        eprintln!("failed to read from socket; err = {:?}", e);
                        return;
                    }
                };

                // Write the data back
                if let Err(e) = socket.write_all(&amp;amp;buf[0..n]).await {
                    eprintln!("failed to write to socket; err = {:?}", e);
                    return;
                }
            }
        });
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;More examples can be found &lt;a href="https://github.com/tokio-rs/tokio/tree/master/examples"&gt;here&lt;/a&gt;. For a larger "real world" example, see the &lt;a href="https://github.com/tokio-rs/mini-redis/"&gt;mini-redis&lt;/a&gt; repository.&lt;/p&gt; 
&lt;p&gt;To see a list of the available features flags that can be enabled, check our &lt;a href="https://docs.rs/tokio/#feature-flags"&gt;docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting Help&lt;/h2&gt; 
&lt;p&gt;First, see if the answer to your question can be found in the &lt;a href="https://tokio.rs/tokio/tutorial"&gt;Guides&lt;/a&gt; or the &lt;a href="https://docs.rs/tokio/latest/tokio"&gt;API documentation&lt;/a&gt;. If the answer is not there, there is an active community in the &lt;a href="https://discord.gg/tokio"&gt;Tokio Discord server&lt;/a&gt;. We would be happy to try to answer your question. You can also ask your question on &lt;a href="https://github.com/tokio-rs/tokio/discussions"&gt;the discussions page&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;&lt;span&gt;🎈&lt;/span&gt; Thanks for your help improving the project! We are so happy to have you! We have a &lt;a href="https://github.com/tokio-rs/tokio/raw/master/CONTRIBUTING.md"&gt;contributing guide&lt;/a&gt; to help you get involved in the Tokio project.&lt;/p&gt; 
&lt;h2&gt;Related Projects&lt;/h2&gt; 
&lt;p&gt;In addition to the crates in this repository, the Tokio project also maintains several other libraries, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/tokio-rs/axum"&gt;&lt;code&gt;axum&lt;/code&gt;&lt;/a&gt;: A web application framework that focuses on ergonomics and modularity.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/hyperium/hyper"&gt;&lt;code&gt;hyper&lt;/code&gt;&lt;/a&gt;: A fast and correct HTTP/1.1 and HTTP/2 implementation for Rust.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/hyperium/tonic"&gt;&lt;code&gt;tonic&lt;/code&gt;&lt;/a&gt;: A gRPC over HTTP/2 implementation focused on high performance, interoperability, and flexibility.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/seanmonstar/warp"&gt;&lt;code&gt;warp&lt;/code&gt;&lt;/a&gt;: A super-easy, composable, web server framework for warp speeds.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/tower-rs/tower"&gt;&lt;code&gt;tower&lt;/code&gt;&lt;/a&gt;: A library of modular and reusable components for building robust networking clients and servers.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/tokio-rs/tracing"&gt;&lt;code&gt;tracing&lt;/code&gt;&lt;/a&gt; (formerly &lt;code&gt;tokio-trace&lt;/code&gt;): A framework for application-level tracing and async-aware diagnostics.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/tokio-rs/mio"&gt;&lt;code&gt;mio&lt;/code&gt;&lt;/a&gt;: A low-level, cross-platform abstraction over OS I/O APIs that powers &lt;code&gt;tokio&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/tokio-rs/bytes"&gt;&lt;code&gt;bytes&lt;/code&gt;&lt;/a&gt;: Utilities for working with bytes, including efficient byte buffers.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/tokio-rs/loom"&gt;&lt;code&gt;loom&lt;/code&gt;&lt;/a&gt;: A testing tool for concurrent Rust code.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Changelog&lt;/h2&gt; 
&lt;p&gt;The Tokio repository contains multiple crates. Each crate has its own changelog.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;tokio&lt;/code&gt; - &lt;a href="https://github.com/tokio-rs/tokio/raw/master/tokio/CHANGELOG.md"&gt;view changelog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tokio-util&lt;/code&gt; - &lt;a href="https://github.com/tokio-rs/tokio/raw/master/tokio-util/CHANGELOG.md"&gt;view changelog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tokio-stream&lt;/code&gt; - &lt;a href="https://github.com/tokio-rs/tokio/raw/master/tokio-stream/CHANGELOG.md"&gt;view changelog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tokio-macros&lt;/code&gt; - &lt;a href="https://github.com/tokio-rs/tokio/raw/master/tokio-macros/CHANGELOG.md"&gt;view changelog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tokio-test&lt;/code&gt; - &lt;a href="https://github.com/tokio-rs/tokio/raw/master/tokio-test/CHANGELOG.md"&gt;view changelog&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Supported Rust Versions&lt;/h2&gt; 
&lt;!--
When updating this, also update:
- .github/workflows/ci.yml
- CONTRIBUTING.md
- README.md
- tokio/README.md
- tokio/Cargo.toml
- tokio-util/Cargo.toml
- tokio-test/Cargo.toml
- tokio-stream/Cargo.toml
--&gt; 
&lt;p&gt;Tokio will keep a rolling MSRV (minimum supported rust version) policy of &lt;strong&gt;at least&lt;/strong&gt; 6 months. When increasing the MSRV, the new Rust version must have been released at least six months ago. The current MSRV is 1.71.&lt;/p&gt; 
&lt;p&gt;Note that the MSRV is not increased automatically, and only as part of a minor release. The MSRV history for past minor releases can be found below:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;1.48 to now - Rust 1.71&lt;/li&gt; 
 &lt;li&gt;1.39 to 1.47 - Rust 1.70&lt;/li&gt; 
 &lt;li&gt;1.30 to 1.38 - Rust 1.63&lt;/li&gt; 
 &lt;li&gt;1.27 to 1.29 - Rust 1.56&lt;/li&gt; 
 &lt;li&gt;1.17 to 1.26 - Rust 1.49&lt;/li&gt; 
 &lt;li&gt;1.15 to 1.16 - Rust 1.46&lt;/li&gt; 
 &lt;li&gt;1.0 to 1.14 - Rust 1.45&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Note that although we try to avoid the situation where a dependency transitively increases the MSRV of Tokio, we do not guarantee that this does not happen. However, every minor release will have some set of versions of dependencies that works with the MSRV of that minor release.&lt;/p&gt; 
&lt;h2&gt;Release schedule&lt;/h2&gt; 
&lt;p&gt;Tokio doesn't follow a fixed release schedule, but we typically make one minor release each month. We make patch releases for bugfixes as necessary.&lt;/p&gt; 
&lt;h2&gt;Bug patching policy&lt;/h2&gt; 
&lt;p&gt;For the purposes of making patch releases with bugfixes, we have designated certain minor releases as LTS (long term support) releases. Whenever a bug warrants a patch release with a fix for the bug, it will be backported and released as a new patch release for each LTS minor version. Our current LTS releases are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;1.43.x&lt;/code&gt; - LTS release until March 2026. (MSRV 1.70)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;1.47.x&lt;/code&gt; - LTS release until September 2026. (MSRV 1.70)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Each LTS release will continue to receive backported fixes for at least a year. If you wish to use a fixed minor release in your project, we recommend that you use an LTS release.&lt;/p&gt; 
&lt;p&gt;To use a fixed minor version, you can specify the version with a tilde. For example, to specify that you wish to use the newest &lt;code&gt;1.43.x&lt;/code&gt; patch release, you can use the following dependency specification:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;tokio = { version = "~1.43", features = [...] }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Previous LTS releases&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;1.8.x&lt;/code&gt; - LTS release until February 2022.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;1.14.x&lt;/code&gt; - LTS release until June 2022.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;1.18.x&lt;/code&gt; - LTS release until June 2023.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;1.20.x&lt;/code&gt; - LTS release until September 2023.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;1.25.x&lt;/code&gt; - LTS release until March 2024.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;1.32.x&lt;/code&gt; - LTS release until September 2024.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;1.36.x&lt;/code&gt; - LTS release until March 2025.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;1.38.x&lt;/code&gt; - LTS release until July 2025.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the &lt;a href="https://github.com/tokio-rs/tokio/raw/master/LICENSE"&gt;MIT license&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Contribution&lt;/h3&gt; 
&lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in Tokio by you, shall be licensed as MIT, without any additional terms or conditions.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>clockworklabs/SpacetimeDB</title>
      <link>https://github.com/clockworklabs/SpacetimeDB</link>
      <description>&lt;p&gt;Multiplayer at the speed of light&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://spacetimedb.com#gh-dark-mode-only" target="_blank"&gt; &lt;img width="320" src="https://raw.githubusercontent.com/clockworklabs/SpacetimeDB/master/images/dark/logo.svg?sanitize=true" alt="SpacetimeDB Logo" /&gt; &lt;/a&gt; &lt;a href="https://spacetimedb.com#gh-light-mode-only" target="_blank"&gt; &lt;img width="320" src="https://raw.githubusercontent.com/clockworklabs/SpacetimeDB/master/images/light/logo.svg?sanitize=true" alt="SpacetimeDB Logo" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://spacetimedb.com#gh-dark-mode-only" target="_blank"&gt; &lt;img width="250" src="https://raw.githubusercontent.com/clockworklabs/SpacetimeDB/master/images/dark/logo-text.svg?sanitize=true" alt="SpacetimeDB" /&gt; &lt;/a&gt; &lt;a href="https://spacetimedb.com#gh-light-mode-only" target="_blank"&gt; &lt;img width="250" src="https://raw.githubusercontent.com/clockworklabs/SpacetimeDB/master/images/light/logo-text.svg?sanitize=true" alt="SpacetimeDB" /&gt; &lt;/a&gt; &lt;/p&gt;
&lt;h3 align="center"&gt; Multiplayer at the speed of light. &lt;/h3&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/clockworklabs/spacetimedb"&gt;&lt;img src="https://img.shields.io/github/v/release/clockworklabs/spacetimedb?color=%23ff00a0&amp;amp;include_prereleases&amp;amp;label=version&amp;amp;sort=semver&amp;amp;style=flat-square" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://github.com/clockworklabs/spacetimedb"&gt;&lt;img src="https://img.shields.io/badge/built_with-Rust-dca282.svg?style=flat-square" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://github.com/clockworklabs/spacetimedb/actions"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/clockworklabs/spacetimedb/ci.yml?style=flat-square&amp;amp;branch=master" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://status.spacetimedb.com"&gt;&lt;img src="https://img.shields.io/uptimerobot/ratio/7/m784409192-e472ca350bb615372ededed7?label=cloud%20uptime&amp;amp;style=flat-square" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://hub.docker.com/r/clockworklabs/spacetimedb"&gt;&lt;img src="https://img.shields.io/docker/pulls/clockworklabs/spacetimedb?style=flat-square" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://github.com/clockworklabs/spacetimedb/raw/master/LICENSE.txt"&gt;&lt;img src="https://img.shields.io/badge/license-BSL_1.1-00bfff.svg?style=flat-square" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://crates.io/crates/spacetimedb"&gt;&lt;img src="https://img.shields.io/crates/d/spacetimedb?color=e45928&amp;amp;label=Rust%20Crate&amp;amp;style=flat-square" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://www.nuget.org/packages/SpacetimeDB.Runtime"&gt;&lt;img src="https://img.shields.io/nuget/dt/spacetimedb.runtime?color=0b6cff&amp;amp;label=NuGet%20Package&amp;amp;style=flat-square" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://discord.gg/spacetimedb"&gt;&lt;img src="https://img.shields.io/discord/1037340874172014652?label=discord&amp;amp;style=flat-square&amp;amp;color=5a66f6" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://twitter.com/spacetime_db"&gt;&lt;img src="https://img.shields.io/badge/twitter-Follow_us-1d9bf0.svg?style=flat-square" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://clockworklabs.io/join"&gt;&lt;img src="https://img.shields.io/badge/careers-Join_us-86f7b7.svg?style=flat-square" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://www.linkedin.com/company/clockworklabs/"&gt;&lt;img src="https://img.shields.io/badge/linkedin-Connect_with_us-0a66c2.svg?style=flat-square" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://discord.gg/spacetimedb"&gt;&lt;img height="25" src="https://raw.githubusercontent.com/clockworklabs/SpacetimeDB/master/images/social/discord.svg?sanitize=true" alt="Discord" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://twitter.com/spacetime_db"&gt;&lt;img height="25" src="https://raw.githubusercontent.com/clockworklabs/SpacetimeDB/master/images/social/twitter.svg?sanitize=true" alt="Twitter" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://github.com/clockworklabs/spacetimedb"&gt;&lt;img height="25" src="https://raw.githubusercontent.com/clockworklabs/SpacetimeDB/master/images/social/github.svg?sanitize=true" alt="GitHub" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://twitch.tv/SpacetimeDB"&gt;&lt;img height="25" src="https://raw.githubusercontent.com/clockworklabs/SpacetimeDB/master/images/social/twitch.svg?sanitize=true" alt="Twitch" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://youtube.com/@SpacetimeDB"&gt;&lt;img height="25" src="https://raw.githubusercontent.com/clockworklabs/SpacetimeDB/master/images/social/youtube.svg?sanitize=true" alt="YouTube" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://www.linkedin.com/company/clockwork-labs/"&gt;&lt;img height="25" src="https://raw.githubusercontent.com/clockworklabs/SpacetimeDB/master/images/social/linkedin.svg?sanitize=true" alt="LinkedIn" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://stackoverflow.com/questions/tagged/spacetimedb"&gt;&lt;img height="25" src="https://raw.githubusercontent.com/clockworklabs/SpacetimeDB/master/images/social/stackoverflow.svg?sanitize=true" alt="StackOverflow" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;What is &lt;a href="https://spacetimedb.com"&gt;SpacetimeDB&lt;/a&gt;?&lt;/h2&gt; 
&lt;p&gt;You can think of SpacetimeDB as both a database and server combined into one.&lt;/p&gt; 
&lt;p&gt;It is a relational database system that lets you upload your application logic directly into the database by way of fancy stored procedures called "modules."&lt;/p&gt; 
&lt;p&gt;Instead of deploying a web or game server that sits in between your clients and your database, your clients connect directly to the database and execute your application logic inside the database itself. You can write all of your permission and authorization logic right inside your module just as you would in a normal server.&lt;/p&gt; 
&lt;p&gt;This means that you can write your entire application in a single language, Rust, and deploy it as a single binary. No more microservices, no more containers, no more Kubernetes, no more Docker, no more VMs, no more DevOps, no more infrastructure, no more ops, no more servers.&lt;/p&gt; 
&lt;figure&gt; 
 &lt;img src="https://raw.githubusercontent.com/clockworklabs/SpacetimeDB/master/images/basic-architecture-diagram.png" alt="SpacetimeDB Architecture" style="width:100%" /&gt; 
 &lt;figcaption align="center"&gt; 
  &lt;p align="center"&gt;&lt;b&gt;SpacetimeDB application architecture&lt;/b&gt;&lt;br /&gt;&lt;sup&gt;&lt;sub&gt;(elements in white are provided by SpacetimeDB)&lt;/sub&gt;&lt;/sup&gt;&lt;/p&gt; 
 &lt;/figcaption&gt; 
&lt;/figure&gt; 
&lt;p&gt;It's actually similar to the idea of smart contracts, except that SpacetimeDB is a database, has nothing to do with blockchain, and is orders of magnitude faster than any smart contract system.&lt;/p&gt; 
&lt;p&gt;So fast, in fact, that the entire backend of our MMORPG &lt;a href="https://bitcraftonline.com"&gt;BitCraft Online&lt;/a&gt; is just a SpacetimeDB module. We don't have any other servers or services running, which means that everything in the game, all of the chat messages, items, resources, terrain, and even the locations of the players are stored and processed by the database before being synchronized out to all of the clients in real-time.&lt;/p&gt; 
&lt;p&gt;SpacetimeDB is optimized for maximum speed and minimum latency rather than batch processing or OLAP workloads. It is designed to be used for real-time applications like games, chat, and collaboration tools.&lt;/p&gt; 
&lt;p&gt;This speed and latency is achieved by holding all of application state in memory, while persisting the data in a write-ahead-log (WAL) which is used to recover application state.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;You can run SpacetimeDB as a standalone database server via the &lt;code&gt;spacetime&lt;/code&gt; CLI tool. Install instructions for supported platforms are outlined below. The same install instructions can be found on our website at &lt;a href="https://spacetimedb.com/install"&gt;https://spacetimedb.com/install&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Install on macOS&lt;/h4&gt; 
&lt;p&gt;Installing on macOS is as simple as running our install script. After that you can use the spacetime command to manage versions.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -sSf https://install.spacetimedb.com | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Install on Linux&lt;/h4&gt; 
&lt;p&gt;Installing on Linux is as simple as running our install script. After that you can use the spacetime command to manage versions.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -sSf https://install.spacetimedb.com | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Install on Windows&lt;/h4&gt; 
&lt;p&gt;Installing on Windows is as simple as pasting the above snippet into PowerShell. If you would like to use WSL instead, please follow the Linux install instructions.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-ps1"&gt;iwr https://windows.spacetimedb.com -useb | iex
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Installing from Source&lt;/h4&gt; 
&lt;p&gt;A quick note on installing from source: we recommend that you don't install from source unless there is a feature that is available in &lt;code&gt;master&lt;/code&gt; that hasn't been released yet, otherwise follow the official installation instructions.&lt;/p&gt; 
&lt;h5&gt;MacOS + Linux&lt;/h5&gt; 
&lt;p&gt;Installing on macOS + Linux is pretty straightforward. First we are going to build all of the binaries that we need:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install rustup, you can skip this step if you have cargo and the wasm32-unknown-unknown target already installed.
curl https://sh.rustup.rs -sSf | sh
# Clone SpacetimeDB
git clone https://github.com/clockworklabs/SpacetimeDB
# Build and install the CLI
cd SpacetimeDB
cargo build --locked --release -p spacetimedb-standalone -p spacetimedb-update -p spacetimedb-cli

# Create directories
mkdir -p ~/.local/bin
export STDB_VERSION="$(./target/release/spacetimedb-cli --version | sed -n 's/.*spacetimedb tool version \([0-9.]*\);.*/\1/p')"
mkdir -p ~/.local/share/spacetime/bin/$STDB_VERSION

# Install the update binary
cp target/release/spacetimedb-update ~/.local/bin/spacetime
cp target/release/spacetimedb-cli ~/.local/share/spacetime/bin/$STDB_VERSION
cp target/release/spacetimedb-standalone ~/.local/share/spacetime/bin/$STDB_VERSION
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;At this stage you'll need to add ~/.local/bin to your path if you haven't already.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# Please add the following line to your shell configuration and open a new shell session:
export PATH="$HOME/.local/bin:$PATH"

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then finally set your SpacetimeDB version:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;
# Then, in a new shell, set the current version:
spacetime version use $STDB_VERSION

# If STDB_VERSION is not set anymore then you can use the following command to list your versions:
spacetime version list
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can verify that the correct version has been installed via &lt;code&gt;spacetime --version&lt;/code&gt;.&lt;/p&gt; 
&lt;h5&gt;Windows&lt;/h5&gt; 
&lt;p&gt;Building on windows is a bit more complicated. You'll need a slightly different version of perl compared to what comes pre-bundled in most Windows terminals. We recommend &lt;a href="https://strawberryperl.com/"&gt;Strawberry Perl&lt;/a&gt;. You may also need access to an &lt;code&gt;openssl&lt;/code&gt; binary which actually comes pre-installed with &lt;a href="https://git-scm.com/downloads/win"&gt;Git for Windows&lt;/a&gt;. Also, you'll need to install &lt;a href="https://rustup.rs/"&gt;rustup&lt;/a&gt; for Windows.&lt;/p&gt; 
&lt;p&gt;In a Git for Windows shell you should have something that looks like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ which perl
/c/Strawberry/perl/bin/perl
$ which openssl
/mingw64/bin/openssl
$ which cargo 
/c/Users/&amp;lt;user&amp;gt;/.cargo/bin/cargo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If that looks correct then you're ready to proceed!&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;# Clone SpacetimeDB
git clone https://github.com/clockworklabs/SpacetimeDB

# Build and install the CLI
cd SpacetimeDB
cargo build --locked --release -p spacetimedb-standalone -p spacetimedb-update -p spacetimedb-cli

# Create directories
$stdbDir = "$HOME\AppData\Local\SpacetimeDB"
$stdbVersion = &amp;amp; ".\target\release\spacetimedb-cli" --version | Select-String -Pattern 'spacetimedb tool version ([0-9.]+);' | ForEach-Object { $_.Matches.Groups[1].Value }
New-Item -ItemType Directory -Path "$stdbDir\bin\$stdbVersion" -Force | Out-Null

# Install the update binary
Copy-Item "target\release\spacetimedb-update.exe" "$stdbDir\spacetime.exe"
Copy-Item "target\release\spacetimedb-cli.exe" "$stdbDir\bin\$stdbVersion\"
Copy-Item "target\release\spacetimedb-standalone.exe" "$stdbDir\bin\$stdbVersion\"

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now add the directory we just created to your path. We recommend adding it to the system path because then it will be available to all of your applications (including Unity3D). After you do this, restart your shell!&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;%USERPROFILE%\AppData\Local\SpacetimeDB
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then finally, open a new shell and use the installed SpacetimeDB version:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;spacetime version use $stdbVersion

# If stdbVersion is no longer set, list versions using the following command:
spacetime version list
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can verify that the correct version has been installed via &lt;code&gt;spacetime --version&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;If you're using Git for Windows you can follow these instructions instead:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone SpacetimeDB
git clone https://github.com/clockworklabs/SpacetimeDB
# Build and install the CLI
cd SpacetimeDB
# Build the CLI binaries - this takes a while on windows so go grab a coffee :)
cargo build --locked --release -p spacetimedb-standalone -p spacetimedb-update -p spacetimedb-cli

# Create directories
export STDB_VERSION="$(./target/release/spacetimedb-cli --version | sed -n 's/.*spacetimedb tool version \([0-9.]*\);.*/\1/p')"
mkdir -p ~/AppData/Local/SpacetimeDB/bin/$STDB_VERSION

# Install the update binary
cp target/release/spacetimedb-update ~/AppData/Local/SpacetimeDB/spacetime
cp target/release/spacetimedb-cli ~/AppData/Local/SpacetimeDB/bin/$STDB_VERSION
cp target/release/spacetimedb-standalone ~/AppData/Local/SpacetimeDB/bin/$STDB_VERSION

# Now add the directory we just created to your path. We recommend adding it to the system path because then it will be available to all of your applications (including Unity3D). After you do this, restart your shell!
# %USERPROFILE%\AppData\Local\SpacetimeDB

# Set the current version
spacetime version use $STDB_VERSION
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can verify that the correct version has been installed via &lt;code&gt;spacetime --version&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;Running with Docker&lt;/h4&gt; 
&lt;p&gt;If you prefer to run Spacetime in a container, you can use the following command to start a new instance.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --rm --pull always -p 3000:3000 clockworklabs/spacetime start
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;For more information about SpacetimeDB, getting started guides, game development guides, and reference material please see our &lt;a href="https://spacetimedb.com/docs"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;We've prepared several getting started guides in each of our supported languages to help you get up and running with SpacetimeDB as quickly as possible. You can find them on our &lt;a href="https://spacetimedb.com/docs"&gt;docs page&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In summary there are only 4 steps to getting started with SpacetimeDB.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install the &lt;code&gt;spacetime&lt;/code&gt; CLI tool.&lt;/li&gt; 
 &lt;li&gt;Start a SpacetimeDB standalone node with &lt;code&gt;spacetime start&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Write and upload a module in one of our supported module languages.&lt;/li&gt; 
 &lt;li&gt;Connect to the database with one of our client libraries.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;You can see a summary of the supported languages below with a link to the getting started guide for each.&lt;/p&gt; 
&lt;h2&gt;Language Support&lt;/h2&gt; 
&lt;p&gt;You can write SpacetimeDB modules in several popular languages, with more to come in the future!&lt;/p&gt; 
&lt;h4&gt;Serverside Libraries&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://spacetimedb.com/docs/modules/rust/quickstart"&gt;Rust&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://spacetimedb.com/docs/modules/c-sharp/quickstart"&gt;C#&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Client Libraries&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://spacetimedb.com/docs/sdks/rust/quickstart"&gt;Rust&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://spacetimedb.com/docs/sdks/c-sharp/quickstart"&gt;C#&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://spacetimedb.com/docs/sdks/typescript/quickstart"&gt;Typescript&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;SpacetimeDB is licensed under the BSL 1.1 license. This is not an open source or free software license, however, it converts to the AGPL v3.0 license with a linking exception after a few years.&lt;/p&gt; 
&lt;p&gt;Note that the AGPL v3.0 does not typically include a linking exception. We have added a custom linking exception to the AGPL license for SpacetimeDB. Our motivation for choosing a free software license is to ensure that contributions made to SpacetimeDB are propagated back to the community. We are expressly not interested in forcing users of SpacetimeDB to open source their own code if they link with SpacetimeDB, so we needed to include a linking exception.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>BurntSushi/ripgrep</title>
      <link>https://github.com/BurntSushi/ripgrep</link>
      <description>&lt;p&gt;ripgrep recursively searches directories for a regex pattern while respecting your gitignore&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;ripgrep (rg)&lt;/h2&gt; 
&lt;p&gt;ripgrep is a line-oriented search tool that recursively searches the current directory for a regex pattern. By default, ripgrep will respect gitignore rules and automatically skip hidden files/directories and binary files. (To disable all automatic filtering by default, use &lt;code&gt;rg -uuu&lt;/code&gt;.) ripgrep has first class support on Windows, macOS and Linux, with binary downloads available for &lt;a href="https://github.com/BurntSushi/ripgrep/releases"&gt;every release&lt;/a&gt;. ripgrep is similar to other popular search tools like The Silver Searcher, ack and grep.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/BurntSushi/ripgrep/actions"&gt;&lt;img src="https://github.com/BurntSushi/ripgrep/workflows/ci/badge.svg?sanitize=true" alt="Build status" /&gt;&lt;/a&gt; &lt;a href="https://crates.io/crates/ripgrep"&gt;&lt;img src="https://img.shields.io/crates/v/ripgrep.svg?sanitize=true" alt="Crates.io" /&gt;&lt;/a&gt; &lt;a href="https://repology.org/project/ripgrep/badges"&gt;&lt;img src="https://repology.org/badge/tiny-repos/ripgrep.svg?sanitize=true" alt="Packaging status" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Dual-licensed under MIT or the &lt;a href="https://unlicense.org"&gt;UNLICENSE&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;CHANGELOG&lt;/h3&gt; 
&lt;p&gt;Please see the &lt;a href="https://raw.githubusercontent.com/BurntSushi/ripgrep/master/CHANGELOG.md"&gt;CHANGELOG&lt;/a&gt; for a release history.&lt;/p&gt; 
&lt;h3&gt;Documentation quick links&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BurntSushi/ripgrep/master/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BurntSushi/ripgrep/master/GUIDE.md"&gt;User Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BurntSushi/ripgrep/master/FAQ.md"&gt;Frequently Asked Questions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.rs/regex/1/regex/#syntax"&gt;Regex syntax&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BurntSushi/ripgrep/master/GUIDE.md#configuration-file"&gt;Configuration files&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BurntSushi/ripgrep/master/FAQ.md#complete"&gt;Shell completions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BurntSushi/ripgrep/master/#building"&gt;Building&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BurntSushi/ripgrep/master/#translations"&gt;Translations&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Screenshot of search results&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://burntsushi.net/stuff/ripgrep1.png"&gt;&lt;img src="https://burntsushi.net/stuff/ripgrep1.png" alt="A screenshot of a sample search with ripgrep" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Quick examples comparing tools&lt;/h3&gt; 
&lt;p&gt;This example searches the entire &lt;a href="https://github.com/BurntSushi/linux"&gt;Linux kernel source tree&lt;/a&gt; (after running &lt;code&gt;make defconfig &amp;amp;&amp;amp; make -j8&lt;/code&gt;) for &lt;code&gt;[A-Z]+_SUSPEND&lt;/code&gt;, where all matches must be words. Timings were collected on a system with an Intel i9-12900K 5.2 GHz.&lt;/p&gt; 
&lt;p&gt;Please remember that a single benchmark is never enough! See my &lt;a href="https://blog.burntsushi.net/ripgrep/"&gt;blog post on ripgrep&lt;/a&gt; for a very detailed comparison with more benchmarks and analysis.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Tool&lt;/th&gt; 
   &lt;th&gt;Command&lt;/th&gt; 
   &lt;th&gt;Line count&lt;/th&gt; 
   &lt;th&gt;Time&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ripgrep (Unicode)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;rg -n -w '[A-Z]+_SUSPEND'&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;536&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.082s&lt;/strong&gt; (1.00x)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/p-ranav/hypergrep"&gt;hypergrep&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;hgrep -n -w '[A-Z]+_SUSPEND'&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;536&lt;/td&gt; 
   &lt;td&gt;0.167s (2.04x)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.kernel.org/pub/software/scm/git/docs/git-grep.html"&gt;git grep&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;git grep -P -n -w '[A-Z]+_SUSPEND'&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;536&lt;/td&gt; 
   &lt;td&gt;0.273s (3.34x)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ggreer/the_silver_searcher"&gt;The Silver Searcher&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ag -w '[A-Z]+_SUSPEND'&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;534&lt;/td&gt; 
   &lt;td&gt;0.443s (5.43x)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Genivia/ugrep"&gt;ugrep&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ugrep -r --ignore-files --no-hidden -I -w '[A-Z]+_SUSPEND'&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;536&lt;/td&gt; 
   &lt;td&gt;0.639s (7.82x)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.kernel.org/pub/software/scm/git/docs/git-grep.html"&gt;git grep&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;LC_ALL=C git grep -E -n -w '[A-Z]+_SUSPEND'&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;536&lt;/td&gt; 
   &lt;td&gt;0.727s (8.91x)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.kernel.org/pub/software/scm/git/docs/git-grep.html"&gt;git grep (Unicode)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;LC_ALL=en_US.UTF-8 git grep -E -n -w '[A-Z]+_SUSPEND'&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;536&lt;/td&gt; 
   &lt;td&gt;2.670s (32.70x)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/beyondgrep/ack3"&gt;ack&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ack -w '[A-Z]+_SUSPEND'&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;2677&lt;/td&gt; 
   &lt;td&gt;2.935s (35.94x)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Here's another benchmark on the same corpus as above that disregards gitignore files and searches with a whitelist instead. The corpus is the same as in the previous benchmark, and the flags passed to each command ensure that they are doing equivalent work:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Tool&lt;/th&gt; 
   &lt;th&gt;Command&lt;/th&gt; 
   &lt;th&gt;Line count&lt;/th&gt; 
   &lt;th&gt;Time&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ripgrep&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;rg -uuu -tc -n -w '[A-Z]+_SUSPEND'&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;447&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.063s&lt;/strong&gt; (1.00x)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Genivia/ugrep"&gt;ugrep&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ugrep -r -n --include='*.c' --include='*.h' -w '[A-Z]+_SUSPEND'&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;447&lt;/td&gt; 
   &lt;td&gt;0.607s (9.62x)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.gnu.org/software/grep/"&gt;GNU grep&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;grep -E -r -n --include='*.c' --include='*.h' -w '[A-Z]+_SUSPEND'&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;447&lt;/td&gt; 
   &lt;td&gt;0.674s (10.69x)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Now we'll move to searching on single large file. Here is a straight-up comparison between ripgrep, ugrep and GNU grep on a file cached in memory (~13GB, &lt;a href="http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2018/mono/OpenSubtitles.raw.en.gz"&gt;&lt;code&gt;OpenSubtitles.raw.en.gz&lt;/code&gt;&lt;/a&gt;, decompressed):&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Tool&lt;/th&gt; 
   &lt;th&gt;Command&lt;/th&gt; 
   &lt;th&gt;Line count&lt;/th&gt; 
   &lt;th&gt;Time&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ripgrep (Unicode)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;rg -w 'Sherlock [A-Z]\w+'&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;7882&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;1.042s&lt;/strong&gt; (1.00x)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Genivia/ugrep"&gt;ugrep&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ugrep -w 'Sherlock [A-Z]\w+'&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;7882&lt;/td&gt; 
   &lt;td&gt;1.339s (1.28x)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.gnu.org/software/grep/"&gt;GNU grep (Unicode)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;LC_ALL=en_US.UTF-8 egrep -w 'Sherlock [A-Z]\w+'&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;7882&lt;/td&gt; 
   &lt;td&gt;6.577s (6.31x)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;In the above benchmark, passing the &lt;code&gt;-n&lt;/code&gt; flag (for showing line numbers) increases the times to &lt;code&gt;1.664s&lt;/code&gt; for ripgrep and &lt;code&gt;9.484s&lt;/code&gt; for GNU grep. ugrep times are unaffected by the presence or absence of &lt;code&gt;-n&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Beware of performance cliffs though:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Tool&lt;/th&gt; 
   &lt;th&gt;Command&lt;/th&gt; 
   &lt;th&gt;Line count&lt;/th&gt; 
   &lt;th&gt;Time&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ripgrep (Unicode)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;rg -w '[A-Z]\w+ Sherlock [A-Z]\w+'&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;485&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;1.053s&lt;/strong&gt; (1.00x)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.gnu.org/software/grep/"&gt;GNU grep (Unicode)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;LC_ALL=en_US.UTF-8 grep -E -w '[A-Z]\w+ Sherlock [A-Z]\w+'&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;485&lt;/td&gt; 
   &lt;td&gt;6.234s (5.92x)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Genivia/ugrep"&gt;ugrep&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ugrep -w '[A-Z]\w+ Sherlock [A-Z]\w+'&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;485&lt;/td&gt; 
   &lt;td&gt;28.973s (27.51x)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;And performance can drop precipitously across the board when searching big files for patterns without any opportunities for literal optimizations:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Tool&lt;/th&gt; 
   &lt;th&gt;Command&lt;/th&gt; 
   &lt;th&gt;Line count&lt;/th&gt; 
   &lt;th&gt;Time&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ripgrep&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;rg '[A-Za-z]{30}'&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;6749&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;15.569s&lt;/strong&gt; (1.00x)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Genivia/ugrep"&gt;ugrep&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ugrep -E '[A-Za-z]{30}'&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;6749&lt;/td&gt; 
   &lt;td&gt;21.857s (1.40x)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.gnu.org/software/grep/"&gt;GNU grep&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;LC_ALL=C grep -E '[A-Za-z]{30}'&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;6749&lt;/td&gt; 
   &lt;td&gt;32.409s (2.08x)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.gnu.org/software/grep/"&gt;GNU grep (Unicode)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;LC_ALL=en_US.UTF-8 grep -E '[A-Za-z]{30}'&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;6795&lt;/td&gt; 
   &lt;td&gt;8m30s (32.74x)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Finally, high match counts also tend to both tank performance and smooth out the differences between tools (because performance is dominated by how quickly one can handle a match and not the algorithm used to detect the match, generally speaking):&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Tool&lt;/th&gt; 
   &lt;th&gt;Command&lt;/th&gt; 
   &lt;th&gt;Line count&lt;/th&gt; 
   &lt;th&gt;Time&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ripgrep&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;rg the&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;83499915&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;6.948s&lt;/strong&gt; (1.00x)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Genivia/ugrep"&gt;ugrep&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ugrep the&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;83499915&lt;/td&gt; 
   &lt;td&gt;11.721s (1.69x)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.gnu.org/software/grep/"&gt;GNU grep&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;LC_ALL=C grep the&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;83499915&lt;/td&gt; 
   &lt;td&gt;15.217s (2.19x)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Why should I use ripgrep?&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;It can replace many use cases served by other search tools because it contains most of their features and is generally faster. (See &lt;a href="https://raw.githubusercontent.com/BurntSushi/ripgrep/master/FAQ.md#posix4ever"&gt;the FAQ&lt;/a&gt; for more details on whether ripgrep can truly replace grep.)&lt;/li&gt; 
 &lt;li&gt;Like other tools specialized to code search, ripgrep defaults to &lt;a href="https://raw.githubusercontent.com/BurntSushi/ripgrep/master/GUIDE.md#recursive-search"&gt;recursive search&lt;/a&gt; and does &lt;a href="https://raw.githubusercontent.com/BurntSushi/ripgrep/master/GUIDE.md#automatic-filtering"&gt;automatic filtering&lt;/a&gt;. Namely, ripgrep won't search files ignored by your &lt;code&gt;.gitignore&lt;/code&gt;/&lt;code&gt;.ignore&lt;/code&gt;/&lt;code&gt;.rgignore&lt;/code&gt; files, it won't search hidden files and it won't search binary files. Automatic filtering can be disabled with &lt;code&gt;rg -uuu&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;ripgrep can &lt;a href="https://raw.githubusercontent.com/BurntSushi/ripgrep/master/GUIDE.md#manual-filtering-file-types"&gt;search specific types of files&lt;/a&gt;. For example, &lt;code&gt;rg -tpy foo&lt;/code&gt; limits your search to Python files and &lt;code&gt;rg -Tjs foo&lt;/code&gt; excludes JavaScript files from your search. ripgrep can be taught about new file types with custom matching rules.&lt;/li&gt; 
 &lt;li&gt;ripgrep supports many features found in &lt;code&gt;grep&lt;/code&gt;, such as showing the context of search results, searching multiple patterns, highlighting matches with color and full Unicode support. Unlike GNU grep, ripgrep stays fast while supporting Unicode (which is always on).&lt;/li&gt; 
 &lt;li&gt;ripgrep has optional support for switching its regex engine to use PCRE2. Among other things, this makes it possible to use look-around and backreferences in your patterns, which are not supported in ripgrep's default regex engine. PCRE2 support can be enabled with &lt;code&gt;-P/--pcre2&lt;/code&gt; (use PCRE2 always) or &lt;code&gt;--auto-hybrid-regex&lt;/code&gt; (use PCRE2 only if needed). An alternative syntax is provided via the &lt;code&gt;--engine (default|pcre2|auto)&lt;/code&gt; option.&lt;/li&gt; 
 &lt;li&gt;ripgrep has &lt;a href="https://raw.githubusercontent.com/BurntSushi/ripgrep/master/GUIDE.md#replacements"&gt;rudimentary support for replacements&lt;/a&gt;, which permit rewriting output based on what was matched.&lt;/li&gt; 
 &lt;li&gt;ripgrep supports &lt;a href="https://raw.githubusercontent.com/BurntSushi/ripgrep/master/GUIDE.md#file-encoding"&gt;searching files in text encodings&lt;/a&gt; other than UTF-8, such as UTF-16, latin-1, GBK, EUC-JP, Shift_JIS and more. (Some support for automatically detecting UTF-16 is provided. Other text encodings must be specifically specified with the &lt;code&gt;-E/--encoding&lt;/code&gt; flag.)&lt;/li&gt; 
 &lt;li&gt;ripgrep supports searching files compressed in a common format (brotli, bzip2, gzip, lz4, lzma, xz, or zstandard) with the &lt;code&gt;-z/--search-zip&lt;/code&gt; flag.&lt;/li&gt; 
 &lt;li&gt;ripgrep supports &lt;a href="https://raw.githubusercontent.com/BurntSushi/ripgrep/master/GUIDE.md#preprocessor"&gt;arbitrary input preprocessing filters&lt;/a&gt; which could be PDF text extraction, less supported decompression, decrypting, automatic encoding detection and so on.&lt;/li&gt; 
 &lt;li&gt;ripgrep can be configured via a &lt;a href="https://raw.githubusercontent.com/BurntSushi/ripgrep/master/GUIDE.md#configuration-file"&gt;configuration file&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In other words, use ripgrep if you like speed, filtering by default, fewer bugs and Unicode support.&lt;/p&gt; 
&lt;h3&gt;Why shouldn't I use ripgrep?&lt;/h3&gt; 
&lt;p&gt;Despite initially not wanting to add every feature under the sun to ripgrep, over time, ripgrep has grown support for most features found in other file searching tools. This includes searching for results spanning across multiple lines, and opt-in support for PCRE2, which provides look-around and backreference support.&lt;/p&gt; 
&lt;p&gt;At this point, the primary reasons not to use ripgrep probably consist of one or more of the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You need a portable and ubiquitous tool. While ripgrep works on Windows, macOS and Linux, it is not ubiquitous and it does not conform to any standard such as POSIX. The best tool for this job is good old grep.&lt;/li&gt; 
 &lt;li&gt;There still exists some other feature (or bug) not listed in this README that you rely on that's in another tool that isn't in ripgrep.&lt;/li&gt; 
 &lt;li&gt;There is a performance edge case where ripgrep doesn't do well where another tool does do well. (Please file a bug report!)&lt;/li&gt; 
 &lt;li&gt;ripgrep isn't possible to install on your machine or isn't available for your platform. (Please file a bug report!)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Is it really faster than everything else?&lt;/h3&gt; 
&lt;p&gt;Generally, yes. A large number of benchmarks with detailed analysis for each is &lt;a href="https://blog.burntsushi.net/ripgrep/"&gt;available on my blog&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Summarizing, ripgrep is fast because:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;It is built on top of &lt;a href="https://github.com/rust-lang/regex"&gt;Rust's regex engine&lt;/a&gt;. Rust's regex engine uses finite automata, SIMD and aggressive literal optimizations to make searching very fast. (PCRE2 support can be opted into with the &lt;code&gt;-P/--pcre2&lt;/code&gt; flag.)&lt;/li&gt; 
 &lt;li&gt;Rust's regex library maintains performance with full Unicode support by building UTF-8 decoding directly into its deterministic finite automaton engine.&lt;/li&gt; 
 &lt;li&gt;It supports searching with either memory maps or by searching incrementally with an intermediate buffer. The former is better for single files and the latter is better for large directories. ripgrep chooses the best searching strategy for you automatically.&lt;/li&gt; 
 &lt;li&gt;Applies your ignore patterns in &lt;code&gt;.gitignore&lt;/code&gt; files using a &lt;a href="https://docs.rs/regex/1/regex/struct.RegexSet.html"&gt;&lt;code&gt;RegexSet&lt;/code&gt;&lt;/a&gt;. That means a single file path can be matched against multiple glob patterns simultaneously.&lt;/li&gt; 
 &lt;li&gt;It uses a lock-free parallel recursive directory iterator, courtesy of &lt;a href="https://docs.rs/crossbeam"&gt;&lt;code&gt;crossbeam&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://docs.rs/ignore"&gt;&lt;code&gt;ignore&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Feature comparison&lt;/h3&gt; 
&lt;p&gt;Andy Lester, author of &lt;a href="https://beyondgrep.com/"&gt;ack&lt;/a&gt;, has published an excellent table comparing the features of ack, ag, git-grep, GNU grep and ripgrep: &lt;a href="https://beyondgrep.com/feature-comparison/"&gt;https://beyondgrep.com/feature-comparison/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Note that ripgrep has grown a few significant new features recently that are not yet present in Andy's table. This includes, but is not limited to, configuration files, passthru, support for searching compressed files, multiline search and opt-in fancy regex support via PCRE2.&lt;/p&gt; 
&lt;h3&gt;Playground&lt;/h3&gt; 
&lt;p&gt;If you'd like to try ripgrep before installing, there's an unofficial &lt;a href="https://codapi.org/ripgrep/"&gt;playground&lt;/a&gt; and an &lt;a href="https://codapi.org/try/ripgrep/"&gt;interactive tutorial&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you have any questions about these, please open an issue in the &lt;a href="https://github.com/nalgeon/tryxinyminutes"&gt;tutorial repo&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;The binary name for ripgrep is &lt;code&gt;rg&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/BurntSushi/ripgrep/releases"&gt;Archives of precompiled binaries for ripgrep are available for Windows, macOS and Linux.&lt;/a&gt;&lt;/strong&gt; Linux and Windows binaries are static executables. Users of platforms not explicitly mentioned below are advised to download one of these archives.&lt;/p&gt; 
&lt;p&gt;If you're a &lt;strong&gt;macOS Homebrew&lt;/strong&gt; or a &lt;strong&gt;Linuxbrew&lt;/strong&gt; user, then you can install ripgrep from homebrew-core:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ brew install ripgrep
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you're a &lt;strong&gt;MacPorts&lt;/strong&gt; user, then you can install ripgrep from the &lt;a href="https://www.macports.org/ports.php?by=name&amp;amp;substr=ripgrep"&gt;official ports&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ sudo port install ripgrep
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you're a &lt;strong&gt;Windows Chocolatey&lt;/strong&gt; user, then you can install ripgrep from the &lt;a href="https://chocolatey.org/packages/ripgrep"&gt;official repo&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ choco install ripgrep
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you're a &lt;strong&gt;Windows Scoop&lt;/strong&gt; user, then you can install ripgrep from the &lt;a href="https://github.com/ScoopInstaller/Main/raw/master/bucket/ripgrep.json"&gt;official bucket&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ scoop install ripgrep
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you're a &lt;strong&gt;Windows Winget&lt;/strong&gt; user, then you can install ripgrep from the &lt;a href="https://github.com/microsoft/winget-pkgs/tree/master/manifests/b/BurntSushi/ripgrep"&gt;winget-pkgs&lt;/a&gt; repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ winget install BurntSushi.ripgrep.MSVC
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you're an &lt;strong&gt;Arch Linux&lt;/strong&gt; user, then you can install ripgrep from the official repos:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ sudo pacman -S ripgrep
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you're a &lt;strong&gt;Gentoo&lt;/strong&gt; user, you can install ripgrep from the &lt;a href="https://packages.gentoo.org/packages/sys-apps/ripgrep"&gt;official repo&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ sudo emerge sys-apps/ripgrep
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you're a &lt;strong&gt;Fedora&lt;/strong&gt; user, you can install ripgrep from official repositories.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ sudo dnf install ripgrep
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you're an &lt;strong&gt;openSUSE&lt;/strong&gt; user, ripgrep is included in &lt;strong&gt;openSUSE Tumbleweed&lt;/strong&gt; and &lt;strong&gt;openSUSE Leap&lt;/strong&gt; since 15.1.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ sudo zypper install ripgrep
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you're a &lt;strong&gt;CentOS Stream 10&lt;/strong&gt; user, you can install ripgrep from the &lt;a href="https://docs.fedoraproject.org/en-US/epel/getting-started/"&gt;EPEL&lt;/a&gt; repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ sudo dnf config-manager --set-enabled crb
$ sudo dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-10.noarch.rpm
$ sudo dnf install ripgrep
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you're a &lt;strong&gt;Red Hat 10&lt;/strong&gt; user, you can install ripgrep from the &lt;a href="https://docs.fedoraproject.org/en-US/epel/getting-started/"&gt;EPEL&lt;/a&gt; repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ sudo subscription-manager repos --enable codeready-builder-for-rhel-10-$(arch)-rpms
$ sudo dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-10.noarch.rpm
$ sudo dnf install ripgrep
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you're a &lt;strong&gt;Rocky Linux 10&lt;/strong&gt; user, you can install ripgrep from the &lt;a href="https://docs.fedoraproject.org/en-US/epel/getting-started/"&gt;EPEL&lt;/a&gt; repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ sudo dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-10.noarch.rpm
$ sudo dnf install ripgrep
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you're a &lt;strong&gt;Nix&lt;/strong&gt; user, you can install ripgrep from &lt;a href="https://github.com/NixOS/nixpkgs/raw/master/pkgs/by-name/ri/ripgrep/package.nix"&gt;nixpkgs&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ nix-env --install ripgrep
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you're a &lt;strong&gt;Flox&lt;/strong&gt; user, you can install ripgrep as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ flox install ripgrep
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you're a &lt;strong&gt;Guix&lt;/strong&gt; user, you can install ripgrep from the official package collection:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ guix install ripgrep
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you're a &lt;strong&gt;Debian&lt;/strong&gt; user (or a user of a Debian derivative like &lt;strong&gt;Ubuntu&lt;/strong&gt;), then ripgrep can be installed using a binary &lt;code&gt;.deb&lt;/code&gt; file provided in each &lt;a href="https://github.com/BurntSushi/ripgrep/releases"&gt;ripgrep release&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ curl -LO https://github.com/BurntSushi/ripgrep/releases/download/14.1.1/ripgrep_14.1.1-1_amd64.deb
$ sudo dpkg -i ripgrep_14.1.1-1_amd64.deb
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you run Debian stable, ripgrep is &lt;a href="https://tracker.debian.org/pkg/rust-ripgrep"&gt;officially maintained by Debian&lt;/a&gt;, although its version may be older than the &lt;code&gt;deb&lt;/code&gt; package available in the previous step.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ sudo apt-get install ripgrep
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you're an &lt;strong&gt;Ubuntu Cosmic (18.10)&lt;/strong&gt; (or newer) user, ripgrep is &lt;a href="https://launchpad.net/ubuntu/+source/rust-ripgrep"&gt;available&lt;/a&gt; using the same packaging as Debian:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ sudo apt-get install ripgrep
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;(N.B. Various snaps for ripgrep on Ubuntu are also available, but none of them seem to work right and generate a number of very strange bug reports that I don't know how to fix and don't have the time to fix. Therefore, it is no longer a recommended installation option.)&lt;/p&gt; 
&lt;p&gt;If you're an &lt;strong&gt;ALT&lt;/strong&gt; user, you can install ripgrep from the &lt;a href="https://packages.altlinux.org/en/search?name=ripgrep"&gt;official repo&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ sudo apt-get install ripgrep
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you're a &lt;strong&gt;FreeBSD&lt;/strong&gt; user, then you can install ripgrep from the &lt;a href="https://www.freshports.org/textproc/ripgrep/"&gt;official ports&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ sudo pkg install ripgrep
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you're an &lt;strong&gt;OpenBSD&lt;/strong&gt; user, then you can install ripgrep from the &lt;a href="https://openports.se/textproc/ripgrep"&gt;official ports&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ doas pkg_add ripgrep
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you're a &lt;strong&gt;NetBSD&lt;/strong&gt; user, then you can install ripgrep from &lt;a href="https://pkgsrc.se/textproc/ripgrep"&gt;pkgsrc&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ sudo pkgin install ripgrep
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you're a &lt;strong&gt;Haiku x86_64&lt;/strong&gt; user, then you can install ripgrep from the &lt;a href="https://github.com/haikuports/haikuports/tree/master/sys-apps/ripgrep"&gt;official ports&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ sudo pkgman install ripgrep
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you're a &lt;strong&gt;Haiku x86_gcc2&lt;/strong&gt; user, then you can install ripgrep from the same port as Haiku x86_64 using the x86 secondary architecture build:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ sudo pkgman install ripgrep_x86
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you're a &lt;strong&gt;Void Linux&lt;/strong&gt; user, then you can install ripgrep from the &lt;a href="https://voidlinux.org/packages/?arch=x86_64&amp;amp;q=ripgrep"&gt;official repository&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ sudo xbps-install -Syv ripgrep
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you're a &lt;strong&gt;Rust programmer&lt;/strong&gt;, ripgrep can be installed with &lt;code&gt;cargo&lt;/code&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Note that the minimum supported version of Rust for ripgrep is &lt;strong&gt;1.85.0&lt;/strong&gt;, although ripgrep may work with older versions.&lt;/li&gt; 
 &lt;li&gt;Note that the binary may be bigger than expected because it contains debug symbols. This is intentional. To remove debug symbols and therefore reduce the file size, run &lt;code&gt;strip&lt;/code&gt; on the binary.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;$ cargo install ripgrep
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, one can use &lt;a href="https://github.com/cargo-bins/cargo-binstall"&gt;&lt;code&gt;cargo binstall&lt;/code&gt;&lt;/a&gt; to install a ripgrep binary directly from GitHub:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ cargo binstall ripgrep
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Building&lt;/h3&gt; 
&lt;p&gt;ripgrep is written in Rust, so you'll need to grab a &lt;a href="https://www.rust-lang.org/"&gt;Rust installation&lt;/a&gt; in order to compile it. ripgrep compiles with Rust 1.85.0 (stable) or newer. In general, ripgrep tracks the latest stable release of the Rust compiler.&lt;/p&gt; 
&lt;p&gt;To build ripgrep:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/BurntSushi/ripgrep
$ cd ripgrep
$ cargo build --release
$ ./target/release/rg --version
0.1.3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; In the past, ripgrep supported a &lt;code&gt;simd-accel&lt;/code&gt; Cargo feature when using a Rust nightly compiler. This only benefited UTF-16 transcoding. Since it required unstable features, this build mode was prone to breakage. Because of that, support for it has been removed. If you want SIMD optimizations for UTF-16 transcoding, then you'll have to petition the &lt;a href="https://github.com/hsivonen/encoding_rs"&gt;&lt;code&gt;encoding_rs&lt;/code&gt;&lt;/a&gt; project to use stable APIs.&lt;/p&gt; 
&lt;p&gt;Finally, optional PCRE2 support can be built with ripgrep by enabling the &lt;code&gt;pcre2&lt;/code&gt; feature:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ cargo build --release --features 'pcre2'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Enabling the PCRE2 feature works with a stable Rust compiler and will attempt to automatically find and link with your system's PCRE2 library via &lt;code&gt;pkg-config&lt;/code&gt;. If one doesn't exist, then ripgrep will build PCRE2 from source using your system's C compiler and then statically link it into the final executable. Static linking can be forced even when there is an available PCRE2 system library by either building ripgrep with the MUSL target or by setting &lt;code&gt;PCRE2_SYS_STATIC=1&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;ripgrep can be built with the MUSL target on Linux by first installing the MUSL library on your system (consult your friendly neighborhood package manager). Then you just need to add MUSL support to your Rust toolchain and rebuild ripgrep, which yields a fully static executable:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ rustup target add x86_64-unknown-linux-musl
$ cargo build --release --target x86_64-unknown-linux-musl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Applying the &lt;code&gt;--features&lt;/code&gt; flag from above works as expected. If you want to build a static executable with MUSL and with PCRE2, then you will need to have &lt;code&gt;musl-gcc&lt;/code&gt; installed, which might be in a separate package from the actual MUSL library, depending on your Linux distribution.&lt;/p&gt; 
&lt;h3&gt;Running tests&lt;/h3&gt; 
&lt;p&gt;ripgrep is relatively well-tested, including both unit tests and integration tests. To run the full test suite, use:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ cargo test --all
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;from the repository root.&lt;/p&gt; 
&lt;h3&gt;Related tools&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dandavison/delta"&gt;delta&lt;/a&gt; is a syntax highlighting pager that supports the &lt;code&gt;rg --json&lt;/code&gt; output format. So all you need to do to make it work is &lt;code&gt;rg --json pattern | delta&lt;/code&gt;. See &lt;a href="https://dandavison.github.io/delta/grep.html"&gt;delta's manual section on grep&lt;/a&gt; for more details.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Vulnerability reporting&lt;/h3&gt; 
&lt;p&gt;For reporting a security vulnerability, please &lt;a href="https://blog.burntsushi.net/about/"&gt;contact Andrew Gallant&lt;/a&gt;. The contact page has my email address and PGP public key if you wish to send an encrypted message.&lt;/p&gt; 
&lt;h3&gt;Translations&lt;/h3&gt; 
&lt;p&gt;The following is a list of known translations of ripgrep's documentation. These are unofficially maintained and may not be up to date.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/chinanf-boy/ripgrep-zh#%E6%9B%B4%E6%96%B0-"&gt;Chinese&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/UltiRequiem/traducciones/tree/master/ripgrep"&gt;Spanish&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>mountain-loop/yaak</title>
      <link>https://github.com/mountain-loop/yaak</link>
      <description>&lt;p&gt;The most intuitive desktop API client. Organize and execute REST, GraphQL, WebSockets, Server Sent Events, and gRPC 🦬&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://github.com/JamesIves/github-sponsors-readme-action"&gt; &lt;img width="200px" src="https://github.com/mountain-loop/yaak/raw/main/src-tauri/icons/icon.png" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h1 align="center"&gt; 💫 Yaak ➟ Desktop API Client 💫 &lt;/h1&gt; 
&lt;p align="center"&gt; A fast, privacy-first API client for REST, GraphQL, SSE, WebSocket, and gRPC – built with Tauri, Rust, and React. &lt;/p&gt; 
&lt;p align="center"&gt; Development is funded by community-purchased &lt;a href="https://yaak.app/pricing"&gt;licenses&lt;/a&gt;. You can also &lt;a href="https://github.com/sponsors/gschier"&gt;become a sponsor&lt;/a&gt; to have your logo appear below. 💖 &lt;/p&gt; 
&lt;br /&gt; 
&lt;p align="center"&gt; 
 &lt;!-- sponsors-premium --&gt;&lt;a href="https://github.com/MVST-Solutions"&gt;&lt;img src="https://github.com/MVST-Solutions.png" width="80px" alt="User avatar: MVST-Solutions" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href="https://github.com/dharsanb"&gt;&lt;img src="https://github.com/dharsanb.png" width="80px" alt="User avatar: dharsanb" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href="https://github.com/railwayapp"&gt;&lt;img src="https://github.com/railwayapp.png" width="80px" alt="User avatar: railwayapp" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href="https://github.com/caseyamcl"&gt;&lt;img src="https://github.com/caseyamcl.png" width="80px" alt="User avatar: caseyamcl" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href="https://github.com/andriyor"&gt;&lt;img src="https://github.com/andriyor.png" width="80px" alt="User avatar: andriyor" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href="https://github.com/"&gt;&lt;img src="https://raw.githubusercontent.com/JamesIves/github-sponsors-readme-action/dev/.github/assets/placeholder.png" width="80px" alt="User avatar: " /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;
 &lt;!-- sponsors-premium --&gt; &lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;!-- sponsors-base --&gt;&lt;a href="https://github.com/seanwash"&gt;&lt;img src="https://github.com/seanwash.png" width="50px" alt="User avatar: seanwash" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href="https://github.com/jerath"&gt;&lt;img src="https://github.com/jerath.png" width="50px" alt="User avatar: jerath" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href="https://github.com/itsa-sh"&gt;&lt;img src="https://github.com/itsa-sh.png" width="50px" alt="User avatar: itsa-sh" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href="https://github.com/dmmulroy"&gt;&lt;img src="https://github.com/dmmulroy.png" width="50px" alt="User avatar: dmmulroy" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href="https://github.com/timcole"&gt;&lt;img src="https://github.com/timcole.png" width="50px" alt="User avatar: timcole" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href="https://github.com/VLZH"&gt;&lt;img src="https://github.com/VLZH.png" width="50px" alt="User avatar: VLZH" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href="https://github.com/terasaka2k"&gt;&lt;img src="https://github.com/terasaka2k.png" width="50px" alt="User avatar: terasaka2k" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href="https://github.com/majudhu"&gt;&lt;img src="https://github.com/majudhu.png" width="50px" alt="User avatar: majudhu" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;
 &lt;!-- sponsors-base --&gt; &lt;/p&gt; 
&lt;p&gt;&lt;img src="https://yaak.app/static/screenshot.png" alt="Yaak API Client" /&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;Yaak is an offline-first API client designed to stay out of your way while giving you everything you need when you need it. Built with &lt;a href="https://tauri.app"&gt;Tauri&lt;/a&gt;, Rust, and React, it’s fast, lightweight, and private. No telemetry, no VC funding, and no cloud lock-in.&lt;/p&gt; 
&lt;h3&gt;🌐 Work with any API&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Import collections from Postman, Insomnia, OpenAPI, Swagger, or Curl.&lt;/li&gt; 
 &lt;li&gt;Send requests via REST, GraphQL, gRPC, WebSocket, or Server-Sent Events.&lt;/li&gt; 
 &lt;li&gt;Filter and inspect responses with JSONPath or XPath.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🔐 Stay secure&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use OAuth 2.0, JWT, Basic Auth, or custom plugins for authentication.&lt;/li&gt; 
 &lt;li&gt;Secure sensitive values with encrypted secrets.&lt;/li&gt; 
 &lt;li&gt;Store secrets in your OS keychain.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;☁️ Organize &amp;amp; collaborate&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Group requests into workspaces and nested folders.&lt;/li&gt; 
 &lt;li&gt;Use environment variables to switch between dev, staging, and prod.&lt;/li&gt; 
 &lt;li&gt;Mirror workspaces to your filesystem for versioning in Git or syncing with Dropbox.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🧩 Extend &amp;amp; customize&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Insert dynamic values like UUIDs or timestamps with template tags.&lt;/li&gt; 
 &lt;li&gt;Pick from built-in themes or build your own.&lt;/li&gt; 
 &lt;li&gt;Create plugins to extend authentication, template tags, or the UI.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contribution Policy&lt;/h2&gt; 
&lt;p&gt;Yaak is open source but only accepting contributions for bug fixes. To get started, visit &lt;a href="https://raw.githubusercontent.com/mountain-loop/yaak/main/DEVELOPMENT.md"&gt;&lt;code&gt;DEVELOPMENT.md&lt;/code&gt;&lt;/a&gt; for tips on setting up your environment.&lt;/p&gt; 
&lt;h2&gt;Useful Resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://feedback.yaak.app"&gt;Feedback and Bug Reports&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://feedback.yaak.app/help"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://yaak.app/alternatives/postman"&gt;Yaak vs Postman&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://yaak.app/alternatives/bruno"&gt;Yaak vs Bruno&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://yaak.app/alternatives/insomnia"&gt;Yaak vs Insomnia&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Atlas-OS/Atlas</title>
      <link>https://github.com/Atlas-OS/Atlas</link>
      <description>&lt;p&gt;🚀 An open and lightweight modification to Windows, designed to optimize performance, privacy and usability.&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; &lt;a href="http://atlasos.net" target="_blank"&gt;&lt;img src="https://gcore.jsdelivr.net/gh/Atlas-OS/branding@main/banners/banner-v3.png" alt="Atlas" width="800" /&gt;&lt;/a&gt; &lt;/h1&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/Atlas-OS/Atlas/raw/main/LICENSE"&gt;&lt;img alt="License" src="https://img.shields.io/github/license/atlas-os/atlas?style=for-the-badge&amp;amp;logo=github&amp;amp;color=1A91FF" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Atlas-OS/Atlas/graphs/contributors"&gt;&lt;img alt="Contributors" src="https://img.shields.io/github/contributors/atlas-os/atlas?style=for-the-badge&amp;amp;color=1A91FF" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Atlas-OS/Atlas/releases/latest"&gt;&lt;img alt="Release" src="https://img.shields.io/github/release/atlas-os/atlas?style=for-the-badge&amp;amp;color=1A91FF" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Atlas-OS/.github/raw/main/profile/CODE_OF_CONDUCT.md"&gt;&lt;img alt="Code of Conduct" src="https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg?style=for-the-badge&amp;amp;color=1A91FF" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt;A transparent and lightweight modification to Windows, designed to optimize performance, privacy and usability.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://atlasos.net" target="_blank"&gt;🌐 Website&lt;/a&gt; • &lt;a href="https://docs.atlasos.net" target="_blank"&gt;📚 Documentation&lt;/a&gt; • &lt;a href="https://discord.atlasos.net" target="_blank"&gt;☎️ Discord&lt;/a&gt; • &lt;a href="https://github.com/Atlas-OS/Atlas/discussions" target="_blank"&gt;💬 Discussions&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;📚 &lt;strong&gt;Important Documentation&lt;/strong&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.atlasos.net/getting-started/installation/"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.atlasos.net/faq-and-troubleshooting/removed-features/"&gt;FAQ &amp;amp; Common Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.atlasos.net/contributions/"&gt;Contribution Guidelines&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.atlasos.net/branding/"&gt;Branding&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🤔 What is Atlas?&lt;/h2&gt; 
&lt;p&gt;AtlasOS, or Atlas, is an open-source project that enhances Windows by conveniently applying privacy, usability, and performance optimizations, all while maintaining functionality and &lt;a href="https://docs.atlasos.net/getting-started/post-installation/atlas-folder/general-configuration/"&gt;customizability&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;👀 Why Atlas?&lt;/h2&gt; 
&lt;h3&gt;🔒 Enhanced Privacy&lt;/h3&gt; 
&lt;p&gt;Atlas removes the majority of telemetry embedded within Windows and implements numerous group policies to minimize data collection. However, it cannot ensure privacy outside the scope of Windows, such as browsers and other third-party applications.&lt;/p&gt; 
&lt;h3&gt;📈 Optimized Performance&lt;/h3&gt; 
&lt;p&gt;Atlas strikes a balance between performance and compatibility. It implements numerous meaningful changes to improve Windows performance and responsiveness without breaking essential features. Atlas will not do tweaks for a placebo effect or marginal gains, making Atlas more stable and compatible.&lt;/p&gt; 
&lt;h3&gt;🛡️ Security Features&lt;/h3&gt; 
&lt;p&gt;Most Windows modifications remove key security features most users need to maintain a secure system. On the other hand, Atlas allows users to customize their security at their own risk while informing users about each option's &lt;a href="https://docs.atlasos.net/getting-started/post-installation/atlas-folder/security/"&gt;pros and cons&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Some optional security features are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Windows Defender &amp;amp; SmartScreen&lt;/li&gt; 
 &lt;li&gt;Windows Update 
  &lt;ul&gt; 
   &lt;li&gt;Automatic updates are togglable&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;CPU mitigations&lt;/li&gt; 
 &lt;li&gt;User Account Control&lt;/li&gt; 
 &lt;li&gt;Core isolation features&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;✅ Increased Usability&lt;/h3&gt; 
&lt;p&gt;Atlas applies many modifications and default settings to make Windows easier to use. This includes removing commonly unneeded applications (which are reinstallable), configuring many aspects of the interface, disabling advertisements, and much more.&lt;/p&gt; 
&lt;h3&gt;🔍 Open Source and Transparent&lt;/h3&gt; 
&lt;p&gt;Unlike custom Windows ISOs, Atlas is more straightforward to audit due to the use of &lt;a href="https://ameliorated.io"&gt;AME Wizard&lt;/a&gt;. AME Wizard is controlled by Playbooks, a customizable script-esque system that can perform various tasks.&lt;/p&gt; 
&lt;p&gt;Playbooks are renamed &lt;strong&gt;.zip&lt;/strong&gt; archives, with the password &lt;a href="https://docs.ameliorated.io/developers/getting-started/creation.html"&gt;&lt;code&gt;malte&lt;/code&gt;&lt;/a&gt;. As they primarily consist of plain text, Playbooks enable transparency, unlike custom Windows ISOs, which have many entry points for malicious activity. The few binaries in the Playbook are open source in our &lt;a href="https://github.com/Atlas-OS/utilities"&gt;&lt;code&gt;utilities&lt;/code&gt; repository&lt;/a&gt;, with the &lt;a href="https://github.com/Atlas-OS/Atlas/raw/main/src/playbook/Executables/AtlasModules/README.md"&gt;hashes listed here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Although the GUI is not open source for AME Wizard, AME Wizard's entire backend (called &lt;a href="https://github.com/Ameliorated-LLC/trusted-uninstaller-cli"&gt;TrustedUninstaller&lt;/a&gt;) is open source under MIT, which contains each action used to run Atlas. The Atlas Playbook is open source under the &lt;a href="https://github.com/Atlas-OS/Atlas/raw/main/LICENSE"&gt;GPLv3 license&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;🔒 Legal Compliance&lt;/h3&gt; 
&lt;p&gt;As Atlas doesn't redistribute a modified Windows ISO, it complies with &lt;a href="https://www.microsoft.com/en-us/useterms/#areaheading-uid6738235"&gt;Windows's Usage Terms&lt;/a&gt;. In addition, Atlas does not alter activation in Windows.&lt;/p&gt; 
&lt;h2&gt;🎨 Brand kit&lt;/h2&gt; 
&lt;p&gt;Want to create your own Atlas wallpaper with some original creative designs? Visit our &lt;a href="https://docs.atlasos.net/branding/"&gt;Branding Kit on Docs&lt;/a&gt; and share your creations on our &lt;a href="https://github.com/Atlas-OS/Atlas/discussions/categories/community-artwork"&gt;GitHub Discussions&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;💙 Contributors&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/Atlas-OS/Atlas/graphs/contributors" target="_blank"&gt;&lt;img src="https://contrib.rocks/image?repo=Atlas-OS/Atlas&amp;amp;columns=18" alt="Avatars of all contributors" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>wavetermdev/waveterm</title>
      <link>https://github.com/wavetermdev/waveterm</link>
      <description>&lt;p&gt;An open-source, cross-platform terminal for seamless workflows&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://www.waveterm.dev"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="./assets/wave-dark.png" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="./assets/wave-light.png" /&gt; 
   &lt;img alt="Wave Terminal Logo" src="https://raw.githubusercontent.com/wavetermdev/waveterm/main/assets/wave-light.png" width="240" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;br /&gt; &lt;/p&gt; 
&lt;h1&gt;Wave Terminal&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://app.fossa.com/projects/git%2Bgithub.com%2Fwavetermdev%2Fwaveterm?ref=badge_shield"&gt;&lt;img src="https://app.fossa.com/api/projects/git%2Bgithub.com%2Fwavetermdev%2Fwaveterm.svg?type=shield" alt="FOSSA Status" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Wave is an open-source terminal that combines traditional terminal features with graphical capabilities like file previews, web browsing, and AI assistance. It runs on MacOS, Linux, and Windows.&lt;/p&gt; 
&lt;p&gt;Modern development involves constantly switching between terminals and browsers - checking documentation, previewing files, monitoring systems, and using AI tools. Wave brings these graphical tools directly into the terminal, letting you control them from the command line. This means you can stay in your terminal workflow while still having access to the visual interfaces you need.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/wavetermdev/waveterm/main/assets/wave-screenshot.webp" alt="WaveTerm Screenshot" /&gt;&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Flexible drag &amp;amp; drop interface to organize terminal blocks, editors, web browsers, and AI assistants&lt;/li&gt; 
 &lt;li&gt;Built-in editor for seamlessly editing remote files with syntax highlighting and modern editor features&lt;/li&gt; 
 &lt;li&gt;Rich file preview system for remote files (markdown, images, video, PDFs, CSVs, directories)&lt;/li&gt; 
 &lt;li&gt;Integrated AI chat with support for multiple models (OpenAI, Claude, Azure, Perplexity, Ollama)&lt;/li&gt; 
 &lt;li&gt;Command Blocks for isolating and monitoring individual commands with auto-close options&lt;/li&gt; 
 &lt;li&gt;One-click remote connections with full terminal and file system access&lt;/li&gt; 
 &lt;li&gt;Rich customization including tab themes, terminal styles, and background images&lt;/li&gt; 
 &lt;li&gt;Powerful &lt;code&gt;wsh&lt;/code&gt; command system for managing your workspace from the CLI and sharing data between terminal sessions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Wave Terminal works on macOS, Linux, and Windows.&lt;/p&gt; 
&lt;p&gt;Platform-specific installation instructions can be found &lt;a href="https://docs.waveterm.dev/gettingstarted"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can also install Wave Terminal directly from: &lt;a href="https://www.waveterm.dev/download"&gt;www.waveterm.dev/download&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Minimum requirements&lt;/h3&gt; 
&lt;p&gt;Wave Terminal runs on the following platforms:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;macOS 11 or later (arm64, x64)&lt;/li&gt; 
 &lt;li&gt;Windows 10 1809 or later (x64)&lt;/li&gt; 
 &lt;li&gt;Linux based on glibc-2.28 or later (Debian 10, RHEL 8, Ubuntu 20.04, etc.) (arm64, x64)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The WSH helper runs on the following platforms:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;macOS 11 or later (arm64, x64)&lt;/li&gt; 
 &lt;li&gt;Windows 10 or later (arm64, x64)&lt;/li&gt; 
 &lt;li&gt;Linux Kernel 2.6.32 or later (x64), Linux Kernel 3.1 or later (arm64)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;p&gt;Wave is constantly improving! Our roadmap will be continuously updated with our goals for each release. You can find it &lt;a href="https://raw.githubusercontent.com/wavetermdev/waveterm/main/ROADMAP.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Want to provide input to our future releases? Connect with us on &lt;a href="https://discord.gg/XfvZ334gwU"&gt;Discord&lt;/a&gt; or open a &lt;a href="https://github.com/wavetermdev/waveterm/issues/new/choose"&gt;Feature Request&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Homepage — &lt;a href="https://www.waveterm.dev"&gt;https://www.waveterm.dev&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Download Page — &lt;a href="https://www.waveterm.dev/download"&gt;https://www.waveterm.dev/download&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Documentation — &lt;a href="https://docs.waveterm.dev"&gt;https://docs.waveterm.dev&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Legacy Documentation — &lt;a href="https://legacydocs.waveterm.dev"&gt;https://legacydocs.waveterm.dev&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Blog — &lt;a href="https://blog.waveterm.dev"&gt;https://blog.waveterm.dev&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;X — &lt;a href="https://x.com/wavetermdev"&gt;https://x.com/wavetermdev&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Discord Community — &lt;a href="https://discord.gg/XfvZ334gwU"&gt;https://discord.gg/XfvZ334gwU&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Building from Source&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/wavetermdev/waveterm/main/BUILD.md"&gt;Building Wave Terminal&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Wave uses GitHub Issues for issue tracking.&lt;/p&gt; 
&lt;p&gt;Find more information in our &lt;a href="https://raw.githubusercontent.com/wavetermdev/waveterm/main/CONTRIBUTING.md"&gt;Contributions Guide&lt;/a&gt;, which includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/wavetermdev/waveterm/main/CONTRIBUTING.md#contributing-to-wave-terminal"&gt;Ways to contribute&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/wavetermdev/waveterm/main/CONTRIBUTING.md#before-you-start"&gt;Contribution guidelines&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.waveterm.dev/storybook"&gt;Storybook&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Wave Terminal is licensed under the Apache-2.0 License. For more information on our dependencies, see &lt;a href="https://raw.githubusercontent.com/wavetermdev/waveterm/main/ACKNOWLEDGEMENTS.md"&gt;here&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>