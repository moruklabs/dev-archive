<rss version="2.0">
  <channel>
    <title>GitHub Python Weekly Trending</title>
    <description>Weekly Trending of Python in GitHub</description>
    <pubDate>Mon, 08 Dec 2025 01:47:08 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>comfyanonymous/ComfyUI</title>
      <link>https://github.com/comfyanonymous/ComfyUI</link>
      <description>&lt;p&gt;The most powerful and modular diffusion model GUI, api and backend with a graph/nodes interface.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;ComfyUI&lt;/h1&gt; 
 &lt;p&gt;&lt;strong&gt;The most powerful and modular visual AI engine and application.&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.comfy.org/"&gt;&lt;img src="https://img.shields.io/badge/ComfyOrg-4285F4?style=flat" alt="Website" /&gt;&lt;/a&gt; &lt;a href="https://www.comfy.org/discord"&gt;&lt;img src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fdiscord.com%2Fapi%2Finvites%2Fcomfyorg%3Fwith_counts%3Dtrue&amp;amp;query=%24.approximate_member_count&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;label=Discord&amp;amp;color=green&amp;amp;suffix=%20total" alt="Dynamic JSON Badge" /&gt;&lt;/a&gt; &lt;a href="https://x.com/ComfyUI"&gt;&lt;img src="https://img.shields.io/twitter/follow/ComfyUI" alt="Twitter" /&gt;&lt;/a&gt; &lt;a href="https://app.element.io/#/room/%23comfyui_space%3Amatrix.org"&gt;&lt;img src="https://img.shields.io/badge/Matrix-000000?style=flat&amp;amp;logo=matrix&amp;amp;logoColor=white" alt="Matrix" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/comfyanonymous/ComfyUI/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/comfyanonymous/ComfyUI?style=flat&amp;amp;sort=semver" alt="" /&gt;&lt;/a&gt; &lt;a href="https://github.com/comfyanonymous/ComfyUI/releases"&gt;&lt;img src="https://img.shields.io/github/release-date/comfyanonymous/ComfyUI?style=flat" alt="" /&gt;&lt;/a&gt; &lt;a href="https://github.com/comfyanonymous/ComfyUI/releases"&gt;&lt;img src="https://img.shields.io/github/downloads/comfyanonymous/ComfyUI/total?style=flat" alt="" /&gt;&lt;/a&gt; &lt;a href="https://github.com/comfyanonymous/ComfyUI/releases"&gt;&lt;img src="https://img.shields.io/github/downloads/comfyanonymous/ComfyUI/latest/total?style=flat&amp;amp;label=downloads%40latest" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;!-- Workaround to display total user from https://github.com/badges/shields/issues/4500#issuecomment-2060079995 --&gt; 
 &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/7ccaf2c1-9b72-41ae-9a89-5688c94b7abe" alt="ComfyUI Screenshot" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;ComfyUI lets you design and execute advanced stable diffusion pipelines using a graph/nodes/flowchart based interface. Available on Windows, Linux, and macOS.&lt;/p&gt; 
&lt;h2&gt;Get Started&lt;/h2&gt; 
&lt;h4&gt;&lt;a href="https://www.comfy.org/download"&gt;Desktop Application&lt;/a&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;The easiest way to get started.&lt;/li&gt; 
 &lt;li&gt;Available on Windows &amp;amp; macOS.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#installing"&gt;Windows Portable Package&lt;/a&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Get the latest commits and completely portable.&lt;/li&gt; 
 &lt;li&gt;Available on Windows.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#manual-install-windows-linux"&gt;Manual Install&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;Supports all operating systems and GPU types (NVIDIA, AMD, Intel, Apple Silicon, Ascend).&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/"&gt;Examples&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;See what ComfyUI can do with the &lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/"&gt;example workflows&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Nodes/graph/flowchart interface to experiment and create complex Stable Diffusion workflows without needing to code anything.&lt;/li&gt; 
 &lt;li&gt;Image Models 
  &lt;ul&gt; 
   &lt;li&gt;SD1.x, SD2.x (&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/unclip/"&gt;unCLIP&lt;/a&gt;)&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/sdxl/"&gt;SDXL&lt;/a&gt;, &lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/sdturbo/"&gt;SDXL Turbo&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/stable_cascade/"&gt;Stable Cascade&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/sd3/"&gt;SD3 and SD3.5&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Pixart Alpha and Sigma&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/aura_flow/"&gt;AuraFlow&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_dit/"&gt;HunyuanDiT&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/flux/"&gt;Flux&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/lumina2/"&gt;Lumina Image 2.0&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hidream/"&gt;HiDream&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/qwen_image/"&gt;Qwen Image&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_image/"&gt;Hunyuan Image 2.1&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/flux2/"&gt;Flux 2&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/z_image/"&gt;Z Image&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Image Editing Models 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/omnigen/"&gt;Omnigen 2&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/flux/#flux-kontext-image-editing-model"&gt;Flux Kontext&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hidream/#hidream-e11"&gt;HiDream E1.1&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/qwen_image/#edit-model"&gt;Qwen Image Edit&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Video Models 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/video/"&gt;Stable Video Diffusion&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/mochi/"&gt;Mochi&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/ltxv/"&gt;LTX-Video&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/"&gt;Hunyuan Video&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/wan/"&gt;Wan 2.1&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/wan22/"&gt;Wan 2.2&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://docs.comfy.org/tutorials/video/hunyuan/hunyuan-video-1-5"&gt;Hunyuan Video 1.5&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Audio Models 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/audio/"&gt;Stable Audio&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/audio/"&gt;ACE Step&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;3D Models 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://docs.comfy.org/tutorials/3d/hunyuan3D-2"&gt;Hunyuan3D 2.0&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Asynchronous Queue system&lt;/li&gt; 
 &lt;li&gt;Many optimizations: Only re-executes the parts of the workflow that changes between executions.&lt;/li&gt; 
 &lt;li&gt;Smart memory management: can automatically run large models on GPUs with as low as 1GB vram with smart offloading.&lt;/li&gt; 
 &lt;li&gt;Works even if you don't have a GPU with: &lt;code&gt;--cpu&lt;/code&gt; (slow)&lt;/li&gt; 
 &lt;li&gt;Can load ckpt and safetensors: All in one checkpoints or standalone diffusion models, VAEs and CLIP models.&lt;/li&gt; 
 &lt;li&gt;Safe loading of ckpt, pt, pth, etc.. files.&lt;/li&gt; 
 &lt;li&gt;Embeddings/Textual inversion&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/lora/"&gt;Loras (regular, locon and loha)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hypernetworks/"&gt;Hypernetworks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Loading full workflows (with seeds) from generated PNG, WebP and FLAC files.&lt;/li&gt; 
 &lt;li&gt;Saving/Loading workflows as Json files.&lt;/li&gt; 
 &lt;li&gt;Nodes interface can be used to create complex workflows like one for &lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/2_pass_txt2img/"&gt;Hires fix&lt;/a&gt; or much more advanced ones.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/area_composition/"&gt;Area Composition&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/inpaint/"&gt;Inpainting&lt;/a&gt; with both regular and inpainting models.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/controlnet/"&gt;ControlNet and T2I-Adapter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/upscale_models/"&gt;Upscale Models (ESRGAN, ESRGAN variants, SwinIR, Swin2SR, etc...)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/gligen/"&gt;GLIGEN&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/model_merging/"&gt;Model Merging&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/lcm/"&gt;LCM models and Loras&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Latent previews with &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#how-to-show-high-quality-previews"&gt;TAESD&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Works fully offline: core will never download anything unless you want to.&lt;/li&gt; 
 &lt;li&gt;Optional API nodes to use paid models from external providers through the online &lt;a href="https://docs.comfy.org/tutorials/api-nodes/overview"&gt;Comfy API&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/extra_model_paths.yaml.example"&gt;Config file&lt;/a&gt; to set the search paths for models.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Workflow examples can be found on the &lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/"&gt;Examples page&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Release Process&lt;/h2&gt; 
&lt;p&gt;ComfyUI follows a weekly release cycle targeting Monday but this regularly changes because of model releases or large changes to the codebase. There are three interconnected repositories:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI"&gt;ComfyUI Core&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Releases a new stable version (e.g., v0.7.0) roughly every week.&lt;/li&gt; 
   &lt;li&gt;Commits outside of the stable release tags may be very unstable and break many custom nodes.&lt;/li&gt; 
   &lt;li&gt;Serves as the foundation for the desktop release&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/Comfy-Org/desktop"&gt;ComfyUI Desktop&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Builds a new release using the latest stable core version&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/Comfy-Org/ComfyUI_frontend"&gt;ComfyUI Frontend&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Weekly frontend updates are merged into the core repository&lt;/li&gt; 
   &lt;li&gt;Features are frozen for the upcoming core release&lt;/li&gt; 
   &lt;li&gt;Development continues for the next release cycle&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Shortcuts&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Keybind&lt;/th&gt; 
   &lt;th&gt;Explanation&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Enter&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Queue up current graph for generation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Shift&lt;/code&gt; + &lt;code&gt;Enter&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Queue up current graph as first for generation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Alt&lt;/code&gt; + &lt;code&gt;Enter&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Cancel current generation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Z&lt;/code&gt;/&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Y&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Undo/Redo&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;S&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Save workflow&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;O&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Load workflow&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;A&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Select all nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Alt &lt;/code&gt;+ &lt;code&gt;C&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Collapse/uncollapse selected nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;M&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Mute/unmute selected nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;B&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Bypass selected nodes (acts like the node was removed from the graph and the wires reconnected through)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Delete&lt;/code&gt;/&lt;code&gt;Backspace&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Delete selected nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Backspace&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Delete the current graph&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Space&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Move the canvas around when held and moving the cursor&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt;/&lt;code&gt;Shift&lt;/code&gt; + &lt;code&gt;Click&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Add clicked node to selection&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;C&lt;/code&gt;/&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;V&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Copy and paste selected nodes (without maintaining connections to outputs of unselected nodes)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;C&lt;/code&gt;/&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Shift&lt;/code&gt; + &lt;code&gt;V&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Copy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Shift&lt;/code&gt; + &lt;code&gt;Drag&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Move multiple selected nodes at the same time&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;D&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Load default graph&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Alt&lt;/code&gt; + &lt;code&gt;+&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Canvas Zoom in&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Alt&lt;/code&gt; + &lt;code&gt;-&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Canvas Zoom out&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Shift&lt;/code&gt; + LMB + Vertical drag&lt;/td&gt; 
   &lt;td&gt;Canvas Zoom in/out&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;P&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Pin/Unpin selected nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;G&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Group selected nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Q&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Toggle visibility of the queue&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;H&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Toggle visibility of history&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;R&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Refresh graph&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;F&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Show/Hide menu&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;.&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Fit view to selection (Whole graph when nothing is selected)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Double-Click LMB&lt;/td&gt; 
   &lt;td&gt;Open node quick search palette&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Shift&lt;/code&gt; + Drag&lt;/td&gt; 
   &lt;td&gt;Move multiple wires at once&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Alt&lt;/code&gt; + LMB&lt;/td&gt; 
   &lt;td&gt;Disconnect all wires from clicked slot&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;code&gt;Ctrl&lt;/code&gt; can also be replaced with &lt;code&gt;Cmd&lt;/code&gt; instead for macOS users&lt;/p&gt; 
&lt;h1&gt;Installing&lt;/h1&gt; 
&lt;h2&gt;Windows Portable&lt;/h2&gt; 
&lt;p&gt;There is a portable standalone build for Windows that should work for running on Nvidia GPUs or for running on your CPU only on the &lt;a href="https://github.com/comfyanonymous/ComfyUI/releases"&gt;releases page&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia.7z"&gt;Direct link to download&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Simply download, extract with &lt;a href="https://7-zip.org"&gt;7-Zip&lt;/a&gt; or with the windows explorer on recent windows versions and run. For smaller models you normally only need to put the checkpoints (the huge ckpt/safetensors files) in: ComfyUI\models\checkpoints but many of the larger models have multiple files. Make sure to follow the instructions to know which subfolder to put them in ComfyUI\models\&lt;/p&gt; 
&lt;p&gt;If you have trouble extracting it, right click the file -&amp;gt; properties -&amp;gt; unblock&lt;/p&gt; 
&lt;p&gt;Update your Nvidia drivers if it doesn't start.&lt;/p&gt; 
&lt;h4&gt;Alternative Downloads:&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_amd.7z"&gt;Experimental portable for AMD GPUs&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia_cu128.7z"&gt;Portable with pytorch cuda 12.8 and python 3.12&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia_cu126.7z"&gt;Portable with pytorch cuda 12.6 and python 3.12&lt;/a&gt; (Supports Nvidia 10 series and older GPUs).&lt;/p&gt; 
&lt;h4&gt;How do I share models between another UI and ComfyUI?&lt;/h4&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/extra_model_paths.yaml.example"&gt;Config file&lt;/a&gt; to set the search paths for models. In the standalone windows build you can find this file in the ComfyUI directory. Rename this file to extra_model_paths.yaml and edit it with your favorite text editor.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://docs.comfy.org/comfy-cli/getting-started"&gt;comfy-cli&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;You can install and start ComfyUI using comfy-cli:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install comfy-cli
comfy install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Manual Install (Windows, Linux)&lt;/h2&gt; 
&lt;p&gt;Python 3.14 works but you may encounter issues with the torch compile node. The free threaded variant is still missing some dependencies.&lt;/p&gt; 
&lt;p&gt;Python 3.13 is very well supported. If you have trouble with some custom node dependencies on 3.13 you can try 3.12&lt;/p&gt; 
&lt;h3&gt;Instructions:&lt;/h3&gt; 
&lt;p&gt;Git clone this repo.&lt;/p&gt; 
&lt;p&gt;Put your SD checkpoints (the huge ckpt/safetensors files) in: models/checkpoints&lt;/p&gt; 
&lt;p&gt;Put your VAE in: models/vae&lt;/p&gt; 
&lt;h3&gt;AMD GPUs (Linux)&lt;/h3&gt; 
&lt;p&gt;AMD users can install rocm and pytorch with pip if you don't have it already installed, this is the command to install the stable version:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.4&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;This is the command to install the nightly with ROCm 7.0 which might have some performance improvements:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm7.1&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;AMD GPUs (Experimental: Windows and Linux), RDNA 3, 3.5 and 4 only.&lt;/h3&gt; 
&lt;p&gt;These have less hardware support than the builds above but they work on windows. You also need to install the pytorch version specific to your hardware.&lt;/p&gt; 
&lt;p&gt;RDNA 3 (RX 7000 series):&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install --pre torch torchvision torchaudio --index-url https://rocm.nightlies.amd.com/v2/gfx110X-dgpu/&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;RDNA 3.5 (Strix halo/Ryzen AI Max+ 365):&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install --pre torch torchvision torchaudio --index-url https://rocm.nightlies.amd.com/v2/gfx1151/&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;RDNA 4 (RX 9000 series):&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install --pre torch torchvision torchaudio --index-url https://rocm.nightlies.amd.com/v2/gfx120X-all/&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Intel GPUs (Windows and Linux)&lt;/h3&gt; 
&lt;p&gt;Intel Arc GPU users can install native PyTorch with torch.xpu support using pip. More information can be found &lt;a href="https://pytorch.org/docs/main/notes/get_start_xpu.html"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;To install PyTorch xpu, use the following command:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;code&gt;pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/xpu&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;This is the command to install the Pytorch xpu nightly which might have some performance improvements:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/xpu&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;NVIDIA&lt;/h3&gt; 
&lt;p&gt;Nvidia users should install stable pytorch using this command:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu130&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;This is the command to install pytorch nightly instead which might have performance improvements.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu130&lt;/code&gt;&lt;/p&gt; 
&lt;h4&gt;Troubleshooting&lt;/h4&gt; 
&lt;p&gt;If you get the "Torch not compiled with CUDA enabled" error, uninstall torch with:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip uninstall torch&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;And install it again with the command above.&lt;/p&gt; 
&lt;h3&gt;Dependencies&lt;/h3&gt; 
&lt;p&gt;Install the dependencies by opening your terminal inside the ComfyUI folder and:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;After this you should have everything installed and can proceed to running ComfyUI.&lt;/p&gt; 
&lt;h3&gt;Others:&lt;/h3&gt; 
&lt;h4&gt;Apple Mac silicon&lt;/h4&gt; 
&lt;p&gt;You can install ComfyUI in Apple Mac silicon (M1 or M2) with any recent macOS version.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install pytorch nightly. For instructions, read the &lt;a href="https://developer.apple.com/metal/pytorch/"&gt;Accelerated PyTorch training on Mac&lt;/a&gt; Apple Developer guide (make sure to install the latest pytorch nightly).&lt;/li&gt; 
 &lt;li&gt;Follow the &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#manual-install-windows-linux"&gt;ComfyUI manual installation&lt;/a&gt; instructions for Windows and Linux.&lt;/li&gt; 
 &lt;li&gt;Install the ComfyUI &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#dependencies"&gt;dependencies&lt;/a&gt;. If you have another Stable Diffusion UI &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#i-already-have-another-ui-for-stable-diffusion-installed-do-i-really-have-to-install-all-of-these-dependencies"&gt;you might be able to reuse the dependencies&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Launch ComfyUI by running &lt;code&gt;python main.py&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Remember to add your models, VAE, LoRAs etc. to the corresponding Comfy folders, as discussed in &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#manual-install-windows-linux"&gt;ComfyUI manual installation&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Ascend NPUs&lt;/h4&gt; 
&lt;p&gt;For models compatible with Ascend Extension for PyTorch (torch_npu). To get started, ensure your environment meets the prerequisites outlined on the &lt;a href="https://ascend.github.io/docs/sources/ascend/quick_install.html"&gt;installation&lt;/a&gt; page. Here's a step-by-step guide tailored to your platform and installation method:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Begin by installing the recommended or newer kernel version for Linux as specified in the Installation page of torch-npu, if necessary.&lt;/li&gt; 
 &lt;li&gt;Proceed with the installation of Ascend Basekit, which includes the driver, firmware, and CANN, following the instructions provided for your specific platform.&lt;/li&gt; 
 &lt;li&gt;Next, install the necessary packages for torch-npu by adhering to the platform-specific instructions on the &lt;a href="https://ascend.github.io/docs/sources/pytorch/install.html#pytorch"&gt;Installation&lt;/a&gt; page.&lt;/li&gt; 
 &lt;li&gt;Finally, adhere to the &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#manual-install-windows-linux"&gt;ComfyUI manual installation&lt;/a&gt; guide for Linux. Once all components are installed, you can run ComfyUI as described earlier.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Cambricon MLUs&lt;/h4&gt; 
&lt;p&gt;For models compatible with Cambricon Extension for PyTorch (torch_mlu). Here's a step-by-step guide tailored to your platform and installation method:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install the Cambricon CNToolkit by adhering to the platform-specific instructions on the &lt;a href="https://www.cambricon.com/docs/sdk_1.15.0/cntoolkit_3.7.2/cntoolkit_install_3.7.2/index.html"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Next, install the PyTorch(torch_mlu) following the instructions on the &lt;a href="https://www.cambricon.com/docs/sdk_1.15.0/cambricon_pytorch_1.17.0/user_guide_1.9/index.html"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Launch ComfyUI by running &lt;code&gt;python main.py&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Iluvatar Corex&lt;/h4&gt; 
&lt;p&gt;For models compatible with Iluvatar Extension for PyTorch. Here's a step-by-step guide tailored to your platform and installation method:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install the Iluvatar Corex Toolkit by adhering to the platform-specific instructions on the &lt;a href="https://support.iluvatar.com/#/DocumentCentre?id=1&amp;amp;nameCenter=2&amp;amp;productId=520117912052801536"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Launch ComfyUI by running &lt;code&gt;python main.py&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;&lt;a href="https://github.com/Comfy-Org/ComfyUI-Manager/tree/manager-v4"&gt;ComfyUI-Manager&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;ComfyUI-Manager&lt;/strong&gt; is an extension that allows you to easily install, update, and manage custom nodes for ComfyUI.&lt;/p&gt; 
&lt;h3&gt;Setup&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install the manager dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r manager_requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Enable the manager with the &lt;code&gt;--enable-manager&lt;/code&gt; flag when running ComfyUI:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --enable-manager
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Command Line Options&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Flag&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--enable-manager&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enable ComfyUI-Manager&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--enable-manager-legacy-ui&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Use the legacy manager UI instead of the new UI (requires &lt;code&gt;--enable-manager&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--disable-manager-ui&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disable the manager UI and endpoints while keeping background features like security checks and scheduled installation completion (requires &lt;code&gt;--enable-manager&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Running&lt;/h1&gt; 
&lt;p&gt;&lt;code&gt;python main.py&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;For AMD cards not officially supported by ROCm&lt;/h3&gt; 
&lt;p&gt;Try running it with this command if you have issues:&lt;/p&gt; 
&lt;p&gt;For 6700, 6600 and maybe other RDNA2 or older: &lt;code&gt;HSA_OVERRIDE_GFX_VERSION=10.3.0 python main.py&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;For AMD 7600 and maybe other RDNA3 cards: &lt;code&gt;HSA_OVERRIDE_GFX_VERSION=11.0.0 python main.py&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;AMD ROCm Tips&lt;/h3&gt; 
&lt;p&gt;You can enable experimental memory efficient attention on recent pytorch in ComfyUI on some AMD GPUs using this command, it should already be enabled by default on RDNA3. If this improves speed for you on latest pytorch on your GPU please report it so that I can enable it by default.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1 python main.py --use-pytorch-cross-attention&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;You can also try setting this env variable &lt;code&gt;PYTORCH_TUNABLEOP_ENABLED=1&lt;/code&gt; which might speed things up at the cost of a very slow initial run.&lt;/p&gt; 
&lt;h1&gt;Notes&lt;/h1&gt; 
&lt;p&gt;Only parts of the graph that have an output with all the correct inputs will be executed.&lt;/p&gt; 
&lt;p&gt;Only parts of the graph that change from each execution to the next will be executed, if you submit the same graph twice only the first will be executed. If you change the last part of the graph only the part you changed and the part that depends on it will be executed.&lt;/p&gt; 
&lt;p&gt;Dragging a generated png on the webpage or loading one will give you the full workflow including seeds that were used to create it.&lt;/p&gt; 
&lt;p&gt;You can use () to change emphasis of a word or phrase like: (good code:1.2) or (bad code:0.8). The default emphasis for () is 1.1. To use () characters in your actual prompt escape them like \( or \).&lt;/p&gt; 
&lt;p&gt;You can use {day|night}, for wildcard/dynamic prompts. With this syntax "{wild|card|test}" will be randomly replaced by either "wild", "card" or "test" by the frontend every time you queue the prompt. To use {} characters in your actual prompt escape them like: \{ or \}.&lt;/p&gt; 
&lt;p&gt;Dynamic prompts also support C-style comments, like &lt;code&gt;// comment&lt;/code&gt; or &lt;code&gt;/* comment */&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;To use a textual inversion concepts/embeddings in a text prompt put them in the models/embeddings directory and use them in the CLIPTextEncode node like this (you can omit the .pt extension):&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;embedding:embedding_filename.pt&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;How to show high-quality previews?&lt;/h2&gt; 
&lt;p&gt;Use &lt;code&gt;--preview-method auto&lt;/code&gt; to enable previews.&lt;/p&gt; 
&lt;p&gt;The default installation includes a fast latent preview method that's low-resolution. To enable higher-quality previews with &lt;a href="https://github.com/madebyollin/taesd"&gt;TAESD&lt;/a&gt;, download the &lt;a href="https://github.com/madebyollin/taesd/"&gt;taesd_decoder.pth, taesdxl_decoder.pth, taesd3_decoder.pth and taef1_decoder.pth&lt;/a&gt; and place them in the &lt;code&gt;models/vae_approx&lt;/code&gt; folder. Once they're installed, restart ComfyUI and launch it with &lt;code&gt;--preview-method taesd&lt;/code&gt; to enable high-quality previews.&lt;/p&gt; 
&lt;h2&gt;How to use TLS/SSL?&lt;/h2&gt; 
&lt;p&gt;Generate a self-signed certificate (not appropriate for shared/production use) and key by running the command: &lt;code&gt;openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -sha256 -days 3650 -nodes -subj "/C=XX/ST=StateName/L=CityName/O=CompanyName/OU=CompanySectionName/CN=CommonNameOrHostname"&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Use &lt;code&gt;--tls-keyfile key.pem --tls-certfile cert.pem&lt;/code&gt; to enable TLS/SSL, the app will now be accessible with &lt;code&gt;https://...&lt;/code&gt; instead of &lt;code&gt;http://...&lt;/code&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: Windows users can use &lt;a href="https://github.com/alexisrolland/docker-openssl"&gt;alexisrolland/docker-openssl&lt;/a&gt; or one of the &lt;a href="https://wiki.openssl.org/index.php/Binaries"&gt;3rd party binary distributions&lt;/a&gt; to run the command example above. &lt;br /&gt;&lt;br /&gt;If you use a container, note that the volume mount &lt;code&gt;-v&lt;/code&gt; can be a relative path so &lt;code&gt;... -v ".\:/openssl-certs" ...&lt;/code&gt; would create the key &amp;amp; cert files in the current directory of your command prompt or powershell terminal.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Support and dev channel&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://comfy.org/discord"&gt;Discord&lt;/a&gt;: Try the #help or #feedback channels.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://app.element.io/#/room/%23comfyui_space%3Amatrix.org"&gt;Matrix space: #comfyui_space:matrix.org&lt;/a&gt; (it's like discord but open source).&lt;/p&gt; 
&lt;p&gt;See also: &lt;a href="https://www.comfy.org/"&gt;https://www.comfy.org/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Frontend Development&lt;/h2&gt; 
&lt;p&gt;As of August 15, 2024, we have transitioned to a new frontend, which is now hosted in a separate repository: &lt;a href="https://github.com/Comfy-Org/ComfyUI_frontend"&gt;ComfyUI Frontend&lt;/a&gt;. This repository now hosts the compiled JS (from TS/Vue) under the &lt;code&gt;web/&lt;/code&gt; directory.&lt;/p&gt; 
&lt;h3&gt;Reporting Issues and Requesting Features&lt;/h3&gt; 
&lt;p&gt;For any bugs, issues, or feature requests related to the frontend, please use the &lt;a href="https://github.com/Comfy-Org/ComfyUI_frontend"&gt;ComfyUI Frontend repository&lt;/a&gt;. This will help us manage and address frontend-specific concerns more efficiently.&lt;/p&gt; 
&lt;h3&gt;Using the Latest Frontend&lt;/h3&gt; 
&lt;p&gt;The new frontend is now the default for ComfyUI. However, please note:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;The frontend in the main ComfyUI repository is updated fortnightly.&lt;/li&gt; 
 &lt;li&gt;Daily releases are available in the separate frontend repository.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;To use the most up-to-date frontend version:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;For the latest daily release, launch ComfyUI with this command line argument:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;--front-end-version Comfy-Org/ComfyUI_frontend@latest
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;For a specific version, replace &lt;code&gt;latest&lt;/code&gt; with the desired version number:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;--front-end-version Comfy-Org/ComfyUI_frontend@1.2.2
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This approach allows you to easily switch between the stable fortnightly release and the cutting-edge daily updates, or even specific versions for testing purposes.&lt;/p&gt; 
&lt;h3&gt;Accessing the Legacy Frontend&lt;/h3&gt; 
&lt;p&gt;If you need to use the legacy frontend for any reason, you can access it using the following command line argument:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;--front-end-version Comfy-Org/ComfyUI_legacy_frontend@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will use a snapshot of the legacy frontend preserved in the &lt;a href="https://github.com/Comfy-Org/ComfyUI_legacy_frontend"&gt;ComfyUI Legacy Frontend repository&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;QA&lt;/h1&gt; 
&lt;h3&gt;Which GPU should I buy for this?&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI/wiki/Which-GPU-should-I-buy-for-ComfyUI"&gt;See this page for some recommendations&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>GoogleCloudPlatform/agent-starter-pack</title>
      <link>https://github.com/GoogleCloudPlatform/agent-starter-pack</link>
      <description>&lt;p&gt;Ship AI Agents to Google Cloud in minutes, not months. Production-ready templates with built-in CI/CD, evaluation, and observability.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üöÄ Agent Starter Pack&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/pypi/v/agent-starter-pack?color=blue" alt="Version" /&gt; &lt;a href="https://youtu.be/jHt-ZVD660g"&gt;&lt;img src="https://img.shields.io/badge/1--Minute%20Overview-gray" alt="1-Minute Video Overview" /&gt;&lt;/a&gt; &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/"&gt;&lt;img src="https://img.shields.io/badge/Documentation-gray" alt="Docs" /&gt;&lt;/a&gt; &lt;a href="https://studio.firebase.google.com/new?template=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fagent-starter-pack%2Ftree%2Fmain%2Fagent_starter_pack%2Fresources%2Fidx"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://cdn.firebasestudio.dev/btn/try_light_20.svg" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="https://cdn.firebasestudio.dev/btn/try_dark_20.svg" /&gt; 
   &lt;img height="20" alt="Try in Firebase Studio" src="https://cdn.firebasestudio.dev/btn/try_blue_20.svg?sanitize=true" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;a href="https://shell.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Feliasecchig%2Fasp-open-in-cloud-shell&amp;amp;cloudshell_print=open-in-cs"&gt;&lt;img src="https://img.shields.io/badge/Launch-in_Cloud_Shell-white" alt="Launch in Cloud Shell" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/stars/GoogleCloudPlatform/agent-starter-pack?color=yellow" alt="Stars" /&gt;&lt;/p&gt; 
&lt;p&gt;A Python package that provides &lt;strong&gt;production-ready templates&lt;/strong&gt; for GenAI agents on Google Cloud.&lt;/p&gt; 
&lt;p&gt;Focus on your agent logic‚Äîthe starter pack provides everything else: infrastructure, CI/CD, observability, and security.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;‚ö°Ô∏è Launch&lt;/th&gt; 
   &lt;th&gt;üß™ Experiment&lt;/th&gt; 
   &lt;th&gt;‚úÖ Deploy&lt;/th&gt; 
   &lt;th&gt;üõ†Ô∏è Customize&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/GoogleCloudPlatform/agent-starter-pack/main/agent_starter_pack/agents/"&gt;Pre-built agent templates&lt;/a&gt; (ReAct, RAG, multi-agent, Live API).&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview"&gt;Vertex AI evaluation&lt;/a&gt; and an interactive playground.&lt;/td&gt; 
   &lt;td&gt;Production-ready infra with &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/observability"&gt;monitoring, observability&lt;/a&gt;, and &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/deployment"&gt;CI/CD&lt;/a&gt; on &lt;a href="https://cloud.google.com/run"&gt;Cloud Run&lt;/a&gt; or &lt;a href="https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview"&gt;Agent Engine&lt;/a&gt;.&lt;/td&gt; 
   &lt;td&gt;Extend and customize templates according to your needs. üÜï Now integrating with &lt;a href="https://github.com/google-gemini/gemini-cli"&gt;Gemini CLI&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚ö° Get Started in 1 Minute&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;From zero to production-ready agent in 60 seconds using &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;&lt;code&gt;uv&lt;/code&gt;&lt;/a&gt;:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx agent-starter-pack create my-awesome-agent
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt; ‚ú® Alternative: Using pip&lt;/summary&gt; 
 &lt;p&gt;If you don't have &lt;a href="https://github.com/astral-sh/uv"&gt;&lt;code&gt;uv&lt;/code&gt;&lt;/a&gt; installed, you can use pip:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Create and activate a Python virtual environment
python -m venv .venv &amp;amp;&amp;amp; source .venv/bin/activate

# Install the agent starter pack
pip install --upgrade agent-starter-pack

# Create a new agent project
agent-starter-pack create my-awesome-agent
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;strong&gt;That's it!&lt;/strong&gt; You now have a fully functional agent project‚Äîcomplete with backend, frontend, and deployment infrastructure‚Äîready for you to explore and customize.&lt;/p&gt; 
&lt;h3&gt;üîß Enhance Existing Agents&lt;/h3&gt; 
&lt;p&gt;Already have an agent? Add production-ready deployment and infrastructure by running this command in your project's root folder:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx agent-starter-pack enhance
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/installation"&gt;Installation Guide&lt;/a&gt; for more options, or try with zero setup in &lt;a href="https://studio.firebase.google.com/new?template=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fagent-starter-pack%2Ftree%2Fmain%2Fsrc%2Fresources%2Fidx"&gt;Firebase Studio&lt;/a&gt; or &lt;a href="https://shell.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Feliasecchig%2Fasp-open-in-cloud-shell&amp;amp;cloudshell_print=open-in-cs"&gt;Cloud Shell&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§ñ Agents&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Agent Name&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;adk_base&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A base ReAct agent implemented using Google's &lt;a href="https://github.com/google/adk-python"&gt;Agent Development Kit&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;adk_a2a_base&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;An ADK agent with &lt;a href="https://a2a-protocol.org/"&gt;Agent2Agent (A2A) Protocol&lt;/a&gt; support for distributed agent communication and interoperability&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;agentic_rag&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A RAG agent for document retrieval and Q&amp;amp;A. Supporting &lt;a href="https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction"&gt;Vertex AI Search&lt;/a&gt; and &lt;a href="https://cloud.google.com/vertex-ai/docs/vector-search/overview"&gt;Vector Search&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;langgraph_base&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A base ReAct agent implemented using LangChain's &lt;a href="https://github.com/langchain-ai/langgraph"&gt;LangGraph&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;adk_live&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A real-time multimodal RAG agent powered by Gemini, supporting audio/video/text chat&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;More agents are on the way!&lt;/strong&gt; We are continuously expanding our &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/agents/overview"&gt;agent library&lt;/a&gt;. Have a specific agent type in mind? &lt;a href="https://github.com/GoogleCloudPlatform/agent-starter-pack/issues/new?labels=enhancement"&gt;Raise an issue as a feature request!&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;üîç ADK Samples&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Looking to explore more ADK examples? Check out the &lt;a href="https://github.com/google/adk-samples"&gt;ADK Samples Repository&lt;/a&gt; for additional examples and use cases demonstrating ADK's capabilities.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üåü Community Showcase&lt;/h2&gt; 
&lt;p&gt;Explore amazing projects built with the Agent Starter Pack!&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/community-showcase"&gt;View Community Showcase ‚Üí&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;agent-starter-pack&lt;/code&gt; offers key features to accelerate and simplify the development of your agent:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üîÑ &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/cli/setup_cicd"&gt;CI/CD Automation&lt;/a&gt;&lt;/strong&gt; - A single command to set up a complete CI/CD pipeline for all environments, supporting both &lt;strong&gt;Google Cloud Build&lt;/strong&gt; and &lt;strong&gt;GitHub Actions&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üì• &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/data-ingestion"&gt;Data Pipeline for RAG with Terraform/CI-CD&lt;/a&gt;&lt;/strong&gt; - Seamlessly integrate a data pipeline to process embeddings for RAG into your agent system. Supporting &lt;a href="https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction"&gt;Vertex AI Search&lt;/a&gt; and &lt;a href="https://cloud.google.com/vertex-ai/docs/vector-search/overview"&gt;Vector Search&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/GoogleCloudPlatform/agent-starter-pack/main/docs/guide/remote-templating.md"&gt;Remote Templates&lt;/a&gt;&lt;/strong&gt;: Create and share your own agent starter packs templates from any Git repository.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ü§ñ Gemini CLI Integration&lt;/strong&gt; - Use the &lt;a href="https://github.com/google-gemini/gemini-cli"&gt;Gemini CLI&lt;/a&gt; and the included &lt;code&gt;GEMINI.md&lt;/code&gt; context file to ask questions about your template, agent architecture, and the path to production. Get instant guidance and code examples directly in your terminal.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;High-Level Architecture&lt;/h2&gt; 
&lt;p&gt;This starter pack covers all aspects of Agent development, from prototyping and evaluation to deployment and monitoring.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/GoogleCloudPlatform/agent-starter-pack/main/docs/images/ags_high_level_architecture.png" alt="High Level Architecture" title="Architecture" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üîß Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cloud.google.com/sdk/docs/install"&gt;Google Cloud SDK&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://developer.hashicorp.com/terraform/downloads"&gt;Terraform&lt;/a&gt; (for deployment)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.gnu.org/software/make/"&gt;Make&lt;/a&gt; (for development tasks)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìö Documentation&lt;/h2&gt; 
&lt;p&gt;Visit our &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/"&gt;documentation site&lt;/a&gt; for comprehensive guides and references!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/getting-started"&gt;Getting Started Guide&lt;/a&gt; - First steps with agent-starter-pack&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/installation"&gt;Installation Guide&lt;/a&gt; - Setting up your environment&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/deployment"&gt;Deployment Guide&lt;/a&gt; - Taking your agent to production&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/agents/overview"&gt;Agent Templates Overview&lt;/a&gt; - Explore available agent patterns&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/cli/"&gt;CLI Reference&lt;/a&gt; - Command-line tool documentation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Video Walkthrough:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=9zqwym-N3lg"&gt;Exploring the Agent Starter Pack&lt;/a&gt;&lt;/strong&gt;: A comprehensive tutorial demonstrating how to rapidly deploy AI Agents using the Agent Starter Pack, covering architecture, templates, and step-by-step deployment.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/live/eZ-8UQ_t4YM?feature=shared&amp;amp;t=2791"&gt;6-minute introduction&lt;/a&gt;&lt;/strong&gt; (April 2024): Explaining the Agent Starter Pack and demonstrating its key features. Part of the Kaggle GenAI intensive course.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Looking for more examples and resources for Generative AI on Google Cloud? Check out the &lt;a href="https://github.com/GoogleCloudPlatform/generative-ai"&gt;GoogleCloudPlatform/generative-ai&lt;/a&gt; repository for notebooks, code samples, and more!&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! See the &lt;a href="https://raw.githubusercontent.com/GoogleCloudPlatform/agent-starter-pack/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Feedback&lt;/h2&gt; 
&lt;p&gt;We value your input! Your feedback helps us improve this starter pack and make it more useful for the community.&lt;/p&gt; 
&lt;h3&gt;Getting Help&lt;/h3&gt; 
&lt;p&gt;If you encounter any issues or have specific suggestions, please first consider &lt;a href="https://github.com/GoogleCloudPlatform/generative-ai/issues"&gt;raising an issue&lt;/a&gt; on our GitHub repository.&lt;/p&gt; 
&lt;h3&gt;Share Your Experience&lt;/h3&gt; 
&lt;p&gt;For other types of feedback, or if you'd like to share a positive experience or success story using this starter pack, we'd love to hear from you! You can reach out to us at &lt;a href="mailto:agent-starter-pack@google.com"&gt;&lt;/a&gt;&lt;a href="mailto:agent-starter-pack@google.com"&gt;agent-starter-pack@google.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Thank you for your contributions!&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This repository is for demonstrative purposes only and is not an officially supported Google product.&lt;/p&gt; 
&lt;h2&gt;Terms of Service&lt;/h2&gt; 
&lt;p&gt;The agent-starter-pack templating CLI and the templates in this starter pack leverage Google Cloud APIs. When you use this starter pack, you'll be deploying resources in your own Google Cloud project and will be responsible for those resources. Please review the &lt;a href="https://cloud.google.com/terms/service-terms"&gt;Google Cloud Service Terms&lt;/a&gt; for details on the terms of service associated with these APIs.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>anthropics/claude-quickstarts</title>
      <link>https://github.com/anthropics/claude-quickstarts</link>
      <description>&lt;p&gt;A collection of projects designed to help developers quickly get started with building deployable applications using the Claude API&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Claude Quickstarts&lt;/h1&gt; 
&lt;p&gt;Claude Quickstarts is a collection of projects designed to help developers quickly get started with building applications using the Claude API. Each quickstart provides a foundation that you can easily build upon and customize for your specific needs.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;To use these quickstarts, you'll need an Claude API key. If you don't have one yet, you can sign up for free at &lt;a href="https://console.anthropic.com"&gt;console.anthropic.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Available Quickstarts&lt;/h2&gt; 
&lt;h3&gt;Customer Support Agent&lt;/h3&gt; 
&lt;p&gt;A customer support agent powered by Claude. This project demonstrates how to leverage Claude's natural language understanding and generation capabilities to create an AI-assisted customer support system with access to a knowledge base.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/claude-quickstarts/main/customer-support-agent"&gt;Go to Customer Support Agent Quickstart&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Financial Data Analyst&lt;/h3&gt; 
&lt;p&gt;A financial data analyst powered by Claude. This project demonstrates how to leverage Claude's capabilities with interactive data visualization to analyze financial data via chat.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/claude-quickstarts/main/financial-data-analyst"&gt;Go to Financial Data Analyst Quickstart&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Computer Use Demo&lt;/h3&gt; 
&lt;p&gt;An environment and tools that Claude can use to control a desktop computer. This project demonstrates how to leverage the computer use capabilities of Claude, including support for the latest &lt;code&gt;computer_use_20251124&lt;/code&gt; tool version with zoom actions.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/claude-quickstarts/main/computer-use-demo"&gt;Go to Computer Use Demo Quickstart&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Autonomous Coding Agent&lt;/h3&gt; 
&lt;p&gt;An autonomous coding agent powered by the Claude Agent SDK. This project demonstrates a two-agent pattern (initializer + coding agent) that can build complete applications over multiple sessions, with progress persisted via git and a feature list that the agent works through incrementally.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/claude-quickstarts/main/autonomous-coding"&gt;Go to Autonomous Coding Agent Quickstart&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;General Usage&lt;/h2&gt; 
&lt;p&gt;Each quickstart project comes with its own README and setup instructions. Generally, you'll follow these steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone this repository&lt;/li&gt; 
 &lt;li&gt;Navigate to the specific quickstart directory&lt;/li&gt; 
 &lt;li&gt;Install the required dependencies&lt;/li&gt; 
 &lt;li&gt;Set up your Claude API key as an environment variable&lt;/li&gt; 
 &lt;li&gt;Run the quickstart application&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Explore Further&lt;/h2&gt; 
&lt;p&gt;To deepen your understanding of working with Claude and the Claude API, check out these resources:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.claude.com"&gt;Claude API Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/claude-cookbooks"&gt;Claude Cookbooks&lt;/a&gt; - A collection of code snippets and guides for common tasks&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/courses/tree/master/anthropic_api_fundamentals"&gt;Claude API Fundamentals Course&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions to the Claude Quickstarts repository! If you have ideas for new quickstart projects or improvements to existing ones, please open an issue or submit a pull request.&lt;/p&gt; 
&lt;h2&gt;Community and Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Join our &lt;a href="https://www.anthropic.com/discord"&gt;Anthropic Discord community&lt;/a&gt; for discussions and support&lt;/li&gt; 
 &lt;li&gt;Check out the &lt;a href="https://support.anthropic.com"&gt;Anthropic support documentation&lt;/a&gt; for additional help&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/anthropics/claude-quickstarts/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>QuentinFuxa/WhisperLiveKit</title>
      <link>https://github.com/QuentinFuxa/WhisperLiveKit</link>
      <description>&lt;p&gt;Simultaneous speech-to-text model&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt;WLK&lt;/h1&gt; 
&lt;p align="center"&gt;&lt;b&gt;WhisperLiveKit: Ultra-low-latency, self-hosted speech-to-text with speaker identification&lt;/b&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/QuentinFuxa/WhisperLiveKit/refs/heads/main/demo.png" alt="WhisperLiveKit Demo" width="730" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://pypi.org/project/whisperlivekit/"&gt;&lt;img alt="PyPI Version" src="https://img.shields.io/pypi/v/whisperlivekit?color=g" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/whisperlivekit"&gt;&lt;img alt="PyPI Downloads" src="https://static.pepy.tech/personalized-badge/whisperlivekit?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=brightgreen&amp;amp;left_text=installations" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/whisperlivekit/"&gt;&lt;img alt="Python Versions" src="https://img.shields.io/badge/python-3.9--3.15-dark_green" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/qfuxa/whisper-base-french-lora"&gt; &lt;img alt="Hugging Face Weights" src="https://img.shields.io/badge/ü§ó-Hugging%20Face%20Weights-yellow" /&gt; &lt;/a&gt; &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/LICENSE"&gt;&lt;img alt="License" src="https://img.shields.io/badge/License-Apache 2.0-dark_green" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h4&gt;Powered by Leading Research:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Simul-&lt;a href="https://arxiv.org/pdf/2406.10052"&gt;Whisper&lt;/a&gt;/&lt;a href="https://arxiv.org/abs/2506.17077"&gt;Streaming&lt;/a&gt; (SOTA 2025) - Ultra-low latency transcription using &lt;a href="https://arxiv.org/pdf/2305.11408"&gt;AlignAtt policy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/QuentinFuxa/NoLanguageLeftWaiting"&gt;NLLW&lt;/a&gt; (2025), based on &lt;a href="https://huggingface.co/entai2965/nllb-200-distilled-600M-ctranslate2"&gt;distilled&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2207.04672"&gt;NLLB&lt;/a&gt; (2022, 2024) - Simulatenous translation from &amp;amp; to 200 languages.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ufal/whisper_streaming"&gt;WhisperStreaming&lt;/a&gt; (SOTA 2023) - Low latency transcription using &lt;a href="https://www.isca-archive.org/interspeech_2020/liu20s_interspeech.pdf"&gt;LocalAgreement policy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2507.18446"&gt;Streaming Sortformer&lt;/a&gt; (SOTA 2025) - Advanced real-time speaker diarization&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/juanmc2005/diart"&gt;Diart&lt;/a&gt; (SOTA 2021) - Real-time speaker diarization&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/snakers4/silero-vad"&gt;Silero VAD&lt;/a&gt; (2024) - Enterprise-grade Voice Activity Detection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Why not just run a simple Whisper model on every audio batch?&lt;/strong&gt; Whisper is designed for complete utterances, not real-time chunks. Processing small segments loses context, cuts off words mid-syllable, and produces poor transcription. WhisperLiveKit uses state-of-the-art simultaneous speech research for intelligent buffering and incremental processing.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Architecture&lt;/h3&gt; 
&lt;img alt="Architecture" src="https://raw.githubusercontent.com/QuentinFuxa/WhisperLiveKit/refs/heads/main/architecture.png" /&gt; 
&lt;p&gt;&lt;em&gt;The backend supports multiple concurrent users. Voice Activity Detection reduces overhead when no voice is detected.&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Installation &amp;amp; Quick Start&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install whisperlivekit
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;You can also clone the repo and &lt;code&gt;pip install -e .&lt;/code&gt; for the latest version.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Quick Start&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start the transcription server:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;wlk --model base --language en
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Open your browser&lt;/strong&gt; and navigate to &lt;code&gt;http://localhost:8000&lt;/code&gt;. Start speaking and watch your words appear in real-time!&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;See &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/simul_whisper/whisper/tokenizer.py"&gt;here&lt;/a&gt; for the list of all available languages.&lt;/li&gt; 
  &lt;li&gt;Check the &lt;a href="https://raw.githubusercontent.com/QuentinFuxa/WhisperLiveKit/main/docs/troubleshooting.md"&gt;troubleshooting guide&lt;/a&gt; for step-by-step fixes collected from recent GPU setup/env issues.&lt;/li&gt; 
  &lt;li&gt;The CLI entry point is exposed as both &lt;code&gt;wlk&lt;/code&gt; and &lt;code&gt;whisperlivekit-server&lt;/code&gt;; they are equivalent.&lt;/li&gt; 
  &lt;li&gt;For HTTPS requirements, see the &lt;strong&gt;Parameters&lt;/strong&gt; section for SSL configuration options.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Use it to capture audio from web pages.&lt;/h4&gt; 
&lt;p&gt;Go to &lt;code&gt;chrome-extension&lt;/code&gt; for instructions.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/QuentinFuxa/WhisperLiveKit/refs/heads/main/chrome-extension/demo-extension.png" alt="WhisperLiveKit Demo" width="600" /&gt; &lt;/p&gt; 
&lt;h4&gt;Optional Dependencies&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Optional&lt;/th&gt; 
   &lt;th&gt;&lt;code&gt;pip install&lt;/code&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Windows/Linux optimizations&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;faster-whisper&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Apple Silicon optimizations&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;mlx-whisper&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Translation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;nllw&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Speaker diarization&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;git+https://github.com/NVIDIA/NeMo.git@main#egg=nemo_toolkit[asr]&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI API&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;openai&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;[Not recommanded]&lt;/em&gt; Speaker diarization with Diart&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;diart&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;See &lt;strong&gt;Parameters &amp;amp; Configuration&lt;/strong&gt; below on how to use them.&lt;/p&gt; 
&lt;h3&gt;Usage Examples&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Command-line Interface&lt;/strong&gt;: Start the transcription server with various options:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Large model and translate from french to danish
wlk --model large-v3 --language fr --target-language da

# Diarization and server listening on */80 
wlk --host 0.0.0.0 --port 80 --model medium --diarization --language fr
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Python API Integration&lt;/strong&gt;: Check &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/basic_server.py"&gt;basic_server&lt;/a&gt; for a more complete example of how to use the functions and classes.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from contextlib import asynccontextmanager

from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse

from whisperlivekit import AudioProcessor, TranscriptionEngine, parse_args

transcription_engine = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global transcription_engine
    transcription_engine = TranscriptionEngine(model="medium", diarization=True, lan="en")
    yield

app = FastAPI(lifespan=lifespan)

async def handle_websocket_results(websocket: WebSocket, results_generator):
    async for response in results_generator:
        await websocket.send_json(response)
    await websocket.send_json({"type": "ready_to_stop"})

@app.websocket("/asr")
async def websocket_endpoint(websocket: WebSocket):
    global transcription_engine

    # Create a new AudioProcessor for each connection, passing the shared engine
    audio_processor = AudioProcessor(transcription_engine=transcription_engine)    
    results_generator = await audio_processor.create_tasks()
    results_task = asyncio.create_task(handle_websocket_results(websocket, results_generator))
    await websocket.accept()
    while True:
        message = await websocket.receive_bytes()
        await audio_processor.process_audio(message)        
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Frontend Implementation&lt;/strong&gt;: The package includes an HTML/JavaScript implementation &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/web/live_transcription.html"&gt;here&lt;/a&gt;. You can also import it using &lt;code&gt;from whisperlivekit import get_inline_ui_html&lt;/code&gt; &amp;amp; &lt;code&gt;page = get_inline_ui_html()&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Parameters &amp;amp; Configuration&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Parameter&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--model&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Whisper model size. List and recommandations &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/docs/default_and_custom_models.md"&gt;here&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;small&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--model-path&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Local .pt file/directory &lt;strong&gt;or&lt;/strong&gt; Hugging Face repo ID containing the Whisper model. Overrides &lt;code&gt;--model&lt;/code&gt;. Recommandations &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/docs/default_and_custom_models.md"&gt;here&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--language&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;List &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/whisper/tokenizer.py"&gt;here&lt;/a&gt;. If you use &lt;code&gt;auto&lt;/code&gt;, the model attempts to detect the language automatically, but it tends to bias towards English.&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;auto&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--target-language&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;If sets, translates using &lt;a href="https://github.com/QuentinFuxa/NoLanguageLeftWaiting"&gt;NLLW&lt;/a&gt;. &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/docs/supported_languages.md"&gt;200 languages available&lt;/a&gt;. If you want to translate to english, you can also use &lt;code&gt;--direct-english-translation&lt;/code&gt;. The STT model will try to directly output the translation.&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--diarization&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enable speaker identification&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--backend-policy&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Streaming strategy: &lt;code&gt;1&lt;/code&gt;/&lt;code&gt;simulstreaming&lt;/code&gt; uses AlignAtt SimulStreaming, &lt;code&gt;2&lt;/code&gt;/&lt;code&gt;localagreement&lt;/code&gt; uses the LocalAgreement policy&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;simulstreaming&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--backend&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Whisper implementation selector. &lt;code&gt;auto&lt;/code&gt; picks MLX on macOS (if installed), otherwise Faster-Whisper, otherwise vanilla Whisper. You can also force &lt;code&gt;mlx-whisper&lt;/code&gt;, &lt;code&gt;faster-whisper&lt;/code&gt;, &lt;code&gt;whisper&lt;/code&gt;, or &lt;code&gt;openai-api&lt;/code&gt; (LocalAgreement only)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;auto&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--no-vac&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disable Voice Activity Controller. NOT ADVISED&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--no-vad&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disable Voice Activity Detection. NOT ADVISED&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--warmup-file&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Audio file path for model warmup&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;jfk.wav&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--host&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Server host address&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;localhost&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--port&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Server port&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;8000&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--ssl-certfile&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to the SSL certificate file (for HTTPS support)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--ssl-keyfile&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to the SSL private key file (for HTTPS support)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--forwarded-allow-ips&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Ip or Ips allowed to reverse proxy the whisperlivekit-server. Supported types are IP Addresses (e.g. 127.0.0.1), IP Networks (e.g. 10.100.0.0/16), or Literals (e.g. /path/to/socket.sock)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--pcm-input&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;raw PCM (s16le) data is expected as input and FFmpeg will be bypassed. Frontend will use AudioWorklet instead of MediaRecorder&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--lora-path&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path or Hugging Face repo ID for LoRA adapter weights (e.g., &lt;code&gt;qfuxa/whisper-base-french-lora&lt;/code&gt;). Only works with native Whisper backend (&lt;code&gt;--backend whisper&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Translation options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--nllb-backend&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;transformers&lt;/code&gt; or &lt;code&gt;ctranslate2&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ctranslate2&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--nllb-size&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;600M&lt;/code&gt; or &lt;code&gt;1.3B&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;600M&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Diarization options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--diarization-backend&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;diart&lt;/code&gt; or &lt;code&gt;sortformer&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sortformer&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--disable-punctuation-split&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;[NOT FUNCTIONAL IN 0.2.15 / 0.2.16] Disable punctuation based splits. See #214&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--segmentation-model&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Hugging Face model ID for Diart segmentation model. &lt;a href="https://github.com/juanmc2005/diart/tree/main?tab=readme-ov-file#pre-trained-models"&gt;Available models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pyannote/segmentation-3.0&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--embedding-model&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Hugging Face model ID for Diart embedding model. &lt;a href="https://github.com/juanmc2005/diart/tree/main?tab=readme-ov-file#pre-trained-models"&gt;Available models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;speechbrain/spkrec-ecapa-voxceleb&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;SimulStreaming backend options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--disable-fast-encoder&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disable Faster Whisper or MLX Whisper backends for the encoder (if installed). Inference can be slower but helpful when GPU memory is limited&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--custom-alignment-heads&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Use your own alignment heads, useful when &lt;code&gt;--model-dir&lt;/code&gt; is used. Use &lt;code&gt;scripts/determine_alignment_heads.py&lt;/code&gt; to extract them. &lt;img src="https://raw.githubusercontent.com/QuentinFuxa/WhisperLiveKit/main/scripts/alignment_heads.png" alt="WhisperLiveKit Demo" width="300" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--frame-threshold&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;AlignAtt frame threshold (lower = faster, higher = more accurate)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;25&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--beams&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Number of beams for beam search (1 = greedy decoding)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--decoder&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Force decoder type (&lt;code&gt;beam&lt;/code&gt; or &lt;code&gt;greedy&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;auto&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--audio-max-len&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Maximum audio buffer length (seconds)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;30.0&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--audio-min-len&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Minimum audio length to process (seconds)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;0.0&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--cif-ckpt-path&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to CIF model for word boundary detection&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--never-fire&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Never truncate incomplete words&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--init-prompt&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Initial prompt for the model&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--static-init-prompt&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Static prompt that doesn't scroll&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--max-context-tokens&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Maximum context tokens&lt;/td&gt; 
   &lt;td&gt;Depends on model used, but usually 448.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;WhisperStreaming backend options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--confidence-validation&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Use confidence scores for faster validation&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--buffer_trimming&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Buffer trimming strategy (&lt;code&gt;sentence&lt;/code&gt; or &lt;code&gt;segment&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;segment&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For diarization using Diart, you need to accept user conditions &lt;a href="https://huggingface.co/pyannote/segmentation"&gt;here&lt;/a&gt; for the &lt;code&gt;pyannote/segmentation&lt;/code&gt; model, &lt;a href="https://huggingface.co/pyannote/segmentation-3.0"&gt;here&lt;/a&gt; for the &lt;code&gt;pyannote/segmentation-3.0&lt;/code&gt; model and &lt;a href="https://huggingface.co/pyannote/embedding"&gt;here&lt;/a&gt; for the &lt;code&gt;pyannote/embedding&lt;/code&gt; model. &lt;strong&gt;Then&lt;/strong&gt;, login to HuggingFace: &lt;code&gt;huggingface-cli login&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;üöÄ Deployment Guide&lt;/h3&gt; 
&lt;p&gt;To deploy WhisperLiveKit in production:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Server Setup&lt;/strong&gt;: Install production ASGI server &amp;amp; launch with multiple workers&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install uvicorn gunicorn
gunicorn -k uvicorn.workers.UvicornWorker -w 4 your_app:app
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: Host your customized version of the &lt;code&gt;html&lt;/code&gt; example &amp;amp; ensure WebSocket connection points correctly&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Nginx Configuration&lt;/strong&gt; (recommended for production):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-nginx"&gt;server {
   listen 80;
   server_name your-domain.com;
    location / {
        proxy_pass http://localhost:8000;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
}}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;HTTPS Support&lt;/strong&gt;: For secure deployments, use "wss://" instead of "ws://" in WebSocket URL&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;üêã Docker&lt;/h2&gt; 
&lt;p&gt;Deploy the application easily using Docker with GPU or CPU support.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Docker installed on your system&lt;/li&gt; 
 &lt;li&gt;For GPU support: NVIDIA Docker runtime installed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;With GPU acceleration (recommended):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -t wlk .
docker run --gpus all -p 8000:8000 --name wlk wlk
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;CPU only:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -f Dockerfile.cpu -t wlk .
docker run -p 8000:8000 --name wlk wlk
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Advanced Usage&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Custom configuration:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Example with custom model and language
docker run --gpus all -p 8000:8000 --name wlk wlk --model large-v3 --language fr
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Memory Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Large models&lt;/strong&gt;: Ensure your Docker runtime has sufficient memory allocated&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Customization&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--build-arg&lt;/code&gt; Options: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;EXTRAS="whisper-timestamped"&lt;/code&gt; - Add extras to the image's installation (no spaces). Remember to set necessary container options!&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;HF_PRECACHE_DIR="./.cache/"&lt;/code&gt; - Pre-load a model cache for faster first-time start&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;HF_TKN_FILE="./token"&lt;/code&gt; - Add your Hugging Face Hub access token to download gated models&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üîÆ Use Cases&lt;/h2&gt; 
&lt;p&gt;Capture discussions in real-time for meeting transcription, help hearing-impaired users follow conversations through accessibility tools, transcribe podcasts or videos automatically for content creation, transcribe support calls with speaker identification for customer service...&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>icip-cas/PPTAgent</title>
      <link>https://github.com/icip-cas/PPTAgent</link>
      <description>&lt;p&gt;PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides [EMNLP 2025]&lt;/p&gt;&lt;hr&gt;&lt;div align="right"&gt; 
 &lt;details&gt; 
  &lt;summary&gt;üåê Language&lt;/summary&gt; 
  &lt;div&gt; 
   &lt;div align="center"&gt; 
    &lt;a href="https://openaitx.github.io/view.html?user=icip-cas&amp;amp;project=PPTAgent&amp;amp;lang=en"&gt;English&lt;/a&gt; | 
    &lt;a href="https://openaitx.github.io/view.html?user=icip-cas&amp;amp;project=PPTAgent&amp;amp;lang=zh-CN"&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; | 
    &lt;a href="https://openaitx.github.io/view.html?user=icip-cas&amp;amp;project=PPTAgent&amp;amp;lang=zh-TW"&gt;ÁπÅÈ´î‰∏≠Êñá&lt;/a&gt; | 
    &lt;a href="https://openaitx.github.io/view.html?user=icip-cas&amp;amp;project=PPTAgent&amp;amp;lang=ja"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | 
    &lt;a href="https://openaitx.github.io/view.html?user=icip-cas&amp;amp;project=PPTAgent&amp;amp;lang=ko"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | 
    &lt;a href="https://openaitx.github.io/view.html?user=icip-cas&amp;amp;project=PPTAgent&amp;amp;lang=hi"&gt;‡§π‡§ø‡§®‡•ç‡§¶‡•Ä&lt;/a&gt; | 
    &lt;a href="https://openaitx.github.io/view.html?user=icip-cas&amp;amp;project=PPTAgent&amp;amp;lang=th"&gt;‡πÑ‡∏ó‡∏¢&lt;/a&gt; | 
    &lt;a href="https://openaitx.github.io/view.html?user=icip-cas&amp;amp;project=PPTAgent&amp;amp;lang=fr"&gt;Fran√ßais&lt;/a&gt; | 
    &lt;a href="https://openaitx.github.io/view.html?user=icip-cas&amp;amp;project=PPTAgent&amp;amp;lang=de"&gt;Deutsch&lt;/a&gt; | 
    &lt;a href="https://openaitx.github.io/view.html?user=icip-cas&amp;amp;project=PPTAgent&amp;amp;lang=es"&gt;Espa√±ol&lt;/a&gt; | 
    &lt;a href="https://openaitx.github.io/view.html?user=icip-cas&amp;amp;project=PPTAgent&amp;amp;lang=it"&gt;Italiano&lt;/a&gt; | 
    &lt;a href="https://openaitx.github.io/view.html?user=icip-cas&amp;amp;project=PPTAgent&amp;amp;lang=ru"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | 
    &lt;a href="https://openaitx.github.io/view.html?user=icip-cas&amp;amp;project=PPTAgent&amp;amp;lang=pt"&gt;Portugu√™s&lt;/a&gt; | 
    &lt;a href="https://openaitx.github.io/view.html?user=icip-cas&amp;amp;project=PPTAgent&amp;amp;lang=nl"&gt;Nederlands&lt;/a&gt; | 
    &lt;a href="https://openaitx.github.io/view.html?user=icip-cas&amp;amp;project=PPTAgent&amp;amp;lang=pl"&gt;Polski&lt;/a&gt; | 
    &lt;a href="https://openaitx.github.io/view.html?user=icip-cas&amp;amp;project=PPTAgent&amp;amp;lang=ar"&gt;ÿßŸÑÿπÿ±ÿ®Ÿäÿ©&lt;/a&gt; | 
    &lt;a href="https://openaitx.github.io/view.html?user=icip-cas&amp;amp;project=PPTAgent&amp;amp;lang=fa"&gt;ŸÅÿßÿ±ÿ≥€å&lt;/a&gt; | 
    &lt;a href="https://openaitx.github.io/view.html?user=icip-cas&amp;amp;project=PPTAgent&amp;amp;lang=tr"&gt;T√ºrk√ße&lt;/a&gt; | 
    &lt;a href="https://openaitx.github.io/view.html?user=icip-cas&amp;amp;project=PPTAgent&amp;amp;lang=vi"&gt;Ti·∫øng Vi·ªát&lt;/a&gt; | 
    &lt;a href="https://openaitx.github.io/view.html?user=icip-cas&amp;amp;project=PPTAgent&amp;amp;lang=id"&gt;Bahasa Indonesia&lt;/a&gt; | 
    &lt;a href="https://openaitx.github.io/view.html?user=icip-cas&amp;amp;project=PPTAgent&amp;amp;lang=as"&gt;‡¶Ö‡¶∏‡¶Æ‡ßÄ‡¶Ø‡¶º‡¶æ
     &lt;!--
      &lt;/div--&gt; &lt;/a&gt;
   &lt;/div&gt;
   &lt;a href="https://openaitx.github.io/view.html?user=icip-cas&amp;amp;project=PPTAgent&amp;amp;lang=as"&gt; &lt;/a&gt;
  &lt;/div&gt;
 &lt;/details&gt;
 &lt;a href="https://openaitx.github.io/view.html?user=icip-cas&amp;amp;project=PPTAgent&amp;amp;lang=as"&gt; &lt;/a&gt;
&lt;/div&gt;
&lt;a href="https://openaitx.github.io/view.html?user=icip-cas&amp;amp;project=PPTAgent&amp;amp;lang=as"&gt; &lt;/a&gt;
&lt;blockquote&gt;
 &lt;a href="https://openaitx.github.io/view.html?user=icip-cas&amp;amp;project=PPTAgent&amp;amp;lang=as"&gt; &lt;p&gt;üßë‚Äçüíª &lt;a href="https://github.com/Force1ess"&gt;Profile/ÁÆÄ‰ªã&lt;/a&gt;&lt;/p&gt;&lt;/a&gt; 
 &lt;p&gt;The main contributor of this repo is a Master's student graduating in 2026, currently on the job market.&lt;/p&gt; 
 &lt;p&gt;Êú¨‰ªìÂ∫ì‰∏ªË¶ÅË¥°ÁåÆËÄÖÊòØ2026Â±äÁ°ïÂ£´ÊØï‰∏öÁîüÔºåÊ≠£Âú®Ê±ÇËÅå‰∏≠ÔºåÊ¨¢ËøéËÅîÁ≥ª„ÄÇ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ Breaking News: Major Update Coming End of 2025&lt;/h2&gt; 
&lt;p&gt;We're excited to announce a significant update planned for release by the end of 2025! This update will introduce powerful new features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Deep Research Integration&lt;/strong&gt; - Enhanced research capabilities built directly into the workflow&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Free-Form Visual Design&lt;/strong&gt; - Create and customize visuals with unprecedented flexibility&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Autonomous Asset Creation&lt;/strong&gt; - Automatically generate assets based on your requirements&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Text-to-Image Generation&lt;/strong&gt; - Transform text descriptions into high-quality images&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Want a sneak peek? Check out our demo and detailed outputs below!&lt;/p&gt; 
&lt;h3&gt;üì∫ Demo Video of V2&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/938889e8-d7d8-4f4f-b2a1-07ee3ef3991a"&gt;https://github.com/user-attachments/assets/938889e8-d7d8-4f4f-b2a1-07ee3ef3991a&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;üí° Case Study of V2&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;h4&gt;Prompt: Please present the given document to me.&lt;/h4&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;div style="display: flex; flex-wrap: wrap; gap: 10px;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/manuscript/0001.jpg" alt="ÂõæÁâá1" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/manuscript/0002.jpg" alt="ÂõæÁâá2" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/manuscript/0003.jpg" alt="ÂõæÁâá3" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/manuscript/0004.jpg" alt="ÂõæÁâá4" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/manuscript/0005.jpg" alt="ÂõæÁâá5" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/manuscript/0006.jpg" alt="ÂõæÁâá6" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/manuscript/0007.jpg" alt="ÂõæÁâá7" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/manuscript/0008.jpg" alt="ÂõæÁâá8" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/manuscript/0009.jpg" alt="ÂõæÁâá9" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/manuscript/0010.jpg" alt="ÂõæÁâá10" width="200" /&gt; 
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;h4&gt;Prompt: ËØ∑‰ªãÁªçÂ∞èÁ±≥ SU7 ÁöÑÂ§ñËßÇÂíå‰ª∑Ê†º&lt;/h4&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;div style="display: flex; flex-wrap: wrap; gap: 10px;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/presentation1/0001.jpg" alt="ÂõæÁâá1" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/presentation1/0002.jpg" alt="ÂõæÁâá2" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/presentation1/0003.jpg" alt="ÂõæÁâá3" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/presentation1/0004.jpg" alt="ÂõæÁâá4" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/presentation1/0005.jpg" alt="ÂõæÁâá5" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/presentation1/0006.jpg" alt="ÂõæÁâá6" width="200" /&gt; 
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;h4&gt;Prompt: ËØ∑Âà∂‰Ωú‰∏Ä‰ªΩÈ´ò‰∏≠ËØæÂ†ÇÂ±ïÁ§∫ËØæ‰ª∂Ôºå‰∏ªÈ¢ò‰∏∫‚ÄúËß£Á†ÅÁ´ãÊ≥ïËøáÁ®ãÔºöÁêÜËß£ÂÖ∂ÂØπÂõΩÈôÖÂÖ≥Á≥ªÁöÑÂΩ±Âìç‚Äù&lt;/h4&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;div style="display: flex; flex-wrap: wrap; gap: 10px;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/presentation2/0001.jpg" alt="ÂõæÁâá1" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/presentation2/0002.jpg" alt="ÂõæÁâá2" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/presentation2/0003.jpg" alt="ÂõæÁâá3" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/presentation2/0004.jpg" alt="ÂõæÁâá4" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/presentation2/0005.jpg" alt="ÂõæÁâá5" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/presentation2/0006.jpg" alt="ÂõæÁâá6" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/presentation2/0007.jpg" alt="ÂõæÁâá7" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/presentation2/0008.jpg" alt="ÂõæÁâá8" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/presentation2/0009.jpg" alt="ÂõæÁâá9" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/presentation2/0010.jpg" alt="ÂõæÁâá10" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/presentation2/0011.jpg" alt="ÂõæÁâá11" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/presentation2/0012.jpg" alt="ÂõæÁâá12" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/presentation2/0013.jpg" alt="ÂõæÁâá13" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/presentation2/0014.jpg" alt="ÂõæÁâá14" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/v2/presentation2/0015.jpg" alt="ÂõæÁâá15" width="200" /&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides&lt;/h1&gt; 
&lt;p align="center"&gt; üìÑ &lt;a href="https://arxiv.org/abs/2501.03936" target="_blank"&gt;Paper&lt;/a&gt; &amp;nbsp; | &amp;nbsp; ü§ó &lt;a href="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/#open-source-" target="_blank"&gt;OpenSource&lt;/a&gt; &amp;nbsp; | &amp;nbsp; üìù &lt;a href="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/DOC.md" target="_blank"&gt;Documentation&lt;/a&gt; &amp;nbsp; | &amp;nbsp; &lt;a href="https://deepwiki.com/icip-cas/PPTAgent" target="_blank"&gt;&lt;img src="https://deepwiki.com/icon.png" alt="Ask DeepWiki" /&gt; DeepWiki&lt;/a&gt; &amp;nbsp; | &amp;nbsp; üôè &lt;a href="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/#citation-" target="_blank"&gt;Citation&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;We present PPTAgent, an innovative system that automatically generates presentations from documents. Drawing inspiration from human presentation creation methods, our system employs a two-step process to ensure excellence in overall quality. Additionally, we introduce &lt;strong&gt;PPTEval&lt;/strong&gt;, a comprehensive evaluation framework that assesses presentations across multiple dimensions.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] üöÄ Get started quickly with our pre-built Docker image - &lt;a href="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/DOC.md/#docker-"&gt;See Docker instructions&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;üìÖ News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2025/09]: üõ†Ô∏è We support MCP server now, see &lt;a href="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/DOC.md#mcp-server-"&gt;MCP Server&lt;/a&gt; for details&lt;/li&gt; 
 &lt;li&gt;[2025/09]: üöÄ Released v2 with major improvements - see &lt;a href="https://github.com/icip-cas/PPTAgent/releases/tag/v0.2.0"&gt;release notes&lt;/a&gt; for details&lt;/li&gt; 
 &lt;li&gt;[2025/08]: üéâ Paper accepted to &lt;strong&gt;EMNLP 2025&lt;/strong&gt;!&lt;/li&gt; 
 &lt;li&gt;[2025/05]: ‚ú® Released v1 with core functionality and üåü breakthrough: reached 1,000 stars on GitHub! - see &lt;a href="https://github.com/icip-cas/PPTAgent/releases/tag/v0.1.0"&gt;release notes&lt;/a&gt; for details&lt;/li&gt; 
 &lt;li&gt;[2025/01]: üîì Open-sourced the codebase, with experimental code archived at &lt;a href="https://github.com/icip-cas/PPTAgent/releases/tag/experiment"&gt;experiment release&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Open Source ü§ó&lt;/h2&gt; 
&lt;p&gt;We have released our model and data at &lt;a href="https://huggingface.co/collections/ICIP/pptagent-68b80af43b4f4e0cb14d0bb2"&gt;HuggingFace&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Demo Video üé•&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/c3935a98-4d2b-4c46-9b36-e7c598d14863"&gt;https://github.com/user-attachments/assets/c3935a98-4d2b-4c46-9b36-e7c598d14863&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Distinctive Features ‚ú®&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Dynamic Content Generation&lt;/strong&gt;: Creates slides with seamlessly integrated text and images&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Reference Learning&lt;/strong&gt;: Leverages existing presentations without requiring manual annotation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Comprehensive Quality Assessment&lt;/strong&gt;: Evaluates presentations through multiple quality metrics&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Case Study üí°&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;h4&gt;&lt;a href="https://www.apple.com/iphone-16-pro/"&gt;Iphone 16 Pro&lt;/a&gt;&lt;/h4&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;div style="display: flex; flex-wrap: wrap; gap: 10px;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/iphone16pro/0001.jpg" alt="ÂõæÁâá1" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/iphone16pro/0002.jpg" alt="ÂõæÁâá2" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/iphone16pro/0003.jpg" alt="ÂõæÁâá3" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/iphone16pro/0004.jpg" alt="ÂõæÁâá4" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/iphone16pro/0005.jpg" alt="ÂõæÁâá5" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/iphone16pro/0006.jpg" alt="ÂõæÁâá6" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/iphone16pro/0007.jpg" alt="ÂõæÁâá7" width="200" /&gt; 
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;h4&gt;&lt;a href="https://www.anthropic.com/research/building-effective-agents"&gt;Build Effective Agents&lt;/a&gt;&lt;/h4&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;div style="display: flex; flex-wrap: wrap; gap: 10px;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/build_effective_agents/0001.jpg" alt="ÂõæÁâá1" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/build_effective_agents/0002.jpg" alt="ÂõæÁâá2" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/build_effective_agents/0003.jpg" alt="ÂõæÁâá3" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/build_effective_agents/0004.jpg" alt="ÂõæÁâá4" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/build_effective_agents/0005.jpg" alt="ÂõæÁâá5" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/build_effective_agents/0006.jpg" alt="ÂõæÁâá6" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/build_effective_agents/0007.jpg" alt="ÂõæÁâá7" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/build_effective_agents/0008.jpg" alt="ÂõæÁâá8" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/build_effective_agents/0009.jpg" alt="ÂõæÁâá9" width="200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/build_effective_agents/0010.jpg" alt="ÂõæÁâá10" width="200" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;PPTAgent ü§ñ&lt;/h2&gt; 
&lt;p&gt;PPTAgent follows a two-phase approach:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Analysis Phase&lt;/strong&gt;: Extracts and learns from patterns in reference presentations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Generation Phase&lt;/strong&gt;: Develops structured outlines and produces visually cohesive slides&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Our system's workflow is illustrated below:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/fig2.jpg" alt="PPTAgent Workflow" /&gt;&lt;/p&gt; 
&lt;h2&gt;PPTEval ‚öñÔ∏è&lt;/h2&gt; 
&lt;p&gt;PPTEval evaluates presentations across three dimensions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Content&lt;/strong&gt;: Check the accuracy and relevance of the slides.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Design&lt;/strong&gt;: Assesses the visual appeal and consistency.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Coherence&lt;/strong&gt;: Ensures the logical flow of ideas.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The workflow of PPTEval is shown below:&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/icip-cas/PPTAgent/main/resource/fig3.jpg" alt="PPTEval Workflow" style="width:70%;" /&gt; &lt;/p&gt; 
&lt;h2&gt;Citation üôè&lt;/h2&gt; 
&lt;p&gt;If you find this project helpful, please use the following to cite it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{zheng2025pptagent,
  title={PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides},
  author={Zheng, Hao and Guan, Xinyan and Kong, Hao and Zheng, Jia and Zhou, Weixiang and Lin, Hongyu and Lu, Yaojie and He, Ben and Han, Xianpei and Sun, Le},
  journal={arXiv preprint arXiv:2501.03936},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#icip-cas/PPTAgent&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=icip-cas/PPTAgent&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>BlackTechX011/Hacx-GPT</title>
      <link>https://github.com/BlackTechX011/Hacx-GPT</link>
      <description>&lt;p&gt;Hacx GPT a powerful, evil brother of WormGPT.&lt;/p&gt;&lt;hr&gt;&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://github.com/BlackTechX011/Hacx-GPT/raw/main/img/HacxGPT.png" alt="HacxGPT logo" /&gt;&lt;/p&gt; 
 &lt;h1&gt;HacxGPT&lt;/h1&gt; 
 &lt;p&gt; &lt;strong&gt;An advanced AI framework, engineered to explore the frontiers of language model interactions.&lt;/strong&gt; &lt;/p&gt; 
 &lt;!-- Badges --&gt; 
 &lt;p&gt; &lt;a href="https://github.com/BlackTechX011/Hacx-GPT" title="View on GitHub"&gt;&lt;img src="https://img.shields.io/static/v1?label=BlackTechX&amp;amp;message=Hacx-GPT&amp;amp;color=blue&amp;amp;logo=github" alt="BlackTechX - Hacx-GPT" /&gt;&lt;/a&gt; &lt;a href="https://github.com/BlackTechX011/Hacx-GPT/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/BlackTechX011/Hacx-GPT?style=social" alt="GitHub Stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/BlackTechX011/Hacx-GPT/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/BlackTechX011/Hacx-GPT?style=social" alt="GitHub Forks" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;img src="https://img.shields.io/github/last-commit/BlackTechX011/Hacx-GPT?color=green&amp;amp;logo=github" alt="Last Commit" /&gt; &lt;img src="https://img.shields.io/github/license/BlackTechX011/Hacx-GPT?color=red" alt="License" /&gt; &lt;/p&gt; 
 &lt;h4&gt; &lt;a href="https://github.com/BlackTechX011/"&gt;GitHub&lt;/a&gt; &lt;span&gt; ¬∑ &lt;/span&gt; &lt;a href="https://www.instagram.com/BlackTechX011/"&gt;Instagram&lt;/a&gt; &lt;span&gt; ¬∑ &lt;/span&gt; &lt;a href="https://x.com/BlackTechX011"&gt;X (Twitter)&lt;/a&gt; &lt;span&gt; ¬∑ &lt;/span&gt; &lt;a href="https://www.youtube.com/@BlackTechX_"&gt;YouTube&lt;/a&gt; &lt;/h4&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ Showcase&lt;/h2&gt; 
&lt;p&gt;Here is a glimpse of the HacxGPT framework in action.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/BlackTechX011/Hacx-GPT/raw/main/img/home.png" alt="HacxGPT Demo Screenshot" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;&lt;span&gt;üìî&lt;/span&gt; Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BlackTechX011/Hacx-GPT/main/#star2-about-the-project"&gt;About The Project&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BlackTechX011/Hacx-GPT/main/#grey_question-what-is-this-repository"&gt;What is this Repository?&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BlackTechX011/Hacx-GPT/main/#gem-the-real-hacxgpt-our-private-model"&gt;The Real HacxGPT: Our Private Model&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BlackTechX011/Hacx-GPT/main/#dart-features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BlackTechX011/Hacx-GPT/main/#electric_plug-getting-started"&gt;Getting Started&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BlackTechX011/Hacx-GPT/main/#key-prerequisites-api-key"&gt;Prerequisites: API Key&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BlackTechX011/Hacx-GPT/main/#gear-installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BlackTechX011/Hacx-GPT/main/#wrench-configuration"&gt;Configuration&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BlackTechX011/Hacx-GPT/main/#eyes-usage"&gt;Usage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BlackTechX011/Hacx-GPT/main/#wave-contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BlackTechX011/Hacx-GPT/main/#warning-license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;&lt;span&gt;üåü&lt;/span&gt; About The Project&lt;/h2&gt; 
&lt;p&gt;HacxGPT is designed to provide powerful, unrestricted, and seamless AI-driven conversations, pushing the boundaries of what is possible with natural language processing.&lt;/p&gt; 
&lt;h3&gt;&lt;span&gt;‚ùî&lt;/span&gt; What is this Repository?&lt;/h3&gt; 
&lt;p&gt;This repository contains an open-source framework that demonstrates the &lt;em&gt;concept&lt;/em&gt; of HacxGPT. It utilizes external, third-party APIs from providers like &lt;strong&gt;OpenRouter&lt;/strong&gt; or &lt;strong&gt;DeepSeek&lt;/strong&gt; and combines them with a specialized system prompt. This allows a standard Large Language Model (LLM) to behave in a manner similar to our private HacxGPT, offering a preview of its capabilities.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;It is important to understand:&lt;/strong&gt; This code is a wrapper and a proof-of-concept, not the core, fine-tuned HacxGPT model itself.&lt;/p&gt; 
&lt;h3&gt;&lt;span&gt;üíé&lt;/span&gt; The Real HacxGPT: Our Private Model&lt;/h3&gt; 
&lt;p&gt;While this repository offers a glimpse into HacxGPT's potential, our flagship offering is a &lt;strong&gt;privately-developed, fine-tuned Large Language Model.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Why choose our private model?&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Ground-Up Development:&lt;/strong&gt; We've trained our model using advanced techniques similar to the DeepSeek methodology, focusing on pre-training, Supervised Fine-Tuning (SFT), and Reinforcement Learning (RL).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Superior Performance:&lt;/strong&gt; The private model is significantly more intelligent, coherent, and capable than what can be achieved with a simple system prompt on a public API.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced Security &amp;amp; Privacy:&lt;/strong&gt; Offered as a private, managed service to ensure security and prevent misuse.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;True Unrestricted Power:&lt;/strong&gt; Built from the core to handle a wider and more complex range of tasks without the limitations of public models.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;How to Access the Private Model&lt;/h4&gt; 
&lt;p&gt;Access to our private model is exclusive. To inquire about services and pricing, please contact our team via Telegram.&lt;/p&gt; 
&lt;p&gt;‚û°Ô∏è &lt;strong&gt;Join our Telegram Channel for more info:&lt;/strong&gt; &lt;a href="https://t.me/HacxGPT"&gt;https://t.me/HacxGPT&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;&lt;span&gt;üéØ&lt;/span&gt; Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Powerful AI Conversations:&lt;/strong&gt; Get intelligent and context-aware answers to your queries.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Unrestricted Framework:&lt;/strong&gt; A system prompt designed to bypass conventional AI limitations.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy-to-Use CLI:&lt;/strong&gt; A clean and simple command-line interface for smooth interaction.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cross-Platform:&lt;/strong&gt; Tested and working on Kali Linux, Ubuntu, and Termux.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;&lt;span&gt;üîå&lt;/span&gt; Getting Started&lt;/h2&gt; 
&lt;p&gt;Follow these steps to get the HacxGPT framework running on your system.&lt;/p&gt; 
&lt;h3&gt;&lt;span&gt;üîë&lt;/span&gt; Prerequisites: API Key&lt;/h3&gt; 
&lt;p&gt;To use this framework, you &lt;strong&gt;must&lt;/strong&gt; obtain an API key from a supported provider. These services offer free tiers that are perfect for getting started.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Choose a provider:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;OpenRouter:&lt;/strong&gt; Visit &lt;a href="https://openrouter.ai/keys"&gt;OpenRouter.ai&lt;/a&gt; to get a free API key. They provide access to a variety of models.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;DeepSeek:&lt;/strong&gt; Visit the &lt;a href="https://platform.deepseek.com/api_keys"&gt;DeepSeek Platform&lt;/a&gt; for a free API key to use their powerful models.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Copy your API key.&lt;/strong&gt; You will need to paste it into the script when prompted during the first run.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;&lt;span&gt;‚öô&lt;/span&gt; Installation&lt;/h3&gt; 
&lt;p&gt;We provide simple, one-command installation scripts for your convenience.&lt;/p&gt; 
&lt;h4&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Download the &lt;code&gt;install.bat&lt;/code&gt; script from this repository.&lt;/li&gt; 
 &lt;li&gt;Double-click the file to run it. It will automatically clone the repository and install all dependencies.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;&lt;strong&gt;Linux / Termux&lt;/strong&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open your terminal.&lt;/li&gt; 
 &lt;li&gt;Run the following command. It will download the installer, make it executable, and run it for you. &lt;pre&gt;&lt;code class="language-bash"&gt;bash &amp;lt;(curl -s https://raw.githubusercontent.com/BlackTechX011/Hacx-GPT/main/install.sh)
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;details&gt; 
 &lt;summary&gt;Manual Installation (Alternative)&lt;/summary&gt; 
 &lt;p&gt;If you prefer to install manually, follow these steps.&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Clone the repository:&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/BlackTechX011/Hacx-GPT.git
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Navigate to the directory:&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd Hacx-GPT
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Install Python dependencies:&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;&lt;span&gt;üîß&lt;/span&gt; Configuration&lt;/h2&gt; 
&lt;p&gt;You can easily switch between API providers.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Open the &lt;code&gt;HacxGPT.py&lt;/code&gt; file in a text editor.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Locate the &lt;code&gt;API_PROVIDER&lt;/code&gt; variable at the top of the file.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Change the value to either &lt;code&gt;"openrouter"&lt;/code&gt; or &lt;code&gt;"deepseek"&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;# HacxGPT.py

# Change this value to "deepseek" or "openrouter"
API_PROVIDER = "openrouter" 
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Save the file. The script will now use the selected provider's API.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;&lt;span&gt;üëÄ&lt;/span&gt; Usage&lt;/h2&gt; 
&lt;p&gt;Once installation and configuration are complete, run the application with this simple command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3 HacxGPT.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The first time you run it, you will be prompted to enter your API key. It will be saved locally for future sessions.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;&lt;span&gt;‚≠ê&lt;/span&gt; Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#BlackTechX011/Hacx-GPT&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=BlackTechX011/Hacx-GPT&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;&lt;span&gt;üëã&lt;/span&gt; Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are what make the open-source community such an amazing place to learn, inspire, and create. Any contributions you make are &lt;strong&gt;greatly appreciated&lt;/strong&gt;.&lt;/p&gt; 
&lt;a href="https://github.com/BlackTechX011/Hacx-GPT/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=BlackTechX011/Hacx-GPT" /&gt; &lt;/a&gt; 
&lt;hr /&gt; 
&lt;h2&gt;&lt;span&gt;‚ö†&lt;/span&gt; License&lt;/h2&gt; 
&lt;p&gt;Distributed under the Personal-Use Only License (PUOL) 1.0. See &lt;code&gt;LICENSE.txt&lt;/code&gt; for more information.&lt;/p&gt; 
&lt;hr /&gt;</description>
    </item>
    
    <item>
      <title>ostris/ai-toolkit</title>
      <link>https://github.com/ostris/ai-toolkit</link>
      <description>&lt;p&gt;The ultimate training toolkit for finetuning diffusion models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Toolkit by Ostris&lt;/h1&gt; 
&lt;p&gt;AI Toolkit is an all in one training suite for diffusion models. I try to support all the latest models on consumer grade hardware. Image and video models. It can be run as a GUI or CLI. It is designed to be easy to use but still have every feature imaginable.&lt;/p&gt; 
&lt;h2&gt;Support My Work&lt;/h2&gt; 
&lt;p&gt;If you enjoy my projects or use them commercially, please consider sponsoring me. Every bit helps! üíñ&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/orgs/ostris"&gt;Sponsor on GitHub&lt;/a&gt; | &lt;a href="https://www.patreon.com/ostris"&gt;Support on Patreon&lt;/a&gt; | &lt;a href="https://www.paypal.com/donate/?hosted_button_id=9GEFUKC8T9R9W"&gt;Donate on PayPal&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Current Sponsors&lt;/h3&gt; 
&lt;p&gt;All of these people / organizations are the ones who selflessly make this project possible. Thank you!!&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Last updated: 2025-10-20 15:52 UTC&lt;/em&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://x.com/NuxZoe" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://pbs.twimg.com/profile_images/1919488160125616128/QAZXTMEj_400x400.png" alt="a16z" width="280" height="280" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/replicate" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/60410876?v=4" alt="Replicate" width="280" height="280" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/huggingface" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/25720743?v=4" alt="Hugging Face" width="280" height="280" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;hr style="width:100%;border:none;height:2px;background:#ddd;margin:30px 0;" /&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.pixelcut.ai/" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://pbs.twimg.com/profile_images/1496882159658885133/11asz2Sc_400x400.jpg" alt="Pixelcut" width="200" height="200" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/josephrocca" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/1167575?u=92d92921b4cb5c8c7e225663fed53c4b41897736&amp;amp;v=4" alt="josephrocca" width="200" height="200" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/weights-ai" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/185568492?v=4" alt="Weights" width="200" height="200" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;hr style="width:100%;border:none;height:2px;background:#ddd;margin:30px 0;" /&gt; 
&lt;p align="center"&gt; &lt;img src="https://c8.patreon.com/4/200/33158543/C" alt="clement Delangue" width="150" height="150" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/8654302/b0f5ebedc62a47c4b56222693e1254e9/eyJ3IjoyMDB9/2.jpeg?token-hash=suI7_QjKUgWpdPuJPaIkElkTrXfItHlL8ZHLPT-w_d4%3D" alt="Misch Strotz" width="150" height="150" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/93304/J" alt="Joseph Rocca" width="150" height="150" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/161471720/dd330b4036d44a5985ed5985c12a5def/eyJ3IjoyMDB9/1.jpeg?token-hash=k1f4Vv7TevzYa9tqlzAjsogYmkZs8nrXQohPCDGJGkc%3D" alt="Vladimir Sotnikov" width="150" height="150" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/120239481/49b1ce70d3d24704b8ec34de24ec8f55/eyJ3IjoyMDB9/1.jpeg?token-hash=o0y1JqSXqtGvVXnxb06HMXjQXs6OII9yMMx5WyyUqT4%3D" alt="nitish PNR" width="150" height="150" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/152118848/3b15a43d71714552b5ed1c9f84e66adf/eyJ3IjoyMDB9/1.png?token-hash=MKf3sWHz0MFPm_OAFjdsNvxoBfN5B5l54mn1ORdlRy8%3D" alt="Kristjan Retter" width="150" height="150" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/2298192/1228b69bd7d7481baf3103315183250d/eyJ3IjoyMDB9/1.jpg?token-hash=opN1e4r4Nnvqbtr8R9HI8eyf9m5F50CiHDOdHzb4UcA%3D" alt="Mohamed Oumoumad" width="150" height="150" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/548524/S" alt="Steve Hanff" width="150" height="150" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/181019370/2050c03b87a54276996539aaa52ccd3e/eyJ3IjoyMDB9/1.png?token-hash=I0Exm-MPbLOnuWcwtEvKm2v3NtbhqPyQJMSMEAvsbWI%3D" alt="Keith  Ruby" width="150" height="150" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/8449560/P" alt="Patron" width="150" height="150" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;/p&gt; 
&lt;hr style="width:100%;border:none;height:2px;background:#ddd;margin:30px 0;" /&gt; 
&lt;p align="center"&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/169502989/220069e79ce745b29237e94c22a729df/eyJ3IjoyMDB9/1.png?token-hash=E8E2JOqx66k2zMtYUw8Gy57dw-gVqA6OPpdCmWFFSFw%3D" alt="Timothy Bielec" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://x.com/NuxZoe" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://pbs.twimg.com/profile_images/1916482710069014528/RDLnPRSg_400x400.jpg" alt="tungsten" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;a href="http://www.ir-ltd.net" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://pbs.twimg.com/profile_images/1602579392198283264/6Tm2GYus_400x400.jpg" alt="IR-Entertainment Ltd" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c8.patreon.com/4/200/96410991/C" alt="cmh" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/9547341/bb35d9a222fd460e862e960ba3eacbaf/eyJ3IjoyMDB9/1.jpeg?token-hash=Q2XGDvkCbiONeWNxBCTeTMOcuwTjOaJ8Z-CAf5xq3Hs%3D" alt="Travis Harrington" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/27288932/6c35d2d961ee4e14a7a368c990791315/eyJ3IjoyMDB9/1.jpeg?token-hash=TGIto_PGEG2NEKNyqwzEnRStOkhrjb3QlMhHA3raKJY%3D" alt="David Garrido" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/5021048/c6beacab0fdb4568bf9f0d549aa4bc44/eyJ3IjoyMDB9/1.jpeg?token-hash=JTEtFVzUeU7pQw4R3eSn6rGgqgi44uc2rDBAv6F6A4o%3D" alt="Infinite " width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/98811435/3a3632d1795b4c2b9f8f0270f2f6a650/eyJ3IjoyMDB9/1.jpeg?token-hash=657rzuJ0bZavMRZW3XZ-xQGqm3Vk6FkMZgFJVMCOPdk%3D" alt="EmmanuelMr18" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://x.com/RalFingerLP" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://pbs.twimg.com/profile_images/919595465041162241/ZU7X3T5k_400x400.jpg" alt="RalFinger" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/93348210/5c650f32a0bc481d80900d2674528777/eyJ3IjoyMDB9/1.jpeg?token-hash=0jiknRw3jXqYWW6En8bNfuHgVDj4LI_rL7lSS4-_xlo%3D" alt="Armin Behjati" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/155963250/6f8fd7075c3b4247bfeb054ba49172d6/eyJ3IjoyMDB9/1.png?token-hash=z81EHmdU2cqSrwa9vJmZTV3h0LG-z9Qakhxq34FrYT4%3D" alt="Un Defined" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/81275465/1e4148fe9c47452b838949d02dd9a70f/eyJ3IjoyMDB9/2.JPG?token-hash=zvDHRVNmXB0SgIU2ECcc0UWYjRa0q8Rjyd9T-SGOLhU%3D" alt="Aaron Amortegui" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/570742/4ceb33453a5a4745b430a216aba9280f/eyJ3IjoyMDB9/1.jpg?token-hash=nPcJ2zj3sloND9jvbnbYnob2vMXRnXdRuujthqDLWlU%3D" alt="Al H" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/jakeblakeley" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/2407659?u=be0bc786663527f2346b2e99ff608796bce19b26&amp;amp;v=4" alt="Jake Blakeley" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c8.patreon.com/4/200/33228112/J" alt="Jimmy Simmons" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/99036356/7ae9c4d80e604e739b68cca12ee2ed01/eyJ3IjoyMDB9/3.png?token-hash=ZhsBMoTOZjJ-Y6h5NOmU5MT-vDb2fjK46JDlpEehkVQ%3D" alt="Noctre" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/55206617/X" alt="xv" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;/p&gt; 
&lt;hr style="width:100%;border:none;height:2px;background:#ddd;margin:30px 0;" /&gt; 
&lt;p align="center"&gt; &lt;img src="https://c8.patreon.com/4/200/27791680/J" alt="Jean-Tristan Marin" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/82763/f99cc484361d4b9d94fe4f0814ada303/eyJ3IjoyMDB9/1.jpeg?token-hash=A3JWlBNL0b24FFWb-FCRDAyhs-OAxg-zrhfBXP_axuU%3D" alt="Doron Adler" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/103077711/bb215761cc004e80bd9cec7d4bcd636d/eyJ3IjoyMDB9/2.jpeg?token-hash=3U8kdZSUpnmeYIDVK4zK9TTXFpnAud_zOwBRXx18018%3D" alt="John Dopamine" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/141098579/1a9f0a1249d447a7a0df718a57343912/eyJ3IjoyMDB9/2.png?token-hash=_n-AQmPgY0FP9zCGTIEsr5ka4Y7YuaMkt3qL26ZqGg8%3D" alt="The Local Lab" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/134129880/680c7e14cd1a4d1a9face921fb010f88/eyJ3IjoyMDB9/1.png?token-hash=5fqqHE6DCTbt7gDQL7VRcWkV71jF7FvWcLhpYl5aMXA%3D" alt="Bharat Prabhakar" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/70218846/C" alt="Cosmosis" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/30931983/54ab4e4ceab946e79a6418d205f9ed51/eyJ3IjoyMDB9/1.png?token-hash=j2phDrgd6IWuqKqNIDbq9fR2B3fMF-GUCQSdETS1w5Y%3D" alt="HestoySeghuro ." width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/Wallawalla47" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/46779408?v=4" alt="Ian R" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c8.patreon.com/4/200/4105384/J" alt="Jack Blakely" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/24653779/R" alt="RayHell" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/4541423/S" alt="S√∂ren " width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/97985240/3d1d0e6905d045aba713e8132cab4a30/eyJ3IjoyMDB9/1.png?token-hash=fRavvbO_yqWKA_OsJb5DzjfKZ1Yt-TG-ihMoeVBvlcM%3D" alt="◊¢◊ï◊û◊® ◊û◊õ◊ú◊ï◊£" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/53077895/M" alt="Marc" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/157407541/bb9d80cffdab4334ad78366060561520/eyJ3IjoyMDB9/2.png?token-hash=WYz-U_9zabhHstOT5UIa5jBaoFwrwwqyWxWEzIR2m_c%3D" alt="Tokio Studio srl IT10640050968" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/44568304/a9d83a0e786b41b4bdada150f7c9271c/eyJ3IjoyMDB9/1.jpeg?token-hash=FtxnwrSrknQUQKvDRv2rqPceX2EF23eLq4pNQYM_fmw%3D" alt="Albert Bukoski" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/5048649/B" alt="Ben Ward" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/111904990/08b1cf65be6a4de091c9b73b693b3468/eyJ3IjoyMDB9/1.png?token-hash=_Odz6RD3CxtubEHbUxYujcjw6zAajbo3w8TRz249VBA%3D" alt="Brian Smith" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/494309/J" alt="Julian Tsependa" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/5602036/K" alt="Kelevra" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/159203973/36c817f941ac4fa18103a4b8c0cb9cae/eyJ3IjoyMDB9/1.png?token-hash=zkt72HW3EoiIEAn3LSk9gJPBsXfuTVcc4rRBS3CeR8w%3D" alt="Marko jak" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/11198131/e696d9647feb4318bcf16243c2425805/eyJ3IjoyMDB9/1.jpeg?token-hash=c2c2p1SaiX86iXAigvGRvzm4jDHvIFCg298A49nIfUM%3D" alt="Nicholas Agranoff" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/785333/bdb9ede5765d42e5a2021a86eebf0d8f/eyJ3IjoyMDB9/2.jpg?token-hash=l_rajMhxTm6wFFPn7YdoKBxeUqhdRXKdy6_8SGCuNsE%3D" alt="Sapjes " width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/76566911/6485eaf5ec6249a7b524ee0b979372f0/eyJ3IjoyMDB9/1.jpeg?token-hash=mwCSkTelDBaengG32NkN0lVl5mRjB-cwo6-a47wnOsU%3D" alt="the biitz" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/83034/W" alt="william tatum" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/32633822/1ab5612efe80417cbebfe91e871fc052/eyJ3IjoyMDB9/1.png?token-hash=pOS_IU3b3RL5-iL96A3Xqoj2bQ-dDo4RUkBylcMED_s%3D" alt="Zack Abrams" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/25209707/36ae876d662d4d85aaf162b6d67d31e7/eyJ3IjoyMDB9/1.png?token-hash=Zows_A6uqlY5jClhfr4Y3QfMnDKVkS3mbxNHUDkVejo%3D" alt="fjioq8" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/46680573/ee3d99c04a674dd5a8e1ecfb926db6a2/eyJ3IjoyMDB9/1.jpeg?token-hash=cgD4EXyfZMPnXIrcqWQ5jGqzRUfqjPafb9yWfZUPB4Q%3D" alt="Neil Murray" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/julien-blanchon" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/11278197?v=4" alt="Blanchon" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c8.patreon.com/4/200/2446176/S" alt="Scott VanKirk" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/Slartibart23" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/133593860?u=31217adb2522fb295805824ffa7e14e8f0fca6fa&amp;amp;v=4" alt="Slarti" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/squewel" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/97603184?v=4" alt="squewel" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/80767260/1fa7b3119f9f4f40a68452e57de59bfe/eyJ3IjoyMDB9/1.jpeg?token-hash=H34Vxnd58NtbuJU1XFYPkQnraVXSynZHSL3SMMcdKbI%3D" alt="nuliajuk" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://www.youtube.com/@happyme7055" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://yt3.googleusercontent.com/ytc/AIdro_mFqhIRk99SoEWY2gvSvVp6u1SkCGMkRqYQ1OlBBeoOVp8=s160-c-k-c0x00ffffff-no-rj" alt="Marcus Rass" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c8.patreon.com/4/200/63510241/A" alt="Andrew Park" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/Spikhalskiy" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/532108?u=2464983638afea8caf4cd9f0e4a7bc3e6a63bb0a&amp;amp;v=4" alt="Dmitry Spikhalsky" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c8.patreon.com/4/200/88567307/E" alt="el Chavo" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/117569999/55f75c57f95343e58402529cec852b26/eyJ3IjoyMDB9/1.jpeg?token-hash=squblHZH4-eMs3gI46Uqu1oTOK9sQ-0gcsFdZcB9xQg%3D" alt="James Thompson" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/99049612/J" alt="Jhonry Tuillier" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/40761075/R" alt="Randy McEntee" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/28533016/e8f6044ccfa7483f87eeaa01c894a773/eyJ3IjoyMDB9/2.png?token-hash=ak-h3JWB50hyenCavcs32AAPw6nNhmH2nBFKpdk5hvM%3D" alt="William Tatum" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="yvggeniy romanskiy" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/11180426/J" alt="jarrett towe" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/91298241/1b1e6d698cde4faaaae6fc4c2d95d257/eyJ3IjoyMDB9/1.jpeg?token-hash=GCo7gAF_UUdJqz3FsCq8p1pq3AEoRAoC6YIvy5xEeZk%3D" alt="Daniel Partzsch" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Joakim S√§llstr√∂m" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/156564939/17dbfd45c59d4cf29853d710cb0c5d6f/eyJ3IjoyMDB9/1.png?token-hash=e6wXA_S8cgJeEDI9eJK934eB0TiM8mxJm9zW_VH0gDU%3D" alt="Hans Untch" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/59408413/B" alt="ByteC" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/55160464/42d4719ba0834e5d83aa989c04e762da/eyJ3IjoyMDB9/1.jpeg?token-hash=_twZUkW3NREIxGUOWskUdvuZQGEcRv9XMfu5NrnCe5M%3D" alt="Chris Canterbury" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/3712451/432e22a355494ec0a1ea1927ff8d452e/eyJ3IjoyMDB9/7.jpeg?token-hash=OpQ9SAfVQ4Un9dSYlGTHuApZo5GlJ797Mo0DtVtMOSc%3D" alt="David Shorey" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/63920575/D" alt="Dutchman5oh" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/27580949/97c7dd2456a34c71b6429612a9e20462/eyJ3IjoyMDB9/1.jpeg?token-hash=cASxwWk8joAXx4tUAHch5CvTiYBR2UOHMeJK6se5fl0%3D" alt="Gergely Mad√°csi" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/33866796/7fd2a214fd5c4062b0dd63a29f8de5bd/eyJ3IjoyMDB9/1.png?token-hash=8s-7yi8GawIlqr0FCTk5JWKy26acMiYlOD8LAk2HqqU%3D" alt="James" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/84891403/83682a2a2d3b49ba9d28e7221edd5752/eyJ3IjoyMDB9/1.jpeg?token-hash=LVB6lta4BonhfPwSUnZIDmSW3IU-eEO4sXD7NSK367g%3D" alt="Koray Birand" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/358350/L" alt="L D" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/63232055/2300b4ab370341b5b476902c9b8218ee/eyJ3IjoyMDB9/1.png?token-hash=R9Nb4O0aLBRwxT1cGHUMThlvf6A2MD5SO88lpZBdH7M%3D" alt="Marek P" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/902918/5344727418634dc7b7fe7709d515a1d9/eyJ3IjoyMDB9/2.jpg?token-hash=myqV_oclkicVk9BDrvTO50jyjxJJGZ8i7oVJHwc05to%3D" alt="Michael Carychao" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/9944625/P" alt="Pomoe " width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/31613309/434500d03f714dc18049306ed3f0165c/eyJ3IjoyMDB9/1.jpg?token-hash=acILbq09wxUfJe-G2nMYUYkvHJ88ZxkzU4JebRPw2P0%3D" alt="Theta Graphics" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/10876902/T" alt="Tyssel" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/137975346/b0ac50eb2432471897ce59ddf1cb6b3d/eyJ3IjoyMDB9/1.png?token-hash=6iqhqukfgHK2IjlwTMsmBj3vratcfJ9pmxCmRkBu22s%3D" alt="G√∂ran Burlin" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Heikki Rinkinen" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="The Rope Dude" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Till Meyer" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Valarm, LLC" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/48109692/4237f732212343448ee87f5badc26e2c/eyJ3IjoyMDB9/1.jpeg?token-hash=gGqrOyctiITIyPZgjmF6YQKNf6cS9OeY4waIav3OAiU%3D" alt="Yves Poezevara" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/89623281/28d0cb75fc68439d9491f4343966f56e/eyJ3IjoyMDB9/1.jpeg?token-hash=Zt5UxtzvxDJGTPVh5Yr5rTY8JrcDsni0Mi89nZuYrp4%3D" alt="michele carlone" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/88656169/dd8943d7421d41bb9a8eb99f6d1279da/eyJ3IjoyMDB9/1.jpeg?token-hash=wT5j273p5pV10l81yR6kYdfYHR_yQ81xUzr3OfcSf7s%3D" alt="Ame Ame" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/5155933/C" alt="Chris Dermody" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="David Hooper" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/6225312/F" alt="Fredrik Normann Johansen" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/44200812/f84fd628abb243bbaded4203761aca29/eyJ3IjoyMDB9/1.png?token-hash=ArthznCCT4BqOSMj_9oP4ECWWHnrb8nYPUDZ6DqSvMU%3D" alt="kingroka" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/mertguvencli" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/29762151?u=16a906d90df96c8cff9ea131a575c4bc171b1523&amp;amp;v=4" alt="Mert Guvencli" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/174319926/f16dc35b5c4741bd9c79fac3a8c8044d/eyJ3IjoyMDB9/1.jpeg?token-hash=GvYgc-XaRGI8BPnoMOo_txDfW0BjVayFdcxkshPyrvg%3D" alt="Philip Ring" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Rudolf Goertz" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/27667925/6dac043a087e4c498e842dfad193baae/eyJ3IjoyMDB9/1.jpeg?token-hash=0bSVQo7QMMdGxFazeM099gsR0wtf28_ZTXeLIHEbIVk%3D" alt="S.Hasan Rizvi" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/2986571/S" alt="stev " width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/2472633/fea4a2888ea74c029e282fcc7ba76dd0/eyJ3IjoyMDB9/1.jpeg?token-hash=9O0lv1GQqftKoo8my9NrWSrRzHu-3IT_6VpCjHYixL8%3D" alt="Teemu Berglund" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Tommy Falkowski" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Victor-Ray Valdez" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/84873332/H" alt="Htango2" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Florian Fiegl" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Karol Stƒôpie≈Ñ" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/2888571/65c717bd8a564e469c25aa5858f9821b/eyJ3IjoyMDB9/1.png?token-hash=zwMOgNEoC9hlr2KamiB7TG004gCfJ2exSRDO4dhxo5Q%3D" alt="Derrick Schultz" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/88088407/86e5b1fd6fc2420aa23f02a61cd23567/eyJ3IjoyMDB9/1.jpeg?token-hash=_OgOjImAEXlTCuUkvRjq11gcBi8vlVCcnxrmrf2Uw7Q%3D" alt="Domagoj Visic" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/131139078/a353438167dc44818c48fc90f6076eb1/eyJ3IjoyMDB9/1.png?token-hash=W9l0spDiIY2ecb3gUY70lXOgKkO-jCE4v12_c0EZhlA%3D" alt="J D" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/138787313/c809120005024afa959231fe8b253fd9/eyJ3IjoyMDB9/1.png?token-hash=O6x0kkR4uKBsg_OODFHjZqwAupVztiZEOiXYF_7yKxM%3D" alt="Metryman55" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/5233761/N" alt="Newtown " width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/31518789/91dbf631441b460594bec9e8145ade11/eyJ3IjoyMDB9/3.jpeg?token-hash=m7vvKg4yoMnsYj6io4FOCyYUv92WoNXqFXz4S7r_Sdk%3D" alt="Number 6" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/7979776/P" alt="PizzaOrNot " width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/82707622/3f0de2ffd6eb4074ba91e81381146e1c/eyJ3IjoyMDB9/1.jpeg?token-hash=wk6wjILO2dDHJla7gn3MH9mEKl08e7PuBDwZRUtEQAw%3D" alt="Russell Norris" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/152671296/b99acfeef2c44ff4b5c9b09dbb4dcb93/eyJ3IjoyMDB9/1.png?token-hash=FQV1mNBKd3FRtF1HU_UJq4xRoG4MbvmLPZebLJTckt0%3D" alt="Vince Cirelli" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Boris HANSSEN" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Juan Franco" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/marksverdhei" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/46672778?u=d1ba8b17516e6ecf1cd55ca4db2b770f82285aad&amp;amp;v=4" alt="Markus / Mark" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Fabrizio Pasqualicchio" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;python &amp;gt;3.10&lt;/li&gt; 
 &lt;li&gt;Nvidia GPU with enough ram to do what you need&lt;/li&gt; 
 &lt;li&gt;python venv&lt;/li&gt; 
 &lt;li&gt;git&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Linux:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/ostris/ai-toolkit.git
cd ai-toolkit
python3 -m venv venv
source venv/bin/activate
# install torch first
pip3 install --no-cache-dir torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu126
pip3 install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Windows:&lt;/p&gt; 
&lt;p&gt;If you are having issues with Windows. I recommend using the easy install script at &lt;a href="https://github.com/Tavris1/AI-Toolkit-Easy-Install"&gt;https://github.com/Tavris1/AI-Toolkit-Easy-Install&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/ostris/ai-toolkit.git
cd ai-toolkit
python -m venv venv
.\venv\Scripts\activate
pip install --no-cache-dir torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu126
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;AI Toolkit UI&lt;/h1&gt; 
&lt;img src="https://ostris.com/wp-content/uploads/2025/02/toolkit-ui.jpg" alt="AI Toolkit UI" width="100%" /&gt; 
&lt;p&gt;The AI Toolkit UI is a web interface for the AI Toolkit. It allows you to easily start, stop, and monitor jobs. It also allows you to easily train models with a few clicks. It also allows you to set a token for the UI to prevent unauthorized access so it is mostly safe to run on an exposed server.&lt;/p&gt; 
&lt;h2&gt;Running the UI&lt;/h2&gt; 
&lt;p&gt;Requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Node.js &amp;gt; 18&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The UI does not need to be kept running for the jobs to run. It is only needed to start/stop/monitor jobs. The commands below will install / update the UI and it's dependencies and start the UI.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd ui
npm run build_and_start
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can now access the UI at &lt;code&gt;http://localhost:8675&lt;/code&gt; or &lt;code&gt;http://&amp;lt;your-ip&amp;gt;:8675&lt;/code&gt; if you are running it on a server.&lt;/p&gt; 
&lt;h2&gt;Securing the UI&lt;/h2&gt; 
&lt;p&gt;If you are hosting the UI on a cloud provider or any network that is not secure, I highly recommend securing it with an auth token. You can do this by setting the environment variable &lt;code&gt;AI_TOOLKIT_AUTH&lt;/code&gt; to super secure password. This token will be required to access the UI. You can set this when starting the UI like so:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Linux
AI_TOOLKIT_AUTH=super_secure_password npm run build_and_start

# Windows
set AI_TOOLKIT_AUTH=super_secure_password &amp;amp;&amp;amp; npm run build_and_start

# Windows Powershell
$env:AI_TOOLKIT_AUTH="super_secure_password"; npm run build_and_start
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;FLUX.1 Training&lt;/h2&gt; 
&lt;h3&gt;Tutorial&lt;/h3&gt; 
&lt;p&gt;To get started quickly, check out &lt;a href="https://x.com/araminta_k"&gt;@araminta_k&lt;/a&gt; tutorial on &lt;a href="https://www.youtube.com/watch?v=HzGW_Kyermg"&gt;Finetuning Flux Dev on a 3090&lt;/a&gt; with 24GB VRAM.&lt;/p&gt; 
&lt;h3&gt;Requirements&lt;/h3&gt; 
&lt;p&gt;You currently need a GPU with &lt;strong&gt;at least 24GB of VRAM&lt;/strong&gt; to train FLUX.1. If you are using it as your GPU to control your monitors, you probably need to set the flag &lt;code&gt;low_vram: true&lt;/code&gt; in the config file under &lt;code&gt;model:&lt;/code&gt;. This will quantize the model on CPU and should allow it to train with monitors attached. Users have gotten it to work on Windows with WSL, but there are some reports of a bug when running on windows natively. I have only tested on linux for now. This is still extremely experimental and a lot of quantizing and tricks had to happen to get it to fit on 24GB at all.&lt;/p&gt; 
&lt;h3&gt;FLUX.1-dev&lt;/h3&gt; 
&lt;p&gt;FLUX.1-dev has a non-commercial license. Which means anything you train will inherit the non-commercial license. It is also a gated model, so you need to accept the license on HF before using it. Otherwise, this will fail. Here are the required steps to setup a license.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Sign into HF and accept the model access here &lt;a href="https://huggingface.co/black-forest-labs/FLUX.1-dev"&gt;black-forest-labs/FLUX.1-dev&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Make a file named &lt;code&gt;.env&lt;/code&gt; in the root on this folder&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/settings/tokens/new?"&gt;Get a READ key from huggingface&lt;/a&gt; and add it to the &lt;code&gt;.env&lt;/code&gt; file like so &lt;code&gt;HF_TOKEN=your_key_here&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;FLUX.1-schnell&lt;/h3&gt; 
&lt;p&gt;FLUX.1-schnell is Apache 2.0. Anything trained on it can be licensed however you want and it does not require a HF_TOKEN to train. However, it does require a special adapter to train with it, &lt;a href="https://huggingface.co/ostris/FLUX.1-schnell-training-adapter"&gt;ostris/FLUX.1-schnell-training-adapter&lt;/a&gt;. It is also highly experimental. For best overall quality, training on FLUX.1-dev is recommended.&lt;/p&gt; 
&lt;p&gt;To use it, You just need to add the assistant to the &lt;code&gt;model&lt;/code&gt; section of your config file like so:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;      model:
        name_or_path: "black-forest-labs/FLUX.1-schnell"
        assistant_lora_path: "ostris/FLUX.1-schnell-training-adapter"
        is_flux: true
        quantize: true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You also need to adjust your sample steps since schnell does not require as many&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;      sample:
        guidance_scale: 1  # schnell does not do guidance
        sample_steps: 4  # 1 - 4 works well
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Training&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Copy the example config file located at &lt;code&gt;config/examples/train_lora_flux_24gb.yaml&lt;/code&gt; (&lt;code&gt;config/examples/train_lora_flux_schnell_24gb.yaml&lt;/code&gt; for schnell) to the &lt;code&gt;config&lt;/code&gt; folder and rename it to &lt;code&gt;whatever_you_want.yml&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Edit the file following the comments in the file&lt;/li&gt; 
 &lt;li&gt;Run the file like so &lt;code&gt;python run.py config/whatever_you_want.yml&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;A folder with the name and the training folder from the config file will be created when you start. It will have all checkpoints and images in it. You can stop the training at any time using ctrl+c and when you resume, it will pick back up from the last checkpoint.&lt;/p&gt; 
&lt;p&gt;IMPORTANT. If you press crtl+c while it is saving, it will likely corrupt that checkpoint. So wait until it is done saving&lt;/p&gt; 
&lt;h3&gt;Need help?&lt;/h3&gt; 
&lt;p&gt;Please do not open a bug report unless it is a bug in the code. You are welcome to &lt;a href="https://discord.gg/VXmU2f5WEU"&gt;Join my Discord&lt;/a&gt; and ask for help there. However, please refrain from PMing me directly with general question or support. Ask in the discord and I will answer when I can.&lt;/p&gt; 
&lt;h2&gt;Gradio UI&lt;/h2&gt; 
&lt;p&gt;To get started training locally with a with a custom UI, once you followed the steps above and &lt;code&gt;ai-toolkit&lt;/code&gt; is installed:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd ai-toolkit #in case you are not yet in the ai-toolkit folder
huggingface-cli login #provide a `write` token to publish your LoRA at the end
python flux_train_ui.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You will instantiate a UI that will let you upload your images, caption them, train and publish your LoRA &lt;img src="https://raw.githubusercontent.com/ostris/ai-toolkit/main/assets/lora_ease_ui.png" alt="image" /&gt;&lt;/p&gt; 
&lt;h2&gt;Training in RunPod&lt;/h2&gt; 
&lt;p&gt;If you would like to use Runpod, but have not signed up yet, please consider using &lt;a href="https://runpod.io?ref=h0y9jyr2"&gt;my Runpod affiliate link&lt;/a&gt; to help support this project.&lt;/p&gt; 
&lt;p&gt;I maintain an official Runpod Pod template here which can be accessed &lt;a href="https://console.runpod.io/deploy?template=0fqzfjy6f3&amp;amp;ref=h0y9jyr2"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;I have also created a short video showing how to get started using AI Toolkit with Runpod &lt;a href="https://youtu.be/HBNeS-F6Zz8"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Training in Modal&lt;/h2&gt; 
&lt;h3&gt;1. Setup&lt;/h3&gt; 
&lt;h4&gt;ai-toolkit:&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/ostris/ai-toolkit.git
cd ai-toolkit
git submodule update --init --recursive
python -m venv venv
source venv/bin/activate
pip install torch
pip install -r requirements.txt
pip install --upgrade accelerate transformers diffusers huggingface_hub #Optional, run it if you run into issues
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Modal:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Run &lt;code&gt;pip install modal&lt;/code&gt; to install the modal Python package.&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;modal setup&lt;/code&gt; to authenticate (if this doesn‚Äôt work, try &lt;code&gt;python -m modal setup&lt;/code&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Hugging Face:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Get a READ token from &lt;a href="https://huggingface.co/settings/tokens"&gt;here&lt;/a&gt; and request access to Flux.1-dev model from &lt;a href="https://huggingface.co/black-forest-labs/FLUX.1-dev"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;huggingface-cli login&lt;/code&gt; and paste your token.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;2. Upload your dataset&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Drag and drop your dataset folder containing the .jpg, .jpeg, or .png images and .txt files in &lt;code&gt;ai-toolkit&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;3. Configs&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Copy an example config file located at &lt;code&gt;config/examples/modal&lt;/code&gt; to the &lt;code&gt;config&lt;/code&gt; folder and rename it to &lt;code&gt;whatever_you_want.yml&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Edit the config following the comments in the file, &lt;strong&gt;&lt;ins&gt;be careful and follow the example &lt;code&gt;/root/ai-toolkit&lt;/code&gt; paths&lt;/ins&gt;&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;4. Edit run_modal.py&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Set your entire local &lt;code&gt;ai-toolkit&lt;/code&gt; path at &lt;code&gt;code_mount = modal.Mount.from_local_dir&lt;/code&gt; like:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;code_mount = modal.Mount.from_local_dir("/Users/username/ai-toolkit", remote_path="/root/ai-toolkit")
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Choose a &lt;code&gt;GPU&lt;/code&gt; and &lt;code&gt;Timeout&lt;/code&gt; in &lt;code&gt;@app.function&lt;/code&gt; &lt;em&gt;(default is A100 40GB and 2 hour timeout)&lt;/em&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;5. Training&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Run the config file in your terminal: &lt;code&gt;modal run run_modal.py --config-file-list-str=/root/ai-toolkit/config/whatever_you_want.yml&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;You can monitor your training in your local terminal, or on &lt;a href="https://modal.com/"&gt;modal.com&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Models, samples and optimizer will be stored in &lt;code&gt;Storage &amp;gt; flux-lora-models&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;6. Saving the model&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check contents of the volume by running &lt;code&gt;modal volume ls flux-lora-models&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Download the content by running &lt;code&gt;modal volume get flux-lora-models your-model-name&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Example: &lt;code&gt;modal volume get flux-lora-models my_first_flux_lora_v1&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Screenshot from Modal&lt;/h3&gt; 
&lt;img width="1728" alt="Modal Traning Screenshot" src="https://github.com/user-attachments/assets/7497eb38-0090-49d6-8ad9-9c8ea7b5388b" /&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Dataset Preparation&lt;/h2&gt; 
&lt;p&gt;Datasets generally need to be a folder containing images and associated text files. Currently, the only supported formats are jpg, jpeg, and png. Webp currently has issues. The text files should be named the same as the images but with a &lt;code&gt;.txt&lt;/code&gt; extension. For example &lt;code&gt;image2.jpg&lt;/code&gt; and &lt;code&gt;image2.txt&lt;/code&gt;. The text file should contain only the caption. You can add the word &lt;code&gt;[trigger]&lt;/code&gt; in the caption file and if you have &lt;code&gt;trigger_word&lt;/code&gt; in your config, it will be automatically replaced.&lt;/p&gt; 
&lt;p&gt;Images are never upscaled but they are downscaled and placed in buckets for batching. &lt;strong&gt;You do not need to crop/resize your images&lt;/strong&gt;. The loader will automatically resize them and can handle varying aspect ratios.&lt;/p&gt; 
&lt;h2&gt;Training Specific Layers&lt;/h2&gt; 
&lt;p&gt;To train specific layers with LoRA, you can use the &lt;code&gt;only_if_contains&lt;/code&gt; network kwargs. For instance, if you want to train only the 2 layers used by The Last Ben, &lt;a href="https://x.com/__TheBen/status/1829554120270987740"&gt;mentioned in this post&lt;/a&gt;, you can adjust your network kwargs like so:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;      network:
        type: "lora"
        linear: 128
        linear_alpha: 128
        network_kwargs:
          only_if_contains:
            - "transformer.single_transformer_blocks.7.proj_out"
            - "transformer.single_transformer_blocks.20.proj_out"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The naming conventions of the layers are in diffusers format, so checking the state dict of a model will reveal the suffix of the name of the layers you want to train. You can also use this method to only train specific groups of weights. For instance to only train the &lt;code&gt;single_transformer&lt;/code&gt; for FLUX.1, you can use the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;      network:
        type: "lora"
        linear: 128
        linear_alpha: 128
        network_kwargs:
          only_if_contains:
            - "transformer.single_transformer_blocks."
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also exclude layers by their names by using &lt;code&gt;ignore_if_contains&lt;/code&gt; network kwarg. So to exclude all the single transformer blocks,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;      network:
        type: "lora"
        linear: 128
        linear_alpha: 128
        network_kwargs:
          ignore_if_contains:
            - "transformer.single_transformer_blocks."
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;ignore_if_contains&lt;/code&gt; takes priority over &lt;code&gt;only_if_contains&lt;/code&gt;. So if a weight is covered by both, if will be ignored.&lt;/p&gt; 
&lt;h2&gt;LoKr Training&lt;/h2&gt; 
&lt;p&gt;To learn more about LoKr, read more about it at &lt;a href="https://github.com/KohakuBlueleaf/LyCORIS/raw/main/docs/Guidelines.md"&gt;KohakuBlueleaf/LyCORIS&lt;/a&gt;. To train a LoKr model, you can adjust the network type in the config file like so:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;      network:
        type: "lokr"
        lokr_full_rank: true
        lokr_factor: 8
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Everything else should work the same including layer targeting.&lt;/p&gt; 
&lt;h2&gt;Updates&lt;/h2&gt; 
&lt;p&gt;Only larger updates are listed here. There are usually smaller daily updated that are omitted.&lt;/p&gt; 
&lt;h3&gt;Jul 17, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Make it easy to add control images to the samples in the ui&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Jul 11, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Added better video config settings to the UI for video models.&lt;/li&gt; 
 &lt;li&gt;Added Wan I2V training to the UI&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;June 29, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fixed issue where Kontext forced sizes on sampling&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;June 26, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Added support for FLUX.1 Kontext training&lt;/li&gt; 
 &lt;li&gt;added support for instruction dataset training&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;June 25, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Added support for OmniGen2 training&lt;/li&gt; 
 &lt;li&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;June 17, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Performance optimizations for batch preparation&lt;/li&gt; 
 &lt;li&gt;Added some docs via a popup for items in the simple ui explaining what settings do. Still a WIP&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;June 16, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Hide control images in the UI when viewing datasets&lt;/li&gt; 
 &lt;li&gt;WIP on mean flow loss&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;June 12, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fixed issue that resulted in blank captions in the dataloader&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;June 10, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Decided to keep track up updates in the readme&lt;/li&gt; 
 &lt;li&gt;Added support for SDXL in the UI&lt;/li&gt; 
 &lt;li&gt;Added support for SD 1.5 in the UI&lt;/li&gt; 
 &lt;li&gt;Fixed UI Wan 2.1 14b name bug&lt;/li&gt; 
 &lt;li&gt;Added support for for conv training in the UI for models that support it&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Comfy-Org/ComfyUI-Manager</title>
      <link>https://github.com/Comfy-Org/ComfyUI-Manager</link>
      <description>&lt;p&gt;ComfyUI-Manager is an extension designed to enhance the usability of ComfyUI. It offers management functions to install, remove, disable, and enable various custom nodes of ComfyUI. Furthermore, this extension provides a hub feature and convenience functions to access a wide range of information within ComfyUI.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ComfyUI Manager&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;ComfyUI-Manager&lt;/strong&gt; is an extension designed to enhance the usability of &lt;a href="https://github.com/comfyanonymous/ComfyUI"&gt;ComfyUI&lt;/a&gt;. It offers management functions to &lt;strong&gt;install, remove, disable, and enable&lt;/strong&gt; various custom nodes of ComfyUI. Furthermore, this extension provides a hub feature and convenience functions to access a wide range of information within ComfyUI.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ltdrdata/ComfyUI-extension-tutorials/refs/heads/Main/ComfyUI-Manager/images/dialog.jpg" alt="menu" /&gt;&lt;/p&gt; 
&lt;h2&gt;NOTICE&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;V3.38: &lt;strong&gt;Security patch&lt;/strong&gt; - Manager data migrated to protected path. See &lt;a href="https://raw.githubusercontent.com/Comfy-Org/ComfyUI-Manager/main/docs/en/v3.38-userdata-security-migration.md"&gt;Migration Guide&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;V3.16: Support for &lt;code&gt;uv&lt;/code&gt; has been added. Set &lt;code&gt;use_uv&lt;/code&gt; in &lt;code&gt;config.ini&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;V3.10: &lt;code&gt;double-click feature&lt;/code&gt; is removed 
  &lt;ul&gt; 
   &lt;li&gt;This feature has been moved to &lt;a href="https://github.com/ltdrdata/comfyui-connection-helper"&gt;https://github.com/ltdrdata/comfyui-connection-helper&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;V3.3.2: Overhauled. Officially supports &lt;a href="https://registry.comfy.org/"&gt;https://registry.comfy.org/&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;You can see whole nodes info on &lt;a href="https://ltdrdata.github.io/"&gt;ComfyUI Nodes Info&lt;/a&gt; page.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Installation[method1] (General installation method: ComfyUI-Manager only)&lt;/h3&gt; 
&lt;p&gt;To install ComfyUI-Manager in addition to an existing installation of ComfyUI, you can follow the following steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Go to &lt;code&gt;ComfyUI/custom_nodes&lt;/code&gt; dir in terminal (cmd)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;git clone https://github.com/ltdrdata/ComfyUI-Manager comfyui-manager&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Restart ComfyUI&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Installation[method2] (Installation for portable ComfyUI version: ComfyUI-Manager only)&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;install git&lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://git-scm.com/download/win"&gt;https://git-scm.com/download/win&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;standalone version&lt;/li&gt; 
 &lt;li&gt;select option: use windows default console window&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Download &lt;a href="https://github.com/ltdrdata/ComfyUI-Manager/raw/main/scripts/install-manager-for-portable-version.bat"&gt;scripts/install-manager-for-portable-version.bat&lt;/a&gt; into installed &lt;code&gt;"ComfyUI_windows_portable"&lt;/code&gt; directory&lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;Don't click. Right-click the link and choose 'Save As...'&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Double-click &lt;code&gt;install-manager-for-portable-version.bat&lt;/code&gt; batch file&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ltdrdata/ComfyUI-extension-tutorials/Main/ComfyUI-Manager/images/portable-install.jpg" alt="portable-install" /&gt;&lt;/p&gt; 
&lt;h3&gt;Installation[method3] (Installation through comfy-cli: install ComfyUI and ComfyUI-Manager at once.)&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;RECOMMENDED: comfy-cli provides various features to manage ComfyUI from the CLI.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;prerequisite: python 3, git&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Windows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-commandline"&gt;python -m venv venv
venv\Scripts\activate
pip install comfy-cli
comfy install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Linux/macOS:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-commandline"&gt;python -m venv venv
. venv/bin/activate
pip install comfy-cli
comfy install
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;See also: &lt;a href="https://github.com/Comfy-Org/comfy-cli"&gt;https://github.com/Comfy-Org/comfy-cli&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installation[method4] (Installation for Linux+venv: ComfyUI + ComfyUI-Manager)&lt;/h3&gt; 
&lt;p&gt;To install ComfyUI with ComfyUI-Manager on Linux using a venv environment, you can follow these steps:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;prerequisite: python-is-python3, python3-venv, git&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol&gt; 
 &lt;li&gt;Download &lt;a href="https://github.com/ltdrdata/ComfyUI-Manager/raw/main/scripts/install-comfyui-venv-linux.sh"&gt;scripts/install-comfyui-venv-linux.sh&lt;/a&gt; into empty install directory&lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;Don't click. Right-click the link and choose 'Save As...'&lt;/li&gt; 
 &lt;li&gt;ComfyUI will be installed in the subdirectory of the specified directory, and the directory will contain the generated executable script.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;&lt;code&gt;chmod +x install-comfyui-venv-linux.sh&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;./install-comfyui-venv-linux.sh&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Installation Precautions&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;DO&lt;/strong&gt;: &lt;code&gt;ComfyUI-Manager&lt;/code&gt; files must be accurately located in the path &lt;code&gt;ComfyUI/custom_nodes/comfyui-manager&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Installing in a compressed file format is not recommended.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DON'T&lt;/strong&gt;: Decompress directly into the &lt;code&gt;ComfyUI/custom_nodes&lt;/code&gt; location, resulting in the Manager contents like &lt;code&gt;__init__.py&lt;/code&gt; being placed directly in that directory. 
  &lt;ul&gt; 
   &lt;li&gt;You have to remove all ComfyUI-Manager files from &lt;code&gt;ComfyUI/custom_nodes&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DON'T&lt;/strong&gt;: In a form where decompression occurs in a path such as &lt;code&gt;ComfyUI/custom_nodes/ComfyUI-Manager/ComfyUI-Manager&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DON'T&lt;/strong&gt;: In a form where decompression occurs in a path such as &lt;code&gt;ComfyUI/custom_nodes/ComfyUI-Manager-main&lt;/code&gt;. 
  &lt;ul&gt; 
   &lt;li&gt;In such cases, &lt;code&gt;ComfyUI-Manager&lt;/code&gt; may operate, but it won't be recognized within &lt;code&gt;ComfyUI-Manager&lt;/code&gt;, and updates cannot be performed. It also poses the risk of duplicate installations. Remove it and install properly via &lt;code&gt;git clone&lt;/code&gt; method.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can execute ComfyUI by running either &lt;code&gt;./run_gpu.sh&lt;/code&gt; or &lt;code&gt;./run_cpu.sh&lt;/code&gt; depending on your system configuration.&lt;/p&gt; 
&lt;h2&gt;Colab Notebook&lt;/h2&gt; 
&lt;p&gt;This repository provides Colab notebooks that allow you to install and use ComfyUI, including ComfyUI-Manager. To use ComfyUI, &lt;a href="https://colab.research.google.com/github/ltdrdata/ComfyUI-Manager/blob/main/notebooks/comfyui_colab_with_manager.ipynb"&gt;click on this link&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Support for installing ComfyUI&lt;/li&gt; 
 &lt;li&gt;Support for basic installation of ComfyUI-Manager&lt;/li&gt; 
 &lt;li&gt;Support for automatically installing dependencies of custom nodes upon restarting Colab notebooks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How To Use&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Click "Manager" button on main menu&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ltdrdata/ComfyUI-extension-tutorials/Main/ComfyUI-Manager/images/topbar.jpg" alt="mainmenu" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If you click on 'Install Custom Nodes' or 'Install Models', an installer dialog will open.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ltdrdata/ComfyUI-extension-tutorials/refs/heads/Main/ComfyUI-Manager/images/dialog.jpg" alt="menu" /&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;There are three DB modes: &lt;code&gt;DB: Channel (1day cache)&lt;/code&gt;, &lt;code&gt;DB: Local&lt;/code&gt;, and &lt;code&gt;DB: Channel (remote)&lt;/code&gt;.&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;Channel (1day cache)&lt;/code&gt; utilizes Channel cache information with a validity period of one day to quickly display the list. 
      &lt;ul&gt; 
       &lt;li&gt;This information will be updated when there is no cache, when the cache expires, or when external information is retrieved through the Channel (remote).&lt;/li&gt; 
       &lt;li&gt;Whenever you start ComfyUI anew, this mode is always set as the &lt;strong&gt;default&lt;/strong&gt; mode.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;Local&lt;/code&gt; uses information stored locally in ComfyUI-Manager. 
      &lt;ul&gt; 
       &lt;li&gt;This information will be updated only when you update ComfyUI-Manager.&lt;/li&gt; 
       &lt;li&gt;For custom node developers, they should use this mode when registering their nodes in &lt;code&gt;custom-node-list.json&lt;/code&gt; and testing them.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;Channel (remote)&lt;/code&gt; retrieves information from the remote channel, always displaying the latest list.&lt;/li&gt; 
     &lt;li&gt;In cases where retrieval is not possible due to network errors, it will forcibly use local information.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;The &lt;code&gt;Fetch Updates&lt;/code&gt; menu retrieves update data for custom nodes locally. Actual updates are applied by clicking the &lt;code&gt;Update&lt;/code&gt; button in the &lt;code&gt;Install Custom Nodes&lt;/code&gt; menu.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Click 'Install' or 'Try Install' button.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ltdrdata/ComfyUI-extension-tutorials/Main/ComfyUI-Manager/images/custom-nodes.jpg" alt="node-install-dialog" /&gt;&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ltdrdata/ComfyUI-extension-tutorials/Main/ComfyUI-Manager/images/models.jpg" alt="model-install-dialog" /&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Installed: This item is already installed.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Install: Clicking this button will install the item.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Try Install: This is a custom node of which installation information cannot be confirmed. Click the button to try installing it.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;If a red background &lt;code&gt;Channel&lt;/code&gt; indicator appears at the top, it means it is not the default channel. Since the amount of information held is different from the default channel, many custom nodes may not appear in this channel state.&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Channel settings have a broad impact, affecting not only the node list but also all functions like "Update all."&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Conflicted Nodes with a yellow background show a list of nodes conflicting with other extensions in the respective extension. This issue needs to be addressed by the developer, and users should be aware that due to these conflicts, some nodes may not function correctly and may need to be installed accordingly.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Share &lt;img src="https://raw.githubusercontent.com/ltdrdata/ComfyUI-extension-tutorials/Main/ComfyUI-Manager/images/topbar.jpg" alt="menu" /&gt; &lt;img src="https://raw.githubusercontent.com/ltdrdata/ComfyUI-extension-tutorials/Main/ComfyUI-Manager/images/share.jpg" alt="share" /&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can share the workflow by clicking the Share button at the bottom of the main menu or selecting Share Output from the Context Menu of the Image node.&lt;/li&gt; 
 &lt;li&gt;Currently, it supports sharing via &lt;a href="https://comfyworkflows.com/"&gt;https://comfyworkflows.com/&lt;/a&gt;, &lt;a href="https://openart.ai/workflows/dev"&gt;https://openart.ai&lt;/a&gt;, &lt;a href="https://youml.com"&gt;https://youml.com&lt;/a&gt; as well as through the Matrix channel.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ltdrdata/ComfyUI-extension-tutorials/Main/ComfyUI-Manager/images/share-setting.jpg" alt="menu" /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Through the Share settings in the Manager menu, you can configure the behavior of the Share button in the Main menu or Share Output button on Context Menu. 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;None&lt;/code&gt;: hide from Main menu&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;All&lt;/code&gt;: Show a dialog where the user can select a title for sharing.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Paths&lt;/h2&gt; 
&lt;p&gt;Starting from V3.38, Manager uses a protected system path for enhanced security.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&amp;lt;USER_DIRECTORY&amp;gt; 
  &lt;ul&gt; 
   &lt;li&gt;If executed without any options, the path defaults to ComfyUI/user.&lt;/li&gt; 
   &lt;li&gt;It can be set using --user-directory &amp;lt;USER_DIRECTORY&amp;gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ComfyUI Version&lt;/th&gt; 
   &lt;th&gt;Manager Path&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;v0.3.76+ (with System User API)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;&amp;lt;USER_DIRECTORY&amp;gt;/__manager/&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Older versions&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;&amp;lt;USER_DIRECTORY&amp;gt;/default/ComfyUI-Manager/&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;Basic config files: &lt;code&gt;config.ini&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Configurable channel lists: &lt;code&gt;channels.list&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Configurable pip overrides: &lt;code&gt;pip_overrides.json&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Configurable pip blacklist: &lt;code&gt;pip_blacklist.list&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Configurable pip auto fix: &lt;code&gt;pip_auto_fix.list&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Saved snapshot files: &lt;code&gt;snapshots/&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Startup script files: &lt;code&gt;startup-scripts/&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Component files: &lt;code&gt;components/&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: See &lt;a href="https://raw.githubusercontent.com/Comfy-Org/ComfyUI-Manager/main/docs/en/v3.38-userdata-security-migration.md"&gt;Migration Guide&lt;/a&gt; for upgrade details.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;code&gt;extra_model_paths.yaml&lt;/code&gt; Configuration&lt;/h2&gt; 
&lt;p&gt;The following settings are applied based on the section marked as &lt;code&gt;is_default&lt;/code&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;custom_nodes&lt;/code&gt;: Path for installing custom nodes 
  &lt;ul&gt; 
   &lt;li&gt;Importing does not need to adhere to the path set as &lt;code&gt;is_default&lt;/code&gt;, but this is the path where custom nodes are installed by the &lt;code&gt;ComfyUI Nodes Manager&lt;/code&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;download_model_base&lt;/code&gt;: Path for downloading models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Snapshot-Manager&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;When you press &lt;code&gt;Save snapshot&lt;/code&gt; or use &lt;code&gt;Update All&lt;/code&gt; on &lt;code&gt;Manager Menu&lt;/code&gt;, the current installation status snapshot is saved. 
  &lt;ul&gt; 
   &lt;li&gt;Snapshot file dir: &lt;code&gt;&amp;lt;USER_DIRECTORY&amp;gt;/default/ComfyUI-Manager/snapshots&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;You can rename snapshot file.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Press the "Restore" button to revert to the installation status of the respective snapshot. 
  &lt;ul&gt; 
   &lt;li&gt;However, for custom nodes not managed by Git, snapshot support is incomplete.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;When you press &lt;code&gt;Restore&lt;/code&gt;, it will take effect on the next ComfyUI startup. 
  &lt;ul&gt; 
   &lt;li&gt;The selected snapshot file is saved in &lt;code&gt;&amp;lt;USER_DIRECTORY&amp;gt;/default/ComfyUI-Manager/startup-scripts/restore-snapshot.json&lt;/code&gt;, and upon restarting ComfyUI, the snapshot is applied and then deleted.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ltdrdata/ComfyUI-extension-tutorials/Main/ComfyUI-Manager/images/snapshot.jpg" alt="model-install-dialog" /&gt;&lt;/p&gt; 
&lt;h2&gt;cm-cli: command line tools for power users&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;A tool is provided that allows you to use the features of ComfyUI-Manager without running ComfyUI.&lt;/li&gt; 
 &lt;li&gt;For more details, please refer to the &lt;a href="https://raw.githubusercontent.com/Comfy-Org/ComfyUI-Manager/main/docs/en/cm-cli.md"&gt;cm-cli documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How to register your custom node into ComfyUI-Manager&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Add an entry to &lt;code&gt;custom-node-list.json&lt;/code&gt; located in the root of ComfyUI-Manager and submit a Pull Request.&lt;/li&gt; 
 &lt;li&gt;NOTE: Before submitting the PR after making changes, please check &lt;code&gt;Use local DB&lt;/code&gt; and ensure that the extension list loads without any issues in the &lt;code&gt;Install custom nodes&lt;/code&gt; dialog. Occasionally, missing or extra commas can lead to JSON syntax errors.&lt;/li&gt; 
 &lt;li&gt;The remaining JSON will be updated through scripts in the future, so you don't need to worry about it.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Custom node support guide&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;NOTICE:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;You should no longer assume that the GitHub repository name will match the subdirectory name under &lt;code&gt;custom_nodes&lt;/code&gt;. The name of the subdirectory under &lt;code&gt;custom_nodes&lt;/code&gt; will now use the normalized name from the &lt;code&gt;name&lt;/code&gt; field in &lt;code&gt;pyproject.toml&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;Avoid relying on directory names for imports whenever possible.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://docs.comfy.org/registry/overview"&gt;https://docs.comfy.org/registry/overview&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/Comfy-Org/rfcs"&gt;https://github.com/Comfy-Org/rfcs&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Special purpose files&lt;/strong&gt; (optional)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;pyproject.toml&lt;/code&gt; - Spec file for comfyregistry.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;node_list.json&lt;/code&gt; - When your custom nodes pattern of NODE_CLASS_MAPPINGS is not conventional, it is used to manually provide a list of nodes for reference. (&lt;a href="https://github.com/melMass/comfy_mtb/raw/main/node_list.json"&gt;example&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;requirements.txt&lt;/code&gt; - When installing, this pip requirements will be installed automatically&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;install.py&lt;/code&gt; - When installing, it is automatically called&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;All scripts are executed from the root path of the corresponding custom node.&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Component Sharing&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Copy &amp;amp; Paste&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://ltdrdata.github.io/component-demo/"&gt;Demo Page&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;When pasting a component from the clipboard, it supports text in the following JSON format. (text/plain) &lt;pre&gt;&lt;code&gt;{
  "kind": "ComfyUI Components",
  "timestamp": &amp;lt;current timestamp&amp;gt;,
  "components": 
    {
      &amp;lt;component name&amp;gt;: &amp;lt;component nodedata&amp;gt;
    }
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;&amp;lt;current timestamp&amp;gt;&lt;/code&gt; Ensure that the timestamp is always unique. 
    &lt;ul&gt; 
     &lt;li&gt;"components" should have the same structure as the content of the file stored in &lt;code&gt;&amp;lt;USER_DIRECTORY&amp;gt;/default/ComfyUI-Manager/components&lt;/code&gt;. 
      &lt;ul&gt; 
       &lt;li&gt;&lt;code&gt;&amp;lt;component name&amp;gt;&lt;/code&gt;: The name should be in the format &lt;code&gt;&amp;lt;prefix&amp;gt;::&amp;lt;node name&amp;gt;&lt;/code&gt;. 
        &lt;ul&gt; 
         &lt;li&gt;&lt;code&gt;&amp;lt;component node data&amp;gt;&lt;/code&gt;: In the node data of the group node. 
          &lt;ul&gt; 
           &lt;li&gt;&lt;code&gt;&amp;lt;version&amp;gt;&lt;/code&gt;: Only two formats are allowed: &lt;code&gt;major.minor.patch&lt;/code&gt; or &lt;code&gt;major.minor&lt;/code&gt;. (e.g. &lt;code&gt;1.0&lt;/code&gt;, &lt;code&gt;2.2.1&lt;/code&gt;)&lt;/li&gt; 
           &lt;li&gt;&lt;code&gt;&amp;lt;datetime&amp;gt;&lt;/code&gt;: Saved time&lt;/li&gt; 
           &lt;li&gt;&lt;code&gt;&amp;lt;packname&amp;gt;&lt;/code&gt;: If the packname is not empty, the category becomes packname/workflow, and it is saved in the 
            &lt;packname&gt;
             .pack file in 
             &lt;code&gt;&amp;lt;USER_DIRECTORY&amp;gt;/default/ComfyUI-Manager/components&lt;/code&gt;.
            &lt;/packname&gt;&lt;/li&gt; 
           &lt;li&gt;&lt;code&gt;&amp;lt;category&amp;gt;&lt;/code&gt;: If there is neither a category nor a packname, it is saved in the components category.&lt;/li&gt; 
          &lt;/ul&gt; &lt;pre&gt;&lt;code&gt;    "version":"1.0",
    "datetime": 1705390656516,
    "packname": "mypack",
    "category": "util/pipe",
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
        &lt;/ul&gt; &lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Drag &amp;amp; Drop&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Dragging and dropping a &lt;code&gt;.pack&lt;/code&gt; or &lt;code&gt;.json&lt;/code&gt; file will add the corresponding components.&lt;/li&gt; 
   &lt;li&gt;Example pack: &lt;a href="https://raw.githubusercontent.com/Comfy-Org/ComfyUI-Manager/main/misc/Impact.pack"&gt;Impact.pack&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Dragging and dropping or pasting a single component will add a node. However, when adding multiple components, nodes will not be added.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Support for installing missing nodes&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ltdrdata/ComfyUI-extension-tutorials/Main/ComfyUI-Manager/images/missing-menu.jpg" alt="missing-menu" /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;When you click on the &lt;code&gt;Install Missing Custom Nodes&lt;/code&gt; button in the menu, it displays a list of extension nodes that contain nodes not currently present in the workflow.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ltdrdata/ComfyUI-extension-tutorials/Main/ComfyUI-Manager/images/missing-list.jpg" alt="missing-list" /&gt;&lt;/p&gt; 
&lt;h1&gt;Config&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;You can modify the &lt;code&gt;config.ini&lt;/code&gt; file to apply the settings for ComfyUI-Manager.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;The path to the &lt;code&gt;config.ini&lt;/code&gt; used by ComfyUI-Manager is displayed in the startup log messages.&lt;/li&gt; 
   &lt;li&gt;See also: [https://github.com/ltdrdata/ComfyUI-Manager#paths]&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Configuration options:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[default]
git_exe = &amp;lt;Manually specify the path to the git executable. If left empty, the default git executable path will be used.&amp;gt;
use_uv = &amp;lt;Use uv instead of pip for dependency installation.&amp;gt;
default_cache_as_channel_url = &amp;lt;Determines whether to retrieve the DB designated as channel_url at startup&amp;gt;
bypass_ssl = &amp;lt;Set to True if SSL errors occur to disable SSL.&amp;gt;
file_logging = &amp;lt;Configure whether to create a log file used by ComfyUI-Manager.&amp;gt;
windows_selector_event_loop_policy = &amp;lt;If an event loop error occurs on Windows, set this to True.&amp;gt;
model_download_by_agent = &amp;lt;When downloading models, use an agent instead of torchvision_download_url.&amp;gt;
downgrade_blacklist = &amp;lt;Set a list of packages to prevent downgrades. List them separated by commas.&amp;gt;
security_level = &amp;lt;Set the security level =&amp;gt; strong|normal|normal-|weak&amp;gt;
always_lazy_install = &amp;lt;Whether to perform dependency installation on restart even in environments other than Windows.&amp;gt;
network_mode = &amp;lt;Set the network mode =&amp;gt; public|private|offline&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
  &lt;ul&gt; 
   &lt;li&gt;network_mode: 
    &lt;ul&gt; 
     &lt;li&gt;public: An environment that uses a typical public network.&lt;/li&gt; 
     &lt;li&gt;private: An environment that uses a closed network, where a private node DB is configured via &lt;code&gt;channel_url&lt;/code&gt;. (Uses cache if available)&lt;/li&gt; 
     &lt;li&gt;offline: An environment that does not use any external connections when using an offline network. (Uses cache if available)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Additional Feature&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Logging to file feature&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;This feature is enabled by default and can be disabled by setting &lt;code&gt;file_logging = False&lt;/code&gt; in the &lt;code&gt;config.ini&lt;/code&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Fix node (recreate): When right-clicking on a node and selecting &lt;code&gt;Fix node (recreate)&lt;/code&gt;, you can recreate the node. The widget's values are reset, while the connections maintain those with the same names.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;It is used to correct errors in nodes of old workflows created before, which are incompatible with the version changes of custom nodes.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Double-Click Node Title: You can set the double-click behavior of nodes in the ComfyUI-Manager menu.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;Copy All Connections&lt;/code&gt;, &lt;code&gt;Copy Input Connections&lt;/code&gt;: Double-clicking a node copies the connections of the nearest node.&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;This action targets the nearest node within a straight-line distance of 1000 pixels from the center of the node.&lt;/li&gt; 
     &lt;li&gt;In the case of &lt;code&gt;Copy All Connections&lt;/code&gt;, it duplicates existing outputs, but since it does not allow duplicate connections, the existing output connections of the original node are disconnected.&lt;/li&gt; 
     &lt;li&gt;This feature copies only the input and output that match the names.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;Possible Input Connections&lt;/code&gt;: It connects all outputs that match the closest type within the specified range.&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;This connection links to the closest outputs among the nodes located on the left side of the target node.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;Possible(left) + Copy(right)&lt;/code&gt;: When you Double-Click on the left half of the title, it operates as &lt;code&gt;Possible Input Connections&lt;/code&gt;, and when you Double-Click on the right half, it operates as &lt;code&gt;Copy All Connections&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Prevent downgrade of specific packages&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;List the package names in the &lt;code&gt;downgrade_blacklist&lt;/code&gt; section of the &lt;code&gt;config.ini&lt;/code&gt; file, separating them with commas. 
    &lt;ul&gt; 
     &lt;li&gt;e.g&lt;/li&gt; 
    &lt;/ul&gt; &lt;pre&gt;&lt;code&gt;  downgrade_blacklist = diffusers, kornia
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Custom pip mapping&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;When you create the &lt;code&gt;pip_overrides.json&lt;/code&gt; file, it changes the installation of specific pip packages to installations defined by the user. 
    &lt;ul&gt; 
     &lt;li&gt;Please refer to the &lt;code&gt;pip_overrides.json.template&lt;/code&gt; file.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Prevent the installation of specific pip packages&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;List the package names one per line in the &lt;code&gt;pip_blacklist.list&lt;/code&gt; file.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Automatically Restoring pip Installation&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If you list pip spec requirements in &lt;code&gt;pip_auto_fix.list&lt;/code&gt;, similar to &lt;code&gt;requirements.txt&lt;/code&gt;, it will automatically restore the specified versions when starting ComfyUI or when versions get mismatched during various custom node installations.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--index-url&lt;/code&gt; can be used.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Use &lt;code&gt;aria2&lt;/code&gt; as downloader&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Comfy-Org/ComfyUI-Manager/main/docs/en/use_aria2.md"&gt;howto&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Environment Variables&lt;/h2&gt; 
&lt;p&gt;The following features can be configured using environment variables:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;COMFYUI_PATH&lt;/strong&gt;: The installation path of ComfyUI&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GITHUB_ENDPOINT&lt;/strong&gt;: Reverse proxy configuration for environments with limited access to GitHub&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;HF_ENDPOINT&lt;/strong&gt;: Reverse proxy configuration for environments with limited access to Hugging Face&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Example 1:&lt;/h3&gt; 
&lt;p&gt;Redirecting &lt;code&gt;https://github.com/ltdrdata/ComfyUI-Impact-Pack&lt;/code&gt; to &lt;code&gt;https://mirror.ghproxy.com/https://github.com/ltdrdata/ComfyUI-Impact-Pack&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;GITHUB_ENDPOINT=https://mirror.ghproxy.com/https://github.com
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Example 2:&lt;/h4&gt; 
&lt;p&gt;Changing &lt;code&gt;https://huggingface.co/path/to/somewhere&lt;/code&gt; to &lt;code&gt;https://some-hf-mirror.com/path/to/somewhere&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;HF_ENDPOINT=https://some-hf-mirror.com 
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Scanner&lt;/h2&gt; 
&lt;p&gt;When you run the &lt;code&gt;scan.sh&lt;/code&gt; script:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;It updates the &lt;code&gt;extension-node-map.json&lt;/code&gt;.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;To do this, it pulls or clones the custom nodes listed in &lt;code&gt;custom-node-list.json&lt;/code&gt; into &lt;code&gt;~/.tmp/default&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;To skip this step, add the &lt;code&gt;--skip-update&lt;/code&gt; option.&lt;/li&gt; 
   &lt;li&gt;If you want to specify a different path instead of &lt;code&gt;~/.tmp/default&lt;/code&gt;, run &lt;code&gt;python scanner.py [path]&lt;/code&gt; directly instead of &lt;code&gt;scan.sh&lt;/code&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;It updates the &lt;code&gt;github-stats.json&lt;/code&gt;.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;This uses the GitHub API, so set your token with &lt;code&gt;export GITHUB_TOKEN=your_token_here&lt;/code&gt; to avoid quickly reaching the rate limit and malfunctioning.&lt;/li&gt; 
   &lt;li&gt;To skip this step, add the &lt;code&gt;--skip-stat-update&lt;/code&gt; option.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The &lt;code&gt;--skip-all&lt;/code&gt; option applies both &lt;code&gt;--skip-update&lt;/code&gt; and &lt;code&gt;--skip-stat-update&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;If your &lt;code&gt;git.exe&lt;/code&gt; is installed in a specific location other than system git, please install ComfyUI-Manager and run ComfyUI. Then, specify the path including the file name in &lt;code&gt;git_exe = &lt;/code&gt; in the &lt;code&gt;&amp;lt;USER_DIRECTORY&amp;gt;/default/ComfyUI-Manager/config.ini&lt;/code&gt; file that is generated.&lt;/li&gt; 
 &lt;li&gt;If updating ComfyUI-Manager itself fails, please go to the &lt;strong&gt;ComfyUI-Manager&lt;/strong&gt; directory and execute the command &lt;code&gt;git update-ref refs/remotes/origin/main a361cc1 &amp;amp;&amp;amp; git fetch --all &amp;amp;&amp;amp; git pull&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;If you encounter the error message &lt;code&gt;Overlapped Object has pending operation at deallocation on ComfyUI Manager load&lt;/code&gt; under Windows 
  &lt;ul&gt; 
   &lt;li&gt;Edit &lt;code&gt;config.ini&lt;/code&gt; file: add &lt;code&gt;windows_selector_event_loop_policy = True&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;If the &lt;code&gt;SSL: CERTIFICATE_VERIFY_FAILED&lt;/code&gt; error occurs. 
  &lt;ul&gt; 
   &lt;li&gt;Edit &lt;code&gt;config.ini&lt;/code&gt; file: add &lt;code&gt;bypass_ssl = True&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Security policy&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Edit &lt;code&gt;config.ini&lt;/code&gt; file: add &lt;code&gt;security_level = &amp;lt;LEVEL&amp;gt;&lt;/code&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;strong&lt;/code&gt; 
    &lt;ul&gt; 
     &lt;li&gt;doesn't allow &lt;code&gt;high&lt;/code&gt; and &lt;code&gt;middle&lt;/code&gt; level risky feature&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;normal&lt;/code&gt; 
    &lt;ul&gt; 
     &lt;li&gt;doesn't allow &lt;code&gt;high&lt;/code&gt; level risky feature&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;middle&lt;/code&gt; level risky feature is available&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;normal-&lt;/code&gt; 
    &lt;ul&gt; 
     &lt;li&gt;doesn't allow &lt;code&gt;high&lt;/code&gt; level risky feature if &lt;code&gt;--listen&lt;/code&gt; is specified and not starts with &lt;code&gt;127.&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;middle&lt;/code&gt; level risky feature is available&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;weak&lt;/code&gt; 
    &lt;ul&gt; 
     &lt;li&gt;all feature is available&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;high&lt;/code&gt; level risky features&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;Install via git url&lt;/code&gt;, &lt;code&gt;pip install&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Installation of custom nodes registered not in the &lt;code&gt;default channel&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;Fix custom nodes&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;middle&lt;/code&gt; level risky features&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Uninstall/Update&lt;/li&gt; 
   &lt;li&gt;Installation of custom nodes registered in the &lt;code&gt;default channel&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;Restore/Remove Snapshot&lt;/li&gt; 
   &lt;li&gt;Restart&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;low&lt;/code&gt; level risky features&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Update ComfyUI&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Disclaimer&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;This extension simply provides the convenience of installing custom nodes and does not guarantee their proper functioning.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Credit&lt;/h2&gt; 
&lt;p&gt;ComfyUI/&lt;a href="https://github.com/comfyanonymous/ComfyUI"&gt;ComfyUI&lt;/a&gt; - A powerful and modular stable diffusion GUI.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;And, for all ComfyUI custom node developers&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/magentic-ui</title>
      <link>https://github.com/microsoft/magentic-ui</link>
      <description>&lt;p&gt;A research prototype of a human-centered web agent&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/microsoft/magentic-ui/main/docs/img/magui-readme-logo.svg?sanitize=true" alt="Magentic-UI Logo" /&gt; 
 &lt;p&gt;&lt;em&gt;Automate your web tasks while you stay in control&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://pypi.python.org/pypi/magentic_ui"&gt;&lt;img src="https://img.shields.io/pypi/v/magentic_ui.svg?sanitize=true" alt="image" /&gt;&lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/magentic_ui"&gt;&lt;img src="https://img.shields.io/pypi/l/magentic_ui.svg?sanitize=true" alt="image" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/python-3.10%20%7C%203.11%20%7C%203.12%20%7C%203.13-blue" alt="Python Versions" /&gt; &lt;a href="https://arxiv.org/abs/2507.22358"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2507.22358-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;Magentic-UI is a &lt;strong&gt;research prototype&lt;/strong&gt; human-centered AI agent that solves complex web and coding tasks that may require monitoring. Unlike other black-box agents, the system reveals its plan before executions, lets you guide its actions, and requests approval for sensitive operations while browsing websites, executing code, and analyzing files. &lt;em&gt;Check out the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#demos"&gt;demo section&lt;/a&gt; for inspiration on what tasks you can accomplish.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;‚ú® What's New&lt;/h2&gt; 
&lt;p&gt;Microsoft latest agentic model &lt;a href="https://www.microsoft.com/en-us/research/blog/fara-7b-an-efficient-agentic-model-for-computer-use/"&gt;Fara-7B&lt;/a&gt; is now integrated in Magentic-UI, read how to launch in &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#fara-7b"&gt; Fara-7B guide&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;"Tell me When"&lt;/strong&gt;: Automate monitoring tasks and repeatable workflows that require web or API access that span minutes to days. &lt;em&gt;Learn more &lt;a href="https://www.microsoft.com/en-us/research/blog/tell-me-when-building-agents-that-can-wait-monitor-and-act/"&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;File Upload Support&lt;/strong&gt;: Upload any file through the UI for analysis or modification&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP Agents&lt;/strong&gt;: Extend capabilities with your favorite MCP servers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easier Installation&lt;/strong&gt;: We have uploaded our docker containers to GHCR so you no longer need to build any containers! Installation time now is much quicker.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;p&gt;Here's how you can get started with Magentic-UI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Setup environment
python3 -m venv .venv
source .venv/bin/activate
pip install magentic-ui --upgrade

# 2. Set your API key
export OPENAI_API_KEY="your-api-key-here"

# 3. Launch Magentic-UI
magentic-ui --port 8081
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then open &lt;a href="http://localhost:8081"&gt;http://localhost:8081&lt;/a&gt; in your browser to interact with Magentic-UI!&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;: Requires Docker and Python 3.10+. Windows users should use WSL2. See &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#%EF%B8%8F-installation"&gt;detailed installation&lt;/a&gt; for more info.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Alternative Usage Options&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Without Docker&lt;/strong&gt; (limited functionality: no code execution):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-ui --run-without-docker --port 8081
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Command Line Interface&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-cli --work-dir PATH/TO/STORE/DATA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Custom LLM Clients&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Azure
pip install magentic-ui[azure]

# Ollama (local models)
pip install magentic-ui[ollama]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can then pass a config file to the &lt;code&gt;magentic-ui&lt;/code&gt; command (&lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#model-client-configuration"&gt; client config&lt;/a&gt;) or change the model client inside the UI settings.&lt;/p&gt; 
&lt;p&gt;For further details on installation please read the &lt;a href="#Ô∏è-installation"&gt;üõ†Ô∏è Installation&lt;/a&gt; section. For common installation issues and their solutions, please refer to the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/TROUBLESHOOTING.md"&gt;troubleshooting document&lt;/a&gt;. See advanced usage instructions with the command &lt;code&gt;magentic-ui --help&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Quick Navigation:&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#demos"&gt;üé¨ Demos&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#how-it-works"&gt;üü™ How it Works&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#installation"&gt;üõ†Ô∏è Installation&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#troubleshooting"&gt;‚ö†Ô∏è Troubleshooting&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#contributing"&gt;ü§ù Contributing&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#license"&gt;üìÑ License&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Demos&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="33%" align="center"&gt; &lt;p&gt;&lt;strong&gt;üçï Pizza Ordering&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;Web automation with human-in-the-loop&lt;/em&gt;&lt;/p&gt; 
    &lt;video src="https://github.com/user-attachments/assets/dc95cf5f-c4b4-4fe0-b708-158ff071e5a9" width="100%" style="max-height: 300px;"&gt; 
    &lt;/video&gt; &lt;/td&gt; 
   &lt;td width="33%" align="center"&gt; &lt;p&gt;&lt;strong&gt;üè† Airbnb Price Analysis&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;MCP agent integration&lt;/em&gt;&lt;/p&gt; 
    &lt;video src="https://github.com/user-attachments/assets/c19ed8c2-e06f-43b7-bee3-5e2ffc4c5e02" width="100%" style="max-height: 300px;"&gt; 
    &lt;/video&gt; &lt;/td&gt; 
   &lt;td width="33%" align="center"&gt; &lt;p&gt;&lt;strong&gt;‚≠ê Star Monitoring&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;Long-running monitoring task&lt;/em&gt;&lt;/p&gt; 
    &lt;video src="https://github.com/user-attachments/assets/d2a463ca-7a94-4414-932d-a69f30fff63b" width="100%" style="max-height: 300px;"&gt; 
    &lt;/video&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;How it Works&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/magentic-ui/main/docs/img/magenticui_running.png" alt="Magentic-UI" height="400" /&gt; &lt;/p&gt; 
&lt;p&gt;Magentic-UI is especially useful for web tasks that require actions on the web (e.g., filling a form, customizing a food order), deep navigation through websites not indexed by search engines (e.g., filtering flights, finding a link from a personal site) or tasks that need web navigation and code execution (e.g., generate a chart from online data).&lt;/p&gt; 
&lt;p&gt;What differentiates Magentic-UI from other browser use offerings is its transparent and controllable interface that allows for efficient human-in-the-loop involvement. Magentic-UI is built using &lt;a href="https://github.com/microsoft/autogen"&gt;AutoGen&lt;/a&gt; and provides a platform to study human-agent interaction and experiment with web agents. Key features include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üßë‚Äçü§ù‚Äçüßë &lt;strong&gt;Co-Planning&lt;/strong&gt;: Collaboratively create and approve step-by-step plans using chat and the plan editor.&lt;/li&gt; 
 &lt;li&gt;ü§ù &lt;strong&gt;Co-Tasking&lt;/strong&gt;: Interrupt and guide the task execution using the web browser directly or through chat. Magentic-UI can also ask for clarifications and help when needed.&lt;/li&gt; 
 &lt;li&gt;üõ°Ô∏è &lt;strong&gt;Action Guards&lt;/strong&gt;: Sensitive actions are only executed with explicit user approvals.&lt;/li&gt; 
 &lt;li&gt;üß† &lt;strong&gt;Plan Learning and Retrieval&lt;/strong&gt;: Learn from previous runs to improve future task automation and save them in a plan gallery. Automatically or manually retrieve saved plans in future tasks.&lt;/li&gt; 
 &lt;li&gt;üîÄ &lt;strong&gt;Parallel Task Execution&lt;/strong&gt;: You can run multiple tasks in parallel and session status indicators will let you know when Magentic-UI needs your input or has completed the task.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://www.youtube.com/watch?v=wOs-5SR8xOc" target="_blank"&gt; &lt;img src="https://img.youtube.com/vi/wOs-5SR8xOc/maxresdefault.jpg" alt="Watch the demo video" width="600" /&gt; &lt;/a&gt; 
 &lt;br /&gt; ‚ñ∂Ô∏è 
 &lt;em&gt; Click to watch a video and learn more about Magentic-UI &lt;/em&gt; 
&lt;/div&gt; 
&lt;h3&gt;Autonomous Evaluation&lt;/h3&gt; 
&lt;p&gt;To evaluate its autonomous capabilities, Magentic-UI has been tested against several benchmarks when running with o4-mini: &lt;a href="https://huggingface.co/datasets/gaia-benchmark/GAIA"&gt;GAIA&lt;/a&gt; test set (42.52%), which assesses general AI assistants across reasoning, tool use, and web interaction tasks ; &lt;a href="https://huggingface.co/AssistantBench"&gt;AssistantBench&lt;/a&gt; test set (27.60%), focusing on realistic, time-consuming web tasks; &lt;a href="https://github.com/MinorJerry/WebVoyager"&gt;WebVoyager&lt;/a&gt; (82.2%), measuring end-to-end web navigation in real-world scenarios; and &lt;a href="https://webgames.convergence.ai/"&gt;WebGames&lt;/a&gt; (45.5%), evaluating general-purpose web-browsing agents through interactive challenges. To reproduce these experimental results, please see the following &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/experiments/eval/README.md"&gt;instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you're interested in reading more checkout our &lt;a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/magentic-ui-report.pdf"&gt;technical report&lt;/a&gt; and &lt;a href="https://www.microsoft.com/en-us/research/blog/magentic-ui-an-experimental-human-centered-web-agent/"&gt;blog post&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Pre-Requisites&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you're using Windows, we highly recommend using &lt;a href="https://docs.microsoft.com/en-us/windows/wsl/install"&gt;WSL2&lt;/a&gt; (Windows Subsystem for Linux).&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;If running on &lt;strong&gt;Windows&lt;/strong&gt; or &lt;strong&gt;Mac&lt;/strong&gt; you should use &lt;a href="https://www.docker.com/products/docker-desktop/"&gt;Docker Desktop&lt;/a&gt; or if inside WSL2 you can install Docker directly inside WSL &lt;a href="https://gist.github.com/dehsilvadeveloper/c3bdf0f4cdcc5c177e2fe9be671820c7"&gt;docker in WSL2 guide&lt;/a&gt;. If running on &lt;strong&gt;Linux&lt;/strong&gt;, you should use &lt;a href="https://docs.docker.com/engine/install/"&gt;Docker Engine&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;If using Docker Desktop, make sure it is set up to use WSL2: - Go to Settings &amp;gt; Resources &amp;gt; WSL Integration - Enable integration with your development distro You can find more detailed instructions about this step &lt;a href="https://docs.microsoft.com/en-us/windows/wsl/tutorials/wsl-containers"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt; &lt;p&gt;During the Installation step, you will need to set up your &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;. To use other models, review the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#model-client-configuration"&gt;Model Client Configuration&lt;/a&gt; section below.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;You need at least &lt;a href="https://www.python.org/downloads/"&gt;Python 3.10&lt;/a&gt; installed.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;If you are on Windows, we recommend to run Magentic-UI inside &lt;a href="https://docs.microsoft.com/en-us/windows/wsl/install"&gt;WSL2&lt;/a&gt; (Windows Subsystem for Linux) for correct Docker and file path compatibility.&lt;/p&gt; 
&lt;h3&gt;PyPI Installation&lt;/h3&gt; 
&lt;p&gt;Magentic-UI is available on PyPI. We recommend using a virtual environment to avoid conflicts with other packages.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3 -m venv .venv
source .venv/bin/activate
pip install magentic-ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, if you use &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;&lt;code&gt;uv&lt;/code&gt;&lt;/a&gt; for dependency management, you can install Magentic-UI with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv --python=3.12 .venv
. .venv/bin/activate
uv pip install magentic-ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running Magentic-UI&lt;/h3&gt; 
&lt;p&gt;To run Magentic-UI, make sure that Docker is running, then run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-ui --port 8081
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Running this command for the first time will pull two docker images required for the Magentic-UI agents. If you encounter problems, you can build them directly with the following command:&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd docker
sh build-all.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you face issues with Docker, please refer to the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/TROUBLESHOOTING.md"&gt;TROUBLESHOOTING.md&lt;/a&gt; document.&lt;/p&gt; 
&lt;p&gt;Once the server is running, you can access the UI at &lt;a href="http://localhost:8081"&gt;http://localhost:8081&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Fara-7B&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;First install magentic-ui with the fara extras:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3 -m venv .venv
source .venv/bin/activate
pip install magentic-ui[fara]
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;In a seperate process, serve the Fara-7B model using vLLM:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vllm serve "microsoft/Fara-7B" --port 5000 --dtype auto 
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;First create a &lt;code&gt;fara_config.yaml&lt;/code&gt; file with the following content:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;model_config_local_surfer: &amp;amp;client_surfer
  provider: OpenAIChatCompletionClient
  config:
    model: "microsoft/Fara-7B"
    base_url: http://localhost:5000/v1
    api_key: not-needed
    model_info:
      vision: true
      function_calling: true
      json_output: false
      family: "unknown" 
      structured_output: false
      multiple_system_messages: false

orchestrator_client: *client_surfer
coder_client: *client_surfer
web_surfer_client: *client_surfer
file_surfer_client: *client_surfer
action_guard_client: *client_surfer
model_client: *client_surfer
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: if you are hosting vLLM on a different port or host, change the &lt;code&gt;base_url&lt;/code&gt; accordingly.&lt;/p&gt; 
&lt;p&gt;Then launch Magentic-UI with the fara agent:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-ui --fara --port 8081 --config fara_config.yaml 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Finally, navigate to &lt;a href="http://localhost:8081"&gt;http://localhost:8081&lt;/a&gt; to access the interface!&lt;/p&gt; 
&lt;h3&gt;Configuration&lt;/h3&gt; 
&lt;h4&gt;Model Client Configuration&lt;/h4&gt; 
&lt;p&gt;If you want to use a different OpenAI key, or if you want to configure use with Azure OpenAI or Ollama, you can do so inside the UI by navigating to settings (top right icon) and changing model configuration. Another option is to pass a yaml config file when you start Magentic-UI which will override any settings in the UI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-ui --port 8081 --config config.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Where the &lt;code&gt;config.yaml&lt;/code&gt; should look as follows with an AutoGen model client configuration:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;gpt4o_client: &amp;amp;gpt4o_client
    provider: OpenAIChatCompletionClient
    config:
      model: gpt-4o-2024-08-06
      api_key: null
      base_url: null
      max_retries: 5

orchestrator_client: *gpt4o_client
coder_client: *gpt4o_client
web_surfer_client: *gpt4o_client
file_surfer_client: *gpt4o_client
action_guard_client: *gpt4o_client
plan_learning_client: *gpt4o_client
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can change the client for each of the agents using the config file and use AzureOpenAI (&lt;code&gt;AzureOpenAIChatCompletionClient&lt;/code&gt;), Ollama and other clients.&lt;/p&gt; 
&lt;h4&gt;MCP Server Configuration&lt;/h4&gt; 
&lt;p&gt;You can also extend Magentic-UI's capabilities by adding custom "McpAgents" to the multi-agent team. Each McpAgent can have access to one or more MCP Servers. You can specify these agents via the &lt;code&gt;mcp_agent_configs&lt;/code&gt; parameter in your &lt;code&gt;config.yaml&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For example, here's an agent called "airbnb_surfer" that has access to the OpenBnb MCP Server running locally via Stdio.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;mcp_agent_configs:
  - name: airbnb_surfer
    description: "The airbnb_surfer has direct access to AirBnB."
    model_client: 
      provider: OpenAIChatCompletionClient
      config:
        model: gpt-4.1-2025-04-14
      max_retries: 10
    system_message: |-
      You are AirBnb Surfer, a helpful digital assistant that can help users acces AirBnB.

      You have access to a suite of tools provided by the AirBnB API. Use those tools to satisfy the users requests.
    reflect_on_tool_use: false
    mcp_servers:
      - server_name: AirBnB
        server_params:
          type: StdioServerParams
          command: npx
          args:
            - -y
            - "@openbnb/mcp-server-airbnb"
            - --ignore-robots-txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Under the hood, each &lt;code&gt;McpAgent&lt;/code&gt; is just a &lt;code&gt;autogen_agentchat.agents.AssistantAgent&lt;/code&gt; with the set of MCP Servers exposed as an &lt;code&gt;AggregateMcpWorkbench&lt;/code&gt; which is simply a named collection of &lt;code&gt;autogen_ext.tools.mcp.McpWorkbench&lt;/code&gt; objects (one per MCP Server).&lt;/p&gt; 
&lt;p&gt;Currently the supported MCP Server types are &lt;code&gt;autogen_ext.tools.mcp.StdioServerParams&lt;/code&gt; and &lt;code&gt;autogen_ext.tools.mcp.SseServerParams&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Building Magentic-UI from source&lt;/h3&gt; 
&lt;p&gt;This step is primarily for users seeking to make modifications to the code, are having trouble with the pypi installation or want the latest code before a pypi version release.&lt;/p&gt; 
&lt;h4&gt;1. Make sure the above prerequisites are installed, and that Docker is running.&lt;/h4&gt; 
&lt;h4&gt;2. Clone the repository to your local machine:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/microsoft/magentic-ui.git
cd magentic-ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3. Install Magentic-UI's dependencies with uv or your favorite package manager:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# install uv through https://docs.astral.sh/uv/getting-started/installation/
uv venv --python=3.12 .venv
uv sync --all-extras
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;4. Build the frontend:&lt;/h4&gt; 
&lt;p&gt;First make sure to install node:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# install nvm to install node
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.1/install.sh | bash
nvm install node
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then install the frontend:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd frontend
npm install -g gatsby-cli
npm install --global yarn
yarn install
yarn build
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;5. Run Magentic-UI, as usual.&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-ui --port 8081
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Running the UI from source&lt;/h4&gt; 
&lt;p&gt;If you are making changes to the source code of the UI, you can run the frontend in development mode so that it will automatically update when you make changes for faster development.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open a separate terminal and change directory to the frontend&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd frontend
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Create a &lt;code&gt;.env.development&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cp .env.default .env.development
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Launch frontend server&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm run start
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Then run the UI:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-ui --port 8081
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The frontend from source will be available at &lt;a href="http://localhost:8000"&gt;http://localhost:8000&lt;/a&gt;, and the compiled frontend will be available at &lt;a href="http://localhost:8081"&gt;http://localhost:8081&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;p&gt;If you were unable to get Magentic-UI running, do not worry! The first step is to make sure you have followed the steps outlined above, particularly with the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#pre-requisites"&gt;pre-requisites&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For common issues and their solutions, please refer to the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/TROUBLESHOOTING.md"&gt;TROUBLESHOOTING.md&lt;/a&gt; file in this repository. If you do not see your problem there, please open a &lt;code&gt;GitHub Issue&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. For information about contributing to Magentic-UI, please see our &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; guide, which includes current issues to be resolved and other forms of contributing.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information, see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;Please cite our paper if you use our work in your research:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{mozannar2025magentic,
  title={Magentic-UI: Towards Human-in-the-loop Agentic Systems},
  author={Mozannar, Hussein and Bansal, Gagan and Tan, Cheng and Fourney, Adam and Dibia, Victor and Chen, Jingya and Gerrits, Jack and Payne, Tyler and Maldaner, Matheus Kunzler and Grunde-McLaughlin, Madeleine and others},
  journal={arXiv preprint arXiv:2507.22358},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Microsoft, and any contributors, grant you a license to any code in the repository under the &lt;a href="https://opensource.org/licenses/MIT"&gt;MIT License&lt;/a&gt;. See the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/LICENSE"&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt; 
&lt;p&gt;Microsoft, Windows, Microsoft Azure, and/or other Microsoft products and services referenced in the documentation may be either trademarks or registered trademarks of Microsoft in the United States and/or other countries. The licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks. Microsoft's general trademark guidelines can be found at &lt;a href="http://go.microsoft.com/fwlink/?LinkID=254653"&gt;http://go.microsoft.com/fwlink/?LinkID=254653&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Any use of third-party trademarks or logos are subject to those third-party's policies.&lt;/p&gt; 
&lt;p&gt;Privacy information can be found at &lt;a href="https://go.microsoft.com/fwlink/?LinkId=521839"&gt;https://go.microsoft.com/fwlink/?LinkId=521839&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Microsoft and any contributors reserve all other rights, whether under their respective copyrights, patents, or trademarks, whether by implication, estoppel, or otherwise.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/VibeVoice</title>
      <link>https://github.com/microsoft/VibeVoice</link>
      <description>&lt;p&gt;Open-Source Frontier Voice AI&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h2&gt;üéôÔ∏è VibeVoice: Open-Source Frontier Voice AI&lt;/h2&gt; 
 &lt;p&gt;&lt;a href="https://microsoft.github.io/VibeVoice"&gt;&lt;img src="https://img.shields.io/badge/Project-Page-blue?logo=microsoft" alt="Project Page" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/collections/microsoft/vibevoice-68a2ef24a875c44be47b034f"&gt;&lt;img src="https://img.shields.io/badge/HuggingFace-Collection-orange?logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2508.19205"&gt;&lt;img src="https://img.shields.io/badge/Technical-Report-red?logo=adobeacrobatreader" alt="Technical Report" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="Figures/VibeVoice_logo_white.png" /&gt; 
  &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/VibeVoice_logo.png" alt="VibeVoice Logo" width="300" /&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;div align="left"&gt; 
 &lt;h3&gt;üì∞ News&lt;/h3&gt; 
 &lt;img src="https://img.shields.io/badge/Status-New-brightgreen?style=flat" alt="New" /&gt; 
 &lt;img src="https://img.shields.io/badge/Feature-Realtime_TTS-blue?style=flat&amp;amp;logo=soundcharts" alt="Realtime TTS" /&gt; 
 &lt;p&gt;&lt;strong&gt;2025-12-03: üì£ We open-sourced &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;&lt;strong&gt;VibeVoice‚ÄëRealtime‚Äë0.5B&lt;/strong&gt;&lt;/a&gt;, a real‚Äëtime text‚Äëto‚Äëspeech model that supports streaming text input and robust long-form speech generation. Try it on &lt;a href="https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/vibevoice_realtime_colab.ipynb"&gt;Colab&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;To mitigate deepfake risks and ensure low latency for the first speech chunk, voice prompts are provided in an embedded format. For users requiring voice customization, please reach out to our team. We will also be expanding the range of available speakers. &lt;br /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc"&gt;https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc&lt;/a&gt;&lt;/p&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;(Launch your own realtime demo via the websocket example in &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md#usage-1-launch-real-time-websocket-demo"&gt;Usage&lt;/a&gt;).&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/div&gt; 
&lt;p&gt;2025-09-05: VibeVoice is an open-source research framework intended to advance collaboration in the speech synthesis community. After release, we discovered instances where the tool was used in ways inconsistent with the stated intent. Since responsible use of AI is one of Microsoft‚Äôs guiding principles, we have disabled this repo until we are confident that out-of-scope use is no longer possible.&lt;/p&gt; 
&lt;h3&gt;Overview&lt;/h3&gt; 
&lt;p&gt;VibeVoice is a novel framework designed for generating &lt;strong&gt;expressive&lt;/strong&gt;, &lt;strong&gt;long-form&lt;/strong&gt;, &lt;strong&gt;multi-speaker&lt;/strong&gt; conversational audio, such as podcasts, from text. It addresses significant challenges in traditional Text-to-Speech (TTS) systems, particularly in scalability, speaker consistency, and natural turn-taking.&lt;/p&gt; 
&lt;p&gt;VibeVoice currently includes two model variants:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Long-form multi-speaker model&lt;/strong&gt;: Synthesizes conversational/single-speaker speech up to &lt;strong&gt;90 minutes&lt;/strong&gt; with up to &lt;strong&gt;4 distinct speakers&lt;/strong&gt;, surpassing the typical 1‚Äì2 speaker limits of many prior models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;Realtime streaming TTS model&lt;/a&gt;&lt;/strong&gt;: Produces initial audible speech in ~&lt;strong&gt;300 ms&lt;/strong&gt; and supports &lt;strong&gt;streaming text input&lt;/strong&gt; for single-speaker &lt;strong&gt;real-time&lt;/strong&gt; speech generation; designed for low-latency generation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;A core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of 7.5 Hz. These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a &lt;a href="https://arxiv.org/abs/2412.08635"&gt;next-token diffusion&lt;/a&gt; framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details.&lt;/p&gt; 
&lt;p align="left"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/MOS-preference.png" alt="MOS Preference Results" height="260px" /&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/VibeVoice.jpg" alt="VibeVoice Overview" height="250px" style="margin-right: 10px;" /&gt; &lt;/p&gt; 
&lt;h3&gt;üéµ Demo Examples&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Video Demo&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We produced this video with &lt;a href="https://github.com/Wan-Video/Wan2.2"&gt;Wan2.2&lt;/a&gt;. We sincerely appreciate the Wan-Video team for their great work.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;English&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784"&gt;https://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Chinese&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f"&gt;https://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Cross-Lingual&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722"&gt;https://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Spontaneous Singing&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730"&gt;https://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Long Conversation with 4 people&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727"&gt;https://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;For more examples, see the &lt;a href="https://microsoft.github.io/VibeVoice"&gt;Project Page&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Risks and limitations&lt;/h2&gt; 
&lt;p&gt;While efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model (specifically, Qwen2.5 1.5b in this release). Potential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content.&lt;/p&gt; 
&lt;p&gt;English and Chinese only: Transcripts in languages other than English or Chinese may result in unexpected audio outputs.&lt;/p&gt; 
&lt;p&gt;Non-Speech Audio: The model focuses solely on speech synthesis and does not handle background noise, music, or other sound effects.&lt;/p&gt; 
&lt;p&gt;Overlapping Speech: The current model does not explicitly model or generate overlapping speech segments in conversations.&lt;/p&gt; 
&lt;p&gt;We do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only. Please use responsibly.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>frappe/erpnext</title>
      <link>https://github.com/frappe/erpnext</link>
      <description>&lt;p&gt;Free and Open Source Enterprise Resource Planning (ERP)&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://frappe.io/erpnext"&gt; &lt;img src="https://raw.githubusercontent.com/frappe/erpnext/develop/erpnext/public/images/v16/erpnext.svg?sanitize=true" alt="ERPNext Logo" height="80px" width="80xp" /&gt; &lt;/a&gt; 
 &lt;h2&gt;ERPNext&lt;/h2&gt; 
 &lt;p align="center"&gt; &lt;/p&gt;
 &lt;p&gt;Powerful, Intuitive and Open-Source ERP&lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://frappe.school"&gt;&lt;img src="https://img.shields.io/badge/Frappe%20School-Learn%20ERPNext-blue?style=flat-square" alt="Learn on Frappe School" /&gt;&lt;/a&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://github.com/frappe/erpnext/actions/workflows/server-tests-mariadb.yml"&gt;&lt;img src="https://github.com/frappe/erpnext/actions/workflows/server-tests-mariadb.yml/badge.svg?event=schedule" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/frappe/erpnext-worker"&gt;&lt;img src="https://img.shields.io/docker/pulls/frappe/erpnext-worker.svg?sanitize=true" alt="docker pulls" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/frappe/erpnext/develop/erpnext/public/images/v16/hero_image.png" /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://erpnext-demo.frappe.cloud/api/method/erpnext_demo.erpnext_demo.auth.login_demo"&gt;Live Demo&lt;/a&gt; - 
 &lt;a href="https://frappe.io/erpnext"&gt;Website&lt;/a&gt; - 
 &lt;a href="https://docs.frappe.io/erpnext/"&gt;Documentation&lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;ERPNext&lt;/h2&gt; 
&lt;p&gt;100% Open-Source ERP system to help you run your business.&lt;/p&gt; 
&lt;h3&gt;Motivation&lt;/h3&gt; 
&lt;p&gt;Running a business is a complex task - handling invoices, tracking stock, managing personnel and even more ad-hoc activities. In a market where software is sold separately to manage each of these tasks, ERPNext does all of the above and more, for free.&lt;/p&gt; 
&lt;h3&gt;Key Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Accounting&lt;/strong&gt;: All the tools you need to manage cash flow in one place, right from recording transactions to summarizing and analyzing financial reports.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Order Management&lt;/strong&gt;: Track inventory levels, replenish stock, and manage sales orders, customers, suppliers, shipments, deliverables, and order fulfillment.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Manufacturing&lt;/strong&gt;: Simplifies the production cycle, helps track material consumption, exhibits capacity planning, handles subcontracting, and more!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Asset Management&lt;/strong&gt;: From purchase to perishment, IT infrastructure to equipment. Cover every branch of your organization, all in one centralized system.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Projects&lt;/strong&gt;: Delivery both internal and external Projects on time, budget and Profitability. Track tasks, timesheets, and issues by project.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details open&gt; 
 &lt;summary&gt;More&lt;/summary&gt; 
 &lt;img src="https://erpnext.com/files/v16_bom.png" /&gt; 
 &lt;img src="https://erpnext.com/files/v16_stock_summary.png" /&gt; 
 &lt;img src="https://erpnext.com/files/v16_job_card.png" /&gt; 
 &lt;img src="https://erpnext.com/files/v16_tasks.png" /&gt; 
&lt;/details&gt; 
&lt;h3&gt;Under the Hood&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/frappe/frappe"&gt;&lt;strong&gt;Frappe Framework&lt;/strong&gt;&lt;/a&gt;: A full-stack web application framework written in Python and Javascript. The framework provides a robust foundation for building web applications, including a database abstraction layer, user authentication, and a REST API.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/frappe/frappe-ui"&gt;&lt;strong&gt;Frappe UI&lt;/strong&gt;&lt;/a&gt;: A Vue-based UI library, to provide a modern user interface. The Frappe UI library provides a variety of components that can be used to build single-page applications on top of the Frappe Framework.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Production Setup&lt;/h2&gt; 
&lt;h3&gt;Managed Hosting&lt;/h3&gt; 
&lt;p&gt;You can try &lt;a href="https://frappecloud.com"&gt;Frappe Cloud&lt;/a&gt;, a simple, user-friendly and sophisticated &lt;a href="https://github.com/frappe/press"&gt;open-source&lt;/a&gt; platform to host Frappe applications with peace of mind.&lt;/p&gt; 
&lt;p&gt;It takes care of installation, setup, upgrades, monitoring, maintenance and support of your Frappe deployments. It is a fully featured developer platform with an ability to manage and control multiple Frappe deployments.&lt;/p&gt; 
&lt;div&gt; 
 &lt;a href="https://erpnext-demo.frappe.cloud/app/home" target="_blank"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://frappe.io/files/try-on-fc-white.png" /&gt; 
   &lt;img src="https://frappe.io/files/try-on-fc-black.png" alt="Try on Frappe Cloud" height="28" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h3&gt;Self-Hosted&lt;/h3&gt; 
&lt;h4&gt;Docker&lt;/h4&gt; 
&lt;p&gt;Prerequisites: docker, docker-compose, git. Refer &lt;a href="https://docs.docker.com"&gt;Docker Documentation&lt;/a&gt; for more details on Docker setup.&lt;/p&gt; 
&lt;p&gt;Run following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/frappe/frappe_docker
cd frappe_docker
docker compose -f pwd.yml up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After a couple of minutes, site should be accessible on your localhost port: 8080. Use below default login credentials to access the site.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Username: Administrator&lt;/li&gt; 
 &lt;li&gt;Password: admin&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See &lt;a href="https://github.com/frappe/frappe_docker?tab=readme-ov-file#to-run-on-arm64-architecture-follow-this-instructions"&gt;Frappe Docker&lt;/a&gt; for ARM based docker setup.&lt;/p&gt; 
&lt;h2&gt;Development Setup&lt;/h2&gt; 
&lt;h3&gt;Manual Install&lt;/h3&gt; 
&lt;p&gt;The Easy Way: our install script for bench will install all dependencies (e.g. MariaDB). See &lt;a href="https://github.com/frappe/bench"&gt;https://github.com/frappe/bench&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;New passwords will be created for the ERPNext "Administrator" user, the MariaDB root user, and the frappe user (the script displays the passwords and saves them to ~/frappe_passwords.txt).&lt;/p&gt; 
&lt;h3&gt;Local&lt;/h3&gt; 
&lt;p&gt;To setup the repository locally follow the steps mentioned below:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Setup bench by following the &lt;a href="https://frappeframework.com/docs/user/en/installation"&gt;Installation Steps&lt;/a&gt; and start the server&lt;/p&gt; &lt;pre&gt;&lt;code&gt;bench start
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;In a separate terminal window, run the following commands:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Create a new site
bench new-site erpnext.localhost
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Get the ERPNext app and install it&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Get the ERPNext app
bench get-app https://github.com/frappe/erpnext

# Install the app
bench --site erpnext.localhost install-app erpnext
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Open the URL &lt;code&gt;http://erpnext.localhost:8000/app&lt;/code&gt; in your browser, you should see the app running&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Learning and community&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://school.frappe.io"&gt;Frappe School&lt;/a&gt; - Learn Frappe Framework and ERPNext from the various courses by the maintainers or from the community.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.erpnext.com/"&gt;Official documentation&lt;/a&gt; - Extensive documentation for ERPNext.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discuss.frappe.io/c/erpnext/6"&gt;Discussion Forum&lt;/a&gt; - Engage with community of ERPNext users and service providers.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://erpnext_public.t.me"&gt;Telegram Group&lt;/a&gt; - Get instant help from huge community of users.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://github.com/frappe/erpnext/wiki/Issue-Guidelines"&gt;Issue Guidelines&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://erpnext.com/security"&gt;Report Security Vulnerabilities&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/frappe/erpnext/wiki/Contribution-Guidelines"&gt;Pull Request Requirements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://crowdin.com/project/frappe"&gt;Translations&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Logo and Trademark Policy&lt;/h2&gt; 
&lt;p&gt;Please read our &lt;a href="https://raw.githubusercontent.com/frappe/erpnext/develop/TRADEMARK_POLICY.md"&gt;Logo and Trademark Policy&lt;/a&gt;.&lt;/p&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;div align="center" style="padding-top: 0.75rem;"&gt; 
 &lt;a href="https://frappe.io" target="_blank"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://frappe.io/files/Frappe-white.png" /&gt; 
   &lt;img src="https://frappe.io/files/Frappe-black.png" alt="Frappe Technologies" height="28" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>trustedsec/social-engineer-toolkit</title>
      <link>https://github.com/trustedsec/social-engineer-toolkit</link>
      <description>&lt;p&gt;The Social-Engineer Toolkit (SET) repository from TrustedSec - All new versions of SET will be deployed here.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The Social-Engineer Toolkit (SET)&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;Copyright &lt;span&gt;¬©&lt;/span&gt; 2020&lt;/li&gt; 
 &lt;li&gt;Written by: David Kennedy (ReL1K) @HackingDave&lt;/li&gt; 
 &lt;li&gt;Company: &lt;a href="https://www.trustedsec.com"&gt;TrustedSec&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h2&gt;Description&lt;/h2&gt; 
&lt;p&gt;The Social-Engineer Toolkit is an open-source penetration testing framework designed for social engineering. SET has a number of custom attack vectors that allow you to make a believable attack quickly. SET is a product of TrustedSec, LLC ‚Äì an information security consulting firm located in Cleveland, Ohio.&lt;/p&gt; 
&lt;p&gt;DISCLAIMER: This is &lt;em&gt;only&lt;/em&gt; for testing purposes and can only be used where strict consent has been given. Do not use this for illegal purposes, period. Please read the LICENSE under readme/LICENSE for the licensing of SET.&lt;/p&gt; 
&lt;h4&gt;Supported platforms:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Linux&lt;/li&gt; 
 &lt;li&gt;Mac OS X (experimental)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;h2&gt;Install via requirements.txt&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip3 install -r requirements.txt
python3 setup.py 
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Install SET&lt;/h2&gt; 
&lt;p&gt;=======&lt;/p&gt; 
&lt;h4&gt;Mac OS X&lt;/h4&gt; 
&lt;p&gt;You will need to use a virtual environment for the Python install if you are using an M2 Macbook with the following instructions in your CLI within the social-engineer-toolkit directory.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;    # to install dependencies, run the following:
    python3 -m venv path/to/venv
    source path/to/venv/bin/activate
    python3 -m pip install -r requirements.txt

    # to install SET
    sudo python3 setup.py 
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h4&gt;Windows 10 WSL/WSL2 Kali Linux&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt install set -y
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Kali Linux on Windows 10 is a minimal installation so it doesn't have any tools installed. You can easily install Social Engineer Toolkit on WSL/WSL2 without needing pip using the above command.&lt;/p&gt; 
&lt;h4&gt;Linux&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/trustedsec/social-engineer-toolkit/ setoolkit/
cd setoolkit
pip3 install -r requirements.txt
python setup.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt; 
&lt;h2&gt;SET Tutorial&lt;/h2&gt; 
&lt;p&gt;For a full document on how to use SET, &lt;a href="https://github.com/trustedsec/social-engineer-toolkit/raw/master/readme/User_Manual.pdf"&gt;visit the SET user manual&lt;/a&gt;.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;Bugs and enhancements&lt;/h2&gt; 
&lt;p&gt;For bug reports or enhancements, please open an &lt;a href="https://github.com/trustedsec/social-engineer-toolkit/issues"&gt;issue&lt;/a&gt; here. &lt;br /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>numz/ComfyUI-SeedVR2_VideoUpscaler</title>
      <link>https://github.com/numz/ComfyUI-SeedVR2_VideoUpscaler</link>
      <description>&lt;p&gt;Official SeedVR2 Video Upscaler for ComfyUI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ComfyUI-SeedVR2_VideoUpscaler&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/numz/ComfyUI-SeedVR2_VideoUpscaler"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%93%82_View_Code-GitHub-181717?style=for-the-badge&amp;amp;logo=github" alt="View Code" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Official release of &lt;a href="https://github.com/ByteDance-Seed/SeedVR"&gt;SeedVR2&lt;/a&gt; for ComfyUI that enables high-quality video and image upscaling.&lt;/p&gt; 
&lt;p&gt;Can run as &lt;strong&gt;Multi-GPU standalone CLI&lt;/strong&gt; too, see &lt;a href="https://raw.githubusercontent.com/numz/ComfyUI-SeedVR2_VideoUpscaler/main/#%EF%B8%8F-run-as-standalone-cli"&gt;üñ•Ô∏è Run as Standalone&lt;/a&gt; section.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://youtu.be/MBtWYXq_r60"&gt;&lt;img src="https://img.youtube.com/vi/MBtWYXq_r60/maxresdefault.jpg" alt="SeedVR2 v2.5 Deep Dive Tutorial" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/numz/ComfyUI-SeedVR2_VideoUpscaler/main/docs/usage_01.png" alt="Usage Example" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/numz/ComfyUI-SeedVR2_VideoUpscaler/main/docs/usage_02.png" alt="Usage Example" /&gt;&lt;/p&gt; 
&lt;h2&gt;üìã Quick Access&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/numz/ComfyUI-SeedVR2_VideoUpscaler/main/#-future-releases"&gt;üÜô Future Releases&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/numz/ComfyUI-SeedVR2_VideoUpscaler/main/#-updates"&gt;üöÄ Updates&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/numz/ComfyUI-SeedVR2_VideoUpscaler/main/#-features"&gt;üéØ Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/numz/ComfyUI-SeedVR2_VideoUpscaler/main/#-requirements"&gt;üîß Requirements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/numz/ComfyUI-SeedVR2_VideoUpscaler/main/#-installation"&gt;üì¶ Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/numz/ComfyUI-SeedVR2_VideoUpscaler/main/#-usage"&gt;üìñ Usage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/numz/ComfyUI-SeedVR2_VideoUpscaler/main/#%EF%B8%8F-run-as-standalone-cli"&gt;üñ•Ô∏è Run as Standalone&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/numz/ComfyUI-SeedVR2_VideoUpscaler/main/#%EF%B8%8F-limitations"&gt;‚ö†Ô∏è Limitations&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/numz/ComfyUI-SeedVR2_VideoUpscaler/main/#-contributing"&gt;ü§ù Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/numz/ComfyUI-SeedVR2_VideoUpscaler/main/#-credits"&gt;üôè Credits&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/numz/ComfyUI-SeedVR2_VideoUpscaler/main/#-license"&gt;üìú License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üÜô Future Releases&lt;/h2&gt; 
&lt;p&gt;We're actively working on improvements and new features. To stay informed:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üìå Track Active Development&lt;/strong&gt;: Visit &lt;a href="https://github.com/numz/ComfyUI-SeedVR2_VideoUpscaler/issues"&gt;Issues&lt;/a&gt; to see active development, report bugs, and request new features&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üí¨ Join the Community&lt;/strong&gt;: Learn from others, share your workflows, and get help in the &lt;a href="https://github.com/numz/ComfyUI-SeedVR2_VideoUpscaler/discussions"&gt;Discussions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîÆ Next Model Survey&lt;/strong&gt;: We're looking for community input on the next open-source super-powerful generic restoration model. Share your suggestions in &lt;a href="https://github.com/numz/ComfyUI-SeedVR2_VideoUpscaler/issues/164"&gt;Issue #164&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üöÄ Updates&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;2025.12.05 - Version 2.5.17&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üîß Fix: Older GPU compatibility (GTX 970, etc.)&lt;/strong&gt; - Runtime bf16 CUBLAS probe replaces compute capability heuristics, correctly detecting unsupported GPUs without affecting RTX 20XX&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2025.12.05 - Version 2.5.16&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üîß Fix: Older GPU compatibility (GTX 970, etc.)&lt;/strong&gt; - Automatic fallback for GPUs without bfloat16 support&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üêõ Fix: Quality regression&lt;/strong&gt; - Reverted bfloat16 detection that was causing artifact issues&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìã Debug: Environment info display&lt;/strong&gt; - Shows system info in debug mode to help with issue reporting&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìö Docs: Simplified contribution workflow&lt;/strong&gt; - Streamlined to main branch only&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2025.12.03 - Version 2.5.15&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üçé Fix: MPS compatibility&lt;/strong&gt; - Disable antialias for MPS tensors and fix bfloat16 arange issues&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;‚ö° Fix: Autocast device type&lt;/strong&gt; - Use proper device type attribute to prevent autocast errors&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìä Memory: Accurate VRAM tracking&lt;/strong&gt; - Use max_memory_reserved for more precise peak reporting&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîß Fix: Triton compatibility&lt;/strong&gt; - Add shim for bitsandbytes 0.45+ / triton 3.0+ (fixes PyTorch 2.7 installation errors)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2025.12.01 - Version 2.5.14&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üçé Fix: MPS device comparison&lt;/strong&gt; - Normalize device strings to prevent unnecessary tensor movements&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìä Memory: VRAM swap detection&lt;/strong&gt; - Peak stats now show GPU+swap breakdown when overflow occurs, with warning when swap detected&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üõ°Ô∏è Memory: Enforce physical VRAM limit&lt;/strong&gt; - PyTorch now OOMs instead of silently swapping to shared memory (prevents extreme slowdowns on Windows)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2025.11.30 - Version 2.5.13&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üîß Fix: PyTorch 2.7+ triton import error&lt;/strong&gt; - Resolved installation crash caused by triton.ops import chain on newer triton versions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üíæ Fix: OOM on float32 conversion for long videos&lt;/strong&gt; - Graceful fallback to native dtype when insufficient memory for float32 conversion&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üçé Fix: CLI watermark error on macOS&lt;/strong&gt; - Resolved MPS-related watermark processing crash on Apple Silicon&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2025.11.28 - Version 2.5.12&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üêõ Fix: Color artifacts regression&lt;/strong&gt; - Reverted in-place tensor operations in video transform pipeline that caused color artifacts on some images&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2025.11.28 - Version 2.5.11&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;‚ö° Feature: CUDNN attention backend&lt;/strong&gt; - Added support for PyTorch 2.3+ CUDNN_ATTENTION backend with automatic fallback for older versions (thanks @eadwu)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üíæ Fix: Memory spike for long videos&lt;/strong&gt; - VAE decode now streams directly to pre-allocated tensor, eliminating OOM errors during long video processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üé® Fix: LAB color correction artifacts&lt;/strong&gt; - Resolved tile boundary artifacts using wavelet reconstruction preprocessing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üé® Fix: Color reference misalignment&lt;/strong&gt; - Fixed color correction frame alignment with temporal overlap&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üçé Fix: MPS detection reliability&lt;/strong&gt; - Switched to canonical &lt;code&gt;torch.backends.mps.is_available()&lt;/code&gt; API for consistent Apple Silicon detection&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üñ•Ô∏è Fix: Mac subprocess error&lt;/strong&gt; - CLI now uses direct processing on Mac to avoid MPS allocator failures in child processes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üñ•Ô∏è Fix: Multi-GPU device assignment&lt;/strong&gt; - CUDA_VISIBLE_DEVICES now set before spawn for proper worker inheritance&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìä Fix: BlockSwap logging&lt;/strong&gt; - Now shows effective/total blocks (e.g., 32/32) instead of raw requested value&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîß Feature: Auto bfloat16 detection&lt;/strong&gt; - Automatically detects bfloat16 support to prevent CUBLAS errors on older GPUs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìä Feature: Peak RAM tracking&lt;/strong&gt; - Added RAM usage alongside VRAM in debug summary&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;‚ö° Performance: In-place tensor ops&lt;/strong&gt; - Reduced memory allocation overhead with in-place operations throughout pipeline&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìñ Docs: Multi-GPU clarification&lt;/strong&gt; - Clarified frame-level parallelism behavior expectations for multi-GPU setups&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2025.11.13 - Version 2.5.10&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üéØ Fix: Deterministic generation&lt;/strong&gt; - Identical images with the same seed now produce identical results across different sessions and batch positions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîß Fix: Model caching with BlockSwap&lt;/strong&gt; - Resolved issue where cached DiT models wouldn't properly reload when VAE caching state changed&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üíæ Fix: Runner caching optimization&lt;/strong&gt; - Runner templates now correctly cache whenever both DiT and VAE are cached, regardless of caching order&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìÅ Fix: Case-insensitive model paths&lt;/strong&gt; - Extra model paths in YAML config now work regardless of case (seedvr2, SEEDVR2, SeedVR2, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üêõ Fix: High resolution tile debug crash&lt;/strong&gt; - Fixed "NoneType has no attribute log" error when using maximum resolution with VAE tiling&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìä Fix: Temporal overlap logging&lt;/strong&gt; - Corrected frame count reporting when temporal overlap is automatically adjusted&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîç Feature: Enhanced model path debugging&lt;/strong&gt; - Added detailed logging to help troubleshoot model loading issues (visible in debug mode)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2025.11.12 - Version 2.5.9&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üêõ Fix: Tile debug visualization crash&lt;/strong&gt; - Fixed OpenCV error when using VAE tile debug mode on certain systems.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üçé Fix: macOS MPS loading error&lt;/strong&gt; - Added automatic CPU fallback for MPS allocator issues on certain PyTorch/macOS versions.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üñ•Ô∏è Fix: Windows log buffering&lt;/strong&gt; - Added flush to print statements for real-time log visibility in ComfyUI on Windows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üì¶ Fix: ComfyUI Registry logo&lt;/strong&gt; - Updated icon URL to display properly in ComfyUI node registry&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;‚ÑπÔ∏è Feature: Version display&lt;/strong&gt; - Added version number to node name and CLI/ComfyUI header for better tracking&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üíù Feature: GitHub Sponsors&lt;/strong&gt; - Added sponsor button to support project development. Thank you everyone for your support!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìú License: Apache 2.0&lt;/strong&gt; - Reverted License from MIT to Apache 2.0 to match ByteDance Seed project&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2025.11.10 - Version 2.5.8&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üêõ Fix (CLI): Windows batch processing duplicate files&lt;/strong&gt; - Fixed CLI batch mode processing each file twice on Windows due to case-insensitive filesystem. Improved directory scanning performance by 2-3x&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìÅ Fix(CLI): Output folder location&lt;/strong&gt; - Output files now created in sensible locations: batch mode creates &lt;code&gt;{folder_name}_upscaled/&lt;/code&gt; sibling folder with original filenames preserved; single file mode adds &lt;code&gt;_upscaled&lt;/code&gt; suffix in same directory. All logs now show absolute paths for clarity&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üé® Fix(CLI): RGBA alpha channel support&lt;/strong&gt; - PNG images with transparency are now properly detected and preserved through the upscaling pipeline, matching ComfyUI behavior&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2025.11.10 - Version 2.5.7&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üîß Fix: Conv3d workaround compatibility&lt;/strong&gt; - Enhanced platform detection and added graceful fallback to prevent errors on PyTorch dev builds and AMD ROCm systems&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2025.11.09 - Version 2.5.6&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;üé® &lt;strong&gt;Fix: Restored natural look for 7b model&lt;/strong&gt; - Corrected torch.compile optimization that was causing overly plastic/ high-specular appearance in upscaled videos with 7b model.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üíæ &lt;strong&gt;Memory: Fixed RAM leak for long videos&lt;/strong&gt; - On-demand reconstruction with lightweight batch indices instead of storing full transformed videos, fixed release_tensor_memory to handle CPU/CUDA/MPS consistently, and refactored batch processing helpers&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2025.11.08 - Version 2.5.4&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üé® &lt;strong&gt;Fix: AdaIN color correction&lt;/strong&gt; - Replace &lt;code&gt;.view()&lt;/code&gt; with &lt;code&gt;.reshape()&lt;/code&gt; to handle non-contiguous tensors after spatial padding, resolving "view size is not compatible with input tensor's size and stride" error&lt;/li&gt; 
 &lt;li&gt;üî¥ &lt;strong&gt;Fix: AMD ROCm compatibility&lt;/strong&gt; - Add cuDNN availability check in Conv3d workaround to prevent "ATen not compiled with cuDNN support" error on ROCm systems (AMD GPUs on Windows/Linux)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2025.11.08 - Version 2.5.3&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üçé &lt;strong&gt;Fix: Apple Silicon MPS device handling&lt;/strong&gt; - Corrected MPS device enumeration to use &lt;code&gt;"mps"&lt;/code&gt; instead of &lt;code&gt;"mps:0"&lt;/code&gt;, resolving invalid device errors on M-series Macs&lt;/li&gt; 
 &lt;li&gt;ü™ü &lt;strong&gt;Fix: torch.mps AttributeError on Windows&lt;/strong&gt; - Add defensive checks for &lt;code&gt;torch.mps.is_available()&lt;/code&gt; to handle PyTorch versions where the method doesn't exist on non-Mac platforms&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2025.11.07 - Version 2.5.0&lt;/strong&gt; üéâ&lt;/p&gt; 
&lt;p&gt;‚ö†Ô∏è &lt;strong&gt;BREAKING CHANGE&lt;/strong&gt;: This is a major update requiring workflow recreation. All nodes and CLI parameters have been redesigned for better usability and consistency. Watch the latest video from &lt;a href="https://www.youtube.com/@AInVFX"&gt;AInVFX&lt;/a&gt; for a deep dive and check out the &lt;a href="https://raw.githubusercontent.com/numz/ComfyUI-SeedVR2_VideoUpscaler/main/#-usage"&gt;usage&lt;/a&gt; section.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;üì¶ Official Release&lt;/strong&gt;: Now available on main branch with ComfyUI Manager support for easy installation and automatic version tracking. Updated dependencies and local imports prevent conflicts with other ComfyUI custom nodes.&lt;/p&gt; 
&lt;h3&gt;üé® ComfyUI Improvements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Four-Node Modular Architecture&lt;/strong&gt;: Split into dedicated nodes for DiT model, VAE model, torch.compile settings, and main upscaler for granular control&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Global Model Cache&lt;/strong&gt;: Models now shared across multiple upscaler instances with automatic config updates - no more redundant loading&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ComfyUI V3 Migration&lt;/strong&gt;: Full compatibility with ComfyUI V3 stateless node design&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;RGBA Support&lt;/strong&gt;: Native alpha channel processing with edge-guided upscaling for clean transparency&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Improved Memory Management&lt;/strong&gt;: Streaming architecture prevents VRAM spikes regardless of video length&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible Resolution Support&lt;/strong&gt;: Upscale to any resolution divisible by 2 with lossless padding approach (replaced restrictive cropping)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced Parameters&lt;/strong&gt;: Added &lt;code&gt;uniform_batch_size&lt;/code&gt;, &lt;code&gt;temporal_overlap&lt;/code&gt;, &lt;code&gt;prepend_frames&lt;/code&gt;, and &lt;code&gt;max_resolution&lt;/code&gt; for better control&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üñ•Ô∏è CLI Enhancements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Batch Directory Processing&lt;/strong&gt;: Process entire folders of videos/images with model caching for efficiency&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Single Image Support&lt;/strong&gt;: Direct image upscaling without video conversion&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Output Detection&lt;/strong&gt;: Auto-detects output format (MP4/PNG) based on input type&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced Multi-GPU&lt;/strong&gt;: Improved workload distribution with temporal overlap blending&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Unified Parameters&lt;/strong&gt;: CLI and ComfyUI now use identical parameter names for consistency&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Better UX&lt;/strong&gt;: Auto-display help, validation improvements, progress tracking, and cleaner output&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;‚ö° Performance &amp;amp; Optimization&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;torch.compile Support&lt;/strong&gt;: 20-40% DiT speedup and 15-25% VAE speedup with full graph compilation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Optimized BlockSwap&lt;/strong&gt;: Adaptive memory clearing (5% threshold), separate I/O component handling, reduced overhead&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced VAE Tiling&lt;/strong&gt;: Tensor offload support for accumulation buffers, separate encode/decode configuration&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Native Dtype Pipeline&lt;/strong&gt;: Eliminated unnecessary conversions, maintains bfloat16 precision throughout for speed and quality&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Optimized Tensor Operations&lt;/strong&gt;: Replaced einops rearrange with native PyTorch ops for 2-5x faster transforms&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üéØ Quality Improvements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;LAB Color Correction&lt;/strong&gt;: New perceptual color transfer method with superior color accuracy (now default)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Additional Color Methods&lt;/strong&gt;: HSV saturation matching, wavelet adaptive, and hybrid approaches&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Deterministic Generation&lt;/strong&gt;: Seed-based reproducibility with phase-specific seeding strategy&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Better Temporal Consistency&lt;/strong&gt;: Hann window blending for smooth transitions between batches&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üíæ Memory Management&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Smarter Offloading&lt;/strong&gt;: Independent device configuration for DiT, VAE, and tensors (CPU/GPU/none)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Four-Phase Pipeline&lt;/strong&gt;: Completes each phase (encode‚Üíupscale‚Üídecode‚Üípostprocess) for all batches before moving to next, minimizing model swaps&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Better Cleanup&lt;/strong&gt;: Phase-specific resource management with proper tensor memory release&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Peak VRAM Tracking&lt;/strong&gt;: Per-phase memory monitoring with summary display&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üîß Technical Improvements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;GGUF Quantization Support&lt;/strong&gt;: Added full GGUF support for 4-bit/8-bit inference on low-VRAM systems&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Improved GGUF Handling&lt;/strong&gt;: Fixed VRAM leaks, torch.compile compatibility, non-persistent buffers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Apple Silicon Support&lt;/strong&gt;: Full MPS (Metal Performance Shaders) support for Apple Silicon Macs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AMD ROCm Compatibility&lt;/strong&gt;: Conditional FSDP imports for PyTorch ROCm 7+ support&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Conv3d Memory Workaround&lt;/strong&gt;: Fixes PyTorch 2.9+ cuDNN memory bug (3x usage reduction)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flash Attention Optional&lt;/strong&gt;: Graceful fallback to SDPA when flash-attn unavailable&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìö Code Quality&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Modular Architecture&lt;/strong&gt;: Split monolithic files into focused modules (generation_phases, model_configuration, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Comprehensive Documentation&lt;/strong&gt;: Extensive docstrings with type hints across all modules&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Better Error Handling&lt;/strong&gt;: Early validation, clear error messages, installation instructions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Consistent Logging&lt;/strong&gt;: Unified indentation, better categorization, concise messages&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2025.08.07&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üéØ &lt;strong&gt;Unified Debug System&lt;/strong&gt;: New structured logging with categories, timers, and memory tracking. &lt;code&gt;enable_debug&lt;/code&gt; now available on main node&lt;/li&gt; 
 &lt;li&gt;‚ö° &lt;strong&gt;Smart FP8 Optimization&lt;/strong&gt;: FP8 models now keep native FP8 storage, converting to BFloat16 only for arithmetic - faster and more memory efficient than FP16&lt;/li&gt; 
 &lt;li&gt;üì¶ &lt;strong&gt;Model Registry&lt;/strong&gt;: Multi-repo support (numz/ &amp;amp; AInVFX/), auto-discovery of user models, added mixed FP8 variants to fix 7B artifacts&lt;/li&gt; 
 &lt;li&gt;üíæ &lt;strong&gt;Model Caching&lt;/strong&gt;: &lt;code&gt;cache_model&lt;/code&gt; moved to main node, fixed memory leaks with proper RoPE/wrapper cleanup&lt;/li&gt; 
 &lt;li&gt;üßπ &lt;strong&gt;Code Cleanup&lt;/strong&gt;: New modular structure (&lt;code&gt;constants.py&lt;/code&gt;, &lt;code&gt;model_registry.py&lt;/code&gt;, &lt;code&gt;debug.py&lt;/code&gt;), removed legacy code&lt;/li&gt; 
 &lt;li&gt;üöÄ &lt;strong&gt;Performance&lt;/strong&gt;: Better memory management with &lt;code&gt;torch.cuda.ipc_collect()&lt;/code&gt;, improved RoPE handling&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2025.07.17&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üõ†Ô∏è Add 7B sharp Models: add 2 new 7B models with sharpen output&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2025.07.11&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üé¨ Complete tutorial released: Adrien from &lt;a href="https://www.youtube.com/@AInVFX"&gt;AInVFX&lt;/a&gt; created an in-depth ComfyUI SeedVR2 guide covering everything from basic setup to advanced BlockSwap techniques for running on consumer GPUs. Perfect for understanding memory optimization and upscaling of image sequences with alpha channel! &lt;a href="https://raw.githubusercontent.com/numz/ComfyUI-SeedVR2_VideoUpscaler/main/#-usage"&gt;Watch the tutorial&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2025.09.07&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üõ†Ô∏è Blockswap Integration: Big thanks to &lt;a href="https://github.com/adrientoupet"&gt;Adrien Toupet&lt;/a&gt; from &lt;a href="https://www.youtube.com/@AInVFX"&gt;AInVFX&lt;/a&gt; for this :), useful for low VRAM users (see &lt;a href="https://raw.githubusercontent.com/numz/ComfyUI-SeedVR2_VideoUpscaler/main/#-usage"&gt;usage&lt;/a&gt; section)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2025.07.03&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üõ†Ô∏è Can run as &lt;strong&gt;standalone mode&lt;/strong&gt; with &lt;strong&gt;Multi GPU&lt;/strong&gt; see &lt;a href="https://raw.githubusercontent.com/numz/ComfyUI-SeedVR2_VideoUpscaler/main/#%EF%B8%8F-run-as-standalone-cli"&gt;üñ•Ô∏è Run as Standalone&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2025.06.30&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üöÄ Speed Up the process and less VRAM used&lt;/li&gt; 
 &lt;li&gt;üõ†Ô∏è Fixed memory leak on 3B models&lt;/li&gt; 
 &lt;li&gt;‚ùå Can now interrupt process if needed&lt;/li&gt; 
 &lt;li&gt;‚úÖ Refactored the code for better sharing with the community, feel free to propose pull requests&lt;/li&gt; 
 &lt;li&gt;üõ†Ô∏è Removed flash attention dependency (thanks to &lt;a href="https://github.com/Luke2642"&gt;luke2642&lt;/a&gt; !!)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2025.06.24&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üöÄ Speed up the process until x4&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2025.06.22&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üí™ FP8 compatibility !&lt;/li&gt; 
 &lt;li&gt;üöÄ Speed Up all Process&lt;/li&gt; 
 &lt;li&gt;üöÄ less VRAM consumption (Stay high, batch_size=1 for RTX4090 max, I'm trying to fix that)&lt;/li&gt; 
 &lt;li&gt;üõ†Ô∏è Better benchmark coming soon&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2025.06.20&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üõ†Ô∏è Initial push&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üéØ Features&lt;/h2&gt; 
&lt;h3&gt;Core Capabilities&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;High-Quality Diffusion-Based Upscaling&lt;/strong&gt;: One-step diffusion model for video and image enhancement&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Temporal Consistency&lt;/strong&gt;: Maintains coherence across video frames with configurable batch processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Format Support&lt;/strong&gt;: Handles RGB and RGBA (alpha channel) for both videos and images&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Any Video Length&lt;/strong&gt;: Suitable for any video length&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Model Support&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Multiple Model Variants&lt;/strong&gt;: 3B and 7B parameter models with different precision options&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;FP16, FP8, and GGUF Quantization&lt;/strong&gt;: Choose between full precision (FP16), mixed precision (FP8), or heavily quantized GGUF models for different VRAM requirements&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic Model Downloads&lt;/strong&gt;: Models are automatically downloaded from HuggingFace on first use&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Memory Optimization&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;BlockSwap Technology&lt;/strong&gt;: Dynamically swap transformer blocks between GPU and CPU memory to run large models on limited VRAM&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;VAE Tiling&lt;/strong&gt;: Process large resolutions with tiled encoding/decoding to reduce VRAM usage&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent Offloading&lt;/strong&gt;: Offload models and intermediate tensors to CPU or secondary GPUs between processing phases&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GGUF Quantization Support&lt;/strong&gt;: Run models with 4-bit or 8-bit quantization for extreme VRAM savings&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Performance Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;torch.compile Integration&lt;/strong&gt;: Optional 20-40% DiT speedup and 15-25% VAE speedup with PyTorch 2.0+ compilation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-GPU CLI&lt;/strong&gt;: Distribute workload across multiple GPUs with automatic temporal overlap blending&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model Caching&lt;/strong&gt;: Keep models loaded in memory for faster batch processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible Attention Backends&lt;/strong&gt;: Choose between PyTorch SDPA (stable, always available) or Flash Attention 2 (faster on supported hardware)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quality Control&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced Color Correction&lt;/strong&gt;: Five methods including LAB (recommended for highest fidelity), wavelet, wavelet adaptive, HSV, and AdaIN&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Noise Injection Controls&lt;/strong&gt;: Fine-tune input and latent noise scales for artifact reduction at high resolutions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configurable Resolution Limits&lt;/strong&gt;: Set target and maximum resolutions with automatic aspect ratio preservation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Workflow Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ComfyUI Integration&lt;/strong&gt;: Four dedicated nodes for complete control over the upscaling pipeline&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Standalone CLI&lt;/strong&gt;: Command-line interface for batch processing and automation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Debug Logging&lt;/strong&gt;: Comprehensive debug mode with memory tracking, timing information, and processing details&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Progress Reporting&lt;/strong&gt;: Real-time progress updates during processing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üîß Requirements&lt;/h2&gt; 
&lt;h3&gt;Hardware&lt;/h3&gt; 
&lt;p&gt;With the current optimizations (tiling, BlockSwap, GGUF quantization), SeedVR2 can run on a wide range of hardware:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Minimal VRAM&lt;/strong&gt; (8GB or less): Use GGUF Q4_K_M models with BlockSwap and VAE tiling enabled&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Moderate VRAM&lt;/strong&gt; (12-16GB): Use FP8 models with BlockSwap or VAE tiling as needed&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High VRAM&lt;/strong&gt; (24GB+): Use FP16 models for best quality and speed without memory optimizations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Software&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ComfyUI&lt;/strong&gt;: Latest version recommended&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt;: 3.12+ (Python 3.12 and 3.13 tested and recommended)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;PyTorch&lt;/strong&gt;: 2.0+ for torch.compile support (optional but recommended)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Triton&lt;/strong&gt;: Required for torch.compile with inductor backend (optional)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flash Attention 2&lt;/strong&gt;: Provides faster attention computation on supported hardware (optional, falls back to PyTorch SDPA)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üì¶ Installation&lt;/h2&gt; 
&lt;h3&gt;Option 1: ComfyUI Manager (Recommended)&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open ComfyUI Manager in your ComfyUI interface&lt;/li&gt; 
 &lt;li&gt;Click "Custom Nodes Manager"&lt;/li&gt; 
 &lt;li&gt;Search for "ComfyUI-SeedVR2_VideoUpscaler"&lt;/li&gt; 
 &lt;li&gt;Click "Install" and restart ComfyUI&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Registry Link&lt;/strong&gt;: &lt;a href="https://registry.comfy.org/nodes/seedvr2_videoupscaler"&gt;ComfyUI Registry - SeedVR2 Video Upscaler&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Option 2: Manual Installation&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Clone the repository&lt;/strong&gt; into your ComfyUI custom nodes directory:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd ComfyUI
git clone https://github.com/numz/ComfyUI-SeedVR2_VideoUpscaler.git custom_nodes/seedvr2_videoupscaler
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;&lt;strong&gt;Install dependencies using standalone Python&lt;/strong&gt;:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install requirements (from same ComfyUI directory)
# Windows:
.venv\Scripts\python.exe -m pip install -r custom_nodes\seedvr2_videoupscaler\requirements.txt
# Linux/macOS:
.venv/bin/python -m pip install -r custom_nodes/seedvr2_videoupscaler/requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;&lt;strong&gt;Restart ComfyUI&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Model Installation&lt;/h3&gt; 
&lt;p&gt;Models will be &lt;strong&gt;automatically downloaded&lt;/strong&gt; on first use and saved to &lt;code&gt;ComfyUI/models/SEEDVR2&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can also manually download models from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Main models available at &lt;a href="https://huggingface.co/numz/SeedVR2_comfyUI/tree/main"&gt;numz/SeedVR2_comfyUI&lt;/a&gt; and &lt;a href="https://huggingface.co/AInVFX/SeedVR2_comfyUI/tree/main"&gt;AInVFX/SeedVR2_comfyUI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Additional GGUF models available at &lt;a href="https://huggingface.co/cmeka/SeedVR2-GGUF/tree/main"&gt;cmeka/SeedVR2-GGUF&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìñ Usage&lt;/h2&gt; 
&lt;h3&gt;üé¨ Video Tutorials&lt;/h3&gt; 
&lt;h4&gt;Latest Version Deep Dive (Recommended)&lt;/h4&gt; 
&lt;p&gt;Complete walkthrough of version 2.5 by Adrien from &lt;a href="https://www.youtube.com/@AInVFX"&gt;AInVFX&lt;/a&gt;, covering the new 4-node architecture, GGUF support, memory optimizations, and production workflows:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://youtu.be/MBtWYXq_r60"&gt;&lt;img src="https://img.youtube.com/vi/MBtWYXq_r60/maxresdefault.jpg" alt="SeedVR2 v2.5 Deep Dive Tutorial" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This comprehensive tutorial covers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Installing v2.5 through ComfyUI Manager and troubleshooting conflicts&lt;/li&gt; 
 &lt;li&gt;Understanding the new 4-node modular architecture and why we rebuilt it&lt;/li&gt; 
 &lt;li&gt;Running 7B models on 8GB VRAM with GGUF quantization&lt;/li&gt; 
 &lt;li&gt;Configuring BlockSwap, VAE tiling, and torch.compile for your hardware&lt;/li&gt; 
 &lt;li&gt;Image and video upscaling workflows with alpha channel support&lt;/li&gt; 
 &lt;li&gt;CLI for batch processing and multi-GPU rendering&lt;/li&gt; 
 &lt;li&gt;Memory optimization strategies for different VRAM levels&lt;/li&gt; 
 &lt;li&gt;Real production tips and the critical batch_size formula (4n+1)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Previous Version Tutorial&lt;/h4&gt; 
&lt;p&gt;For reference, here's the original tutorial covering the initial release:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://youtu.be/I0sl45GMqNg"&gt;&lt;img src="https://img.youtube.com/vi/I0sl45GMqNg/maxresdefault.jpg" alt="SeedVR2 Deep Dive Tutorial" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Note: This tutorial covers the previous single-node architecture. While the UI has changed significantly in v2.5, the core concepts about BlockSwap and memory management remain valuable.&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Node Setup&lt;/h3&gt; 
&lt;p&gt;SeedVR2 uses a modular node architecture with four specialized nodes:&lt;/p&gt; 
&lt;h4&gt;1. SeedVR2 (Down)Load DiT Model&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/numz/ComfyUI-SeedVR2_VideoUpscaler/main/docs/dit_model_loader.png" alt="SeedVR2 (Down)Load DiT Model" /&gt;&lt;/p&gt; 
&lt;p&gt;Configure the DiT (Diffusion Transformer) model for video upscaling.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Parameters:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;model&lt;/strong&gt;: Choose your DiT model&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;3B Models&lt;/strong&gt;: Faster, lower VRAM requirements 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;seedvr2_ema_3b_fp16.safetensors&lt;/code&gt;: FP16 (best quality)&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;seedvr2_ema_3b_fp8_e4m3fn.safetensors&lt;/code&gt;: FP8 8-bit (good quality)&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;seedvr2_ema_3b-Q4_K_M.gguf&lt;/code&gt;: GGUF 4-bit quantized (acceptable quality)&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;seedvr2_ema_3b-Q8_0.gguf&lt;/code&gt;: GGUF 8-bit quantized (good quality)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;7B Models&lt;/strong&gt;: Higher quality, higher VRAM requirements 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;seedvr2_ema_7b_fp16.safetensors&lt;/code&gt;: FP16 (best quality)&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;seedvr2_ema_7b_fp8_e4m3fn_mixed_block35_fp16.safetensors&lt;/code&gt;: FP8 with last block in FP16 to reduce artifacts (good quality)&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;seedvr2_ema_7b-Q4_K_M.gguf&lt;/code&gt;: GGUF 4-bit quantized (acceptable quality)&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;seedvr2_ema_7b_sharp_*&lt;/code&gt;: Sharp variants for enhanced detail&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;device&lt;/strong&gt;: GPU device for DiT inference (e.g., &lt;code&gt;cuda:0&lt;/code&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;offload_device&lt;/strong&gt;: Device to offload DiT model when not actively processing&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;none&lt;/code&gt;: Keep model on inference device (fastest, highest VRAM)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;cpu&lt;/code&gt;: Offload to system RAM (reduces VRAM)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;cuda:X&lt;/code&gt;: Offload to another GPU (good balance if available)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;cache_model&lt;/strong&gt;: Keep DiT model loaded on offload_device between workflow runs&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Useful for batch processing to avoid repeated loading&lt;/li&gt; 
   &lt;li&gt;Requires offload_device to be set&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;blocks_to_swap&lt;/strong&gt;: BlockSwap memory optimization&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;0&lt;/code&gt;: Disabled (default)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;1-32&lt;/code&gt;: Number of transformer blocks to swap for 3B model&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;1-36&lt;/code&gt;: Number of transformer blocks to swap for 7B model&lt;/li&gt; 
   &lt;li&gt;Higher values = more VRAM savings but slower processing&lt;/li&gt; 
   &lt;li&gt;Requires offload_device to be set and different from device&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;swap_io_components&lt;/strong&gt;: Offload input/output embeddings and normalization layers&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Additional VRAM savings when combined with blocks_to_swap&lt;/li&gt; 
   &lt;li&gt;Requires offload_device to be set and different from device&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;attention_mode&lt;/strong&gt;: Attention computation backend&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;sdpa&lt;/code&gt;: PyTorch scaled_dot_product_attention (default, stable, always available)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;flash_attn&lt;/code&gt;: Flash Attention 2 (faster on supported hardware, requires flash-attn package)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;torch_compile_args&lt;/strong&gt;: Connect to SeedVR2 Torch Compile Settings node for 20-40% speedup&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;BlockSwap Explained:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;BlockSwap enables running large models on GPUs with limited VRAM by dynamically swapping transformer blocks between GPU and CPU memory during inference. Here's how it works:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;What it does&lt;/strong&gt;: Keeps only the currently-needed transformer blocks on the GPU, while storing the rest on CPU or another device&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;When to use it&lt;/strong&gt;: When you get OOM (Out of Memory) errors during the upscaling phase&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;How to configure&lt;/strong&gt;: 
  &lt;ol&gt; 
   &lt;li&gt;Set &lt;code&gt;offload_device&lt;/code&gt; to &lt;code&gt;cpu&lt;/code&gt; or another GPU&lt;/li&gt; 
   &lt;li&gt;Start with &lt;code&gt;blocks_to_swap=16&lt;/code&gt; (half the blocks)&lt;/li&gt; 
   &lt;li&gt;If still getting OOM, increase to 24 or 32 (3B) / 36 (7B)&lt;/li&gt; 
   &lt;li&gt;Enable &lt;code&gt;swap_io_components&lt;/code&gt; for maximum VRAM savings&lt;/li&gt; 
   &lt;li&gt;If you have plenty of VRAM, decrease or set to 0 for faster processing&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Example Configuration for Low VRAM (8GB)&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;model: &lt;code&gt;seedvr2_ema_3b-Q8_0.gguf&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;device: &lt;code&gt;cuda:0&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;offload_device: &lt;code&gt;cpu&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;blocks_to_swap: &lt;code&gt;32&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;swap_io_components: &lt;code&gt;True&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;2. SeedVR2 (Down)Load VAE Model&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/numz/ComfyUI-SeedVR2_VideoUpscaler/main/docs/vae_model_loader.png" alt="SeedVR2 (Down)Load VAE Model" /&gt;&lt;/p&gt; 
&lt;p&gt;Configure the VAE (Variational Autoencoder) model for encoding/decoding video frames.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Parameters:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;model&lt;/strong&gt;: VAE model selection&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;ema_vae_fp16.safetensors&lt;/code&gt;: Default and recommended&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;device&lt;/strong&gt;: GPU device for VAE inference (e.g., &lt;code&gt;cuda:0&lt;/code&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;offload_device&lt;/strong&gt;: Device to offload VAE model when not actively processing&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;none&lt;/code&gt;: Keep model on inference device (default, fastest)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;cpu&lt;/code&gt;: Offload to system RAM (reduces VRAM)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;cuda:X&lt;/code&gt;: Offload to another GPU (good balance if available)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;cache_model&lt;/strong&gt;: Keep VAE model loaded on offload_device between workflow runs&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Requires offload_device to be set&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;encode_tiled&lt;/strong&gt;: Enable tiled encoding to reduce VRAM usage during encoding phase&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Enable if you see OOM errors during the "Encoding" phase in debug logs&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;encode_tile_size&lt;/strong&gt;: Encoding tile size in pixels (default: 1024)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Applied to both height and width&lt;/li&gt; 
   &lt;li&gt;Lower values reduce VRAM but may increase processing time&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;encode_tile_overlap&lt;/strong&gt;: Encoding tile overlap in pixels (default: 128)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Reduces visible seams between tiles&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;decode_tiled&lt;/strong&gt;: Enable tiled decoding to reduce VRAM usage during decoding phase&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Enable if you see OOM errors during the "Decoding" phase in debug logs&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;decode_tile_size&lt;/strong&gt;: Decoding tile size in pixels (default: 1024)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;decode_tile_overlap&lt;/strong&gt;: Decoding tile overlap in pixels (default: 128)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;torch_compile_args&lt;/strong&gt;: Connect to SeedVR2 Torch Compile Settings node for 15-25% speedup&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;VAE Tiling Explained:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;VAE tiling processes large resolutions in smaller tiles to reduce VRAM requirements. Here's how to use it:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Run without tiling first&lt;/strong&gt; and monitor the debug logs (enable &lt;code&gt;enable_debug&lt;/code&gt; on main node)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;If OOM during "Encoding" phase&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;Enable &lt;code&gt;encode_tiled&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;If still OOM, reduce &lt;code&gt;encode_tile_size&lt;/code&gt; (try 768, 512, etc.)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;If OOM during "Decoding" phase&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;Enable &lt;code&gt;decode_tiled&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;If still OOM, reduce &lt;code&gt;decode_tile_size&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Adjust overlap&lt;/strong&gt; (default 128) if you see visible seams in output (increase it) or processing times are too slow (decrease it).&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Example Configuration for High Resolution (4K)&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;encode_tiled: &lt;code&gt;True&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;encode_tile_size: &lt;code&gt;1024&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;encode_tile_overlap: &lt;code&gt;128&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;decode_tiled: &lt;code&gt;True&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;decode_tile_size: &lt;code&gt;1024&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;decode_tile_overlap: &lt;code&gt;128&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;3. SeedVR2 Torch Compile Settings (Optional)&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/numz/ComfyUI-SeedVR2_VideoUpscaler/main/docs/torch_compile_settings.png" alt="SeedVR2 Torch Compile Settings" /&gt;&lt;/p&gt; 
&lt;p&gt;Configure torch.compile optimization for 20-40% DiT speedup and 15-25% VAE speedup.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PyTorch 2.0+&lt;/li&gt; 
 &lt;li&gt;Triton (for inductor backend)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Parameters:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;backend&lt;/strong&gt;: Compilation backend&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;inductor&lt;/code&gt;: Full optimization with Triton kernel generation and fusion (recommended)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;cudagraphs&lt;/code&gt;: Lightweight wrapper using CUDA graphs, no kernel optimization&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;mode&lt;/strong&gt;: Optimization level (compilation time vs runtime performance)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;default&lt;/code&gt;: Fast compilation with good speedup (recommended for development)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;reduce-overhead&lt;/code&gt;: Lower overhead, optimized for smaller models&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;max-autotune&lt;/code&gt;: Slowest compilation, best runtime performance (recommended for production)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;max-autotune-no-cudagraphs&lt;/code&gt;: Like max-autotune but without CUDA graphs&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;fullgraph&lt;/strong&gt;: Compile entire model as single graph without breaks&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;False&lt;/code&gt;: Allow graph breaks for better compatibility (default, recommended)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;True&lt;/code&gt;: Enforce no breaks for maximum optimization (may fail with dynamic shapes)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;dynamic&lt;/strong&gt;: Handle varying input shapes without recompilation&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;False&lt;/code&gt;: Specialize for exact input shapes (default)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;True&lt;/code&gt;: Create dynamic kernels that adapt to shape variations (enable when processing different resolutions or batch sizes)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;dynamo_cache_size_limit&lt;/strong&gt;: Max cached compiled versions per function (default: 64)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Higher = more memory, lower = more recompilation&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;dynamo_recompile_limit&lt;/strong&gt;: Max recompilation attempts before falling back to eager mode (default: 128)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Safety limit to prevent compilation loops&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Usage:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Add this node to your workflow&lt;/li&gt; 
 &lt;li&gt;Connect its output to the &lt;code&gt;torch_compile_args&lt;/code&gt; input of DiT and/or VAE loader nodes&lt;/li&gt; 
 &lt;li&gt;First run will be slow (compilation), subsequent runs will be much faster&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;When to use:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;torch.compile only makes sense when processing &lt;strong&gt;multiple batches, long videos, or many tiles&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;For single images or short clips, the compilation time outweighs the speed improvement&lt;/li&gt; 
 &lt;li&gt;Best suited for batch processing workflows or long videos&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Recommended Settings:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For development/testing: &lt;code&gt;mode=default&lt;/code&gt;, &lt;code&gt;backend=inductor&lt;/code&gt;, &lt;code&gt;fullgraph=False&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;For production: &lt;code&gt;mode=max-autotune&lt;/code&gt;, &lt;code&gt;backend=inductor&lt;/code&gt;, &lt;code&gt;fullgraph=False&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;4. SeedVR2 Video Upscaler (Main Node)&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/numz/ComfyUI-SeedVR2_VideoUpscaler/main/docs/video_upscaler.png" alt="SeedVR2 Video Upscaler" /&gt;&lt;/p&gt; 
&lt;p&gt;Main upscaling node that processes video frames using DiT and VAE models.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Required Inputs:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;image&lt;/strong&gt;: Input video frames as image batch (RGB or RGBA format)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;dit&lt;/strong&gt;: DiT model configuration from SeedVR2 (Down)Load DiT Model node&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;vae&lt;/strong&gt;: VAE model configuration from SeedVR2 (Down)Load VAE Model node&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Parameters:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;seed&lt;/strong&gt;: Random seed for reproducible generation (default: 42)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Same seed with same inputs produces identical output&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;resolution&lt;/strong&gt;: Target resolution for shortest edge in pixels (default: 1080)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Maintains aspect ratio automatically&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;max_resolution&lt;/strong&gt;: Maximum resolution for any edge (default: 0 = no limit)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Automatically scales down if exceeded to prevent OOM&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;batch_size&lt;/strong&gt;: Frames per batch (default: 5)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;CRITICAL REQUIREMENT&lt;/strong&gt;: Must follow the &lt;strong&gt;4n+1 formula&lt;/strong&gt; (1, 5, 9, 13, 17, 21, 25, ...)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Why this matters&lt;/strong&gt;: The model uses these frames for temporal consistency calculations&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Minimum 5 for temporal consistency&lt;/strong&gt;: Use 1 only for single images or when temporal consistency isn't needed&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Match shot length ideally&lt;/strong&gt;: For best results, set batch_size to match your shot length (e.g., batch_size=21 for a 20-frame shot)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;VRAM impact&lt;/strong&gt;: Higher batch_size = better quality and speed but requires more VRAM&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;If you get OOM with batch_size=5&lt;/strong&gt;: Try optimization techniques first (model offloading, BlockSwap, GGUF models...) before reducing batch_size or input resolution, as these directly impact quality&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;uniform_batch_size&lt;/strong&gt; (default: False)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Pads the final batch to match &lt;code&gt;batch_size&lt;/code&gt; for uniform processing&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Prevents temporal artifacts when the last batch is significantly smaller than others&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Example: 45 frames with &lt;code&gt;batch_size=33&lt;/code&gt; creates [33, 33] instead of [33, 12]&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Recommended when using large batch sizes and video length is not a multiple of &lt;code&gt;batch_size&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Increases VRAM usage slightly but ensures consistent temporal coherence across all batches&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;temporal_overlap&lt;/strong&gt;: Overlapping frames between batches (default: 0)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Used for blending between batches to reduce temporal artifacts&lt;/li&gt; 
   &lt;li&gt;Range: 0-16 frames&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;prepend_frames&lt;/strong&gt;: Frames to prepend (default: 0)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Prepends reversed frames to reduce artifacts at video start&lt;/li&gt; 
   &lt;li&gt;Automatically removed after processing&lt;/li&gt; 
   &lt;li&gt;Range: 0-32 frames&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;color_correction&lt;/strong&gt;: Color correction method (default: "wavelet")&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;code&gt;lab&lt;/code&gt;&lt;/strong&gt;: Full perceptual color matching with detail preservation (recommended for highest fidelity to original)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;code&gt;wavelet&lt;/code&gt;&lt;/strong&gt;: Frequency-based natural colors, preserves details well&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;code&gt;wavelet_adaptive&lt;/code&gt;&lt;/strong&gt;: Wavelet base + targeted saturation correction&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;code&gt;hsv&lt;/code&gt;&lt;/strong&gt;: Hue-conditional saturation matching&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;code&gt;adain&lt;/code&gt;&lt;/strong&gt;: Statistical style transfer&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;code&gt;none&lt;/code&gt;&lt;/strong&gt;: No color correction&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;input_noise_scale&lt;/strong&gt;: Input noise injection scale 0.0-1.0 (default: 0.0)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Adds noise to input frames to reduce artifacts at very high resolutions&lt;/li&gt; 
   &lt;li&gt;Try 0.1-0.3 if you see artifacts with high output resolutions&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;latent_noise_scale&lt;/strong&gt;: Latent space noise scale 0.0-1.0 (default: 0.0)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Adds noise during diffusion process, can soften excessive detail&lt;/li&gt; 
   &lt;li&gt;Use if input_noise doesn't help, try 0.05-0.15&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;offload_device&lt;/strong&gt;: Device for storing intermediate tensors between processing phases (default: "cpu")&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;none&lt;/code&gt;: Keep all tensors on inference device (fastest but highest VRAM)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;cpu&lt;/code&gt;: Offload to system RAM (recommended for long videos, slower transfers)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;cuda:X&lt;/code&gt;: Offload to another GPU (good balance if available, faster than CPU)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;enable_debug&lt;/strong&gt;: Enable detailed debug logging (default: False)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Shows memory usage, timing information, and processing details&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Highly recommended&lt;/strong&gt; for troubleshooting OOM issues&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Output:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Upscaled video frames with color correction applied&lt;/li&gt; 
 &lt;li&gt;Format (RGB/RGBA) matches input&lt;/li&gt; 
 &lt;li&gt;Range [0, 1] normalized for ComfyUI compatibility&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Typical Workflow Setup&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Basic Workflow (High VRAM - 24GB+)&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Load Video Frames
    ‚Üì
SeedVR2 Load DiT Model
  ‚îú‚îÄ model: seedvr2_ema_3b_fp16.safetensors
  ‚îî‚îÄ device: cuda:0
    ‚Üì
SeedVR2 Load VAE Model
  ‚îú‚îÄ model: ema_vae_fp16.safetensors
  ‚îî‚îÄ device: cuda:0
    ‚Üì
SeedVR2 Video Upscaler
  ‚îú‚îÄ batch_size: 21
  ‚îî‚îÄ resolution: 1080
    ‚Üì
Save Video/Frames
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Low VRAM Workflow (8-12GB)&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Load Video Frames
    ‚Üì
SeedVR2 Load DiT Model
  ‚îú‚îÄ model: seedvr2_ema_3b-Q8_0.gguf
  ‚îú‚îÄ device: cuda:0
  ‚îú‚îÄ offload_device: cpu
  ‚îú‚îÄ blocks_to_swap: 32
  ‚îî‚îÄ swap_io_components: True
    ‚Üì
SeedVR2 Load VAE Model
  ‚îú‚îÄ model: ema_vae_fp16.safetensors
  ‚îú‚îÄ device: cuda:0
  ‚îú‚îÄ encode_tiled: True
  ‚îî‚îÄ decode_tiled: True
    ‚Üì
SeedVR2 Video Upscaler
  ‚îú‚îÄ batch_size: 5
  ‚îî‚îÄ resolution: 720
    ‚Üì
Save Video/Frames
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;High Performance Workflow (24GB+ with torch.compile)&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Load Video Frames
    ‚Üì
SeedVR2 Torch Compile Settings
  ‚îú‚îÄ mode: max-autotune
  ‚îî‚îÄ backend: inductor
    ‚Üì
SeedVR2 Load DiT Model
  ‚îú‚îÄ model: seedvr2_ema_7b_sharp_fp16.safetensors
  ‚îú‚îÄ device: cuda:0
  ‚îî‚îÄ torch_compile_args: connected
    ‚Üì
SeedVR2 Load VAE Model
  ‚îú‚îÄ model: ema_vae_fp16.safetensors
  ‚îú‚îÄ device: cuda:0
  ‚îî‚îÄ torch_compile_args: connected
    ‚Üì
SeedVR2 Video Upscaler
  ‚îú‚îÄ batch_size: 81
  ‚îî‚îÄ resolution: 1080
    ‚Üì
Save Video/Frames
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üñ•Ô∏è Run as Standalone (CLI)&lt;/h2&gt; 
&lt;p&gt;The standalone CLI provides powerful batch processing capabilities with multi-GPU support and sophisticated optimization options.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;p&gt;Choose the appropriate setup based on your installation:&lt;/p&gt; 
&lt;h4&gt;Option 1: Already Have ComfyUI with SeedVR2 Installed&lt;/h4&gt; 
&lt;p&gt;If you've already installed SeedVR2 as part of ComfyUI (via &lt;a href="https://raw.githubusercontent.com/numz/ComfyUI-SeedVR2_VideoUpscaler/main/#-installation"&gt;ComfyUI installation&lt;/a&gt;), you can use the CLI directly:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Navigate to your ComfyUI directory
cd ComfyUI

# Run the CLI using standalone Python (display help message)
# Windows:
.venv\Scripts\python.exe custom_nodes\seedvr2_videoupscaler\inference_cli.py --help
# Linux/macOS:
.venv/bin/python custom_nodes/seedvr2_videoupscaler/inference_cli.py --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Skip to &lt;a href="https://raw.githubusercontent.com/numz/ComfyUI-SeedVR2_VideoUpscaler/main/#command-line-usage"&gt;Command Line Usage&lt;/a&gt; below.&lt;/strong&gt;&lt;/p&gt; 
&lt;h4&gt;Option 2: Standalone Installation (Without ComfyUI)&lt;/h4&gt; 
&lt;p&gt;If you want to use the CLI without ComfyUI installation, follow these steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Install &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;uv&lt;/a&gt;&lt;/strong&gt; (modern Python package manager):&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Windows
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"

# macOS and Linux
curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;&lt;strong&gt;Clone the repository&lt;/strong&gt;:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/numz/ComfyUI-SeedVR2_VideoUpscaler.git seedvr2_videoupscaler
cd seedvr2_videoupscaler
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;&lt;strong&gt;Create virtual environment and install dependencies&lt;/strong&gt;:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create virtual environment with Python 3.13
uv venv --python 3.13

# Activate virtual environment
# Windows:
.venv\Scripts\activate
# Linux/macOS:
source .venv/bin/activate

# Install PyTorch with CUDA support
# Check command line based on your environment: https://pytorch.org/get-started/locally/
uv pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu130

# Install SeedVR2 requirements
uv pip install -r requirements.txt

# Run the CLI (display help message)
# Windows:
.venv\Scripts\python.exe inference_cli.py --help
# Linux/macOS:
.venv/bin/python inference_cli.py --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Command Line Usage&lt;/h3&gt; 
&lt;p&gt;The CLI provides comprehensive options for single-GPU, multi-GPU, and batch processing workflows.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Basic Usage Examples:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Basic image upscaling
python inference_cli.py image.jpg

# Basic video video upscaling with temporal consistency
python inference_cli.py video.mp4 --resolution 720 --batch_size 33

# Multi-GPU processing with temporal overlap
python inference_cli.py video.mp4 \
    --cuda_device 0,1 \
    --resolution 1080 \
    --batch_size 81 \
    --uniform_batch_size \
    --temporal_overlap 3 \
    --prepend_frames 4

# Memory-optimized for low VRAM (8GB)
python inference_cli.py image.png \
    --dit_model seedvr2_ema_3b-Q8_0.gguf \
    --resolution 1080 \
    --blocks_to_swap 32 \
    --swap_io_components \
    --dit_offload_device cpu \
    --vae_offload_device cpu

# High resolution with VAE tiling
python inference_cli.py video.mp4 \
    --resolution 1440 \
    --batch_size 31 \
    --uniform_batch_size \
    --temporal_overlap 3 \
    --vae_encode_tiled \
    --vae_decode_tiled

# Batch directory processing with model caching
python inference_cli.py media_folder/ \
    --output processed/ \
    --cuda_device 0 \
    --cache_dit \
    --cache_vae \
    --dit_offload_device cpu \
    --vae_offload_device cpu \
    --resolution 1080 \
    --max_resolution 1920
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Command Line Arguments&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Input/Output:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;input&amp;gt;&lt;/code&gt;: Input file (.mp4, .avi, .png, .jpg, etc.) or directory&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--output&lt;/code&gt;: Output path (default: auto-generated in 'output/' directory)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--output_format&lt;/code&gt;: Output format: 'mp4' (video) or 'png' (image sequence). Default: auto-detect from input type&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_dir&lt;/code&gt;: Model directory (default: ./models/SEEDVR2)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Model Selection:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--dit_model&lt;/code&gt;: DiT model to use. Options: 3B/7B with fp16/fp8/GGUF variants (default: 3B FP8)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Processing Parameters:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--resolution&lt;/code&gt;: Target short-side resolution in pixels (default: 1080)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--max_resolution&lt;/code&gt;: Maximum resolution for any edge. Scales down if exceeded. 0 = no limit (default: 0)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--batch_size&lt;/code&gt;: Frames per batch (must follow 4n+1: 1, 5, 9, 13, 17, 21...). Ideally matches shot length for best temporal consistency (default: 5)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--seed&lt;/code&gt;: Random seed for reproducibility (default: 42)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--skip_first_frames&lt;/code&gt;: Skip N initial frames (default: 0)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--load_cap&lt;/code&gt;: Load maximum N frames from video. 0 = load all (default: 0)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--prepend_frames&lt;/code&gt;: Prepend N reversed frames to reduce start artifacts (auto-removed) (default: 0)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--temporal_overlap&lt;/code&gt;: Frames to overlap between batches/GPUs for smooth blending (default: 0)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Quality Control:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--color_correction&lt;/code&gt;: Color correction method: 'lab' (perceptual, recommended), 'wavelet', 'wavelet_adaptive', 'hsv', 'adain', or 'none' (default: lab)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--input_noise_scale&lt;/code&gt;: Input noise injection scale (0.0-1.0). Reduces artifacts at high resolutions (default: 0.0)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--latent_noise_scale&lt;/code&gt;: Latent space noise scale (0.0-1.0). Softens details if needed (default: 0.0)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Memory Management:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--dit_offload_device&lt;/code&gt;: Device to offload DiT model: 'none' (keep on GPU), 'cpu', or 'cuda:X' (default: none)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--vae_offload_device&lt;/code&gt;: Device to offload VAE model: 'none', 'cpu', or 'cuda:X' (default: none)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--blocks_to_swap&lt;/code&gt;: Number of transformer blocks to swap (0=disabled, 3B: 0-32, 7B: 0-36). Requires dit_offload_device (default: 0)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--swap_io_components&lt;/code&gt;: Offload I/O components for additional VRAM savings. Requires dit_offload_device&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--use_non_blocking&lt;/code&gt;: Use non-blocking memory transfers for BlockSwap (recommended)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;VAE Tiling:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--vae_encode_tiled&lt;/code&gt;: Enable VAE encode tiling to reduce VRAM during encoding&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--vae_encode_tile_size&lt;/code&gt;: VAE encode tile size in pixels (default: 1024)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--vae_encode_tile_overlap&lt;/code&gt;: VAE encode tile overlap in pixels (default: 128)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--vae_decode_tiled&lt;/code&gt;: Enable VAE decode tiling to reduce VRAM during decoding&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--vae_decode_tile_size&lt;/code&gt;: VAE decode tile size in pixels (default: 1024)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--vae_decode_tile_overlap&lt;/code&gt;: VAE decode tile overlap in pixels (default: 128)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--tile_debug&lt;/code&gt;: Visualize tiles: 'false' (default), 'encode', or 'decode'&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Performance Optimization:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--attention_mode&lt;/code&gt;: Attention backend: 'sdpa' (default, stable) or 'flash_attn' (faster, requires package)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--compile_dit&lt;/code&gt;: Enable torch.compile for DiT model (20-40% speedup, requires PyTorch 2.0+ and Triton)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--compile_vae&lt;/code&gt;: Enable torch.compile for VAE model (15-25% speedup, requires PyTorch 2.0+ and Triton)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--compile_backend&lt;/code&gt;: Compilation backend: 'inductor' (full optimization) or 'cudagraphs' (lightweight) (default: inductor)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--compile_mode&lt;/code&gt;: Optimization level: 'default', 'reduce-overhead', 'max-autotune', 'max-autotune-no-cudagraphs' (default: default)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--compile_fullgraph&lt;/code&gt;: Compile entire model as single graph (faster but less flexible) (default: False)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--compile_dynamic&lt;/code&gt;: Handle varying input shapes without recompilation (default: False)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--compile_dynamo_cache_size_limit&lt;/code&gt;: Max cached compiled versions per function (default: 64)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--compile_dynamo_recompile_limit&lt;/code&gt;: Max recompilation attempts before fallback (default: 128)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Model Caching (batch processing):&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--cache_dit&lt;/code&gt;: Cache DiT model between files (single GPU only, speeds up directory processing)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--cache_vae&lt;/code&gt;: Cache VAE model between files (single GPU only, speeds up directory processing)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Multi-GPU:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--cuda_device&lt;/code&gt;: CUDA device id(s). Single id (e.g., '0') or comma-separated list '0,1' for multi-GPU&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Debugging:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--debug&lt;/code&gt;: Enable verbose debug logging&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Multi-GPU Processing Explained&lt;/h3&gt; 
&lt;p&gt;The CLI's multi-GPU mode uses &lt;strong&gt;frame-level parallelism&lt;/strong&gt;: the video is split into chunks and each GPU processes its chunk independently through all 4 phases (encode ‚Üí upscale ‚Üí decode ‚Üí postprocess). This is ideal for long videos where you want to reduce total processing time by dividing the workload.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Video frames are split evenly across GPUs (e.g., 100 frames on 2 GPUs ‚Üí 50 frames each)&lt;/li&gt; 
 &lt;li&gt;Each GPU loads its own copy of the models and processes its chunk independently&lt;/li&gt; 
 &lt;li&gt;When &lt;code&gt;--temporal_overlap&lt;/code&gt; is set, chunks include overlapping frames for seamless blending&lt;/li&gt; 
 &lt;li&gt;Results are concatenated (and blended at overlap regions) into the final video&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Example for 100 frames on 2 GPUs with temporal_overlap=4:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;GPU 0: Frames 0-53 (50 base + 4 overlap at end, processed as independent video)
GPU 1: Frames 50-99 (50 frames, 4 overlap at start, processed as independent video)
Result: Frames 0-99 with smooth blending at the transition point
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Important considerations:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Each GPU processes its chunk as a separate video with its own batch splitting&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;batch_size&lt;/code&gt; controls batching &lt;em&gt;within&lt;/em&gt; each GPU's chunk, not across GPUs&lt;/li&gt; 
 &lt;li&gt;For short videos (&amp;lt; 100 frames), single GPU is often more efficient due to model loading overhead&lt;/li&gt; 
 &lt;li&gt;Multi-GPU doubles VRAM usage (each GPU loads full models) but roughly halves processing time&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;When to use multi-GPU:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Long videos (100+ frames) where splitting provides significant time savings&lt;/li&gt; 
 &lt;li&gt;When you have multiple GPUs with sufficient VRAM each&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;When to use single GPU:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Short videos where model loading overhead outweighs parallel gains&lt;/li&gt; 
 &lt;li&gt;When you want all frames processed together for maximum temporal coherence&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Best practices:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Set &lt;code&gt;--temporal_overlap&lt;/code&gt; to 2-4 frames for smooth blending between GPU chunks&lt;/li&gt; 
 &lt;li&gt;Higher overlap = smoother transitions but more redundant processing&lt;/li&gt; 
 &lt;li&gt;Use &lt;code&gt;--prepend_frames&lt;/code&gt; to reduce artifacts at video start&lt;/li&gt; 
 &lt;li&gt;For optimal quality on short videos, use single GPU with &lt;code&gt;batch_size&lt;/code&gt; matching your shot length&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚ö†Ô∏è Limitations&lt;/h2&gt; 
&lt;h3&gt;Model Limitations&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Batch Size Constraint&lt;/strong&gt;: The model requires batch_size to follow the &lt;strong&gt;4n+1 formula&lt;/strong&gt; (1, 5, 9, 13, 17, 21, 25, ...) due to temporal consistency architecture. All frames in a batch are processed together for temporal coherence, then batches can be blended using temporal_overlap. Ideally, set batch_size to match your shot length for optimal quality.&lt;/p&gt; 
&lt;h3&gt;Performance Considerations&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;VAE Bottleneck&lt;/strong&gt;: Even with optimized DiT upscaling (BlockSwap, GGUF, torch.compile), the VAE encoding/decoding stages can be the bottleneck, especially for high resolutions. The VAE is slow. Use large batch_size to mitigate this.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;VRAM Usage&lt;/strong&gt;: While the integration now supports low VRAM systems (8GB or less with proper optimization), VRAM usage varies based on:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Input/output resolution (larger = more VRAM)&lt;/li&gt; 
 &lt;li&gt;Batch size (higher = more VRAM but better temporal consistency and speed)&lt;/li&gt; 
 &lt;li&gt;Model choice (FP16 &amp;gt; FP8 &amp;gt; GGUF in VRAM usage)&lt;/li&gt; 
 &lt;li&gt;Optimization settings (BlockSwap, VAE tiling significantly reduce VRAM)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Speed&lt;/strong&gt;: Processing speed depends on:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;GPU capabilities (compute performance, VRAM bandwidth, and architecture generation)&lt;/li&gt; 
 &lt;li&gt;Model size (3B faster than 7B)&lt;/li&gt; 
 &lt;li&gt;Batch size (larger batch sizes are faster per frame due to better GPU utilization)&lt;/li&gt; 
 &lt;li&gt;Optimization settings (torch.compile provides significant speedup)&lt;/li&gt; 
 &lt;li&gt;Resolution (higher resolutions are slower)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Best Practices&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Start with debug enabled&lt;/strong&gt; to understand where VRAM is being used&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;For OOM errors during encoding&lt;/strong&gt;: Enable VAE encode tiling and reduce tile size&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;For OOM errors during upscaling&lt;/strong&gt;: Enable BlockSwap and increase blocks_to_swap&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;For OOM errors during decoding&lt;/strong&gt;: Enable VAE decode tiling and reduce tile size 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;If still getting OOM after trying all above&lt;/strong&gt;: Reduce batch_size or resolution&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;For best quality&lt;/strong&gt;: Use higher batch_size matching your shot length, FP16 models, and LAB color correction&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;For speed&lt;/strong&gt;: Use FP8/GGUF models, enable torch.compile, and use Flash Attention if available&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test settings with a short clip first&lt;/strong&gt; before processing long videos&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! We value community input and improvements.&lt;/p&gt; 
&lt;p&gt;For detailed contribution guidelines, see &lt;a href="https://raw.githubusercontent.com/numz/ComfyUI-SeedVR2_VideoUpscaler/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Quick Start:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repository&lt;/li&gt; 
 &lt;li&gt;Create your feature branch (&lt;code&gt;git checkout -b feature/AmazingFeature&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Commit your changes (&lt;code&gt;git commit -m 'Add some AmazingFeature'&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Push to the branch (&lt;code&gt;git push origin feature/AmazingFeature&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Open a Pull Request to the &lt;strong&gt;main&lt;/strong&gt; branch&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Get Help:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;YouTube: &lt;a href="https://www.youtube.com/@AInVFX"&gt;AInVFX Channel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;GitHub &lt;a href="https://github.com/numz/ComfyUI-SeedVR2_VideoUpscaler/issues"&gt;Issues&lt;/a&gt;: For bug reports and feature requests&lt;/li&gt; 
 &lt;li&gt;GitHub &lt;a href="https://github.com/numz/ComfyUI-SeedVR2_VideoUpscaler/discussions"&gt;Discussions&lt;/a&gt;: For questions and community support&lt;/li&gt; 
 &lt;li&gt;Discord: adrientoupet &amp;amp; NumZ#7184&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üôè Credits&lt;/h2&gt; 
&lt;p&gt;This ComfyUI implementation is a collaborative project by &lt;strong&gt;&lt;a href="https://github.com/numz"&gt;NumZ&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href="https://www.youtube.com/@AInVFX"&gt;AInVFX&lt;/a&gt;&lt;/strong&gt; (Adrien Toupet), based on the original &lt;a href="https://github.com/ByteDance-Seed/SeedVR"&gt;SeedVR2&lt;/a&gt; by ByteDance Seed Team.&lt;/p&gt; 
&lt;p&gt;Special thanks to our community contributors including &lt;a href="https://github.com/benjaminherb"&gt;benjaminherb&lt;/a&gt;, &lt;a href="https://github.com/cmeka"&gt;cmeka&lt;/a&gt;, &lt;a href="https://github.com/FurkanGozukara"&gt;FurkanGozukara&lt;/a&gt;, &lt;a href="https://github.com/JohnAlcatraz"&gt;JohnAlcatraz&lt;/a&gt;, &lt;a href="https://github.com/lihaoyun6"&gt;lihaoyun6&lt;/a&gt;, &lt;a href="https://github.com/Luchuanzhao"&gt;Luchuanzhao&lt;/a&gt;, &lt;a href="https://github.com/Luke2642"&gt;Luke2642&lt;/a&gt;, &lt;a href="https://github.com/naxci1"&gt;naxci1&lt;/a&gt;, &lt;a href="https://github.com/q5sys"&gt;q5sys&lt;/a&gt;, and many others for their improvements, bug fixes, and testing.&lt;/p&gt; 
&lt;h2&gt;üìú License&lt;/h2&gt; 
&lt;p&gt;The code in this repository is released under the Apache 2.0 license as found in the &lt;a href="https://raw.githubusercontent.com/numz/ComfyUI-SeedVR2_VideoUpscaler/main/LICENSE"&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>aliasrobotics/cai</title>
      <link>https://github.com/aliasrobotics/cai</link>
      <description>&lt;p&gt;Cybersecurity AI (CAI), the framework for AI Security&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;)&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a align="center" href="" target="https://github.com/aliasrobotics/CAI"&gt; &lt;img width="100%" src="https://github.com/aliasrobotics/cai/raw/main/media/cai.png" /&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/14317" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14317" alt="aliasrobotics%2Fcai | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;a href="https://defiant.vc/api/european-open-source/badge?domain=aliasrobotics.com&amp;amp;style=most-starred-top-3" target="_blank"&gt;&lt;img src="https://defiant.vc/api/european-open-source/badge?domain=aliasrobotics.com&amp;amp;style=most-starred-top-3" alt="European Open Source - Most Starred Top 3" style=" height: 75px;" height="75" /&gt;&lt;/a&gt; &lt;a href="https://defiant.vc/api/european-open-source/badge?domain=aliasrobotics.com&amp;amp;style=most-forked-top-3" target="_blank"&gt;&lt;img src="https://defiant.vc/api/european-open-source/badge?domain=aliasrobotics.com&amp;amp;style=most-forked-top-3" alt="European Open Source - Most Forked Top 3" style="height: 75px;" height="75" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://badge.fury.io/py/cai-framework"&gt;&lt;img src="https://badge.fury.io/py/cai-framework.svg?sanitize=true" alt="version" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/projects/cai-framework"&gt;&lt;img src="https://static.pepy.tech/badge/cai-framework" alt="downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/aliasrobotics/cai"&gt;&lt;img src="https://img.shields.io/badge/Linux-Supported-brightgreen?logo=linux&amp;amp;logoColor=white" alt="Linux" /&gt;&lt;/a&gt; &lt;a href="https://github.com/aliasrobotics/cai"&gt;&lt;img src="https://img.shields.io/badge/OS%20X-Supported-brightgreen?logo=apple&amp;amp;logoColor=white" alt="OS X" /&gt;&lt;/a&gt; &lt;a href="https://github.com/aliasrobotics/cai"&gt;&lt;img src="https://img.shields.io/badge/Windows-Supported-brightgreen?logo=windows&amp;amp;logoColor=white" alt="Windows" /&gt;&lt;/a&gt; &lt;a href="https://github.com/aliasrobotics/cai"&gt;&lt;img src="https://img.shields.io/badge/Android-Supported-brightgreen?logo=android&amp;amp;logoColor=white" alt="Android" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/fnUFcTaQAC"&gt;&lt;img src="https://img.shields.io/badge/Discord-7289DA?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2506.23592"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2506.23592-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2508.13588"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.13588-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2508.21669"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.21669-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2509.14096"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2509.14096-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2509.14139"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2509.14139-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2510.17521"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2510.17521-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2510.24317"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2510.24317-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;!-- CAI PRO - Professional Edition Banner --&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://aliasrobotics.com/cybersecurityai.php" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/media/cai-banner.svg?sanitize=true" alt="CAI - Community and Professional Editions" width="100%" style="max-width: 900px;" /&gt; &lt;/a&gt; 
 &lt;p&gt;&lt;sub&gt;&lt;i&gt;Professional Edition with unlimited &lt;code&gt;alias1&lt;/code&gt; tokens&lt;/i&gt; | &lt;a href="https://aliasrobotics.com/alias1.php#benchmarking"&gt;üìä View Benchmarks&lt;/a&gt; | &lt;a href="https://aliasrobotics.com/cybersecurityai.php"&gt;üöÄ Learn More&lt;/a&gt;&lt;/sub&gt;&lt;/p&gt; 
 &lt;table style="border-collapse: collapse; width: 100%"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td width="50%" align="center" style="padding: 0; border: none;"&gt; &lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/media/cai_poc.gif" alt="CAI Community Edition Demo" width="100%" /&gt; &lt;/td&gt; 
    &lt;td width="50%" align="center" style="padding: 0; border: none;"&gt; &lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/media/caipro_poc.gif" alt="CAI PRO Professional Edition Demo" width="100%" /&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;!-- Alternative HTML version (kept as comment for reference) --&gt; 
&lt;!--
&lt;div align="center"&gt;
  &lt;table style="border-collapse: collapse; width: 100%; max-width: 900px; box-shadow: 0 4px 12px rgba(82, 157, 134, 0.15);"&gt;
    &lt;tr&gt;
      &lt;td align="center" width="50%" style="padding: 20px; border: 3px solid #529d86; border-right: 1.5px solid #529d86; border-radius: 10px 0 0 10px; background: linear-gradient(135deg, #f0f8f6 0%, #ffffff 100%);"&gt;
        &lt;h3 style="color: #3d7b6b;"&gt;üîì Community Edition&lt;/h3&gt;
        &lt;sub style="color: #529d86;"&gt;&lt;b&gt;Research &amp; Learning ¬∑ Perfect for Researchers &amp; Students&lt;/b&gt;&lt;/sub&gt;&lt;br&gt;&lt;br&gt;
        &lt;code style="background: linear-gradient(135deg, #e8f5f1 0%, #d4ede5 100%); padding: 8px 16px; border-radius: 6px; font-size: 14px; border: 1px solid #529d86; color: #2d5a4d;"&gt;pip install cai-framework&lt;/code&gt;&lt;br&gt;&lt;br&gt;
        &lt;div align="left" style="margin: 10px auto; max-width: 200px; color: #2d2d2d;"&gt;
          ‚úÖ &lt;b style="color: #529d86;"&gt;Free&lt;/b&gt; for research&lt;br&gt;
          ü§ñ &lt;b style="color: #529d86;"&gt;300+&lt;/b&gt; AI models&lt;br&gt;
          üåç &lt;b style="color: #529d86;"&gt;Community&lt;/b&gt; driven&lt;br&gt;
          üìö &lt;b style="color: #529d86;"&gt;Open&lt;/b&gt; source&lt;br&gt;
          üîß &lt;b style="color: #529d86;"&gt;Extensible&lt;/b&gt; framework&lt;br&gt;
        &lt;/div&gt;
      &lt;/td&gt;
      &lt;td align="center" width="50%" style="padding: 20px; border: 3px solid #529d86; border-left: 1.5px solid #529d86; border-radius: 0 10px 10px 0; background: linear-gradient(135deg, #529d86 0%, #6bb09a 100%); position: relative; box-shadow: inset 0 0 30px rgba(255, 255, 255, 0.1);"&gt;
        &lt;h3 style="color: #ffffff; text-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);"&gt;üöÄ &lt;a href="https://aliasrobotics.com/cybersecurityai.php" style="text-decoration: none; color: #ffffff;"&gt;Professional Edition&lt;/a&gt;&lt;/h3&gt;
        &lt;sub style="color: #e8f5f1;"&gt;&lt;b&gt;Enterprise &amp; Production ¬∑ ‚Ç¨350/month ¬∑ Unlimited &lt;code style="background: rgba(255, 255, 255, 0.2); padding: 2px 6px; border-radius: 3px; color: #ffffff;"&gt;alias1&lt;/code&gt; Tokens&lt;/b&gt;&lt;/sub&gt;&lt;br&gt;&lt;br&gt;
        &lt;a href="https://aliasrobotics.com/cybersecurityai.php"&gt;
          &lt;code style="background: linear-gradient(135deg, #ffffff 0%, #f0f8f6 100%); color: #529d86; padding: 10px 20px; border-radius: 6px; font-size: 14px; font-weight: bold; border: 2px solid #ffffff; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2);"&gt;‚Üí Upgrade to PRO&lt;/code&gt;
        &lt;/a&gt;&lt;br&gt;&lt;br&gt;
        &lt;div align="left" style="margin: 10px auto; max-width: 280px; color: #ffffff;"&gt;
          ‚ö° &lt;b&gt;&lt;a href="https://aliasrobotics.com/alias1.php#benchmarking" style="color: #ffffff; text-decoration: underline;"&gt;alias1&lt;/a&gt;&lt;/b&gt; model - ‚àû unlimited tokens&lt;br&gt;
          üö´ &lt;b&gt;Zero refusals&lt;/b&gt; - Unrestricted AI&lt;br&gt;
          üèÜ &lt;b&gt;Beats GPT-5&lt;/b&gt; in CTF benchmarks&lt;br&gt;
          üõ°Ô∏è &lt;b&gt;Professional&lt;/b&gt; support included&lt;br&gt;
          üá™üá∫ &lt;b&gt;European&lt;/b&gt; data sovereignty&lt;br&gt;
        &lt;/div&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td colspan="2" align="center" style="padding: 10px; background: #f6f8fa;"&gt;
        &lt;sub&gt;
          &lt;a href="https://aliasrobotics.com/cybersecurityai.php"&gt;&lt;/a&gt;&lt;br&gt;
          &lt;i&gt;CAI PRO w/ &lt;code&gt;alias1&lt;/code&gt; model outperforms GPT-5 in AI vs AI cybersecurity benchmarks&lt;/i&gt; | &lt;a href="https://aliasrobotics.com/alias1.php#benchmarking"&gt;View Full Benchmarks ‚Üí&lt;/a&gt;
        &lt;/sub&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;
--&gt; 
&lt;p&gt;Cybersecurity AI (CAI) is a lightweight, open-source framework that empowers security professionals to build and deploy AI-powered offensive and defensive automation. CAI is the &lt;em&gt;de facto&lt;/em&gt; framework for AI Security, already used by thousands of individual users and hundreds of organizations. Whether you're a security researcher, ethical hacker, IT professional, or organization looking to enhance your security posture, CAI provides the building blocks to create specialized AI agents that can assist with mitigation, vulnerability discovery, exploitation, and security assessment.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ü§ñ &lt;strong&gt;300+ AI Models&lt;/strong&gt;: Support for OpenAI, Anthropic, DeepSeek, Ollama, and more&lt;/li&gt; 
 &lt;li&gt;üîß &lt;strong&gt;Built-in Security Tools&lt;/strong&gt;: Ready-to-use tools for reconnaissance, exploitation, and privilege escalation&lt;/li&gt; 
 &lt;li&gt;üèÜ &lt;strong&gt;Battle-tested&lt;/strong&gt;: Proven in HackTheBox CTFs, bug bounties, and real-world security &lt;a href="https://aliasrobotics.com/case-studies-robot-cybersecurity.php"&gt;case studies&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üéØ &lt;strong&gt;Agent-based Architecture&lt;/strong&gt;: Modular framework design to build specialized agents for different security tasks&lt;/li&gt; 
 &lt;li&gt;üõ°Ô∏è &lt;strong&gt;Guardrails Protection&lt;/strong&gt;: Built-in defenses against prompt injection and dangerous command execution&lt;/li&gt; 
 &lt;li&gt;üìö &lt;strong&gt;Research-oriented&lt;/strong&gt;: Research foundation to democratize cybersecurity AI for the community&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Read the technical report: &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;CAI: An Open, Bug Bounty-Ready Cybersecurity AI&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;For further readings, refer to our &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-impact"&gt;impact&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#citation"&gt;CAI citation&lt;/a&gt; sections.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-humanoid-robot-g1.php"&gt;&lt;code&gt;Robotics&lt;/code&gt; - CAI and alias1 on: Unitree G1 Humanoid Robot&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-dragos-CTF.php"&gt;&lt;code&gt;OT&lt;/code&gt; - CAI and alias1 on: Dragos OT CTF 2025&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CAI uncovers vulnerabilities and privacy violations in Unitree G1 humanoid robots including unauthorized telemetry transmission to China-related servers, exposed RSA keys with world-writable permissions, and potential surveillance capabilities violating GDPR and international privacy laws.&lt;/td&gt; 
   &lt;td&gt;CAI powered by alias1, demonstrates exceptional performance in operational technology cybersecurity by achieving a Top-10 ranking in the Dragos OT CTF 2025. The AI agent reached Rank 1 during competition hours 7-8, completed 32 of 34 challenges, and maintained a 37% velocity advantage over top human teams.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-humanoid-robot-g1.php"&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/docs/assets/images/case-study-humanoid-portada.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-dragos-CTF.php"&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/docs/assets/images/case-study-dragosCTF.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-hackerone.php"&gt;&lt;code&gt;IT&lt;/code&gt; (Bug Bounty) - CAI on: HackerOne Platform&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-ecoforest.php"&gt;&lt;code&gt;OT&lt;/code&gt; - CAI and alias0 on: Ecoforest Heat Pumps&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;HackerOne's top engineers leverage CAI to explore next-gen agentic AI architectures and build their own security products. CAI's Retester agent directly inspired HackerOne's AI-powered Deduplication Agent, now deployed in production to handle millions of vulnerability reports at scale.&lt;/td&gt; 
   &lt;td&gt;CAI discovers critical vulnerability in Ecoforest heat pumps allowing unauthorized remote access and potential catastrophic failures. AI-powered security testing reveals exposed credentials and DES encryption weaknesses affecting all of their deployed units across Europe.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-hackerone.php"&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/docs/assets/images/case-study-hackerone.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-ecoforest.php"&gt;&lt;img src="https://aliasrobotics.com/img/case-study-portada-ecoforest.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-cai-mir.php"&gt;&lt;code&gt;Robotics&lt;/code&gt; - CAI and alias0 on: Mobile Industrial Robots (MiR)&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-mercado-libre.php"&gt;&lt;code&gt;IT&lt;/code&gt; (Web) - CAI and alias0 on: Mercado Libre's e-commerce&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CAI-powered security testing of MiR (Mobile Industrial Robot) platform through automated ROS message injection attacks. This study demonstrates how AI-driven vulnerability discovery can expose unauthorized access to robot control systems and alarm triggers.&lt;/td&gt; 
   &lt;td&gt;CAI-powered API vulnerability discovery at Mercado Libre through automated enumeration attacks. This study demonstrates how AI-driven security testing can expose user data exposure risks in e-commerce platforms at scale.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-cai-mir.php"&gt;&lt;img src="https://aliasrobotics.com/img/case-study-portada-mir-cai.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-mercado-libre.php"&gt;&lt;img src="https://aliasrobotics.com/img/case-study-portada-mercado-libre.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-cai-mqtt-broker.php"&gt;&lt;code&gt;OT&lt;/code&gt; - CAI and alias0 on: MQTT broker&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-portswigger-1.php"&gt;&lt;code&gt;IT&lt;/code&gt; (Web) - CAI and alias0 on: PortSwigger Web Security Academy&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CAI-powered testing exposed critical flaws in an MQTT broker within a Dockerized OT network. Without authentication, CAI subscribed to temperature and humidity topics and injected false values, corrupting data shown in Grafana dashboards.&lt;/td&gt; 
   &lt;td&gt;CAI-powered race condition exploitation in file upload vulnerability. This study demonstrates how AI-driven security testing can identify and exploit timing windows in web applications, successfully uploading and executing web shells through automated parallel requests.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-cai-mqtt-broker.php"&gt;&lt;img src="https://aliasrobotics.com/img/case-study-portada-mqtt-broker-cai.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-portswigger-1.php"&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/docs/assets/images/portada-portswigger-web-1.jpg" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] &lt;span&gt;‚ö†&lt;/span&gt; CAI is in active development, so don't expect it to work flawlessly. Instead, contribute by raising an issue or &lt;a href="https://github.com/aliasrobotics/cai/pulls"&gt;sending a PR&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;Access to this library and the use of information, materials (or portions thereof), is &lt;strong&gt;&lt;u&gt;not intended&lt;/u&gt;, and is &lt;u&gt;prohibited&lt;/u&gt;, where such access or use violates applicable laws or regulations&lt;/strong&gt;. By no means the authors encourage or promote the unauthorized tampering with running systems. This can cause serious human harm and material damages.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;By no means the authors of CAI encourage or promote the unauthorized tampering with compute systems. Please don't use the source code in here for cybercrime. &lt;u&gt;Pentest for good instead&lt;/u&gt;&lt;/em&gt;. By downloading, using, or modifying this source code, you agree to the terms of the &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/LICENSE"&gt;&lt;code&gt;LICENSE&lt;/code&gt;&lt;/a&gt; and the limitations outlined in the &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/DISCLAIMER"&gt;&lt;code&gt;DISCLAIMER&lt;/code&gt;&lt;/a&gt; file.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;span&gt;üîñ&lt;/span&gt; Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#cybersecurity-ai-cai"&gt;Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;)&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#bookmark-table-of-contents"&gt;&lt;span&gt;üîñ&lt;/span&gt; Table of Contents&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-impact"&gt;üéØ Impact&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-competitions-and-challenges"&gt;üèÜ Competitions and challenges&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-research-impact"&gt;üìä Research Impact&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-research-products-cybersecurity-ai"&gt;üìö Research products: &lt;code&gt;Cybersecurity AI&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#pocs"&gt;PoCs&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#motivation"&gt;Motivation&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#bust_in_silhouette-why-cai"&gt;&lt;span&gt;üë§&lt;/span&gt; Why CAI?&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#ethical-principles-behind-cai"&gt;Ethical principles behind CAI&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#closed-source-alternatives"&gt;Closed-source alternatives&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#learn---cai-fluency"&gt;Learn - &lt;code&gt;CAI&lt;/code&gt; Fluency&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#nut_and_bolt-install"&gt;&lt;span&gt;üî©&lt;/span&gt; Install&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#os-x"&gt;OS X&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#ubuntu-2404"&gt;Ubuntu 24.04&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#ubuntu-2004"&gt;Ubuntu 20.04&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#windows-wsl"&gt;Windows WSL&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#android"&gt;Android&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#nut_and_bolt-setup-env-file"&gt;&lt;span&gt;üî©&lt;/span&gt; Setup &lt;code&gt;.env&lt;/code&gt; file&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-custom-openai-base-url-support"&gt;üîπ Custom OpenAI Base URL Support&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#triangular_ruler-architecture"&gt;&lt;span&gt;üìê&lt;/span&gt; Architecture:&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-agent"&gt;üîπ Agent&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-tools"&gt;üîπ Tools&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-handoffs"&gt;üîπ Handoffs&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-patterns"&gt;üîπ Patterns&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-turns-and-interactions"&gt;üîπ Turns and Interactions&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-tracing"&gt;üîπ Tracing&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-guardrails"&gt;üîπ Guardrails&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-human-in-the-loop-hitl"&gt;üîπ Human-In-The-Loop (HITL)&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#rocket-quickstart"&gt;&lt;span&gt;üöÄ&lt;/span&gt; Quickstart&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#environment-variables"&gt;Environment Variables&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#openrouter-integration"&gt;OpenRouter Integration&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#azure-openai"&gt;Azure OpenAI&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#mcp"&gt;MCP&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#development"&gt;Development&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#contributions"&gt;Contributions&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#optional-requirements-caiextensions"&gt;Optional Requirements: caiextensions&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#information_source-usage-data-collection"&gt;&lt;span&gt;‚Ñπ&lt;/span&gt; Usage Data Collection&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#reproduce-ci-setup-locally"&gt;Reproduce CI-Setup locally&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#faq"&gt;FAQ&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#acknowledgements"&gt;Acknowledgements&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#academic-collaborations"&gt;Academic Collaborations&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üéØ Impact&lt;/h2&gt; 
&lt;h3&gt;üèÜ Competitions and challenges&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://app.hackthebox.com/users/2268644"&gt;&lt;img src="https://img.shields.io/badge/HTB_ranking-top_90_Spain_(5_days)-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://app.hackthebox.com/users/2268644"&gt;&lt;img src="https://img.shields.io/badge/HTB_ranking-top_50_Spain_(6_days)-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://app.hackthebox.com/users/2268644"&gt;&lt;img src="https://img.shields.io/badge/HTB_ranking-top_30_Spain_(7_days)-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://app.hackthebox.com/users/2268644"&gt;&lt;img src="https://img.shields.io/badge/HTB_ranking-top_500_World_(7_days)-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://ctf.hackthebox.com/event/2000/scoreboard"&gt;&lt;img src="https://img.shields.io/badge/HTB_%22Human_vs_AI%22_CTF-top_1_(AIs)_world-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://ctf.hackthebox.com/event/2000/scoreboard"&gt;&lt;img src="https://img.shields.io/badge/HTB_%22Human_vs_AI%22_CTF-top_1_Spain-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://ctf.hackthebox.com/event/2000/scoreboard"&gt;&lt;img src="https://img.shields.io/badge/HTB_%22Human_vs_AI%22_CTF-top_20_World-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://ctf.hackthebox.com/event/2000/scoreboard"&gt;&lt;img src="https://img.shields.io/badge/HTB_%22Human_vs_AI%22_CTF-750_$-yellow.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://lu.ma/roboticshack?tk=RuryKF"&gt;&lt;img src="https://img.shields.io/badge/Mistral_AI_Robotics_Hackathon-2500_$-yellow.svg?sanitize=true" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;üìä Research Impact&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Pioneered LLM-powered AI Security with PentestGPT, establishing the foundation for the &lt;code&gt;Cybersecurity AI&lt;/code&gt; research domain &lt;a href="https://arxiv.org/pdf/2308.06782"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2308.06782-4a9b8e.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Established the &lt;code&gt;Cybersecurity AI&lt;/code&gt; research line with &lt;strong&gt;8 papers and technical reports&lt;/strong&gt;, with active research collaborations &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-63bfab.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2506.23592"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2506.23592-7dd3c0.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2508.13588"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.13588-52a896.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2508.21669"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.21669-85e0d1.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2509.14096"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2509.14096-3e8b7a.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2509.14139"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2509.14139-6bc7b5.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2510.17521"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2510.17521-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2510.24317"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2510.24317-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Demonstrated &lt;strong&gt;3,600√ó performance improvement&lt;/strong&gt; over human penetration testers in standardized CTF benchmark evaluations &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-63bfab.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Identified &lt;strong&gt;CVSS 4.3-7.5 severity vulnerabilities&lt;/strong&gt; in production systems through automated security assessment &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-63bfab.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Democratization of AI-empowered vulnerability research&lt;/strong&gt;: CAI enables both non-security domain experts and experienced researchers to conduct more efficient vulnerability discovery, expanding the security research community while empowering small and medium enterprises to conduct autonomous security assessments &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-63bfab.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Systematic evaluation of large language models&lt;/strong&gt; across both proprietary and open-weight architectures, revealing &lt;u&gt;substantial gaps&lt;/u&gt; between vendor-reported capabilities and empirical cybersecurity performance metrics &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-63bfab.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Established the &lt;strong&gt;autonomy levels in cybersecurity&lt;/strong&gt; and argued about autonomy vs automation in the field &lt;a href="https://arxiv.org/abs/2506.23592"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2506.23592-7dd3c0.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Collaborative research initiatives&lt;/strong&gt; with international academic institutions focused on developing cybersecurity education curricula and training methodologies &lt;a href="https://arxiv.org/abs/2508.13588"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.13588-52a896.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Contributed a comprehensive defense framework against prompt injection in AI security agents&lt;/strong&gt;: developed and empirically validated a multi-layered defense system that addresses the identified prompt injection issues &lt;a href="https://arxiv.org/abs/2508.21669"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.21669-85e0d1.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Explord the Cybersecurity of Humanoid Robots with CAI and identified new attack vectors showing how it &lt;code&gt;(a)&lt;/code&gt; operates simultaneously as a covert surveillance node and &lt;code&gt;(b)&lt;/code&gt; can be purposed as an active cyber operations platform &lt;a href="https://arxiv.org/abs/2509.14096"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2509.14096-3e8b7a.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2509.14139"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2509.14139-6bc7b5.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìö Research products: &lt;code&gt;Cybersecurity AI&lt;/code&gt;&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;CAI, An Open, Bug Bounty-Ready Cybersecurity AI &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-63bfab.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;The Dangerous Gap Between Automation and Autonomy &lt;a href="https://arxiv.org/abs/2506.23592"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2506.23592-7dd3c0.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;CAI Fluency, A Framework for Cybersecurity AI Fluency &lt;a href="https://arxiv.org/abs/2508.13588"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.13588-52a896.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;Hacking the AI Hackers via Prompt Injection &lt;a href="https://arxiv.org/abs/2508.21669"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.21669-85e0d1.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://aliasrobotics.com/img/paper-cai.png" width="350" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.arxiv.org/pdf/2506.23592"&gt;&lt;img src="https://aliasrobotics.com/img/cai_automation_vs_autonomy.png" width="350" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2508.13588"&gt;&lt;img src="https://aliasrobotics.com/img/cai_fluency_cover.png" width="350" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2508.21669"&gt;&lt;img src="https://aliasrobotics.com/img/aihackers.jpeg" width="350" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Humanoid Robots as Attack Vectors &lt;a href="https://arxiv.org/abs/2509.14139"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2509.14139-6bc7b5.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;The Cybersecurity of a Humanoid Robot &lt;a href="https://arxiv.org/abs/2509.14096"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2509.14096-3e8b7a.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;Evaluating Agentic Cybersecurity in Attack/Defense CTFs &lt;a href="https://arxiv.org/abs/2510.17521"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2510.17521-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;CAIBench: A Meta-Benchmark for Evaluating Cybersecurity AI Agents &lt;a href="https://arxiv.org/abs/2510.24317"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2510.24317-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2509.14139"&gt;&lt;img src="https://aliasrobotics.com/img/humanoids-cover.png" width="350" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2509.14096"&gt;&lt;img src="https://aliasrobotics.com/img/humanoid.png" width="350" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2510.17521"&gt;&lt;img src="https://aliasrobotics.com/img/cai_ad.png" width="350" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2510.24317"&gt;&lt;img src="https://aliasrobotics.com/img/caibench_banner2.png" width="350" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;PoCs&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;CAI with &lt;code&gt;alias0&lt;/code&gt; on ROS message injection attacks in MiR-100 robot&lt;/th&gt; 
   &lt;th&gt;CAI with &lt;code&gt;alias0&lt;/code&gt; on API vulnerability discovery at Mercado Libre&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://asciinema.org/a/dNv705hZel2Rzrw0cju9HBGPh"&gt;&lt;img src="https://asciinema.org/a/dNv705hZel2Rzrw0cju9HBGPh.svg?sanitize=true" alt="asciicast" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://asciinema.org/a/9Hc9z1uFcdNjqP3bY5y7wO1Ww"&gt;&lt;img src="https://asciinema.org/a/9Hc9z1uFcdNjqP3bY5y7wO1Ww.svg?sanitize=true" alt="asciicast" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;CAI on JWT@PortSwigger CTF ‚Äî Cybersecurity AI&lt;/th&gt; 
   &lt;th&gt;CAI on HackableII Boot2Root CTF ‚Äî Cybersecurity AI&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://asciinema.org/a/713487"&gt;&lt;img src="https://asciinema.org/a/713487.svg?sanitize=true" alt="asciicast" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://asciinema.org/a/713485"&gt;&lt;img src="https://asciinema.org/a/713485.svg?sanitize=true" alt="asciicast" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;More case studies and PoCs are available at &lt;a href="https://aliasrobotics.com/case-studies-robot-cybersecurity.php"&gt;https://aliasrobotics.com/case-studies-robot-cybersecurity.php&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Motivation&lt;/h2&gt; 
&lt;h3&gt;&lt;span&gt;üë§&lt;/span&gt; Why CAI?&lt;/h3&gt; 
&lt;p&gt;The cybersecurity landscape is undergoing a dramatic transformation as AI becomes increasingly integrated into security operations. &lt;strong&gt;We predict that by 2028, AI-powered security testing tools will outnumber human pentesters&lt;/strong&gt;. This shift represents a fundamental change in how we approach cybersecurity challenges. &lt;em&gt;AI is not just another tool - it's becoming essential for addressing complex security vulnerabilities and staying ahead of sophisticated threats. As organizations face more advanced cyber attacks, AI-enhanced security testing will be crucial for maintaining robust defenses.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;This work builds upon prior efforts[^4] and similarly, we believe that democratizing access to advanced cybersecurity AI tools is vital for the entire security community. That's why we're releasing Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;) as an open source framework. Our goal is to empower security researchers, ethical hackers, and organizations to build and deploy powerful AI-driven security tools. By making these capabilities openly available, we aim to level the playing field and ensure that cutting-edge security AI technology isn't limited to well-funded private companies or state actors.&lt;/p&gt; 
&lt;p&gt;Bug Bounty programs have become a cornerstone of modern cybersecurity, providing a crucial mechanism for organizations to identify and fix vulnerabilities in their systems before they can be exploited. These programs have proven highly effective at securing both public and private infrastructure, with researchers discovering critical vulnerabilities that might have otherwise gone unnoticed. CAI is specifically designed to enhance these efforts by providing a lightweight, ergonomic framework for building specialized AI agents that can assist in various aspects of Bug Bounty hunting - from initial reconnaissance to vulnerability validation and reporting. Our framework aims to augment human expertise with AI capabilities, helping researchers work more efficiently and thoroughly in their quest to make digital systems more secure.&lt;/p&gt; 
&lt;h3&gt;Ethical principles behind CAI&lt;/h3&gt; 
&lt;p&gt;You might be wondering if releasing CAI &lt;em&gt;in-the-wild&lt;/em&gt; given its capabilities and security implications is ethical. Our decision to open-source this framework is guided by two core ethical principles:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Democratizing Cybersecurity AI&lt;/strong&gt;: We believe that advanced cybersecurity AI tools should be accessible to the entire security community, not just well-funded private companies or state actors. By releasing CAI as an open source framework, we aim to empower security researchers, ethical hackers, and organizations to build and deploy powerful AI-driven security tools, leveling the playing field in cybersecurity.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Transparency in AI Security Capabilities&lt;/strong&gt;: Based on our research results, understanding of the technology, and dissection of top technical reports, we argue that current LLM vendors are undermining their cybersecurity capabilities. This is extremely dangerous and misleading. By developing CAI openly, we provide a transparent benchmark of what AI systems can actually do in cybersecurity contexts, enabling more informed decisions about security postures.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;CAI is built on the following core principles:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Cybersecurity oriented AI framework&lt;/strong&gt;: CAI is specifically designed for cybersecurity use cases, aiming at semi- and fully-automating offensive and defensive security tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open source, free for research&lt;/strong&gt;: CAI is open source and free for research purposes. We aim at democratizing access to AI and Cybersecurity. For professional or commercial use, including on-premise deployments, dedicated technical support and custom extensions &lt;a href="mailto:research@aliasrobotics.com"&gt;reach out&lt;/a&gt; to obtain a license.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Lightweight&lt;/strong&gt;: CAI is designed to be fast, and easy to use.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Modular and agent-centric design&lt;/strong&gt;: CAI operates on the basis of agents and agentic patterns, which allows flexibility and scalability. You can easily add the most suitable agents and pattern for your cybersecuritytarget case.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tool-integration&lt;/strong&gt;: CAI integrates already built-in tools, and allows the user to integrate their own tools with their own logic easily.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Logging and tracing integrated&lt;/strong&gt;: using &lt;a href="https://github.com/Arize-ai/phoenix"&gt;&lt;code&gt;phoenix&lt;/code&gt;&lt;/a&gt;, the open source tracing and logging tool for LLMs. This provides the user with a detailed traceability of the agents and their execution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Model Support&lt;/strong&gt;: more than 300 supported and empowered by &lt;a href="https://github.com/BerriAI/litellm"&gt;LiteLLM&lt;/a&gt;. The most popular providers: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Anthropic&lt;/strong&gt;: &lt;code&gt;Claude 3.7&lt;/code&gt;, &lt;code&gt;Claude 3.5&lt;/code&gt;, &lt;code&gt;Claude 3&lt;/code&gt;, &lt;code&gt;Claude 3 Opus&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;OpenAI&lt;/strong&gt;: &lt;code&gt;O1&lt;/code&gt;, &lt;code&gt;O1 Mini&lt;/code&gt;, &lt;code&gt;O3 Mini&lt;/code&gt;, &lt;code&gt;GPT-4o&lt;/code&gt;, &lt;code&gt;GPT-4.5 Preview&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;DeepSeek&lt;/strong&gt;: &lt;code&gt;DeepSeek V3&lt;/code&gt;, &lt;code&gt;DeepSeek R1&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt;: &lt;code&gt;Qwen2.5 72B&lt;/code&gt;, &lt;code&gt;Qwen2.5 14B&lt;/code&gt;, etc&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Closed-source alternatives&lt;/h3&gt; 
&lt;p&gt;Cybersecurity AI is a critical field, yet many groups are misguidedly pursuing it through closed-source methods for pure economic return, leveraging similar techniques and building upon existing closed-source (&lt;em&gt;often third-party owned&lt;/em&gt;) models. This approach not only squanders valuable engineering resources but also represents an economic waste and results in redundant efforts, as they often end up reinventing the wheel. Here are some of the closed-source initiatives we keep track of and attempting to leverage genAI and agentic frameworks in cybersecurity AI:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.acyber.co/"&gt;Autonomous Cyber&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cracken.ai/"&gt;CrackenAGI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ethiack.com/"&gt;ETHIACK&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://horizon3.ai/"&gt;Horizon3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.irregular.com/"&gt;Irregular&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.kindo.ai/"&gt;Kindo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://lakera.ai"&gt;Lakera&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/www.mindfort.ai"&gt;Mindfort&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mindgard.ai/"&gt;Mindgard&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ndaysecurity.com/"&gt;NDAY Security&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://penligent.ai/"&gt;Penligent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.runsybil.com"&gt;Runsybil&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.selfhack.fi"&gt;Selfhack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://sola.security/"&gt;Sola Security&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://squr.ai/"&gt;SQUR&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://staris.tech/"&gt;Staris&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.sxipher.com/"&gt;Sxipher&lt;/a&gt; (seems discontinued)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.terra.security"&gt;Terra Security&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://vibeproxy.app/"&gt;Vibeproxy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://xint.io/"&gt;Xint&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.xbow.com"&gt;XBOW&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.zeropath.com"&gt;ZeroPath&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.zynap.com"&gt;Zynap&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://7ai.com"&gt;7ai&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Learn - &lt;code&gt;CAI&lt;/code&gt; Fluency&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a align="center" href="" target="https://github.com/aliasrobotics/CAI"&gt; &lt;img width="100%" src="https://github.com/aliasrobotics/cai/raw/main/media/caiedu.PNG" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;CAI Fluency technical report (&lt;a href="https://arxiv.org/pdf/2508.13588"&gt;arXiv:2508.13588&lt;/a&gt;) establishes formal educational frameworks for cybersecurity AI literacy.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;English&lt;/th&gt; 
   &lt;th&gt;Spanish&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 0&lt;/strong&gt;: What is CAI?&lt;/td&gt; 
   &lt;td&gt;Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;) explained&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=nBdTxbKM4oo"&gt;&lt;img src="https://img.youtube.com/vi/nBdTxbKM4oo/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=FaUL9HXrQ5k"&gt;&lt;img src="https://img.youtube.com/vi/FaUL9HXrQ5k/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 1&lt;/strong&gt;: The &lt;code&gt;CAI&lt;/code&gt; Framework&lt;/td&gt; 
   &lt;td&gt;Vision &amp;amp; Ethics - Explore the core motivation behind CAI and delve into the crucial ethical principles guiding its development. Understand the motivation behind CAI and how you can actively contribute to the future of cybersecurity and the CAI framework.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=QEiGdsMf29M&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=3"&gt;&lt;img src="https://img.youtube.com/vi/QEiGdsMf29M/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 2&lt;/strong&gt;: From Zero to Cyber Hero&lt;/td&gt; 
   &lt;td&gt;Breaking into Cybersecurity with AI - A comprehensive guide for complete beginners to become cybersecurity practitioners using CAI and AI tools. Learn how to leverage artificial intelligence to accelerate your cybersecurity learning journey, from understanding basic security concepts to performing real-world security assessments, all without requiring prior cybersecurity experience.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=hSTLHOOcQoY&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=14"&gt;&lt;img src="https://img.youtube.com/vi/hSTLHOOcQoY/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 3&lt;/strong&gt;: Vibe-Hacking Tutorial&lt;/td&gt; 
   &lt;td&gt;"My first Hack" - A Vibe-Hacking guide for newbies. We demonstrate a simple web security hack using a default agent and show how to leverage tools and interpret CAI output with the help of the CAI Python API. You'll also learn to compare different LLM models to find the best fit for your hacking endeavors.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=9vZ_Iyex7uI&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=1"&gt;&lt;img src="https://img.youtube.com/vi/9vZ_Iyex7uI/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=iAOMaI1ftiA&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=2"&gt;&lt;img src="https://img.youtube.com/vi/iAOMaI1ftiA/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 4&lt;/strong&gt;: Intro ReAct&lt;/td&gt; 
   &lt;td&gt;The Evolution of LLMs - Learn how LLMs evolved from basic language models to advanced multiagency AI systems. From basic LLMs to Chain-of-Thought and Reasoning LLMs towards ReAct and Multi-Agent Architectures. Get to know the basic terms&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=tLdFO1flj_o&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=13"&gt;&lt;img src="https://img.youtube.com/vi/tLdFO1flj_o/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 5&lt;/strong&gt;: CAI on CTF challenges&lt;/td&gt; 
   &lt;td&gt;Dive into Capture The Flag (CTF) competitions using CAI. Learn how to leverage AI agents to solve various cybersecurity challenges including web exploitation, cryptography, reverse engineering, and forensics. Discover how to configure CAI for competitive hacking scenarios and maximize your CTF performance with intelligent automation.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=MrXTQ0e2to4&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=13"&gt;&lt;img src="https://img.youtube.com/vi/MrXTQ0e2to4/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=r9US_JZa9_c&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=12"&gt;&lt;img src="https://img.youtube.com/vi/r9US_JZa9_c/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Annex 1&lt;/strong&gt;: &lt;code&gt;CAI&lt;/code&gt; 0.5.x release&lt;/td&gt; 
   &lt;td&gt;Introduce version 0.5 of &lt;code&gt;CAI&lt;/code&gt; including new multi-agent functionality, new commands such as &lt;code&gt;/history&lt;/code&gt;, &lt;code&gt;/compact&lt;/code&gt;, &lt;code&gt;/graph&lt;/code&gt; or &lt;code&gt;/memory&lt;/code&gt; and a case study showing how &lt;code&gt;CAI&lt;/code&gt; found a critical security flaw in OT heap pumps spread around the world.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=OPFH0ANUMMw"&gt;&lt;img src="https://img.youtube.com/vi/OPFH0ANUMMw/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=Q8AI4E4gH8k"&gt;&lt;img src="https://img.youtube.com/vi/Q8AI4E4gH8k/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Annex 2&lt;/strong&gt;: &lt;code&gt;CAI&lt;/code&gt; 0.4.x release and &lt;code&gt;alias0&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Introducing version 0.4 of &lt;code&gt;CAI&lt;/code&gt; with &lt;em&gt;streaming&lt;/em&gt; and improved MCP support. We also introduce &lt;code&gt;alias0&lt;/code&gt;, the Privacy-First Cybersecurity AI, a Model-of-Models Intelligence that implements a Privacy-by-Design architecture and obtains state-of-the-art results in cybersecurity benchmarks.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=NZjzfnvAZcc"&gt;&lt;img src="https://img.youtube.com/vi/NZjzfnvAZcc/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Annex 3&lt;/strong&gt;: Cybersecurity AI Community Meeting #1&lt;/td&gt; 
   &lt;td&gt;First Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;) community meeting, over 40 participants from academia, industry, and defense gathered to discuss the open-source scaffolding behind CAI ‚Äî a project designed to build agentic AI systems for cybersecurity that are open, modular, and Bug Bounty-ready.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=4JqaTiVlgsw"&gt;&lt;img src="https://img.youtube.com/vi/4JqaTiVlgsw/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Annex 4&lt;/strong&gt;: &lt;code&gt;CAI PRO&lt;/code&gt; PoC&lt;/td&gt; 
   &lt;td&gt;Short proof-of-concept demonstration of &lt;a href="https://aliasrobotics.com/cybersecurityai.php"&gt;CAI PRO&lt;/a&gt; capabilities showcasing the Professional Edition with unlimited &lt;code&gt;alias1&lt;/code&gt; tokens, unrestricted AI, and enterprise-grade security testing features.&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/media/caipro_poc.gif" alt="CAI PRO Demo" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Annex 5&lt;/strong&gt;: &lt;code&gt;CAI&lt;/code&gt; PoC&lt;/td&gt; 
   &lt;td&gt;Short proof-of-concept demonstration of CAI Community Edition showcasing the open-source framework's core capabilities for AI-powered security testing and vulnerability discovery.&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/media/cai_poc.gif" alt="CAI Demo" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Annex 6&lt;/strong&gt;: CAI in &lt;code&gt;Jaula del N00B&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;CAI (CIBERSEGURIDAD CON IA) LUIJAIT EN LA JAULA DEL N00B - Demonstration and discussion of CAI framework capabilities in the popular Spanish cybersecurity podcast/show.&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=KD2_xzIOkWg"&gt;&lt;img src="https://img.youtube.com/vi/KD2_xzIOkWg/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;&lt;span&gt;üî©&lt;/span&gt; Install&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] &lt;strong&gt;CAI Professional Edition Users&lt;/strong&gt;: If you have an active CAI Pro subscription, we provide dedicated installation guides for versions 0.5 and 0.6. Official support is available for Ubuntu 24.04 (x86_64). Installation instructions for other operating systems are provided as-is without official support:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/docs/Installation_Guide_for_CAI_Pro_v0.6.md"&gt;CAI Pro v0.6 Installation Guide&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/docs/Installation_Guide_for_CAI_Pro_v0.5.md"&gt;CAI Pro v0.5 Installation Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Community Edition Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install cai-framework
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Always create a new virtual environment to ensure proper dependency installation when updating CAI.&lt;/p&gt; 
&lt;p&gt;The following subsections provide a more detailed walkthrough on selected popular Operating Systems. Refer to the &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#development"&gt;Development&lt;/a&gt; section for developer-related install instructions. For API Keys env syntax check litellm Documentation. &lt;a href="https://docs.litellm.ai/docs/tutorials/installation"&gt;LiteLLM Documentation&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;OS X&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew update &amp;amp;&amp;amp; \
    brew install git python@3.12

# Create virtual environment
python3.12 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip install cai-framework

# Generate a .env file and set up with defaults
echo -e 'OPENAI_API_KEY="sk-1234"\nANTHROPIC_API_KEY=""\nOLLAMA=""\nPROMPT_TOOLKIT_NO_CPR=1\nCAI_STREAM=false' &amp;gt; .env

# Launch CAI
cai  # first launch it can take up to 30 seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Ubuntu 24.04&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update &amp;amp;&amp;amp; \
    sudo apt-get install -y git python3-pip python3.12-venv

# Create the virtual environment
python3.12 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip install cai-framework

# Generate a .env file and set up with defaults
echo -e 'OPENAI_API_KEY="sk-1234"\nANTHROPIC_API_KEY=""\nOLLAMA=""\nPROMPT_TOOLKIT_NO_CPR=1\nCAI_STREAM=false' &amp;gt; .env

# Launch CAI
cai  # first launch it can take up to 30 seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Ubuntu 20.04&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update &amp;amp;&amp;amp; \
    sudo apt-get install -y software-properties-common

# Fetch Python 3.12
sudo add-apt-repository ppa:deadsnakes/ppa &amp;amp;&amp;amp; sudo apt update
sudo apt install python3.12 python3.12-venv python3.12-dev -y

# Create the virtual environment
python3.12 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip install cai-framework

# Generate a .env file and set up with defaults
echo -e 'OPENAI_API_KEY="sk-1234"\nANTHROPIC_API_KEY=""\nOLLAMA=""\nPROMPT_TOOLKIT_NO_CPR=1\nCAI_STREAM=false' &amp;gt; .env

# Launch CAI
cai  # first launch it can take up to 30 seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Windows WSL&lt;/h3&gt; 
&lt;p&gt;Go to the Microsoft page: &lt;a href="https://learn.microsoft.com/en-us/windows/wsl/install"&gt;https://learn.microsoft.com/en-us/windows/wsl/install&lt;/a&gt;. Here you will find all the instructions to install WSL&lt;/p&gt; 
&lt;p&gt;From Powershell write: wsl --install&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;
sudo apt-get update &amp;amp;&amp;amp; \
    sudo apt-get install -y git python3-pip python3-venv

# Create the virtual environment
python3 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip install cai-framework

# Generate a .env file and set up with defaults. If Ollama runs on your windows host, wsl needs to use your host IP for it to become reachable
echo -e 'OPENAI_API_KEY="sk-1234"\nANTHROPIC_API_KEY=""\nOLLAMA=""\nOLLAMA_API_BASE="http://Your.Host.Ip.Here:11434"\nPROMPT_TOOLKIT_NO_CPR=1\nCAI_STREAM=false' &amp;gt; .env

# Launch CAI
cai  # first launch it can take up to 30 seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You might run into issues running cai on ubuntu since some agents assume they are running on a Kali Instance and are not able to find the tools needed. So as an alternative you can use the docker compose file in the dockerized folder instead. This also works from within wsl if docker is installed. in that case fetch the dockerized folder (no need for the whole repo) and run from within it. For API Keys env syntax check litellm Documentation. &lt;a href="https://docs.litellm.ai/docs/tutorials/installation"&gt;LiteLLM Documentation&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;#build and run docker compose Build takes around 20 min.
docker compose build &amp;amp;&amp;amp; docker compose up -d

#access cai
docker compose exec cai cai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Android&lt;/h3&gt; 
&lt;p&gt;We recommend having at least 8 GB of RAM:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;First of all, install userland &lt;a href="https://play.google.com/store/apps/details?id=tech.ula&amp;amp;hl=es"&gt;https://play.google.com/store/apps/details?id=tech.ula&amp;amp;hl=es&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install Kali minimal in basic options (for free). [Or any other kali option if preferred]&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Update apt keys like in this example: &lt;a href="https://superuser.com/questions/1644520/apt-get-update-issue-in-kali"&gt;https://superuser.com/questions/1644520/apt-get-update-issue-in-kali&lt;/a&gt;, inside UserLand's Kali terminal execute&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Get new apt keys
wget http://http.kali.org/kali/pool/main/k/kali-archive-keyring/kali-archive-keyring_2024.1_all.deb

# Install new apt keys
sudo dpkg -i kali-archive-keyring_2024.1_all.deb &amp;amp;&amp;amp; rm kali-archive-keyring_2024.1_all.deb

# Update APT repository
sudo apt-get update

# CAI requieres python 3.12, lets install it (CAI for kali in Android)
sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install -y git python3-pip build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev libsqlite3-dev wget libbz2-dev pkg-config
wget https://www.python.org/ftp/python/3.12.4/Python-3.12.4.tar.xz
tar xf Python-3.12.4.tar.xz
cd ./configure --enable-optimizations
sudo make altinstall # This command takes long to execute

# Clone CAI's source code
git clone https://github.com/aliasrobotics/cai &amp;amp;&amp;amp; cd cai

# Create virtual environment
python3.12 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip3 install -e .

# Generate a .env file and set up
cp .env.example .env  # edit here your keys/models

# Launch CAI
cai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;span&gt;üî©&lt;/span&gt; Setup &lt;code&gt;.env&lt;/code&gt; file&lt;/h3&gt; 
&lt;p&gt;CAI leverages the &lt;code&gt;.env&lt;/code&gt; file to load configuration at launch. To facilitate the setup, the repo provides an exemplary &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/.env.example"&gt;&lt;code&gt;.env.example&lt;/code&gt;&lt;/a&gt; file provides a template for configuring CAI's setup and your LLM API keys to work with desired LLM models.&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;‚ö†&lt;/span&gt; Important:&lt;/p&gt; 
&lt;p&gt;CAI does NOT provide API keys for any model by default. Don't ask us to provide keys, use your own or host your own models.&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;‚ö†&lt;/span&gt; Note:&lt;/p&gt; 
&lt;p&gt;The OPENAI_API_KEY must not be left blank. It should contain either "sk-123" (as a placeholder) or your actual API key. See &lt;a href="https://github.com/aliasrobotics/cai/issues/27"&gt;https://github.com/aliasrobotics/cai/issues/27&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;‚ö†&lt;/span&gt; Note:&lt;/p&gt; 
&lt;p&gt;If you are using alias1 model, make sure that CAI is &amp;gt;0.4.0 version and here you have an .env example to be able to use it.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OPENAI_API_KEY="sk-1234"
OLLAMA=""
ALIAS_API_KEY="&amp;lt;sk-your-key&amp;gt;"  # note, add yours
CAI_STEAM=False
CAI_MODEL="alias1"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üîπ Custom OpenAI Base URL Support&lt;/h3&gt; 
&lt;p&gt;CAI supports configuring a custom OpenAI API base URL via the &lt;code&gt;OPENAI_BASE_URL&lt;/code&gt; environment variable. This allows users to redirect API calls to a custom endpoint, such as a proxy or self-hosted OpenAI-compatible service.&lt;/p&gt; 
&lt;p&gt;Example &lt;code&gt;.env&lt;/code&gt; entry configuration:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;OLLAMA_API_BASE="https://custom-openai-proxy.com/v1"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or directly from the command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OLLAMA_API_BASE="https://custom-openai-proxy.com/v1" cai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;span&gt;üìê&lt;/span&gt; Architecture:&lt;/h2&gt; 
&lt;p&gt;CAI focuses on making cybersecurity agent &lt;strong&gt;coordination&lt;/strong&gt; and &lt;strong&gt;execution&lt;/strong&gt; lightweight, highly controllable, and useful for humans. To do so it builds upon 8 pillars: &lt;code&gt;Agents&lt;/code&gt;, &lt;code&gt;Tools&lt;/code&gt;, &lt;code&gt;Handoffs&lt;/code&gt;, &lt;code&gt;Patterns&lt;/code&gt;, &lt;code&gt;Turns&lt;/code&gt;, &lt;code&gt;Tracing&lt;/code&gt;, &lt;code&gt;Guardrails&lt;/code&gt; and &lt;code&gt;HITL&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚îÇ      HITL     ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Turns   ‚îÇ
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ
                          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Patterns ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Handoffs ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ   Agents  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    LLMs   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ                   ‚îÇ
                          ‚îÇ                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Extensions ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Tracing  ‚îÇ       ‚îÇ   Tools   ‚îÇ‚óÄ‚îÄ‚îÄ‚ñ∂‚îÇ Guardrails ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                              ‚îÇ
                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                          ‚ñº             ‚ñº          ‚ñº             ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ LinuxCmd  ‚îÇ‚îÇ WebSearch ‚îÇ‚îÇ    Code    ‚îÇ‚îÇ SSHTunnel ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to dive deeper into the code, check the following files as a start point for using CAI:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/__init__.py"&gt;&lt;strong&gt;init&lt;/strong&gt;.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/cli.py"&gt;cli.py&lt;/a&gt; - entrypoint for command line interface&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/util.py"&gt;util.py&lt;/a&gt; - utility functions&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/agents"&gt;agents&lt;/a&gt; - Agent implementations&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/internal"&gt;internal&lt;/a&gt; - CAI internal functions (endpoints, metrics, logging, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/prompts"&gt;prompts&lt;/a&gt; - Agent Prompt Database&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/repl"&gt;repl&lt;/a&gt; - CLI aesthetics and commands&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/sdk"&gt;sdk&lt;/a&gt; - CAI command sdk&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/tree/main/src/cai/tools"&gt;tools&lt;/a&gt; - agent tools&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üîπ Agent&lt;/h3&gt; 
&lt;p&gt;At its core, CAI abstracts its cybersecurity behavior via &lt;code&gt;Agents&lt;/code&gt; and agentic &lt;code&gt;Patterns&lt;/code&gt;. An Agent in &lt;em&gt;an intelligent system that interacts with some environment&lt;/em&gt;. More technically, within CAI we embrace a robotics-centric definition wherein an agent is anything that can be viewed as a system perceiving its environment through sensors, reasoning about its goals and and acting accordingly upon that environment through actuators (&lt;em&gt;adapted&lt;/em&gt; from Russel &amp;amp; Norvig, AI: A Modern Approach). In cybersecurity, an &lt;code&gt;Agent&lt;/code&gt; interacts with systems and networks, using peripherals and network interfaces as sensors, reasons accordingly and then executes network actions as if actuators. Correspondingly, in CAI, &lt;code&gt;Agent&lt;/code&gt;s implement the &lt;code&gt;ReACT&lt;/code&gt; (Reasoning and Action) agent model[^3]. For more information, see the &lt;a href="https://github.com/aliasrobotics/cai/raw/main/examples/basic/hello_world.py"&gt;example here&lt;/a&gt; for the full execution code, and refer to this &lt;a href="https://github.com/aliasrobotics/cai/raw/main/fluency/my-first-hack/my_first_hack.ipynb"&gt;jupyter notebook&lt;/a&gt; for a tutorial on how to use it.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from cai.sdk.agents import Agent, Runner, OpenAIChatCompletionsModel

import os
from openai import AsyncOpenAI
from dotenv import load_dotenv
load_dotenv()

agent = Agent(
      name="Custom Agent",
      instructions="""You are a Cybersecurity expert Leader""",
      model=OpenAIChatCompletionsModel(
          model=os.getenv('CAI_MODEL', "openai/gpt-4o"),
          openai_client=AsyncOpenAI(),
          )
      )

message = "Tell me about recursion in programming."
result = await Runner.run(agent, message)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üîπ Tools&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;Tools&lt;/code&gt; let cybersecurity agents take actions by providing interfaces to execute system commands, run security scans, analyze vulnerabilities, and interact with target systems and APIs - they are the core capabilities that enable CAI agents to perform security tasks effectively; in CAI, tools include built-in cybersecurity utilities (like LinuxCmd for command execution, WebSearch for OSINT gathering, Code for dynamic script execution, and SSHTunnel for secure remote access), function calling mechanisms that allow integration of any Python function as a security tool, and agent-as-tool functionality that enables specialized security agents (such as reconnaissance or exploit agents) to be used by other agents, creating powerful collaborative security workflows without requiring formal handoffs between agents. For more information, please refer to the &lt;a href="https://github.com/aliasrobotics/cai/raw/main/examples/basic/tools.py"&gt;example here&lt;/a&gt; for the complete configuration of custom functions.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from cai.sdk.agents import Agent, Runner, OpenAIChatCompletionsModel
from cai.tools.reconnaissance.exec_code import execute_code
from cai.tools.reconnaissance.generic_linux_command import generic_linux_command

import os
from openai import AsyncOpenAI
from dotenv import load_dotenv
load_dotenv()

agent = Agent(
      name="Custom Agent",
      instructions="""You are a Cybersecurity expert Leader""",
      tools= [
        generic_linux_command,
        execute_code
      ],
      model=OpenAIChatCompletionsModel(
          model=os.getenv('CAI_MODEL', "openai/gpt-4o"),
          openai_client=AsyncOpenAI(),
          )
      )

message = "Tell me about recursion in programming."
result = await Runner.run(agent, message)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You may find different &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/tools"&gt;tools&lt;/a&gt;. They are grouped in 6 major categories inspired by the security kill chain [^2]:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Reconnaissance and weaponization - &lt;em&gt;reconnaissance&lt;/em&gt; (crypto, listing, etc)&lt;/li&gt; 
 &lt;li&gt;Exploitation - &lt;em&gt;exploitation&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Privilege escalation - &lt;em&gt;escalation&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Lateral movement - &lt;em&gt;lateral&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Data exfiltration - &lt;em&gt;exfiltration&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Command and control - &lt;em&gt;control&lt;/em&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;üîπ Handoffs&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;Handoffs&lt;/code&gt; allow an &lt;code&gt;Agent&lt;/code&gt; to delegate tasks to another agent, which is crucial in cybersecurity operations where specialized expertise is needed for different phases of an engagement. In our framework, &lt;code&gt;Handoffs&lt;/code&gt; are implemented as tools for the LLM, where a &lt;strong&gt;handoff/transfer function&lt;/strong&gt; like &lt;code&gt;transfer_to_flag_discriminator&lt;/code&gt; enables the &lt;code&gt;ctf_agent&lt;/code&gt; to pass control to the &lt;code&gt;flag_discriminator_agent&lt;/code&gt; once it believes it has found the flag. This creates a security validation chain where the first agent handles exploitation and flag discovery, while the second agent specializes in flag verification, ensuring proper segregation of duties and leveraging specialized capabilities of different models for distinct security tasks. For more information, please refer to the &lt;a href="https://github.com/aliasrobotics/cai/raw/main/examples/cai/agent_patterns/handoffs.py"&gt;example here&lt;/a&gt; for the full execution code.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from cai.sdk.agents import function_tool
from cai.tools.common import run_command
from cai.sdk.agents import Agent, OpenAIChatCompletionsModel, HandoffInputData, Runner, function_tool, handoff, trace
from cai.sdk.agents.extensions import handoff_filters

import os
from openai import AsyncOpenAI
from dotenv import load_dotenv
load_dotenv()

@function_tool
def execute_cli_command(command: str) -&amp;gt; str:
    return run_command(command)

flag_discriminator = Agent(
    name="Flag discriminator",
    description="Agent focused on extracting the flag from the output",
    instructions="You are an agent tailored to extract the flag from a given output.",
    model=OpenAIChatCompletionsModel(
        model=os.getenv('CAI_MODEL', "qwen2.5:14b"),
        openai_client=AsyncOpenAI(),
    ) 
)

ctf_agent = Agent(
    name="CTF agent",
    description="Agent focused on conquering security challenges",
    instructions="You are a Cybersecurity expert Leader facing a CTF",
    tools=[
        execute_cli_command,
    ],
    model=OpenAIChatCompletionsModel(
        model= os.getenv('CAI_MODEL', "qwen2.5:14b"),
        openai_client=AsyncOpenAI(),
    ), 
    handoffs = [flag_discriminator]
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üîπ Patterns&lt;/h3&gt; 
&lt;p&gt;An agentic &lt;code&gt;Pattern&lt;/code&gt; is a &lt;em&gt;structured design paradigm&lt;/em&gt; in artificial intelligence systems where autonomous or semi-autonomous agents operate within a defined &lt;em&gt;interaction framework&lt;/em&gt; (the pattern) to achieve a goal. These &lt;code&gt;Patterns&lt;/code&gt; specify the organization, coordination, and communication methods among agents, guiding decision-making, task execution, and delegation.&lt;/p&gt; 
&lt;p&gt;An agentic pattern (&lt;code&gt;AP&lt;/code&gt;) can be formally defined as a tuple:&lt;/p&gt; 
&lt;p&gt;\[ AP = (A, H, D, C, E) \]&lt;/p&gt; 
&lt;p&gt;wherein:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;\(A\) (Agents):&lt;/strong&gt; A set of autonomous entities, \( A = \{a_1, a_2, ..., a_n\} \), each with defined roles, capabilities, and internal states.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;\(H\) (Handoffs):&lt;/strong&gt; A function \( H: A \times T \to A \) that governs how tasks \( T \) are transferred between agents based on predefined logic (e.g., rules, negotiation, bidding).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;\(D\) (Decision Mechanism):&lt;/strong&gt; A decision function \( D: S \to A \) where \( S \) represents system states, and \( D \) determines which agent takes action at any given time.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;\(C\) (Communication Protocol):&lt;/strong&gt; A messaging function \( C: A \times A \to M \), where \( M \) is a message space, defining how agents share information.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;\(E\) (Execution Model):&lt;/strong&gt; A function \( E: A \times I \to O \) where \( I \) is the input space and \( O \) is the output space, defining how agents perform tasks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;When building &lt;code&gt;Patterns&lt;/code&gt;, we generall y classify them among one of the following categories, though others exist:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Agentic&lt;/strong&gt; &lt;code&gt;Pattern&lt;/code&gt; &lt;strong&gt;categories&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Swarm&lt;/code&gt; (Decentralized)&lt;/td&gt; 
   &lt;td&gt;Agents share tasks and self-assign responsibilities without a central orchestrator. Handoffs occur dynamically. &lt;em&gt;An example of a peer-to-peer agentic pattern is the &lt;code&gt;CTF Agentic Pattern&lt;/code&gt;, which involves a team of agents working together to solve a CTF challenge with dynamic handoffs.&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Hierarchical&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A top-level agent (e.g., "PlannerAgent") assigns tasks via structured handoffs to specialized sub-agents. Alternatively, the structure of the agents is harcoded into the agentic pattern with pre-defined handoffs.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Chain-of-Thought&lt;/code&gt; (Sequential Workflow)&lt;/td&gt; 
   &lt;td&gt;A structured pipeline where Agent A produces an output, hands it to Agent B for reuse or refinement, and so on. Handoffs follow a linear sequence. &lt;em&gt;An example of a chain-of-thought agentic pattern is the &lt;code&gt;ReasonerAgent&lt;/code&gt;, which involves a Reasoning-type LLM that provides context to the main agent to solve a CTF challenge with a linear sequence.&lt;/em&gt;[^1]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Auction-Based&lt;/code&gt; (Competitive Allocation)&lt;/td&gt; 
   &lt;td&gt;Agents "bid" on tasks based on priority, capability, or cost. A decision agent evaluates bids and hands off tasks to the best-fit agent.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Recursive&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A single agent continuously refines its own output, treating itself as both executor and evaluator, with handoffs (internal or external) to itself. &lt;em&gt;An example of a recursive agentic pattern is the &lt;code&gt;CodeAgent&lt;/code&gt; (when used as a recursive agent), which continuously refines its own output by executing code and updating its own instructions.&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;For more information and examples of common agentic patterns, see the &lt;a href="https://github.com/aliasrobotics/cai/raw/main/examples/agent_patterns/README.md"&gt;examples folder&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;üîπ Turns and Interactions&lt;/h3&gt; 
&lt;p&gt;During the agentic flow (conversation), we distinguish between &lt;strong&gt;interactions&lt;/strong&gt; and &lt;strong&gt;turns&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Interactions&lt;/strong&gt; are sequential exchanges between one or multiple agents. Each agent executing its logic corresponds with one &lt;em&gt;interaction&lt;/em&gt;. Since an &lt;code&gt;Agent&lt;/code&gt; in CAI generally implements the &lt;code&gt;ReACT&lt;/code&gt; agent model[^3], each &lt;em&gt;interaction&lt;/em&gt; consists of 1) a reasoning step via an LLM inference and 2) act by calling zero-to-n &lt;code&gt;Tools&lt;/code&gt;. This is defined in&lt;code&gt;process_interaction()&lt;/code&gt; in &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/core.py"&gt;core.py&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Turns&lt;/strong&gt;: A turn represents a cycle of one ore more &lt;strong&gt;interactions&lt;/strong&gt; which finishes when the &lt;code&gt;Agent&lt;/code&gt; (or &lt;code&gt;Pattern&lt;/code&gt;) executing returns &lt;code&gt;None&lt;/code&gt;, judging there're no further actions to undertake. This is defined in &lt;code&gt;run()&lt;/code&gt;, see &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/core.py"&gt;core.py&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] CAI Agents are not related to Assistants in the Assistants API. They are named similarly for convenience, but are otherwise completely unrelated. CAI is entirely powered by the Chat Completions API and is hence stateless between calls.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;üîπ Tracing&lt;/h3&gt; 
&lt;p&gt;CAI implements AI observability by adopting the OpenTelemetry standard and to do so, it leverages &lt;a href="https://github.com/Arize-ai/phoenix"&gt;Phoenix&lt;/a&gt; which provides comprehensive tracing capabilities through OpenTelemetry-based instrumentation, allowing you to monitor and analyze your security operations in real-time. This integration enables detailed visibility into agent interactions, tool usage, and attack vectors throughout penetration testing workflows, making it easier to debug complex exploitation chains, track vulnerability discovery processes, and optimize agent performance for more effective security assessments.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/media/tracing.png" alt="" /&gt;&lt;/p&gt; 
&lt;h3&gt;üîπ Guardrails&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;Guardrails&lt;/code&gt; provide a critical security layer for CAI agents, protecting against prompt injection attacks and preventing execution of dangerous commands. These guardrails run in parallel to agents, validating both input and output to ensure safe operation. The framework includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Input Guardrails&lt;/strong&gt;: Detect and block prompt injection attempts before they reach agents, using pattern matching, Unicode homograph detection, and AI-powered analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Output Guardrails&lt;/strong&gt;: Validate agent outputs before execution, preventing dangerous commands like reverse shells, fork bombs, or data exfiltration&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-layered Defense&lt;/strong&gt;: Protection at input, processing, and execution stages with tool-level validation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Base64/Base32 Aware&lt;/strong&gt;: Automatically decodes and analyzes encoded payloads to detect hidden malicious commands&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configurable&lt;/strong&gt;: Can be enabled/disabled via &lt;code&gt;CAI_GUARDRAILS&lt;/code&gt; environment variable&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For detailed implementation, see &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/docs/guardrails.md"&gt;docs/guardrails.md&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/docs/cai_prompt_injection.md"&gt;docs/cai_prompt_injection.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;üîπ Human-In-The-Loop (HITL)&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                      ‚îÇ                                 ‚îÇ
                      ‚îÇ      Cybersecurity AI (CAI)     ‚îÇ
                      ‚îÇ                                 ‚îÇ
                      ‚îÇ       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
                      ‚îÇ       ‚îÇ  Autonomous AI  ‚îÇ       ‚îÇ
                      ‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
                      ‚îÇ                ‚îÇ                ‚îÇ
                      ‚îÇ                ‚îÇ                ‚îÇ
                      ‚îÇ       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
                      ‚îÇ       ‚îÇ HITL Interaction ‚îÇ      ‚îÇ
                      ‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
                      ‚îÇ                ‚îÇ                ‚îÇ
                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                       ‚îÇ
                                       ‚îÇ Ctrl+C (cli.py)
                                       ‚îÇ
                           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                           ‚îÇ   Human Operator(s)   ‚îÇ
                           ‚îÇ  Expertise | Judgment ‚îÇ
                           ‚îÇ    Teleoperation      ‚îÇ
                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;CAI delivers a framework for building Cybersecurity AIs with a strong emphasis on &lt;em&gt;semi-autonomous&lt;/em&gt; operation, as the reality is that &lt;strong&gt;fully-autonomous&lt;/strong&gt; cybersecurity systems remain premature and face significant challenges when tackling complex tasks. While CAI explores autonomous capabilities, we recognize that effective security operations still require human teleoperation providing expertise, judgment, and oversight in the security process.&lt;/p&gt; 
&lt;p&gt;Accordingly, the Human-In-The-Loop (&lt;code&gt;HITL&lt;/code&gt;) module is a core design principle of CAI, acknowledging that human intervention and teleoperation are essential components of responsible security testing. Through the &lt;code&gt;cli.py&lt;/code&gt; interface, users can seamlessly interact with agents at any point during execution by simply pressing &lt;code&gt;Ctrl+C&lt;/code&gt;. This is implemented across &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/core.py"&gt;core.py&lt;/a&gt; and also in the REPL abstractions &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/repl"&gt;REPL&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;üöÄ&lt;/span&gt; Quickstart&lt;/h2&gt; 
&lt;p&gt;To start CAI after installing it, just type &lt;code&gt;cai&lt;/code&gt; in the CLI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;‚îî‚îÄ# cai

          CCCCCCCCCCCCC      ++++++++   ++++++++      IIIIIIIIII
       CCC::::::::::::C  ++++++++++       ++++++++++  I::::::::I
     CC:::::::::::::::C ++++++++++         ++++++++++ I::::::::I
    C:::::CCCCCCCC::::C +++++++++    ++     +++++++++ II::::::II
   C:::::C       CCCCCC +++++++     +++++     +++++++   I::::I
  C:::::C                +++++     +++++++     +++++    I::::I
  C:::::C                ++++                   ++++    I::::I
  C:::::C                 ++                     ++     I::::I
  C:::::C                  +   +++++++++++++++   +      I::::I
  C:::::C                    +++++++++++++++++++        I::::I
  C:::::C                     +++++++++++++++++         I::::I
   C:::::C       CCCCCC        +++++++++++++++          I::::I
    C:::::CCCCCCCC::::C         +++++++++++++         II::::::II
     CC:::::::::::::::C           +++++++++           I::::::::I
       CCC::::::::::::C             +++++             I::::::::I
          CCCCCCCCCCCCC               ++              IIIIIIIIII

                      Cybersecurity AI (CAI), vX.Y.Z
                          Bug bounty-ready AI

CAI&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That should initialize CAI and provide a prompt to execute any security task you want to perform. The navigation bar at the bottom displays important system information. This information helps you understand your environment while working with CAI.&lt;/p&gt; 
&lt;p&gt;Here's a quick &lt;a href="https://asciinema.org/a/zm7wS5DA2o0S9pu1Tb44pnlvy"&gt;demo video&lt;/a&gt; to help you get started with CAI. We'll walk through the basic steps ‚Äî from launching the tool to running your first AI-powered task in the terminal. Whether you're a beginner or just curious, this guide will show you how easy it is to begin using CAI.&lt;/p&gt; 
&lt;p&gt;From here on, type on &lt;code&gt;CAI&lt;/code&gt; and start your security exercise. Best way to learn is by example:&lt;/p&gt; 
&lt;h3&gt;Environment Variables&lt;/h3&gt; 
&lt;p&gt;For using private models, you are given a &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/.env.example"&gt;&lt;code&gt;.env.example&lt;/code&gt;&lt;/a&gt; file. Copy it and rename it as &lt;code&gt;.env&lt;/code&gt;. Fill in your corresponding API keys, and you are ready to use CAI.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;List of Environment Variables&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Variable&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_NAME&lt;/td&gt; 
    &lt;td&gt;Name of the CTF challenge to run (e.g. "picoctf_static_flag")&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_CHALLENGE&lt;/td&gt; 
    &lt;td&gt;Specific challenge name within the CTF to test&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_SUBNET&lt;/td&gt; 
    &lt;td&gt;Network subnet for the CTF container&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_IP&lt;/td&gt; 
    &lt;td&gt;IP address for the CTF container&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_INSIDE&lt;/td&gt; 
    &lt;td&gt;Whether to conquer the CTF from within container&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MODEL&lt;/td&gt; 
    &lt;td&gt;Model to use for agents&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_DEBUG&lt;/td&gt; 
    &lt;td&gt;Set debug output level (0: Only tool outputs, 1: Verbose debug output, 2: CLI debug output)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_BRIEF&lt;/td&gt; 
    &lt;td&gt;Enable/disable brief output mode&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MAX_TURNS&lt;/td&gt; 
    &lt;td&gt;Maximum number of turns for agent interactions&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_TRACING&lt;/td&gt; 
    &lt;td&gt;Enable/disable OpenTelemetry tracing&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_AGENT_TYPE&lt;/td&gt; 
    &lt;td&gt;Specify the agents to use (boot2root, one_tool...)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_STATE&lt;/td&gt; 
    &lt;td&gt;Enable/disable stateful mode&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MEMORY&lt;/td&gt; 
    &lt;td&gt;Enable/disable memory mode (episodic, semantic, all)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MEMORY_ONLINE&lt;/td&gt; 
    &lt;td&gt;Enable/disable online memory mode&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MEMORY_OFFLINE&lt;/td&gt; 
    &lt;td&gt;Enable/disable offline memory&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_ENV_CONTEXT&lt;/td&gt; 
    &lt;td&gt;Add dirs and current env to llm context&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MEMORY_ONLINE_INTERVAL&lt;/td&gt; 
    &lt;td&gt;Number of turns between online memory updates&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_PRICE_LIMIT&lt;/td&gt; 
    &lt;td&gt;Price limit for the conversation in dollars&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_REPORT&lt;/td&gt; 
    &lt;td&gt;Enable/disable reporter mode (ctf, nis2, pentesting)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_SUPPORT_MODEL&lt;/td&gt; 
    &lt;td&gt;Model to use for the support agent&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_SUPPORT_INTERVAL&lt;/td&gt; 
    &lt;td&gt;Number of turns between support agent executions&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_WORKSPACE&lt;/td&gt; 
    &lt;td&gt;Defines the name of the workspace&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_WORKSPACE_DIR&lt;/td&gt; 
    &lt;td&gt;Specifies the directory path where the workspace is located&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_GUARDRAILS&lt;/td&gt; 
    &lt;td&gt;Enable/disable guardrails for prompt injection protection (default: true)&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;OpenRouter Integration&lt;/h3&gt; 
&lt;p&gt;The Cybersecurity AI (CAI) platform offers seamless integration with OpenRouter, a unified interface for Large Language Models (LLMs). This integration is crucial for users who wish to leverage advanced AI capabilities in their cybersecurity tasks. OpenRouter acts as a bridge, allowing CAI to communicate with various LLMs, thereby enhancing the flexibility and power of the AI agents used within CAI.&lt;/p&gt; 
&lt;p&gt;To enable OpenRouter support in CAI, you need to configure your environment by adding specific entries to your &lt;code&gt;.env&lt;/code&gt; file. This setup ensures that CAI can interact with the OpenRouter API, facilitating the use of sophisticated models like Meta-LLaMA. Here‚Äôs how you can configure it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI_AGENT_TYPE=redteam_agent
CAI_MODEL=openrouter/meta-llama/llama-4-maverick
OPENROUTER_API_KEY=&amp;lt;sk-your-key&amp;gt;  # note, add yours
OPENROUTER_API_BASE=https://openrouter.ai/api/v1
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Azure OpenAI&lt;/h3&gt; 
&lt;p&gt;The Cybersecurity AI (CAI) platform integrates seamlessly with Azure OpenAI, enabling organizations to run CAI against enterprise-hosted models (e.g., gpt-4o). This pathway is ideal for teams that must operate within Azure governance while leveraging advanced model capabilities. To enable Azure OpenAI support in CAI, configure your environment by adding the following entries to your .env. This ensures CAI can reach your Azure deployment endpoint and authenticate correctly.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI_AGENT_TYPE=redteam_agent
CAI_MODEL=azure/&amp;lt;model-name-deployed&amp;gt;
# Required: keep non-empty even when using Azure
OPENAI_API_KEY=dummy
# Azure credentials and endpoint
AZURE_API_KEY=&amp;lt;your-azure-openai-key&amp;gt;
AZURE_API_BASE=https://&amp;lt;resource&amp;gt;.openai.azure.com/openai/deployments/&amp;lt;deployment-name&amp;gt;/chat/completions?api-version=2025-01-01-preview
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;MCP&lt;/h3&gt; 
&lt;p&gt;CAI supports the Model Context Protocol (MCP) for integrating external tools and services with AI agents. MCP is supported via two transport mechanisms:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;SSE (Server-Sent Events)&lt;/strong&gt; - For web-based servers that push updates over HTTP connections:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/mcp load http://localhost:9876/sse burp
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;&lt;strong&gt;STDIO (Standard Input/Output)&lt;/strong&gt; - For local inter-process communication:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/mcp load stdio myserver python mcp_server.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once connected, you can add the MCP tools to any agent:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/mcp add burp redteam_agent
Adding tools from MCP server 'burp' to agent 'Red Team Agent'...
                                 Adding tools to Red Team Agent
‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ Tool                              ‚îÉ Status ‚îÉ Details                                         ‚îÉ
‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©
‚îÇ send_http_request                 ‚îÇ Added  ‚îÇ Available as: send_http_request                 ‚îÇ
‚îÇ create_repeater_tab               ‚îÇ Added  ‚îÇ Available as: create_repeater_tab               ‚îÇ
‚îÇ send_to_intruder                  ‚îÇ Added  ‚îÇ Available as: send_to_intruder                  ‚îÇ
‚îÇ url_encode                        ‚îÇ Added  ‚îÇ Available as: url_encode                        ‚îÇ
‚îÇ url_decode                        ‚îÇ Added  ‚îÇ Available as: url_decode                        ‚îÇ
‚îÇ base64encode                      ‚îÇ Added  ‚îÇ Available as: base64encode                      ‚îÇ
‚îÇ base64decode                      ‚îÇ Added  ‚îÇ Available as: base64decode                      ‚îÇ
‚îÇ generate_random_string            ‚îÇ Added  ‚îÇ Available as: generate_random_string            ‚îÇ
‚îÇ output_project_options            ‚îÇ Added  ‚îÇ Available as: output_project_options            ‚îÇ
‚îÇ output_user_options               ‚îÇ Added  ‚îÇ Available as: output_user_options               ‚îÇ
‚îÇ set_project_options               ‚îÇ Added  ‚îÇ Available as: set_project_options               ‚îÇ
‚îÇ set_user_options                  ‚îÇ Added  ‚îÇ Available as: set_user_options                  ‚îÇ
‚îÇ get_proxy_http_history            ‚îÇ Added  ‚îÇ Available as: get_proxy_http_history            ‚îÇ
‚îÇ get_proxy_http_history_regex      ‚îÇ Added  ‚îÇ Available as: get_proxy_http_history_regex      ‚îÇ
‚îÇ get_proxy_websocket_history       ‚îÇ Added  ‚îÇ Available as: get_proxy_websocket_history       ‚îÇ
‚îÇ get_proxy_websocket_history_regex ‚îÇ Added  ‚îÇ Available as: get_proxy_websocket_history_regex ‚îÇ
‚îÇ set_task_execution_engine_state   ‚îÇ Added  ‚îÇ Available as: set_task_execution_engine_state   ‚îÇ
‚îÇ set_proxy_intercept_state         ‚îÇ Added  ‚îÇ Available as: set_proxy_intercept_state         ‚îÇ
‚îÇ get_active_editor_contents        ‚îÇ Added  ‚îÇ Available as: get_active_editor_contents        ‚îÇ
‚îÇ set_active_editor_contents        ‚îÇ Added  ‚îÇ Available as: set_active_editor_contents        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
Added 20 tools from server 'burp' to agent 'Red Team Agent'.
CAI&amp;gt;/agent 13
CAI&amp;gt;Create a repeater tab
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can list all active MCP connections and their transport types:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/mcp list
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/386a1fd3-3469-4f84-9396-2a5236febe1f"&gt;https://github.com/user-attachments/assets/386a1fd3-3469-4f84-9396-2a5236febe1f&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;Development is facilitated via VS Code dev. environments. To try out our development environment, clone the repository, open VS Code and enter de dev. container mode:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/media/cai_devenv.gif" alt="CAI Development Environment" /&gt;&lt;/p&gt; 
&lt;h3&gt;Contributions&lt;/h3&gt; 
&lt;p&gt;If you want to contribute to this project, use &lt;a href="https://pre-commit.com/"&gt;&lt;strong&gt;Pre-commit&lt;/strong&gt;&lt;/a&gt; before your MR&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install pre-commit
pre-commit # files staged
pre-commit run --all-files # all files
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Optional Requirements: caiextensions&lt;/h3&gt; 
&lt;p&gt;Currently, the extensions are not publicly available as the engineering endeavour to maintain them is significant. Instead, we're making selected custom caiextensions available for partner companies across collaborations.&lt;/p&gt; 
&lt;h3&gt;&lt;span&gt;‚Ñπ&lt;/span&gt; Usage Data Collection&lt;/h3&gt; 
&lt;p&gt;CAI is provided free of charge for researchers. To improve CAI‚Äôs detection accuracy and publish open security research, instead of payment for research use cases, we ask you to contribute to the CAI community by allowing usage data collection. This data helps us identify areas for improvement, understand how the framework is being used, and prioritize new features. Legal basis of data collection is under Art. 6 (1)(f) GDPR ‚Äî CAI‚Äôs legitimate interest in maintaining and improving security tooling, with Art. 89 safeguards for research. The collected data includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Basic system information (OS type, Python version)&lt;/li&gt; 
 &lt;li&gt;Username and IP information&lt;/li&gt; 
 &lt;li&gt;Tool usage patterns and performance metrics&lt;/li&gt; 
 &lt;li&gt;Model interactions and token usage statistics&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We take your privacy seriously and only collect what's needed to make CAI better. For further info, reach out to researchÔº†aliasrobotics.com. You can disable some of the data collection features via the &lt;code&gt;CAI_TELEMETRY&lt;/code&gt; environment variable but we encourage you to keep it enabled and contribute back to research:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI_TELEMETRY=False cai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Reproduce CI-Setup locally&lt;/h3&gt; 
&lt;p&gt;To simulate the CI/CD pipeline, you can run the following in the Gitlab runner machines:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --rm -it \
  --privileged \
  --network=exploitflow_net \
  --add-host="host.docker.internal:host-gateway" \
  -v /cache:/cache \
  -v /var/run/docker.sock:/var/run/docker.sock:rw \
  registry.gitlab.com/aliasrobotics/alias_research/cai:latest bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;details&gt;
 &lt;summary&gt;OLLAMA is giving me 404 errors&lt;/summary&gt; 
 &lt;p&gt;Ollama's API in OpenAI mode uses &lt;code&gt;/v1/chat/completions&lt;/code&gt; whereas the &lt;code&gt;openai&lt;/code&gt; library uses &lt;code&gt;base_url&lt;/code&gt; + &lt;code&gt;/chat/completions&lt;/code&gt;.&lt;/p&gt; 
 &lt;p&gt;We adopt the latter for overall alignment with the gen AI community and empower the former by allowing users to add the &lt;code&gt;v1&lt;/code&gt; themselves via:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;OLLAMA_API_BASE=http://IP:PORT/v1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;See the following issues that treat this topic in more detail:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/issues/76"&gt;https://github.com/aliasrobotics/cai/issues/76&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/issues/83"&gt;https://github.com/aliasrobotics/cai/issues/83&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/issues/82"&gt;https://github.com/aliasrobotics/cai/issues/82&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Where are all the caiextensions?&lt;/summary&gt; 
 &lt;p&gt;See &lt;a href="https://gitlab.com/aliasrobotics/alias_research/caiextensions"&gt;all caiextensions&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;How do I install the report caiextension?&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#optional-requirements-caiextensions"&gt;See here&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;How do I set up SSH access for Gitlab?&lt;/summary&gt; 
 &lt;p&gt;Generate a new SSH key&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ssh-keygen -t ed25519
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Add the key to the SSH agent&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ssh-add ~/.ssh/id_ed25519
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Add the public key to Gitlab Copy the key and add it to Gitlab under &lt;a href="https://gitlab.com/-/user_settings/ssh_keys"&gt;https://gitlab.com/-/user_settings/ssh_keys&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;cat ~/.ssh/id_ed25519.pub
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To verify it:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ssh -T git@gitlab.com
Welcome to GitLab, @vmayoral!
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;How do I clear Python cache?&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;find . -name "*.pyc" -delete &amp;amp;&amp;amp; find . -name "__pycache__" -delete
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;If host networking is not working with ollama check whether it has been disabled in Docker because you are not signed in&lt;/summary&gt; 
 &lt;p&gt;Docker in OS X behaves funny sometimes. Check if the following message has shown up:&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Host networking has been disabled because you are not signed in. Please sign in to enable it&lt;/em&gt;.&lt;/p&gt; 
 &lt;p&gt;Make sure this has been addressed and also that the Dev Container is not forwarding the 8000 port (click on x, if necessary in the ports section).&lt;/p&gt; 
 &lt;p&gt;To verify connection, from within the VSCode devcontainer:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;curl -v http://host.docker.internal:8000/api/version
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Run CAI against any target&lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-004-first-message.png" alt="cai-004-first-message" /&gt;&lt;/p&gt; 
 &lt;p&gt;The starting user prompt in this case is: &lt;code&gt;Target IP: 192.168.3.10, perform a full network scan&lt;/code&gt;.&lt;/p&gt; 
 &lt;p&gt;The agent started performing a nmap scan. You could either interact with the agent and give it more instructions, or let it run to see what it explores next.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How do I interact with the agent? Type twice CTRL + C &lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-005-ctrl-c.png" alt="cai-005-ctrl-c" /&gt;&lt;/p&gt; 
 &lt;p&gt;If you want to use the HITL mode, you can do it by presssing twice &lt;code&gt;Ctrl + C&lt;/code&gt;. This will allow you to interact (prompt) with the agent whenever you want. The agent will not lose the previous context, as it is stored in the &lt;code&gt;history&lt;/code&gt; variable, which is passed to it and any agent that is called. This enables any agent to use the previous information and be more accurate and efficient.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; Can I change the model while CAI is running? /model &lt;/summary&gt; 
 &lt;p&gt;Use &lt;code&gt;/model&lt;/code&gt; to change the model.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-007-model-change.png" alt="cai-007-model-change" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How can I list all the agents available? /agent &lt;/summary&gt; 
 &lt;p&gt;Use &lt;code&gt;/agent&lt;/code&gt; to list all the agents available.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-010-agents-menu.png" alt="cai-010-agents-menu" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; Where can I list all the environment variables? /config &lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-008-config.png" alt="cai-008-config" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; How can I monitor context usage and token consumption? /context or /ctx üöÄ CAI PRO &lt;/summary&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;‚ö° CAI PRO Exclusive Feature&lt;/strong&gt; The &lt;code&gt;/context&lt;/code&gt; command is available exclusively in &lt;strong&gt;&lt;a href="https://aliasrobotics.com/cybersecurityai.php"&gt;CAI PRO&lt;/a&gt;&lt;/strong&gt;. To access this feature and unlock advanced monitoring capabilities, visit &lt;a href="https://aliasrobotics.com/cybersecurityai.php"&gt;Alias Robotics&lt;/a&gt; for more information.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;p&gt;Use &lt;code&gt;/context&lt;/code&gt; (or the short form &lt;code&gt;/ctx&lt;/code&gt;) to view your current context window usage and token statistics.&lt;/p&gt; 
 &lt;p&gt;This command displays:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Total context usage (used tokens / max tokens) with percentage&lt;/li&gt; 
  &lt;li&gt;Visual grid representation with the CAI logo showing filled context&lt;/li&gt; 
  &lt;li&gt;Detailed breakdown by category: 
   &lt;ul&gt; 
    &lt;li&gt;System prompt tokens&lt;/li&gt; 
    &lt;li&gt;Tool definitions tokens&lt;/li&gt; 
    &lt;li&gt;Memory/RAG tokens&lt;/li&gt; 
    &lt;li&gt;User prompts tokens&lt;/li&gt; 
    &lt;li&gt;Assistant responses tokens&lt;/li&gt; 
    &lt;li&gt;Tool calls tokens&lt;/li&gt; 
    &lt;li&gt;Tool results tokens&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Free space available&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Why this matters&lt;/strong&gt;: Different models have different context limits (e.g., GPT-4: 128k tokens, Claude: 200k tokens). Monitoring your context usage helps you avoid hitting these limits during long conversations, which could cause errors or require conversation truncation.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Show context usage
/context

# Or use the short form
/ctx
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; How to know more about the CLI? /help &lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-006-help.png" alt="cai-006-help" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How can I trace the whole execution?&lt;/summary&gt; The environment variable `CAI_TRACING` allows the user to set it to `CAI_TRACING=true` to enable tracing, or `CAI_TRACING=false` to disable it. When CAI is prompted by the first time, the user is provided with two paths, the execution log, and the tracing log. 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-009-logs.png" alt="cai-009-logs" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can I expand CAI capabilities using previous run logs?&lt;/summary&gt; 
 &lt;p&gt;Yes. Today CAI performs best by relying on In‚ÄëContext Learning (ICL). Rather than building long‚Äëterm stores, the recommended workflow is to load relevant prior logs directly into the current session so the model can reason with them in context.&lt;/p&gt; 
 &lt;p&gt;Use the &lt;code&gt;/load&lt;/code&gt; command to bring JSONL logs into CAI‚Äôs context (this replaces the legacy memory-loading tool):&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/load logs/cai_20250408_111856.jsonl         # Load into current agent
CAI&amp;gt;/load &amp;lt;file&amp;gt; agent &amp;lt;name&amp;gt;                    # Load into a specific agent
CAI&amp;gt;/load &amp;lt;file&amp;gt; all                             # Distribute across all agents
CAI&amp;gt;/load &amp;lt;file&amp;gt; parallel                        # Match to configured parallel agents
# Tip: if you omit &amp;lt;file&amp;gt;, /load uses `logs/last`. Alias: /l
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;CAI prints the path to the current run‚Äôs JSONL log at startup (highlighted in orange), which you can pass to &lt;code&gt;/load&lt;/code&gt;:&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-009-logs.png" alt="cai-009-logs" /&gt;&lt;/p&gt; 
 &lt;p&gt;Legacy notes: earlier ‚Äúmemory extension‚Äù mechanisms (episodic/semantic stores and offline ingestion) are retained for reference only. See &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/src/cai/agents/memory.py"&gt;src/cai/agents/memory.py&lt;/a&gt; for background and legacy details. Our current direction prioritizes ICL over persistent memory.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can I expand CAI capabilities using scripts or extra information?&lt;/summary&gt; 
 &lt;p&gt;Currently, CAI supports text based information. You can add any extra information on the target you are facing by copy-pasting it directly into the system or user prompt.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;How?&lt;/strong&gt; By adding it to the system (&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/repl/templates/system_master_template.md"&gt;&lt;code&gt;system_master_template.md&lt;/code&gt;&lt;/a&gt;) or the user prompt (&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/repl/templates/user_master_template.md"&gt;&lt;code&gt;user_master_template.md&lt;/code&gt;&lt;/a&gt;). You can always directly prompt the path to the model, and it will &lt;code&gt;cat&lt;/code&gt; it.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;How CAI licence works?&lt;/summary&gt; 
 &lt;p&gt;CAI‚Äôs current license does not restrict usage for research purposes. You are free to use CAI for security assessments (pentests), to develop additional features, and to integrate it into your research activities, as long as you comply with local laws.&lt;/p&gt; 
 &lt;p&gt;If you or your organization start benefiting commercially from CAI (e.g., offering pentesting services powered by CAI), then a commercial license will be required to help sustain the project.&lt;/p&gt; 
 &lt;p&gt;CAI itself is not a profit-seeking initiative. Our goal is to build a sustainable open-source project. We simply ask that those who profit from CAI contribute back and support our ongoing development.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;I get a `Unable to locate package python3.12-venv` when installing the prerequisites on my debian based system!&lt;/summary&gt; 
 &lt;p&gt;The easiest way to get around this is to simply install &lt;a href="https://www.python.org/downloads/release/python-3120/"&gt;&lt;code&gt;python3.12&lt;/code&gt;&lt;/a&gt; from source.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you want to cite our work, please use the following (ordered by publication date):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{mayoral2025cai,
  title={CAI: An Open, Bug Bounty-Ready Cybersecurity AI},
  author={Mayoral-Vilches, V{\'\i}ctor and Navarrete-Lozano, Luis Javier and Sanz-G{\'o}mez, Mar{\'\i}a and Espejo, Lidia Salas and Crespo-{\'A}lvarez, Marti{\~n}o and Oca-Gonzalez, Francisco and Balassone, Francesco and Glera-Pic{\'o}n, Alfonso and Ayucar-Carbajo, Unai and Ruiz-Alcalde, Jon Ander and Rass, Stefan and Pinzger, Martin and Gil-Uriarte, Endika},
  journal={arXiv preprint arXiv:2504.06017},
  year={2025}
}

@article{mayoral2025automation,
  title={Cybersecurity AI: The Dangerous Gap Between Automation and Autonomy},
  author={Mayoral-Vilches, V{\'\i}ctor},
  journal={arXiv preprint arXiv:2506.23592},
  year={2025}
}

@article{mayoral2025fluency,
  title={CAI Fluency: A Framework for Cybersecurity AI Fluency},
  author={Mayoral-Vilches, V{\'\i}ctor and Wachter, Jasmin and Chavez, Crist{\'o}bal RJ and Schachner, Cathrin and Navarrete-Lozano, Luis Javier and Sanz-G{\'o}mez, Mar{\'\i}a},
  journal={arXiv preprint arXiv:2508.13588},
  year={2025}
}

@article{mayoral2025hacking,
  title={Cybersecurity AI: Hacking the AI Hackers via Prompt Injection},
  author={Mayoral-Vilches, V{\'\i}ctor and Rynning, Per Mannermaa},
  journal={arXiv preprint arXiv:2508.21669},
  year={2025}
}

@article{mayoral2025humanoid,
  title={Cybersecurity AI: Humanoid Robots as Attack Vectors},
  author={Mayoral-Vilches, V{\'\i}ctor},
  journal={arXiv preprint arXiv:2509.14139},
  year={2025}
}

@article{balassone2025evaluation,
  title={Cybersecurity AI: Evaluating Agentic Cybersecurity in Attack/Defense CTFs},
  author={Balassone, Francesco and Mayoral-Vilches, V{\'\i}ctor and Rass, Stefan and Pinzger, Martin and Perrone, Gaetano and Romano, Simon Pietro and Schartner, Peter},
  journal={arXiv preprint arXiv:2510.17521},
  year={2025}
}

@article{mayoral2025caibench,
  title={CAIBench: A Meta-Benchmark for Evaluating Cybersecurity AI Agents},
  author={Mayoral-Vilches, V{\'\i}ctor and Balassone, Francesco and Navarrete-Lozano, Luis Javier and Sanz-G{\'o}mez, Mar{\'\i}a and Crespo-{\'A}lvarez, Marti{\~n}o and Rass, Stefan and Pinzger, Martin},
  journal={arXiv preprint arXiv:2510.24317},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;CAI was initially developed by &lt;a href="https://aliasrobotics.com"&gt;Alias Robotics&lt;/a&gt; and co-funded by the European EIC accelerator project RIS (GA 101161136) - HORIZON-EIC-2023-ACCELERATOR-01 call. The original agentic principles are inspired from OpenAI's &lt;a href="https://github.com/openai/swarm"&gt;&lt;code&gt;swarm&lt;/code&gt;&lt;/a&gt; library and translated into newer prototypes. This project also makes use of other relevant open source building blocks including &lt;a href="https://github.com/BerriAI/litellm"&gt;&lt;code&gt;LiteLLM&lt;/code&gt;&lt;/a&gt;, and &lt;a href="https://github.com/Arize-ai/phoenix"&gt;&lt;code&gt;phoenix&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Academic Collaborations&lt;/h3&gt; 
&lt;p&gt;CAI benefits from ongoing research collaborations with academic institutions. Researchers interested in collaborative projects, dataset access, or academic licenses should contact &lt;a href="mailto:research@aliasrobotics.com"&gt;research@aliasrobotics.com&lt;/a&gt;. We provide special support for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PhD research projects&lt;/li&gt; 
 &lt;li&gt;Academic benchmarking studies&lt;/li&gt; 
 &lt;li&gt;Security education initiatives&lt;/li&gt; 
 &lt;li&gt;Open-source contributions from research labs&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- Footnotes --&gt; 
&lt;p&gt;[^1]: Arguably, the Chain-of-Thought agentic pattern is a special case of the Hierarchical agentic pattern. [^2]: Kamhoua, C. A., Leslie, N. O., &amp;amp; Weisman, M. J. (2018). Game theoretic modeling of advanced persistent threat in internet of things. Journal of Cyber Security and Information Systems. [^3]: Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., &amp;amp; Cao, Y. (2023, January). React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR). [^4]: Deng, G., Liu, Y., Mayoral-Vilches, V., Liu, P., Li, Y., Xu, Y., ... &amp;amp; Rass, S. (2024). {PentestGPT}: Evaluating and harnessing large language models for automated penetration testing. In 33rd USENIX Security Symposium (USENIX Security 24) (pp. 847-864).&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>opengeos/segment-geospatial</title>
      <link>https://github.com/opengeos/segment-geospatial</link>
      <description>&lt;p&gt;A Python package for segmenting geospatial data with the Segment Anything Model (SAM)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;segment-geospatial&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://studiolab.sagemaker.aws/import/github/opengeos/segment-geospatial/blob/main/docs/examples/satellite.ipynb"&gt;&lt;img src="https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true" alt="image" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/opengeos/segment-geospatial/blob/main/docs/examples/satellite.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="image" /&gt;&lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/segment-geospatial"&gt;&lt;img src="https://img.shields.io/pypi/v/segment-geospatial.svg?sanitize=true" alt="image" /&gt;&lt;/a&gt; &lt;a href="https://anaconda.org/conda-forge/segment-geospatial"&gt;&lt;img src="https://img.shields.io/conda/vn/conda-forge/segment-geospatial.svg?sanitize=true" alt="image" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/giswqs/segment-geospatial"&gt;&lt;img src="https://badgen.net/docker/pulls/giswqs/segment-geospatial?icon=docker&amp;amp;label=pulls" alt="Docker Pulls" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/segment-geospatial"&gt;&lt;img src="https://static.pepy.tech/badge/segment-geospatial" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/conda-forge/segment-geospatial-feedstock"&gt;&lt;img src="https://img.shields.io/badge/recipe-segment--geospatial-green.svg?sanitize=true" alt="Conda Recipe" /&gt;&lt;/a&gt; &lt;a href="https://anaconda.org/conda-forge/segment-geospatial"&gt;&lt;img src="https://anaconda.org/conda-forge/segment-geospatial/badges/downloads.svg?sanitize=true" alt="Conda Downloads" /&gt;&lt;/a&gt; &lt;a href="https://doi.org/10.21105/joss.05663"&gt;&lt;img src="https://joss.theoj.org/papers/10.21105/joss.05663/status.svg?sanitize=true" alt="DOI" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/opengeos/segment-geospatial/raw/main/docs/assets/logo.png"&gt;&lt;img src="https://raw.githubusercontent.com/opengeos/segment-geospatial/main/docs/assets/logo_rect.png" alt="logo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;A Python package for segmenting geospatial data with the Segment Anything Model (SAM)&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;The &lt;strong&gt;segment-geospatial&lt;/strong&gt; package draws its inspiration from &lt;a href="https://github.com/aliaksandr960/segment-anything-eo"&gt;segment-anything-eo&lt;/a&gt; repository authored by &lt;a href="https://github.com/aliaksandr960"&gt;Aliaksandr Hancharenka&lt;/a&gt;. To facilitate the use of the Segment Anything Model (SAM) for geospatial data, I have developed the &lt;a href="https://github.com/opengeos/segment-anything"&gt;segment-anything-py&lt;/a&gt; and &lt;a href="https://github.com/opengeos/segment-geospatial"&gt;segment-geospatial&lt;/a&gt; Python packages, which are now available on PyPI and conda-forge. My primary objective is to simplify the process of leveraging SAM for geospatial data analysis by enabling users to achieve this with minimal coding effort. I have adapted the source code of segment-geospatial from the &lt;a href="https://github.com/aliaksandr960/segment-anything-eo"&gt;segment-anything-eo&lt;/a&gt; repository, and credit for its original version goes to Aliaksandr Hancharenka.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Free software: MIT license&lt;/li&gt; 
 &lt;li&gt;Documentation: &lt;a href="https://samgeo.gishub.org"&gt;https://samgeo.gishub.org&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citations&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Wu, Q., &amp;amp; Osco, L. (2023). samgeo: A Python package for segmenting geospatial data with the Segment Anything Model (SAM). &lt;em&gt;Journal of Open Source Software&lt;/em&gt;, 8(89), 5663. &lt;a href="https://doi.org/10.21105/joss.05663"&gt;https://doi.org/10.21105/joss.05663&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Osco, L. P., Wu, Q., de Lemos, E. L., Gon√ßalves, W. N., Ramos, A. P. M., Li, J., &amp;amp; Junior, J. M. (2023). The Segment Anything Model (SAM) for remote sensing applications: From zero to one shot. &lt;em&gt;International Journal of Applied Earth Observation and Geoinformation&lt;/em&gt;, 124, 103540. &lt;a href="https://doi.org/10.1016/j.jag.2023.103540"&gt;https://doi.org/10.1016/j.jag.2023.103540&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Download map tiles from Tile Map Service (TMS) servers and create GeoTIFF files&lt;/li&gt; 
 &lt;li&gt;Segment GeoTIFF files using the Segment Anything Model (&lt;a href="https://github.com/facebookresearch/segment-anything"&gt;SAM&lt;/a&gt;) and &lt;a href="https://github.com/SysCV/sam-hq"&gt;HQ-SAM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Segment remote sensing imagery with text prompts&lt;/li&gt; 
 &lt;li&gt;Create foreground and background markers interactively&lt;/li&gt; 
 &lt;li&gt;Load existing markers from vector datasets&lt;/li&gt; 
 &lt;li&gt;Save segmentation results as common vector formats (GeoPackage, Shapefile, GeoJSON)&lt;/li&gt; 
 &lt;li&gt;Save input prompts as GeoJSON files&lt;/li&gt; 
 &lt;li&gt;Visualize segmentation results on interactive maps&lt;/li&gt; 
 &lt;li&gt;Segment objects from timeseries remote sensing imagery&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Install from PyPI&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;segment-geospatial&lt;/strong&gt; is available on &lt;a href="https://pypi.org/project/segment-geospatial/"&gt;PyPI&lt;/a&gt; and can be installed in several ways so that its dependencies can be controlled more granularly. This reduces package size for CI environments, since not every time all of the models will be used.&lt;/p&gt; 
&lt;p&gt;Depending on what tools you need to use, you might want to do:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;segment-geospatial&lt;/code&gt; or &lt;code&gt;segment-geospatial[samgeo]&lt;/code&gt;: Installs only the minimum required dependencies to run SAMGeo&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;segment-geospatial[samgeo2]&lt;/code&gt;: Installs the dependencies to run SAMGeo 2&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;segment-geospatial[fast]&lt;/code&gt;: Installs the dependencies to run Fast SAM&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;segment-geospatial[hq]&lt;/code&gt;: Installs the dependencies to run HQ-SAM&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;segment-geospatial[text]&lt;/code&gt;: Installs Grounding DINO to use SAMGeo 1 and 2 with text prompts&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;segment-geospatial[fer]&lt;/code&gt;: Installs the dependencies to run the feature edge reconstruction algorithm&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Additionally, these other two optional imports are defined:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;segment-geospatial[all]&lt;/code&gt;: Installs the dependencies to run all of the SAMGeo models&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;segment-geospatial[extra]&lt;/code&gt;: Installs the dependencies to run all of the SAMGeo models and other utilities to run the examples like Jupyter notebook support, &lt;code&gt;leafmap&lt;/code&gt;, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Simply running the following should install the dependencies for each use case:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install segment-geospatial[samgeo2] # Or any other choice of the above
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To see more in detail what packages come with each choice, please refer to &lt;code&gt;pyproject.toml&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Install from conda-forge&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;segment-geospatial&lt;/strong&gt; is also available on &lt;a href="https://anaconda.org/conda-forge/segment-geospatial"&gt;conda-forge&lt;/a&gt;. If you have &lt;a href="https://www.anaconda.com/distribution/#download-section"&gt;Anaconda&lt;/a&gt; or &lt;a href="https://docs.conda.io/en/latest/miniconda.html"&gt;Miniconda&lt;/a&gt; installed on your computer, you can install segment-geospatial using the following commands. It is recommended to create a fresh conda environment for &lt;strong&gt;segment-geospatial&lt;/strong&gt;. The following commands will create a new conda environment named &lt;code&gt;geo&lt;/code&gt; and install &lt;strong&gt;segment-geospatial&lt;/strong&gt; and its dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n geo python
conda activate geo
conda install -c conda-forge mamba
mamba install -c conda-forge segment-geospatial
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If your system has a GPU, but the above commands do not install the GPU version of pytorch, you can force the installation of the GPU version of pytorch using the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mamba install -c conda-forge segment-geospatial "pytorch=*=cuda*"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Samgeo-geospatial has some optional dependencies that are not included in the default conda environment. To install these dependencies, run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mamba install -c conda-forge groundingdino-py segment-anything-fast
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://samgeo.gishub.org/examples/satellite"&gt;Segmenting remote sensing imagery&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://samgeo.gishub.org/examples/automatic_mask_generator"&gt;Automatically generating object masks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://samgeo.gishub.org/examples/input_prompts"&gt;Segmenting remote sensing imagery with input prompts&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://samgeo.gishub.org/examples/box_prompts"&gt;Segmenting remote sensing imagery with box prompts&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://samgeo.gishub.org/examples/text_prompts"&gt;Segmenting remote sensing imagery with text prompts&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://samgeo.gishub.org/examples/text_prompts_batch"&gt;Batch segmentation with text prompts&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://samgeo.gishub.org/examples/arcgis"&gt;Using segment-geospatial with ArcGIS Pro&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://samgeo.gishub.org/examples/swimming_pools"&gt;Segmenting swimming pools with text prompts&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://samgeo.gishub.org/examples/max_open_data"&gt;Segmenting satellite imagery from the Maxar Open Data Program&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Demos&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Automatic mask generator&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://i.imgur.com/I1IhDgz.gif" alt="" /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Interactive segmentation with input prompts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://i.imgur.com/2Nyg9uW.gif" alt="" /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Input prompts from existing files&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://i.imgur.com/Cb4ZaKY.gif" alt="" /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Interactive segmentation with text prompts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://i.imgur.com/wydt5Xt.gif" alt="" /&gt;&lt;/p&gt; 
&lt;h2&gt;Tutorials&lt;/h2&gt; 
&lt;p&gt;Video tutorials are available on my &lt;a href="https://youtube.com/@giswqs"&gt;YouTube Channel&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Automatic mask generation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/playlist?list=PLAxJ4-o7ZoPcrg5RnZjkB_KY6tv96WO2h"&gt;&lt;img src="https://img.youtube.com/vi/YHA_-QMB8_U/0.jpg" alt="Alt text" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Using SAM with ArcGIS Pro&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/playlist?list=PLAxJ4-o7ZoPcrg5RnZjkB_KY6tv96WO2h"&gt;&lt;img src="https://img.youtube.com/vi/VvyInoQ6N8Q/0.jpg" alt="Alt text" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Interactive segmentation with text prompts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/playlist?list=PLAxJ4-o7ZoPcrg5RnZjkB_KY6tv96WO2h"&gt;&lt;img src="https://img.youtube.com/vi/cSDvuv1zRos/0.jpg" alt="Alt text" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Using SAM with Desktop GIS&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;QGIS&lt;/strong&gt;: Check out the &lt;a href="https://github.com/BjornNyberg/Geometric-Attributes-Toolbox/wiki/User-Guide#segment-anything-model"&gt;Geometric Attributes plugin for QGIS&lt;/a&gt;. Credit goes to &lt;a href="https://github.com/BjornNyberg"&gt;Bjorn Nyberg&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ArcGIS&lt;/strong&gt;: Check out the &lt;a href="https://www.arcgis.com/home/item.html?id=9b67b441f29f4ce6810979f5f0667ebe"&gt;Segment Anything Model (SAM) Toolbox for ArcGIS&lt;/a&gt; and the &lt;a href="https://community.esri.com/t5/education-blog/resources-for-unlocking-the-power-of-deep-learning/ba-p/1293098"&gt;Resources for Unlocking the Power of Deep Learning Applications Using ArcGIS&lt;/a&gt;. Credit goes to &lt;a href="https://www.esri.com"&gt;Esri&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Computing Resources&lt;/h2&gt; 
&lt;p&gt;The Segment Anything Model is computationally intensive, and a powerful GPU is recommended to process large datasets. It is recommended to have a GPU with at least 8 GB of GPU memory. You can utilize the free GPU resources provided by Google Colab. Alternatively, you can apply for &lt;a href="https://aws.amazon.com/government-education/research-and-technical-computing/cloud-credit-for-research"&gt;AWS Cloud Credit for Research&lt;/a&gt;, which offers cloud credits to support academic research. If you are in the Greater China region, apply for the AWS Cloud Credit &lt;a href="https://aws.amazon.com/cn/events/educate_cloud/research-credits"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Legal Notice&lt;/h2&gt; 
&lt;p&gt;This repository and its content are provided for educational purposes only. By using the information and code provided, users acknowledge that they are using the APIs and models at their own risk and agree to comply with any applicable laws and regulations. Users who intend to download a large number of image tiles from any basemap are advised to contact the basemap provider to obtain permission before doing so. Unauthorized use of the basemap or any of its components may be a violation of copyright laws or other applicable laws and regulations.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Please refer to the &lt;a href="https://samgeo.gishub.org/contributing"&gt;contributing guidelines&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;This project is based upon work partially supported by the National Aeronautics and Space Administration (NASA) under Grant No. 80NSSC22K1742 issued through the &lt;a href="https://bit.ly/3RVBRcQ"&gt;Open Source Tools, Frameworks, and Libraries 2020 Program&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This project is also supported by Amazon Web Services (&lt;a href="https://aws.amazon.com/"&gt;AWS&lt;/a&gt;). In addition, this package was made possible by the following open source projects. Credit goes to the developers of these projects.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/facebookresearch/segment-anything"&gt;segment-anything&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliaksandr960/segment-anything-eo"&gt;segment-anything-eo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/gumblex/tms2geotiff"&gt;tms2geotiff&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/IDEA-Research/GroundingDINO"&gt;GroundingDINO&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/luca-medeiros/lang-segment-anything"&gt;lang-segment-anything&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>