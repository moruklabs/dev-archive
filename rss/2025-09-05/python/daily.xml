<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Thu, 04 Sep 2025 01:36:32 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>lllyasviel/Fooocus</title>
      <link>https://github.com/lllyasviel/Fooocus</link>
      <description>&lt;p&gt;Focus on prompting and generating&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://github.com/lllyasviel/Fooocus/assets/19834515/483fb86d-c9a2-4c20-997c-46dafc124f25" /&gt; 
&lt;/div&gt; 
&lt;h1&gt;Fooocus&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/lllyasviel/Fooocus/main/#download"&gt;&amp;gt;&amp;gt;&amp;gt; Click Here to Install Fooocus &amp;lt;&amp;lt;&amp;lt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Fooocus is an image generating software (based on &lt;a href="https://www.gradio.app/"&gt;Gradio&lt;/a&gt; &lt;a href="https://github.com/gradio-app/gradio"&gt;&lt;img src="https://img.shields.io/github/stars/gradio-app/gradio" /&gt;&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;Fooocus presents a rethinking of image generator designs. The software is offline, open source, and free, while at the same time, similar to many online image generators like Midjourney, the manual tweaking is not needed, and users only need to focus on the prompts and images. Fooocus has also simplified the installation: between pressing "download" and generating the first image, the number of needed mouse clicks is strictly limited to less than 3. Minimal GPU memory requirement is 4GB (Nvidia).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Recently many fake websites exist on Google when you search ‚Äúfooocus‚Äù. Do not trust those ‚Äì here is the only official source of Fooocus.&lt;/strong&gt;&lt;/p&gt; 
&lt;h1&gt;Project Status: Limited Long-Term Support (LTS) with Bug Fixes Only&lt;/h1&gt; 
&lt;p&gt;The Fooocus project, built entirely on the &lt;strong&gt;Stable Diffusion XL&lt;/strong&gt; architecture, is now in a state of limited long-term support (LTS) with bug fixes only. As the existing functionalities are considered as nearly free of programmartic issues (Thanks to &lt;a href="https://github.com/mashb1t"&gt;mashb1t&lt;/a&gt;'s huge efforts), future updates will focus exclusively on addressing any bugs that may arise.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;There are no current plans to migrate to or incorporate newer model architectures.&lt;/strong&gt; However, this may change during time with the development of open-source community. For example, if the community converge to one single dominant method for image generation (which may really happen in half or one years given the current status), Fooocus may also migrate to that exact method.&lt;/p&gt; 
&lt;p&gt;For those interested in utilizing newer models such as &lt;strong&gt;Flux&lt;/strong&gt;, we recommend exploring alternative platforms such as &lt;a href="https://github.com/lllyasviel/stable-diffusion-webui-forge"&gt;WebUI Forge&lt;/a&gt; (also from us), &lt;a href="https://github.com/comfyanonymous/ComfyUI"&gt;ComfyUI/SwarmUI&lt;/a&gt;. Additionally, several &lt;a href="https://github.com/lllyasviel/Fooocus?tab=readme-ov-file#forks"&gt;excellent forks of Fooocus&lt;/a&gt; are available for experimentation.&lt;/p&gt; 
&lt;p&gt;Again, recently many fake websites exist on Google when you search ‚Äúfooocus‚Äù. Do &lt;strong&gt;NOT&lt;/strong&gt; get Fooocus from those websites ‚Äì this page is the only official source of Fooocus. We never have any website like such as ‚Äúfooocus.com‚Äù, ‚Äúfooocus.net‚Äù, ‚Äúfooocus.co‚Äù, ‚Äúfooocus.ai‚Äù, ‚Äúfooocus.org‚Äù, ‚Äúfooocus.pro‚Äù, ‚Äúfooocus.one‚Äù. Those websites are ALL FAKE. &lt;strong&gt;They have ABSOLUTLY no relationship to us. Fooocus is a 100% non-commercial offline open-source software.&lt;/strong&gt;&lt;/p&gt; 
&lt;h1&gt;Features&lt;/h1&gt; 
&lt;p&gt;Below is a quick list using Midjourney's examples:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Midjourney&lt;/th&gt; 
   &lt;th&gt;Fooocus&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;High-quality text-to-image without needing much prompt engineering or parameter tuning. &lt;br /&gt; (Unknown method)&lt;/td&gt; 
   &lt;td&gt;High-quality text-to-image without needing much prompt engineering or parameter tuning. &lt;br /&gt; (Fooocus has an offline GPT-2 based prompt processing engine and lots of sampling improvements so that results are always beautiful, no matter if your prompt is as short as ‚Äúhouse in garden‚Äù or as long as 1000 words)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;V1 V2 V3 V4&lt;/td&gt; 
   &lt;td&gt;Input Image -&amp;gt; Upscale or Variation -&amp;gt; Vary (Subtle) / Vary (Strong)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;U1 U2 U3 U4&lt;/td&gt; 
   &lt;td&gt;Input Image -&amp;gt; Upscale or Variation -&amp;gt; Upscale (1.5x) / Upscale (2x)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Inpaint / Up / Down / Left / Right (Pan)&lt;/td&gt; 
   &lt;td&gt;Input Image -&amp;gt; Inpaint or Outpaint -&amp;gt; Inpaint / Up / Down / Left / Right &lt;br /&gt; (Fooocus uses its own inpaint algorithm and inpaint models so that results are more satisfying than all other software that uses standard SDXL inpaint method/model)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Image Prompt&lt;/td&gt; 
   &lt;td&gt;Input Image -&amp;gt; Image Prompt &lt;br /&gt; (Fooocus uses its own image prompt algorithm so that result quality and prompt understanding are more satisfying than all other software that uses standard SDXL methods like standard IP-Adapters or Revisions)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;--style&lt;/td&gt; 
   &lt;td&gt;Advanced -&amp;gt; Style&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;--stylize&lt;/td&gt; 
   &lt;td&gt;Advanced -&amp;gt; Advanced -&amp;gt; Guidance&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;--niji&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/lllyasviel/Fooocus/discussions/679"&gt;Multiple launchers: "run.bat", "run_anime.bat", and "run_realistic.bat".&lt;/a&gt; &lt;br /&gt; Fooocus support SDXL models on Civitai &lt;br /&gt; (You can google search ‚ÄúCivitai‚Äù if you do not know about it)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;--quality&lt;/td&gt; 
   &lt;td&gt;Advanced -&amp;gt; Quality&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;--repeat&lt;/td&gt; 
   &lt;td&gt;Advanced -&amp;gt; Image Number&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Multi Prompts (::)&lt;/td&gt; 
   &lt;td&gt;Just use multiple lines of prompts&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Prompt Weights&lt;/td&gt; 
   &lt;td&gt;You can use " I am (happy:1.5)". &lt;br /&gt; Fooocus uses A1111's reweighting algorithm so that results are better than ComfyUI if users directly copy prompts from Civitai. (Because if prompts are written in ComfyUI's reweighting, users are less likely to copy prompt texts as they prefer dragging files) &lt;br /&gt; To use embedding, you can use "(embedding:file_name:1.1)"&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;--no&lt;/td&gt; 
   &lt;td&gt;Advanced -&amp;gt; Negative Prompt&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;--ar&lt;/td&gt; 
   &lt;td&gt;Advanced -&amp;gt; Aspect Ratios&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;InsightFace&lt;/td&gt; 
   &lt;td&gt;Input Image -&amp;gt; Image Prompt -&amp;gt; Advanced -&amp;gt; FaceSwap&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Describe&lt;/td&gt; 
   &lt;td&gt;Input Image -&amp;gt; Describe&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Below is a quick list using LeonardoAI's examples:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;LeonardoAI&lt;/th&gt; 
   &lt;th&gt;Fooocus&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Prompt Magic&lt;/td&gt; 
   &lt;td&gt;Advanced -&amp;gt; Style -&amp;gt; Fooocus V2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Advanced Sampler Parameters (like Contrast/Sharpness/etc)&lt;/td&gt; 
   &lt;td&gt;Advanced -&amp;gt; Advanced -&amp;gt; Sampling Sharpness / etc&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;User-friendly ControlNets&lt;/td&gt; 
   &lt;td&gt;Input Image -&amp;gt; Image Prompt -&amp;gt; Advanced&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Also, &lt;a href="https://github.com/lllyasviel/Fooocus/discussions/117"&gt;click here to browse the advanced features.&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Download&lt;/h1&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;p&gt;You can directly download Fooocus with:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/lllyasviel/Fooocus/releases/download/v2.5.0/Fooocus_win64_2-5-0.7z"&gt;&amp;gt;&amp;gt;&amp;gt; Click here to download &amp;lt;&amp;lt;&amp;lt;&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;After you download the file, please uncompress it and then run the "run.bat".&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/lllyasviel/Fooocus/assets/19834515/c49269c4-c274-4893-b368-047c401cc58c" alt="image" /&gt;&lt;/p&gt; 
&lt;p&gt;The first time you launch the software, it will automatically download models:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;It will download &lt;a href="https://raw.githubusercontent.com/lllyasviel/Fooocus/main/#models"&gt;default models&lt;/a&gt; to the folder "Fooocus\models\checkpoints" given different presets. You can download them in advance if you do not want automatic download.&lt;/li&gt; 
 &lt;li&gt;Note that if you use inpaint, at the first time you inpaint an image, it will download &lt;a href="https://huggingface.co/lllyasviel/fooocus_inpaint/resolve/main/inpaint_v26.fooocus.patch"&gt;Fooocus's own inpaint control model from here&lt;/a&gt; as the file "Fooocus\models\inpaint\inpaint_v26.fooocus.patch" (the size of this file is 1.28GB).&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;After Fooocus 2.1.60, you will also have &lt;code&gt;run_anime.bat&lt;/code&gt; and &lt;code&gt;run_realistic.bat&lt;/code&gt;. They are different model presets (and require different models, but they will be automatically downloaded). &lt;a href="https://github.com/lllyasviel/Fooocus/discussions/679"&gt;Check here for more details&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;After Fooocus 2.3.0 you can also switch presets directly in the browser. Keep in mind to add these arguments if you want to change the default behavior:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use &lt;code&gt;--disable-preset-selection&lt;/code&gt; to disable preset selection in the browser.&lt;/li&gt; 
 &lt;li&gt;Use &lt;code&gt;--always-download-new-model&lt;/code&gt; to download missing models on preset switch. Default is fallback to &lt;code&gt;previous_default_models&lt;/code&gt; defined in the corresponding preset, also see terminal output.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://github.com/lllyasviel/Fooocus/assets/19834515/d386f817-4bd7-490c-ad89-c1e228c23447" alt="image" /&gt;&lt;/p&gt; 
&lt;p&gt;If you already have these files, you can copy them to the above locations to speed up installation.&lt;/p&gt; 
&lt;p&gt;Note that if you see &lt;strong&gt;"MetadataIncompleteBuffer" or "PytorchStreamReader"&lt;/strong&gt;, then your model files are corrupted. Please download models again.&lt;/p&gt; 
&lt;p&gt;Below is a test on a relatively low-end laptop with &lt;strong&gt;16GB System RAM&lt;/strong&gt; and &lt;strong&gt;6GB VRAM&lt;/strong&gt; (Nvidia 3060 laptop). The speed on this machine is about 1.35 seconds per iteration. Pretty impressive ‚Äì nowadays laptops with 3060 are usually at very acceptable price.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/lllyasviel/Fooocus/assets/19834515/938737a5-b105-4f19-b051-81356cb7c495" alt="image" /&gt;&lt;/p&gt; 
&lt;p&gt;Besides, recently many other software report that Nvidia driver above 532 is sometimes 10x slower than Nvidia driver 531. If your generation time is very long, consider download &lt;a href="https://www.nvidia.com/download/driverResults.aspx/199991/en-us/"&gt;Nvidia Driver 531 Laptop&lt;/a&gt; or &lt;a href="https://www.nvidia.com/download/driverResults.aspx/199990/en-us/"&gt;Nvidia Driver 531 Desktop&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Note that the minimal requirement is &lt;strong&gt;4GB Nvidia GPU memory (4GB VRAM)&lt;/strong&gt; and &lt;strong&gt;8GB system memory (8GB RAM)&lt;/strong&gt;. This requires using Microsoft‚Äôs Virtual Swap technique, which is automatically enabled by your Windows installation in most cases, so you often do not need to do anything about it. However, if you are not sure, or if you manually turned it off (would anyone really do that?), or &lt;strong&gt;if you see any "RuntimeError: CPUAllocator"&lt;/strong&gt;, you can enable it here:&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click here to see the image instructions. &lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://github.com/lllyasviel/Fooocus/assets/19834515/2a06b130-fe9b-4504-94f1-2763be4476e9" alt="image" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;And make sure that you have at least 40GB free space on each drive if you still see "RuntimeError: CPUAllocator" !&lt;/strong&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;p&gt;Please open an issue if you use similar devices but still cannot achieve acceptable performances.&lt;/p&gt; 
&lt;p&gt;Note that the &lt;a href="https://raw.githubusercontent.com/lllyasviel/Fooocus/main/#minimal-requirement"&gt;minimal requirement&lt;/a&gt; for different platforms is different.&lt;/p&gt; 
&lt;p&gt;See also the common problems and troubleshoots &lt;a href="https://raw.githubusercontent.com/lllyasviel/Fooocus/main/troubleshoot.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Colab&lt;/h3&gt; 
&lt;p&gt;(Last tested - 2024 Aug 12 by &lt;a href="https://github.com/mashb1t"&gt;mashb1t&lt;/a&gt;)&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Colab&lt;/th&gt; 
   &lt;th&gt;Info&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/lllyasviel/Fooocus/blob/main/fooocus_colab.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Fooocus Official&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;In Colab, you can modify the last line to &lt;code&gt;!python entry_with_update.py --share --always-high-vram&lt;/code&gt; or &lt;code&gt;!python entry_with_update.py --share --always-high-vram --preset anime&lt;/code&gt; or &lt;code&gt;!python entry_with_update.py --share --always-high-vram --preset realistic&lt;/code&gt; for Fooocus Default/Anime/Realistic Edition.&lt;/p&gt; 
&lt;p&gt;You can also change the preset in the UI. Please be aware that this may lead to timeouts after 60 seconds. If this is the case, please wait until the download has finished, change the preset to initial and back to the one you've selected or reload the page.&lt;/p&gt; 
&lt;p&gt;Note that this Colab will disable refiner by default because Colab free's resources are relatively limited (and some "big" features like image prompt may cause free-tier Colab to disconnect). We make sure that basic text-to-image is always working on free-tier Colab.&lt;/p&gt; 
&lt;p&gt;Using &lt;code&gt;--always-high-vram&lt;/code&gt; shifts resource allocation from RAM to VRAM and achieves the overall best balance between performance, flexibility and stability on the default T4 instance. Please find more information &lt;a href="https://github.com/lllyasviel/Fooocus/pull/1710#issuecomment-1989185346"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Thanks to &lt;a href="https://github.com/camenduru"&gt;camenduru&lt;/a&gt; for the template!&lt;/p&gt; 
&lt;h3&gt;Linux (Using Anaconda)&lt;/h3&gt; 
&lt;p&gt;If you want to use Anaconda/Miniconda, you can&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/lllyasviel/Fooocus.git
cd Fooocus
conda env create -f environment.yaml
conda activate fooocus
pip install -r requirements_versions.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then download the models: download &lt;a href="https://raw.githubusercontent.com/lllyasviel/Fooocus/main/#models"&gt;default models&lt;/a&gt; to the folder "Fooocus\models\checkpoints". &lt;strong&gt;Or let Fooocus automatically download the models&lt;/strong&gt; using the launcher:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;conda activate fooocus
python entry_with_update.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, if you want to open a remote port, use&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;conda activate fooocus
python entry_with_update.py --listen
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Use &lt;code&gt;python entry_with_update.py --preset anime&lt;/code&gt; or &lt;code&gt;python entry_with_update.py --preset realistic&lt;/code&gt; for Fooocus Anime/Realistic Edition.&lt;/p&gt; 
&lt;h3&gt;Linux (Using Python Venv)&lt;/h3&gt; 
&lt;p&gt;Your Linux needs to have &lt;strong&gt;Python 3.10&lt;/strong&gt; installed, and let's say your Python can be called with the command &lt;strong&gt;python3&lt;/strong&gt; with your venv system working; you can&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/lllyasviel/Fooocus.git
cd Fooocus
python3 -m venv fooocus_env
source fooocus_env/bin/activate
pip install -r requirements_versions.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the above sections for model downloads. You can launch the software with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;source fooocus_env/bin/activate
python entry_with_update.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, if you want to open a remote port, use&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;source fooocus_env/bin/activate
python entry_with_update.py --listen
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Use &lt;code&gt;python entry_with_update.py --preset anime&lt;/code&gt; or &lt;code&gt;python entry_with_update.py --preset realistic&lt;/code&gt; for Fooocus Anime/Realistic Edition.&lt;/p&gt; 
&lt;h3&gt;Linux (Using native system Python)&lt;/h3&gt; 
&lt;p&gt;If you know what you are doing, and your Linux already has &lt;strong&gt;Python 3.10&lt;/strong&gt; installed, and your Python can be called with the command &lt;strong&gt;python3&lt;/strong&gt; (and Pip with &lt;strong&gt;pip3&lt;/strong&gt;), you can&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/lllyasviel/Fooocus.git
cd Fooocus
pip3 install -r requirements_versions.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the above sections for model downloads. You can launch the software with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python3 entry_with_update.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, if you want to open a remote port, use&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python3 entry_with_update.py --listen
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Use &lt;code&gt;python entry_with_update.py --preset anime&lt;/code&gt; or &lt;code&gt;python entry_with_update.py --preset realistic&lt;/code&gt; for Fooocus Anime/Realistic Edition.&lt;/p&gt; 
&lt;h3&gt;Linux (AMD GPUs)&lt;/h3&gt; 
&lt;p&gt;Note that the &lt;a href="https://raw.githubusercontent.com/lllyasviel/Fooocus/main/#minimal-requirement"&gt;minimal requirement&lt;/a&gt; for different platforms is different.&lt;/p&gt; 
&lt;p&gt;Same with the above instructions. You need to change torch to the AMD version&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip uninstall torch torchvision torchaudio torchtext functorch xformers 
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.6
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;AMD is not intensively tested, however. The AMD support is in beta.&lt;/p&gt; 
&lt;p&gt;Use &lt;code&gt;python entry_with_update.py --preset anime&lt;/code&gt; or &lt;code&gt;python entry_with_update.py --preset realistic&lt;/code&gt; for Fooocus Anime/Realistic Edition.&lt;/p&gt; 
&lt;h3&gt;Windows (AMD GPUs)&lt;/h3&gt; 
&lt;p&gt;Note that the &lt;a href="https://raw.githubusercontent.com/lllyasviel/Fooocus/main/#minimal-requirement"&gt;minimal requirement&lt;/a&gt; for different platforms is different.&lt;/p&gt; 
&lt;p&gt;Same with Windows. Download the software and edit the content of &lt;code&gt;run.bat&lt;/code&gt; as:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;.\python_embeded\python.exe -m pip uninstall torch torchvision torchaudio torchtext functorch xformers -y
.\python_embeded\python.exe -m pip install torch-directml
.\python_embeded\python.exe -s Fooocus\entry_with_update.py --directml
pause
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then run the &lt;code&gt;run.bat&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;AMD is not intensively tested, however. The AMD support is in beta.&lt;/p&gt; 
&lt;p&gt;For AMD, use &lt;code&gt;.\python_embeded\python.exe Fooocus\entry_with_update.py --directml --preset anime&lt;/code&gt; or &lt;code&gt;.\python_embeded\python.exe Fooocus\entry_with_update.py --directml --preset realistic&lt;/code&gt; for Fooocus Anime/Realistic Edition.&lt;/p&gt; 
&lt;h3&gt;Mac&lt;/h3&gt; 
&lt;p&gt;Note that the &lt;a href="https://raw.githubusercontent.com/lllyasviel/Fooocus/main/#minimal-requirement"&gt;minimal requirement&lt;/a&gt; for different platforms is different.&lt;/p&gt; 
&lt;p&gt;Mac is not intensively tested. Below is an unofficial guideline for using Mac. You can discuss problems &lt;a href="https://github.com/lllyasviel/Fooocus/pull/129"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can install Fooocus on Apple Mac silicon (M1 or M2) with macOS 'Catalina' or a newer version. Fooocus runs on Apple silicon computers via &lt;a href="https://pytorch.org/get-started/locally/"&gt;PyTorch&lt;/a&gt; MPS device acceleration. Mac Silicon computers don't come with a dedicated graphics card, resulting in significantly longer image processing times compared to computers with dedicated graphics cards.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install the conda package manager and pytorch nightly. Read the &lt;a href="https://developer.apple.com/metal/pytorch/"&gt;Accelerated PyTorch training on Mac&lt;/a&gt; Apple Developer guide for instructions. Make sure pytorch recognizes your MPS device.&lt;/li&gt; 
 &lt;li&gt;Open the macOS Terminal app and clone this repository with &lt;code&gt;git clone https://github.com/lllyasviel/Fooocus.git&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Change to the new Fooocus directory, &lt;code&gt;cd Fooocus&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Create a new conda environment, &lt;code&gt;conda env create -f environment.yaml&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Activate your new conda environment, &lt;code&gt;conda activate fooocus&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Install the packages required by Fooocus, &lt;code&gt;pip install -r requirements_versions.txt&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Launch Fooocus by running &lt;code&gt;python entry_with_update.py&lt;/code&gt;. (Some Mac M2 users may need &lt;code&gt;python entry_with_update.py --disable-offload-from-vram&lt;/code&gt; to speed up model loading/unloading.) The first time you run Fooocus, it will automatically download the Stable Diffusion SDXL models and will take a significant amount of time, depending on your internet connection.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Use &lt;code&gt;python entry_with_update.py --preset anime&lt;/code&gt; or &lt;code&gt;python entry_with_update.py --preset realistic&lt;/code&gt; for Fooocus Anime/Realistic Edition.&lt;/p&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/lllyasviel/Fooocus/main/docker.md"&gt;docker.md&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Download Previous Version&lt;/h3&gt; 
&lt;p&gt;See the guidelines &lt;a href="https://github.com/lllyasviel/Fooocus/discussions/1405"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Minimal Requirement&lt;/h2&gt; 
&lt;p&gt;Below is the minimal requirement for running Fooocus locally. If your device capability is lower than this spec, you may not be able to use Fooocus locally. (Please let us know, in any case, if your device capability is lower but Fooocus still works.)&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Operating System&lt;/th&gt; 
   &lt;th&gt;GPU&lt;/th&gt; 
   &lt;th&gt;Minimal GPU Memory&lt;/th&gt; 
   &lt;th&gt;Minimal System Memory&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://raw.githubusercontent.com/lllyasviel/Fooocus/main/troubleshoot.md"&gt;System Swap&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;Note&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows/Linux&lt;/td&gt; 
   &lt;td&gt;Nvidia RTX 4XXX&lt;/td&gt; 
   &lt;td&gt;4GB&lt;/td&gt; 
   &lt;td&gt;8GB&lt;/td&gt; 
   &lt;td&gt;Required&lt;/td&gt; 
   &lt;td&gt;fastest&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows/Linux&lt;/td&gt; 
   &lt;td&gt;Nvidia RTX 3XXX&lt;/td&gt; 
   &lt;td&gt;4GB&lt;/td&gt; 
   &lt;td&gt;8GB&lt;/td&gt; 
   &lt;td&gt;Required&lt;/td&gt; 
   &lt;td&gt;usually faster than RTX 2XXX&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows/Linux&lt;/td&gt; 
   &lt;td&gt;Nvidia RTX 2XXX&lt;/td&gt; 
   &lt;td&gt;4GB&lt;/td&gt; 
   &lt;td&gt;8GB&lt;/td&gt; 
   &lt;td&gt;Required&lt;/td&gt; 
   &lt;td&gt;usually faster than GTX 1XXX&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows/Linux&lt;/td&gt; 
   &lt;td&gt;Nvidia GTX 1XXX&lt;/td&gt; 
   &lt;td&gt;8GB (* 6GB uncertain)&lt;/td&gt; 
   &lt;td&gt;8GB&lt;/td&gt; 
   &lt;td&gt;Required&lt;/td&gt; 
   &lt;td&gt;only marginally faster than CPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows/Linux&lt;/td&gt; 
   &lt;td&gt;Nvidia GTX 9XX&lt;/td&gt; 
   &lt;td&gt;8GB&lt;/td&gt; 
   &lt;td&gt;8GB&lt;/td&gt; 
   &lt;td&gt;Required&lt;/td&gt; 
   &lt;td&gt;faster or slower than CPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows/Linux&lt;/td&gt; 
   &lt;td&gt;Nvidia GTX &amp;lt; 9XX&lt;/td&gt; 
   &lt;td&gt;Not supported&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows&lt;/td&gt; 
   &lt;td&gt;AMD GPU&lt;/td&gt; 
   &lt;td&gt;8GB (updated 2023 Dec 30)&lt;/td&gt; 
   &lt;td&gt;8GB&lt;/td&gt; 
   &lt;td&gt;Required&lt;/td&gt; 
   &lt;td&gt;via DirectML (* ROCm is on hold), about 3x slower than Nvidia RTX 3XXX&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Linux&lt;/td&gt; 
   &lt;td&gt;AMD GPU&lt;/td&gt; 
   &lt;td&gt;8GB&lt;/td&gt; 
   &lt;td&gt;8GB&lt;/td&gt; 
   &lt;td&gt;Required&lt;/td&gt; 
   &lt;td&gt;via ROCm, about 1.5x slower than Nvidia RTX 3XXX&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mac&lt;/td&gt; 
   &lt;td&gt;M1/M2 MPS&lt;/td&gt; 
   &lt;td&gt;Shared&lt;/td&gt; 
   &lt;td&gt;Shared&lt;/td&gt; 
   &lt;td&gt;Shared&lt;/td&gt; 
   &lt;td&gt;about 9x slower than Nvidia RTX 3XXX&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows/Linux/Mac&lt;/td&gt; 
   &lt;td&gt;only use CPU&lt;/td&gt; 
   &lt;td&gt;0GB&lt;/td&gt; 
   &lt;td&gt;32GB&lt;/td&gt; 
   &lt;td&gt;Required&lt;/td&gt; 
   &lt;td&gt;about 17x slower than Nvidia RTX 3XXX&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;* AMD GPU ROCm (on hold): The AMD is still working on supporting ROCm on Windows.&lt;/p&gt; 
&lt;p&gt;* Nvidia GTX 1XXX 6GB uncertain: Some people report 6GB success on GTX 10XX, but some other people report failure cases.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Note that Fooocus is only for extremely high quality image generating. We will not support smaller models to reduce the requirement and sacrifice result quality.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Troubleshoot&lt;/h2&gt; 
&lt;p&gt;See the common problems &lt;a href="https://raw.githubusercontent.com/lllyasviel/Fooocus/main/troubleshoot.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Default Models&lt;/h2&gt; 
&lt;p&gt;&lt;a name="models"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Given different goals, the default models and configs of Fooocus are different:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Task&lt;/th&gt; 
   &lt;th&gt;Windows&lt;/th&gt; 
   &lt;th&gt;Linux args&lt;/th&gt; 
   &lt;th&gt;Main Model&lt;/th&gt; 
   &lt;th&gt;Refiner&lt;/th&gt; 
   &lt;th&gt;Config&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;General&lt;/td&gt; 
   &lt;td&gt;run.bat&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;juggernautXL_v8Rundiffusion&lt;/td&gt; 
   &lt;td&gt;not used&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/lllyasviel/Fooocus/raw/main/presets/default.json"&gt;here&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Realistic&lt;/td&gt; 
   &lt;td&gt;run_realistic.bat&lt;/td&gt; 
   &lt;td&gt;--preset realistic&lt;/td&gt; 
   &lt;td&gt;realisticStockPhoto_v20&lt;/td&gt; 
   &lt;td&gt;not used&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/lllyasviel/Fooocus/raw/main/presets/realistic.json"&gt;here&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Anime&lt;/td&gt; 
   &lt;td&gt;run_anime.bat&lt;/td&gt; 
   &lt;td&gt;--preset anime&lt;/td&gt; 
   &lt;td&gt;animaPencilXL_v500&lt;/td&gt; 
   &lt;td&gt;not used&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/lllyasviel/Fooocus/raw/main/presets/anime.json"&gt;here&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Note that the download is &lt;strong&gt;automatic&lt;/strong&gt; - you do not need to do anything if the internet connection is okay. However, you can download them manually if you (or move them from somewhere else) have your own preparation.&lt;/p&gt; 
&lt;h2&gt;UI Access and Authentication&lt;/h2&gt; 
&lt;p&gt;In addition to running on localhost, Fooocus can also expose its UI in two ways:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Local UI listener: use &lt;code&gt;--listen&lt;/code&gt; (specify port e.g. with &lt;code&gt;--port 8888&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;API access: use &lt;code&gt;--share&lt;/code&gt; (registers an endpoint at &lt;code&gt;.gradio.live&lt;/code&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In both ways the access is unauthenticated by default. You can add basic authentication by creating a file called &lt;code&gt;auth.json&lt;/code&gt; in the main directory, which contains a list of JSON objects with the keys &lt;code&gt;user&lt;/code&gt; and &lt;code&gt;pass&lt;/code&gt; (see example in &lt;a href="https://raw.githubusercontent.com/lllyasviel/Fooocus/main/auth-example.json"&gt;auth-example.json&lt;/a&gt;).&lt;/p&gt; 
&lt;h2&gt;List of "Hidden" Tricks&lt;/h2&gt; 
&lt;p&gt;&lt;a name="tech_list"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to see a list of tricks. Those are based on SDXL and are not very up-to-date with latest models.&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;GPT2-based &lt;a href="https://github.com/lllyasviel/Fooocus/discussions/117#raw"&gt;prompt expansion as a dynamic style "Fooocus V2".&lt;/a&gt; (similar to Midjourney's hidden pre-processing and "raw" mode, or the LeonardoAI's Prompt Magic).&lt;/li&gt; 
  &lt;li&gt;Native refiner swap inside one single k-sampler. The advantage is that the refiner model can now reuse the base model's momentum (or ODE's history parameters) collected from k-sampling to achieve more coherent sampling. In Automatic1111's high-res fix and ComfyUI's node system, the base model and refiner use two independent k-samplers, which means the momentum is largely wasted, and the sampling continuity is broken. Fooocus uses its own advanced k-diffusion sampling that ensures seamless, native, and continuous swap in a refiner setup. (Update Aug 13: Actually, I discussed this with Automatic1111 several days ago, and it seems that the ‚Äúnative refiner swap inside one single k-sampler‚Äù is &lt;a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/12371"&gt;merged&lt;/a&gt; into the dev branch of webui. Great!)&lt;/li&gt; 
  &lt;li&gt;Negative ADM guidance. Because the highest resolution level of XL Base does not have cross attentions, the positive and negative signals for XL's highest resolution level cannot receive enough contrasts during the CFG sampling, causing the results to look a bit plastic or overly smooth in certain cases. Fortunately, since the XL's highest resolution level is still conditioned on image aspect ratios (ADM), we can modify the adm on the positive/negative side to compensate for the lack of CFG contrast in the highest resolution level. (Update Aug 16, the IOS App &lt;a href="https://apps.apple.com/us/app/draw-things-ai-generation/id6444050820"&gt;Draw Things&lt;/a&gt; will support Negative ADM Guidance. Great!)&lt;/li&gt; 
  &lt;li&gt;We implemented a carefully tuned variation of Section 5.1 of &lt;a href="https://arxiv.org/pdf/2210.00939.pdf"&gt;"Improving Sample Quality of Diffusion Models Using Self-Attention Guidance"&lt;/a&gt;. The weight is set to very low, but this is Fooocus's final guarantee to make sure that the XL will never yield an overly smooth or plastic appearance (examples &lt;a href="https://github.com/lllyasviel/Fooocus/discussions/117#sharpness"&gt;here&lt;/a&gt;). This can almost eliminate all cases for which XL still occasionally produces overly smooth results, even with negative ADM guidance. (Update 2023 Aug 18, the Gaussian kernel of SAG is changed to an anisotropic kernel for better structure preservation and fewer artifacts.)&lt;/li&gt; 
  &lt;li&gt;We modified the style templates a bit and added the "cinematic-default".&lt;/li&gt; 
  &lt;li&gt;We tested the "sd_xl_offset_example-lora_1.0.safetensors" and it seems that when the lora weight is below 0.5, the results are always better than XL without lora.&lt;/li&gt; 
  &lt;li&gt;The parameters of samplers are carefully tuned.&lt;/li&gt; 
  &lt;li&gt;Because XL uses positional encoding for generation resolution, images generated by several fixed resolutions look a bit better than those from arbitrary resolutions (because the positional encoding is not very good at handling int numbers that are unseen during training). This suggests that the resolutions in UI may be hard coded for best results.&lt;/li&gt; 
  &lt;li&gt;Separated prompts for two different text encoders seem unnecessary. Separated prompts for the base model and refiner may work, but the effects are random, and we refrain from implementing this.&lt;/li&gt; 
  &lt;li&gt;The DPM family seems well-suited for XL since XL sometimes generates overly smooth texture, but the DPM family sometimes generates overly dense detail in texture. Their joint effect looks neutral and appealing to human perception.&lt;/li&gt; 
  &lt;li&gt;A carefully designed system for balancing multiple styles as well as prompt expansion.&lt;/li&gt; 
  &lt;li&gt;Using automatic1111's method to normalize prompt emphasizing. This significantly improves results when users directly copy prompts from civitai.&lt;/li&gt; 
  &lt;li&gt;The joint swap system of the refiner now also supports img2img and upscale in a seamless way.&lt;/li&gt; 
  &lt;li&gt;CFG Scale and TSNR correction (tuned for SDXL) when CFG is bigger than 10.&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h2&gt;Customization&lt;/h2&gt; 
&lt;p&gt;After the first time you run Fooocus, a config file will be generated at &lt;code&gt;Fooocus\config.txt&lt;/code&gt;. This file can be edited to change the model path or default parameters.&lt;/p&gt; 
&lt;p&gt;For example, an edited &lt;code&gt;Fooocus\config.txt&lt;/code&gt; (this file will be generated after the first launch) may look like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
    "path_checkpoints": "D:\\Fooocus\\models\\checkpoints",
    "path_loras": "D:\\Fooocus\\models\\loras",
    "path_embeddings": "D:\\Fooocus\\models\\embeddings",
    "path_vae_approx": "D:\\Fooocus\\models\\vae_approx",
    "path_upscale_models": "D:\\Fooocus\\models\\upscale_models",
    "path_inpaint": "D:\\Fooocus\\models\\inpaint",
    "path_controlnet": "D:\\Fooocus\\models\\controlnet",
    "path_clip_vision": "D:\\Fooocus\\models\\clip_vision",
    "path_fooocus_expansion": "D:\\Fooocus\\models\\prompt_expansion\\fooocus_expansion",
    "path_outputs": "D:\\Fooocus\\outputs",
    "default_model": "realisticStockPhoto_v10.safetensors",
    "default_refiner": "",
    "default_loras": [["lora_filename_1.safetensors", 0.5], ["lora_filename_2.safetensors", 0.5]],
    "default_cfg_scale": 3.0,
    "default_sampler": "dpmpp_2m",
    "default_scheduler": "karras",
    "default_negative_prompt": "low quality",
    "default_positive_prompt": "",
    "default_styles": [
        "Fooocus V2",
        "Fooocus Photograph",
        "Fooocus Negative"
    ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Many other keys, formats, and examples are in &lt;code&gt;Fooocus\config_modification_tutorial.txt&lt;/code&gt; (this file will be generated after the first launch).&lt;/p&gt; 
&lt;p&gt;Consider twice before you really change the config. If you find yourself breaking things, just delete &lt;code&gt;Fooocus\config.txt&lt;/code&gt;. Fooocus will go back to default.&lt;/p&gt; 
&lt;p&gt;A safer way is just to try "run_anime.bat" or "run_realistic.bat" - they should already be good enough for different tasks.&lt;/p&gt; 
&lt;p&gt;&lt;del&gt;Note that &lt;code&gt;user_path_config.txt&lt;/code&gt; is deprecated and will be removed soon.&lt;/del&gt; (Edit: it is already removed.)&lt;/p&gt; 
&lt;h3&gt;All CMD Flags&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;entry_with_update.py  [-h] [--listen [IP]] [--port PORT]
                      [--disable-header-check [ORIGIN]]
                      [--web-upload-size WEB_UPLOAD_SIZE]
                      [--hf-mirror HF_MIRROR]
                      [--external-working-path PATH [PATH ...]]
                      [--output-path OUTPUT_PATH]
                      [--temp-path TEMP_PATH] [--cache-path CACHE_PATH]
                      [--in-browser] [--disable-in-browser]
                      [--gpu-device-id DEVICE_ID]
                      [--async-cuda-allocation | --disable-async-cuda-allocation]
                      [--disable-attention-upcast]
                      [--all-in-fp32 | --all-in-fp16]
                      [--unet-in-bf16 | --unet-in-fp16 | --unet-in-fp8-e4m3fn | --unet-in-fp8-e5m2]
                      [--vae-in-fp16 | --vae-in-fp32 | --vae-in-bf16]
                      [--vae-in-cpu]
                      [--clip-in-fp8-e4m3fn | --clip-in-fp8-e5m2 | --clip-in-fp16 | --clip-in-fp32]
                      [--directml [DIRECTML_DEVICE]]
                      [--disable-ipex-hijack]
                      [--preview-option [none,auto,fast,taesd]]
                      [--attention-split | --attention-quad | --attention-pytorch]
                      [--disable-xformers]
                      [--always-gpu | --always-high-vram | --always-normal-vram | --always-low-vram | --always-no-vram | --always-cpu [CPU_NUM_THREADS]]
                      [--always-offload-from-vram]
                      [--pytorch-deterministic] [--disable-server-log]
                      [--debug-mode] [--is-windows-embedded-python]
                      [--disable-server-info] [--multi-user] [--share]
                      [--preset PRESET] [--disable-preset-selection]
                      [--language LANGUAGE]
                      [--disable-offload-from-vram] [--theme THEME]
                      [--disable-image-log] [--disable-analytics]
                      [--disable-metadata] [--disable-preset-download]
                      [--disable-enhance-output-sorting]
                      [--enable-auto-describe-image]
                      [--always-download-new-model]
                      [--rebuild-hash-cache [CPU_NUM_THREADS]]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Inline Prompt Features&lt;/h2&gt; 
&lt;h3&gt;Wildcards&lt;/h3&gt; 
&lt;p&gt;Example prompt: &lt;code&gt;__color__ flower&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Processed for positive and negative prompt.&lt;/p&gt; 
&lt;p&gt;Selects a random wildcard from a predefined list of options, in this case the &lt;code&gt;wildcards/color.txt&lt;/code&gt; file. The wildcard will be replaced with a random color (randomness based on seed). You can also disable randomness and process a wildcard file from top to bottom by enabling the checkbox &lt;code&gt;Read wildcards in order&lt;/code&gt; in Developer Debug Mode.&lt;/p&gt; 
&lt;p&gt;Wildcards can be nested and combined, and multiple wildcards can be used in the same prompt (example see &lt;code&gt;wildcards/color_flower.txt&lt;/code&gt;).&lt;/p&gt; 
&lt;h3&gt;Array Processing&lt;/h3&gt; 
&lt;p&gt;Example prompt: &lt;code&gt;[[red, green, blue]] flower&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Processed only for positive prompt.&lt;/p&gt; 
&lt;p&gt;Processes the array from left to right, generating a separate image for each element in the array. In this case 3 images would be generated, one for each color. Increase the image number to 3 to generate all 3 variants.&lt;/p&gt; 
&lt;p&gt;Arrays can not be nested, but multiple arrays can be used in the same prompt. Does support inline LoRAs as array elements!&lt;/p&gt; 
&lt;h3&gt;Inline LoRAs&lt;/h3&gt; 
&lt;p&gt;Example prompt: &lt;code&gt;flower &amp;lt;lora:sunflowers:1.2&amp;gt;&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Processed only for positive prompt.&lt;/p&gt; 
&lt;p&gt;Applies a LoRA to the prompt. The LoRA file must be located in the &lt;code&gt;models/loras&lt;/code&gt; directory.&lt;/p&gt; 
&lt;h2&gt;Advanced Features&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/lllyasviel/Fooocus/discussions/117"&gt;Click here to browse the advanced features.&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Forks&lt;/h2&gt; 
&lt;p&gt;Below are some Forks to Fooocus:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Fooocus' forks&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/fenneishi/Fooocus-Control"&gt;fenneishi/Fooocus-Control&lt;/a&gt; &lt;br /&gt;&lt;a href="https://github.com/runew0lf/RuinedFooocus"&gt;runew0lf/RuinedFooocus&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/MoonRide303/Fooocus-MRE"&gt;MoonRide303/Fooocus-MRE&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/mashb1t/Fooocus"&gt;mashb1t/Fooocus&lt;/a&gt; &lt;br /&gt; and so on ...&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Thanks&lt;/h2&gt; 
&lt;p&gt;Many thanks to &lt;a href="https://github.com/twri"&gt;twri&lt;/a&gt; and &lt;a href="https://github.com/3Diva"&gt;3Diva&lt;/a&gt; and &lt;a href="https://github.com/K3nt3L"&gt;Marc K3nt3L&lt;/a&gt; for creating additional SDXL styles available in Fooocus.&lt;/p&gt; 
&lt;p&gt;The project starts from a mixture of &lt;a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui"&gt;Stable Diffusion WebUI&lt;/a&gt; and &lt;a href="https://github.com/comfyanonymous/ComfyUI"&gt;ComfyUI&lt;/a&gt; codebases.&lt;/p&gt; 
&lt;p&gt;Also, thanks &lt;a href="https://github.com/daswer123"&gt;daswer123&lt;/a&gt; for contributing the Canvas Zoom!&lt;/p&gt; 
&lt;h2&gt;Update Log&lt;/h2&gt; 
&lt;p&gt;The log is &lt;a href="https://raw.githubusercontent.com/lllyasviel/Fooocus/main/update_log.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Localization/Translation/I18N&lt;/h2&gt; 
&lt;p&gt;You can put json files in the &lt;code&gt;language&lt;/code&gt; folder to translate the user interface.&lt;/p&gt; 
&lt;p&gt;For example, below is the content of &lt;code&gt;Fooocus/language/example.json&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "Generate": "ÁîüÊàê",
  "Input Image": "ÂÖ•ÂäõÁîªÂÉè",
  "Advanced": "Í≥†Í∏â",
  "SAI 3D Model": "SAI 3D Mod√®le"
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you add &lt;code&gt;--language example&lt;/code&gt; arg, Fooocus will read &lt;code&gt;Fooocus/language/example.json&lt;/code&gt; to translate the UI.&lt;/p&gt; 
&lt;p&gt;For example, you can edit the ending line of Windows &lt;code&gt;run.bat&lt;/code&gt; as&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;.\python_embeded\python.exe -s Fooocus\entry_with_update.py --language example
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or &lt;code&gt;run_anime.bat&lt;/code&gt; as&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;.\python_embeded\python.exe -s Fooocus\entry_with_update.py --language example --preset anime
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or &lt;code&gt;run_realistic.bat&lt;/code&gt; as&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;.\python_embeded\python.exe -s Fooocus\entry_with_update.py --language example --preset realistic
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For practical translation, you may create your own file like &lt;code&gt;Fooocus/language/jp.json&lt;/code&gt; or &lt;code&gt;Fooocus/language/cn.json&lt;/code&gt; and then use flag &lt;code&gt;--language jp&lt;/code&gt; or &lt;code&gt;--language cn&lt;/code&gt;. Apparently, these files do not exist now. &lt;strong&gt;We need your help to create these files!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Note that if no &lt;code&gt;--language&lt;/code&gt; is given and at the same time &lt;code&gt;Fooocus/language/default.json&lt;/code&gt; exists, Fooocus will always load &lt;code&gt;Fooocus/language/default.json&lt;/code&gt; for translation. By default, the file &lt;code&gt;Fooocus/language/default.json&lt;/code&gt; does not exist.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>HKUDS/DeepCode</title>
      <link>https://github.com/HKUDS/DeepCode</link>
      <description>&lt;p&gt;"DeepCode: Open Agentic Coding (Paper2Code &amp; Text2Web &amp; Text2Backend)"&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;table style="border: none; margin: 0 auto; padding: 0; border-collapse: collapse;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center" style="vertical-align: middle; padding: 10px; border: none; width: 250px;"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/DeepCode/main/assets/logo.png" alt="DeepCode Logo" width="200" style="margin: 0; padding: 0; display: block;" /&gt; &lt;/td&gt; 
    &lt;td align="left" style="vertical-align: middle; padding: 10px 0 10px 30px; border: none;"&gt; &lt;pre style="font-family: 'Courier New', monospace; font-size: 16px; color: #0EA5E9; margin: 0; padding: 0; text-shadow: 0 0 10px #0EA5E9, 0 0 20px rgba(14,165,233,0.5); line-height: 1.2; transform: skew(-1deg, 0deg); display: block;"&gt;    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
    ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù
    ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
    ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïù ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù
    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë     ‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù      ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù&lt;/pre&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://trendshift.io/repositories/14665" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14665" alt="HKUDS%2FDeepCode | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
 &lt;/div&gt; 
 &lt;!-- &lt;img src="https://readme-typing-svg.herokuapp.com?font=Russo+One&amp;size=28&amp;duration=2000&amp;pause=800&amp;color=06B6D4&amp;background=00000000&amp;center=true&amp;vCenter=true&amp;width=800&amp;height=50&amp;lines=%E2%9A%A1+OPEN+AGENTIC+CODING+%E2%9A%A1" alt="DeepCode Tech Subtitle" style="margin-top: 5px; filter: drop-shadow(0 0 12px #06B6D4) drop-shadow(0 0 24px rgba(6,182,212,0.4));"/&gt; --&gt; 
 &lt;h1&gt;&lt;img src="https://github.com/Zongwei9888/Experiment_Images/raw/43c585dca3d21b8e4b6390d835cdd34dc4b4b23d/DeepCode_images/title_logo.svg?sanitize=true" alt="DeepCode Logo" width="32" height="32" style="vertical-align: middle; margin-right: 8px;" /&gt; DeepCode: Open Agentic Coding&lt;/h1&gt; 
 &lt;h3&gt;&lt;em&gt;Advancing Code Generation with Multi-Agent Systems&lt;/em&gt;&lt;/h3&gt; 
 &lt;!-- &lt;p align="center"&gt;
  &lt;img src="https://img.shields.io/badge/Version-1.0.0-00d4ff?style=for-the-badge&amp;logo=rocket&amp;logoColor=white" alt="Version"&gt;

  &lt;img src="https://img.shields.io/badge/License-MIT-4ecdc4?style=for-the-badge&amp;logo=opensourceinitiative&amp;logoColor=white" alt="License"&gt;
  &lt;img src="https://img.shields.io/badge/AI-Multi--Agent-9b59b6?style=for-the-badge&amp;logo=brain&amp;logoColor=white" alt="AI"&gt;
  &lt;img src="https://img.shields.io/badge/HKU-Data_Intelligence_Lab-f39c12?style=for-the-badge&amp;logo=university&amp;logoColor=white" alt="HKU"&gt;
&lt;/p&gt; --&gt; 
 &lt;p&gt; &lt;a href="https://github.com/HKUDS/DeepCode/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/HKUDS/DeepCode?color=00d9ff&amp;amp;style=for-the-badge&amp;amp;logo=star&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/üêçPython-3.13-4ecdc4?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;a href="https://pypi.org/project/deepcode-hku/"&gt;&lt;img src="https://img.shields.io/pypi/v/deepcode-hku.svg?style=for-the-badge&amp;amp;logo=pypi&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e&amp;amp;color=ff6b6b" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;a href="https://discord.gg/yF2MmDJyGJ"&gt;&lt;img src="https://img.shields.io/badge/üí¨Discord-Community-7289da?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/DeepCode/issues/11"&gt;&lt;img src="https://img.shields.io/badge/üí¨WeChat-Group-07c160?style=for-the-badge&amp;amp;logo=wechat&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;div style="width: 100%; height: 2px; margin: 20px 0; background: linear-gradient(90deg, transparent, #00d9ff, transparent);"&gt;&lt;/div&gt; 
 &lt;/div&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-quick-start" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/Quick%20Start-Get%20Started%20Now-00d9ff?style=for-the-badge&amp;amp;logo=rocket&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;/a&gt; 
 &lt;/div&gt; 
 &lt;h3&gt;üñ•Ô∏è &lt;strong&gt;Interface Showcase&lt;/strong&gt;&lt;/h3&gt; 
 &lt;table align="center" width="100%" style="border: none; border-collapse: collapse; margin: 30px 0;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td width="50%" align="center" style="vertical-align: top; padding: 20px;"&gt; &lt;h4&gt;üñ•Ô∏è &lt;strong&gt;CLI Interface&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Terminal-Based Development&lt;/strong&gt;&lt;/p&gt; 
     &lt;div align="center"&gt; 
      &lt;img src="https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/CLI.gif" alt="CLI Interface Demo" width="100%" style="border-radius: 10px; box-shadow: 0 8px 20px rgba(45,55,72,0.3); margin: 15px 0;" /&gt; 
      &lt;div style="background: linear-gradient(135deg, #2D3748 0%, #4A5568 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;"&gt; 
       &lt;strong&gt;üöÄ Advanced Terminal Experience&lt;/strong&gt;
       &lt;br /&gt; 
       &lt;small&gt;‚ö° Fast command-line workflow&lt;br /&gt;üîß Developer-friendly interface&lt;br /&gt;üìä Real-time progress tracking&lt;/small&gt; 
      &lt;/div&gt; 
      &lt;p&gt;&lt;em&gt;Professional terminal interface for advanced users and CI/CD integration&lt;/em&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td width="50%" align="center" style="vertical-align: top; padding: 20px;"&gt; &lt;h4&gt;üåê &lt;strong&gt;Web Interface&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Visual Interactive Experience&lt;/strong&gt;&lt;/p&gt; 
     &lt;div align="center"&gt; 
      &lt;img src="https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/UI.gif" alt="Web Interface Demo" width="100%" style="border-radius: 10px; box-shadow: 0 8px 20px rgba(14,165,233,0.3); margin: 15px 0;" /&gt; 
      &lt;div style="background: linear-gradient(135deg, #0EA5E9 0%, #00D4FF 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;"&gt; 
       &lt;strong&gt;üé® Modern Web Dashboard&lt;/strong&gt;
       &lt;br /&gt; 
       &lt;small&gt;üñ±Ô∏è Intuitive drag-and-drop&lt;br /&gt;üì± Responsive design&lt;br /&gt;üéØ Visual progress tracking&lt;/small&gt; 
      &lt;/div&gt; 
      &lt;p&gt;&lt;em&gt;Beautiful web interface with streamlined workflow for all skill levels&lt;/em&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;hr /&gt; 
 &lt;div align="center"&gt; 
  &lt;h3&gt;üé¨ &lt;strong&gt;Introduction Video&lt;/strong&gt;&lt;/h3&gt; 
  &lt;div style="margin: 20px 0;"&gt; 
   &lt;a href="https://youtu.be/PRgmP8pOI08" target="_blank"&gt; &lt;img src="https://img.youtube.com/vi/PRgmP8pOI08/maxresdefault.jpg" alt="DeepCode Introduction Video" width="75%" style="border-radius: 12px; box-shadow: 0 8px 25px rgba(0,0,0,0.15); transition: transform 0.3s ease;" /&gt; &lt;/a&gt; 
  &lt;/div&gt; 
  &lt;p&gt;&lt;em&gt;üéØ &lt;strong&gt;Watch our complete introduction&lt;/strong&gt; - See how DeepCode transforms research papers and natural language into production-ready code&lt;/em&gt;&lt;/p&gt; 
  &lt;p&gt; &lt;a href="https://youtu.be/PRgmP8pOI08" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/‚ñ∂Ô∏è_Watch_Video-FF0000?style=for-the-badge&amp;amp;logo=youtube&amp;amp;logoColor=white" alt="Watch Video" /&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;/div&gt; 
 &lt;hr /&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;em&gt;"Where AI Agents Transform Ideas into Production-Ready Code"&lt;/em&gt;&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìë Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-key-features"&gt;üöÄ Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#%EF%B8%8F-architecture"&gt;üèóÔ∏è Architecture&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-quick-start"&gt;üöÄ Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-examples"&gt;üí° Examples&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-live-demonstrations"&gt;üé¨ Live Demonstrations&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-star-history"&gt;‚≠ê Star History&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-license"&gt;üìÑ License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ Key Features&lt;/h2&gt; 
&lt;br /&gt; 
&lt;table align="center" width="100%" style="border: none; table-layout: fixed;"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="30%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;üöÄ &lt;strong&gt;Paper2Code&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/ALGORITHM-IMPLEMENTATION-ff6b6b?style=for-the-badge&amp;amp;logo=algorithm&amp;amp;logoColor=white" alt="Algorithm Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Automated Implementation of Complex Algorithms&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Effortlessly converts complex algorithms from research papers into &lt;strong&gt;high-quality&lt;/strong&gt;, &lt;strong&gt;production-ready&lt;/strong&gt; code, accelerating algorithm reproduction.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="30%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;üé® &lt;strong&gt;Text2Web&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/FRONTEND-DEVELOPMENT-4ecdc4?style=for-the-badge&amp;amp;logo=react&amp;amp;logoColor=white" alt="Frontend Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Automated Front-End Web Development&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Translates plain textual descriptions into &lt;strong&gt;fully functional&lt;/strong&gt;, &lt;strong&gt;visually appealing&lt;/strong&gt; front-end web code for rapid interface creation.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="30%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;‚öôÔ∏è &lt;strong&gt;Text2Backend&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/BACKEND-DEVELOPMENT-9b59b6?style=for-the-badge&amp;amp;logo=server&amp;amp;logoColor=white" alt="Backend Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Automated Back-End Development&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Generates &lt;strong&gt;efficient&lt;/strong&gt;, &lt;strong&gt;scalable&lt;/strong&gt;, and &lt;strong&gt;feature-rich&lt;/strong&gt; back-end code from simple text inputs, streamlining server-side development.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;br /&gt; 
&lt;h3&gt;üéØ &lt;strong&gt;Autonomous Multi-Agent Workflow&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;The Challenges&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;üìÑ &lt;strong&gt;Implementation Complexity&lt;/strong&gt;: Converting academic papers and complex algorithms into working code requires significant technical effort and domain expertise&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üî¨ &lt;strong&gt;Research Bottleneck&lt;/strong&gt;: Researchers spend valuable time implementing algorithms instead of focusing on their core research and discovery work&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚è±Ô∏è &lt;strong&gt;Development Delays&lt;/strong&gt;: Product teams experience long wait times between concept and testable prototypes, slowing down innovation cycles&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üîÑ &lt;strong&gt;Repetitive Coding&lt;/strong&gt;: Developers repeatedly implement similar patterns and functionality instead of building on existing solutions&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;DeepCode&lt;/strong&gt; addresses these workflow inefficiencies by providing reliable automation for common development tasks, streamlining your development workflow from concept to code.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;pre&gt;&lt;code class="language-mermaid"&gt;flowchart LR
    A["üìÑ Research Papers&amp;lt;br/&amp;gt;üí¨ Text Prompts&amp;lt;br/&amp;gt;üåê URLs &amp;amp; Document&amp;lt;br/&amp;gt;üìé Files: PDF, DOC, PPTX, TXT, HTML"] --&amp;gt; B["üß† DeepCode&amp;lt;br/&amp;gt;Multi-Agent Engine"]
    B --&amp;gt; C["üöÄ Algorithm Implementation &amp;lt;br/&amp;gt;üé® Frontend Development &amp;lt;br/&amp;gt;‚öôÔ∏è Backend Development"]

    style A fill:#ff6b6b,stroke:#c0392b,stroke-width:2px,color:#000
    style B fill:#00d4ff,stroke:#0984e3,stroke-width:3px,color:#000
    style C fill:#00b894,stroke:#00a085,stroke-width:2px,color:#000
&lt;/code&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üèóÔ∏è Architecture&lt;/h2&gt; 
&lt;h3&gt;üìä &lt;strong&gt;System Overview&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;DeepCode&lt;/strong&gt; is an AI-powered development platform that automates code generation and implementation tasks. Our multi-agent system handles the complexity of translating requirements into functional, well-structured code, allowing you to focus on innovation rather than implementation details.&lt;/p&gt; 
&lt;p&gt;üéØ &lt;strong&gt;Technical Capabilities&lt;/strong&gt;:&lt;/p&gt; 
&lt;p&gt;üß¨ &lt;strong&gt;Research-to-Production Pipeline&lt;/strong&gt;&lt;br /&gt; Multi-modal document analysis engine that extracts algorithmic logic and mathematical models from academic papers. Generates optimized implementations with proper data structures while preserving computational complexity characteristics.&lt;/p&gt; 
&lt;p&gt;ü™Ñ &lt;strong&gt;Natural Language Code Synthesis&lt;/strong&gt;&lt;br /&gt; Context-aware code generation using fine-tuned language models trained on curated code repositories. Maintains architectural consistency across modules while supporting multiple programming languages and frameworks.&lt;/p&gt; 
&lt;p&gt;‚ö° &lt;strong&gt;Automated Prototyping Engine&lt;/strong&gt;&lt;br /&gt; Intelligent scaffolding system generating complete application structures including database schemas, API endpoints, and frontend components. Uses dependency analysis to ensure scalable architecture from initial generation.&lt;/p&gt; 
&lt;p&gt;üíé &lt;strong&gt;Quality Assurance Automation&lt;/strong&gt;&lt;br /&gt; Integrated static analysis with automated unit test generation and documentation synthesis. Employs AST analysis for code correctness and property-based testing for comprehensive coverage.&lt;/p&gt; 
&lt;p&gt;üîÆ &lt;strong&gt;CodeRAG Integration System&lt;/strong&gt;&lt;br /&gt; Advanced retrieval-augmented generation combining semantic vector embeddings with graph-based dependency analysis. Automatically discovers optimal libraries and implementation patterns from large-scale code corpus.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üîß &lt;strong&gt;Core Techniques&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;üß† &lt;strong&gt;Intelligent Orchestration Agent&lt;/strong&gt;: Central decision-making system that coordinates workflow phases and analyzes requirements. Employs dynamic planning algorithms to adapt execution strategies in real-time based on evolving project complexity. Dynamically selects optimal processing strategies for each implementation step. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üíæ &lt;strong&gt;Efficient Memory Mechanism&lt;/strong&gt;: Advanced context engineering system that manages large-scale code contexts efficiently. Implements hierarchical memory structures with intelligent compression for handling complex codebases. This component enables instant retrieval of implementation patterns and maintains semantic coherence across extended development sessions. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üîç &lt;strong&gt;Advanced CodeRAG System&lt;/strong&gt;: Global code comprehension engine that analyzes complex inter-dependencies across repositories. Performs cross-codebase relationship mapping to understand architectural patterns from a holistic perspective. This module leverages dependency graphs and semantic analysis to provide globally-aware code recommendations during implementation.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;ü§ñ &lt;strong&gt;Multi-Agent Architecture of DeepCode&lt;/strong&gt;:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üéØ Central Orchestrating Agent&lt;/strong&gt;: Orchestrates entire workflow execution and makes strategic decisions. Coordinates specialized agents based on input complexity analysis. Implements dynamic task planning and resource allocation algorithms. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìù Intent Understanding Agent&lt;/strong&gt;: Performs deep semantic analysis of user requirements to decode complex intentions. Extracts functional specifications and technical constraints through advanced NLP processing. Transforms ambiguous human descriptions into precise, actionable development specifications with structured task decomposition. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìÑ Document Parsing Agent&lt;/strong&gt;: Processes complex technical documents and research papers with advanced parsing capabilities. Extracts algorithms and methodologies using document understanding models. Converts academic concepts into practical implementation specifications through intelligent content analysis. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üèóÔ∏è Code Planning Agent&lt;/strong&gt;: Performs architectural design and technology stack optimization. Dynamic planning for adaptive development roadmaps. Enforces coding standards and generates modular structures through automated design pattern selection.&lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üîç Code Reference Mining Agent&lt;/strong&gt;: Discovers relevant repositories and frameworks through intelligent search algorithms. Analyzes codebases for compatibility and integration potential. Provides recommendations based on similarity metrics and automated dependency analysis. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìö Code Indexing Agent&lt;/strong&gt;: Builds comprehensive knowledge graphs of discovered codebases. Maintains semantic relationships between code components. Enables intelligent retrieval and cross-reference capabilities. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üß¨ Code Generation Agent&lt;/strong&gt;: Synthesizes gathered information into executable code implementations. Creates functional interfaces and integrates discovered components. Generates comprehensive test suites and documentation for reproducibility.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h4&gt;üõ†Ô∏è &lt;strong&gt;Implementation Tools Matrix&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;üîß Powered by MCP (Model Context Protocol)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;DeepCode leverages the &lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt; standard to seamlessly integrate with various tools and services. This standardized approach ensures reliable communication between AI agents and external systems, enabling powerful automation capabilities.&lt;/p&gt; 
&lt;h5&gt;üì° &lt;strong&gt;MCP Servers &amp;amp; Tools&lt;/strong&gt;&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;üõ†Ô∏è &lt;strong&gt;MCP Server&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;üîß &lt;strong&gt;Primary Function&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;üí° &lt;strong&gt;Purpose &amp;amp; Capabilities&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üîç brave&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Web Search Engine&lt;/td&gt; 
   &lt;td&gt;Real-time information retrieval via Brave Search API&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üåê bocha-mcp&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alternative Search&lt;/td&gt; 
   &lt;td&gt;Secondary search option with independent API access&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìÇ filesystem&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;File System Operations&lt;/td&gt; 
   &lt;td&gt;Local file and directory management, read/write operations&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üåê fetch&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Web Content Retrieval&lt;/td&gt; 
   &lt;td&gt;Fetch and extract content from URLs and web resources&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üì• github-downloader&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Repository Management&lt;/td&gt; 
   &lt;td&gt;Clone and download GitHub repositories for analysis&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìã file-downloader&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Document Processing&lt;/td&gt; 
   &lt;td&gt;Download and convert files (PDF, DOCX, etc.) to Markdown&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;‚ö° command-executor&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;System Commands&lt;/td&gt; 
   &lt;td&gt;Execute bash/shell commands for environment management&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üß¨ code-implementation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Code Generation Hub&lt;/td&gt; 
   &lt;td&gt;Comprehensive code reproduction with execution and testing&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìö code-reference-indexer&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Smart Code Search&lt;/td&gt; 
   &lt;td&gt;Intelligent indexing and search of code repositories&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìÑ document-segmentation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Smart Document Analysis&lt;/td&gt; 
   &lt;td&gt;Intelligent document segmentation for large papers and technical documents&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h5&gt;üîß &lt;strong&gt;Legacy Tool Functions&lt;/strong&gt; &lt;em&gt;(for reference)&lt;/em&gt;&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;üõ†Ô∏è &lt;strong&gt;Function&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;üéØ &lt;strong&gt;Usage Context&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìÑ read_code_mem&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Efficient code context retrieval from memory&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;‚úçÔ∏è write_file&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Direct file content generation and modification&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üêç execute_python&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Python code testing and validation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìÅ get_file_structure&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Project structure analysis and organization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;‚öôÔ∏è set_workspace&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Dynamic workspace and environment configuration&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìä get_operation_history&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Process monitoring and operation tracking&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;p&gt;üéõÔ∏è &lt;strong&gt;Multi-Interface Framework&lt;/strong&gt;&lt;br /&gt; RESTful API with CLI and web frontends featuring real-time code streaming, interactive debugging, and extensible plugin architecture for CI/CD integration.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;üöÄ Multi-Agent Intelligent Pipeline:&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;üåü &lt;strong&gt;Intelligence Processing Flow&lt;/strong&gt;&lt;/h3&gt; 
 &lt;table align="center" width="100%" style="border: none; border-collapse: collapse;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; color: white; font-weight: bold;"&gt; üí° &lt;strong&gt;INPUT LAYER&lt;/strong&gt;&lt;br /&gt; üìÑ Research Papers ‚Ä¢ üí¨ Natural Language ‚Ä¢ üåê URLs ‚Ä¢ üìã Requirements &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="20"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; üéØ &lt;strong&gt;CENTRAL ORCHESTRATION&lt;/strong&gt;&lt;br /&gt; Strategic Decision Making ‚Ä¢ Workflow Coordination ‚Ä¢ Agent Management &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #3742fa 0%, #2f3542 100%); border-radius: 10px; color: white; width: 50%;"&gt; üìù &lt;strong&gt;TEXT ANALYSIS&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Requirement Processing&lt;/small&gt; &lt;/td&gt; 
    &lt;td width="10"&gt;&lt;/td&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #8c7ae6 0%, #9c88ff 100%); border-radius: 10px; color: white; width: 50%;"&gt; üìÑ &lt;strong&gt;DOCUMENT ANALYSIS&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Paper &amp;amp; Spec Processing&lt;/small&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #00d2d3 0%, #54a0ff 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; üìã &lt;strong&gt;REPRODUCTION PLANNING&lt;/strong&gt;&lt;br /&gt; Deep Paper Analysis ‚Ä¢ Code Requirements Parsing ‚Ä¢ Reproduction Strategy Development &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #ffa726 0%, #ff7043 100%); border-radius: 10px; color: white; width: 50%;"&gt; üîç &lt;strong&gt;REFERENCE ANALYSIS&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Repository Discovery&lt;/small&gt; &lt;/td&gt; 
    &lt;td width="10"&gt;&lt;/td&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #e056fd 0%, #f368e0 100%); border-radius: 10px; color: white; width: 50%;"&gt; üìö &lt;strong&gt;CODE INDEXING&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Knowledge Graph Building&lt;/small&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #26de81 0%, #20bf6b 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; üß¨ &lt;strong&gt;CODE IMPLEMENTATION&lt;/strong&gt;&lt;br /&gt; Implementation Generation ‚Ä¢ Testing ‚Ä¢ Documentation &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 20px; background: linear-gradient(135deg, #045de9 0%, #09c6f9 100%); border-radius: 15px; color: white; font-weight: bold;"&gt; ‚ö° &lt;strong&gt;OUTPUT DELIVERY&lt;/strong&gt;&lt;br /&gt; üì¶ Complete Codebase ‚Ä¢ üß™ Test Suite ‚Ä¢ üìö Documentation ‚Ä¢ üöÄ Deployment Ready &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;br /&gt; 
 &lt;h3&gt;üîÑ &lt;strong&gt;Process Intelligence Features&lt;/strong&gt;&lt;/h3&gt; 
 &lt;table align="center" style="border: none;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #ff6b6b;"&gt; 
      &lt;h4&gt;üéØ Adaptive Flow&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Dynamic agent selection based on input complexity&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #4ecdc4;"&gt; 
      &lt;h4&gt;üß† Smart Coordination&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Intelligent task distribution and parallel processing&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #45b7d1;"&gt; 
      &lt;h4&gt;üîç Context Awareness&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Deep understanding through CodeRAG integration&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #96ceb4;"&gt; 
      &lt;h4&gt;‚ö° Quality Assurance&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Automated testing and validation throughout&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;h3&gt;üì¶ &lt;strong&gt;Step 1: Installation&lt;/strong&gt;&lt;/h3&gt; 
&lt;h4&gt;‚ö° &lt;strong&gt;Direct Installation (Recommended)&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# üöÄ Install DeepCode package directly
pip install deepcode-hku

# üîë Download configuration files
curl -O https://raw.githubusercontent.com/HKUDS/DeepCode/main/mcp_agent.config.yaml
curl -O https://raw.githubusercontent.com/HKUDS/DeepCode/main/mcp_agent.secrets.yaml

# üîë Configure API keys (required)
# Edit mcp_agent.secrets.yaml with your API keys and base_url:
# - openai: api_key, base_url (for OpenAI/custom endpoints)
# - anthropic: api_key (for Claude models)

# üîë Configure search API keys for web search (optional)
# Edit mcp_agent.config.yaml to set your API keys:
# - For Brave Search: Set BRAVE_API_KEY: "your_key_here" in brave.env section (line ~28)
# - For Bocha-MCP: Set BOCHA_API_KEY: "your_key_here" in bocha-mcp.env section (line ~74)

# üìÑ Configure document segmentation (optional)
# Edit mcp_agent.config.yaml to control document processing:
# - enabled: true/false (whether to use intelligent document segmentation)
# - size_threshold_chars: 50000 (document size threshold to trigger segmentation)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;üîß &lt;strong&gt;Development Installation (From Source)&lt;/strong&gt;&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìÇ Click to expand development installation options&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h5&gt;üî• &lt;strong&gt;Using UV (Recommended for Development)&lt;/strong&gt;&lt;/h5&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# üîΩ Clone the repository
git clone https://github.com/HKUDS/DeepCode.git
cd DeepCode/

# üì¶ Install UV package manager
curl -LsSf https://astral.sh/uv/install.sh | sh

# üîß Install dependencies with UV
uv venv --python=3.13
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
uv pip install -r requirements.txt

# üîë Configure API keys (required)
# Edit mcp_agent.secrets.yaml with your API keys and base_url:
# - openai: api_key, base_url (for OpenAI/custom endpoints)
# - anthropic: api_key (for Claude models)

# üîë Configure search API keys for web search (optional)
# Edit mcp_agent.config.yaml to set your API keys:
# - For Brave Search: Set BRAVE_API_KEY: "your_key_here" in brave.env section (line ~28)
# - For Bocha-MCP: Set BOCHA_API_KEY: "your_key_here" in bocha-mcp.env section (line ~74)

# üìÑ Configure document segmentation (optional)
# Edit mcp_agent.config.yaml to control document processing:
# - enabled: true/false (whether to use intelligent document segmentation)
# - size_threshold_chars: 50000 (document size threshold to trigger segmentation)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h5&gt;üêç &lt;strong&gt;Using Traditional pip&lt;/strong&gt;&lt;/h5&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# üîΩ Clone the repository
git clone https://github.com/HKUDS/DeepCode.git
cd DeepCode/

# üì¶ Install dependencies
pip install -r requirements.txt

# üîë Configure API keys (required)
# Edit mcp_agent.secrets.yaml with your API keys and base_url:
# - openai: api_key, base_url (for OpenAI/custom endpoints)
# - anthropic: api_key (for Claude models)

# üîë Configure search API keys for web search (optional)
# Edit mcp_agent.config.yaml to set your API keys:
# - For Brave Search: Set BRAVE_API_KEY: "your_key_here" in brave.env section (line ~28)
# - For Bocha-MCP: Set BOCHA_API_KEY: "your_key_here" in bocha-mcp.env section (line ~74)

# üìÑ Configure document segmentation (optional)
# Edit mcp_agent.config.yaml to control document processing:
# - enabled: true/false (whether to use intelligent document segmentation)
# - size_threshold_chars: 50000 (document size threshold to trigger segmentation)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;ü™ü &lt;strong&gt;Windows Users: Additional MCP Server Configuration&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;If you're using Windows, you may need to configure MCP servers manually in &lt;code&gt;mcp_agent.config.yaml&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Install MCP servers globally
npm i -g @modelcontextprotocol/server-brave-search
npm i -g @modelcontextprotocol/server-filesystem

# 2. Find your global node_modules path
npm -g root
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then update your &lt;code&gt;mcp_agent.config.yaml&lt;/code&gt; to use absolute paths:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;mcp:
  servers:
    brave:
      command: "node"
      args: ["C:/Program Files/nodejs/node_modules/@modelcontextprotocol/server-brave-search/dist/index.js"]
    filesystem:
      command: "node"
      args: ["C:/Program Files/nodejs/node_modules/@modelcontextprotocol/server-filesystem/dist/index.js", "."]
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Replace the path with your actual global node_modules path from step 2.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;üîç &lt;strong&gt;Search Server Configuration (Optional)&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;DeepCode supports multiple search servers for web search functionality. You can configure your preferred option in &lt;code&gt;mcp_agent.config.yaml&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# Default search server configuration
# Options: "brave" or "bocha-mcp"
default_search_server: "brave"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Available Options:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üîç Brave Search&lt;/strong&gt; (&lt;code&gt;"brave"&lt;/code&gt;):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Default option with high-quality search results&lt;/li&gt; 
   &lt;li&gt;Requires BRAVE_API_KEY configuration&lt;/li&gt; 
   &lt;li&gt;Recommended for most users&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üåê Bocha-MCP&lt;/strong&gt; (&lt;code&gt;"bocha-mcp"&lt;/code&gt;):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Alternative search server option&lt;/li&gt; 
   &lt;li&gt;Requires BOCHA_API_KEY configuration&lt;/li&gt; 
   &lt;li&gt;Uses local Python server implementation&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;API Key Configuration in mcp_agent.config.yaml:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# For Brave Search (default) - around line 28
brave:
  command: "npx"
  args: ["-y", "@modelcontextprotocol/server-brave-search"]
  env:
    BRAVE_API_KEY: "your_brave_api_key_here"

# For Bocha-MCP (alternative) - around line 74
bocha-mcp:
  command: "python"
  args: ["tools/bocha_search_server.py"]
  env:
    PYTHONPATH: "."
    BOCHA_API_KEY: "your_bocha_api_key_here"
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üí° Tip&lt;/strong&gt;: Both search servers require API key configuration. Choose the one that best fits your API access and requirements.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;‚ö° &lt;strong&gt;Step 2: Launch Application&lt;/strong&gt;&lt;/h3&gt; 
&lt;h4&gt;üöÄ &lt;strong&gt;Using Installed Package (Recommended)&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# üåê Launch web interface directly
deepcode

# The application will automatically start at http://localhost:8501
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;üõ†Ô∏è &lt;strong&gt;Using Source Code&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;Choose your preferred interface:&lt;/p&gt; 
&lt;h5&gt;üåê &lt;strong&gt;Web Interface&lt;/strong&gt; (Recommended)&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Using UV
uv run streamlit run ui/streamlit_app.py
# Or using traditional Python
streamlit run ui/streamlit_app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://img.shields.io/badge/Access-localhost:8501-00d4ff?style=flat-square&amp;amp;logo=streamlit&amp;amp;logoColor=white" alt="Web Access" /&gt; 
&lt;/div&gt; 
&lt;h5&gt;üñ•Ô∏è &lt;strong&gt;CLI Interface&lt;/strong&gt; (Advanced Users)&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Using UV
uv run python cli/main_cli.py
# Or using traditional Python
python cli/main_cli.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://img.shields.io/badge/Mode-Interactive_Terminal-9b59b6?style=flat-square&amp;amp;logo=terminal&amp;amp;logoColor=white" alt="CLI Mode" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;üéØ &lt;strong&gt;Step 3: Generate Code&lt;/strong&gt;&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;üìÑ Input&lt;/strong&gt;: Upload your research paper, provide requirements, or paste a URL&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ü§ñ Processing&lt;/strong&gt;: Watch the multi-agent system analyze and plan&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;‚ö° Output&lt;/strong&gt;: Receive production-ready code with tests and documentation&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üí° Examples&lt;/h2&gt; 
&lt;h3&gt;üé¨ &lt;strong&gt;Live Demonstrations&lt;/strong&gt;&lt;/h3&gt; 
&lt;table align="center"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="33%" align="center"&gt; &lt;h4&gt;üìÑ &lt;strong&gt;Paper2Code Demo&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Research to Implementation&lt;/strong&gt;&lt;/p&gt; 
    &lt;div align="center"&gt; 
     &lt;a href="https://www.youtube.com/watch?v=MQZYpLkzsbw"&gt; &lt;img src="https://img.youtube.com/vi/MQZYpLkzsbw/maxresdefault.jpg" alt="Paper2Code Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);" /&gt; &lt;/a&gt; 
     &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=MQZYpLkzsbw"&gt;‚ñ∂Ô∏è Watch Demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
     &lt;p&gt;&lt;em&gt;Transform academic papers into production-ready code automatically&lt;/em&gt;&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="33%" align="center"&gt; &lt;h4&gt;üñºÔ∏è &lt;strong&gt;Image Processing Demo&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;AI-Powered Image Tools&lt;/strong&gt;&lt;/p&gt; 
    &lt;div align="center"&gt; 
     &lt;a href="https://www.youtube.com/watch?v=nFt5mLaMEac"&gt; &lt;img src="https://img.youtube.com/vi/nFt5mLaMEac/maxresdefault.jpg" alt="Image Processing Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);" /&gt; &lt;/a&gt; 
     &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=nFt5mLaMEac"&gt;‚ñ∂Ô∏è Watch Demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
     &lt;p&gt;&lt;em&gt;Intelligent image processing with background removal and enhancement&lt;/em&gt;&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="33%" align="center"&gt; &lt;h4&gt;üåê &lt;strong&gt;Frontend Implementation&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Complete Web Application&lt;/strong&gt;&lt;/p&gt; 
    &lt;div align="center"&gt; 
     &lt;a href="https://www.youtube.com/watch?v=78wx3dkTaAU"&gt; &lt;img src="https://img.youtube.com/vi/78wx3dkTaAU/maxresdefault.jpg" alt="Frontend Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);" /&gt; &lt;/a&gt; 
     &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=78wx3dkTaAU"&gt;‚ñ∂Ô∏è Watch Demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
     &lt;p&gt;&lt;em&gt;Full-stack web development from concept to deployment&lt;/em&gt;&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;üÜï &lt;strong&gt;Recent Updates&lt;/strong&gt;&lt;/h3&gt; 
&lt;h4&gt;üìÑ &lt;strong&gt;Smart Document Segmentation (v1.2.0)&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent Processing&lt;/strong&gt;: Automatically handles large research papers and technical documents that exceed LLM token limits&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configurable Control&lt;/strong&gt;: Toggle segmentation via configuration with size-based thresholds&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Semantic Analysis&lt;/strong&gt;: Advanced content understanding with algorithm, concept, and formula preservation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backward Compatibility&lt;/strong&gt;: Seamlessly falls back to traditional processing for smaller documents&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üöÄ &lt;strong&gt;Coming Soon&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;We're continuously enhancing DeepCode with exciting new features:&lt;/p&gt; 
&lt;h4&gt;üîß &lt;strong&gt;Enhanced Code Reliability &amp;amp; Validation&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Automated Testing&lt;/strong&gt;: Comprehensive functionality testing with execution verification and error detection.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code Quality Assurance&lt;/strong&gt;: Multi-level validation through static analysis, dynamic testing, and performance benchmarking.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Debugging&lt;/strong&gt;: AI-powered error detection with automatic correction suggestions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;üìä &lt;strong&gt;PaperBench Performance Showcase&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Benchmark Dashboard&lt;/strong&gt;: Comprehensive performance metrics on the PaperBench evaluation suite.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Accuracy Metrics&lt;/strong&gt;: Detailed comparison with state-of-the-art paper reproduction systems.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Success Analytics&lt;/strong&gt;: Statistical analysis across paper categories and complexity levels.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;‚ö° &lt;strong&gt;System-wide Optimizations&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Performance Boost&lt;/strong&gt;: Multi-threaded processing and optimized agent coordination for faster generation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced Reasoning&lt;/strong&gt;: Advanced reasoning capabilities with improved context understanding.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Expanded Support&lt;/strong&gt;: Extended compatibility with additional programming languages and frameworks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚≠ê Star History&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;em&gt;Community Growth Trajectory&lt;/em&gt;&lt;/p&gt; 
 &lt;a href="https://star-history.com/#HKUDS/DeepCode&amp;amp;Date"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=HKUDS/DeepCode&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=HKUDS/DeepCode&amp;amp;type=Date" /&gt; 
   &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=HKUDS/DeepCode&amp;amp;type=Date" style="border-radius: 15px; box-shadow: 0 0 30px rgba(0, 217, 255, 0.3);" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üöÄ &lt;strong&gt;Ready to Transform Development?&lt;/strong&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-quick-start"&gt;&lt;img src="https://img.shields.io/badge/üöÄ_Get_Started-00d4ff?style=for-the-badge&amp;amp;logo=rocket&amp;amp;logoColor=white" alt="Get Started" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS"&gt;&lt;img src="https://img.shields.io/badge/üèõÔ∏è_View_on_GitHub-00d4ff?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" alt="View on GitHub" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/deepcode-agent"&gt;&lt;img src="https://img.shields.io/badge/‚≠ê_Star_Project-00d4ff?style=for-the-badge&amp;amp;logo=star&amp;amp;logoColor=white" alt="Star Project" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;üìÑ &lt;strong&gt;License&lt;/strong&gt;&lt;/h3&gt; 
 &lt;img src="https://img.shields.io/badge/License-MIT-4ecdc4?style=for-the-badge&amp;amp;logo=opensourceinitiative&amp;amp;logoColor=white" alt="MIT License" /&gt; 
 &lt;p&gt;&lt;strong&gt;MIT License&lt;/strong&gt; - Copyright (c) 2025 Data Intelligence Lab, The University of Hong Kong&lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;img src="https://visitor-badge.laobi.icu/badge?page_id=deepcode.readme&amp;amp;style=for-the-badge&amp;amp;color=00d4ff" alt="Visitors" /&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>murtaza-nasir/maestro</title>
      <link>https://github.com/murtaza-nasir/maestro</link>
      <description>&lt;p&gt;MAESTRO is an AI-powered research application designed to streamline complex research tasks.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/murtaza-nasir/maestro/main/images/logo.png" alt="MAESTRO Logo" width="200" /&gt; &lt;/p&gt; 
&lt;h1&gt;MAESTRO: Your Self-Hosted AI Research Assistant&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://www.gnu.org/licenses/agpl-3.0"&gt;&lt;img src="https://img.shields.io/badge/License-AGPL_v3-blue.svg?sanitize=true" alt="License: AGPL v3" /&gt;&lt;/a&gt; &lt;a href="https://github.com/murtaza-nasir/maestro.git"&gt;&lt;img src="https://img.shields.io/badge/Version-0.1.5--alpha-green.svg?sanitize=true" alt="Version" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/murtaza-nasir/maestro"&gt;&lt;img src="https://img.shields.io/badge/Docker-Ready-blue.svg?sanitize=true" alt="Docker" /&gt;&lt;/a&gt; &lt;a href="https://murtaza-nasir.github.io/maestro/"&gt;&lt;img src="https://img.shields.io/badge/Docs-Available-brightgreen.svg?sanitize=true" alt="Documentation" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Version 0.1.5-alpha (Sep 2, 2025) - Major Update&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: Complete async backend migration (2-3x faster)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Stability&lt;/strong&gt;: 50+ bug fixes and mission recovery improvements&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: Complete overhaul with example reports and guides&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;UI/UX&lt;/strong&gt;: Enhanced interface with LaTeX support and better navigation&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;MAESTRO is an AI-powered research platform you can host on your own hardware. It's designed to manage complex research tasks from start to finish in a collaborative research environment. Plan your research, let AI agents carry it out, and watch as they generate detailed reports based on your documents and sources from the web.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://murtaza-nasir.github.io/maestro/"&gt;View Full Documentation&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://murtaza-nasir.github.io/maestro/getting-started/quickstart/"&gt;Quick Start&lt;/a&gt;&lt;/strong&gt; - Get up and running in minutes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://murtaza-nasir.github.io/maestro/getting-started/installation/"&gt;Installation&lt;/a&gt;&lt;/strong&gt; - Platform-specific setup&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://murtaza-nasir.github.io/maestro/getting-started/configuration/overview/"&gt;Configuration&lt;/a&gt;&lt;/strong&gt; - AI providers and settings&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://murtaza-nasir.github.io/maestro/user-guide/"&gt;User Guide&lt;/a&gt;&lt;/strong&gt; - Complete feature guide&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://murtaza-nasir.github.io/maestro/example-reports/"&gt;Example Reports&lt;/a&gt;&lt;/strong&gt; - Sample outputs from various models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://murtaza-nasir.github.io/maestro/troubleshooting/"&gt;Troubleshooting&lt;/a&gt;&lt;/strong&gt; - Common issues and solutions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Screenshots&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/murtaza-nasir/maestro/main/images/10-research-draft.png" alt="Final Draft" width="700" /&gt; &lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Document Library&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/murtaza-nasir/maestro/main/images/01-document-library.png" alt="Document Library" width="700" /&gt; &lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Document Groups&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/murtaza-nasir/maestro/main/images/02-document-groups.png" alt="Document Groups" width="700" /&gt; &lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Mission Settings&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/murtaza-nasir/maestro/main/images/03-mission-settings.png" alt="Mission Settings" width="700" /&gt; &lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Chat Interface&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/murtaza-nasir/maestro/main/images/04-chat-with-docs.png" alt="Chat with Documents" width="700" /&gt; &lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Writing Assistant&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/murtaza-nasir/maestro/main/images/05-writing-assistant.png" alt="Writing Assistant" width="700" /&gt; &lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Research Transparency&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/murtaza-nasir/maestro/main/images/06-research-transparency.png" alt="Research Transparency" width="700" /&gt; &lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;AI-Generated Notes&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/murtaza-nasir/maestro/main/images/07-automated-notes.png" alt="Automated Notes" width="700" /&gt; &lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Mission Tracking&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/murtaza-nasir/maestro/main/images/08-mission-tracking.png" alt="Mission Tracking" width="700" /&gt; &lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Agent Reflection&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/murtaza-nasir/maestro/main/images/09-agent-reflection.png" alt="Agent Reflection" width="700" /&gt; &lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Docker and Docker Compose (v2.0+)&lt;/li&gt; 
 &lt;li&gt;16GB RAM minimum (32GB recommended)&lt;/li&gt; 
 &lt;li&gt;30GB free disk space&lt;/li&gt; 
 &lt;li&gt;API keys for at least one AI provider&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone and setup
git clone https://github.com/murtaza-nasir/maestro.git
cd maestro
./setup-env.sh    # Linux/macOS
# or
.\setup-env.ps1   # Windows PowerShell

# Start services
docker compose up -d

# Monitor startup (takes 5-10 minutes first time)
docker compose logs -f maestro-backend
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Access at &lt;strong&gt;&lt;a href="http://localhost"&gt;http://localhost&lt;/a&gt;&lt;/strong&gt; ‚Ä¢ Default: &lt;code&gt;admin&lt;/code&gt; / &lt;code&gt;pass found in .env&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;For detailed installation instructions, see the &lt;a href="https://murtaza-nasir.github.io/maestro/getting-started/installation/"&gt;Installation Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Configuration&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;CPU Mode&lt;/strong&gt;: Use &lt;code&gt;docker compose -f docker-compose.cpu.yml up -d&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GPU Support&lt;/strong&gt;: Automatic detection on Linux/Windows with NVIDIA GPUs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Network Access&lt;/strong&gt;: Configure via setup script options&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For troubleshooting and advanced configuration, see the &lt;a href="https://murtaza-nasir.github.io/maestro/"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Core Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Agent Research System&lt;/strong&gt;: Planning, Research, Reflection, and Writing agents working in concert&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced RAG Pipeline&lt;/strong&gt;: Dual BGE-M3 embeddings with PostgreSQL + pgvector&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Document Management&lt;/strong&gt;: PDF, Word, and Markdown support with semantic search&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Web Integration&lt;/strong&gt;: Multiple search providers (Tavily, LinkUp, Jina, SearXNG)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Self-Hosted&lt;/strong&gt;: Complete control over your data and infrastructure&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Local LLM Support&lt;/strong&gt;: OpenAI-compatible API for running your own models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is &lt;strong&gt;dual-licensed&lt;/strong&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;GNU Affero General Public License v3.0 (AGPLv3)&lt;/strong&gt;: MAESTRO is offered under the AGPLv3 as its open-source license.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Commercial License&lt;/strong&gt;: For users or organizations who cannot comply with the AGPLv3, a separate commercial license is available. Please contact the maintainers for more details.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Feedback, bug reports, and feature suggestions are highly valuable. Please feel free to open an Issue on the GitHub repository.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>oraios/serena</title>
      <link>https://github.com/oraios/serena</link>
      <description>&lt;p&gt;A powerful coding agent toolkit providing semantic retrieval and editing capabilities (MCP server &amp; other integrations)&lt;/p&gt;&lt;hr&gt;&lt;p align="center" style="text-align:center"&gt; &lt;img src="https://raw.githubusercontent.com/oraios/serena/main/resources/serena-logo.svg#gh-light-mode-only" style="width:500px" /&gt; &lt;img src="https://raw.githubusercontent.com/oraios/serena/main/resources/serena-logo-dark-mode.svg#gh-dark-mode-only" style="width:500px" /&gt; &lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span&gt;üöÄ&lt;/span&gt; Serena is a powerful &lt;strong&gt;coding agent toolkit&lt;/strong&gt; capable of turning an LLM into a fully-featured agent that works &lt;strong&gt;directly on your codebase&lt;/strong&gt;. Unlike most other tools, it is not tied to an LLM, framework or an interface, making it easy to use it in a variety of ways.&lt;/li&gt; 
 &lt;li&gt;&lt;span&gt;üîß&lt;/span&gt; Serena provides essential &lt;strong&gt;semantic code retrieval and editing tools&lt;/strong&gt; that are akin to an IDE's capabilities, extracting code entities at the symbol level and exploiting relational structure. When combined with an existing coding agent, these tools greatly enhance (token) efficiency.&lt;/li&gt; 
 &lt;li&gt;&lt;span&gt;üÜì&lt;/span&gt; Serena is &lt;strong&gt;free &amp;amp; open-source&lt;/strong&gt;, enhancing the capabilities of LLMs you already have access to free of charge.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can think of Serena as providing IDE-like tools to your LLM/coding agent. With it, the agent no longer needs to read entire files, perform grep-like searches or string replacements to find and edit the right code. Instead, it can use code centered tools like &lt;code&gt;find_symbol&lt;/code&gt;, &lt;code&gt;find_referencing_symbols&lt;/code&gt; and &lt;code&gt;insert_after_symbol&lt;/code&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;em&gt;Serena is under active development! See the latest updates, upcoming features, and lessons learned to stay up to date.&lt;/em&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/CHANGELOG.md"&gt; &lt;img src="https://img.shields.io/badge/Updates-1e293b?style=flat&amp;amp;logo=rss&amp;amp;logoColor=white&amp;amp;labelColor=1e293b" alt="Changelog" /&gt; &lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/roadmap.md"&gt; &lt;img src="https://img.shields.io/badge/Roadmap-14532d?style=flat&amp;amp;logo=target&amp;amp;logoColor=white&amp;amp;labelColor=14532d" alt="Roadmap" /&gt; &lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/lessons_learned.md"&gt; &lt;img src="https://img.shields.io/badge/Lessons-Learned-7c4700?style=flat&amp;amp;logo=readthedocs&amp;amp;logoColor=white&amp;amp;labelColor=7c4700" alt="Lessons Learned" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h3&gt;LLM Integration&lt;/h3&gt; 
&lt;p&gt;Serena provides the necessary &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#list-of-tools"&gt;tools&lt;/a&gt; for coding workflows, but an LLM is required to do the actual work, orchestrating tool use.&lt;/p&gt; 
&lt;p&gt;For example, &lt;strong&gt;supercharge the performance of Claude Code&lt;/strong&gt; with a &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#claude-code"&gt;one-line shell command&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In general, Serena can be integrated with an LLM in several ways:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;by using the &lt;strong&gt;model context protocol (MCP)&lt;/strong&gt;. Serena provides an MCP server which integrates with 
  &lt;ul&gt; 
   &lt;li&gt;Claude Code and Claude Desktop,&lt;/li&gt; 
   &lt;li&gt;Terminal-based clients like Codex, Gemini-CLI, Qwen3-Coder, rovodev, OpenHands CLI and others,&lt;/li&gt; 
   &lt;li&gt;IDEs like VSCode, Cursor or IntelliJ,&lt;/li&gt; 
   &lt;li&gt;Extensions like Cline or Roo Code&lt;/li&gt; 
   &lt;li&gt;Local clients like &lt;a href="https://docs.openwebui.com/openapi-servers/mcp"&gt;OpenWebUI&lt;/a&gt;, &lt;a href="https://jan.ai/docs/mcp-examples/browser/browserbase#enable-mcp"&gt;Jan&lt;/a&gt;, &lt;a href="https://docs.agno.com/introduction/playground"&gt;Agno&lt;/a&gt; and others&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;by using &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/docs/serena_on_chatgpt.md"&gt;mcpo to connect it to ChatGPT&lt;/a&gt; or other clients that don't support MCP but do support tool calling via OpenAPI.&lt;/li&gt; 
 &lt;li&gt;by incorporating Serena's tools into an agent framework of your choice, as illustrated &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/docs/custom_agent.md"&gt;here&lt;/a&gt;. Serena's tool implementation is decoupled from the framework-specific code and can thus easily be adapted to any agent framework.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Serena in Action&lt;/h3&gt; 
&lt;h4&gt;Demonstration 1: Efficient Operation in Claude Code&lt;/h4&gt; 
&lt;p&gt;A demonstration of Serena efficiently retrieving and editing code within Claude Code, thereby saving tokens and time. Efficient operations are not only useful for saving costs, but also for generally improving the generated code's quality. This effect may be less pronounced in very small projects, but often becomes of crucial importance in larger ones.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/ab78ebe0-f77d-43cc-879a-cc399efefd87"&gt;https://github.com/user-attachments/assets/ab78ebe0-f77d-43cc-879a-cc399efefd87&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Demonstration 2: Serena in Claude Desktop&lt;/h4&gt; 
&lt;p&gt;A demonstration of Serena implementing a small feature for itself (a better log GUI) with Claude Desktop. Note how Serena's tools enable Claude to find and edit the right symbols.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/6eaa9aa1-610d-4723-a2d6-bf1e487ba753"&gt;https://github.com/user-attachments/assets/6eaa9aa1-610d-4723-a2d6-bf1e487ba753&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Programming Language Support &amp;amp; Semantic Analysis Capabilities&lt;/h3&gt; 
&lt;p&gt;Serena's semantic code analysis capabilities build on &lt;strong&gt;language servers&lt;/strong&gt; using the widely implemented language server protocol (LSP). The LSP provides a set of versatile code querying and editing functionalities based on symbolic understanding of the code. Equipped with these capabilities, Serena discovers and edits code just like a seasoned developer making use of an IDE's capabilities would. Serena can efficiently find the right context and do the right thing even in very large and complex projects! So not only is it free and open-source, it frequently achieves better results than existing solutions that charge a premium.&lt;/p&gt; 
&lt;p&gt;Language servers provide support for a wide range of programming languages. With Serena, we provide direct, out-of-the-box support for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python&lt;/li&gt; 
 &lt;li&gt;TypeScript/Javascript&lt;/li&gt; 
 &lt;li&gt;PHP (uses Intelephense LSP; set &lt;code&gt;INTELEPHENSE_LICENSE_KEY&lt;/code&gt; environment variable for premium features)&lt;/li&gt; 
 &lt;li&gt;Go (requires installation of gopls)&lt;/li&gt; 
 &lt;li&gt;R (requires installation of the &lt;code&gt;languageserver&lt;/code&gt; R package)&lt;/li&gt; 
 &lt;li&gt;Rust (requires &lt;a href="https://rustup.rs/"&gt;rustup&lt;/a&gt; - uses rust-analyzer from your toolchain)&lt;/li&gt; 
 &lt;li&gt;C/C++ (you may experience issues with finding references, we are working on it)&lt;/li&gt; 
 &lt;li&gt;Zig (requires installation of ZLS - Zig Language Server)&lt;/li&gt; 
 &lt;li&gt;C#&lt;/li&gt; 
 &lt;li&gt;Ruby (by default, uses &lt;a href="https://github.com/Shopify/ruby-lsp"&gt;ruby-lsp&lt;/a&gt;, specify ruby_solargraph as your language to use the previous solargraph based implementation)&lt;/li&gt; 
 &lt;li&gt;Swift&lt;/li&gt; 
 &lt;li&gt;Kotlin (uses the pre-alpha &lt;a href="https://github.com/Kotlin/kotlin-lsp"&gt;official kotlin LS&lt;/a&gt;, some issues may appear)&lt;/li&gt; 
 &lt;li&gt;Java (&lt;em&gt;Note&lt;/em&gt;: startup is slow, initial startup especially so. There may be issues with java on macos and linux, we are working on it.)&lt;/li&gt; 
 &lt;li&gt;Clojure&lt;/li&gt; 
 &lt;li&gt;Dart&lt;/li&gt; 
 &lt;li&gt;Bash&lt;/li&gt; 
 &lt;li&gt;Lua (automatically downloads lua-language-server if not installed)&lt;/li&gt; 
 &lt;li&gt;Nix (requires nixd installation)&lt;/li&gt; 
 &lt;li&gt;Elixir (requires installation of NextLS and Elixir; &lt;strong&gt;Windows not supported&lt;/strong&gt;)&lt;/li&gt; 
 &lt;li&gt;Erlang (requires installation of beam and &lt;a href="https://github.com/erlang-ls/erlang_ls"&gt;erlang_ls&lt;/a&gt;, experimental, might be slow or hang)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Support for further languages can easily be added by providing a shallow adapter for a new language server implementation, see Serena's &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/.serena/memories/adding_new_language_support_guide.md"&gt;memory on that&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Community Feedback&lt;/h3&gt; 
&lt;p&gt;Most users report that Serena has strong positive effects on the results of their coding agents, even when used within very capable agents like Claude Code. Serena is often described to be a &lt;a href="https://www.reddit.com/r/ClaudeAI/comments/1lfsdll/try_out_serena_mcp_thank_me_later/"&gt;game changer&lt;/a&gt;, providing an enormous &lt;a href="https://www.reddit.com/r/ClaudeCode/comments/1mguoia/absolutely_insane_improvement_of_claude_code"&gt;productivity boost&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Serena excels at navigating and manipulating complex codebases, providing tools that support precise code retrieval and editing in the presence of large, strongly structured codebases. However, when dealing with tasks that involve only very few/small files, you may not benefit from including Serena on top of your existing coding agent. In particular, when writing code from scratch, Serena will not provide much value initially, as the more complex structures that Serena handles more gracefully than simplistic, file-based approaches are yet to be created.&lt;/p&gt; 
&lt;p&gt;Several videos and blog posts have talked about Serena:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;YouTube:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=wYWyJNs1HVk&amp;amp;t=1s"&gt;AI Labs&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=UqfxuQKuMo8&amp;amp;t=45s"&gt;Yo Van Eyck&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=fzPnM3ySmjE&amp;amp;t=32s"&gt;JeredBlu&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Blog posts:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://medium.com/@souradip1000/deconstructing-serenas-mcp-powered-semantic-code-understanding-architecture-75802515d116"&gt;Serena's Design Principles&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://blog.lai.so/serena/"&gt;Serena with Claude Code (in Japanese)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://robertmarshall.dev/blog/turning-claude-code-into-a-development-powerhouse/"&gt;Turning Claude Code into a Development Powerhouse&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;!-- Created with markdown-toc -i README.md --&gt; 
&lt;!-- Install it with npm install -g markdown-toc --&gt; 
&lt;!-- toc --&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#quick-start"&gt;Quick Start&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#running-the-serena-mcp-server"&gt;Running the Serena MCP Server&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#usage"&gt;Usage&lt;/a&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#using-uvx"&gt;Using uvx&lt;/a&gt; 
        &lt;ul&gt; 
         &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#local-installation"&gt;Local Installation&lt;/a&gt;&lt;/li&gt; 
        &lt;/ul&gt; &lt;/li&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#using-docker-experimental"&gt;Using Docker (Experimental)&lt;/a&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#sse-mode"&gt;SSE Mode&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#command-line-arguments"&gt;Command-Line Arguments&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#configuration"&gt;Configuration&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#project-activation--indexing"&gt;Project Activation &amp;amp; Indexing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#claude-code"&gt;Claude Code&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#codex"&gt;Codex&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#other-terminal-based-clients"&gt;Other Terminal-Based Clients&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#claude-desktop"&gt;Claude Desktop&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#mcp-coding-clients-cline-roo-code-cursor-windsurf-etc"&gt;MCP Coding Clients (Cline, Roo-Code, Cursor, Windsurf, etc.)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#local-guis-and-frameworks"&gt;Local GUIs and Frameworks&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#detailed-usage-and-recommendations"&gt;Detailed Usage and Recommendations&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#tool-execution"&gt;Tool Execution&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#shell-execution-and-editing-tools"&gt;Shell Execution and Editing Tools&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#modes-and-contexts"&gt;Modes and Contexts&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#contexts"&gt;Contexts&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#modes"&gt;Modes&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#customization"&gt;Customization&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#onboarding-and-memories"&gt;Onboarding and Memories&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#prepare-your-project"&gt;Prepare Your Project&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#structure-your-codebase"&gt;Structure Your Codebase&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#start-from-a-clean-state"&gt;Start from a Clean State&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#logging-linting-and-automated-tests"&gt;Logging, Linting, and Automated Tests&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#prompting-strategies"&gt;Prompting Strategies&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#potential-issues-in-code-editing"&gt;Potential Issues in Code Editing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#running-out-of-context"&gt;Running Out of Context&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#combining-serena-with-other-mcp-servers"&gt;Combining Serena with Other MCP Servers&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#serenas-logs-the-dashboard-and-gui-tool"&gt;Serena's Logs: The Dashboard and GUI Tool&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#troubleshooting"&gt;Troubleshooting&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#comparison-with-other-coding-agents"&gt;Comparison with Other Coding Agents&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#subscription-based-coding-agents"&gt;Subscription-Based Coding Agents&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#api-based-coding-agents"&gt;API-Based Coding Agents&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#other-mcp-based-coding-agents"&gt;Other MCP-Based Coding Agents&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#acknowledgements"&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#customizing-and-extending-serena"&gt;Customizing and Extending Serena&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#list-of-tools"&gt;List of Tools&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- tocstop --&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;Serena can be used in various ways, below you will find instructions for selected integrations.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For coding with Claude, we recommend using Serena through &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#claude-code"&gt;Claude Code&lt;/a&gt; or &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#claude-desktop"&gt;Claude Desktop&lt;/a&gt;. You can also use Serena in most other &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#other-terminal-based-clients"&gt;terminal-based clients&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;If you want a GUI experience outside an IDE, you can use one of the many &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#local-guis-and-frameworks"&gt;local GUIs&lt;/a&gt; that support MCP servers. You can also connect Serena to many web clients (including ChatGPT) using &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/docs/serena_on_chatgpt.md"&gt;mcpo&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;If you want to use Serena integrated in your IDE, see the section on &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#other-mcp-clients---cline-roo-code-cursor-windsurf-etc"&gt;other MCP clients&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;You can use Serena as a library for building your own applications. We try to keep the public API stable, but you should still expect breaking changes and pin Serena to a fixed version if you use it as a dependency.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Serena is managed by &lt;code&gt;uv&lt;/code&gt;, so you will need to &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;install it&lt;/a&gt;).&lt;/p&gt; 
&lt;h3&gt;Running the Serena MCP Server&lt;/h3&gt; 
&lt;p&gt;You have several options for running the MCP server, which are explained in the subsections below.&lt;/p&gt; 
&lt;h4&gt;Usage&lt;/h4&gt; 
&lt;p&gt;The typical usage involves the client (Claude Code, Claude Desktop, etc.) running the MCP server as a subprocess (using stdio communication), so the client needs to be provided with the command to run the MCP server. (Alternatively, you can run the MCP server in SSE mode and tell your client how to connect to it.)&lt;/p&gt; 
&lt;p&gt;Note that no matter how you run the MCP server, Serena will, by default, start a small web-based dashboard on localhost that will display logs and allow shutting down the MCP server (since many clients fail to clean up processes correctly). This and other settings can be adjusted in the &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#configuration"&gt;configuration&lt;/a&gt; and/or by providing &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#command-line-arguments"&gt;command-line arguments&lt;/a&gt;.&lt;/p&gt; 
&lt;h5&gt;Using uvx&lt;/h5&gt; 
&lt;p&gt;&lt;code&gt;uvx&lt;/code&gt; can be used to run the latest version of Serena directly from the repository, without an explicit local installation.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;uvx --from git+https://github.com/oraios/serena serena start-mcp-server
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Explore the CLI to see some of the customization options that serena provides (more info on them below).&lt;/p&gt; 
&lt;h6&gt;Local Installation&lt;/h6&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Clone the repository and change into it.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/oraios/serena
cd serena
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Optionally edit the configuration file in your home directory with&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;uv run serena config edit
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you just want the default config, you can skip this part, and a config file will be created when you first run Serena.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the server with &lt;code&gt;uv&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;uv run serena start-mcp-server
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When running from outside the serena installation directory, be sure to pass it, i.e., use&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt; uv run --directory /abs/path/to/serena serena start-mcp-server
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h5&gt;Using Docker (Experimental)&lt;/h5&gt; 
&lt;p&gt;‚ö†Ô∏è Docker support is currently experimental with several limitations. Please read the &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/DOCKER.md"&gt;Docker documentation&lt;/a&gt; for important caveats before using it.&lt;/p&gt; 
&lt;p&gt;You can run the Serena MCP server directly via docker as follows, assuming that the projects you want to work on are all located in &lt;code&gt;/path/to/your/projects&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;docker run --rm -i --network host -v /path/to/your/projects:/workspaces/projects ghcr.io/oraios/serena:latest serena start-mcp-server --transport stdio
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Replace &lt;code&gt;/path/to/your/projects&lt;/code&gt; with the absolute path to your projects directory. The Docker approach provides:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Better security isolation for shell command execution&lt;/li&gt; 
 &lt;li&gt;No need to install language servers and dependencies locally&lt;/li&gt; 
 &lt;li&gt;Consistent environment across different systems&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Alternatively, use docker compose with the &lt;code&gt;compose.yml&lt;/code&gt; file provided in the repository.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/DOCKER.md"&gt;Docker documentation&lt;/a&gt; for detailed setup instructions, configuration options, and known limitations.&lt;/p&gt; 
&lt;h5&gt;Using Nix&lt;/h5&gt; 
&lt;p&gt;If you are using Nix and &lt;a href="https://nixos.wiki/wiki/flakes"&gt;have enabled the &lt;code&gt;nix-command&lt;/code&gt; and &lt;code&gt;flakes&lt;/code&gt; features&lt;/a&gt;, you can run Serena using the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;nix run github:oraios/serena -- start-mcp-server --transport stdio
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also install Serena by referencing this repo (&lt;code&gt;github:oraios/serena&lt;/code&gt;) and using it in your Nix flake. The package is exported as &lt;code&gt;serena&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;SSE Mode&lt;/h4&gt; 
&lt;p&gt;‚ÑπÔ∏è Note that MCP servers which use stdio as a protocol are somewhat unusual as far as client/server architectures go, as the server necessarily has to be started by the client in order for communication to take place via the server's standard input/output stream. In other words, you do not need to start the server yourself. The client application (e.g. Claude Desktop) takes care of this and therefore needs to be configured with a launch command.&lt;/p&gt; 
&lt;p&gt;When using instead the SSE mode, which uses HTTP-based communication, you control the server lifecycle yourself, i.e. you start the server and provide the client with the URL to connect to it.&lt;/p&gt; 
&lt;p&gt;Simply provide &lt;code&gt;start-mcp-server&lt;/code&gt; with the &lt;code&gt;--transport sse&lt;/code&gt; option and optionally provide the port. For example, to run the Serena MCP server in SSE mode on port 9121 using a local installation, you would run this command from the Serena directory,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;uv run serena start-mcp-server --transport sse --port 9121
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and then configure your client to connect to &lt;code&gt;http://localhost:9121/sse&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;Command-Line Arguments&lt;/h4&gt; 
&lt;p&gt;The Serena MCP server supports a wide range of additional command-line options, including the option to run in SSE mode and to adapt Serena to various &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#modes-and-contexts"&gt;contexts and modes of operation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Run with parameter &lt;code&gt;--help&lt;/code&gt; to get a list of available options.&lt;/p&gt; 
&lt;h3&gt;Configuration&lt;/h3&gt; 
&lt;p&gt;Serena is very flexible in terms of configuration. While for most users, the default configurations will work, you can fully adjust it to your needs by editing a few yaml files. You can disable tools, change Serena's instructions (what we denote as the &lt;code&gt;system_prompt&lt;/code&gt;), adjust the output of tools that just provide a prompt, and even adjust tool descriptions.&lt;/p&gt; 
&lt;p&gt;Serena is configured in four places:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;The &lt;code&gt;serena_config.yml&lt;/code&gt; for general settings that apply to all clients and projects. It is located in your user directory under &lt;code&gt;.serena/serena_config.yml&lt;/code&gt;. If you do not explicitly create the file, it will be auto-generated when you first run Serena. You can edit it directly or use&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;uvx --from git+https://github.com/oraios/serena serena config edit
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(or use the &lt;code&gt;--directory&lt;/code&gt; command version).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;In the arguments passed to the &lt;code&gt;start-mcp-server&lt;/code&gt; in your client's config (see below), which will apply to all sessions started by the respective client. In particular, the &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#contexts"&gt;context&lt;/a&gt; parameter should be set appropriately for Serena to be best adjusted to existing tools and capabilities of your client. See for a detailed explanation. You can override all entries from the &lt;code&gt;serena_config.yml&lt;/code&gt; through command line arguments.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;In the &lt;code&gt;.serena/project.yml&lt;/code&gt; file within your project. This will hold project-level configuration that is used whenever that project is activated. This file will be autogenerated when you first use Serena on that project, but you can also generate it explicitly with&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;uvx --from git+https://github.com/oraios/serena serena project generate-yml
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(or use the &lt;code&gt;--directory&lt;/code&gt; command version).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Through the context and modes. Explore the &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#modes-and-contexts"&gt;modes and contexts&lt;/a&gt; section for more details.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;After the initial setup, continue with one of the sections below, depending on how you want to use Serena.&lt;/p&gt; 
&lt;h3&gt;Project Activation &amp;amp; Indexing&lt;/h3&gt; 
&lt;p&gt;If you are mostly working with the same project, you can configure to always activate it at startup by passing &lt;code&gt;--project &amp;lt;path_or_name&amp;gt;&lt;/code&gt; to the &lt;code&gt;start-mcp-server&lt;/code&gt; command in your client's MCP config. This is especially useful for clients which configure MCP servers on a per-project basis, like Claude Code.&lt;/p&gt; 
&lt;p&gt;Otherwise, the recommended way is to just ask the LLM to activate a project by providing it an absolute path to, or, in case the project was activated in the past, by its name. The default project name is the directory name.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;"Activate the project /path/to/my_project"&lt;/li&gt; 
 &lt;li&gt;"Activate the project my_project"&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;All projects that have been activated will be automatically added to your &lt;code&gt;serena_config.yml&lt;/code&gt;, and for each project, the file &lt;code&gt;.serena/project.yml&lt;/code&gt; will be generated. You can adjust the latter, e.g., by changing the name (which you refer to during the activation) or other options. Make sure to not have two different projects with the same name.&lt;/p&gt; 
&lt;p&gt;‚ÑπÔ∏è For larger projects, we recommend that you index your project to accelerate Serena's tools; otherwise the first tool application may be very slow. To do so, run this from the project directory (or pass the path to the project as an argument):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;uvx --from git+https://github.com/oraios/serena serena project index
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;(or use the &lt;code&gt;--directory&lt;/code&gt; command version).&lt;/p&gt; 
&lt;h3&gt;Claude Code&lt;/h3&gt; 
&lt;p&gt;Serena is a great way to make Claude Code both cheaper and more powerful!&lt;/p&gt; 
&lt;p&gt;From your project directory, add serena with a command like this,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;claude mcp add serena -- &amp;lt;serena-mcp-server&amp;gt; --context ide-assistant --project $(pwd)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;where &lt;code&gt;&amp;lt;serena-mcp-server&amp;gt;&lt;/code&gt; is your way of &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#running-the-serena-mcp-server"&gt;running the Serena MCP server&lt;/a&gt;. For example, when using &lt;code&gt;uvx&lt;/code&gt;, you would run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;claude mcp add serena -- uvx --from git+https://github.com/oraios/serena serena start-mcp-server --context ide-assistant --project $(pwd)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;‚ÑπÔ∏è Serena comes with an instruction text, and Claude needs to read it to properly use Serena's tools. As of version &lt;code&gt;v1.0.52&lt;/code&gt;, claude code reads the instructions of the MCP server, so this &lt;strong&gt;is handled automatically&lt;/strong&gt;. If you are using an older version, or if Claude fails to read the instructions, you can ask it explicitly to "read Serena's initial instructions" or run &lt;code&gt;/mcp__serena__initial_instructions&lt;/code&gt; to load the instruction text. If you want to make use of that, you will have to enable the corresponding tool explicitly by adding &lt;code&gt;initial_instructions&lt;/code&gt; to the &lt;code&gt;included_optional_tools&lt;/code&gt; in your config. Note that you may have to make Claude read the instructions when you start a new conversation and after any compacting operation to ensure Claude remains properly configured to use Serena's tools.&lt;/p&gt; 
&lt;h3&gt;Codex&lt;/h3&gt; 
&lt;p&gt;Serena works with OpenAI's Codex CLI out of the box, but you have to use the &lt;code&gt;codex&lt;/code&gt; context for it to work properly. (The technical reason is that Codex doesn't fully support the MCP specifications, so some massaging of tools is required.).&lt;/p&gt; 
&lt;p&gt;Unlike Claude Code, in Codex you add an MCP server globally and not per project. Add the following to &lt;code&gt;~/.codex/config.toml&lt;/code&gt; (create the file if it does not exist):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;[mcp_servers.serena]
command = "uvx"
args = ["--from", "git+https://github.com/oraios/serena", "serena", "start-mcp-server", "--context", "codex"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After codex has started, you need to activate the project, which you can do by saying:&lt;/p&gt; 
&lt;p&gt;"Activate the current dir as project using serena"&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;If you don't activate the project, you will not be able to use Serena's tools!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;That's it! Have a look at &lt;code&gt;~/.codex/log/codex-tui.log&lt;/code&gt; to see if any errors occurred.&lt;/p&gt; 
&lt;p&gt;The Serena dashboard will run if you have not disabled it in the configuration, but due to Codex's sandboxing the webbrowser may not open automatically. You can open it manually by going to &lt;code&gt;http://localhost:24282/dashboard/index.html&lt;/code&gt; (or a higher port, if that was already taken).&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Codex will often show the tools as &lt;code&gt;failed&lt;/code&gt; even though they are successfully executed. This is not a problem, seems to be a bug in Codex. Despite the error message, everything works as expected.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Other Terminal-Based Clients&lt;/h3&gt; 
&lt;p&gt;There are many terminal-based coding assistants that support MCP servers, such as &lt;a href="https://github.com/openai/codex?tab=readme-ov-file#model-context-protocol-mcp"&gt;Codex&lt;/a&gt;, &lt;a href="https://github.com/google-gemini/gemini-cli"&gt;Gemini-CLI&lt;/a&gt;, &lt;a href="https://github.com/QwenLM/Qwen3-Coder"&gt;Qwen3-Coder&lt;/a&gt;, &lt;a href="https://community.atlassian.com/forums/Rovo-for-Software-Teams-Beta/Introducing-Rovo-Dev-CLI-AI-Powered-Development-in-your-terminal/ba-p/3043623"&gt;rovodev&lt;/a&gt;, the &lt;a href="https://docs.all-hands.dev/usage/how-to/cli-mode"&gt;OpenHands CLI&lt;/a&gt; and &lt;a href="https://github.com/sst/opencode"&gt;opencode&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;They generally benefit from the symbolic tools provided by Serena. You might want to customize some aspects of Serena by writing your own context, modes or prompts to adjust it to your workflow, to other MCP servers you are using, and to the client's internal capabilities.&lt;/p&gt; 
&lt;h3&gt;Claude Desktop&lt;/h3&gt; 
&lt;p&gt;For &lt;a href="https://claude.ai/download"&gt;Claude Desktop&lt;/a&gt; (available for Windows and macOS), go to File / Settings / Developer / MCP Servers / Edit Config, which will let you open the JSON file &lt;code&gt;claude_desktop_config.json&lt;/code&gt;. Add the &lt;code&gt;serena&lt;/code&gt; MCP server configuration, using a &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#running-the-serena-mcp-server"&gt;run command&lt;/a&gt; depending on your setup.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;local installation:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-json"&gt;{
    "mcpServers": {
        "serena": {
            "command": "/abs/path/to/uv",
            "args": ["run", "--directory", "/abs/path/to/serena", "serena", "start-mcp-server"]
        }
    }
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;uvx:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-json"&gt;{
    "mcpServers": {
        "serena": {
            "command": "/abs/path/to/uvx",
            "args": ["--from", "git+https://github.com/oraios/serena", "serena", "start-mcp-server"]
        }
    }
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;docker:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-json"&gt; {
     "mcpServers": {
         "serena": {
             "command": "docker",
             "args": ["run", "--rm", "-i", "--network", "host", "-v", "/path/to/your/projects:/workspaces/projects", "ghcr.io/oraios/serena:latest", "serena", "start-mcp-server", "--transport", "stdio"]
         }
     }
 }
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you are using paths containing backslashes for paths on Windows (note that you can also just use forward slashes), be sure to escape them correctly (&lt;code&gt;\\&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;That's it! Save the config and then restart Claude Desktop. You are ready for activating your first project.&lt;/p&gt; 
&lt;p&gt;‚ÑπÔ∏è You can further customize the run command using additional arguments (see &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#command-line-arguments"&gt;above&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;Note: on Windows and macOS there are official Claude Desktop applications by Anthropic, for Linux there is an &lt;a href="https://github.com/aaddrick/claude-desktop-debian"&gt;open-source community version&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;‚ö†Ô∏è Be sure to fully quit the Claude Desktop application, as closing Claude will just minimize it to the system tray ‚Äì at least on Windows.&lt;/p&gt; 
&lt;p&gt;‚ö†Ô∏è Some clients may leave behind zombie processes. You will have to find and terminate them manually then. With Serena, you can activate the &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#serenas-logs-the-dashboard-and-gui-tool"&gt;dashboard&lt;/a&gt; to prevent unnoted processes and also use the dashboard for shutting down Serena.&lt;/p&gt; 
&lt;p&gt;After restarting, you should see Serena's tools in your chat interface (notice the small hammer icon).&lt;/p&gt; 
&lt;p&gt;For more information on MCP servers with Claude Desktop, see &lt;a href="https://modelcontextprotocol.io/quickstart/user"&gt;the official quick start guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;MCP Coding Clients (Cline, Roo-Code, Cursor, Windsurf, etc.)&lt;/h3&gt; 
&lt;p&gt;Being an MCP Server, Serena can be included in any MCP Client. The same configuration as above, perhaps with small client-specific modifications, should work. Most of the popular existing coding assistants (IDE extensions or VSCode-like IDEs) support connections to MCP Servers. It is &lt;strong&gt;recommended to use the &lt;code&gt;ide-assistant&lt;/code&gt; context&lt;/strong&gt; for these integrations by adding &lt;code&gt;"--context", "ide-assistant"&lt;/code&gt; to the &lt;code&gt;args&lt;/code&gt; in your MCP client's configuration. Including Serena generally boosts their performance by providing them tools for symbolic operations.&lt;/p&gt; 
&lt;p&gt;In this case, the billing for the usage continues to be controlled by the client of your choice (unlike with the Claude Desktop client). But you may still want to use Serena through such an approach, e.g., for one of the following reasons:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;You are already using a coding assistant (say Cline or Cursor) and just want to make it more powerful.&lt;/li&gt; 
 &lt;li&gt;You are on Linux and don't want to use the &lt;a href="https://github.com/aaddrick/claude-desktop-debian"&gt;community-created Claude Desktop&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;You want tighter integration of Serena into your IDE and don't mind paying for that.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Local GUIs and Frameworks&lt;/h3&gt; 
&lt;p&gt;Over the last months, several technologies have emerged that allow you to run a powerful local GUI and connect it to an MCP server. They will work with Serena out of the box. Some of the leading open source GUI technologies offering this are &lt;a href="https://jan.ai/docs/mcp"&gt;Jan&lt;/a&gt;, &lt;a href="https://github.com/All-Hands-AI/OpenHands/"&gt;OpenHands&lt;/a&gt;, &lt;a href="https://docs.openwebui.com/openapi-servers/mcp"&gt;OpenWebUI&lt;/a&gt; and &lt;a href="https://docs.agno.com/introduction/playground"&gt;Agno&lt;/a&gt;. They allow combining Serena with almost any LLM (including locally running ones) and offer various other integrations.&lt;/p&gt; 
&lt;h2&gt;Detailed Usage and Recommendations&lt;/h2&gt; 
&lt;h3&gt;Tool Execution&lt;/h3&gt; 
&lt;p&gt;Serena combines tools for semantic code retrieval with editing capabilities and shell execution. Serena's behavior can be further customized through &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#modes-and-contexts"&gt;Modes and Contexts&lt;/a&gt;. Find the complete list of tools &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#full-list-of-tools"&gt;below&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The use of all tools is generally recommended, as this allows Serena to provide the most value: Only by executing shell commands (in particular, tests) can Serena identify and correct mistakes autonomously.&lt;/p&gt; 
&lt;h4&gt;Shell Execution and Editing Tools&lt;/h4&gt; 
&lt;p&gt;However, it should be noted that the &lt;code&gt;execute_shell_command&lt;/code&gt; tool allows for arbitrary code execution. When using Serena as an MCP Server, clients will typically ask the user for permission before executing a tool, so as long as the user inspects execution parameters beforehand, this should not be a problem. However, if you have concerns, you can choose to disable certain commands in your project's .yml configuration file. If you only want to use Serena purely for analyzing code and suggesting implementations without modifying the codebase, you can enable read-only mode by setting &lt;code&gt;read_only: true&lt;/code&gt; in your project configuration file. This will automatically disable all editing tools and prevent any modifications to your codebase while still allowing all analysis and exploration capabilities.&lt;/p&gt; 
&lt;p&gt;In general, be sure to back up your work and use a version control system in order to avoid losing any work.&lt;/p&gt; 
&lt;h3&gt;Modes and Contexts&lt;/h3&gt; 
&lt;p&gt;Serena's behavior and toolset can be adjusted using contexts and modes. These allow for a high degree of customization to best suit your workflow and the environment Serena is operating in.&lt;/p&gt; 
&lt;h4&gt;Contexts&lt;/h4&gt; 
&lt;p&gt;A context defines the general environment in which Serena is operating. It influences the initial system prompt and the set of available tools. A context is set at startup when launching Serena (e.g., via CLI options for an MCP server or in the agent script) and cannot be changed during an active session.&lt;/p&gt; 
&lt;p&gt;Serena comes with pre-defined contexts:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;desktop-app&lt;/code&gt;: Tailored for use with desktop applications like Claude Desktop. This is the default.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;agent&lt;/code&gt;: Designed for scenarios where Serena acts as a more autonomous agent, for example, when used with Agno.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ide-assistant&lt;/code&gt;: Optimized for integration into IDEs like VSCode, Cursor, or Cline, focusing on in-editor coding assistance. Choose the context that best matches the type of integration you are using.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;When launching Serena, specify the context using &lt;code&gt;--context &amp;lt;context-name&amp;gt;&lt;/code&gt;. Note that for cases where parameter lists are specified (e.g. Claude Desktop), you must add two parameters to the list.&lt;/p&gt; 
&lt;p&gt;If you are using a local server (such as Llama.cpp) which requires you to use OpenAI-compatible tool descriptions, use context &lt;code&gt;oaicompat-agent&lt;/code&gt; instead of &lt;code&gt;agent&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;Modes&lt;/h4&gt; 
&lt;p&gt;Modes further refine Serena's behavior for specific types of tasks or interaction styles. Multiple modes can be active simultaneously, allowing you to combine their effects. Modes influence the system prompt and can also alter the set of available tools by excluding certain ones.&lt;/p&gt; 
&lt;p&gt;Examples of built-in modes include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;planning&lt;/code&gt;: Focuses Serena on planning and analysis tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;editing&lt;/code&gt;: Optimizes Serena for direct code modification tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;interactive&lt;/code&gt;: Suitable for a conversational, back-and-forth interaction style.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;one-shot&lt;/code&gt;: Configures Serena for tasks that should be completed in a single response, often used with &lt;code&gt;planning&lt;/code&gt; for generating reports or initial plans.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;no-onboarding&lt;/code&gt;: Skips the initial onboarding process if it's not needed for a particular session.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;onboarding&lt;/code&gt;: (Usually triggered automatically) Focuses on the project onboarding process.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Modes can be set at startup (similar to contexts) but can also be &lt;em&gt;switched dynamically&lt;/em&gt; during a session. You can instruct the LLM to use the &lt;code&gt;switch_modes&lt;/code&gt; tool to activate a different set of modes (e.g., "switch to planning and one-shot modes").&lt;/p&gt; 
&lt;p&gt;When launching Serena, specify modes using &lt;code&gt;--mode &amp;lt;mode-name&amp;gt;&lt;/code&gt;; multiple modes can be specified, e.g. &lt;code&gt;--mode planning --mode no-onboarding&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;‚ö†&lt;/span&gt; &lt;strong&gt;Mode Compatibility&lt;/strong&gt;: While you can combine modes, some may be semantically incompatible (e.g., &lt;code&gt;interactive&lt;/code&gt; and &lt;code&gt;one-shot&lt;/code&gt;). Serena currently does not prevent incompatible combinations; it is up to the user to choose sensible mode configurations.&lt;/p&gt; 
&lt;h4&gt;Customization&lt;/h4&gt; 
&lt;p&gt;You can create your own contexts and modes to precisely tailor Serena to your needs in two ways:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;You can use Serena's CLI to manage modes and contexts. Check out&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;uvx --from git+https://github.com/oraios/serena serena mode --help
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;uvx --from git+https://github.com/oraios/serena serena context --help
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;NOTE&lt;/em&gt;: Custom contexts/modes are simply YAML files in &lt;code&gt;&amp;lt;home&amp;gt;/.serena&lt;/code&gt;, they are automatically registered and available for use by their name (filename without the &lt;code&gt;.yml&lt;/code&gt; extension). If you don't want to use Serena's CLI, you can create and manage them in any way you see fit.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Using external YAML files&lt;/strong&gt;: When starting Serena, you can also provide an absolute path to a custom &lt;code&gt;.yml&lt;/code&gt; file for a context or mode.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This customization allows for deep integration and adaptation of Serena to specific project requirements or personal preferences.&lt;/p&gt; 
&lt;h3&gt;Onboarding and Memories&lt;/h3&gt; 
&lt;p&gt;By default, Serena will perform an &lt;strong&gt;onboarding process&lt;/strong&gt; when it is started for the first time for a project. The goal of the onboarding is for Serena to get familiar with the project and to store memories, which it can then draw upon in future interactions. If an LLM should fail to complete the onboarding and does not actually write the respective memories to disk, you may need to ask it to do so explicitly.&lt;/p&gt; 
&lt;p&gt;The onboarding will usually read a lot of content from the project, thus filling up the context. It can therefore be advisable to switch to another conversation once the onboarding is complete. After the onboarding, we recommend that you have a quick look at the memories and, if necessary, edit them or add additional ones.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Memories&lt;/strong&gt; are files stored in &lt;code&gt;.serena/memories/&lt;/code&gt; in the project directory, which the agent can choose to read in subsequent interactions. Feel free to read and adjust them as needed; you can also add new ones manually. Every file in the &lt;code&gt;.serena/memories/&lt;/code&gt; directory is a memory file. Whenever Serena starts working on a project, the list of memories is provided, and the agent can decide to read them. We found that memories can significantly improve the user experience with Serena.&lt;/p&gt; 
&lt;h3&gt;Prepare Your Project&lt;/h3&gt; 
&lt;h4&gt;Structure Your Codebase&lt;/h4&gt; 
&lt;p&gt;Serena uses the code structure for finding, reading and editing code. This means that it will work well with well-structured code but may perform poorly on fully unstructured one (like a "God class" with enormous, non-modular functions). Furthermore, for languages that are not statically typed, type annotations are highly beneficial.&lt;/p&gt; 
&lt;h4&gt;Start from a Clean State&lt;/h4&gt; 
&lt;p&gt;It is best to start a code generation task from a clean git state. Not only will this make it easier for you to inspect the changes, but also the model itself will have a chance of seeing what it has changed by calling &lt;code&gt;git diff&lt;/code&gt; and thereby correct itself or continue working in a followup conversation if needed.&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;‚ö†&lt;/span&gt; &lt;strong&gt;Important&lt;/strong&gt;: since Serena will write to files using the system-native line endings and it might want to look at the git diff, it is important to set &lt;code&gt;git config core.autocrlf&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt; on Windows. With &lt;code&gt;git config core.autocrlf&lt;/code&gt; set to &lt;code&gt;false&lt;/code&gt; on Windows, you may end up with huge diffs only due to line endings. It is generally a good idea to globally enable this git setting on Windows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git config --global core.autocrlf true
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Logging, Linting, and Automated Tests&lt;/h4&gt; 
&lt;p&gt;Serena can successfully complete tasks in an &lt;em&gt;agent loop&lt;/em&gt;, where it iteratively acquires information, performs actions, and reflects on the results. However, Serena cannot use a debugger; it must rely on the results of program executions, linting results, and test results to assess the correctness of its actions. Therefore, software that is designed to meaningful interpretable outputs (e.g. log messages) and that has a good test coverage is much easier to work with for Serena.&lt;/p&gt; 
&lt;p&gt;We generally recommend to start an editing task from a state where all linting checks and tests pass.&lt;/p&gt; 
&lt;h3&gt;Prompting Strategies&lt;/h3&gt; 
&lt;p&gt;We found that it is often a good idea to spend some time conceptualizing and planning a task before actually implementing it, especially for non-trivial task. This helps both in achieving better results and in increasing the feeling of control and staying in the loop. You can make a detailed plan in one session, where Serena may read a lot of your code to build up the context, and then continue with the implementation in another (potentially after creating suitable memories).&lt;/p&gt; 
&lt;h3&gt;Potential Issues in Code Editing&lt;/h3&gt; 
&lt;p&gt;In our experience, LLMs are bad at counting, i.e. they have problems inserting blocks of code in the right place. Most editing operations can be performed at the symbolic level, allowing this problem is overcome. However, sometimes, line-level insertions are useful.&lt;/p&gt; 
&lt;p&gt;Serena is instructed to double-check the line numbers and any code blocks that it will edit, but you may find it useful to explicitly tell it how to edit code if you run into problems. We are working on making Serena's editing capabilities more robust.&lt;/p&gt; 
&lt;h3&gt;Running Out of Context&lt;/h3&gt; 
&lt;p&gt;For long and complicated tasks, or tasks where Serena has read a lot of content, you may come close to the limits of context tokens. In that case, it is often a good idea to continue in a new conversation. Serena has a dedicated tool to create a summary of the current state of the progress and all relevant info for continuing it. You can request to create this summary and write it to a memory. Then, in a new conversation, you can just ask Serena to read the memory and continue with the task. In our experience, this worked really well. On the up-side, since in a single session there is no summarization involved, Serena does not usually get lost (unlike some other agents that summarize under the hood), and it is also instructed to occasionally check whether it's on the right track.&lt;/p&gt; 
&lt;p&gt;Moreover, Serena is instructed to be frugal with context (e.g., to not read bodies of code symbols unnecessarily), but we found that Claude is not always very good in being frugal (Gemini seemed better at it). You can explicitly instruct it to not read the bodies if you know that it's not needed.&lt;/p&gt; 
&lt;h3&gt;Combining Serena with Other MCP Servers&lt;/h3&gt; 
&lt;p&gt;When using Serena through an MCP Client, you can use it together with other MCP servers. However, beware of tool name collisions! See info on that above.&lt;/p&gt; 
&lt;p&gt;Currently, there is a collision with the popular Filesystem MCP Server. Since Serena also provides filesystem operations, there is likely no need to ever enable these two simultaneously.&lt;/p&gt; 
&lt;h3&gt;Serena's Logs: The Dashboard and GUI Tool&lt;/h3&gt; 
&lt;p&gt;Serena provides two convenient ways of accessing the logs of the current session:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;via the &lt;strong&gt;web-based dashboard&lt;/strong&gt; (enabled by default)&lt;/p&gt; &lt;p&gt;This is supported on all platforms. By default, it will be accessible at &lt;code&gt;http://localhost:24282/dashboard/index.html&lt;/code&gt;, but a higher port may be used if the default port is unavailable/multiple instances are running.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;via the &lt;strong&gt;GUI tool&lt;/strong&gt; (disabled by default)&lt;/p&gt; &lt;p&gt;This is mainly supported on Windows, but it may also work on Linux; macOS is unsupported.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Both can be enabled, configured or disabled in Serena's configuration file (&lt;code&gt;serena_config.yml&lt;/code&gt;, see above). If enabled, they will automatically be opened as soon as the Serena agent/MCP server is started. The web dashboard will display usage statistics of Serena's tools if you set &lt;code&gt;record_tool_usage_stats: True&lt;/code&gt; in your config.&lt;/p&gt; 
&lt;p&gt;In addition to viewing logs, both tools allow to shut down the Serena agent. This function is provided, because clients like Claude Desktop may fail to terminate the MCP server subprocess when they themselves are closed.&lt;/p&gt; 
&lt;h3&gt;Troubleshooting&lt;/h3&gt; 
&lt;p&gt;Support for MCP Servers in Claude Desktop and the various MCP Server SDKs are relatively new developments and may display instabilities.&lt;/p&gt; 
&lt;p&gt;The working configuration of an MCP server may vary from platform to platform and from client to client. We recommend always using absolute paths, as relative paths may be sources of errors. The language server is running in a separate sub-process and is called with asyncio ‚Äì sometimes a client may make it crash. If you have Serena's log window enabled, and it disappears, you'll know what happened.&lt;/p&gt; 
&lt;p&gt;Some clients may not properly terminate MCP servers, look out for hanging python processes and terminate them manually, if needed.&lt;/p&gt; 
&lt;h2&gt;Comparison with Other Coding Agents&lt;/h2&gt; 
&lt;p&gt;To our knowledge, Serena is the first fully-featured coding agent where the entire functionality is available through an MCP server, thus not requiring API keys or subscriptions.&lt;/p&gt; 
&lt;h3&gt;Subscription-Based Coding Agents&lt;/h3&gt; 
&lt;p&gt;Many prominent subscription-based coding agents are parts of IDEs like Windsurf, Cursor and VSCode. Serena's functionality is similar to Cursor's Agent, Windsurf's Cascade or VSCode's agent mode.&lt;/p&gt; 
&lt;p&gt;Serena has the advantage of not requiring a subscription. A potential disadvantage is that it is not directly integrated into an IDE, so the inspection of newly written code is not as seamless.&lt;/p&gt; 
&lt;p&gt;More technical differences are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Serena is not bound to a specific IDE or CLI. Serena's MCP server can be used with any MCP client (including some IDEs), and the Agno-based agent provides additional ways of applying its functionality.&lt;/li&gt; 
 &lt;li&gt;Serena is not bound to a specific large language model or API.&lt;/li&gt; 
 &lt;li&gt;Serena navigates and edits code using a language server, so it has a symbolic understanding of the code. IDE-based tools often use a RAG-based or purely text-based approach, which is often less powerful, especially for large codebases.&lt;/li&gt; 
 &lt;li&gt;Serena is open-source and has a small codebase, so it can be easily extended and modified.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;API-Based Coding Agents&lt;/h3&gt; 
&lt;p&gt;An alternative to subscription-based agents are API-based agents like Claude Code, Cline, Aider, Roo Code and others, where the usage costs map directly to the API costs of the underlying LLM. Some of them (like Cline) can even be included in IDEs as an extension. They are often very powerful and their main downside are the (potentially very high) API costs.&lt;/p&gt; 
&lt;p&gt;Serena itself can be used as an API-based agent (see the section on Agno above). We have not yet written a CLI tool or a dedicated IDE extension for Serena (and there is probably no need for the latter, as Serena can already be used with any IDE that supports MCP servers). If there is demand for a Serena as a CLI tool like Claude Code, we will consider writing one.&lt;/p&gt; 
&lt;p&gt;The main difference between Serena and other API-based agents is that Serena can also be used as an MCP server, thus not requiring an API key and bypassing the API costs. This is a unique feature of Serena.&lt;/p&gt; 
&lt;h3&gt;Other MCP-Based Coding Agents&lt;/h3&gt; 
&lt;p&gt;There are other MCP servers designed for coding, like &lt;a href="https://github.com/wonderwhy-er/DesktopCommanderMCP"&gt;DesktopCommander&lt;/a&gt; and &lt;a href="https://github.com/ezyang/codemcp"&gt;codemcp&lt;/a&gt;. However, to the best of our knowledge, none of them provide semantic code retrieval and editing tools; they rely purely on text-based analysis. It is the integration of language servers and the MCP that makes Serena unique and so powerful for challenging coding tasks, especially in the context of larger codebases.&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;We built Serena on top of multiple existing open-source technologies, the most important ones being:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/multilspy"&gt;multilspy&lt;/a&gt;. A library which wraps language server implementations and adapts them for interaction via Python and which provided the basis for our library Solid-LSP (src/solidlsp). Solid-LSP provides pure synchronous LSP calls and extends the original library with the symbolic logic that Serena required.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/modelcontextprotocol/python-sdk"&gt;Python MCP SDK&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/agno-agi/agno"&gt;Agno&lt;/a&gt; and the associated &lt;a href="https://github.com/agno-agi/agent-ui"&gt;agent-ui&lt;/a&gt;, which we use to allow Serena to work with any model, beyond the ones supporting the MCP.&lt;/li&gt; 
 &lt;li&gt;All the language servers that we use through Solid-LSP.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Without these projects, Serena would not have been possible (or would have been significantly more difficult to build).&lt;/p&gt; 
&lt;h2&gt;Customizing and Extending Serena&lt;/h2&gt; 
&lt;p&gt;It is straightforward to extend Serena's AI functionality with your own ideas. Simply implement a new tool by subclassing &lt;code&gt;serena.agent.Tool&lt;/code&gt; and implement the &lt;code&gt;apply&lt;/code&gt; method with a signature that matches the tool's requirements. Once implemented, &lt;code&gt;SerenaAgent&lt;/code&gt; will automatically have access to the new tool.&lt;/p&gt; 
&lt;p&gt;It is also relatively straightforward to add &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/.serena/memories/adding_new_language_support_guide.md"&gt;support for a new programming language&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We look forward to seeing what the community will come up with! For details on contributing, see &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;List of Tools&lt;/h2&gt; 
&lt;p&gt;Here is the list of Serena's default tools with a short description (output of &lt;code&gt;uv run serena tools list&lt;/code&gt;):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;activate_project&lt;/code&gt;: Activates a project by name.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;check_onboarding_performed&lt;/code&gt;: Checks whether project onboarding was already performed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;create_text_file&lt;/code&gt;: Creates/overwrites a file in the project directory.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;delete_memory&lt;/code&gt;: Deletes a memory from Serena's project-specific memory store.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;execute_shell_command&lt;/code&gt;: Executes a shell command.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;find_file&lt;/code&gt;: Finds files in the given relative paths&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;find_referencing_symbols&lt;/code&gt;: Finds symbols that reference the symbol at the given location (optionally filtered by type).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;find_symbol&lt;/code&gt;: Performs a global (or local) search for symbols with/containing a given name/substring (optionally filtered by type).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;get_symbols_overview&lt;/code&gt;: Gets an overview of the top-level symbols defined in a given file.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;insert_after_symbol&lt;/code&gt;: Inserts content after the end of the definition of a given symbol.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;insert_before_symbol&lt;/code&gt;: Inserts content before the beginning of the definition of a given symbol.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;list_dir&lt;/code&gt;: Lists files and directories in the given directory (optionally with recursion).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;list_memories&lt;/code&gt;: Lists memories in Serena's project-specific memory store.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;onboarding&lt;/code&gt;: Performs onboarding (identifying the project structure and essential tasks, e.g. for testing or building).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;prepare_for_new_conversation&lt;/code&gt;: Provides instructions for preparing for a new conversation (in order to continue with the necessary context).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;read_file&lt;/code&gt;: Reads a file within the project directory.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;read_memory&lt;/code&gt;: Reads the memory with the given name from Serena's project-specific memory store.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;replace_regex&lt;/code&gt;: Replaces content in a file by using regular expressions.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;replace_symbol_body&lt;/code&gt;: Replaces the full definition of a symbol.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;search_for_pattern&lt;/code&gt;: Performs a search for a pattern in the project.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;think_about_collected_information&lt;/code&gt;: Thinking tool for pondering the completeness of collected information.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;think_about_task_adherence&lt;/code&gt;: Thinking tool for determining whether the agent is still on track with the current task.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;think_about_whether_you_are_done&lt;/code&gt;: Thinking tool for determining whether the task is truly completed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;write_memory&lt;/code&gt;: Writes a named memory (for future reference) to Serena's project-specific memory store.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;There are several tools that are disabled by default, and have to be enabled explicitly, e.g., through the context or modes. Note that several of our default contexts do enable some of these tools. For example, the &lt;code&gt;desktop-app&lt;/code&gt; context enables the &lt;code&gt;execute_shell_command&lt;/code&gt; tool.&lt;/p&gt; 
&lt;p&gt;The full list of optional tools is (output of &lt;code&gt;uv run serena tools list --only-optional&lt;/code&gt;):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;delete_lines&lt;/code&gt;: Deletes a range of lines within a file.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;get_current_config&lt;/code&gt;: Prints the current configuration of the agent, including the active and available projects, tools, contexts, and modes.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;initial_instructions&lt;/code&gt;: Gets the initial instructions for the current project. Should only be used in settings where the system prompt cannot be set, e.g. in clients you have no control over, like Claude Desktop.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;insert_at_line&lt;/code&gt;: Inserts content at a given line in a file.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jet_brains_find_referencing_symbols&lt;/code&gt;: Finds symbols that reference the given symbol&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jet_brains_find_symbol&lt;/code&gt;: Performs a global (or local) search for symbols with/containing a given name/substring (optionally filtered by type).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jet_brains_get_symbols_overview&lt;/code&gt;: Retrieves an overview of the top-level symbols within a specified file&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;remove_project&lt;/code&gt;: Removes a project from the Serena configuration.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;replace_lines&lt;/code&gt;: Replaces a range of lines within a file with new content.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;restart_language_server&lt;/code&gt;: Restarts the language server, may be necessary when edits not through Serena happen.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;summarize_changes&lt;/code&gt;: Provides instructions for summarizing the changes made to the codebase.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;switch_modes&lt;/code&gt;: Activates modes by providing a list of their names&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>crewAIInc/crewAI</title>
      <link>https://github.com/crewAIInc/crewAI</link>
      <description>&lt;p&gt;Framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://github.com/crewAIInc/crewAI"&gt; &lt;img src="https://raw.githubusercontent.com/crewAIInc/crewAI/main/docs/images/crewai_logo.png" width="600px" alt="Open source Multi-AI Agent orchestration framework" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center" style="display: flex; justify-content: center; gap: 20px; align-items: center;"&gt; &lt;a href="https://trendshift.io/repositories/11239" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/11239" alt="crewAIInc%2FcrewAI | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://crewai.com"&gt;Homepage&lt;/a&gt; ¬∑ &lt;a href="https://docs.crewai.com"&gt;Docs&lt;/a&gt; ¬∑ &lt;a href="https://app.crewai.com"&gt;Start Cloud Trial&lt;/a&gt; ¬∑ &lt;a href="https://blog.crewai.com"&gt;Blog&lt;/a&gt; ¬∑ &lt;a href="https://community.crewai.com"&gt;Forum&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/crewAIInc/crewAI"&gt; &lt;img src="https://img.shields.io/github/stars/crewAIInc/crewAI" alt="GitHub Repo stars" /&gt; &lt;/a&gt; &lt;a href="https://github.com/crewAIInc/crewAI/network/members"&gt; &lt;img src="https://img.shields.io/github/forks/crewAIInc/crewAI" alt="GitHub forks" /&gt; &lt;/a&gt; &lt;a href="https://github.com/crewAIInc/crewAI/issues"&gt; &lt;img src="https://img.shields.io/github/issues/crewAIInc/crewAI" alt="GitHub issues" /&gt; &lt;/a&gt; &lt;a href="https://github.com/crewAIInc/crewAI/pulls"&gt; &lt;img src="https://img.shields.io/github/issues-pr/crewAIInc/crewAI" alt="GitHub pull requests" /&gt; &lt;/a&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt; &lt;img src="https://img.shields.io/badge/License-MIT-green.svg?sanitize=true" alt="License: MIT" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://pypi.org/project/crewai/"&gt; &lt;img src="https://img.shields.io/pypi/v/crewai" alt="PyPI version" /&gt; &lt;/a&gt; &lt;a href="https://pypi.org/project/crewai/"&gt; &lt;img src="https://img.shields.io/pypi/dm/crewai" alt="PyPI downloads" /&gt; &lt;/a&gt; &lt;a href="https://twitter.com/crewAIInc"&gt; &lt;img src="https://img.shields.io/twitter/follow/crewAIInc?style=social" alt="Twitter Follow" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h3&gt;Fast and Flexible Multi-Agent Automation Framework&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;CrewAI is a lean, lightning-fast Python framework built entirely from scratch‚Äîcompletely &lt;strong&gt;independent of LangChain or other agent frameworks&lt;/strong&gt;. It empowers developers with both high-level simplicity and precise low-level control, ideal for creating autonomous AI agents tailored to any scenario.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;CrewAI Crews&lt;/strong&gt;: Optimize for autonomy and collaborative intelligence.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CrewAI Flows&lt;/strong&gt;: Enable granular, event-driven control, single LLM calls for precise task orchestration and supports Crews natively&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;With over 100,000 developers certified through our community courses at &lt;a href="https://learn.crewai.com"&gt;learn.crewai.com&lt;/a&gt;, CrewAI is rapidly becoming the standard for enterprise-ready AI automation.&lt;/p&gt; 
&lt;h1&gt;CrewAI Enterprise Suite&lt;/h1&gt; 
&lt;p&gt;CrewAI Enterprise Suite is a comprehensive bundle tailored for organizations that require secure, scalable, and easy-to-manage agent-driven automation.&lt;/p&gt; 
&lt;p&gt;You can try one part of the suite the &lt;a href="https://app.crewai.com"&gt;Crew Control Plane for free&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Crew Control Plane Key Features:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Tracing &amp;amp; Observability&lt;/strong&gt;: Monitor and track your AI agents and workflows in real-time, including metrics, logs, and traces.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Unified Control Plane&lt;/strong&gt;: A centralized platform for managing, monitoring, and scaling your AI agents and workflows.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Seamless Integrations&lt;/strong&gt;: Easily connect with existing enterprise systems, data sources, and cloud infrastructure.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced Security&lt;/strong&gt;: Built-in robust security and compliance measures ensuring safe deployment and management.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Actionable Insights&lt;/strong&gt;: Real-time analytics and reporting to optimize performance and decision-making.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;24/7 Support&lt;/strong&gt;: Dedicated enterprise support to ensure uninterrupted operation and quick resolution of issues.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;On-premise and Cloud Deployment Options&lt;/strong&gt;: Deploy CrewAI Enterprise on-premise or in the cloud, depending on your security and compliance requirements.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;CrewAI Enterprise is designed for enterprises seeking a powerful, reliable solution to transform complex business processes into efficient, intelligent automations.&lt;/p&gt; 
&lt;h2&gt;Table of contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#why-crewai"&gt;Why CrewAI?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#getting-started"&gt;Getting Started&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#key-features"&gt;Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#understanding-flows-and-crews"&gt;Understanding Flows and Crews&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#how-crewai-compares"&gt;CrewAI vs LangGraph&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#examples"&gt;Examples&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#quick-tutorial"&gt;Quick Tutorial&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#write-job-descriptions"&gt;Write Job Descriptions&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#trip-planner"&gt;Trip Planner&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#stock-analysis"&gt;Stock Analysis&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#using-crews-and-flows-together"&gt;Using Crews and Flows Together&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#connecting-your-crew-to-a-model"&gt;Connecting Your Crew to a Model&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#how-crewai-compares"&gt;How CrewAI Compares&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#frequently-asked-questions-faq"&gt;Frequently Asked Questions (FAQ)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#contribution"&gt;Contribution&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#telemetry"&gt;Telemetry&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Why CrewAI?&lt;/h2&gt; 
&lt;div align="center" style="margin-bottom: 30px;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/crewAIInc/crewAI/main/docs/images/asset.png" alt="CrewAI Logo" width="100%" /&gt; 
&lt;/div&gt; 
&lt;p&gt;CrewAI unlocks the true potential of multi-agent automation, delivering the best-in-class combination of speed, flexibility, and control with either Crews of AI Agents or Flows of Events:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Standalone Framework&lt;/strong&gt;: Built from scratch, independent of LangChain or any other agent framework.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High Performance&lt;/strong&gt;: Optimized for speed and minimal resource usage, enabling faster execution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible Low Level Customization&lt;/strong&gt;: Complete freedom to customize at both high and low levels - from overall workflows and system architecture to granular agent behaviors, internal prompts, and execution logic.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ideal for Every Use Case&lt;/strong&gt;: Proven effective for both simple tasks and highly complex, real-world, enterprise-grade scenarios.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Robust Community&lt;/strong&gt;: Backed by a rapidly growing community of over &lt;strong&gt;100,000 certified&lt;/strong&gt; developers offering comprehensive support and resources.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;CrewAI empowers developers and enterprises to confidently build intelligent automations, bridging the gap between simplicity, flexibility, and performance.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Setup and run your first CrewAI agents by following this tutorial.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=-kSOTtYzgEw" title="CrewAI Getting Started Tutorial"&gt;&lt;img src="https://img.youtube.com/vi/-kSOTtYzgEw/hqdefault.jpg" alt="CrewAI Getting Started Tutorial" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;/h3&gt; 
&lt;p&gt;Learning Resources&lt;/p&gt; 
&lt;p&gt;Learn CrewAI through our comprehensive courses:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.deeplearning.ai/short-courses/multi-ai-agent-systems-with-crewai/"&gt;Multi AI Agent Systems with CrewAI&lt;/a&gt; - Master the fundamentals of multi-agent systems&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.deeplearning.ai/short-courses/practical-multi-ai-agents-and-advanced-use-cases-with-crewai/"&gt;Practical Multi AI Agents and Advanced Use Cases&lt;/a&gt; - Deep dive into advanced implementations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Understanding Flows and Crews&lt;/h3&gt; 
&lt;p&gt;CrewAI offers two powerful, complementary approaches that work seamlessly together to build sophisticated AI applications:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Crews&lt;/strong&gt;: Teams of AI agents with true autonomy and agency, working together to accomplish complex tasks through role-based collaboration. Crews enable:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Natural, autonomous decision-making between agents&lt;/li&gt; 
   &lt;li&gt;Dynamic task delegation and collaboration&lt;/li&gt; 
   &lt;li&gt;Specialized roles with defined goals and expertise&lt;/li&gt; 
   &lt;li&gt;Flexible problem-solving approaches&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Flows&lt;/strong&gt;: Production-ready, event-driven workflows that deliver precise control over complex automations. Flows provide:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fine-grained control over execution paths for real-world scenarios&lt;/li&gt; 
   &lt;li&gt;Secure, consistent state management between tasks&lt;/li&gt; 
   &lt;li&gt;Clean integration of AI agents with production Python code&lt;/li&gt; 
   &lt;li&gt;Conditional branching for complex business logic&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The true power of CrewAI emerges when combining Crews and Flows. This synergy allows you to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Build complex, production-grade applications&lt;/li&gt; 
 &lt;li&gt;Balance autonomy with precise control&lt;/li&gt; 
 &lt;li&gt;Handle sophisticated real-world scenarios&lt;/li&gt; 
 &lt;li&gt;Maintain clean, maintainable code structure&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Getting Started with Installation&lt;/h3&gt; 
&lt;p&gt;To get started with CrewAI, follow these simple steps:&lt;/p&gt; 
&lt;h3&gt;1. Installation&lt;/h3&gt; 
&lt;p&gt;Ensure you have Python &amp;gt;=3.10 &amp;lt;3.14 installed on your system. CrewAI uses &lt;a href="https://docs.astral.sh/uv/"&gt;UV&lt;/a&gt; for dependency management and package handling, offering a seamless setup and execution experience.&lt;/p&gt; 
&lt;p&gt;First, install CrewAI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install crewai
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to install the 'crewai' package along with its optional features that include additional tools for agents, you can do so by using the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install 'crewai[tools]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The command above installs the basic package and also adds extra components which require more dependencies to function.&lt;/p&gt; 
&lt;h3&gt;Troubleshooting Dependencies&lt;/h3&gt; 
&lt;p&gt;If you encounter issues during installation or usage, here are some common solutions:&lt;/p&gt; 
&lt;h4&gt;Common Issues&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ModuleNotFoundError: No module named 'tiktoken'&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Install tiktoken explicitly: &lt;code&gt;pip install 'crewai[embeddings]'&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;If using embedchain or other tools: &lt;code&gt;pip install 'crewai[tools]'&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Failed building wheel for tiktoken&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Ensure Rust compiler is installed (see installation steps above)&lt;/li&gt; 
   &lt;li&gt;For Windows: Verify Visual C++ Build Tools are installed&lt;/li&gt; 
   &lt;li&gt;Try upgrading pip: &lt;code&gt;pip install --upgrade pip&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;If issues persist, use a pre-built wheel: &lt;code&gt;pip install tiktoken --prefer-binary&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;2. Setting Up Your Crew with the YAML Configuration&lt;/h3&gt; 
&lt;p&gt;To create a new CrewAI project, run the following CLI (Command Line Interface) command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;crewai create crew &amp;lt;project_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command creates a new project folder with the following structure:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;my_project/
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ .env
‚îî‚îÄ‚îÄ src/
    ‚îî‚îÄ‚îÄ my_project/
        ‚îú‚îÄ‚îÄ __init__.py
        ‚îú‚îÄ‚îÄ main.py
        ‚îú‚îÄ‚îÄ crew.py
        ‚îú‚îÄ‚îÄ tools/
        ‚îÇ   ‚îú‚îÄ‚îÄ custom_tool.py
        ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
        ‚îî‚îÄ‚îÄ config/
            ‚îú‚îÄ‚îÄ agents.yaml
            ‚îî‚îÄ‚îÄ tasks.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can now start developing your crew by editing the files in the &lt;code&gt;src/my_project&lt;/code&gt; folder. The &lt;code&gt;main.py&lt;/code&gt; file is the entry point of the project, the &lt;code&gt;crew.py&lt;/code&gt; file is where you define your crew, the &lt;code&gt;agents.yaml&lt;/code&gt; file is where you define your agents, and the &lt;code&gt;tasks.yaml&lt;/code&gt; file is where you define your tasks.&lt;/p&gt; 
&lt;h4&gt;To customize your project, you can:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Modify &lt;code&gt;src/my_project/config/agents.yaml&lt;/code&gt; to define your agents.&lt;/li&gt; 
 &lt;li&gt;Modify &lt;code&gt;src/my_project/config/tasks.yaml&lt;/code&gt; to define your tasks.&lt;/li&gt; 
 &lt;li&gt;Modify &lt;code&gt;src/my_project/crew.py&lt;/code&gt; to add your own logic, tools, and specific arguments.&lt;/li&gt; 
 &lt;li&gt;Modify &lt;code&gt;src/my_project/main.py&lt;/code&gt; to add custom inputs for your agents and tasks.&lt;/li&gt; 
 &lt;li&gt;Add your environment variables into the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Example of a simple crew with a sequential process:&lt;/h4&gt; 
&lt;p&gt;Instantiate your crew:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;crewai create crew latest-ai-development
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Modify the files as needed to fit your use case:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;agents.yaml&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# src/my_project/config/agents.yaml
researcher:
  role: &amp;gt;
    {topic} Senior Data Researcher
  goal: &amp;gt;
    Uncover cutting-edge developments in {topic}
  backstory: &amp;gt;
    You're a seasoned researcher with a knack for uncovering the latest
    developments in {topic}. Known for your ability to find the most relevant
    information and present it in a clear and concise manner.

reporting_analyst:
  role: &amp;gt;
    {topic} Reporting Analyst
  goal: &amp;gt;
    Create detailed reports based on {topic} data analysis and research findings
  backstory: &amp;gt;
    You're a meticulous analyst with a keen eye for detail. You're known for
    your ability to turn complex data into clear and concise reports, making
    it easy for others to understand and act on the information you provide.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;tasks.yaml&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# src/my_project/config/tasks.yaml
research_task:
  description: &amp;gt;
    Conduct a thorough research about {topic}
    Make sure you find any interesting and relevant information given
    the current year is 2025.
  expected_output: &amp;gt;
    A list with 10 bullet points of the most relevant information about {topic}
  agent: researcher

reporting_task:
  description: &amp;gt;
    Review the context you got and expand each topic into a full section for a report.
    Make sure the report is detailed and contains any and all relevant information.
  expected_output: &amp;gt;
    A fully fledge reports with the mains topics, each with a full section of information.
    Formatted as markdown without '```'
  agent: reporting_analyst
  output_file: report.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;crew.py&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# src/my_project/crew.py
from crewai import Agent, Crew, Process, Task
from crewai.project import CrewBase, agent, crew, task
from crewai_tools import SerperDevTool
from crewai.agents.agent_builder.base_agent import BaseAgent
from typing import List

@CrewBase
class LatestAiDevelopmentCrew():
	"""LatestAiDevelopment crew"""
	agents: List[BaseAgent]
	tasks: List[Task]

	@agent
	def researcher(self) -&amp;gt; Agent:
		return Agent(
			config=self.agents_config['researcher'],
			verbose=True,
			tools=[SerperDevTool()]
		)

	@agent
	def reporting_analyst(self) -&amp;gt; Agent:
		return Agent(
			config=self.agents_config['reporting_analyst'],
			verbose=True
		)

	@task
	def research_task(self) -&amp;gt; Task:
		return Task(
			config=self.tasks_config['research_task'],
		)

	@task
	def reporting_task(self) -&amp;gt; Task:
		return Task(
			config=self.tasks_config['reporting_task'],
			output_file='report.md'
		)

	@crew
	def crew(self) -&amp;gt; Crew:
		"""Creates the LatestAiDevelopment crew"""
		return Crew(
			agents=self.agents, # Automatically created by the @agent decorator
			tasks=self.tasks, # Automatically created by the @task decorator
			process=Process.sequential,
			verbose=True,
		)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;main.py&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;#!/usr/bin/env python
# src/my_project/main.py
import sys
from latest_ai_development.crew import LatestAiDevelopmentCrew

def run():
    """
    Run the crew.
    """
    inputs = {
        'topic': 'AI Agents'
    }
    LatestAiDevelopmentCrew().crew().kickoff(inputs=inputs)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Running Your Crew&lt;/h3&gt; 
&lt;p&gt;Before running your crew, make sure you have the following keys set as environment variables in your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;An &lt;a href="https://platform.openai.com/account/api-keys"&gt;OpenAI API key&lt;/a&gt; (or other LLM API key): &lt;code&gt;OPENAI_API_KEY=sk-...&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;A &lt;a href="https://serper.dev/"&gt;Serper.dev&lt;/a&gt; API key: &lt;code&gt;SERPER_API_KEY=YOUR_KEY_HERE&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Lock the dependencies and install them by using the CLI command but first, navigate to your project directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cd my_project
crewai install (Optional)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run your crew, execute the following command in the root of your project:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;crewai run
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python src/my_project/main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If an error happens due to the usage of poetry, please run the following command to update your crewai package:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;crewai update
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should see the output in the console and the &lt;code&gt;report.md&lt;/code&gt; file should be created in the root of your project with the full final report.&lt;/p&gt; 
&lt;p&gt;In addition to the sequential process, you can use the hierarchical process, which automatically assigns a manager to the defined crew to properly coordinate the planning and execution of tasks through delegation and validation of results. &lt;a href="https://docs.crewai.com/core-concepts/Processes/"&gt;See more about the processes here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;p&gt;CrewAI stands apart as a lean, standalone, high-performance multi-AI Agent framework delivering simplicity, flexibility, and precise control‚Äîfree from the complexity and limitations found in other agent frameworks.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Standalone &amp;amp; Lean&lt;/strong&gt;: Completely independent from other frameworks like LangChain, offering faster execution and lighter resource demands.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible &amp;amp; Precise&lt;/strong&gt;: Easily orchestrate autonomous agents through intuitive &lt;a href="https://docs.crewai.com/concepts/crews"&gt;Crews&lt;/a&gt; or precise &lt;a href="https://docs.crewai.com/concepts/flows"&gt;Flows&lt;/a&gt;, achieving perfect balance for your needs.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Seamless Integration&lt;/strong&gt;: Effortlessly combine Crews (autonomy) and Flows (precision) to create complex, real-world automations.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Deep Customization&lt;/strong&gt;: Tailor every aspect‚Äîfrom high-level workflows down to low-level internal prompts and agent behaviors.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reliable Performance&lt;/strong&gt;: Consistent results across simple tasks and complex, enterprise-level automations.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Thriving Community&lt;/strong&gt;: Backed by robust documentation and over 100,000 certified developers, providing exceptional support and guidance.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Choose CrewAI to easily build powerful, adaptable, and production-ready AI automations.&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;You can test different real life examples of AI crews in the &lt;a href="https://github.com/crewAIInc/crewAI-examples?tab=readme-ov-file"&gt;CrewAI-examples repo&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/crews/landing_page_generator"&gt;Landing Page Generator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.crewai.com/how-to/Human-Input-on-Execution"&gt;Having Human input on the execution&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/crews/trip_planner"&gt;Trip Planner&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/crews/stock_analysis"&gt;Stock Analysis&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Tutorial&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=tnejrr-0a94" title="CrewAI Tutorial"&gt;&lt;img src="https://img.youtube.com/vi/tnejrr-0a94/maxresdefault.jpg" alt="CrewAI Tutorial" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Write Job Descriptions&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/crews/job-posting"&gt;Check out code for this example&lt;/a&gt; or watch a video below:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=u98wEMz-9to" title="Jobs postings"&gt;&lt;img src="https://img.youtube.com/vi/u98wEMz-9to/maxresdefault.jpg" alt="Jobs postings" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Trip Planner&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/crews/trip_planner"&gt;Check out code for this example&lt;/a&gt; or watch a video below:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=xis7rWp-hjs" title="Trip Planner"&gt;&lt;img src="https://img.youtube.com/vi/xis7rWp-hjs/maxresdefault.jpg" alt="Trip Planner" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Stock Analysis&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/crews/stock_analysis"&gt;Check out code for this example&lt;/a&gt; or watch a video below:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=e0Uj4yWdaAg" title="Stock Analysis"&gt;&lt;img src="https://img.youtube.com/vi/e0Uj4yWdaAg/maxresdefault.jpg" alt="Stock Analysis" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Using Crews and Flows Together&lt;/h3&gt; 
&lt;p&gt;CrewAI's power truly shines when combining Crews with Flows to create sophisticated automation pipelines. CrewAI flows support logical operators like &lt;code&gt;or_&lt;/code&gt; and &lt;code&gt;and_&lt;/code&gt; to combine multiple conditions. This can be used with &lt;code&gt;@start&lt;/code&gt;, &lt;code&gt;@listen&lt;/code&gt;, or &lt;code&gt;@router&lt;/code&gt; decorators to create complex triggering conditions.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;or_&lt;/code&gt;: Triggers when any of the specified conditions are met.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;and_&lt;/code&gt;Triggers when all of the specified conditions are met.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Here's how you can orchestrate multiple Crews within a Flow:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from crewai.flow.flow import Flow, listen, start, router, or_
from crewai import Crew, Agent, Task, Process
from pydantic import BaseModel

# Define structured state for precise control
class MarketState(BaseModel):
    sentiment: str = "neutral"
    confidence: float = 0.0
    recommendations: list = []

class AdvancedAnalysisFlow(Flow[MarketState]):
    @start()
    def fetch_market_data(self):
        # Demonstrate low-level control with structured state
        self.state.sentiment = "analyzing"
        return {"sector": "tech", "timeframe": "1W"}  # These parameters match the task description template

    @listen(fetch_market_data)
    def analyze_with_crew(self, market_data):
        # Show crew agency through specialized roles
        analyst = Agent(
            role="Senior Market Analyst",
            goal="Conduct deep market analysis with expert insight",
            backstory="You're a veteran analyst known for identifying subtle market patterns"
        )
        researcher = Agent(
            role="Data Researcher",
            goal="Gather and validate supporting market data",
            backstory="You excel at finding and correlating multiple data sources"
        )

        analysis_task = Task(
            description="Analyze {sector} sector data for the past {timeframe}",
            expected_output="Detailed market analysis with confidence score",
            agent=analyst
        )
        research_task = Task(
            description="Find supporting data to validate the analysis",
            expected_output="Corroborating evidence and potential contradictions",
            agent=researcher
        )

        # Demonstrate crew autonomy
        analysis_crew = Crew(
            agents=[analyst, researcher],
            tasks=[analysis_task, research_task],
            process=Process.sequential,
            verbose=True
        )
        return analysis_crew.kickoff(inputs=market_data)  # Pass market_data as named inputs

    @router(analyze_with_crew)
    def determine_next_steps(self):
        # Show flow control with conditional routing
        if self.state.confidence &amp;gt; 0.8:
            return "high_confidence"
        elif self.state.confidence &amp;gt; 0.5:
            return "medium_confidence"
        return "low_confidence"

    @listen("high_confidence")
    def execute_strategy(self):
        # Demonstrate complex decision making
        strategy_crew = Crew(
            agents=[
                Agent(role="Strategy Expert",
                      goal="Develop optimal market strategy")
            ],
            tasks=[
                Task(description="Create detailed strategy based on analysis",
                     expected_output="Step-by-step action plan")
            ]
        )
        return strategy_crew.kickoff()

    @listen(or_("medium_confidence", "low_confidence"))
    def request_additional_analysis(self):
        self.state.recommendations.append("Gather more data")
        return "Additional analysis required"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This example demonstrates how to:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Use Python code for basic data operations&lt;/li&gt; 
 &lt;li&gt;Create and execute Crews as steps in your workflow&lt;/li&gt; 
 &lt;li&gt;Use Flow decorators to manage the sequence of operations&lt;/li&gt; 
 &lt;li&gt;Implement conditional branching based on Crew results&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Connecting Your Crew to a Model&lt;/h2&gt; 
&lt;p&gt;CrewAI supports using various LLMs through a variety of connection options. By default your agents will use the OpenAI API when querying the model. However, there are several other ways to allow your agents to connect to models. For example, you can configure your agents to use a local model via the Ollama tool.&lt;/p&gt; 
&lt;p&gt;Please refer to the &lt;a href="https://docs.crewai.com/how-to/LLM-Connections/"&gt;Connect CrewAI to LLMs&lt;/a&gt; page for details on configuring your agents' connections to models.&lt;/p&gt; 
&lt;h2&gt;How CrewAI Compares&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;CrewAI's Advantage&lt;/strong&gt;: CrewAI combines autonomous agent intelligence with precise workflow control through its unique Crews and Flows architecture. The framework excels at both high-level orchestration and low-level customization, enabling complex, production-grade systems with granular control.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;LangGraph&lt;/strong&gt;: While LangGraph provides a foundation for building agent workflows, its approach requires significant boilerplate code and complex state management patterns. The framework's tight coupling with LangChain can limit flexibility when implementing custom agent behaviors or integrating with external systems.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;P.S. CrewAI demonstrates significant performance advantages over LangGraph, executing 5.76x faster in certain cases like this QA task example (&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/Notebooks/CrewAI%20Flows%20%26%20Langgraph/QA%20Agent"&gt;see comparison&lt;/a&gt;) while achieving higher evaluation scores with faster completion times in certain coding tasks, like in this example (&lt;a href="https://github.com/crewAIInc/crewAI-examples/raw/main/Notebooks/CrewAI%20Flows%20%26%20Langgraph/Coding%20Assistant/coding_assistant_eval.ipynb"&gt;detailed analysis&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Autogen&lt;/strong&gt;: While Autogen excels at creating conversational agents capable of working together, it lacks an inherent concept of process. In Autogen, orchestrating agents' interactions requires additional programming, which can become complex and cumbersome as the scale of tasks grows.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ChatDev&lt;/strong&gt;: ChatDev introduced the idea of processes into the realm of AI agents, but its implementation is quite rigid. Customizations in ChatDev are limited and not geared towards production environments, which can hinder scalability and flexibility in real-world applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;p&gt;CrewAI is open-source and we welcome contributions. If you're looking to contribute, please:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fork the repository.&lt;/li&gt; 
 &lt;li&gt;Create a new branch for your feature.&lt;/li&gt; 
 &lt;li&gt;Add your feature or improvement.&lt;/li&gt; 
 &lt;li&gt;Send a pull request.&lt;/li&gt; 
 &lt;li&gt;We appreciate your input!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installing Dependencies&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv lock
uv sync
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Virtual Env&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Pre-commit hooks&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pre-commit install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running Tests&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run pytest .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running static type checks&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx mypy src
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Packaging&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv build
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Installing Locally&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install dist/*.tar.gz
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Telemetry&lt;/h2&gt; 
&lt;p&gt;CrewAI uses anonymous telemetry to collect usage data with the main purpose of helping us improve the library by focusing our efforts on the most used features, integrations and tools.&lt;/p&gt; 
&lt;p&gt;It's pivotal to understand that &lt;strong&gt;NO data is collected&lt;/strong&gt; concerning prompts, task descriptions, agents' backstories or goals, usage of tools, API calls, responses, any data processed by the agents, or secrets and environment variables, with the exception of the conditions mentioned. When the &lt;code&gt;share_crew&lt;/code&gt; feature is enabled, detailed data including task descriptions, agents' backstories or goals, and other specific attributes are collected to provide deeper insights while respecting user privacy. Users can disable telemetry by setting the environment variable OTEL_SDK_DISABLED to true.&lt;/p&gt; 
&lt;p&gt;Data collected includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Version of CrewAI 
  &lt;ul&gt; 
   &lt;li&gt;So we can understand how many users are using the latest version&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Version of Python 
  &lt;ul&gt; 
   &lt;li&gt;So we can decide on what versions to better support&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;General OS (e.g. number of CPUs, macOS/Windows/Linux) 
  &lt;ul&gt; 
   &lt;li&gt;So we know what OS we should focus on and if we could build specific OS related features&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Number of agents and tasks in a crew 
  &lt;ul&gt; 
   &lt;li&gt;So we make sure we are testing internally with similar use cases and educate people on the best practices&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Crew Process being used 
  &lt;ul&gt; 
   &lt;li&gt;Understand where we should focus our efforts&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;If Agents are using memory or allowing delegation 
  &lt;ul&gt; 
   &lt;li&gt;Understand if we improved the features or maybe even drop them&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;If Tasks are being executed in parallel or sequentially 
  &lt;ul&gt; 
   &lt;li&gt;Understand if we should focus more on parallel execution&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Language model being used 
  &lt;ul&gt; 
   &lt;li&gt;Improved support on most used languages&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Roles of agents in a crew 
  &lt;ul&gt; 
   &lt;li&gt;Understand high level use cases so we can build better tools, integrations and examples about it&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Tools names available 
  &lt;ul&gt; 
   &lt;li&gt;Understand out of the publicly available tools, which ones are being used the most so we can improve them&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Users can opt-in to Further Telemetry, sharing the complete telemetry data by setting the &lt;code&gt;share_crew&lt;/code&gt; attribute to &lt;code&gt;True&lt;/code&gt; on their Crews. Enabling &lt;code&gt;share_crew&lt;/code&gt; results in the collection of detailed crew and task execution data, including &lt;code&gt;goal&lt;/code&gt;, &lt;code&gt;backstory&lt;/code&gt;, &lt;code&gt;context&lt;/code&gt;, and &lt;code&gt;output&lt;/code&gt; of tasks. This enables a deeper insight into usage patterns while respecting the user's choice to share.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;CrewAI is released under the &lt;a href="https://github.com/crewAIInc/crewAI/raw/main/LICENSE"&gt;MIT License&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Frequently Asked Questions (FAQ)&lt;/h2&gt; 
&lt;h3&gt;General&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-what-exactly-is-crewai"&gt;What exactly is CrewAI?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-how-do-i-install-crewai"&gt;How do I install CrewAI?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-does-crewai-depend-on-langchain"&gt;Does CrewAI depend on LangChain?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-is-crewai-open-source"&gt;Is CrewAI open-source?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-does-crewai-collect-data-from-users"&gt;Does CrewAI collect data from users?&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Features and Capabilities&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-can-crewai-handle-complex-use-cases"&gt;Can CrewAI handle complex use cases?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-can-i-use-crewai-with-local-ai-models"&gt;Can I use CrewAI with local AI models?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-what-makes-crews-different-from-flows"&gt;What makes Crews different from Flows?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-how-is-crewai-better-than-langchain"&gt;How is CrewAI better than LangChain?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-does-crewai-support-fine-tuning-or-training-custom-models"&gt;Does CrewAI support fine-tuning or training custom models?&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Resources and Community&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-where-can-i-find-real-world-crewai-examples"&gt;Where can I find real-world CrewAI examples?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-how-can-i-contribute-to-crewai"&gt;How can I contribute to CrewAI?&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Enterprise Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-what-additional-features-does-crewai-enterprise-offer"&gt;What additional features does CrewAI Enterprise offer?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-is-crewai-enterprise-available-for-cloud-and-on-premise-deployments"&gt;Is CrewAI Enterprise available for cloud and on-premise deployments?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-can-i-try-crewai-enterprise-for-free"&gt;Can I try CrewAI Enterprise for free?&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Q: What exactly is CrewAI?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI is a standalone, lean, and fast Python framework built specifically for orchestrating autonomous AI agents. Unlike frameworks like LangChain, CrewAI does not rely on external dependencies, making it leaner, faster, and simpler.&lt;/p&gt; 
&lt;h3&gt;Q: How do I install CrewAI?&lt;/h3&gt; 
&lt;p&gt;A: Install CrewAI using pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install crewai
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For additional tools, use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install 'crewai[tools]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Q: Does CrewAI depend on LangChain?&lt;/h3&gt; 
&lt;p&gt;A: No. CrewAI is built entirely from the ground up, with no dependencies on LangChain or other agent frameworks. This ensures a lean, fast, and flexible experience.&lt;/p&gt; 
&lt;h3&gt;Q: Can CrewAI handle complex use cases?&lt;/h3&gt; 
&lt;p&gt;A: Yes. CrewAI excels at both simple and highly complex real-world scenarios, offering deep customization options at both high and low levels, from internal prompts to sophisticated workflow orchestration.&lt;/p&gt; 
&lt;h3&gt;Q: Can I use CrewAI with local AI models?&lt;/h3&gt; 
&lt;p&gt;A: Absolutely! CrewAI supports various language models, including local ones. Tools like Ollama and LM Studio allow seamless integration. Check the &lt;a href="https://docs.crewai.com/how-to/LLM-Connections/"&gt;LLM Connections documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;Q: What makes Crews different from Flows?&lt;/h3&gt; 
&lt;p&gt;A: Crews provide autonomous agent collaboration, ideal for tasks requiring flexible decision-making and dynamic interaction. Flows offer precise, event-driven control, ideal for managing detailed execution paths and secure state management. You can seamlessly combine both for maximum effectiveness.&lt;/p&gt; 
&lt;h3&gt;Q: How is CrewAI better than LangChain?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI provides simpler, more intuitive APIs, faster execution speeds, more reliable and consistent results, robust documentation, and an active community‚Äîaddressing common criticisms and limitations associated with LangChain.&lt;/p&gt; 
&lt;h3&gt;Q: Is CrewAI open-source?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI is open-source and actively encourages community contributions and collaboration.&lt;/p&gt; 
&lt;h3&gt;Q: Does CrewAI collect data from users?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI collects anonymous telemetry data strictly for improvement purposes. Sensitive data such as prompts, tasks, or API responses are never collected unless explicitly enabled by the user.&lt;/p&gt; 
&lt;h3&gt;Q: Where can I find real-world CrewAI examples?&lt;/h3&gt; 
&lt;p&gt;A: Check out practical examples in the &lt;a href="https://github.com/crewAIInc/crewAI-examples"&gt;CrewAI-examples repository&lt;/a&gt;, covering use cases like trip planners, stock analysis, and job postings.&lt;/p&gt; 
&lt;h3&gt;Q: How can I contribute to CrewAI?&lt;/h3&gt; 
&lt;p&gt;A: Contributions are warmly welcomed! Fork the repository, create your branch, implement your changes, and submit a pull request. See the Contribution section of the README for detailed guidelines.&lt;/p&gt; 
&lt;h3&gt;Q: What additional features does CrewAI Enterprise offer?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI Enterprise provides advanced features such as a unified control plane, real-time observability, secure integrations, advanced security, actionable insights, and dedicated 24/7 enterprise support.&lt;/p&gt; 
&lt;h3&gt;Q: Is CrewAI Enterprise available for cloud and on-premise deployments?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI Enterprise supports both cloud-based and on-premise deployment options, allowing enterprises to meet their specific security and compliance requirements.&lt;/p&gt; 
&lt;h3&gt;Q: Can I try CrewAI Enterprise for free?&lt;/h3&gt; 
&lt;p&gt;A: Yes, you can explore part of the CrewAI Enterprise Suite by accessing the &lt;a href="https://app.crewai.com"&gt;Crew Control Plane&lt;/a&gt; for free.&lt;/p&gt; 
&lt;h3&gt;Q: Does CrewAI support fine-tuning or training custom models?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI can integrate with custom-trained or fine-tuned models, allowing you to enhance your agents with domain-specific knowledge and accuracy.&lt;/p&gt; 
&lt;h3&gt;Q: Can CrewAI agents interact with external tools and APIs?&lt;/h3&gt; 
&lt;p&gt;A: Absolutely! CrewAI agents can easily integrate with external tools, APIs, and databases, empowering them to leverage real-world data and resources.&lt;/p&gt; 
&lt;h3&gt;Q: Is CrewAI suitable for production environments?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI is explicitly designed with production-grade standards, ensuring reliability, stability, and scalability for enterprise deployments.&lt;/p&gt; 
&lt;h3&gt;Q: How scalable is CrewAI?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI is highly scalable, supporting simple automations and large-scale enterprise workflows involving numerous agents and complex tasks simultaneously.&lt;/p&gt; 
&lt;h3&gt;Q: Does CrewAI offer debugging and monitoring tools?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI Enterprise includes advanced debugging, tracing, and real-time observability features, simplifying the management and troubleshooting of your automations.&lt;/p&gt; 
&lt;h3&gt;Q: What programming languages does CrewAI support?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI is primarily Python-based but easily integrates with services and APIs written in any programming language through its flexible API integration capabilities.&lt;/p&gt; 
&lt;h3&gt;Q: Does CrewAI offer educational resources for beginners?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI provides extensive beginner-friendly tutorials, courses, and documentation through learn.crewai.com, supporting developers at all skill levels.&lt;/p&gt; 
&lt;h3&gt;Q: Can CrewAI automate human-in-the-loop workflows?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI fully supports human-in-the-loop workflows, allowing seamless collaboration between human experts and AI agents for enhanced decision-making.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/rStar</title>
      <link>https://github.com/microsoft/rStar</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; &lt;br /&gt; rStar2-Agent &lt;/h1&gt; 
&lt;p align="center"&gt; üìÉ &lt;a href="https://huggingface.co/papers/2508.20722" target="_blank"&gt;[Paper]&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;Repo for "&lt;a href="https://huggingface.co/papers/2508.20722"&gt;rStar2-Agent: Agentic Reasoning Technical Report&lt;/a&gt;".&lt;/p&gt; 
&lt;p&gt;Authors: Ning Shang*, Yifei Liu*, Yi Zhu*, Li Lyna Zhang*‚Ä†, Weijiang Xu, Xinyu Guan, Buze Zhang, Bingcheng Dong, Xudong Zhou, Bowen Zhang, Ying Xin, Ziming Miao, Scarlett Li, Fan Yang, Mao Yang‚Ä†&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/rStar/main/images/figure-1.png" width="1000" /&gt; &lt;br /&gt; &lt;em&gt;Figure 1: rStar2-Agent-14B reaches frontier-level math reasoning in just 510 RL training step&lt;/em&gt; &lt;/p&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;[07/15/2025]&lt;/strong&gt; Our rStar-Coder &lt;a href="https://arxiv.org/abs/2505.21297"&gt;paper&lt;/a&gt; and &lt;a href="https://huggingface.co/datasets/microsoft/rStar-Coder"&gt;dataset&lt;/a&gt; are released. We introduce a large-scale, verified dataset of 418K competition-level code problems with &lt;strong&gt;test cases&lt;/strong&gt; of varying difficulty, enabling small LLMs (1.5B-14B) to achieve frontier-level code reasoning performance.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[02/10/2025]&lt;/strong&gt; We are hiring interns! If you are interested in improving LLM reasoning, please send your CV to &lt;a href="mailto:lzhani@microsoft.com"&gt;lzhani@microsoft.com&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[01/21/2025]&lt;/strong&gt; rStar-Math code has been open-sourced.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[01/09/2025]&lt;/strong&gt; rStar-Math paper is released: &lt;a href="https://huggingface.co/papers/2501.04519"&gt;https://huggingface.co/papers/2501.04519&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Note: Our prior work &lt;a href="https://huggingface.co/papers/2408.06195"&gt;Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers&lt;/a&gt; is open-sourced on the &lt;a href="https://github.com/microsoft/rStar/tree/rStar-mutualreasoning"&gt;rStar-mutualreasoning b&lt;/a&gt; branch.&lt;/p&gt; 
&lt;p&gt;Note: Our prior work &lt;a href="https://huggingface.co/papers/2501.04519"&gt;rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking&lt;/a&gt; is open-sourced on the &lt;a href="https://github.com/microsoft/rStar/tree/rStar-math"&gt;rStar-math&lt;/a&gt; branch.&lt;/p&gt; 
&lt;h2&gt;Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/rStar/main/#Introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/rStar/main/#Try-rStar2-Agent-with-Tool-Calling"&gt;Try rStar2-Agent with Tool Calling&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/rStar/main/#Evaluation"&gt;Evaluation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/rStar/main/#rStar2-Agent-RL-Training"&gt;rStar2-Agent RL Training&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/rStar/main/#Citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;We introduce rStar2-Agent, a 14B math reasoning model that thinks smarter rather than merely longer, achieving performance comparable to 671B DeepSeek-R1 through pure agentic reinforcement learning. The model plans, reasons, and autonomously uses coding tools to efficiently explore, verify, and reflect for more complex problem-solving. This capability relies on three key innovations: (i) GRPO-RoC, an effective agentic reinforcement learning algorithm with a novel Resample-on-Correct rollout strategy that optimizes coding tool usage and enables shorter, smarter reasoning by selectively retaining higher-quality positive trajectories while preserving all failure cases; (ii) a scalable and efficient RL infrastructure that supports high-throughput tool call execution and mitigates the high costs of agentic RL rollout, enabling efficient training on limited GPU resources (64 MI300X GPUs); (iii) an agent training recipe that starts with non-reasoning SFT and proceeds through multi-stage RL with concise maximum response lengths per stage and increasing dataset difficulty. To this end, rStar2-Agent boosts a pre-trained 14B model to state-of-the-art levels in only 510 RL steps within one week, achieving 80.6% and 69.8% average pass@1 on AIME24 and AIME25, surpassing DeepSeek-R1 (671B) with shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates strong generalization to alignment, scientific reasoning, and agentic tool-use tasks.&lt;/p&gt; 
&lt;h2&gt;Try rStar2-Agent with Tool Calling&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;h4&gt;Option 1: Manual Installation&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Initialize and update submodules
git submodule init
git submodule update

# install verl
pip install "torch&amp;lt;2.8"
pip install -r verl/requirements_sglang.txt
pip install -e verl

# install code judge
pip install -r code-judge/requirements.txt
pip install -e code-judge

# install rstar2_agent
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Option 2: Automated Installation&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;bash install.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Code Judge Server Setup&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Security Warning&lt;/strong&gt;: Code Judge executes arbitrary code. Always deploy in an isolated environment (preferably Docker) and never expose to external networks.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;The rStar2-Agent uses Code Judge as a tool call server to execute model-generated Python code.&lt;/p&gt; 
&lt;h4&gt;1. Start Redis Server&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update -y &amp;amp;&amp;amp; sudo apt-get install redis -y
redis-server --daemonize yes --protected-mode no --bind 0.0.0.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;2. Launch Code Judge Server&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start the main server (master node only)
# Environment variables can be configured as per: https://github.com/0xWJ/code-judge/blob/main/app/config.py
# Replace $WORKSPACE and $MASTER_ADDR with your actual paths

tmux new-session -d -s server \
  'cd $WORKSPACE/code-judge &amp;amp;&amp;amp; \
   MAX_EXECUTION_TIME=4 \
   REDIS_URI="redis://$MASTER_ADDR:6379" \
   RUN_WORKERS=0 \
   uvicorn app.main:app --host 0.0.0.0 --port 8088 --workers 16 \
   2&amp;gt;&amp;amp;1 | tee server.log'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3. Start Code Judge Workers&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Launch workers (can be deployed on multiple nodes for increased parallelism)
# Adjust MAX_WORKERS based on your CPU count per node

tmux new-session -d -s worker \
  'cd $WORKSPACE/code-judge &amp;amp;&amp;amp; \
   MAX_EXECUTION_TIME=4 \
   REDIS_URI="redis://$MASTER_ADDR:6379" \
   MAX_WORKERS=64 \
   python run_workers.py \
   2&amp;gt;&amp;amp;1 | tee worker.log'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Launch the VLLM Server&lt;/h3&gt; 
&lt;p&gt;First, start the VLLM server:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vllm serve /path/to/your/model \
    --host 0.0.0.0 \
    --port 8000 \
    --enable-auto-tool-choice \
    --tool-call-parser hermes
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Replace &lt;code&gt;/path/to/your/model&lt;/code&gt; with the actual path to your downloaded model.&lt;/p&gt; 
&lt;h3&gt;Verify Server Status&lt;/h3&gt; 
&lt;p&gt;Check if the server is running properly:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl http://localhost:8000/v1/models
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Run Interactive Chat with Tool Calling&lt;/h3&gt; 
&lt;p&gt;Use the provided script to interact with your model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python examples/chat_with_tool_call.py \
    --model /path/to/your/model \
    --prompt "Solve the system of equations: 2x + 3y = 7, x - y = 1" \
    --max_tokens 8192
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Script Options&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;examples/chat_with_tool_call.py&lt;/code&gt; script supports the following arguments:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model&lt;/code&gt;: Path to your model&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--prompt&lt;/code&gt;: Input prompt for the model&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--max_tokens&lt;/code&gt;: Maximum number of tokens to generate&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Evaluation&lt;/h2&gt; 
&lt;h3&gt;Environment Setup&lt;/h3&gt; 
&lt;p&gt;Please view &lt;a href="https://raw.githubusercontent.com/microsoft/rStar/main/#Installation"&gt;Installation&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/microsoft/rStar/main/#Code-Judge-Server-Setup"&gt;Code Judge Server Setup&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Run Evaluation Script&lt;/h3&gt; 
&lt;p&gt;We evaluate following mathematical reasoning benchmarks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;AIME 2024/2025 (American Invitational Mathematics Examination)&lt;/strong&gt;: High-school level competition mathematics&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MATH500&lt;/strong&gt;: A subset of the MATH dataset containing 500 challenging problems&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;MODEL_PATH=/path/to/your/model bash examples/aime_eval.sh
MODEL_PATH=/path/to/your/model bash examples/math500_eval.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;rStar2-Agent RL Training&lt;/h2&gt; 
&lt;p&gt;A comprehensive reinforcement learning training framework for the rStar2-Agent, built on &lt;a href="https://github.com/volcengine/verl"&gt;Verl&lt;/a&gt; and &lt;a href="https://github.com/0xWJ/code-judge"&gt;Code Judge&lt;/a&gt;. This framework enables training models after instruction-following supervised fine-tuning (SFT).&lt;/p&gt; 
&lt;h3&gt;Environment Setup&lt;/h3&gt; 
&lt;p&gt;Please view &lt;a href="https://raw.githubusercontent.com/microsoft/rStar/main/#Installation"&gt;Installation&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/microsoft/rStar/main/#Code-Judge-Server-Setup"&gt;Code Judge Server Setup&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Data Preparation&lt;/h3&gt; 
&lt;p&gt;This example uses:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Training Dataset&lt;/strong&gt;: DAPO-17k (English subset)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test Dataset&lt;/strong&gt;: AIME24&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Process AIME 2024 dataset
python data_preprocess/aime2024_rstar2_agent_loop.py

# Process DAPO dataset
python data_preprocess/dapo_rstar2_agent_loop.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Model Setup&lt;/h3&gt; 
&lt;p&gt;Download the base model (Qwen3-14B-Base):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;huggingface-cli download Qwen/Qwen3-14B-Base --local-dir $HOME/models/Qwen3-14B-Base
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The base model requires instruction-following SFT before RL training for optimal performance.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Training&lt;/h3&gt; 
&lt;h4&gt;Basic Training&lt;/h4&gt; 
&lt;p&gt;Run the training script (for 8x A100/H100 GPUs):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;bash examples/run_qwen3-14b_rstar2_agent_weave.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Adjust configuration parameters based on your hardware environment.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Configuration&lt;/h3&gt; 
&lt;h4&gt;Data Augmentation Settings&lt;/h4&gt; 
&lt;p&gt;The framework supports various sampling strategies to improve training efficiency:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Global Settings
augmentation.do_down_sampling=True                                   # Enable down sampling
augmentation.down_sampling_config.down_sample_to_n=16                # Target number of traces per data point

# Sampling Strategies
augmentation.down_sampling_config.reject_equal_reward=True           # Enable reject sampling for equal rewards
augmentation.down_sampling_config.roc_error_ratio=True               # Resample correct traces by tool call error ratio
augmentation.down_sampling_config.roc_answer_format=True             # Resample correct traces by answer format

# Minimum Trace Requirements
augmentation.down_sampling_config.min_zero_reward_trace_num=2        # Minimum negative traces to retain
augmentation.down_sampling_config.min_non_zero_reward_trace_num=2    # Minimum positive traces to retain
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Important Note&lt;/h3&gt; 
&lt;p&gt;rStar2-Agent was originally training based on VERL v0.2 with our custom multi-turn tool calling training framework. The current training framework released here has been migrated to VERL v0.5 to ensure compatibility with the latest community standards. While this release framework hasn't been used to train a complete model yet, we have verified that the first 50 training steps show minimal differences between our original and migrated frameworks, maintaining the core functionality of our proven training approach.&lt;/p&gt; 
&lt;p&gt;Although our original framework includes additional advanced features such as rollout request load balance scheduler, we chose to migrate to the latest VERL version to maintain community compatibility and facilitate easier customization by users. This approach ensures you can benefit from ongoing VERL improvements and easily integrate with the latest open-source developments. We also consider migrating all features to the current version in the future.&lt;/p&gt; 
&lt;p&gt;If you encounter any issues during usage or need assistance with the training framework, please contact us.&lt;/p&gt; 
&lt;h3&gt;Troubleshooting&lt;/h3&gt; 
&lt;h4&gt;Common Issues&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Redis Connection Errors&lt;/strong&gt;: Ensure Redis is running and accessible at the specified address&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GPU Memory Issues&lt;/strong&gt;: Adjust batch sizes and model parameters for your hardware&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code Judge Timeouts&lt;/strong&gt;: Increase &lt;code&gt;MAX_EXECUTION_TIME&lt;/code&gt; for complex computations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Worker Scaling&lt;/strong&gt;: Adjust &lt;code&gt;MAX_WORKERS&lt;/code&gt; based on available CPU cores&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Log Locations&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Server logs: &lt;code&gt;server.log&lt;/code&gt; in the code-judge directory&lt;/li&gt; 
 &lt;li&gt;Worker logs: &lt;code&gt;worker.log&lt;/code&gt; in the code-judge directory&lt;/li&gt; 
 &lt;li&gt;Training logs: Check your training script output directory&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find this repo useful for your research, please consider citing the paper&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{shang2025rstar2agentagenticreasoningtechnical,
      title={rStar2-Agent: Agentic Reasoning Technical Report}, 
      author={Ning Shang and Yifei Liu and Yi Zhu and Li Lyna Zhang and Weijiang Xu and Xinyu Guan and Buze Zhang and Bingcheng Dong and Xudong Zhou and Bowen Zhang and Ying Xin and Ziming Miao and Scarlett Li and Fan Yang and Mao Yang},
      year={2025},
      eprint={2508.20722},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2508.20722}, 
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>lfnovo/open-notebook</title>
      <link>https://github.com/lfnovo/open-notebook</link>
      <description>&lt;p&gt;An Open Source implementation of Notebook LM with more flexibility and features&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a id="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- [![Contributors][contributors-shield]][contributors-url] --&gt; 
&lt;p&gt;&lt;a href="https://github.com/lfnovo/open-notebook/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/lfnovo/open-notebook.svg?style=for-the-badge" alt="Forks" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lfnovo/open-notebook/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/lfnovo/open-notebook.svg?style=for-the-badge" alt="Stargazers" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;&lt;img src="https://img.shields.io/github/issues/lfnovo/open-notebook.svg?style=for-the-badge" alt="Issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lfnovo/open-notebook/raw/master/LICENSE.txt"&gt;&lt;img src="https://img.shields.io/github/license/lfnovo/open-notebook.svg?style=for-the-badge" alt="MIT License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- [![LinkedIn][linkedin-shield]][linkedin-url] --&gt; 
&lt;!-- PROJECT LOGO --&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/lfnovo/open-notebook"&gt; &lt;img src="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/assets/hero.svg?sanitize=true" alt="Logo" /&gt; &lt;/a&gt; 
 &lt;h3 align="center"&gt;Open Notebook&lt;/h3&gt; 
 &lt;p align="center"&gt; An open source, privacy-focused alternative to Google's Notebook LM! &lt;br /&gt;&lt;strong&gt;Join our &lt;a href="https://discord.gg/37XJPXfz2w"&gt;Discord server&lt;/a&gt; for help, to share workflow ideas, and suggest features!&lt;/strong&gt; &lt;br /&gt; &lt;a href="https://www.open-notebook.ai"&gt;&lt;strong&gt;Checkout our website ¬ª&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt; &lt;br /&gt; &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/index.md"&gt;üìö Get Started&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/index.md"&gt;üìñ User Guide&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/index.md"&gt;‚ú® Features&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/index.md"&gt;üöÄ Deploy&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üì¢ Open Notebook is under very active development&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Open Notebook is under active development! We're moving fast and making improvements every week. Your feedback is incredibly valuable to me during this exciting phase and it gives me motivation to keep improving and building this amazing tool. Please feel free to star the project if you find it useful, and don't hesitate to reach out with any questions or suggestions. I'm excited to see how you'll use it and what ideas you'll bring to the project! Let's build something amazing together! üöÄ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;About The Project&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/assets/asset_list.png" alt="New Notebook" /&gt;&lt;/p&gt; 
&lt;p&gt;An open source, privacy-focused alternative to Google's Notebook LM. Why give Google more of our data when we can take control of our own research workflows?&lt;/p&gt; 
&lt;p&gt;In a world dominated by Artificial Intelligence, having the ability to think üß† and acquire new knowledge üí°, is a skill that should not be a privilege for a few, nor restricted to a single provider.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Open Notebook empowers you to:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üîí &lt;strong&gt;Control your data&lt;/strong&gt; - Keep your research private and secure&lt;/li&gt; 
 &lt;li&gt;ü§ñ &lt;strong&gt;Choose your AI models&lt;/strong&gt; - Support for 16+ providers including OpenAI, Anthropic, Ollama, LM Studio, and more&lt;/li&gt; 
 &lt;li&gt;üìö &lt;strong&gt;Organize multi-modal content&lt;/strong&gt; - PDFs, videos, audio, web pages, and more&lt;/li&gt; 
 &lt;li&gt;üéôÔ∏è &lt;strong&gt;Generate professional podcasts&lt;/strong&gt; - Advanced multi-speaker podcast generation&lt;/li&gt; 
 &lt;li&gt;üîç &lt;strong&gt;Search intelligently&lt;/strong&gt; - Full-text and vector search across all your content&lt;/li&gt; 
 &lt;li&gt;üí¨ &lt;strong&gt;Chat with context&lt;/strong&gt; - AI conversations powered by your research&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Learn more about our project at &lt;a href="https://www.open-notebook.ai"&gt;https://www.open-notebook.ai&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üÜö Open Notebook vs Google Notebook LM&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;Open Notebook&lt;/th&gt; 
   &lt;th&gt;Google Notebook LM&lt;/th&gt; 
   &lt;th&gt;Advantage&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Privacy &amp;amp; Control&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Self-hosted, your data&lt;/td&gt; 
   &lt;td&gt;Google cloud only&lt;/td&gt; 
   &lt;td&gt;Complete data sovereignty&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;AI Provider Choice&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;16+ providers (OpenAI, Anthropic, Ollama, LM Studio, etc.)&lt;/td&gt; 
   &lt;td&gt;Google models only&lt;/td&gt; 
   &lt;td&gt;Flexibility and cost optimization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Podcast Speakers&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;1-4 speakers with custom profiles&lt;/td&gt; 
   &lt;td&gt;2 speakers only&lt;/td&gt; 
   &lt;td&gt;Extreme flexibility&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Context Control&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;3 granular levels&lt;/td&gt; 
   &lt;td&gt;All-or-nothing&lt;/td&gt; 
   &lt;td&gt;Privacy and performance tuning&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Content Transformations&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Custom and built-in&lt;/td&gt; 
   &lt;td&gt;Limited options&lt;/td&gt; 
   &lt;td&gt;Unlimited processing power&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;API Access&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Full REST API&lt;/td&gt; 
   &lt;td&gt;No API&lt;/td&gt; 
   &lt;td&gt;Complete automation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Deployment&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Docker, cloud, or local&lt;/td&gt; 
   &lt;td&gt;Google hosted only&lt;/td&gt; 
   &lt;td&gt;Deploy anywhere&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Citations&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Comprehensive with sources&lt;/td&gt; 
   &lt;td&gt;Basic references&lt;/td&gt; 
   &lt;td&gt;Research integrity&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Customization&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Open source, fully customizable&lt;/td&gt; 
   &lt;td&gt;Closed system&lt;/td&gt; 
   &lt;td&gt;Unlimited extensibility&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Cost&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Pay only for AI usage&lt;/td&gt; 
   &lt;td&gt;Monthly subscription + usage&lt;/td&gt; 
   &lt;td&gt;Transparent and controllable&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Why Choose Open Notebook?&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üîí &lt;strong&gt;Privacy First&lt;/strong&gt;: Your sensitive research stays completely private&lt;/li&gt; 
 &lt;li&gt;üí∞ &lt;strong&gt;Cost Control&lt;/strong&gt;: Choose cheaper AI providers or run locally with Ollama&lt;/li&gt; 
 &lt;li&gt;üéôÔ∏è &lt;strong&gt;Better Podcasts&lt;/strong&gt;: Full script control and multi-speaker flexibility vs limited 2-speaker deep-dive format&lt;/li&gt; 
 &lt;li&gt;üîß &lt;strong&gt;Unlimited Customization&lt;/strong&gt;: Modify, extend, and integrate as needed&lt;/li&gt; 
 &lt;li&gt;üåê &lt;strong&gt;No Vendor Lock-in&lt;/strong&gt;: Switch providers, deploy anywhere, own your data&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Built With&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.python.org/"&gt;&lt;img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white" alt="Python" /&gt;&lt;/a&gt; &lt;a href="https://surrealdb.com/"&gt;&lt;img src="https://img.shields.io/badge/SurrealDB-FF5E00?style=for-the-badge&amp;amp;logo=databricks&amp;amp;logoColor=white" alt="SurrealDB" /&gt;&lt;/a&gt; &lt;a href="https://www.langchain.com/"&gt;&lt;img src="https://img.shields.io/badge/LangChain-3A3A3A?style=for-the-badge&amp;amp;logo=chainlink&amp;amp;logoColor=white" alt="LangChain" /&gt;&lt;/a&gt; &lt;a href="https://streamlit.io/"&gt;&lt;img src="https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&amp;amp;logo=streamlit&amp;amp;logoColor=white" alt="Streamlit" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;p&gt;Ready to try Open Notebook? Choose your preferred method:&lt;/p&gt; 
&lt;h3&gt;‚ö° Instant Setup (Recommended)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create a new directory for your Open Notebook installation
mkdir open-notebook
cd open-notebook

# Using Docker - Get started in 2 minutes
docker run -d \
  --name open-notebook \
  -p 8502:8502 -p 5055:5055 \
  -v ./notebook_data:/app/data \
  -v ./surreal_data:/mydata \
  -e OPENAI_API_KEY=your_key \
  lfnovo/open_notebook:latest-single
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;What gets created:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;open-notebook/
‚îú‚îÄ‚îÄ notebook_data/     # Your notebooks and research content
‚îî‚îÄ‚îÄ surreal_data/      # Database files
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Access your installation:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üñ•Ô∏è Main Interface&lt;/strong&gt;: &lt;a href="http://localhost:8502"&gt;http://localhost:8502&lt;/a&gt; (Streamlit UI)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîß API Access&lt;/strong&gt;: &lt;a href="http://localhost:5055"&gt;http://localhost:5055&lt;/a&gt; (REST API)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìö API Documentation&lt;/strong&gt;: &lt;a href="http://localhost:5055/docs"&gt;http://localhost:5055/docs&lt;/a&gt; (Interactive Swagger UI)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;‚ö†Ô∏è Important&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Run from a dedicated folder&lt;/strong&gt;: Create and run this from inside a new &lt;code&gt;open-notebook&lt;/code&gt; folder so your data volumes are properly organized&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Volume persistence&lt;/strong&gt;: The volumes (&lt;code&gt;-v ./notebook_data:/app/data&lt;/code&gt; and &lt;code&gt;-v ./surreal_data:/mydata&lt;/code&gt;) are essential to persist your data between container restarts. Without them, you'll lose all your notebooks and research when the container stops.&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;üõ†Ô∏è Full Installation&lt;/h3&gt; 
&lt;p&gt;For development or customization:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/lfnovo/open-notebook
cd open-notebook
make start-all
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üìñ Need Help?&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ü§ñ AI Installation Assistant&lt;/strong&gt;: We have a &lt;a href="https://chatgpt.com/g/g-68776e2765b48191bd1bae3f30212631-open-notebook-installation-assistant"&gt;CustomGPT built to help you install Open Notebook&lt;/a&gt; - it will guide you through each step!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;New to Open Notebook?&lt;/strong&gt; Start with our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/index.md"&gt;Getting Started Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Need installation help?&lt;/strong&gt; Check our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/installation.md"&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Want to see it in action?&lt;/strong&gt; Try our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/quick-start.md"&gt;Quick Start Tutorial&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Provider Support Matrix&lt;/h2&gt; 
&lt;p&gt;Thanks to the &lt;a href="https://github.com/lfnovo/esperanto"&gt;Esperanto&lt;/a&gt; library, we support this providers out of the box!&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Provider&lt;/th&gt; 
   &lt;th&gt;LLM Support&lt;/th&gt; 
   &lt;th&gt;Embedding Support&lt;/th&gt; 
   &lt;th&gt;Speech-to-Text&lt;/th&gt; 
   &lt;th&gt;Text-to-Speech&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Anthropic&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Groq&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Google (GenAI)&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vertex AI&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ollama&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Perplexity&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ElevenLabs&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Azure OpenAI&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mistral&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DeepSeek&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Voyage&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;xAI&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenRouter&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI Compatible*&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;*Supports LM Studio and any OpenAI-compatible endpoint&lt;/p&gt; 
&lt;h2&gt;‚ú® Key Features&lt;/h2&gt; 
&lt;h3&gt;Core Capabilities&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üîí Privacy-First&lt;/strong&gt;: Your data stays under your control - no cloud dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üéØ Multi-Notebook Organization&lt;/strong&gt;: Manage multiple research projects seamlessly&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìö Universal Content Support&lt;/strong&gt;: PDFs, videos, audio, web pages, Office docs, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ü§ñ Multi-Model AI Support&lt;/strong&gt;: 16+ providers including OpenAI, Anthropic, Ollama, Google, LM Studio, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üéôÔ∏è Professional Podcast Generation&lt;/strong&gt;: Advanced multi-speaker podcasts with Episode Profiles&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîç Intelligent Search&lt;/strong&gt;: Full-text and vector search across all your content&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üí¨ Context-Aware Chat&lt;/strong&gt;: AI conversations powered by your research materials&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìù AI-Assisted Notes&lt;/strong&gt;: Generate insights or write notes manually&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;‚ö° Reasoning Model Support&lt;/strong&gt;: Full support for thinking models like DeepSeek-R1 and Qwen3&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîß Content Transformations&lt;/strong&gt;: Powerful customizable actions to summarize and extract insights&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üåê Comprehensive REST API&lt;/strong&gt;: Full programmatic access for custom integrations &lt;a href="http://localhost:5055/docs"&gt;&lt;img src="https://img.shields.io/badge/API-Documentation-blue?style=flat-square" alt="API Docs" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîê Optional Password Protection&lt;/strong&gt;: Secure public deployments with authentication&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìä Fine-Grained Context Control&lt;/strong&gt;: Choose exactly what to share with AI models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìé Citations&lt;/strong&gt;: Get answers with proper source citations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Three-Column Interface&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Sources&lt;/strong&gt;: Manage all your research materials&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Notes&lt;/strong&gt;: Create manual or AI-generated notes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chat&lt;/strong&gt;: Converse with AI using your content as context&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=D-760MlGwaI"&gt;&lt;img src="https://img.youtube.com/vi/D-760MlGwaI/0.jpg" alt="Check out our podcast sample" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üìö Documentation&lt;/h2&gt; 
&lt;h3&gt;Getting Started&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/introduction.md"&gt;üìñ Introduction&lt;/a&gt;&lt;/strong&gt; - Learn what Open Notebook offers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/quick-start.md"&gt;‚ö° Quick Start&lt;/a&gt;&lt;/strong&gt; - Get up and running in 5 minutes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/installation.md"&gt;üîß Installation&lt;/a&gt;&lt;/strong&gt; - Comprehensive setup guide&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/first-notebook.md"&gt;üéØ Your First Notebook&lt;/a&gt;&lt;/strong&gt; - Step-by-step tutorial&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;User Guide&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/interface-overview.md"&gt;üì± Interface Overview&lt;/a&gt;&lt;/strong&gt; - Understanding the layout&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/notebooks.md"&gt;üìö Notebooks&lt;/a&gt;&lt;/strong&gt; - Organizing your research&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/sources.md"&gt;üìÑ Sources&lt;/a&gt;&lt;/strong&gt; - Managing content types&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/notes.md"&gt;üìù Notes&lt;/a&gt;&lt;/strong&gt; - Creating and managing notes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/chat.md"&gt;üí¨ Chat&lt;/a&gt;&lt;/strong&gt; - AI conversations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/search.md"&gt;üîç Search&lt;/a&gt;&lt;/strong&gt; - Finding information&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Topics&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/podcasts.md"&gt;üéôÔ∏è Podcast Generation&lt;/a&gt;&lt;/strong&gt; - Create professional podcasts&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/transformations.md"&gt;üîß Content Transformations&lt;/a&gt;&lt;/strong&gt; - Customize content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/ai-models.md"&gt;ü§ñ AI Models&lt;/a&gt;&lt;/strong&gt; - AI model configuration&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/development/api-reference.md"&gt;üîß REST API Reference&lt;/a&gt;&lt;/strong&gt; - Complete API documentation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/security.md"&gt;üîê Security&lt;/a&gt;&lt;/strong&gt; - Password protection and privacy&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/index.md"&gt;üöÄ Deployment&lt;/a&gt;&lt;/strong&gt; - Complete deployment guides for all scenarios&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;üó∫Ô∏è Roadmap&lt;/h2&gt; 
&lt;h3&gt;Upcoming Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;React Frontend&lt;/strong&gt;: Modern React-based frontend to replace Streamlit&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Live Front-End Updates&lt;/strong&gt;: Real-time UI updates for smoother experience&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Async Processing&lt;/strong&gt;: Faster UI through asynchronous content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cross-Notebook Sources&lt;/strong&gt;: Reuse research materials across projects&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bookmark Integration&lt;/strong&gt;: Connect with your favorite bookmarking apps&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Recently Completed ‚úÖ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Comprehensive REST API&lt;/strong&gt;: Full programmatic access to all functionality&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Model Support&lt;/strong&gt;: 16+ AI providers including OpenAI, Anthropic, Ollama, LM Studio&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced Podcast Generator&lt;/strong&gt;: Professional multi-speaker podcasts with Episode Profiles&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Content Transformations&lt;/strong&gt;: Powerful customizable actions for content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced Citations&lt;/strong&gt;: Improved layout and finer control for source citations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multiple Chat Sessions&lt;/strong&gt;: Manage different conversations within notebooks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;open issues&lt;/a&gt; for a full list of proposed features and known issues.&lt;/p&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;ü§ù Community &amp;amp; Contributing&lt;/h2&gt; 
&lt;h3&gt;Join the Community&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üí¨ &lt;strong&gt;&lt;a href="https://discord.gg/37XJPXfz2w"&gt;Discord Server&lt;/a&gt;&lt;/strong&gt; - Get help, share ideas, and connect with other users&lt;/li&gt; 
 &lt;li&gt;üêõ &lt;strong&gt;&lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/strong&gt; - Report bugs and request features&lt;/li&gt; 
 &lt;li&gt;‚≠ê &lt;strong&gt;Star this repo&lt;/strong&gt; - Show your support and help others discover Open Notebook&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing&lt;/h3&gt; 
&lt;p&gt;We welcome contributions! We're especially looking for help with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Frontend Development&lt;/strong&gt;: Help build a modern React-based UI (planned replacement for current Streamlit interface)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Testing &amp;amp; Bug Fixes&lt;/strong&gt;: Make Open Notebook more robust&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Feature Development&lt;/strong&gt;: Build the coolest research tool together&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: Improve guides and tutorials&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Current Tech Stack&lt;/strong&gt;: Python, FastAPI, SurrealDB, Streamlit&lt;br /&gt; &lt;strong&gt;Future Roadmap&lt;/strong&gt;: React frontend, enhanced real-time updates&lt;/p&gt; 
&lt;p&gt;See our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; for detailed information on how to get started.&lt;/p&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;Open Notebook is MIT licensed. See the &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;üìû Contact&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Luis Novo&lt;/strong&gt; - &lt;a href="https://twitter.com/lfnovo"&gt;@lfnovo&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Community Support&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üí¨ &lt;a href="https://discord.gg/37XJPXfz2w"&gt;Discord Server&lt;/a&gt; - Get help, share ideas, and connect with users&lt;/li&gt; 
 &lt;li&gt;üêõ &lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;GitHub Issues&lt;/a&gt; - Report bugs and request features&lt;/li&gt; 
 &lt;li&gt;üåê &lt;a href="https://www.open-notebook.ai"&gt;Website&lt;/a&gt; - Learn more about the project&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üôè Acknowledgments&lt;/h2&gt; 
&lt;p&gt;Open Notebook is built on the shoulders of amazing open-source projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/podcast-creator"&gt;Podcast Creator&lt;/a&gt;&lt;/strong&gt; - Advanced podcast generation capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/surreal-commands"&gt;Surreal Commands&lt;/a&gt;&lt;/strong&gt; - Background job processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/content-core"&gt;Content Core&lt;/a&gt;&lt;/strong&gt; - Content processing and management&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/esperanto"&gt;Esperanto&lt;/a&gt;&lt;/strong&gt; - Multi-provider AI model abstraction&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/docling-project/docling"&gt;Docling&lt;/a&gt;&lt;/strong&gt; - Document processing and parsing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; 
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</description>
    </item>
    
    <item>
      <title>NVIDIA/Megatron-LM</title>
      <link>https://github.com/NVIDIA/Megatron-LM</link>
      <description>&lt;p&gt;Ongoing research training transformer models at scale&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;Megatron-LM &amp;amp; Megatron Core&lt;/h1&gt; 
 &lt;h4&gt;GPU-optimized library for training transformer models at scale&lt;/h4&gt; 
 &lt;p&gt;&lt;a href="https://docs.nvidia.com/Megatron-Core/developer-guide/latest/index.html"&gt;&lt;img src="https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/CHANGELOG.md"&gt;&lt;img src="https://img.shields.io/badge/release-0.12.0-green" alt="version" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-Apache-blue" alt="license" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;div align="left"&gt; 
  &lt;h2&gt;‚ö° Quick Start&lt;/h2&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Install Megatron Core with required dependencies
pip install megatron-core
pip install --no-build-isolation transformer-engine[pytorch]

# 2. Clone repository for examples
git clone https://github.com/NVIDIA/Megatron-LM.git
cd Megatron-LM
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;&lt;strong&gt;‚Üí &lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#installation"&gt;Complete Installation Guide&lt;/a&gt;&lt;/strong&gt; - Docker, pip variants (dev,lts,etc.), source installation, and system requirements&lt;/p&gt; 
  &lt;h1&gt;Latest News&lt;/h1&gt; 
  &lt;ul&gt; 
   &lt;li&gt;üîÑ NEW! &lt;strong&gt;&lt;a href="https://github.com/NVIDIA-NeMo/Megatron-Bridge"&gt;Megatron Bridge&lt;/a&gt;&lt;/strong&gt; - Bidirectional converter for interoperability between Hugging Face and Megatron checkpoints, featuring production-ready recipes for popular models.&lt;/li&gt; 
   &lt;li&gt;üó∫Ô∏è &lt;strong&gt;&lt;a href="https://github.com/NVIDIA/Megatron-LM/issues/1729"&gt;MoE Q3-Q4 2025 Roadmap&lt;/a&gt;&lt;/strong&gt; - Comprehensive roadmap for MoE features including DeepSeek-V3, Qwen3, advanced parallelism strategies, FP8 optimizations, and Blackwell performance enhancements.&lt;/li&gt; 
   &lt;li&gt;üöÄ &lt;strong&gt;&lt;a href="https://github.com/NVIDIA/Megatron-LM/issues/1739"&gt;GPT-OSS Implementation&lt;/a&gt;&lt;/strong&gt; - Advanced features including YaRN RoPE scaling, attention sinks, and custom activation functions are being integrated into Megatron Core.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;[2025/06]&lt;/strong&gt; &lt;strong&gt;&lt;a href="https://github.com/yanring/Megatron-MoE-ModelZoo"&gt;Megatron MoE Model Zoo&lt;/a&gt;&lt;/strong&gt; - Best practices and optimized configurations for training DeepSeek-V3, Mixtral, and Qwen3 MoE models with performance benchmarking and checkpoint conversion tools.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;[2025/05]&lt;/strong&gt; Megatron Core v0.11.0 brings new capabilities for multi-data center LLM training (&lt;a href="https://developer.nvidia.com/blog/turbocharge-llm-training-across-long-haul-data-center-networks-with-nvidia-nemo-framework/"&gt;blog&lt;/a&gt;).&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Previous News&lt;/summary&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;[2024/07]&lt;/strong&gt; Megatron Core v0.7 improves scalability and training resiliency and adds support for multimodal training (&lt;a href="https://developer.nvidia.com/blog/train-generative-ai-models-more-efficiently-with-new-nvidia-Megatron-Core-functionalities/"&gt;blog&lt;/a&gt;).&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;[2024/06]&lt;/strong&gt; Megatron Core added supports for Mamba-based models. Check out our paper &lt;a href="https://arxiv.org/pdf/2406.07887"&gt;An Empirical Study of Mamba-based Language Models&lt;/a&gt; and &lt;a href="https://github.com/NVIDIA/Megatron-LM/tree/ssm/examples/mamba"&gt;code example&lt;/a&gt;.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;[2024/01 Announcement]&lt;/strong&gt; NVIDIA has released the core capabilities in &lt;strong&gt;Megatron-LM&lt;/strong&gt; into &lt;a href="https://github.com/NVIDIA/Megatron-LM/tree/main/megatron/core"&gt;&lt;strong&gt;Megatron Core&lt;/strong&gt;&lt;/a&gt; in this repository. Megatron Core expands upon Megatron-LM's GPU-optimized techniques with more cutting-edge innovations on system-level optimizations, featuring composable and modular APIs. Explore the [Megatron Core intro](#Megatron Core) for more details.&lt;/li&gt; 
   &lt;/ul&gt; 
  &lt;/details&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Table of Contents&lt;/summary&gt; 
   &lt;p&gt;&lt;strong&gt;Getting Started&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#-quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#latest-news"&gt;Latest News&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#megatron-overview"&gt;Megatron Overview&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#project-structure"&gt;Project Structure&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#megatron-lm-reference-implementation"&gt;Megatron-LM: Reference Implementation&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#megatron-core-production-library"&gt;Megatron Core: Production Library&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#installation"&gt;Installation&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#-docker-recommended"&gt;Docker (Recommended)&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#-pip-installation"&gt;Pip Installation&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#-source-installation"&gt;Source Installation&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#system-requirements"&gt;System Requirements&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;Core Features&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#performance-benchmarking"&gt;Performance Benchmarking&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#weak-scaling-results"&gt;Weak Scaling Results&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#strong-scaling-results"&gt;Strong Scaling Results&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#ecosystem-libraries"&gt;Ecosystem Libraries&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#training"&gt;Training&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#getting-started"&gt;Getting Started&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#data-preparation"&gt;Data Preparation&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#parallelism-strategies"&gt;Parallelism Strategies&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#data-parallelism-dp"&gt;Data Parallelism (DP)&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#tensor-parallelism-tp"&gt;Tensor Parallelism (TP)&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#pipeline-parallelism-pp"&gt;Pipeline Parallelism (PP)&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#context-parallelism-cp"&gt;Context Parallelism (CP)&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#expert-parallelism-ep"&gt;Expert Parallelism (EP)&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#parallelism-selection-guide"&gt;Parallelism Selection Guide&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#performance-optimizations"&gt;Performance Optimizations&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;Resources&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/examples/"&gt;Examples&lt;/a&gt; - Training scripts and tutorials&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://docs.nvidia.com/Megatron-Core/"&gt;Documentation&lt;/a&gt; - Official docs&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#roadmaps"&gt;Roadmaps&lt;/a&gt; - Development roadmaps and feature tracking&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#-community--support"&gt;Community &amp;amp; Support&lt;/a&gt; - Get help and contribute 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#getting-help"&gt;Getting Help&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ul&gt; 
  &lt;/details&gt; 
  &lt;h1&gt;Megatron Overview&lt;/h1&gt; 
  &lt;h2&gt;Project Structure&lt;/h2&gt; 
  &lt;pre&gt;&lt;code&gt;Megatron-LM/
‚îú‚îÄ‚îÄ megatron/                    
‚îÇ   ‚îú‚îÄ‚îÄ core/                    # Megatron Core (kernels, parallelism, building blocks)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/              # Transformer models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transformer/         # Transformer building blocks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tensor_parallel/     # Tensor parallelism
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipeline_parallel/   # Pipeline parallelism
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ distributed/         # Distributed training (FSDP, DDP)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ optimizer/           # Optimizers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ datasets/            # Dataset loaders
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inference/           # Inference engines
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ export/              # Model export (e.g. TensorRT-LLM)
‚îÇ   ‚îú‚îÄ‚îÄ training/                # Training scripts
‚îÇ   ‚îú‚îÄ‚îÄ inference/               # Inference server
‚îÇ   ‚îú‚îÄ‚îÄ legacy/                  # Legacy components
‚îÇ   ‚îî‚îÄ‚îÄ post_training/           # Post-training (RLHF, etc.)
‚îú‚îÄ‚îÄ examples/                    # Ready-to-use training examples
‚îú‚îÄ‚îÄ tools/                       # Utility tools
‚îú‚îÄ‚îÄ tests/                       # Comprehensive test suite
‚îî‚îÄ‚îÄ docs/                        # Documentation
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h3&gt;Megatron-LM: Reference Implementation&lt;/h3&gt; 
  &lt;p&gt;&lt;strong&gt;Reference implementation&lt;/strong&gt; that includes Megatron Core plus everything needed to train models.&lt;/p&gt; 
  &lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Training state-of-the-art foundation models&lt;/strong&gt; at scale with cutting-edge performance on latest NVIDIA hardware&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Research teams&lt;/strong&gt; exploring new architectures and training techniques&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Learning distributed training&lt;/strong&gt; concepts and best practices&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Quick experimentation&lt;/strong&gt; with proven model configurations&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;&lt;strong&gt;What you get:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Pre-configured training scripts for GPT, LLama, DeepSeek, Qwen, and more.&lt;/li&gt; 
   &lt;li&gt;End-to-end examples from data prep to evaluation&lt;/li&gt; 
   &lt;li&gt;Research-focused tools and utilities&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;h3&gt;Megatron Core: Composable Library&lt;/h3&gt; 
  &lt;p&gt;&lt;strong&gt;Composable library&lt;/strong&gt; with GPU-optimized building blocks for custom training frameworks.&lt;/p&gt; 
  &lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Framework developers&lt;/strong&gt; building on top of modular and optimized components&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Research teams&lt;/strong&gt; needing custom training loops, optimizers, or data pipelines&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;ML engineers&lt;/strong&gt; requiring fault-tolerant training pipelines&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;&lt;strong&gt;What you get:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Composable transformer building blocks (attention, MLP, etc.)&lt;/li&gt; 
   &lt;li&gt;Advanced parallelism strategies (TP, PP, DP, EP, CP)&lt;/li&gt; 
   &lt;li&gt;Pipeline schedules and distributed optimizers&lt;/li&gt; 
   &lt;li&gt;Mixed precision support (FP16, BF16, FP8)&lt;/li&gt; 
   &lt;li&gt;GPU-optimized kernels and memory management&lt;/li&gt; 
   &lt;li&gt;High-performance dataloaders and dataset utilities&lt;/li&gt; 
   &lt;li&gt;Model architectures (LLaMA, Qwen, GPT, Mixtral, Mamba, etc.)&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;h2&gt;Ecosystem Libraries&lt;/h2&gt; 
  &lt;p&gt;&lt;strong&gt;Libraries used by Megatron Core:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/NVIDIA/Megatron-Energon"&gt;Megatron Energon&lt;/a&gt;&lt;/strong&gt; üì£ &lt;strong&gt;NEW!&lt;/strong&gt; - Multi-modal data loader (text, images, video, audio) with distributed loading and dataset blending&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/NVIDIA/TransformerEngine"&gt;Transformer Engine&lt;/a&gt;&lt;/strong&gt; - Optimized kernels and FP8 mixed precision support&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/NVIDIA/nvidia-resiliency-ext"&gt;Resiliency Extension (NVRx)&lt;/a&gt;&lt;/strong&gt; - Fault tolerant training with failure detection and recovery&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;&lt;strong&gt;Libraries using Megatron Core:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/NVIDIA-NeMo/Megatron-Bridge"&gt;Megatron Bridge&lt;/a&gt;&lt;/strong&gt; - Training library with bidirectional Hugging Face ‚Üî Megatron checkpoint conversion, flexible training loops, and production-ready recipes&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/NVIDIA-NeMo/RL"&gt;NeMo RL&lt;/a&gt;&lt;/strong&gt; - Scalable toolkit for efficient reinforcement learning with RLHF, DPO, and other post-training methods&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/overview.html"&gt;NeMo Framework&lt;/a&gt;&lt;/strong&gt; - Enterprise framework with cloud-native support and end-to-end examples&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/NVIDIA/TensorRT-Model-Optimizer"&gt;TensorRT Model Optimizer (ModelOpt)&lt;/a&gt;&lt;/strong&gt; - Model optimization toolkit for quantization, pruning, and distillation&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;&lt;strong&gt;Compatible with:&lt;/strong&gt; &lt;a href="https://github.com/huggingface/accelerate"&gt;Hugging Face Accelerate&lt;/a&gt;, &lt;a href="https://github.com/hpcaitech/ColossalAI"&gt;Colossal-AI&lt;/a&gt;, &lt;a href="https://github.com/microsoft/DeepSpeed"&gt;DeepSpeed&lt;/a&gt;&lt;/p&gt; 
  &lt;h1&gt;Installation&lt;/h1&gt; 
  &lt;h2&gt;üê≥ Docker (Recommended)&lt;/h2&gt; 
  &lt;p&gt;We strongly recommend using the previous releases of &lt;a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch"&gt;PyTorch NGC Container&lt;/a&gt; rather than the latest one for optimal compatibility with Megatron Core release and testing. Our releases are always based on the previous month's NGC container, so this ensures compatibility and stability.&lt;/p&gt; 
  &lt;p&gt;This container comes with all dependencies pre-installed with compatible versions and optimized configurations for NVIDIA GPUs:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;PyTorch (latest stable version)&lt;/li&gt; 
   &lt;li&gt;CUDA, cuDNN, NCCL (latest stable versions)&lt;/li&gt; 
   &lt;li&gt;Support for FP8 on NVIDIA Hopper, Ada, and Blackwell GPUs&lt;/li&gt; 
   &lt;li&gt;For best performance, use NVIDIA Turing GPU architecture generations and later&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;# Run container with mounted directories
docker run --runtime --nvidia --gpus all -it --rm \
  -v /path/to/megatron:/workspace/megatron \
  -v /path/to/dataset:/workspace/dataset \
  -v /path/to/checkpoints:/workspace/checkpoints \
  nvcr.io/nvidia/pytorch:25.04-py3
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h2&gt;Pip Installation&lt;/h2&gt; 
  &lt;p&gt;Megatron Core offers support for two NGC PyTorch containers:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;dev&lt;/code&gt;: Moving head that supports the most recent upstream dependencies&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;lts&lt;/code&gt;: Long-term support of NGC PyTorch 24.01&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;Both containers can be combined with &lt;code&gt;mlm&lt;/code&gt; which adds package dependencies for Megatron-LM on top of Megatron Core.&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;# Install the latest release with minimal dependencies (no Transformer Engine)
pip install megatron-core[dev]
&lt;/code&gt;&lt;/pre&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;# Install packages for LTS support NGC PyTorch 24.01
pip install megatron-core[lts]
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;For a version of Megatron Core with only torch, run:&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;pip install megatron-core
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;For dependencies required by Megatron-LM, please run:&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;pip install megatron-core[mlm]
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h2&gt;Source Installation&lt;/h2&gt; 
  &lt;p&gt;For development or latest features:&lt;/p&gt; 
  &lt;p&gt;For Hybrid models, Megatron Core requires &lt;a href="https://github.com/state-spaces/mamba"&gt;mamba&lt;/a&gt;. If the pre-built wheel in PyPI does not fit your environment, you can fall back to an install script Megatron Core uses in its CI system. For this, please install &lt;code&gt;uv&lt;/code&gt; first:&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;export UV_VERSION=0.7.2
export PATH="$HOME/.local/bin:$PATH"
curl -LsSf https://astral.sh/uv/${UV_VERSION}/install.sh | sh
export UV_PROJECT_ENVIRONMENT=./venv
export PATH="$UV_PROJECT_ENVIRONMENT/bin:$PATH"
export UV_LINK_MODE=copy
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;Run the following command to build upstream dependencies from source:&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;# Clone and install
git clone https://github.com/NVIDIA/Megatron-LM.git
cd Megatron-LM

# Optional: checkout specific release
git checkout core_r0.13.0

bash docker/common/install.sh --environment {dev,lts}
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h2&gt;System Requirements&lt;/h2&gt; 
  &lt;h3&gt;Hardware Requirements&lt;/h3&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;FP8 Support&lt;/strong&gt;: NVIDIA Hopper, Ada, Blackwell GPUs&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Recommended&lt;/strong&gt;: NVIDIA Turing architecture or later&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;h3&gt;Software Requirements&lt;/h3&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;CUDA/cuDNN/NCCL&lt;/strong&gt;: Latest stable versions&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;PyTorch&lt;/strong&gt;: Latest stable version&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Transformer Engine&lt;/strong&gt;: Latest stable version&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt;: 3.12 recommended&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;h1&gt;Performance Benchmarking&lt;/h1&gt; 
  &lt;p&gt;For our latest performance benchmarking results, please refer to &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance_summary.html"&gt;NVIDIA NeMo Framework Performance Summary&lt;/a&gt;.&lt;/p&gt; 
  &lt;p&gt;Our codebase efficiently trains models from 2B to 462B parameters across thousands of GPUs, achieving up to &lt;strong&gt;47% Model FLOP Utilization (MFU)&lt;/strong&gt; on H100 clusters.&lt;/p&gt; 
  &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/images/model_table.png" alt="Model table" /&gt;&lt;/p&gt; 
  &lt;p&gt;&lt;strong&gt;Benchmark Configuration:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Vocabulary size&lt;/strong&gt;: 131,072 tokens&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Sequence length&lt;/strong&gt;: 4096 tokens&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Model scaling&lt;/strong&gt;: Varied hidden size, attention heads, and layers to achieve target parameter counts&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Communication optimizations&lt;/strong&gt;: Fine-grained overlapping with DP (&lt;code&gt;--overlap-grad-reduce&lt;/code&gt;, &lt;code&gt;--overlap-param-gather&lt;/code&gt;), TP (&lt;code&gt;--tp-comm-overlap&lt;/code&gt;), and PP (enabled by default)&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;&lt;strong&gt;Key Results:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;6144 H100 GPUs&lt;/strong&gt;: Successfully benchmarked 462B parameter model training&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Superlinear scaling&lt;/strong&gt;: MFU increases from 41% to 47-48% with model size&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;End-to-end measurement&lt;/strong&gt;: Throughputs include all operations (data loading, optimizer steps, communication, logging)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Production ready&lt;/strong&gt;: Full training pipeline with checkpointing and fault tolerance&lt;/li&gt; 
   &lt;li&gt;&lt;em&gt;Note: Performance results measured without training to convergence&lt;/em&gt;&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;h2&gt;Weak Scaling Results&lt;/h2&gt; 
  &lt;p&gt;Our weak scaled results show superlinear scaling (MFU increases from 41% for the smallest model considered to 47-48% for the largest models); this is because larger GEMMs have higher arithmetic intensity and are consequently more efficient to execute.&lt;/p&gt; 
  &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/images/weak_scaling.png" alt="Weak scaling" /&gt;&lt;/p&gt; 
  &lt;h2&gt;Strong Scaling Results&lt;/h2&gt; 
  &lt;p&gt;We also strong scaled the standard GPT-3 model (our version has slightly more than 175 billion parameters due to larger vocabulary size) from 96 H100 GPUs to 4608 GPUs, using the same batch size of 1152 sequences throughout. Communication becomes more exposed at larger scale, leading to a reduction in MFU from 47% to 42%.&lt;/p&gt; 
  &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/images/strong_scaling.png" alt="Strong scaling" /&gt;&lt;/p&gt; 
  &lt;h1&gt;Training&lt;/h1&gt; 
  &lt;h2&gt;Getting Started&lt;/h2&gt; 
  &lt;h3&gt;Simple Training Example&lt;/h3&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;# Distributed training example (2 GPUs, mock data)
torchrun --nproc_per_node=2 examples/run_simple_mcore_train_loop.py
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h3&gt;LLama-3 Training Example&lt;/h3&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;# 8 GPUs, FP8 precision, mock data
./examples/llama/train_llama3_8b_fp8.sh
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h2&gt;Data Preparation&lt;/h2&gt; 
  &lt;h3&gt;JSONL Data Format&lt;/h3&gt; 
  &lt;pre&gt;&lt;code class="language-json"&gt;{"text": "Your training text here..."}
{"text": "Another training sample..."}
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h3&gt;Basic Preprocessing&lt;/h3&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;python tools/preprocess_data.py \
    --input data.jsonl \
    --output-prefix processed_data \
    --tokenizer-type HuggingFaceTokenizer \
    --tokenizer-model /path/to/tokenizer.model \
    --workers 8 \
    --append-eod
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h3&gt;Key Arguments&lt;/h3&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;--input&lt;/code&gt;: Path to input JSON/JSONL file&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;--output-prefix&lt;/code&gt;: Prefix for output binary files (.bin and .idx)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;--tokenizer-type&lt;/code&gt;: Tokenizer type (&lt;code&gt;HuggingFaceTokenizer&lt;/code&gt;, &lt;code&gt;GPT2BPETokenizer&lt;/code&gt;, etc.)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;--tokenizer-model&lt;/code&gt;: Path to tokenizer model file&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;--workers&lt;/code&gt;: Number of parallel workers for processing&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;--append-eod&lt;/code&gt;: Add end-of-document token&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;!-- **‚Üí [Complete Data Preparation Guide](./docs/data-preparation.md)** - Comprehensive guide covering advanced preprocessing, dataset collection, deduplication, and optimization strategies --&gt; 
  &lt;h1&gt;Parallelism Strategies&lt;/h1&gt; 
  &lt;h2&gt;Data Parallelism (DP)&lt;/h2&gt; 
  &lt;h3&gt;Standard Data Parallel&lt;/h3&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;# Standard DDP - replicate model on each GPU
torchrun --nproc_per_node=8 pretrain_gpt.py \
    --data-parallel-sharding-strategy no_shard
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h3&gt;Fully Sharded Data Parallel (FSDP)&lt;/h3&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;# Megatron's optimized FSDP (~15% faster than PyTorch FSDP2)
--use-custom-fsdp

# PyTorch FSDP2
--use-torch-fsdp2

# Sharding strategies
--data-parallel-sharding-strategy optim              # Shard optimizer states (ZeRO-1)
--data-parallel-sharding-strategy optim_grads        # Shard gradients + optimizer (ZeRO-2)
--data-parallel-sharding-strategy optim_grads_params # Shard parameters + gradients + optimizer (ZeRO-3)
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h2&gt;Tensor Parallelism (TP)&lt;/h2&gt; 
  &lt;p&gt;Split individual model layers across GPUs:&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;--tensor-model-parallel-size 4  # 4-way tensor parallelism
--sequence-parallel             # Enable sequence parallelism (recommended with TP)
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h2&gt;Pipeline Parallelism (PP)&lt;/h2&gt; 
  &lt;p&gt;Split model depth across GPUs:&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;--pipeline-model-parallel-size 8     # 8 pipeline stages
--virtual-pipeline-model-parallel-size 4  # Virtual pipeline for better load balancing
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h2&gt;Context Parallelism (CP)&lt;/h2&gt; 
  &lt;p&gt;Split long sequences across GPUs for handling long contexts:&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;--context-parallel-size 2                    # 2-way context parallelism
--cp-comm-type p2p                          # Communication: p2p, a2a, allgather, a2a+p2p
--hierarchical-context-parallel-sizes 2 4   # Hierarchical context parallelism
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h2&gt;Expert Parallelism (EP)&lt;/h2&gt; 
  &lt;p&gt;For Mixture of Experts (MoE) models:&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;--expert-model-parallel-size 4  # 4-way expert parallelism
--num-experts 8                 # 8 experts per MoE layer
--moe-grouped-gemm              # Optimize expert computation
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h2&gt;Combining Parallelism Strategies&lt;/h2&gt; 
  &lt;h3&gt;Parallelism Selection Guide&lt;/h3&gt; 
  &lt;p&gt;Based on &lt;a href="https://github.com/NVIDIA/NeMo/tree/main/scripts/performance/recommended_model_configs"&gt;NVIDIA NeMo production configurations&lt;/a&gt;:&lt;/p&gt; 
  &lt;table&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th&gt;Model&lt;/th&gt; 
     &lt;th&gt;Size&lt;/th&gt; 
     &lt;th&gt;GPUs&lt;/th&gt; 
     &lt;th&gt;TP&lt;/th&gt; 
     &lt;th&gt;PP&lt;/th&gt; 
     &lt;th&gt;CP&lt;/th&gt; 
     &lt;th&gt;EP&lt;/th&gt; 
     &lt;th&gt;Notes&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;LLama-3&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;8&lt;/td&gt; 
     &lt;td&gt;1&lt;/td&gt; 
     &lt;td&gt;1&lt;/td&gt; 
     &lt;td&gt;2&lt;/td&gt; 
     &lt;td&gt;1&lt;/td&gt; 
     &lt;td&gt;CP for long seqlen (8K)&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;LLama-3&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;70B&lt;/td&gt; 
     &lt;td&gt;64&lt;/td&gt; 
     &lt;td&gt;4&lt;/td&gt; 
     &lt;td&gt;4&lt;/td&gt; 
     &lt;td&gt;2&lt;/td&gt; 
     &lt;td&gt;1&lt;/td&gt; 
     &lt;td&gt;TP+PP&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;LLama-3.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;405B&lt;/td&gt; 
     &lt;td&gt;1024&lt;/td&gt; 
     &lt;td&gt;8&lt;/td&gt; 
     &lt;td&gt;8&lt;/td&gt; 
     &lt;td&gt;2&lt;/td&gt; 
     &lt;td&gt;1&lt;/td&gt; 
     &lt;td&gt;3D parallelism for scale&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;GPT-3&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;175B&lt;/td&gt; 
     &lt;td&gt;128-512&lt;/td&gt; 
     &lt;td&gt;4&lt;/td&gt; 
     &lt;td&gt;8&lt;/td&gt; 
     &lt;td&gt;1&lt;/td&gt; 
     &lt;td&gt;1&lt;/td&gt; 
     &lt;td&gt;Large model config&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;Mixtral&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;8x7B&lt;/td&gt; 
     &lt;td&gt;64&lt;/td&gt; 
     &lt;td&gt;1&lt;/td&gt; 
     &lt;td&gt;4&lt;/td&gt; 
     &lt;td&gt;1&lt;/td&gt; 
     &lt;td&gt;8&lt;/td&gt; 
     &lt;td&gt;EP for MoE&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;Mixtral&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;8x22B&lt;/td&gt; 
     &lt;td&gt;256&lt;/td&gt; 
     &lt;td&gt;4&lt;/td&gt; 
     &lt;td&gt;4&lt;/td&gt; 
     &lt;td&gt;8&lt;/td&gt; 
     &lt;td&gt;8&lt;/td&gt; 
     &lt;td&gt;Combined TP+EP for large MoE&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;DeepSeek-V3&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;671B&lt;/td&gt; 
     &lt;td&gt;1024&lt;/td&gt; 
     &lt;td&gt;2&lt;/td&gt; 
     &lt;td&gt;16&lt;/td&gt; 
     &lt;td&gt;1&lt;/td&gt; 
     &lt;td&gt;64&lt;/td&gt; 
     &lt;td&gt;Large MoE config&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
  &lt;h3&gt;MoE-Specific Requirements&lt;/h3&gt; 
  &lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: When combining Expert Parallelism (EP) with Tensor Parallelism (TP), &lt;strong&gt;Sequence Parallelism (SP) must be enabled&lt;/strong&gt;.&lt;/p&gt; 
  &lt;h2&gt;Performance Optimizations&lt;/h2&gt; 
  &lt;table&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th&gt;Feature&lt;/th&gt; 
     &lt;th&gt;Flag&lt;/th&gt; 
     &lt;th&gt;Benefit&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;FlashAttention&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;code&gt;--attention-backend&lt;/code&gt;&lt;/td&gt; 
     &lt;td&gt;Faster attention and lower memory usage&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;FP8 Training&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;code&gt;--fp8-hybrid&lt;/code&gt;&lt;/td&gt; 
     &lt;td&gt;Faster training&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;Activation Checkpointing&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;code&gt;--recompute-activations&lt;/code&gt;&lt;/td&gt; 
     &lt;td&gt;Reduced memory usage&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;Data Parallelism Communication Overlap&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;code&gt;--overlap-grad-reduce&lt;/code&gt;&lt;/td&gt; 
     &lt;td&gt;Faster distributed training&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;Distributed Optimizer&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;code&gt;--use-distributed-optimizer&lt;/code&gt;&lt;/td&gt; 
     &lt;td&gt;Reduced checkpointing time&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
  &lt;p&gt;&lt;strong&gt;‚Üí &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance-guide.html#performance-tuning-guide"&gt;NVIDIA NeMo Framework Performance Tuning Guide&lt;/a&gt;&lt;/strong&gt; - Comprehensive performance optimization guide covering advanced tuning techniques, communication overlaps, memory optimizations, and profiling options.&lt;/p&gt; 
  &lt;h3&gt;FlashAttention&lt;/h3&gt; 
  &lt;p&gt;&lt;a href="https://github.com/Dao-AILab/flash-attention"&gt;FlashAttention&lt;/a&gt; is a fast and memory-efficient attention algorithm. We recommend the default usage, which uses cuDNN for attention via Transformer Engine and provides up to 50% speedups on forward and 84% on backward propagation with FP8 kernels. The &lt;code&gt;flash-attn&lt;/code&gt; package is also supported via &lt;code&gt;--use-flash-attn&lt;/code&gt;.&lt;/p&gt; 
  &lt;h3&gt;Mixed Precision Training&lt;/h3&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;--fp16                    # Standard FP16
--bf16                    # BFloat16 (recommended for large models)
--fp8-hybrid              # FP8 training (Hopper, Ada, and Blackwell GPUs)
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h3&gt;Activation Checkpointing and Recomputation&lt;/h3&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;# For limited memory
--recompute-activations

# For extreme memory constraints
--recompute-granularity full \
--recompute-method uniform
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h3&gt;Data Parallelism Communication Overlap&lt;/h3&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;--overlap-grad-reduce
--overlap-param-gather
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h3&gt;Distributed Optimizer&lt;/h3&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;--use-distributed-optimizer
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h1&gt;Roadmaps&lt;/h1&gt; 
  &lt;p&gt;Stay up-to-date with our development roadmaps and planned features:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/NVIDIA/Megatron-LM/issues/1729"&gt;MoE Q3-Q4 2025 Roadmap&lt;/a&gt;&lt;/strong&gt; - Comprehensive MoE feature development including DeepSeek-V3, Qwen3, advanced parallelism, FP8 optimizations, and Blackwell enhancements&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/NVIDIA/Megatron-LM/issues/1739"&gt;GPT-OSS Implementation Tracker&lt;/a&gt;&lt;/strong&gt; - Advanced features including YaRN RoPE scaling, attention sinks, and custom activation functions&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;&lt;em&gt;More roadmap trackers will be added soon.&lt;/em&gt;&lt;/p&gt; 
  &lt;h1&gt;Community &amp;amp; Support&lt;/h1&gt; 
  &lt;h2&gt;Getting Help&lt;/h2&gt; 
  &lt;ul&gt; 
   &lt;li&gt;üìñ &lt;strong&gt;&lt;a href="https://docs.nvidia.com/Megatron-Core/"&gt;Documentation&lt;/a&gt;&lt;/strong&gt; - Official documentation&lt;/li&gt; 
   &lt;li&gt;üêõ &lt;strong&gt;&lt;a href="https://github.com/NVIDIA/Megatron-LM/issues"&gt;Issues&lt;/a&gt;&lt;/strong&gt; - Bug reports and feature requests&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;h2&gt;Contributing&lt;/h2&gt; 
  &lt;p&gt;We ‚ù§Ô∏è contributions! Ways to contribute:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;üêõ &lt;strong&gt;Report bugs&lt;/strong&gt; - Help us improve reliability&lt;/li&gt; 
   &lt;li&gt;üí° &lt;strong&gt;Suggest features&lt;/strong&gt; - Shape the future of Megatron Core&lt;/li&gt; 
   &lt;li&gt;üìù &lt;strong&gt;Improve docs&lt;/strong&gt; - Make Megatron Core more accessible&lt;/li&gt; 
   &lt;li&gt;üîß &lt;strong&gt;Submit PRs&lt;/strong&gt; - Contribute code improvements&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;&lt;strong&gt;‚Üí &lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;h2&gt;Citation&lt;/h2&gt; 
  &lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{megatron-lm,
  title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>QwenLM/Qwen3</title>
      <link>https://github.com/QwenLM/Qwen3</link>
      <description>&lt;p&gt;Qwen3 is the large language model series developed by Qwen team, Alibaba Cloud.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Qwen3&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/logo_qwen3.png" width="400" /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align="center"&gt; üíú &lt;a href="https://chat.qwen.ai/"&gt;&lt;b&gt;Qwen Chat&lt;/b&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ó &lt;a href="https://huggingface.co/Qwen"&gt;Hugging Face&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ñ &lt;a href="https://modelscope.cn/organization/qwen"&gt;ModelScope&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; üìë &lt;a href="https://arxiv.org/abs/2505.09388"&gt;Paper&lt;/a&gt; &amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; üìë &lt;a href="https://qwenlm.github.io/blog/qwen3/"&gt;Blog&lt;/a&gt; &amp;nbsp;&amp;nbsp; ÔΩú &amp;nbsp;&amp;nbsp;üìñ &lt;a href="https://qwen.readthedocs.io/"&gt;Documentation&lt;/a&gt; &lt;br /&gt; üñ•Ô∏è &lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-Demo"&gt;Demo&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üí¨ &lt;a href="https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png"&gt;WeChat (ÂæÆ‰ø°)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü´® &lt;a href="https://discord.gg/CV4E9rpNSD"&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;/p&gt; 
&lt;p&gt;Visit our Hugging Face or ModelScope organization (click links above), search checkpoints with names starting with &lt;code&gt;Qwen3-&lt;/code&gt; or visit the &lt;a href="https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f"&gt;Qwen3 collection&lt;/a&gt;, and you will find all you need! Enjoy!&lt;/p&gt; 
&lt;p&gt;To learn more about Qwen3, feel free to read our documentation [&lt;a href="https://qwen.readthedocs.io/en/latest/"&gt;EN&lt;/a&gt;|&lt;a href="https://qwen.readthedocs.io/zh-cn/latest/"&gt;ZH&lt;/a&gt;]. Our documentation consists of the following sections:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Quickstart: the basic usages and demonstrations;&lt;/li&gt; 
 &lt;li&gt;Inference: the guidance for the inference with Transformers, including batch inference, streaming, etc.;&lt;/li&gt; 
 &lt;li&gt;Run Locally: the instructions for running LLM locally on CPU and GPU, with frameworks like llama.cpp and Ollama;&lt;/li&gt; 
 &lt;li&gt;Deployment: the demonstration of how to deploy Qwen for large-scale inference with frameworks like SGLang, vLLM, TGI, etc.;&lt;/li&gt; 
 &lt;li&gt;Quantization: the practice of quantizing LLMs with GPTQ, AWQ, as well as the guidance for how to make high-quality quantized GGUF files;&lt;/li&gt; 
 &lt;li&gt;Training: the instructions for post-training, including SFT and RLHF (TODO) with frameworks like Axolotl, LLaMA-Factory, etc.&lt;/li&gt; 
 &lt;li&gt;Framework: the usage of Qwen with frameworks for application, e.g., RAG, Agent, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;h3&gt;Qwen3-2507&lt;/h3&gt; 
&lt;p&gt;Over the past three months, we continued to explore the potential of the Qwen3 families and we are excited to introduce the updated &lt;strong&gt;Qwen3-2507&lt;/strong&gt; in two variants, Qwen3-Instruct-2507 and Qwen3-Thinking-2507, and three sizes, 235B-A22B, 30B-A3B, and 4B.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Qwen3-Instruct-2507&lt;/strong&gt; is the updated version of the previous Qwen3 non-thinking mode, featuring the following key enhancements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Significant improvements&lt;/strong&gt; in general capabilities, including &lt;strong&gt;instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Substantial gains&lt;/strong&gt; in long-tail knowledge coverage across &lt;strong&gt;multiple languages&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Markedly better alignment&lt;/strong&gt; with user preferences in &lt;strong&gt;subjective and open-ended tasks&lt;/strong&gt;, enabling more helpful responses and higher-quality text generation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced capabilities&lt;/strong&gt; in &lt;strong&gt;256K-token long-context understanding&lt;/strong&gt;, extendable up to &lt;strong&gt;1 million tokens&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Qwen3-Thinking-2507&lt;/strong&gt; is the continuation of Qwen3 thinking model, with improved quality and depth of reasoning, featuring the following key enhancements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Significantly improved performance&lt;/strong&gt; on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise ‚Äî achieving &lt;strong&gt;state-of-the-art results among open-weight thinking models&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Markedly better general capabilities&lt;/strong&gt;, such as instruction following, tool usage, text generation, and alignment with human preferences.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced 256K long-context understanding&lt;/strong&gt; capabilities, extendable up to &lt;strong&gt;1 million tokens&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Previous Qwen3 Release&lt;/b&gt;&lt;/summary&gt; 
 &lt;h3&gt;Qwen3 (aka Qwen3-2504)&lt;/h3&gt; 
 &lt;p&gt; We are excited to announce the release of Qwen3, the latest addition to the Qwen family of large language models. These models represent our most advanced and intelligent systems to date, improving from our experience in building QwQ and Qwen2.5. We are making the weights of Qwen3 available to the public, including both dense and Mixture-of-Expert (MoE) models. &lt;br /&gt;&lt;br /&gt; The highlights from Qwen3 include: &lt;/p&gt;
 &lt;ul&gt; 
  &lt;li&gt;&lt;b&gt;Dense and Mixture-of-Experts (MoE) models of various sizes&lt;/b&gt;, available in 0.6B, 1.7B, 4B, 8B, 14B, 32B and 30B-A3B, 235B-A22B.&lt;/li&gt; 
  &lt;li&gt;&lt;b&gt;Seamless switching between thinking mode&lt;/b&gt; (for complex logical reasoning, math, and coding) and &lt;b&gt;non-thinking mode&lt;/b&gt; (for efficient, general-purpose chat), ensuring optimal performance across various scenarios.&lt;/li&gt; 
  &lt;li&gt;&lt;b&gt;Significantly enhancement in reasoning capabilities&lt;/b&gt;, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.&lt;/li&gt; 
  &lt;li&gt;&lt;b&gt;Superior human preference alignment&lt;/b&gt;, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.&lt;/li&gt; 
  &lt;li&gt;&lt;b&gt;Expertise in agent capabilities&lt;/b&gt;, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.&lt;/li&gt; 
  &lt;li&gt;&lt;b&gt;Support of 100+ languages and dialects&lt;/b&gt; with strong capabilities for &lt;b&gt;multilingual instruction following&lt;/b&gt; and &lt;b&gt;translation&lt;/b&gt;.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;2025.08.08: You can now use Qwen3-2507 to handle ultra-long inputs of &lt;strong&gt;1 million tokens&lt;/strong&gt;! See the update modelcards (&lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507"&gt;235B-A22B-Instruct-2507&lt;/a&gt;, &lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507"&gt;235B-A22B-Thinking-2507&lt;/a&gt;, &lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507"&gt;A30B-A3B-Instruct-2507&lt;/a&gt;, &lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507"&gt;A30B-A3B-Thinking-2507&lt;/a&gt;) for how to enable this feature.&lt;/li&gt; 
 &lt;li&gt;2025.08.06: The final open release of Qwen3-2507, &lt;a href="https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507"&gt;Qwen3-4B-Instruct-2507&lt;/a&gt; and &lt;a href="https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507"&gt;Qwen3-4B-Thinking-2507&lt;/a&gt;, is out!&lt;/li&gt; 
 &lt;li&gt;2025.07.31: Qwen3-30B-A3B-Thinking-2507 is released. Check out the &lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507"&gt;modelcard&lt;/a&gt; for more details!&lt;/li&gt; 
 &lt;li&gt;2025.07.30: Qwen3-30B-A3B-Instruct-2507 is released. Check out the &lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507"&gt;modelcard&lt;/a&gt; for more details!&lt;/li&gt; 
 &lt;li&gt;2025.07.25: We released the updated version of Qwen3-235B-A22B thinking mode, named Qwen3-235B-A22B-Thinking-2507. Check out the &lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507"&gt;modelcard&lt;/a&gt; for more details!&lt;/li&gt; 
 &lt;li&gt;2025.07.21: We released the updated version of Qwen3-235B-A22B non-thinking mode, named Qwen3-235B-A22B-Instruct-2507, featuring significant enhancements over the previous version and supporting 256K-token long-context understanding. Check our &lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507"&gt;modelcard&lt;/a&gt; for more details!&lt;/li&gt; 
 &lt;li&gt;2025.04.29: We released the Qwen3 series. Check our &lt;a href="https://qwenlm.github.io/blog/qwen3"&gt;blog&lt;/a&gt; for more details!&lt;/li&gt; 
 &lt;li&gt;2024.09.19: We released the Qwen2.5 series. This time there are 3 extra model sizes: 3B, 14B, and 32B for more possibilities. Check our &lt;a href="https://qwenlm.github.io/blog/qwen2.5"&gt;blog&lt;/a&gt; for more!&lt;/li&gt; 
 &lt;li&gt;2024.06.06: We released the Qwen2 series. Check our &lt;a href="https://qwenlm.github.io/blog/qwen2/"&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2024.03.28: We released the first MoE model of Qwen: Qwen1.5-MoE-A2.7B! Temporarily, only HF transformers and vLLM support the model. We will soon add the support of llama.cpp, mlx-lm, etc. Check our &lt;a href="https://qwenlm.github.io/blog/qwen-moe/"&gt;blog&lt;/a&gt; for more information!&lt;/li&gt; 
 &lt;li&gt;2024.02.05: We released the Qwen1.5 series.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;p&gt;Detailed evaluation results are reported in this &lt;a href="https://qwenlm.github.io/blog/qwen3/"&gt;üìë blog (Qwen3-2504)&lt;/a&gt; and this &lt;a href=""&gt;üìë blog (Qwen3-2507) [coming soon]&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For requirements on GPU memory and the respective throughput, see results &lt;a href="https://qwen.readthedocs.io/en/latest/getting_started/speed_benchmark.html"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Run Qwen3&lt;/h2&gt; 
&lt;h3&gt;ü§ó Transformers&lt;/h3&gt; 
&lt;p&gt;Transformers is a library of pretrained natural language processing for inference and training. The latest version of &lt;code&gt;transformers&lt;/code&gt; is recommended and &lt;code&gt;transformers&amp;gt;=4.51.0&lt;/code&gt; is required.&lt;/p&gt; 
&lt;h4&gt;Qwen3-Instruct-2507&lt;/h4&gt; 
&lt;p&gt;The following contains a code snippet illustrating how to use Qwen3-30B-A3B-Instruct-2507 to generate content based on given inputs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen3-30B-A3B-Instruct-2507"

# load the tokenizer and the model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)

# prepare the model input
prompt = "Give me a short introduction to large language model."
messages = [
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# conduct text completion
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=16384
)
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() 

content = tokenizer.decode(output_ids, skip_special_tokens=True)

print("content:", content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] Qwen3-Instruct-2507 supports only non-thinking mode and does not generate &lt;code&gt;&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;&lt;/code&gt; blocks in its output. Meanwhile, specifying &lt;code&gt;enable_thinking=False&lt;/code&gt; is no longer required.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Qwen3-Thinking-2507&lt;/h4&gt; 
&lt;p&gt;The following contains a code snippet illustrating how to use Qwen3-30B-A3B-Thinking-2507 to generate content based on given inputs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen3-30B-A3B-Thinking-2507"

# load the tokenizer and the model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)

# prepare the model input
prompt = "Give me a short introduction to large language model."
messages = [
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# conduct text completion
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=32768
)
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() 

# parsing thinking content
try:
    # rindex finding 151668 (&amp;lt;/think&amp;gt;)
    index = len(output_ids) - output_ids[::-1].index(151668)
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip("\n")
content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip("\n")

print("thinking content:", thinking_content)  # no opening &amp;lt;think&amp;gt; tag
print("content:", content)

&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] Qwen3-Thinking-2507 supports only thinking mode. Additionally, to enforce model thinking, the default chat template automatically includes &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt;. Therefore, it is normal for the model's output to contain only &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt; without an explicit opening &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; tag.&lt;/p&gt; 
 &lt;p&gt;Qwen3-Thinking-2507 also features an increased thinking length. We strongly recommend its use in highly complex reasoning tasks with adequate maximum generation length.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Switching Thinking/Non-thinking Modes for Previous Qwen3 Models&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt; By default, Qwen3 models will think before response. This could be controlled by &lt;/p&gt;
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;enable_thinking=False&lt;/code&gt;: Passing &lt;code&gt;enable_thinking=False&lt;/code&gt; to `tokenizer.apply_chat_template` will strictly prevent the model from generating thinking content.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;/think&lt;/code&gt; and &lt;code&gt;/no_think&lt;/code&gt; instructions: Use those words in the system or user message to signify whether Qwen3 should think. In multi-turn conversations, the latest instruction is followed.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;ModelScope&lt;/h3&gt; 
&lt;p&gt;We strongly advise users especially those in mainland China to use ModelScope. ModelScope adopts a Python API similar to Transformers. The CLI tool &lt;code&gt;modelscope download&lt;/code&gt; can help you solve issues concerning downloading checkpoints. For vLLM and SGLang, the environment variable &lt;code&gt;VLLM_USE_MODELSCOPE=true&lt;/code&gt; and &lt;code&gt;SGLANG_USE_MODELSCOPE=true&lt;/code&gt; can be used respectively.&lt;/p&gt; 
&lt;h3&gt;llama.cpp&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp"&gt;&lt;code&gt;llama.cpp&lt;/code&gt;&lt;/a&gt; enables LLM inference with minimal setup and state-of-the-art performance on a wide range of hardware. &lt;code&gt;llama.cpp&amp;gt;=b5401&lt;/code&gt; is recommended for the full support of Qwen3.&lt;/p&gt; 
&lt;p&gt;To use the CLI, run the following in a terminal:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;./llama-cli -hf Qwen/Qwen3-8B-GGUF:Q8_0 --jinja --color -ngl 99 -fa -sm row --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0 -c 40960 -n 32768 --no-context-shift
# CTRL+C to exit
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use the API server, run the following in a terminal:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;./llama-server -hf Qwen/Qwen3-8B-GGUF:Q8_0 --jinja --reasoning-format deepseek -ngl 99 -fa -sm row --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0 -c 40960 -n 32768 --no-context-shift --port 8080
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;A simple web front end will be at &lt;code&gt;http://localhost:8080&lt;/code&gt; and an OpenAI-compatible API will be at &lt;code&gt;http://localhost:8080/v1&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For additional guides, please refer to &lt;a href="https://qwen.readthedocs.io/en/latest/run_locally/llama.cpp.html"&gt;our documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] llama.cpp adopts "rotating context management" and infinite generation is made possible by evicting earlier tokens. It could configured by parameters and the commands above effectively disable it. For more details, please refer to &lt;a href="https://qwen.readthedocs.io/en/latest/run_locally/llama.cpp.html#llama-cli"&gt;our documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Ollama&lt;/h3&gt; 
&lt;p&gt;After &lt;a href="https://ollama.com/"&gt;installing Ollama&lt;/a&gt;, you can initiate the Ollama service with the following command (Ollama v0.9.0 or higher is recommended):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ollama serve
# You need to keep this service running whenever you are using ollama
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To pull a model checkpoint and run the model, use the &lt;code&gt;ollama run&lt;/code&gt; command. You can specify a model size by adding a suffix to &lt;code&gt;qwen3&lt;/code&gt;, such as &lt;code&gt;:8b&lt;/code&gt; or &lt;code&gt;:30b-a3b&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ollama run qwen3:8b
# Setting parameters, type "/set parameter num_ctx 40960" and "/set parameter num_predict 32768"
# To exit, type "/bye" and press ENTER
# For Qwen3-2504 models,
# - To enable thinking, which is the default, type "/set think"
# - To disable thinking, type "/set nothink"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also access the Ollama service via its OpenAI-compatible API. Please note that you need to (1) keep &lt;code&gt;ollama serve&lt;/code&gt; running while using the API, and (2) execute &lt;code&gt;ollama run qwen3:8b&lt;/code&gt; before utilizing this API to ensure that the model checkpoint is prepared. The API is at &lt;code&gt;http://localhost:11434/v1/&lt;/code&gt; by default.&lt;/p&gt; 
&lt;p&gt;For additional details, please visit &lt;a href="https://ollama.com/"&gt;ollama.ai&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] Ollama's naming may not be consistent with the Qwen's original naming. For example, &lt;code&gt;qwen3:30b-a3b&lt;/code&gt; in Ollama points to &lt;code&gt;qwen3:30b-a3b-thinking-2507-q4_K_M&lt;/code&gt; as of August 2025. Please check &lt;a href="https://ollama.com/library/qwen3/tags"&gt;https://ollama.com/library/qwen3/tags&lt;/a&gt; before use.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] Ollama adopts the same "rotating context management" with llama.cpp. However, its default settings (&lt;code&gt;num_ctx&lt;/code&gt; 2048 and &lt;code&gt;num_predict&lt;/code&gt; -1), suggesting infinite generation with a 2048-token context, could lead to trouble for Qwen3 models. We recommend setting &lt;code&gt;num_ctx&lt;/code&gt; and &lt;code&gt;num_predict&lt;/code&gt; properly.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;LMStudio&lt;/h3&gt; 
&lt;p&gt;Qwen3 has already been supported by &lt;a href="https://lmstudio.ai/"&gt;lmstudio.ai&lt;/a&gt;. You can directly use LMStudio with our GGUF files.&lt;/p&gt; 
&lt;h3&gt;ExecuTorch&lt;/h3&gt; 
&lt;p&gt;To export and run on ExecuTorch (iOS, Android, Mac, Linux, and more), please follow this &lt;a href="https://github.com/pytorch/executorch/raw/main/examples/models/qwen3/README.md"&gt;example&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;MNN&lt;/h3&gt; 
&lt;p&gt;To export and run on MNN, which supports Qwen3 on mobile devices, please visit &lt;a href="https://github.com/alibaba/MNN"&gt;Alibaba MNN&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;MLX LM&lt;/h3&gt; 
&lt;p&gt;If you are running on Apple Silicon, &lt;a href="https://github.com/ml-explore/mlx-lm"&gt;&lt;code&gt;mlx-lm&lt;/code&gt;&lt;/a&gt; also supports Qwen3 (&lt;code&gt;mlx-lm&amp;gt;=0.24.0&lt;/code&gt;). Look for models ending with MLX on Hugging Face Hub.&lt;/p&gt; 
&lt;h3&gt;OpenVINO&lt;/h3&gt; 
&lt;p&gt;If you are running on Intel CPU or GPU, &lt;a href="https://github.com/openvinotoolkit"&gt;OpenVINO toolkit&lt;/a&gt; supports Qwen3. You can follow this &lt;a href="https://github.com/openvinotoolkit/openvino_notebooks/raw/latest/notebooks/llm-chatbot/llm-chatbot.ipynb"&gt;chatbot example&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Deploy Qwen3&lt;/h2&gt; 
&lt;p&gt;Qwen3 is supported by multiple inference frameworks. Here we demonstrate the usage of &lt;code&gt;SGLang&lt;/code&gt;, &lt;code&gt;vLLM&lt;/code&gt; and &lt;code&gt;TensorRT-LLM&lt;/code&gt;. You can also find Qwen3 models from various inference providers, e.g., &lt;a href="https://www.alibabacloud.com/en/product/modelstudio"&gt;Alibaba Cloud Model Studio&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;SGLang&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/sgl-project/sglang"&gt;SGLang&lt;/a&gt; is a fast serving framework for large language models and vision language models. SGLang could be used to launch a server with OpenAI-compatible API service. &lt;code&gt;sglang&amp;gt;=0.4.6.post1&lt;/code&gt; is required.&lt;/p&gt; 
&lt;p&gt;For Qwen3-Instruct-2507,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B-Instruct-2507 --port 30000 --context-length 262144
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Qwen3-Thinking-2507,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B-Thinking-2507 --port 30000 --context-length 262144 --reasoning-parser deepseek-r1
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Qwen3, it is&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python -m sglang.launch_server --model-path Qwen/Qwen3-8B --port 30000 --context-length 131072 --reasoning-parser qwen3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;An OpenAI-compatible API will be available at &lt;code&gt;http://localhost:30000/v1&lt;/code&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] Due to the preprocessing of API requests in SGLang, which drops all &lt;code&gt;reasoning_content&lt;/code&gt; fields, the quality of &lt;strong&gt;multi-step tool use with Qwen3 thinking models&lt;/strong&gt; may be suboptimal, which requires the existence of the related thinking content. While the fixes are being worked on, as a workdaround, we recommend passing the content as it is, without extracting thinking content, and the chat template will correctly handle the processing.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;vLLM&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt; is a high-throughput and memory-efficient inference and serving engine for LLMs. &lt;code&gt;vllm&amp;gt;=0.9.0&lt;/code&gt; is recommended.&lt;/p&gt; 
&lt;p&gt;For Qwen3-Instruct-2507,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;vllm serve Qwen/Qwen3-30B-A3B-Instruct-2507 --port 8000 --max-model-len 262144
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Qwen3-Thinking-2507,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;vllm serve Qwen/Qwen3-30B-A3B-Thinking-2507 --port 8000 --max-model-len 262144 --enable-reasoning --reasoning-parser deepseek_r1
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Qwen3, it is&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;vllm serve Qwen/Qwen3-8B --port 8000 --max-model-len 131072 --enable-reasoning --reasoning-parser qwen3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;An OpenAI-compatible API will be available at &lt;code&gt;http://localhost:8000/v1&lt;/code&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] Due to the preprocessing of API requests in vLLM, which drops all &lt;code&gt;reasoning_content&lt;/code&gt; fields, the quality of &lt;strong&gt;multi-step tool use with Qwen3 thinking models&lt;/strong&gt; may be suboptimal, which requires the existence of the related thinking content. While the fixes are being worked on, as a workdaround, we recommend passing the content as it is, without extracting thinking content, and the chat template will correctly handle the processing.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;TensorRT-LLM&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/NVIDIA/TensorRT-LLM"&gt;TensorRT-LLM&lt;/a&gt; is an open-source LLM inference engine from NVIDIA, which provides optimizations including custom attention kernels, quantization and more on NVIDIA GPUs. Qwen3 is supported in its re-architected &lt;a href="https://nvidia.github.io/TensorRT-LLM/torch.html"&gt;PyTorch backend&lt;/a&gt;. &lt;code&gt;tensorrt_llm&amp;gt;=0.20.0rc3&lt;/code&gt; is recommended. Please refer to the &lt;a href="https://github.com/NVIDIA/TensorRT-LLM/raw/main/examples/models/core/qwen/README.md#qwen3"&gt;README&lt;/a&gt; page for more details.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;trtllm-serve Qwen/Qwen3-8B --host localhost --port 8000 --backend pytorch
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;An OpenAI-compatible API will be available at &lt;code&gt;http://localhost:8000/v1&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;MindIE&lt;/h3&gt; 
&lt;p&gt;For deployment on Ascend NPUs, please visit &lt;a href="https://modelers.cn/"&gt;Modelers&lt;/a&gt; and search for Qwen3.&lt;/p&gt; 
&lt;!-- 
### OpenLLM

[OpenLLM](https://github.com/bentoml/OpenLLM) allows you to easily run¬†Qwen2.5 as OpenAI-compatible APIs. You can start a model server using `openllm serve`. For example:

```bash
openllm serve qwen2.5:7b
```

The server is active at `http://localhost:3000/`, providing OpenAI-compatible APIs. You can create an OpenAI client to call its chat API. For more information, refer to [our documentation](https://qwen.readthedocs.io/en/latest/deployment/openllm.html). --&gt; 
&lt;h2&gt;Build with Qwen3&lt;/h2&gt; 
&lt;h3&gt;Tool Use&lt;/h3&gt; 
&lt;p&gt;For tool use capabilities, we recommend taking a look at &lt;a href="https://github.com/QwenLM/Qwen-Agent"&gt;Qwen-Agent&lt;/a&gt;, which provides a wrapper around these APIs to support tool use or function calling with MCP support. Tool use with Qwen3 can also be conducted with SGLang, vLLM, Transformers, llama.cpp, Ollama, etc. Follow guides in our documentation to see how to enable the support.&lt;/p&gt; 
&lt;h3&gt;Finetuning&lt;/h3&gt; 
&lt;p&gt;We advise you to use training frameworks, including &lt;a href="https://github.com/OpenAccess-AI-Collective/axolotl"&gt;Axolotl&lt;/a&gt;, &lt;a href="https://github.com/unslothai/unsloth"&gt;UnSloth&lt;/a&gt;, &lt;a href="https://github.com/modelscope/swift"&gt;Swift&lt;/a&gt;, &lt;a href="https://github.com/hiyouga/LLaMA-Factory"&gt;Llama-Factory&lt;/a&gt;, etc., to finetune your models with SFT, DPO, GRPO, etc.&lt;/p&gt; 
&lt;h2&gt;License Agreement&lt;/h2&gt; 
&lt;p&gt;All our open-weight models are licensed under Apache 2.0. You can find the license files in the respective Hugging Face repositories.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find our work helpful, feel free to give us a cite.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{qwen3,
    title={Qwen3 Technical Report}, 
    author={An Yang and Anfeng Li and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Gao and Chengen Huang and Chenxu Lv and Chujie Zheng and Dayiheng Liu and Fan Zhou and Fei Huang and Feng Hu and Hao Ge and Haoran Wei and Huan Lin and Jialong Tang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jing Zhou and Jingren Zhou and Junyang Lin and Kai Dang and Keqin Bao and Kexin Yang and Le Yu and Lianghao Deng and Mei Li and Mingfeng Xue and Mingze Li and Pei Zhang and Peng Wang and Qin Zhu and Rui Men and Ruize Gao and Shixuan Liu and Shuang Luo and Tianhao Li and Tianyi Tang and Wenbiao Yin and Xingzhang Ren and Xinyu Wang and Xinyu Zhang and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yinger Zhang and Yu Wan and Yuqiong Liu and Zekun Wang and Zeyu Cui and Zhenru Zhang and Zhipeng Zhou and Zihan Qiu},
    journal = {arXiv preprint arXiv:2505.09388},
    year={2025}
}

@article{qwen2.5,
    title   = {Qwen2.5 Technical Report}, 
    author  = {An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},
    journal = {arXiv preprint arXiv:2412.15115},
    year    = {2024}
}

@article{qwen2,
    title   = {Qwen2 Technical Report}, 
    author  = {An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},
    journal = {arXiv preprint arXiv:2407.10671},
    year    = {2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contact Us&lt;/h2&gt; 
&lt;p&gt;If you are interested to leave a message to either our research team or product team, join our &lt;a href="https://discord.gg/z3GAxXZ9Ce"&gt;Discord&lt;/a&gt; or &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen3/main/assets/wechat.png"&gt;WeChat groups&lt;/a&gt;!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>livekit/agents</title>
      <link>https://github.com/livekit/agents</link>
      <description>&lt;p&gt;A powerful framework for building realtime voice AI agents ü§ñüéôÔ∏èüìπ&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="/.github/banner_dark.png" /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="/.github/banner_light.png" /&gt; 
 &lt;img style="width:100%;" alt="The LiveKit icon, the name of the repository and some sample code in the background." src="https://raw.githubusercontent.com/livekit/agents/main/.github/banner_light.png" /&gt; 
&lt;/picture&gt; 
&lt;!--END_BANNER_IMAGE--&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/pypi/v/livekit-agents" alt="PyPI - Version" /&gt; &lt;a href="https://pepy.tech/projects/livekit-agents"&gt;&lt;img src="https://static.pepy.tech/badge/livekit-agents/month" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://livekit.io/join-slack"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Flivekit.io%2Fbadges%2Fslack" alt="Slack community" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/livekit"&gt;&lt;img src="https://img.shields.io/twitter/follow/livekit" alt="Twitter Follow" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/livekit/agents"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki for understanding the codebase" /&gt;&lt;/a&gt; &lt;a href="https://github.com/livekit/livekit/raw/master/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/livekit/livekit" alt="License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;Looking for the JS/TS library? Check out &lt;a href="https://github.com/livekit/agents-js"&gt;AgentsJS&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What is Agents?&lt;/h2&gt; 
&lt;!--BEGIN_DESCRIPTION--&gt; 
&lt;p&gt;The Agent Framework is designed for building realtime, programmable participants that run on servers. Use it to create conversational, multi-modal voice agents that can see, hear, and understand.&lt;/p&gt; 
&lt;!--END_DESCRIPTION--&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible integrations&lt;/strong&gt;: A comprehensive ecosystem to mix and match the right STT, LLM, TTS, and Realtime API to suit your use case.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Integrated job scheduling&lt;/strong&gt;: Built-in task scheduling and distribution with &lt;a href="https://docs.livekit.io/agents/build/dispatch/"&gt;dispatch APIs&lt;/a&gt; to connect end users to agents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensive WebRTC clients&lt;/strong&gt;: Build client applications using LiveKit's open-source SDK ecosystem, supporting all major platforms.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Telephony integration&lt;/strong&gt;: Works seamlessly with LiveKit's &lt;a href="https://docs.livekit.io/sip/"&gt;telephony stack&lt;/a&gt;, allowing your agent to make calls to or receive calls from phones.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Exchange data with clients&lt;/strong&gt;: Use &lt;a href="https://docs.livekit.io/home/client/data/rpc/"&gt;RPCs&lt;/a&gt; and other &lt;a href="https://docs.livekit.io/home/client/data/"&gt;Data APIs&lt;/a&gt; to seamlessly exchange data with clients.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Semantic turn detection&lt;/strong&gt;: Uses a transformer model to detect when a user is done with their turn, helps to reduce interruptions.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP support&lt;/strong&gt;: Native support for MCP. Integrate tools provided by MCP servers with one loc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Builtin test framework&lt;/strong&gt;: Write tests and use judges to ensure your agent is performing as expected.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open-source&lt;/strong&gt;: Fully open-source, allowing you to run the entire stack on your own servers, including &lt;a href="https://github.com/livekit/livekit"&gt;LiveKit server&lt;/a&gt;, one of the most widely used WebRTC media servers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install the core Agents library, along with plugins for popular model providers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "livekit-agents[openai,silero,deepgram,cartesia,turn-detector]~=1.0"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Docs and guides&lt;/h2&gt; 
&lt;p&gt;Documentation on the framework and how to use it can be found &lt;a href="https://docs.livekit.io/agents/"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Core concepts&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Agent: An LLM-based application with defined instructions.&lt;/li&gt; 
 &lt;li&gt;AgentSession: A container for agents that manages interactions with end users.&lt;/li&gt; 
 &lt;li&gt;entrypoint: The starting point for an interactive session, similar to a request handler in a web server.&lt;/li&gt; 
 &lt;li&gt;Worker: The main process that coordinates job scheduling and launches agents for user sessions.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Simple voice agent&lt;/h3&gt; 
&lt;hr /&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from livekit.agents import (
    Agent,
    AgentSession,
    JobContext,
    RunContext,
    WorkerOptions,
    cli,
    function_tool,
)
from livekit.plugins import deepgram, elevenlabs, openai, silero

@function_tool
async def lookup_weather(
    context: RunContext,
    location: str,
):
    """Used to look up weather information."""

    return {"weather": "sunny", "temperature": 70}


async def entrypoint(ctx: JobContext):
    await ctx.connect()

    agent = Agent(
        instructions="You are a friendly voice assistant built by LiveKit.",
        tools=[lookup_weather],
    )
    session = AgentSession(
        vad=silero.VAD.load(),
        # any combination of STT, LLM, TTS, or realtime API can be used
        stt=deepgram.STT(model="nova-3"),
        llm=openai.LLM(model="gpt-4o-mini"),
        tts=elevenlabs.TTS(),
    )

    await session.start(agent=agent, room=ctx.room)
    await session.generate_reply(instructions="greet the user and ask about their day")


if __name__ == "__main__":
    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You'll need the following environment variables for this example:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;DEEPGRAM_API_KEY&lt;/li&gt; 
 &lt;li&gt;OPENAI_API_KEY&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Multi-agent handoff&lt;/h3&gt; 
&lt;hr /&gt; 
&lt;p&gt;This code snippet is abbreviated. For the full example, see &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/multi_agent.py"&gt;multi_agent.py&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;...
class IntroAgent(Agent):
    def __init__(self) -&amp;gt; None:
        super().__init__(
            instructions=f"You are a story teller. Your goal is to gather a few pieces of information from the user to make the story personalized and engaging."
            "Ask the user for their name and where they are from"
        )

    async def on_enter(self):
        self.session.generate_reply(instructions="greet the user and gather information")

    @function_tool
    async def information_gathered(
        self,
        context: RunContext,
        name: str,
        location: str,
    ):
        """Called when the user has provided the information needed to make the story personalized and engaging.

        Args:
            name: The name of the user
            location: The location of the user
        """

        context.userdata.name = name
        context.userdata.location = location

        story_agent = StoryAgent(name, location)
        return story_agent, "Let's start the story!"


class StoryAgent(Agent):
    def __init__(self, name: str, location: str) -&amp;gt; None:
        super().__init__(
            instructions=f"You are a storyteller. Use the user's information in order to make the story personalized."
            f"The user's name is {name}, from {location}"
            # override the default model, switching to Realtime API from standard LLMs
            llm=openai.realtime.RealtimeModel(voice="echo"),
            chat_ctx=chat_ctx,
        )

    async def on_enter(self):
        self.session.generate_reply()


async def entrypoint(ctx: JobContext):
    await ctx.connect()

    userdata = StoryData()
    session = AgentSession[StoryData](
        vad=silero.VAD.load(),
        stt=deepgram.STT(model="nova-3"),
        llm=openai.LLM(model="gpt-4o-mini"),
        tts=openai.TTS(voice="echo"),
        userdata=userdata,
    )

    await session.start(
        agent=IntroAgent(),
        room=ctx.room,
    )
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Testing&lt;/h3&gt; 
&lt;p&gt;Automated tests are essential for building reliable agents, especially with the non-deterministic behavior of LLMs. LiveKit Agents include native test integration to help you create dependable agents.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@pytest.mark.asyncio
async def test_no_availability() -&amp;gt; None:
    llm = google.LLM()
    async AgentSession(llm=llm) as sess:
        await sess.start(MyAgent())
        result = await sess.run(
            user_input="Hello, I need to place an order."
        )
        result.expect.skip_next_event_if(type="message", role="assistant")
        result.expect.next_event().is_function_call(name="start_order")
        result.expect.next_event().is_function_call_output()
        await (
            result.expect.next_event()
            .is_message(role="assistant")
            .judge(llm, intent="assistant should be asking the user what they would like")
        )

&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üéôÔ∏è Starter Agent&lt;/h3&gt; &lt;p&gt;A starter agent optimized for voice conversations.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/basic_agent.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üîÑ Multi-user push to talk&lt;/h3&gt; &lt;p&gt;Responds to multiple users in the room via push-to-talk.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/push_to_talk.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üéµ Background audio&lt;/h3&gt; &lt;p&gt;Background ambient and thinking audio to improve realism.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/background_audio.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üõ†Ô∏è Dynamic tool creation&lt;/h3&gt; &lt;p&gt;Creating function tools dynamically.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/dynamic_tool_creation.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;‚òéÔ∏è Outbound caller&lt;/h3&gt; &lt;p&gt;Agent that makes outbound phone calls&lt;/p&gt; &lt;p&gt; &lt;a href="https://github.com/livekit-examples/outbound-caller-python"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üìã Structured output&lt;/h3&gt; &lt;p&gt;Using structured output from LLM to guide TTS tone.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/structured_output.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üîå MCP support&lt;/h3&gt; &lt;p&gt;Use tools from MCP servers&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/mcp"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üí¨ Text-only agent&lt;/h3&gt; &lt;p&gt;Skip voice altogether and use the same code for text-only integrations&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/other/text_only.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üìù Multi-user transcriber&lt;/h3&gt; &lt;p&gt;Produce transcriptions from all users in the room&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/other/transcription/multi-user-transcriber.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üé• Video avatars&lt;/h3&gt; &lt;p&gt;Add an AI avatar with Tavus, Beyond Presence, and Bithuman&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/avatar_agents/"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üçΩÔ∏è Restaurant ordering and reservations&lt;/h3&gt; &lt;p&gt;Full example of an agent that handles calls for a restaurant.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/restaurant_agent.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üëÅÔ∏è Gemini Live vision&lt;/h3&gt; &lt;p&gt;Full example (including iOS app) of Gemini Live agent that can see.&lt;/p&gt; &lt;p&gt; &lt;a href="https://github.com/livekit-examples/vision-demo"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Running your agent&lt;/h2&gt; 
&lt;h3&gt;Testing in terminal&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python myagent.py console
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Runs your agent in terminal mode, enabling local audio input and output for testing. This mode doesn't require external servers or dependencies and is useful for quickly validating behavior.&lt;/p&gt; 
&lt;h3&gt;Developing with LiveKit clients&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python myagent.py dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Starts the agent server and enables hot reloading when files change. This mode allows each process to host multiple concurrent agents efficiently.&lt;/p&gt; 
&lt;p&gt;The agent connects to LiveKit Cloud or your self-hosted server. Set the following environment variables:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;LIVEKIT_URL&lt;/li&gt; 
 &lt;li&gt;LIVEKIT_API_KEY&lt;/li&gt; 
 &lt;li&gt;LIVEKIT_API_SECRET&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can connect using any LiveKit client SDK or telephony integration. To get started quickly, try the &lt;a href="https://agents-playground.livekit.io/"&gt;Agents Playground&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Running for production&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python myagent.py start
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Runs the agent with production-ready optimizations.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;The Agents framework is under active development in a rapidly evolving field. We welcome and appreciate contributions of any kind, be it feedback, bugfixes, features, new plugins and tools, or better documentation. You can file issues under this repo, open a PR, or chat with us in LiveKit's &lt;a href="https://livekit.io/join-slack"&gt;Slack community&lt;/a&gt;.&lt;/p&gt; 
&lt;!--BEGIN_REPO_NAV--&gt; 
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;table&gt; 
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th colspan="2"&gt;LiveKit Ecosystem&lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt;
   &lt;td&gt;LiveKit SDKs&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://github.com/livekit/client-sdk-js"&gt;Browser&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-swift"&gt;iOS/macOS/visionOS&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-android"&gt;Android&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-flutter"&gt;Flutter&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-react-native"&gt;React Native&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/rust-sdks"&gt;Rust&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/node-sdks"&gt;Node.js&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/python-sdks"&gt;Python&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-unity"&gt;Unity&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-unity-web"&gt;Unity (WebGL)&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-esp32"&gt;ESP32&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Server APIs&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://github.com/livekit/node-sdks"&gt;Node.js&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/server-sdk-go"&gt;Golang&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/server-sdk-ruby"&gt;Ruby&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/server-sdk-kotlin"&gt;Java/Kotlin&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/python-sdks"&gt;Python&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/rust-sdks"&gt;Rust&lt;/a&gt; ¬∑ &lt;a href="https://github.com/agence104/livekit-server-sdk-php"&gt;PHP (community)&lt;/a&gt; ¬∑ &lt;a href="https://github.com/pabloFuente/livekit-server-sdk-dotnet"&gt;.NET (community)&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;UI Components&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://github.com/livekit/components-js"&gt;React&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/components-android"&gt;Android Compose&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/components-swift"&gt;SwiftUI&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/components-flutter"&gt;Flutter&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Agents Frameworks&lt;/td&gt;
   &lt;td&gt;&lt;b&gt;Python&lt;/b&gt; ¬∑ &lt;a href="https://github.com/livekit/agents-js"&gt;Node.js&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/agent-playground"&gt;Playground&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Services&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://github.com/livekit/livekit"&gt;LiveKit server&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/egress"&gt;Egress&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/ingress"&gt;Ingress&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/sip"&gt;SIP&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Resources&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://docs.livekit.io"&gt;Docs&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit-examples"&gt;Example apps&lt;/a&gt; ¬∑ &lt;a href="https://livekit.io/cloud"&gt;Cloud&lt;/a&gt; ¬∑ &lt;a href="https://docs.livekit.io/home/self-hosting/deployment"&gt;Self-hosting&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/livekit-cli"&gt;CLI&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!--END_REPO_NAV--&gt;</description>
    </item>
    
    <item>
      <title>JefferyHcool/BiliNote</title>
      <link>https://github.com/JefferyHcool/BiliNote</link>
      <description>&lt;p&gt;AI ËßÜÈ¢ëÁ¨îËÆ∞ÁîüÊàêÂ∑•ÂÖ∑ ËÆ© AI ‰∏∫‰Ω†ÁöÑËßÜÈ¢ëÂÅöÁ¨îËÆ∞&lt;/p&gt;&lt;hr&gt;&lt;div style="display: flex; justify-content: center; align-items: center; gap: 10px;
"&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/JefferyHcool/BiliNote/master/doc/icon.svg?sanitize=true" alt="BiliNote Banner" width="50" height="50" /&gt; &lt;/p&gt; 
 &lt;h1 align="center"&gt; BiliNote v1.8.1&lt;/h1&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt;&lt;i&gt;AI ËßÜÈ¢ëÁ¨îËÆ∞ÁîüÊàêÂ∑•ÂÖ∑ ËÆ© AI ‰∏∫‰Ω†ÁöÑËßÜÈ¢ëÂÅöÁ¨îËÆ∞&lt;/i&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" /&gt; &lt;img src="https://img.shields.io/badge/frontend-react-blue" /&gt; &lt;img src="https://img.shields.io/badge/backend-fastapi-green" /&gt; &lt;img src="https://img.shields.io/badge/GPT-openai%20%7C%20deepseek%20%7C%20qwen-ff69b4" /&gt; &lt;img src="https://img.shields.io/badge/docker-compose-blue" /&gt; &lt;img src="https://img.shields.io/badge/status-active-success" /&gt; &lt;img src="https://img.shields.io/github/stars/jefferyhcool/BiliNote?style=social" /&gt; &lt;/p&gt; 
&lt;h2&gt;‚ú® È°πÁõÆÁÆÄ‰ªã&lt;/h2&gt; 
&lt;p&gt;BiliNote ÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÁöÑ AI ËßÜÈ¢ëÁ¨îËÆ∞Âä©ÊâãÔºåÊîØÊåÅÈÄöËøáÂìîÂì©ÂìîÂì©„ÄÅYouTube„ÄÅÊäñÈü≥Á≠âËßÜÈ¢ëÈìæÊé•ÔºåËá™Âä®ÊèêÂèñÂÜÖÂÆπÂπ∂ÁîüÊàêÁªìÊûÑÊ∏ÖÊô∞„ÄÅÈáçÁÇπÊòéÁ°ÆÁöÑ Markdown Ê†ºÂºèÁ¨îËÆ∞„ÄÇÊîØÊåÅÊèíÂÖ•Êà™Âõæ„ÄÅÂéüÁâáË∑≥ËΩ¨Á≠âÂäüËÉΩ„ÄÇ&lt;/p&gt; 
&lt;h2&gt;üìù ‰ΩøÁî®ÊñáÊ°£&lt;/h2&gt; 
&lt;p&gt;ËØ¶ÁªÜÊñáÊ°£ÂèØ‰ª•Êü•Áúã&lt;a href="https://docs.bilinote.app/"&gt;ËøôÈáå&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;‰ΩìÈ™åÂú∞ÂùÄ&lt;/h2&gt; 
&lt;p&gt;ÂèØ‰ª•ÈÄöËøáËÆøÈóÆ &lt;a href="https://www.bilinote.app/"&gt;ËøôÈáå&lt;/a&gt; ËøõË°å‰ΩìÈ™åÔºåÈÄüÂ∫¶Áï•ÊÖ¢Ôºå‰∏çÊîØÊåÅÈïøËßÜÈ¢ë„ÄÇ&lt;/p&gt; 
&lt;h2&gt;üì¶ Windows ÊâìÂåÖÁâà&lt;/h2&gt; 
&lt;p&gt;Êú¨È°πÁõÆÊèê‰æõ‰∫Ü Windows Á≥ªÁªüÁöÑ exe Êñá‰ª∂ÔºåÂèØÂú®&lt;a href="https://github.com/JefferyHcool/BiliNote/releases/tag/v1.1.1"&gt;release&lt;/a&gt;ËøõË°å‰∏ãËΩΩ„ÄÇ&lt;strong&gt;Ê≥®ÊÑè‰∏ÄÂÆöË¶ÅÂú®Ê≤°Êúâ‰∏≠ÊñáË∑ØÂæÑÁöÑÁéØÂ¢É‰∏ãËøêË°å„ÄÇ&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;üîß ÂäüËÉΩÁâπÊÄß&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ÊîØÊåÅÂ§öÂπ≥Âè∞ÔºöBilibili„ÄÅYouTube„ÄÅÊú¨Âú∞ËßÜÈ¢ë„ÄÅÊäñÈü≥ÔºàÂêéÁª≠‰ºöÂä†ÂÖ•Êõ¥Â§öÂπ≥Âè∞Ôºâ&lt;/li&gt; 
 &lt;li&gt;ÊîØÊåÅËøîÂõûÁ¨îËÆ∞Ê†ºÂºèÈÄâÊã©&lt;/li&gt; 
 &lt;li&gt;ÊîØÊåÅÁ¨îËÆ∞È£éÊ†ºÈÄâÊã©&lt;/li&gt; 
 &lt;li&gt;ÊîØÊåÅÂ§öÊ®°ÊÄÅËßÜÈ¢ëÁêÜËß£&lt;/li&gt; 
 &lt;li&gt;ÊîØÊåÅÂ§öÁâàÊú¨ËÆ∞ÂΩï‰øùÁïô&lt;/li&gt; 
 &lt;li&gt;ÊîØÊåÅËá™Ë°åÈÖçÁΩÆ GPT Â§ßÊ®°Âûã&lt;/li&gt; 
 &lt;li&gt;Êú¨Âú∞Ê®°ÂûãÈü≥È¢ëËΩ¨ÂÜôÔºàÊîØÊåÅ Fast-WhisperÔºâ&lt;/li&gt; 
 &lt;li&gt;GPT Â§ßÊ®°ÂûãÊÄªÁªìËßÜÈ¢ëÂÜÖÂÆπ&lt;/li&gt; 
 &lt;li&gt;Ëá™Âä®ÁîüÊàêÁªìÊûÑÂåñ Markdown Á¨îËÆ∞&lt;/li&gt; 
 &lt;li&gt;ÂèØÈÄâÊèíÂÖ•Êà™ÂõæÔºàËá™Âä®Êà™ÂèñÔºâ&lt;/li&gt; 
 &lt;li&gt;ÂèØÈÄâÂÜÖÂÆπË∑≥ËΩ¨ÈìæÊé•ÔºàÂÖ≥ËÅîÂéüËßÜÈ¢ëÔºâ&lt;/li&gt; 
 &lt;li&gt;‰ªªÂä°ËÆ∞ÂΩï‰∏éÂéÜÂè≤ÂõûÁúã&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üì∏ Êà™ÂõæÈ¢ÑËßà&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/JefferyHcool/BiliNote/master/doc/image1.png" alt="screenshot" /&gt; &lt;img src="https://raw.githubusercontent.com/JefferyHcool/BiliNote/master/doc/image3.png" alt="screenshot" /&gt; &lt;img src="https://raw.githubusercontent.com/JefferyHcool/BiliNote/master/doc/image.png" alt="screenshot" /&gt; &lt;img src="https://raw.githubusercontent.com/JefferyHcool/BiliNote/master/doc/image4.png" alt="screenshot" /&gt; &lt;img src="https://raw.githubusercontent.com/JefferyHcool/BiliNote/master/doc/image5.png" alt="screenshot" /&gt;&lt;/p&gt; 
&lt;h2&gt;üöÄ Âø´ÈÄüÂºÄÂßã&lt;/h2&gt; 
&lt;h3&gt;1. ÂÖãÈöÜ‰ªìÂ∫ì&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/JefferyHcool/BiliNote.git
cd BiliNote
mv .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. ÂêØÂä®ÂêéÁ´ØÔºàFastAPIÔºâ&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd backend
pip install -r requirements.txt
python main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. ÂêØÂä®ÂâçÁ´ØÔºàVite + ReactÔºâ&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd BillNote_frontend
pnpm install
pnpm dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ËÆøÈóÆÔºö&lt;code&gt;http://localhost:5173&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;‚öôÔ∏è ‰æùËµñËØ¥Êòé&lt;/h2&gt; 
&lt;h3&gt;üé¨ FFmpeg&lt;/h3&gt; 
&lt;p&gt;Êú¨È°πÁõÆ‰æùËµñ ffmpeg Áî®‰∫éÈü≥È¢ëÂ§ÑÁêÜ‰∏éËΩ¨Á†ÅÔºåÂøÖÈ°ªÂÆâË£ÖÔºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Mac (brew)
brew install ffmpeg

# Ubuntu / Debian
sudo apt install ffmpeg

# Windows
# ËØ∑‰ªéÂÆòÁΩë‰∏ãËΩΩÂÆâË£ÖÔºöhttps://ffmpeg.org/download.html
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ö†Ô∏è Ëã•Á≥ªÁªüÊó†Ê≥ïËØÜÂà´ ffmpegÔºåËØ∑Â∞ÜÂÖ∂Âä†ÂÖ•Á≥ªÁªüÁéØÂ¢ÉÂèòÈáè PATH&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;üöÄ CUDA Âä†ÈÄüÔºàÂèØÈÄâÔºâ&lt;/h3&gt; 
&lt;p&gt;Ëã•‰Ω†Â∏åÊúõÊõ¥Âø´Âú∞ÊâßË°åÈü≥È¢ëËΩ¨ÂÜô‰ªªÂä°ÔºåÂèØ‰ΩøÁî®ÂÖ∑Â§á NVIDIA GPU ÁöÑÊú∫Âô®ÔºåÂπ∂ÂêØÁî® fast-whisper + CUDA Âä†ÈÄüÁâàÊú¨Ôºö&lt;/p&gt; 
&lt;p&gt;ÂÖ∑‰Ωì &lt;code&gt;fast-whisper&lt;/code&gt; ÈÖçÁΩÆÊñπÊ≥ïÔºåËØ∑ÂèÇËÄÉÔºö&lt;a href="http://github.com/SYSTRAN/faster-whisper#requirements"&gt;fast-whisper È°πÁõÆÂú∞ÂùÄ&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;üê≥ ‰ΩøÁî® Docker ‰∏ÄÈîÆÈÉ®ÁΩ≤&lt;/h3&gt; 
&lt;p&gt;Á°Æ‰øù‰Ω†Â∑≤ÂÆâË£Ö Docker Âíå Docker ComposeÔºö&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/JefferyHcool/bilinote-deploy/raw/master/README.md"&gt;docker ÈÉ®ÁΩ≤&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üß† TODO&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; ÊîØÊåÅÊäñÈü≥ÂèäÂø´ÊâãÁ≠âËßÜÈ¢ëÂπ≥Âè∞&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; ÊîØÊåÅÂâçÁ´ØËÆæÁΩÆÂàáÊç¢ AI Ê®°ÂûãÂàáÊç¢„ÄÅËØ≠Èü≥ËΩ¨ÊñáÂ≠óÊ®°Âûã&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; AI ÊëòË¶ÅÈ£éÊ†ºËá™ÂÆö‰πâÔºàÂ≠¶ÊúØÈ£é„ÄÅÂè£ËØ≠È£é„ÄÅÈáçÁÇπÊèêÂèñÁ≠âÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Á¨îËÆ∞ÂØºÂá∫‰∏∫ PDF / Word / Notion&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Âä†ÂÖ•Êõ¥Â§öÊ®°ÂûãÊîØÊåÅ&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Âä†ÂÖ•Êõ¥Â§öÈü≥È¢ëËΩ¨ÊñáÊú¨Ê®°ÂûãÊîØÊåÅ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contact and Join-ËÅîÁ≥ªÂíåÂä†ÂÖ•Á§æÂå∫&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;BiliNote ‰∫§ÊµÅQQÁæ§Ôºö785367111&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;BiliNote ‰∫§ÊµÅÂæÆ‰ø°Áæ§:&lt;/p&gt; &lt;img src="https://raw.githubusercontent.com/JefferyHcool/BiliNote/master/doc/wechat.png" alt="wechat" style="zoom:33%;" /&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üîé‰ª£Á†ÅÂèÇËÄÉ&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Êú¨È°πÁõÆ‰∏≠ÁöÑ &lt;code&gt;ÊäñÈü≥‰∏ãËΩΩÂäüËÉΩ&lt;/code&gt; ÈÉ®ÂàÜ‰ª£Á†ÅÂèÇËÄÉÂºïÁî®Ëá™Ôºö&lt;a href="https://github.com/Evil0ctal/Douyin_TikTok_Download_API"&gt;Evil0ctal/Douyin_TikTok_Download_API&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìú License&lt;/h2&gt; 
&lt;p&gt;MIT License&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;üí¨ ‰Ω†ÁöÑÊîØÊåÅ‰∏éÂèçÈ¶àÊòØÊàëÊåÅÁª≠‰ºòÂåñÁöÑÂä®ÂäõÔºÅÊ¨¢Ëøé PR„ÄÅÊèê issue„ÄÅStar ‚≠êÔ∏è&lt;/p&gt; 
&lt;h2&gt;Buy Me a Coffee / ÊçêËµ†&lt;/h2&gt; 
&lt;p&gt;Â¶ÇÊûú‰Ω†ËßâÂæóÈ°πÁõÆÂØπ‰Ω†ÊúâÂ∏ÆÂä©ÔºåËÄÉËôëÊîØÊåÅÊàë‰∏Ä‰∏ãÂêß&lt;/p&gt; 
&lt;div style="display:inline;"&gt; 
 &lt;img width="30%" src="https://common-1304618721.cos.ap-chengdu.myqcloud.com/8986c9eb29c356a0cfa3d470c23d3b6.jpg" /&gt; 
 &lt;img width="30%" src="https://common-1304618721.cos.ap-chengdu.myqcloud.com/2a049ea298b206bcd0d8b8da3219d6b.jpg" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;‚≠ê Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#JefferyHcool/BiliNote&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=JefferyHcool/BiliNote&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>resemble-ai/chatterbox</title>
      <link>https://github.com/resemble-ai/chatterbox</link>
      <description>&lt;p&gt;SoTA open-source TTS&lt;/p&gt;&lt;hr&gt;&lt;img width="1200" alt="cb-big2" src="https://github.com/user-attachments/assets/bd8c5f03-e91d-4ee5-b680-57355da204d1" /&gt; 
&lt;h1&gt;Chatterbox TTS&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://resemble-ai.github.io/chatterbox_demopage/"&gt;&lt;img src="https://img.shields.io/badge/listen-demo_samples-blue" alt="Alt Text" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/spaces/ResembleAI/Chatterbox"&gt;&lt;img src="https://huggingface.co/datasets/huggingface/badges/resolve/main/open-in-hf-spaces-sm.svg?sanitize=true" alt="Alt Text" /&gt;&lt;/a&gt; &lt;a href="https://podonos.com/resembleai/chatterbox"&gt;&lt;img src="https://static-public.podonos.com/badges/insight-on-pdns-sm-dark.svg?sanitize=true" alt="Alt Text" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/rJq9cRJBJ6"&gt;&lt;img src="https://img.shields.io/discord/1377773249798344776?label=join%20discord&amp;amp;logo=discord&amp;amp;style=flat" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;_Made with ‚ô•Ô∏è by &lt;a href="https://resemble.ai" target="_blank"&gt;&lt;img width="100" alt="resemble-logo-horizontal" src="https://github.com/user-attachments/assets/35cf756b-3506-4943-9c72-c05ddfa4e525" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We're excited to introduce Chatterbox, &lt;a href="https://resemble.ai"&gt;Resemble AI's&lt;/a&gt; first production-grade open source TTS model. Licensed under MIT, Chatterbox has been benchmarked against leading closed-source systems like ElevenLabs, and is consistently preferred in side-by-side evaluations.&lt;/p&gt; 
&lt;p&gt;Whether you're working on memes, videos, games, or AI agents, Chatterbox brings your content to life. It's also the first open source TTS model to support &lt;strong&gt;emotion exaggeration control&lt;/strong&gt;, a powerful feature that makes your voices stand out. Try it now on our &lt;a href="https://huggingface.co/spaces/ResembleAI/Chatterbox"&gt;Hugging Face Gradio app.&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;If you like the model but need to scale or tune it for higher accuracy, check out our competitively priced TTS service (&lt;a href="https://resemble.ai"&gt;link&lt;/a&gt;). It delivers reliable performance with ultra-low latency of sub 200ms‚Äîideal for production use in agents, applications, or interactive media.&lt;/p&gt; 
&lt;h1&gt;Key Details&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;SoTA zeroshot TTS&lt;/li&gt; 
 &lt;li&gt;0.5B Llama backbone&lt;/li&gt; 
 &lt;li&gt;Unique exaggeration/intensity control&lt;/li&gt; 
 &lt;li&gt;Ultra-stable with alignment-informed inference&lt;/li&gt; 
 &lt;li&gt;Trained on 0.5M hours of cleaned data&lt;/li&gt; 
 &lt;li&gt;Watermarked outputs&lt;/li&gt; 
 &lt;li&gt;Easy voice conversion script&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://podonos.com/resembleai/chatterbox"&gt;Outperforms ElevenLabs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Tips&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;General Use (TTS and Voice Agents):&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;The default settings (&lt;code&gt;exaggeration=0.5&lt;/code&gt;, &lt;code&gt;cfg_weight=0.5&lt;/code&gt;) work well for most prompts.&lt;/li&gt; 
   &lt;li&gt;If the reference speaker has a fast speaking style, lowering &lt;code&gt;cfg_weight&lt;/code&gt; to around &lt;code&gt;0.3&lt;/code&gt; can improve pacing.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Expressive or Dramatic Speech:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Try lower &lt;code&gt;cfg_weight&lt;/code&gt; values (e.g. &lt;code&gt;~0.3&lt;/code&gt;) and increase &lt;code&gt;exaggeration&lt;/code&gt; to around &lt;code&gt;0.7&lt;/code&gt; or higher.&lt;/li&gt; 
   &lt;li&gt;Higher &lt;code&gt;exaggeration&lt;/code&gt; tends to speed up speech; reducing &lt;code&gt;cfg_weight&lt;/code&gt; helps compensate with slower, more deliberate pacing.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install chatterbox-tts
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can install from source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# conda create -yn chatterbox python=3.11
# conda activate chatterbox

git clone https://github.com/resemble-ai/chatterbox.git
cd chatterbox
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We developed and tested Chatterbox on Python 3.11 on Debain 11 OS; the versions of the dependencies are pinned in &lt;code&gt;pyproject.toml&lt;/code&gt; to ensure consistency. You can modify the code or dependencies in this installation mode.&lt;/p&gt; 
&lt;h1&gt;Usage&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import torchaudio as ta
from chatterbox.tts import ChatterboxTTS

model = ChatterboxTTS.from_pretrained(device="cuda")

text = "Ezreal and Jinx teamed up with Ahri, Yasuo, and Teemo to take down the enemy's Nexus in an epic late-game pentakill."
wav = model.generate(text)
ta.save("test-1.wav", wav, model.sr)

# If you want to synthesize with a different voice, specify the audio prompt
AUDIO_PROMPT_PATH = "YOUR_FILE.wav"
wav = model.generate(text, audio_prompt_path=AUDIO_PROMPT_PATH)
ta.save("test-2.wav", wav, model.sr)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;code&gt;example_tts.py&lt;/code&gt; and &lt;code&gt;example_vc.py&lt;/code&gt; for more examples.&lt;/p&gt; 
&lt;h1&gt;Supported Lanugage&lt;/h1&gt; 
&lt;p&gt;Currenlty only English.&lt;/p&gt; 
&lt;h1&gt;Acknowledgements&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/FunAudioLLM/CosyVoice"&gt;Cosyvoice&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning"&gt;Real-Time-Voice-Cloning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yl4579/HiFTNet"&gt;HiFT-GAN&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/meta-llama/llama3"&gt;Llama 3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/xingchensong/S3Tokenizer"&gt;S3Tokenizer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Built-in PerTh Watermarking for Responsible AI&lt;/h1&gt; 
&lt;p&gt;Every audio file generated by Chatterbox includes &lt;a href="https://github.com/resemble-ai/perth"&gt;Resemble AI's Perth (Perceptual Threshold) Watermarker&lt;/a&gt; - imperceptible neural watermarks that survive MP3 compression, audio editing, and common manipulations while maintaining nearly 100% detection accuracy.&lt;/p&gt; 
&lt;h2&gt;Watermark extraction&lt;/h2&gt; 
&lt;p&gt;You can look for the watermark using the following script.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import perth
import librosa

AUDIO_PATH = "YOUR_FILE.wav"

# Load the watermarked audio
watermarked_audio, sr = librosa.load(AUDIO_PATH, sr=None)

# Initialize watermarker (same as used for embedding)
watermarker = perth.PerthImplicitWatermarker()

# Extract watermark
watermark = watermarker.get_watermark(watermarked_audio, sample_rate=sr)
print(f"Extracted watermark: {watermark}")
# Output: 0.0 (no watermark) or 1.0 (watermarked)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Official Discord&lt;/h1&gt; 
&lt;p&gt;üëã Join us on &lt;a href="https://discord.gg/rJq9cRJBJ6"&gt;Discord&lt;/a&gt; and let's build something awesome together!&lt;/p&gt; 
&lt;h1&gt;Citation&lt;/h1&gt; 
&lt;p&gt;If you find this model useful, please consider citing.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{chatterboxtts2025,
  author       = {{Resemble AI}},
  title        = {{Chatterbox-TTS}},
  year         = {2025},
  howpublished = {\url{https://github.com/resemble-ai/chatterbox}},
  note         = {GitHub repository}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Disclaimer&lt;/h1&gt; 
&lt;p&gt;Don't use this model to do bad things. Prompts are sourced from freely available data on the internet.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>denizsafak/abogen</title>
      <link>https://github.com/denizsafak/abogen</link>
      <description>&lt;p&gt;Generate audiobooks from EPUBs, PDFs and text with synchronized captions.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;abogen &lt;img width="40px" title="abogen icon" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/abogen/assets/icon.ico" align="right" style="padding-left: 10px; padding-top:5px;" /&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/denizsafak/abogen/actions"&gt;&lt;img src="https://github.com/denizsafak/abogen/actions/workflows/test_pip.yml/badge.svg?sanitize=true" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/denizsafak/abogen/releases/latest"&gt;&lt;img src="https://img.shields.io/github/v/release/denizsafak/abogen" alt="GitHub Release" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/abogen/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/abogen" alt="Abogen PyPi Python Versions" /&gt;&lt;/a&gt; &lt;a href="https://github.com/denizsafak/abogen/releases/latest"&gt;&lt;img src="https://img.shields.io/badge/os-windows%20%7C%20linux%20%7C%20macos%20-blue" alt="Operating Systems" /&gt;&lt;/a&gt; &lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true" alt="Code style: black" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-maroon.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Abogen is a powerful text-to-speech conversion tool that makes it easy to turn ePub, PDF, or text files into high-quality audio with matching subtitles in seconds. Use it for audiobooks, voiceovers for Instagram, YouTube, TikTok, or any project that needs natural-sounding text-to-speech, using &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;Kokoro-82M&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img title="Abogen Main" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen.png" width="380" /&gt; &lt;img title="Abogen Processing" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen2.png" width="380" /&gt;&lt;/p&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/094ba3df-7d66-494a-bc31-0e4b41d0b865"&gt;https://github.com/user-attachments/assets/094ba3df-7d66-494a-bc31-0e4b41d0b865&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;This demo was generated in just 5&amp;nbsp;seconds, producing ‚àº1&amp;nbsp;minute of audio with perfectly synced subtitles. To create a similar video, see &lt;a href="https://github.com/denizsafak/abogen/tree/main/demo"&gt;the demo guide&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;code&gt;How to install?&lt;/code&gt; &lt;a href="https://pypi.org/project/abogen/" target="_blank"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/abogen" alt="Abogen Compatible PyPi Python Versions" align="right" style="margin-top:6px;" /&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;p&gt;Go to &lt;a href="https://github.com/espeak-ng/espeak-ng/releases/latest"&gt;espeak-ng latest release&lt;/a&gt; download and run the *.msi file.&lt;/p&gt; 
&lt;h4&gt;OPTION 1: Install using script&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://github.com/denizsafak/abogen/archive/refs/heads/main.zip"&gt;Download&lt;/a&gt; the repository&lt;/li&gt; 
 &lt;li&gt;Extract the ZIP file&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;WINDOWS_INSTALL.bat&lt;/code&gt; by double-clicking it&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This method handles everything automatically - installing all dependencies including CUDA in a self-contained environment without requiring a separate Python installation. (You still need to install &lt;a href="https://github.com/espeak-ng/espeak-ng/releases/latest"&gt;espeak-ng&lt;/a&gt;.)&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] You don't need to install Python separately. The script will install Python automatically.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;OPTION 2: Install using pip&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create a virtual environment (optional)
mkdir abogen &amp;amp;&amp;amp; cd abogen
python -m venv venv
venv\Scripts\activate

# For NVIDIA GPUs:
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128

# For AMD GPUs:
# Not supported yet, because ROCm is not available on Windows. Use Linux if you have AMD GPU.

# Install abogen
pip install abogen
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Mac&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install espeak-ng
brew install espeak-ng

# Create a virtual environment (recommended)
mkdir abogen &amp;amp;&amp;amp; cd abogen
python3 -m venv venv
source venv/bin/activate

# Install abogen
pip3 install abogen

# For Silicon Mac (M1, M2 etc.)
# After installing abogen, we need to install Kokoro's development version which includes MPS support.
pip3 install git+https://github.com/hexgrad/kokoro.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Linux&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install espeak-ng
sudo apt install espeak-ng # Ubuntu/Debian
sudo pacman -S espeak-ng # Arch Linux
sudo dnf install espeak-ng # Fedora

# Create a virtual environment (recommended)
mkdir abogen &amp;amp;&amp;amp; cd abogen
python3 -m venv venv
source venv/bin/activate

# Install abogen
pip3 install abogen

# For NVIDIA GPUs:
# Already supported, no need to install CUDA separately.

# For AMD GPUs:
# After installing abogen, we need to uninstall the existing torch package
pip3 uninstall torch 
pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.4
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] If you get &lt;code&gt;WARNING: The script abogen-cli is installed in '/home/username/.local/bin' which is not on PATH.&lt;/code&gt; error, run the following command to add it to your PATH:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;echo "export PATH=\"/home/$USER/.local/bin:\$PATH\"" &amp;gt;&amp;gt; ~/.bashrc &amp;amp;&amp;amp; source ~/.bashrc
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] If you get "No matching distribution found" error, try installing it on supported Python (3.10 to 3.12). You can use &lt;a href="https://github.com/pyenv/pyenv"&gt;pyenv&lt;/a&gt; to manage multiple Python versions easily in Linux. Watch this &lt;a href="https://www.youtube.com/watch?v=MVyb-nI4KyI"&gt;video&lt;/a&gt; by NetworkChuck for a quick guide.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Special thanks to &lt;a href="https://github.com/hg000125"&gt;@hg000125&lt;/a&gt; for his contribution in &lt;a href="https://github.com/denizsafak/abogen/issues/23"&gt;#23&lt;/a&gt;. AMD GPU support is possible thanks to his work.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;code&gt;How to run?&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;If you installed using pip, you can simply run the following command to start Abogen:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;abogen
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] If you installed using the Windows installer &lt;code&gt;(WINDOWS_INSTALL.bat)&lt;/code&gt;, It should have created a shortcut in the same folder, or your desktop. You can run it from there. If you lost the shortcut, Abogen is located in &lt;code&gt;python_embedded/Scripts/abogen.exe&lt;/code&gt;. You can run it from there directly.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;code&gt;How to use?&lt;/code&gt;&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Drag and drop any ePub, PDF, or text file (or use the built-in text editor)&lt;/li&gt; 
 &lt;li&gt;Configure the settings: 
  &lt;ul&gt; 
   &lt;li&gt;Set speech speed&lt;/li&gt; 
   &lt;li&gt;Select a voice (or create a custom voice using voice mixer)&lt;/li&gt; 
   &lt;li&gt;Select subtitle generation style (by sentence, word, etc.)&lt;/li&gt; 
   &lt;li&gt;Select output format&lt;/li&gt; 
   &lt;li&gt;Select where to save the output&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Hit Start&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;&lt;code&gt;In action&lt;/code&gt;&lt;/h2&gt; 
&lt;img title="Abogen in action" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen.gif" /&gt; 
&lt;p&gt;Here‚Äôs Abogen in action: in this demo, it processes ‚àº3,000 characters of text in just 11 seconds and turns it into 3 minutes and 28 seconds of audio, and I have a low-end &lt;strong&gt;RTX&amp;nbsp;2060&amp;nbsp;Mobile laptop GPU&lt;/strong&gt;. Your results may vary depending on your hardware.&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;Configuration&lt;/code&gt;&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Input Box&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Drag and drop &lt;code&gt;ePub&lt;/code&gt;, &lt;code&gt;PDF&lt;/code&gt;, or &lt;code&gt;.TXT&lt;/code&gt; files (or use built-in text editor)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Queue options&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Add multiple files to a queue and process them in batch, with individual settings for each file. See &lt;a href="https://raw.githubusercontent.com/denizsafak/abogen/main/#queue-mode"&gt;Queue mode&lt;/a&gt; for more details.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Speed&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Adjust speech rate from &lt;code&gt;0.1x&lt;/code&gt; to &lt;code&gt;2.0x&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Select Voice&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;First letter of the language code (e.g., &lt;code&gt;a&lt;/code&gt; for American English, &lt;code&gt;b&lt;/code&gt; for British English, etc.), second letter is for &lt;code&gt;m&lt;/code&gt; for male and &lt;code&gt;f&lt;/code&gt; for female.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Voice mixer&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Create custom voices by mixing different voice models with a profile system. See &lt;a href="https://raw.githubusercontent.com/denizsafak/abogen/main/#voice-mixer"&gt;Voice Mixer&lt;/a&gt; for more details.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Voice preview&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Listen to the selected voice before processing.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Generate subtitles&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;Disabled&lt;/code&gt;, &lt;code&gt;Sentence&lt;/code&gt;, &lt;code&gt;Sentence + Comma&lt;/code&gt;, &lt;code&gt;Sentence + Highlighting&lt;/code&gt;, &lt;code&gt;1 word&lt;/code&gt;, &lt;code&gt;2 words&lt;/code&gt;, &lt;code&gt;3 words&lt;/code&gt;, etc. (Represents the number of words in each subtitle entry)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Output voice format&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;.WAV&lt;/code&gt;, &lt;code&gt;.FLAC&lt;/code&gt;, &lt;code&gt;.MP3&lt;/code&gt;, &lt;code&gt;.OPUS (best compression)&lt;/code&gt; and &lt;code&gt;M4B (with chapters)&lt;/code&gt; (Special thanks to &lt;a href="https://github.com/jborza"&gt;@jborza&lt;/a&gt; for chapter support in PR &lt;a href="https://github.com/denizsafak/abogen/pull/10"&gt;#10&lt;/a&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Output subtitle format&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Configures the subtitle format as &lt;code&gt;SRT (standard)&lt;/code&gt;, &lt;code&gt;ASS (wide)&lt;/code&gt;, &lt;code&gt;ASS (narrow)&lt;/code&gt;, &lt;code&gt;ASS (centered wide)&lt;/code&gt;, or &lt;code&gt;ASS (centered narrow)&lt;/code&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Replace single newlines with spaces&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Replaces single newlines with spaces in the text. This is useful for texts that have imaginary line breaks.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Save location&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;Save next to input file&lt;/code&gt;, &lt;code&gt;Save to desktop&lt;/code&gt;, or &lt;code&gt;Choose output folder&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Book handler options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Chapter Control&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Select specific &lt;code&gt;chapters&lt;/code&gt; from ePUBs or &lt;code&gt;chapters + pages&lt;/code&gt; from PDFs.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Save each chapter separately&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Save each chapter in e-books as a separate audio file.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Create a merged version&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Create a single audio file that combines all chapters. (If &lt;code&gt;Save each chapter separately&lt;/code&gt; is disabled, this option will be the default behavior.)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Save in a project folder with metadata&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Save the converted items in a project folder with available metadata files.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Menu options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Theme&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Change the application's theme using &lt;code&gt;System&lt;/code&gt;, &lt;code&gt;Light&lt;/code&gt;, or &lt;code&gt;Dark&lt;/code&gt; options.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Configure max words per subtitle&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Configures the maximum number of words per subtitle entry.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Configure max lines in log window&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Configures the maximum number of lines to display in the log window.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Separate chapters audio format&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Configures the audio format for separate chapters as &lt;code&gt;wav&lt;/code&gt;, &lt;code&gt;flac&lt;/code&gt;, &lt;code&gt;mp3&lt;/code&gt;, or &lt;code&gt;opus&lt;/code&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Create desktop shortcut&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Creates a shortcut on your desktop for easy access.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Open config directory&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Opens the directory where the configuration file is stored.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Open cache directory&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Opens the cache directory where converted text files are stored.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Clear cache files&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Deletes cache files created during the conversion or preview.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Check for updates at startup&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Automatically checks for updates when the program starts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Disable Kokoro's internet access&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Prevents Kokoro from downloading models or voices from HuggingFace Hub, useful for offline use.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Reset to default settings&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Resets all settings to their default values.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Special thanks to &lt;a href="https://github.com/robmckinnon"&gt;@robmckinnon&lt;/a&gt; for adding Sentence + Highlighting feature in PR &lt;a href="https://github.com/denizsafak/abogen/pull/65"&gt;#65&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;code&gt;Voice Mixer&lt;/code&gt;&lt;/h2&gt; 
&lt;img title="Abogen Voice Mixer" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/voice_mixer.png" /&gt; 
&lt;p&gt;With voice mixer, you can create custom voices by mixing different voice models. You can adjust the weight of each voice and save your custom voice as a profile for future use. The voice mixer allows you to create unique and personalized voices. (Huge thanks to &lt;a href="https://github.com/jborza"&gt;@jborza&lt;/a&gt; for making this possible through his contributions in &lt;a href="https://github.com/denizsafak/abogen/pull/5"&gt;#5&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;Queue Mode&lt;/code&gt;&lt;/h2&gt; 
&lt;img title="Abogen queue mode" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/queue.png" /&gt; 
&lt;p&gt;Abogen supports &lt;strong&gt;queue mode&lt;/strong&gt;, allowing you to add multiple files to a processing queue. This is useful if you want to convert several files in one batch.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can add text files (&lt;code&gt;.txt&lt;/code&gt;) directly using the &lt;strong&gt;Add files&lt;/strong&gt; button in the Queue Manager. To add PDF or EPUB files, use the input box in the main window and click the &lt;strong&gt;Add to Queue&lt;/strong&gt; button.&lt;/li&gt; 
 &lt;li&gt;Each file in the queue keeps the configuration settings that were active when it was added. Changing the main window configuration afterward does &lt;strong&gt;not&lt;/strong&gt; affect files already in the queue.&lt;/li&gt; 
 &lt;li&gt;You can view each file's configuration by hovering over them.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Abogen will process each item in the queue automatically, saving outputs as configured.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Special thanks to &lt;a href="https://github.com/jborza"&gt;@jborza&lt;/a&gt; for adding queue mode in PR &lt;a href="https://github.com/denizsafak/abogen/pull/35"&gt;#35&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;code&gt;About Chapter Markers&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;When you process ePUB or PDF files, Abogen converts them into text files stored in your cache directory. When you click "Edit," you're actually modifying these converted text files. In these text files, you'll notice tags that look like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;lt;&amp;lt;CHAPTER_MARKER:Chapter Title&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These are chapter markers. They are automatically added when you process ePUB or PDF files, based on the chapters you select. They serve an important purpose:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Allow you to split the text into separate audio files for each chapter&lt;/li&gt; 
 &lt;li&gt;Save time by letting you reprocess only specific chapters if errors occur, rather than the entire file&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can manually add these markers to plain text files for the same benefits. Simply include them in your text like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;lt;&amp;lt;CHAPTER_MARKER:Introduction&amp;gt;&amp;gt;
This is the beginning of my text...  

&amp;lt;&amp;lt;CHAPTER_MARKER:Main Content&amp;gt;&amp;gt; 
Here's another part...  
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When you process the text file, Abogen will detect these markers automatically and ask if you want to save each chapter separately and create a merged version.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/chapter_marker.png" alt="Abogen Chapter Marker" /&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;About Metadata Tags&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;Similar to chapter markers, it is possible to add metadata tags for &lt;code&gt;M4B&lt;/code&gt; files. This is useful for audiobook players that support metadata, allowing you to add information like title, author, year, etc. Abogen automatically adds these tags when you process ePUB or PDF files, but you can also add them manually to your text files. Add metadata tags &lt;strong&gt;at the beginning of your text file&lt;/strong&gt; like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;lt;&amp;lt;METADATA_TITLE:Title&amp;gt;&amp;gt;
&amp;lt;&amp;lt;METADATA_ARTIST:Author&amp;gt;&amp;gt;
&amp;lt;&amp;lt;METADATA_ALBUM:Album Title&amp;gt;&amp;gt;
&amp;lt;&amp;lt;METADATA_YEAR:Year&amp;gt;&amp;gt;
&amp;lt;&amp;lt;METADATA_ALBUM_ARTIST:Album Artist&amp;gt;&amp;gt;
&amp;lt;&amp;lt;METADATA_COMPOSER:Narrator&amp;gt;&amp;gt;
&amp;lt;&amp;lt;METADATA_GENRE:Audiobook&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;code&gt;Supported Languages&lt;/code&gt;&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;# üá∫üá∏ 'a' =&amp;gt; American English, üá¨üáß 'b' =&amp;gt; British English
# üá™üá∏ 'e' =&amp;gt; Spanish es
# üá´üá∑ 'f' =&amp;gt; French fr-fr
# üáÆüá≥ 'h' =&amp;gt; Hindi hi
# üáÆüáπ 'i' =&amp;gt; Italian it
# üáØüáµ 'j' =&amp;gt; Japanese: pip install misaki[ja]
# üáßüá∑ 'p' =&amp;gt; Brazilian Portuguese pt-br
# üá®üá≥ 'z' =&amp;gt; Mandarin Chinese: pip install misaki[zh]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For a complete list of supported languages and voices, refer to Kokoro's &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md"&gt;VOICES.md&lt;/a&gt;. To listen to sample audio outputs, see &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M/blob/main/SAMPLES.md"&gt;SAMPLES.md&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Japanese audio may require additional configuration. Please check &lt;a href="https://github.com/denizsafak/abogen/issues/56"&gt;#56&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;code&gt;MPV Config&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;I highly recommend using &lt;a href="https://mpv.io/installation/"&gt;MPV&lt;/a&gt; to play your audio files, as it supports displaying subtitles even without a video track. Here's my &lt;code&gt;mpv.conf&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# --- MPV Settings ---
save-position-on-quit
keep-open=yes
# --- Subtitle ---
sub-ass-override=no
sub-margin-y=50
sub-margin-x=50
# --- Audio Quality ---
audio-spdif=ac3,dts,eac3,truehd,dts-hd
audio-channels=auto
audio-samplerate=48000
volume-max=200
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;code&gt;Docker Guide&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;If you want to run Abogen in a Docker container:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://github.com/denizsafak/abogen/archive/refs/heads/main.zip"&gt;Download the repository&lt;/a&gt; and extract, or clone it using git.&lt;/li&gt; 
 &lt;li&gt;Go to &lt;code&gt;abogen&lt;/code&gt; folder. You should see &lt;code&gt;Dockerfile&lt;/code&gt; there.&lt;/li&gt; 
 &lt;li&gt;Open your termminal in that directory and run the following commands:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Build the Docker image:
docker build --progress plain -t abogen .

# Note that building the image may take a while.
# After building is complete, run the Docker container:

# Windows
docker run --name abogen -v %cd%:/shared -p 5800:5800 -p 5900:5900 --gpus all abogen

# Linux
docker run --name abogen -v $(pwd):/shared -p 5800:5800 -p 5900:5900 --gpus all abogen

# MacOS
docker run --name abogen -v $(pwd):/shared -p 5800:5800 -p 5900:5900 abogen

# We expose port 5800 for use by a web browser, 5900 if you want to connect with a VNC client.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Abogen launches automatically inside the container.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can access it via a web browser at &lt;a href="http://localhost:5800"&gt;http://localhost:5800&lt;/a&gt; or connect to it using a VNC client at &lt;code&gt;localhost:5900&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;You can use &lt;code&gt;/shared&lt;/code&gt; directory to share files between your host and the container.&lt;/li&gt; 
 &lt;li&gt;For later use, start it with &lt;code&gt;docker start abogen&lt;/code&gt; and stop it with &lt;code&gt;docker stop abogen&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Known issues:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Audio preview is not working inside container (ALSA error).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Open cache directory&lt;/code&gt; and &lt;code&gt;Open configuration directory&lt;/code&gt; options in settings not working. (Tried pcmanfm, did not work with Abogen).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;(Special thanks to &lt;a href="https://www.reddit.com/user/geo38/"&gt;@geo38&lt;/a&gt; from Reddit, who provided the Dockerfile and instructions in &lt;a href="https://www.reddit.com/r/selfhosted/comments/1k8x1yo/comment/mpe0bz8/"&gt;this comment&lt;/a&gt;.)&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;Similar Projects&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;Abogen is a standalone project, but it is inspired by and shares some similarities with other projects. Here are a few:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/santinic/audiblez"&gt;audiblez&lt;/a&gt;: Generate audiobooks from e-books. &lt;strong&gt;(Has CLI and GUI support)&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/plusuncold/autiobooks"&gt;autiobooks&lt;/a&gt;: Automatically convert epubs to audiobooks&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mateogon/pdf-narrator"&gt;pdf-narrator&lt;/a&gt;: Convert your PDFs and EPUBs into audiobooks effortlessly.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/p0n1/epub_to_audiobook"&gt;epub_to_audiobook&lt;/a&gt;: EPUB to audiobook converter, optimized for Audiobookshelf&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook"&gt;ebook2audiobook&lt;/a&gt;: Convert ebooks to audiobooks with chapters and metadata using dynamic AI models and voice cloning&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;code&gt;Roadmap&lt;/code&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add OCR scan feature for PDF files using docling/teserract.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Add chapter metadata for .m4a files. (Issue &lt;a href="https://github.com/denizsafak/abogen/issues/9"&gt;#9&lt;/a&gt;, PR &lt;a href="https://github.com/denizsafak/abogen/pull/10"&gt;#10&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add support for different languages in GUI.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Add voice formula feature that enables mixing different voice models. (Issue &lt;a href="https://github.com/denizsafak/abogen/issues/1"&gt;#1&lt;/a&gt;, PR &lt;a href="https://github.com/denizsafak/abogen/pull/5"&gt;#5&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add support for kokoro-onnx (If it's necessary).&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Add dark mode.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;code&gt;Troubleshooting&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;If you encounter any issues while running Abogen, try launching it from the command line with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;abogen-cli
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will start Abogen in command-line mode and display detailed error messages. Please open a new issue on the &lt;a href="https://github.com/denizsafak/abogen/issues"&gt;Issues&lt;/a&gt; page with the error message and a description of your problem.&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;Contributing&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;I welcome contributions! If you have ideas for new features, improvements, or bug fixes, please fork the repository and submit a pull request.&lt;/p&gt; 
&lt;h3&gt;For developers and contributors&lt;/h3&gt; 
&lt;p&gt;If you'd like to modify the code and contribute to development, you can &lt;a href="https://github.com/denizsafak/abogen/archive/refs/heads/main.zip"&gt;download the repository&lt;/a&gt;, extract it and run the following commands to build &lt;strong&gt;or&lt;/strong&gt; install the package:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Go to the directory where you extracted the repository and run:
pip install -e .      # Installs the package in editable mode
pip install build     # Install the build package
python -m build       # Builds the package in dist folder (optional)
abogen                # Opens the GUI
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Feel free to explore the code and make any changes you like.&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;Credits&lt;/code&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Abogen uses &lt;a href="https://github.com/hexgrad/kokoro"&gt;Kokoro&lt;/a&gt; for its high-quality, natural-sounding text-to-speech synthesis. Huge thanks to the Kokoro team for making this possible.&lt;/li&gt; 
 &lt;li&gt;Thanks to &lt;a href="https://github.com/wojiushixiaobai"&gt;@wojiushixiaobai&lt;/a&gt; for &lt;a href="https://github.com/wojiushixiaobai/Python-Embed-Win64"&gt;Embedded Python&lt;/a&gt; packages. These modified packages include pip pre-installed, enabling Abogen to function as a standalone application without requiring users to separately install Python in Windows.&lt;/li&gt; 
 &lt;li&gt;Thanks to creators of &lt;a href="https://github.com/aerkalov/ebooklib"&gt;EbookLib&lt;/a&gt;, a Python library for reading and writing ePub files, which is used for extracting text from ePub files.&lt;/li&gt; 
 &lt;li&gt;Special thanks to the &lt;a href="https://www.riverbankcomputing.com/software/pyqt/"&gt;PyQt&lt;/a&gt; team for providing the cross-platform GUI toolkit that powers Abogen's interface.&lt;/li&gt; 
 &lt;li&gt;Icons: &lt;a href="https://icons8.com/icon/aRiu1GGi6Aoe/usa"&gt;US&lt;/a&gt;, &lt;a href="https://icons8.com/icon/t3NE3BsOAQwq/great-britain"&gt;Great Britain&lt;/a&gt;, &lt;a href="https://icons8.com/icon/ly7tzANRt33n/spain"&gt;Spain&lt;/a&gt;, &lt;a href="https://icons8.com/icon/3muzEmi4dpD5/france"&gt;France&lt;/a&gt;, &lt;a href="https://icons8.com/icon/esGVrxg9VCJ1/india"&gt;India&lt;/a&gt;, &lt;a href="https://icons8.com/icon/PW8KZnP7qXzO/italy"&gt;Italy&lt;/a&gt;, &lt;a href="https://icons8.com/icon/McQbrq9qaQye/japan"&gt;Japan&lt;/a&gt;, &lt;a href="https://icons8.com/icon/zHmH8HpOmM90/brazil"&gt;Brazil&lt;/a&gt;, &lt;a href="https://icons8.com/icon/Ej50Oe3crXwF/china"&gt;China&lt;/a&gt;, &lt;a href="https://icons8.com/icon/uI49hxbpxTkp/female"&gt;Female&lt;/a&gt;, &lt;a href="https://icons8.com/icon/12351/male"&gt;Male&lt;/a&gt;, &lt;a href="https://icons8.com/icon/21698/adjust"&gt;Adjust&lt;/a&gt; and &lt;a href="https://icons8.com/icon/GskSeVoroQ7u/voice-id"&gt;Voice Id&lt;/a&gt; icons by &lt;a href="https://icons8.com/"&gt;Icons8&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;code&gt;License&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;This project is available under the MIT License - see the &lt;a href="https://github.com/denizsafak/abogen/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details. &lt;a href="https://github.com/hexgrad/kokoro"&gt;Kokoro&lt;/a&gt; is licensed under &lt;a href="https://github.com/hexgrad/kokoro/raw/main/LICENSE"&gt;Apache-2.0&lt;/a&gt; which allows commercial use, modification, distribution, and private use.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Subtitle generation currently works only for English. This is because Kokoro provides timestamp tokens only for English text. If you want subtitles in other languages, please request this feature in the &lt;a href="https://github.com/hexgrad/kokoro"&gt;Kokoro project&lt;/a&gt;. For more technical details, see &lt;a href="https://github.com/hexgrad/kokoro/raw/6d87f4ae7abc2d14dbc4b3ef2e5f19852e861ac2/kokoro/pipeline.py#L383"&gt;this line&lt;/a&gt; in the Kokoro's code.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Tags: audiobook, kokoro, text-to-speech, TTS, audiobook generator, audiobooks, text to speech, audiobook maker, audiobook creator, audiobook generator, voice-synthesis, text to audio, text to audio converter, text to speech converter, text to speech generator, text to speech software, text to speech app, epub to audio, pdf to audio, content-creation, media-generation&lt;/p&gt; 
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>PaddlePaddle/PaddleOCR</title>
      <link>https://github.com/PaddlePaddle/PaddleOCR</link>
      <description>&lt;p&gt;Awesome multilingual OCR and Document Parsing toolkits based on PaddlePaddle (practical ultra lightweight OCR system, support 80+ languages recognition, provide data annotation and synthesis tools, support training and deployment among server, mobile, embedded and IoT devices)&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/Banner.png" alt="PaddleOCR Banner" /&gt; &lt;/p&gt; 
 &lt;p&gt;English | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_cn.md"&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_tcn.md"&gt;ÁπÅÈ´î‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ja.md"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ko.md"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_fr.md"&gt;Fran√ßais&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ru.md"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_es.md"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ar.md"&gt;ÿßŸÑÿπÿ±ÿ®Ÿäÿ©&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleOCR"&gt;&lt;img src="https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?color=ccf" alt="stars" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2507.05595"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2507.05595-b31b1b.svg?logo=arXiv" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/paddleocr"&gt;&lt;img src="https://static.pepy.tech/badge/paddleocr/month" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/paddleocr"&gt;&lt;img src="https://static.pepy.tech/badge/paddleocr" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/PaddlePaddle/PaddleOCR/network/dependents"&gt;&lt;img src="https://img.shields.io/badge/Used%20by-5.8k%2B%20repositories-blue" alt="Used by" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://img.shields.io/badge/python-3.8~3.12-aff.svg?sanitize=true" alt="python" /&gt; &lt;img src="https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg?sanitize=true" alt="os" /&gt; &lt;img src="https://img.shields.io/badge/hardware-cpu%2C%20gpu%2C%20xpu%2C%20npu-yellow.svg?sanitize=true" alt="hardware" /&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-Apache_2.0-green" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/PaddlePaddle/PaddleOCR"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;PaddleOCR is an industry-leading, production-ready OCR and document AI engine, offering end-to-end solutions from text extraction to intelligent document understanding&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h1&gt;PaddleOCR&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://www.paddlepaddle.org.cn/en"&gt;&lt;img src="https://img.shields.io/badge/PaddlePaddle-3.0-orange" alt="Framework" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Recognition%20Accuracy-%F0%9F%8F%86-green" alt="Accuracy" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Support_Languages-80+-brightgreen" alt="Multi-Language" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Handwriting-%E2%9C%93-success" alt="Handwriting" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Heterogeneous%20Hardware-Kunlunxin%20%7C%20Ascend_NPU-red" alt="Hardware" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] PaddleOCR now provides an MCP server that supports integration with Agent applications like Claude Desktop. For details, please refer to &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html"&gt;PaddleOCR MCP Server&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;The PaddleOCR 3.0 Technical Report is now available. See details at: &lt;a href="https://arxiv.org/abs/2507.05595"&gt;PaddleOCR 3.0 Technical Report&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;PaddleOCR&lt;/strong&gt; converts documents and images into &lt;strong&gt;structured, AI-friendly data&lt;/strong&gt; (like JSON and Markdown) with &lt;strong&gt;industry-leading accuracy&lt;/strong&gt;‚Äîpowering AI applications for everyone from indie developers and startups to large enterprises worldwide. With over &lt;strong&gt;50,000 stars&lt;/strong&gt; and deep integration into leading projects like &lt;strong&gt;MinerU, RAGFlow, and OmniParser&lt;/strong&gt;, PaddleOCR has become the &lt;strong&gt;premier solution&lt;/strong&gt; for developers building intelligent document applications in the &lt;strong&gt;AI era&lt;/strong&gt;.&lt;/p&gt; 
&lt;h3&gt;PaddleOCR 3.0 Core Features&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aistudio.baidu.com/community/app/91660/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_OCRv5-Demo_on_AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518494/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_StructureV3-Demo_on_AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518493/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_ChatOCRv4-Demo_on_AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://www.modelscope.cn/organization/PaddlePaddle"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%96_Demo_on_ModelScope-purple" alt="ModelScope" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/PaddlePaddle"&gt;&lt;img src="https://img.shields.io/badge/Demo_on_HuggingFace-purple.svg?logo=huggingface" alt="HuggingFace" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-OCRv5 ‚Äî Universal Scene Text Recognition&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;Single model supports five text types&lt;/strong&gt; (Simplified Chinese, Traditional Chinese, English, Japanese, and Pinyin) with &lt;strong&gt;13% accuracy improvement&lt;/strong&gt;. Solves multilingual mixed document recognition challenges.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-StructureV3 ‚Äî Complex Document Parsing&lt;/strong&gt;&lt;br /&gt; Intelligently converts complex PDFs and document images into &lt;strong&gt;Markdown and JSON files that preserve original structure&lt;/strong&gt;. &lt;strong&gt;Outperforms&lt;/strong&gt; numerous commercial solutions in public benchmarks. &lt;strong&gt;Perfectly maintains document layout and hierarchical structure&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-ChatOCRv4 ‚Äî Intelligent Information Extraction&lt;/strong&gt;&lt;br /&gt; Natively integrates ERNIE 4.5 to &lt;strong&gt;precisely extract key information&lt;/strong&gt; from massive documents, with 15% accuracy improvement over previous generation. Makes documents "&lt;strong&gt;understand&lt;/strong&gt;" your questions and provide accurate answers.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In addition to providing an outstanding model library, PaddleOCR 3.0 also offers user-friendly tools covering model training, inference, and service deployment, so developers can rapidly bring AI applications to production.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/Arch.jpg" alt="PaddleOCR Architecture" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Special Note&lt;/strong&gt;: PaddleOCR 3.x introduces several significant interface changes. &lt;strong&gt;Old code written based on PaddleOCR 2.x is likely incompatible with PaddleOCR 3.x&lt;/strong&gt;. Please ensure that the documentation you are reading matches the version of PaddleOCR you are using. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/upgrade_notes.html"&gt;This document&lt;/a&gt; explains the reasons for the upgrade and the major changes from PaddleOCR 2.x to 3.x.&lt;/p&gt; 
&lt;h2&gt;üì£ Recent updates&lt;/h2&gt; 
&lt;h3&gt;üî•üî•2025.08.21: Release of PaddleOCR 3.2.0, includes:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Significant Model Additions:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Introduced training, inference, and deployment for PP-OCRv5 recognition models in English, Thai, and Greek. &lt;strong&gt;The PP-OCRv5 English model delivers an 11% improvement in English scenarios compared to the main PP-OCRv5 model, with the Thai and Greek recognition models achieving accuracies of 82.68% and 89.28%, respectively.&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deployment Capability Upgrades:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Full support for PaddlePaddle framework versions 3.1.0 and 3.1.1.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Comprehensive upgrade of the PP-OCRv5 C++ local deployment solution, now supporting both Linux and Windows, with feature parity and identical accuracy to the Python implementation.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;High-performance inference now supports CUDA 12, and inference can be performed using either the Paddle Inference or ONNX Runtime backends.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;The high-stability service-oriented deployment solution is now fully open-sourced, allowing users to customize Docker images and SDKs as required.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;The high-stability service-oriented deployment solution also supports invocation via manually constructed HTTP requests, enabling client-side code development in any programming language.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Benchmark Support:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;All production lines now support fine-grained benchmarking, enabling measurement of end-to-end inference time as well as per-layer and per-module latency data to assist with performance analysis. &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/version3.x/pipeline_usage/instructions/benchmark.en.md"&gt;Here's&lt;/a&gt; how to set up and use the benchmark feature.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Documentation has been updated to include key metrics for commonly used configurations on mainstream hardware, such as inference latency and memory usage, providing deployment references for users.&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Resolved the issue of failed log saving during model training.&lt;/li&gt; 
   &lt;li&gt;Upgraded the data augmentation component for formula models for compatibility with newer versions of the albumentations dependency, and fixed deadlock warnings when using the tokenizers package in multi-process scenarios.&lt;/li&gt; 
   &lt;li&gt;Fixed inconsistencies in switch behaviors (e.g., &lt;code&gt;use_chart_parsing&lt;/code&gt;) in the PP-StructureV3 configuration files compared to other pipelines.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Other Enhancements:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Separated core and optional dependencies. Only minimal core dependencies are required for basic text recognition; additional dependencies for document parsing and information extraction can be installed as needed.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Enabled support for NVIDIA RTX 50 series graphics cards on Windows; users can refer to the &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/version3.x/installation.en.md"&gt;installation guide&lt;/a&gt; for the corresponding PaddlePaddle framework versions.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;PP-OCR series models now support returning single-character coordinates.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;Added AIStudio, ModelScope, and other model download sources, allowing users to specify the source for model downloads.&lt;/li&gt; 
   &lt;li&gt;Added support for chart-to-table conversion via the PP-Chart2Table module.&lt;/li&gt; 
   &lt;li&gt;Optimized documentation descriptions to improve usability.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.08.15: PaddleOCR 3.1.1 Released&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Added the missing methods &lt;code&gt;save_vector&lt;/code&gt;, &lt;code&gt;save_visual_info_list&lt;/code&gt;, &lt;code&gt;load_vector&lt;/code&gt;, and &lt;code&gt;load_visual_info_list&lt;/code&gt; in the &lt;code&gt;PP-ChatOCRv4&lt;/code&gt; class.&lt;/li&gt; 
    &lt;li&gt;Added the missing parameters &lt;code&gt;glossary&lt;/code&gt; and &lt;code&gt;llm_request_interval&lt;/code&gt; to the &lt;code&gt;translate&lt;/code&gt; method in the &lt;code&gt;PPDocTranslation&lt;/code&gt; class.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Improvements:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Added a demo to the MCP documentation.&lt;/li&gt; 
    &lt;li&gt;Added information about the PaddlePaddle and PaddleOCR version used for performance metrics testing in the documentation.&lt;/li&gt; 
    &lt;li&gt;Fixed errors and omissions in the production line document translation.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Others:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Changed the MCP server dependency to use the pure Python library &lt;code&gt;puremagic&lt;/code&gt; instead of &lt;code&gt;python-magic&lt;/code&gt; to reduce installation issues.&lt;/li&gt; 
    &lt;li&gt;Retested PP-OCRv5 performance metrics with PaddleOCR version 3.1.0 and updated the documentation.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.29: PaddleOCR 3.1.0 Released&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Key Models and Pipelines:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Added PP-OCRv5 Multilingual Text Recognition Model&lt;/strong&gt;, which supports the training and inference process for text recognition models in 37 languages, including French, Spanish, Portuguese, Russian, Korean, etc. &lt;strong&gt;Average accuracy improved by over 30%.&lt;/strong&gt; &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/algorithm/PP-OCRv5/PP-OCRv5_multi_languages.html"&gt;Details&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Upgraded the &lt;strong&gt;PP-Chart2Table model&lt;/strong&gt; in PP-StructureV3, further enhancing the capability of converting charts to tables. On internal custom evaluation sets, the metric (RMS-F1) &lt;strong&gt;increased by 9.36 percentage points (71.24% -&amp;gt; 80.60%).&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Newly launched &lt;strong&gt;document translation pipeline, PP-DocTranslation, based on PP-StructureV3 and ERNIE 4.5&lt;/strong&gt;, which supports the translation of Markdown format documents, various complex-layout PDF documents, and document images, with the results saved as Markdown format documents. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/pipeline_usage/PP-DocTranslation.html"&gt;Details&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;New MCP server:&lt;/strong&gt; &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html"&gt;Details&lt;/a&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Supports both OCR and PP-StructureV3 pipelines.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Supports three working modes: local Python library, AIStudio Community Cloud Service, and self-hosted service.&lt;/li&gt; 
    &lt;li&gt;Supports invoking local services via stdio and remote services via Streamable HTTP.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Optimization:&lt;/strong&gt; Improved the descriptions in some user guides for a smoother reading experience.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.26: PaddleOCR 3.0.3 Released&lt;/strong&gt;&lt;/summary&gt; - Bug Fix: Resolved the issue where the `enable_mkldnn` parameter was not effective, restoring the default behavior of using MKL-DNN for CPU inference. 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.19: PaddleOCR 3.0.2 Released&lt;/strong&gt;&lt;/summary&gt; - **New Features:** 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;The default download source has been changed from &lt;code&gt;BOS&lt;/code&gt; to &lt;code&gt;HuggingFace&lt;/code&gt;. Users can also change the environment variable &lt;code&gt;PADDLE_PDX_MODEL_SOURCE&lt;/code&gt; to &lt;code&gt;BOS&lt;/code&gt; to set the model download source back to Baidu Object Storage (BOS).&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Added service invocation examples for six languages‚ÄîC++, Java, Go, C#, Node.js, and PHP‚Äîfor pipelines like PP-OCRv5, PP-StructureV3, and PP-ChatOCRv4.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Improved the layout partition sorting algorithm in the PP-StructureV3 pipeline, enhancing the sorting logic for complex vertical layouts to deliver better results.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Enhanced model selection logic: when a language is specified but a model version is not, the system will automatically select the latest model version supporting that language.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Set a default upper limit for MKL-DNN cache size to prevent unlimited growth, while also allowing users to configure cache capacity.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Updated default configurations for high-performance inference to support Paddle MKL-DNN acceleration and optimized the logic for automatic configuration selection for smarter choices.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Adjusted the logic for obtaining the default device to consider the actual support for computing devices by the installed Paddle framework, making program behavior more intuitive.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Added Android example for PP-OCRv5. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/on_device_deployment.html"&gt;Details&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Fixed an issue with some CLI parameters in PP-StructureV3 not taking effect.&lt;/li&gt; 
    &lt;li&gt;Resolved an issue where &lt;code&gt;export_paddlex_config_to_yaml&lt;/code&gt; would not function correctly in certain cases.&lt;/li&gt; 
    &lt;li&gt;Corrected the discrepancy between the actual behavior of &lt;code&gt;save_path&lt;/code&gt; and its documentation description.&lt;/li&gt; 
    &lt;li&gt;Fixed potential multithreading errors when using MKL-DNN in basic service deployment.&lt;/li&gt; 
    &lt;li&gt;Corrected channel order errors in image preprocessing for the Latex-OCR model.&lt;/li&gt; 
    &lt;li&gt;Fixed channel order errors in saving visualized images within the text recognition module.&lt;/li&gt; 
    &lt;li&gt;Resolved channel order errors in visualized table results within PP-StructureV3 pipeline.&lt;/li&gt; 
    &lt;li&gt;Fixed an overflow issue in the calculation of &lt;code&gt;overlap_ratio&lt;/code&gt; under extremely special circumstances in the PP-StructureV3 pipeline.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Improvements:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Updated the description of the &lt;code&gt;enable_mkldnn&lt;/code&gt; parameter in the documentation to accurately reflect the program's actual behavior.&lt;/li&gt; 
    &lt;li&gt;Fixed errors in the documentation regarding the &lt;code&gt;lang&lt;/code&gt; and &lt;code&gt;ocr_version&lt;/code&gt; parameters.&lt;/li&gt; 
    &lt;li&gt;Added instructions for exporting pipeline configuration files via CLI.&lt;/li&gt; 
    &lt;li&gt;Fixed missing columns in the performance data table for PP-OCRv5.&lt;/li&gt; 
    &lt;li&gt;Refined benchmark metrics for PP-StructureV3 across different configurations.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Others:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Relaxed version restrictions on dependencies like numpy and pandas, restoring support for Python 3.12.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;History Log&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;2025.06.05: &lt;strong&gt;PaddleOCR 3.0.1 Released&lt;/strong&gt;, includes:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Optimisation of certain models and model configurations:&lt;/strong&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Updated the default model configuration for PP-OCRv5, changing both detection and recognition from mobile to server models. To improve default performance in most scenarios, the parameter &lt;code&gt;limit_side_len&lt;/code&gt; in the configuration has been changed from 736 to 64.&lt;/li&gt; 
    &lt;li&gt;Added a new text line orientation classification model &lt;code&gt;PP-LCNet_x1_0_textline_ori&lt;/code&gt; with an accuracy of 99.42%. The default text line orientation classifier for OCR, PP-StructureV3, and PP-ChatOCRv4 pipelines has been updated to this model.&lt;/li&gt; 
    &lt;li&gt;Optimized the text line orientation classification model &lt;code&gt;PP-LCNet_x0_25_textline_ori&lt;/code&gt;, improving accuracy by 3.3 percentage points to a current accuracy of 98.85%.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Optimizations and fixes for some issues in version 3.0.0, &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/update.html"&gt;details&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;üî•üî•2025.05.20: Official Release of &lt;strong&gt;PaddleOCR v3.0&lt;/strong&gt;, including:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-OCRv5&lt;/strong&gt;: High-Accuracy Text Recognition Model for All Scenarios - Instant Text from Images/PDFs.&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;üåê Single-model support for &lt;strong&gt;five&lt;/strong&gt; text types - Seamlessly process &lt;strong&gt;Simplified Chinese, Traditional Chinese, Simplified Chinese Pinyin, English&lt;/strong&gt; and &lt;strong&gt;Japanese&lt;/strong&gt; within a single model.&lt;/li&gt; 
    &lt;li&gt;‚úçÔ∏è Improved &lt;strong&gt;handwriting recognition&lt;/strong&gt;: Significantly better at complex cursive scripts and non-standard handwriting.&lt;/li&gt; 
    &lt;li&gt;üéØ &lt;strong&gt;13-point accuracy gain&lt;/strong&gt; over PP-OCRv4, achieving state-of-the-art performance across a variety of real-world scenarios.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-StructureV3&lt;/strong&gt;: General-Purpose Document Parsing ‚Äì Unleash SOTA Images/PDFs Parsing for Real-World Scenarios!&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;üßÆ &lt;strong&gt;High-Accuracy multi-scene PDF parsing&lt;/strong&gt;, leading both open- and closed-source solutions on the OmniDocBench benchmark.&lt;/li&gt; 
    &lt;li&gt;üß† Specialized capabilities include &lt;strong&gt;seal recognition&lt;/strong&gt;, &lt;strong&gt;chart-to-table conversion&lt;/strong&gt;, &lt;strong&gt;table recognition with nested formulas/images&lt;/strong&gt;, &lt;strong&gt;vertical text document parsing&lt;/strong&gt;, and &lt;strong&gt;complex table structure analysis&lt;/strong&gt;.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-ChatOCRv4&lt;/strong&gt;: Intelligent Document Understanding ‚Äì Extract Key Information, not just text from Images/PDFs.&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;üî• &lt;strong&gt;15-point accuracy gain&lt;/strong&gt; in key-information extraction on PDF/PNG/JPG files over the previous generation.&lt;/li&gt; 
    &lt;li&gt;üíª Native support for &lt;strong&gt;ERNIE 4.5&lt;/strong&gt;, with compatibility for large-model deployments via PaddleNLP, Ollama, vLLM, and more.&lt;/li&gt; 
    &lt;li&gt;ü§ù Integrated &lt;a href="https://github.com/PaddlePaddle/PaddleMIX/tree/develop/paddlemix/examples/ppdocbee2"&gt;PP-DocBee2&lt;/a&gt;, enabling extraction and understanding of printed text, handwriting, seals, tables, charts, and other common elements in complex documents.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/update.html"&gt;History Log&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚ö° Quick Start&lt;/h2&gt; 
&lt;h3&gt;1. Run online demo&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aistudio.baidu.com/community/app/91660/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_OCRv5-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518494/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_StructureV3-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518493/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_ChatOCRv4-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;2. Installation&lt;/h3&gt; 
&lt;p&gt;Install PaddlePaddle refer to &lt;a href="https://www.paddlepaddle.org.cn/en/install/quick?docurl=/documentation/docs/en/develop/install/pip/linux-pip_en.html"&gt;Installation Guide&lt;/a&gt;, after then, install the PaddleOCR toolkit.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# If you only want to use the basic text recognition feature (returns text position coordinates and content), including the PP-OCR series
python -m pip install paddleocr
# If you want to use all features such as document parsing, document understanding, document translation, key information extraction, etc.
# python -m pip install "paddleocr[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Starting from version 3.2.0, in addition to the &lt;code&gt;all&lt;/code&gt; dependency group demonstrated above, PaddleOCR also supports installing partial optional features by specifying other dependency groups. All dependency groups provided by PaddleOCR are as follows:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Dependency Group Name&lt;/th&gt; 
   &lt;th&gt;Corresponding Functionality&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;doc-parser&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Document parsing: can be used to extract layout elements such as tables, formulas, stamps, images, etc. from documents; includes models like PP-StructureV3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ie&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Information extraction: can be used to extract key information from documents, such as names, dates, addresses, amounts, etc.; includes models like PP-ChatOCRv4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;trans&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Document translation: can be used to translate documents from one language to another; includes models like PP-DocTranslation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;all&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Complete functionality&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;3. Run inference by CLI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run PP-OCRv5 inference
paddleocr ocr -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png --use_doc_orientation_classify False --use_doc_unwarping False --use_textline_orientation False  

# Run PP-StructureV3 inference
paddleocr pp_structurev3 -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png --use_doc_orientation_classify False --use_doc_unwarping False

# Get the Qianfan API Key at first, and then run PP-ChatOCRv4 inference
paddleocr pp_chatocrv4_doc -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png -k È©æÈ©∂ÂÆ§ÂáÜ‰πò‰∫∫Êï∞ --qianfan_api_key your_api_key --use_doc_orientation_classify False --use_doc_unwarping False 

# Get more information about "paddleocr ocr"
paddleocr ocr --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4. Run inference by API&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;4.1 PP-OCRv5 Example&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Initialize PaddleOCR instance
from paddleocr import PaddleOCR
ocr = PaddleOCR(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False,
    use_textline_orientation=False)

# Run OCR inference on a sample image 
result = ocr.predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png")

# Visualize the results and save the JSON results
for res in result:
    res.print()
    res.save_to_img("output")
    res.save_to_json("output")
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;4.2 PP-StructureV3 Example&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from pathlib import Path
from paddleocr import PPStructureV3

pipeline = PPStructureV3(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False
)

# For Image
output = pipeline.predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png",
)

# Visualize the results and save the JSON results
for res in output:
    res.print() 
    res.save_to_json(save_path="output") 
    res.save_to_markdown(save_path="output")           
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;4.3 PP-ChatOCRv4 Example&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from paddleocr import PPChatOCRv4Doc

chat_bot_config = {
    "module_name": "chat_bot",
    "model_name": "ernie-3.5-8k",
    "base_url": "https://qianfan.baidubce.com/v2",
    "api_type": "openai",
    "api_key": "api_key",  # your api_key
}

retriever_config = {
    "module_name": "retriever",
    "model_name": "embedding-v1",
    "base_url": "https://qianfan.baidubce.com/v2",
    "api_type": "qianfan",
    "api_key": "api_key",  # your api_key
}

pipeline = PPChatOCRv4Doc(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False
)

visual_predict_res = pipeline.visual_predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png",
    use_common_ocr=True,
    use_seal_recognition=True,
    use_table_recognition=True,
)

mllm_predict_info = None
use_mllm = False
# If a multimodal large model is used, the local mllm service needs to be started. You can refer to the documentation: https://github.com/PaddlePaddle/PaddleX/blob/release/3.0/docs/pipeline_usage/tutorials/vlm_pipelines/doc_understanding.en.md performs deployment and updates the mllm_chat_bot_config configuration.
if use_mllm:
    mllm_chat_bot_config = {
        "module_name": "chat_bot",
        "model_name": "PP-DocBee",
        "base_url": "http://127.0.0.1:8080/",  # your local mllm service url
        "api_type": "openai",
        "api_key": "api_key",  # your api_key
    }

    mllm_predict_res = pipeline.mllm_pred(
        input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png",
        key_list=["È©æÈ©∂ÂÆ§ÂáÜ‰πò‰∫∫Êï∞"],
        mllm_chat_bot_config=mllm_chat_bot_config,
    )
    mllm_predict_info = mllm_predict_res["mllm_res"]

visual_info_list = []
for res in visual_predict_res:
    visual_info_list.append(res["visual_info"])
    layout_parsing_result = res["layout_parsing_result"]

vector_info = pipeline.build_vector(
    visual_info_list, flag_save_bytes_vector=True, retriever_config=retriever_config
)
chat_result = pipeline.chat(
    key_list=["È©æÈ©∂ÂÆ§ÂáÜ‰πò‰∫∫Êï∞"],
    visual_info=visual_info_list,
    vector_info=vector_info,
    mllm_predict_info=mllm_predict_info,
    chat_bot_config=chat_bot_config,
    retriever_config=retriever_config,
)
print(chat_result)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;5. Chinese Heterogeneous AI Accelerators&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/other_devices_support/paddlepaddle_install_NPU.html"&gt;Huawei Ascend&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/other_devices_support/paddlepaddle_install_XPU.html"&gt;KUNLUNXIN&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;More Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Convert models to ONNX format: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/obtaining_onnx_models.html"&gt;Obtaining ONNX Models&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Accelerate inference using engines like OpenVINO, ONNX Runtime, TensorRT, or perform inference using ONNX format models: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/high_performance_inference.html"&gt;High-Performance Inference&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Accelerate inference using multi-GPU and multi-process: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/pipeline_usage/instructions/parallel_inference.html"&gt;Parallel Inference for Pipelines&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Integrate PaddleOCR into applications written in C++, C#, Java, etc.: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/serving.html"&gt;Serving&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚õ∞Ô∏è Advanced Tutorials&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/OCR.html"&gt;PP-OCRv5 Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PP-StructureV3.html"&gt;PP-StructureV3 Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PP-ChatOCRv4.html"&gt;PP-ChatOCRv4 Tutorial&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üîÑ Quick Overview of Execution Results&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/demo.gif" alt="PP-OCRv5 Demo" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/blue_v3.gif" alt="PP-StructureV3 Demo" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üë©‚Äçüë©‚Äçüëß‚Äçüë¶ Community&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;PaddlePaddle WeChat official account&lt;/th&gt; 
   &lt;th align="center"&gt;Join the tech discussion group&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/refs/heads/main/images/paddleocr/README/qrcode_for_paddlepaddle_official_account.jpg" width="150" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/refs/heads/main/images/paddleocr/README/qr_code_for_the_questionnaire.jpg" width="150" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üòÉ Awesome Projects Leveraging PaddleOCR&lt;/h2&gt; 
&lt;p&gt;PaddleOCR wouldn't be where it is today without its incredible community! üíó A massive thank you to all our longtime partners, new collaborators, and everyone who's poured their passion into PaddleOCR ‚Äî whether we've named you or not. Your support fuels our fire!&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Project Name&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/infiniflow/ragflow"&gt;RAGFlow&lt;/a&gt; &lt;a href="https://github.com/infiniflow/ragflow"&gt;&lt;img src="https://img.shields.io/github/stars/infiniflow/ragflow" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;RAG engine based on deep document understanding.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/opendatalab/MinerU"&gt;MinerU&lt;/a&gt; &lt;a href="https://github.com/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/github/stars/opendatalab/MinerU" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Multi-type Document to Markdown Conversion Tool&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/hiroi-sora/Umi-OCR"&gt;Umi-OCR&lt;/a&gt; &lt;a href="https://github.com/hiroi-sora/Umi-OCR"&gt;&lt;img src="https://img.shields.io/github/stars/hiroi-sora/Umi-OCR" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Free, Open-source, Batch Offline OCR Software.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/OmniParser"&gt;OmniParser&lt;/a&gt;&lt;a href="https://github.com/microsoft/OmniParser"&gt;&lt;img src="https://img.shields.io/github/stars/microsoft/OmniParser" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;OmniParser: Screen Parsing tool for Pure Vision Based GUI Agent.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/netease-youdao/QAnything"&gt;QAnything&lt;/a&gt;&lt;a href="https://github.com/netease-youdao/QAnything"&gt;&lt;img src="https://img.shields.io/github/stars/netease-youdao/QAnything" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Question and Answer based on Anything.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;PDF-Extract-Kit&lt;/a&gt; &lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;&lt;img src="https://img.shields.io/github/stars/opendatalab/PDF-Extract-Kit" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A powerful open-source toolkit designed to efficiently extract high-quality content from complex and diverse PDF documents.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/PantsuDango/Dango-Translator"&gt;Dango-Translator&lt;/a&gt;&lt;a href="https://github.com/PantsuDango/Dango-Translator"&gt;&lt;img src="https://img.shields.io/github/stars/PantsuDango/Dango-Translator" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Recognize text on the screen, translate it and show the translation results in real time.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/awesome_projects.md"&gt;Learn more projects&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/awesome_projects.md"&gt;More projects based on PaddleOCR&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üë©‚Äçüë©‚Äçüëß‚Äçüë¶ Contributors&lt;/h2&gt; 
&lt;a href="https://github.com/PaddlePaddle/PaddleOCR/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=PaddlePaddle/PaddleOCR&amp;amp;max=400&amp;amp;columns=20" width="800" /&gt; &lt;/a&gt; 
&lt;h2&gt;üåü Star&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#PaddlePaddle/PaddleOCR&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=PaddlePaddle/PaddleOCR&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;This project is released under the &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/LICENSE"&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üéì Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@misc{cui2025paddleocr30technicalreport,
      title={PaddleOCR 3.0 Technical Report}, 
      author={Cheng Cui and Ting Sun and Manhui Lin and Tingquan Gao and Yubo Zhang and Jiaxuan Liu and Xueqing Wang and Zelun Zhang and Changda Zhou and Hongen Liu and Yue Zhang and Wenyu Lv and Kui Huang and Yichao Zhang and Jing Zhang and Jun Zhang and Yi Liu and Dianhai Yu and Yanjun Ma},
      year={2025},
      eprint={2507.05595},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2507.05595}, 
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>huggingface/diffusers</title>
      <link>https://github.com/huggingface/diffusers</link>
      <description>&lt;p&gt;ü§ó Diffusers: State-of-the-art diffusion models for image, video, and audio generation in PyTorch.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;br /&gt; &lt;img src="https://raw.githubusercontent.com/huggingface/diffusers/main/docs/source/en/imgs/diffusers_library.jpg" width="400" /&gt; &lt;br /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align="center"&gt; &lt;a href="https://github.com/huggingface/diffusers/raw/main/LICENSE"&gt;&lt;img alt="GitHub" src="https://img.shields.io/github/license/huggingface/datasets.svg?color=blue" /&gt;&lt;/a&gt; &lt;a href="https://github.com/huggingface/diffusers/releases"&gt;&lt;img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/diffusers.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/diffusers"&gt;&lt;img alt="GitHub release" src="https://static.pepy.tech/badge/diffusers/month" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/huggingface/diffusers/main/CODE_OF_CONDUCT.md"&gt;&lt;img alt="Contributor Covenant" src="https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/diffuserslib"&gt;&lt;img alt="X account" src="https://img.shields.io/twitter/url/https/twitter.com/diffuserslib.svg?style=social&amp;amp;label=Follow%20%40diffuserslib" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;ü§ó Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or training your own diffusion models, ü§ó Diffusers is a modular toolbox that supports both. Our library is designed with a focus on &lt;a href="https://huggingface.co/docs/diffusers/conceptual/philosophy#usability-over-performance"&gt;usability over performance&lt;/a&gt;, &lt;a href="https://huggingface.co/docs/diffusers/conceptual/philosophy#simple-over-easy"&gt;simple over easy&lt;/a&gt;, and &lt;a href="https://huggingface.co/docs/diffusers/conceptual/philosophy#tweakable-contributorfriendly-over-abstraction"&gt;customizability over abstractions&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;ü§ó Diffusers offers three core components:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;State-of-the-art &lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/overview"&gt;diffusion pipelines&lt;/a&gt; that can be run in inference with just a few lines of code.&lt;/li&gt; 
 &lt;li&gt;Interchangeable noise &lt;a href="https://huggingface.co/docs/diffusers/api/schedulers/overview"&gt;schedulers&lt;/a&gt; for different diffusion speeds and output quality.&lt;/li&gt; 
 &lt;li&gt;Pretrained &lt;a href="https://huggingface.co/docs/diffusers/api/models/overview"&gt;models&lt;/a&gt; that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;We recommend installing ü§ó Diffusers in a virtual environment from PyPI or Conda. For more details about installing &lt;a href="https://pytorch.org/get-started/locally/"&gt;PyTorch&lt;/a&gt;, please refer to their official documentation.&lt;/p&gt; 
&lt;h3&gt;PyTorch&lt;/h3&gt; 
&lt;p&gt;With &lt;code&gt;pip&lt;/code&gt; (official package):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade diffusers[torch]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With &lt;code&gt;conda&lt;/code&gt; (maintained by the community):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;conda install -c conda-forge diffusers
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Apple Silicon (M1/M2) support&lt;/h3&gt; 
&lt;p&gt;Please refer to the &lt;a href="https://huggingface.co/docs/diffusers/optimization/mps"&gt;How to use Stable Diffusion in Apple Silicon&lt;/a&gt; guide.&lt;/p&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;Generating outputs is super easy with ü§ó Diffusers. To generate an image from text, use the &lt;code&gt;from_pretrained&lt;/code&gt; method to load any pretrained diffusion model (browse the &lt;a href="https://huggingface.co/models?library=diffusers&amp;amp;sort=downloads"&gt;Hub&lt;/a&gt; for 30,000+ checkpoints):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from diffusers import DiffusionPipeline
import torch

pipeline = DiffusionPipeline.from_pretrained("stable-diffusion-v1-5/stable-diffusion-v1-5", torch_dtype=torch.float16)
pipeline.to("cuda")
pipeline("An image of a squirrel in Picasso style").images[0]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also dig into the models and schedulers toolbox to build your own diffusion system:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from diffusers import DDPMScheduler, UNet2DModel
from PIL import Image
import torch

scheduler = DDPMScheduler.from_pretrained("google/ddpm-cat-256")
model = UNet2DModel.from_pretrained("google/ddpm-cat-256").to("cuda")
scheduler.set_timesteps(50)

sample_size = model.config.sample_size
noise = torch.randn((1, 3, sample_size, sample_size), device="cuda")
input = noise

for t in scheduler.timesteps:
    with torch.no_grad():
        noisy_residual = model(input, t).sample
        prev_noisy_sample = scheduler.step(noisy_residual, t, input).prev_sample
        input = prev_noisy_sample

image = (input / 2 + 0.5).clamp(0, 1)
image = image.cpu().permute(0, 2, 3, 1).numpy()[0]
image = Image.fromarray((image * 255).round().astype("uint8"))
image
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Check out the &lt;a href="https://huggingface.co/docs/diffusers/quicktour"&gt;Quickstart&lt;/a&gt; to launch your diffusion journey today!&lt;/p&gt; 
&lt;h2&gt;How to navigate the documentation&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;What can I learn?&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/tutorials/tutorial_overview"&gt;Tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A basic crash course for learning how to use the library's most important features like using models and schedulers to build your own diffusion system, and training your own diffusion model.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/using-diffusers/loading"&gt;Loading&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Guides for how to load and configure all the components (pipelines, models, and schedulers) of the library, as well as how to use different schedulers.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/using-diffusers/overview_techniques"&gt;Pipelines for inference&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Guides for how to use pipelines for different inference tasks, batched generation, controlling generated outputs and randomness, and how to contribute a pipeline to the library.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/optimization/fp16"&gt;Optimization&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Guides for how to optimize your diffusion model to run faster and consume less memory.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/training/overview"&gt;Training&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Guides for how to train a diffusion model for different tasks with different training techniques.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;p&gt;We ‚ù§Ô∏è contributions from the open-source community! If you want to contribute to this library, please check out our &lt;a href="https://github.com/huggingface/diffusers/raw/main/CONTRIBUTING.md"&gt;Contribution guide&lt;/a&gt;. You can look out for &lt;a href="https://github.com/huggingface/diffusers/issues"&gt;issues&lt;/a&gt; you'd like to tackle to contribute to the library.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;See &lt;a href="https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22"&gt;Good first issues&lt;/a&gt; for general opportunities to contribute&lt;/li&gt; 
 &lt;li&gt;See &lt;a href="https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3A%22New+pipeline%2Fmodel%22"&gt;New model/pipeline&lt;/a&gt; to contribute exciting new diffusion models / diffusion pipelines&lt;/li&gt; 
 &lt;li&gt;See &lt;a href="https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3A%22New+scheduler%22"&gt;New scheduler&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Also, say üëã in our public Discord channel &lt;a href="https://discord.gg/G7tWnz98XR"&gt;&lt;img alt="Join us on Discord" src="https://img.shields.io/discord/823813159592001537?color=5865F2&amp;amp;logo=discord&amp;amp;logoColor=white" /&gt;&lt;/a&gt;. We discuss the hottest trends about diffusion models, help each other with contributions, personal projects or just hang out ‚òï.&lt;/p&gt; 
&lt;h2&gt;Popular Tasks &amp;amp; Pipelines&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th&gt;Task&lt;/th&gt; 
   &lt;th&gt;Pipeline&lt;/th&gt; 
   &lt;th&gt;ü§ó Hub&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr style="border-top: 2px solid black"&gt; 
   &lt;td&gt;Unconditional Image Generation&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/ddpm"&gt; DDPM &lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/google/ddpm-ema-church-256"&gt; google/ddpm-ema-church-256 &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="border-top: 2px solid black"&gt; 
   &lt;td&gt;Text-to-Image&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/text2img"&gt;Stable Diffusion Text-to-Image&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5"&gt; stable-diffusion-v1-5/stable-diffusion-v1-5 &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Text-to-Image&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/unclip"&gt;unCLIP&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/kakaobrain/karlo-v1-alpha"&gt; kakaobrain/karlo-v1-alpha &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Text-to-Image&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/deepfloyd_if"&gt;DeepFloyd IF&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/DeepFloyd/IF-I-XL-v1.0"&gt; DeepFloyd/IF-I-XL-v1.0 &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Text-to-Image&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/kandinsky"&gt;Kandinsky&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder"&gt; kandinsky-community/kandinsky-2-2-decoder &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="border-top: 2px solid black"&gt; 
   &lt;td&gt;Text-guided Image-to-Image&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/controlnet"&gt;ControlNet&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/lllyasviel/sd-controlnet-canny"&gt; lllyasviel/sd-controlnet-canny &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Text-guided Image-to-Image&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/pix2pix"&gt;InstructPix2Pix&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/timbrooks/instruct-pix2pix"&gt; timbrooks/instruct-pix2pix &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Text-guided Image-to-Image&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/img2img"&gt;Stable Diffusion Image-to-Image&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5"&gt; stable-diffusion-v1-5/stable-diffusion-v1-5 &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="border-top: 2px solid black"&gt; 
   &lt;td&gt;Text-guided Image Inpainting&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/inpaint"&gt;Stable Diffusion Inpainting&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/runwayml/stable-diffusion-inpainting"&gt; runwayml/stable-diffusion-inpainting &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="border-top: 2px solid black"&gt; 
   &lt;td&gt;Image Variation&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/image_variation"&gt;Stable Diffusion Image Variation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/lambdalabs/sd-image-variations-diffusers"&gt; lambdalabs/sd-image-variations-diffusers &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="border-top: 2px solid black"&gt; 
   &lt;td&gt;Super Resolution&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/upscale"&gt;Stable Diffusion Upscale&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler"&gt; stabilityai/stable-diffusion-x4-upscaler &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Super Resolution&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/latent_upscale"&gt;Stable Diffusion Latent Upscale&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/stabilityai/sd-x2-latent-upscaler"&gt; stabilityai/sd-x2-latent-upscaler &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Popular libraries using üß® Diffusers&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/TaskMatrix"&gt;https://github.com/microsoft/TaskMatrix&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/invoke-ai/InvokeAI"&gt;https://github.com/invoke-ai/InvokeAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/InstantID/InstantID"&gt;https://github.com/InstantID/InstantID&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/apple/ml-stable-diffusion"&gt;https://github.com/apple/ml-stable-diffusion&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Sanster/lama-cleaner"&gt;https://github.com/Sanster/lama-cleaner&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/IDEA-Research/Grounded-Segment-Anything"&gt;https://github.com/IDEA-Research/Grounded-Segment-Anything&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ashawkey/stable-dreamfusion"&gt;https://github.com/ashawkey/stable-dreamfusion&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/deep-floyd/IF"&gt;https://github.com/deep-floyd/IF&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/bentoml/BentoML"&gt;https://github.com/bentoml/BentoML&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/bmaltais/kohya_ss"&gt;https://github.com/bmaltais/kohya_ss&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;+14,000 other amazing GitHub repositories üí™&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Thank you for using us ‚ù§Ô∏è.&lt;/p&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;p&gt;This library concretizes previous work by many different authors and would not have been possible without their great research and implementations. We'd like to thank, in particular, the following implementations which have helped us in our development and without which the API could not have been as polished today:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;@CompVis' latent diffusion models library, available &lt;a href="https://github.com/CompVis/latent-diffusion"&gt;here&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;@hojonathanho original DDPM implementation, available &lt;a href="https://github.com/hojonathanho/diffusion"&gt;here&lt;/a&gt; as well as the extremely useful translation into PyTorch by @pesser, available &lt;a href="https://github.com/pesser/pytorch_diffusion"&gt;here&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;@ermongroup's DDIM implementation, available &lt;a href="https://github.com/ermongroup/ddim"&gt;here&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;@yang-song's Score-VE and Score-VP implementations, available &lt;a href="https://github.com/yang-song/score_sde_pytorch"&gt;here&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We also want to thank @heejkoo for the very helpful overview of papers, code and resources on diffusion models, available &lt;a href="https://github.com/heejkoo/Awesome-Diffusion-Models"&gt;here&lt;/a&gt; as well as @crowsonkb and @rromb for useful discussions and insights.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{von-platen-etal-2022-diffusers,
  author = {Patrick von Platen and Suraj Patil and Anton Lozhkov and Pedro Cuenca and Nathan Lambert and Kashif Rasul and Mishig Davaadorj and Dhruv Nair and Sayak Paul and William Berman and Yiyi Xu and Steven Liu and Thomas Wolf},
  title = {Diffusers: State-of-the-art diffusion models},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/huggingface/diffusers}}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
  </channel>
</rss>