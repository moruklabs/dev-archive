<rss version="2.0">
  <channel>
    <title>GitHub Rust Daily Trending</title>
    <description>Daily Trending of Rust in GitHub</description>
    <pubDate>Fri, 25 Jul 2025 01:35:31 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>ankitects/anki</title>
      <link>https://github.com/ankitects/anki</link>
      <description>&lt;p&gt;Anki is a smart spaced repetition flashcard program&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Anki¬Æ&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://buildkite.com/ankitects/anki-ci"&gt;&lt;img src="https://badge.buildkite.com/c9edf020a4aec976f9835e54751cc5409d843adbb66d043bd3.svg?branch=main" alt="Build status"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This repo contains the source code for the computer version of &lt;a href="https://apps.ankiweb.net"&gt;Anki&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;About&lt;/h1&gt; 
&lt;p&gt;Anki is a spaced repetition program. Please see the &lt;a href="https://apps.ankiweb.net"&gt;website&lt;/a&gt; to learn more.&lt;/p&gt; 
&lt;h1&gt;Getting Started&lt;/h1&gt; 
&lt;h3&gt;Anki Betas&lt;/h3&gt; 
&lt;p&gt;If you'd like to try development builds of Anki but don't feel comfortable building the code, please see &lt;a href="https://betas.ankiweb.net/"&gt;Anki betas&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Developing&lt;/h3&gt; 
&lt;p&gt;For more information on building and developing, please see &lt;a href="https://raw.githubusercontent.com/ankitects/anki/main/docs/development.md"&gt;Development&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Contributing&lt;/h3&gt; 
&lt;p&gt;Want to contribute to Anki? Check out the &lt;a href="https://raw.githubusercontent.com/ankitects/anki/main/docs/contributing.md"&gt;Contribution Guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Anki Contributors&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/ankitects/anki/main/CONTRIBUTORS"&gt;CONTRIBUTORS&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;Anki's license: &lt;a href="https://raw.githubusercontent.com/ankitects/anki/main/LICENSE"&gt;LICENSE&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>oxc-project/oxc</title>
      <link>https://github.com/oxc-project/oxc</link>
      <description>&lt;p&gt;‚öì A collection of JavaScript tools written in Rust.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img alt="OXC Logo" src="https://cdn.jsdelivr.net/gh/oxc-project/oxc-assets/preview-universal.png" width="700"&gt; &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/oxc-project/oxc/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" alt="MIT licensed"&gt;&lt;/a&gt; &lt;a href="https://github.com/oxc-project/oxc/actions/workflows/ci.yml?query=event%3Apush+branch%3Amain"&gt;&lt;img src="https://github.com/oxc-project/oxc/actions/workflows/ci.yml/badge.svg?event=push&amp;amp;branch=main" alt="Build Status"&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/oxc-project/oxc"&gt;&lt;img src="https://codecov.io/gh/oxc-project/oxc/graph/badge.svg?token=FVHEH0BQLJ" alt="Code Coverage"&gt;&lt;/a&gt; &lt;a href="https://codspeed.io/oxc-project/oxc"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://codspeed.io/badge.json" alt="CodSpeed Badge"&gt;&lt;/a&gt; &lt;a href="https://github.com/sponsors/Boshen"&gt;&lt;img src="https://img.shields.io/github/sponsors/Boshen" alt="Sponsors"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://discord.gg/9uXCAwqQZW"&gt;&lt;img src="https://img.shields.io/discord/1079625926024900739?logo=discord&amp;amp;label=Discord" alt="Discord chat"&gt;&lt;/a&gt; &lt;a href="https://playground.oxc.rs/"&gt;&lt;img src="https://img.shields.io/badge/Playground-blue?color=9BE4E0" alt="Playground"&gt;&lt;/a&gt; &lt;a href="https://oxc.rs"&gt;&lt;img src="https://img.shields.io/badge/Website-blue" alt="Website"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;‚öì Oxc&lt;/h2&gt; 
&lt;p&gt;The Oxidation Compiler is a collection of high-performance tools for JavaScript and TypeScript.&lt;/p&gt; 
&lt;p&gt;We are building a parser, linter, formatter, transformer, minifier, resolver ... all written in Rust.&lt;/p&gt; 
&lt;p&gt;For more information, please check out the documentation at &lt;a href="https://oxc.rs"&gt;oxc.rs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;VoidZero Inc.&lt;/h2&gt; 
&lt;p&gt;Oxc is a project of &lt;a href="https://voidzero.dev/"&gt;VoidZero&lt;/a&gt;, see our announcement &lt;a href="https://voidzero.dev/blog"&gt;Announcing VoidZero - Next Generation Toolchain for JavaScript&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you have requirements for JavaScript tools at scale, please &lt;a href="https://forms.gle/WQgjyzYJpwurpxWKA"&gt;get in touch&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;üôãWho's using Oxc?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://rolldown.rs"&gt;Rolldown&lt;/a&gt; uses the &lt;a href="https://docs.rs/oxc"&gt;oxc&lt;/a&gt; crate for parsing and transformation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://trynova.dev"&gt;Nova engine&lt;/a&gt; uses the &lt;a href="https://docs.rs/oxc"&gt;oxc&lt;/a&gt; crate for parsing.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://rolldown.rs"&gt;Rolldown&lt;/a&gt;, &lt;a href="https://github.com/swc-project/swc-node"&gt;swc-node&lt;/a&gt; and &lt;a href="https://github.com/webpro-nl/knip"&gt;knip&lt;/a&gt; use the &lt;a href="https://docs.rs/oxc_resolver"&gt;oxc_resolver&lt;/a&gt; crate for module resolution.&lt;/li&gt; 
 &lt;li&gt;Projects and companies like &lt;a href="https://github.com/preactjs/preact/raw/4c20c23c16dd60f380ce9fe98afc93041a7e1562/oxlint.json"&gt;Preact&lt;/a&gt;, &lt;a href="https://oxc.rs/blog/2023-12-12-announcing-oxlint.html#_50-100-times-faster-than-eslint"&gt;Shopify&lt;/a&gt;, ByteDance and Shopee uses oxlint for linting.&lt;/li&gt; 
 &lt;li&gt;...&lt;a href="https://oxc.rs/docs/guide/projects.html"&gt;and many more&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚úçÔ∏è Contribute&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/oxc-project/oxc/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for guidance.&lt;/p&gt; 
&lt;p&gt;Check out some of the &lt;a href="https://github.com/oxc-project/oxc/contribute"&gt;good first issues&lt;/a&gt; or ask us on &lt;a href="https://discord.gg/9uXCAwqQZW"&gt;Discord&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you are unable to contribute by code, you can still participate by:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Add a &lt;a href="https://github.com/oxc-project/oxc/stargazers"&gt;GitHub Star&lt;/a&gt; to the project.&lt;/li&gt; 
 &lt;li&gt;Join us on &lt;a href="https://discord.gg/9uXCAwqQZW"&gt;Discord&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/boshen_c"&gt;Follow me on twitter&lt;/a&gt; and tweet about this project.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚ö°Ô∏è Linter Quick Start&lt;/h2&gt; 
&lt;p&gt;The linter is ready to catch mistakes for you. It comes with 93 rules turned on by default (out of 430+ in total) and no configuration is required.&lt;/p&gt; 
&lt;p&gt;To get started, run &lt;a href="https://www.npmjs.com/package/oxlint"&gt;oxlint&lt;/a&gt; or via &lt;code&gt;npx&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx oxlint@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To give you an idea of its capabilities, here is an example from the &lt;a href="https://github.com/microsoft/vscode"&gt;vscode&lt;/a&gt; repository, which finishes linting 4800+ files in 0.7 seconds.&lt;/p&gt; 
&lt;p float="left" align="left"&gt; &lt;img src="https://cdn.jsdelivr.net/gh/oxc-project/oxc-assets/linter-screenshot.png" width="60%"&gt; &lt;/p&gt; 
&lt;h2&gt;‚ö°Ô∏è Performance&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;The parser aims to be the fastest Rust-based ready-for-production parser.&lt;/li&gt; 
 &lt;li&gt;The linter is more than 50 times faster than &lt;a href="https://eslint.org/"&gt;ESLint&lt;/a&gt;, and scales with the number of CPU cores.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p float="left" align="middle"&gt; &lt;img src="https://raw.githubusercontent.com/Boshen/bench-javascript-parser-written-in-rust/main/bar-graph.svg?sanitize=true" width="49%"&gt; &lt;img src="https://raw.githubusercontent.com/Boshen/bench-javascript-linter/main/bar-graph.svg?sanitize=true" width="49%"&gt; &lt;/p&gt; 
&lt;h2&gt;‚å®Ô∏è Rust, Node.js and Wasm Usage&lt;/h2&gt; 
&lt;h3&gt;Rust&lt;/h3&gt; 
&lt;p&gt;Individual crates are published, you may use them to build your own JavaScript tools.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The umbrella crate &lt;a href="https://docs.rs/oxc"&gt;oxc&lt;/a&gt; exports all public crates from this repository.&lt;/li&gt; 
 &lt;li&gt;The AST and parser crates &lt;a href="https://docs.rs/oxc_ast"&gt;oxc_ast&lt;/a&gt; and &lt;a href="https://docs.rs/oxc_parser"&gt;oxc_parser&lt;/a&gt; are production ready.&lt;/li&gt; 
 &lt;li&gt;The resolver crate &lt;a href="https://docs.rs/oxc_resolver"&gt;oxc_resolver&lt;/a&gt; for module resolution is also production ready.&lt;/li&gt; 
 &lt;li&gt;Example usages of these crates can be found in their respective &lt;code&gt;crates/*/examples&lt;/code&gt; directory.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;While Rust has gained a reputation for its comparatively slower compilation speed, we have dedicated significant effort to fine-tune the Rust compilation speed. Our aim is to minimize any impact on your development workflow, ensuring that developing your own Oxc based tools remains a smooth and efficient experience.&lt;/p&gt; 
&lt;p&gt;This is demonstrated by our &lt;a href="https://github.com/oxc-project/oxc/actions/workflows/ci.yml?query=branch%3Amain"&gt;CI runs&lt;/a&gt;, where warm runs complete in 3 minutes.&lt;/p&gt; 
&lt;h3&gt;Node.js&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;via napi: &lt;a href="https://www.npmjs.com/package/oxc-parser"&gt;oxc-parser&lt;/a&gt;, &lt;a href="https://www.npmjs.com/package/oxc-transform"&gt;oxc-transform&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Wasm&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.npmjs.com/package/@oxc-parser/wasm"&gt;@oxc-parser/wasm&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;h2&gt;üéØ Tools&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oxc-project/oxc/main/#-ast-and-parser"&gt;AST and Parser&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oxc-project/oxc/main/#-linter"&gt;Linter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oxc-project/oxc/main/#-resolver"&gt;Resolver&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oxc-project/oxc/main/#-minifier"&gt;Minifier&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oxc-project/oxc/main/#-formatter"&gt;Formatter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oxc-project/oxc/main/#-transformer"&gt;Transformer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üî∏ AST and Parser&lt;/h3&gt; 
&lt;p&gt;Oxc maintains its own AST and parser, which is by far the fastest and most conformant JavaScript and TypeScript (including JSX and TSX) parser written in Rust.&lt;/p&gt; 
&lt;p&gt;As the parser often represents a key performance bottleneck in JavaScript tooling, any minor improvements can have a cascading effect on our downstream tools. By developing our parser, we have the opportunity to explore and implement well-researched performance techniques.&lt;/p&gt; 
&lt;p&gt;While many existing JavaScript tools rely on &lt;a href="https://github.com/estree/estree"&gt;estree&lt;/a&gt; as their AST specification, a notable drawback is its abundance of ambiguous nodes. This ambiguity often leads to confusion during development with &lt;a href="https://github.com/estree/estree"&gt;estree&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The Oxc AST differs slightly from the &lt;a href="https://github.com/estree/estree"&gt;estree&lt;/a&gt; AST by removing ambiguous nodes and introducing distinct types. For example, instead of using a generic &lt;a href="https://github.com/estree/estree"&gt;estree&lt;/a&gt; &lt;code&gt;Identifier&lt;/code&gt;, the Oxc AST provides specific types such as &lt;code&gt;BindingIdentifier&lt;/code&gt;, &lt;code&gt;IdentifierReference&lt;/code&gt;, and &lt;code&gt;IdentifierName&lt;/code&gt;. This clear distinction greatly enhances the development experience by aligning more closely with the ECMAScript specification.&lt;/p&gt; 
&lt;h4&gt;üèÜ Parser Performance&lt;/h4&gt; 
&lt;p&gt;Our &lt;a href="https://github.com/Boshen/bench-javascript-parser-written-in-rust"&gt;benchmark&lt;/a&gt; reveals that the Oxc parser surpasses the speed of the &lt;a href="https://swc.rs"&gt;swc&lt;/a&gt; parser by approximately 3 times and the &lt;a href="https://biomejs.dev/"&gt;Biome&lt;/a&gt; parser by 5 times.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How is it so fast?&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;AST is allocated in a memory arena (&lt;a href="https://crates.io/crates/bumpalo"&gt;bumpalo&lt;/a&gt;) for fast AST memory allocation and deallocation.&lt;/li&gt; 
  &lt;li&gt;Short strings are inlined by &lt;a href="https://crates.io/crates/compact_str"&gt;CompactString&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;No other heap allocations are done except the above two.&lt;/li&gt; 
  &lt;li&gt;Scope binding, symbol resolution and some syntax errors are not done in the parser, they are delegated to the semantic analyzer.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;üî∏ Linter&lt;/h3&gt; 
&lt;p&gt;The linter embraces convention over configuration, eliminating the need for extensive configuration and plugin setup. Unlike other linters like &lt;a href="https://eslint.org/"&gt;ESLint&lt;/a&gt;, which often require intricate configurations and plugin installations (e.g. &lt;a href="https://typescript-eslint.io"&gt;@typescript-eslint&lt;/a&gt;), our linter only requires a single command that you can immediately run on your codebase:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx oxlint@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;üèÜ Linter Performance&lt;/h4&gt; 
&lt;p&gt;The linter is 50 - 100 times faster than &lt;a href="https://eslint.org/"&gt;ESLint&lt;/a&gt; depending on the number of rules and number of CPU cores used. It completes in less than a second for most codebases with a few hundred files and completes in a few seconds for larger monorepos. See &lt;a href="https://github.com/Boshen/bench-javascript-linter"&gt;bench-javascript-linter&lt;/a&gt; for details.&lt;/p&gt; 
&lt;p&gt;As an upside, the binary is approximately 5MB, whereas &lt;a href="https://eslint.org/"&gt;ESLint&lt;/a&gt; and its associated plugin dependencies can easily exceed 100.&lt;/p&gt; 
&lt;p&gt;You may also download the linter binary from the &lt;a href="https://github.com/oxc-project/oxc/releases/latest"&gt;latest release tag&lt;/a&gt; as a standalone binary, this lets you run the linter without a Node.js installation in your CI.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How is it so fast?&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Oxc parser is used.&lt;/li&gt; 
  &lt;li&gt;AST visit is a fast operation due to linear memory scan from the memory arena.&lt;/li&gt; 
  &lt;li&gt;Files are linted in a multi-threaded environment, so scales with the total number of CPU cores.&lt;/li&gt; 
  &lt;li&gt;Every single lint rule is tuned for performance.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;üî∏ Resolver&lt;/h3&gt; 
&lt;p&gt;Module resolution plays a crucial role in JavaScript tooling, especially for tasks like multi-file analysis or bundling. However, it can often become a performance bottleneck. To address this, we developed &lt;a href="https://docs.rs/oxc_resolver"&gt;oxc_resolver&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The resolver is production-ready and is currently being used in &lt;a href="https://rolldown.rs"&gt;Rolldown&lt;/a&gt;. Usage and examples can be found in its own &lt;a href="https://github.com/oxc-project/oxc_resolver"&gt;repository&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;üî∏ Transformer&lt;/h3&gt; 
&lt;p&gt;A transformer is responsible for turning higher versions of ECMAScript to a lower version that can be used in older browsers.&lt;/p&gt; 
&lt;p&gt;TypeScript, React, ES6 transforms are complete.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.npmjs.com/package/oxc-transform"&gt;oxc-transform&lt;/a&gt; can be used for experimentation.&lt;/p&gt; 
&lt;h3&gt;üî∏ Isolated Declarations&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://devblogs.microsoft.com/typescript/announcing-typescript-5-5/#isolated-declarations"&gt;TypeScript Isolated Declarations Emit&lt;/a&gt; without using the TypeScript compiler.&lt;/p&gt; 
&lt;p&gt;Our &lt;a href="https://github.com/oxc-project/bench-transformer"&gt;benchmark&lt;/a&gt; indicates that our implementation is at least 20 times faster than the TypeScript compiler.&lt;/p&gt; 
&lt;p&gt;The &lt;a href="https://www.npmjs.com/package/oxc-transform"&gt;npm package&lt;/a&gt; or &lt;a href="https://crates.io/crates/oxc_isolated_declarations"&gt;crate&lt;/a&gt; can be used for this task.&lt;/p&gt; 
&lt;h3&gt;üî∏ Minifier&lt;/h3&gt; 
&lt;p&gt;JavaScript minification plays a crucial role in optimizing website performance as it reduces the amount of data sent to users, resulting in faster page loads. This holds tremendous economic value, particularly for e-commerce websites, where every second can equate to millions of dollars.&lt;/p&gt; 
&lt;p&gt;However, existing minifiers typically require a trade-off between compression quality and speed. You have to choose between the slowest for the best compression or the fastest for less compression. But what if we could develop a faster minifier without compromising on compression?&lt;/p&gt; 
&lt;p&gt;We are actively working on a prototype that aims to achieve this goal, by porting all test cases from well-known minifiers such as &lt;a href="https://github.com/google/closure-compiler"&gt;google-closure-compiler&lt;/a&gt;, &lt;a href="https://terser.org"&gt;terser&lt;/a&gt;, &lt;a href="https://esbuild.github.io/"&gt;esbuild&lt;/a&gt;, and &lt;a href="https://github.com/tdewolff/minify"&gt;tdewolff-minify&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Preliminary results indicate that we are on track to achieve our objectives. With the Oxc minifier, you can expect faster minification times without sacrificing compression quality.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://github.com/privatenumber/minification-benchmarks"&gt;minification benchmarks&lt;/a&gt; for comparisons.&lt;/p&gt; 
&lt;h3&gt;üî∏ Formatter&lt;/h3&gt; 
&lt;p&gt;While &lt;a href="https://prettier.io"&gt;prettier&lt;/a&gt; has established itself as the de facto code formatter for JavaScript, there is a significant demand in the developer community for a less opinionated alternative. Recognizing this need, our ambition is to undertake research and development to create a new JavaScript formatter that offers increased flexibility and customization options.&lt;/p&gt; 
&lt;p&gt;The &lt;a href="https://github.com/oxc-project/oxc/tree/main/crates/oxc_formatter"&gt;prototype&lt;/a&gt; is currently work in progress.&lt;/p&gt; 
&lt;hr&gt; 
&lt;h2&gt;üß™Test Infrastructure&lt;/h2&gt; 
&lt;p&gt;In Oxc, correctness and reliability are taken extremely seriously.&lt;/p&gt; 
&lt;p&gt;We spend half of our time on strengthening the test infrastructure to prevent problems from propagating to downstream tools.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://oxc.rs/docs/learn/architecture/test.html"&gt;Test Infrastructure&lt;/a&gt; documents our test procedures:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Conformance suite on Test262, Babel, TypeScript&lt;/li&gt; 
 &lt;li&gt;Lots of fuzzing&lt;/li&gt; 
 &lt;li&gt;Linter snapshot diagnostics&lt;/li&gt; 
 &lt;li&gt;oxlint ecosystem ci&lt;/li&gt; 
 &lt;li&gt;Idempotency testing&lt;/li&gt; 
 &lt;li&gt;Code coverage&lt;/li&gt; 
 &lt;li&gt;End to end 3000 top npm packages&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;h2&gt;üìö Learning Resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;My small tutorial on &lt;a href="https://oxc.rs/docs/learn/parser_in_rust/intro.html"&gt;how to write a JavaScript Parser in Rust&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;My small article &lt;a href="https://oxc.rs/docs/learn/performance.html"&gt;Pursuit of Performance on Building a JavaScript Compiler&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://oxc.rs/docs/learn/references.html"&gt;And more&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ü§ù Credits&lt;/h2&gt; 
&lt;p&gt;This project was incubated with the assistance of these exceptional mentors and their projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://biomejs.dev/"&gt;Biome&lt;/a&gt; - &lt;a href="https://github.com/ematipico"&gt;@ematipico&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://beta.ruff.rs"&gt;Ruff&lt;/a&gt; - &lt;a href="https://github.com/charliermarsh"&gt;@charliermarsh&lt;/a&gt;, &lt;a href="https://github.com/MichaReiser"&gt;@MichaReiser&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/quick-lint/quick-lint-js"&gt;quick-lint-js&lt;/a&gt; - &lt;a href="https://github.com/strager"&gt;@strager&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://package.elm-lang.org/packages/jfmengels/elm-review/latest"&gt;elm-review&lt;/a&gt; - &lt;a href="https://github.com/jfmengels"&gt;@jfmengels&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Special thanks go to&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/domonji"&gt;@domonji&lt;/a&gt; for bootstrapping this project together, and also completing the TypeScript parser.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tongtong-lu"&gt;@tongtong-lu&lt;/a&gt; and &lt;a href="https://github.com/guan-wy"&gt;@guan-wy&lt;/a&gt; for designing the &lt;a href="https://github.com/oxc-project/oxc-assets"&gt;project logo&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚ù§ Who's &lt;a href="https://github.com/sponsors/Boshen"&gt;Sponsoring Oxc&lt;/a&gt;?&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/sponsors/Boshen"&gt; &lt;img src="https://raw.githubusercontent.com/Boshen/sponsors/main/sponsors.svg?sanitize=true" alt="My sponsors"&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;üìñ License&lt;/h2&gt; 
&lt;p&gt;Oxc is free and open-source software licensed under the &lt;a href="https://raw.githubusercontent.com/oxc-project/oxc/main/LICENSE"&gt;MIT License&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Oxc ports or copies code from other open source projects, their licenses are listed in &lt;a href="https://raw.githubusercontent.com/oxc-project/oxc/main/THIRD-PARTY-LICENSE"&gt;&lt;strong&gt;Third-party library licenses&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Eventual-Inc/Daft</title>
      <link>https://github.com/Eventual-Inc/Daft</link>
      <description>&lt;p&gt;Distributed query engine providing simple and reliable data processing for any modality and scale&lt;/p&gt;&lt;hr&gt;&lt;p&gt;|Banner|&lt;/p&gt; 
&lt;p&gt;|CI| |PyPI| |Latest Tag| |Coverage| |Slack|&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Website &amp;lt;https://www.getdaft.io&amp;gt;&lt;/code&gt;_ ‚Ä¢ &lt;code&gt;Docs &amp;lt;https://docs.getdaft.io&amp;gt;&lt;/code&gt;_ ‚Ä¢ &lt;code&gt;Installation &amp;lt;https://docs.getdaft.io/en/stable/install/&amp;gt;&lt;/code&gt;_ ‚Ä¢ &lt;code&gt;Daft Quickstart &amp;lt;https://docs.getdaft.io/en/stable/quickstart/&amp;gt;&lt;/code&gt;_ ‚Ä¢ &lt;code&gt;Community and Support &amp;lt;https://github.com/Eventual-Inc/Daft/discussions&amp;gt;&lt;/code&gt;_&lt;/p&gt; 
&lt;h1&gt;Daft: Unified Engine for Data Analytics, Engineering &amp;amp; ML/AI&lt;/h1&gt; 
&lt;p&gt;&lt;code&gt;Daft &amp;lt;https://www.getdaft.io&amp;gt;&lt;/code&gt;_ is a distributed query engine for large-scale data processing using Python or SQL, implemented in Rust.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Familiar interactive API:&lt;/strong&gt; Lazy Python Dataframe for rapid and interactive iteration, or SQL for analytical queries&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Focus on the what:&lt;/strong&gt; Powerful Query Optimizer that rewrites queries to be as efficient as possible&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Catalog integrations:&lt;/strong&gt; Full integration with data catalogs such as Apache Iceberg&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Rich multimodal type-system:&lt;/strong&gt; Supports multimodal types such as Images, URLs, Tensors and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Seamless Interchange&lt;/strong&gt;: Built on the &lt;code&gt;Apache Arrow &amp;lt;https://arrow.apache.org/docs/index.html&amp;gt;&lt;/code&gt;_ In-Memory Format&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Built for the cloud:&lt;/strong&gt; &lt;code&gt;Record-setting &amp;lt;https://blog.getdaft.io/p/announcing-daft-02-10x-faster-io&amp;gt;&lt;/code&gt;_ I/O performance for integrations with S3 cloud storage&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;About Daft&lt;/code&gt;_&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Getting Started&lt;/code&gt;_&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Benchmarks&lt;/code&gt;_&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Contributing&lt;/code&gt;_&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Telemetry&lt;/code&gt;_&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Related Projects&lt;/code&gt;_&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;License&lt;/code&gt;_&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;About Daft&lt;/h2&gt; 
&lt;p&gt;Daft was designed with the following principles in mind:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Any Data&lt;/strong&gt;: Beyond the usual strings/numbers/dates, Daft columns can also hold complex or nested multimodal data such as Images, Embeddings and Python objects efficiently with it's Arrow based memory representation. Ingestion and basic transformations of multimodal data is extremely easy and performant in Daft.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interactive Computing&lt;/strong&gt;: Daft is built for the interactive developer experience through notebooks or REPLs - intelligent caching/query optimizations accelerates your experimentation and data exploration.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Distributed Computing&lt;/strong&gt;: Some workloads can quickly outgrow your local laptop's computational resources - Daft integrates natively with &lt;code&gt;Ray &amp;lt;https://www.ray.io&amp;gt;&lt;/code&gt;_ for running dataframes on large clusters of machines with thousands of CPUs/GPUs.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Installation ^^^^^^^^^^^^&lt;/p&gt; 
&lt;p&gt;Install Daft with &lt;code&gt;pip install daft&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For more advanced installations (e.g. installing from source or with extra dependencies such as Ray and AWS utilities), please see our &lt;code&gt;Installation Guide &amp;lt;https://docs.getdaft.io/en/stable/install/&amp;gt;&lt;/code&gt;_&lt;/p&gt; 
&lt;p&gt;Quickstart ^^^^^^^^^^&lt;/p&gt; 
&lt;p&gt;Check out our &lt;code&gt;quickstart &amp;lt;https://docs.getdaft.io/en/stable/quickstart/&amp;gt;&lt;/code&gt;_!&lt;/p&gt; 
&lt;p&gt;In this example, we load images from an AWS S3 bucket's URLs and resize each image in the dataframe:&lt;/p&gt; 
&lt;p&gt;.. code:: python&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;import daft

# Load a dataframe from filepaths in an S3 bucket
df = daft.from_glob_path("s3://daft-public-data/laion-sample-images/*")

# 1. Download column of image URLs as a column of bytes
# 2. Decode the column of bytes into a column of images
df = df.with_column("image", df["path"].url.download().image.decode())

# Resize each image into 32x32
df = df.with_column("resized", df["image"].image.resize(32, 32))

df.show(3)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;|Quickstart Image|&lt;/p&gt; 
&lt;h2&gt;Benchmarks&lt;/h2&gt; 
&lt;p&gt;|Benchmark Image|&lt;/p&gt; 
&lt;p&gt;To see the full benchmarks, detailed setup, and logs, check out our &lt;code&gt;benchmarking page. &amp;lt;https://docs.getdaft.io/en/stable/resources/benchmarks/tpch/&amp;gt;&lt;/code&gt;_&lt;/p&gt; 
&lt;p&gt;More Resources ^^^^^^^^^^^^^^&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;Daft Quickstart &amp;lt;https://docs.getdaft.io/en/stable/quickstart/&amp;gt;&lt;/code&gt;_ - learn more about Daft's full range of capabilities including dataloading from URLs, joins, user-defined functions (UDF), groupby, aggregations and more.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;User Guide &amp;lt;https://docs.getdaft.io/en/stable/&amp;gt;&lt;/code&gt;_ - take a deep-dive into each topic within Daft&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;API Reference &amp;lt;https://docs.getdaft.io/en/stable/api/&amp;gt;&lt;/code&gt;_ - API reference for public classes/functions of Daft&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;SQL Reference &amp;lt;https://docs.getdaft.io/en/stable/sql/&amp;gt;&lt;/code&gt;_ - Daft SQL reference&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We &amp;lt;3 developers! To start contributing to Daft, please read &lt;code&gt;CONTRIBUTING.md &amp;lt;https://github.com/Eventual-Inc/Daft/blob/main/CONTRIBUTING.md&amp;gt;&lt;/code&gt;_ This document describes the development lifecycle and toolchain for working on Daft. It also details how to add new functionality to the core engine and expose it through a Python API.&lt;/p&gt; 
&lt;p&gt;Here's a list of &lt;code&gt;good first issues &amp;lt;https://github.com/Eventual-Inc/Daft/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22&amp;gt;&lt;/code&gt;_ to get yourself warmed up with Daft. Comment in the issue to pick it up, and feel free to ask any questions!&lt;/p&gt; 
&lt;h2&gt;Telemetry&lt;/h2&gt; 
&lt;p&gt;To help improve Daft, we collect non-identifiable data via Scarf (&lt;a href="https://scarf.sh"&gt;https://scarf.sh&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;To disable this behavior, set the environment variable &lt;code&gt;DO_NOT_TRACK=true&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;The data that we collect is:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Non-identifiable:&lt;/strong&gt; Events are keyed by a session ID which is generated on import of Daft&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Metadata-only:&lt;/strong&gt; We do not collect any of our users‚Äô proprietary code or data&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;For development only:&lt;/strong&gt; We do not buy or sell any user data&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Please see our &lt;code&gt;documentation &amp;lt;https://docs.getdaft.io/en/stable/resources/telemetry/&amp;gt;&lt;/code&gt;_ for more details.&lt;/p&gt; 
&lt;p&gt;.. image:: &lt;a href="https://static.scarf.sh/a.png?x-pxid=31f8d5ba-7e09-4d75-8895-5252bbf06cf6"&gt;https://static.scarf.sh/a.png?x-pxid=31f8d5ba-7e09-4d75-8895-5252bbf06cf6&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Related Projects&lt;/h2&gt; 
&lt;p&gt;+---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+ | Engine | Query Optimizer | Multimodal | Distributed | Arrow Backed | Vectorized Execution Engine | Out-of-core | +===================================================+=================+===============+=============+=================+=============================+=============+ | Daft | Yes | Yes | Yes | Yes | Yes | Yes | +---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+ | &lt;code&gt;Pandas &amp;lt;https://github.com/pandas-dev/pandas&amp;gt;&lt;/code&gt;_ | No | Python object | No | optional &amp;gt;= 2.0 | Some(Numpy) | No | +---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+ | &lt;code&gt;Polars &amp;lt;https://github.com/pola-rs/polars&amp;gt;&lt;/code&gt;_ | Yes | Python object | No | Yes | Yes | Yes | +---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+ | &lt;code&gt;Modin &amp;lt;https://github.com/modin-project/modin&amp;gt;&lt;/code&gt;_ | Eagar | Python object | Yes | No | Some(Pandas) | Yes | +---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+ | &lt;code&gt;Pyspark &amp;lt;https://github.com/apache/spark&amp;gt;&lt;/code&gt;_ | Yes | No | Yes | Pandas UDF/IO | Pandas UDF | Yes | +---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+ | &lt;code&gt;Dask DF &amp;lt;https://github.com/dask/dask&amp;gt;&lt;/code&gt;_ | No | Python object | Yes | No | Some(Pandas) | Yes | +---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+&lt;/p&gt; 
&lt;p&gt;Check out our &lt;code&gt;engine comparison page &amp;lt;https://docs.getdaft.io/en/stable/resources/engine_comparison/&amp;gt;&lt;/code&gt;_ for more details!&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Daft has an Apache 2.0 license - please see the LICENSE file.&lt;/p&gt; 
&lt;p&gt;.. |Quickstart Image| image:: &lt;a href="https://github.com/Eventual-Inc/Daft/assets/17691182/dea2f515-9739-4f3e-ac58-cd96d51e44a8"&gt;https://github.com/Eventual-Inc/Daft/assets/17691182/dea2f515-9739-4f3e-ac58-cd96d51e44a8&lt;/a&gt; :alt: Dataframe code to load a folder of images from AWS S3 and create thumbnails :height: 256&lt;/p&gt; 
&lt;p&gt;.. |Benchmark Image| image:: &lt;a href="https://github-production-user-asset-6210df.s3.amazonaws.com/2550285/243524430-338e427d-f049-40b3-b555-4059d6be7bfd.png"&gt;https://github-production-user-asset-6210df.s3.amazonaws.com/2550285/243524430-338e427d-f049-40b3-b555-4059d6be7bfd.png&lt;/a&gt; :alt: Benchmarks for SF100 TPCH&lt;/p&gt; 
&lt;p&gt;.. |Banner| image:: &lt;a href="https://daft.ai/images/diagram.png"&gt;https://daft.ai/images/diagram.png&lt;/a&gt; :target: &lt;a href="https://www.daft.ai"&gt;https://www.daft.ai&lt;/a&gt; :alt: Daft dataframes can load any data such as PDF documents, images, protobufs, csv, parquet and audio files into a table dataframe structure for easy querying&lt;/p&gt; 
&lt;p&gt;.. |CI| image:: &lt;a href="https://github.com/Eventual-Inc/Daft/actions/workflows/pr-test-suite.yml/badge.svg"&gt;https://github.com/Eventual-Inc/Daft/actions/workflows/pr-test-suite.yml/badge.svg&lt;/a&gt; :target: &lt;a href="https://github.com/Eventual-Inc/Daft/actions/workflows/pr-test-suite.yml?query=branch:main"&gt;https://github.com/Eventual-Inc/Daft/actions/workflows/pr-test-suite.yml?query=branch:main&lt;/a&gt; :alt: Github Actions tests&lt;/p&gt; 
&lt;p&gt;.. |PyPI| image:: &lt;a href="https://img.shields.io/pypi/v/daft.svg?label=pip&amp;amp;logo=PyPI&amp;amp;logoColor=white"&gt;https://img.shields.io/pypi/v/daft.svg?label=pip&amp;amp;logo=PyPI&amp;amp;logoColor=white&lt;/a&gt; :target: &lt;a href="https://pypi.org/project/daft"&gt;https://pypi.org/project/daft&lt;/a&gt; :alt: PyPI&lt;/p&gt; 
&lt;p&gt;.. |Latest Tag| image:: &lt;a href="https://img.shields.io/github/v/tag/Eventual-Inc/Daft?label=latest&amp;amp;logo=GitHub"&gt;https://img.shields.io/github/v/tag/Eventual-Inc/Daft?label=latest&amp;amp;logo=GitHub&lt;/a&gt; :target: &lt;a href="https://github.com/Eventual-Inc/Daft/tags"&gt;https://github.com/Eventual-Inc/Daft/tags&lt;/a&gt; :alt: latest tag&lt;/p&gt; 
&lt;p&gt;.. |Coverage| image:: &lt;a href="https://codecov.io/gh/Eventual-Inc/Daft/branch/main/graph/badge.svg?token=J430QVFE89"&gt;https://codecov.io/gh/Eventual-Inc/Daft/branch/main/graph/badge.svg?token=J430QVFE89&lt;/a&gt; :target: &lt;a href="https://codecov.io/gh/Eventual-Inc/Daft"&gt;https://codecov.io/gh/Eventual-Inc/Daft&lt;/a&gt; :alt: Coverage&lt;/p&gt; 
&lt;p&gt;.. |Slack| image:: &lt;a href="https://img.shields.io/badge/slack-@distdata-purple.svg?logo=slack"&gt;https://img.shields.io/badge/slack-@distdata-purple.svg?logo=slack&lt;/a&gt; :target: &lt;a href="https://join.slack.com/t/dist-data/shared_invite/zt-2e77olvxw-uyZcPPV1SRchhi8ah6ZCtg"&gt;https://join.slack.com/t/dist-data/shared_invite/zt-2e77olvxw-uyZcPPV1SRchhi8ah6ZCtg&lt;/a&gt; :alt: slack community&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>jj-vcs/jj</title>
      <link>https://github.com/jj-vcs/jj</link>
      <description>&lt;p&gt;A Git-compatible VCS that is both simple and powerful&lt;/p&gt;&lt;hr&gt;&lt;div class="title-block" style="text-align: center;" align="center"&gt; 
 &lt;h1&gt;Jujutsu‚Äîa version control system&lt;/h1&gt; 
 &lt;p&gt;&lt;img title="jj logo" src="https://raw.githubusercontent.com/jj-vcs/jj/main/docs/images/jj-logo.svg?sanitize=true" width="320" height="320"&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/jj-vcs/jj/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/martinvonz/jj" alt="Release"&gt;&lt;/a&gt; &lt;a href="https://github.com/jj-vcs/jj/releases"&gt;&lt;img src="https://img.shields.io/github/release-date/martinvonz/jj" alt="Release date"&gt;&lt;/a&gt; &lt;br&gt; &lt;a href="https://github.com/jj-vcs/jj/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/martinvonz/jj" alt="License"&gt;&lt;/a&gt; &lt;a href="https://discord.gg/dkmfj3aGQN"&gt;&lt;img src="https://img.shields.io/discord/968932220549103686.svg?label=&amp;amp;logo=discord&amp;amp;logoColor=ffffff&amp;amp;color=7389D8&amp;amp;labelColor=6A7EC2" alt="Discord"&gt;&lt;/a&gt; &lt;a href="https://web.libera.chat/?channel=#jujutsu"&gt;&lt;img src="https://img.shields.io/badge/irc-%23jujutsu-blue.svg?sanitize=true" alt="IRC"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://jj-vcs.github.io/jj"&gt;Homepage&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚Ä¢&amp;nbsp;&amp;nbsp;&lt;/strong&gt; &lt;strong&gt;&lt;a href="https://jj-vcs.github.io/jj/latest/install-and-setup"&gt;Installation&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚Ä¢&amp;nbsp;&amp;nbsp;&lt;/strong&gt; &lt;strong&gt;&lt;a href="https://jj-vcs.github.io/jj/latest/tutorial"&gt;Getting Started&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚Ä¢&amp;nbsp;&amp;nbsp;&lt;/strong&gt; &lt;strong&gt;&lt;a href="https://jj-vcs.github.io/jj/latest/roadmap"&gt;Development Roadmap&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚Ä¢&amp;nbsp;&amp;nbsp;&lt;/strong&gt; &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/jj-vcs/jj/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Jujutsu is a powerful &lt;a href="https://en.wikipedia.org/wiki/Version_control"&gt;version control system&lt;/a&gt; for software projects. You use it to get a copy of your code, track changes to the code, and finally publish those changes for others to see and use. It is designed from the ground up to be easy to use‚Äîwhether you're new or experienced, working on brand new projects alone, or large scale software projects with large histories and teams.&lt;/p&gt; 
&lt;p&gt;Jujutsu is unlike most other systems, because internally it abstracts the user interface and version control algorithms from the &lt;em&gt;storage systems&lt;/em&gt; used to serve your content. This allows it to serve as a VCS with many possible physical backends, that may have their own data or networking models‚Äîlike &lt;a href="https://www.mercurial-scm.org/"&gt;Mercurial&lt;/a&gt; or &lt;a href="https://www.breezy-vcs.org/"&gt;Breezy&lt;/a&gt;, or hybrid systems like Google's cloud-based design, &lt;a href="https://youtu.be/W71BTkUbdqE?t=645"&gt;Piper/CitC&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Today, we use Git repositories as a storage layer to serve and track content, making it &lt;strong&gt;compatible with many of your favorite Git-based tools, right now!&lt;/strong&gt; All core developers use Jujutsu to develop Jujutsu, right here on GitHub. But it should hopefully work with your favorite Git forges, too.&lt;/p&gt; 
&lt;p&gt;We combine many distinct design choices and concepts from other version control systems into a single tool. Some of those sources of inspiration include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Git&lt;/strong&gt;: We make an effort to &lt;a href="https://github.com/jj-vcs/jj/discussions/49"&gt;be fast&lt;/a&gt;‚Äîwith a snappy UX, efficient algorithms, correct data structures, and good-old-fashioned attention to detail. The default storage backend uses Git repositories for "physical storage", for wide interoperability and ease of onboarding.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Mercurial &amp;amp; Sapling&lt;/strong&gt;: There are many Mercurial-inspired features, such as the &lt;a href="https://jj-vcs.github.io/jj/latest/revsets/"&gt;revset&lt;/a&gt; language to select commits. There is &lt;a href="https://jj-vcs.github.io/jj/latest/git-comparison/#the-index"&gt;no explicit index&lt;/a&gt; or staging area. Branches are "anonymous" like Mercurial, so you don't need to make up a name for each small change. Primitives for rewriting history are powerful and simple. Formatting output is done with a robust template language that can be configured by the user.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Darcs&lt;/strong&gt;: Jujutsu keeps track of conflicts as &lt;a href="https://jj-vcs.github.io/jj/latest/conflicts/"&gt;first-class objects&lt;/a&gt; in its model; they are first-class in the same way commits are, while alternatives like Git simply think of conflicts as textual diffs. While not as rigorous as systems like Darcs (which is based on a formalized theory of patches, as opposed to snapshots), the effect is that many forms of conflict resolution can be performed and propagated automatically.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;And it adds several innovative, useful features of its own:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Working-copy-as-a-commit&lt;/strong&gt;: Changes to files are &lt;a href="https://jj-vcs.github.io/jj/latest/working-copy/"&gt;recorded automatically&lt;/a&gt; as normal commits, and amended on every subsequent change. This "snapshot" design simplifies the user-facing data model (commits are the only visible object), simplifies internal algorithms, and completely subsumes features like Git's stashes or the index/staging-area.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Operation log &amp;amp; undo&lt;/strong&gt;: Jujutsu records every operation that is performed on the repository, from commits, to pulls, to pushes. This makes debugging problems like "what just happened?" or "how did I end up here?" easier, &lt;em&gt;especially&lt;/em&gt; when you're helping your coworker answer those questions about their repository! And because everything is recorded, you can undo that mistake you just made with ease. Version control has finally entered &lt;a href="https://en.wikipedia.org/wiki/Undo#History"&gt;the 1960s&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Automatic rebase and conflict resolution&lt;/strong&gt;: When you modify a commit, every descendent is automatically rebased on top of the freshly-modified one. This makes "patch-based" workflows a breeze. If you resolve a conflict in a commit, the &lt;em&gt;resolution&lt;/em&gt; of that conflict is also propagated through descendants as well. In effect, this is a completely transparent version of &lt;code&gt;git rebase --update-refs&lt;/code&gt; combined with &lt;code&gt;git rerere&lt;/code&gt;, supported by design.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] The following features are available for use, but experimental; they may have bugs, backwards incompatible storage changes, and user-interface changes!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Safe, concurrent replication&lt;/strong&gt;: Have you ever wanted to store your version controlled repositories inside a Dropbox folder? Or continuously backup repositories to S3? No? Well, now you can!&lt;/p&gt; &lt;p&gt;The fundamental problem with using filesystems like Dropbox and backup tools like &lt;code&gt;rsync&lt;/code&gt; on your typical Git/Mercurial repositories is that they rely on &lt;em&gt;local filesystem operations&lt;/em&gt; being atomic, serialized, and non-concurrent with respect to other reads and writes‚Äîwhich is &lt;em&gt;not&lt;/em&gt; true when operating on distributed file systems, or when operations like concurrent file copies (for backup) happen while lock files are being held.&lt;/p&gt; &lt;p&gt;Jujutsu is instead designed to be &lt;a href="https://jj-vcs.github.io/jj/latest/technical/concurrency/"&gt;safe under concurrent scenarios&lt;/a&gt;; simply using rsync or Dropbox and then using that resulting repository should never result in a repository in a &lt;em&gt;corrupt state&lt;/em&gt;. The worst that &lt;em&gt;should&lt;/em&gt; happen is that it will expose conflicts between the local and remote state, leaving you to resolve them.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The command-line tool is called &lt;code&gt;jj&lt;/code&gt; for now because it's easy to type and easy to replace (rare in English). The project is called "Jujutsu" because it matches "jj".&lt;/p&gt; 
&lt;p&gt;Jujutsu is relatively young, with lots of work to still be done. If you have any questions, or want to talk about future plans, please join us on Discord &lt;a href="https://discord.gg/dkmfj3aGQN"&gt;&lt;img src="https://img.shields.io/discord/968932220549103686.svg?label=&amp;amp;logo=discord&amp;amp;logoColor=ffffff&amp;amp;color=7389D8&amp;amp;labelColor=6A7EC2" alt="Discord"&gt;&lt;/a&gt;, start a &lt;a href="https://github.com/jj-vcs/jj/discussions"&gt;GitHub Discussion&lt;/a&gt;, or send an IRC message to &lt;a href="https://web.libera.chat/?channel=#jujutsu"&gt;&lt;code&gt;#jujutsu&lt;/code&gt; on Libera Chat&lt;/a&gt;. The developers monitor all of these channels[^bridge].&lt;/p&gt; 
&lt;p&gt;[^bridge]: To be more precise, the &lt;code&gt;#jujutsu&lt;/code&gt; Libera IRC channel is bridged to one of the channels on jj's Discord. Some of the developers stay on Discord and use the bridge to follow IRC.&lt;/p&gt; 
&lt;h3&gt;News and Updates üì£&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;December 2024&lt;/strong&gt;: The &lt;code&gt;jj&lt;/code&gt; Repository has moved to the &lt;code&gt;jj-vcs&lt;/code&gt; GitHub organisation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;November 2024&lt;/strong&gt;: Version 0.24 is released which adds &lt;code&gt;jj file annotate&lt;/code&gt;, which is equivalent to &lt;code&gt;git blame&lt;/code&gt; or &lt;code&gt;hg annotate&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;September 2024&lt;/strong&gt;: Martin gave a &lt;a href="https://www.youtube.com/watch?v=LV0JzI8IcCY"&gt;presentation about Jujutsu&lt;/a&gt; at Git Merge 2024.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Feb 2024&lt;/strong&gt;: Version 0.14 is released, which deprecates &lt;a href="https://raw.githubusercontent.com/jj-vcs/jj/main/CHANGELOG.md#0140---2024-02-07"&gt;"jj checkout" and "jj merge"&lt;/a&gt;, as well as &lt;code&gt;jj init --git&lt;/code&gt;, which is now just called &lt;code&gt;jj git init&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Oct 2023&lt;/strong&gt;: Version 0.10.0 is released! Now includes a bundled merge and diff editor for all platforms, "immutable revsets" to avoid accidentally &lt;code&gt;edit&lt;/code&gt;-ing the wrong revisions, and lots of polish.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Jan 2023&lt;/strong&gt;: Martin gave a presentation about Google's plans for Jujutsu at Git Merge 2022! See the &lt;a href="https://docs.google.com/presentation/d/1F8j9_UOOSGUN9MvHxPZX_L4bQ9NMcYOp1isn17kTC_M/view"&gt;slides&lt;/a&gt; or the &lt;a href="https://www.youtube.com/watch?v=bx_LGilOuE4"&gt;recording&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Related Media&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Mar 2024&lt;/strong&gt;: Chris Krycho started &lt;a href="https://www.youtube.com/playlist?list=PLelyiwKWHHAq01Pvmpf6x7J0y-yQpmtxp"&gt;a YouTube series about Jujutsu&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Feb 2024&lt;/strong&gt;: Chris Krycho published an article about Jujutsu called &lt;a href="https://v5.chriskrycho.com/essays/jj-init/"&gt;jj init&lt;/a&gt; and Steve Klabnik followed up with the &lt;a href="https://steveklabnik.github.io/jujutsu-tutorial/"&gt;Jujutsu Tutorial&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Jan 2024&lt;/strong&gt;: Jujutsu was featured in an LWN.net article called &lt;a href="https://lwn.net/Articles/958468/"&gt;Jujutsu: a new, Git-compatible version control system&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Jan 2023&lt;/strong&gt;: Martin's Talk about Jujutsu at Git Merge 2022, &lt;a href="https://www.youtube.com/watch?v=bx_LGilOuE4"&gt;video&lt;/a&gt; and the associated &lt;a href="https://docs.google.com/presentation/d/1F8j9_UOOSGUN9MvHxPZX_L4bQ9NMcYOp1isn17kTC_M/view"&gt;slides&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The wiki also contains a more extensive list of &lt;a href="https://github.com/jj-vcs/jj/wiki/Media"&gt;media references&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Jujutsu is an &lt;strong&gt;experimental version control system&lt;/strong&gt;. While Git compatibility is stable, and most developers use it daily for all their needs, there may still be work-in-progress features, suboptimal UX, and workflow gaps that make it unusable for your particular use.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Follow the &lt;a href="https://jj-vcs.github.io/jj/latest/install-and-setup"&gt;installation instructions&lt;/a&gt; to obtain and configure &lt;code&gt;jj&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;The best way to get started is probably to go through &lt;a href="https://jj-vcs.github.io/jj/latest/tutorial"&gt;the tutorial&lt;/a&gt;. Also see the &lt;a href="https://jj-vcs.github.io/jj/latest/git-comparison"&gt;Git comparison&lt;/a&gt;, which includes a table of &lt;code&gt;jj&lt;/code&gt; vs. &lt;code&gt;git&lt;/code&gt; commands.&lt;/p&gt; 
&lt;p&gt;As you become more familiar with Jujutsu, the following resources may be helpful:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The &lt;a href="https://jj-vcs.github.io/jj/latest/FAQ"&gt;FAQ&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;The &lt;a href="https://jj-vcs.github.io/jj/latest/glossary"&gt;Glossary&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;The &lt;code&gt;jj help&lt;/code&gt; command (e.g. &lt;code&gt;jj help rebase&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;The &lt;code&gt;jj help -k &amp;lt;keyword&amp;gt;&lt;/code&gt; command (e.g. &lt;code&gt;jj help -k config&lt;/code&gt;). Use &lt;code&gt;jj help --help&lt;/code&gt; to see what keywords are available.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you are using a &lt;strong&gt;prerelease&lt;/strong&gt; version of &lt;code&gt;jj&lt;/code&gt;, you would want to consult &lt;a href="https://jj-vcs.github.io/jj/prerelease/"&gt;the docs for the prerelease (main branch) version&lt;/a&gt;. You can also get there from the docs for the latest release by using the website's version switcher. The version switcher is visible in the header of the website when you scroll to the top of any page.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;h3&gt;Compatible with Git&lt;/h3&gt; 
&lt;p&gt;Jujutsu is designed so that the underlying data and storage model is abstract. Today, only the Git backend is production-ready. The Git backend uses the &lt;a href="https://github.com/Byron/gitoxide"&gt;gitoxide&lt;/a&gt; Rust library.&lt;/p&gt; 
&lt;p&gt;The Git backend is fully featured and maintained, and allows you to use Jujutsu with any Git remote. The commits you create will look like regular Git commits. You can fetch branches from a regular Git remote and push branches to the remote. You can always switch back to Git.&lt;/p&gt; 
&lt;p&gt;Here is how you can explore a GitHub repository with &lt;code&gt;jj&lt;/code&gt;.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/jj-vcs/jj/main/demos/git_compat.png"&gt; 
&lt;p&gt;You can even have a &lt;a href="https://jj-vcs.github.io/jj/latest/git-compatibility#co-located-jujutsugit-repos"&gt;"co-located" local repository&lt;/a&gt; where you can use both &lt;code&gt;jj&lt;/code&gt; and &lt;code&gt;git&lt;/code&gt; commands interchangeably.&lt;/p&gt; 
&lt;h3&gt;The working copy is automatically committed&lt;/h3&gt; 
&lt;p&gt;Jujutsu uses a real commit to represent the working copy. Checking out a commit results a new working-copy commit on top of the target commit. Almost all commands automatically amend the working-copy commit.&lt;/p&gt; 
&lt;p&gt;The working-copy being a commit means that commands never fail because the working copy is dirty (no "error: Your local changes to the following files..."), and there is no need for &lt;code&gt;git stash&lt;/code&gt;. Also, because the working copy is a commit, commands work the same way on the working-copy commit as on any other commit, so you can set the commit message before you're done with the changes.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/jj-vcs/jj/main/demos/working_copy.png"&gt; 
&lt;h3&gt;The repo is the source of truth&lt;/h3&gt; 
&lt;p&gt;With Jujutsu, the working copy plays a smaller role than with Git. Commands snapshot the working copy before they start, then they update the repo, and then the working copy is updated (if the working-copy commit was modified). Almost all commands (even checkout!) operate on the commits in the repo, leaving the common functionality of snapshotting and updating of the working copy to centralized code. For example, &lt;code&gt;jj restore&lt;/code&gt; (similar to &lt;code&gt;git restore&lt;/code&gt;) can restore from any commit and into any commit, and &lt;code&gt;jj describe&lt;/code&gt; can set the commit message of any commit (defaults to the working-copy commit).&lt;/p&gt; 
&lt;h3&gt;Entire repo is under version control&lt;/h3&gt; 
&lt;p&gt;All operations you perform in the repo are recorded, along with a snapshot of the repo state after the operation. This means that you can easily revert to an earlier repo state, or to simply undo a particular operation (which does not necessarily have to be the most recent operation).&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/jj-vcs/jj/main/demos/operation_log.png"&gt; 
&lt;h3&gt;Conflicts can be recorded in commits&lt;/h3&gt; 
&lt;p&gt;If an operation results in &lt;a href="https://jj-vcs.github.io/jj/latest/glossary#conflict"&gt;conflicts&lt;/a&gt;, information about those conflicts will be recorded in the commit(s). The operation will succeed. You can then resolve the conflicts later. One consequence of this design is that there's no need to continue interrupted operations. Instead, you get a single workflow for resolving conflicts, regardless of which command caused them. This design also lets Jujutsu rebase merge commits correctly (unlike both Git and Mercurial).&lt;/p&gt; 
&lt;p&gt;Basic conflict resolution:&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/jj-vcs/jj/main/demos/resolve_conflicts.png"&gt; 
&lt;p&gt;Juggling conflicts:&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/jj-vcs/jj/main/demos/juggle_conflicts.png"&gt; 
&lt;h3&gt;Automatic rebase&lt;/h3&gt; 
&lt;p&gt;Whenever you modify a commit, any descendants of the old commit will be rebased onto the new commit. Thanks to the conflict design described above, that can be done even if there are conflicts. Bookmarks pointing to rebased commits will be updated. So will the working copy if it points to a rebased commit.&lt;/p&gt; 
&lt;h3&gt;Comprehensive support for rewriting history&lt;/h3&gt; 
&lt;p&gt;Besides the usual rebase command, there's &lt;code&gt;jj describe&lt;/code&gt; for editing the description (commit message) of an arbitrary commit. There's also &lt;code&gt;jj diffedit&lt;/code&gt;, which lets you edit the changes in a commit without checking it out. To split a commit into two, use &lt;code&gt;jj split&lt;/code&gt;. You can even move part of the changes in a commit to any other commit using &lt;code&gt;jj squash -i --from X --into Y&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Status&lt;/h2&gt; 
&lt;p&gt;The tool is fairly feature-complete, but some important features like support for Git submodules are not yet completed. There are also several performance bugs. It's likely that workflows and setups different from what the core developers use are not well supported, e.g. there is no native support for email-based workflows.&lt;/p&gt; 
&lt;p&gt;Today, all core developers use &lt;code&gt;jj&lt;/code&gt; to work on &lt;code&gt;jj&lt;/code&gt;. I (Martin von Zweigbergk) have almost exclusively used &lt;code&gt;jj&lt;/code&gt; to develop the project itself since early January 2021. I haven't had to re-clone from source (I don't think I've even had to restore from backup).&lt;/p&gt; 
&lt;p&gt;There &lt;em&gt;will&lt;/em&gt; be changes to workflows and backward-incompatible changes to the on-disk formats before version 1.0.0. For any format changes, we'll try to implement transparent upgrades (as we've done with recent changes), or provide upgrade commands or scripts if requested.&lt;/p&gt; 
&lt;h2&gt;Related work&lt;/h2&gt; 
&lt;p&gt;There are several tools trying to solve similar problems as Jujutsu. See &lt;a href="https://jj-vcs.github.io/jj/latest/related-work"&gt;related work&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome outside contributions, and there's plenty of things to do, so don't be shy. Please ask if you want a pointer on something you can help with, and hopefully we can all figure something out.&lt;/p&gt; 
&lt;p&gt;We do have &lt;a href="https://jj-vcs.github.io/jj/prerelease/contributing/"&gt;a few policies and suggestions&lt;/a&gt; for contributors. The broad TL;DR:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Bug reports are very welcome!&lt;/li&gt; 
 &lt;li&gt;Every commit that lands in the &lt;code&gt;main&lt;/code&gt; branch is code reviewed.&lt;/li&gt; 
 &lt;li&gt;Please behave yourself, and obey the Community Guidelines.&lt;/li&gt; 
 &lt;li&gt;There &lt;strong&gt;is&lt;/strong&gt; a mandatory CLA you must agree to. Importantly, it &lt;strong&gt;does not&lt;/strong&gt; transfer copyright ownership to Google or anyone else; it simply gives us the right to safely redistribute and use your changes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Mandatory Google Disclaimer&lt;/h3&gt; 
&lt;p&gt;I (Martin von Zweigbergk, &lt;a href="mailto:martinvonz@google.com"&gt;martinvonz@google.com&lt;/a&gt;) started Jujutsu as a hobby project in late 2019, and it has evolved into my full-time project at Google, with several other Googlers (now) assisting development in various capacities. That said, &lt;strong&gt;this is not a Google product&lt;/strong&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Jujutsu is available as Open Source Software, under the Apache 2.0 license. See &lt;a href="https://raw.githubusercontent.com/jj-vcs/jj/main/LICENSE"&gt;&lt;code&gt;LICENSE&lt;/code&gt;&lt;/a&gt; for details about copyright and redistribution.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;jj&lt;/code&gt; logo was contributed by J. Jennings and is licensed under a Creative Commons License, see &lt;a href="https://raw.githubusercontent.com/jj-vcs/jj/main/docs/images/LICENSE"&gt;&lt;code&gt;docs/images/LICENSE&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>juspay/hyperswitch</title>
      <link>https://github.com/juspay/hyperswitch</link>
      <description>&lt;p&gt;An open source payments switch written in Rust to make payments fast, reliable and affordable&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/juspay/hyperswitch/main/docs/imgs/hyperswitch-logo-dark.svg#gh-dark-mode-only" alt="Hyperswitch-Logo" width="40%"&gt; &lt;img src="https://raw.githubusercontent.com/juspay/hyperswitch/main/docs/imgs/hyperswitch-logo-light.svg#gh-light-mode-only" alt="Hyperswitch-Logo" width="40%"&gt; &lt;/p&gt; 
&lt;h1 align="center"&gt;Composable Open-Source Payments Infrastructure&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/juspay/hyperswitch/main/docs/gifs/quickstart.gif" alt="Quickstart demo"&gt; &lt;/p&gt; 
&lt;!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} --&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/juspay/hyperswitch/actions?query=workflow%3ACI+branch%3Amain"&gt; &lt;img src="https://github.com/juspay/hyperswitch/workflows/CI-push/badge.svg?sanitize=true"&gt; &lt;/a&gt; &lt;a href="https://github.com/juspay/hyperswitch/raw/main/LICENSE"&gt; &lt;img src="https://img.shields.io/github/license/juspay/hyperswitch"&gt; &lt;/a&gt; &lt;a href="https://github.com/juspay/hyperswitch/raw/main/LICENSE"&gt; &lt;img src="https://img.shields.io/badge/Made_in-Rust-orange"&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.linkedin.com/company/hyperswitch/"&gt; &lt;img src="https://img.shields.io/badge/follow-hyperswitch-blue?logo=linkedin&amp;amp;labelColor=grey"&gt; &lt;/a&gt; &lt;a href="https://x.com/hyperswitchio"&gt; &lt;img src="https://img.shields.io/badge/follow-%40hyperswitchio-white?logo=x&amp;amp;labelColor=grey"&gt; &lt;/a&gt; &lt;a href="https://inviter.co/hyperswitch-slack"&gt; &lt;img src="https://img.shields.io/badge/chat-on_slack-blue?logo=slack&amp;amp;labelColor=grey&amp;amp;color=%233f0e40"&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;hr&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìÅ Table of Contents&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#-what-can-i-do-with-hyperswitch"&gt;What Can I Do with Hyperswitch?&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#-quickstart-local-setup"&gt;Quickstart (Local Setup)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#cloud-deployment"&gt;Cloud Deployment&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#hosted-sandbox-no-setup-required"&gt;Hosted Sandbox (No Setup Required)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#-why-hyperswitch"&gt;Why Hyperswitch?&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#architectural-overview"&gt;Architectural Overview&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#our-vision"&gt;Our Vision&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#community--contributions"&gt;Community &amp;amp; Contributions&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#feature-requests--bugs"&gt;Feature Requests &amp;amp; Bugs&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#versioning"&gt;Versioning&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#copyright-and-license"&gt;License&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#team-behind-hyperswitch"&gt;Team Behind Hyperswitch&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;summary&gt;&lt;h2&gt; What Can I Do with Hyperswitch?&lt;/h2&gt;&lt;/summary&gt; 
&lt;p&gt;Hyperswitch offers a modular, open-source payments infrastructure designed for flexibility and control. Apart from our Payment Suite offering, this solution allows businesses to pick and integrate only the modules they need on top of their existing payment stack ‚Äî without unnecessary complexity or vendor lock-in.&lt;/p&gt; 
&lt;p&gt;Each module is independent and purpose-built to optimize different aspects of payment processing.&lt;/p&gt; 
&lt;h3&gt; Learn More About The Payment Modules &lt;/h3&gt; 
&lt;details&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Cost Observability&lt;/strong&gt;&lt;br&gt; Advanced observability tools to audit, monitor, and optimize your payment costs. Detect hidden fees, downgrades, and penalties with self-serve dashboards and actionable insights.&lt;br&gt; &lt;em&gt;&lt;a href="https://docs.hyperswitch.io/about-hyperswitch/payments-modules/ai-powered-cost-observability"&gt;Read more&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Revenue Recovery&lt;/strong&gt;&lt;br&gt; Combat passive churn with intelligent retry strategies tuned by card bin, region, method, and more. Offers fine-grained control over retry algorithms, penalty budgets, and recovery transparency.&lt;br&gt; &lt;em&gt;&lt;a href="https://docs.hyperswitch.io/about-hyperswitch/payments-modules/revenue-recovery"&gt;Read more&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Vault&lt;/strong&gt;&lt;br&gt; A PCI-compliant vault service to store cards, tokens, wallets, and bank credentials. Provides a unified, secure, and reusable store of customer-linked payment methods.&lt;br&gt; &lt;em&gt;&lt;a href="https://docs.hyperswitch.io/about-hyperswitch/payments-modules/vault"&gt;Read more&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Intelligent Routing&lt;/strong&gt;&lt;br&gt; Route each transaction to the PSP with the highest predicted auth rate. Reduce retries, avoid downtime, and minimize latency while maximizing first attempt success.&lt;br&gt; &lt;em&gt;&lt;a href="https://docs.hyperswitch.io/about-hyperswitch/payments-modules/intelligent-routing"&gt;Read more&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Reconciliation&lt;/strong&gt;&lt;br&gt; Automate 2-way and 3-way reconciliation with backdated support, staggered scheduling, and customizable outputs. Reduces manual ops effort and increases audit confidence.&lt;br&gt; &lt;em&gt;&lt;a href="https://docs.hyperswitch.io/about-hyperswitch/payments-modules/reconciliation"&gt;Read more&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alternate Payment Methods&lt;/strong&gt;&lt;br&gt; Drop-in widgets for PayPal, Apple Pay, Google Pay, Samsung Pay, Pay by Bank, and BNPL providers like Klarna. Maximizes conversions with seamless one-click checkout.&lt;br&gt; &lt;em&gt;&lt;a href="https://docs.hyperswitch.io/about-hyperswitch/payments-modules/enable-alternate-payment-method-widgets"&gt;Read more&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;h3&gt; Local Setup via Docker &lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# One-click local setup

git clone --depth 1 --branch latest https://github.com/juspay/hyperswitch

cd hyperswitch

scripts/setup.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;This script: &lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Detects Docker/Podman&lt;/li&gt; 
  &lt;li&gt;Offers multiple deployment profiles: 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Standard&lt;/strong&gt;: App server + Control Center&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Full&lt;/strong&gt;: Includes monitoring + schedulers&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Minimal&lt;/strong&gt;: Standalone App server&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Provides access links when done&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;If you need further help, check out our &lt;a href="https://docs.hyperswitch.io/hyperswitch-open-source/overview/unified-local-setup-using-docker"&gt;video tutorial&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;üëâ After setup, &lt;a href="https://docs.hyperswitch.io/hyperswitch-open-source/account-setup/using-hyperswitch-control-center#add-a-payment-processor"&gt;configure a connector&lt;/a&gt; and &lt;a href="https://docs.hyperswitch.io/hyperswitch-open-source/account-setup/test-a-payment"&gt;test a payment&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;Hosted Sandbox (No Setup Required)&lt;/h3&gt; 
&lt;p&gt;Hyperswitch offers a fully hosted sandbox environment that requires no setup. You can explore the Control Center, configure payment connectors, and test payments directly from the UI.&lt;/p&gt; 
&lt;a href="https://app.hyperswitch.io"&gt; &lt;img src="https://github.com/juspay/hyperswitch/raw/main/docs/imgs/try-the-sandbox.png?raw=true" height="35"&gt; &lt;/a&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt; What you can do in the Hosted Sandbox&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Access the full Control Center&lt;/li&gt; 
  &lt;li&gt;Configure payment connectors&lt;/li&gt; 
  &lt;li&gt;View logs, routing rules, and retry strategies&lt;/li&gt; 
  &lt;li&gt;Try payments directly from the UI&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;&lt;strong&gt;Cloud Deployment&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;You can deploy to AWS, GCP, or Azure using Helm or CDK scripts. Fastest path:&lt;/p&gt; 
&lt;p&gt;Click to deploy via AWS:&lt;/p&gt; 
&lt;a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=HyperswitchBootstarp&amp;amp;templateURL=https://hyperswitch-synth.s3.eu-central-1.amazonaws.com/hs-starter-config.yaml"&gt; &lt;img src="https://github.com/juspay/hyperswitch/raw/main/docs/imgs/aws_button.png?raw=true" height="35"&gt; &lt;/a&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Cloud Deployment Instructions&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Click the AWS deployment button above to launch the stack.&lt;/li&gt; 
  &lt;li&gt;Follow the guided steps in the AWS Console (approx. 30‚Äì45 mins).&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;‚úÖ This setup provisions Hyperswitch on your cloud account using CloudFormation.&lt;/p&gt; 
 &lt;p&gt;üìò For full instructions and Helm-based deployments, check out the&lt;br&gt; &lt;a href="https://docs.hyperswitch.io/hyperswitch-open-source/deploy-on-kubernetes-using-helm"&gt;Cloud Install Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#architectural-overview"&gt; &lt;h2 id="architectural-overview"&gt;Architectural Overview&lt;/h2&gt; &lt;/a&gt; 
&lt;img src="https://raw.githubusercontent.com/juspay/hyperswitch/main/docs/imgs/features.png"&gt; 
&lt;img src="https://raw.githubusercontent.com/juspay/hyperswitch/main/docs/imgs/non-functional-features.png"&gt; 
&lt;img src="https://raw.githubusercontent.com/juspay/hyperswitch/main/docs/imgs/hyperswitch-architecture-v1.png"&gt; 
&lt;h2&gt;Why Hyperswitch?&lt;/h2&gt; 
&lt;p&gt;Hyperswitch is a commercial open-source payments stack purpose-built for scale, flexibility, and developer experience. Designed with a modular architecture, Hyperswitch lets you pick only the components you need‚Äîwhether it‚Äôs routing, retries, vaulting, or observability‚Äîwithout vendor lock-in or bloated integrations.&lt;/p&gt; 
&lt;p&gt;Built in Rust for performance and reliability, Hyperswitch supports global payment methods (cards, wallets, BNPL, UPI, Pay by Bank), exposes smart routing and retry logic, and provides a visual workflow builder in the Control Center. Whether you're integrating a full payment suite or augmenting an existing stack with a single module, Hyperswitch meets you where you are.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;‚ÄúLinux for Payments‚Äù&lt;/strong&gt; ‚Äî Hyperswitch is a well-architected reference for teams who want to own their payments stack.&lt;/p&gt; 
&lt;p&gt;We believe in:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt; Embracing Payment Diversity:&lt;/strong&gt; Innovation comes from enabling choice‚Äîacross payment methods, processors, and flows.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt; Open Source by Default:&lt;/strong&gt; Transparency drives trust and builds better, reusable software.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt; Community-Driven Development:&lt;/strong&gt; Our roadmap is shaped by real-world use cases and contributors.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt; Systems-Level Engineering:&lt;/strong&gt; We hold ourselves to a high bar for reliability, security, and performance.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt; Maximizing Value Creation:&lt;/strong&gt; For developers, customers, and partners alike.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt; Community-Driven, Enterprise-Tested:&lt;/strong&gt; Hyperswitch is built in the open with real-world feedback from developers and contributors, and maintained by Juspay, the team powering payment infrastructure for 400+ leading enterprises worldwide.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributors from around the world to help build Hyperswitch. Whether you're fixing bugs, improving documentation, or adding new features, your help is appreciated.&lt;/p&gt; 
&lt;p&gt;Please read our &lt;a href="https://github.com/juspay/hyperswitch/raw/main/docs/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;p&gt;Join the conversation on &lt;a href="https://inviter.co/hyperswitch-slack"&gt;Slack&lt;/a&gt; or explore open issues on &lt;a href="https://github.com/juspay/hyperswitch/issues"&gt;GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#feature-requests"&gt; &lt;h2 id="feature-requests"&gt;Feature requests &amp;amp; Bugs&lt;/h2&gt; &lt;/a&gt; 
&lt;p&gt;For new product features, enhancements, roadmap discussions, or to share queries and ideas, visit our &lt;a href="https://github.com/juspay/hyperswitch/discussions"&gt;GitHub Discussions&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For reporting a bug, please read the issue guidelines and search for &lt;a href="https://github.com/juspay/hyperswitch/issues"&gt;existing and closed issues&lt;/a&gt;. If your problem or idea is not addressed yet, please &lt;a href="https://github.com/juspay/hyperswitch/issues/new/choose"&gt;open a new issue&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#versioning"&gt; &lt;h2 id="versioning"&gt;Versioning&lt;/h2&gt; &lt;/a&gt; 
&lt;p&gt;Check the &lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/CHANGELOG.md"&gt;CHANGELOG.md&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#copyright-and-license"&gt; &lt;h2 id="copyright-and-license"&gt;Copyright and License&lt;/h2&gt; &lt;/a&gt; 
&lt;p&gt;This product is licensed under the &lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/LICENSE"&gt;Apache 2.0 License&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#team-behind-hyperswitch"&gt; &lt;h2 id="team-behind-hyperswitch"&gt;Team behind Hyperswitch&lt;/h2&gt; &lt;/a&gt; 
&lt;p&gt;The core team of 150+ engineers building Hyperswitch. Keep up the great work! ü•Ç&lt;/p&gt; 
&lt;a href="https://github.com/juspay/hyperswitch/graphs/contributors"&gt; &lt;img src="https://contributors-img.web.app/image?repo=juspay/hyperswitch" alt="Contributors"&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>fujiapple852/trippy</title>
      <link>https://github.com/fujiapple852/trippy</link>
      <description>&lt;p&gt;A network diagnostic tool&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/fujiapple852/trippy/master/docs/src/assets/Trippy-Vertical-DarkMode.svg#gh-dark-mode-only" width="300"&gt; &lt;img src="https://raw.githubusercontent.com/fujiapple852/trippy/master/docs/src/assets/Trippy-Vertical.svg#gh-light-mode-only" width="300"&gt;&lt;br&gt; &lt;br&gt; &lt;a href="https://github.com/fujiapple852/trippy/actions/workflows/ci.yml"&gt; &lt;img src="https://github.com/fujiapple852/trippy/actions/workflows/ci.yml/badge.svg?branch=master"&gt;&lt;/a&gt; &lt;a href="https://crates.io/crates/trippy/0.13.0"&gt; &lt;img src="https://img.shields.io/crates/v/trippy.svg?sanitize=true"&gt;&lt;/a&gt; &lt;a href="https://repology.org/project/trippy/versions"&gt; &lt;img src="https://repology.org/badge/tiny-repos/trippy.svg?sanitize=true"&gt;&lt;/a&gt; &lt;a href="https://trippy.zulipchat.com"&gt; &lt;img src="https://img.shields.io/badge/zulip-join_chat-brightgreen.svg?sanitize=true"&gt;&lt;/a&gt; &lt;a href="https://matrix.to/#/%23trippy-dev:matrix.org"&gt; &lt;img src="https://img.shields.io/badge/matrix/trippy-dev:matrix.org-blue"&gt;&lt;/a&gt; &lt;br&gt; &lt;br&gt; Trippy combines the functionality of traceroute and ping and is designed to assist with the analysis of networking issues. &lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/fujiapple852/trippy/master/assets/0.12.0/demo.gif" alt="trippy"&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://trippy.rs/start/getting-started"&gt;getting started&lt;/a&gt; guide.&lt;/p&gt; 
&lt;h3&gt;Install&lt;/h3&gt; 
&lt;p&gt;Trippy runs on Linux, BSD, macOS, and Windows. It can be installed from most package managers, precompiled binaries, or source.&lt;/p&gt; 
&lt;p&gt;For example, to install Trippy from &lt;code&gt;cargo&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cargo install trippy --locked
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;All package managers&lt;/summary&gt; 
 &lt;h3&gt;Cargo&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://crates.io/crates/trippy/0.13.0"&gt;&lt;img src="https://img.shields.io/crates/v/trippy" alt="Crates.io"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;cargo install trippy --locked
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;APT (Debian)&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://tracker.debian.org/pkg/trippy"&gt;&lt;img src="https://repology.org/badge/version-for-repo/debian_13/trippy.svg?sanitize=true" alt="Debian 13 package"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;apt install trippy
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;‚ìò Note:&lt;/p&gt; 
  &lt;p&gt;Only available for Debian 13 (&lt;code&gt;trixie&lt;/code&gt;) and later.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h3&gt;PPA (Ubuntu)&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://launchpad.net/~fujiapple/+archive/ubuntu/trippy/+packages"&gt;&lt;img src="https://img.shields.io/badge/Ubuntu%20PPA-0.13.0-brightgreen" alt="Ubuntu PPA"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;add-apt-repository ppa:fujiapple/trippy
apt update &amp;amp;&amp;amp; apt install trippy
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;‚ìò Note:&lt;/p&gt; 
  &lt;p&gt;Only available for Ubuntu 24.04 (&lt;code&gt;Noble&lt;/code&gt;) and 22.04 (&lt;code&gt;Jammy&lt;/code&gt;).&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h3&gt;Snap (Linux)&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://snapcraft.io/trippy"&gt;&lt;img src="https://snapcraft.io/trippy/badge.svg?sanitize=true" alt="trippy"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;snap install trippy
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Homebrew (macOS)&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://formulae.brew.sh/formula/trippy"&gt;&lt;img src="https://repology.org/badge/version-for-repo/homebrew/trippy.svg?sanitize=true" alt="Homebrew package"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;brew install trippy
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;WinGet (Windows)&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://github.com/microsoft/winget-pkgs/tree/master/manifests/f/FujiApple/Trippy/0.13.0"&gt;&lt;img src="https://img.shields.io/badge/WinGet-0.13.0-brightgreen" alt="winget package"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;winget install trippy
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Scoop (Windows)&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://github.com/ScoopInstaller/Main/raw/master/bucket/trippy.json"&gt;&lt;img src="https://img.shields.io/scoop/v/trippy?style=flat&amp;amp;labelColor=5c5c5c&amp;amp;color=%234dc71f" alt="Scoop package"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;scoop install trippy
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Chocolatey (Windows)&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://community.chocolatey.org/packages/trippy"&gt;&lt;img src="https://repology.org/badge/version-for-repo/chocolatey/trippy.svg?sanitize=true" alt="Chocolatey package"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;choco install trippy
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;NetBSD&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://pkgsrc.se/net/trippy"&gt;&lt;img src="https://repology.org/badge/version-for-repo/pkgsrc_current/trippy.svg?sanitize=true" alt="pkgsrc current package"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;pkgin install trippy
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;FreeBSD&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://www.freshports.org/net/trippy/"&gt;&lt;img src="https://repology.org/badge/version-for-repo/freebsd/trippy.svg?sanitize=true" alt="FreeBSD port"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;pkg install trippy
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;OpenBSD&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://openports.pl/path/net/trippy"&gt;&lt;img src="https://repology.org/badge/version-for-repo/openbsd/trippy.svg?sanitize=true" alt="OpenBSD port"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;pkg_add trippy
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Arch Linux&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://archlinux.org/packages/extra/x86_64/trippy"&gt;&lt;img src="https://repology.org/badge/version-for-repo/arch/trippy.svg?sanitize=true" alt="Arch package"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;pacman -S trippy
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Gentoo Linux&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://packages.gentoo.org/packages/net-analyzer/trippy"&gt;&lt;img src="https://repology.org/badge/version-for-repo/gentoo/trippy.svg?sanitize=true" alt="Gentoo package"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;emerge -av net-analyzer/trippy
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Void Linux&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://github.com/void-linux/void-packages/tree/master/srcpkgs/trippy"&gt;&lt;img src="https://repology.org/badge/version-for-repo/void_x86_64/trippy.svg?sanitize=true" alt="Void Linux x86_64 package"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;xbps-install -S trippy
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;ALT Sisyphus&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://packages.altlinux.org/en/sisyphus/srpms/trippy/"&gt;&lt;img src="https://repology.org/badge/version-for-repo/altsisyphus/trippy.svg?sanitize=true" alt="ALT Sisyphus package"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;apt-get install trippy
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Chimera Linux&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://github.com/chimera-linux/cports/tree/master/user/trippy"&gt;&lt;img src="https://repology.org/badge/version-for-repo/chimera/trippy.svg?sanitize=true" alt="Chimera Linux package"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;apk add trippy
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Nix&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://github.com/NixOS/nixpkgs/raw/master/pkgs/by-name/tr/trippy/package.nix"&gt;&lt;img src="https://repology.org/badge/version-for-repo/nix_unstable/trippy.svg?sanitize=true" alt="nixpkgs unstable package"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;nix-env -iA trippy
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Docker&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://hub.docker.com/r/fujiapple/trippy/"&gt;&lt;img src="https://img.shields.io/docker/v/fujiapple/trippy" alt="Docker Image Version (latest by date)"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;docker run -it fujiapple/trippy
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;All Repositories&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://repology.org/project/trippy/versions"&gt;&lt;img src="https://repology.org/badge/vertical-allrepos/trippy.svg?sanitize=true" alt="Packaging status"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;p&gt;See the &lt;a href="https://trippy.rs/start/installation"&gt;installation&lt;/a&gt; guide for details of how to install Trippy on your system.&lt;/p&gt; 
&lt;h3&gt;Run&lt;/h3&gt; 
&lt;p&gt;To run a basic trace to &lt;code&gt;example.com&lt;/code&gt; with default settings, use the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;sudo trip example.com
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href="https://trippy.rs/guides/usage"&gt;usage examples&lt;/a&gt; and &lt;a href="https://trippy.rs/reference/cli"&gt;CLI reference&lt;/a&gt; for details of how to use Trippy. To use Trippy without elevated privileges, see the &lt;a href="https://trippy.rs/guides/privileges"&gt;privileges&lt;/a&gt; guide.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Full documentation is available at &lt;a href="https://trippy.rs"&gt;trippy.rs&lt;/a&gt;.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;documentation links&lt;/summary&gt; 
 &lt;h2&gt;Getting Started&lt;/h2&gt; 
 &lt;p&gt;See the &lt;a href="https://trippy.rs/start/getting-started/"&gt;Getting Started&lt;/a&gt; guide.&lt;/p&gt; 
 &lt;h2&gt;Features&lt;/h2&gt; 
 &lt;p&gt;See the &lt;a href="https://trippy.rs/start/features/"&gt;Features&lt;/a&gt; list.&lt;/p&gt; 
 &lt;h2&gt;Distributions&lt;/h2&gt; 
 &lt;p&gt;See the &lt;a href="https://trippy.rs/start/installation/"&gt;Distributions&lt;/a&gt; list.&lt;/p&gt; 
 &lt;h2&gt;Privileges&lt;/h2&gt; 
 &lt;p&gt;See the &lt;a href="https://trippy.rs/guides/privileges/"&gt;Privileges&lt;/a&gt; guide.&lt;/p&gt; 
 &lt;h2&gt;Usage Examples&lt;/h2&gt; 
 &lt;p&gt;See the &lt;a href="https://trippy.rs/guides/usage/"&gt;Usage Examples&lt;/a&gt;.&lt;/p&gt; 
 &lt;h2&gt;Command Reference&lt;/h2&gt; 
 &lt;p&gt;See the &lt;a href="https://trippy.rs/reference/cli/"&gt;Command Reference&lt;/a&gt;.&lt;/p&gt; 
 &lt;h2&gt;Theme Reference&lt;/h2&gt; 
 &lt;p&gt;See the &lt;a href="https://trippy.rs/reference/theme/"&gt;Theme Reference&lt;/a&gt;.&lt;/p&gt; 
 &lt;h2&gt;Column Reference&lt;/h2&gt; 
 &lt;p&gt;See the &lt;a href="https://trippy.rs/reference/column/"&gt;Column Reference&lt;/a&gt;.&lt;/p&gt; 
 &lt;h2&gt;Configuration Reference&lt;/h2&gt; 
 &lt;p&gt;See the &lt;a href="https://trippy.rs/reference/configuration/"&gt;Configuration Reference&lt;/a&gt;.&lt;/p&gt; 
 &lt;h2&gt;Locale Reference&lt;/h2&gt; 
 &lt;p&gt;See the &lt;a href="https://trippy.rs/reference/locale/"&gt;Locale Reference&lt;/a&gt;.&lt;/p&gt; 
 &lt;h2&gt;Versions&lt;/h2&gt; 
 &lt;p&gt;See the &lt;a href="https://trippy.rs/reference/version/"&gt;Version Reference&lt;/a&gt;.&lt;/p&gt; 
 &lt;h2&gt;Frequently Asked Questions&lt;/h2&gt; 
 &lt;h3&gt;Why does Trippy show "Awaiting data..."?&lt;/h3&gt; 
 &lt;p&gt;See the &lt;a href="https://trippy.rs/guides/faq/"&gt;Awaiting Data&lt;/a&gt; guide.&lt;/p&gt; 
 &lt;p&gt;&lt;a name="windows-defender"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;h3&gt;How do I allow incoming ICMP traffic in the Windows Defender firewall?&lt;/h3&gt; 
 &lt;p&gt;See the &lt;a href="https://trippy.rs/guides/windows_firewall/"&gt;Windows Defender Firewall&lt;/a&gt; guide.&lt;/p&gt; 
 &lt;h3&gt;What are the recommended settings for Trippy?&lt;/h3&gt; 
 &lt;p&gt;See the &lt;a href="https://trippy.rs/guides/recommendation/"&gt;Recommended Tracing Settings&lt;/a&gt; guide.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;Trippy is made possible by &lt;a href="https://github.com/ratatui-org/ratatui"&gt;ratatui&lt;/a&gt; ( formerly &lt;a href="https://github.com/fdehau/tui-rs"&gt;tui-rs&lt;/a&gt;), &lt;a href="https://github.com/crossterm-rs/crossterm"&gt;crossterm&lt;/a&gt; as well as &lt;a href="https://github.com/fujiapple852/trippy/raw/master/Cargo.toml"&gt;several&lt;/a&gt; foundational Rust libraries.&lt;/p&gt; 
&lt;p&gt;Trippy draws heavily from &lt;a href="https://github.com/traviscross/mtr"&gt;mtr&lt;/a&gt; and also incorporates ideas from both &lt;a href="https://github.com/libparistraceroute/libparistraceroute"&gt;libparistraceroute&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/insomniacslk/dublin-traceroute"&gt;Dublin Traceroute&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The Trippy networking code is inspired by &lt;a href="https://github.com/libpnet/libpnet"&gt;pnet&lt;/a&gt; and some elements of that codebase are incorporated in Trippy.&lt;/p&gt; 
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Autonomous_system_(Internet)"&gt;AS&lt;/a&gt; data is retrieved from the &lt;a href="https://team-cymru.com/community-services/ip-asn-mapping/#dns"&gt;IP to ASN Mapping Service&lt;/a&gt; provided by &lt;a href="https://team-cymru.com"&gt;Team Cymru&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The &lt;a href="https://trippy.cli.rs"&gt;trippy.cli.rs&lt;/a&gt; CNAME hosting is provided by &lt;a href="https://cli.rs"&gt;cli.rs&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The Trippy chat room is sponsored by &lt;a href="https://zulip.com"&gt;Zulip&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Trippy logo designed by &lt;a href="https://www.instagram.com/harunocaksiz"&gt;Harun Ocaksiz Design&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is distributed under the terms of the Apache License (Version 2.0).&lt;/p&gt; 
&lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in time by you, as defined in the Apache-2.0 license, shall be licensed as above, without any additional terms or conditions.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/fujiapple852/trippy/master/LICENSE"&gt;LICENSE&lt;/a&gt; for details.&lt;/p&gt; 
&lt;p&gt;Copyright 2022 &lt;a href="https://github.com/fujiapple852/trippy/graphs/contributors"&gt;Trippy Contributors&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>rustdesk/rustdesk</title>
      <link>https://github.com/rustdesk/rustdesk</link>
      <description>&lt;p&gt;An open-source remote desktop application designed for self-hosting, as an alternative to TeamViewer.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/rustdesk/rustdesk/master/res/logo-header.svg?sanitize=true" alt="RustDesk - Your remote desktop"&gt;&lt;br&gt; &lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/#raw-steps-to-build"&gt;Build&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/#how-to-build-with-docker"&gt;Docker&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/#file-structure"&gt;Structure&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/#snapshot"&gt;Snapshot&lt;/a&gt;&lt;br&gt; [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-UA.md"&gt;–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-CS.md"&gt;ƒçesky&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-ZH.md"&gt;‰∏≠Êñá&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-HU.md"&gt;Magyar&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-ES.md"&gt;Espa√±ol&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-FA.md"&gt;ŸÅÿßÿ±ÿ≥€å&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-FR.md"&gt;Fran√ßais&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-DE.md"&gt;Deutsch&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-PL.md"&gt;Polski&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-ID.md"&gt;Indonesian&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-FI.md"&gt;Suomi&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-ML.md"&gt;‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-JP.md"&gt;Êó•Êú¨Ë™û&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-NL.md"&gt;Nederlands&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-IT.md"&gt;Italiano&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-RU.md"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-PTBR.md"&gt;Portugu√™s (Brasil)&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-EO.md"&gt;Esperanto&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-KR.md"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-AR.md"&gt;ÿßŸÑÿπÿ±ÿ®Ÿä&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-VN.md"&gt;Ti·∫øng Vi·ªát&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-DA.md"&gt;Dansk&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-GR.md"&gt;ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-TR.md"&gt;T√ºrk√ße&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-NO.md"&gt;Norsk&lt;/a&gt;]&lt;br&gt; &lt;b&gt;We need your help to translate this README, &lt;a href="https://github.com/rustdesk/rustdesk/tree/master/src/lang"&gt;RustDesk UI&lt;/a&gt; and &lt;a href="https://github.com/rustdesk/doc.rustdesk.com"&gt;RustDesk Doc&lt;/a&gt; to your native language&lt;/b&gt; &lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Caution] &lt;strong&gt;Misuse Disclaimer:&lt;/strong&gt; &lt;br&gt; The developers of RustDesk do not condone or support any unethical or illegal use of this software. Misuse, such as unauthorized access, control or invasion of privacy, is strictly against our guidelines. The authors are not responsible for any misuse of the application.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Chat with us: &lt;a href="https://discord.gg/nDceKgxnkV"&gt;Discord&lt;/a&gt; | &lt;a href="https://twitter.com/rustdesk"&gt;Twitter&lt;/a&gt; | &lt;a href="https://www.reddit.com/r/rustdesk"&gt;Reddit&lt;/a&gt; | &lt;a href="https://www.youtube.com/@rustdesk"&gt;YouTube&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://ko-fi.com/I2I04VU09"&gt;&lt;img src="https://ko-fi.com/img/githubbutton_sm.svg?sanitize=true" alt="ko-fi"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Yet another remote desktop solution, written in Rust. Works out of the box with no configuration required. You have full control of your data, with no concerns about security. You can use our rendezvous/relay server, &lt;a href="https://rustdesk.com/server"&gt;set up your own&lt;/a&gt;, or &lt;a href="https://github.com/rustdesk/rustdesk-server-demo"&gt;write your own rendezvous/relay server&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/71636191/171661982-430285f0-2e12-4b1d-9957-4a58e375304d.png" alt="image"&gt;&lt;/p&gt; 
&lt;p&gt;RustDesk welcomes contribution from everyone. See &lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for help getting started.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/rustdesk/rustdesk/wiki/FAQ"&gt;&lt;strong&gt;FAQ&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/rustdesk/rustdesk/releases"&gt;&lt;strong&gt;BINARY DOWNLOAD&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/rustdesk/rustdesk/releases/tag/nightly"&gt;&lt;strong&gt;NIGHTLY BUILD&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://f-droid.org/en/packages/com.carriez.flutter_hbb"&gt;&lt;img src="https://f-droid.org/badge/get-it-on.png" alt="Get it on F-Droid" height="80"&gt;&lt;/a&gt; &lt;a href="https://flathub.org/apps/com.rustdesk.RustDesk"&gt;&lt;img src="https://flathub.org/api/badge?svg&amp;amp;locale=en" alt="Get it on Flathub" height="80"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Dependencies&lt;/h2&gt; 
&lt;p&gt;Desktop versions use Flutter or Sciter (deprecated) for GUI, this tutorial is for Sciter only, since it is easier and more friendly to start. Check out our &lt;a href="https://github.com/rustdesk/rustdesk/raw/master/.github/workflows/flutter-build.yml"&gt;CI&lt;/a&gt; for building Flutter version.&lt;/p&gt; 
&lt;p&gt;Please download Sciter dynamic library yourself.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.win/x64/sciter.dll"&gt;Windows&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.lnx/x64/libsciter-gtk.so"&gt;Linux&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.osx/libsciter.dylib"&gt;macOS&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Raw Steps to build&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Prepare your Rust development env and C++ build env&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;a href="https://github.com/microsoft/vcpkg"&gt;vcpkg&lt;/a&gt;, and set &lt;code&gt;VCPKG_ROOT&lt;/code&gt; env variable correctly&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Windows: vcpkg install libvpx:x64-windows-static libyuv:x64-windows-static opus:x64-windows-static aom:x64-windows-static&lt;/li&gt; 
   &lt;li&gt;Linux/macOS: vcpkg install libvpx libyuv opus aom&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;run &lt;code&gt;cargo run&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href="https://rustdesk.com/docs/en/dev/build/"&gt;Build&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;How to Build on Linux&lt;/h2&gt; 
&lt;h3&gt;Ubuntu 18 (Debian 10)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;sudo apt install -y zip g++ gcc git curl wget nasm yasm libgtk-3-dev clang libxcb-randr0-dev libxdo-dev \
        libxfixes-dev libxcb-shape0-dev libxcb-xfixes0-dev libasound2-dev libpulse-dev cmake make \
        libclang-dev ninja-build libgstreamer1.0-dev libgstreamer-plugins-base1.0-dev libpam0g-dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;openSUSE Tumbleweed&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;sudo zypper install gcc-c++ git curl wget nasm yasm gcc gtk3-devel clang libxcb-devel libXfixes-devel cmake alsa-lib-devel gstreamer-devel gstreamer-plugins-base-devel xdotool-devel pam-devel
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Fedora 28 (CentOS 8)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;sudo yum -y install gcc-c++ git curl wget nasm yasm gcc gtk3-devel clang libxcb-devel libxdo-devel libXfixes-devel pulseaudio-libs-devel cmake alsa-lib-devel gstreamer1-devel gstreamer1-plugins-base-devel pam-devel
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Arch (Manjaro)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;sudo pacman -Syu --needed unzip git cmake gcc curl wget yasm nasm zip make pkg-config clang gtk3 xdotool libxcb libxfixes alsa-lib pipewire
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install vcpkg&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/microsoft/vcpkg
cd vcpkg
git checkout 2023.04.15
cd ..
vcpkg/bootstrap-vcpkg.sh
export VCPKG_ROOT=$HOME/vcpkg
vcpkg/vcpkg install libvpx libyuv opus aom
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Fix libvpx (For Fedora)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cd vcpkg/buildtrees/libvpx/src
cd *
./configure
sed -i 's/CFLAGS+=-I/CFLAGS+=-fPIC -I/g' Makefile
sed -i 's/CXXFLAGS+=-I/CXXFLAGS+=-fPIC -I/g' Makefile
make
cp libvpx.a $HOME/vcpkg/installed/x64-linux/lib/
cd
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Build&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source $HOME/.cargo/env
git clone --recurse-submodules https://github.com/rustdesk/rustdesk
cd rustdesk
mkdir -p target/debug
wget https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.lnx/x64/libsciter-gtk.so
mv libsciter-gtk.so target/debug
VCPKG_ROOT=$HOME/vcpkg cargo run
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;How to build with Docker&lt;/h2&gt; 
&lt;p&gt;Begin by cloning the repository and building the Docker container:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/rustdesk/rustdesk
cd rustdesk
git submodule update --init --recursive
docker build -t "rustdesk-builder" .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, each time you need to build the application, run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;docker run --rm -it -v $PWD:/home/user/rustdesk -v rustdesk-git-cache:/home/user/.cargo/git -v rustdesk-registry-cache:/home/user/.cargo/registry -e PUID="$(id -u)" -e PGID="$(id -g)" rustdesk-builder
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that the first build may take longer before dependencies are cached, subsequent builds will be faster. Additionally, if you need to specify different arguments to the build command, you may do so at the end of the command in the &lt;code&gt;&amp;lt;OPTIONAL-ARGS&amp;gt;&lt;/code&gt; position. For instance, if you wanted to build an optimized release version, you would run the command above followed by &lt;code&gt;--release&lt;/code&gt;. The resulting executable will be available in the target folder on your system, and can be run with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;target/debug/rustdesk
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, if you're running a release executable:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;target/release/rustdesk
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please ensure that you run these commands from the root of the RustDesk repository, or the application may not find the required resources. Also note that other cargo subcommands such as &lt;code&gt;install&lt;/code&gt; or &lt;code&gt;run&lt;/code&gt; are not currently supported via this method as they would install or run the program inside the container instead of the host.&lt;/p&gt; 
&lt;h2&gt;File Structure&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/libs/hbb_common"&gt;libs/hbb_common&lt;/a&gt;&lt;/strong&gt;: video codec, config, tcp/udp wrapper, protobuf, fs functions for file transfer, and some other utility functions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/libs/scrap"&gt;libs/scrap&lt;/a&gt;&lt;/strong&gt;: screen capture&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/libs/enigo"&gt;libs/enigo&lt;/a&gt;&lt;/strong&gt;: platform specific keyboard/mouse control&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/libs/clipboard"&gt;libs/clipboard&lt;/a&gt;&lt;/strong&gt;: file copy and paste implementation for Windows, Linux, macOS.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/src/ui"&gt;src/ui&lt;/a&gt;&lt;/strong&gt;: obsolete Sciter UI (deprecated)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/src/server"&gt;src/server&lt;/a&gt;&lt;/strong&gt;: audio/clipboard/input/video services, and network connections&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/src/client.rs"&gt;src/client.rs&lt;/a&gt;&lt;/strong&gt;: start a peer connection&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/src/rendezvous_mediator.rs"&gt;src/rendezvous_mediator.rs&lt;/a&gt;&lt;/strong&gt;: Communicate with &lt;a href="https://github.com/rustdesk/rustdesk-server"&gt;rustdesk-server&lt;/a&gt;, wait for remote direct (TCP hole punching) or relayed connection&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/src/platform"&gt;src/platform&lt;/a&gt;&lt;/strong&gt;: platform specific code&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/flutter"&gt;flutter&lt;/a&gt;&lt;/strong&gt;: Flutter code for desktop and mobile&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/flutter/web/v1/js"&gt;flutter/web/js&lt;/a&gt;&lt;/strong&gt;: JavaScript for Flutter web client&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Screenshots&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://github.com/rustdesk/rustdesk/assets/28412477/db82d4e7-c4bc-4823-8e6f-6af7eadf7651" alt="Connection Manager"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/rustdesk/rustdesk/assets/28412477/9baa91e9-3362-4d06-aa1a-7518edcbd7ea" alt="Connected to a Windows PC"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/rustdesk/rustdesk/assets/28412477/39511ad3-aa9a-4f8c-8947-1cce286a46ad" alt="File Transfer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/rustdesk/rustdesk/assets/28412477/78e8708f-e87e-4570-8373-1360033ea6c5" alt="TCP Tunneling"&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>wssheldon/osintui</title>
      <link>https://github.com/wssheldon/osintui</link>
      <description>&lt;p&gt;OSINT from your favorite services in a friendly terminal user interface - integrations for Virustotal, Shodan, and Censys&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;osintui&lt;/h1&gt; 
 &lt;p&gt; Open Source Intelligence Terminal User Interface &lt;/p&gt; 
 &lt;!-- Badges --&gt; 
 &lt;p&gt; &lt;a href="https://github.com/wssheldon/osintui/graphs/contributors"&gt; &lt;img src="https://img.shields.io/github/contributors/wssheldon/osintui" alt="contributors"&gt; &lt;/a&gt; &lt;a href=""&gt; &lt;img src="https://img.shields.io/github/last-commit/wssheldon/osintui" alt="last update"&gt; &lt;/a&gt; &lt;a href="https://github.com/wssheldon/osintui/stargazers"&gt; &lt;img src="https://img.shields.io/github/stars/wssheldon/osintui" alt="stars"&gt; &lt;/a&gt; &lt;a href="https://github.com/wssheldon/osintui/issues/"&gt; &lt;img src="https://img.shields.io/github/issues/wssheldon/osintui" alt="open issues"&gt; &lt;/a&gt; &lt;a href="https://github.com/wssheldon/osintui/raw/master/LICENSE"&gt; &lt;img src="https://img.shields.io/github/license/wssheldon/osintui.svg?sanitize=true" alt="license"&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;h4&gt; &lt;a href="https://github.com/wssheldon/osintui/issues/"&gt;Report Bug&lt;/a&gt; &lt;span&gt; ¬∑ &lt;/span&gt; &lt;a href="https://github.com/wssheldon/osintui/issues/"&gt;Request Feature&lt;/a&gt; &lt;/h4&gt; 
&lt;/div&gt; 
&lt;br&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/wssheldon/osintui/main/assets/demo.gif" alt="screenshot"&gt; 
&lt;/div&gt; 
&lt;hr&gt; 
&lt;h2&gt;Integrations&lt;/h2&gt; 
&lt;br&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a href="https://shodan.io/"&gt; &lt;img src="https://raw.githubusercontent.com/wssheldon/osintui/main/assets/logos/shodan_logo.png" alt="shodan"&gt; &lt;/a&gt; &lt;a href="https://censys.io/"&gt; &lt;img src="https://raw.githubusercontent.com/wssheldon/osintui/main/assets/logos/censys_logo.png" alt="censys"&gt; &lt;/a&gt; &lt;a href="https://virustotal.com/"&gt; &lt;img src="https://raw.githubusercontent.com/wssheldon/osintui/main/assets/logos/virustotal_logo.png" alt="virustotal"&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;First, install &lt;a href="https://www.rust-lang.org/tools/install"&gt;Rust&lt;/a&gt; (using the recommended rustup installation method) and then&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cargo install osintui
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Configuration&lt;/h2&gt; 
&lt;p&gt;osintui expects a TOML configuration file stored at &lt;code&gt;~/.osintui/config/config.toml&lt;/code&gt; that sets the necessary API tokens for each service. The configuration file will be created for you on first run if one was not found.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;[keys]
virustotal = "api_key"
shodan = "api_key"
censys_id = "api_id"
censys_secret = "api_key"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Hotkeys&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Key&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;h&lt;/td&gt; 
   &lt;td&gt;Home&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;/&lt;/td&gt; 
   &lt;td&gt;Input&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;q&lt;/td&gt; 
   &lt;td&gt;Back&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;c&lt;/td&gt; 
   &lt;td&gt;Censys&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;s&lt;/td&gt; 
   &lt;td&gt;Shodan&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;v&lt;/td&gt; 
   &lt;td&gt;Virustotal&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;‚Üí&lt;/td&gt; 
   &lt;td&gt;Move Right&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;‚Üê&lt;/td&gt; 
   &lt;td&gt;Move Left&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;‚Üë&lt;/td&gt; 
   &lt;td&gt;Move Up&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;‚Üì&lt;/td&gt; 
   &lt;td&gt;Move Down&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;p&gt;‚≠ê &lt;strong&gt;&lt;a href="https://github.com/Rigellute/spotify-tui"&gt;spotify-tui&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The software architecture is almost entirely modeled after spotify-tui. The codebase was invaluable in learning how to cleanly manage complex TUI state and implement generic handling of TUI components.&lt;/p&gt; 
&lt;p&gt;‚≠ê &lt;strong&gt;&lt;a href="https://github.com/pirxthepilot/wtfis"&gt;wtfis&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;I needed a good first project to learn rust and wtfis was the primary source of inspiration for osintui.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>apache/datafusion</title>
      <link>https://github.com/apache/datafusion</link>
      <description>&lt;p&gt;Apache DataFusion SQL Query Engine&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Apache DataFusion&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://crates.io/crates/datafusion"&gt;&lt;img src="https://img.shields.io/crates/v/datafusion.svg?sanitize=true" alt="Crates.io"&gt;&lt;/a&gt; &lt;a href="https://github.com/apache/datafusion/raw/main/LICENSE.txt"&gt;&lt;img src="https://img.shields.io/badge/license-Apache%20v2-blue.svg?sanitize=true" alt="Apache licensed"&gt;&lt;/a&gt; &lt;a href="https://github.com/apache/datafusion/actions?query=branch%3Amain"&gt;&lt;img src="https://github.com/apache/datafusion/actions/workflows/rust.yml/badge.svg?sanitize=true" alt="Build Status"&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/commit-activity/m/apache/datafusion" alt="Commit Activity"&gt; &lt;a href="https://github.com/apache/datafusion/issues"&gt;&lt;img src="https://img.shields.io/github/issues-raw/apache/datafusion" alt="Open Issues"&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/Qw5gKqHxUM"&gt;&lt;img src="https://img.shields.io/badge/Chat-Discord-purple" alt="Discord chat"&gt;&lt;/a&gt; &lt;a href="https://www.linkedin.com/company/apache-datafusion/"&gt;&lt;img src="https://img.shields.io/badge/Follow-Linkedin-blue" alt="Linkedin"&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/crates/msrv/datafusion?label=Min%20Rust%20Version" alt="Crates.io MSRV"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://datafusion.apache.org/"&gt;Website&lt;/a&gt; | &lt;a href="https://docs.rs/datafusion/latest/datafusion/"&gt;API Docs&lt;/a&gt; | &lt;a href="https://discord.com/channels/885562378132000778/885562378132000781"&gt;Chat&lt;/a&gt;&lt;/p&gt; 
&lt;a href="https://datafusion.apache.org/"&gt; &lt;img src="https://github.com/apache/datafusion/raw/HEAD/docs/source/_static/images/2x_bgwhite_original.png" width="512" alt="logo"&gt; &lt;/a&gt; 
&lt;p&gt;DataFusion is an extensible query engine written in &lt;a href="http://rustlang.org"&gt;Rust&lt;/a&gt; that uses &lt;a href="https://arrow.apache.org"&gt;Apache Arrow&lt;/a&gt; as its in-memory format.&lt;/p&gt; 
&lt;p&gt;This crate provides libraries and binaries for developers building fast and feature rich database and analytic systems, customized to particular workloads. See &lt;a href="https://datafusion.apache.org/user-guide/introduction.html#use-cases"&gt;use cases&lt;/a&gt; for examples. The following related subprojects target end users:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/apache/datafusion-python/"&gt;DataFusion Python&lt;/a&gt; offers a Python interface for SQL and DataFrame queries.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/apache/datafusion-ray/"&gt;DataFusion Ray&lt;/a&gt; provides a distributed version of DataFusion that scales out on Ray clusters.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/apache/datafusion-comet/"&gt;DataFusion Comet&lt;/a&gt; is an accelerator for Apache Spark based on DataFusion.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;"Out of the box," DataFusion offers [SQL] and [&lt;code&gt;Dataframe&lt;/code&gt;] APIs, excellent &lt;a href="https://benchmark.clickhouse.com/"&gt;performance&lt;/a&gt;, built-in support for CSV, Parquet, JSON, and Avro, extensive customization, and a great community.&lt;/p&gt; 
&lt;p&gt;DataFusion features a full query planner, a columnar, streaming, multi-threaded, vectorized execution engine, and partitioned data sources. You can customize DataFusion at almost all points including additional data sources, query languages, functions, custom operators and more. See the &lt;a href="https://datafusion.apache.org/contributor-guide/architecture.html"&gt;Architecture&lt;/a&gt; section for more details.&lt;/p&gt; 
&lt;p&gt;Here are links to some important information&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://datafusion.apache.org/"&gt;Project Site&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://datafusion.apache.org/user-guide/cli/installation.html"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://datafusion.apache.org/user-guide/example-usage.html"&gt;Rust Getting Started&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://datafusion.apache.org/user-guide/dataframe.html"&gt;Rust DataFrame API&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.rs/datafusion/latest/datafusion"&gt;Rust API docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/apache/datafusion/tree/main/datafusion-examples"&gt;Rust Examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arrow.apache.org/datafusion-python/"&gt;Python DataFrame API&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.rs/datafusion/latest/datafusion/index.html#architecture"&gt;Architecture&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What can you do with this crate?&lt;/h2&gt; 
&lt;p&gt;DataFusion is great for building projects such as domain specific query engines, new database platforms and data pipelines, query languages and more. It lets you start quickly from a fully working engine, and then customize those features specific to your use. &lt;a href="https://datafusion.apache.org/user-guide/introduction.html#known-users"&gt;Click Here&lt;/a&gt; to see a list known users.&lt;/p&gt; 
&lt;h2&gt;Contributing to DataFusion&lt;/h2&gt; 
&lt;p&gt;Please see the &lt;a href="https://datafusion.apache.org/contributor-guide"&gt;contributor guide&lt;/a&gt; and &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html"&gt;communication&lt;/a&gt; pages for more information.&lt;/p&gt; 
&lt;h2&gt;Crate features&lt;/h2&gt; 
&lt;p&gt;This crate has several &lt;a href="https://doc.rust-lang.org/cargo/reference/features.html"&gt;features&lt;/a&gt; which can be specified in your &lt;code&gt;Cargo.toml&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Default features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;nested_expressions&lt;/code&gt;: functions for working with nested type function such as &lt;code&gt;array_to_string&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;compression&lt;/code&gt;: reading files compressed with &lt;code&gt;xz2&lt;/code&gt;, &lt;code&gt;bzip2&lt;/code&gt;, &lt;code&gt;flate2&lt;/code&gt;, and &lt;code&gt;zstd&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;crypto_expressions&lt;/code&gt;: cryptographic functions such as &lt;code&gt;md5&lt;/code&gt; and &lt;code&gt;sha256&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;datetime_expressions&lt;/code&gt;: date and time functions such as &lt;code&gt;to_timestamp&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;encoding_expressions&lt;/code&gt;: &lt;code&gt;encode&lt;/code&gt; and &lt;code&gt;decode&lt;/code&gt; functions&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;parquet&lt;/code&gt;: support for reading the &lt;a href="https://parquet.apache.org/"&gt;Apache Parquet&lt;/a&gt; format&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;parquet_encryption&lt;/code&gt;: support for using &lt;a href="https://parquet.apache.org/docs/file-format/data-pages/encryption/"&gt;Parquet Modular Encryption&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;regex_expressions&lt;/code&gt;: regular expression functions, such as &lt;code&gt;regexp_match&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;unicode_expressions&lt;/code&gt;: Include unicode aware functions such as &lt;code&gt;character_length&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;unparser&lt;/code&gt;: enables support to reverse LogicalPlans back into SQL&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;recursive_protection&lt;/code&gt;: uses &lt;a href="https://docs.rs/recursive/latest/recursive/"&gt;recursive&lt;/a&gt; for stack overflow protection.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Optional features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;avro&lt;/code&gt;: support for reading the &lt;a href="https://avro.apache.org/"&gt;Apache Avro&lt;/a&gt; format&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;backtrace&lt;/code&gt;: include backtrace information in error messages&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;pyarrow&lt;/code&gt;: conversions between PyArrow and DataFusion types&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;serde&lt;/code&gt;: enable arrow-schema's &lt;code&gt;serde&lt;/code&gt; feature&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;DataFusion API Evolution and Deprecation Guidelines&lt;/h2&gt; 
&lt;p&gt;Public methods in Apache DataFusion evolve over time: while we try to maintain a stable API, we also improve the API over time. As a result, we typically deprecate methods before removing them, according to the &lt;a href="https://datafusion.apache.org/library-user-guide/api-health.html"&gt;deprecation guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Dependencies and &lt;code&gt;Cargo.lock&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;Following the &lt;a href="https://blog.rust-lang.org/2023/08/29/committing-lockfiles.html"&gt;guidance&lt;/a&gt; on committing &lt;code&gt;Cargo.lock&lt;/code&gt; files, this project commits its &lt;code&gt;Cargo.lock&lt;/code&gt; file.&lt;/p&gt; 
&lt;p&gt;CI uses the committed &lt;code&gt;Cargo.lock&lt;/code&gt; file, and dependencies are updated regularly using &lt;a href="https://docs.github.com/en/code-security/dependabot/working-with-dependabot"&gt;Dependabot&lt;/a&gt; PRs.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>tursodatabase/turso</title>
      <link>https://github.com/tursodatabase/turso</link>
      <description>&lt;p&gt;Turso Database is a project to build the next evolution of SQLite.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/tursodatabase/turso/main/turso.png" alt="Turso Database" width="800"&gt; &lt;/p&gt;
&lt;h1 align="center"&gt;Turso Database&lt;/h1&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;i&gt;Turso Database&lt;/i&gt; is an in-process SQL database, compatible with SQLite. &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a title="Build Status" target="_blank" href="https://github.com/tursodatabase/turso/actions/workflows/rust.yml"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/tursodatabase/turso/rust.yml?style=flat-square"&gt;&lt;/a&gt; &lt;a title="Releases" target="_blank" href="https://github.com/tursodatabase/turso/releases"&gt;&lt;img src="https://img.shields.io/github/release/tursodatabase/turso?style=flat-square&amp;amp;color=9CF"&gt;&lt;/a&gt; &lt;a title="Rust" target="_blank" href="https://crates.io/crates/turso"&gt;&lt;img alt="PyPI" src="https://img.shields.io/crates/v/turso"&gt;&lt;/a&gt; &lt;a title="JavaScript" target="_blank" href="https://www.npmjs.com/package/@tursodatabase/turso"&gt;&lt;img alt="PyPI" src="https://img.shields.io/npm/v/@tursodatabase/turso"&gt;&lt;/a&gt; &lt;a title="Python" target="_blank" href="https://pypi.org/project/pyturso/"&gt;&lt;img alt="PyPI" src="https://img.shields.io/pypi/v/pyturso"&gt;&lt;/a&gt; &lt;a title="MIT" target="_blank" href="https://github.com/tursodatabase/turso/raw/main/LICENSE.md"&gt;&lt;img src="http://img.shields.io/badge/license-MIT-orange.svg?style=flat-square"&gt;&lt;/a&gt; &lt;br&gt; &lt;a title="GitHub Pull Requests" target="_blank" href="https://github.com/tursodatabase/turso/pulls"&gt;&lt;img src="https://img.shields.io/github/issues-pr-closed/tursodatabase/turso.svg?style=flat-square&amp;amp;color=FF9966"&gt;&lt;/a&gt; &lt;a title="GitHub Commits" target="_blank" href="https://github.com/tursodatabase/turso/commits/main"&gt;&lt;img src="https://img.shields.io/github/commit-activity/m/tursodatabase/turso.svg?style=flat-square"&gt;&lt;/a&gt; &lt;a title="Last Commit" target="_blank" href="https://github.com/tursodatabase/turso/commits/main"&gt;&lt;img src="https://img.shields.io/github/last-commit/tursodatabase/turso.svg?style=flat-square&amp;amp;color=FF9900"&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a title="Developer's Discord" target="_blank" href="https://discord.gg/jgjmyYgHwB"&gt;&lt;img alt="Chat with the Core Developers on Discord" src="https://img.shields.io/discord/1258658826257961020?label=Discord&amp;amp;logo=Discord&amp;amp;style=social&amp;amp;label=Core%20Developers"&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a title="Users's Discord" target="_blank" href="https://tur.so/discord"&gt;&lt;img alt="Chat with other users of Turso (and Turso Cloud) on Discord" src="https://img.shields.io/discord/933071162680958986?label=Discord&amp;amp;logo=Discord&amp;amp;style=social&amp;amp;label=Users"&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;hr&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;Turso Database is a &lt;em&gt;work-in-progress&lt;/em&gt;, in-process OLTP database engine library written in Rust that has:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;SQLite compatibility&lt;/strong&gt; for SQL dialect, file formats, and the C API [see &lt;a href="https://raw.githubusercontent.com/tursodatabase/turso/main/COMPAT.md"&gt;document&lt;/a&gt; for details]&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Change data capture (CDC)&lt;/strong&gt; for real-time tracking of database changes.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Language support&lt;/strong&gt; for 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tursodatabase/turso/main/bindings/go"&gt;Go&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tursodatabase/turso/main/bindings/javascript"&gt;JavaScript&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tursodatabase/turso/main/bindings/java"&gt;Java&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tursodatabase/turso/main/bindings/python"&gt;Python&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tursodatabase/turso/main/bindings/rust"&gt;Rust&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tursodatabase/turso/main/bindings/wasm"&gt;WebAssembly&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Asynchronous I/O&lt;/strong&gt; support on Linux with &lt;code&gt;io_uring&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;OS support&lt;/strong&gt; for Linux, macOS, and Windows&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Roadmap&lt;/h3&gt; 
&lt;p&gt;The following features are on our current roadmap:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;BEGIN CONCURRENT&lt;/code&gt;&lt;/strong&gt; for improved write throughput using multi-version concurrency control (MVCC).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Better schema management&lt;/strong&gt;, including extended &lt;code&gt;ALTER&lt;/code&gt; support, faster schema changes, and strict column types by default.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Incremental computation&lt;/strong&gt; using DBSP to implement query subscriptions, incremental view maintenance, and triggers.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Vector indexing&lt;/strong&gt; for fast approximate vector search, similar to &lt;a href="https://turso.tech/vector"&gt;libSQL vector search&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Please see the &lt;a href="https://raw.githubusercontent.com/tursodatabase/turso/main/docs/manual.md"&gt;Turso Database Manual&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;üíª Command Line&lt;/summary&gt; 
 &lt;br&gt; You can install the latest `turso` release with: 
 &lt;pre&gt;&lt;code class="language-shell"&gt;curl --proto '=https' --tlsv1.2 -LsSf \
  https://github.com/tursodatabase/turso/releases/latest/download/turso_cli-installer.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Then launch the shell to execute SQL statements:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-console"&gt;Turso
Enter ".help" for usage hints.
Connected to a transient in-memory database.
Use ".open FILENAME" to reopen on a persistent database
turso&amp;gt; CREATE TABLE users (id INT, username TEXT);
turso&amp;gt; INSERT INTO users VALUES (1, 'alice');
turso&amp;gt; INSERT INTO users VALUES (2, 'bob');
turso&amp;gt; SELECT * FROM users;
1|alice
2|bob
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;You can also build and run the latest development version with:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;cargo run
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;MCP Server Mode&lt;/h3&gt; 
 &lt;p&gt;The Turso CLI includes a built-in &lt;a href="https://modelcontextprotocol.io/"&gt;Model Context Protocol (MCP)&lt;/a&gt; server that allows AI assistants to interact with your databases. Start the MCP server with:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;tursodb your_database.db --mcp
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;The MCP server provides seven tools for database interaction:&lt;/p&gt; 
 &lt;h4&gt;Available Tools&lt;/h4&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;code&gt;list_tables&lt;/code&gt;&lt;/strong&gt; - List all tables in the database&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;code&gt;describe_table&lt;/code&gt;&lt;/strong&gt; - Describe the structure of a specific table&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;code&gt;execute_query&lt;/code&gt;&lt;/strong&gt; - Execute read-only SELECT queries&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;code&gt;insert_data&lt;/code&gt;&lt;/strong&gt; - Insert new data into tables&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;code&gt;update_data&lt;/code&gt;&lt;/strong&gt; - Update existing data in tables&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;code&gt;delete_data&lt;/code&gt;&lt;/strong&gt; - Delete data from tables&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;code&gt;schema_change&lt;/code&gt;&lt;/strong&gt; - Execute schema modification statements (CREATE TABLE, ALTER TABLE, DROP TABLE)&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h4&gt;Example Usage&lt;/h4&gt; 
 &lt;p&gt;The MCP server runs as a single process that handles multiple JSON-RPC requests over stdin/stdout. Here's how to interact with it:&lt;/p&gt; 
 &lt;h4&gt;Example with In-Memory Database&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;cat &amp;lt;&amp;lt; 'EOF' | tursodb --mcp
{"jsonrpc": "2.0", "id": 1, "method": "initialize", "params": {"protocolVersion": "2024-11-05", "capabilities": {}, "clientInfo": {"name": "client", "version": "1.0"}}}
{"jsonrpc": "2.0", "id": 2, "method": "tools/call", "params": {"name": "schema_change", "arguments": {"query": "CREATE TABLE users (id INTEGER, name TEXT, email TEXT)"}}}
{"jsonrpc": "2.0", "id": 3, "method": "tools/call", "params": {"name": "list_tables", "arguments": {}}}
{"jsonrpc": "2.0", "id": 4, "method": "tools/call", "params": {"name": "insert_data", "arguments": {"query": "INSERT INTO users VALUES (1, 'Alice', 'alice@example.com')"}}}
{"jsonrpc": "2.0", "id": 5, "method": "tools/call", "params": {"name": "execute_query", "arguments": {"query": "SELECT * FROM users"}}}
EOF
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example with Existing Database&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Working with an existing database file
cat &amp;lt;&amp;lt; 'EOF' | tursodb mydb.db --mcp
{"jsonrpc": "2.0", "id": 1, "method": "initialize", "params": {"protocolVersion": "2024-11-05", "capabilities": {}, "clientInfo": {"name": "client", "version": "1.0"}}}
{"jsonrpc": "2.0", "id": 2, "method": "tools/call", "params": {"name": "list_tables", "arguments": {}}}
EOF
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ü¶Ä Rust&lt;/summary&gt; 
 &lt;br&gt; 
 &lt;pre&gt;&lt;code class="language-console"&gt;cargo add turso
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Example usage:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-rust"&gt;let db = Builder::new_local("sqlite.db").build().await?;
let conn = db.connect()?;

let res = conn.query("SELECT * FROM users", ()).await?;
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;‚ú® JavaScript&lt;/summary&gt; 
 &lt;br&gt; 
 &lt;pre&gt;&lt;code class="language-console"&gt;npm i @tursodatabase/turso
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Example usage:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-js"&gt;import { Database } from '@tursodatabase/turso';

const db = new Database('sqlite.db');
const stmt = db.prepare('SELECT * FROM users');
const users = stmt.all();
console.log(users);
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;üêç Python&lt;/summary&gt; 
 &lt;br&gt; 
 &lt;pre&gt;&lt;code class="language-console"&gt;pip install pyturso
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Example usage:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import turso

con = turso.connect("sqlite.db")
cur = con.cursor()
res = cur.execute("SELECT * FROM users")
print(res.fetchone())
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ü¶´ Go&lt;/summary&gt; 
 &lt;br&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Clone the repository&lt;/li&gt; 
  &lt;li&gt;Build the library and set your LD_LIBRARY_PATH to include turso's target directory&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-console"&gt;cargo build --package limbo-go
export LD_LIBRARY_PATH=/path/to/limbo/target/debug:$LD_LIBRARY_PATH
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="3"&gt; 
  &lt;li&gt;Use the driver&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-console"&gt;go get github.com/tursodatabase/turso
go install github.com/tursodatabase/turso
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Example usage:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-go"&gt;import (
    "database/sql"
    _ "github.com/tursodatabase/turso"
)

conn, _ = sql.Open("sqlite3", "sqlite.db")
defer conn.Close()

stmt, _ := conn.Prepare("select * from users")
defer stmt.Close()

rows, _ = stmt.Query()
for rows.Next() {
    var id int
    var username string
    _ := rows.Scan(&amp;amp;id, &amp;amp;username)
    fmt.Printf("User: ID: %d, Username: %s\n", id, username)
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;‚òïÔ∏è Java&lt;/summary&gt; 
 &lt;br&gt; 
 &lt;p&gt;We integrated Turso Database into JDBC. For detailed instructions on how to use Turso Database with java, please refer to the &lt;a href="https://raw.githubusercontent.com/tursodatabase/turso/main/bindings/java/README.md"&gt;README.md under bindings/java&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We'd love to have you contribute to Turso Database! Please check out the &lt;a href="https://raw.githubusercontent.com/tursodatabase/turso/main/CONTRIBUTING.md"&gt;contribution guide&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;h3&gt;Found a data corruption bug? Get up to $1,000.00&lt;/h3&gt; 
&lt;p&gt;SQLite is loved because it is the most reliable database in the world. The next evolution of SQLite has to match or surpass this level of reliability. Turso is built with &lt;a href="https://raw.githubusercontent.com/tursodatabase/turso/main/simulator/"&gt;Deterministic Simulation Testing&lt;/a&gt; from the ground up, and is also tested by &lt;a href="https://antithesis.com"&gt;Antithesis&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Even during Alpha, if you find a bug that leads to a data corruption and demonstrate how our simulator failed to catch it, you can get up to $1,000.00. As the project matures we will increase the size of the prize, and the scope of the bugs.&lt;/p&gt; 
&lt;p&gt;More details &lt;a href="https://turso.algora.io"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can see an example of an awarded case on &lt;a href="https://github.com/tursodatabase/turso/issues/2049"&gt;#2049&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Turso core staff are not eligible.&lt;/p&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;h3&gt;Is Turso Database ready for production use?&lt;/h3&gt; 
&lt;p&gt;Turso Database is currently under heavy development and is &lt;strong&gt;not&lt;/strong&gt; ready for production use.&lt;/p&gt; 
&lt;h3&gt;How is Turso Database different from Turso's libSQL?&lt;/h3&gt; 
&lt;p&gt;Turso Database is a project to build the next evolution of SQLite in Rust, with a strong open contribution focus and features like native async support, vector search, and more. The libSQL project is also an attempt to evolve SQLite in a similar direction, but through a fork rather than a rewrite.&lt;/p&gt; 
&lt;p&gt;Rewriting SQLite in Rust started as an unassuming experiment, and due to its incredible success, replaces libSQL as our intended direction. At this point, libSQL is production ready, Turso Database is not - although it is evolving rapidly. More details &lt;a href="https://turso.tech/blog/we-will-rewrite-sqlite-and-we-are-going-all-in"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Publications&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Pekka Enberg, Sasu Tarkoma, Jon Crowcroft Ashwin Rao (2024). Serverless Runtime / Database Co-Design With Asynchronous I/O. In &lt;em&gt;EdgeSys ‚Äò24&lt;/em&gt;. &lt;a href="https://penberg.org/papers/penberg-edgesys24.pdf"&gt;[PDF]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Pekka Enberg, Sasu Tarkoma, and Ashwin Rao (2023). Towards Database and Serverless Runtime Co-Design. In &lt;em&gt;CoNEXT-SW ‚Äô23&lt;/em&gt;. [&lt;a href="https://penberg.org/papers/penberg-conext-sw-23.pdf"&gt;PDF&lt;/a&gt;] [&lt;a href="https://penberg.org/papers/penberg-conext-sw-23-slides.pdf"&gt;Slides&lt;/a&gt;]&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the &lt;a href="https://raw.githubusercontent.com/tursodatabase/turso/main/LICENSE.md"&gt;MIT license&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Contribution&lt;/h3&gt; 
&lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in Turso Database by you, shall be licensed as MIT, without any additional terms or conditions.&lt;/p&gt; 
&lt;h2&gt;Partners&lt;/h2&gt; 
&lt;p&gt;Thanks to all the partners of Turso!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://antithesis.com/"&gt;&lt;img src="https://raw.githubusercontent.com/tursodatabase/turso/main/assets/antithesis.jpg" width="400"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://blacksmith.sh"&gt;&lt;img src="https://raw.githubusercontent.com/tursodatabase/turso/main/assets/blacksmith.svg?sanitize=true" width="400"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://nyrkio.com/"&gt;&lt;img src="https://raw.githubusercontent.com/tursodatabase/turso/main/assets/turso-nyrkio.png" width="400"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;Thanks to all the contributors to Turso Database!&lt;/p&gt; 
&lt;a href="https://github.com/tursodatabase/turso/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=tursodatabase/turso"&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>EFForg/rayhunter</title>
      <link>https://github.com/EFForg/rayhunter</link>
      <description>&lt;p&gt;Rust tool to detect cell site simulators on an orbic mobile hotspot&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://www.eff.org/files/styles/media_browser_preview/public/banner_library/rayhunter-banner.png" alt="Rayhunter Logo - An Orca taking a bite out of a cellular signal bar"&gt;&lt;/p&gt; 
&lt;h1&gt;Rayhunter&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://github.com/EFForg/rayhunter/actions/workflows/main.yml/badge.svg?sanitize=true" alt="Tests"&gt;&lt;/p&gt; 
&lt;p&gt;Rayhunter is an IMSI Catcher Catcher for the Orbic mobile hotspot. To learn more, check out the &lt;a href="https://efforg.github.io/rayhunter/"&gt;Rayhunter Book&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>modelcontextprotocol/rust-sdk</title>
      <link>https://github.com/modelcontextprotocol/rust-sdk</link>
      <description>&lt;p&gt;The official Rust SDK for the Model Context Protocol&lt;/p&gt;&lt;hr&gt;&lt;div align="right"&gt; 
 &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/rust-sdk/main/docs/readme/README.zh-cn.md"&gt;ÁÆÄ‰Ωì‰∏≠Êñá(ÂæÖÊõ¥Êñ∞)&lt;/a&gt; 
&lt;/div&gt; 
&lt;h1&gt;RMCP&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://crates.io/crates/rmcp"&gt;&lt;img src="https://img.shields.io/crates/v/rmcp" alt="Crates.io Version"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- ![Release status](https://github.com/modelcontextprotocol/rust-sdk/actions/workflows/release.yml/badge.svg) --&gt; 
&lt;!-- [![docs.rs](todo)](todo) --&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/modelcontextprotocol/rust-sdk/main/docs/coverage.svg?sanitize=true" alt="Coverage"&gt;&lt;/p&gt; 
&lt;p&gt;An official Rust Model Context Protocol SDK implementation with tokio async runtime.&lt;/p&gt; 
&lt;p&gt;This repository contains the following crates:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/rust-sdk/main/crates/rmcp"&gt;rmcp&lt;/a&gt;: The core crate providing the RMCP protocol implementation( If you want to get more information, please visit &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/rust-sdk/main/crates/rmcp/README.md"&gt;rmcp&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/rust-sdk/main/crates/rmcp-macros"&gt;rmcp-macros&lt;/a&gt;: A procedural macro crate for generating RMCP tool implementations(If you want to get more information, please visit &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/rust-sdk/main/crates/rmcp-macros/README.md"&gt;rmcp-macros&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Import the crate&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;rmcp = { version = "0.2.0", features = ["server"] }
## or dev channel
rmcp = { git = "https://github.com/modelcontextprotocol/rust-sdk", branch = "main" }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Third Dependencies&lt;/h3&gt; 
&lt;p&gt;Basic dependencies:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tokio-rs/tokio"&gt;tokio required&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/serde-rs/serde"&gt;serde required&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Build a Client&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;Start a client&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-rust,"&gt;use rmcp::{ServiceExt, transport::{TokioChildProcess, ConfigureCommandExt}};
use tokio::process::Command;

#[tokio::main]
async fn main() -&amp;gt; Result&amp;lt;(), Box&amp;lt;dyn std::error::Error&amp;gt;&amp;gt; {
    let client = ().serve(TokioChildProcess::new(Command::new("npx").configure(|cmd| {
        cmd.arg("-y").arg("@modelcontextprotocol/server-everything");
    }))?).await?;
    Ok(())
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Build a Server&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;Build a transport&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-rust,"&gt;use tokio::io::{stdin, stdout};
let transport = (stdin(), stdout());
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Build a service&lt;/summary&gt; 
 &lt;p&gt;You can easily build a service by using &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/rust-sdk/main/crates/rmcp/src/handler/server.rs"&gt;&lt;code&gt;ServerHandler&lt;/code&gt;&lt;/a&gt; or &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/rust-sdk/main/crates/rmcp/src/handler/client.rs"&gt;&lt;code&gt;ClientHandler&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-rust,"&gt;let service = common::counter::Counter::new();
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Start the server&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-rust,"&gt;// this call will finish the initialization process
let server = service.serve(transport).await?;
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Interact with the server&lt;/summary&gt; 
 &lt;p&gt;Once the server is initialized, you can send requests or notifications:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-rust,"&gt;// request
let roots = server.list_roots().await?;

// or send notification
server.notify_cancelled(...).await?;
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Waiting for service shutdown&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-rust,"&gt;let quit_reason = server.waiting().await?;
// or cancel it
let quit_reason = server.cancel().await?;
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/rust-sdk/main/examples/README.md"&gt;examples&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;OAuth Support&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/rust-sdk/main/docs/OAUTH_SUPPORT.md"&gt;oauth_support&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h2&gt;Related Resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://spec.modelcontextprotocol.io/specification/2024-11-05/"&gt;MCP Specification&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/modelcontextprotocol/specification/raw/main/schema/2024-11-05/schema.ts"&gt;Schema&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Related Projects&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jokemanfire/mcp-containerd"&gt;containerd-mcp-server&lt;/a&gt; - A containerd-based MCP server implementation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;h3&gt;Tips for Contributors&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/rust-sdk/main/docs/CONTRIBUTE.MD"&gt;docs/CONTRIBUTE.MD&lt;/a&gt; to get some tips for contributing.&lt;/p&gt; 
&lt;h3&gt;Using Dev Container&lt;/h3&gt; 
&lt;p&gt;If you want to use dev container, see &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/rust-sdk/main/docs/DEVCONTAINER.md"&gt;docs/DEVCONTAINER.md&lt;/a&gt; for instructions on using Dev Container for development.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>launchbadge/sqlx</title>
      <link>https://github.com/launchbadge/sqlx</link>
      <description>&lt;p&gt;üß∞ The Rust SQL Toolkit. An async, pure Rust SQL crate featuring compile-time checked queries without a DSL. Supports PostgreSQL, MySQL, and SQLite.&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt;SQLx&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;strong&gt; üß∞ The Rust SQL Toolkit &lt;/strong&gt; 
&lt;/div&gt; 
&lt;br&gt; 
&lt;div align="center"&gt; 
 &lt;!-- Github Actions --&gt; 
 &lt;a href="https://github.com/launchbadge/sqlx/actions/workflows/sqlx.yml?query=branch%3Amain"&gt; &lt;img src="https://img.shields.io/github/actions/workflow/status/launchbadge/sqlx/sqlx.yml?branch=main&amp;amp;style=flat-square" alt="actions status"&gt;&lt;/a&gt; 
 &lt;!-- Version --&gt; 
 &lt;a href="https://crates.io/crates/sqlx"&gt; &lt;img src="https://img.shields.io/crates/v/sqlx.svg?style=flat-square" alt="Crates.io version"&gt;&lt;/a&gt; 
 &lt;!-- Discord --&gt; 
 &lt;a href="https://discord.gg/uuruzJ7"&gt; &lt;img src="https://img.shields.io/discord/665528275556106240?style=flat-square" alt="chat"&gt;&lt;/a&gt; 
 &lt;!-- Docs --&gt; 
 &lt;a href="https://docs.rs/sqlx"&gt; &lt;img src="https://img.shields.io/badge/docs-latest-blue.svg?style=flat-square" alt="docs.rs docs"&gt;&lt;/a&gt; 
 &lt;!-- Downloads --&gt; 
 &lt;a href="https://crates.io/crates/sqlx"&gt; &lt;img src="https://img.shields.io/crates/d/sqlx.svg?style=flat-square" alt="Download"&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;h4&gt; &lt;a href="https://raw.githubusercontent.com/launchbadge/sqlx/main/#install"&gt; Install &lt;/a&gt; &lt;span&gt; | &lt;/span&gt; &lt;a href="https://raw.githubusercontent.com/launchbadge/sqlx/main/#usage"&gt; Usage &lt;/a&gt; &lt;span&gt; | &lt;/span&gt; &lt;a href="https://docs.rs/sqlx"&gt; Docs &lt;/a&gt; &lt;span&gt; | &lt;/span&gt; &lt;a href="https://github.com/launchbadge/sqlx/wiki/Ecosystem"&gt; Ecosystem &lt;/a&gt; &lt;span&gt; | &lt;/span&gt; &lt;a href="https://discord.gg/uuruzJ7"&gt; Discord &lt;/a&gt; &lt;/h4&gt; 
&lt;/div&gt; 
&lt;br&gt; 
&lt;div align="center"&gt; 
 &lt;small&gt;Built with ‚ù§Ô∏è by &lt;a href="https://launchbadge.com"&gt;The LaunchBadge team&lt;/a&gt;&lt;/small&gt; 
&lt;/div&gt; 
&lt;br&gt; 
&lt;div align="center"&gt; 
 &lt;h5&gt;Have a question? Be sure to &lt;a href="https://raw.githubusercontent.com/launchbadge/sqlx/main/FAQ.md"&gt;check the FAQ first!&lt;/a&gt;&lt;/h5&gt; 
&lt;/div&gt; 
&lt;br&gt; 
&lt;p&gt;SQLx is an async, pure Rust&lt;sub&gt;‚Ä†&lt;/sub&gt; SQL crate featuring compile-time checked queries without a DSL.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Truly Asynchronous&lt;/strong&gt;. Built from the ground-up using async/await for maximum concurrency.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Compile-time checked queries&lt;/strong&gt; (if you want). See &lt;a href="https://raw.githubusercontent.com/launchbadge/sqlx/main/#sqlx-is-not-an-orm"&gt;SQLx is not an ORM&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Database Agnostic&lt;/strong&gt;. Support for &lt;a href="http://postgresql.org/"&gt;PostgreSQL&lt;/a&gt;, &lt;a href="https://www.mysql.com/"&gt;MySQL&lt;/a&gt;, &lt;a href="https://www.mariadb.org/"&gt;MariaDB&lt;/a&gt;, &lt;a href="https://sqlite.org/"&gt;SQLite&lt;/a&gt;.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://www.microsoft.com/en-us/sql-server"&gt;MSSQL&lt;/a&gt; was supported prior to version 0.7, but has been removed pending a full rewrite of the driver as part of our &lt;a href="https://github.com/launchbadge/sqlx/discussions/1616"&gt;SQLx Pro initiative&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Pure Rust&lt;/strong&gt;. The Postgres and MySQL/MariaDB drivers are written in pure Rust using &lt;strong&gt;zero&lt;/strong&gt; unsafe&lt;sub&gt;‚Ä†‚Ä†&lt;/sub&gt; code.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Runtime Agnostic&lt;/strong&gt;. Works on different runtimes (&lt;a href="https://github.com/async-rs/async-std"&gt;&lt;code&gt;async-std&lt;/code&gt;&lt;/a&gt; / &lt;a href="https://github.com/tokio-rs/tokio"&gt;&lt;code&gt;tokio&lt;/code&gt;&lt;/a&gt; / &lt;a href="https://github.com/actix/actix-net"&gt;&lt;code&gt;actix&lt;/code&gt;&lt;/a&gt;) and TLS backends (&lt;a href="https://crates.io/crates/native-tls"&gt;&lt;code&gt;native-tls&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://crates.io/crates/rustls"&gt;&lt;code&gt;rustls&lt;/code&gt;&lt;/a&gt;).&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;small&gt;&lt;small&gt;&lt;/small&gt;&lt;/small&gt;&lt;/p&gt;
&lt;small&gt;&lt;small&gt; &lt;p&gt;‚Ä† The SQLite driver uses the libsqlite3 C library as SQLite is an embedded database (the only way we could be pure Rust for SQLite is by porting &lt;em&gt;all&lt;/em&gt; of SQLite to Rust).&lt;/p&gt; &lt;p&gt;‚Ä†‚Ä† SQLx uses &lt;code&gt;#![forbid(unsafe_code)]&lt;/code&gt; unless the &lt;code&gt;sqlite&lt;/code&gt; feature is enabled. The SQLite driver directly invokes the SQLite3 API via &lt;code&gt;libsqlite3-sys&lt;/code&gt;, which requires &lt;code&gt;unsafe&lt;/code&gt;.&lt;/p&gt; &lt;/small&gt;&lt;/small&gt;
&lt;p&gt;&lt;small&gt;&lt;small&gt;&lt;/small&gt;&lt;/small&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Cross-platform. Being native Rust, SQLx will compile anywhere Rust is supported.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Built-in connection pooling with &lt;code&gt;sqlx::Pool&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Row streaming. Data is read asynchronously from the database and decoded on demand.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Automatic statement preparation and caching. When using the high-level query API (&lt;code&gt;sqlx::query&lt;/code&gt;), statements are prepared and cached per connection.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Simple (unprepared) query execution including fetching results into the same &lt;code&gt;Row&lt;/code&gt; types used by the high-level API. Supports batch execution and returns results from all statements.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Transport Layer Security (TLS) where supported (&lt;a href="https://www.mysql.com/"&gt;MySQL&lt;/a&gt;, &lt;a href="https://www.mariadb.org/"&gt;MariaDB&lt;/a&gt; and &lt;a href="http://postgresql.org/"&gt;PostgreSQL&lt;/a&gt;).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Asynchronous notifications using &lt;code&gt;LISTEN&lt;/code&gt; and &lt;code&gt;NOTIFY&lt;/code&gt; for &lt;a href="http://postgresql.org/"&gt;PostgreSQL&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Nested transactions with support for save points.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Any&lt;/code&gt; database driver for changing the database driver at runtime. An &lt;code&gt;AnyPool&lt;/code&gt; connects to the driver indicated by the URL scheme.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Install&lt;/h2&gt; 
&lt;p&gt;SQLx is compatible with the &lt;a href="https://github.com/async-rs/async-std"&gt;&lt;code&gt;async-std&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://github.com/tokio-rs/tokio"&gt;&lt;code&gt;tokio&lt;/code&gt;&lt;/a&gt;, and &lt;a href="https://github.com/actix/actix-net"&gt;&lt;code&gt;actix&lt;/code&gt;&lt;/a&gt; runtimes; and, the &lt;a href="https://crates.io/crates/native-tls"&gt;&lt;code&gt;native-tls&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://crates.io/crates/rustls"&gt;&lt;code&gt;rustls&lt;/code&gt;&lt;/a&gt; TLS backends. When adding the dependency, you must choose a runtime feature that is &lt;code&gt;runtime&lt;/code&gt; + &lt;code&gt;tls&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;# Cargo.toml
[dependencies]
# PICK ONE OF THE FOLLOWING:

# tokio (no TLS)
sqlx = { version = "0.8", features = [ "runtime-tokio" ] }
# tokio + native-tls
sqlx = { version = "0.8", features = [ "runtime-tokio", "tls-native-tls" ] }
# tokio + rustls with ring and WebPKI CA certificates
sqlx = { version = "0.8", features = [ "runtime-tokio", "tls-rustls-ring-webpki" ] }
# tokio + rustls with ring and platform's native CA certificates
sqlx = { version = "0.8", features = [ "runtime-tokio", "tls-rustls-ring-native-roots" ] }
# tokio + rustls with aws-lc-rs
sqlx = { version = "0.8", features = [ "runtime-tokio", "tls-rustls-aws-lc-rs" ] }

# async-std (no TLS)
sqlx = { version = "0.8", features = [ "runtime-async-std" ] }
# async-std + native-tls
sqlx = { version = "0.8", features = [ "runtime-async-std", "tls-native-tls" ] }
# async-std + rustls with ring and WebPKI CA certificates
sqlx = { version = "0.8", features = [ "runtime-async-std", "tls-rustls-ring-webpki" ] }
# async-std + rustls with ring and platform's native CA certificates
sqlx = { version = "0.8", features = [ "runtime-async-std", "tls-rustls-ring-native-roots" ] }
# async-std + rustls with aws-lc-rs
sqlx = { version = "0.8", features = [ "runtime-async-std", "tls-rustls-aws-lc-rs" ] }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Cargo Feature Flags&lt;/h4&gt; 
&lt;p&gt;For backward-compatibility reasons, the runtime and TLS features can either be chosen together as a single feature, or separately.&lt;/p&gt; 
&lt;p&gt;For forward compatibility, you should use the separate runtime and TLS features as the combination features may be removed in the future.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;runtime-async-std&lt;/code&gt;: Use the &lt;code&gt;async-std&lt;/code&gt; runtime without enabling a TLS backend.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;runtime-tokio&lt;/code&gt;: Use the &lt;code&gt;tokio&lt;/code&gt; runtime without enabling a TLS backend.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Actix-web is fully compatible with Tokio and so a separate runtime feature is no longer needed.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;tls-native-tls&lt;/code&gt;: Use the &lt;code&gt;native-tls&lt;/code&gt; TLS backend (OpenSSL on *nix, SChannel on Windows, Secure Transport on macOS).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;tls-rustls&lt;/code&gt;: Use the &lt;code&gt;rustls&lt;/code&gt; TLS backend (cross-platform backend, only supports TLS 1.2 and 1.3).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;postgres&lt;/code&gt;: Add support for the Postgres database server.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;mysql&lt;/code&gt;: Add support for the MySQL/MariaDB database server.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;mssql&lt;/code&gt;: Add support for the MSSQL database server.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;sqlite&lt;/code&gt;: Add support for the self-contained &lt;a href="https://sqlite.org/"&gt;SQLite&lt;/a&gt; database engine with SQLite bundled and statically-linked.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;sqlite-unbundled&lt;/code&gt;: The same as above (&lt;code&gt;sqlite&lt;/code&gt;), but link SQLite from the system instead of the bundled version.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Allows updating SQLite independently of SQLx or using forked versions.&lt;/li&gt; 
   &lt;li&gt;You must have SQLite installed on the system or provide a path to the library at build time. See &lt;a href="https://github.com/rusqlite/rusqlite?tab=readme-ov-file#notes-on-building-rusqlite-and-libsqlite3-sys"&gt;the &lt;code&gt;rusqlite&lt;/code&gt; README&lt;/a&gt; for details.&lt;/li&gt; 
   &lt;li&gt;May result in link errors if the SQLite version is too old. Version &lt;code&gt;3.20.0&lt;/code&gt; or newer is recommended.&lt;/li&gt; 
   &lt;li&gt;Can increase build time due to the use of bindgen.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;sqlite-preupdate-hook&lt;/code&gt;: enables SQLite's &lt;a href="https://sqlite.org/c3ref/preupdate_count.html"&gt;preupdate hook&lt;/a&gt; API.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Exposed as a separate feature because it's generally not enabled by default.&lt;/li&gt; 
   &lt;li&gt;Using this feature with &lt;code&gt;sqlite-unbundled&lt;/code&gt; may cause linker failures if the system SQLite version does not support it.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;any&lt;/code&gt;: Add support for the &lt;code&gt;Any&lt;/code&gt; database driver, which can proxy to a database driver at runtime.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;derive&lt;/code&gt;: Add support for the derive family macros, those are &lt;code&gt;FromRow&lt;/code&gt;, &lt;code&gt;Type&lt;/code&gt;, &lt;code&gt;Encode&lt;/code&gt;, &lt;code&gt;Decode&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;macros&lt;/code&gt;: Add support for the &lt;code&gt;query*!&lt;/code&gt; macros, which allows compile-time checked queries.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;migrate&lt;/code&gt;: Add support for the migration management and &lt;code&gt;migrate!&lt;/code&gt; macro, which allow compile-time embedded migrations.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;uuid&lt;/code&gt;: Add support for UUID.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;chrono&lt;/code&gt;: Add support for date and time types from &lt;code&gt;chrono&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;time&lt;/code&gt;: Add support for date and time types from &lt;code&gt;time&lt;/code&gt; crate (alternative to &lt;code&gt;chrono&lt;/code&gt;, which is preferred by &lt;code&gt;query!&lt;/code&gt; macro, if both enabled)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;bstr&lt;/code&gt;: Add support for &lt;code&gt;bstr::BString&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;bigdecimal&lt;/code&gt;: Add support for &lt;code&gt;NUMERIC&lt;/code&gt; using the &lt;code&gt;bigdecimal&lt;/code&gt; crate.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;rust_decimal&lt;/code&gt;: Add support for &lt;code&gt;NUMERIC&lt;/code&gt; using the &lt;code&gt;rust_decimal&lt;/code&gt; crate.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;ipnet&lt;/code&gt;: Add support for &lt;code&gt;INET&lt;/code&gt; and &lt;code&gt;CIDR&lt;/code&gt; (in postgres) using the &lt;code&gt;ipnet&lt;/code&gt; crate.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;ipnetwork&lt;/code&gt;: Add support for &lt;code&gt;INET&lt;/code&gt; and &lt;code&gt;CIDR&lt;/code&gt; (in postgres) using the &lt;code&gt;ipnetwork&lt;/code&gt; crate.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;json&lt;/code&gt;: Add support for &lt;code&gt;JSON&lt;/code&gt; and &lt;code&gt;JSONB&lt;/code&gt; (in postgres) using the &lt;code&gt;serde_json&lt;/code&gt; crate.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Offline mode is now always enabled. See &lt;a href="https://raw.githubusercontent.com/launchbadge/sqlx/main/sqlx-cli/README.md#enable-building-in-offline-mode-with-query"&gt;sqlx-cli/README.md&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;SQLx is not an ORM!&lt;/h2&gt; 
&lt;p&gt;SQLx supports &lt;strong&gt;compile-time checked queries&lt;/strong&gt;. It does not, however, do this by providing a Rust API or DSL (domain-specific language) for building queries. Instead, it provides macros that take regular SQL as input and ensure that it is valid for your database. The way this works is that SQLx connects to your development DB at compile time to have the database itself verify (and return some info on) your SQL queries. This has some potentially surprising implications:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Since SQLx never has to parse the SQL string itself, any syntax that the development DB accepts can be used (including things added by database extensions)&lt;/li&gt; 
 &lt;li&gt;Due to the different amount of information databases let you retrieve about queries, the extent of SQL verification you get from the query macros depends on the database&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;If you are looking for an (asynchronous) ORM,&lt;/strong&gt; you can check out our new &lt;a href="https://github.com/launchbadge/sqlx/wiki/Ecosystem#orms"&gt;Ecosystem wiki page&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;See the &lt;code&gt;examples/&lt;/code&gt; folder for more in-depth usage.&lt;/p&gt; 
&lt;h3&gt;Quickstart&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;use sqlx::postgres::PgPoolOptions;
// use sqlx::mysql::MySqlPoolOptions;
// etc.

#[async_std::main] // Requires the `attributes` feature of `async-std`
// or #[tokio::main]
// or #[actix_web::main]
async fn main() -&amp;gt; Result&amp;lt;(), sqlx::Error&amp;gt; {
    // Create a connection pool
    //  for MySQL/MariaDB, use MySqlPoolOptions::new()
    //  for SQLite, use SqlitePoolOptions::new()
    //  etc.
    let pool = PgPoolOptions::new()
        .max_connections(5)
        .connect("postgres://postgres:password@localhost/test").await?;

    // Make a simple query to return the given parameter (use a question mark `?` instead of `$1` for MySQL/MariaDB)
    let row: (i64,) = sqlx::query_as("SELECT $1")
        .bind(150_i64)
        .fetch_one(&amp;amp;pool).await?;

    assert_eq!(row.0, 150);

    Ok(())
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Connecting&lt;/h3&gt; 
&lt;p&gt;A single connection can be established using any of the database connection types and calling &lt;code&gt;connect()&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;use sqlx::Connection;

let conn = SqliteConnection::connect("sqlite::memory:").await?;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Generally, you will want to instead create a connection pool (&lt;code&gt;sqlx::Pool&lt;/code&gt;) for the application to regulate how many server-side connections it's using.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;let pool = MySqlPool::connect("mysql://user:pass@host/database").await?;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Querying&lt;/h3&gt; 
&lt;p&gt;In SQL, queries can be separated into prepared (parameterized) or unprepared (simple). Prepared queries have their query plan &lt;em&gt;cached&lt;/em&gt;, use a binary mode of communication (lower bandwidth and faster decoding), and utilize parameters to avoid SQL injection. Unprepared queries are simple and intended only for use where a prepared statement will not work, such as various database commands (e.g., &lt;code&gt;PRAGMA&lt;/code&gt; or &lt;code&gt;SET&lt;/code&gt; or &lt;code&gt;BEGIN&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;SQLx supports all operations with both types of queries. In SQLx, a &lt;code&gt;&amp;amp;str&lt;/code&gt; is treated as an unprepared query, and a &lt;code&gt;Query&lt;/code&gt; or &lt;code&gt;QueryAs&lt;/code&gt; struct is treated as a prepared query.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;// low-level, Executor trait
conn.execute("BEGIN").await?; // unprepared, simple query
conn.execute(sqlx::query("DELETE FROM table")).await?; // prepared, cached query
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We should prefer to use the high-level &lt;code&gt;query&lt;/code&gt; interface whenever possible. To make this easier, there are finalizers on the type to avoid the need to wrap with an executor.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;sqlx::query("DELETE FROM table").execute(&amp;amp;mut conn).await?;
sqlx::query("DELETE FROM table").execute(&amp;amp;pool).await?;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;execute&lt;/code&gt; query finalizer returns the number of affected rows, if any, and drops all received results. In addition, there are &lt;code&gt;fetch&lt;/code&gt;, &lt;code&gt;fetch_one&lt;/code&gt;, &lt;code&gt;fetch_optional&lt;/code&gt;, and &lt;code&gt;fetch_all&lt;/code&gt; to receive results.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;Query&lt;/code&gt; type returned from &lt;code&gt;sqlx::query&lt;/code&gt; will return &lt;code&gt;Row&amp;lt;'conn&amp;gt;&lt;/code&gt; from the database. Column values can be accessed by ordinal or by name with &lt;code&gt;row.get()&lt;/code&gt;. As the &lt;code&gt;Row&lt;/code&gt; retains an immutable borrow on the connection, only one &lt;code&gt;Row&lt;/code&gt; may exist at a time.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;fetch&lt;/code&gt; query finalizer returns a stream-like type that iterates through the rows in the result sets.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;// provides `try_next`
use futures_util::TryStreamExt;
// provides `try_get`
use sqlx::Row;

let mut rows = sqlx::query("SELECT * FROM users WHERE email = ?")
    .bind(email)
    .fetch(&amp;amp;mut conn);

while let Some(row) = rows.try_next().await? {
    // map the row into a user-defined domain type
    let email: &amp;amp;str = row.try_get("email")?;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To assist with mapping the row into a domain type, one of two idioms may be used:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;let mut stream = sqlx::query("SELECT * FROM users")
    .map(|row: PgRow| {
        // map the row into a user-defined domain type
    })
    .fetch(&amp;amp;mut conn);
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;#[derive(sqlx::FromRow)]
struct User { name: String, id: i64 }

let mut stream = sqlx::query_as::&amp;lt;_, User&amp;gt;("SELECT * FROM users WHERE email = ? OR name = ?")
    .bind(user_email)
    .bind(user_name)
    .fetch(&amp;amp;mut conn);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Instead of a stream of results, we can use &lt;code&gt;fetch_one&lt;/code&gt; or &lt;code&gt;fetch_optional&lt;/code&gt; to request one required or optional result from the database.&lt;/p&gt; 
&lt;h3&gt;Compile-time verification&lt;/h3&gt; 
&lt;p&gt;We can use the macro, &lt;code&gt;sqlx::query!&lt;/code&gt; to achieve compile-time syntactic and semantic verification of the SQL, with an output to an anonymous record type where each SQL column is a Rust field (using raw identifiers where needed).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;let countries = sqlx::query!(
        "
SELECT country, COUNT(*) as count
FROM users
GROUP BY country
WHERE organization = ?
        ",
        organization
    )
    .fetch_all(&amp;amp;pool) // -&amp;gt; Vec&amp;lt;{ country: String, count: i64 }&amp;gt;
    .await?;

// countries[0].country
// countries[0].count
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Differences from &lt;code&gt;query()&lt;/code&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;The input (or bind) parameters must be given all at once (and they are compile-time validated to be the right number and the right type).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The output type is an anonymous record. In the above example the type would be similar to:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-rust"&gt;{ country: String, count: i64 }
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The &lt;code&gt;DATABASE_URL&lt;/code&gt; environment variable must be set at build time to a database which it can prepare queries against; the database does not have to contain any data but must be the same kind (MySQL, Postgres, etc.) and have the same schema as the database you will be connecting to at runtime.&lt;/p&gt; &lt;p&gt;For convenience, you can use &lt;a href="https://github.com/dotenv-rs/dotenv#examples"&gt;a &lt;code&gt;.env&lt;/code&gt; file&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt; to set DATABASE_URL so that you don't have to pass it every time:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;DATABASE_URL=mysql://localhost/my_database
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The biggest downside to &lt;code&gt;query!()&lt;/code&gt; is that the output type cannot be named (due to Rust not officially supporting anonymous records). To address that, there is a &lt;code&gt;query_as!()&lt;/code&gt; macro that is mostly identical except that you can name the output type.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;// no traits are needed
struct Country { country: String, count: i64 }

let countries = sqlx::query_as!(Country,
        "
SELECT country, COUNT(*) as count
FROM users
GROUP BY country
WHERE organization = ?
        ",
        organization
    )
    .fetch_all(&amp;amp;pool) // -&amp;gt; Vec&amp;lt;Country&amp;gt;
    .await?;

// countries[0].country
// countries[0].count
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To avoid the need of having a development database around to compile the project even when no modifications (to the database-accessing parts of the code) are done, you can enable "offline mode" to cache the results of the SQL query analysis using the &lt;code&gt;sqlx&lt;/code&gt; command-line tool. See &lt;a href="https://raw.githubusercontent.com/launchbadge/sqlx/main/sqlx-cli/README.md#enable-building-in-offline-mode-with-query"&gt;sqlx-cli/README.md&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Compile-time verified queries do quite a bit of work at compile time. Incremental actions like &lt;code&gt;cargo check&lt;/code&gt; and &lt;code&gt;cargo build&lt;/code&gt; can be significantly faster when using an optimized build by putting the following in your &lt;code&gt;Cargo.toml&lt;/code&gt; (More information in the &lt;a href="https://doc.rust-lang.org/cargo/reference/profiles.html"&gt;Profiles section&lt;/a&gt; of The Cargo Book)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;[profile.dev.package.sqlx-macros]
opt-level = 3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; The &lt;code&gt;dotenv&lt;/code&gt; crate itself appears abandoned as of &lt;a href="https://github.com/dotenv-rs/dotenv/issues/74"&gt;December 2021&lt;/a&gt; so we now use the &lt;code&gt;dotenvy&lt;/code&gt; crate instead. The file format is the same.&lt;/p&gt; 
&lt;h2&gt;Safety&lt;/h2&gt; 
&lt;p&gt;This crate uses &lt;code&gt;#![forbid(unsafe_code)]&lt;/code&gt; to ensure everything is implemented in 100% Safe Rust.&lt;/p&gt; 
&lt;p&gt;If the &lt;code&gt;sqlite&lt;/code&gt; feature is enabled, this is downgraded to &lt;code&gt;#![deny(unsafe_code)]&lt;/code&gt; with &lt;code&gt;#![allow(unsafe_code)]&lt;/code&gt; on the &lt;code&gt;sqlx::sqlite&lt;/code&gt; module. There are several places where we interact with the C SQLite API. We try to document each call for the invariants we're assuming. We absolutely welcome auditing of, and feedback on, our unsafe code usage.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Licensed under either of&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Apache License, Version 2.0 (&lt;a href="https://raw.githubusercontent.com/launchbadge/sqlx/main/LICENSE-APACHE"&gt;LICENSE-APACHE&lt;/a&gt; or &lt;a href="http://www.apache.org/licenses/LICENSE-2.0"&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;MIT license (&lt;a href="https://raw.githubusercontent.com/launchbadge/sqlx/main/LICENSE-MIT"&gt;LICENSE-MIT&lt;/a&gt; or &lt;a href="http://opensource.org/licenses/MIT"&gt;http://opensource.org/licenses/MIT&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;at your option.&lt;/p&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;p&gt;Unless you explicitly state otherwise, any Contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>tracel-ai/burn</title>
      <link>https://github.com/tracel-ai/burn</link>
      <description>&lt;p&gt;Burn is a next generation Deep Learning Framework that doesn't compromise on flexibility, efficiency and portability.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/tracel-ai/burn/main/assets/logo-burn-neutral.webp" width="350px"&gt; 
 &lt;p&gt;&lt;a href="https://discord.gg/uPEBbYYDB6"&gt;&lt;img src="https://img.shields.io/discord/1038839012602941528.svg?color=7289da&amp;amp;&amp;amp;logo=discord" alt="Discord"&gt;&lt;/a&gt; &lt;a href="https://crates.io/crates/burn"&gt;&lt;img src="https://img.shields.io/crates/v/burn.svg?sanitize=true" alt="Current Crates.io Version"&gt;&lt;/a&gt; &lt;a href="https://crates.io/crates/burn"&gt;&lt;img src="https://img.shields.io/crates/msrv/burn" alt="Minimum Supported Rust Version"&gt;&lt;/a&gt; &lt;a href="https://burn.dev/docs/burn"&gt;&lt;img src="https://img.shields.io/badge/docs-latest-blue" alt="Documentation"&gt;&lt;/a&gt; &lt;a href="https://github.com/tracel-ai/burn/actions/workflows/test.yml"&gt;&lt;img src="https://github.com/tracel-ai/burn/actions/workflows/test.yml/badge.svg?sanitize=true" alt="Test Status"&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/#license"&gt;&lt;img src="https://shields.io/badge/license-MIT%2FApache--2.0-blue" alt="license"&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/tracel-ai/burn"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.runblaze.dev"&gt;&lt;img src="https://www.runblaze.dev/ci-blaze-powered.png" width="125px"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;hr&gt; 
 &lt;p&gt;&lt;strong&gt;Burn is a next generation Deep Learning Framework that doesn't compromise on &lt;br&gt; flexibility, efficiency and portability.&lt;/strong&gt;&lt;/p&gt; 
 &lt;br&gt; 
&lt;/div&gt; 
&lt;div align="left"&gt; 
 &lt;h2&gt;Performance&lt;/h2&gt; 
 &lt;div align="left"&gt; 
  &lt;img align="right" src="https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-blazingly-fast.png" height="96px"&gt; 
  &lt;p&gt;Because we believe the goal of a deep learning framework is to convert computation into useful intelligence, we have made performance a core pillar of Burn. We strive to achieve top efficiency by leveraging multiple optimization techniques described below.&lt;/p&gt; 
  &lt;p&gt;&lt;strong&gt;Click on each section for more details&lt;/strong&gt; üëá&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;br&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Automatic kernel fusion üí• &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;Using Burn means having your models optimized on any backend. When possible, we provide a way to automatically and dynamically create custom kernels that minimize data relocation between different memory spaces, extremely useful when moving memory is the bottleneck.&lt;/p&gt; 
  &lt;p&gt;As an example, you could write your own GELU activation function with the high level tensor api (see Rust code snippet below).&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-rust"&gt;fn gelu_custom&amp;lt;B: Backend, const D: usize&amp;gt;(x: Tensor&amp;lt;B, D&amp;gt;) -&amp;gt; Tensor&amp;lt;B, D&amp;gt; {
    let x = x.clone() * ((x / SQRT_2).erf() + 1);
    x / 2
}
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;Then, at runtime, a custom low-level kernel will be automatically created for your specific implementation and will rival a handcrafted GPU implementation. The kernel consists of about 60 lines of WGSL &lt;a href="%22https://www.w3.org/TR/WGSL/https://www.w3.org/TR/WGSL/%22"&gt;WebGPU Shading Language&lt;/a&gt;, an extremely verbose lower level shader language you probably don't want to program your deep learning models in!&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Asynchronous execution ‚ù§Ô∏è‚Äçüî• &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;For &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/#backends"&gt;first-party backends&lt;/a&gt;, an asynchronous execution style is used, which allows to perform various optimizations, such as the previously mentioned automatic kernel fusion.&lt;/p&gt; 
  &lt;p&gt;Asynchronous execution also ensures that the normal execution of the framework does not block the model computations, which implies that the framework overhead won't impact the speed of execution significantly. Conversely, the intense computations in the model do not interfere with the responsiveness of the framework. For more information about our asynchronous backends, see &lt;a href="https://burn.dev/blog/creating-high-performance-asynchronous-backends-with-burn-compute"&gt;this blog post&lt;/a&gt;.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Thread-safe building blocks ü¶û &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;Burn emphasizes thread safety by leveraging the &lt;a href="https://doc.rust-lang.org/book/ch04-00-understanding-ownership.html"&gt;ownership system of Rust&lt;/a&gt;. With Burn, each module is the owner of its weights. It is therefore possible to send a module to another thread for computing the gradients, then send the gradients to the main thread that can aggregate them, and &lt;em&gt;voil√†&lt;/em&gt;, you get multi-device training.&lt;/p&gt; 
  &lt;p&gt;This is a very different approach from what PyTorch does, where backpropagation actually mutates the &lt;em&gt;grad&lt;/em&gt; attribute of each tensor parameter. This is not a thread-safe operation and therefore requires lower level synchronization primitives, see &lt;a href="https://pytorch.org/docs/stable/distributed.html"&gt;distributed training&lt;/a&gt; for reference. Note that this is still very fast, but not compatible across different backends and quite hard to implement.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Intelligent memory management ü¶Ä &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;One of the main roles of a deep learning framework is to reduce the amount of memory necessary to run models. The naive way of handling memory is that each tensor has its own memory space, which is allocated when the tensor is created then deallocated as the tensor gets out of scope. However, allocating and deallocating data is very costly, so a memory pool is often required to achieve good throughput. Burn offers an infrastructure that allows for easily creating and selecting memory management strategies for backends. For more details on memory management in Burn, see &lt;a href="https://burn.dev/blog/creating-high-performance-asynchronous-backends-with-burn-compute"&gt;this blog post&lt;/a&gt;.&lt;/p&gt; 
  &lt;p&gt;Another very important memory optimization of Burn is that we keep track of when a tensor can be mutated in-place just by using the ownership system well. Even though it is a rather small memory optimization on its own, it adds up considerably when training or running inference with larger models and contributes to reduce the memory usage even more. For more information, see &lt;a href="https://burn.dev/blog/burn-rusty-approach-to-tensor-handling"&gt;this blog post about tensor handling&lt;/a&gt;.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Automatic kernel selection üéØ &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;A good deep learning framework should ensure that models run smoothly on all hardware. However, not all hardware share the same behavior in terms of execution speed. For instance, a matrix multiplication kernel can be launched with many different parameters, which are highly sensitive to the size of the matrices and the hardware. Using the wrong configuration could reduce the speed of execution by a large factor (10 times or even more in extreme cases), so choosing the right kernels becomes a priority.&lt;/p&gt; 
  &lt;p&gt;With our home-made backends, we run benchmarks automatically and choose the best configuration for the current hardware and matrix sizes with a reasonable caching strategy.&lt;/p&gt; 
  &lt;p&gt;This adds a small overhead by increasing the warmup execution time, but stabilizes quickly after a few forward and backward passes, saving lots of time in the long run. Note that this feature isn't mandatory, and can be disabled when cold starts are a priority over optimized throughput.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Hardware specific features üî• &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;It is no secret that deep learning is mostly relying on matrix multiplication as its core operation, since this is how fully-connected neural networks are modeled.&lt;/p&gt; 
  &lt;p&gt;More and more, hardware manufacturers optimize their chips specifically for matrix multiplication workloads. For instance, Nvidia has its &lt;em&gt;Tensor Cores&lt;/em&gt; and today most cellphones have AI specialized chips. As of this moment, we support Tensor Cores with our LibTorch, Candle, CUDA, Metal and WGPU/SPIR-V backends, but not other accelerators yet. We hope &lt;a href="https://github.com/gpuweb/gpuweb/issues/4195"&gt;this issue&lt;/a&gt; gets resolved at some point to bring support to our WGPU backend.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Custom Backend Extension üéí &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;Burn aims to be the most flexible deep learning framework. While it's crucial to maintain compatibility with a wide variety of backends, Burn also provides the ability to extend the functionalities of a backend implementation to suit your personal modeling requirements.&lt;/p&gt; 
  &lt;p&gt;This versatility is advantageous in numerous ways, such as supporting custom operations like flash attention or manually writing your own kernel for a specific backend to enhance performance. See &lt;a href="https://burn.dev/books/burn/advanced/backend-extension/index.html"&gt;this section&lt;/a&gt; in the Burn Book üî• for more details.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br&gt; 
 &lt;h2&gt;Backend&lt;/h2&gt; 
 &lt;div align="left"&gt; 
  &lt;img align="right" src="https://raw.githubusercontent.com/tracel-ai/burn/main/assets/backend-chip.png" height="96px"&gt; 
  &lt;p&gt;Burn strives to be as fast as possible on as many hardwares as possible, with robust implementations. We believe this flexibility is crucial for modern needs where you may train your models in the cloud, then deploy on customer hardwares, which vary from user to user.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;br&gt; 
 &lt;p&gt;&lt;strong&gt;Supported Backends&lt;/strong&gt;&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Backend&lt;/th&gt; 
    &lt;th&gt;Devices&lt;/th&gt; 
    &lt;th&gt;Class&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CUDA&lt;/td&gt; 
    &lt;td&gt;NVIDIA GPUs&lt;/td&gt; 
    &lt;td&gt;First-Party&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;ROCm&lt;/td&gt; 
    &lt;td&gt;AMD GPUs&lt;/td&gt; 
    &lt;td&gt;First-Party&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Metal&lt;/td&gt; 
    &lt;td&gt;Apple GPUs&lt;/td&gt; 
    &lt;td&gt;First-Party&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Vulkan&lt;/td&gt; 
    &lt;td&gt;Most GPUs on Linux &amp;amp; Windows&lt;/td&gt; 
    &lt;td&gt;First-Party&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Wgpu&lt;/td&gt; 
    &lt;td&gt;Most GPUs&lt;/td&gt; 
    &lt;td&gt;First-Party&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;NdArray&lt;/td&gt; 
    &lt;td&gt;Most CPUs&lt;/td&gt; 
    &lt;td&gt;Third-Party&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;LibTorch&lt;/td&gt; 
    &lt;td&gt;Most GPUs &amp;amp; CPUs&lt;/td&gt; 
    &lt;td&gt;Third-Party&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Candle&lt;/td&gt; 
    &lt;td&gt;Nvidia, Apple GPUs &amp;amp; CPUs&lt;/td&gt; 
    &lt;td&gt;Third-Party&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;br&gt; 
 &lt;p&gt;Compared to other frameworks, Burn has a very different approach to supporting many backends. By design, most code is generic over the Backend trait, which allows us to build Burn with swappable backends. This makes composing backend possible, augmenting them with additional functionalities such as autodifferentiation and automatic kernel fusion.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Autodiff: Backend decorator that brings backpropagation to any backend üîÑ &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;Contrary to the aforementioned backends, Autodiff is actually a backend &lt;em&gt;decorator&lt;/em&gt;. This means that it cannot exist by itself; it must encapsulate another backend.&lt;/p&gt; 
  &lt;p&gt;The simple act of wrapping a base backend with Autodiff transparently equips it with autodifferentiation support, making it possible to call backward on your model.&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-rust"&gt;use burn::backend::{Autodiff, Wgpu};
use burn::tensor::{Distribution, Tensor};

fn main() {
    type Backend = Autodiff&amp;lt;Wgpu&amp;gt;;

    let x: Tensor&amp;lt;Backend, 2&amp;gt; = Tensor::random([32, 32], Distribution::Default);
    let y: Tensor&amp;lt;Backend, 2&amp;gt; = Tensor::random([32, 32], Distribution::Default).require_grad();

    let tmp = x.clone() + y.clone();
    let tmp = tmp.matmul(x);
    let tmp = tmp.exp();

    let grads = tmp.backward();
    let y_grad = y.grad(&amp;amp;grads).unwrap();
    println!("{y_grad}");
}
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;Of note, it is impossible to make the mistake of calling backward on a model that runs on a backend that does not support autodiff (for inference), as this method is only offered by an Autodiff backend.&lt;/p&gt; 
  &lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/crates/burn-autodiff/README.md"&gt;Autodiff Backend README&lt;/a&gt; for more details.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Fusion: Backend decorator that brings kernel fusion to all first-party backends &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;This backend decorator enhances a backend with kernel fusion, provided that the inner backend supports it. Note that you can compose this backend with other backend decorators such as Autodiff. For now, only the WGPU and CUDA backends have support for fused kernels.&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-rust"&gt;use burn::backend::{Autodiff, Fusion, Wgpu};
use burn::tensor::{Distribution, Tensor};

fn main() {
    type Backend = Autodiff&amp;lt;Fusion&amp;lt;Wgpu&amp;gt;&amp;gt;;

    let x: Tensor&amp;lt;Backend, 2&amp;gt; = Tensor::random([32, 32], Distribution::Default);
    let y: Tensor&amp;lt;Backend, 2&amp;gt; = Tensor::random([32, 32], Distribution::Default).require_grad();

    let tmp = x.clone() + y.clone();
    let tmp = tmp.matmul(x);
    let tmp = tmp.exp();

    let grads = tmp.backward();
    let y_grad = y.grad(&amp;amp;grads).unwrap();
    println!("{y_grad}");
}

&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;Of note, we plan to implement automatic gradient checkpointing based on compute bound and memory bound operations, which will work gracefully with the fusion backend to make your code run even faster during training, see &lt;a href="https://github.com/tracel-ai/burn/issues/936"&gt;this issue&lt;/a&gt;.&lt;/p&gt; 
  &lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/crates/burn-fusion/README.md"&gt;Fusion Backend README&lt;/a&gt; for more details.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Router (Beta): Backend decorator that composes multiple backends into a single one &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;That backend simplifies hardware operability, if for instance you want to execute some operations on the CPU and other operations on the GPU.&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-rust"&gt;use burn::tensor::{Distribution, Tensor};
use burn::backend::{
    NdArray, Router, Wgpu, ndarray::NdArrayDevice, router::duo::MultiDevice, wgpu::WgpuDevice,
};

fn main() {
    type Backend = Router&amp;lt;(Wgpu, NdArray)&amp;gt;;

    let device_0 = MultiDevice::B1(WgpuDevice::DiscreteGpu(0));
    let device_1 = MultiDevice::B2(NdArrayDevice::Cpu);

    let tensor_gpu =
        Tensor::&amp;lt;Backend, 2&amp;gt;::random([3, 3], burn::tensor::Distribution::Default, &amp;amp;device_0);
    let tensor_cpu =
        Tensor::&amp;lt;Backend, 2&amp;gt;::random([3, 3], burn::tensor::Distribution::Default, &amp;amp;device_1);
}

&lt;/code&gt;&lt;/pre&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Remote (Beta): Backend decorator for remote backend execution, useful for distributed computations &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;That backend has two parts, one client and one server. The client sends tensor operations over the network to a remote compute backend. You can use any first-party backend as server in a single line of code:&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-rust"&gt;fn main_server() {
    // Start a server on port 3000.
    burn::server::start::&amp;lt;burn::backend::Cuda&amp;gt;(Default::default(), 3000);
}

fn main_client() {
    // Create a client that communicate with the server on port 3000.
    use burn::backend::{Autodiff, RemoteBackend};

    type Backend = Autodiff&amp;lt;RemoteDevice&amp;gt;;

    let device = RemoteDevice::new("ws://localhost:3000");
    let tensor_gpu =
        Tensor::&amp;lt;Backend, 2&amp;gt;::random([3, 3], Distribution::Default, &amp;amp;device);
}

&lt;/code&gt;&lt;/pre&gt; 
 &lt;/details&gt; 
 &lt;br&gt; 
 &lt;h2&gt;Training &amp;amp; Inference&lt;/h2&gt; 
 &lt;div align="left"&gt; 
  &lt;img align="right" src="https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-wall.png" height="96px"&gt; 
  &lt;p&gt;The whole deep learning workflow is made easy with Burn, as you can monitor your training progress with an ergonomic dashboard, and run inference everywhere from embedded devices to large GPU clusters.&lt;/p&gt; 
  &lt;p&gt;Burn was built from the ground up with training and inference in mind. It's also worth noting how Burn, in comparison to frameworks like PyTorch, simplifies the transition from training to deployment, eliminating the need for code changes.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div align="center"&gt; 
  &lt;br&gt; 
  &lt;a href="https://www.youtube.com/watch?v=N9RM5CQbNQc" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/tracel-ai/burn/main/assets/burn-train-tui.png" alt="Burn Train TUI" width="75%"&gt; &lt;/a&gt; 
 &lt;/div&gt; 
 &lt;br&gt; 
 &lt;p&gt;&lt;strong&gt;Click on the following sections to expand üëá&lt;/strong&gt;&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Training Dashboard üìà &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;As you can see in the previous video (click on the picture!), a new terminal UI dashboard based on the &lt;a href="https://github.com/ratatui-org/ratatui"&gt;Ratatui&lt;/a&gt; crate allows users to follow their training with ease without having to connect to any external application.&lt;/p&gt; 
  &lt;p&gt;You can visualize your training and validation metrics updating in real-time and analyze the lifelong progression or recent history of any registered metrics using only the arrow keys. Break from the training loop without crashing, allowing potential checkpoints to be fully written or important pieces of code to complete without interruption üõ°&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; ONNX Support üê´ &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;ONNX (Open Neural Network Exchange) is an open-standard format that exports both the architecture and the weights of a deep learning model.&lt;/p&gt; 
  &lt;p&gt;Burn supports the importation of models that follow the ONNX standard so you can easily port a model you have written in another framework like TensorFlow or PyTorch to Burn to benefit from all the advantages our framework offers.&lt;/p&gt; 
  &lt;p&gt;Our ONNX support is further described in &lt;a href="https://burn.dev/books/burn/import/onnx-model.html"&gt;this section of the Burn Book üî•&lt;/a&gt;.&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This crate is in active development and currently supports a &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/crates/burn-import/SUPPORTED-ONNX-OPS.md"&gt;limited set of ONNX operators&lt;/a&gt;.&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Importing PyTorch or Safetensors Models üöö &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;You can load weights from PyTorch or Safetensors formats directly into your Burn-defined models. This makes it easy to reuse existing models while benefiting from Burn's performance and deployment features.&lt;/p&gt; 
  &lt;p&gt;Learn more:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://burn.dev/books/burn/import/pytorch-model.html"&gt;Import pre-trained PyTorch models into Burn&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://burn.dev/books/burn/import/safetensors-model.html"&gt;Load models from Safetensors format&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Inference in the Browser üåê &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;Several of our backends can compile to Web Assembly: Candle and NdArray for CPU, and WGPU for GPU. This means that you can run inference directly within a browser. We provide several examples of this:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/mnist-inference-web"&gt;MNIST&lt;/a&gt; where you can draw digits and a small convnet tries to find which one it is! 2Ô∏è‚É£ 7Ô∏è‚É£ üò∞&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/image-classification-web"&gt;Image Classification&lt;/a&gt; where you can upload images and classify them! üåÑ&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Embedded: &lt;i&gt;no_std&lt;/i&gt; support ‚öôÔ∏è &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;Burn's core components support &lt;a href="https://docs.rust-embedded.org/book/intro/no-std.html"&gt;no_std&lt;/a&gt;. This means it can run in bare metal environment such as embedded devices without an operating system.&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;As of now, only the NdArray backend can be used in a &lt;em&gt;no_std&lt;/em&gt; environment.&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/details&gt; 
 &lt;br&gt; 
 &lt;h3&gt;Benchmarks&lt;/h3&gt; 
 &lt;p&gt;To evaluate performance across different backends and track improvements over time, we provide a dedicated benchmarking suite.&lt;/p&gt; 
 &lt;p&gt;Run and compare benchmarks using &lt;a href="https://github.com/tracel-ai/burn-bench"&gt;burn-bench&lt;/a&gt;.&lt;/p&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Warning&lt;/strong&gt; When using one of the &lt;code&gt;wgpu&lt;/code&gt; backends, you may encounter compilation errors related to recursive type evaluation. This is due to complex type nesting within the &lt;code&gt;wgpu&lt;/code&gt; dependency chain. To resolve this issue, add the following line at the top of your &lt;code&gt;main.rs&lt;/code&gt; or &lt;code&gt;lib.rs&lt;/code&gt; file:&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-rust"&gt;#![recursion_limit = "256"]
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;The default recursion limit (128) is often just below the required depth (typically 130-150) due to deeply nested associated types and trait bounds.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h2&gt;Getting Started&lt;/h2&gt; 
 &lt;div align="left"&gt; 
  &lt;img align="right" src="https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-walking.png" height="96px"&gt; 
  &lt;p&gt;Just heard of Burn? You are at the right place! Just continue reading this section and we hope you can get on board really quickly.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;details&gt; 
  &lt;summary&gt; The Burn Book üî• &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;To begin working effectively with Burn, it is crucial to understand its key components and philosophy. This is why we highly recommend new users to read the first sections of &lt;a href="https://burn.dev/books/burn/"&gt;The Burn Book üî•&lt;/a&gt;. It provides detailed examples and explanations covering every facet of the framework, including building blocks like tensors, modules, and optimizers, all the way to advanced usage, like coding your own GPU kernels.&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;The project is constantly evolving, and we try as much as possible to keep the book up to date with new additions. However, we might miss some details sometimes, so if you see something weird, let us know! We also gladly accept Pull Requests üòÑ&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Examples üôè &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;Let's start with a code snippet that shows how intuitive the framework is to use! In the following, we declare a neural network module with some parameters along with its forward pass.&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-rust"&gt;use burn::nn;
use burn::module::Module;
use burn::tensor::backend::Backend;

#[derive(Module, Debug)]
pub struct PositionWiseFeedForward&amp;lt;B: Backend&amp;gt; {
    linear_inner: nn::Linear&amp;lt;B&amp;gt;,
    linear_outer: nn::Linear&amp;lt;B&amp;gt;,
    dropout: nn::Dropout,
    gelu: nn::Gelu,
}

impl&amp;lt;B: Backend&amp;gt; PositionWiseFeedForward&amp;lt;B&amp;gt; {
    pub fn forward&amp;lt;const D: usize&amp;gt;(&amp;amp;self, input: Tensor&amp;lt;B, D&amp;gt;) -&amp;gt; Tensor&amp;lt;B, D&amp;gt; {
        let x = self.linear_inner.forward(input);
        let x = self.gelu.forward(x);
        let x = self.dropout.forward(x);

        self.linear_outer.forward(x)
    }
}
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;We have a somewhat large amount of &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples"&gt;examples&lt;/a&gt; in the repository that shows how to use the framework in different scenarios.&lt;/p&gt; 
  &lt;p&gt;Following &lt;a href="https://burn.dev/books/burn/"&gt;the book&lt;/a&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/guide"&gt;Basic Workflow&lt;/a&gt; : Creates a custom CNN &lt;code&gt;Module&lt;/code&gt; to train on the MNIST dataset and use for inference.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/custom-training-loop"&gt;Custom Training Loop&lt;/a&gt; : Implements a basic training loop instead of using the &lt;code&gt;Learner&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/custom-wgpu-kernel"&gt;Custom WGPU Kernel&lt;/a&gt; : Learn how to create your own custom operation with the WGPU backend.&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;Additional examples:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/custom-csv-dataset"&gt;Custom CSV Dataset&lt;/a&gt; : Implements a dataset to parse CSV data for a regression task.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/simple-regression"&gt;Regression&lt;/a&gt; : Trains a simple MLP on the California Housing dataset to predict the median house value for a district.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/custom-image-dataset"&gt;Custom Image Dataset&lt;/a&gt; : Trains a simple CNN on custom image dataset following a simple folder structure.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/custom-renderer"&gt;Custom Renderer&lt;/a&gt; : Implements a custom renderer to display the &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/building-blocks/learner.md"&gt;&lt;code&gt;Learner&lt;/code&gt;&lt;/a&gt; progress.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/image-classification-web"&gt;Image Classification Web&lt;/a&gt; : Image classification web browser demo using Burn, WGPU and WebAssembly.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/mnist-inference-web"&gt;MNIST Inference on Web&lt;/a&gt; : An interactive MNIST inference demo in the browser. The demo is available &lt;a href="https://burn.dev/demo/"&gt;online&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/mnist"&gt;MNIST Training&lt;/a&gt; : Demonstrates how to train a custom &lt;code&gt;Module&lt;/code&gt; (MLP) with the &lt;code&gt;Learner&lt;/code&gt; configured to log metrics and keep training checkpoints.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/named-tensor"&gt;Named Tensor&lt;/a&gt; : Performs operations with the experimental &lt;code&gt;NamedTensor&lt;/code&gt; feature.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/onnx-inference"&gt;ONNX Import Inference&lt;/a&gt; : Imports an ONNX model pre-trained on MNIST to perform inference on a sample image with Burn.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/pytorch-import"&gt;PyTorch Import Inference&lt;/a&gt; : Imports a PyTorch model pre-trained on MNIST to perform inference on a sample image with Burn.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/text-classification"&gt;Text Classification&lt;/a&gt; : Trains a text classification transformer model on the AG News or DbPedia dataset. The trained model can then be used to classify a text sample.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/text-generation"&gt;Text Generation&lt;/a&gt; : Trains a text generation transformer model on the DbPedia dataset.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/wgan"&gt;Wasserstein GAN MNIST&lt;/a&gt; : Trains a WGAN model to generate new handwritten digits based on MNIST.&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;For more practical insights, you can clone the repository and run any of them directly on your computer!&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Pre-trained Models ü§ñ &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;We keep an updated and curated list of models and examples built with Burn, see the &lt;a href="https://github.com/tracel-ai/models"&gt;tracel-ai/models repository&lt;/a&gt; for more details.&lt;/p&gt; 
  &lt;p&gt;Don't see the model you want? Don't hesitate to open an issue, and we may prioritize it. Built a model using Burn and want to share it? You can also open a Pull Request and add your model under the community section!&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Why use Rust for Deep Learning? ü¶Ä &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;Deep Learning is a special form of software where you need very high level abstractions as well as extremely fast execution time. Rust is the perfect candidate for that use case since it provides zero-cost abstractions to easily create neural network modules, and fine-grained control over memory to optimize every detail.&lt;/p&gt; 
  &lt;p&gt;It's important that a framework be easy to use at a high level so that its users can focus on innovating in the AI field. However, since running models relies so heavily on computations, performance can't be neglected.&lt;/p&gt; 
  &lt;p&gt;To this day, the mainstream solution to this problem has been to offer APIs in Python, but rely on bindings to low-level languages such as C/C++. This reduces portability, increases complexity and creates frictions between researchers and engineers. We feel like Rust's approach to abstractions makes it versatile enough to tackle this two languages dichotomy.&lt;/p&gt; 
  &lt;p&gt;Rust also comes with the Cargo package manager, which makes it incredibly easy to build, test, and deploy from any environment, which is usually a pain in Python.&lt;/p&gt; 
  &lt;p&gt;Although Rust has the reputation of being a difficult language at first, we strongly believe it leads to more reliable, bug-free solutions built faster (after some practice üòÖ)!&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;Deprecation Note&lt;/strong&gt;&lt;br&gt;Since &lt;code&gt;0.14.0&lt;/code&gt;, the internal structure for tensor data has changed. The previous &lt;code&gt;Data&lt;/code&gt; struct was deprecated and officially removed since &lt;code&gt;0.17.0&lt;/code&gt; in favor of the new &lt;code&gt;TensorData&lt;/code&gt; struct, which allows for more flexibility by storing the underlying data as bytes and keeping the data type as a field. If you are using &lt;code&gt;Data&lt;/code&gt; in your code, make sure to switch to &lt;code&gt;TensorData&lt;/code&gt;.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;!-- &gt;
&gt; In the event that you are trying to load a model record saved in a previous version, make sure to
&gt; enable the `record-backward-compat` feature using a previous version of burn (&lt;=0.16.0). Otherwise,
&gt; the record won't be deserialized correctly and you will get an error message (which will also point
&gt; you to the backward compatible feature flag). The backward compatibility was maintained for
&gt; deserialization (loading), so as soon as you have saved the record again it will be saved according
&gt; to the new structure and you will be able to upgrade to this version. Please note that binary formats
&gt; are not backward compatible. Thus, you will need to load your record in a previous version and save it
&gt; to another of the self-describing record formats before using a compatible version (as described) with the
&gt; `record-backward-compat` feature flag. --&gt; 
 &lt;details id="deprecation"&gt; 
  &lt;summary&gt; Loading Model Records From Previous Versions ‚ö†Ô∏è &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;In the event that you are trying to load a model record saved in a version older than &lt;code&gt;0.14.0&lt;/code&gt;, make sure to use a compatible version (&lt;code&gt;0.14&lt;/code&gt;, &lt;code&gt;0.15&lt;/code&gt; or &lt;code&gt;0.16&lt;/code&gt;) with the &lt;code&gt;record-backward-compat&lt;/code&gt; feature flag.&lt;/p&gt; 
  &lt;pre&gt;&lt;code&gt;features = [..., "record-backward-compat"]
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;Otherwise, the record won't be deserialized correctly and you will get an error message. This error will also point you to the backward compatible feature flag.&lt;/p&gt; 
  &lt;p&gt;The backward compatibility was maintained for deserialization when loading records. Therefore, as soon as you have saved the record again it will be saved according to the new structure and you can upgrade back to the current version&lt;/p&gt; 
  &lt;p&gt;Please note that binary formats are not backward compatible. Thus, you will need to load your record in a previous version and save it in any of the other self-describing record format (e.g., using the &lt;code&gt;NamedMpkFileRecorder&lt;/code&gt;) before using a compatible version (as described) with the &lt;code&gt;record-backward-compat&lt;/code&gt; feature flag.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;h2&gt;Community&lt;/h2&gt; 
 &lt;div align="left"&gt; 
  &lt;img align="right" src="https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-community.png" height="96px"&gt; 
  &lt;p&gt;If you are excited about the project, don't hesitate to join our &lt;a href="https://discord.gg/uPEBbYYDB6"&gt;Discord&lt;/a&gt;! We try to be as welcoming as possible to everybody from any background. You can ask your questions and share what you built with the community!&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;br&gt; 
 &lt;p&gt;&lt;strong&gt;Contributing&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Before contributing, please take a moment to review our &lt;a href="https://github.com/tracel-ai/burn/tree/main/CODE-OF-CONDUCT.md"&gt;code of conduct&lt;/a&gt;. It's also highly recommended to read the &lt;a href="https://github.com/tracel-ai/burn/tree/main/contributor-book/src/project-architecture"&gt;architecture overview&lt;/a&gt;, which explains some of our architectural decisions. Refer to our &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/CONTRIBUTING.md"&gt;contributing guide&lt;/a&gt; for more details.&lt;/p&gt; 
 &lt;h2&gt;Status&lt;/h2&gt; 
 &lt;p&gt;Burn is currently in active development, and there will be breaking changes. While any resulting issues are likely to be easy to fix, there are no guarantees at this stage.&lt;/p&gt; 
 &lt;h2&gt;License&lt;/h2&gt; 
 &lt;p&gt;Burn is distributed under the terms of both the MIT license and the Apache License (Version 2.0). See &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/LICENSE-APACHE"&gt;LICENSE-APACHE&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/LICENSE-MIT"&gt;LICENSE-MIT&lt;/a&gt; for details. Opening a pull request is assumed to signal agreement with these licensing terms.&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>topgrade-rs/topgrade</title>
      <link>https://github.com/topgrade-rs/topgrade</link>
      <description>&lt;p&gt;Upgrade all the things&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt; &lt;img alt="Topgrade" src="https://raw.githubusercontent.com/topgrade-rs/topgrade/main/doc/topgrade_transparent.png" width="850px"&gt; &lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://github.com/topgrade-rs/topgrade/releases"&gt;&lt;img alt="GitHub Release" src="https://img.shields.io/github/release/topgrade-rs/topgrade.svg?sanitize=true"&gt;&lt;/a&gt; &lt;a href="https://crates.io/crates/topgrade"&gt;&lt;img alt="crates.io" src="https://img.shields.io/crates/v/topgrade.svg?sanitize=true"&gt;&lt;/a&gt; &lt;a href="https://aur.archlinux.org/packages/topgrade"&gt;&lt;img alt="AUR" src="https://img.shields.io/aur/version/topgrade.svg?sanitize=true"&gt;&lt;/a&gt; &lt;a href="https://formulae.brew.sh/formula/topgrade"&gt;&lt;img alt="Homebrew" src="https://img.shields.io/homebrew/v/topgrade.svg?sanitize=true"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;img alt="Demo" src="https://raw.githubusercontent.com/topgrade-rs/topgrade/main/doc/topgrade_demo.gif"&gt; 
&lt;/div&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; This is a fork of &lt;a href="https://github.com/r-darwish/topgrade"&gt;topgrade by r-darwish&lt;/a&gt; to keep it maintained.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Keeping your system up to date usually involves invoking multiple package managers. This results in big, non-portable shell one-liners saved in your shell. To remedy this, &lt;strong&gt;Topgrade&lt;/strong&gt; detects which tools you use and runs the appropriate commands to update them.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://repology.org/project/topgrade/versions"&gt;&lt;img src="https://repology.org/badge/vertical-allrepos/topgrade.svg?sanitize=true" alt="Packaging status"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Arch Linux: &lt;a href="https://aur.archlinux.org/packages/topgrade"&gt;AUR&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;NixOS: &lt;a href="https://search.nixos.org/packages?show=topgrade"&gt;Nixpkgs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Void Linux: &lt;a href="https://voidlinux.org/packages/?arch=x86_64&amp;amp;q=topgrade"&gt;XBPS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;macOS: &lt;a href="https://formulae.brew.sh/formula/topgrade"&gt;Homebrew&lt;/a&gt; or &lt;a href="https://ports.macports.org/port/topgrade/"&gt;MacPorts&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Windows: &lt;a href="https://community.chocolatey.org/packages/topgrade"&gt;Chocolatey&lt;/a&gt;, &lt;a href="https://scoop.sh/#/apps?q=topgrade"&gt;Scoop&lt;/a&gt; or &lt;a href="https://winstall.app/apps/topgrade-rs.topgrade"&gt;Winget&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;PyPi: &lt;a href="https://pypi.org/project/topgrade/"&gt;pip&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Fedora: &lt;a href="https://copr.fedorainfracloud.org/coprs/lilay/topgrade/"&gt;Copr&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Other systems users can either use &lt;code&gt;cargo install&lt;/code&gt; or the compiled binaries from the release page. The compiled binaries contain a self-upgrading feature.&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;Just run &lt;code&gt;topgrade&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Configuration&lt;/h2&gt; 
&lt;p&gt;See &lt;code&gt;config.example.toml&lt;/code&gt; for an example configuration file.&lt;/p&gt; 
&lt;h2&gt;Migration and Breaking Changes&lt;/h2&gt; 
&lt;p&gt;Whenever there is a &lt;strong&gt;breaking change&lt;/strong&gt;, the major version number will be bumped, and we will document these changes in the release note, please take a look at it when updated to a major release.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Got a question? Feel free to open an issue or discussion!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Configuration Path&lt;/h3&gt; 
&lt;h4&gt;&lt;code&gt;CONFIG_DIR&lt;/code&gt; on each platform&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: &lt;code&gt;%APPDATA%&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt; and &lt;strong&gt;other Unix systems&lt;/strong&gt;: &lt;code&gt;${XDG_CONFIG_HOME:-~/.config}&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;code&gt;topgrade&lt;/code&gt; will look for the configuration file in the following places, in order of priority:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;code&gt;CONFIG_DIR/topgrade.toml&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;CONFIG_DIR/topgrade/topgrade.toml&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;If the file with higher priority is present, no matter it is valid or not, the other configuration files will be ignored.&lt;/p&gt; 
&lt;p&gt;On the first run(no configuration file exists), &lt;code&gt;topgrade&lt;/code&gt; will create a configuration file at &lt;code&gt;CONFIG_DIR/topgrade.toml&lt;/code&gt; for you.&lt;/p&gt; 
&lt;h3&gt;Custom Commands&lt;/h3&gt; 
&lt;p&gt;Custom commands can be defined in the config file which can be run before, during, or after the inbuilt commands, as required. By default, the custom commands are run using a new shell according to the &lt;code&gt;$SHELL&lt;/code&gt; environment variable on unix (falls back to &lt;code&gt;sh&lt;/code&gt;) or &lt;code&gt;pwsh&lt;/code&gt; on windows (falls back to &lt;code&gt;powershell&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;On unix, if you want to run your command using an interactive shell, for example to source your shell's rc files, you can add &lt;code&gt;-i&lt;/code&gt; at the start of your custom command. But note that this requires the command to exit the shell correctly or else the shell will hang indefinitely.&lt;/p&gt; 
&lt;h2&gt;Remote Execution&lt;/h2&gt; 
&lt;p&gt;You can specify a key called &lt;code&gt;remote_topgrades&lt;/code&gt; in the configuration file. This key should contain a list of hostnames that have Topgrade installed on them. Topgrade will use &lt;code&gt;ssh&lt;/code&gt; to run &lt;code&gt;topgrade&lt;/code&gt; on remote hosts before acting locally. To limit the execution only to specific hosts use the &lt;code&gt;--remote-host-limit&lt;/code&gt; parameter.&lt;/p&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;h3&gt;Problems or missing features?&lt;/h3&gt; 
&lt;p&gt;Open a new issue describing your problem and if possible provide a solution.&lt;/p&gt; 
&lt;h3&gt;Missing a feature or found an unsupported tool/distro?&lt;/h3&gt; 
&lt;p&gt;Just let us now what you are missing by opening an issue. For tools, please open an issue describing the tool, which platforms it supports and if possible, give us an example of its usage.&lt;/p&gt; 
&lt;h3&gt;Want to contribute to the code?&lt;/h3&gt; 
&lt;p&gt;Just fork the repository and start coding.&lt;/p&gt; 
&lt;h3&gt;Contribution Guidelines&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://github.com/topgrade-rs/topgrade/raw/master/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled&gt; Add a proper testing framework to the code base.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled&gt; Add unit tests for package managers.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled&gt; Split up code into more maintainable parts, eg. putting every linux package manager in a own submodule of linux.rs.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Discord server&lt;/h2&gt; 
&lt;p&gt;Welcome to &lt;a href="https://discord.gg/Q8HGGWundY"&gt;join&lt;/a&gt; our Discord server if you want to discuss Topgrade!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>cube-js/cube</title>
      <link>https://github.com/cube-js/cube</link>
      <description>&lt;p&gt;üìä Cube‚Äôs universal semantic layer platform is the next evolution of OLAP technology for AI, BI, spreadsheets, and embedded analytics&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://cube.dev?ref=github-readme"&gt;&lt;img src="https://raw.githubusercontent.com/cube-js/cube/master/docs/content/cube-logo-with-bg.png" alt="Cube ‚Äî Semantic Layer for Data Applications" width="300px"&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;br&gt; 
&lt;p&gt;&lt;a href="https://cube.dev?ref=github-readme"&gt;Website&lt;/a&gt; ‚Ä¢ &lt;a href="https://cube.dev/docs/getting-started?ref=github-readme"&gt;Getting Started&lt;/a&gt; ‚Ä¢ &lt;a href="https://cube.dev/docs?ref=github-readme"&gt;Docs&lt;/a&gt; ‚Ä¢ &lt;a href="https://cube.dev/docs/examples?ref=github-readme"&gt;Examples&lt;/a&gt; ‚Ä¢ &lt;a href="https://cube.dev/blog?ref=github-readme"&gt;Blog&lt;/a&gt; ‚Ä¢ &lt;a href="https://slack.cube.dev?ref=github-readme"&gt;Slack&lt;/a&gt; ‚Ä¢ &lt;a href="https://twitter.com/the_cube_dev"&gt;X&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://badge.fury.io/js/%40cubejs-backend%2Fserver"&gt;&lt;img src="https://badge.fury.io/js/%40cubejs-backend%2Fserver.svg?sanitize=true" alt="npm version"&gt;&lt;/a&gt; &lt;a href="https://github.com/cube-js/cube/actions?query=workflow%3ABuild+branch%3Amaster"&gt;&lt;img src="https://github.com/cube-js/cube/workflows/Build/badge.svg?sanitize=true" alt="GitHub Actions"&gt;&lt;/a&gt; &lt;a href="https://app.fossa.io/projects/git%2Bgithub.com%2Fcube-js%2Fcube.js?ref=badge_shield"&gt;&lt;img src="https://app.fossa.io/api/projects/git%2Bgithub.com%2Fcube-js%2Fcube.js.svg?type=shield" alt="FOSSA Status"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Cube is the universal semantic layer for modern data applications.&lt;/strong&gt; Born in the cloud era, Cube represents the next evolution of OLAP technology, helping data engineers and application developers access data from modern data stores, organize it into consistent definitions, and deliver it to every application.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://ucarecdn.com/8d945f29-e9eb-4e7f-9e9e-29ae7074e195/" style="border: none" width="100%"&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;i&gt;Learn more about connecting Cube to &lt;a href="https://cube.dev/docs/config/databases?ref=github-readme" target="_blank"&gt;data sources&lt;/a&gt; and &lt;a href="https://cube.dev/docs/config/downstream?ref=github-readme" target="_blank"&gt;analytics &amp;amp; visualization tools&lt;/a&gt;.&lt;/i&gt; &lt;/p&gt; 
&lt;p&gt;Cube was designed to work with all SQL-enabled data sources, including cloud data warehouses like Snowflake or Google BigQuery, query engines like Presto or Amazon Athena, and application databases like Postgres. Cube has a built-in relational caching engine to provide sub-second latency and high concurrency for API requests.&lt;/p&gt; 
&lt;p&gt;For more details, see the &lt;a href="https://cube.dev/docs/cubejs-introduction?ref=github-readme"&gt;introduction&lt;/a&gt; page in our documentation.&lt;/p&gt; 
&lt;h2&gt;Why Cube?&lt;/h2&gt; 
&lt;p&gt;As data infrastructure evolved from traditional relational databases to cloud data platforms, OLAP capabilities that once lived in specialized servers like SQL Server Analysis Services and Oracle Essbase were left behind. Today's organizations face several challenges:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Analytics Modeling and Multidimensionality.&lt;/strong&gt; Modern cloud data platforms excel at processing large volumes of data but lack native support for multidimensional analysis and modeling. Cube brings OLAP-style analytics to these platforms, enabling consistent metric definitions and multidimensional analysis.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Performance Optimization.&lt;/strong&gt; While cloud data warehouses have improved query performance through column-oriented storage and distributed processing, they still struggle with complex analytical workloads. Cube provides intelligent caching and pre-aggregation strategies that dramatically improve query response times.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Access Control and Governance.&lt;/strong&gt; Securing and governing access to data across all consuming applications remains critical. Cube offers robust access control to ensure consistent security across your entire data ecosystem.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;API Flexibility.&lt;/strong&gt; Legacy OLAP tools were limited in how they exposed data. Cube provides modern REST, GraphQL, and SQL APIs along with support for traditional MDX and DAX interfaces, making it a truly universal semantic layer.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Cube is the missing OLAP engine for the cloud data platform era that provides the necessary infrastructure and features to implement efficient data modeling, access control, and performance optimizations without duplicating analytics modeling, data, or security permissions across different tools.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/cube-js/cube.js/master/docs/content/old-was-vs-cubejs-way.png" alt=""&gt;&lt;/p&gt; 
&lt;h2&gt;Getting Started üöÄ&lt;/h2&gt; 
&lt;h3&gt;Cube Cloud&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://cube.dev/cloud?ref=github-readme"&gt;Cube Cloud&lt;/a&gt; is the fastest way to get started with Cube. It provides managed infrastructure as well as an instant and free access for development projects and proofs of concept.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://cubecloud.dev/auth/signup?ref=github-readme"&gt;&lt;img src="https://cubedev-blog-images.s3.us-east-2.amazonaws.com/f1f1eac0-0b44-4c47-936e-33b5c06eedf0.png" alt="Get started now" width="200px"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For a step-by-step guide on Cube Cloud, &lt;a href="https://cube.dev/docs/getting-started/cloud/overview?ref=github-readme"&gt;see the docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;p&gt;Alternatively, you can get started with Cube locally or self-host it with &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Once Docker is installed, in a new folder for your project, run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -p 4000:4000 \
  -p 15432:15432 \
  -v ${PWD}:/cube/conf \
  -e CUBEJS_DEV_MODE=true \
  cubejs/cube
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, open &lt;a href="http://localhost:4000"&gt;http://localhost:4000&lt;/a&gt; in your browser to continue setup.&lt;/p&gt; 
&lt;p&gt;For a step-by-step guide on Docker, &lt;a href="https://cube.dev/docs/getting-started-docker?ref=github-readme"&gt;see the docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://cube.dev/docs?ref=github-readme"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cube.dev/docs/getting-started?ref=github-readme"&gt;Getting Started&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cube.dev/docs/examples?ref=github-readme"&gt;Examples &amp;amp; Tutorials&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cube.dev/docs/product/introduction#four-layers-of-semantic-layer"&gt;Architecture&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;There are many ways you can contribute to Cube! Here are a few possibilities:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Star this repo and follow us on &lt;a href="https://twitter.com/the_cube_dev"&gt;X&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Add Cube to your stack on &lt;a href="https://stackshare.io/cube-js"&gt;Stackshare&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Upvote issues with üëç reaction so we know what's the demand for particular issue to prioritize it within road map.&lt;/li&gt; 
 &lt;li&gt;Create issues every time you feel something is missing or goes wrong.&lt;/li&gt; 
 &lt;li&gt;Ask questions on &lt;a href="https://stackoverflow.com/questions/tagged/cube.js"&gt;Stack Overflow with cube.js tag&lt;/a&gt; if others can have these questions as well.&lt;/li&gt; 
 &lt;li&gt;Provide pull requests for all open issues and especially for those with &lt;a href="https://github.com/cube-js/cube/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22"&gt;help wanted&lt;/a&gt; and &lt;a href="https://github.com/cube-js/cube/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;good first issue&lt;/a&gt; labels.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;All sort of contributions are &lt;strong&gt;welcome and extremely helpful&lt;/strong&gt; üôå Please refer to &lt;a href="https://github.com/cube-js/cube/raw/master/CONTRIBUTING.md"&gt;the contribution guide&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Cube Client is &lt;a href="https://raw.githubusercontent.com/cube-js/cube/master/packages/cubejs-client-core/LICENSE"&gt;MIT licensed&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Cube Backend is &lt;a href="https://raw.githubusercontent.com/cube-js/cube/master/packages/cubejs-server/LICENSE"&gt;Apache 2.0 licensed&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://app.fossa.io/projects/git%2Bgithub.com%2Fcube-js%2Fcube.js?ref=badge_large"&gt;&lt;img src="https://app.fossa.io/api/projects/git%2Bgithub.com%2Fcube-js%2Fcube.js.svg?type=large" alt="FOSSA Status"&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>BoundaryML/baml</title>
      <link>https://github.com/BoundaryML/baml</link>
      <description>&lt;p&gt;The AI framework that adds the engineering to prompt engineering (Python/TS/Ruby/Java/C#/Rust/Go compatible)&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://boundaryml.com?utm_source=github" target="_blank" rel="noopener noreferrer"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="fern/assets/baml-lamb-white.png"&gt; 
   &lt;img src="https://raw.githubusercontent.com/BoundaryML/baml/canary/fern/assets/baml-lamb-white.png" height="64" id="top"&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://pypi.org/project/baml-py/"&gt;&lt;img src="https://img.shields.io/pypi/v/baml-py?color=006dad&amp;amp;label=BAML%20Version" alt="BAML Version"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;h2&gt;BAML: Basically a Made-up Language&lt;/h2&gt; 
 &lt;h4&gt; &lt;p&gt;&lt;a href="https://www.boundaryml.com/"&gt;Homepage&lt;/a&gt; | &lt;a href="https://docs.boundaryml.com"&gt;Docs&lt;/a&gt; | &lt;a href="https://www.boundaryml.com/chat"&gt;BAML AI Chat&lt;/a&gt; | &lt;a href="https://discord.gg/BTNBeXGuaS"&gt;Discord&lt;/a&gt;&lt;/p&gt; &lt;/h4&gt; 
&lt;/div&gt; 
&lt;p&gt;BAML is a simple prompting language for building reliable &lt;strong&gt;AI workflows and agents&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;BAML makes prompt engineering easy by turning it into &lt;em&gt;schema engineering&lt;/em&gt; -- where you mostly focus on the models of your prompt -- to get more reliable outputs. You don't need to write your whole app in BAML, only the prompts! You can wire-up your LLM Functions in any language of your choice! See our quickstarts for &lt;a href="https://docs.boundaryml.com/guide/installation-language/python"&gt;Python&lt;/a&gt;, &lt;a href="https://docs.boundaryml.com/guide/installation-language/typescript"&gt;TypeScript&lt;/a&gt;, &lt;a href="https://docs.boundaryml.com/guide/installation-language/ruby"&gt;Ruby&lt;/a&gt; and &lt;a href="https://docs.boundaryml.com/guide/installation-language/rest-api-other-languages"&gt;Go, and more&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;BAML comes with all batteries included -- with full typesafety, streaming, retries, wide model support, even when they don't support native &lt;a href="https://raw.githubusercontent.com/BoundaryML/baml/canary/#enable-reliable-tool-calling-with-any-model-even-when-they-dont-support-it"&gt;tool-calling APIs&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Try BAML&lt;/strong&gt;: &lt;a href="https://www.promptfiddle.com"&gt;Prompt Fiddle&lt;/a&gt; ‚Ä¢ &lt;a href="https://baml-examples.vercel.app/"&gt;Interactive App Examples&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;The core BAML principle: LLM Prompts are functions&lt;/h2&gt; 
&lt;p&gt;The fundamental building block in BAML is a function. Every prompt is a function that takes in parameters and returns a type.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;function ChatAgent(message: Message[], tone: "happy" | "sad") -&amp;gt; string
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Every function additionally defines which models it uses and what its prompt is.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;function ChatAgent(message: Message[], tone: "happy" | "sad") -&amp;gt; StopTool | ReplyTool {
    client "openai/gpt-4o-mini"

    prompt #"
        Be a {{ tone }} bot.

        {{ ctx.output_format }}

        {% for m in message %}
        {{ _.role(m.role) }}
        {{ m.content }}
        {% endfor %}
    "#
}

class Message {
    role string
    content string
}

class ReplyTool {
  response string
}

class StopTool {
  action "stop" @description(#"
    when it might be a good time to end the conversation
  "#)
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;BAML Functions can be called from any language&lt;/h2&gt; 
&lt;p&gt;Below we call the ChatAgent function we defined in BAML through Python. BAML's Rust compiler generates a "baml_client" to access and call them.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from baml_client import b
from baml_client.types import Message, StopTool

messages = [Message(role="assistant", content="How can I help?")]

while True:
  print(messages[-1].content)
  user_reply = input()
  messages.append(Message(role="user", content=user_reply))
  tool = b.ChatAgent(messages, "happy")
  if isinstance(tool, StopTool):
    print("Goodbye!")
    break
  else:
    messages.append(Message(role="assistant", content=tool.response))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can write any kind of agent or workflow using chained BAML functions. An agent is a while loop that calls a Chat BAML Function with some state.&lt;/p&gt; 
&lt;p&gt;And if you need to stream, add a couple more lines:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;stream = b.stream.ChatAgent(messages, "happy")
# partial is a Partial type with all Optional fields
for tool in stream:
    if isinstance(tool, StopTool):
      ...
    
final = stream.get_final_response()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And get fully type-safe outputs for each chunk in the stream.&lt;/p&gt; 
&lt;h2&gt;Test prompts 10x faster, right in your IDE&lt;/h2&gt; 
&lt;p&gt;BAML comes with native tooling for VSCode (jetbrains + neovim coming soon).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Visualize full prompt (including any multi-modal assets), and the API request&lt;/strong&gt;. BAML gives you full transparency and control of the prompt.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/c0b34db9-80cd-45a7-a356-6b5ab4a9c5b7" alt="raw-curl"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Using AI is all about iteration speed.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;If testing your pipeline takes 2 minutes, you can only test 10 ideas in 20 minutes.&lt;/p&gt; 
&lt;p&gt;If you reduce it to 5 seconds, you can test 240 ideas in the same amount of time. &lt;img src="https://github.com/user-attachments/assets/6fc6b8a6-ffed-4cfc-80b8-78bc8a3d66a6" alt="resume-attempt2-smaller2"&gt;&lt;/p&gt; 
&lt;p&gt;The playground also allows you to run tests in parallel -- for even faster iteration speeds üöÄ.&lt;/p&gt; 
&lt;p&gt;No need to login to websites, and no need to manually define json schemas.&lt;/p&gt; 
&lt;h2&gt;Enable reliable tool-calling with any model&lt;/h2&gt; 
&lt;p&gt;BAML works even when the models don't support native tool-calling APIs. We created the SAP (schema-aligned parsing) algorithm to support the flexible outputs LLMs can provide, like markdown within a JSON blob or chain-of-thought prior to answering. &lt;a href="https://www.boundaryml.com/blog/schema-aligned-parsing"&gt;Read more about SAP&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;With BAML, your structured outputs work in Day-1 of a model release. No need to figure out whether a model supports parallel tool calls, or whether it supports recursive schemas, or &lt;code&gt;anyOf&lt;/code&gt; or &lt;code&gt;oneOf&lt;/code&gt; etc.&lt;/p&gt; 
&lt;p&gt;See it in action with: &lt;strong&gt;&lt;a href="https://www.boundaryml.com/blog/deepseek-r1-function-calling"&gt;Deepseek-R1&lt;/a&gt;&lt;/strong&gt; and &lt;a href="https://www.boundaryml.com/blog/openai-o1"&gt;OpenAI O1&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Switch from 100s of models in a couple lines&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-diff"&gt;function Extract() -&amp;gt; Resume {
+  client openai/o3-mini
  prompt #"
    ....
  "#
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://docs.boundaryml.com/ref/llm-client-strategies/retry-policy"&gt;Retry policies&lt;/a&gt; ‚Ä¢ &lt;a href="https://docs.boundaryml.com/ref/llm-client-strategies/fallback"&gt;fallbacks&lt;/a&gt; ‚Ä¢ &lt;a href="https://docs.boundaryml.com/ref/llm-client-strategies/round-robin"&gt;model rotations&lt;/a&gt;. All statically defined. &lt;img src="https://www.boundaryml.com/blog/2025-01-24-ai-agents-need-a-new-syntax/06-fallback-retry.gif" alt="Fallback Retry"&gt; Want to do pick models at runtime? Check out the &lt;a href="https://docs.boundaryml.com/guide/baml-advanced/llm-client-registry"&gt;Client Registry&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We support: &lt;a href="https://docs.boundaryml.com/ref/llm-client-providers/open-ai"&gt;OpenAI&lt;/a&gt; ‚Ä¢ &lt;a href="https://docs.boundaryml.com/ref/llm-client-providers/anthropic"&gt;Anthropic&lt;/a&gt; ‚Ä¢ &lt;a href="https://docs.boundaryml.com/ref/llm-client-providers/google-ai-gemini"&gt;Gemini&lt;/a&gt; ‚Ä¢ &lt;a href="https://docs.boundaryml.com/ref/llm-client-providers/google-vertex"&gt;Vertex&lt;/a&gt; ‚Ä¢ &lt;a href="https://docs.boundaryml.com/ref/llm-client-providers/aws-bedrock"&gt;Bedrock&lt;/a&gt; ‚Ä¢ &lt;a href="https://docs.boundaryml.com/ref/llm-client-providers/open-ai-from-azure"&gt;Azure OpenAI&lt;/a&gt; ‚Ä¢ &lt;a href="https://docs.boundaryml.com/ref/llm-client-providers/openai-generic"&gt;Anything OpenAI Compatible&lt;/a&gt; (&lt;a href="https://docs.boundaryml.com/ref/llm-client-providers/openai-generic-ollama"&gt;Ollama&lt;/a&gt;, &lt;a href="https://docs.boundaryml.com/ref/llm-client-providers/openai-generic-open-router"&gt;OpenRouter&lt;/a&gt;, &lt;a href="https://docs.boundaryml.com/ref/llm-client-providers/openai-generic-v-llm"&gt;VLLM&lt;/a&gt;, &lt;a href="https://docs.boundaryml.com/ref/llm-client-providers/openai-generic-lm-studio"&gt;LMStudio&lt;/a&gt;, &lt;a href="https://docs.boundaryml.com/ref/llm-client-providers/openai-generic-together-ai"&gt;TogetherAI&lt;/a&gt;, and more)&lt;/p&gt; 
&lt;h2&gt;Build beautiful streaming UIs&lt;/h2&gt; 
&lt;p&gt;BAML generates a ton of utilities for NextJS, Python (and any language) to make streaming UIs easy. &lt;img src="https://github.com/user-attachments/assets/cf82495b-21fc-40bf-ae98-93eef923d620" alt="recipe-generator"&gt;&lt;/p&gt; 
&lt;p&gt;BAML's streaming interfaces are fully type-safe. Check out the &lt;a href="https://docs.boundaryml.com/guide/baml-basics/streaming"&gt;Streaming Docs&lt;/a&gt;, and our &lt;a href="https://docs.boundaryml.com/guide/framework-integration/react-next-js/quick-start"&gt;React hooks&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Fully Open-Source, and offline&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;100% open-source (Apache 2)&lt;/li&gt; 
 &lt;li&gt;100% private. AGI will not require an internet connection, neither will BAML 
  &lt;ul&gt; 
   &lt;li&gt;No network requests beyond model calls you explicitly set&lt;/li&gt; 
   &lt;li&gt;Not stored or used for any training data&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;BAML files can be saved locally on your machine and checked into Github for easy diffs.&lt;/li&gt; 
 &lt;li&gt;Built in Rust. So fast, you can't even tell it's there.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;BAML's Design Philosophy&lt;/h2&gt; 
&lt;p&gt;Everything is fair game when making new syntax. If you can code it, it can be yours. This is our design philosophy to help restrict ideas:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;1:&lt;/strong&gt; Avoid invention when possible 
  &lt;ul&gt; 
   &lt;li&gt;Yes, prompts need versioning ‚Äî we have a great versioning tool: git&lt;/li&gt; 
   &lt;li&gt;Yes, you need to save prompts ‚Äî we have a great storage tool: filesystems&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;2:&lt;/strong&gt; Any file editor and any terminal should be enough to use it&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;3:&lt;/strong&gt; Be fast&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;4:&lt;/strong&gt; A first year university student should be able to understand it&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Why a new programming language&lt;/h2&gt; 
&lt;p&gt;We used to write websites like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;def home():
    return "&amp;lt;button onclick=\"() =&amp;gt; alert(\\\"hello!\\\")\"&amp;gt;Click&amp;lt;/button&amp;gt;"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And now we do this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;function Home() {
  return &amp;lt;button onClick={() =&amp;gt; setCount(prev =&amp;gt; prev + 1)}&amp;gt;
          {count} clicks!
         &amp;lt;/button&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;New syntax can be incredible at expressing new ideas. Plus the idea of maintaining hundreds of f-strings for prompts kind of disgusts us ü§Æ. Strings are bad for maintainable codebases. We prefer structured strings.&lt;/p&gt; 
&lt;p&gt;The goal of BAML is to give you the expressiveness of English, but the structure of code.&lt;/p&gt; 
&lt;p&gt;Full &lt;a href="https://www.boundaryml.com/blog/ai-agents-need-new-syntax"&gt;blog post&lt;/a&gt; by us.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;As models get better, we'll continue expecting even more out of them. But what will never change is that we'll want a way to write maintainable code that uses those models. The current way we all just assemble strings is very reminiscent of the early days PHP/HTML soup in web development. We hope some of the ideas we shared today can make a tiny dent in helping us all shape the way we all code tomorrow.&lt;/p&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Do I need to write my whole app in BAML?&lt;/td&gt; 
   &lt;td&gt;Nope, only the prompts! BAML translates definitions into the language of your choice! &lt;a href="https://docs.boundaryml.com/guide/installation-language/python"&gt;Python&lt;/a&gt;, &lt;a href="https://docs.boundaryml.com/guide/installation-language/typescript"&gt;TypeScript&lt;/a&gt;, &lt;a href="https://docs.boundaryml.com/guide/installation-language/ruby"&gt;Ruby&lt;/a&gt; and &lt;a href="https://docs.boundaryml.com/guide/installation-language/rest-api-other-languages"&gt;more&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Is BAML stable?&lt;/td&gt; 
   &lt;td&gt;Yes, many companies use it in production! We ship updates weekly!&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Why a new language?&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/BoundaryML/baml/canary/#why-a-new-programming-language"&gt;Jump to section&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Checkout our &lt;a href="https://raw.githubusercontent.com/BoundaryML/baml/canary/CONTRIBUTING.md"&gt;guide on getting started&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;You can cite the BAML repo as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@software{baml,
  author = {Vaibhav Gupta, Aaron Villalpando and Boundary ML team},
  title = {BAML},
  url = {https://github.com/boundaryml/baml},
  year = {2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;p&gt;Made with ‚ù§Ô∏è by Boundary&lt;/p&gt; 
&lt;p&gt;HQ in Seattle, WA&lt;/p&gt; 
&lt;p&gt;P.S. We're hiring for software engineers that love rust. &lt;a href="https://raw.githubusercontent.com/BoundaryML/baml/canary/founders@boundaryml.com"&gt;Email us&lt;/a&gt; or reach out on &lt;a href="https://discord.gg/ENtBB6kkXH"&gt;discord&lt;/a&gt;!&lt;/p&gt; 
&lt;div align="left" style="align-items: left;"&gt; 
 &lt;a href="https://raw.githubusercontent.com/BoundaryML/baml/canary/#top"&gt; &lt;img src="https://img.shields.io/badge/Back%20to%20Top-000000?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" alt="Back to Top"&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;img src="https://imgs.xkcd.com/comics/standards.png" alt_text="hi"&gt;</description>
    </item>
    
    <item>
      <title>aptos-labs/aptos-core</title>
      <link>https://github.com/aptos-labs/aptos-core</link>
      <description>&lt;p&gt;Aptos is a layer 1 blockchain built to support the widespread use of blockchain through better technology and user experience.&lt;/p&gt;&lt;hr&gt;&lt;a href="https://aptos.dev"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/aptos-labs/aptos-core/main/.assets/aptos_banner.png" alt="Aptos Banner"&gt; &lt;/a&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/aptos-labs/aptos-core/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-Apache-green.svg?sanitize=true" alt="License"&gt;&lt;/a&gt; &lt;a href="https://github.com/aptos-labs/aptos-core/actions/workflows/lint-test.yaml"&gt;&lt;img src="https://github.com/aptos-labs/aptos-core/actions/workflows/lint-test.yaml/badge.svg?sanitize=true" alt="Lint+Test"&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/aptos-labs/aptos-core"&gt;&lt;img src="https://codecov.io/gh/aptos-labs/aptos-core/branch/main/graph/badge.svg?token=X01RKXSGDE" alt="codecov"&gt;&lt;/a&gt; &lt;a href="https://discord.gg/aptosnetwork"&gt;&lt;img src="https://img.shields.io/discord/945856774056083548?style=flat-square" alt="Discord chat"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Aptos is a layer 1 blockchain bringing a paradigm shift to Web3 through better technology and user experience. Built with Move to create a home for developers building next-gen applications.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aptosfoundation.org/"&gt;Aptos Foundation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aptos.dev"&gt;Aptos Developer Network&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aptos.dev/guides/system-integrators-guide"&gt;Guide - Integrate with the Aptos Blockchain&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aptos.dev/tutorials"&gt;Tutorials&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Follow us on &lt;a href="https://twitter.com/Aptos"&gt;Twitter&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Join us on the &lt;a href="https://discord.gg/aptosnetwork"&gt;Aptos Discord&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;You can learn more about contributing to the Aptos project by reading our &lt;a href="https://github.com/aptos-labs/aptos-core/raw/main/CONTRIBUTING.md"&gt;Contribution Guide&lt;/a&gt; and by viewing our &lt;a href="https://github.com/aptos-labs/aptos-core/raw/main/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Aptos Core is licensed under &lt;a href="https://github.com/aptos-labs/aptos-core/raw/main/LICENSE"&gt;Apache 2.0&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>idootop/open-xiaoai</title>
      <link>https://github.com/idootop/open-xiaoai</link>
      <description>&lt;p&gt;ËÆ©Â∞èÁà±Èü≥ÁÆ±„ÄåÂê¨ËßÅ‰Ω†ÁöÑÂ£∞Èü≥„ÄçÔºåËß£ÈîÅÊó†ÈôêÂèØËÉΩ„ÄÇ&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Open-XiaoAI&lt;/h1&gt; 
&lt;p&gt;ËÆ©Â∞èÁà±Èü≥ÁÆ±„ÄåÂê¨ËßÅ‰Ω†ÁöÑÂ£∞Èü≥„ÄçÔºåËß£ÈîÅÊó†ÈôêÂèØËÉΩ„ÄÇ&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/idootop/open-xiaoai/main/docs/images/cover.jpg" alt=""&gt;&lt;/p&gt; 
&lt;h2&gt;ÁÆÄ‰ªã&lt;/h2&gt; 
&lt;p&gt;2017 Âπ¥ÔºåÂΩìÂÖ®ÁêÉÈ¶ñÊ¨æÂçÉ‰∏áÁ∫ßÈîÄÈáèÁöÑÊô∫ËÉΩÈü≥ÁÆ±ËØûÁîüÊó∂ÔºåÊàë‰ª¨‰ª•‰∏∫Ëß¶Êë∏Âà∞‰∫ÜÊú™Êù•„ÄÇ‰ΩÜÂæàÂø´ÂèëÁé∞ÔºåËøô‰∫õËÆæÂ§áË¢´Âõ∞Âú®„ÄåÊåá‰ª§-ÂìçÂ∫î„ÄçÁöÑÁâ¢Á¨ºÈáåÔºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ÂÆÉÂê¨ÂæóËßÅÂàÜË¥ùÔºåÂç¥Âê¨‰∏çÊáÇÊÉÖÊÑü&lt;/li&gt; 
 &lt;li&gt;ÂÆÉËÉΩÊâßË°åÂëΩ‰ª§ÔºåÂç¥‰∏ç‰ºö‰∏ªÂä®ÊÄùËÄÉ&lt;/li&gt; 
 &lt;li&gt;ÂÆÉÊúâÂçÉ‰∏áÁî®Êà∑ÔºåÂç¥Âè™Êúâ‰∏ÄÂ•óÊÄùÁª¥&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Êàë‰ª¨ÊõæÂπªÊÉ≥‰∏≠ÁöÑ"Ë¥æÁª¥ÊñØ"Á∫ß‰∫∫Â∑•Êô∫ËÉΩÔºåÂú®Áé∞ÂÆûÂú∫ÊôØ‰∏≠Ê≤¶‰∏∫"Â§©Ê∞îÈ¢ÑÊä•+Èü≥‰πêÊí≠ÊîæÂô®"„ÄÇ&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ÁúüÊ≠£ÁöÑÊô∫ËÉΩ‰∏çÂ∫îË¢´È¢ÑËÆæÁöÑ‰ª£Á†ÅÈÄªËæëÊâÄÊùüÁºöÔºåËÄåÂ∫îÂÉèÁîüÂëΩ‰ΩìËà¨Âú®‰∫§‰∫í‰∏≠ËøõÂåñ„ÄÇ&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Âú®‰∏ä‰∏Ä‰∏™ &lt;a href="https://github.com/idootop/mi-gpt"&gt;MiGPT&lt;/a&gt; È°πÁõÆ‰∏≠ÔºåÊàë‰ª¨Â∑≤ÁªèÂÆûÁé∞Â∞Ü ChatGPT Êé•ÂÖ•Âà∞Â∞èÁà±Èü≥ÁÆ±„ÄÇ&lt;/p&gt; 
&lt;p&gt;Ëøô‰∏ÄÊ¨° &lt;a href="https://github.com/idootop/open-xiaoai"&gt;Open-XiaoAI&lt;/a&gt; ÂÜçÊ¨°ËøõÂåñÔºåÁõ¥Êé•Êé•ÁÆ°Â∞èÁà±Èü≥ÁÆ±ÁöÑ‚ÄúËÄ≥Êúµ‚ÄùÂíå‚ÄúÂò¥Â∑¥‚ÄùÔºå&lt;/p&gt; 
&lt;p&gt;ÈÄöËøáÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÂíå AI AgentÔºåÂ∞ÜÂ∞èÁà±Èü≥ÁÆ±ÁöÑÊΩúÂäõÂÆåÂÖ®ÈáäÊîæÔºåËß£ÈîÅÊó†ÈôêÂèØËÉΩ„ÄÇ&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Êú™Êù•Áî±‰Ω†ÂÆö‰πâ!&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;‰Ω†ÁöÑÂ£∞Èü≥ + Â∞èÁà±Èü≥ÁÆ± = Êó†ÈôêÂèØËÉΩ&lt;/h2&gt; 
&lt;p&gt;üëâ &lt;a href="https://www.bilibili.com/video/BV1TxJhzvEhz"&gt;Â∞èÁà±Èü≥ÁÆ±Êé•ÂÖ•Â∞èÊô∫ AI ÊºîÁ§∫ËßÜÈ¢ë&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.bilibili.com/video/BV1TxJhzvEhz"&gt;&lt;img src="https://raw.githubusercontent.com/idootop/open-xiaoai/main/docs/images/xiaozhi.jpg" alt=""&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;üëâ &lt;a href="https://www.bilibili.com/video/BV1YfVUz5EMj"&gt;Â∞èÁà±Èü≥ÁÆ±Ëá™ÂÆö‰πâÂî§ÈÜíËØçÊºîÁ§∫ËßÜÈ¢ë&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.bilibili.com/video/BV1YfVUz5EMj"&gt;&lt;img src="https://raw.githubusercontent.com/idootop/open-xiaoai/main/docs/images/kws.jpg" alt=""&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;üëâ &lt;a href="https://www.bilibili.com/video/BV1N1421y7qn"&gt;Â∞èÁà±Èü≥ÁÆ±Êé•ÂÖ• MiGPT ÊºîÁ§∫ËßÜÈ¢ë&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.bilibili.com/video/BV1N1421y7qn"&gt;&lt;img src="https://raw.githubusercontent.com/idootop/open-xiaoai/main/docs/images/migpt.jpg" alt=""&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Âø´ÈÄüÂºÄÂßã&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Êú¨ÊïôÁ®ã‰ªÖÈÄÇÁî®‰∫é &lt;strong&gt;Â∞èÁà±Èü≥ÁÆ± ProÔºàLX06Ôºâ&lt;/strong&gt; Âíå &lt;strong&gt;Xiaomi Êô∫ËÉΩÈü≥ÁÆ± ProÔºàOH2PÔºâ&lt;/strong&gt; Ëøô‰∏§Ê¨æÊú∫ÂûãÔºå&lt;strong&gt;ÂÖ∂‰ªñÂûãÂè∑&lt;/strong&gt;ÁöÑÂ∞èÁà±Èü≥ÁÆ±ËØ∑ÂãøÁõ¥Êé•‰ΩøÁî®ÔºÅüö®&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Êú¨È°πÁõÆÁî± Client Á´Ø + Server Á´Ø‰∏§ÈÉ®ÂàÜÁªÑÊàêÔºå‰Ω†ÂèØ‰ª•ÊåâÁÖß‰ª•‰∏ãÈ°∫Â∫èËøêË°åËØ•È°πÁõÆÔºö&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Âà∑Êú∫Êõ¥Êñ∞Â∞èÁà±Èü≥ÁÆ±Ë°•‰∏ÅÂõ∫‰ª∂ÔºåÂºÄÂêØÂπ∂ SSH ËøûÊé•Âà∞Â∞èÁà±Èü≥ÁÆ± üëâ &lt;a href="https://raw.githubusercontent.com/idootop/open-xiaoai/main/docs/flash.md"&gt;ÊïôÁ®ã&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Âú®Â∞èÁà±Èü≥ÁÆ±‰∏äÂÆâË£ÖËøêË°å Client Á´ØË°•‰∏ÅÁ®ãÂ∫è üëâ &lt;a href="https://raw.githubusercontent.com/idootop/open-xiaoai/main/packages/client-rust/README.md"&gt;ÊïôÁ®ã&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ËøêË°å‰ª•‰∏ãÊºîÁ§∫Á®ãÂ∫èÔºå‰ΩìÈ™åÂ∞èÁà±Èü≥ÁÆ±ÁöÑÂÖ®Êñ∞ËÉΩÂäõ ‚ú® 
  &lt;ul&gt; 
   &lt;li&gt;üëâ &lt;a href="https://raw.githubusercontent.com/idootop/open-xiaoai/main/examples/xiaozhi/README.md"&gt;Â∞èÁà±Èü≥ÁÆ±Êé•ÂÖ•Â∞èÊô∫ AI&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;üëâ &lt;a href="https://raw.githubusercontent.com/idootop/open-xiaoai/main/examples/kws/README.md"&gt;Â∞èÁà±Èü≥ÁÆ±Ëá™ÂÆö‰πâÂî§ÈÜíËØç&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;üëâ &lt;a href="https://raw.githubusercontent.com/idootop/open-xiaoai/main/examples/migpt/README.md"&gt;Â∞èÁà±Èü≥ÁÆ±Êé•ÂÖ• MiGPTÔºàÂÆåÁæéÁâàÔºâ&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;üëâ &lt;a href="https://raw.githubusercontent.com/idootop/open-xiaoai/main/examples/gemini/README.md"&gt;Â∞èÁà±Èü≥ÁÆ±Êé•ÂÖ• Gemini Live API&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;‰ª•‰∏äÁöÜ‰∏∫ÊäõÁ†ñÂºïÁéâÔºå‰Ω†‰πüÂèØ‰ª•‰∫≤ÊâãÁºñÂÜôËá™Â∑±ÊÉ≥Ë¶ÅÁöÑÂäüËÉΩÔºå‰∏ÄÂàáÁî±‰Ω†ÂÆö‰πâÔºÅ&lt;/p&gt; 
&lt;h2&gt;Áõ∏ÂÖ≥È°πÁõÆ&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] ÊäÄÊúØÁöÑÊÑè‰πâÂú®‰∫éÂàÜ‰∫´‰∏éÂÖ±Âàõ„ÄÇÂ¶ÇÊûú‰Ω†ÊâìÁÆóÊàñÊ≠£Âú®‰ΩøÁî®Êú¨È°πÁõÆÂÅö‰∫õÊúâË∂£ÁöÑ‰∫ãÊÉÖÔºå Ê¨¢ËøéÊèê‰∫§ PR Êàñ issue ÂàÜ‰∫´‰Ω†ÁöÑÈ°πÁõÆÂíåÂàõÊÑè„ÄÇ‚ú®&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Â¶ÇÊûú‰Ω†‰∏çÊÉ≥Âà∑Êú∫ÔºåÊàñËÄÖ‰∏çÊòØÂ∞èÁà±Èü≥ÁÆ± ProÔºå‰∏ãÈù¢ÁöÑÈ°πÁõÆÊàñËÆ∏ÂØπ‰Ω†ÊúâÁî®Ôºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/idootop/mi-gpt"&gt;https://github.com/idootop/mi-gpt&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/idootop/migpt-next"&gt;https://github.com/idootop/migpt-next&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yihong0618/xiaogpt"&gt;https://github.com/yihong0618/xiaogpt&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hanxi/xiaomusic"&gt;https://github.com/hanxi/xiaomusic&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ÂèÇËÄÉÈìæÊé•&lt;/h2&gt; 
&lt;p&gt;Â¶ÇÊûú‰Ω†ÊÉ≥Ë¶Å‰∫ÜËß£Êõ¥Â§öÊäÄÊúØÁªÜËäÇÔºå‰∏ãÈù¢ÁöÑÈìæÊé•ÂèØËÉΩÂØπ‰Ω†ÊúâÁî®Ôºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yihong0618/gitblog/issues/258"&gt;https://github.com/yihong0618/gitblog/issues/258&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jialeicui/open-lx01"&gt;https://github.com/jialeicui/open-lx01&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/duhow/xiaoai-patch"&gt;https://github.com/duhow/xiaoai-patch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://javabin.cn/2021/xiaoai_fm.html"&gt;https://javabin.cn/2021/xiaoai_fm.html&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://xuanxuanblingbling.github.io/iot/2022/09/16/mi/"&gt;https://xuanxuanblingbling.github.io/iot/2022/09/16/mi/&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ÂÖçË¥£Â£∞Êòé&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;ÈÄÇÁî®ËåÉÂõ¥&lt;/strong&gt; Êú¨È°πÁõÆ‰∏∫ÂºÄÊ∫êÈùûËê•Âà©È°πÁõÆÔºå‰ªÖ‰æõÂ≠¶ÊúØÁ†îÁ©∂Êàñ‰∏™‰∫∫ÊµãËØïÁî®ÈÄî„ÄÇ‰∏•Á¶ÅÁî®‰∫éÂïÜ‰∏öÊúçÂä°„ÄÅÁΩëÁªúÊîªÂáª„ÄÅÊï∞ÊçÆÁ™ÉÂèñ„ÄÅÁ≥ªÁªüÁ†¥ÂùèÁ≠âËøùÂèç„ÄäÁΩëÁªúÂÆâÂÖ®Ê≥ï„ÄãÂèä‰ΩøÁî®ËÄÖÊâÄÂú®Âú∞Âè∏Ê≥ïÁÆ°ËæñÂå∫ÁöÑÊ≥ïÂæãËßÑÂÆöÁöÑÂú∫ÊôØ„ÄÇ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÈùûÂÆòÊñπÂ£∞Êòé&lt;/strong&gt; Êú¨È°πÁõÆÁî±Á¨¨‰∏âÊñπÂºÄÂèëËÄÖÁã¨Á´ãÂºÄÂèëÔºå‰∏éÂ∞èÁ±≥ÈõÜÂõ¢ÂèäÂÖ∂ÂÖ≥ËÅîÊñπÔºà‰∏ãÁß∞"ÊùÉÂà©Êñπ"ÔºâÊó†‰ªª‰ΩïÈö∂Â±û/Âêà‰ΩúÂÖ≥Á≥ªÔºå‰∫¶Êú™Ëé∑ÂÖ∂ÂÆòÊñπÊéàÊùÉ/ËÆ§ÂèØÊàñÊäÄÊúØÊîØÊåÅ„ÄÇÈ°πÁõÆ‰∏≠Ê∂âÂèäÁöÑÂïÜÊ†á„ÄÅÂõ∫‰ª∂„ÄÅ‰∫ëÊúçÂä°ÁöÑÊâÄÊúâÊùÉÂà©ÂΩíÂ±ûÂ∞èÁ±≥ÈõÜÂõ¢„ÄÇËã•ÊùÉÂà©Êñπ‰∏ªÂº†ÊùÉÁõäÔºå‰ΩøÁî®ËÄÖÂ∫îÁ´ãÂç≥‰∏ªÂä®ÂÅúÊ≠¢‰ΩøÁî®Âπ∂Âà†Èô§Êú¨È°πÁõÆ„ÄÇ&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;ÁªßÁª≠‰∏ãËΩΩÊàñËøêË°åÊú¨È°πÁõÆÔºåÂç≥Ë°®Á§∫ÊÇ®Â∑≤ÂÆåÊï¥ÈòÖËØªÂπ∂ÂêåÊÑè&lt;a href="https://raw.githubusercontent.com/idootop/open-xiaoai/main/agreement.md"&gt;Áî®Êà∑ÂçèËÆÆ&lt;/a&gt;ÔºåÂê¶ÂàôËØ∑Á´ãÂç≥ÁªàÊ≠¢‰ΩøÁî®Âπ∂ÂΩªÂ∫ïÂà†Èô§Êú¨È°πÁõÆ„ÄÇ&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/idootop/open-xiaoai/main/LICENSE"&gt;MIT&lt;/a&gt; License ¬© 2024-PRESENT Del Wang&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>bytecodealliance/wasmtime</title>
      <link>https://github.com/bytecodealliance/wasmtime</link>
      <description>&lt;p&gt;A lightweight WebAssembly runtime that is fast, secure, and standards-compliant&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;&lt;code&gt;wasmtime&lt;/code&gt;&lt;/h1&gt; 
 &lt;p&gt; &lt;strong&gt;A standalone runtime for &lt;a href="https://webassembly.org/"&gt;WebAssembly&lt;/a&gt;&lt;/strong&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;A &lt;a href="https://bytecodealliance.org/"&gt;Bytecode Alliance&lt;/a&gt; project&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt; &lt;a href="https://github.com/bytecodealliance/wasmtime/actions?query=workflow%3ACI"&gt;&lt;img src="https://github.com/bytecodealliance/wasmtime/workflows/CI/badge.svg?sanitize=true" alt="build status"&gt;&lt;/a&gt; &lt;a href="https://bytecodealliance.zulipchat.com/#narrow/stream/217126-wasmtime"&gt;&lt;img src="https://img.shields.io/badge/zulip-join_chat-brightgreen.svg?sanitize=true" alt="zulip chat"&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/rustc-stable+-green.svg?sanitize=true" alt="supported rustc stable"&gt; &lt;a href="https://docs.rs/wasmtime"&gt;&lt;img src="https://docs.rs/wasmtime/badge.svg?sanitize=true" alt="Documentation Status"&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;h3&gt; &lt;a href="https://bytecodealliance.github.io/wasmtime/"&gt;Guide&lt;/a&gt; &lt;span&gt; | &lt;/span&gt; &lt;a href="https://bytecodealliance.github.io/wasmtime/contributing.html"&gt;Contributing&lt;/a&gt; &lt;span&gt; | &lt;/span&gt; &lt;a href="https://wasmtime.dev/"&gt;Website&lt;/a&gt; &lt;span&gt; | &lt;/span&gt; &lt;a href="https://bytecodealliance.zulipchat.com/#narrow/stream/217126-wasmtime"&gt;Chat&lt;/a&gt; &lt;/h3&gt; 
&lt;/div&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;The Wasmtime CLI can be installed on Linux and macOS (locally) with a small install script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;curl https://wasmtime.dev/install.sh -sSf | bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This script installs into &lt;code&gt;$WASMTIME_HOME&lt;/code&gt; (defaults to &lt;code&gt;$HOME/.wasmtime&lt;/code&gt;), and executable is placed in &lt;code&gt;$WASMTIME_HOME/bin&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Windows or otherwise interested users can download installers and binaries directly from the &lt;a href="https://github.com/bytecodealliance/wasmtime/releases"&gt;GitHub Releases&lt;/a&gt; page.&lt;/p&gt; 
&lt;p&gt;Documentation on Wasmtime's currently supported versions can be found &lt;a href="https://docs.wasmtime.dev/stability-release.html#current-versions"&gt;in the online book documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Example&lt;/h2&gt; 
&lt;p&gt;If you've got the &lt;a href="https://www.rust-lang.org/tools/install"&gt;Rust compiler installed&lt;/a&gt; then you can take some Rust source code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;fn main() {
    println!("Hello, world!");
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and compile it into a WebAssembly component with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;rustup target add wasm32-wasip2
rustc hello.rs --target wasm32-wasip2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once compiled, you can can run your component:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;wasmtime hello.wasm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should see the following output:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;Hello, world!
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;(Note: make sure you installed Rust using the &lt;a href="https://rustup.rs"&gt;&lt;code&gt;rustup&lt;/code&gt;&lt;/a&gt; method in the official instructions above, and do not have a copy of the Rust toolchain installed on your system in some other way as well (e.g. the system package manager). Otherwise, the &lt;code&gt;rustup target add...&lt;/code&gt; command may not install the target for the correct copy of Rust.)&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Fast&lt;/strong&gt;. Wasmtime is built on the optimizing &lt;a href="https://cranelift.dev/"&gt;Cranelift&lt;/a&gt; code generator to quickly generate high-quality machine code either at runtime or ahead-of-time. Wasmtime is optimized for efficient instantiation, low-overhead calls between the embedder and wasm, and scalability of concurrent instances.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.wasmtime.dev/security.html"&gt;Secure&lt;/a&gt;&lt;/strong&gt;. Wasmtime's development is strongly focused on correctness and security. Building on top of Rust's runtime safety guarantees, each Wasmtime feature goes through careful review and consideration via an &lt;a href="https://github.com/bytecodealliance/rfcs"&gt;RFC process&lt;/a&gt;. Once features are designed and implemented, they undergo 24/7 fuzzing donated by &lt;a href="https://google.github.io/oss-fuzz/"&gt;Google's OSS Fuzz&lt;/a&gt;. As features stabilize they become part of a &lt;a href="https://docs.wasmtime.dev/stability-release.html"&gt;release&lt;/a&gt;, and when things go wrong we have a well-defined &lt;a href="https://bytecodealliance.org/security"&gt;security policy&lt;/a&gt; in place to quickly mitigate and patch any issues. We follow best practices for defense-in-depth and integrate protections and mitigations for issues like Spectre. Finally, we're working to push the state-of-the-art by collaborating with academic researchers to formally verify critical parts of Wasmtime and Cranelift.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html"&gt;Configurable&lt;/a&gt;&lt;/strong&gt;. Wasmtime uses sensible defaults, but can also be configured to provide more fine-grained control over things like CPU and memory consumption. Whether you want to run Wasmtime in a tiny environment or on massive servers with many concurrent instances, we've got you covered.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.rs/wasmtime-wasi/latest/wasmtime_wasi/"&gt;WASI&lt;/a&gt;&lt;/strong&gt;. Wasmtime supports a rich set of APIs for interacting with the host environment through the &lt;a href="https://wasi.dev"&gt;WASI standard&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.wasmtime.dev/stability-tiers.html"&gt;Standards Compliant&lt;/a&gt;&lt;/strong&gt;. Wasmtime passes the &lt;a href="https://github.com/WebAssembly/testsuite"&gt;official WebAssembly test suite&lt;/a&gt;, implements the &lt;a href="https://github.com/WebAssembly/wasm-c-api"&gt;official C API of wasm&lt;/a&gt;, and implements &lt;a href="https://github.com/WebAssembly/proposals"&gt;future proposals to WebAssembly&lt;/a&gt; as well. Wasmtime developers are intimately engaged with the WebAssembly standards process all along the way too.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Language Support&lt;/h2&gt; 
&lt;p&gt;You can use Wasmtime from a variety of different languages through embeddings of the implementation.&lt;/p&gt; 
&lt;p&gt;Languages supported by the Bytecode Alliance:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://bytecodealliance.github.io/wasmtime/lang-rust.html"&gt;Rust&lt;/a&gt;&lt;/strong&gt; - the &lt;a href="https://crates.io/crates/wasmtime"&gt;&lt;code&gt;wasmtime&lt;/code&gt; crate&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://bytecodealliance.github.io/wasmtime/lang-c.html"&gt;C&lt;/a&gt;&lt;/strong&gt; - the &lt;a href="https://bytecodealliance.github.io/wasmtime/c-api/"&gt;&lt;code&gt;wasm.h&lt;/code&gt;, &lt;code&gt;wasi.h&lt;/code&gt;, and &lt;code&gt;wasmtime.h&lt;/code&gt; headers&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/bytecodealliance/wasmtime/main/crates/c-api/CMakeLists.txt"&gt;CMake&lt;/a&gt; or &lt;a href="https://conan.io/center/wasmtime"&gt;&lt;code&gt;wasmtime&lt;/code&gt; Conan package&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;C++&lt;/strong&gt; - the &lt;a href="https://bytecodealliance.github.io/wasmtime/c-api/"&gt;&lt;code&gt;wasmtime.hh&lt;/code&gt; header&lt;/a&gt; or the &lt;a href="https://conan.io/center/wasmtime-cpp"&gt;&lt;code&gt;wasmtime-cpp&lt;/code&gt; Conan package&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://bytecodealliance.github.io/wasmtime/lang-python.html"&gt;Python&lt;/a&gt;&lt;/strong&gt; - the &lt;a href="https://pypi.org/project/wasmtime/"&gt;&lt;code&gt;wasmtime&lt;/code&gt; PyPI package&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://bytecodealliance.github.io/wasmtime/lang-dotnet.html"&gt;.NET&lt;/a&gt;&lt;/strong&gt; - the &lt;a href="https://www.nuget.org/packages/Wasmtime"&gt;&lt;code&gt;Wasmtime&lt;/code&gt; NuGet package&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://bytecodealliance.github.io/wasmtime/lang-go.html"&gt;Go&lt;/a&gt;&lt;/strong&gt; - the &lt;a href="https://pkg.go.dev/github.com/bytecodealliance/wasmtime-go"&gt;&lt;code&gt;wasmtime-go&lt;/code&gt; repository&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://bytecodealliance.github.io/wasmtime/lang-ruby.html"&gt;Ruby&lt;/a&gt;&lt;/strong&gt; - the &lt;a href="https://rubygems.org/gems/wasmtime"&gt;&lt;code&gt;wasmtime&lt;/code&gt; gem&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Languages supported by the community:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.wasmtime.dev/lang-elixir.html"&gt;Elixir&lt;/a&gt;&lt;/strong&gt; - the &lt;a href="https://hex.pm/packages/wasmex"&gt;&lt;code&gt;wasmex&lt;/code&gt; hex package&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Perl&lt;/strong&gt; - the &lt;a href="https://metacpan.org/pod/Wasm::Wasmtime"&gt;&lt;code&gt;Wasm&lt;/code&gt; Perl package's &lt;code&gt;Wasm::Wasmtime&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://bytecodealliance.github.io/wasmtime"&gt;üìö Read the Wasmtime guide here! üìö&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The &lt;a href="https://bytecodealliance.github.io/wasmtime"&gt;wasmtime guide&lt;/a&gt; is the best starting point to learn about what Wasmtime can do for you or help answer your questions about Wasmtime. If you're curious in contributing to Wasmtime, &lt;a href="https://bytecodealliance.github.io/wasmtime/contributing.html"&gt;it can also help you do that&lt;/a&gt;!&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;It's Wasmtime.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>GraphiteEditor/Graphite</title>
      <link>https://github.com/GraphiteEditor/Graphite</link>
      <description>&lt;p&gt;An open source graphics editor for 2025: comprehensive 2D content creation tool suite for graphic design, digital art, and interactive real-time motion graphics ‚Äî featuring node-based procedural editing&lt;/p&gt;&lt;hr&gt;&lt;a href="https://graphite.rs/"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/user-attachments/assets/9366c148-4405-484f-909a-9a3526eb9209"&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://github.com/user-attachments/assets/791508ab-bcd5-4e31-a3b9-1187cfd7a2f6"&gt; 
  &lt;img alt="Graphite logo" src="https://github.com/user-attachments/assets/791508ab-bcd5-4e31-a3b9-1187cfd7a2f6"&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;h1&gt;Your procedural toolbox for 2D content creation&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Graphite is a free, open source vector and raster graphics engine, &lt;a href="https://editor.graphite.rs"&gt;available now&lt;/a&gt; in alpha. Get creative with a fully nondestructive editing workflow that combines layer-based compositing with node-based generative design.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Having begun life as a vector editor, Graphite continues evolving into a generalized, all-in-one graphics toolbox that's built more like a game engine than a conventional creative app. The editor's tools wrap its node graph core, providing user-friendly workflows for vector, raster, and beyond. Photo editing, motion graphics, digital painting, desktop publishing, and VFX compositing are additional competencies on the planned &lt;a href="https://graphite.rs/features/#roadmap"&gt;roadmap&lt;/a&gt; making Graphite into a highly versatile content creation tool.&lt;/p&gt; 
&lt;p&gt;Learn more from the &lt;a href="https://graphite.rs/"&gt;website&lt;/a&gt;, subscribe to the &lt;a href="https://graphite.rs/#newsletter"&gt;newsletter&lt;/a&gt;, consider &lt;a href="https://graphite.rs/volunteer/"&gt;volunteering&lt;/a&gt; or &lt;a href="https://graphite.rs/donate/"&gt;donating&lt;/a&gt;, and remember to give this repository a ‚≠ê!&lt;/p&gt; 
&lt;br&gt; 
&lt;a href="https://discord.graphite.rs/"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/user-attachments/assets/ad185fac-3b48-446d-863c-2bcb0724abee"&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://github.com/user-attachments/assets/aa23f503-f3bf-444a-9080-8eaa19fa2fa8"&gt; 
  &lt;img alt="Discord" src="https://github.com/user-attachments/assets/aa23f503-f3bf-444a-9080-8eaa19fa2fa8" width="48" height="48"&gt; 
 &lt;/picture&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
&lt;a href="https://www.reddit.com/r/graphite/"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/user-attachments/assets/d8c05686-2eb9-4ac1-8149-728c12b4e71a"&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://github.com/user-attachments/assets/6f32329a-4d6f-42d8-9a2f-42977c0b3c05"&gt; 
  &lt;img alt="Reddit" src="https://github.com/user-attachments/assets/6f32329a-4d6f-42d8-9a2f-42977c0b3c05" width="48" height="48"&gt; 
 &lt;/picture&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
&lt;a href="https://bsky.app/profile/graphiteeditor.bsky.social"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/user-attachments/assets/c736d80c-e9bf-4591-a7e0-a7723057a906"&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://github.com/user-attachments/assets/3db9b0a1-5ab7-4bff-bfd3-8a4ade7b98bd"&gt; 
  &lt;img alt="Bluesky" src="https://github.com/user-attachments/assets/3db9b0a1-5ab7-4bff-bfd3-8a4ade7b98bd" width="48" height="48"&gt; 
 &lt;/picture&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
&lt;a href="https://twitter.com/graphiteeditor"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/user-attachments/assets/115f04cc-e3c2-4f90-ac35-eb9edd3ca9be"&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://github.com/user-attachments/assets/4ed4185d-a622-418c-bbf4-a0419e690ca9"&gt; 
  &lt;img alt="Twitter" src="https://github.com/user-attachments/assets/4ed4185d-a622-418c-bbf4-a0419e690ca9" width="48" height="48"&gt; 
 &lt;/picture&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
&lt;a href="https://www.youtube.com/@GraphiteEditor"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/user-attachments/assets/cbc02fad-5cbc-4715-a8e5-860198e989c7"&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://github.com/user-attachments/assets/d13b484d-97a8-4d9e-bbe4-c60348b3f676"&gt; 
  &lt;img alt="YouTube" src="https://github.com/user-attachments/assets/d13b484d-97a8-4d9e-bbe4-c60348b3f676" width="48" height="48"&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;br&gt;
&lt;br&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/f4604aea-e8f1-45ce-9218-46ddc666f11d"&gt;https://github.com/user-attachments/assets/f4604aea-e8f1-45ce-9218-46ddc666f11d&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Support our mission ‚ù§Ô∏è&lt;/h2&gt; 
&lt;p&gt;Graphite is 100% community built and funded. Please become a part of keeping the project alive and thriving with a &lt;a href="https://graphite.rs/donate/"&gt;donation&lt;/a&gt; if you share a belief in our &lt;strong&gt;mission&lt;/strong&gt;:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Graphite strives to unshackle the creativity of every budding artist and seasoned professional by building the best comprehensive art and design tool that's accessible to all.&lt;/p&gt; 
 &lt;p&gt;Mission success will come when Graphite is an industry standard. A cohesive product vision and focus on innovation over imitation is the strategy that will make that possible.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Screenshots&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/decb7011-18c2-4c68-82af-d1fa5064244a" alt="Made using nondestructive boolean operations and procedural polka dot patterns"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/9e023997-185b-4f43-a724-797d308d9e7b" alt="Mandelbrot fractal filled with a noise pattern, procedurally generated and infinitely scalable"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/90eca551-5868-4f8d-9016-33958bf96345" alt="Design for a magazine spread, a preview of the upcoming focus on desktop publishing"&gt;&lt;/p&gt; 
&lt;h2&gt;Contributing/building the code&lt;/h2&gt; 
&lt;p&gt;Are you a graphics programmer or Rust developer? Graphite aims to be one of the most approachable projects for putting your engineering skills to use in the world of open source. See &lt;a href="https://graphite.rs/volunteer/guide/"&gt;instructions here&lt;/a&gt; for setting up the project and getting started.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;By submitting code for inclusion in the project, you are agreeing to license your changes under the Apache 2.0 license, and that you have the authority to do so. Some directories may have other licenses, like dual-licensed MIT/Apache 2.0, and code submissions to those directories mean you agree to the applicable license(s).&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>MystenLabs/sui</title>
      <link>https://github.com/MystenLabs/sui</link>
      <description>&lt;p&gt;Sui, a next-generation smart contract platform with high throughput, low latency, and an asset-oriented programming model powered by the Move programming language&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/MystenLabs/sui/refs/heads/main/docs/site/static/img/logo.svg?sanitize=true" alt="Logo" width="100" height="100"&gt; &lt;/p&gt; 
&lt;h1&gt;Welcome to Sui&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/MystenLabs/sui/releases/latest"&gt;&lt;img src="https://img.shields.io/github/v/tag/MystenLabs/sui.svg?sort=semver" alt="Github release"&gt;&lt;/a&gt; &lt;a href="https://github.com/MystenLabs/sui/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/MystenLabs/sui" alt="License"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://sui.io"&gt;Sui&lt;/a&gt; is a next-generation smart contract platform with high throughput, low latency, and an asset-oriented programming model powered by the &lt;a href="https://github.com/MystenLabs/awesome-move"&gt;Move programming language&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Sui Highlights&lt;/h2&gt; 
&lt;p&gt;Sui offers the following benefits and capabilities:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Unmatched scalability, instant settlement&lt;/li&gt; 
 &lt;li&gt;A safe smart contract language accessible to mainstream developers&lt;/li&gt; 
 &lt;li&gt;Ability to define rich and composable on-chain assets&lt;/li&gt; 
 &lt;li&gt;Better user experience for web3 apps&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Sui is the only blockchain today that can scale with the growth of web3 while achieving industry-leading performance, cost, programmability, and usability. As Sui approaches Mainnet launch, it will demonstrate capacity beyond the transaction processing capabilities of established systems ‚Äì traditional and blockchain alike. Sui is the first internet-scale programmable blockchain platform, a foundational layer for web3.&lt;/p&gt; 
&lt;h2&gt;Sui Architecture&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-mermaid"&gt;flowchart LR
    CC(CLI Client) --&amp;gt; ClientService
    RC(Rest Client) --&amp;gt; ClientService
    RPCC(RPC Client) --&amp;gt; ClientService
    ClientService --&amp;gt; AuthorityAggregator
    AuthorityAggregator --&amp;gt; AC1[AuthorityClient] &amp;amp; AC2[AuthorityClient]
    subgraph Authority1
      AS[AuthorityState]
    end
    subgraph Authority2
      AS2[AuthorityState]
    end
    AC1 &amp;lt;==&amp;gt;|Network TCP| Authority1
    AC2 &amp;lt;==&amp;gt;|Network TCP| Authority2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Sui Overview&lt;/h2&gt; 
&lt;p&gt;Sui is a smart contract platform maintained by a permissionless set of authorities that play a role similar to validators or miners in other blockchain systems.&lt;/p&gt; 
&lt;p&gt;Sui offers scalability and unprecedented low-latency for common use cases. Sui makes the vast majority of transactions processable in parallel, which makes better use of processing resources, and offers the option to increase throughput with more resources. Sui forgoes consensus to instead use simpler and lower-latency primitives for common use cases, such as payment transactions and asset transfers. This is unprecedented in the blockchain world and enables a number of new latency-sensitive distributed applications, ranging from gaming to retail payment at physical points of sale.&lt;/p&gt; 
&lt;p&gt;Sui is written in &lt;a href="https://www.rust-lang.org"&gt;Rust&lt;/a&gt; and supports smart contracts written in the &lt;a href="https://github.com/move-language/move"&gt;Move programming language&lt;/a&gt; to define assets that may have an owner. Move programs define operations on these assets including custom rules for their creation, the transfer of these assets to new owners, and operations that mutate assets.&lt;/p&gt; 
&lt;p&gt;Sui has a native token called SUI, with a fixed supply. The SUI token is used to pay for gas, and is also used as &lt;a href="https://learn.bybit.com/blockchain/delegated-proof-of-stake-dpos/"&gt;delegated stake on authorities&lt;/a&gt; within an epoch. The voting power of authorities within this epoch is a function of this delegated stake. Authorities are periodically reconfigured according to the stake delegated to them. In any epoch, the set of authorities is &lt;a href="https://pmg.csail.mit.edu/papers/osdi99.pdf"&gt;Byzantine fault tolerant&lt;/a&gt;. At the end of the epoch, fees collected through all transactions processed are distributed to authorities according to their contribution to the operation of the system. Authorities can in turn share some of the fees as rewards to users that delegated stakes to them.&lt;/p&gt; 
&lt;p&gt;Sui is supported by several cutting-edge &lt;a href="https://github.com/MystenLabs/sui/raw/main/docs/content/concepts/research-papers.mdx"&gt;peer-reviewed studies&lt;/a&gt; and extensive years of open-source development.&lt;/p&gt; 
&lt;h2&gt;More About Sui&lt;/h2&gt; 
&lt;p&gt;Use the following links to learn more about Sui and the Sui ecosystem:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Learn more about working with Sui in the &lt;a href="https://docs.sui.io/"&gt;Sui Documentation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Join the Sui community on &lt;a href="https://discord.gg/sui"&gt;Sui Discord&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Find out more about the Sui ecosystem on the &lt;a href="https://sui.io/resources/"&gt;Sui Resources&lt;/a&gt; page.&lt;/li&gt; 
 &lt;li&gt;Review information about Sui governance, &lt;a href="https://suifoundation.org/decentralization"&gt;decentralization&lt;/a&gt;, and &lt;a href="https://sui.io/grants-hub"&gt;Developer Grants Program&lt;/a&gt; on the &lt;a href="https://sui.io/about"&gt;Sui Foundation&lt;/a&gt; site.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How to Contribute&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/MystenLabs/sui/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; for details on how to contribute to Sui.&lt;/p&gt; 
&lt;h2&gt;Code of Conduct&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/MystenLabs/sui/main/CODE_OF_CONDUCT.MD"&gt;Code of Conduct&lt;/a&gt; for details on our code of conduct.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/MystenLabs/sui/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for more details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>tensorzero/tensorzero</title>
      <link>https://github.com/tensorzero/tensorzero</link>
      <description>&lt;p&gt;TensorZero is an open-source stack for industrial-grade LLM applications. It unifies an LLM gateway, observability, optimization, evaluation, and experimentation.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;
 &lt;picture&gt;
  &lt;img src="https://github.com/user-attachments/assets/47d67430-386d-4675-82ad-d4734d3262d9" alt="TensorZero Logo" width="128" height="128"&gt;
 &lt;/picture&gt;&lt;/p&gt; 
&lt;h1&gt;TensorZero&lt;/h1&gt; 
&lt;p&gt;
 &lt;picture&gt;
  &lt;img src="https://www.tensorzero.com/github-trending-badge.svg?sanitize=true" alt="#1 Repository Of The Day"&gt;
 &lt;/picture&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;TensorZero is an open-source stack for &lt;em&gt;industrial-grade LLM applications&lt;/em&gt;:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Gateway:&lt;/strong&gt; access every LLM provider through a unified API, built for performance (&amp;lt;1ms p99 latency)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Observability:&lt;/strong&gt; store inferences and feedback in your database, available programmatically or in the UI&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Optimization:&lt;/strong&gt; collect metrics and human feedback to optimize prompts, models, and inference strategies&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Evaluation:&lt;/strong&gt; benchmark individual inferences or end-to-end workflows using heuristics, LLM judges, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Experimentation:&lt;/strong&gt; ship with confidence with built-in A/B testing, routing, fallbacks, retries, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Take what you need, adopt incrementally, and complement with other tools.&lt;/p&gt; 
&lt;hr&gt; 
&lt;p align="center"&gt; &lt;b&gt;&lt;a href="https://www.tensorzero.com/" target="_blank"&gt;Website&lt;/a&gt;&lt;/b&gt; ¬∑ &lt;b&gt;&lt;a href="https://www.tensorzero.com/docs" target="_blank"&gt;Docs&lt;/a&gt;&lt;/b&gt; ¬∑ &lt;b&gt;&lt;a href="https://www.x.com/tensorzero" target="_blank"&gt;Twitter&lt;/a&gt;&lt;/b&gt; ¬∑ &lt;b&gt;&lt;a href="https://www.tensorzero.com/slack" target="_blank"&gt;Slack&lt;/a&gt;&lt;/b&gt; ¬∑ &lt;b&gt;&lt;a href="https://www.tensorzero.com/discord" target="_blank"&gt;Discord&lt;/a&gt;&lt;/b&gt; &lt;br&gt; &lt;br&gt; &lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/quickstart" target="_blank"&gt;Quick Start (5min)&lt;/a&gt;&lt;/b&gt; ¬∑ &lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/deployment" target="_blank"&gt;Deployment Guide&lt;/a&gt;&lt;/b&gt; ¬∑ &lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/api-reference" target="_blank"&gt;API Reference&lt;/a&gt;&lt;/b&gt; ¬∑ &lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/deployment" target="_blank"&gt;Configuration Reference&lt;/a&gt;&lt;/b&gt; &lt;/p&gt; 
&lt;hr&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="30%" valign="top"&gt;&lt;b&gt;What is TensorZero?&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="70%" valign="top"&gt;TensorZero is an open-source stack for industrial-grade LLM applications. It unifies an LLM gateway, observability, optimization, evaluation, and experimentation.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="30%" valign="top"&gt;&lt;b&gt;How is TensorZero different from other LLM frameworks?&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="70%" valign="top"&gt; 1. TensorZero enables you to optimize complex LLM applications based on production metrics and human feedback.&lt;br&gt; 2. TensorZero supports the needs of industrial-grade LLM applications: low latency, high throughput, type safety, self-hosted, GitOps, customizability, etc.&lt;br&gt; 3. TensorZero unifies the entire LLMOps stack, creating compounding benefits. For example, LLM evaluations can be used for fine-tuning models alongside AI judges. &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="30%" valign="top"&gt;&lt;b&gt;Can I use TensorZero with ___?&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="70%" valign="top"&gt;Yes. Every major programming language is supported. You can use TensorZero with our Python client, any OpenAI SDK or OpenAI-compatible client, or our HTTP API.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="30%" valign="top"&gt;&lt;b&gt;Is TensorZero production-ready?&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="70%" valign="top"&gt;Yes. Here's a case study: &lt;b&gt;&lt;a href="https://www.tensorzero.com/blog/case-study-automating-code-changelogs-at-a-large-bank-with-llms"&gt;Automating Code Changelogs at a Large Bank with LLMs&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="30%" valign="top"&gt;&lt;b&gt;How much does TensorZero cost?&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="70%" valign="top"&gt;Nothing. TensorZero is 100% self-hosted and open-source. There are no paid features.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="30%" valign="top"&gt;&lt;b&gt;Who is building TensorZero?&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="70%" valign="top"&gt;Our technical team includes a former Rust compiler maintainer, machine learning researchers (Stanford, CMU, Oxford, Columbia) with thousands of citations, and the chief product officer of a decacorn startup. We're backed by the same investors as leading open-source projects (e.g. ClickHouse, CockroachDB) and AI labs (e.g. OpenAI, Anthropic).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="30%" valign="top"&gt;&lt;b&gt;How do I get started?&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="70%" valign="top"&gt;You can adopt TensorZero incrementally. Our &lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/quickstart"&gt;Quick Start&lt;/a&gt;&lt;/b&gt; goes from a vanilla OpenAI wrapper to a production-ready LLM application with observability and fine-tuning in just 5 minutes.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;hr&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;h3&gt;üåê LLM Gateway&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Integrate with TensorZero once and access every major LLM provider.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Access every major LLM provider (API or self-hosted) through a single unified API&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Infer with streaming, tool use, structured generation (JSON mode), batch, multimodal (VLMs), file inputs, caching, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Define prompt templates and schemas to enforce a consistent, typed interface between your application and the LLMs&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Satisfy extreme throughput and latency needs, thanks to ü¶Ä Rust: &amp;lt;1ms p99 latency overhead at 10k+ QPS&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Integrate using our Python client, any OpenAI SDK or OpenAI-compatible client, or our HTTP API (use any programming language)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Ensure high availability with routing, retries, fallbacks, load balancing, granular timeouts, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled&gt; Soon: embeddings; real-time voice&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;!-- flip highlight order --&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;Model Providers&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;Features&lt;/b&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="left" valign="top"&gt; &lt;p&gt; The TensorZero Gateway natively supports: &lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/anthropic"&gt;Anthropic&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/aws-bedrock"&gt;AWS Bedrock&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/aws-sagemaker"&gt;AWS SageMaker&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/azure"&gt;Azure OpenAI Service&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/deepseek"&gt;DeepSeek&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/fireworks"&gt;Fireworks&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/gcp-vertex-ai-anthropic"&gt;GCP Vertex AI Anthropic&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/gcp-vertex-ai-gemini"&gt;GCP Vertex AI Gemini&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/google-ai-studio-gemini"&gt;Google AI Studio (Gemini API)&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/groq"&gt;Groq&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/hyperbolic"&gt;Hyperbolic&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/mistral"&gt;Mistral&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/openai"&gt;OpenAI&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/openrouter"&gt;OpenRouter&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/sglang"&gt;SGLang&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/tgi"&gt;TGI&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/together"&gt;Together AI&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/vllm"&gt;vLLM&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/xai"&gt;xAI (Grok)&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;p&gt; &lt;em&gt; Need something else? Your provider is most likely supported because TensorZero integrates with &lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/openai-compatible"&gt;any OpenAI-compatible API (e.g. Ollama)&lt;/a&gt;&lt;/b&gt;. &lt;/em&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%" align="left" valign="top"&gt; &lt;p&gt; The TensorZero Gateway supports advanced features like: &lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/retries-fallbacks"&gt;Retries &amp;amp; Fallbacks&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations"&gt;Inference-Time Optimizations&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/prompt-templates-schemas"&gt;Prompt Templates &amp;amp; Schemas&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/experimentation/"&gt;Experimentation (A/B Testing)&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/configuration-reference"&gt;Configuration-as-Code (GitOps)&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/batch-inference"&gt;Batch Inference&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/multimodal-inference"&gt;Multimodal Inference (VLMs)&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/inference-caching"&gt;Inference Caching&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/metrics-feedback"&gt;Metrics &amp;amp; Feedback&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/episodes"&gt;Multi-Step LLM Workflows (Episodes)&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;em&gt;&amp;amp; a lot more...&lt;/em&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;p&gt; The TensorZero Gateway is written in Rust ü¶Ä with &lt;b&gt;performance&lt;/b&gt; in mind (&amp;lt;1ms p99 latency overhead @ 10k QPS). See &lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/benchmarks"&gt;Benchmarks&lt;/a&gt;&lt;/b&gt;.&lt;br&gt; &lt;/p&gt; &lt;p&gt; You can run inference using the &lt;b&gt;TensorZero client&lt;/b&gt; (recommended), the &lt;b&gt;OpenAI client&lt;/b&gt;, or the &lt;b&gt;HTTP API&lt;/b&gt;. &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;br&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;b&gt;Usage: Python ‚Äî TensorZero Client (Recommended)&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;You can access any provider using the TensorZero Python client.&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;code&gt;pip install tensorzero&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;Optional: Set up the TensorZero configuration.&lt;/li&gt; 
  &lt;li&gt;Run inference:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from tensorzero import TensorZeroGateway  # or AsyncTensorZeroGateway


with TensorZeroGateway.build_embedded(clickhouse_url="...", config_file="...") as client:
    response = client.inference(
        model_name="openai::gpt-4o-mini",
        # Try other providers easily: "anthropic::claude-3-7-sonnet-20250219"
        input={
            "messages": [
                {
                    "role": "user",
                    "content": "Write a haiku about artificial intelligence.",
                }
            ]
        },
    )
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;See &lt;strong&gt;&lt;a href="https://www.tensorzero.com/docs/quickstart"&gt;Quick Start&lt;/a&gt;&lt;/strong&gt; for more information.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Usage: Python ‚Äî OpenAI Client&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;You can access any provider using the OpenAI Python client with TensorZero.&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;code&gt;pip install tensorzero&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;Optional: Set up the TensorZero configuration.&lt;/li&gt; 
  &lt;li&gt;Run inference:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI  # or AsyncOpenAI
from tensorzero import patch_openai_client

client = OpenAI()

patch_openai_client(
    client,
    clickhouse_url="http://chuser:chpassword@localhost:8123/tensorzero",
    config_file="config/tensorzero.toml",
    async_setup=False,
)

response = client.chat.completions.create(
    model="tensorzero::model_name::openai::gpt-4o-mini",
    # Try other providers easily: "tensorzero::model_name::anthropic::claude-3-7-sonnet-20250219"
    messages=[
        {
            "role": "user",
            "content": "Write a haiku about artificial intelligence.",
        }
    ],
)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;See &lt;strong&gt;&lt;a href="https://www.tensorzero.com/docs/quickstart"&gt;Quick Start&lt;/a&gt;&lt;/strong&gt; for more information.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Usage: JavaScript / TypeScript (Node) ‚Äî OpenAI Client&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;You can access any provider using the OpenAI Node client with TensorZero.&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Deploy &lt;code&gt;tensorzero/gateway&lt;/code&gt; using Docker. &lt;strong&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/deployment"&gt;Detailed instructions ‚Üí&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;Set up the TensorZero configuration.&lt;/li&gt; 
  &lt;li&gt;Run inference:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-ts"&gt;import OpenAI from "openai";

const client = new OpenAI({
  baseURL: "http://localhost:3000/openai/v1",
});

const response = await client.chat.completions.create({
  model: "tensorzero::model_name::openai::gpt-4o-mini",
  // Try other providers easily: "tensorzero::model_name::anthropic::claude-3-7-sonnet-20250219"
  messages: [
    {
      role: "user",
      content: "Write a haiku about artificial intelligence.",
    },
  ],
});
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;See &lt;strong&gt;&lt;a href="https://www.tensorzero.com/docs/quickstart"&gt;Quick Start&lt;/a&gt;&lt;/strong&gt; for more information.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Usage: Other Languages &amp;amp; Platforms ‚Äî HTTP API&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;TensorZero supports virtually any programming language or platform via its HTTP API.&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Deploy &lt;code&gt;tensorzero/gateway&lt;/code&gt; using Docker. &lt;strong&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/deployment"&gt;Detailed instructions ‚Üí&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;Optional: Set up the TensorZero configuration.&lt;/li&gt; 
  &lt;li&gt;Run inference:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;curl -X POST "http://localhost:3000/inference" \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "openai::gpt-4o-mini",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "Write a haiku about artificial intelligence."
        }
      ]
    }
  }'
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;See &lt;strong&gt;&lt;a href="https://www.tensorzero.com/docs/quickstart"&gt;Quick Start&lt;/a&gt;&lt;/strong&gt; for more information.&lt;/p&gt; 
&lt;/details&gt; 
&lt;br&gt; 
&lt;h3&gt;üîç LLM Observability&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Zoom in to debug individual API calls, or zoom out to monitor metrics across models and prompts over time ‚Äî all using the open-source TensorZero UI.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Store inferences and feedback (metrics, human edits, etc.) in your own database&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Dive into individual inferences or high-level aggregate patterns using the TensorZero UI or programmatically&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Build datasets for optimization, evaluation, and other workflows&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Replay historical inferences with new prompts, models, inference strategies, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Export OpenTelemetry (OTLP) traces to your favorite general-purpose observability tool&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled&gt; Soon: AI-assisted debugging and root cause analysis; AI-assisted data labeling&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;!-- flip highlight order --&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;Observability ¬ª Inference&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;Observability ¬ª Function&lt;/b&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;img src="https://github.com/user-attachments/assets/2cc3cc9a-f33f-4e94-b8de-07522326f80a"&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;img src="https://github.com/user-attachments/assets/00ae6605-8fa0-4efd-8238-ae8ea589860f"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;br&gt; 
&lt;h3&gt;üìà LLM Optimization&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Send production metrics and human feedback to easily optimize your prompts, models, and inference strategies ‚Äî using the UI or programmatically.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Optimize your models with supervised fine-tuning, RLHF, and other techniques&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Optimize your prompts with automated prompt engineering algorithms like MIPROv2&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Optimize your inference strategy with dynamic in-context learning, chain of thought, best/mixture-of-N sampling, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Enable a feedback loop for your LLMs: a data &amp;amp; learning flywheel turning production data into smarter, faster, and cheaper models&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled&gt; Soon: programmatic optimization; synthetic data generation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Model Optimization&lt;/h4&gt; 
&lt;p&gt;Optimize closed-source and open-source models using supervised fine-tuning (SFT) and preference fine-tuning (DPO).&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;!-- flip highlight order --&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;Supervised Fine-tuning ‚Äî UI&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;Preference Fine-tuning (DPO) ‚Äî Jupyter Notebook&lt;/b&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;img src="https://github.com/user-attachments/assets/cf7acf66-732b-43b3-af2a-5eba1ce40f6f"&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;img src="https://github.com/user-attachments/assets/a67a0634-04a7-42b0-b934-9130cb7cdf51"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h4&gt;Inference-Time Optimization&lt;/h4&gt; 
&lt;p&gt;Boost performance by dynamically updating your prompts with relevant examples, combining responses from multiple inferences, and more.&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;!-- flip highlight order --&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#best-of-n-sampling"&gt;Best-of-N Sampling&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#mixture-of-n-sampling"&gt;Mixture-of-N Sampling&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;img src="https://github.com/user-attachments/assets/c0edfa4c-713c-4996-9964-50c0d26e6970"&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;img src="https://github.com/user-attachments/assets/75b5bf05-4c1f-43c4-b158-d69d1b8d05be"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#dynamic-in-context-learning-dicl"&gt;Dynamic In-Context Learning (DICL)&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#chain-of-thought-cot"&gt;Chain-of-Thought (CoT)&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;img src="https://github.com/user-attachments/assets/d8489e92-ce93-46ac-9aab-289ce19bb67d"&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;img src="https://github.com/user-attachments/assets/ea13d73c-76a4-4e0c-a35b-0c648f898311" height="320"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;p&gt;&lt;em&gt;More coming soon...&lt;/em&gt;&lt;/p&gt; 
&lt;br&gt; 
&lt;h4&gt;Prompt Optimization&lt;/h4&gt; 
&lt;p&gt;Optimize your prompts programmatically using research-driven optimization techniques.&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;!-- flip highlight order --&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#best-of-n-sampling"&gt;MIPROv2&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;&lt;a href="https://github.com/tensorzero/tensorzero/tree/main/examples/gsm8k-custom-recipe-dspy"&gt;DSPy Integration&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;img src="https://github.com/user-attachments/assets/d81a7c37-382f-4c46-840f-e6c2593301db" alt="MIPROv2 diagram"&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt; TensorZero comes with several optimization recipes, but you can also easily create your own. This example shows how to optimize a TensorZero function using an arbitrary tool ‚Äî here, DSPy, a popular library for automated prompt engineering. &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;p&gt;&lt;em&gt;More coming soon...&lt;/em&gt;&lt;/p&gt; 
&lt;br&gt; 
&lt;h3&gt;üìä LLM Evaluation&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Compare prompts, models, and inference strategies using evaluations powered by heuristics and LLM judges.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Evaluate individual inferences with &lt;em&gt;static evaluations&lt;/em&gt; powered by heuristics or LLM judges (‚âà unit tests for LLMs)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Evaluate end-to-end workflows with &lt;em&gt;dynamic evaluations&lt;/em&gt; with complete flexibility (‚âà integration tests for LLMs)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Optimize LLM judges just like any other TensorZero function to align them to human preferences&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled&gt; Soon: more built-in evaluators; headless evaluations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;!-- flip highlight order --&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;Evaluation ¬ª UI&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;Evaluation ¬ª CLI&lt;/b&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;img src="https://github.com/user-attachments/assets/f4bf54e3-1b63-46c8-be12-2eaabf615699"&gt;&lt;/td&gt; 
   &lt;td width="50%" align="left" valign="middle"&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker compose run --rm evaluations \
  --evaluation-name extract_data \
  --dataset-name hard_test_cases \
  --variant-name gpt_4o \
  --concurrency 5&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;Run ID: 01961de9-c8a4-7c60-ab8d-15491a9708e4
Number of datapoints: 100
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100/100
exact_match: 0.83 ¬± 0.03
semantic_match: 0.98 ¬± 0.01
item_count: 7.15 ¬± 0.39&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;üß™ LLM Experimentation&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Ship with confidence with built-in A/B testing, routing, fallbacks, retries, etc.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Ship with confidence with built-in A/B testing for models, prompts, providers, hyperparameters, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Enforce principled experiments (RCTs) in complex workflows, including multi-turn and compound LLM systems&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled&gt; Soon: multi-armed bandits; AI-managed experiments&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&amp;amp; more!&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Build with an open-source stack well-suited for prototypes but designed from the ground up to support the most complex LLM applications and deployments.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Build simple applications or massive deployments with GitOps-friendly orchestration&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Extend TensorZero with built-in escape hatches, programmatic-first usage, direct database access, and more&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Integrate with third-party tools: specialized observability and evaluations, model providers, agent orchestration frameworks, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled&gt; Soon: UI playground&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Watch LLMs get better at data extraction in real-time with TensorZero!&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#dynamic-in-context-learning-dicl"&gt;Dynamic in-context learning (DICL)&lt;/a&gt;&lt;/strong&gt; is a powerful inference-time optimization available out of the box with TensorZero. It enhances LLM performance by automatically incorporating relevant historical examples into the prompt, without the need for model fine-tuning.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/4df1022e-886e-48c2-8f79-6af3cdad79cb"&gt;https://github.com/user-attachments/assets/4df1022e-886e-48c2-8f79-6af3cdad79cb&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Get Started&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Start building today.&lt;/strong&gt; The &lt;strong&gt;&lt;a href="https://www.tensorzero.com/docs/quickstart"&gt;Quick Start&lt;/a&gt;&lt;/strong&gt; shows it's easy to set up an LLM application with TensorZero.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Questions?&lt;/strong&gt; Ask us on &lt;strong&gt;&lt;a href="https://www.tensorzero.com/slack"&gt;Slack&lt;/a&gt;&lt;/strong&gt; or &lt;strong&gt;&lt;a href="https://www.tensorzero.com/discord"&gt;Discord&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Using TensorZero at work?&lt;/strong&gt; Email us at &lt;strong&gt;&lt;a href="mailto:hello@tensorzero.com"&gt;hello@tensorzero.com&lt;/a&gt;&lt;/strong&gt; to set up a Slack or Teams channel with your team (free).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Work with us.&lt;/strong&gt; We're &lt;strong&gt;&lt;a href="https://www.tensorzero.com/jobs"&gt;hiring in NYC&lt;/a&gt;&lt;/strong&gt;. We'd also welcome &lt;strong&gt;&lt;a href="https://github.com/tensorzero/tensorzero/raw/main/CONTRIBUTING.md"&gt;open-source contributions&lt;/a&gt;&lt;/strong&gt;!&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;We are working on a series of &lt;strong&gt;complete runnable examples&lt;/strong&gt; illustrating TensorZero's data &amp;amp; learning flywheel.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/tensorzero/tensorzero/tree/main/examples/data-extraction-ner"&gt;Optimizing Data Extraction (NER) with TensorZero&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;This example shows how to use TensorZero to optimize a data extraction pipeline. We demonstrate techniques like fine-tuning and dynamic in-context learning (DICL). In the end, an optimized GPT-4o Mini model outperforms GPT-4o on this task ‚Äî at a fraction of the cost and latency ‚Äî using a small amount of training data.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/tensorzero/tensorzero/tree/main/examples/rag-retrieval-augmented-generation/simple-agentic-rag/"&gt;Agentic RAG ‚Äî Multi-Hop Question Answering with LLMs&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;This example shows how to build a multi-hop retrieval agent using TensorZero. The agent iteratively searches Wikipedia to gather information, and decides when it has enough context to answer a complex question.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/tensorzero/tensorzero/tree/main/examples/haiku-hidden-preferences"&gt;Writing Haikus to Satisfy a Judge with Hidden Preferences&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;This example fine-tunes GPT-4o Mini to generate haikus tailored to a specific taste. You'll see TensorZero's "data flywheel in a box" in action: better variants leads to better data, and better data leads to better variants. You'll see progress by fine-tuning the LLM multiple times.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/tensorzero/tensorzero/tree/main/examples/chess-puzzles/"&gt;Improving LLM Chess Ability with Best-of-N Sampling&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;This example showcases how best-of-N sampling can significantly enhance an LLM's chess-playing abilities by selecting the most promising moves from multiple generated options.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/tensorzero/tensorzero/tree/main/examples/gsm8k-custom-recipe-dspy"&gt;Improving Math Reasoning with a Custom Recipe for Automated Prompt Engineering (DSPy)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;TensorZero provides a number of pre-built optimization recipes covering common LLM engineering workflows. But you can also easily create your own recipes and workflows! This example shows how to optimize a TensorZero function using an arbitrary tool ‚Äî here, DSPy.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;em&gt;&amp;amp; many more on the way!&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>