<rss version="2.0">
  <channel>
    <title>GitHub Python Weekly Trending</title>
    <description>Weekly Trending of Python in GitHub</description>
    <pubDate>Fri, 26 Sep 2025 01:48:25 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>virattt/ai-hedge-fund</title>
      <link>https://github.com/virattt/ai-hedge-fund</link>
      <description>&lt;p&gt;An AI Hedge Fund Team&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Hedge Fund&lt;/h1&gt; 
&lt;p&gt;This is a proof of concept for an AI-powered hedge fund. The goal of this project is to explore the use of AI to make trading decisions. This project is for &lt;strong&gt;educational&lt;/strong&gt; purposes only and is not intended for real trading or investment.&lt;/p&gt; 
&lt;p&gt;This system employs several agents working together:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Aswath Damodaran Agent - The Dean of Valuation, focuses on story, numbers, and disciplined valuation&lt;/li&gt; 
 &lt;li&gt;Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety&lt;/li&gt; 
 &lt;li&gt;Bill Ackman Agent - An activist investor, takes bold positions and pushes for change&lt;/li&gt; 
 &lt;li&gt;Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption&lt;/li&gt; 
 &lt;li&gt;Charlie Munger Agent - Warren Buffett's partner, only buys wonderful businesses at fair prices&lt;/li&gt; 
 &lt;li&gt;Michael Burry Agent - The Big Short contrarian who hunts for deep value&lt;/li&gt; 
 &lt;li&gt;Mohnish Pabrai Agent - The Dhandho investor, who looks for doubles at low risk&lt;/li&gt; 
 &lt;li&gt;Peter Lynch Agent - Practical investor who seeks "ten-baggers" in everyday businesses&lt;/li&gt; 
 &lt;li&gt;Phil Fisher Agent - Meticulous growth investor who uses deep "scuttlebutt" research&lt;/li&gt; 
 &lt;li&gt;Rakesh Jhunjhunwala Agent - The Big Bull of India&lt;/li&gt; 
 &lt;li&gt;Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential&lt;/li&gt; 
 &lt;li&gt;Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price&lt;/li&gt; 
 &lt;li&gt;Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Sentiment Agent - Analyzes market sentiment and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Fundamentals Agent - Analyzes fundamental data and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Technicals Agent - Analyzes technical indicators and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Risk Manager - Calculates risk metrics and sets position limits&lt;/li&gt; 
 &lt;li&gt;Portfolio Manager - Makes final trading decisions and generates orders&lt;/li&gt; 
&lt;/ol&gt; 
&lt;img width="1042" alt="Screenshot 2025-03-22 at 6 19 07 PM" src="https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4" /&gt; 
&lt;p&gt;Note: the system does not actually make any trades.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://twitter.com/virattt"&gt;&lt;img src="https://img.shields.io/twitter/follow/virattt?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This project is for &lt;strong&gt;educational and research purposes only&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Not intended for real trading or investment&lt;/li&gt; 
 &lt;li&gt;No investment advice or guarantees provided&lt;/li&gt; 
 &lt;li&gt;Creator assumes no liability for financial losses&lt;/li&gt; 
 &lt;li&gt;Consult a financial advisor for investment decisions&lt;/li&gt; 
 &lt;li&gt;Past performance does not indicate future results&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By using this software, you agree to use it solely for learning purposes.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-install"&gt;How to Install&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-run"&gt;How to Run&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#%EF%B8%8F-command-line-interface"&gt;âŒ¨ï¸ Command Line Interface&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#%EF%B8%8F-web-application"&gt;ğŸ–¥ï¸ Web Application&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-contribute"&gt;How to Contribute&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#feature-requests"&gt;Feature Requests&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How to Install&lt;/h2&gt; 
&lt;p&gt;Before you can run the AI Hedge Fund, you'll need to install it and set up your API keys. These steps are common to both the full-stack web application and command line interface.&lt;/p&gt; 
&lt;h3&gt;1. Clone the Repository&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Set up API keys&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file for your API keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create .env file for your API keys (in the root directory)
cp .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open and edit the &lt;code&gt;.env&lt;/code&gt; file to add your API keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
OPENAI_API_KEY=your-openai-api-key

# For getting financial data to power the hedge fund
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: You must set at least one LLM API key (e.g. &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;, &lt;code&gt;GROQ_API_KEY&lt;/code&gt;, &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;, or &lt;code&gt;DEEPSEEK_API_KEY&lt;/code&gt;) for the hedge fund to work.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Financial Data&lt;/strong&gt;: Data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key. For any other ticker, you will need to set the &lt;code&gt;FINANCIAL_DATASETS_API_KEY&lt;/code&gt; in the .env file.&lt;/p&gt; 
&lt;h2&gt;How to Run&lt;/h2&gt; 
&lt;h3&gt;âŒ¨ï¸ Command Line Interface&lt;/h3&gt; 
&lt;p&gt;You can run the AI Hedge Fund directly via terminal. This approach offers more granular control and is useful for automation, scripting, and integration purposes.&lt;/p&gt; 
&lt;img width="992" alt="Screenshot 2025-01-06 at 5 50 17 PM" src="https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b" /&gt; 
&lt;h4&gt;Quick Start&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install Poetry (if not already installed):&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -sSL https://install.python-poetry.org | python3 -
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install dependencies:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run the AI Hedge Fund&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also specify a &lt;code&gt;--ollama&lt;/code&gt; flag to run the AI hedge fund using local LLMs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA --ollama
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can optionally specify the start and end dates to make decisions over a specific time period.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run the Backtester&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Example Output:&lt;/strong&gt; &lt;img width="941" alt="Screenshot 2025-01-06 at 5 47 52 PM" src="https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47" /&gt;&lt;/p&gt; 
&lt;p&gt;Note: The &lt;code&gt;--ollama&lt;/code&gt;, &lt;code&gt;--start-date&lt;/code&gt;, and &lt;code&gt;--end-date&lt;/code&gt; flags work for the backtester, as well!&lt;/p&gt; 
&lt;h3&gt;ğŸ–¥ï¸ Web Application&lt;/h3&gt; 
&lt;p&gt;The new way to run the AI Hedge Fund is through our web application that provides a user-friendly interface. This is recommended for users who prefer visual interfaces over command line tools.&lt;/p&gt; 
&lt;p&gt;Please see detailed instructions on how to install and run the web application &lt;a href="https://github.com/virattt/ai-hedge-fund/tree/main/app"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;img width="1721" alt="Screenshot 2025-06-28 at 6 41 03â€¯PM" src="https://github.com/user-attachments/assets/b95ab696-c9f4-416c-9ad1-51feb1f5374b" /&gt; 
&lt;h2&gt;How to Contribute&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repository&lt;/li&gt; 
 &lt;li&gt;Create a feature branch&lt;/li&gt; 
 &lt;li&gt;Commit your changes&lt;/li&gt; 
 &lt;li&gt;Push to the branch&lt;/li&gt; 
 &lt;li&gt;Create a Pull Request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Please keep your pull requests small and focused. This will make it easier to review and merge.&lt;/p&gt; 
&lt;h2&gt;Feature Requests&lt;/h2&gt; 
&lt;p&gt;If you have a feature request, please open an &lt;a href="https://github.com/virattt/ai-hedge-fund/issues"&gt;issue&lt;/a&gt; and make sure it is tagged with &lt;code&gt;enhancement&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>WECENG/ticket-purchase</title>
      <link>https://github.com/WECENG/ticket-purchase</link>
      <description>&lt;p&gt;å¤§éº¦è‡ªåŠ¨æŠ¢ç¥¨ï¼Œæ”¯æŒäººå‘˜ã€åŸå¸‚ã€æ—¥æœŸåœºæ¬¡ã€ä»·æ ¼é€‰æ‹©&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;å¤§éº¦æŠ¢ç¥¨è„šæœ¬ V1.0&lt;/h1&gt; 
&lt;h3&gt;ç‰¹å¾&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;è‡ªåŠ¨æ— å»¶æ—¶æŠ¢ç¥¨&lt;/li&gt; 
 &lt;li&gt;æ”¯æŒäººå‘˜ã€åŸå¸‚ã€æ—¥æœŸåœºæ¬¡ã€ä»·æ ¼é€‰æ‹©&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;åŠŸèƒ½ä»‹ç»&lt;/h2&gt; 
&lt;p&gt;é€šè¿‡seleniumæ‰“å¼€é¡µé¢è¿›è¡Œç™»å½•ï¼Œæ¨¡æ‹Ÿç”¨æˆ·è´­ç¥¨æµç¨‹è‡ªåŠ¨è´­ç¥¨&lt;/p&gt; 
&lt;p&gt;å…¶æµç¨‹å›¾å¦‚ä¸‹:&lt;/p&gt; 
&lt;img src="img/å¤§éº¦æŠ¢ç¥¨æµç¨‹.png" width="50%" height="50%" /&gt; 
&lt;h2&gt;å‡†å¤‡å·¥ä½œ&lt;/h2&gt; 
&lt;h3&gt;1. é…ç½®ç¯å¢ƒ&lt;/h3&gt; 
&lt;h4&gt;1.1å®‰è£…python3ç¯å¢ƒ&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;è®¿é—®Pythonå®˜æ–¹ç½‘ç«™ï¼š&lt;a href="https://www.python.org/downloads/windows/"&gt;https://www.python.org/downloads/windows/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ä¸‹è½½æœ€æ–°çš„Python 3.9+ç‰ˆæœ¬çš„å®‰è£…ç¨‹åºã€‚&lt;/li&gt; 
 &lt;li&gt;è¿è¡Œå®‰è£…ç¨‹åºã€‚&lt;/li&gt; 
 &lt;li&gt;åœ¨å®‰è£…ç¨‹åºä¸­ï¼Œç¡®ä¿å‹¾é€‰ "Add Python X.X to PATH" é€‰é¡¹ï¼Œè¿™å°†è‡ªåŠ¨å°†Pythonæ·»åŠ åˆ°ç³»ç»Ÿç¯å¢ƒå˜é‡ä¸­ï¼Œæ–¹ä¾¿åœ¨å‘½ä»¤è¡Œä¸­ä½¿ç”¨Pythonã€‚&lt;/li&gt; 
 &lt;li&gt;å®Œæˆå®‰è£…åï¼Œä½ å¯ä»¥åœ¨å‘½ä»¤æç¤ºç¬¦æˆ–PowerShellä¸­è¾“å…¥ &lt;code&gt;python3&lt;/code&gt; æ¥å¯åŠ¨Pythonè§£é‡Šå™¨ã€‚&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;macOS&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;ä½ å¯ä»¥ä½¿ç”¨Homebrewæ¥å®‰è£…Python 3ã€‚&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;å®‰è£…Homebrewï¼ˆå¦‚æœæœªå®‰è£…ï¼‰ï¼šæ‰“å¼€ç»ˆç«¯å¹¶è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;å®‰è£…Python 3ï¼šè¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥å®‰è£…Python 3ï¼š&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;brew install python@3
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;1.2 å®‰è£…æ‰€éœ€è¦çš„ç¯å¢ƒ&lt;/h4&gt; 
&lt;p&gt;åœ¨å‘½ä»¤çª—å£è¾“å…¥å¦‚ä¸‹æŒ‡ä»¤&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip3 install selenium
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;1.3 ä¸‹è½½google chromeæµè§ˆå™¨&lt;/h4&gt; 
&lt;p&gt;ä¸‹è½½åœ°å€: &lt;a href="https://www.google.cn/intl/zh-CN/chrome/?brand=YTUH&amp;amp;gclid=Cj0KCQjwj5mpBhDJARIsAOVjBdoV_1sBwdqKGHV3rUU1vJmNKZdy5QNzbRT8F5O0-_jq1WHXurE8a7MaAkWrEALw_wcB&amp;amp;gclsrc=aw.ds"&gt;https://www.google.cn/intl/zh-CN/chrome/?brand=YTUH&amp;amp;gclid=Cj0KCQjwj5mpBhDJARIsAOVjBdoV_1sBwdqKGHV3rUU1vJmNKZdy5QNzbRT8F5O0-_jq1WHXurE8a7MaAkWrEALw_wcB&amp;amp;gclsrc=aw.ds&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;2. ä¿®æ”¹é…ç½®æ–‡ä»¶&lt;/h3&gt; 
&lt;p&gt;åœ¨è¿è¡Œç¨‹åºä¹‹å‰ï¼Œéœ€è¦å…ˆä¿®æ”¹&lt;code&gt;config.json&lt;/code&gt;æ–‡ä»¶ã€‚è¯¥æ–‡ä»¶ç”¨äºæŒ‡å®šç”¨æˆ·éœ€è¦æŠ¢ç¥¨çš„ç›¸å…³ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ¼”å”±ä¼šçš„åœºæ¬¡ã€è§‚æ¼”çš„äººå‘˜ã€åŸå¸‚ã€æ—¥æœŸã€ä»·æ ¼ç­‰ã€‚æ–‡ä»¶ç»“æœå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/WECENG/ticket-purchase/master/img/config_json.png" width="50%" height="50%" /&gt; 
&lt;h4&gt;2.1 æ–‡ä»¶å†…å®¹è¯´æ˜&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;index_url&lt;/code&gt;ä¸ºå¤§éº¦ç½‘çš„åœ°å€ï¼Œ&lt;strong&gt;æ— éœ€ä¿®æ”¹&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;login_url&lt;/code&gt;ä¸ºå¤§éº¦ç½‘çš„ç™»å½•åœ°å€ï¼Œ&lt;strong&gt;æ— éœ€ä¿®æ”¹&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;target_url&lt;/code&gt;ä¸ºç”¨æˆ·éœ€è¦æŠ¢çš„æ¼”å”±ä¼šç¥¨çš„ç›®æ ‡åœ°å€ï¼Œ&lt;strong&gt;å¾…ä¿®æ”¹&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;users&lt;/code&gt;ä¸ºè§‚æ¼”äººçš„å§“åï¼Œ&lt;strong&gt;è§‚æ¼”äººéœ€è¦ç”¨æˆ·åœ¨æ‰‹æœºå¤§éº¦APPä¸­å…ˆå¡«å†™å¥½ï¼Œç„¶åå†å¡«å…¥è¯¥é…ç½®æ–‡ä»¶ä¸­&lt;/strong&gt;ï¼Œ&lt;strong&gt;å¾…ä¿®æ”¹&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;city&lt;/code&gt;ä¸ºåŸå¸‚ï¼Œ&lt;strong&gt;å¦‚æœç”¨æˆ·éœ€è¦æŠ¢çš„æ¼”å”±ä¼šç¥¨éœ€è¦é€‰æ‹©åŸå¸‚ï¼Œè¯·æŠŠåŸå¸‚å¡«å…¥æ­¤å¤„ã€‚å¦‚æ— éœ€é€‰æ‹©ï¼Œåˆ™ä¸å¡«&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;date&lt;/code&gt;ä¸ºåœºæ¬¡æ—¥æœŸï¼Œ&lt;strong&gt;å¾…ä¿®æ”¹ï¼Œå¯å¤šé€‰&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;price&lt;/code&gt;ä¸ºç¥¨æ¡£çš„ä»·æ ¼ï¼Œ&lt;strong&gt;å¾…ä¿®æ”¹ï¼Œå¯å¤šé€‰&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;if_commit_order&lt;/code&gt;ä¸ºæ˜¯å¦è¦è‡ªåŠ¨æäº¤è®¢å•ï¼Œ&lt;strong&gt;æ”¹æˆ true&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;if_listenä¸ºæ˜¯å¦å›æµç›‘å¬ï¼Œ&lt;strong&gt;æ”¹æˆtrue&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;2.2 ç¤ºä¾‹è¯´æ˜&lt;/h4&gt; 
&lt;p&gt;è¿›å…¥å¤§éº¦ç½‘&lt;a href="https://www.damai.cn/%EF%BC%8C%E9%80%89%E6%8B%A9%E4%BD%A0%E9%9C%80%E8%A6%81%E6%8A%A2%E7%A5%A8%E7%9A%84%E6%BC%94%E5%94%B1%E4%BC%9A%E3%80%82%E5%81%87%E8%AE%BE%E5%A6%82%E4%B8%8B%E5%9B%BE%E6%89%80%E7%A4%BA%EF%BC%9A"&gt;https://www.damai.cn/ï¼Œé€‰æ‹©ä½ éœ€è¦æŠ¢ç¥¨çš„æ¼”å”±ä¼šã€‚å‡è®¾å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š&lt;/a&gt;&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/WECENG/ticket-purchase/master/img/example.png" width="50%" height="50%" /&gt; 
&lt;p&gt;æ¥ä¸‹æ¥æŒ‰ç…§ä¸‹å›¾çš„æ ‡æ³¨å¯¹é…ç½®æ–‡ä»¶è¿›è¡Œä¿®æ”¹ï¼š&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/WECENG/ticket-purchase/master/img/example_detail.png" width="50%" height="50%" /&gt; 
&lt;p&gt;æœ€ç»ˆ&lt;code&gt;config.json&lt;/code&gt;çš„æ–‡ä»¶å†…å®¹å¦‚ä¸‹ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "index_url": "https://www.damai.cn/",
  "login_url": "https://passport.damai.cn/login?ru=https%3A%2F%2Fwww.damai.cn%2F",
  "target_url": "https://detail.damai.cn/item.htm?spm=a2oeg.home.card_0.ditem_1.591b23e1JQGWHg&amp;amp;id=740680932762",
  "users": [
    "åå­—1",
    "åå­—2"
  ],
  "city": "å¹¿å·",
  "date": "2023-10-28",
  "price": "1039",
  "if_listen":true,
  "if_commit_order": true
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3.è¿è¡Œç¨‹åº&lt;/h3&gt; 
&lt;p&gt;è¿è¡Œç¨‹åºå¼€å§‹æŠ¢ç¥¨ï¼Œè¿›å…¥å‘½ä»¤çª—å£ï¼Œæ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cd damai
python3 damai.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;å¤§éº¦appæŠ¢ç¥¨&lt;/h1&gt; 
&lt;p&gt;å¤§éº¦appæŠ¢ç¥¨è„šæœ¬éœ€è¦ä¾èµ–appiumï¼Œå› æ­¤éœ€è¦ç°åœ¨å®‰è£…appium server&amp;amp;clientç¯å¢ƒï¼Œæ­¥éª¤å¦‚ä¸‹ï¼š&lt;/p&gt; 
&lt;h2&gt;appium server&lt;/h2&gt; 
&lt;h3&gt;ä¸‹è½½&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;å…ˆå®‰è£…å¥½nodeç¯å¢ƒï¼ˆå…·å¤‡npmï¼‰nodeç‰ˆæœ¬å·18.0.0&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;å…ˆä¸‹è½½å¹¶å®‰è£…å¥½android sdkï¼Œå¹¶é…ç½®ç¯å¢ƒå˜é‡ï¼ˆappium serverè¿è¡Œéœ€ä¾èµ–android sdk)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ä¸‹è½½appium&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;npm install -g appium
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;æŸ¥çœ‹appiumæ˜¯å¦å®‰è£…æˆåŠŸ&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;appium -v
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ä¸‹è½½UiAutomator2é©±åŠ¨&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;npm install appium-uiautomator2-driver
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;â€‹ å¯èƒ½ä¼šé‡åˆ°å¦‚ä¸‹é”™è¯¯ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-tex"&gt;âœ  xcode git:(master) âœ— npm install appium-uiautomator2-driver

npm ERR! code 1
npm ERR! path /Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver/node_modules/appium-chromedriver
npm ERR! command failed
npm ERR! command sh -c node install-npm.js
npm ERR! [11:57:54] Error installing Chromedriver: Request failed with status code 404
npm ERR! [11:57:54] AxiosError: Request failed with status code 404
npm ERR!     at settle (/Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver/node_modules/axios/lib/core/settle.js:19:12)
npm ERR!     at IncomingMessage.handleStreamEnd (/Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver/node_modules/axios/lib/adapters/http.js:572:11)
npm ERR!     at IncomingMessage.emit (node:events:539:35)
npm ERR!     at endReadableNT (node:internal/streams/readable:1344:12)
npm ERR!     at processTicksAndRejections (node:internal/process/task_queues:82:21)
npm ERR! [11:57:54] Downloading Chromedriver can be skipped by setting the'APPIUM_SKIP_CHROMEDRIVER_INSTALL' environment variable.

npm ERR! A complete log of this run can be found in:
npm ERR!     /Users/chenweicheng/.npm/_logs/2023-10-26T03_57_35_950Z-debug-0.log
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;â€‹ è§£å†³åŠæ³•ï¼ˆæ·»åŠ ç¯å¢ƒå˜é‡ï¼Œé”™è¯¯åŸå› æ˜¯æ²¡æœ‰æ‰¾åˆ°chromeæµè§ˆå™¨é©±åŠ¨ï¼Œå¿½ç•¥å³å¯ï¼‰&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;export APPIUM_SKIP_CHROMEDRIVER_INSTALL=true
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;å¯åŠ¨&lt;/h3&gt; 
&lt;p&gt;å¯åŠ¨appium serverå¹¶ä½¿ç”¨uiautomator2é©±åŠ¨&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;appium --use-plugins uiautomator2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;å¯åŠ¨æˆåŠŸå°†å‡ºç°å¦‚ä¸‹ä¿¡æ¯ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;[Appium] Welcome to Appium v2.2.1 (REV 2176894a5be5da17a362bf3f20678641a78f4b69)
[Appium] Non-default server args:
[Appium] {
[Appium]   usePlugins: [
[Appium]     'uiautomator2'
[Appium]   ]
[Appium] }
[Appium] Attempting to load driver uiautomator2...
[Appium] Requiring driver at /Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver
[Appium] Appium REST http interface listener started on http://0.0.0.0:4723
[Appium] You can provide the following URLs in your client code to connect to this server:
[Appium] 	http://127.0.0.1:4723/ (only accessible from the same host)
[Appium] 	http://172.31.102.45:4723/
[Appium] 	http://198.18.0.1:4723/
[Appium] Available drivers:
[Appium]   - uiautomator2@2.32.3 (automationName 'UiAutomator2')
[Appium] No plugins have been installed. Use the "appium plugin" command to install the one(s) you want to use.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;å…¶ä¸­&lt;code&gt;[Appium] http://127.0.0.1:4723/ (only accessible from the same host) [Appium] http://172.31.102.45:4723/ [Appium] http://198.18.0.1:4723/&lt;/code&gt;ä¸ºappium serverè¿æ¥åœ°å€&lt;/p&gt; 
&lt;h2&gt;appium client&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;å…ˆä¸‹è½½å¹¶å®‰è£…å¥½python3å’Œpip3&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;å®‰è£…&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;pip3 install appium-python-client
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;åœ¨ä»£ç ä¸­å¼•å…¥å¹¶ä½¿ç”¨appium&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;from appium import webdriver
from appium.options.common.base import AppiumOptions

device_app_info = AppiumOptions()
device_app_info.set_capability('platformName', 'Android')
device_app_info.set_capability('platformVersion', '10')
device_app_info.set_capability('deviceName', 'YourDeviceName')
device_app_info.set_capability('appPackage', 'cn.damai')
device_app_info.set_capability('appActivity', '.launcher.splash.SplashMainActivity')
device_app_info.set_capability('unicodeKeyboard', True)
device_app_info.set_capability('resetKeyboard', True)
device_app_info.set_capability('noReset', True)
device_app_info.set_capability('newCommandTimeout', 6000)
device_app_info.set_capability('automationName', 'UiAutomator2')

# è¿æ¥appium serverï¼Œserveråœ°å€æŸ¥çœ‹appiumå¯åŠ¨ä¿¡æ¯
driver = webdriver.Remote('http://127.0.0.1:4723', options=device_app_info)

&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;å¯åŠ¨è„šæœ¬ç¨‹åº&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;cd damai_appium
python3 damai_appium.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>TheAlgorithms/Python</title>
      <link>https://github.com/TheAlgorithms/Python</link>
      <description>&lt;p&gt;All Algorithms implemented in Python&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;!-- Title: --&gt; 
 &lt;a href="https://github.com/TheAlgorithms/"&gt; &lt;img src="https://raw.githubusercontent.com/TheAlgorithms/website/1cd824df116b27029f17c2d1b42d81731f28a920/public/logo.svg?sanitize=true" height="100" /&gt; &lt;/a&gt; 
 &lt;h1&gt;&lt;a href="https://github.com/TheAlgorithms/"&gt;The Algorithms&lt;/a&gt; - Python&lt;/h1&gt; 
 &lt;!-- Labels: --&gt; 
 &lt;!-- First row: --&gt; 
 &lt;a href="https://gitpod.io/#https://github.com/TheAlgorithms/Python"&gt; &lt;img src="https://img.shields.io/badge/Gitpod-Ready--to--Code-blue?logo=gitpod&amp;amp;style=flat-square" height="20" alt="Gitpod Ready-to-Code" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/TheAlgorithms/Python/raw/master/CONTRIBUTING.md"&gt; &lt;img src="https://img.shields.io/static/v1.svg?label=Contributions&amp;amp;message=Welcome&amp;amp;color=0059b3&amp;amp;style=flat-square" height="20" alt="Contributions Welcome" /&gt; &lt;/a&gt; 
 &lt;img src="https://img.shields.io/github/repo-size/TheAlgorithms/Python.svg?label=Repo%20size&amp;amp;style=flat-square" height="20" /&gt; 
 &lt;a href="https://the-algorithms.com/discord"&gt; &lt;img src="https://img.shields.io/discord/808045925556682782.svg?logo=discord&amp;amp;colorB=7289DA&amp;amp;style=flat-square" height="20" alt="Discord chat" /&gt; &lt;/a&gt; 
 &lt;a href="https://gitter.im/TheAlgorithms/community"&gt; &lt;img src="https://img.shields.io/badge/Chat-Gitter-ff69b4.svg?label=Chat&amp;amp;logo=gitter&amp;amp;style=flat-square" height="20" alt="Gitter chat" /&gt; &lt;/a&gt; 
 &lt;!-- Second row: --&gt; 
 &lt;br /&gt; 
 &lt;a href="https://github.com/TheAlgorithms/Python/actions"&gt; &lt;img src="https://img.shields.io/github/actions/workflow/status/TheAlgorithms/Python/build.yml?branch=master&amp;amp;label=CI&amp;amp;logo=github&amp;amp;style=flat-square" height="20" alt="GitHub Workflow Status" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/pre-commit/pre-commit"&gt; &lt;img src="https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&amp;amp;logoColor=white&amp;amp;style=flat-square" height="20" alt="pre-commit" /&gt; &lt;/a&gt; 
 &lt;a href="https://docs.astral.sh/ruff/formatter/"&gt; &lt;img src="https://img.shields.io/static/v1?label=code%20style&amp;amp;message=ruff&amp;amp;color=black&amp;amp;style=flat-square" height="20" alt="code style: black" /&gt; &lt;/a&gt; 
 &lt;!-- Short description: --&gt; 
 &lt;h3&gt;All algorithms implemented in Python - for education ğŸ“š&lt;/h3&gt; 
&lt;/div&gt; 
&lt;p&gt;Implementations are for learning purposes only. They may be less efficient than the implementations in the Python standard library. Use them at your discretion.&lt;/p&gt; 
&lt;h2&gt;ğŸš€ Getting Started&lt;/h2&gt; 
&lt;p&gt;ğŸ“‹ Read through our &lt;a href="https://raw.githubusercontent.com/TheAlgorithms/Python/master/CONTRIBUTING.md"&gt;Contribution Guidelines&lt;/a&gt; before you contribute.&lt;/p&gt; 
&lt;h2&gt;ğŸŒ Community Channels&lt;/h2&gt; 
&lt;p&gt;We are on &lt;a href="https://the-algorithms.com/discord"&gt;Discord&lt;/a&gt; and &lt;a href="https://gitter.im/TheAlgorithms/community"&gt;Gitter&lt;/a&gt;! Community channels are a great way for you to ask questions and get help. Please join us!&lt;/p&gt; 
&lt;h2&gt;ğŸ“œ List of Algorithms&lt;/h2&gt; 
&lt;p&gt;See our &lt;a href="https://raw.githubusercontent.com/TheAlgorithms/Python/master/DIRECTORY.md"&gt;directory&lt;/a&gt; for easier navigation and a better overview of the project.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PaddlePaddle/PaddleOCR</title>
      <link>https://github.com/PaddlePaddle/PaddleOCR</link>
      <description>&lt;p&gt;Turn any PDF or image document into structured data for your AI. A powerful, lightweight OCR toolkit that bridges the gap between images/PDFs and LLMs. Supports 80+ languages.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/Banner.png" alt="PaddleOCR Banner" /&gt; &lt;/p&gt; 
 &lt;p&gt;English | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_cn.md"&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_tcn.md"&gt;ç¹é«”ä¸­æ–‡&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ja.md"&gt;æ—¥æœ¬èª&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ko.md"&gt;í•œêµ­ì–´&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_fr.md"&gt;FranÃ§ais&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ru.md"&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_es.md"&gt;EspaÃ±ol&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ar.md"&gt;Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleOCR"&gt;&lt;img src="https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?color=ccf" alt="stars" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2507.05595"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2507.05595-b31b1b.svg?logo=arXiv" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/paddleocr"&gt;&lt;img src="https://static.pepy.tech/badge/paddleocr/month" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/paddleocr"&gt;&lt;img src="https://static.pepy.tech/badge/paddleocr" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/PaddlePaddle/PaddleOCR/network/dependents"&gt;&lt;img src="https://img.shields.io/badge/Used%20by-5.9k%2B%20repositories-blue" alt="Used by" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://img.shields.io/badge/python-3.8~3.12-aff.svg?sanitize=true" alt="python" /&gt; &lt;img src="https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg?sanitize=true" alt="os" /&gt; &lt;img src="https://img.shields.io/badge/hardware-cpu%2C%20gpu%2C%20xpu%2C%20npu-yellow.svg?sanitize=true" alt="hardware" /&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-Apache_2.0-green" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/PaddlePaddle/PaddleOCR"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;PaddleOCR is an industry-leading, production-ready OCR and document AI engine, offering end-to-end solutions from text extraction to intelligent document understanding&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h1&gt;PaddleOCR&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://www.paddlepaddle.org.cn/en"&gt;&lt;img src="https://img.shields.io/badge/PaddlePaddle-3.0-orange" alt="Framework" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Recognition%20Accuracy-%F0%9F%8F%86-green" alt="Accuracy" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Support_Languages-80+-brightgreen" alt="Multi-Language" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Handwriting-%E2%9C%93-success" alt="Handwriting" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Heterogeneous%20Hardware-Kunlunxin%20%7C%20Ascend_NPU-red" alt="Hardware" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] PaddleOCR now provides an MCP server that supports integration with Agent applications like Claude Desktop. For details, please refer to &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html"&gt;PaddleOCR MCP Server&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;The PaddleOCR 3.0 Technical Report is now available. See details at: &lt;a href="https://arxiv.org/abs/2507.05595"&gt;PaddleOCR 3.0 Technical Report&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;PaddleOCR&lt;/strong&gt; converts documents and images into &lt;strong&gt;structured, AI-friendly data&lt;/strong&gt; (like JSON and Markdown) with &lt;strong&gt;industry-leading accuracy&lt;/strong&gt;â€”powering AI applications for everyone from indie developers and startups to large enterprises worldwide. With over &lt;strong&gt;50,000 stars&lt;/strong&gt; and deep integration into leading projects like &lt;strong&gt;MinerU, RAGFlow, and OmniParser&lt;/strong&gt;, PaddleOCR has become the &lt;strong&gt;premier solution&lt;/strong&gt; for developers building intelligent document applications in the &lt;strong&gt;AI era&lt;/strong&gt;.&lt;/p&gt; 
&lt;h3&gt;PaddleOCR 3.0 Core Features&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aistudio.baidu.com/community/app/91660/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_OCRv5-Demo_on_AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518494/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_StructureV3-Demo_on_AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518493/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_ChatOCRv4-Demo_on_AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://www.modelscope.cn/organization/PaddlePaddle"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%96_Demo_on_ModelScope-purple" alt="ModelScope" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/PaddlePaddle"&gt;&lt;img src="https://img.shields.io/badge/Demo_on_HuggingFace-purple.svg?logo=huggingface" alt="HuggingFace" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-OCRv5 â€” Universal Scene Text Recognition&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;Single model supports five text types&lt;/strong&gt; (Simplified Chinese, Traditional Chinese, English, Japanese, and Pinyin) with &lt;strong&gt;13% accuracy improvement&lt;/strong&gt;. Solves multilingual mixed document recognition challenges.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-StructureV3 â€” Complex Document Parsing&lt;/strong&gt;&lt;br /&gt; Intelligently converts complex PDFs and document images into &lt;strong&gt;Markdown and JSON files that preserve original structure&lt;/strong&gt;. &lt;strong&gt;Outperforms&lt;/strong&gt; numerous commercial solutions in public benchmarks. &lt;strong&gt;Perfectly maintains document layout and hierarchical structure&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-ChatOCRv4 â€” Intelligent Information Extraction&lt;/strong&gt;&lt;br /&gt; Natively integrates ERNIE 4.5 to &lt;strong&gt;precisely extract key information&lt;/strong&gt; from massive documents, with 15% accuracy improvement over previous generation. Makes documents "&lt;strong&gt;understand&lt;/strong&gt;" your questions and provide accurate answers.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In addition to providing an outstanding model library, PaddleOCR 3.0 also offers user-friendly tools covering model training, inference, and service deployment, so developers can rapidly bring AI applications to production.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/Arch.jpg" alt="PaddleOCR Architecture" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Special Note&lt;/strong&gt;: PaddleOCR 3.x introduces several significant interface changes. &lt;strong&gt;Old code written based on PaddleOCR 2.x is likely incompatible with PaddleOCR 3.x&lt;/strong&gt;. Please ensure that the documentation you are reading matches the version of PaddleOCR you are using. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/upgrade_notes.html"&gt;This document&lt;/a&gt; explains the reasons for the upgrade and the major changes from PaddleOCR 2.x to 3.x.&lt;/p&gt; 
&lt;h2&gt;ğŸ“£ Recent updates&lt;/h2&gt; 
&lt;h3&gt;ğŸ”¥ğŸ”¥2025.08.21: Release of PaddleOCR 3.2.0, includes:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Significant Model Additions:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Introduced training, inference, and deployment for PP-OCRv5 recognition models in English, Thai, and Greek. &lt;strong&gt;The PP-OCRv5 English model delivers an 11% improvement in English scenarios compared to the main PP-OCRv5 model, with the Thai and Greek recognition models achieving accuracies of 82.68% and 89.28%, respectively.&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deployment Capability Upgrades:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Full support for PaddlePaddle framework versions 3.1.0 and 3.1.1.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Comprehensive upgrade of the PP-OCRv5 C++ local deployment solution, now supporting both Linux and Windows, with feature parity and identical accuracy to the Python implementation.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;High-performance inference now supports CUDA 12, and inference can be performed using either the Paddle Inference or ONNX Runtime backends.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;The high-stability service-oriented deployment solution is now fully open-sourced, allowing users to customize Docker images and SDKs as required.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;The high-stability service-oriented deployment solution also supports invocation via manually constructed HTTP requests, enabling client-side code development in any programming language.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Benchmark Support:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;All production lines now support fine-grained benchmarking, enabling measurement of end-to-end inference time as well as per-layer and per-module latency data to assist with performance analysis. &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/version3.x/pipeline_usage/instructions/benchmark.en.md"&gt;Here's&lt;/a&gt; how to set up and use the benchmark feature.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Documentation has been updated to include key metrics for commonly used configurations on mainstream hardware, such as inference latency and memory usage, providing deployment references for users.&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Resolved the issue of failed log saving during model training.&lt;/li&gt; 
   &lt;li&gt;Upgraded the data augmentation component for formula models for compatibility with newer versions of the albumentations dependency, and fixed deadlock warnings when using the tokenizers package in multi-process scenarios.&lt;/li&gt; 
   &lt;li&gt;Fixed inconsistencies in switch behaviors (e.g., &lt;code&gt;use_chart_parsing&lt;/code&gt;) in the PP-StructureV3 configuration files compared to other pipelines.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Other Enhancements:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Separated core and optional dependencies. Only minimal core dependencies are required for basic text recognition; additional dependencies for document parsing and information extraction can be installed as needed.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Enabled support for NVIDIA RTX 50 series graphics cards on Windows; users can refer to the &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/version3.x/installation.en.md"&gt;installation guide&lt;/a&gt; for the corresponding PaddlePaddle framework versions.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;PP-OCR series models now support returning single-character coordinates.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;Added AIStudio, ModelScope, and other model download sources, allowing users to specify the source for model downloads.&lt;/li&gt; 
   &lt;li&gt;Added support for chart-to-table conversion via the PP-Chart2Table module.&lt;/li&gt; 
   &lt;li&gt;Optimized documentation descriptions to improve usability.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.08.15: PaddleOCR 3.1.1 Released&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Added the missing methods &lt;code&gt;save_vector&lt;/code&gt;, &lt;code&gt;save_visual_info_list&lt;/code&gt;, &lt;code&gt;load_vector&lt;/code&gt;, and &lt;code&gt;load_visual_info_list&lt;/code&gt; in the &lt;code&gt;PP-ChatOCRv4&lt;/code&gt; class.&lt;/li&gt; 
    &lt;li&gt;Added the missing parameters &lt;code&gt;glossary&lt;/code&gt; and &lt;code&gt;llm_request_interval&lt;/code&gt; to the &lt;code&gt;translate&lt;/code&gt; method in the &lt;code&gt;PPDocTranslation&lt;/code&gt; class.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Improvements:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Added a demo to the MCP documentation.&lt;/li&gt; 
    &lt;li&gt;Added information about the PaddlePaddle and PaddleOCR version used for performance metrics testing in the documentation.&lt;/li&gt; 
    &lt;li&gt;Fixed errors and omissions in the production line document translation.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Others:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Changed the MCP server dependency to use the pure Python library &lt;code&gt;puremagic&lt;/code&gt; instead of &lt;code&gt;python-magic&lt;/code&gt; to reduce installation issues.&lt;/li&gt; 
    &lt;li&gt;Retested PP-OCRv5 performance metrics with PaddleOCR version 3.1.0 and updated the documentation.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.29: PaddleOCR 3.1.0 Released&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Key Models and Pipelines:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Added PP-OCRv5 Multilingual Text Recognition Model&lt;/strong&gt;, which supports the training and inference process for text recognition models in 37 languages, including French, Spanish, Portuguese, Russian, Korean, etc. &lt;strong&gt;Average accuracy improved by over 30%.&lt;/strong&gt; &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/algorithm/PP-OCRv5/PP-OCRv5_multi_languages.html"&gt;Details&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Upgraded the &lt;strong&gt;PP-Chart2Table model&lt;/strong&gt; in PP-StructureV3, further enhancing the capability of converting charts to tables. On internal custom evaluation sets, the metric (RMS-F1) &lt;strong&gt;increased by 9.36 percentage points (71.24% -&amp;gt; 80.60%).&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Newly launched &lt;strong&gt;document translation pipeline, PP-DocTranslation, based on PP-StructureV3 and ERNIE 4.5&lt;/strong&gt;, which supports the translation of Markdown format documents, various complex-layout PDF documents, and document images, with the results saved as Markdown format documents. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/pipeline_usage/PP-DocTranslation.html"&gt;Details&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;New MCP server:&lt;/strong&gt; &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html"&gt;Details&lt;/a&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Supports both OCR and PP-StructureV3 pipelines.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Supports three working modes: local Python library, AIStudio Community Cloud Service, and self-hosted service.&lt;/li&gt; 
    &lt;li&gt;Supports invoking local services via stdio and remote services via Streamable HTTP.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Optimization:&lt;/strong&gt; Improved the descriptions in some user guides for a smoother reading experience.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.26: PaddleOCR 3.0.3 Released&lt;/strong&gt;&lt;/summary&gt; - Bug Fix: Resolved the issue where the `enable_mkldnn` parameter was not effective, restoring the default behavior of using MKL-DNN for CPU inference. 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.19: PaddleOCR 3.0.2 Released&lt;/strong&gt;&lt;/summary&gt; - **New Features:** 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;The default download source has been changed from &lt;code&gt;BOS&lt;/code&gt; to &lt;code&gt;HuggingFace&lt;/code&gt;. Users can also change the environment variable &lt;code&gt;PADDLE_PDX_MODEL_SOURCE&lt;/code&gt; to &lt;code&gt;BOS&lt;/code&gt; to set the model download source back to Baidu Object Storage (BOS).&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Added service invocation examples for six languagesâ€”C++, Java, Go, C#, Node.js, and PHPâ€”for pipelines like PP-OCRv5, PP-StructureV3, and PP-ChatOCRv4.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Improved the layout partition sorting algorithm in the PP-StructureV3 pipeline, enhancing the sorting logic for complex vertical layouts to deliver better results.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Enhanced model selection logic: when a language is specified but a model version is not, the system will automatically select the latest model version supporting that language.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Set a default upper limit for MKL-DNN cache size to prevent unlimited growth, while also allowing users to configure cache capacity.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Updated default configurations for high-performance inference to support Paddle MKL-DNN acceleration and optimized the logic for automatic configuration selection for smarter choices.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Adjusted the logic for obtaining the default device to consider the actual support for computing devices by the installed Paddle framework, making program behavior more intuitive.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Added Android example for PP-OCRv5. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/on_device_deployment.html"&gt;Details&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Fixed an issue with some CLI parameters in PP-StructureV3 not taking effect.&lt;/li&gt; 
    &lt;li&gt;Resolved an issue where &lt;code&gt;export_paddlex_config_to_yaml&lt;/code&gt; would not function correctly in certain cases.&lt;/li&gt; 
    &lt;li&gt;Corrected the discrepancy between the actual behavior of &lt;code&gt;save_path&lt;/code&gt; and its documentation description.&lt;/li&gt; 
    &lt;li&gt;Fixed potential multithreading errors when using MKL-DNN in basic service deployment.&lt;/li&gt; 
    &lt;li&gt;Corrected channel order errors in image preprocessing for the Latex-OCR model.&lt;/li&gt; 
    &lt;li&gt;Fixed channel order errors in saving visualized images within the text recognition module.&lt;/li&gt; 
    &lt;li&gt;Resolved channel order errors in visualized table results within PP-StructureV3 pipeline.&lt;/li&gt; 
    &lt;li&gt;Fixed an overflow issue in the calculation of &lt;code&gt;overlap_ratio&lt;/code&gt; under extremely special circumstances in the PP-StructureV3 pipeline.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Improvements:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Updated the description of the &lt;code&gt;enable_mkldnn&lt;/code&gt; parameter in the documentation to accurately reflect the program's actual behavior.&lt;/li&gt; 
    &lt;li&gt;Fixed errors in the documentation regarding the &lt;code&gt;lang&lt;/code&gt; and &lt;code&gt;ocr_version&lt;/code&gt; parameters.&lt;/li&gt; 
    &lt;li&gt;Added instructions for exporting pipeline configuration files via CLI.&lt;/li&gt; 
    &lt;li&gt;Fixed missing columns in the performance data table for PP-OCRv5.&lt;/li&gt; 
    &lt;li&gt;Refined benchmark metrics for PP-StructureV3 across different configurations.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Others:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Relaxed version restrictions on dependencies like numpy and pandas, restoring support for Python 3.12.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;History Log&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;2025.06.05: &lt;strong&gt;PaddleOCR 3.0.1 Released&lt;/strong&gt;, includes:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Optimisation of certain models and model configurations:&lt;/strong&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Updated the default model configuration for PP-OCRv5, changing both detection and recognition from mobile to server models. To improve default performance in most scenarios, the parameter &lt;code&gt;limit_side_len&lt;/code&gt; in the configuration has been changed from 736 to 64.&lt;/li&gt; 
    &lt;li&gt;Added a new text line orientation classification model &lt;code&gt;PP-LCNet_x1_0_textline_ori&lt;/code&gt; with an accuracy of 99.42%. The default text line orientation classifier for OCR, PP-StructureV3, and PP-ChatOCRv4 pipelines has been updated to this model.&lt;/li&gt; 
    &lt;li&gt;Optimized the text line orientation classification model &lt;code&gt;PP-LCNet_x0_25_textline_ori&lt;/code&gt;, improving accuracy by 3.3 percentage points to a current accuracy of 98.85%.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Optimizations and fixes for some issues in version 3.0.0, &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/update.html"&gt;details&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;ğŸ”¥ğŸ”¥2025.05.20: Official Release of &lt;strong&gt;PaddleOCR v3.0&lt;/strong&gt;, including:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-OCRv5&lt;/strong&gt;: High-Accuracy Text Recognition Model for All Scenarios - Instant Text from Images/PDFs.&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;ğŸŒ Single-model support for &lt;strong&gt;five&lt;/strong&gt; text types - Seamlessly process &lt;strong&gt;Simplified Chinese, Traditional Chinese, Simplified Chinese Pinyin, English&lt;/strong&gt; and &lt;strong&gt;Japanese&lt;/strong&gt; within a single model.&lt;/li&gt; 
    &lt;li&gt;âœï¸ Improved &lt;strong&gt;handwriting recognition&lt;/strong&gt;: Significantly better at complex cursive scripts and non-standard handwriting.&lt;/li&gt; 
    &lt;li&gt;ğŸ¯ &lt;strong&gt;13-point accuracy gain&lt;/strong&gt; over PP-OCRv4, achieving state-of-the-art performance across a variety of real-world scenarios.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-StructureV3&lt;/strong&gt;: General-Purpose Document Parsing â€“ Unleash SOTA Images/PDFs Parsing for Real-World Scenarios!&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;ğŸ§® &lt;strong&gt;High-Accuracy multi-scene PDF parsing&lt;/strong&gt;, leading both open- and closed-source solutions on the OmniDocBench benchmark.&lt;/li&gt; 
    &lt;li&gt;ğŸ§  Specialized capabilities include &lt;strong&gt;seal recognition&lt;/strong&gt;, &lt;strong&gt;chart-to-table conversion&lt;/strong&gt;, &lt;strong&gt;table recognition with nested formulas/images&lt;/strong&gt;, &lt;strong&gt;vertical text document parsing&lt;/strong&gt;, and &lt;strong&gt;complex table structure analysis&lt;/strong&gt;.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-ChatOCRv4&lt;/strong&gt;: Intelligent Document Understanding â€“ Extract Key Information, not just text from Images/PDFs.&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;ğŸ”¥ &lt;strong&gt;15-point accuracy gain&lt;/strong&gt; in key-information extraction on PDF/PNG/JPG files over the previous generation.&lt;/li&gt; 
    &lt;li&gt;ğŸ’» Native support for &lt;strong&gt;ERNIE 4.5&lt;/strong&gt;, with compatibility for large-model deployments via PaddleNLP, Ollama, vLLM, and more.&lt;/li&gt; 
    &lt;li&gt;ğŸ¤ Integrated &lt;a href="https://github.com/PaddlePaddle/PaddleMIX/tree/develop/paddlemix/examples/ppdocbee2"&gt;PP-DocBee2&lt;/a&gt;, enabling extraction and understanding of printed text, handwriting, seals, tables, charts, and other common elements in complex documents.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/update.html"&gt;History Log&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;âš¡ Quick Start&lt;/h2&gt; 
&lt;h3&gt;1. Run online demo&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aistudio.baidu.com/community/app/91660/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_OCRv5-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518494/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_StructureV3-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518493/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_ChatOCRv4-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;2. Installation&lt;/h3&gt; 
&lt;p&gt;Install PaddlePaddle refer to &lt;a href="https://www.paddlepaddle.org.cn/en/install/quick?docurl=/documentation/docs/en/develop/install/pip/linux-pip_en.html"&gt;Installation Guide&lt;/a&gt;, after then, install the PaddleOCR toolkit.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# If you only want to use the basic text recognition feature (returns text position coordinates and content), including the PP-OCR series
python -m pip install paddleocr
# If you want to use all features such as document parsing, document understanding, document translation, key information extraction, etc.
# python -m pip install "paddleocr[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Starting from version 3.2.0, in addition to the &lt;code&gt;all&lt;/code&gt; dependency group demonstrated above, PaddleOCR also supports installing partial optional features by specifying other dependency groups. All dependency groups provided by PaddleOCR are as follows:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Dependency Group Name&lt;/th&gt; 
   &lt;th&gt;Corresponding Functionality&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;doc-parser&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Document parsing: can be used to extract layout elements such as tables, formulas, stamps, images, etc. from documents; includes models like PP-StructureV3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ie&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Information extraction: can be used to extract key information from documents, such as names, dates, addresses, amounts, etc.; includes models like PP-ChatOCRv4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;trans&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Document translation: can be used to translate documents from one language to another; includes models like PP-DocTranslation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;all&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Complete functionality&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;3. Run inference by CLI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run PP-OCRv5 inference
paddleocr ocr -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png --use_doc_orientation_classify False --use_doc_unwarping False --use_textline_orientation False  

# Run PP-StructureV3 inference
paddleocr pp_structurev3 -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png --use_doc_orientation_classify False --use_doc_unwarping False

# Get the Qianfan API Key at first, and then run PP-ChatOCRv4 inference
paddleocr pp_chatocrv4_doc -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png -k é©¾é©¶å®¤å‡†ä¹˜äººæ•° --qianfan_api_key your_api_key --use_doc_orientation_classify False --use_doc_unwarping False 

# Get more information about "paddleocr ocr"
paddleocr ocr --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4. Run inference by API&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;4.1 PP-OCRv5 Example&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Initialize PaddleOCR instance
from paddleocr import PaddleOCR
ocr = PaddleOCR(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False,
    use_textline_orientation=False)

# Run OCR inference on a sample image 
result = ocr.predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png")

# Visualize the results and save the JSON results
for res in result:
    res.print()
    res.save_to_img("output")
    res.save_to_json("output")
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;4.2 PP-StructureV3 Example&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from pathlib import Path
from paddleocr import PPStructureV3

pipeline = PPStructureV3(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False
)

# For Image
output = pipeline.predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png",
)

# Visualize the results and save the JSON results
for res in output:
    res.print() 
    res.save_to_json(save_path="output") 
    res.save_to_markdown(save_path="output")           
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;4.3 PP-ChatOCRv4 Example&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from paddleocr import PPChatOCRv4Doc

chat_bot_config = {
    "module_name": "chat_bot",
    "model_name": "ernie-3.5-8k",
    "base_url": "https://qianfan.baidubce.com/v2",
    "api_type": "openai",
    "api_key": "api_key",  # your api_key
}

retriever_config = {
    "module_name": "retriever",
    "model_name": "embedding-v1",
    "base_url": "https://qianfan.baidubce.com/v2",
    "api_type": "qianfan",
    "api_key": "api_key",  # your api_key
}

pipeline = PPChatOCRv4Doc(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False
)

visual_predict_res = pipeline.visual_predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png",
    use_common_ocr=True,
    use_seal_recognition=True,
    use_table_recognition=True,
)

mllm_predict_info = None
use_mllm = False
# If a multimodal large model is used, the local mllm service needs to be started. You can refer to the documentation: https://github.com/PaddlePaddle/PaddleX/blob/release/3.0/docs/pipeline_usage/tutorials/vlm_pipelines/doc_understanding.en.md performs deployment and updates the mllm_chat_bot_config configuration.
if use_mllm:
    mllm_chat_bot_config = {
        "module_name": "chat_bot",
        "model_name": "PP-DocBee",
        "base_url": "http://127.0.0.1:8080/",  # your local mllm service url
        "api_type": "openai",
        "api_key": "api_key",  # your api_key
    }

    mllm_predict_res = pipeline.mllm_pred(
        input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png",
        key_list=["é©¾é©¶å®¤å‡†ä¹˜äººæ•°"],
        mllm_chat_bot_config=mllm_chat_bot_config,
    )
    mllm_predict_info = mllm_predict_res["mllm_res"]

visual_info_list = []
for res in visual_predict_res:
    visual_info_list.append(res["visual_info"])
    layout_parsing_result = res["layout_parsing_result"]

vector_info = pipeline.build_vector(
    visual_info_list, flag_save_bytes_vector=True, retriever_config=retriever_config
)
chat_result = pipeline.chat(
    key_list=["é©¾é©¶å®¤å‡†ä¹˜äººæ•°"],
    visual_info=visual_info_list,
    vector_info=vector_info,
    mllm_predict_info=mllm_predict_info,
    chat_bot_config=chat_bot_config,
    retriever_config=retriever_config,
)
print(chat_result)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;5. Chinese Heterogeneous AI Accelerators&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/other_devices_support/paddlepaddle_install_NPU.html"&gt;Huawei Ascend&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/other_devices_support/paddlepaddle_install_XPU.html"&gt;KUNLUNXIN&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ§© More Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Convert models to ONNX format: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/obtaining_onnx_models.html"&gt;Obtaining ONNX Models&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Accelerate inference using engines like OpenVINO, ONNX Runtime, TensorRT, or perform inference using ONNX format models: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/high_performance_inference.html"&gt;High-Performance Inference&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Accelerate inference using multi-GPU and multi-process: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/pipeline_usage/instructions/parallel_inference.html"&gt;Parallel Inference for Pipelines&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Integrate PaddleOCR into applications written in C++, C#, Java, etc.: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/serving.html"&gt;Serving&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;â›°ï¸ Advanced Tutorials&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/OCR.html"&gt;PP-OCRv5 Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PP-StructureV3.html"&gt;PP-StructureV3 Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PP-ChatOCRv4.html"&gt;PP-ChatOCRv4 Tutorial&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ”„ Quick Overview of Execution Results&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/demo.gif" alt="PP-OCRv5 Demo" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/blue_v3.gif" alt="PP-StructureV3 Demo" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;âœ¨ Stay Tuned&lt;/h2&gt; 
&lt;p&gt;â­ &lt;strong&gt;Star this repository to keep up with exciting updates and new releases, including powerful OCR and document parsing capabilities!&lt;/strong&gt; â­&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="1200" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/star_paddleocr.en.gif" alt="Star-Project" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Community&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;PaddlePaddle WeChat official account&lt;/th&gt; 
    &lt;th align="center"&gt;Join the tech discussion group&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/refs/heads/main/images/paddleocr/README/qrcode_for_paddlepaddle_official_account.jpg" width="150" /&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/refs/heads/main/images/paddleocr/README/qr_code_for_the_questionnaire.jpg" width="150" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ˜ƒ Awesome Projects Leveraging PaddleOCR&lt;/h2&gt; 
&lt;p&gt;PaddleOCR wouldn't be where it is today without its incredible community! ğŸ’— A massive thank you to all our longtime partners, new collaborators, and everyone who's poured their passion into PaddleOCR â€” whether we've named you or not. Your support fuels our fire!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Project Name&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/infiniflow/ragflow"&gt;RAGFlow&lt;/a&gt; &lt;a href="https://github.com/infiniflow/ragflow"&gt;&lt;img src="https://img.shields.io/github/stars/infiniflow/ragflow" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;RAG engine based on deep document understanding.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/opendatalab/MinerU"&gt;MinerU&lt;/a&gt; &lt;a href="https://github.com/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/github/stars/opendatalab/MinerU" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Multi-type Document to Markdown Conversion Tool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hiroi-sora/Umi-OCR"&gt;Umi-OCR&lt;/a&gt; &lt;a href="https://github.com/hiroi-sora/Umi-OCR"&gt;&lt;img src="https://img.shields.io/github/stars/hiroi-sora/Umi-OCR" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Free, Open-source, Batch Offline OCR Software.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/OmniParser"&gt;OmniParser&lt;/a&gt;&lt;a href="https://github.com/microsoft/OmniParser"&gt;&lt;img src="https://img.shields.io/github/stars/microsoft/OmniParser" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;OmniParser: Screen Parsing tool for Pure Vision Based GUI Agent.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/netease-youdao/QAnything"&gt;QAnything&lt;/a&gt;&lt;a href="https://github.com/netease-youdao/QAnything"&gt;&lt;img src="https://img.shields.io/github/stars/netease-youdao/QAnything" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Question and Answer based on Anything.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;PDF-Extract-Kit&lt;/a&gt; &lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;&lt;img src="https://img.shields.io/github/stars/opendatalab/PDF-Extract-Kit" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;A powerful open-source toolkit designed to efficiently extract high-quality content from complex and diverse PDF documents.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/PantsuDango/Dango-Translator"&gt;Dango-Translator&lt;/a&gt;&lt;a href="https://github.com/PantsuDango/Dango-Translator"&gt;&lt;img src="https://img.shields.io/github/stars/PantsuDango/Dango-Translator" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Recognize text on the screen, translate it and show the translation results in real time.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/awesome_projects.md"&gt;Learn more projects&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/awesome_projects.md"&gt;More projects based on PaddleOCR&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Contributors&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/PaddlePaddle/PaddleOCR/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=PaddlePaddle/PaddleOCR&amp;amp;max=400&amp;amp;columns=20" width="800" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸŒŸ Star&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="800" src="https://api.star-history.com/svg?repos=PaddlePaddle/PaddleOCR&amp;amp;type=Date" alt="Star-history" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; 
&lt;p&gt;This project is released under the &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/LICENSE"&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ“ Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@misc{cui2025paddleocr30technicalreport,
      title={PaddleOCR 3.0 Technical Report}, 
      author={Cheng Cui and Ting Sun and Manhui Lin and Tingquan Gao and Yubo Zhang and Jiaxuan Liu and Xueqing Wang and Zelun Zhang and Changda Zhou and Hongen Liu and Yue Zhang and Wenyu Lv and Kui Huang and Yichao Zhang and Jing Zhang and Jun Zhang and Yi Liu and Dianhai Yu and Yanjun Ma},
      year={2025},
      eprint={2507.05595},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2507.05595}, 
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>CorentinJ/Real-Time-Voice-Cloning</title>
      <link>https://github.com/CorentinJ/Real-Time-Voice-Cloning</link>
      <description>&lt;p&gt;Clone a voice in 5 seconds to generate arbitrary speech in real-time&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Real-Time Voice Cloning&lt;/h1&gt; 
&lt;p&gt;This repository is an implementation of &lt;a href="https://arxiv.org/pdf/1806.04558.pdf"&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/a&gt; (SV2TTS) with a vocoder that works in real-time. This was my &lt;a href="https://matheo.uliege.be/handle/2268.2/6801"&gt;master's thesis&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;SV2TTS is a deep learning framework in three stages. In the first stage, one creates a digital representation of a voice from a few seconds of audio. In the second and third stages, this representation is used as reference to generate speech given arbitrary text.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Video demonstration&lt;/strong&gt; (click the picture):&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=-O_hYhToKoA"&gt;&lt;img src="https://i.imgur.com/8lFUlgz.png" alt="Toolbox demo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Papers implemented&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;URL&lt;/th&gt; 
   &lt;th&gt;Designation&lt;/th&gt; 
   &lt;th&gt;Title&lt;/th&gt; 
   &lt;th&gt;Implementation source&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/1806.04558.pdf"&gt;&lt;strong&gt;1806.04558&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;SV2TTS&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;This repo&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/1802.08435.pdf"&gt;1802.08435&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;WaveRNN (vocoder)&lt;/td&gt; 
   &lt;td&gt;Efficient Neural Audio Synthesis&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/fatchord/WaveRNN"&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/1703.10135.pdf"&gt;1703.10135&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Tacotron (synthesizer)&lt;/td&gt; 
   &lt;td&gt;Tacotron: Towards End-to-End Speech Synthesis&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/fatchord/WaveRNN"&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/1710.10467.pdf"&gt;1710.10467&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;GE2E (encoder)&lt;/td&gt; 
   &lt;td&gt;Generalized End-To-End Loss for Speaker Verification&lt;/td&gt; 
   &lt;td&gt;This repo&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Heads up&lt;/h2&gt; 
&lt;p&gt;Like everything else in Deep Learning, this repo has quickly gotten old. Many SaaS apps (often paying) will give you a better audio quality than this repository will. If you wish for an open-source solution with a high voice quality:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check out &lt;a href="https://paperswithcode.com/task/speech-synthesis/"&gt;paperswithcode&lt;/a&gt; for other repositories and recent research in the field of speech synthesis.&lt;/li&gt; 
 &lt;li&gt;Check out &lt;a href="https://github.com/resemble-ai/chatterbox"&gt;Chatterbox&lt;/a&gt; for a similar project up to date with the 2025 SOTA in voice cloning&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Running the toolbox&lt;/h2&gt; 
&lt;p&gt;Both Windows and Linux are supported.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install &lt;a href="https://ffmpeg.org/download.html#get-packages"&gt;ffmpeg&lt;/a&gt;. This is necessary for reading audio files. Check if it's installed by running in a command line&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;ffmpeg
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install uv for python package management&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;# On Windows:
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
# On Linux
curl -LsSf https://astral.sh/uv/install.sh | sh

# Alternatively, on any platform if you have pip installed you can do
pip install -U uv
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Run one of the following commands&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;# Run the toolbox if you have an NVIDIA GPU
uv run --extra cuda demo_toolbox.py
# Use this if you don't
uv run --extra cpu demo_toolbox.py

# Run in command line if you don't want the GUI
uv run --extra cuda demo_cli.py
uv run --extra cpu demo_cli.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Uv will automatically create a .venv directory for you with an appropriate python environment. &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues"&gt;Open an issue&lt;/a&gt; if this fails for you&lt;/p&gt; 
&lt;h3&gt;(Optional) Download Pretrained Models&lt;/h3&gt; 
&lt;p&gt;Pretrained models are now downloaded automatically. If this doesn't work for you, you can manually download them &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;(Optional) Download Datasets&lt;/h3&gt; 
&lt;p&gt;For playing with the toolbox alone, I only recommend downloading &lt;a href="https://www.openslr.org/resources/12/train-clean-100.tar.gz"&gt;&lt;code&gt;LibriSpeech/train-clean-100&lt;/code&gt;&lt;/a&gt;. Extract the contents as &lt;code&gt;&amp;lt;datasets_root&amp;gt;/LibriSpeech/train-clean-100&lt;/code&gt; where &lt;code&gt;&amp;lt;datasets_root&amp;gt;&lt;/code&gt; is a directory of your choosing. Other datasets are supported in the toolbox, see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#datasets"&gt;here&lt;/a&gt;. You're free not to download any dataset, but then you will need your own data as audio files or you will have to record it with the toolbox.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>bytedance/Dolphin</title>
      <link>https://github.com/bytedance/Dolphin</link>
      <description>&lt;p&gt;The official repo for â€œDolphin: Document Image Parsing via Heterogeneous Anchor Promptingâ€, ACL, 2025.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/bytedance/Dolphin/master/assets/dolphin.png" width="300" /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://arxiv.org/abs/2505.14059"&gt; &lt;img src="https://img.shields.io/badge/Paper-arXiv-red" /&gt; &lt;/a&gt; 
 &lt;a href="https://huggingface.co/ByteDance/Dolphin"&gt; &lt;img src="https://img.shields.io/badge/HuggingFace-Dolphin-yellow" /&gt; &lt;/a&gt; 
 &lt;a href="https://modelscope.cn/models/ByteDance/Dolphin"&gt; &lt;img src="https://img.shields.io/badge/ModelScope-Dolphin-purple" /&gt; &lt;/a&gt; 
 &lt;a href="https://huggingface.co/spaces/ByteDance/Dolphin"&gt; &lt;img src="https://img.shields.io/badge/Demo-Dolphin-blue" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/bytedance/Dolphin"&gt; &lt;img src="https://img.shields.io/badge/Code-Github-green" /&gt; &lt;/a&gt; 
 &lt;a href="https://opensource.org/licenses/MIT"&gt; &lt;img src="https://img.shields.io/badge/License-MIT-lightgray" /&gt; &lt;/a&gt; 
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/bytedance/Dolphin/master/assets/demo.gif" width="800" /&gt; 
&lt;/div&gt; 
&lt;h1&gt;Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting&lt;/h1&gt; 
&lt;p&gt;Dolphin (&lt;strong&gt;Do&lt;/strong&gt;cument Image &lt;strong&gt;P&lt;/strong&gt;arsing via &lt;strong&gt;H&lt;/strong&gt;eterogeneous Anchor Prompt&lt;strong&gt;in&lt;/strong&gt;g) is a novel multimodal document image parsing model following an analyze-then-parse paradigm. This repository contains the demo code and pre-trained models for Dolphin.&lt;/p&gt; 
&lt;h2&gt;ğŸ“‘ Overview&lt;/h2&gt; 
&lt;p&gt;Document image parsing is challenging due to its complexly intertwined elements such as text paragraphs, figures, formulas, and tables. Dolphin addresses these challenges through a two-stage approach:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ” Stage 1&lt;/strong&gt;: Comprehensive page-level layout analysis by generating element sequence in natural reading order&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ§© Stage 2&lt;/strong&gt;: Efficient parallel parsing of document elements using heterogeneous anchors and task-specific prompts&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/bytedance/Dolphin/master/assets/framework.png" width="680" /&gt; 
&lt;/div&gt; 
&lt;p&gt;Dolphin achieves promising performance across diverse page-level and element-level parsing tasks while ensuring superior efficiency through its lightweight architecture and parallel parsing mechanism.&lt;/p&gt; 
&lt;h2&gt;ğŸš€ Demo&lt;/h2&gt; 
&lt;p&gt;Try our demo on &lt;a href="http://115.190.42.15:8888/dolphin/"&gt;Demo-Dolphin&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ“… Changelog&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ”¥ &lt;strong&gt;2025.07.10&lt;/strong&gt; Released the &lt;em&gt;Fox-Page Benchmark&lt;/em&gt;, a manually refined subset of the original &lt;a href="https://github.com/ucaslcl/Fox"&gt;Fox dataset&lt;/a&gt;. Download via: &lt;a href="https://pan.baidu.com/share/init?surl=t746ULp6iU5bUraVrPlMSw&amp;amp;pwd=fox1"&gt;Baidu Yun&lt;/a&gt; | &lt;a href="https://drive.google.com/file/d/1yZQZqI34QCqvhB4Tmdl3X_XEvYvQyP0q/view?usp=sharing"&gt;Google Drive&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;ğŸ”¥ &lt;strong&gt;2025.06.30&lt;/strong&gt; Added &lt;a href="https://github.com/bytedance/Dolphin/raw/master/deployment/tensorrt_llm/ReadMe.md"&gt;TensorRT-LLM support&lt;/a&gt; for accelerated inferenceï¼&lt;/li&gt; 
 &lt;li&gt;ğŸ”¥ &lt;strong&gt;2025.06.27&lt;/strong&gt; Added &lt;a href="https://github.com/bytedance/Dolphin/raw/master/deployment/vllm/ReadMe.md"&gt;vLLM support&lt;/a&gt; for accelerated inferenceï¼&lt;/li&gt; 
 &lt;li&gt;ğŸ”¥ &lt;strong&gt;2025.06.13&lt;/strong&gt; Added multi-page PDF document parsing capability.&lt;/li&gt; 
 &lt;li&gt;ğŸ”¥ &lt;strong&gt;2025.05.21&lt;/strong&gt; Our demo is released at &lt;a href="http://115.190.42.15:8888/dolphin/"&gt;link&lt;/a&gt;. Check it out!&lt;/li&gt; 
 &lt;li&gt;ğŸ”¥ &lt;strong&gt;2025.05.20&lt;/strong&gt; The pretrained model and inference code of Dolphin are released.&lt;/li&gt; 
 &lt;li&gt;ğŸ”¥ &lt;strong&gt;2025.05.16&lt;/strong&gt; Our paper has been accepted by ACL 2025. Paper link: &lt;a href="https://arxiv.org/abs/2505.14059"&gt;arXiv&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ› ï¸ Installation&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Clone the repository:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/ByteDance/Dolphin.git
cd Dolphin
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install the dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Download the pre-trained models using one of the following options:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Option A: Original Model Format (config-based)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Download from &lt;a href="https://pan.baidu.com/s/15zcARoX0CTOHKbW8bFZovQ?pwd=9rpx"&gt;Baidu Yun&lt;/a&gt; or &lt;a href="https://drive.google.com/drive/folders/1PQJ3UutepXvunizZEw-uGaQ0BCzf-mie?usp=sharing"&gt;Google Drive&lt;/a&gt; and put them in the &lt;code&gt;./checkpoints&lt;/code&gt; folder.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Option B: Hugging Face Model Format&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Visit our Huggingface &lt;a href="https://huggingface.co/ByteDance/Dolphin"&gt;model card&lt;/a&gt;, or download model by:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Download the model from Hugging Face Hub
git lfs install
git clone https://huggingface.co/ByteDance/Dolphin ./hf_model
# Or use the Hugging Face CLI
pip install huggingface_hub
huggingface-cli download ByteDance/Dolphin --local-dir ./hf_model
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;âš¡ Inference&lt;/h2&gt; 
&lt;p&gt;Dolphin provides two inference frameworks with support for two parsing granularities:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Page-level Parsing&lt;/strong&gt;: Parse the entire document page into a structured JSON and Markdown format&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Element-level Parsing&lt;/strong&gt;: Parse individual document elements (text, table, formula)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ“„ Page-level Parsing&lt;/h3&gt; 
&lt;h4&gt;Using Original Framework (config-based)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Process a single document image
python demo_page.py --config ./config/Dolphin.yaml --input_path ./demo/page_imgs/page_1.jpeg --save_dir ./results

# Process a single document pdf
python demo_page.py --config ./config/Dolphin.yaml --input_path ./demo/page_imgs/page_6.pdf --save_dir ./results

# Process all documents in a directory
python demo_page.py --config ./config/Dolphin.yaml --input_path ./demo/page_imgs --save_dir ./results

# Process with custom batch size for parallel element decoding
python demo_page.py --config ./config/Dolphin.yaml --input_path ./demo/page_imgs --save_dir ./results --max_batch_size 8
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Using Hugging Face Framework&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Process a single document image
python demo_page_hf.py --model_path ./hf_model --input_path ./demo/page_imgs/page_1.jpeg --save_dir ./results

# Process a single document pdf
python demo_page_hf.py --model_path ./hf_model --input_path ./demo/page_imgs/page_6.pdf --save_dir ./results

# Process all documents in a directory
python demo_page_hf.py --model_path ./hf_model --input_path ./demo/page_imgs --save_dir ./results

# Process with custom batch size for parallel element decoding
python demo_page_hf.py --model_path ./hf_model --input_path ./demo/page_imgs --save_dir ./results --max_batch_size 16
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ğŸ§© Element-level Parsing&lt;/h3&gt; 
&lt;h4&gt;Using Original Framework (config-based)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Process a single table image
python demo_element.py --config ./config/Dolphin.yaml --input_path ./demo/element_imgs/table_1.jpeg --element_type table

# Process a single formula image
python demo_element.py --config ./config/Dolphin.yaml --input_path ./demo/element_imgs/line_formula.jpeg --element_type formula

# Process a single text paragraph image
python demo_element.py --config ./config/Dolphin.yaml --input_path ./demo/element_imgs/para_1.jpg --element_type text
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Using Hugging Face Framework&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Process a single table image
python demo_element_hf.py --model_path ./hf_model --input_path ./demo/element_imgs/table_1.jpeg --element_type table

# Process a single formula image
python demo_element_hf.py --model_path ./hf_model --input_path ./demo/element_imgs/line_formula.jpeg --element_type formula

# Process a single text paragraph image
python demo_element_hf.py --model_path ./hf_model --input_path ./demo/element_imgs/para_1.jpg --element_type text
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸŒŸ Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ”„ Two-stage analyze-then-parse approach based on a single VLM&lt;/li&gt; 
 &lt;li&gt;ğŸ“Š Promising performance on document parsing tasks&lt;/li&gt; 
 &lt;li&gt;ğŸ” Natural reading order element sequence generation&lt;/li&gt; 
 &lt;li&gt;ğŸ§© Heterogeneous anchor prompting for different document elements&lt;/li&gt; 
 &lt;li&gt;â±ï¸ Efficient parallel parsing mechanism&lt;/li&gt; 
 &lt;li&gt;ğŸ¤— Support for Hugging Face Transformers for easier integration&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“® Notice&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Call for Bad Cases:&lt;/strong&gt; If you have encountered any cases where the model performs poorly, we would greatly appreciate it if you could share them in the issue. We are continuously working to optimize and improve the model.&lt;/p&gt; 
&lt;h2&gt;ğŸ’– Acknowledgement&lt;/h2&gt; 
&lt;p&gt;We would like to acknowledge the following open-source projects that provided inspiration and reference for this work:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/clovaai/donut/"&gt;Donut&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/facebookresearch/nougat"&gt;Nougat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Ucas-HaoranWei/GOT-OCR2.0"&gt;GOT&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/MinerU/tree/master"&gt;MinerU&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/Swin-Transformer"&gt;Swin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/transformers"&gt;Hugging Face Transformers&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“ Citation&lt;/h2&gt; 
&lt;p&gt;If you find this code useful for your research, please use the following BibTeX entry.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{feng2025dolphin,
  title={Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting},
  author={Feng, Hao and Wei, Shu and Fei, Xiang and Shi, Wei and Han, Yingdong and Liao, Lei and Lu, Jinghui and Wu, Binghong and Liu, Qi and Lin, Chunhui and others},
  journal={arXiv preprint arXiv:2505.14059},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#bytedance/Dolphin&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=bytedance/Dolphin&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>EbookFoundation/free-programming-books</title>
      <link>https://github.com/EbookFoundation/free-programming-books</link>
      <description>&lt;p&gt;ğŸ“š Freely available programming books&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;List of Free Learning Resources In Many Languages&lt;/h1&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/sindresorhus/awesome"&gt;&lt;img src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true" alt="Awesome" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://creativecommons.org/licenses/by/4.0/"&gt;&lt;img src="https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg?sanitize=true" alt="License: CC BY 4.0" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Apr+is%3Amerged+created%3A2023-10-01..2023-10-31"&gt;&lt;img src="https://img.shields.io/github/hacktoberfest/2023/EbookFoundation/free-programming-books?label=Hacktoberfest+2023" alt="Hacktoberfest 2023 stats" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;Search the list at &lt;a href="https://ebookfoundation.github.io/free-programming-books-search/"&gt;https://ebookfoundation.github.io/free-programming-books-search/&lt;/a&gt; &lt;a href="https://ebookfoundation.github.io/free-programming-books-search/"&gt;&lt;img src="https://img.shields.io/website?style=flat&amp;amp;logo=www&amp;amp;logoColor=whitesmoke&amp;amp;label=Dynamic%20search%20site&amp;amp;down_color=red&amp;amp;down_message=down&amp;amp;up_color=green&amp;amp;up_message=up&amp;amp;url=https%3A%2F%2Febookfoundation.github.io%2Ffree-programming-books-search%2F" alt="https://ebookfoundation.github.io/free-programming-books-search/" /&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This page is available as an easy-to-read website. Access it by clicking on &lt;a href="https://ebookfoundation.github.io/free-programming-books/"&gt;&lt;img src="https://img.shields.io/website?style=flat&amp;amp;logo=www&amp;amp;logoColor=whitesmoke&amp;amp;label=Static%20site&amp;amp;down_color=red&amp;amp;down_message=down&amp;amp;up_color=green&amp;amp;up_message=up&amp;amp;url=https%3A%2F%2Febookfoundation.github.io%2Ffree-programming-books%2F" alt="https://ebookfoundation.github.io/free-programming-books/" /&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;form action="https://ebookfoundation.github.io/free-programming-books-search"&gt; 
  &lt;input type="text" id="fpbSearch" name="search" required placeholder="Search Book or Author" /&gt; 
  &lt;label for="submit"&gt; &lt;/label&gt; 
  &lt;input type="submit" id="submit" name="submit" value="Search" /&gt; 
 &lt;/form&gt; 
&lt;/div&gt; 
&lt;h2&gt;Intro&lt;/h2&gt; 
&lt;p&gt;This list was originally a clone of &lt;a href="https://web.archive.org/web/20140606191453/http://stackoverflow.com/questions/194812/list-of-freely-available-programming-books/392926"&gt;StackOverflow - List of Freely Available Programming Books&lt;/a&gt; with contributions from Karan Bhangui and George Stocker.&lt;/p&gt; 
&lt;p&gt;The list was moved to GitHub by Victor Felder for collaborative updating and maintenance. It has grown to become one of &lt;a href="https://octoverse.github.com/"&gt;GitHub's most popular repositories&lt;/a&gt;.&lt;/p&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/EbookFoundation/free-programming-books/network"&gt;&lt;img src="https://img.shields.io/github/forks/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Forks" alt="GitHub repo forks" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Stars" alt="GitHub repo stars" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors-anon/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Contributors" alt="GitHub repo contributors" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/sponsors/EbookFoundation"&gt;&lt;img src="https://img.shields.io/github/sponsors/EbookFoundation?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Sponsors" alt="GitHub org sponsors" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/watchers"&gt;&lt;img src="https://img.shields.io/github/watchers/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Watchers" alt="GitHub repo watchers" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/archive/refs/heads/main.zip"&gt;&lt;img src="https://img.shields.io/github/repo-size/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Repo%20Size" alt="GitHub repo size" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;The &lt;a href="https://ebookfoundation.org"&gt;Free Ebook Foundation&lt;/a&gt; now administers the repo, a not-for-profit organization devoted to promoting the creation, distribution, archiving, and sustainability of free ebooks. &lt;a href="https://ebookfoundation.org/contributions.html"&gt;Donations&lt;/a&gt; to the Free Ebook Foundation are tax-deductible in the US.&lt;/p&gt; 
&lt;h2&gt;How To Contribute&lt;/h2&gt; 
&lt;p&gt;Please read &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt;. If you're new to GitHub, &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/HOWTO.md"&gt;welcome&lt;/a&gt;! Remember to abide by our adapted from &lt;img src="https://img.shields.io/badge/Contributor%20Covenant-1.3-4baaaa.svg?sanitize=true" alt="Contributor Covenant 1.3" /&gt; &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt; too (&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/#translations"&gt;translations&lt;/a&gt; also available).&lt;/p&gt; 
&lt;p&gt;Click on these badges to see how you might be able to help:&lt;/p&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/EbookFoundation/free-programming-books/issues"&gt;&lt;img src="https://img.shields.io/github/issues/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=red&amp;amp;label=Issues" alt="GitHub repo Issues" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22"&gt;&lt;img src="https://img.shields.io/github/issues/EbookFoundation/free-programming-books/good%20first%20issue?style=flat&amp;amp;logo=github&amp;amp;logoColor=green&amp;amp;label=Good%20First%20issues" alt="GitHub repo Good Issues for newbies" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22"&gt;&lt;img src="https://img.shields.io/github/issues/EbookFoundation/free-programming-books/help%20wanted?style=flat&amp;amp;logo=github&amp;amp;logoColor=b545d1&amp;amp;label=%22Help%20Wanted%22%20issues" alt="GitHub Help Wanted issues" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls"&gt;&lt;img src="https://img.shields.io/github/issues-pr/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=orange&amp;amp;label=PRs" alt="GitHub repo PRs" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Apr+is%3Amerged"&gt;&lt;img src="https://img.shields.io/github/issues-search/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=green&amp;amp;label=Merged%20PRs&amp;amp;query=is%3Amerged" alt="GitHub repo Merged PRs" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22"&gt;&lt;img src="https://img.shields.io/github/issues-pr/EbookFoundation/free-programming-books/help%20wanted?style=flat&amp;amp;logo=github&amp;amp;logoColor=b545d1&amp;amp;label=%22Help%20Wanted%22%20PRs" alt="GitHub Help Wanted PRs" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;How To Share&lt;/h2&gt; 
&lt;div align="left" markdown="1"&gt; 
 &lt;a href="https://www.facebook.com/share.php?u=https%3A%2F%2Fgithub.com%2FEbookFoundation%2Ffree-programming-books&amp;amp;p%5Bimages%5D%5B0%5D=&amp;amp;p%5Btitle%5D=Free%20Programming%20Books&amp;amp;p%5Bsummary%5D="&gt;Share on Facebook&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://github.com/EbookFoundation/free-programming-books&amp;amp;title=Free%20Programming%20Books&amp;amp;summary=&amp;amp;source="&gt;Share on LinkedIn&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="https://toot.kytta.dev/?mini=true&amp;amp;url=https://github.com/EbookFoundation/free-programming-books&amp;amp;title=Free%20Programming%20Books&amp;amp;summary=&amp;amp;source="&gt;Share on Mastodon/Fediverse&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="https://t.me/share/url?url=https://github.com/EbookFoundation/free-programming-books"&gt;Share on Telegram&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="https://twitter.com/intent/tweet?text=https://github.com/EbookFoundation/free-programming-books%0AFree%20Programming%20Books"&gt;Share on ğ• (Twitter)&lt;/a&gt;
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Resources&lt;/h2&gt; 
&lt;p&gt;This project lists books and other resources grouped by genres:&lt;/p&gt; 
&lt;h3&gt;Books&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-langs.md"&gt;English, By Programming Language&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-subjects.md"&gt;English, By Subject&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Other Languages&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ar.md"&gt;Arabic / al arabiya / Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-hy.md"&gt;Armenian / Õ€Õ¡ÕµÕ¥Ö€Õ¥Õ¶&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-az.md"&gt;Azerbaijani / ĞĞ·Ó™Ñ€Ğ±Ğ°Ñ˜Ò¹Ğ°Ğ½ Ğ´Ğ¸Ğ»Ğ¸ / Ø¢Ø°Ø±Ø¨Ø§ÙŠØ¬Ø§Ù†Ø¬Ø§ Ø¯ÙŠÙ„ÙŠ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-bn.md"&gt;Bengali / à¦¬à¦¾à¦‚à¦²à¦¾&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-bg.md"&gt;Bulgarian / Ğ±ÑŠĞ»Ğ³Ğ°Ñ€ÑĞºĞ¸&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-my.md"&gt;Burmese / á€™á€¼á€”á€ºá€™á€¬á€˜á€¬á€á€¬&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-zh.md"&gt;Chinese / ä¸­æ–‡&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-cs.md"&gt;Czech / ÄeÅ¡tina / ÄeskÃ½ jazyk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ca.md"&gt;Catalan / catalan/ catalÃ &lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-da.md"&gt;Danish / dansk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-nl.md"&gt;Dutch / Nederlands&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-et.md"&gt;Estonian / eesti keel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-fi.md"&gt;Finnish / suomi / suomen kieli&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-fr.md"&gt;French / franÃ§ais&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-el.md"&gt;Greek / ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-he.md"&gt;Hebrew / ×¢×‘×¨×™×ª&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-hi.md"&gt;Hindi / à¤¹à¤¿à¤¨à¥à¤¦à¥€&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-hu.md"&gt;Hungarian / magyar / magyar nyelv&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-id.md"&gt;Indonesian / Bahasa Indonesia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-it.md"&gt;Italian / italiano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ja.md"&gt;Japanese / æ—¥æœ¬èª&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ko.md"&gt;Korean / í•œêµ­ì–´&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-lv.md"&gt;Latvian / LatvieÅ¡u&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ml.md"&gt;Malayalam / à´®à´²à´¯à´¾à´³à´‚&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-no.md"&gt;Norwegian / Norsk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-fa_IR.md"&gt;Persian / Farsi (Iran) / ÙØ§Ø±Ø³Ù‰&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-pl.md"&gt;Polish / polski / jÄ™zyk polski / polszczyzna&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-pt_BR.md"&gt;Portuguese (Brazil)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-pt_PT.md"&gt;Portuguese (Portugal)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ro.md"&gt;Romanian (Romania) / limba romÃ¢nÄƒ / romÃ¢n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ru.md"&gt;Russian / Ğ ÑƒÑÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-sr.md"&gt;Serbian / ÑÑ€Ğ¿ÑĞºĞ¸ Ñ˜ĞµĞ·Ğ¸Ğº / srpski jezik&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-sk.md"&gt;Slovak / slovenÄina&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-es.md"&gt;Spanish / espaÃ±ol / castellano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-sv.md"&gt;Swedish / Svenska&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ta.md"&gt;Tamil / à®¤à®®à®¿à®´à¯&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-te.md"&gt;Telugu / à°¤à±†à°²à±à°—à±&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-th.md"&gt;Thai / à¹„à¸—à¸¢&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-tr.md"&gt;Turkish / TÃ¼rkÃ§e&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-uk.md"&gt;Ukrainian / Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-vi.md"&gt;Vietnamese / Tiáº¿ng Viá»‡t&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Cheat Sheets&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-cheatsheets.md"&gt;All Languages&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Free Online Courses&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ar.md"&gt;Arabic / al arabiya / Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-bn.md"&gt;Bengali / à¦¬à¦¾à¦‚à¦²à¦¾&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-bg.md"&gt;Bulgarian / Ğ±ÑŠĞ»Ğ³Ğ°Ñ€ÑĞºĞ¸&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-my.md"&gt;Burmese / á€™á€¼á€”á€ºá€™á€¬á€˜á€¬á€á€¬&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-zh.md"&gt;Chinese / ä¸­æ–‡&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-en.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-fi.md"&gt;Finnish / suomi / suomen kieli&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-fr.md"&gt;French / franÃ§ais&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-el.md"&gt;Greek / ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-he.md"&gt;Hebrew / ×¢×‘×¨×™×ª&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-hi.md"&gt;Hindi / à¤¹à¤¿à¤‚à¤¦à¥€&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-id.md"&gt;Indonesian / Bahasa Indonesia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-it.md"&gt;Italian / italiano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ja.md"&gt;Japanese / æ—¥æœ¬èª&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-kn.md"&gt;Kannada/à²•à²¨à³à²¨à²¡&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-kk.md"&gt;Kazakh / Ò›Ğ°Ğ·Ğ°Ò›ÑˆĞ°&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-km.md"&gt;Khmer / á—á¶áŸá¶ááŸ’á˜áŸ‚áš&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ko.md"&gt;Korean / í•œêµ­ì–´&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ml.md"&gt;Malayalam / à´®à´²à´¯à´¾à´³à´‚&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-mr.md"&gt;Marathi / à¤®à¤°à¤¾à¤ à¥€&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ne.md"&gt;Nepali / à¤¨à¥‡à¤ªà¤¾à¤²à¥€&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-no.md"&gt;Norwegian / Norsk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-fa_IR.md"&gt;Persian / Farsi (Iran) / ÙØ§Ø±Ø³Ù‰&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-pl.md"&gt;Polish / polski / jÄ™zyk polski / polszczyzna&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-pt_BR.md"&gt;Portuguese (Brazil)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-pt_PT.md"&gt;Portuguese (Portugal)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ru.md"&gt;Russian / Ğ ÑƒÑÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-si.md"&gt;Sinhala / à·ƒà·’à¶‚à·„à¶½&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-es.md"&gt;Spanish / espaÃ±ol / castellano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-sv.md"&gt;Swedish / svenska&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ta.md"&gt;Tamil / à®¤à®®à®¿à®´à¯&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-te.md"&gt;Telugu / à°¤à±†à°²à±à°—à±&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-th.md"&gt;Thai / à¸ à¸²à¸©à¸²à¹„à¸—à¸¢&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-tr.md"&gt;Turkish / TÃ¼rkÃ§e&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-uk.md"&gt;Ukrainian / Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ur.md"&gt;Urdu / Ø§Ø±Ø¯Ùˆ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-vi.md"&gt;Vietnamese / Tiáº¿ng Viá»‡t&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Interactive Programming Resources&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-zh.md"&gt;Chinese / ä¸­æ–‡&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-en.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-ja.md"&gt;Japanese / æ—¥æœ¬èª&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-ru.md"&gt;Russian / Ğ ÑƒÑÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Problem Sets and Competitive Programming&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/problem-sets-competitive-programming.md"&gt;Problem Sets&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Podcast - Screencast&lt;/h3&gt; 
&lt;p&gt;Free Podcasts and Screencasts:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-ar.md"&gt;Arabic / al Arabiya / Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-my.md"&gt;Burmese / á€™á€¼á€”á€ºá€™á€¬á€˜á€¬á€á€¬&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-zh.md"&gt;Chinese / ä¸­æ–‡&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-cs.md"&gt;Czech / ÄeÅ¡tina / ÄeskÃ½ jazyk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-nl.md"&gt;Dutch / Nederlands&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-en.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-fi.md"&gt;Finnish / Suomi&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-fr.md"&gt;French / franÃ§ais&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-he.md"&gt;Hebrew / ×¢×‘×¨×™×ª&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-id.md"&gt;Indonesian / Bahasa Indonesia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-fa_IR.md"&gt;Persian / Farsi (Iran) / ÙØ§Ø±Ø³Ù‰&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-pl.md"&gt;Polish / polski / jÄ™zyk polski / polszczyzna&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-pt_BR.md"&gt;Portuguese (Brazil)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-pt_PT.md"&gt;Portuguese (Portugal)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-ru.md"&gt;Russian / Ğ ÑƒÑÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-si.md"&gt;Sinhala / à·ƒà·’à¶‚à·„à¶½&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-es.md"&gt;Spanish / espaÃ±ol / castellano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-sv.md"&gt;Swedish / Svenska&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-tr.md"&gt;Turkish / TÃ¼rkÃ§e&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-uk.md"&gt;Ukrainian / Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Programming Playgrounds&lt;/h3&gt; 
&lt;p&gt;Write, compile, and run your code within a browser. Try it out!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-playgrounds-zh.md"&gt;Chinese / ä¸­æ–‡&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-playgrounds.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-playgrounds-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Translations&lt;/h2&gt; 
&lt;p&gt;Volunteers have translated many of our Contributing, How-to, and Code of Conduct documents into languages covered by our lists.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;English 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CONTRIBUTING.md"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/HOWTO.md"&gt;How-to&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;... &lt;em&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/README.md#translations"&gt;More languages&lt;/a&gt;&lt;/em&gt; ...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You might notice that there are &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/README.md#translations"&gt;some missing translations here&lt;/a&gt; - perhaps you would like to help out by &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CONTRIBUTING.md#help-out-by-contributing-a-translation"&gt;contributing a translation&lt;/a&gt;?&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Each file included in this repository is licensed under the &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/LICENSE"&gt;CC BY License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/markitdown</title>
      <link>https://github.com/microsoft/markitdown</link>
      <description>&lt;p&gt;Python tool for converting files and office documents to Markdown.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MarkItDown&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/markitdown/"&gt;&lt;img src="https://img.shields.io/pypi/v/markitdown.svg?sanitize=true" alt="PyPI" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/dd/markitdown" alt="PyPI - Downloads" /&gt; &lt;a href="https://github.com/microsoft/autogen"&gt;&lt;img src="https://img.shields.io/badge/Built%20by-AutoGen%20Team-blue" alt="Built by AutoGen Team" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] MarkItDown now offers an MCP (Model Context Protocol) server for integration with LLM applications like Claude Desktop. See &lt;a href="https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp"&gt;markitdown-mcp&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Breaking changes between 0.0.1 to 0.1.0:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Dependencies are now organized into optional feature-groups (further details below). Use &lt;code&gt;pip install 'markitdown[all]'&lt;/code&gt; to have backward-compatible behavior.&lt;/li&gt; 
  &lt;li&gt;convert_stream() now requires a binary file-like object (e.g., a file opened in binary mode, or an io.BytesIO object). This is a breaking change from the previous version, where it previously also accepted text file-like objects, like io.StringIO.&lt;/li&gt; 
  &lt;li&gt;The DocumentConverter class interface has changed to read from file-like streams rather than file paths. &lt;em&gt;No temporary files are created anymore&lt;/em&gt;. If you are the maintainer of a plugin, or custom DocumentConverter, you likely need to update your code. Otherwise, if only using the MarkItDown class or CLI (as in these examples), you should not need to change anything.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;MarkItDown is a lightweight Python utility for converting various files to Markdown for use with LLMs and related text analysis pipelines. To this end, it is most comparable to &lt;a href="https://github.com/deanmalmgren/textract"&gt;textract&lt;/a&gt;, but with a focus on preserving important document structure and content as Markdown (including: headings, lists, tables, links, etc.) While the output is often reasonably presentable and human-friendly, it is meant to be consumed by text analysis tools -- and may not be the best option for high-fidelity document conversions for human consumption.&lt;/p&gt; 
&lt;p&gt;MarkItDown currently supports the conversion from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PDF&lt;/li&gt; 
 &lt;li&gt;PowerPoint&lt;/li&gt; 
 &lt;li&gt;Word&lt;/li&gt; 
 &lt;li&gt;Excel&lt;/li&gt; 
 &lt;li&gt;Images (EXIF metadata and OCR)&lt;/li&gt; 
 &lt;li&gt;Audio (EXIF metadata and speech transcription)&lt;/li&gt; 
 &lt;li&gt;HTML&lt;/li&gt; 
 &lt;li&gt;Text-based formats (CSV, JSON, XML)&lt;/li&gt; 
 &lt;li&gt;ZIP files (iterates over contents)&lt;/li&gt; 
 &lt;li&gt;Youtube URLs&lt;/li&gt; 
 &lt;li&gt;EPubs&lt;/li&gt; 
 &lt;li&gt;... and more!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Why Markdown?&lt;/h2&gt; 
&lt;p&gt;Markdown is extremely close to plain text, with minimal markup or formatting, but still provides a way to represent important document structure. Mainstream LLMs, such as OpenAI's GPT-4o, natively "&lt;em&gt;speak&lt;/em&gt;" Markdown, and often incorporate Markdown into their responses unprompted. This suggests that they have been trained on vast amounts of Markdown-formatted text, and understand it well. As a side benefit, Markdown conventions are also highly token-efficient.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;MarkItDown requires Python 3.10 or higher. It is recommended to use a virtual environment to avoid dependency conflicts.&lt;/p&gt; 
&lt;p&gt;With the standard Python installation, you can create and activate a virtual environment using the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv .venv
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If using &lt;code&gt;uv&lt;/code&gt;, you can create a virtual environment with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv --python=3.12 .venv
source .venv/bin/activate
# NOTE: Be sure to use 'uv pip install' rather than just 'pip install' to install packages in this virtual environment
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are using Anaconda, you can create a virtual environment with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n markitdown python=3.12
conda activate markitdown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install MarkItDown, use pip: &lt;code&gt;pip install 'markitdown[all]'&lt;/code&gt;. Alternatively, you can install it from the source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone git@github.com:microsoft/markitdown.git
cd markitdown
pip install -e 'packages/markitdown[all]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Command-Line&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf &amp;gt; document.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or use &lt;code&gt;-o&lt;/code&gt; to specify the output file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf -o document.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also pipe content:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cat path-to-file.pdf | markitdown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Optional Dependencies&lt;/h3&gt; 
&lt;p&gt;MarkItDown has optional dependencies for activating various file formats. Earlier in this document, we installed all optional dependencies with the &lt;code&gt;[all]&lt;/code&gt; option. However, you can also install them individually for more control. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install 'markitdown[pdf, docx, pptx]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;will install only the dependencies for PDF, DOCX, and PPTX files.&lt;/p&gt; 
&lt;p&gt;At the moment, the following optional dependencies are available:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;[all]&lt;/code&gt; Installs all optional dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[pptx]&lt;/code&gt; Installs dependencies for PowerPoint files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[docx]&lt;/code&gt; Installs dependencies for Word files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[xlsx]&lt;/code&gt; Installs dependencies for Excel files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[xls]&lt;/code&gt; Installs dependencies for older Excel files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[pdf]&lt;/code&gt; Installs dependencies for PDF files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[outlook]&lt;/code&gt; Installs dependencies for Outlook messages&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[az-doc-intel]&lt;/code&gt; Installs dependencies for Azure Document Intelligence&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[audio-transcription]&lt;/code&gt; Installs dependencies for audio transcription of wav and mp3 files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[youtube-transcription]&lt;/code&gt; Installs dependencies for fetching YouTube video transcription&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Plugins&lt;/h3&gt; 
&lt;p&gt;MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown --list-plugins
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To enable plugins use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown --use-plugins path-to-file.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To find available plugins, search GitHub for the hashtag &lt;code&gt;#markitdown-plugin&lt;/code&gt;. To develop a plugin, see &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Azure Document Intelligence&lt;/h3&gt; 
&lt;p&gt;To use Microsoft Document Intelligence for conversion:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf -o document.md -d -e "&amp;lt;document_intelligence_endpoint&amp;gt;"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;More information about how to set up an Azure Document Intelligence Resource can be found &lt;a href="https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/create-document-intelligence-resource?view=doc-intel-4.0.0"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Python API&lt;/h3&gt; 
&lt;p&gt;Basic usage in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown

md = MarkItDown(enable_plugins=False) # Set to True to enable plugins
result = md.convert("test.xlsx")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Document Intelligence conversion in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown

md = MarkItDown(docintel_endpoint="&amp;lt;document_intelligence_endpoint&amp;gt;")
result = md.convert("test.pdf")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use Large Language Models for image descriptions (currently only for pptx and image files), provide &lt;code&gt;llm_client&lt;/code&gt; and &lt;code&gt;llm_model&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown
from openai import OpenAI

client = OpenAI()
md = MarkItDown(llm_client=client, llm_model="gpt-4o", llm_prompt="optional custom prompt")
result = md.convert("example.jpg")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;docker build -t markitdown:latest .
docker run --rm -i markitdown:latest &amp;lt; ~/your-file.pdf &amp;gt; output.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h3&gt;How to Contribute&lt;/h3&gt; 
&lt;p&gt;You can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as 'open for contribution' and 'open for reviewing' to help facilitate community contributions. These are of course just suggestions and you are welcome to contribute in any way you like.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;/th&gt; 
    &lt;th&gt;All&lt;/th&gt; 
    &lt;th&gt;Especially Needs Help from Community&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Issues&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/issues"&gt;All Issues&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/issues?q=is%3Aissue+is%3Aopen+label%3A%22open+for+contribution%22"&gt;Issues open for contribution&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;PRs&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/pulls"&gt;All PRs&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/pulls?q=is%3Apr+is%3Aopen+label%3A%22open+for+reviewing%22"&gt;PRs open for reviewing&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h3&gt;Running Tests and Checks&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Navigate to the MarkItDown package:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;cd packages/markitdown
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;code&gt;hatch&lt;/code&gt; in your environment and run tests:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/
hatch shell
hatch test
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(Alternative) Use the Devcontainer which has all the dependencies installed:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;# Reopen the project in Devcontainer and run:
hatch test
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run pre-commit checks before submitting a PR: &lt;code&gt;pre-commit run --all-files&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing 3rd-party Plugins&lt;/h3&gt; 
&lt;p&gt;You can also contribute by creating and sharing 3rd party plugins. See &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Trademarks&lt;/h2&gt; 
&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general"&gt;Microsoft's Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>kaixxx/noScribe</title>
      <link>https://github.com/kaixxx/noScribe</link>
      <description>&lt;p&gt;Cutting edge AI technology for automated audio transcription. A nice GUI for OpenAIs Whisper and pyannote (speaker identification)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;noScribe&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;h2&gt;Cutting Edge AI Technology for Automated Audio Transcription&lt;/h2&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;What is noScribe?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;An AI-based software that &lt;strong&gt;transcribes interviews&lt;/strong&gt; for qualitative social research or journalistic use&lt;/li&gt; 
 &lt;li&gt;noScribe is &lt;strong&gt;free and open source&lt;/strong&gt; (&lt;a href="https://www.gnu.org/licenses/gpl-3.0.html"&gt;GPL-3.0&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;It runs &lt;strong&gt;completely local&lt;/strong&gt; on your computer. No data is sent to the internet. No cloud, no worries&lt;/li&gt; 
 &lt;li&gt;It can distinguish different &lt;strong&gt;speakers&lt;/strong&gt; and understands around 60 languages (more or less, see below)&lt;/li&gt; 
 &lt;li&gt;It includes a &lt;strong&gt;nice editor&lt;/strong&gt; to review, verify and correct the resulting transcript&lt;/li&gt; 
 &lt;li&gt;It is standing on the shoulders of giants: &lt;a href="https://github.com/openai/whisper"&gt;Whisper from OpenAI&lt;/a&gt;, &lt;a href="https://github.com/guillaumekln/faster-whisper"&gt;faster-whisper by Guillaume Klein&lt;/a&gt; and &lt;a href="https://github.com/pyannote/pyannote-audio"&gt;pyannote from HervÃ© Bredin&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/kaixxx/noScribe/main/img/noScribe_main_window.png" alt="Main window" /&gt; (The transcript is from &lt;a href="https://www.youtube.com/watch?v=vOwajAbvPzQ&amp;amp;t=2018s"&gt;this interview&lt;/a&gt; which I did in May 2022 with the Russian sociologist Natalia Savelyeva.)&lt;/p&gt; 
&lt;h2&gt;Limitations&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;noScribe needs a fairly up-to-date computer, or the transcription will take very long. (Consider letting it run over night on a slower machine.)&lt;/li&gt; 
 &lt;li&gt;Since it uses sophisticated AI models, the download is quite large â€“ about 3.7 GB&lt;/li&gt; 
 &lt;li&gt;Poor audio quality will lead to poor transcription results.&lt;/li&gt; 
 &lt;li&gt;No automatic transcription is perfect, there will always be some manual revision necessary. Use the &lt;a href="https://raw.githubusercontent.com/kaixxx/noScribe/main/#noscribeedit"&gt;included Editor&lt;/a&gt; to check your transcripts thouroughly. (See also &lt;a href="https://raw.githubusercontent.com/kaixxx/noScribe/main/#factors-influencing-the-quality-of-the-transcription"&gt;"Factors Influencing the Quality"&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/kaixxx/noScribe/main/#known-issues"&gt;"Known Issues"&lt;/a&gt; below.)&lt;/li&gt; 
 &lt;li&gt;If you want to know more an can understand German, Rebecca Schmidt from the University of Paderborn wrote a nice &lt;a href="https://sozmethode.hypotheses.org/2315"&gt;review of noScribe,&lt;/a&gt; also discussing its limitations. Also the German &lt;a href="https://www.heise.de/select/ct/2025/2/2433207582191637980"&gt;computer magazine c't recommended noScribe in a recent review&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Why the Name "noScribe"?&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://www.urbandictionary.com/define.php?term=Scribe"&gt;urban dictionary&lt;/a&gt; defines &lt;strong&gt;scribe&lt;/strong&gt; as &lt;em&gt;"a person whose entire miserable existence has been reduced to academic grunge and pain".&lt;/em&gt; I hope this software will make your academic life a little less painful and grungy, hence the name noScribe :)&lt;/p&gt; 
&lt;h2&gt;About Me&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Kai DrÃ¶ge&lt;/strong&gt;, PhD in sociology (with a background in computer science), qualitative researcher and teacher, &lt;a href="https://www.hslu.ch/de-ch/hochschule-luzern/ueber-uns/personensuche/profile/?pid=823"&gt;Lucerne University for Applied Science (Switzerland)&lt;/a&gt; and &lt;a href="https://www.ifs.uni-frankfurt.de/personendetails/kai-droege.html"&gt;Institute for Social Research, Frankfurt/M. (Germany)&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Download and Installation&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Current Version Number: 0.6.2&lt;/strong&gt; (see &lt;a href="https://raw.githubusercontent.com/kaixxx/noScribe/main/CHANGELOG.md"&gt;changelog&lt;/a&gt;)&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;All releases are hosted on SWITCHdrive, a secure data sharing platform for Swiss universities.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;The &lt;strong&gt;general purpose version&lt;/strong&gt; for normal PCs without a NVIDIA graphics card: &lt;a href="https://drive.switch.ch/index.php/s/HtKDKYRZRNaYBeI?path=%2FWindows%2Fnormal"&gt;https://drive.switch.ch/index.php/s/HtKDKYRZRNaYBeI?path=%2FWindows%2Fnormal&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;A special version using &lt;strong&gt;CUDA acceleration on NVIDIA graphics cards&lt;/strong&gt; with at least 6 GB of VRAM: &lt;a href="https://drive.switch.ch/index.php/s/HtKDKYRZRNaYBeI?path=%2FWindows%2Fcuda1"&gt;https://drive.switch.ch/index.php/s/HtKDKYRZRNaYBeI?path=%2FWindows%2Fcuda1&lt;/a&gt;. Make sure that your NVIDIA drivers are on version 570.65 or higher. You must also install the &lt;a href="https://developer.nvidia.com/cuda-downloads?target_os=Windows"&gt;CUDA toolkit from here&lt;/a&gt; (a reboot is required afterwards).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Installation&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;Start the downloaded setup file. This may take a while, be patient.&lt;/li&gt; 
   &lt;li&gt;If you get a warning that "Windows protected your PC" and the app comes from an "Unknown publisher", you have to trust us and click "Run anyway"&lt;/li&gt; 
   &lt;li&gt;To do a silent install on a larger group of computers, start the setup with the argument &lt;code&gt;/S&lt;/code&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;MacOS&lt;/h3&gt; 
&lt;p&gt;ported by &lt;a href="https://github.com/gernophil"&gt;gernophil&lt;/a&gt; &lt;br /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Newer Macs with Apple Silicon M1-M4 processors&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Download: &lt;a href="https://drive.switch.ch/index.php/s/HtKDKYRZRNaYBeI?path=%2FmacOS%2Farm64%20(Apple%20Silicon)"&gt;https://drive.switch.ch/index.php/s/HtKDKYRZRNaYBeI?path=%2FmacOS%2Farm64%20(Apple%20Silicon)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Double-click on the downloaded dmg-file, then drag noScribe and noScribeEdit into the link to your applications folder (labeled "drag both here to install").&lt;/li&gt; 
   &lt;li&gt;You will need Apple's Rosetta2 Intel emulator since one component (ffmpeg) is still made the Intel CPUs. If you don't have it installed already, do this as follows: 
    &lt;ul&gt; 
     &lt;li&gt;Open the Terminal (located at &lt;code&gt;/Applications/Utilities/Terminal.app&lt;/code&gt;).&lt;/li&gt; 
     &lt;li&gt;Type &lt;code&gt;softwareupdate --install-rosetta&lt;/code&gt; or &lt;code&gt;softwareupdate --install-rosetta --agree-to-license&lt;/code&gt;.&lt;/li&gt; 
     &lt;li&gt;Hit enter and follow the instructions on the screen.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Start noScribe and/or noScribeEdit by double-clicking the app within your applications.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Older Macs with Intel processors&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;Note: Version 0.6.2 on Intel based Macs is currently experimental and may not fully work. Please help us testing it. &lt;a href="https://github.com/kaixxx/noScribe/discussions/143"&gt;You can download it here.&lt;/a&gt; &lt;br /&gt; Otherwise, you can use the stable version version 0.5:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;for macOS Sonoma (14) and Sequoia (15): &lt;a href="https://drive.switch.ch/index.php/s/EIVup04qkSHb54j?path=%2FnoScribe%20vers.%200.5%2FmacOS%2Fx86_64%20(Intel)"&gt;https://drive.switch.ch/index.php/s/EIVup04qkSHb54j?path=%2FnoScribe%20vers.%200.5%2FmacOS%2Fx86_64%20(Intel)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;for macOS 11 (Big Sur), 12 (Monterey) and 13 (Ventura): &lt;a href="https://drive.switch.ch/index.php/s/EIVup04qkSHb54j?path=%2FnoScribe%20vers.%200.5%2FmacOS%2Fx86_64_legacy%20(old%20Intel)"&gt;https://drive.switch.ch/index.php/s/EIVup04qkSHb54j?path=%2FnoScribe%20vers.%200.5%2FmacOS%2Fx86_64_legacy%20(old%20Intel)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Note: Unfortunately, we are currently not able to sign the x86_64 package correctly, so you will get a warning that noScribe and noScribeEdit are from unregistered developers. You have to manually allow noScribe and noScribeEdit to be executed, if your Gatekeeper is active. Follow these steps:&lt;/li&gt; 
   &lt;li&gt;Double-click the downloaded dmg-file.&lt;/li&gt; 
   &lt;li&gt;Drag noScribe and noScribeEdit into the link to your applications folder (labeled "drag both here to install").&lt;/li&gt; 
   &lt;li&gt;Start noScribe by double-clicking the app within your applications folder. You will get an error that noScribe is from an unregistered developer. Do the same with the noScribe Editor.&lt;/li&gt; 
   &lt;li&gt;Go to Settings -&amp;gt; Privacy and Security -&amp;gt; Scroll down until you see a message stating noScribe was prevented from starting and click "open anyway". Again, do the same with the noScribe Editor.&lt;/li&gt; 
   &lt;li&gt;From now on, both programs should start without issues.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Linux&lt;/h3&gt; 
&lt;p&gt;ported by &lt;a href="https://github.com/eckhrd"&gt;Eckhard Kadasch&lt;/a&gt; and &lt;a href="https://github.com/domna"&gt;Florian Dobener&lt;/a&gt;; executable generated by &lt;a href="https://github.com/gernophil"&gt;gernophil&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Executable installation:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Download the CUDA or CPU version of noScribe 0.6.2 for Linux here: &lt;a href="https://drive.switch.ch/index.php/s/HtKDKYRZRNaYBeI?path=%2FLinux"&gt;https://drive.switch.ch/index.php/s/HtKDKYRZRNaYBeI?path=%2FLinux&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Untar the file using the terminal command &lt;code&gt;tar -xzvf noScribe_0.6.2_cpu_linux_amd64.tar.gz&lt;/code&gt; or &lt;code&gt;tar -xzvf noScribe_0.6.2_cuda_linux_amd64.tar.gz&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;Execute noScribe using the terminal by &lt;code&gt;cd&lt;/code&gt;ing into the noScribe folder and executing &lt;code&gt;./noScribe&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;Optionally: Edit the files &lt;code&gt;noScribe.desktop&lt;/code&gt; and &lt;code&gt;noScribeEdit.desktop&lt;/code&gt; with a text editor and enter the complete path in the lines starting with &lt;code&gt;Exce=&lt;/code&gt; and &lt;code&gt;Icon=&lt;/code&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Manual installation from source:&lt;/strong&gt; Based on &lt;a href="https://github.com/kaixxx/noScribe/discussions/83"&gt;instructions by mael-lenoc&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;# release ( must be &amp;gt; 0.6 in order to include the latest fixes for linux!)
NOS_REL=0.6.1
wget https://github.com/kaixxx/noScribe/archive/refs/tags/v${NOS_REL).tar.gz
tar xvz -f v${NOS_REL).tar.gz
cd noScribe-${NOS_REL)/  # from here on all happens in this directory

# alternative: current main branch
wget -O noScribe-main.zip https://github.com/kaixxx/noScribe/archive/refs/heads/main.zip
unzip noScribe-main.zip
cd noScribe-main # from here on all happens in this directory

# install noScribeEdit
rm -rf noScribeEdit/
git clone https://github.com/kaixxx/noScribeEditor.git noScribeEdit

# venv
python3 -m venv .venv
source .venv/bin/activate  # from here on all happens in the venv

# requirements
pip install -r environments/requirements_linux.txt
pip install -r noScribeEdit/environments/requirements.txt

# models/precise
# this assumes you have git large file support enabled: apt install git-lfs
rm -rf models/precise
git clone https://huggingface.co/mobiuslabsgmbh/faster-whisper-large-v3-turbo models/precise
for f in config.json  model.bin  preprocessor_config.json  tokenizer.json  vocabulary.json; do wget -O models/fast/$f "https://huggingface.co/mukowaty/faster-whisper-int8/resolve/main/faster-whisper-large-v3-turbo-int8/${f}?download=true"; done

# run
python3 ./noScribe.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Old versions:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://drive.switch.ch/index.php/s/EIVup04qkSHb54j"&gt;https://drive.switch.ch/index.php/s/EIVup04qkSHb54j&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citation (APA Style)&lt;/h2&gt; 
&lt;p&gt;DrÃ¶ge, K. (2024). noScribe. AI-powered Audio Transcription (Version XXX) [Computer software]. &lt;a href="https://github.com/kaixxx/noScribe"&gt;https://github.com/kaixxx/noScribe&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Settings&lt;/h3&gt; 
&lt;img align="left" src="https://raw.githubusercontent.com/kaixxx/noScribe/main/img/noScribe_settings.png" width="300" /&gt; 
&lt;ul&gt; 
 &lt;li&gt;Select your &lt;strong&gt;audio file&lt;/strong&gt;. NoScribe supports almost any audio or video format.&lt;/li&gt; 
 &lt;li&gt;Select the &lt;strong&gt;filename for the transcript.&lt;/strong&gt; You can also choose the file type: *.html is the default, supported also by the noScribe editor. *.vtt is a video subtitles format and is especially useful if you want to import your transcript into &lt;a href="https://exmaralda.org/"&gt;EXMARaLDA&lt;/a&gt; for further annotation. *.txt exports the transcript as plain text.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Start&lt;/strong&gt; and &lt;strong&gt;Stop&lt;/strong&gt; accept timestamps in the format hh:mm:ss. Use this to limit the transcription to a particular part of the recording. This is especially helpful for testing your settings with a small sample before committing to transcribing the whole interview, which may take several hours. Leave &lt;strong&gt;Stop&lt;/strong&gt; empty if you want to transcribe until the end of the audio file.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Language:&lt;/strong&gt; Select the language of your transcript, set it to 'auto' to detect the language, or choose "multilingual" if your audio contains more than one language (experimental).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quality:&lt;/strong&gt; 'Precise' is the recommended setting for the most accurate transcript. On slower machines, you may opt for the 'fast' option. This will be quicker but might necessitate more manual revision later. You can also &lt;a href="https://github.com/kaixxx/noScribe/wiki/Add-custom-Whisper-models-for-transcription"&gt;install custom models&lt;/a&gt;, fine-tuned for specific languages, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Mark Pause&lt;/strong&gt;: If enabled, parts of your audio without voice activity will be marked as pauses. Pauses are transcribed as round brackets with one dot per second inside, e.g., '(..)' for a two-second pause. Pauses longer than 10 seconds are written out as '(XX seconds pause)' or '(XX minutes pause)'. You have the option to mark either pauses of one second and more ('1sec+'), two seconds and more ('2sec+'), or only the longer ones of three seconds and more ('3sec+'). Choose 'none' to disable this feature entirely.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Speaker Detection:&lt;/strong&gt; This feature uses the Pyannote AI model to identify distinct speakers in your audio and organizes the transcript accordingly. Choose the number of speakers if known, or select 'auto.' Opting for 'none' bypasses this step altogether, reducing the processing time by approximately half. However, the resultant transcript will be a continuous block of text without any indicators of speaker transitions.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Overlapping Speech&lt;/strong&gt;: If enabled, noScribe attempts to mark instances where two people speak simultaneously. The overlapping section is demarcated with //double slashes//. (Note: This is an experimental feature.)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Disfluencies&lt;/strong&gt;: If enabled, common speech disfluencies like filler words ("um"), unfinished words or sentences, etc. will also be transcribed.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Timestamps&lt;/strong&gt;: When enabled, noScribe incorporates timestamps in the format [hh:mm:ss] into the transcript either at every change of speaker or every 60 seconds. I find these timestamps somewhat distracting, hence my decision to disable them by default. However, they can be quite useful in certain contexts. Even with timestamps disabled, determining the audio timecode for a specific segment is straightforward: simply open the transcript in the noScribe Editor, navigate through the text, and the corresponding timecode will appear in the bottom right corner of the app.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Transcription process&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you are ready, click the &lt;strong&gt;Start&lt;/strong&gt;-button in the bottom left. &lt;strong&gt;Cancel&lt;/strong&gt; will abort the process.&lt;/li&gt; 
 &lt;li&gt;Be aware that &lt;strong&gt;a one-hour interview can take up to three hours processing time&lt;/strong&gt; and will put a heavy load on your machine. Doing this on battery-power is not recommended.&lt;/li&gt; 
 &lt;li&gt;A &lt;strong&gt;progress indicator&lt;/strong&gt; at the bottom of the app will show how far you are into the whole process.&lt;/li&gt; 
 &lt;li&gt;The &lt;strong&gt;main window&lt;/strong&gt; will log progress-messages and errors. It will also show the text of your interview during the last step of the transcription.&lt;/li&gt; 
 &lt;li&gt;The transcript will be auto saved every few seconds under the given filename.&lt;/li&gt; 
 &lt;li&gt;By default, noScribe produces an HTML-file. This can be opened in every common word editor (including MS Word, LibreOffice) or QDA-package (MAXQDA, ATLAS.ti, QualCoder...).&lt;/li&gt; 
 &lt;li&gt;Before working with the transcript though, you should check it with the included editor. There will always be some errors.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;noScribeEdit&lt;/h2&gt; 
&lt;p&gt;The included editor to check the final transcript.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/kaixxx/noScribe/main/img/noScribe_Editor.png" alt="The transcript in the noScribe Editor" /&gt;&lt;/p&gt; 
&lt;p&gt;The noScribe Editor is a separate app. It will open automatically once the transcript is finished, but can also be run independent from noScribe. It contains some handy features to check your finished transcript for errors and correct them:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Press &lt;strong&gt;Ctrl + Spacebar&lt;/strong&gt; (^Space on Mac) or the &lt;strong&gt;orange button in the toolbar&lt;/strong&gt; to hear the audio which corresponds to your current position in the text.&lt;/li&gt; 
 &lt;li&gt;The &lt;strong&gt;selection of the text will follow the audio that you hear&lt;/strong&gt;. If you want to &lt;strong&gt;make changes,&lt;/strong&gt; click anywhere in the text with your mouse or use the arrow keys to move the cursor. The audio will stop, and you can edit the text.&lt;/li&gt; 
 &lt;li&gt;You can also &lt;strong&gt;stop the audio&lt;/strong&gt; by pressing Ctrl + Spacebar again or clicking the orange button.&lt;/li&gt; 
 &lt;li&gt;If you want to &lt;strong&gt;speed up or slow down the audio&lt;/strong&gt;, change the "100%"-field next to the "Play/Pause Audio"-Button to the appropriate speed.&lt;/li&gt; 
 &lt;li&gt;To change the &lt;strong&gt;speaker names,&lt;/strong&gt; use the Search &amp;amp; Replace feature, accessible from the magnifying glass icon or the Edit menu.&lt;/li&gt; 
 &lt;li&gt;Use the plus und minus icons in the toolbar to &lt;strong&gt;zoom in or out&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;You will find the &lt;strong&gt;most common features of a basic text editor&lt;/strong&gt; in the toolbar as well as in the menu at the top (basic text formatting, cut, copy &amp;amp; paste, undo &amp;amp; redo).&lt;/li&gt; 
 &lt;li&gt;Your typical &lt;strong&gt;hotkeys&lt;/strong&gt; will also work (e.g., Ctrl+S for Save, Ctrl+F for Find &amp;amp; Replace). You can see all the hotkeys if you open the menu. As already mentioned, 'Ctrl+Space' is the hotkey you'll use the most as it starts or pauses the audio.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The source code of the editor can be found here: &lt;a href="https://github.com/kaixxx/noScribeEditor"&gt;https://github.com/kaixxx/noScribeEditor&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Factors Influencing the Quality of the Transcription&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;A &lt;strong&gt;good audio recording with clear voices and no ambient noise&lt;/strong&gt; is crucial for a high-quality transcription. Investing some effort in the quality of the recording will save you much time in the manual revision process later.&lt;/li&gt; 
 &lt;li&gt;Whisper (the AI powering noScribe) understands around 60 different languages, but the quality of the transcription varies widely between them. &lt;strong&gt;Spanish, Italian, English, Portuguese and German&lt;/strong&gt; are best supported (see &lt;a href="https://github.com/openai/whisper#available-models-and-languages"&gt;here for more info&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;Whisper handles &lt;strong&gt;dialects&lt;/strong&gt; fairly well (e.g., Swiss-German), but the transcript might need more manual work in the revision.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Known Issues&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;The whisper AI can sometimes get &lt;strong&gt;stuck in a loop of repeating text,&lt;/strong&gt; especially on longer audio files. If this happens, try to transcribe shorter sections (using the "Start" and "Stop" fields in noScribe), and join them manually.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multilingual audio&lt;/strong&gt;is now supported, but experimental.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Nonverbal expressions&lt;/strong&gt; like laughter are not included in the transcript and must be added later in the editor if you need them.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Speaker identification:&lt;/strong&gt; In some recordings, the AI used by noScribe may not be able to tell the voices of certain speakers apart, even if they sound quite different to the human ear. Check the results carefully.&lt;/li&gt; 
 &lt;li&gt;The whisper AI can sometimes &lt;strong&gt;hallucinate&lt;/strong&gt;, especially in silent parts of the recording when it interprets background noise as 'text' (see &lt;a href="https://facctconference.org/static/papers24/facct24-111.pdf"&gt;this study from the Cornell University&lt;/a&gt; for more info about the issue).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Advanced Options&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;After the app has run for the first time, you will find a file named &lt;strong&gt;config.yml&lt;/strong&gt; in the user config directory (on windows: C:\Users&amp;lt;username&amp;gt;\AppData\Local\noScribe\noScribe\config.yml; on Mac: "~/Library/Application Support/noscribe/config.yml"). Here, you can change a few &lt;strong&gt;extra settings,&lt;/strong&gt; e.g., the language of the user interface.&lt;/li&gt; 
 &lt;li&gt;Also in the user config directory you will find a folder named &lt;strong&gt;log&lt;/strong&gt; with detailed log-files for every transcript (also unfinished ones). This can be helpful in the case of any errors. Be aware though that these files also contain the text of your transcripts which might include sensitive information.&lt;/li&gt; 
 &lt;li&gt;If you want to use &lt;strong&gt;custom whisper models&lt;/strong&gt; with noScribe, follow the &lt;a href="https://github.com/kaixxx/noScribe/wiki/Add-custom-Whisper-models-for-transcription"&gt;instructions in the Wiki&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Development and Contribution&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;I developed noScribe in python 3.12&lt;/li&gt; 
 &lt;li&gt;I cannot host the whisper-models on GitHub because they are too large. There is a readme in the models-folder with instructions on how to get them.&lt;/li&gt; 
 &lt;li&gt;I am happy to review tests, bug reports and pull requests (if my time allows it)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Translations&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;The noScribe UI has already been translated into many languages (thanks mlynar-czyk).&lt;/li&gt; 
 &lt;li&gt;Since most of the translations have been created with ChatGPT, there will be problems. Please report any errors that youâ€™ll find and make â€“ if possible â€“ a pull request with a better translation.&lt;/li&gt; 
 &lt;li&gt;You will find the language files in the folder "trans".&lt;/li&gt; 
 &lt;li&gt;If you change anything in the language files, make sure to follow the conventions of the YAML language.&lt;/li&gt; 
 &lt;li&gt;If you want to change the language of the user interface, you have to change the value of the "locale" setting in the advanced settings (see above).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Other Software&lt;/h2&gt; 
&lt;p&gt;If you are interested in open source software for the analysis of qualitative data, take a look at &lt;a href="https://github.com/ccbogel/QualCoder"&gt;QualCoder&lt;/a&gt; and &lt;a href="https://www.taguette.org/"&gt;Taguette&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Alibaba-NLP/DeepResearch</title>
      <link>https://github.com/Alibaba-NLP/DeepResearch</link>
      <description>&lt;p&gt;Tongyi Deep Research, the Leading Open-source Deep Research Agent&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/logo.png" width="100%" /&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center" style="line-height: 1;"&gt; 
 &lt;p&gt;&lt;a href="https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B"&gt;&lt;img src="https://img.shields.io/badge/Models-5EDDD2?style=for-the-badge&amp;amp;logo=huggingface&amp;amp;logoColor=ffffff&amp;amp;labelColor" alt="MODELS" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Alibaba-NLP/DeepResearch"&gt;&lt;img src="https://img.shields.io/badge/Github-24292F?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" alt="GITHUB" /&gt;&lt;/a&gt; &lt;a href="https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/"&gt;&lt;img src="https://img.shields.io/badge/Blog-4285F4?style=for-the-badge&amp;amp;logo=google-chrome&amp;amp;logoColor=white" alt="Blog" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;p align="center"&gt; ğŸ¤— &lt;a href="https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B" target="_blank"&gt;HuggingFace&lt;/a&gt; ï½œ &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/tongyi.png" width="14px" style="display:inline;" /&gt; &lt;a href="https://modelscope.cn/models/iic/Tongyi-DeepResearch-30B-A3B" target="_blank"&gt;ModelScope&lt;/a&gt; | ğŸ’¬ &lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/wechat.jpg"&gt;WeChat(å¾®ä¿¡)&lt;/a&gt; &lt;/p&gt;
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/14895" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14895" alt="Alibaba-NLP%2FDeepResearch | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;ğŸ‘ Welcome to try Tongyi DeepResearch via our &lt;strong&gt;&lt;a href="https://www.modelscope.cn/studios/jialongwu/Tongyi-DeepResearch"&gt;&lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/tongyi.png" width="14px" style="display:inline;" /&gt; Modelscope online demo&lt;/a&gt;&lt;/strong&gt; or &lt;strong&gt;&lt;a href="https://huggingface.co/spaces/Alibaba-NLP/Tongyi-DeepResearch"&gt;ğŸ¤— Huggingface online demo&lt;/a&gt;&lt;/strong&gt; or &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/WebAgent/assets/aliyun.png" width="14px" style="display:inline;" /&gt; &lt;strong&gt;&lt;a href="https://bailian.console.aliyun.com/?spm=a2ty02.31808181.d_app-market.1.6c4974a1tFmoFc&amp;amp;tab=app#/app/app-market/deep-search/"&gt;bailian service&lt;/a&gt;&lt;/strong&gt;!&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] This demo is for quick exploration only. Response times may vary or fail intermittently due to model latency and tool QPS limits. For a stable experience we recommend local deployment; for a production-ready service, visit &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/WebAgent/assets/aliyun.png" width="14px" style="display:inline;" /&gt; &lt;a href="https://bailian.console.aliyun.com/?spm=a2ty02.31808181.d_app-market.1.6c4974a1tFmoFc&amp;amp;tab=app#/app/app-market/deep-search/"&gt;bailian&lt;/a&gt; and follow the guided setup.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;Introduction&lt;/h1&gt; 
&lt;p&gt;We present &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/tongyi.png" width="14px" style="display:inline;" /&gt; &lt;strong&gt;Tongyi DeepResearch&lt;/strong&gt;, an agentic large language model featuring 30.5 billion total parameters, with only 3.3 billion activated per token. Developed by Tongyi Lab, the model is specifically designed for &lt;strong&gt;long-horizon, deep information-seeking&lt;/strong&gt; tasks. Tongyi DeepResearch demonstrates state-of-the-art performance across a range of agentic search benchmarks, including Humanity's Last Exam, BrowserComp, BrowserComp-ZH, WebWalkerQA,xbench-DeepSearch, FRAMES and SimpleQA.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Tongyi DeepResearch builds upon our previous work on the &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/tongyi.png" width="14px" style="display:inline;" /&gt; &lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/WebAgent/"&gt;WebAgent&lt;/a&gt; project.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;More details can be found in our ğŸ“°&amp;nbsp;&lt;a href="https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/"&gt;Tech Blog&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/performance.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;âš™ï¸ &lt;strong&gt;Fully automated synthetic data generation pipeline&lt;/strong&gt;: We design a highly scalable data synthesis pipeline, which is fully automatic and empowers agentic pre-training, supervised fine-tuning, and reinforcement learning.&lt;/li&gt; 
 &lt;li&gt;ğŸ”„ &lt;strong&gt;Large-scale continual pre-training on agentic data&lt;/strong&gt;: Leveraging diverse, high-quality agentic interaction data to extend model capabilities, maintain freshness, and strengthen reasoning performance.&lt;/li&gt; 
 &lt;li&gt;ğŸ” &lt;strong&gt;End-to-end reinforcement learning&lt;/strong&gt;: We employ a strictly on-policy RL approach based on a customized Group Relative Policy Optimization framework, with token-level policy gradients, leave-one-out advantage estimation, and selective filtering of negative samples to stabilize training in a nonâ€‘stationary environment.&lt;/li&gt; 
 &lt;li&gt;ğŸ¤– &lt;strong&gt;Agent Inference Paradigm Compatibility&lt;/strong&gt;: At inference, Tongyi DeepResearch is compatible with two inference paradigms: ReAct, for rigorously evaluating the model's core intrinsic abilities, and an IterResearch-based 'Heavy' mode, which uses a test-time scaling strategy to unlock the model's maximum performance ceiling.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Model Download&lt;/h1&gt; 
&lt;p&gt;You can directly download the model by following the links below.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Model&lt;/th&gt; 
   &lt;th align="center"&gt;Download Links&lt;/th&gt; 
   &lt;th align="center"&gt;Model Size&lt;/th&gt; 
   &lt;th align="center"&gt;Context Length&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Tongyi-DeepResearch-30B-A3B&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B"&gt;ğŸ¤— HuggingFace&lt;/a&gt;&lt;br /&gt; &lt;a href="https://modelscope.cn/models/iic/Tongyi-DeepResearch-30B-A3B"&gt;ğŸ¤– ModelScope&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;30B-A3B&lt;/td&gt; 
   &lt;td align="center"&gt;128K&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;News&lt;/h1&gt; 
&lt;p&gt;[2025/09/20]ğŸš€ Tongyi-DeepResearch-30B-A3B is now on &lt;a href="https://openrouter.ai/alibaba/tongyi-deepresearch-30b-a3b"&gt;OpenRouter&lt;/a&gt;! Follow the &lt;a href="https://github.com/Alibaba-NLP/DeepResearch?tab=readme-ov-file#6-you-can-use-openrouters-api-to-call-our-model"&gt;Quick-start&lt;/a&gt; guide.&lt;/p&gt; 
&lt;p&gt;[2025/09/17]ğŸ”¥ We have released &lt;strong&gt;Tongyi-DeepResearch-30B-A3B&lt;/strong&gt;.&lt;/p&gt; 
&lt;h1&gt;Deep Research Benchmark Results&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/benchmark.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;This guide provides instructions for setting up the environment and running inference scripts located in the &lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/inference/"&gt;inference&lt;/a&gt; folder.&lt;/p&gt; 
&lt;h3&gt;1. Environment Setup&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Recommended Python version: &lt;strong&gt;3.10.0&lt;/strong&gt; (using other versions may cause dependency issues).&lt;/li&gt; 
 &lt;li&gt;It is strongly advised to create an isolated environment using &lt;code&gt;conda&lt;/code&gt; or &lt;code&gt;virtualenv&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Example with Conda
conda create -n react_infer_env python=3.10.0
conda activate react_infer_env
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Installation&lt;/h3&gt; 
&lt;p&gt;Install the required dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Environment Configuration and Prepare Evaluation Data&lt;/h3&gt; 
&lt;h4&gt;Environment Configuration&lt;/h4&gt; 
&lt;p&gt;Configure your API keys and settings by copying the example environment file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Copy the example environment file
cp .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Edit the &lt;code&gt;.env&lt;/code&gt; file and provide your actual API keys and configuration values:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;SERPER_KEY_ID&lt;/strong&gt;: Get your key from &lt;a href="https://serper.dev/"&gt;Serper.dev&lt;/a&gt; for web search and Google Scholar&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;JINA_API_KEYS&lt;/strong&gt;: Get your key from &lt;a href="https://jina.ai/"&gt;Jina.ai&lt;/a&gt; for web page reading&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;API_KEY/API_BASE&lt;/strong&gt;: OpenAI-compatible API for page summarization from &lt;a href="https://platform.openai.com/"&gt;OpenAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DASHSCOPE_API_KEY&lt;/strong&gt;: Get your key from &lt;a href="https://dashscope.aliyun.com/"&gt;Dashscope&lt;/a&gt; for file parsing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SANDBOX_FUSION_ENDPOINT&lt;/strong&gt;: Python interpreter sandbox endpoints (see &lt;a href="https://github.com/bytedance/SandboxFusion"&gt;SandboxFusion&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MODEL_PATH&lt;/strong&gt;: Path to your model weights&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DATASET&lt;/strong&gt;: Name of your evaluation dataset&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;OUTPUT_PATH&lt;/strong&gt;: Directory for saving results&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The &lt;code&gt;.env&lt;/code&gt; file is gitignored, so your secrets will not be committed to the repository.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Prepare Evaluation Data&lt;/h4&gt; 
&lt;p&gt;The system supports two input file formats: &lt;strong&gt;JSON&lt;/strong&gt; and &lt;strong&gt;JSONL&lt;/strong&gt;.&lt;/p&gt; 
&lt;h4&gt;Supported File Formats:&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Option 1: JSONL Format (recommended)&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create your data file with &lt;code&gt;.jsonl&lt;/code&gt; extension (e.g., &lt;code&gt;my_questions.jsonl&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Each line must be a valid JSON object with &lt;code&gt;question&lt;/code&gt; and &lt;code&gt;answer&lt;/code&gt; keys: &lt;pre&gt;&lt;code class="language-json"&gt;{"question": "What is the capital of France?", "answer": "Paris"}
{"question": "Explain quantum computing", "answer": ""}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Option 2: JSON Format&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create your data file with &lt;code&gt;.json&lt;/code&gt; extension (e.g., &lt;code&gt;my_questions.json&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;File must contain a JSON array of objects, each with &lt;code&gt;question&lt;/code&gt; and &lt;code&gt;answer&lt;/code&gt; keys: &lt;pre&gt;&lt;code class="language-json"&gt;[
  {"question": "What is the capital of France?", "answer": "Paris"},
  {"question": "Explain quantum computing", "answer": ""}
]
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Important Note:&lt;/strong&gt; The &lt;code&gt;answer&lt;/code&gt; field contains the &lt;strong&gt;ground truth/reference answer&lt;/strong&gt; used for evaluation. The system generates its own responses to the questions, and these reference answers are used to automatically judge the quality of the generated responses during benchmark evaluation.&lt;/p&gt; 
&lt;h4&gt;File References for Document Processing:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;If using the &lt;em&gt;file parser&lt;/em&gt; tool, &lt;strong&gt;prepend the filename to the &lt;code&gt;question&lt;/code&gt; field&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Place referenced files in &lt;code&gt;eval_data/file_corpus/&lt;/code&gt; directory&lt;/li&gt; 
 &lt;li&gt;Example: &lt;code&gt;{"question": "report.pdf What are the key findings?", "answer": "..."}&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;File Organization:&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;project_root/
â”œâ”€â”€ eval_data/
â”‚   â”œâ”€â”€ my_questions.jsonl          # Your evaluation data
â”‚   â””â”€â”€ file_corpus/                # Referenced documents
â”‚       â”œâ”€â”€ report.pdf
â”‚       â””â”€â”€ data.xlsx
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4. Configure the Inference Script&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Open &lt;code&gt;run_react_infer.sh&lt;/code&gt; and modify the following variables as instructed in the comments: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;MODEL_PATH&lt;/code&gt; - path to the local or remote model weights.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;DATASET&lt;/code&gt; - full path to your evaluation file, e.g. &lt;code&gt;eval_data/my_questions.jsonl&lt;/code&gt; or &lt;code&gt;/path/to/my_questions.json&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;OUTPUT_PATH&lt;/code&gt; - path for saving the prediction results, e.g. &lt;code&gt;./outputs&lt;/code&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Depending on the tools you enable (retrieval, calculator, web search, etc.), provide the required &lt;code&gt;API_KEY&lt;/code&gt;, &lt;code&gt;BASE_URL&lt;/code&gt;, or other credentials. Each key is explained inline in the bash script.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;5. Run the Inference Script&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;bash run_react_infer.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;With these steps, you can fully prepare the environment, configure the dataset, and run the model. For more details, consult the inline comments in each script or open an issue.&lt;/p&gt; 
&lt;h3&gt;6. You can use OpenRouter's API to call our model&lt;/h3&gt; 
&lt;p&gt;Tongyi-DeepResearch-30B-A3B is now available at &lt;a href="https://openrouter.ai/alibaba/tongyi-deepresearch-30b-a3b"&gt;OpenRouter&lt;/a&gt;. You can run the inference without any GPUs.&lt;/p&gt; 
&lt;p&gt;You need to modify the following in the file &lt;a href="https://github.com/Alibaba-NLP/DeepResearch/raw/main/inference/react_agent.py"&gt;inference/react_agent.py&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;In the call_server function: Set the API key and URL to your OpenRouter accountâ€™s API and URL.&lt;/li&gt; 
 &lt;li&gt;Change the model name to alibaba/tongyi-deepresearch-30b-a3b.&lt;/li&gt; 
 &lt;li&gt;Adjust the content concatenation way as described in the comments on lines &lt;strong&gt;88â€“90.&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Benchmark Evaluation&lt;/h2&gt; 
&lt;p&gt;We provide benchmark evaluation scripts for various datasets. Please refer to the &lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/evaluation/"&gt;evaluation scripts&lt;/a&gt; directory for more details.&lt;/p&gt; 
&lt;h2&gt;Deep Research Agent Family&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/family.png" /&gt; &lt;/p&gt; 
&lt;p&gt;Tongyi DeepResearch also has an extensive deep research agent family. You can find more information in the following paper:&lt;/p&gt; 
&lt;p&gt;[1] &lt;a href="https://arxiv.org/pdf/2501.07572"&gt;WebWalker: Benchmarking LLMs in Web Traversal&lt;/a&gt; (ACL 2025)&lt;br /&gt; [2] &lt;a href="https://arxiv.org/pdf/2505.22648"&gt;WebDancer: Towards Autonomous Information Seeking Agency&lt;/a&gt; (NeurIPS 2025)&lt;br /&gt; [3] &lt;a href="https://arxiv.org/pdf/2507.02592"&gt;WebSailor: Navigating Super-human Reasoning for Web Agent&lt;/a&gt;&lt;br /&gt; [4] &lt;a href="https://arxiv.org/pdf/2507.15061"&gt;WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization&lt;/a&gt;&lt;br /&gt; [5] &lt;a href="https://arxiv.org/pdf/2508.05748"&gt;WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent&lt;/a&gt;&lt;br /&gt; [6] &lt;a href="https://arxiv.org/pdf/2509.13309"&gt;WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents&lt;/a&gt;&lt;br /&gt; [7] &lt;a href="https://arxiv.org/pdf/2509.13313"&gt;ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization&lt;/a&gt;&lt;br /&gt; [8] &lt;a href="https://arxiv.org/pdf/2509.13312"&gt;WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research&lt;/a&gt;&lt;br /&gt; [9] &lt;a href="https://arxiv.org/pdf/2509.13305"&gt;WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning&lt;/a&gt;&lt;br /&gt; [10] &lt;a href="https://arxiv.org/pdf/2509.13310"&gt;Scaling Agents via Continual Pre-training&lt;/a&gt;&lt;br /&gt; [11] &lt;a href="https://arxiv.org/pdf/2509.13311"&gt;Towards General Agentic Intelligence via Environment Scaling&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸŒŸ Misc&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://www.star-history.com/#Alibaba-NLP/DeepResearch&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Alibaba-NLP/DeepResearch&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸš© Talent Recruitment&lt;/h2&gt; 
&lt;p&gt;ğŸ”¥ğŸ”¥ğŸ”¥ We are hiring! Research intern positions are open (based in Hangzhouã€Beijingã€Shanghai)&lt;/p&gt; 
&lt;p&gt;ğŸ“š &lt;strong&gt;Research Area&lt;/strong&gt;ï¼šWeb Agent, Search Agent, Agent RL, MultiAgent RL, Agentic RAG&lt;/p&gt; 
&lt;p&gt;â˜ï¸ &lt;strong&gt;Contact&lt;/strong&gt;ï¼š&lt;a href=""&gt;yongjiang.jy@alibaba-inc.com&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contact Information&lt;/h2&gt; 
&lt;p&gt;For communications, please contact Yong Jiang (&lt;a href="mailto:yongjiang.jy@alibaba-inc.com"&gt;yongjiang.jy@alibaba-inc.com&lt;/a&gt;).&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{tongyidr,
  author={Tongyi DeepResearch Team},
  title={Tongyi-DeepResearch},
  year={2025},
  howpublished={\url{https://github.com/Alibaba-NLP/DeepResearch}}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>google-agentic-commerce/a2a-x402</title>
      <link>https://github.com/google-agentic-commerce/a2a-x402</link>
      <description>&lt;p&gt;The A2A x402 Extension brings cryptocurrency payments to the Agent-to-Agent (A2A) protocol, enabling agents to monetize their services through on-chain payments. This extension revives the spirit of HTTP 402 "Payment Required" for the decentralized agent ecosystem.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;A2A x402 Extension&lt;/h1&gt; 
&lt;p&gt;The &lt;strong&gt;A2A x402 Extension&lt;/strong&gt; brings cryptocurrency payments to the Agent-to-Agent (A2A) protocol, enabling agents to monetize their services through on-chain payments. This extension revives the spirit of HTTP 402 "Payment Required" for the decentralized agent ecosystem.&lt;/p&gt; 
&lt;h2&gt;ğŸ¯ &lt;strong&gt;Goal&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;Enable &lt;strong&gt;agent commerce&lt;/strong&gt; by providing a standardized way for agents to charge for their services and receive payments on-chain. This transforms any A2A agent into a commercial service that can charge for API calls, data processing, AI inference, or any other valuable capability.&lt;/p&gt; 
&lt;h2&gt;ğŸ—‚ï¸ &lt;strong&gt;Repository Structure&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;This repository contains the specification, core libraries, and example implementations for the A2A x402 extension, supporting multiple languages.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;x402-a2a/
â”œâ”€â”€ spec/
â”‚   â””â”€â”€ v0.1/
â”‚       â””â”€â”€ spec.md         # The official x402 extension specification
â”‚
â”œâ”€â”€ schemes/                # Directory contains experimental x402 payment schemes drafted by partners and other contributors
â”‚
â””â”€â”€ {language}/             # Language-specific implementations (e.g., python/, typescript/)
    â”œâ”€â”€ x402_a2a/           # The core library for the x402 extension
    â”‚
    â””â”€â”€ examples/
        â””â”€â”€ {demo}/         # Demonstrations for each language implementation
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ¤” &lt;strong&gt;How It Works&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;The x402 extension defines a simple, robust payment flow between agents:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Payment Required:&lt;/strong&gt; A merchant agent, when payment is required for a service, responds with a &lt;code&gt;payment-required&lt;/code&gt; message.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Payment Submitted:&lt;/strong&gt; The client agent signs the payment details and sends them back to the merchant in a &lt;code&gt;payment-submitted&lt;/code&gt; message.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Payment Completed:&lt;/strong&gt; The merchant verifies the payment, settles it on-chain, and responds with a &lt;code&gt;payment-completed&lt;/code&gt; message, delivering the requested service.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This flow is designed to be implemented in any language, allowing developers to focus on their agent's core logic.&lt;/p&gt; 
&lt;h2&gt;ğŸš€ &lt;strong&gt;Getting Started&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;Each language-specific implementation (e.g., &lt;code&gt;python/x402_a2a&lt;/code&gt;) contains its own &lt;code&gt;README.md&lt;/code&gt; with detailed instructions on how to install dependencies, run tests, and use the library.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;examples/&lt;/code&gt; directory contains various demonstrations of the x402 extension. Each example also has its own &lt;code&gt;README.md&lt;/code&gt; with instructions on how to run it.&lt;/p&gt; 
&lt;h2&gt;ğŸ—ï¸ &lt;strong&gt;Architecture&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;x402_a2a&lt;/code&gt; libraries follow a &lt;strong&gt;functional core, imperative shell&lt;/strong&gt; architecture:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Core Protocol:&lt;/strong&gt; The fundamental data structures and functions for creating, signing, and verifying payments.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Executors:&lt;/strong&gt; Middleware that automates the payment flow, making it easy to add payment capabilities to any agent.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This design provides both flexibility and ease of use, allowing developers to either build custom payment logic with the core protocol or use the executors for a more hands-off approach.&lt;/p&gt; 
&lt;h2&gt;ğŸ“š &lt;strong&gt;Learn More&lt;/strong&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/google-agentic-commerce/a2a-x402/main/spec/v0.1/spec.md"&gt;Specification&lt;/a&gt;&lt;/strong&gt;: The complete technical specification for the x402 extension.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/google-agentic-commerce/a2a-x402/main/python/x402_a2a/README.md"&gt;Python Library&lt;/a&gt;&lt;/strong&gt;: The documentation for the Python implementation of the x402 extension.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/google-agentic-commerce/a2a-x402/main/python/examples/"&gt;Python Examples&lt;/a&gt;&lt;/strong&gt;: The directory containing demonstration applications for the Python implementation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/a2aproject/a2a-python"&gt;A2A Protocol&lt;/a&gt;&lt;/strong&gt;: The core agent-to-agent protocol.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://x402.gitbook.io/x402"&gt;x402 Protocol&lt;/a&gt;&lt;/strong&gt;: The underlying payment protocol.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ¤ &lt;strong&gt;Contributing&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! Please read the &lt;a href="https://raw.githubusercontent.com/google-agentic-commerce/a2a-x402/main/spec/v0.1/spec.md"&gt;specification&lt;/a&gt; and the existing code to understand the project's design and goals. Then, feel free to open a pull request with your changes.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google-research/timesfm</title>
      <link>https://github.com/google-research/timesfm</link>
      <description>&lt;p&gt;TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TimesFM&lt;/h1&gt; 
&lt;p&gt;TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2310.10688"&gt;A decoder-only foundation model for time-series forecasting&lt;/a&gt;, ICML 2024.&lt;/li&gt; 
 &lt;li&gt;All checkpoints: &lt;a href="https://huggingface.co/collections/google/timesfm-release-66e4be5fdb56e960c1e482a6"&gt;TimesFM Hugging Face Collection&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/"&gt;Google Research blog&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cloud.google.com/bigquery/docs/timesfm-model"&gt;TimesFM in BigQuery&lt;/a&gt;: an official Google product.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This open version is not an officially supported Google product.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Latest Model Version:&lt;/strong&gt; TimesFM 2.5&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Archived Model Versions:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;1.0 and 2.0: relevant code archived in the sub directory &lt;code&gt;v1&lt;/code&gt;. You can &lt;code&gt;pip install timesfm==1.3.0&lt;/code&gt; to install an older version of this package to load them.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Update - Sept. 15, 2025&lt;/h2&gt; 
&lt;p&gt;TimesFM 2.5 is out!&lt;/p&gt; 
&lt;p&gt;Comparing to TimesFM 2.0, this new 2.5 model:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;uses 200M parameters, down from 500M.&lt;/li&gt; 
 &lt;li&gt;supports up to 16k context length, up from 2048.&lt;/li&gt; 
 &lt;li&gt;supports continuous quantile forecast up to 1k horizon via an optional 30M quantile head.&lt;/li&gt; 
 &lt;li&gt;gets rid of the &lt;code&gt;frequency&lt;/code&gt; indicator.&lt;/li&gt; 
 &lt;li&gt;has a couple of new forecasting flags.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Along with the model upgrade we have also upgraded the inference API. This repo will be under construction over the next few weeks to&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;add support for an upcoming Flax version of the model (faster inference).&lt;/li&gt; 
 &lt;li&gt;add back covariate support.&lt;/li&gt; 
 &lt;li&gt;populate more docstrings, docs and notebook.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Install&lt;/h3&gt; 
&lt;p&gt;TODO(siriuz42): Package timesfm==2.0.0 and upload to PyPI .&lt;/p&gt; 
&lt;p&gt;Run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/google-research/timesfm.git
cd timesfm
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Code Example&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np
import timesfm
model = timesfm.TimesFM_2p5_200M_torch()
model.load_checkpoint()
model.compile(
    timesfm.ForecastConfig(
        max_context=1024,
        max_horizon=256,
        normalize_inputs=True,
        use_continuous_quantile_head=True,
        force_flip_invariance=True,
        infer_is_positive=True,
        fix_quantile_crossing=True,
    )
)
point_forecast, quantile_forecast = model.forecast(
    horizon=12,
    inputs=[
        np.linspace(0, 1, 100),
        np.sin(np.linspace(0, 20, 67)),
    ],  # Two dummy inputs
)
point_forecast.shape  # (2, 12)
quantile_forecast.shape  # (2, 12, 10): mean, then 10th to 90th quantiles.
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Kludex/uvicorn</title>
      <link>https://github.com/Kludex/uvicorn</link>
      <description>&lt;p&gt;An ASGI web server, for Python. ğŸ¦„&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img width="320" height="320" src="https://raw.githubusercontent.com/tomchristie/uvicorn/main/docs/uvicorn.png" alt="uvicorn" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;em&gt;An ASGI web server, for Python.&lt;/em&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href="https://github.com/Kludex/uvicorn/actions"&gt;&lt;img src="https://github.com/Kludex/uvicorn/workflows/Test%20Suite/badge.svg?sanitize=true" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/uvicorn"&gt;&lt;img src="https://badge.fury.io/py/uvicorn.svg?sanitize=true" alt="Package version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/uvicorn"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/uvicorn.svg?color=%2334D058" alt="Supported Python Version" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: &lt;a href="https://uvicorn.dev"&gt;https://uvicorn.dev&lt;/a&gt; &lt;strong&gt;Source Code&lt;/strong&gt;: &lt;a href="https://www.github.com/Kludex/uvicorn"&gt;https://www.github.com/Kludex/uvicorn&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Uvicorn is an ASGI web server implementation for Python.&lt;/p&gt; 
&lt;p&gt;Until recently Python has lacked a minimal low-level server/application interface for async frameworks. The &lt;a href="https://asgi.readthedocs.io/en/latest/"&gt;ASGI specification&lt;/a&gt; fills this gap, and means we're now able to start building a common set of tooling usable across all async frameworks.&lt;/p&gt; 
&lt;p&gt;Uvicorn supports HTTP/1.1 and WebSockets.&lt;/p&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;Install using &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;$ pip install uvicorn
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will install uvicorn with minimal (pure Python) dependencies.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;$ pip install 'uvicorn[standard]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will install uvicorn with "Cython-based" dependencies (where possible) and other "optional extras".&lt;/p&gt; 
&lt;p&gt;In this context, "Cython-based" means the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the event loop &lt;code&gt;uvloop&lt;/code&gt; will be installed and used if possible.&lt;/li&gt; 
 &lt;li&gt;the http protocol will be handled by &lt;code&gt;httptools&lt;/code&gt; if possible.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Moreover, "optional extras" means that:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the websocket protocol will be handled by &lt;code&gt;websockets&lt;/code&gt; (should you want to use &lt;code&gt;wsproto&lt;/code&gt; you'd need to install it manually) if possible.&lt;/li&gt; 
 &lt;li&gt;the &lt;code&gt;--reload&lt;/code&gt; flag in development mode will use &lt;code&gt;watchfiles&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;windows users will have &lt;code&gt;colorama&lt;/code&gt; installed for the colored logs.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;python-dotenv&lt;/code&gt; will be installed should you want to use the &lt;code&gt;--env-file&lt;/code&gt; option.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;PyYAML&lt;/code&gt; will be installed to allow you to provide a &lt;code&gt;.yaml&lt;/code&gt; file to &lt;code&gt;--log-config&lt;/code&gt;, if desired.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Create an application, in &lt;code&gt;example.py&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;async def app(scope, receive, send):
    assert scope['type'] == 'http'

    await send({
        'type': 'http.response.start',
        'status': 200,
        'headers': [
            (b'content-type', b'text/plain'),
        ],
    })
    await send({
        'type': 'http.response.body',
        'body': b'Hello, world!',
    })
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run the server:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;$ uvicorn example:app
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Why ASGI?&lt;/h2&gt; 
&lt;p&gt;Most well established Python Web frameworks started out as WSGI-based frameworks.&lt;/p&gt; 
&lt;p&gt;WSGI applications are a single, synchronous callable that takes a request and returns a response. This doesnâ€™t allow for long-lived connections, like you get with long-poll HTTP or WebSocket connections, which WSGI doesn't support well.&lt;/p&gt; 
&lt;p&gt;Having an async concurrency model also allows for options such as lightweight background tasks, and can be less of a limiting factor for endpoints that have long periods being blocked on network I/O such as dealing with slow HTTP requests.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Alternative ASGI servers&lt;/h2&gt; 
&lt;p&gt;A strength of the ASGI protocol is that it decouples the server implementation from the application framework. This allows for an ecosystem of interoperating webservers and application frameworks.&lt;/p&gt; 
&lt;h3&gt;Daphne&lt;/h3&gt; 
&lt;p&gt;The first ASGI server implementation, originally developed to power Django Channels, is &lt;a href="https://github.com/django/daphne"&gt;the Daphne webserver&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;It is run widely in production, and supports HTTP/1.1, HTTP/2, and WebSockets.&lt;/p&gt; 
&lt;p&gt;Any of the example applications given here can equally well be run using &lt;code&gt;daphne&lt;/code&gt; instead.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ pip install daphne
$ daphne app:App
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Hypercorn&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/pgjones/hypercorn"&gt;Hypercorn&lt;/a&gt; was initially part of the Quart web framework, before being separated out into a standalone ASGI server.&lt;/p&gt; 
&lt;p&gt;Hypercorn supports HTTP/1.1, HTTP/2, and WebSockets.&lt;/p&gt; 
&lt;p&gt;It also supports &lt;a href="https://trio.readthedocs.io"&gt;the excellent &lt;code&gt;trio&lt;/code&gt; async framework&lt;/a&gt;, as an alternative to &lt;code&gt;asyncio&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ pip install hypercorn
$ hypercorn app:App
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Mangum&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/jordaneremieff/mangum"&gt;Mangum&lt;/a&gt; is an adapter for using ASGI applications with AWS Lambda &amp;amp; API Gateway.&lt;/p&gt; 
&lt;h3&gt;Granian&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/emmett-framework/granian"&gt;Granian&lt;/a&gt; is an ASGI compatible Rust HTTP server which supports HTTP/2, TLS and WebSockets.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt;&lt;i&gt;Uvicorn is &lt;a href="https://github.com/Kludex/uvicorn/raw/main/LICENSE.md"&gt;BSD licensed&lt;/a&gt; code.&lt;br /&gt;Designed &amp;amp; crafted with care.&lt;/i&gt;&lt;br /&gt;â€” ğŸ¦„ â€”&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>HKUDS/AI-Researcher</title>
      <link>https://github.com/HKUDS/AI-Researcher</link>
      <description>&lt;p&gt;[NeurIPS2025] "AI-Researcher: Autonomous Scientific Innovation" -- A production-ready version: https://novix.science/chat&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/assets/ai-researcher.png" alt="Logo" width="400" /&gt; 
 &lt;h1 align="center"&gt;"AI-Researcher: Autonomous Scientific Innovation" &lt;/h1&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/14638" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14638" alt="HKUDS%2FAI-Researcher | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://autoresearcher.github.io"&gt;&lt;img src="https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&amp;amp;color=FFE165&amp;amp;logo=homepage&amp;amp;logoColor=white" alt="Project Page" /&gt;&lt;/a&gt; 
 &lt;a href="https://join.slack.com/t/ai-researchergroup/shared_invite/zt-30y5a070k-C0ajQt1zmVczFnfGkIicvA"&gt;&lt;img src="https://img.shields.io/badge/Slack-Join%20Us-red?logo=slack&amp;amp;logoColor=white&amp;amp;style=for-the-badge" alt="Join our Slack community" /&gt;&lt;/a&gt; 
 &lt;a href="https://discord.gg/zBNYTk5q2g"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20Us-purple?logo=discord&amp;amp;logoColor=white&amp;amp;style=for-the-badge" alt="Join our Discord community" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://autoresearcher.github.io/docs"&gt;&lt;img src="https://img.shields.io/badge/Documentation-000?logo=googledocs&amp;amp;logoColor=FFE165&amp;amp;style=for-the-badge" alt="Check out the documentation" /&gt;&lt;/a&gt; 
 &lt;a href="https://arxiv.org/abs/2505.18705"&gt;&lt;img src="https://img.shields.io/badge/Paper%20on%20Arxiv-000?logoColor=FFE165&amp;amp;logo=arxiv&amp;amp;style=for-the-badge" alt="Paper" /&gt;&lt;/a&gt; 
 &lt;a href="https://autoresearcher.github.io/leaderboard"&gt;&lt;img src="https://img.shields.io/badge/DATASETS-000?logoColor=FFE165&amp;amp;logo=huggingface&amp;amp;style=for-the-badge" alt="Benchmark" /&gt;&lt;/a&gt; 
 &lt;hr /&gt; 
&lt;/div&gt; 
&lt;p&gt;Welcome to &lt;strong&gt;AI-Researcher&lt;/strong&gt;ğŸ¤— AI-Researcher introduces a revolutionary breakthrough in &lt;strong&gt;Automated Scientific Discovery&lt;/strong&gt;ğŸ”¬, presenting a new system that fundamentally &lt;strong&gt;Reshapes the Traditional Research Paradigm&lt;/strong&gt;. This state-of-the-art platform empowers researchers with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ¯ &lt;strong&gt;Full Autonomy&lt;/strong&gt;: Complete end-to-end research automation&lt;/li&gt; 
 &lt;li&gt;ğŸ”„ &lt;strong&gt;Seamless Orchestration&lt;/strong&gt;: From concept to publication&lt;/li&gt; 
 &lt;li&gt;ğŸ§  &lt;strong&gt;Advanced AI Integration&lt;/strong&gt;: Powered by cutting-edge AI agents&lt;/li&gt; 
 &lt;li&gt;ğŸš€ &lt;strong&gt;Research Acceleration&lt;/strong&gt;: Streamlined scientific innovation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;âœ¨ The AI-Researcher system accepts user input queries at two distinct levels âœ¨&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Level 1: Detailed Idea Description&lt;/strong&gt; &lt;br /&gt; At this level, users provide comprehensive descriptions of their specific research ideas. The system processes these detailed inputs to develop implementation strategies based on the user's explicit requirements.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Level 2: Reference-Based Ideation&lt;/strong&gt; &lt;br /&gt; This simpler level involves users submitting reference papers without a specific idea in mind. The user query typically follows the format: "I have some reference papers, please come up with an innovative idea and implement it with these papers." The system then analyzes the provided references to generate and develop novel research concepts.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;ğŸŒŸ&lt;strong&gt;Core Capabilities &amp;amp; Integration&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;AI-Researcher&lt;/strong&gt; delivers a &lt;strong&gt;Comprehensive Research Ecosystem&lt;/strong&gt; through seamless integration of critical components:&lt;/p&gt; 
&lt;p&gt;ğŸš€&lt;strong&gt;Primary Research Functions&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“š &lt;strong&gt;Literature Review&lt;/strong&gt;: Conducts comprehensive analysis and synthesis of existing research.&lt;/li&gt; 
 &lt;li&gt;ğŸ“Š &lt;strong&gt;Idea Generation&lt;/strong&gt;: Systematically gathers, organizes, and formulates novel research directions.&lt;/li&gt; 
 &lt;li&gt;ğŸ§ª &lt;strong&gt;Algorithm Design and Implementation&lt;/strong&gt;: Develops methodologies and transforms ideas into functional implementations.&lt;/li&gt; 
 &lt;li&gt;ğŸ’» &lt;strong&gt;Algorithm Validation and Refinement&lt;/strong&gt;: Automates testing, performance evaluation, and iterative optimization.&lt;/li&gt; 
 &lt;li&gt;ğŸ“ˆ &lt;strong&gt;Result Analysis&lt;/strong&gt;: Delivers advanced interpretation of experimental data and insights.&lt;/li&gt; 
 &lt;li&gt;âœï¸ &lt;strong&gt;Manuscript Creation&lt;/strong&gt;: Automatically generates polished, full-length academic papers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;!-- &lt;img src="./assets/AI-Researchernew-intro.pdf" alt="Logo" width="100%"&gt; --&gt; 
 &lt;figure&gt; 
  &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/assets/AI-Researcher-Framework.png" alt="Logo" style="max-width: 100%; height: auto;" /&gt; 
  &lt;br /&gt; 
  &lt;figcaption&gt;
   &lt;em&gt;Quick Overview of AI-Researcher.&lt;/em&gt;
  &lt;/figcaption&gt; 
 &lt;/figure&gt; 
&lt;/div&gt; 
&lt;span id="news"&gt;&lt;/span&gt; 
&lt;h2&gt;ğŸ”¥ News&lt;/h2&gt; 
&lt;div class="scrollable"&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;[2025. 09]&lt;/strong&gt;: &amp;nbsp; ğŸ¯ğŸ¯ğŸ“¢ğŸ“¢ Exciting News! We are thrilled to announce that our ğŸŒŸAI-ResearcherğŸŒŸ has been accepted as a Spotlight paper at NeurIPS 2025! ğŸ‰ğŸ‰ Thanks to all the team members ğŸ¤—  &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2025. 05]&lt;/strong&gt;: &amp;nbsp;ğŸ‰ğŸ‰ &lt;b&gt;Major Release! AI-Researcher Comprehensive Upgrade!&lt;/b&gt; ğŸš€ &lt;br /&gt;We are excited to announce a significant milestone for AI-Researcher: 
   &lt;ul&gt; 
    &lt;li&gt;ğŸ“„ &lt;b&gt;&lt;a href="https://arxiv.org/abs/2505.18705"&gt;Academic Paper Release&lt;/a&gt;&lt;/b&gt;: Detailed exposition of our innovative methods and experimental results&lt;/li&gt; 
    &lt;li&gt;ğŸ“Š &lt;b&gt;&lt;a href="https://autoresearcher.github.io/leaderboard"&gt;Benchmark Suite&lt;/a&gt;&lt;/b&gt;: Comprehensive evaluation framework and datasets&lt;/li&gt; 
    &lt;li&gt;ğŸ–¥ï¸ &lt;b&gt;Web GUI Interface&lt;/b&gt;: User-friendly graphical interface making research more convenient&lt;/li&gt; 
   &lt;/ul&gt; &lt;b&gt;ğŸ¤ Join Us!&lt;/b&gt; We welcome researchers, developers, and AI enthusiasts to contribute together and advance AI research development. Whether it's code contributions, bug reports, feature suggestions, or documentation improvements, every contribution is valuable! &lt;br /&gt;ğŸ’¡ &lt;i&gt;Let's build a smarter AI research assistant together!&lt;/i&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2025, Mar 04]&lt;/strong&gt;: &amp;nbsp;ğŸ‰ğŸ‰We've launched &lt;b&gt;AI-Researcher!&lt;/b&gt;, The release includes the complete framework, datasets, benchmark construction pipeline, and much more. Stay tunedâ€”there's plenty more to come! ğŸš€&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;span id="table-of-contents"&gt;&lt;/span&gt; 
&lt;h2&gt;ğŸ“‘ Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#news"&gt;ğŸ”¥ News&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#quick-start"&gt;âš¡ Quick Start&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#api-keys-setup"&gt;API Keys Setup&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#examples"&gt;â¬‡ï¸ Examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#how-it-works"&gt;âœ¨ How AI-Researcher works&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#how-to-use"&gt;ğŸ” How to use AI-Researcher&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#documentation"&gt;ğŸ“– Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#community"&gt;ğŸ¤ Join the Community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#acknowledgements"&gt;ğŸ™ Acknowledgements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#cite"&gt;ğŸŒŸ Cite&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="quick-start"&gt;&lt;/span&gt; 
&lt;h2&gt;âš¡ Quick Start&lt;/h2&gt; 
&lt;span id="installation"&gt;&lt;/span&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;h4&gt;AI Installation&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Using &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;We recommend to use &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt; to manage packages in our project (Much more faster than conda)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# install uv
curl -LsSf https://astral.sh/uv/install.sh | sh
source ~/.bashrc

# clone the project
git clone https://github.com/HKUDS/AI-Researcher.git
cd AI-Researcher

# install and activate enviroment
uv venv --python 3.11
source ./.venv/bin/activate
uv pip install -e .
playwright install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Docker Installation&lt;/h4&gt; 
&lt;p&gt;To set up the agent-interactive environment, we use Docker for containerization. Please ensure you have &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt; installed on your system before proceeding. For running the research agent, we utilize the Docker image 'tjbtech1/airesearcher:v1t'. You can pull this image by executing the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker pull tjbtech1/airesearcher:v1
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or you can build the docker image from our provided &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/docker/Dockerfile"&gt;Dockerfile&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd ./docker &amp;amp;&amp;amp; docker build -t tjbtech1/airesearcher:v1 .
&lt;/code&gt;&lt;/pre&gt; 
&lt;span id="api-keys-setup"&gt;&lt;/span&gt; 
&lt;h3&gt;API Keys Setup&lt;/h3&gt; 
&lt;p&gt;Create an environment variable file based on the provided '.env.template' file. In this file, you should set the configuration including api key, instance id of the test case.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;
# ================ container configuration ================
# workplace of the research agent
DOCKER_WORKPLACE_NAME=workplace_paper
# base image of the research agent
BASE_IMAGES=tjbtech1/airesearcher:v1
# completion model name, configuration details see: https://docs.litellm.ai/docs/
COMPLETION_MODEL=openrouter/google/gemini-2.5-pro-preview-05-20
# cheep model name, configuration details see: https://docs.litellm.ai/docs/
CHEEP_MODEL=openrouter/google/gemini-2.5-pro-preview-05-20
# specific gpu of the research agent, can be: 
# '"device=0"' using the first gpu
# '"device=0,1"' using the first and second gpu
# '"all"' using all gpus
# None for no gpu
GPUS='"device=0"'
# name of the container
CONTAINER_NAME=paper_eval
# name of the workplace
WORKPLACE_NAME=workplace
# path of the cache
CACHE_PATH=cache
# port of the research agent
PORT=7020
# platform of the research agent
PLATFORM=linux/amd64

# ================ llm configuration ================
# github ai token of the research agent
GITHUB_AI_TOKEN=your_github_ai_token
# openrouter api key of the research agent
OPENROUTER_API_KEY=your_openrouter_api_key
# openrouter api base url of the research agent
OPENROUTER_API_BASE=https://openrouter.ai/api/v1

# ================ task configuration ================
# category of the research agent, based on: ./benchmark/final. Can be: 
# diffu_flow
# gnn
# reasoning
# recommendation
# vq
# example: ./benchmark/final/vq
CATEGORY=vq
# instance id of the research agent, example: ./benchmark/final/vq/one_layer_vq.json
INSTANCE_ID=one_layer_vq
# task level of the research agent, can be: 
# task1
# task2
TASK_LEVEL=task1
# maximum iteration times of the research agent
MAX_ITER_TIMES=0
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ğŸ”¥ Web GUI&lt;/h3&gt; 
&lt;p&gt;We add a webgui based on gradio. Just run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python web_ai_researcher.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/assets/webgui/image-20250606135137558.png" alt="image-20250606135137558" /&gt;&lt;/p&gt; 
&lt;p&gt;You can configure the environment variables in the following tab:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/assets/webgui/image-20250606135325373.png" alt="image-20250606135325373" /&gt;&lt;/p&gt; 
&lt;p&gt;Select the following example to run our AI-Researcher:&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/assets/webgui/image-20250606135507970.png" alt="image-20250606135507970" style="zoom:67%;" /&gt; 
&lt;span id="examples"&gt;&lt;/span&gt; 
&lt;h2&gt;â¬‡ï¸ Examples&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;âš ï¸ &lt;strong&gt;ALERT&lt;/strong&gt;: The GIFs below are large files and may &lt;strong&gt;take some time to load&lt;/strong&gt;. &lt;strong&gt;Please be patient while they render completely&lt;/strong&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Example 1 (Vector Quantized)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed model designed in this paper is designed to improve the performance of Vector Quantized Variational AutoEncoders (VQ-VAEs) by addressing issues with gradient propagation through the non-differentiable vector quantization layer.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;The core methodologies utilized include: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Rotation and Rescaling Transformation&lt;/strong&gt;: A linear transformation that alters the encoder output to align it with the nearest codebook vector without changing the forward pass output.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Gradient Propagation Method&lt;/strong&gt;: The proposed model ensures that gradients flow from the decoder to the encoder while preserving the angle between the gradient and codebook vector.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Codebook Management&lt;/strong&gt;: Utilizes the connection between the encoder output and the corresponding codebook vectors to mitigate codebook collapse and improve utilization.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;The primary functions of these components are: 
    &lt;ul&gt; 
     &lt;li&gt;The rotation and rescaling transformation modifies how the encoder output is quantized and how information is retained during backpropagation, enabling gradients to reflect the true positioning of the encoder output relative to the codebook vectors.&lt;/li&gt; 
     &lt;li&gt;The gradient propagation method redefines how gradients are transported back to the encoder, allowing for an enhanced and nuanced movement through the quantization layer, which leads to a better performance during training.&lt;/li&gt; 
     &lt;li&gt;Codebook management practices help in maintaining a diverse set of codebook vectors throughout training, avoiding scenarios where multiple vectors become redundant or unused.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Implementation details for each component: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Key Parameters&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Codebook size should be configured based on the complexity of the dataset (e.g., 1024 or 8192).&lt;/li&gt; 
       &lt;li&gt;Commitment loss coefficient (Î²) is typically set within [0.25, 2].&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Input/Output Specifications&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Input to the encoder is a continuous high-dimensional vector, while the output is a corresponding quantized vector from the codebook.&lt;/li&gt; 
       &lt;li&gt;The output for reconstruction is generated using the decoder applied to the transformed codebook vectors.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Important Constraints&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Ensure that the codebook is updated correctly with an exponential moving average procedure, and treat both rotation and rescaling during the forward pass as constants with respect to the gradient.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Step-by-Step Integration of Components: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: Input the data vector into the encoder to obtain the continuous representation.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: Identify the nearest codebook vector to the encoder output.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 3&lt;/strong&gt;: Compute the rotation matrix that aligns the encoder output to the codebook vector.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 4&lt;/strong&gt;: Apply the rotation and rescaling transformation to obtain the modified output for the decoder (i.e., `Ëœ q`).&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 5&lt;/strong&gt;: Feed `Ëœ q` into the decoder to produce the reconstructed output.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 6&lt;/strong&gt;: Compute the loss using the reconstruction and apply backpropagation.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 7&lt;/strong&gt;: During backpropagation, modify the gradient transfer process to maintain the angle using the proposed model, replacing traditional shortcuts in gradient computation.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Critical implementation details affecting performance: 
    &lt;ul&gt; 
     &lt;li&gt;The choice of rotation matrix calculation should ensure computational efficiencyâ€”using Householder transformations to minimize resource demands.&lt;/li&gt; 
     &lt;li&gt;The deployment of the stop-gradient technique effectively turns off the back-propagation through the quantization layer, which is essential to reflect the intended change without inducing undesired noise in the gradient updates.&lt;/li&gt; 
     &lt;li&gt;Monitor the codebook usage regularly during training to detect any potential collapse early and adjust the training dynamics (e.g., learning rate) accordingly to maintain effective utilization throughout the training period.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Neural discrete representation learning&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;Neural discrete representation learning&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Straightening out the straight-through estimator: Overcoming optimization challenges in vector quantized networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Estimating or propagating gradients through stochastic neurons for conditional computation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;High-resolution image synthesis with latent diffusion models&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Finite scalar quantization: Vq-vae made simple&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Elements of information theory&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Vector-quantized image modeling with improved vqgan&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Uvim: A unified modeling approach for vision with learned guiding codes&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Auto-encoding variational bayes&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Categorical reparameterization with gumbel-softmax&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/rotation_vq/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/rotation_vq/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/rotation_vq/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/rotation_vq/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Example 2 (Category: Vector Quantized)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed model focuses on discrete representation learning for tasks such as image generation, depth estimation, colorization, and segmentation using the proposed approach integrated into architectures like autoregressive transformers.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;Core Techniques: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Simplified Quantization&lt;/strong&gt;: Use a simplified quantization approach utilizing scalar quantization instead of VQ.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Dimensionality Projection&lt;/strong&gt;: Define a function to project the encoder output to a manageable dimensionality (typically between 3 to 10).&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Gradient Propagation&lt;/strong&gt;: Implement the Straight-Through Estimator (STE) for gradient propagation through the quantization operation.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Technical Components: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Bounding Function&lt;/strong&gt;: This compresses data dimensionality and confines values to a desired range. Use a function like \(f(z) = \left\lfloor \frac{L}{2} \right\rfloor \tanh(z)\) to project the data, where \(L\) is the number of quantization levels.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Quantization process&lt;/strong&gt;: Round each bounded dimension to its nearest integer to yield the quantized output.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Loss function&lt;/strong&gt;: Operate under a reconstruction loss paradigm typical in VAEs to optimize the proposed model parameters.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Key Parameters&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Number of dimensions \(d\) and levels \(L\) per dimension should be defined based on the codebook size you aim to replicate (e.g., set \(L_i \geq 5\) for all \(i\)).&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Input/Output Specifications&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;The input to the bounding function will be the output from the final encoder layer; the output after quantization will be in the format \(\hat{z}\), with shape matching the original \(z\).&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Constraints&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Ensure all inputs are preprocessed adequately to be within the functioning range of the bounding function.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Step-by-Step Integration: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: Train a standard VAE model and obtain its encoder output \(z\).&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: Apply the bounding function \(f\) on \(z\) to limit the output dimensions to usable values.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 3&lt;/strong&gt;: Quantize the resultant bounded \(z\) using the rounding procedure to generate \( \hat{z} \).&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 4&lt;/strong&gt;: Use the original \(z\) and \(\hat{z}\) in conjunction with the reconstruction loss to backpropagate through the network using the STE for gradient calculation.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Critical Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;Ensure the rounding process is correctly differentiable; utilize the STE to maintain gradient flow during backpropagation.&lt;/li&gt; 
     &lt;li&gt;Maintain high codebook utilization by selecting optimal dimensions and levels based on empirical trials, and monitor performance to refine the parameters if needed.&lt;/li&gt; 
     &lt;li&gt;Adjust the proposed model configurations (number of epochs, batch size) based on the structures laid out in this paper, ensuring hyperparameters match those recommended for the proposed approach integration.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Neural discrete representation learning&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;Neural discrete representation learning&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Conditional probability models for deep image compression&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;High-fidelity generative image compression&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;End-to-end optimized image compression&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Taming transformers for high-resolution image generation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;An algorithm for vector quantizer design&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Joint autoregressive and hierarchical priors for learned image compression&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Assessing generative models via precision and recall&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Variational bayes on discrete representation with self-annealed stochastic quantization&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;High quality monocular depth estimation via transfer learning&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/fsq/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/fsq/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/fsq/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/fsq/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Example 3 (Category: Recommendation)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed model aims to improve user-item interaction predictions in recommendation systems by leveraging heterogeneous relational information.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;Core Techniques/Algorithms: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Heterogeneous Graph Neural Networks (GNNs)&lt;/strong&gt;: Used for embedding initialization and message propagation across different types of user-item and user-user/item-item graphs.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Contrastive Learning&lt;/strong&gt;: Specifically, a cross-view contrastive learning framework is utilized to enhance representation learning by aligning embeddings from auxiliary views with user-item interaction embeddings.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Meta Networks&lt;/strong&gt;: Employed to extract personalized knowledge and facilitate customized knowledge transfer between auxiliary views and the user-item interaction view.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Purpose and Function of Each Major Component: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Heterogeneous GNN&lt;/strong&gt;: Encodes user and item relationships into embeddings that capture the semantics of various interactions.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Contrastive Learning&lt;/strong&gt;: Provides self-supervision signals to enhance the robustness of learned representations, allowing the proposed model to distinguish between relevant and irrelevant interactions.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Meta Network&lt;/strong&gt;: Models personalized characteristics to facilitate adaptive knowledge transfer, ensuring that the influence of auxiliary information is tailored to individual users and items.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Heterogeneous GNN&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;&lt;strong&gt;Key Parameters&lt;/strong&gt;: Use Xavier initializer for embedding initialization; set the hidden dimensionality &lt;code&gt;d&lt;/code&gt;.&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Input/Output&lt;/strong&gt;: Take adjacency matrices for user-item, user-user, and item-item graphs as input; output relation-aware embeddings.&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Constraints&lt;/strong&gt;: Ensure that the GNN can handle varying types of nodes and relations.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Contrastive Learning&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;&lt;strong&gt;Key Parameters&lt;/strong&gt;: Use cosine similarity as the similarity function; define a temperature coefficient for handling negative samples.&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Input/Output&lt;/strong&gt;: Input embeddings from the meta network and user/item views; output contrastive loss values.&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Constraints&lt;/strong&gt;: Maintain diverse representations to avoid overfitting.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Meta Network&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;&lt;strong&gt;Key Parameters&lt;/strong&gt;: Set up fully connected layers with PReLU activation to generate personalized transformation matrices.&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Input/Output&lt;/strong&gt;: Input user and item embeddings; output transformed embeddings for personalized knowledge transfer.&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Constraints&lt;/strong&gt;: Ensure low-rank decomposition of transformation matrices to reduce parameter count.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Step-by-Step Interaction: 
    &lt;ul&gt; 
     &lt;li&gt;Initialize user and item embeddings using a heterogeneous GNN.&lt;/li&gt; 
     &lt;li&gt;Perform heterogeneous message propagation to refine embeddings iteratively across user-item, user-user, and item-item graphs.&lt;/li&gt; 
     &lt;li&gt;Aggregate the refined embeddings from various views using a mean pooling function to retain heterogeneous semantics.&lt;/li&gt; 
     &lt;li&gt;Extract meta knowledge from the learned embeddings to create personalized mapping functions using the meta network.&lt;/li&gt; 
     &lt;li&gt;Apply contrastive learning to align embeddings from auxiliary views with the user-item interaction embeddings, generating a contrastive loss.&lt;/li&gt; 
     &lt;li&gt;Combine the contrastive loss with a pairwise loss function (like Bayesian Personalized Ranking) to optimize the proposed model.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Critical Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;Choose appropriate hyperparameters such as embedding size, learning rate, and the number of GNN layers through systematic experimentation.&lt;/li&gt; 
     &lt;li&gt;Monitor the proposed model for signs of overfitting, especially when increasing the number of GNN layers or embedding dimensions.&lt;/li&gt; 
     &lt;li&gt;Ensure diverse user-item interaction patterns are captured through sufficient training data and effective augmentation techniques.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Revisiting Graph Based Collaborative Filtering: A Linear Residual Graph Convolutional Network Approach&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;Revisiting Graph Based Collaborative Filtering: A Linear Residual Graph Convolutional Network Approach&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Graph Neural Networks for Social Recommendation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Improving Graph Collaborative Filtering with Neighborhood-enriched Contrastive Learning&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Knowledge-aware Coupled Graph Neural Network for Social Recommendation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Heterogeneous Graph Transformer&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Sequential Recommendation with Graph Neural Networks&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/hgcl/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/hgcl/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/hgcl/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/hgcl/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Example 4 (Category: Recommendation)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed model focuses on collaborative filtering for recommendation systems by leveraging graph neural networks (GNNs) and contrastive learning to address the issue of sparse user-item interactions.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;Core Techniques: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Graph Neural Networks&lt;/strong&gt;: Utilize GNNs for message passing to learn user and item embeddings from the interaction graph.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Disentangled Representations&lt;/strong&gt;: Implement a mechanism to model multiple latent intent factors driving user-item interactions.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Contrastive Learning&lt;/strong&gt;: Use contrastive learning techniques to generate adaptive self-supervised signals from augmented views of user-item interactions.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Purpose of Components: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;GNN Layers&lt;/strong&gt;: Capture high-order interactions among users and items through iterative message passing.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Intent Encoding&lt;/strong&gt;: Differentiate latent intents to improve the representation of user preferences.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Adaptive Augmentation&lt;/strong&gt;: Generate contrastive views that account for both local and global dependencies to enhance robustness against noise.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Graph Construction&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;&lt;strong&gt;Input&lt;/strong&gt;: User-item interaction matrix \( A \) of size \( I \times J \) (where \( I \) is the number of users and \( J \) is the number of items).&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: Normalized adjacency matrix \( \bar{A} \).&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;GNN Configuration&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Number of layers \( L \): Choose based on your dataset, typically 2 or 3 layers.&lt;/li&gt; 
       &lt;li&gt;Dimensionality \( d \) of embeddings: Start with \( d = 32 \).&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Intent Prototypes&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Number of intents \( K \): Experiment with values from {32, 64, 128, 256}, starting with \( K = 128 \).&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Learning Rate&lt;/strong&gt;: Use Adam optimizer with a learning rate around \( 1e-3 \).&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Loss Functions&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Use Bayesian Personalized Ranking (BPR) loss for the recommendation task.&lt;/li&gt; 
       &lt;li&gt;Implement InfoNCE loss for contrastive learning, incorporating both local and global augmented views.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Step-by-Step Interaction: 
    &lt;ul&gt; 
     &lt;li&gt;Construct the interaction graph from the user-item matrix.&lt;/li&gt; 
     &lt;li&gt;For each GNN layer: 
      &lt;ul&gt; 
       &lt;li&gt;Compute the aggregated embeddings \( Z(u) \) and \( Z(v) \) using the normalized adjacency matrix.&lt;/li&gt; 
       &lt;li&gt;Update user and item embeddings using residual connections to prevent over-smoothing.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;Generate intent-aware representations by aggregating embeddings over the latent intents.&lt;/li&gt; 
     &lt;li&gt;Apply the learned parameterized masks for adaptive augmentation during message passing to create multiple contrastive views.&lt;/li&gt; 
     &lt;li&gt;Calculate contrastive learning signals using the generated augmented representations and optimize using the combined loss function.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Critical Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;Ensure that the augmentation matrices are learned adaptively based on the current user-item embeddings to differentiate the importance of interactions.&lt;/li&gt; 
     &lt;li&gt;Monitor the performance with different numbers of latent intents \( K \) to find an optimal balance between expressiveness and noise.&lt;/li&gt; 
     &lt;li&gt;Regularly assess the proposed model for over-smoothing by checking the Mean Average Distance (MAD) metric on the embeddings.&lt;/li&gt; 
     &lt;li&gt;Tune hyperparameters \( \lambda_1, \lambda_2, \lambda_3 \) for the multi-task loss to balance the contribution of the self-supervised learning signals.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Lightgcn: Simplifying and powering graph convolution network for recommendation&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;Lightgcn: Simplifying and powering graph convolution network for recommendation&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Neural collaborative filtering&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Disentangled contrastive learning on graphs&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Improving Graph Collaborative Filtering with Neighborhood-enriched Contrastive Learning&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Curriculum Disentangled Recommendation with Noisy Multi-feedback&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Disentangled heterogeneous graph attention network for recommendation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Learning intents behind interactions with knowledge graph for recommendation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;LightGCL: Simple Yet Effective Graph Contrastive Learning for Recommendation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Self-supervised graph learning for recommendation&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/dccf/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/dccf/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/dccf/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/dccf/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Example 5 (Category: Diffusion and Flow Matching)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed model presented in this paper focuses on the task of generative modeling through the framework of Continuous Normalizing Flows (CNFs) to define straight flows between noise and data samples.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;Architecture: 
    &lt;ul&gt; 
     &lt;li&gt;Implement a neural network to parameterize the velocity field \( v_{\theta}(t, x) \) that maps from noise to data distributions.&lt;/li&gt; 
     &lt;li&gt;Use architectures suitable for continuous functions, such as feedforward or convolutional networks.&lt;/li&gt; 
     &lt;li&gt;Each layer should have non-linear activation functions (e.g., ReLU, Tanh).&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Loss Functions: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Velocity Consistency Loss&lt;/strong&gt;: This should be structured as: \[ L_{\theta} = E_{t \sim U} E_{x_t, x_{t+\Delta t}} \| f_{\theta}(t, x_t) - f_{\theta}(t+\Delta t, x_{t+\Delta t}) \|^2_2 + \alpha \| v_{\theta}(t, x_t) - v_{\theta}(t+\Delta t, x_{t+\Delta t}) \|^2_2 \] where \( f_{\theta}(t, x_t) = x_t + (1 - t) v_{\theta}(t, x_t) \). Choose \( \alpha \) based on cross-validation performance. &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Training Procedure: 
    &lt;ul&gt; 
     &lt;li&gt;Sample \( x_0 \) from the noise distribution \( p_0 \).&lt;/li&gt; 
     &lt;li&gt;For multiple time segments, define intervals and compute velocity fields iteratively.&lt;/li&gt; 
     &lt;li&gt;Use the weights of the proposed approach in an exponential moving average to stabilize training.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Sampling Process: 
    &lt;ul&gt; 
     &lt;li&gt;For single-step or multi-step generation, heuristically sample from the noise distribution and use the learned velocity field as follows: \[ x_{i/k} = x_{(i-1)/k} + \frac{1}{k} v_{i\theta}((i-1)/k, x_{(i-1)/k}) \] &lt;/li&gt; 
     &lt;li&gt;Apply the Euler method for iterative updates: \[ x_{t + \Delta t} = x_t + \Delta t v_i(t, x_t) \] where \( t \in [i/k, (i + 1)/k - \Delta t] \). &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Key Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;Ensure the network is equipped with a suitable optimizer such as Adam with a learning rate around \( 2 \times 10^{-4} \).&lt;/li&gt; 
     &lt;li&gt;The batch size should be appropriately set (e.g., 512 for CIFAR-10).&lt;/li&gt; 
     &lt;li&gt;Employ an ODE solver, suggested as Euler's method, during the training and sampling processes.&lt;/li&gt; 
     &lt;li&gt;Maintain a uniform distribution for sampling time intervals \( U \).&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Performance Considerations: 
    &lt;ul&gt; 
     &lt;li&gt;Monitor convergence rates and empirically validate parameter configurations through experiments. Start with fewer segments and gradually increase to capture complex distributions better.&lt;/li&gt; 
     &lt;li&gt;Adjust the decay rate for the EMA based on the stability of convergence (commonly around 0.999).&lt;/li&gt; 
     &lt;li&gt;Analyze the trade-offs between sampling efficiency and sample quality, ensuring a balance during proposed model development.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Flow matching for generative modeling&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;Flow matching for generative modeling&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Consistency models&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Rectified Flow&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Denoising diffusion probabilistic models&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Optimal flow matching: Learning straight trajectories in just one step&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Maximum likelihood training of score-based diffusion models&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/con_flowmatching/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/con_flowmatching/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/con_flowmatching/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/con_flowmatching/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Example 6 (Category: Graph Neural Networks)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed model focuses on the task of node classification in large graphs, addressing challenges like scalability, heterophily, long-range dependencies, and the absence of edges.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;The core techniques used in this study include a kernelized Gumbel-Softmax operator for all-pair message passing, which reduces computational complexity to linear (O(N)), and a Transformer-style network architecture designed for layer-wise learning of latent graph structures.&lt;/li&gt; 
   &lt;li&gt;The purpose of the kernelized Gumbel-Softmax operator is to enable differentiable learning of discrete graph structures by approximating categorical distributions. The Transformer-style architecture facilitates information propagation between arbitrary pairs of nodes through learned latent graphs.&lt;/li&gt; 
   &lt;li&gt;Implementation details for each component: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Kernelized Gumbel-Softmax Operator&lt;/strong&gt;: Set the temperature parameter (Ï„) to a range typically between 0.25 and 0.4 for training. It operates on node feature representations (D-dimensional feature vectors). The output of this operator is a distribution over node connections, facilitating the selection of neighbors for message passing.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Node Feature Input&lt;/strong&gt;: Each node input should be represented as a feature vector (e.g., {x_u} âˆˆ R^D), and the output is an updated representation of the node embedding after message passing.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Relational Bias (if applicable)&lt;/strong&gt;: Introduces activation (e.g., sigmoid) to adjust the message passing weights based on an observed adjacency matrix, which enhances weight assignment for connected nodes.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Edge Regularization Loss&lt;/strong&gt;: Combines categorical edge probabilities with a supervised classification loss, encouraging the network to maintain predicted edges consistent with observed edges.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;The step-by-step interaction of these components includes: 
    &lt;ul&gt; 
     &lt;li&gt;Begin with an input matrix of node embeddings (X) and, if available, an adjacency matrix (A).&lt;/li&gt; 
     &lt;li&gt;Apply the kernelized Gumbel-Softmax operator to the embedding matrix to generate a probability distribution over neighbor selection for each node.&lt;/li&gt; 
     &lt;li&gt;Use these probabilities to sample neighbors, allowing for message passing where each node aggregates information from its selected neighbors.&lt;/li&gt; 
     &lt;li&gt;Update the node embeddings using an attention mechanism, which can be enhanced by relational bias if edges are available.&lt;/li&gt; 
     &lt;li&gt;After K iterations of neighbor sampling, apply loss functions comprising a supervised classification loss and, if applicable, edge-level regularization loss to optimize the embedding representations.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Critical implementation details affecting performance involve: 
    &lt;ul&gt; 
     &lt;li&gt;Careful tuning of the temperature parameter (Ï„) in the Gumbel-Softmax operator, as it significantly influences the proposed approach's capacity to capture the discrete nature of graph structures.&lt;/li&gt; 
     &lt;li&gt;Utilizing appropriate batch sizes for large-scale graphs, ensuring enough memory is available while also maintaining computational efficiency.&lt;/li&gt; 
     &lt;li&gt;Choosing the correct dimensionality for random features in the kernel approximation, balancing model expressiveness and training stability.&lt;/li&gt; 
     &lt;li&gt;The use of dropout or other regularization techniques such as edge-level regularization can influence the proposed model's generalization capabilities on unseen data.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;On the bottleneck of graph neural networks and its practical implications&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;On the bottleneck of graph neural networks and its practical implications&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Semi-supervised classification with graph convolutional networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Categorical reparameterization with gumbel-softmax&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Learning discrete structures for graph neural networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Graph attention networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Geometric deep learning: going beyond euclidean data&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Graph structure learning for robust graph neural networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Geom-gcn: Geometric graph convolutional networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;New benchmarks for learning on non-homophilous graphs&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Latent patient network learning for automatic diagnosis&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Few-shot learning with graph neural networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;The graph neural network model&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Characteristic functions on graphs: Birds of a feather, from statistical descriptors to parametric models&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Beyond homophily in graph neural networks: Current limitations and effective designs&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_nodeformer/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_nodeformer/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_nodeformer/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_nodeformer/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Example 7 (Category: Graph Neural Networks)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed approach works on the task of uncovering data dependencies and learning instance representations from datasets that may not have complete or reliable relationships, particularly in semi-supervised contexts like node classification, image/text classification, and spatial-temporal dynamics prediction.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;The core techniques/algorithms used in this paper include an energy-constrained diffusion model represented as a partial differential equation (PDE), an explicit Euler scheme for numerical solutions, and a form of adaptive diffusivity function based on the energy function. The proposed architecture utilizes a diffusion-based Transformer framework that allows for all-pair feature propagation among instances.&lt;/li&gt; 
   &lt;li&gt;The major technical components serve the following purposes: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Diffusion Process:&lt;/strong&gt; Encodes instances into evolving states by modeling information flow, where instance representations evolve according to a PDE illuminating the relationships among the instances.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Energy Function:&lt;/strong&gt; Provides constraints to regularize the diffusion process and guide the proposed model towards desired low-energy embeddings, enhancing the quality of representations.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Diffusivity Function:&lt;/strong&gt; Specifies the strength of information flow between instances, adapting based on the instance states, and allows for flexible and efficient propagation strategies.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Implementation details for each component: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Diffusion Process Input:&lt;/strong&gt; Requires a batch of instances represented as a matrix of size \(N \times D\), where \(N\) is the number of instances and \(D\) is the input feature dimension.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Diffusion Process Output:&lt;/strong&gt; Produces the updated instance representations after \(K\) propagation steps. The step size \(\tau\) should be set within the range (0, 1).&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Energy Function:&lt;/strong&gt; Implemented as \(E(Z, k; \delta) = ||Z - Z^{(k)}||^2_F + \lambda \sum_{i,j} \delta(||z_i - z_j||^2_2)\), with \(\delta\) being a non-decreasing, concave function.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Key Parameters:&lt;/strong&gt; 
      &lt;ul&gt; 
       &lt;li&gt;Step size \(\tau\)&lt;/li&gt; 
       &lt;li&gt;Layer number \(K\) (number of diffusion propagation steps)&lt;/li&gt; 
       &lt;li&gt;Regularization weight \(\lambda\).&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Step-by-step description of interactions: 
    &lt;ul&gt; 
     &lt;li&gt;Start by initializing the instance representations.&lt;/li&gt; 
     &lt;li&gt;For each layer of diffusion, compute the diffusivity \(S(k)\) based on current embeddings through a function \(f\) which can be defined differently depending on the proposed model implementation.&lt;/li&gt; 
     &lt;li&gt;Update the instance representations using the defined diffusion equations, ensuring to conserve states and introduce propagation according to the computed diffusivity.&lt;/li&gt; 
     &lt;li&gt;After \(K\) layers of diffusion, apply a final output layer to produce logits for predictions.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Critical implementation details that affect performance: 
    &lt;ul&gt; 
     &lt;li&gt;The choice of diffusivity function \(f\) greatly impacts the proposed model's capacity to learn complex dependencies, where specific formulations (like linear or logistic) yield different abilities in capturing inter-instance relationships.&lt;/li&gt; 
     &lt;li&gt;Ensure that the values of \(\tau\) and \(\lambda\) are set appropriately to balance convergence speed and representation quality; using a smaller \(\tau\) may require deeper layers to learn effectively.&lt;/li&gt; 
     &lt;li&gt;Optimization parameters like learning rate and early stopping criteria are essential, particularly for large-scale datasets where convergence behavior can vary widely depending on architecture size and complexity.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Diffusion-convolutional neural networks&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;Diffusion-convolutional neural networks&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Semi-supervised classification with graph convolutional networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Manifold regularization: A geometric framework for learning from labeled and unlabeled examples&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Geometric deep learning: going beyond euclidean data&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Artificial neural networks for solving ordinary and partial differential equations&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Scaling graph neural networks with approximate pagerank&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Learning discrete structures for graph neural networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Semi-supervised learning using gaussian fields and harmonic functions&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Graph convolutional networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Deep learning via semi-supervised embedding&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;A generalization of transformer networks to graphs&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Graph Convolution and Quadratic Time Complexity&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Bayesian graph convolutional neural networks for semi-supervised classification&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Do transformers really perform bad for graph representation?&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Big bird: Transformers for longer sequences&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Adaptive graph diffusion networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Transformers are RNNs&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Collective classification in network data&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;NodeFormer: A scalable graph structure learning transformer for node classification&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_difformer/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_difformer/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_difformer/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_difformer/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;span id="how-it-works"&gt;&lt;/span&gt; 
&lt;h2&gt;âœ¨How AI-Researcher works&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ”„ &lt;strong&gt;End-to-End Scientific Research Automation System&lt;/strong&gt; &lt;br /&gt;Our &lt;strong&gt;AI-Researcher&lt;/strong&gt; provides comprehensive automation for the complete scientific research lifecycle through an integrated pipeline. The system orchestrates research activities across three strategic phases: 
  &lt;ol&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Literature Review &amp;amp; Idea Generation&lt;/strong&gt; ğŸ“šğŸ’¡&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt; &lt;p&gt;ğŸ” &lt;strong&gt;Resource Collector&lt;/strong&gt;: Systematically gathers comprehensive research materials across multiple scientific domains through automated collection from major academic databases (e.g., arXiv, IEEE Xplore, ACM Digital Library, and Google Scholar), code platforms (e.g., GitHub, Hugging Face), and open datasets across scientific domains.&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;ğŸ§  &lt;strong&gt;Resource Filter&lt;/strong&gt;: Evaluates and selects high-impact papers, well-maintained code implementations, and benchmark datasets through quality metrics (e.g., citation count, code maintenance, data completeness) and relevance assessment.&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;ğŸ’­ &lt;strong&gt;Idea Generator&lt;/strong&gt;: Leveraging the identified research resources, including high-impact papers and code repositories, the Idea Generator systematically formulates novel research directions through comprehensive analysis. It automatedly evaluates current methodological limitations, map emerging technological trends, and explore uncharted research territories.&lt;/p&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;New Algorithm Design, Implementation &amp;amp; Validation&lt;/strong&gt; ğŸ§ªğŸ’» &lt;br /&gt;&lt;strong&gt;Design â†’ Implementation â†’ Validation â†’ Refinement&lt;/strong&gt;&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt; &lt;p&gt;ğŸ“&lt;strong&gt;Design Phase&lt;/strong&gt;: The initial phase focuses on conceptual development, where novel algorithmic ideas are formulated and theoretical foundations are established. During this stage, we carefully plan the implementation strategy, ensuring the proposed solution advances beyond existing approaches while maintaining practical feasibility.&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;âš™ï¸&lt;strong&gt;Implementation Phase&lt;/strong&gt;: proceed to transform abstract concepts into concrete code implementations. This phase involves developing functional modules, establishing a robust testing environment, and creating necessary infrastructure for experimental validation.&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;ğŸ”¬&lt;strong&gt;Validation Phase&lt;/strong&gt;: Systematic experimentation forms the core of our validation process. We execute comprehensive tests to evaluate algorithm performance, collect metrics, and document all findings. This phase ensures rigorous implementation verification with practical requirements.&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;ğŸ”§&lt;strong&gt;Refinement Phase&lt;/strong&gt; ğŸ”¬: Based on validation results, we enter an iterative refinement cycle. This phase involves identifying areas for improvement, optimizing code efficiency, and implementing necessary enhancements. We carefully analyze performance bottlenecks and plan strategic improvements for the next development iteration.&lt;/p&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Paper Writing&lt;/strong&gt; âœï¸ğŸ“&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Writer Agent&lt;/strong&gt; ğŸ“„: Automatically generates full-length academic papers by integrating research ideas, motivations, newly designed algorithm frameworks, and algorithm validation performance. Leveraging a hierarchical writing approach, it creates polished manuscripts with precision and clarity.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ğŸš€ This fully automated system removes the need for manual intervention across the entire research lifecycle, enabling effortless and seamless scientific discoveryâ€”from initial concept to final publication. ğŸš€ It serves as an excellent research assistant, aiding researchers in achieving their goals efficiently and effectively.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ”¬ &lt;strong&gt;Comprehensive Benchmark Suite&lt;/strong&gt; &lt;br /&gt;We have developed a comprehensive and standardized evaluation framework to objectively assess the academic capabilities of AI researchers and the quality of their scholarly work, integrating several key innovations to ensure thorough and reliable evaluation.&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt; &lt;p&gt;ğŸ‘¨â€ğŸ”¬ &lt;strong&gt;Expert-Level Ground Truth&lt;/strong&gt;: TThe benchmark leverages human expert-written papers as ground truth references, establishing a high-quality standard for comparison and validation.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;ğŸŒˆ &lt;strong&gt;Multi-Domain Coverage&lt;/strong&gt;: Our benchmark is designed to comprehensively span 4 major research domains, ensuring broad applicability: Computer Vision (CV), Nature Language Processing (NLP), Data Mining (DM), and Information Retrieval (IR).&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;ğŸŒ &lt;strong&gt;Fully Open-Source Benchmark Construction&lt;/strong&gt;: We have fully open-sourced the methodology and process for building the benchmark, including complete access to processed datasets, data collection pipelines, and processing code. This ensures &lt;strong&gt;Transparency in Evaluation&lt;/strong&gt; while empowering the community to customize and construct benchmarks tailored to their specific domains for testing AI researchers.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;ğŸ“Š &lt;strong&gt;Comprehensive Evaluation Metrics&lt;/strong&gt;: Our evaluation framework adopts a hierarchical and systematic approach, where tasks are organized into two levels based on the extent of idea provision. Leveraging specialized &lt;strong&gt;Evaluator Agents&lt;/strong&gt;, the framework conducts thorough assessments across multiple dimensions, ensuring a robust and comprehensive evaluation. Key evaluation metrics include: 1) &lt;strong&gt;Novelty&lt;/strong&gt;: Assessing the innovation and uniqueness of the research work. 2) &lt;strong&gt;Experimental Comprehensiveness&lt;/strong&gt;: Evaluating the design, execution, and rigor of the experiments. 3) &lt;strong&gt;Theoretical Foundation&lt;/strong&gt;: Measuring the strength of the theoretical background and foundations. 4) &lt;strong&gt;Result Analysis&lt;/strong&gt;: Analyzing the depth and accuracy of result interpretation. 5) &lt;strong&gt;Writing Quality&lt;/strong&gt;: Reviewing the clarity, coherence, and structure of the written report.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ğŸš€ &lt;strong&gt;Advancing Research Automation&lt;/strong&gt;. This benchmark suite provides an objective framework for assessing research automation capabilities. It is designed to evolve continuously, incorporating new advancements and expanding its scope to meet the growing demands of the research community.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;ğŸŒŸ &lt;strong&gt;Easy-to-Use AI Research Assistant&lt;/strong&gt; &lt;br /&gt;&lt;strong&gt;AI-Researcher&lt;/strong&gt;E delivers a truly seamless and accessible experience for research automation, empowering users to focus on innovation without technical barriers. Key features include:&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt; &lt;p&gt;ğŸŒ &lt;strong&gt;Multi-LLM Provider Support&lt;/strong&gt;: Effortlessly integrates with leading language model providers such as Claude, OpenAI, Deepseek, and more. Researchers can select the most suitable AI capabilities for their specific needs.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;ğŸ“š &lt;strong&gt;Effortless Research Kickoff&lt;/strong&gt;: Kickstart your research journey with unparalleled ease! Simply provide a list of relevant papers, and &lt;strong&gt;AI-Researcher&lt;/strong&gt; takes care of the restâ€”no need to upload files, contribute initial ideas, or navigate complex configurations. It's the ultimate tool to help you jumpstart your research process efficiently and effectively.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;ğŸ§  &lt;strong&gt;Minimal Domain Expertise Needed&lt;/strong&gt;: AI-Researcher simplifies the research process by autonomously identifying critical research gaps, proposing innovative approaches, and executing the entire research pipeline. While some domain understanding can enhance results, the tool is designed to empower users of all expertise levels to achieve impactful outcomes with ease.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;ğŸ“¦ &lt;strong&gt;Out-of-the-Box Functionality&lt;/strong&gt;: Experience seamless research automation right from the start. AI-Researcher is ready to use with minimal setup, giving you instant access to advanced capabilities. Skip the hassle of complex configurations and dive straight into accelerating your research process with ease and efficiency.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="how-to-use"&gt;&lt;/span&gt; 
&lt;h2&gt;ğŸ” How to use AI-Researcher&lt;/h2&gt; 
&lt;h3&gt;1. Research Agent&lt;/h3&gt; 
&lt;p&gt;If you want to use research agent with the given idea (Level 1 tasks), conducting extensive survey and experiments, you can use the following command in the &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/research_agent/run_infer_level_1.sh"&gt;&lt;code&gt;research_agent/run_infer_level_1.sh&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;current_dir=$(dirname "$(readlink -f "$0")")
cd $current_dir
export DOCKER_WORKPLACE_NAME=workplace_paper

export BASE_IMAGES=tjbtech1/paperagent:latest

export COMPLETION_MODEL=claude-3-5-sonnet-20241022
export CHEEP_MODEL=claude-3-5-haiku-20241022

category=vq
instance_id=one_layer_vq
export GPUS='"device=0,1"'

python run_infer_plan.py --instance_path ../benchmark/final/${category}/${instance_id}.json --container_name paper_eval --task_level task1 --model $COMPLETION_MODEL --workplace_name workplace --cache_path cache --port 12372 --max_iter_times 0 --category ${category}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to just give the reference papers, and let the research agent to generate the idea then conduct the experiments (Level 2 tasks), you can use the following command in the &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/research_agent/run_infer_level_2.sh"&gt;&lt;code&gt;research_agent/run_infer_level_2.sh&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;current_dir=$(dirname "$(readlink -f "$0")")
cd $current_dir
export DOCKER_WORKPLACE_NAME=workplace_paper

export BASE_IMAGES=tjbtech1/paperagent:latest

export COMPLETION_MODEL=claude-3-5-sonnet-20241022
export CHEEP_MODEL=claude-3-5-haiku-20241022

category=vq
instance_id=one_layer_vq
export GPUS='"device=0,1"'

python run_infer_idea.py --instance_path ../benchmark/final/${category}/${instance_id}.json --container_name paper_eval --model $COMPLETION_MODEL --workplace_name workplace --cache_path cache --port 12372 --max_iter_times 0 --category ${category}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Paper Writing Agent&lt;/h3&gt; 
&lt;p&gt;If you want to generate the paper after the research agent has conducted the research, you can use the following command in the &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/paper_agent/run_paper.sh"&gt;&lt;code&gt;paper_agent/run_infer.sh&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;#!/bin/bash

cd path/to/AI-Researcher/paper_agent

export OPENAI_API_KEY=sk-SKlupNntta4WPmvDCRo7uuPbYGwOnUQcb25Twn8c718tPpXN


research_field=vq
instance_id=rotated_vq

python path/to/AI-Researcher/paper_agent/writing.py --research_field ${research_field} --instance_id ${instance_id}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Benchmark Data and Collection&lt;/h3&gt; 
&lt;p&gt;Our benchmark is also fully-open-sourced:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Detailed benchmark data is available in the &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/benchmark"&gt;&lt;code&gt;benchmark&lt;/code&gt;&lt;/a&gt; folder.&lt;/li&gt; 
 &lt;li&gt;Detailed benchmark collection process is available in the &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/benchmark_collection"&gt;&lt;code&gt;benchmark_collection&lt;/code&gt;&lt;/a&gt; folder.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="documentation"&gt;&lt;/span&gt; 
&lt;h2&gt;ğŸ“– Documentation&lt;/h2&gt; 
&lt;p&gt;Comprehensive documentation is on its way ğŸš€! Stay tuned for updates on our &lt;a href="https://auto-researcher.github.io/docs"&gt;Documentation&lt;/a&gt; page.&lt;/p&gt; 
&lt;span id="community"&gt;&lt;/span&gt; 
&lt;h2&gt;ğŸ¤ Join the Community&lt;/h2&gt; 
&lt;p&gt;We aim to build a vibrant community around AI-Researcher and warmly invite everyone to join us. Here's how you can become part of our community:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://join.slack.com/t/ai-researchergroup/shared_invite/zt-30y5a070k-C0ajQt1zmVczFnfGkIicvA"&gt;Join our Slack workspace&lt;/a&gt; - Here we talk about research, architecture, and future development.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/ghSnKGkq"&gt;Join our Discord server&lt;/a&gt; - This is a community-run server for general discussion, questions, and feedback.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/HKUDS/AI-Researcher/issues"&gt;Read or post Github Issues&lt;/a&gt; - Check out the issues we're working on, or add your own ideas.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Misc&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/HKUDS/AI-Researcher/stargazers"&gt;&lt;img src="https://reporoster.com/stars/HKUDS/AI-Researcher" alt="Stargazers repo roster for @HKUDS/AI-Researcher" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/HKUDS/AI-Researcher/network/members"&gt;&lt;img src="https://reporoster.com/forks/HKUDS/AI-Researcher" alt="Forkers repo roster for @HKUDS/AI-Researcher" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://star-history.com/#HKUDS/AI-Researcher&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=HKUDS/AI-Researcher&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;span id="cite"&gt;&lt;/span&gt; 
&lt;h2&gt;ğŸŒŸ Cite&lt;/h2&gt; 
&lt;p&gt;A more detailed technical report will be released soon. ğŸš€:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-tex"&gt;@misc{airesearcher,
      title={{AI-Researcher: Autonomous Scientific Innovation}},
      author={Jiabin Tang, Lianghao Xia, Zhonghang Li, Chao Huang},
      year={2025},
      eprint={2505.18705},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2505.18705},
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Asabeneh/30-Days-Of-Python</title>
      <link>https://github.com/Asabeneh/30-Days-Of-Python</link>
      <description>&lt;p&gt;30 days of Python programming challenge is a step-by-step guide to learn the Python programming language in 30 days. This challenge may take more than100 days, follow your own pace. These videos may help too: https://www.youtube.com/channel/UC7PNRuno1rzYPb1xLa4yktw&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ğŸ 30 Days Of Python&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Learn with Asabeneh by joining the upcoming &lt;a href="https://docs.google.com/forms/d/e/1FAIpQLSf0oNIYR9XU1DCctfl-pY36KbWse-SQX5aQaUgetqSinFYnmQ/viewform"&gt;&lt;em&gt;CODING BOOTCAMP&lt;/em&gt;&lt;/a&gt; &lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;# Day&lt;/th&gt; 
   &lt;th align="center"&gt;Topics&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;01&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/readme.md"&gt;Introduction&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;02&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/02_Day_Variables_builtin_functions/02_variables_builtin_functions.md"&gt;Variables, Built-in Functions&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;03&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/03_Day_Operators/03_operators.md"&gt;Operators&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;04&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/04_Day_Strings/04_strings.md"&gt;Strings&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;05&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/05_Day_Lists/05_lists.md"&gt;Lists&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;06&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/06_Day_Tuples/06_tuples.md"&gt;Tuples&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;07&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/07_Day_Sets/07_sets.md"&gt;Sets&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;08&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/08_Day_Dictionaries/08_dictionaries.md"&gt;Dictionaries&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;09&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/09_Day_Conditionals/09_conditionals.md"&gt;Conditionals&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/10_Day_Loops/10_loops.md"&gt;Loops&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;11&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/11_Day_Functions/11_functions.md"&gt;Functions&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/12_Day_Modules/12_modules.md"&gt;Modules&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;13&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/13_Day_List_comprehension/13_list_comprehension.md"&gt;List Comprehension&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/14_Day_Higher_order_functions/14_higher_order_functions.md"&gt;Higher Order Functions&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/15_Day_Python_type_errors/15_python_type_errors.md"&gt;Python Type Errors&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/16_Day_Python_date_time/16_python_datetime.md"&gt;Python Date time&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/17_Day_Exception_handling/17_exception_handling.md"&gt;Exception Handling&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/18_Day_Regular_expressions/18_regular_expressions.md"&gt;Regular Expressions&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;19&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/19_Day_File_handling/19_file_handling.md"&gt;File Handling&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;20&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/20_Day_Python_package_manager/20_python_package_manager.md"&gt;Python Package Manager&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;21&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/21_Day_Classes_and_objects/21_classes_and_objects.md"&gt;Classes and Objects&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;22&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/22_Day_Web_scraping/22_web_scraping.md"&gt;Web Scraping&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;23&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/23_Day_Virtual_environment/23_virtual_environment.md"&gt;Virtual Environment&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;24&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/24_Day_Statistics/24_statistics.md"&gt;Statistics&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/25_Day_Pandas/25_pandas.md"&gt;Pandas&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/26_Day_Python_web/26_python_web.md"&gt;Python web&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;27&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/27_Day_Python_with_mongodb/27_python_with_mongodb.md"&gt;Python with MongoDB&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;28&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/28_Day_API/28_API.md"&gt;API&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;29&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/29_Day_Building_API/29_building_API.md"&gt;Building API&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;30&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/30_Day_Conclusions/30_conclusions.md"&gt;Conclusions&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Learn with Asabeneh by joining the upcoming &lt;a href="https://docs.google.com/forms/d/e/1FAIpQLSf0oNIYR9XU1DCctfl-pY36KbWse-SQX5aQaUgetqSinFYnmQ/viewform"&gt;&lt;em&gt;CODING BOOTCAMP&lt;/em&gt;&lt;/a&gt; &lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;ğŸ§¡ğŸ§¡ğŸ§¡ HAPPY CODING ğŸ§¡ğŸ§¡ğŸ§¡&lt;/p&gt; 
&lt;div&gt; 
 &lt;small&gt;Support the &lt;strong&gt;author&lt;/strong&gt; to create more educational materials&lt;/small&gt; 
 &lt;br /&gt; 
 &lt;a href="https://www.paypal.me/asabeneh"&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/paypal_lg.png" alt="Paypal Logo" style="width:10%" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;h1&gt; 30 Days Of Python: Day 1 - Introduction&lt;/h1&gt; 
 &lt;a class="header-badge" target="_blank" href="https://www.linkedin.com/in/asabeneh/"&gt; &lt;img src="https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&amp;amp;logo=linkedin&amp;amp;style=social" /&gt; &lt;/a&gt; 
 &lt;a class="header-badge" target="_blank" href="https://twitter.com/Asabeneh"&gt; &lt;img alt="Twitter Follow" src="https://img.shields.io/twitter/follow/asabeneh?style=social" /&gt; &lt;/a&gt; 
 &lt;p&gt;&lt;sub&gt;Author: &lt;a href="https://www.linkedin.com/in/asabeneh/" target="_blank"&gt;Asabeneh Yetayeh&lt;/a&gt;&lt;br /&gt; &lt;small&gt; Second Edition: July, 2021&lt;/small&gt; &lt;/sub&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;ğŸ‡§ğŸ‡· &lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/Portuguese/README.md"&gt;Portuguese&lt;/a&gt; ğŸ‡¨ğŸ‡³ &lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/Chinese/README.md"&gt;ä¸­æ–‡&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/02_Day_Variables_builtin_functions/02_variables_builtin_functions.md"&gt;Day 2 &amp;gt;&amp;gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/30DaysOfPython_banner3@2x.png" alt="30DaysOfPython" /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#-30-days-of-python"&gt;ğŸ 30 Days Of Python&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#-day-1"&gt;ğŸ“˜ Day 1&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#welcome"&gt;Welcome&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#why-python-"&gt;Why Python ?&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#environment-setup"&gt;Environment Setup&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#installing-python"&gt;Installing Python&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#python-shell"&gt;Python Shell&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#installing-visual-studio-code"&gt;Installing Visual Studio Code&lt;/a&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#how-to-use-visual-studio-code"&gt;How to use visual studio code&lt;/a&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#basic-python"&gt;Basic Python&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#python-syntax"&gt;Python Syntax&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#python-indentation"&gt;Python Indentation&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#comments"&gt;Comments&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#data-types"&gt;Data types&lt;/a&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#number"&gt;Number&lt;/a&gt;&lt;/li&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#string"&gt;String&lt;/a&gt;&lt;/li&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#booleans"&gt;Booleans&lt;/a&gt;&lt;/li&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#list"&gt;List&lt;/a&gt;&lt;/li&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#dictionary"&gt;Dictionary&lt;/a&gt;&lt;/li&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#tuple"&gt;Tuple&lt;/a&gt;&lt;/li&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#set"&gt;Set&lt;/a&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#checking-data-types"&gt;Checking Data types&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#python-file"&gt;Python File&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#-exercises---day-1"&gt;ğŸ’» Exercises - Day 1&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#exercise-level-1"&gt;Exercise: Level 1&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#exercise-level-2"&gt;Exercise: Level 2&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#exercise-level-3"&gt;Exercise: Level 3&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;ğŸ“˜ Day 1&lt;/h1&gt; 
&lt;h2&gt;Welcome&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Congratulations&lt;/strong&gt; for deciding to participate in a &lt;em&gt;30 days of Python&lt;/em&gt; programming challenge. In this challenge, you will learn everything you need to be a python programmer and the whole concept of programming. At the end of the challenge, you will get a &lt;em&gt;30DaysOfPython&lt;/em&gt; programming challenge certificate.&lt;/p&gt; 
&lt;p&gt;If you would like to actively engage in the challenge, you may join the &lt;a href="https://t.me/ThirtyDaysOfPython"&gt;30DaysOfPython challenge&lt;/a&gt; telegram group.&lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Python is a high-level programming language for general-purpose programming. It is an open source, interpreted, objected-oriented programming language. Python was created by a Dutch programmer, Guido van Rossum. The name of the Python programming language was derived from a British sketch comedy series, &lt;em&gt;Monty Python's Flying Circus&lt;/em&gt;. The first version was released on February 20, 1991. This 30 days of Python challenge will help you learn the latest version of Python, Python 3 step by step. The topics are broken down into 30 days, where each day contains several topics with easy-to-understand explanations, real-world examples, and many hands on exercises and projects.&lt;/p&gt; 
&lt;p&gt;This challenge is designed for beginners and professionals who want to learn python programming language. It may take 30 to 100 days to complete the challenge. People who actively participate in the telegram group have a high probability of completing the challenge.&lt;/p&gt; 
&lt;p&gt;This challenge is easy to read, written in conversational English, engaging, motivating and at the same time, it is very demanding. You need to allocate much time to finish this challenge. If you are a visual learner, you may get the video lesson on &lt;a href="https://www.youtube.com/channel/UC7PNRuno1rzYPb1xLa4yktw"&gt; Washera&lt;/a&gt; YouTube channel. You may start from &lt;a href="https://youtu.be/OCCWZheOesI"&gt;Python for Absolute Beginners video&lt;/a&gt;. Subscribe the channel, comment and ask questions on YouTube vidoes and be proactive, the author will eventually notice you.&lt;/p&gt; 
&lt;p&gt;The author likes to hear your opinion about the challenge, share the author by expressing your thoughts about the 30DaysOfPython challenge. You can leave your testimonial on this &lt;a href="https://www.asabeneh.com/testimonials"&gt;link&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Why Python ?&lt;/h2&gt; 
&lt;p&gt;It is a programming language which is very close to human language and because of that, it is easy to learn and use. Python is used by various industries and companies (including Google). It has been used to develop web applications, desktop applications, system administration, and machine learning libraries. Python is a highly embraced language in the data science and machine learning community. I hope this is enough to convince you to start learning Python. Python is eating the world and you are killing it before it eats you.&lt;/p&gt; 
&lt;h2&gt;Environment Setup&lt;/h2&gt; 
&lt;h3&gt;Installing Python&lt;/h3&gt; 
&lt;p&gt;To run a python script you need to install python. Let's &lt;a href="https://www.python.org/"&gt;download&lt;/a&gt; python. If your are a windows user. Click the button encircled in red.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.python.org/"&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/installing_on_windows.png" alt="installing on Windows" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;If you are a macOS user. Click the button encircled in red.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.python.org/"&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/installing_on_macOS.png" alt="installing on Windows" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;To check if python is installed write the following command on your device terminal.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python --version
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/python_versio.png" alt="Python Version" /&gt;&lt;/p&gt; 
&lt;p&gt;As you can see from the terminal, I am using &lt;em&gt;Python 3.7.5&lt;/em&gt; version at the moment. Your version of Python might be different from mine by but it should be 3.6 or above. If you mange to see the python version, well done. Python has been installed on your machine. Continue to the next section.&lt;/p&gt; 
&lt;h3&gt;Python Shell&lt;/h3&gt; 
&lt;p&gt;Python is an interpreted scripting language, so it does not need to be compiled. It means it executes the code line by line. Python comes with a &lt;em&gt;Python Shell (Python Interactive Shell)&lt;/em&gt;. It is used to execute a single python command and get the result.&lt;/p&gt; 
&lt;p&gt;Python Shell waits for the Python code from the user. When you enter the code, it interprets the code and shows the result in the next line. Open your terminal or command prompt(cmd) and write:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/opening_python_shell.png" alt="Python Scripting Shell" /&gt;&lt;/p&gt; 
&lt;p&gt;The Python interactive shell is opened and it is waiting for you to write Python code(Python script). You will write your Python script next to this symbol &amp;gt;&amp;gt;&amp;gt; and then click Enter. Let us write our very first script on the Python scripting shell.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/adding_on_python_shell.png" alt="Python script on Python shell" /&gt;&lt;/p&gt; 
&lt;p&gt;Well done, you wrote your first Python script on Python interactive shell. How do we close the Python interactive shell ? To close the shell, next to this symbol &amp;gt;&amp;gt; write &lt;strong&gt;exit()&lt;/strong&gt; command and press Enter.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/exit_from_shell.png" alt="Exit from python shell" /&gt;&lt;/p&gt; 
&lt;p&gt;Now, you know how to open the Python interactive shell and how to exit from it.&lt;/p&gt; 
&lt;p&gt;Python will give you results if you write scripts that Python understands, if not it returns errors. Let's make a deliberate mistake and see what Python will return.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/invalid_syntax_error.png" alt="Invalid Syntax Error" /&gt;&lt;/p&gt; 
&lt;p&gt;As you can see from the returned error, Python is so clever that it knows the mistake we made and which was &lt;em&gt;Syntax Error: invalid syntax&lt;/em&gt;. Using x as multiplication in Python is a syntax error because (x) is not a valid syntax in Python. Instead of (&lt;strong&gt;x&lt;/strong&gt;) we use asterisk (*) for multiplication. The returned error clearly shows what to fix.&lt;/p&gt; 
&lt;p&gt;The process of identifying and removing errors from a program is called &lt;em&gt;debugging&lt;/em&gt;. Let us debug it by putting * in place of &lt;strong&gt;x&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/fixing_syntax_error.png" alt="Fixing Syntax Error" /&gt;&lt;/p&gt; 
&lt;p&gt;Our bug was fixed, the code ran and we got a result we were expecting. As a programmer you will see such kind of errors on daily basis. It is good to know how to debug. To be good at debugging you should understand what kind of errors you are facing. Some of the Python errors you may encounter are &lt;em&gt;SyntaxError&lt;/em&gt;, &lt;em&gt;IndexError&lt;/em&gt;, &lt;em&gt;NameError&lt;/em&gt;, &lt;em&gt;ModuleNotFoundError&lt;/em&gt;, &lt;em&gt;KeyError&lt;/em&gt;, &lt;em&gt;ImportError&lt;/em&gt;, &lt;em&gt;AttributeError&lt;/em&gt;, &lt;em&gt;TypeError&lt;/em&gt;, &lt;em&gt;ValueError&lt;/em&gt;, &lt;em&gt;ZeroDivisionError&lt;/em&gt; etc. We will see more about different Python &lt;strong&gt;&lt;em&gt;error types&lt;/em&gt;&lt;/strong&gt; in later sections.&lt;/p&gt; 
&lt;p&gt;Let us practice more how to use Python interactive shell. Go to your terminal or command prompt and write the word &lt;strong&gt;python&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/opening_python_shell.png" alt="Python Scripting Shell" /&gt;&lt;/p&gt; 
&lt;p&gt;The Python interactive shell is opened. Let us do some basic mathematical operations (addition, subtraction, multiplication, division, modulus, exponential).&lt;/p&gt; 
&lt;p&gt;Let us do some maths first before we write any Python code:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;2 + 3 is 5&lt;/li&gt; 
 &lt;li&gt;3 - 2 is 1&lt;/li&gt; 
 &lt;li&gt;3 * 2 is 6&lt;/li&gt; 
 &lt;li&gt;3 / 2 is 1.5&lt;/li&gt; 
 &lt;li&gt;3 ** 2 is the same as 3 * 3&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In python we have the following additional operations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;3 % 2 = 1 =&amp;gt; which means finding the remainder&lt;/li&gt; 
 &lt;li&gt;3 // 2 = 1 =&amp;gt; which means removing the remainder&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Let us change the above mathematical expressions to Python code. The Python shell has been opened and let us write a comment at the very beginning of the shell.&lt;/p&gt; 
&lt;p&gt;A &lt;em&gt;comment&lt;/em&gt; is a part of the code which is not executed by python. So we can leave some text in our code to make our code more readable. Python does not run the comment part. A comment in python starts with hash(#) symbol. This is how you write a comment in python&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt; # comment starts with hash
 # this is a python comment, because it starts with a (#) symbol
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/maths_on_python_shell.png" alt="Maths on python shell" /&gt;&lt;/p&gt; 
&lt;p&gt;Before we move on to the next section, let us practice more on the Python interactive shell. Close the opened shell by writing &lt;em&gt;exit()&lt;/em&gt; on the shell and open it again and let us practice how to write text on the Python shell.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/writing_string_on_shell.png" alt="Writing String on python shell" /&gt;&lt;/p&gt; 
&lt;h3&gt;Installing Visual Studio Code&lt;/h3&gt; 
&lt;p&gt;The Python interactive shell is good to try and test small script codes but it will not be for a big project. In real work environment, developers use different code editors to write codes. In this 30 days of Python programming challenge we will use visual studio code. Visual studio code is a very popular open source text editor. I am a fan of vscode and I would recommend to &lt;a href="https://code.visualstudio.com/"&gt;download&lt;/a&gt; visual studio code, but if you are in favor of other editors, feel free to follow with what you have.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://code.visualstudio.com/"&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/vscode.png" alt="Visual Studio Code" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;If you installed visual studio code, let us see how to use it. If you prefer a video, you can follow this Visual Studio Code for Python &lt;a href="https://www.youtube.com/watch?v=bn7Cx4z-vSo"&gt;Video tutorial&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;How to use visual studio code&lt;/h4&gt; 
&lt;p&gt;Open the visual studio code by double clicking the visual studio icon. When you open it you will get this kind of interface. Try to interact with the labeled icons.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/vscode_ui.png" alt="Visual studio Code" /&gt;&lt;/p&gt; 
&lt;p&gt;Create a folder named 30DaysOfPython on your desktop. Then open it using visual studio code.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/how_to_open_project_on_vscode.png" alt="Opening Project on Visual studio" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/opening_project.png" alt="Opening a project" /&gt;&lt;/p&gt; 
&lt;p&gt;After opening it you will see shortcuts for creating files and folders inside of 30DaysOfPython project's directory. As you can see below, I have created the very first file, helloworld.py. You can do the same.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/helloworld.png" alt="Creating a python file" /&gt;&lt;/p&gt; 
&lt;p&gt;After a long day of coding, you want to close your code editor, right? This is how you will close the opened project.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/closing_opened_project.png" alt="Closing project" /&gt;&lt;/p&gt; 
&lt;p&gt;Congratulations, you have finished setting up the development environment. Let us start coding.&lt;/p&gt; 
&lt;h2&gt;Basic Python&lt;/h2&gt; 
&lt;h3&gt;Python Syntax&lt;/h3&gt; 
&lt;p&gt;A Python script can be written in Python interactive shell or in the code editor. A Python file has an extension .py.&lt;/p&gt; 
&lt;h3&gt;Python Indentation&lt;/h3&gt; 
&lt;p&gt;An indentation is a white space in a text. Indentation in many languages is used to increase code readability; however, Python uses indentation to create blocks of code. In other programming languages, curly brackets are used to create code blocks instead of indentation. One of the common bugs when writing Python code is incorrect indentation.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/indentation.png" alt="Indentation Error" /&gt;&lt;/p&gt; 
&lt;h3&gt;Comments&lt;/h3&gt; 
&lt;p&gt;Comments play a crucial role in enhancing code readability and allowing developers to leave notes within their code. In Python, any text preceded by a hash (#) symbol is considered a comment and is not executed when the code runs.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Example: Single Line Comment&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;    # This is the first comment
    # This is the second comment
    # Python is eating the world
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Example: Multiline Comment&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Triple quote can be used for multiline comment if it is not assigned to a variable&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;"""This is multiline comment
multiline comment takes multiple lines.
python is eating the world
"""
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Data types&lt;/h3&gt; 
&lt;p&gt;In Python there are several types of data types. Let us get started with the most common ones. Different data types will be covered in detail in other sections. For the time being, let us just go through the different data types and get familiar with them. You do not have to have a clear understanding now.&lt;/p&gt; 
&lt;h4&gt;Number&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Integer: Integer(negative, zero and positive) numbers Example: ... -3, -2, -1, 0, 1, 2, 3 ...&lt;/li&gt; 
 &lt;li&gt;Float: Decimal number Example ... -3.5, -2.25, -1.0, 0.0, 1.1, 2.2, 3.5 ...&lt;/li&gt; 
 &lt;li&gt;Complex Example 1 + j, 2 + 4j&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;String&lt;/h4&gt; 
&lt;p&gt;A collection of one or more characters under a single or double quote. If a string is more than one sentence then we use a triple quote.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;'Asabeneh'
'Finland'
'Python'
'I love teaching'
'I hope you are enjoying the first day of 30DaysOfPython Challenge'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Booleans&lt;/h4&gt; 
&lt;p&gt;A boolean data type is either a True or False value. T and F should be always uppercase.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;    True  #  Is the light on? If it is on, then the value is True
    False # Is the light on? If it is off, then the value is False
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;List&lt;/h4&gt; 
&lt;p&gt;Python list is an ordered collection which allows to store different data type items. A list is similar to an array in JavaScript.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;[0, 1, 2, 3, 4, 5]  # all are the same data types - a list of numbers
['Banana', 'Orange', 'Mango', 'Avocado'] # all the same data types - a list of strings (fruits)
['Finland','Estonia', 'Sweden','Norway'] # all the same data types - a list of strings (countries)
['Banana', 10, False, 9.81] # different data types in the list - string, integer, boolean and float
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Dictionary&lt;/h4&gt; 
&lt;p&gt;A Python dictionary object is an unordered collection of data in a key value pair format.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;{
'first_name':'Asabeneh',
'last_name':'Yetayeh',
'country':'Finland', 
'age':250, 
'is_married':True,
'skills':['JS', 'React', 'Node', 'Python']
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Tuple&lt;/h4&gt; 
&lt;p&gt;A tuple is an ordered collection of different data types like list but tuples can not be modified once they are created. They are immutable.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;('Asabeneh', 'Pawel', 'Brook', 'Abraham', 'Lidiya') # Names
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;('Earth', 'Jupiter', 'Neptune', 'Mars', 'Venus', 'Saturn', 'Uranus', 'Mercury') # planets
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Set&lt;/h4&gt; 
&lt;p&gt;A set is a collection of data types similar to list and tuple. Unlike list and tuple, set is not an ordered collection of items. Like in Mathematics, set in Python stores only unique items.&lt;/p&gt; 
&lt;p&gt;In later sections, we will go in detail about each and every Python data type.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;{2, 4, 3, 5}
{3.14, 9.81, 2.7} # order is not important in set
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Checking Data types&lt;/h3&gt; 
&lt;p&gt;To check the data type of certain data/variable we use the &lt;strong&gt;type&lt;/strong&gt; function. In the following terminal you will see different python data types:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/checking_data_types.png" alt="Checking Data types" /&gt;&lt;/p&gt; 
&lt;h3&gt;Python File&lt;/h3&gt; 
&lt;p&gt;First open your project folder, 30DaysOfPython. If you don't have this folder, create a folder name called 30DaysOfPython. Inside this folder, create a file called helloworld.py. Now, let's do what we did on python interactive shell using visual studio code.&lt;/p&gt; 
&lt;p&gt;The Python interactive shell was printing without using &lt;strong&gt;print&lt;/strong&gt; but on visual studio code to see our result we should use a built in function _print(). The &lt;em&gt;print()&lt;/em&gt; built-in function takes one or more arguments as follows &lt;em&gt;print('arument1', 'argument2', 'argument3')&lt;/em&gt;. See the examples below.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The file name is helloworld.py&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;# Day 1 - 30DaysOfPython Challenge

print(2 + 3)             # addition(+)
print(3 - 1)             # subtraction(-)
print(2 * 3)             # multiplication(*)
print(3 / 2)             # division(/)
print(3 ** 2)            # exponential(**)
print(3 % 2)             # modulus(%)
print(3 // 2)            # Floor division operator(//)

# Checking data types
print(type(10))          # Int
print(type(3.14))        # Float
print(type(1 + 3j))      # Complex number
print(type('Asabeneh'))  # String
print(type([1, 2, 3]))   # List
print(type({'name':'Asabeneh'})) # Dictionary
print(type({9.8, 3.14, 2.7}))    # Set
print(type((9.8, 3.14, 2.7)))    # Tuple
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run the python file check the image below. You can run the python file either by running the green button on Visual Studio Code or by typing &lt;em&gt;python helloworld.py&lt;/em&gt; in the terminal .&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/running_python_script.png" alt="Running python script" /&gt;&lt;/p&gt; 
&lt;p&gt;ğŸŒ• You are amazing. You have just completed day 1 challenge and you are on your way to greatness. Now do some exercises for your brain and muscles.&lt;/p&gt; 
&lt;h2&gt;ğŸ’» Exercises - Day 1&lt;/h2&gt; 
&lt;h3&gt;Exercise: Level 1&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Check the python version you are using&lt;/li&gt; 
 &lt;li&gt;Open the python interactive shell and do the following operations. The operands are 3 and 4. 
  &lt;ul&gt; 
   &lt;li&gt;addition(+)&lt;/li&gt; 
   &lt;li&gt;subtraction(-)&lt;/li&gt; 
   &lt;li&gt;multiplication(*)&lt;/li&gt; 
   &lt;li&gt;modulus(%)&lt;/li&gt; 
   &lt;li&gt;division(/)&lt;/li&gt; 
   &lt;li&gt;exponential(**)&lt;/li&gt; 
   &lt;li&gt;floor division operator(//)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Write strings on the python interactive shell. The strings are the following: 
  &lt;ul&gt; 
   &lt;li&gt;Your name&lt;/li&gt; 
   &lt;li&gt;Your family name&lt;/li&gt; 
   &lt;li&gt;Your country&lt;/li&gt; 
   &lt;li&gt;I am enjoying 30 days of python&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Check the data types of the following data: 
  &lt;ul&gt; 
   &lt;li&gt;10&lt;/li&gt; 
   &lt;li&gt;9.8&lt;/li&gt; 
   &lt;li&gt;3.14&lt;/li&gt; 
   &lt;li&gt;4 - 4j&lt;/li&gt; 
   &lt;li&gt;['Asabeneh', 'Python', 'Finland']&lt;/li&gt; 
   &lt;li&gt;Your name&lt;/li&gt; 
   &lt;li&gt;Your family name&lt;/li&gt; 
   &lt;li&gt;Your country&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Exercise: Level 2&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Create a folder named day_1 inside 30DaysOfPython folder. Inside day_1 folder, create a python file helloworld.py and repeat questions 1, 2, 3 and 4. Remember to use &lt;em&gt;print()&lt;/em&gt; when you are working on a python file. Navigate to the directory where you have saved your file, and run it.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Exercise: Level 3&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Write an example for different Python data types such as Number(Integer, Float, Complex), String, Boolean, List, Tuple, Set and Dictionary.&lt;/li&gt; 
 &lt;li&gt;Find an &lt;a href="https://en.wikipedia.org/wiki/Euclidean_distance#:~:text=In%20mathematics%2C%20the%20Euclidean%20distance,being%20called%20the%20Pythagorean%20distance."&gt;Euclidian distance&lt;/a&gt; between (2, 3) and (10, 8)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;ğŸ‰ CONGRATULATIONS ! ğŸ‰&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/02_Day_Variables_builtin_functions/02_variables_builtin_functions.md"&gt;Day 2 &amp;gt;&amp;gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Skyvern-AI/skyvern</title>
      <link>https://github.com/Skyvern-AI/skyvern</link>
      <description>&lt;p&gt;Automate browser-based workflows with LLMs and Computer Vision&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; &lt;a href="https://www.skyvern.com"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="fern/images/skyvern_logo.png" /&gt; 
   &lt;img height="120" src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/skyvern_logo_blackbg.png" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;br /&gt; &lt;/h1&gt; 
&lt;p align="center"&gt; ğŸ‰ Automate Browser-based workflows using LLMs and Computer Vision ğŸ‰ &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.skyvern.com/"&gt;&lt;img src="https://img.shields.io/badge/Website-blue?logo=googlechrome&amp;amp;logoColor=black" /&gt;&lt;/a&gt; &lt;a href="https://docs.skyvern.com/"&gt;&lt;img src="https://img.shields.io/badge/Docs-yellow?logo=gitbook&amp;amp;logoColor=black" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;&lt;img src="https://img.shields.io/discord/1212486326352617534?logo=discord&amp;amp;label=discord" /&gt;&lt;/a&gt; 
 &lt;!-- &lt;a href="https://pepy.tech/project/skyvern" target="_blank"&gt;&lt;img src="https://static.pepy.tech/badge/skyvern" alt="Total Downloads"/&gt;&lt;/a&gt; --&gt; &lt;a href="https://github.com/skyvern-ai/skyvern"&gt;&lt;img src="https://img.shields.io/github/stars/skyvern-ai/skyvern" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Skyvern-AI/skyvern/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/skyvern-ai/skyvern" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/skyvernai"&gt;&lt;img src="https://img.shields.io/twitter/follow/skyvernai?style=social" /&gt;&lt;/a&gt; &lt;a href="https://www.linkedin.com/company/95726232"&gt;&lt;img src="https://img.shields.io/badge/Follow%20 on%20LinkedIn-8A2BE2?logo=linkedin" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.skyvern.com"&gt;Skyvern&lt;/a&gt; automates browser-based workflows using LLMs and computer vision. It provides a simple API endpoint to fully automate manual workflows on a large number of websites, replacing brittle or unreliable automation solutions.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/geico_shu_recording_cropped.gif" /&gt; &lt;/p&gt; 
&lt;p&gt;Traditional approaches to browser automations required writing custom scripts for websites, often relying on DOM parsing and XPath-based interactions which would break whenever the website layouts changed.&lt;/p&gt; 
&lt;p&gt;Instead of only relying on code-defined XPath interactions, Skyvern relies on Vision LLMs to learn and interact with the websites.&lt;/p&gt; 
&lt;h1&gt;How it works&lt;/h1&gt; 
&lt;p&gt;Skyvern was inspired by the Task-Driven autonomous agent design popularized by &lt;a href="https://github.com/yoheinakajima/babyagi"&gt;BabyAGI&lt;/a&gt; and &lt;a href="https://github.com/Significant-Gravitas/AutoGPT"&gt;AutoGPT&lt;/a&gt; -- with one major bonus: we give Skyvern the ability to interact with websites using browser automation libraries like &lt;a href="https://playwright.dev/"&gt;Playwright&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Skyvern uses a swarm of agents to comprehend a website, and plan and execute its actions:&lt;/p&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="fern/images/skyvern_2_0_system_diagram.png" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/skyvern_2_0_system_diagram.png" /&gt; 
&lt;/picture&gt; 
&lt;p&gt;This approach has a few advantages:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Skyvern can operate on websites it's never seen before, as it's able to map visual elements to actions necessary to complete a workflow, without any customized code&lt;/li&gt; 
 &lt;li&gt;Skyvern is resistant to website layout changes, as there are no pre-determined XPaths or other selectors our system is looking for while trying to navigate&lt;/li&gt; 
 &lt;li&gt;Skyvern is able to take a single workflow and apply it to a large number of websites, as it's able to reason through the interactions necessary to complete the workflow&lt;/li&gt; 
 &lt;li&gt;Skyvern leverages LLMs to reason through interactions to ensure we can cover complex situations. Examples include: 
  &lt;ol&gt; 
   &lt;li&gt;If you wanted to get an auto insurance quote from Geico, the answer to a common question "Were you eligible to drive at 18?" could be inferred from the driver receiving their license at age 16&lt;/li&gt; 
   &lt;li&gt;If you were doing competitor analysis, it's understanding that an Arnold Palmer 22 oz can at 7/11 is almost definitely the same product as a 23 oz can at Gopuff (even though the sizes are slightly different, which could be a rounding error!)&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;A detailed technical report can be found &lt;a href="https://blog.skyvern.com/skyvern-2-0-state-of-the-art-web-navigation-with-85-8-on-webvoyager-eval/"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Demo&lt;/h1&gt; 
&lt;!-- Redo demo --&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/5cab4668-e8e2-4982-8551-aab05ff73a7f"&gt;https://github.com/user-attachments/assets/5cab4668-e8e2-4982-8551-aab05ff73a7f&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Performance &amp;amp; Evaluation&lt;/h1&gt; 
&lt;p&gt;Skyvern has SOTA performance on the &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/webbench.ai"&gt;WebBench benchmark&lt;/a&gt; with a 64.4% accuracy. The technical report + evaluation can be found &lt;a href="https://blog.skyvern.com/web-bench-a-new-way-to-compare-ai-browser-agents/"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/performance/webbench_overall.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Performance on WRITE tasks (eg filling out forms, logging in, downloading files, etc)&lt;/h2&gt; 
&lt;p&gt;Skyvern is the best performing agent on WRITE tasks (eg filling out forms, logging in, downloading files, etc), which is primarily used for RPA (Robotic Process Automation) adjacent tasks.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/performance/webbench_write.png" /&gt; &lt;/p&gt; 
&lt;h1&gt;Quickstart&lt;/h1&gt; 
&lt;h2&gt;Skyvern Cloud&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com"&gt;Skyvern Cloud&lt;/a&gt; is a managed cloud version of Skyvern that allows you to run Skyvern without worrying about the infrastructure. It allows you to run multiple Skyvern instances in parallel and comes bundled with anti-bot detection mechanisms, proxy network, and CAPTCHA solvers.&lt;/p&gt; 
&lt;p&gt;If you'd like to try it out, navigate to &lt;a href="https://app.skyvern.com"&gt;app.skyvern.com&lt;/a&gt; and create an account.&lt;/p&gt; 
&lt;h2&gt;Install &amp;amp; Run&lt;/h2&gt; 
&lt;p&gt;Dependencies needed:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.python.org/downloads/"&gt;Python 3.11.x&lt;/a&gt;, works with 3.12, not ready yet for 3.13&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nodejs.org/en/download/"&gt;NodeJS &amp;amp; NPM&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Additionally, for Windows:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://rustup.rs/"&gt;Rust&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;VS Code with C++ dev tools and Windows SDK&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;1. Install Skyvern&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install skyvern
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Run Skyvern&lt;/h3&gt; 
&lt;p&gt;This is most helpful for first time run (db setup, db migrations etc).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;skyvern quickstart
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Run task&lt;/h3&gt; 
&lt;h4&gt;UI (Recommended)&lt;/h4&gt; 
&lt;p&gt;Start the Skyvern service and UI (when DB is up and running)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;skyvern run all
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Go to &lt;a href="http://localhost:8080"&gt;http://localhost:8080&lt;/a&gt; and use the UI to run a task&lt;/p&gt; 
&lt;h4&gt;Code&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

skyvern = Skyvern()
task = await skyvern.run_task(prompt="Find the top post on hackernews today")
print(task)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Skyvern starts running the task in a browser that pops up and closes it when the task is done. You will be able to view the task from &lt;a href="http://localhost:8080/history"&gt;http://localhost:8080/history&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You can also run a task on different targets:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

# Run on Skyvern Cloud
skyvern = Skyvern(api_key="SKYVERN API KEY")

# Local Skyvern service
skyvern = Skyvern(base_url="http://localhost:8000", api_key="LOCAL SKYVERN API KEY")

task = await skyvern.run_task(prompt="Find the top post on hackernews today")
print(task)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Advanced Usage&lt;/h2&gt; 
&lt;h3&gt;Control your own browser (Chrome)&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;âš ï¸ WARNING: Since &lt;a href="https://developer.chrome.com/blog/remote-debugging-port"&gt;Chrome 136&lt;/a&gt;, Chrome refuses any CDP connect to the browser using the default user_data_dir. In order to use your browser data, Skyvern copies your default user_data_dir to &lt;code&gt;./tmp/user_data_dir&lt;/code&gt; the first time connecting to your local browser. âš ï¸&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt;Just With Python Code&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

# The path to your Chrome browser. This example path is for Mac.
browser_path = "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
skyvern = Skyvern(
    base_url="http://localhost:8000",
    api_key="YOUR_API_KEY",
    browser_path=browser_path,
)
task = await skyvern.run_task(
    prompt="Find the top post on hackernews today",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;With Skyvern Service&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Add two variables to your .env file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# The path to your Chrome browser. This example path is for Mac.
CHROME_EXECUTABLE_PATH="/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
BROWSER_TYPE=cdp-connect
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Restart Skyvern service &lt;code&gt;skyvern run all&lt;/code&gt; and run the task through UI or code&lt;/p&gt; 
&lt;h3&gt;Run Skyvern with any remote browser&lt;/h3&gt; 
&lt;p&gt;Grab the cdp connection url and pass it to Skyvern&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

skyvern = Skyvern(cdp_url="your cdp connection url")
task = await skyvern.run_task(
    prompt="Find the top post on hackernews today",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Get consistent output schema from your run&lt;/h3&gt; 
&lt;p&gt;You can do this by adding the &lt;code&gt;data_extraction_schema&lt;/code&gt; parameter:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

skyvern = Skyvern()
task = await skyvern.run_task(
    prompt="Find the top post on hackernews today",
    data_extraction_schema={
        "type": "object",
        "properties": {
            "title": {
                "type": "string",
                "description": "The title of the top post"
            },
            "url": {
                "type": "string",
                "description": "The URL of the top post"
            },
            "points": {
                "type": "integer",
                "description": "Number of points the post has received"
            }
        }
    }
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Helpful commands to debug issues&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Launch the Skyvern Server Separately*
skyvern run server

# Launch the Skyvern UI
skyvern run ui

# Check status of the Skyvern service
skyvern status

# Stop the Skyvern service
skyvern stop all

# Stop the Skyvern UI
skyvern stop ui

# Stop the Skyvern Server Separately
skyvern stop server
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Docker Compose setup&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Make sure you have &lt;a href="https://www.docker.com/products/docker-desktop/"&gt;Docker Desktop&lt;/a&gt; installed and running on your machine&lt;/li&gt; 
 &lt;li&gt;Make sure you don't have postgres running locally (Run &lt;code&gt;docker ps&lt;/code&gt; to check)&lt;/li&gt; 
 &lt;li&gt;Clone the repository and navigate to the root directory&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;skyvern init llm&lt;/code&gt; to generate a &lt;code&gt;.env&lt;/code&gt; file. This will be copied into the Docker image.&lt;/li&gt; 
 &lt;li&gt;Fill in the LLM provider key on the &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt;. &lt;em&gt;If you want to run Skyvern on a remote server, make sure you set the correct server ip for the UI container in &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt;.&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Run the following command via the commandline: &lt;pre&gt;&lt;code class="language-bash"&gt; docker compose up -d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Navigate to &lt;code&gt;http://localhost:8080&lt;/code&gt; in your browser to start using the UI&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; Only one Postgres container can run on port 5432 at a time. If you switch from the CLI-managed Postgres to Docker Compose, you must first remove the original container:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;docker rm -f postgresql-container
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;If you encounter any database related errors while using Docker to run Skyvern, check which Postgres container is running with &lt;code&gt;docker ps&lt;/code&gt;.&lt;/p&gt; 
&lt;h1&gt;Skyvern Features&lt;/h1&gt; 
&lt;h2&gt;Skyvern Tasks&lt;/h2&gt; 
&lt;p&gt;Tasks are the fundamental building block inside Skyvern. Each task is a single request to Skyvern, instructing it to navigate through a website and accomplish a specific goal.&lt;/p&gt; 
&lt;p&gt;Tasks require you to specify a &lt;code&gt;url&lt;/code&gt;, &lt;code&gt;prompt&lt;/code&gt;, and can optionally include a &lt;code&gt;data schema&lt;/code&gt; (if you want the output to conform to a specific schema) and &lt;code&gt;error codes&lt;/code&gt; (if you want Skyvern to stop running in specific situations).&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/skyvern_2_0_screenshot.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Skyvern Workflows&lt;/h2&gt; 
&lt;p&gt;Workflows are a way to chain multiple tasks together to form a cohesive unit of work.&lt;/p&gt; 
&lt;p&gt;For example, if you wanted to download all invoices newer than January 1st, you could create a workflow that first navigated to the invoices page, then filtered down to only show invoices newer than January 1st, extracted a list of all eligible invoices, and iterated through each invoice to download it.&lt;/p&gt; 
&lt;p&gt;Another example is if you wanted to automate purchasing products from an e-commerce store, you could create a workflow that first navigated to the desired product, then added it to a cart. Second, it would navigate to the cart and validate the cart state. Finally, it would go through the checkout process to purchase the items.&lt;/p&gt; 
&lt;p&gt;Supported workflow features include:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Browser Task&lt;/li&gt; 
 &lt;li&gt;Browser Action&lt;/li&gt; 
 &lt;li&gt;Data Extraction&lt;/li&gt; 
 &lt;li&gt;Validation&lt;/li&gt; 
 &lt;li&gt;For Loops&lt;/li&gt; 
 &lt;li&gt;File parsing&lt;/li&gt; 
 &lt;li&gt;Sending emails&lt;/li&gt; 
 &lt;li&gt;Text Prompts&lt;/li&gt; 
 &lt;li&gt;HTTP Request Block&lt;/li&gt; 
 &lt;li&gt;Custom Code Block&lt;/li&gt; 
 &lt;li&gt;Uploading files to block storage&lt;/li&gt; 
 &lt;li&gt;(Coming soon) Conditionals&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/block_example_v2.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Livestreaming&lt;/h2&gt; 
&lt;p&gt;Skyvern allows you to livestream the viewport of the browser to your local machine so that you can see exactly what Skyvern is doing on the web. This is useful for debugging and understanding how Skyvern is interacting with a website, and intervening when necessary&lt;/p&gt; 
&lt;h2&gt;Form Filling&lt;/h2&gt; 
&lt;p&gt;Skyvern is natively capable of filling out form inputs on websites. Passing in information via the &lt;code&gt;navigation_goal&lt;/code&gt; will allow Skyvern to comprehend the information and fill out the form accordingly.&lt;/p&gt; 
&lt;h2&gt;Data Extraction&lt;/h2&gt; 
&lt;p&gt;Skyvern is also capable of extracting data from a website.&lt;/p&gt; 
&lt;p&gt;You can also specify a &lt;code&gt;data_extraction_schema&lt;/code&gt; directly within the main prompt to tell Skyvern exactly what data you'd like to extract from the website, in jsonc format. Skyvern's output will be structured in accordance to the supplied schema.&lt;/p&gt; 
&lt;h2&gt;File Downloading&lt;/h2&gt; 
&lt;p&gt;Skyvern is also capable of downloading files from a website. All downloaded files are automatically uploaded to block storage (if configured), and you can access them via the UI.&lt;/p&gt; 
&lt;h2&gt;Authentication&lt;/h2&gt; 
&lt;p&gt;Skyvern supports a number of different authentication methods to make it easier to automate tasks behind a login. If you'd like to try it out, please reach out to us &lt;a href="mailto:founders@skyvern.com"&gt;via email&lt;/a&gt; or &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;discord&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/secure_password_task_example.png" /&gt; &lt;/p&gt; 
&lt;h3&gt;ğŸ” 2FA Support (TOTP)&lt;/h3&gt; 
&lt;p&gt;Skyvern supports a number of different 2FA methods to allow you to automate workflows that require 2FA.&lt;/p&gt; 
&lt;p&gt;Examples include:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;QR-based 2FA (e.g. Google Authenticator, Authy)&lt;/li&gt; 
 &lt;li&gt;Email based 2FA&lt;/li&gt; 
 &lt;li&gt;SMS based 2FA&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;ğŸ” Learn more about 2FA support &lt;a href="https://docs.skyvern.com/credentials/totp"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Password Manager Integrations&lt;/h3&gt; 
&lt;p&gt;Skyvern currently supports the following password manager integrations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Bitwarden&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; 1Password&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; LastPass&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Model Context Protocol (MCP)&lt;/h2&gt; 
&lt;p&gt;Skyvern supports the Model Context Protocol (MCP) to allow you to use any LLM that supports MCP.&lt;/p&gt; 
&lt;p&gt;See the MCP documentation &lt;a href="https://github.com/Skyvern-AI/skyvern/raw/main/integrations/mcp/README.md"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Zapier / Make.com / N8N Integration&lt;/h2&gt; 
&lt;p&gt;Skyvern supports Zapier, Make.com, and N8N to allow you to connect your Skyvern workflows to other apps.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.skyvern.com/integrations/zapier"&gt;Zapier&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.skyvern.com/integrations/make.com"&gt;Make.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.skyvern.com/integrations/n8n"&gt;N8N&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ğŸ” Learn more about 2FA support &lt;a href="https://docs.skyvern.com/credentials/totp"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Real-world examples of Skyvern&lt;/h1&gt; 
&lt;p&gt;We love to see how Skyvern is being used in the wild. Here are some examples of how Skyvern is being used to automate workflows in the real world. Please open PRs to add your own examples!&lt;/p&gt; 
&lt;h2&gt;Invoice Downloading on many different websites&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://meetings.hubspot.com/skyvern/demo"&gt;Book a demo to see it live&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/invoice_downloading.gif" /&gt; &lt;/p&gt; 
&lt;h2&gt;Automate the job application process&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/job_application"&gt;ğŸ’¡ See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/job_application_demo.gif" /&gt; &lt;/p&gt; 
&lt;h2&gt;Automate materials procurement for a manufacturing company&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/finditparts"&gt;ğŸ’¡ See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/finditparts_recording_crop.gif" /&gt; &lt;/p&gt; 
&lt;h2&gt;Navigating to government websites to register accounts or fill out forms&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/california_edd"&gt;ğŸ’¡ See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/edd_services.gif" /&gt; &lt;/p&gt; 
&lt;!-- Add example of delaware entity lookups x2 --&gt; 
&lt;h2&gt;Filling out random contact us forms&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/contact_us_forms"&gt;ğŸ’¡ See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/contact_forms.gif" /&gt; &lt;/p&gt; 
&lt;h2&gt;Retrieving insurance quotes from insurance providers in any language&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/bci_seguros"&gt;ğŸ’¡ See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/bci_seguros_recording.gif" /&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/geico"&gt;ğŸ’¡ See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/geico_shu_recording_cropped.gif" /&gt; &lt;/p&gt; 
&lt;h1&gt;Documentation&lt;/h1&gt; 
&lt;p&gt;More extensive documentation can be found on our &lt;a href="https://docs.skyvern.com"&gt;ğŸ“• docs page&lt;/a&gt;. Please let us know if something is unclear or missing by opening an issue or reaching out to us &lt;a href="mailto:founders@skyvern.com"&gt;via email&lt;/a&gt; or &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;discord&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Supported LLMs&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Provider&lt;/th&gt; 
   &lt;th&gt;Supported Models&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
   &lt;td&gt;gpt4-turbo, gpt-4o, gpt-4o-mini&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Anthropic&lt;/td&gt; 
   &lt;td&gt;Claude 3 (Haiku, Sonnet, Opus), Claude 3.5 (Sonnet)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Azure OpenAI&lt;/td&gt; 
   &lt;td&gt;Any GPT models. Better performance with a multimodal llm (azure/gpt4-o)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AWS Bedrock&lt;/td&gt; 
   &lt;td&gt;Anthropic Claude 3 (Haiku, Sonnet, Opus), Claude 3.5 (Sonnet)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Gemini&lt;/td&gt; 
   &lt;td&gt;Gemini 2.5 Pro and flash, Gemini 2.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ollama&lt;/td&gt; 
   &lt;td&gt;Run any locally hosted model via &lt;a href="https://github.com/ollama/ollama"&gt;Ollama&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenRouter&lt;/td&gt; 
   &lt;td&gt;Access models through &lt;a href="https://openrouter.ai"&gt;OpenRouter&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI-compatible&lt;/td&gt; 
   &lt;td&gt;Any custom API endpoint that follows OpenAI's API format (via &lt;a href="https://docs.litellm.ai/docs/providers/openai_compatible"&gt;liteLLM&lt;/a&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Environment Variables&lt;/h4&gt; 
&lt;h5&gt;OpenAI&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_OPENAI&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register OpenAI models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI API Key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_API_BASE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI API Base, optional&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://openai.api.base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_ORGANIZATION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI Organization ID, optional&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;your-org-id&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;OPENAI_GPT4O&lt;/code&gt;, &lt;code&gt;OPENAI_GPT4O_MINI&lt;/code&gt;, &lt;code&gt;OPENAI_GPT4_1&lt;/code&gt;, &lt;code&gt;OPENAI_O4_MINI&lt;/code&gt;, &lt;code&gt;OPENAI_O3&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;Anthropic&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_ANTHROPIC&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register Anthropic models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Anthropic API key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended&lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;ANTHROPIC_CLAUDE3.5_SONNET&lt;/code&gt;, &lt;code&gt;ANTHROPIC_CLAUDE3.7_SONNET&lt;/code&gt;, &lt;code&gt;ANTHROPIC_CLAUDE4_OPUS&lt;/code&gt;, &lt;code&gt;ANTHROPIC_CLAUDE4_SONNET&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;Azure OpenAI&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_AZURE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register Azure OpenAI models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure deployment API key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_DEPLOYMENT&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure OpenAI Deployment Name&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;skyvern-deployment&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_API_BASE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure deployment api base url&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://skyvern-deployment.openai.azure.com/&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_API_VERSION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure API Version&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;2024-02-01&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;AZURE_OPENAI&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;AWS Bedrock&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_BEDROCK&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register AWS Bedrock models. To use AWS Bedrock, you need to make sure your &lt;a href="https://github.com/boto/boto3?tab=readme-ov-file#using-boto3"&gt;AWS configurations&lt;/a&gt; are set up correctly first.&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;BEDROCK_ANTHROPIC_CLAUDE3.7_SONNET_INFERENCE_PROFILE&lt;/code&gt;, &lt;code&gt;BEDROCK_ANTHROPIC_CLAUDE4_OPUS_INFERENCE_PROFILE&lt;/code&gt;, &lt;code&gt;BEDROCK_ANTHROPIC_CLAUDE4_SONNET_INFERENCE_PROFILE&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;Gemini&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_GEMINI&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register Gemini models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;GEMINI_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Gemini API Key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;your_google_gemini_api_key&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;GEMINI_2.5_PRO_PREVIEW&lt;/code&gt;, &lt;code&gt;GEMINI_2.5_FLASH_PREVIEW&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;Ollama&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_OLLAMA&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register local models via Ollama&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OLLAMA_SERVER_URL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;URL for your Ollama server&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;http://host.docker.internal:11434&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OLLAMA_MODEL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Ollama model name to load&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;qwen2.5:7b-instruct&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;OLLAMA&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Note: Ollama does not support vision yet.&lt;/p&gt; 
&lt;h5&gt;OpenRouter&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_OPENROUTER&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register OpenRouter models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenRouter API key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENROUTER_MODEL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenRouter model name&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;mistralai/mistral-small-3.1-24b-instruct&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENROUTER_API_BASE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenRouter API base URL&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://api.openrouter.ai/v1&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;OPENROUTER&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;OpenAI-Compatible&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_OPENAI_COMPATIBLE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register a custom OpenAI-compatible API endpoint&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_MODEL_NAME&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Model name for OpenAI-compatible endpoint&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;yi-34b&lt;/code&gt;, &lt;code&gt;gpt-3.5-turbo&lt;/code&gt;, &lt;code&gt;mistral-large&lt;/code&gt;, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;API key for OpenAI-compatible endpoint&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_API_BASE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Base URL for OpenAI-compatible endpoint&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://api.together.xyz/v1&lt;/code&gt;, &lt;code&gt;http://localhost:8000/v1&lt;/code&gt;, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_API_VERSION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;API version for OpenAI-compatible endpoint, optional&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;2023-05-15&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_MAX_TOKENS&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Maximum tokens for completion, optional&lt;/td&gt; 
   &lt;td&gt;Integer&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;4096&lt;/code&gt;, &lt;code&gt;8192&lt;/code&gt;, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_TEMPERATURE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Temperature setting, optional&lt;/td&gt; 
   &lt;td&gt;Float&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;0.0&lt;/code&gt;, &lt;code&gt;0.5&lt;/code&gt;, &lt;code&gt;0.7&lt;/code&gt;, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_SUPPORTS_VISION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Whether model supports vision, optional&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Supported LLM Key: &lt;code&gt;OPENAI_COMPATIBLE&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;General LLM Configuration&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;LLM_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The name of the model you want to use&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;See supported LLM keys above&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;SECONDARY_LLM_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The name of the model for mini agents skyvern runs with&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;See supported LLM keys above&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;LLM_CONFIG_MAX_TOKENS&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Override the max tokens used by the LLM&lt;/td&gt; 
   &lt;td&gt;Integer&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;128000&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Feature Roadmap&lt;/h1&gt; 
&lt;p&gt;This is our planned roadmap for the next few months. If you have any suggestions or would like to see a feature added, please don't hesitate to reach out to us &lt;a href="mailto:founders@skyvern.com"&gt;via email&lt;/a&gt; or &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;discord&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Open Source&lt;/strong&gt; - Open Source Skyvern's core codebase&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Workflow support&lt;/strong&gt; - Allow support to chain multiple Skyvern calls together&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Improved context&lt;/strong&gt; - Improve Skyvern's ability to understand content around interactable elements by introducing feeding relevant label context through the text prompt&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Cost Savings&lt;/strong&gt; - Improve Skyvern's stability and reduce the cost of running Skyvern by optimizing the context tree passed into Skyvern&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Self-serve UI&lt;/strong&gt; - Deprecate the Streamlit UI in favour of a React-based UI component that allows users to kick off new jobs in Skyvern&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Workflow UI Builder&lt;/strong&gt; - Introduce a UI to allow users to build and analyze workflows visually&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Chrome Viewport streaming&lt;/strong&gt; - Introduce a way to live-stream the Chrome viewport to the user's browser (as a part of the self-serve UI)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Past Runs UI&lt;/strong&gt; - Deprecate the Streamlit UI in favour of a React-based UI that allows you to visualize past runs and their results&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Auto workflow builder ("Observer") mode&lt;/strong&gt; - Allow Skyvern to auto-generate workflows as it's navigating the web to make it easier to build new workflows&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Prompt Caching&lt;/strong&gt; - Introduce a caching layer to the LLM calls to dramatically reduce the cost of running Skyvern (memorize past actions and repeat them!)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Web Evaluation Dataset&lt;/strong&gt; - Integrate Skyvern with public benchmark tests to track the quality of our models over time&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Improved Debug mode&lt;/strong&gt; - Allow Skyvern to plan its actions and get "approval" before running them, allowing you to debug what it's doing and more easily iterate on the prompt&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Chrome Extension&lt;/strong&gt; - Allow users to interact with Skyvern through a Chrome extension (incl voice mode, saving tasks, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Skyvern Action Recorder&lt;/strong&gt; - Allow Skyvern to watch a user complete a task and then automatically generate a workflow for it&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Interactable Livestream&lt;/strong&gt; - Allow users to interact with the livestream in real-time to intervene when necessary (such as manually submitting sensitive forms)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Integrate LLM Observability tools&lt;/strong&gt; - Integrate LLM Observability tools to allow back-testing prompt changes with specific data sets + visualize the performance of Skyvern over time&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Langchain Integration&lt;/strong&gt; - Create langchain integration in langchain_community to use Skyvern as a "tool".&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Contributing&lt;/h1&gt; 
&lt;p&gt;We welcome PRs and suggestions! Don't hesitate to open a PR/issue or to reach out to us &lt;a href="mailto:founders@skyvern.com"&gt;via email&lt;/a&gt; or &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;discord&lt;/a&gt;. Please have a look at our &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/CONTRIBUTING.md"&gt;contribution guide&lt;/a&gt; and &lt;a href="https://github.com/skyvern-ai/skyvern/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22"&gt;"Help Wanted" issues&lt;/a&gt; to get started!&lt;/p&gt; 
&lt;p&gt;If you want to chat with the skyvern repository to get a high level overview of how it is structured, how to build off it, and how to resolve usage questions, check out &lt;a href="https://sage.storia.ai?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=skyvern-readme"&gt;Code Sage&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Telemetry&lt;/h1&gt; 
&lt;p&gt;By Default, Skyvern collects basic usage statistics to help us understand how Skyvern is being used. If you would like to opt-out of telemetry, please set the &lt;code&gt;SKYVERN_TELEMETRY&lt;/code&gt; environment variable to &lt;code&gt;false&lt;/code&gt;.&lt;/p&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;Skyvern's open source repository is supported via a managed cloud. All of the core logic powering Skyvern is available in this open source repository licensed under the &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/LICENSE"&gt;AGPL-3.0 License&lt;/a&gt;, with the exception of anti-bot measures available in our managed cloud offering.&lt;/p&gt; 
&lt;p&gt;If you have any questions or concerns around licensing, please &lt;a href="mailto:support@skyvern.com"&gt;contact us&lt;/a&gt; and we would be happy to help.&lt;/p&gt; 
&lt;h1&gt;Star History&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#Skyvern-AI/skyvern&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Skyvern-AI/skyvern&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ytdl-org/youtube-dl</title>
      <link>https://github.com/ytdl-org/youtube-dl</link>
      <description>&lt;p&gt;Command-line program to download videos from YouTube.com and other video sites&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://github.com/ytdl-org/youtube-dl/actions?query=workflow%3ACI"&gt;&lt;img src="https://github.com/ytdl-org/youtube-dl/workflows/CI/badge.svg?sanitize=true" alt="Build Status" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;youtube-dl - download videos from youtube.com or other video platforms&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#installation"&gt;INSTALLATION&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#description"&gt;DESCRIPTION&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#options"&gt;OPTIONS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#configuration"&gt;CONFIGURATION&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#output-template"&gt;OUTPUT TEMPLATE&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#format-selection"&gt;FORMAT SELECTION&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#video-selection"&gt;VIDEO SELECTION&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#faq"&gt;FAQ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#developer-instructions"&gt;DEVELOPER INSTRUCTIONS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#embedding-youtube-dl"&gt;EMBEDDING YOUTUBE-DL&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#bugs"&gt;BUGS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#copyright"&gt;COPYRIGHT&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;INSTALLATION&lt;/h1&gt; 
&lt;p&gt;To install it right away for all UNIX users (Linux, macOS, etc.), type:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo curl -L https://yt-dl.org/downloads/latest/youtube-dl -o /usr/local/bin/youtube-dl
sudo chmod a+rx /usr/local/bin/youtube-dl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you do not have curl, you can alternatively use a recent wget:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo wget https://yt-dl.org/downloads/latest/youtube-dl -O /usr/local/bin/youtube-dl
sudo chmod a+rx /usr/local/bin/youtube-dl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Windows users can &lt;a href="https://yt-dl.org/latest/youtube-dl.exe"&gt;download an .exe file&lt;/a&gt; and place it in any location on their &lt;a href="https://en.wikipedia.org/wiki/PATH_%28variable%29"&gt;PATH&lt;/a&gt; except for &lt;code&gt;%SYSTEMROOT%\System32&lt;/code&gt; (e.g. &lt;strong&gt;do not&lt;/strong&gt; put in &lt;code&gt;C:\Windows\System32&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;You can also use pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo -H pip install --upgrade youtube-dl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command will update youtube-dl if you have already installed it. See the &lt;a href="https://pypi.python.org/pypi/youtube_dl"&gt;pypi page&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;p&gt;macOS users can install youtube-dl with &lt;a href="https://brew.sh/"&gt;Homebrew&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;brew install youtube-dl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or with &lt;a href="https://www.macports.org/"&gt;MacPorts&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo port install youtube-dl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, refer to the &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#developer-instructions"&gt;developer instructions&lt;/a&gt; for how to check out and work with the git repository. For further options, including PGP signatures, see the &lt;a href="https://ytdl-org.github.io/youtube-dl/download.html"&gt;youtube-dl Download Page&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;DESCRIPTION&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;youtube-dl&lt;/strong&gt; is a command-line program to download videos from YouTube.com and a few more sites. It requires the Python interpreter, version 2.6, 2.7, or 3.2+, and it is not platform specific. It should work on your Unix box, on Windows or on macOS. It is released to the public domain, which means you can modify it, redistribute it or use it however you like.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;youtube-dl [OPTIONS] URL [URL...]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;OPTIONS&lt;/h1&gt; 
&lt;pre&gt;&lt;code&gt;-h, --help                           Print this help text and exit
--version                            Print program version and exit
-U, --update                         Update this program to latest version.
                                     Make sure that you have sufficient
                                     permissions (run with sudo if needed)
-i, --ignore-errors                  Continue on download errors, for
                                     example to skip unavailable videos in a
                                     playlist
--abort-on-error                     Abort downloading of further videos (in
                                     the playlist or the command line) if an
                                     error occurs
--dump-user-agent                    Display the current browser
                                     identification
--list-extractors                    List all supported extractors
--extractor-descriptions             Output descriptions of all supported
                                     extractors
--force-generic-extractor            Force extraction to use the generic
                                     extractor
--default-search PREFIX              Use this prefix for unqualified URLs.
                                     For example "gvsearch2:" downloads two
                                     videos from google videos for youtube-
                                     dl "large apple". Use the value "auto"
                                     to let youtube-dl guess ("auto_warning"
                                     to emit a warning when guessing).
                                     "error" just throws an error. The
                                     default value "fixup_error" repairs
                                     broken URLs, but emits an error if this
                                     is not possible instead of searching.
--ignore-config                      Do not read configuration files. When
                                     given in the global configuration file
                                     /etc/youtube-dl.conf: Do not read the
                                     user configuration in
                                     ~/.config/youtube-dl/config
                                     (%APPDATA%/youtube-dl/config.txt on
                                     Windows)
--config-location PATH               Location of the configuration file;
                                     either the path to the config or its
                                     containing directory.
--flat-playlist                      Do not extract the videos of a
                                     playlist, only list them.
--mark-watched                       Mark videos watched (YouTube only)
--no-mark-watched                    Do not mark videos watched (YouTube
                                     only)
--no-color                           Do not emit color codes in output
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Network Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;--proxy URL                          Use the specified HTTP/HTTPS/SOCKS
                                     proxy. To enable SOCKS proxy, specify a
                                     proper scheme. For example
                                     socks5://127.0.0.1:1080/. Pass in an
                                     empty string (--proxy "") for direct
                                     connection
--socket-timeout SECONDS             Time to wait before giving up, in
                                     seconds
--source-address IP                  Client-side IP address to bind to
-4, --force-ipv4                     Make all connections via IPv4
-6, --force-ipv6                     Make all connections via IPv6
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Geo Restriction:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;--geo-verification-proxy URL         Use this proxy to verify the IP address
                                     for some geo-restricted sites. The
                                     default proxy specified by --proxy (or
                                     none, if the option is not present) is
                                     used for the actual downloading.
--geo-bypass                         Bypass geographic restriction via
                                     faking X-Forwarded-For HTTP header
--no-geo-bypass                      Do not bypass geographic restriction
                                     via faking X-Forwarded-For HTTP header
--geo-bypass-country CODE            Force bypass geographic restriction
                                     with explicitly provided two-letter ISO
                                     3166-2 country code
--geo-bypass-ip-block IP_BLOCK       Force bypass geographic restriction
                                     with explicitly provided IP block in
                                     CIDR notation
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Video Selection:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;--playlist-start NUMBER              Playlist video to start at (default is
                                     1)
--playlist-end NUMBER                Playlist video to end at (default is
                                     last)
--playlist-items ITEM_SPEC           Playlist video items to download.
                                     Specify indices of the videos in the
                                     playlist separated by commas like: "--
                                     playlist-items 1,2,5,8" if you want to
                                     download videos indexed 1, 2, 5, 8 in
                                     the playlist. You can specify range: "
                                     --playlist-items 1-3,7,10-13", it will
                                     download the videos at index 1, 2, 3,
                                     7, 10, 11, 12 and 13.
--match-title REGEX                  Download only matching titles (regex or
                                     caseless sub-string)
--reject-title REGEX                 Skip download for matching titles
                                     (regex or caseless sub-string)
--max-downloads NUMBER               Abort after downloading NUMBER files
--min-filesize SIZE                  Do not download any videos smaller than
                                     SIZE (e.g. 50k or 44.6m)
--max-filesize SIZE                  Do not download any videos larger than
                                     SIZE (e.g. 50k or 44.6m)
--date DATE                          Download only videos uploaded in this
                                     date
--datebefore DATE                    Download only videos uploaded on or
                                     before this date (i.e. inclusive)
--dateafter DATE                     Download only videos uploaded on or
                                     after this date (i.e. inclusive)
--min-views COUNT                    Do not download any videos with less
                                     than COUNT views
--max-views COUNT                    Do not download any videos with more
                                     than COUNT views
--match-filter FILTER                Generic video filter. Specify any key
                                     (see the "OUTPUT TEMPLATE" for a list
                                     of available keys) to match if the key
                                     is present, !key to check if the key is
                                     not present, key &amp;gt; NUMBER (like
                                     "comment_count &amp;gt; 12", also works with
                                     &amp;gt;=, &amp;lt;, &amp;lt;=, !=, =) to compare against a
                                     number, key = 'LITERAL' (like "uploader
                                     = 'Mike Smith'", also works with !=) to
                                     match against a string literal and &amp;amp; to
                                     require multiple matches. Values which
                                     are not known are excluded unless you
                                     put a question mark (?) after the
                                     operator. For example, to only match
                                     videos that have been liked more than
                                     100 times and disliked less than 50
                                     times (or the dislike functionality is
                                     not available at the given service),
                                     but who also have a description, use
                                     --match-filter "like_count &amp;gt; 100 &amp;amp;
                                     dislike_count &amp;lt;? 50 &amp;amp; description" .
--no-playlist                        Download only the video, if the URL
                                     refers to a video and a playlist.
--yes-playlist                       Download the playlist, if the URL
                                     refers to a video and a playlist.
--age-limit YEARS                    Download only videos suitable for the
                                     given age
--download-archive FILE              Download only videos not listed in the
                                     archive file. Record the IDs of all
                                     downloaded videos in it.
--include-ads                        Download advertisements as well
                                     (experimental)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Download Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;-r, --limit-rate RATE                Maximum download rate in bytes per
                                     second (e.g. 50K or 4.2M)
-R, --retries RETRIES                Number of retries (default is 10), or
                                     "infinite".
--fragment-retries RETRIES           Number of retries for a fragment
                                     (default is 10), or "infinite" (DASH,
                                     hlsnative and ISM)
--skip-unavailable-fragments         Skip unavailable fragments (DASH,
                                     hlsnative and ISM)
--abort-on-unavailable-fragment      Abort downloading when some fragment is
                                     not available
--keep-fragments                     Keep downloaded fragments on disk after
                                     downloading is finished; fragments are
                                     erased by default
--buffer-size SIZE                   Size of download buffer (e.g. 1024 or
                                     16K) (default is 1024)
--no-resize-buffer                   Do not automatically adjust the buffer
                                     size. By default, the buffer size is
                                     automatically resized from an initial
                                     value of SIZE.
--http-chunk-size SIZE               Size of a chunk for chunk-based HTTP
                                     downloading (e.g. 10485760 or 10M)
                                     (default is disabled). May be useful
                                     for bypassing bandwidth throttling
                                     imposed by a webserver (experimental)
--playlist-reverse                   Download playlist videos in reverse
                                     order
--playlist-random                    Download playlist videos in random
                                     order
--xattr-set-filesize                 Set file xattribute ytdl.filesize with
                                     expected file size
--hls-prefer-native                  Use the native HLS downloader instead
                                     of ffmpeg
--hls-prefer-ffmpeg                  Use ffmpeg instead of the native HLS
                                     downloader
--hls-use-mpegts                     Use the mpegts container for HLS
                                     videos, allowing to play the video
                                     while downloading (some players may not
                                     be able to play it)
--external-downloader COMMAND        Use the specified external downloader.
                                     Currently supports aria2c,avconv,axel,c
                                     url,ffmpeg,httpie,wget
--external-downloader-args ARGS      Give these arguments to the external
                                     downloader
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Filesystem Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;-a, --batch-file FILE                File containing URLs to download ('-'
                                     for stdin), one URL per line. Lines
                                     starting with '#', ';' or ']' are
                                     considered as comments and ignored.
--id                                 Use only video ID in file name
-o, --output TEMPLATE                Output filename template, see the
                                     "OUTPUT TEMPLATE" for all the info
--output-na-placeholder PLACEHOLDER  Placeholder value for unavailable meta
                                     fields in output filename template
                                     (default is "NA")
--autonumber-start NUMBER            Specify the start value for
                                     %(autonumber)s (default is 1)
--restrict-filenames                 Restrict filenames to only ASCII
                                     characters, and avoid "&amp;amp;" and spaces in
                                     filenames
-w, --no-overwrites                  Do not overwrite files
-c, --continue                       Force resume of partially downloaded
                                     files. By default, youtube-dl will
                                     resume downloads if possible.
--no-continue                        Do not resume partially downloaded
                                     files (restart from beginning)
--no-part                            Do not use .part files - write directly
                                     into output file
--no-mtime                           Do not use the Last-modified header to
                                     set the file modification time
--write-description                  Write video description to a
                                     .description file
--write-info-json                    Write video metadata to a .info.json
                                     file
--write-annotations                  Write video annotations to a
                                     .annotations.xml file
--load-info-json FILE                JSON file containing the video
                                     information (created with the "--write-
                                     info-json" option)
--cookies FILE                       File to read cookies from and dump
                                     cookie jar in
--cache-dir DIR                      Location in the filesystem where
                                     youtube-dl can store some downloaded
                                     information permanently. By default
                                     $XDG_CACHE_HOME/youtube-dl or
                                     ~/.cache/youtube-dl . At the moment,
                                     only YouTube player files (for videos
                                     with obfuscated signatures) are cached,
                                     but that may change.
--no-cache-dir                       Disable filesystem caching
--rm-cache-dir                       Delete all filesystem cache files
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Thumbnail Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;--write-thumbnail                    Write thumbnail image to disk
--write-all-thumbnails               Write all thumbnail image formats to
                                     disk
--list-thumbnails                    Simulate and list all available
                                     thumbnail formats
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Verbosity / Simulation Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;-q, --quiet                          Activate quiet mode
--no-warnings                        Ignore warnings
-s, --simulate                       Do not download the video and do not
                                     write anything to disk
--skip-download                      Do not download the video
-g, --get-url                        Simulate, quiet but print URL
-e, --get-title                      Simulate, quiet but print title
--get-id                             Simulate, quiet but print id
--get-thumbnail                      Simulate, quiet but print thumbnail URL
--get-description                    Simulate, quiet but print video
                                     description
--get-duration                       Simulate, quiet but print video length
--get-filename                       Simulate, quiet but print output
                                     filename
--get-format                         Simulate, quiet but print output format
-j, --dump-json                      Simulate, quiet but print JSON
                                     information. See the "OUTPUT TEMPLATE"
                                     for a description of available keys.
-J, --dump-single-json               Simulate, quiet but print JSON
                                     information for each command-line
                                     argument. If the URL refers to a
                                     playlist, dump the whole playlist
                                     information in a single line.
--print-json                         Be quiet and print the video
                                     information as JSON (video is still
                                     being downloaded).
--newline                            Output progress bar as new lines
--no-progress                        Do not print progress bar
--console-title                      Display progress in console titlebar
-v, --verbose                        Print various debugging information
--dump-pages                         Print downloaded pages encoded using
                                     base64 to debug problems (very verbose)
--write-pages                        Write downloaded intermediary pages to
                                     files in the current directory to debug
                                     problems
--print-traffic                      Display sent and read HTTP traffic
-C, --call-home                      Contact the youtube-dl server for
                                     debugging
--no-call-home                       Do NOT contact the youtube-dl server
                                     for debugging
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Workarounds:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;--encoding ENCODING                  Force the specified encoding
                                     (experimental)
--no-check-certificate               Suppress HTTPS certificate validation
--prefer-insecure                    Use an unencrypted connection to
                                     retrieve information about the video.
                                     (Currently supported only for YouTube)
--user-agent UA                      Specify a custom user agent
--referer URL                        Specify a custom referer, use if the
                                     video access is restricted to one
                                     domain
--add-header FIELD:VALUE             Specify a custom HTTP header and its
                                     value, separated by a colon ':'. You
                                     can use this option multiple times
--bidi-workaround                    Work around terminals that lack
                                     bidirectional text support. Requires
                                     bidiv or fribidi executable in PATH
--sleep-interval SECONDS             Number of seconds to sleep before each
                                     download when used alone or a lower
                                     bound of a range for randomized sleep
                                     before each download (minimum possible
                                     number of seconds to sleep) when used
                                     along with --max-sleep-interval.
--max-sleep-interval SECONDS         Upper bound of a range for randomized
                                     sleep before each download (maximum
                                     possible number of seconds to sleep).
                                     Must only be used along with --min-
                                     sleep-interval.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Video Format Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;-f, --format FORMAT                  Video format code, see the "FORMAT
                                     SELECTION" for all the info
--all-formats                        Download all available video formats
--prefer-free-formats                Prefer free video formats unless a
                                     specific one is requested
-F, --list-formats                   List all available formats of requested
                                     videos
--youtube-skip-dash-manifest         Do not download the DASH manifests and
                                     related data on YouTube videos
--merge-output-format FORMAT         If a merge is required (e.g.
                                     bestvideo+bestaudio), output to given
                                     container format. One of mkv, mp4, ogg,
                                     webm, flv. Ignored if no merge is
                                     required
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Subtitle Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;--write-sub                          Write subtitle file
--write-auto-sub                     Write automatically generated subtitle
                                     file (YouTube only)
--all-subs                           Download all the available subtitles of
                                     the video
--list-subs                          List all available subtitles for the
                                     video
--sub-format FORMAT                  Subtitle format, accepts formats
                                     preference, for example: "srt" or
                                     "ass/srt/best"
--sub-lang LANGS                     Languages of the subtitles to download
                                     (optional) separated by commas, use
                                     --list-subs for available language tags
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Authentication Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;-u, --username USERNAME              Login with this account ID
-p, --password PASSWORD              Account password. If this option is
                                     left out, youtube-dl will ask
                                     interactively.
-2, --twofactor TWOFACTOR            Two-factor authentication code
-n, --netrc                          Use .netrc authentication data
--video-password PASSWORD            Video password (vimeo, youku)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Adobe Pass Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;--ap-mso MSO                         Adobe Pass multiple-system operator (TV
                                     provider) identifier, use --ap-list-mso
                                     for a list of available MSOs
--ap-username USERNAME               Multiple-system operator account login
--ap-password PASSWORD               Multiple-system operator account
                                     password. If this option is left out,
                                     youtube-dl will ask interactively.
--ap-list-mso                        List all supported multiple-system
                                     operators
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Post-processing Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;-x, --extract-audio                  Convert video files to audio-only files
                                     (requires ffmpeg/avconv and
                                     ffprobe/avprobe)
--audio-format FORMAT                Specify audio format: "best", "aac",
                                     "flac", "mp3", "m4a", "opus", "vorbis",
                                     or "wav"; "best" by default; No effect
                                     without -x
--audio-quality QUALITY              Specify ffmpeg/avconv audio quality,
                                     insert a value between 0 (better) and 9
                                     (worse) for VBR or a specific bitrate
                                     like 128K (default 5)
--recode-video FORMAT                Encode the video to another format if
                                     necessary (currently supported:
                                     mp4|flv|ogg|webm|mkv|avi)
--postprocessor-args ARGS            Give these arguments to the
                                     postprocessor
-k, --keep-video                     Keep the video file on disk after the
                                     post-processing; the video is erased by
                                     default
--no-post-overwrites                 Do not overwrite post-processed files;
                                     the post-processed files are
                                     overwritten by default
--embed-subs                         Embed subtitles in the video (only for
                                     mp4, webm and mkv videos)
--embed-thumbnail                    Embed thumbnail in the audio as cover
                                     art
--add-metadata                       Write metadata to the video file
--metadata-from-title FORMAT         Parse additional metadata like song
                                     title / artist from the video title.
                                     The format syntax is the same as
                                     --output. Regular expression with named
                                     capture groups may also be used. The
                                     parsed parameters replace existing
                                     values. Example: --metadata-from-title
                                     "%(artist)s - %(title)s" matches a
                                     title like "Coldplay - Paradise".
                                     Example (regex): --metadata-from-title
                                     "(?P&amp;lt;artist&amp;gt;.+?) - (?P&amp;lt;title&amp;gt;.+)"
--xattrs                             Write metadata to the video file's
                                     xattrs (using dublin core and xdg
                                     standards)
--fixup POLICY                       Automatically correct known faults of
                                     the file. One of never (do nothing),
                                     warn (only emit a warning),
                                     detect_or_warn (the default; fix file
                                     if we can, warn otherwise)
--prefer-avconv                      Prefer avconv over ffmpeg for running
                                     the postprocessors
--prefer-ffmpeg                      Prefer ffmpeg over avconv for running
                                     the postprocessors (default)
--ffmpeg-location PATH               Location of the ffmpeg/avconv binary;
                                     either the path to the binary or its
                                     containing directory.
--exec CMD                           Execute a command on the file after
                                     downloading and post-processing,
                                     similar to find's -exec syntax.
                                     Example: --exec 'adb push {}
                                     /sdcard/Music/ &amp;amp;&amp;amp; rm {}'
--convert-subs FORMAT                Convert the subtitles to other format
                                     (currently supported: srt|ass|vtt|lrc)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;CONFIGURATION&lt;/h1&gt; 
&lt;p&gt;You can configure youtube-dl by placing any supported command line option to a configuration file. On Linux and macOS, the system wide configuration file is located at &lt;code&gt;/etc/youtube-dl.conf&lt;/code&gt; and the user wide configuration file at &lt;code&gt;~/.config/youtube-dl/config&lt;/code&gt;. On Windows, the user wide configuration file locations are &lt;code&gt;%APPDATA%\youtube-dl\config.txt&lt;/code&gt; or &lt;code&gt;C:\Users\&amp;lt;user name&amp;gt;\youtube-dl.conf&lt;/code&gt;. Note that by default configuration file may not exist so you may need to create it yourself.&lt;/p&gt; 
&lt;p&gt;For example, with the following configuration file youtube-dl will always extract the audio, not copy the mtime, use a proxy and save all videos under &lt;code&gt;Movies&lt;/code&gt; directory in your home directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# Lines starting with # are comments

# Always extract audio
-x

# Do not copy the mtime
--no-mtime

# Use this proxy
--proxy 127.0.0.1:3128

# Save all videos under Movies directory in your home directory
-o ~/Movies/%(title)s.%(ext)s
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that options in configuration file are just the same options aka switches used in regular command line calls thus there &lt;strong&gt;must be no whitespace&lt;/strong&gt; after &lt;code&gt;-&lt;/code&gt; or &lt;code&gt;--&lt;/code&gt;, e.g. &lt;code&gt;-o&lt;/code&gt; or &lt;code&gt;--proxy&lt;/code&gt; but not &lt;code&gt;- o&lt;/code&gt; or &lt;code&gt;-- proxy&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can use &lt;code&gt;--ignore-config&lt;/code&gt; if you want to disable the configuration file for a particular youtube-dl run.&lt;/p&gt; 
&lt;p&gt;You can also use &lt;code&gt;--config-location&lt;/code&gt; if you want to use custom configuration file for a particular youtube-dl run.&lt;/p&gt; 
&lt;h3&gt;Authentication with &lt;code&gt;.netrc&lt;/code&gt; file&lt;/h3&gt; 
&lt;p&gt;You may also want to configure automatic credentials storage for extractors that support authentication (by providing login and password with &lt;code&gt;--username&lt;/code&gt; and &lt;code&gt;--password&lt;/code&gt;) in order not to pass credentials as command line arguments on every youtube-dl execution and prevent tracking plain text passwords in the shell command history. You can achieve this using a &lt;a href="https://stackoverflow.com/tags/.netrc/info"&gt;&lt;code&gt;.netrc&lt;/code&gt; file&lt;/a&gt; on a per extractor basis. For that you will need to create a &lt;code&gt;.netrc&lt;/code&gt; file in your &lt;code&gt;$HOME&lt;/code&gt; and restrict permissions to read/write by only you:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;touch $HOME/.netrc
chmod a-rwx,u+rw $HOME/.netrc
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After that you can add credentials for an extractor in the following format, where &lt;em&gt;extractor&lt;/em&gt; is the name of the extractor in lowercase:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;machine &amp;lt;extractor&amp;gt; login &amp;lt;login&amp;gt; password &amp;lt;password&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;machine youtube login myaccount@gmail.com password my_youtube_password
machine twitch login my_twitch_account_name password my_twitch_password
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To activate authentication with the &lt;code&gt;.netrc&lt;/code&gt; file you should pass &lt;code&gt;--netrc&lt;/code&gt; to youtube-dl or place it in the &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#configuration"&gt;configuration file&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;On Windows you may also need to setup the &lt;code&gt;%HOME%&lt;/code&gt; environment variable manually. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;set HOME=%USERPROFILE%
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;OUTPUT TEMPLATE&lt;/h1&gt; 
&lt;p&gt;The &lt;code&gt;-o&lt;/code&gt; option allows users to indicate a template for the output file names.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;tl;dr:&lt;/strong&gt; &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#output-template-examples"&gt;navigate me to examples&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The basic usage is not to set any template arguments when downloading a single file, like in &lt;code&gt;youtube-dl -o funny_video.flv "https://some/video"&lt;/code&gt;. However, it may contain special sequences that will be replaced when downloading each video. The special sequences may be formatted according to &lt;a href="https://docs.python.org/2/library/stdtypes.html#string-formatting"&gt;python string formatting operations&lt;/a&gt;. For example, &lt;code&gt;%(NAME)s&lt;/code&gt; or &lt;code&gt;%(NAME)05d&lt;/code&gt;. To clarify, that is a percent symbol followed by a name in parentheses, followed by formatting operations. Allowed names along with sequence type are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;id&lt;/code&gt; (string): Video identifier&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;title&lt;/code&gt; (string): Video title&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;url&lt;/code&gt; (string): Video URL&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ext&lt;/code&gt; (string): Video filename extension&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;alt_title&lt;/code&gt; (string): A secondary title of the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;display_id&lt;/code&gt; (string): An alternative identifier for the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;uploader&lt;/code&gt; (string): Full name of the video uploader&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;license&lt;/code&gt; (string): License name the video is licensed under&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;creator&lt;/code&gt; (string): The creator of the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;release_date&lt;/code&gt; (string): The date (YYYYMMDD) when the video was released&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;timestamp&lt;/code&gt; (numeric): UNIX timestamp of the moment the video became available&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;upload_date&lt;/code&gt; (string): Video upload date (YYYYMMDD)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;uploader_id&lt;/code&gt; (string): Nickname or id of the video uploader&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;channel&lt;/code&gt; (string): Full name of the channel the video is uploaded on&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;channel_id&lt;/code&gt; (string): Id of the channel&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;location&lt;/code&gt; (string): Physical location where the video was filmed&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;duration&lt;/code&gt; (numeric): Length of the video in seconds&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;view_count&lt;/code&gt; (numeric): How many users have watched the video on the platform&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;like_count&lt;/code&gt; (numeric): Number of positive ratings of the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;dislike_count&lt;/code&gt; (numeric): Number of negative ratings of the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;repost_count&lt;/code&gt; (numeric): Number of reposts of the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;average_rating&lt;/code&gt; (numeric): Average rating give by users, the scale used depends on the webpage&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;comment_count&lt;/code&gt; (numeric): Number of comments on the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;age_limit&lt;/code&gt; (numeric): Age restriction for the video (years)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;is_live&lt;/code&gt; (boolean): Whether this video is a live stream or a fixed-length video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;start_time&lt;/code&gt; (numeric): Time in seconds where the reproduction should start, as specified in the URL&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;end_time&lt;/code&gt; (numeric): Time in seconds where the reproduction should end, as specified in the URL&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;format&lt;/code&gt; (string): A human-readable description of the format&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;format_id&lt;/code&gt; (string): Format code specified by &lt;code&gt;--format&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;format_note&lt;/code&gt; (string): Additional info about the format&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;width&lt;/code&gt; (numeric): Width of the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;height&lt;/code&gt; (numeric): Height of the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;resolution&lt;/code&gt; (string): Textual description of width and height&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tbr&lt;/code&gt; (numeric): Average bitrate of audio and video in KBit/s&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;abr&lt;/code&gt; (numeric): Average audio bitrate in KBit/s&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;acodec&lt;/code&gt; (string): Name of the audio codec in use&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;asr&lt;/code&gt; (numeric): Audio sampling rate in Hertz&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vbr&lt;/code&gt; (numeric): Average video bitrate in KBit/s&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;fps&lt;/code&gt; (numeric): Frame rate&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vcodec&lt;/code&gt; (string): Name of the video codec in use&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;container&lt;/code&gt; (string): Name of the container format&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;filesize&lt;/code&gt; (numeric): The number of bytes, if known in advance&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;filesize_approx&lt;/code&gt; (numeric): An estimate for the number of bytes&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;protocol&lt;/code&gt; (string): The protocol that will be used for the actual download&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;extractor&lt;/code&gt; (string): Name of the extractor&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;extractor_key&lt;/code&gt; (string): Key name of the extractor&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;epoch&lt;/code&gt; (numeric): Unix epoch when creating the file&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;autonumber&lt;/code&gt; (numeric): Number that will be increased with each download, starting at &lt;code&gt;--autonumber-start&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist&lt;/code&gt; (string): Name or id of the playlist that contains the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_index&lt;/code&gt; (numeric): Index of the video in the playlist padded with leading zeros according to the total length of the playlist&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_id&lt;/code&gt; (string): Playlist identifier&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_title&lt;/code&gt; (string): Playlist title&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_uploader&lt;/code&gt; (string): Full name of the playlist uploader&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_uploader_id&lt;/code&gt; (string): Nickname or id of the playlist uploader&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Available for the video that belongs to some logical chapter or section:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;chapter&lt;/code&gt; (string): Name or title of the chapter the video belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;chapter_number&lt;/code&gt; (numeric): Number of the chapter the video belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;chapter_id&lt;/code&gt; (string): Id of the chapter the video belongs to&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Available for the video that is an episode of some series or programme:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;series&lt;/code&gt; (string): Title of the series or programme the video episode belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;season&lt;/code&gt; (string): Title of the season the video episode belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;season_number&lt;/code&gt; (numeric): Number of the season the video episode belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;season_id&lt;/code&gt; (string): Id of the season the video episode belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;episode&lt;/code&gt; (string): Title of the video episode&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;episode_number&lt;/code&gt; (numeric): Number of the video episode within a season&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;episode_id&lt;/code&gt; (string): Id of the video episode&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Available for the media that is a track or a part of a music album:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;track&lt;/code&gt; (string): Title of the track&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;track_number&lt;/code&gt; (numeric): Number of the track within an album or a disc&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;track_id&lt;/code&gt; (string): Id of the track&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;artist&lt;/code&gt; (string): Artist(s) of the track&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;genre&lt;/code&gt; (string): Genre(s) of the track&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;album&lt;/code&gt; (string): Title of the album the track belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;album_type&lt;/code&gt; (string): Type of the album&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;album_artist&lt;/code&gt; (string): List of all artists appeared on the album&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;disc_number&lt;/code&gt; (numeric): Number of the disc or other physical medium the track belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;release_year&lt;/code&gt; (numeric): Year (YYYY) when the album was released&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Each aforementioned sequence when referenced in an output template will be replaced by the actual value corresponding to the sequence name. Note that some of the sequences are not guaranteed to be present since they depend on the metadata obtained by a particular extractor. Such sequences will be replaced with placeholder value provided with &lt;code&gt;--output-na-placeholder&lt;/code&gt; (&lt;code&gt;NA&lt;/code&gt; by default).&lt;/p&gt; 
&lt;p&gt;For example for &lt;code&gt;-o %(title)s-%(id)s.%(ext)s&lt;/code&gt; and an mp4 video with title &lt;code&gt;youtube-dl test video&lt;/code&gt; and id &lt;code&gt;BaW_jenozKcj&lt;/code&gt;, this will result in a &lt;code&gt;youtube-dl test video-BaW_jenozKcj.mp4&lt;/code&gt; file created in the current directory.&lt;/p&gt; 
&lt;p&gt;For numeric sequences you can use numeric related formatting, for example, &lt;code&gt;%(view_count)05d&lt;/code&gt; will result in a string with view count padded with zeros up to 5 characters, like in &lt;code&gt;00042&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Output templates can also contain arbitrary hierarchical path, e.g. &lt;code&gt;-o '%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s'&lt;/code&gt; which will result in downloading each video in a directory corresponding to this path template. Any missing directory will be automatically created for you.&lt;/p&gt; 
&lt;p&gt;To use percent literals in an output template use &lt;code&gt;%%&lt;/code&gt;. To output to stdout use &lt;code&gt;-o -&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;The current default template is &lt;code&gt;%(title)s-%(id)s.%(ext)s&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;In some cases, you don't want special characters such as ä¸­, spaces, or &amp;amp;, such as when transferring the downloaded filename to a Windows system or the filename through an 8bit-unsafe channel. In these cases, add the &lt;code&gt;--restrict-filenames&lt;/code&gt; flag to get a shorter title.&lt;/p&gt; 
&lt;h4&gt;Output template and Windows batch files&lt;/h4&gt; 
&lt;p&gt;If you are using an output template inside a Windows batch file then you must escape plain percent characters (&lt;code&gt;%&lt;/code&gt;) by doubling, so that &lt;code&gt;-o "%(title)s-%(id)s.%(ext)s"&lt;/code&gt; should become &lt;code&gt;-o "%%(title)s-%%(id)s.%%(ext)s"&lt;/code&gt;. However you should not touch &lt;code&gt;%&lt;/code&gt;'s that are not plain characters, e.g. environment variables for expansion should stay intact: &lt;code&gt;-o "C:\%HOMEPATH%\Desktop\%%(title)s.%%(ext)s"&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;Output template examples&lt;/h4&gt; 
&lt;p&gt;Note that on Windows you may need to use double quotes instead of single.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ youtube-dl --get-filename -o '%(title)s.%(ext)s' BaW_jenozKc
youtube-dl test video ''_Ã¤â†­ğ•.mp4    # All kinds of weird characters

$ youtube-dl --get-filename -o '%(title)s.%(ext)s' BaW_jenozKc --restrict-filenames
youtube-dl_test_video_.mp4          # A simple file name

# Download YouTube playlist videos in separate directory indexed by video order in a playlist
$ youtube-dl -o '%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s' https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re

# Download all playlists of YouTube channel/user keeping each playlist in separate directory:
$ youtube-dl -o '%(uploader)s/%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s' https://www.youtube.com/user/TheLinuxFoundation/playlists

# Download Udemy course keeping each chapter in separate directory under MyVideos directory in your home
$ youtube-dl -u user -p password -o '~/MyVideos/%(playlist)s/%(chapter_number)s - %(chapter)s/%(title)s.%(ext)s' https://www.udemy.com/java-tutorial/

# Download entire series season keeping each series and each season in separate directory under C:/MyVideos
$ youtube-dl -o "C:/MyVideos/%(series)s/%(season_number)s - %(season)s/%(episode_number)s - %(episode)s.%(ext)s" https://videomore.ru/kino_v_detalayah/5_sezon/367617

# Stream the video being downloaded to stdout
$ youtube-dl -o - BaW_jenozKc
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;FORMAT SELECTION&lt;/h1&gt; 
&lt;p&gt;By default youtube-dl tries to download the best available quality, i.e. if you want the best quality you &lt;strong&gt;don't need&lt;/strong&gt; to pass any special options, youtube-dl will guess it for you by &lt;strong&gt;default&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;But sometimes you may want to download in a different format, for example when you are on a slow or intermittent connection. The key mechanism for achieving this is so-called &lt;em&gt;format selection&lt;/em&gt; based on which you can explicitly specify desired format, select formats based on some criterion or criteria, setup precedence and much more.&lt;/p&gt; 
&lt;p&gt;The general syntax for format selection is &lt;code&gt;--format FORMAT&lt;/code&gt; or shorter &lt;code&gt;-f FORMAT&lt;/code&gt; where &lt;code&gt;FORMAT&lt;/code&gt; is a &lt;em&gt;selector expression&lt;/em&gt;, i.e. an expression that describes format or formats you would like to download.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;tl;dr:&lt;/strong&gt; &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#format-selection-examples"&gt;navigate me to examples&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The simplest case is requesting a specific format, for example with &lt;code&gt;-f 22&lt;/code&gt; you can download the format with format code equal to 22. You can get the list of available format codes for particular video using &lt;code&gt;--list-formats&lt;/code&gt; or &lt;code&gt;-F&lt;/code&gt;. Note that these format codes are extractor specific.&lt;/p&gt; 
&lt;p&gt;You can also use a file extension (currently &lt;code&gt;3gp&lt;/code&gt;, &lt;code&gt;aac&lt;/code&gt;, &lt;code&gt;flv&lt;/code&gt;, &lt;code&gt;m4a&lt;/code&gt;, &lt;code&gt;mp3&lt;/code&gt;, &lt;code&gt;mp4&lt;/code&gt;, &lt;code&gt;ogg&lt;/code&gt;, &lt;code&gt;wav&lt;/code&gt;, &lt;code&gt;webm&lt;/code&gt; are supported) to download the best quality format of a particular file extension served as a single file, e.g. &lt;code&gt;-f webm&lt;/code&gt; will download the best quality format with the &lt;code&gt;webm&lt;/code&gt; extension served as a single file.&lt;/p&gt; 
&lt;p&gt;You can also use special names to select particular edge case formats:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;best&lt;/code&gt;: Select the best quality format represented by a single file with video and audio.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;worst&lt;/code&gt;: Select the worst quality format represented by a single file with video and audio.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;bestvideo&lt;/code&gt;: Select the best quality video-only format (e.g. DASH video). May not be available.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;worstvideo&lt;/code&gt;: Select the worst quality video-only format. May not be available.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;bestaudio&lt;/code&gt;: Select the best quality audio only-format. May not be available.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;worstaudio&lt;/code&gt;: Select the worst quality audio only-format. May not be available.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For example, to download the worst quality video-only format you can use &lt;code&gt;-f worstvideo&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;If you want to download multiple videos and they don't have the same formats available, you can specify the order of preference using slashes. Note that slash is left-associative, i.e. formats on the left hand side are preferred, for example &lt;code&gt;-f 22/17/18&lt;/code&gt; will download format 22 if it's available, otherwise it will download format 17 if it's available, otherwise it will download format 18 if it's available, otherwise it will complain that no suitable formats are available for download.&lt;/p&gt; 
&lt;p&gt;If you want to download several formats of the same video use a comma as a separator, e.g. &lt;code&gt;-f 22,17,18&lt;/code&gt; will download all these three formats, of course if they are available. Or a more sophisticated example combined with the precedence feature: &lt;code&gt;-f 136/137/mp4/bestvideo,140/m4a/bestaudio&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can also filter the video formats by putting a condition in brackets, as in &lt;code&gt;-f "best[height=720]"&lt;/code&gt; (or &lt;code&gt;-f "[filesize&amp;gt;10M]"&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;The following numeric meta fields can be used with comparisons &lt;code&gt;&amp;lt;&lt;/code&gt;, &lt;code&gt;&amp;lt;=&lt;/code&gt;, &lt;code&gt;&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;gt;=&lt;/code&gt;, &lt;code&gt;=&lt;/code&gt; (equals), &lt;code&gt;!=&lt;/code&gt; (not equals):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;filesize&lt;/code&gt;: The number of bytes, if known in advance&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;width&lt;/code&gt;: Width of the video, if known&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;height&lt;/code&gt;: Height of the video, if known&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tbr&lt;/code&gt;: Average bitrate of audio and video in KBit/s&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;abr&lt;/code&gt;: Average audio bitrate in KBit/s&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vbr&lt;/code&gt;: Average video bitrate in KBit/s&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;asr&lt;/code&gt;: Audio sampling rate in Hertz&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;fps&lt;/code&gt;: Frame rate&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Also filtering work for comparisons &lt;code&gt;=&lt;/code&gt; (equals), &lt;code&gt;^=&lt;/code&gt; (starts with), &lt;code&gt;$=&lt;/code&gt; (ends with), &lt;code&gt;*=&lt;/code&gt; (contains) and following string meta fields:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;ext&lt;/code&gt;: File extension&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;acodec&lt;/code&gt;: Name of the audio codec in use&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vcodec&lt;/code&gt;: Name of the video codec in use&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;container&lt;/code&gt;: Name of the container format&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;protocol&lt;/code&gt;: The protocol that will be used for the actual download, lower-case (&lt;code&gt;http&lt;/code&gt;, &lt;code&gt;https&lt;/code&gt;, &lt;code&gt;rtsp&lt;/code&gt;, &lt;code&gt;rtmp&lt;/code&gt;, &lt;code&gt;rtmpe&lt;/code&gt;, &lt;code&gt;mms&lt;/code&gt;, &lt;code&gt;f4m&lt;/code&gt;, &lt;code&gt;ism&lt;/code&gt;, &lt;code&gt;http_dash_segments&lt;/code&gt;, &lt;code&gt;m3u8&lt;/code&gt;, or &lt;code&gt;m3u8_native&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;format_id&lt;/code&gt;: A short description of the format&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;language&lt;/code&gt;: Language code&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Any string comparison may be prefixed with negation &lt;code&gt;!&lt;/code&gt; in order to produce an opposite comparison, e.g. &lt;code&gt;!*=&lt;/code&gt; (does not contain).&lt;/p&gt; 
&lt;p&gt;Note that none of the aforementioned meta fields are guaranteed to be present since this solely depends on the metadata obtained by particular extractor, i.e. the metadata offered by the video hoster.&lt;/p&gt; 
&lt;p&gt;Formats for which the value is not known are excluded unless you put a question mark (&lt;code&gt;?&lt;/code&gt;) after the operator. You can combine format filters, so &lt;code&gt;-f "[height &amp;lt;=? 720][tbr&amp;gt;500]"&lt;/code&gt; selects up to 720p videos (or videos where the height is not known) with a bitrate of at least 500 KBit/s.&lt;/p&gt; 
&lt;p&gt;You can merge the video and audio of two formats into a single file using &lt;code&gt;-f &amp;lt;video-format&amp;gt;+&amp;lt;audio-format&amp;gt;&lt;/code&gt; (requires ffmpeg or avconv installed), for example &lt;code&gt;-f bestvideo+bestaudio&lt;/code&gt; will download the best video-only format, the best audio-only format and mux them together with ffmpeg/avconv.&lt;/p&gt; 
&lt;p&gt;Format selectors can also be grouped using parentheses, for example if you want to download the best mp4 and webm formats with a height lower than 480 you can use &lt;code&gt;-f '(mp4,webm)[height&amp;lt;480]'&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Since the end of April 2015 and version 2015.04.26, youtube-dl uses &lt;code&gt;-f bestvideo+bestaudio/best&lt;/code&gt; as the default format selection (see &lt;a href="https://github.com/ytdl-org/youtube-dl/issues/5447"&gt;#5447&lt;/a&gt;, &lt;a href="https://github.com/ytdl-org/youtube-dl/issues/5456"&gt;#5456&lt;/a&gt;). If ffmpeg or avconv are installed this results in downloading &lt;code&gt;bestvideo&lt;/code&gt; and &lt;code&gt;bestaudio&lt;/code&gt; separately and muxing them together into a single file giving the best overall quality available. Otherwise it falls back to &lt;code&gt;best&lt;/code&gt; and results in downloading the best available quality served as a single file. &lt;code&gt;best&lt;/code&gt; is also needed for videos that don't come from YouTube because they don't provide the audio and video in two different files. If you want to only download some DASH formats (for example if you are not interested in getting videos with a resolution higher than 1080p), you can add &lt;code&gt;-f bestvideo[height&amp;lt;=?1080]+bestaudio/best&lt;/code&gt; to your configuration file. Note that if you use youtube-dl to stream to &lt;code&gt;stdout&lt;/code&gt; (and most likely to pipe it to your media player then), i.e. you explicitly specify output template as &lt;code&gt;-o -&lt;/code&gt;, youtube-dl still uses &lt;code&gt;-f best&lt;/code&gt; format selection in order to start content delivery immediately to your player and not to wait until &lt;code&gt;bestvideo&lt;/code&gt; and &lt;code&gt;bestaudio&lt;/code&gt; are downloaded and muxed.&lt;/p&gt; 
&lt;p&gt;If you want to preserve the old format selection behavior (prior to youtube-dl 2015.04.26), i.e. you want to download the best available quality media served as a single file, you should explicitly specify your choice with &lt;code&gt;-f best&lt;/code&gt;. You may want to add it to the &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#configuration"&gt;configuration file&lt;/a&gt; in order not to type it every time you run youtube-dl.&lt;/p&gt; 
&lt;h4&gt;Format selection examples&lt;/h4&gt; 
&lt;p&gt;Note that on Windows you may need to use double quotes instead of single.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Download best mp4 format available or any other best if no mp4 available
$ youtube-dl -f 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best'

# Download best format available but no better than 480p
$ youtube-dl -f 'bestvideo[height&amp;lt;=480]+bestaudio/best[height&amp;lt;=480]'

# Download best video only format but no bigger than 50 MB
$ youtube-dl -f 'best[filesize&amp;lt;50M]'

# Download best format available via direct link over HTTP/HTTPS protocol
$ youtube-dl -f '(bestvideo+bestaudio/best)[protocol^=http]'

# Download the best video format and the best audio format without merging them
$ youtube-dl -f 'bestvideo,bestaudio' -o '%(title)s.f%(format_id)s.%(ext)s'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that in the last example, an output template is recommended as bestvideo and bestaudio may have the same file name.&lt;/p&gt; 
&lt;h1&gt;VIDEO SELECTION&lt;/h1&gt; 
&lt;p&gt;Videos can be filtered by their upload date using the options &lt;code&gt;--date&lt;/code&gt;, &lt;code&gt;--datebefore&lt;/code&gt; or &lt;code&gt;--dateafter&lt;/code&gt;. They accept dates in two formats:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Absolute dates: Dates in the format &lt;code&gt;YYYYMMDD&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Relative dates: Dates in the format &lt;code&gt;(now|today)[+-][0-9](day|week|month|year)(s)?&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Examples:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Download only the videos uploaded in the last 6 months
$ youtube-dl --dateafter now-6months

# Download only the videos uploaded on January 1, 1970
$ youtube-dl --date 19700101

$ # Download only the videos uploaded in the 200x decade
$ youtube-dl --dateafter 20000101 --datebefore 20091231
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;FAQ&lt;/h1&gt; 
&lt;h3&gt;How do I update youtube-dl?&lt;/h3&gt; 
&lt;p&gt;If you've followed &lt;a href="https://ytdl-org.github.io/youtube-dl/download.html"&gt;our manual installation instructions&lt;/a&gt;, you can simply run &lt;code&gt;youtube-dl -U&lt;/code&gt; (or, on Linux, &lt;code&gt;sudo youtube-dl -U&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;If you have used pip, a simple &lt;code&gt;sudo pip install -U youtube-dl&lt;/code&gt; is sufficient to update.&lt;/p&gt; 
&lt;p&gt;If you have installed youtube-dl using a package manager like &lt;em&gt;apt-get&lt;/em&gt; or &lt;em&gt;yum&lt;/em&gt;, use the standard system update mechanism to update. Note that distribution packages are often outdated. As a rule of thumb, youtube-dl releases at least once a month, and often weekly or even daily. Simply go to &lt;a href="https://yt-dl.org"&gt;https://yt-dl.org&lt;/a&gt; to find out the current version. Unfortunately, there is nothing we youtube-dl developers can do if your distribution serves a really outdated version. You can (and should) complain to your distribution in their bugtracker or support forum.&lt;/p&gt; 
&lt;p&gt;As a last resort, you can also uninstall the version installed by your package manager and follow our manual installation instructions. For that, remove the distribution's package, with a line like&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo apt-get remove -y youtube-dl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Afterwards, simply follow &lt;a href="https://ytdl-org.github.io/youtube-dl/download.html"&gt;our manual installation instructions&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo wget https://yt-dl.org/downloads/latest/youtube-dl -O /usr/local/bin/youtube-dl
sudo chmod a+rx /usr/local/bin/youtube-dl
hash -r
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Again, from then on you'll be able to update with &lt;code&gt;sudo youtube-dl -U&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;youtube-dl is extremely slow to start on Windows&lt;/h3&gt; 
&lt;p&gt;Add a file exclusion for &lt;code&gt;youtube-dl.exe&lt;/code&gt; in Windows Defender settings.&lt;/p&gt; 
&lt;h3&gt;I'm getting an error &lt;code&gt;Unable to extract OpenGraph title&lt;/code&gt; on YouTube playlists&lt;/h3&gt; 
&lt;p&gt;YouTube changed their playlist format in March 2014 and later on, so you'll need at least youtube-dl 2014.07.25 to download all YouTube videos.&lt;/p&gt; 
&lt;p&gt;If you have installed youtube-dl with a package manager, pip, setup.py or a tarball, please use that to update. Note that Ubuntu packages do not seem to get updated anymore. Since we are not affiliated with Ubuntu, there is little we can do. Feel free to &lt;a href="https://bugs.launchpad.net/ubuntu/+source/youtube-dl/+filebug"&gt;report bugs&lt;/a&gt; to the &lt;a href="mailto:ubuntu-motu@lists.ubuntu.com?subject=outdated%20version%20of%20youtube-dl"&gt;Ubuntu packaging people&lt;/a&gt; - all they have to do is update the package to a somewhat recent version. See above for a way to update.&lt;/p&gt; 
&lt;h3&gt;I'm getting an error when trying to use output template: &lt;code&gt;error: using output template conflicts with using title, video ID or auto number&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;Make sure you are not using &lt;code&gt;-o&lt;/code&gt; with any of these options &lt;code&gt;-t&lt;/code&gt;, &lt;code&gt;--title&lt;/code&gt;, &lt;code&gt;--id&lt;/code&gt;, &lt;code&gt;-A&lt;/code&gt; or &lt;code&gt;--auto-number&lt;/code&gt; set in command line or in a configuration file. Remove the latter if any.&lt;/p&gt; 
&lt;h3&gt;Do I always have to pass &lt;code&gt;-citw&lt;/code&gt;?&lt;/h3&gt; 
&lt;p&gt;By default, youtube-dl intends to have the best options (incidentally, if you have a convincing case that these should be different, &lt;a href="https://yt-dl.org/bug"&gt;please file an issue where you explain that&lt;/a&gt;). Therefore, it is unnecessary and sometimes harmful to copy long option strings from webpages. In particular, the only option out of &lt;code&gt;-citw&lt;/code&gt; that is regularly useful is &lt;code&gt;-i&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Can you please put the &lt;code&gt;-b&lt;/code&gt; option back?&lt;/h3&gt; 
&lt;p&gt;Most people asking this question are not aware that youtube-dl now defaults to downloading the highest available quality as reported by YouTube, which will be 1080p or 720p in some cases, so you no longer need the &lt;code&gt;-b&lt;/code&gt; option. For some specific videos, maybe YouTube does not report them to be available in a specific high quality format you're interested in. In that case, simply request it with the &lt;code&gt;-f&lt;/code&gt; option and youtube-dl will try to download it.&lt;/p&gt; 
&lt;h3&gt;I get HTTP error 402 when trying to download a video. What's this?&lt;/h3&gt; 
&lt;p&gt;Apparently YouTube requires you to pass a CAPTCHA test if you download too much. We're &lt;a href="https://github.com/ytdl-org/youtube-dl/issues/154"&gt;considering to provide a way to let you solve the CAPTCHA&lt;/a&gt;, but at the moment, your best course of action is pointing a web browser to the youtube URL, solving the CAPTCHA, and restart youtube-dl.&lt;/p&gt; 
&lt;h3&gt;Do I need any other programs?&lt;/h3&gt; 
&lt;p&gt;youtube-dl works fine on its own on most sites. However, if you want to convert video/audio, you'll need &lt;a href="https://libav.org/"&gt;avconv&lt;/a&gt; or &lt;a href="https://www.ffmpeg.org/"&gt;ffmpeg&lt;/a&gt;. On some sites - most notably YouTube - videos can be retrieved in a higher quality format without sound. youtube-dl will detect whether avconv/ffmpeg is present and automatically pick the best option.&lt;/p&gt; 
&lt;p&gt;Videos or video formats streamed via RTMP protocol can only be downloaded when &lt;a href="https://rtmpdump.mplayerhq.hu/"&gt;rtmpdump&lt;/a&gt; is installed. Downloading MMS and RTSP videos requires either &lt;a href="https://mplayerhq.hu/"&gt;mplayer&lt;/a&gt; or &lt;a href="https://mpv.io/"&gt;mpv&lt;/a&gt; to be installed.&lt;/p&gt; 
&lt;h3&gt;I have downloaded a video but how can I play it?&lt;/h3&gt; 
&lt;p&gt;Once the video is fully downloaded, use any video player, such as &lt;a href="https://mpv.io/"&gt;mpv&lt;/a&gt;, &lt;a href="https://www.videolan.org/"&gt;vlc&lt;/a&gt; or &lt;a href="https://www.mplayerhq.hu/"&gt;mplayer&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;I extracted a video URL with &lt;code&gt;-g&lt;/code&gt;, but it does not play on another machine / in my web browser.&lt;/h3&gt; 
&lt;p&gt;It depends a lot on the service. In many cases, requests for the video (to download/play it) must come from the same IP address and with the same cookies and/or HTTP headers. Use the &lt;code&gt;--cookies&lt;/code&gt; option to write the required cookies into a file, and advise your downloader to read cookies from that file. Some sites also require a common user agent to be used, use &lt;code&gt;--dump-user-agent&lt;/code&gt; to see the one in use by youtube-dl. You can also get necessary cookies and HTTP headers from JSON output obtained with &lt;code&gt;--dump-json&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;It may be beneficial to use IPv6; in some cases, the restrictions are only applied to IPv4. Some services (sometimes only for a subset of videos) do not restrict the video URL by IP address, cookie, or user-agent, but these are the exception rather than the rule.&lt;/p&gt; 
&lt;p&gt;Please bear in mind that some URL protocols are &lt;strong&gt;not&lt;/strong&gt; supported by browsers out of the box, including RTMP. If you are using &lt;code&gt;-g&lt;/code&gt;, your own downloader must support these as well.&lt;/p&gt; 
&lt;p&gt;If you want to play the video on a machine that is not running youtube-dl, you can relay the video content from the machine that runs youtube-dl. You can use &lt;code&gt;-o -&lt;/code&gt; to let youtube-dl stream a video to stdout, or simply allow the player to download the files written by youtube-dl in turn.&lt;/p&gt; 
&lt;h3&gt;ERROR: no fmt_url_map or conn information found in video info&lt;/h3&gt; 
&lt;p&gt;YouTube has switched to a new video info format in July 2011 which is not supported by old versions of youtube-dl. See &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#how-do-i-update-youtube-dl"&gt;above&lt;/a&gt; for how to update youtube-dl.&lt;/p&gt; 
&lt;h3&gt;ERROR: unable to download video&lt;/h3&gt; 
&lt;p&gt;YouTube requires an additional signature since September 2012 which is not supported by old versions of youtube-dl. See &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#how-do-i-update-youtube-dl"&gt;above&lt;/a&gt; for how to update youtube-dl.&lt;/p&gt; 
&lt;h3&gt;Video URL contains an ampersand and I'm getting some strange output &lt;code&gt;[1] 2839&lt;/code&gt; or &lt;code&gt;'v' is not recognized as an internal or external command&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;That's actually the output from your shell. Since ampersand is one of the special shell characters it's interpreted by the shell preventing you from passing the whole URL to youtube-dl. To disable your shell from interpreting the ampersands (or any other special characters) you have to either put the whole URL in quotes or escape them with a backslash (which approach will work depends on your shell).&lt;/p&gt; 
&lt;p&gt;For example if your URL is &lt;a href="https://www.youtube.com/watch?t=4&amp;amp;v=BaW_jenozKc"&gt;https://www.youtube.com/watch?t=4&amp;amp;v=BaW_jenozKc&lt;/a&gt; you should end up with following command:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;youtube-dl 'https://www.youtube.com/watch?t=4&amp;amp;v=BaW_jenozKc'&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;or&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;youtube-dl https://www.youtube.com/watch?t=4\&amp;amp;v=BaW_jenozKc&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;For Windows you have to use the double quotes:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;youtube-dl "https://www.youtube.com/watch?t=4&amp;amp;v=BaW_jenozKc"&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;ExtractorError: Could not find JS function u'OF'&lt;/h3&gt; 
&lt;p&gt;In February 2015, the new YouTube player contained a character sequence in a string that was misinterpreted by old versions of youtube-dl. See &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#how-do-i-update-youtube-dl"&gt;above&lt;/a&gt; for how to update youtube-dl.&lt;/p&gt; 
&lt;h3&gt;HTTP Error 429: Too Many Requests or 402: Payment Required&lt;/h3&gt; 
&lt;p&gt;These two error codes indicate that the service is blocking your IP address because of overuse. Usually this is a soft block meaning that you can gain access again after solving CAPTCHA. Just open a browser and solve a CAPTCHA the service suggests you and after that &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#how-do-i-pass-cookies-to-youtube-dl"&gt;pass cookies&lt;/a&gt; to youtube-dl. Note that if your machine has multiple external IPs then you should also pass exactly the same IP you've used for solving CAPTCHA with &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#network-options"&gt;&lt;code&gt;--source-address&lt;/code&gt;&lt;/a&gt;. Also you may need to pass a &lt;code&gt;User-Agent&lt;/code&gt; HTTP header of your browser with &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#workarounds"&gt;&lt;code&gt;--user-agent&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If this is not the case (no CAPTCHA suggested to solve by the service) then you can contact the service and ask them to unblock your IP address, or - if you have acquired a whitelisted IP address already - use the &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#network-options"&gt;&lt;code&gt;--proxy&lt;/code&gt; or &lt;code&gt;--source-address&lt;/code&gt; options&lt;/a&gt; to select another IP address.&lt;/p&gt; 
&lt;h3&gt;SyntaxError: Non-ASCII character&lt;/h3&gt; 
&lt;p&gt;The error&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;File "youtube-dl", line 2
SyntaxError: Non-ASCII character '\x93' ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;means you're using an outdated version of Python. Please update to Python 2.6 or 2.7.&lt;/p&gt; 
&lt;h3&gt;What is this binary file? Where has the code gone?&lt;/h3&gt; 
&lt;p&gt;Since June 2012 (&lt;a href="https://github.com/ytdl-org/youtube-dl/issues/342"&gt;#342&lt;/a&gt;) youtube-dl is packed as an executable zipfile, simply unzip it (might need renaming to &lt;code&gt;youtube-dl.zip&lt;/code&gt; first on some systems) or clone the git repository, as laid out above. If you modify the code, you can run it by executing the &lt;code&gt;__main__.py&lt;/code&gt; file. To recompile the executable, run &lt;code&gt;make youtube-dl&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;The exe throws an error due to missing &lt;code&gt;MSVCR100.dll&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;To run the exe you need to install first the &lt;a href="https://download.microsoft.com/download/1/6/5/165255E7-1014-4D0A-B094-B6A430A6BFFC/vcredist_x86.exe"&gt;Microsoft Visual C++ 2010 Service Pack 1 Redistributable Package (x86)&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;On Windows, how should I set up ffmpeg and youtube-dl? Where should I put the exe files?&lt;/h3&gt; 
&lt;p&gt;If you put youtube-dl and ffmpeg in the same directory that you're running the command from, it will work, but that's rather cumbersome.&lt;/p&gt; 
&lt;p&gt;To make a different directory work - either for ffmpeg, or for youtube-dl, or for both - simply create the directory (say, &lt;code&gt;C:\bin&lt;/code&gt;, or &lt;code&gt;C:\Users\&amp;lt;User name&amp;gt;\bin&lt;/code&gt;), put all the executables directly in there, and then &lt;a href="https://www.java.com/en/download/help/path.xml"&gt;set your PATH environment variable&lt;/a&gt; to include that directory.&lt;/p&gt; 
&lt;p&gt;From then on, after restarting your shell, you will be able to access both youtube-dl and ffmpeg (and youtube-dl will be able to find ffmpeg) by simply typing &lt;code&gt;youtube-dl&lt;/code&gt; or &lt;code&gt;ffmpeg&lt;/code&gt;, no matter what directory you're in.&lt;/p&gt; 
&lt;h3&gt;How do I put downloads into a specific folder?&lt;/h3&gt; 
&lt;p&gt;Use the &lt;code&gt;-o&lt;/code&gt; to specify an &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#output-template"&gt;output template&lt;/a&gt;, for example &lt;code&gt;-o "/home/user/videos/%(title)s-%(id)s.%(ext)s"&lt;/code&gt;. If you want this for all of your downloads, put the option into your &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#configuration"&gt;configuration file&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;How do I download a video starting with a &lt;code&gt;-&lt;/code&gt;?&lt;/h3&gt; 
&lt;p&gt;Either prepend &lt;code&gt;https://www.youtube.com/watch?v=&lt;/code&gt; or separate the ID from the options with &lt;code&gt;--&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;youtube-dl -- -wNyEUrxzFU
youtube-dl "https://www.youtube.com/watch?v=-wNyEUrxzFU"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;How do I pass cookies to youtube-dl?&lt;/h3&gt; 
&lt;p&gt;Use the &lt;code&gt;--cookies&lt;/code&gt; option, for example &lt;code&gt;--cookies /path/to/cookies/file.txt&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;In order to extract cookies from browser use any conforming browser extension for exporting cookies. For example, &lt;a href="https://chrome.google.com/webstore/detail/get-cookiestxt-locally/cclelndahbckbenkjhflpdbgdldlbecc"&gt;Get cookies.txt LOCALLY&lt;/a&gt; (for Chrome) or &lt;a href="https://addons.mozilla.org/en-US/firefox/addon/cookies-txt/"&gt;cookies.txt&lt;/a&gt; (for Firefox).&lt;/p&gt; 
&lt;p&gt;Note that the cookies file must be in Mozilla/Netscape format and the first line of the cookies file must be either &lt;code&gt;# HTTP Cookie File&lt;/code&gt; or &lt;code&gt;# Netscape HTTP Cookie File&lt;/code&gt;. Make sure you have correct &lt;a href="https://en.wikipedia.org/wiki/Newline"&gt;newline format&lt;/a&gt; in the cookies file and convert newlines if necessary to correspond with your OS, namely &lt;code&gt;CRLF&lt;/code&gt; (&lt;code&gt;\r\n&lt;/code&gt;) for Windows and &lt;code&gt;LF&lt;/code&gt; (&lt;code&gt;\n&lt;/code&gt;) for Unix and Unix-like systems (Linux, macOS, etc.). &lt;code&gt;HTTP Error 400: Bad Request&lt;/code&gt; when using &lt;code&gt;--cookies&lt;/code&gt; is a good sign of invalid newline format.&lt;/p&gt; 
&lt;p&gt;Passing cookies to youtube-dl is a good way to workaround login when a particular extractor does not implement it explicitly. Another use case is working around &lt;a href="https://en.wikipedia.org/wiki/CAPTCHA"&gt;CAPTCHA&lt;/a&gt; some websites require you to solve in particular cases in order to get access (e.g. YouTube, CloudFlare).&lt;/p&gt; 
&lt;h3&gt;How do I stream directly to media player?&lt;/h3&gt; 
&lt;p&gt;You will first need to tell youtube-dl to stream media to stdout with &lt;code&gt;-o -&lt;/code&gt;, and also tell your media player to read from stdin (it must be capable of this for streaming) and then pipe former to latter. For example, streaming to &lt;a href="https://www.videolan.org/"&gt;vlc&lt;/a&gt; can be achieved with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;youtube-dl -o - "https://www.youtube.com/watch?v=BaW_jenozKcj" | vlc -
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;How do I download only new videos from a playlist?&lt;/h3&gt; 
&lt;p&gt;Use download-archive feature. With this feature you should initially download the complete playlist with &lt;code&gt;--download-archive /path/to/download/archive/file.txt&lt;/code&gt; that will record identifiers of all the videos in a special file. Each subsequent run with the same &lt;code&gt;--download-archive&lt;/code&gt; will download only new videos and skip all videos that have been downloaded before. Note that only successful downloads are recorded in the file.&lt;/p&gt; 
&lt;p&gt;For example, at first,&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;youtube-dl --download-archive archive.txt "https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;will download the complete &lt;code&gt;PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re&lt;/code&gt; playlist and create a file &lt;code&gt;archive.txt&lt;/code&gt;. Each subsequent run will only download new videos if any:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;youtube-dl --download-archive archive.txt "https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Should I add &lt;code&gt;--hls-prefer-native&lt;/code&gt; into my config?&lt;/h3&gt; 
&lt;p&gt;When youtube-dl detects an HLS video, it can download it either with the built-in downloader or ffmpeg. Since many HLS streams are slightly invalid and ffmpeg/youtube-dl each handle some invalid cases better than the other, there is an option to switch the downloader if needed.&lt;/p&gt; 
&lt;p&gt;When youtube-dl knows that one particular downloader works better for a given website, that downloader will be picked. Otherwise, youtube-dl will pick the best downloader for general compatibility, which at the moment happens to be ffmpeg. This choice may change in future versions of youtube-dl, with improvements of the built-in downloader and/or ffmpeg.&lt;/p&gt; 
&lt;p&gt;In particular, the generic extractor (used when your website is not in the &lt;a href="https://ytdl-org.github.io/youtube-dl/supportedsites.html"&gt;list of supported sites by youtube-dl&lt;/a&gt; cannot mandate one specific downloader.&lt;/p&gt; 
&lt;p&gt;If you put either &lt;code&gt;--hls-prefer-native&lt;/code&gt; or &lt;code&gt;--hls-prefer-ffmpeg&lt;/code&gt; into your configuration, a different subset of videos will fail to download correctly. Instead, it is much better to &lt;a href="https://yt-dl.org/bug"&gt;file an issue&lt;/a&gt; or a pull request which details why the native or the ffmpeg HLS downloader is a better choice for your use case.&lt;/p&gt; 
&lt;h3&gt;Can you add support for this anime video site, or site which shows current movies for free?&lt;/h3&gt; 
&lt;p&gt;As a matter of policy (as well as legality), youtube-dl does not include support for services that specialize in infringing copyright. As a rule of thumb, if you cannot easily find a video that the service is quite obviously allowed to distribute (i.e. that has been uploaded by the creator, the creator's distributor, or is published under a free license), the service is probably unfit for inclusion to youtube-dl.&lt;/p&gt; 
&lt;p&gt;A note on the service that they don't host the infringing content, but just link to those who do, is evidence that the service should &lt;strong&gt;not&lt;/strong&gt; be included into youtube-dl. The same goes for any DMCA note when the whole front page of the service is filled with videos they are not allowed to distribute. A "fair use" note is equally unconvincing if the service shows copyright-protected videos in full without authorization.&lt;/p&gt; 
&lt;p&gt;Support requests for services that &lt;strong&gt;do&lt;/strong&gt; purchase the rights to distribute their content are perfectly fine though. If in doubt, you can simply include a source that mentions the legitimate purchase of content.&lt;/p&gt; 
&lt;h3&gt;How can I speed up work on my issue?&lt;/h3&gt; 
&lt;p&gt;(Also known as: Help, my important issue not being solved!) The youtube-dl core developer team is quite small. While we do our best to solve as many issues as possible, sometimes that can take quite a while. To speed up your issue, here's what you can do:&lt;/p&gt; 
&lt;p&gt;First of all, please do report the issue &lt;a href="https://yt-dl.org/bugs"&gt;at our issue tracker&lt;/a&gt;. That allows us to coordinate all efforts by users and developers, and serves as a unified point. Unfortunately, the youtube-dl project has grown too large to use personal email as an effective communication channel.&lt;/p&gt; 
&lt;p&gt;Please read the &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#bugs"&gt;bug reporting instructions&lt;/a&gt; below. A lot of bugs lack all the necessary information. If you can, offer proxy, VPN, or shell access to the youtube-dl developers. If you are able to, test the issue from multiple computers in multiple countries to exclude local censorship or misconfiguration issues.&lt;/p&gt; 
&lt;p&gt;If nobody is interested in solving your issue, you are welcome to take matters into your own hands and submit a pull request (or coerce/pay somebody else to do so).&lt;/p&gt; 
&lt;p&gt;Feel free to bump the issue from time to time by writing a small comment ("Issue is still present in youtube-dl version ...from France, but fixed from Belgium"), but please not more than once a month. Please do not declare your issue as &lt;code&gt;important&lt;/code&gt; or &lt;code&gt;urgent&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;How can I detect whether a given URL is supported by youtube-dl?&lt;/h3&gt; 
&lt;p&gt;For one, have a look at the &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/docs/supportedsites.md"&gt;list of supported sites&lt;/a&gt;. Note that it can sometimes happen that the site changes its URL scheme (say, from &lt;a href="https://example.com/video/1234567"&gt;https://example.com/video/1234567&lt;/a&gt; to &lt;a href="https://example.com/v/1234567"&gt;https://example.com/v/1234567&lt;/a&gt; ) and youtube-dl reports an URL of a service in that list as unsupported. In that case, simply report a bug.&lt;/p&gt; 
&lt;p&gt;It is &lt;em&gt;not&lt;/em&gt; possible to detect whether a URL is supported or not. That's because youtube-dl contains a generic extractor which matches &lt;strong&gt;all&lt;/strong&gt; URLs. You may be tempted to disable, exclude, or remove the generic extractor, but the generic extractor not only allows users to extract videos from lots of websites that embed a video from another service, but may also be used to extract video from a service that it's hosting itself. Therefore, we neither recommend nor support disabling, excluding, or removing the generic extractor.&lt;/p&gt; 
&lt;p&gt;If you want to find out whether a given URL is supported, simply call youtube-dl with it. If you get no videos back, chances are the URL is either not referring to a video or unsupported. You can find out which by examining the output (if you run youtube-dl on the console) or catching an &lt;code&gt;UnsupportedError&lt;/code&gt; exception if you run it from a Python program.&lt;/p&gt; 
&lt;h1&gt;Why do I need to go through that much red tape when filing bugs?&lt;/h1&gt; 
&lt;p&gt;Before we had the issue template, despite our extensive &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#bugs"&gt;bug reporting instructions&lt;/a&gt;, about 80% of the issue reports we got were useless, for instance because people used ancient versions hundreds of releases old, because of simple syntactic errors (not in youtube-dl but in general shell usage), because the problem was already reported multiple times before, because people did not actually read an error message, even if it said "please install ffmpeg", because people did not mention the URL they were trying to download and many more simple, easy-to-avoid problems, many of whom were totally unrelated to youtube-dl.&lt;/p&gt; 
&lt;p&gt;youtube-dl is an open-source project manned by too few volunteers, so we'd rather spend time fixing bugs where we are certain none of those simple problems apply, and where we can be reasonably confident to be able to reproduce the issue without asking the reporter repeatedly. As such, the output of &lt;code&gt;youtube-dl -v YOUR_URL_HERE&lt;/code&gt; is really all that's required to file an issue. The issue template also guides you through some basic steps you can do, such as checking that your version of youtube-dl is current.&lt;/p&gt; 
&lt;h1&gt;DEVELOPER INSTRUCTIONS&lt;/h1&gt; 
&lt;p&gt;Most users do not need to build youtube-dl and can &lt;a href="https://ytdl-org.github.io/youtube-dl/download.html"&gt;download the builds&lt;/a&gt; or get them from their distribution.&lt;/p&gt; 
&lt;p&gt;To run youtube-dl as a developer, you don't need to build anything either. Simply execute&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python -m youtube_dl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run the test, simply invoke your favorite test runner, or execute a test file directly; any of the following work:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python -m unittest discover
python test/test_download.py
nosetests
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Python versions 3.6 and later, you can use &lt;a href="https://pypi.org/project/pynose/"&gt;pynose&lt;/a&gt; to implement &lt;code&gt;nosetests&lt;/code&gt;. The original &lt;a href="https://pypi.org/project/nose/"&gt;nose&lt;/a&gt; has not been upgraded for 3.10 and later.&lt;/p&gt; 
&lt;p&gt;See item 6 of &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#adding-support-for-a-new-site"&gt;new extractor tutorial&lt;/a&gt; for how to run extractor specific test cases.&lt;/p&gt; 
&lt;p&gt;If you want to create a build of youtube-dl yourself, you'll need&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;python&lt;/li&gt; 
 &lt;li&gt;make (only GNU make is supported)&lt;/li&gt; 
 &lt;li&gt;pandoc&lt;/li&gt; 
 &lt;li&gt;zip&lt;/li&gt; 
 &lt;li&gt;nosetests&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Adding support for a new site&lt;/h3&gt; 
&lt;p&gt;If you want to add support for a new site, first of all &lt;strong&gt;make sure&lt;/strong&gt; this site is &lt;strong&gt;not dedicated to &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/README.md#can-you-add-support-for-this-anime-video-site-or-site-which-shows-current-movies-for-free"&gt;copyright infringement&lt;/a&gt;&lt;/strong&gt;. youtube-dl does &lt;strong&gt;not support&lt;/strong&gt; such sites thus pull requests adding support for them &lt;strong&gt;will be rejected&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;After you have ensured this site is distributing its content legally, you can follow this quick list (assuming your service is called &lt;code&gt;yourextractor&lt;/code&gt;):&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/ytdl-org/youtube-dl/fork"&gt;Fork this repository&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Check out the source code with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; git clone git@github.com:YOUR_GITHUB_USERNAME/youtube-dl.git
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Start a new git branch with&lt;/p&gt; &lt;pre&gt;&lt;code&gt; cd youtube-dl
 git checkout -b yourextractor
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Start with this simple template and save it to &lt;code&gt;youtube_dl/extractor/yourextractor.py&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;# coding: utf-8
from __future__ import unicode_literals

from .common import InfoExtractor


class YourExtractorIE(InfoExtractor):
    _VALID_URL = r'https?://(?:www\.)?yourextractor\.com/watch/(?P&amp;lt;id&amp;gt;[0-9]+)'
    _TEST = {
        'url': 'https://yourextractor.com/watch/42',
        'md5': 'TODO: md5 sum of the first 10241 bytes of the video file (use --test)',
        'info_dict': {
            'id': '42',
            'ext': 'mp4',
            'title': 'Video title goes here',
            'thumbnail': r're:^https?://.*\.jpg$',
            # TODO more properties, either as:
            # * A value
            # * MD5 checksum; start the string with md5:
            # * A regular expression; start the string with re:
            # * Any Python type (for example int or float)
        }
    }

    def _real_extract(self, url):
        video_id = self._match_id(url)
        webpage = self._download_webpage(url, video_id)

        # TODO more code goes here, for example ...
        title = self._html_search_regex(r'&amp;lt;h1&amp;gt;(.+?)&amp;lt;/h1&amp;gt;', webpage, 'title')

        return {
            'id': video_id,
            'title': title,
            'description': self._og_search_description(webpage),
            'uploader': self._search_regex(r'&amp;lt;div[^&amp;gt;]+id="uploader"[^&amp;gt;]*&amp;gt;([^&amp;lt;]+)&amp;lt;', webpage, 'uploader', fatal=False),
            # TODO more properties (see youtube_dl/extractor/common.py)
        }
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Add an import in &lt;a href="https://github.com/ytdl-org/youtube-dl/raw/master/youtube_dl/extractor/extractors.py"&gt;&lt;code&gt;youtube_dl/extractor/extractors.py&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run &lt;code&gt;python test/test_download.py TestDownload.test_YourExtractor&lt;/code&gt;. This &lt;em&gt;should fail&lt;/em&gt; at first, but you can continually re-run it until you're done. If you decide to add more than one test (actually, test case) then rename &lt;code&gt;_TEST&lt;/code&gt; to &lt;code&gt;_TESTS&lt;/code&gt; and make it into a list of dictionaries. The tests will then be named &lt;code&gt;TestDownload.test_YourExtractor&lt;/code&gt;, &lt;code&gt;TestDownload.test_YourExtractor_1&lt;/code&gt;, &lt;code&gt;TestDownload.test_YourExtractor_2&lt;/code&gt;, etc. Note:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;the test names use the extractor class name &lt;strong&gt;without the trailing &lt;code&gt;IE&lt;/code&gt;&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;tests with &lt;code&gt;only_matching&lt;/code&gt; key in test's dict are not counted.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Have a look at &lt;a href="https://github.com/ytdl-org/youtube-dl/raw/master/youtube_dl/extractor/common.py"&gt;&lt;code&gt;youtube_dl/extractor/common.py&lt;/code&gt;&lt;/a&gt; for possible helper methods and a &lt;a href="https://github.com/ytdl-org/youtube-dl/raw/7f41a598b3fba1bcab2817de64a08941200aa3c8/youtube_dl/extractor/common.py#L94-L303"&gt;detailed description of what your extractor should and may return&lt;/a&gt;. Add tests and code for as many as you want.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Make sure your code follows &lt;a href="https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/#youtube-dl-coding-conventions"&gt;youtube-dl coding conventions&lt;/a&gt; and check the code with &lt;a href="https://flake8.pycqa.org/en/latest/index.html#quickstart"&gt;flake8&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; $ flake8 youtube_dl/extractor/yourextractor.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Make sure your code works under all &lt;a href="https://www.python.org/"&gt;Python&lt;/a&gt; versions claimed supported by youtube-dl, namely 2.6, 2.7, and 3.2+.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;When the tests pass, &lt;a href="https://git-scm.com/docs/git-add"&gt;add&lt;/a&gt; the new files and &lt;a href="https://git-scm.com/docs/git-commit"&gt;commit&lt;/a&gt; them and &lt;a href="https://git-scm.com/docs/git-push"&gt;push&lt;/a&gt; the result, like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ git add youtube_dl/extractor/extractors.py
$ git add youtube_dl/extractor/yourextractor.py
$ git commit -m '[yourextractor] Add new extractor'
$ git push origin yourextractor
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Finally, &lt;a href="https://help.github.com/articles/creating-a-pull-request"&gt;create a pull request&lt;/a&gt;. We'll then review and merge it.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;In any case, thank you very much for your contributions!&lt;/p&gt; 
&lt;h2&gt;youtube-dl coding conventions&lt;/h2&gt; 
&lt;p&gt;This section introduces guidelines for writing idiomatic, robust and future-proof extractor code.&lt;/p&gt; 
&lt;p&gt;Extractors are very fragile by nature since they depend on the layout of the source data provided by 3rd party media hosters out of your control and this layout tends to change. As an extractor implementer your task is not only to write code that will extract media links and metadata correctly but also to minimize dependency on the source's layout and even to make the code foresee potential future changes and be ready for that. This is important because it will allow the extractor not to break on minor layout changes thus keeping old youtube-dl versions working. Even though this breakage issue is easily fixed by emitting a new version of youtube-dl with a fix incorporated, all the previous versions become broken in all repositories and distros' packages that may not be so prompt in fetching the update from us. Needless to say, some non rolling release distros may never receive an update at all.&lt;/p&gt; 
&lt;h3&gt;Mandatory and optional metafields&lt;/h3&gt; 
&lt;p&gt;For extraction to work youtube-dl relies on metadata your extractor extracts and provides to youtube-dl expressed by an &lt;a href="https://github.com/ytdl-org/youtube-dl/raw/7f41a598b3fba1bcab2817de64a08941200aa3c8/youtube_dl/extractor/common.py#L94-L303"&gt;information dictionary&lt;/a&gt; or simply &lt;em&gt;info dict&lt;/em&gt;. Only the following meta fields in the &lt;em&gt;info dict&lt;/em&gt; are considered mandatory for a successful extraction process by youtube-dl:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;id&lt;/code&gt; (media identifier)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;title&lt;/code&gt; (media title)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;url&lt;/code&gt; (media download URL) or &lt;code&gt;formats&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In fact only the last option is technically mandatory (i.e. if you can't figure out the download location of the media the extraction does not make any sense). But by convention youtube-dl also treats &lt;code&gt;id&lt;/code&gt; and &lt;code&gt;title&lt;/code&gt; as mandatory. Thus the aforementioned metafields are the critical data that the extraction does not make any sense without and if any of them fail to be extracted then the extractor is considered completely broken.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/ytdl-org/youtube-dl/raw/7f41a598b3fba1bcab2817de64a08941200aa3c8/youtube_dl/extractor/common.py#L188-L303"&gt;Any field&lt;/a&gt; apart from the aforementioned ones are considered &lt;strong&gt;optional&lt;/strong&gt;. That means that extraction should be &lt;strong&gt;tolerant&lt;/strong&gt; to situations when sources for these fields can potentially be unavailable (even if they are always available at the moment) and &lt;strong&gt;future-proof&lt;/strong&gt; in order not to break the extraction of general purpose mandatory fields.&lt;/p&gt; 
&lt;h4&gt;Example&lt;/h4&gt; 
&lt;p&gt;Say you have some source dictionary &lt;code&gt;meta&lt;/code&gt; that you've fetched as JSON with HTTP request and it has a key &lt;code&gt;summary&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;meta = self._download_json(url, video_id)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Assume at this point &lt;code&gt;meta&lt;/code&gt;'s layout is:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;{
    ...
    "summary": "some fancy summary text",
    ...
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Assume you want to extract &lt;code&gt;summary&lt;/code&gt; and put it into the resulting info dict as &lt;code&gt;description&lt;/code&gt;. Since &lt;code&gt;description&lt;/code&gt; is an optional meta field you should be ready that this key may be missing from the &lt;code&gt;meta&lt;/code&gt; dict, so that you should extract it like:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;description = meta.get('summary')  # correct
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and not like:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;description = meta['summary']  # incorrect
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The latter will break extraction process with &lt;code&gt;KeyError&lt;/code&gt; if &lt;code&gt;summary&lt;/code&gt; disappears from &lt;code&gt;meta&lt;/code&gt; at some later time but with the former approach extraction will just go ahead with &lt;code&gt;description&lt;/code&gt; set to &lt;code&gt;None&lt;/code&gt; which is perfectly fine (remember &lt;code&gt;None&lt;/code&gt; is equivalent to the absence of data).&lt;/p&gt; 
&lt;p&gt;Similarly, you should pass &lt;code&gt;fatal=False&lt;/code&gt; when extracting optional data from a webpage with &lt;code&gt;_search_regex&lt;/code&gt;, &lt;code&gt;_html_search_regex&lt;/code&gt; or similar methods, for instance:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;description = self._search_regex(
    r'&amp;lt;span[^&amp;gt;]+id="title"[^&amp;gt;]*&amp;gt;([^&amp;lt;]+)&amp;lt;',
    webpage, 'description', fatal=False)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With &lt;code&gt;fatal&lt;/code&gt; set to &lt;code&gt;False&lt;/code&gt; if &lt;code&gt;_search_regex&lt;/code&gt; fails to extract &lt;code&gt;description&lt;/code&gt; it will emit a warning and continue extraction.&lt;/p&gt; 
&lt;p&gt;You can also pass &lt;code&gt;default=&amp;lt;some fallback value&amp;gt;&lt;/code&gt;, for example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;description = self._search_regex(
    r'&amp;lt;span[^&amp;gt;]+id="title"[^&amp;gt;]*&amp;gt;([^&amp;lt;]+)&amp;lt;',
    webpage, 'description', default=None)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;On failure this code will silently continue the extraction with &lt;code&gt;description&lt;/code&gt; set to &lt;code&gt;None&lt;/code&gt;. That is useful for metafields that may or may not be present.&lt;/p&gt; 
&lt;h3&gt;Provide fallbacks&lt;/h3&gt; 
&lt;p&gt;When extracting metadata try to do so from multiple sources. For example if &lt;code&gt;title&lt;/code&gt; is present in several places, try extracting from at least some of them. This makes it more future-proof in case some of the sources become unavailable.&lt;/p&gt; 
&lt;h4&gt;Example&lt;/h4&gt; 
&lt;p&gt;Say &lt;code&gt;meta&lt;/code&gt; from the previous example has a &lt;code&gt;title&lt;/code&gt; and you are about to extract it. Since &lt;code&gt;title&lt;/code&gt; is a mandatory meta field you should end up with something like:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;title = meta['title']
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If &lt;code&gt;title&lt;/code&gt; disappears from &lt;code&gt;meta&lt;/code&gt; in future due to some changes on the hoster's side the extraction would fail since &lt;code&gt;title&lt;/code&gt; is mandatory. That's expected.&lt;/p&gt; 
&lt;p&gt;Assume that you have some another source you can extract &lt;code&gt;title&lt;/code&gt; from, for example &lt;code&gt;og:title&lt;/code&gt; HTML meta of a &lt;code&gt;webpage&lt;/code&gt;. In this case you can provide a fallback scenario:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;title = meta.get('title') or self._og_search_title(webpage)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This code will try to extract from &lt;code&gt;meta&lt;/code&gt; first and if it fails it will try extracting &lt;code&gt;og:title&lt;/code&gt; from a &lt;code&gt;webpage&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Regular expressions&lt;/h3&gt; 
&lt;h4&gt;Don't capture groups you don't use&lt;/h4&gt; 
&lt;p&gt;Capturing group must be an indication that it's used somewhere in the code. Any group that is not used must be non capturing.&lt;/p&gt; 
&lt;h5&gt;Example&lt;/h5&gt; 
&lt;p&gt;Don't capture id attribute name here since you can't use it for anything anyway.&lt;/p&gt; 
&lt;p&gt;Correct:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;r'(?:id|ID)=(?P&amp;lt;id&amp;gt;\d+)'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Incorrect:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;r'(id|ID)=(?P&amp;lt;id&amp;gt;\d+)'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Make regular expressions relaxed and flexible&lt;/h4&gt; 
&lt;p&gt;When using regular expressions try to write them fuzzy, relaxed and flexible, skipping insignificant parts that are more likely to change, allowing both single and double quotes for quoted values and so on.&lt;/p&gt; 
&lt;h5&gt;Example&lt;/h5&gt; 
&lt;p&gt;Say you need to extract &lt;code&gt;title&lt;/code&gt; from the following HTML code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;span style="position: absolute; left: 910px; width: 90px; float: right; z-index: 9999;" class="title"&amp;gt;some fancy title&amp;lt;/span&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The code for that task should look similar to:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;title = self._search_regex(
    r'&amp;lt;span[^&amp;gt;]+class="title"[^&amp;gt;]*&amp;gt;([^&amp;lt;]+)', webpage, 'title')
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or even better:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;title = self._search_regex(
    r'&amp;lt;span[^&amp;gt;]+class=(["\'])title\1[^&amp;gt;]*&amp;gt;(?P&amp;lt;title&amp;gt;[^&amp;lt;]+)',
    webpage, 'title', group='title')
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note how you tolerate potential changes in the &lt;code&gt;style&lt;/code&gt; attribute's value or switch from using double quotes to single for &lt;code&gt;class&lt;/code&gt; attribute:&lt;/p&gt; 
&lt;p&gt;The code definitely should not look like:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;title = self._search_regex(
    r'&amp;lt;span style="position: absolute; left: 910px; width: 90px; float: right; z-index: 9999;" class="title"&amp;gt;(.*?)&amp;lt;/span&amp;gt;',
    webpage, 'title', group='title')
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Long lines policy&lt;/h3&gt; 
&lt;p&gt;There is a soft limit to keep lines of code under 80 characters long. This means it should be respected if possible and if it does not make readability and code maintenance worse.&lt;/p&gt; 
&lt;p&gt;For example, you should &lt;strong&gt;never&lt;/strong&gt; split long string literals like URLs or some other often copied entities over multiple lines to fit this limit:&lt;/p&gt; 
&lt;p&gt;Correct:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;'https://www.youtube.com/watch?v=FqZTN594JQw&amp;amp;list=PLMYEtVRpaqY00V9W81Cwmzp6N6vZqfUKD4'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Incorrect:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;'https://www.youtube.com/watch?v=FqZTN594JQw&amp;amp;list='
'PLMYEtVRpaqY00V9W81Cwmzp6N6vZqfUKD4'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Inline values&lt;/h3&gt; 
&lt;p&gt;Extracting variables is acceptable for reducing code duplication and improving readability of complex expressions. However, you should avoid extracting variables used only once and moving them to opposite parts of the extractor file, which makes reading the linear flow difficult.&lt;/p&gt; 
&lt;h4&gt;Example&lt;/h4&gt; 
&lt;p&gt;Correct:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;title = self._html_search_regex(r'&amp;lt;title&amp;gt;([^&amp;lt;]+)&amp;lt;/title&amp;gt;', webpage, 'title')
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Incorrect:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;TITLE_RE = r'&amp;lt;title&amp;gt;([^&amp;lt;]+)&amp;lt;/title&amp;gt;'
# ...some lines of code...
title = self._html_search_regex(TITLE_RE, webpage, 'title')
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Collapse fallbacks&lt;/h3&gt; 
&lt;p&gt;Multiple fallback values can quickly become unwieldy. Collapse multiple fallback values into a single expression via a list of patterns.&lt;/p&gt; 
&lt;h4&gt;Example&lt;/h4&gt; 
&lt;p&gt;Good:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;description = self._html_search_meta(
    ['og:description', 'description', 'twitter:description'],
    webpage, 'description', default=None)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Unwieldy:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;description = (
    self._og_search_description(webpage, default=None)
    or self._html_search_meta('description', webpage, default=None)
    or self._html_search_meta('twitter:description', webpage, default=None))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Methods supporting list of patterns are: &lt;code&gt;_search_regex&lt;/code&gt;, &lt;code&gt;_html_search_regex&lt;/code&gt;, &lt;code&gt;_og_search_property&lt;/code&gt;, &lt;code&gt;_html_search_meta&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Trailing parentheses&lt;/h3&gt; 
&lt;p&gt;Always move trailing parentheses after the last argument.&lt;/p&gt; 
&lt;h4&gt;Example&lt;/h4&gt; 
&lt;p&gt;Correct:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;    lambda x: x['ResultSet']['Result'][0]['VideoUrlSet']['VideoUrl'],
    list)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Incorrect:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;    lambda x: x['ResultSet']['Result'][0]['VideoUrlSet']['VideoUrl'],
    list,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Use convenience conversion and parsing functions&lt;/h3&gt; 
&lt;p&gt;Wrap all extracted numeric data into safe functions from &lt;a href="https://github.com/ytdl-org/youtube-dl/raw/master/youtube_dl/utils.py"&gt;&lt;code&gt;youtube_dl/utils.py&lt;/code&gt;&lt;/a&gt;: &lt;code&gt;int_or_none&lt;/code&gt;, &lt;code&gt;float_or_none&lt;/code&gt;. Use them for string to number conversions as well.&lt;/p&gt; 
&lt;p&gt;Use &lt;code&gt;url_or_none&lt;/code&gt; for safe URL processing.&lt;/p&gt; 
&lt;p&gt;Use &lt;code&gt;traverse_obj&lt;/code&gt; for safe metadata extraction from parsed JSON.&lt;/p&gt; 
&lt;p&gt;Use &lt;code&gt;unified_strdate&lt;/code&gt; for uniform &lt;code&gt;upload_date&lt;/code&gt; or any &lt;code&gt;YYYYMMDD&lt;/code&gt; meta field extraction, &lt;code&gt;unified_timestamp&lt;/code&gt; for uniform &lt;code&gt;timestamp&lt;/code&gt; extraction, &lt;code&gt;parse_filesize&lt;/code&gt; for &lt;code&gt;filesize&lt;/code&gt; extraction, &lt;code&gt;parse_count&lt;/code&gt; for count meta fields extraction, &lt;code&gt;parse_resolution&lt;/code&gt;, &lt;code&gt;parse_duration&lt;/code&gt; for &lt;code&gt;duration&lt;/code&gt; extraction, &lt;code&gt;parse_age_limit&lt;/code&gt; for &lt;code&gt;age_limit&lt;/code&gt; extraction.&lt;/p&gt; 
&lt;p&gt;Explore &lt;a href="https://github.com/ytdl-org/youtube-dl/raw/master/youtube_dl/utils.py"&gt;&lt;code&gt;youtube_dl/utils.py&lt;/code&gt;&lt;/a&gt; for more useful convenience functions.&lt;/p&gt; 
&lt;h4&gt;More examples&lt;/h4&gt; 
&lt;h5&gt;Safely extract optional description from parsed JSON&lt;/h5&gt; 
&lt;p&gt;When processing complex JSON, as often returned by site API requests or stashed in web pages for "hydration", you can use the &lt;code&gt;traverse_obj()&lt;/code&gt; utility function to handle multiple fallback values and to ensure the expected type of metadata items. The function's docstring defines how the function works: also review usage in the codebase for more examples.&lt;/p&gt; 
&lt;p&gt;In this example, a text &lt;code&gt;description&lt;/code&gt;, or &lt;code&gt;None&lt;/code&gt;, is pulled from the &lt;code&gt;.result.video[0].summary&lt;/code&gt; member of the parsed JSON &lt;code&gt;response&lt;/code&gt;, if available.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;description = traverse_obj(response, ('result', 'video', 0, 'summary', T(compat_str)))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;T(...)&lt;/code&gt; is a shorthand for a set literal; if you hate people who still run Python 2.6, &lt;code&gt;T(type_or_transformation)&lt;/code&gt; could be written as a set literal &lt;code&gt;{type_or_transformation}&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Some extractors use the older and less capable &lt;code&gt;try_get()&lt;/code&gt; function in the same way.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;description = try_get(response, lambda x: x['result']['video'][0]['summary'], compat_str)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Safely extract more optional metadata&lt;/h5&gt; 
&lt;p&gt;In this example, various optional metadata values are extracted from the &lt;code&gt;.result.video[0]&lt;/code&gt; member of the parsed JSON &lt;code&gt;response&lt;/code&gt;, which is expected to be a JS object, parsed into a &lt;code&gt;dict&lt;/code&gt;, with no crash if that isn't so, or if any of the target values are missing or invalid.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;video = traverse_obj(response, ('result', 'video', 0, T(dict))) or {}
# formerly:
# video = try_get(response, lambda x: x['result']['video'][0], dict) or {}
description = video.get('summary')
duration = float_or_none(video.get('durationMs'), scale=1000)
view_count = int_or_none(video.get('views'))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Safely extract nested lists&lt;/h4&gt; 
&lt;p&gt;Suppose you've extracted JSON like this into a Python data structure named &lt;code&gt;media_json&lt;/code&gt; using, say, the &lt;code&gt;_download_json()&lt;/code&gt; or &lt;code&gt;_parse_json()&lt;/code&gt; methods of &lt;code&gt;InfoExtractor&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
    "title": "Example video",
    "comment": "try extracting this",
    "media": [{
        "type": "bad",
        "size": 320,
        "url": "https://some.cdn.site/bad.mp4"
    }, {
        "type": "streaming",
        "url": "https://some.cdn.site/hls.m3u8"
    }, {
        "type": "super",
        "size": 1280,
        "url": "https://some.cdn.site/good.webm"
    }],
    "moreStuff": "more values",
    ...
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then extractor code like this can collect the various fields of the JSON:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;...
from ..utils import (
    determine_ext,
    int_or_none,
    T,
    traverse_obj,
    txt_or_none,
    url_or_none,
)
...
        ...
        info_dict = {}
        # extract title and description if valid and not empty
        info_dict.update(traverse_obj(media_json, {
            'title': ('title', T(txt_or_none)),
            'description': ('comment', T(txt_or_none)),
        }))

        # extract any recognisable media formats
        fmts = []
        # traverse into "media" list, extract `dict`s with desired keys
        for fmt in traverse_obj(media_json, ('media', Ellipsis, {
                'format_id': ('type', T(txt_or_none)),
                'url': ('url', T(url_or_none)),
                'width': ('size', T(int_or_none)), })):
            # bad `fmt` values were `None` and removed
            if 'url' not in fmt:
                continue
            fmt_url = fmt['url']  # known to be valid URL
            ext = determine_ext(fmt_url)
            if ext == 'm3u8':
                fmts.extend(self._extract_m3u8_formats(fmt_url, video_id, 'mp4', fatal=False))
            else:
                fmt['ext'] = ext
                fmts.append(fmt)

        # sort, raise if no formats
        self._sort_formats(fmts)

        info_dict['formats'] = fmts
        ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The extractor raises an exception rather than random crashes if the JSON structure changes so that no formats are found.&lt;/p&gt; 
&lt;h1&gt;EMBEDDING YOUTUBE-DL&lt;/h1&gt; 
&lt;p&gt;youtube-dl makes the best effort to be a good command-line program, and thus should be callable from any programming language. If you encounter any problems parsing its output, feel free to &lt;a href="https://github.com/ytdl-org/youtube-dl/issues/new"&gt;create a report&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;From a Python program, you can embed youtube-dl in a more powerful fashion, like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from __future__ import unicode_literals
import youtube_dl

ydl_opts = {}
with youtube_dl.YoutubeDL(ydl_opts) as ydl:
    ydl.download(['https://www.youtube.com/watch?v=BaW_jenozKc'])
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Most likely, you'll want to use various options. For a list of options available, have a look at &lt;a href="https://github.com/ytdl-org/youtube-dl/raw/3e4cedf9e8cd3157df2457df7274d0c842421945/youtube_dl/YoutubeDL.py#L137-L312"&gt;&lt;code&gt;youtube_dl/YoutubeDL.py&lt;/code&gt;&lt;/a&gt;. For a start, if you want to intercept youtube-dl's output, set a &lt;code&gt;logger&lt;/code&gt; object.&lt;/p&gt; 
&lt;p&gt;Here's a more complete example of a program that outputs only errors (and a short message after the download is finished), and downloads/converts the video to an mp3 file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from __future__ import unicode_literals
import youtube_dl


class MyLogger(object):
    def debug(self, msg):
        pass

    def warning(self, msg):
        pass

    def error(self, msg):
        print(msg)


def my_hook(d):
    if d['status'] == 'finished':
        print('Done downloading, now converting ...')


ydl_opts = {
    'format': 'bestaudio/best',
    'postprocessors': [{
        'key': 'FFmpegExtractAudio',
        'preferredcodec': 'mp3',
        'preferredquality': '192',
    }],
    'logger': MyLogger(),
    'progress_hooks': [my_hook],
}
with youtube_dl.YoutubeDL(ydl_opts) as ydl:
    ydl.download(['https://www.youtube.com/watch?v=BaW_jenozKc'])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;BUGS&lt;/h1&gt; 
&lt;p&gt;Bugs and suggestions should be reported in the issue tracker: &lt;a href="https://github.com/ytdl-org/youtube-dl/issues"&gt;https://github.com/ytdl-org/youtube-dl/issues&lt;/a&gt; (&lt;a href="https://yt-dl.org/bug"&gt;https://yt-dl.org/bug&lt;/a&gt; is an alias for this). Unless you were prompted to or there is another pertinent reason (e.g. GitHub fails to accept the bug report), please do not send bug reports via personal email. For discussions, join us in the IRC channel &lt;a href="irc://chat.freenode.net/#youtube-dl"&gt;#youtube-dl&lt;/a&gt; on freenode (&lt;a href="https://webchat.freenode.net/?randomnick=1&amp;amp;channels=youtube-dl"&gt;webchat&lt;/a&gt;).&lt;/p&gt; 
&lt;h2&gt;Opening a bug report or suggestion&lt;/h2&gt; 
&lt;p&gt;Be sure to follow instructions provided &lt;strong&gt;below&lt;/strong&gt; and &lt;strong&gt;in the issue tracker&lt;/strong&gt;. Complete the appropriate issue template fully. Consider whether your problem is covered by an existing issue: if so, follow the discussion there. Avoid commenting on existing duplicate issues as such comments do not add to the discussion of the issue and are liable to be treated as spam.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Please include the full output of youtube-dl when run with &lt;code&gt;-v&lt;/code&gt;&lt;/strong&gt;, i.e. &lt;strong&gt;add&lt;/strong&gt; &lt;code&gt;-v&lt;/code&gt; flag to &lt;strong&gt;your command line&lt;/strong&gt;, copy the &lt;strong&gt;whole&lt;/strong&gt; output and post it in the issue body wrapped in ``` for better formatting. It should look similar to this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ youtube-dl -v &amp;lt;your command line&amp;gt;
[debug] System config: []
[debug] User config: []
[debug] Command-line args: [u'-v', u'https://www.youtube.com/watch?v=BaW_jenozKcj']
[debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251
[debug] youtube-dl version 2015.12.06
[debug] Git HEAD: 135392e
[debug] Python version 2.6.6 - Windows-2003Server-5.2.3790-SP2
[debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4
[debug] Proxy map: {}
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Do not post screenshots of verbose logs; only plain text is acceptable.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The output (including the first lines) contains important debugging information. Issues without the full output are often not reproducible and therefore do not get solved in short order, if ever.&lt;/p&gt; 
&lt;p&gt;Finally please review your issue to avoid various common mistakes (you can and should use this as a checklist) listed below.&lt;/p&gt; 
&lt;h3&gt;Is the description of the issue itself sufficient?&lt;/h3&gt; 
&lt;p&gt;We often get issue reports that are hard to understand. To avoid subsequent clarifications, and to assist participants who are not native English speakers, please elaborate on what feature you are requesting, or what bug you want to be fixed.&lt;/p&gt; 
&lt;p&gt;Make sure that it's obvious&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;What the problem is&lt;/li&gt; 
 &lt;li&gt;How it could be fixed&lt;/li&gt; 
 &lt;li&gt;How your proposed solution would look&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If your report is shorter than two lines, it is almost certainly missing some of these, which makes it hard for us to respond to it. We're often too polite to close the issue outright, but the missing info makes misinterpretation likely. As a committer myself, I often get frustrated by these issues, since the only possible way for me to move forward on them is to ask for clarification over and over.&lt;/p&gt; 
&lt;p&gt;For bug reports, this means that your report should contain the &lt;em&gt;complete&lt;/em&gt; output of youtube-dl when called with the &lt;code&gt;-v&lt;/code&gt; flag. The error message you get for (most) bugs even says so, but you would not believe how many of our bug reports do not contain this information.&lt;/p&gt; 
&lt;p&gt;If your server has multiple IPs or you suspect censorship, adding &lt;code&gt;--call-home&lt;/code&gt; may be a good idea to get more diagnostics. If the error is &lt;code&gt;ERROR: Unable to extract ...&lt;/code&gt; and you cannot reproduce it from multiple countries, add &lt;code&gt;--dump-pages&lt;/code&gt; (warning: this will yield a rather large output, redirect it to the file &lt;code&gt;log.txt&lt;/code&gt; by adding &lt;code&gt;&amp;gt;log.txt 2&amp;gt;&amp;amp;1&lt;/code&gt; to your command-line) or upload the &lt;code&gt;.dump&lt;/code&gt; files you get when you add &lt;code&gt;--write-pages&lt;/code&gt; &lt;a href="https://gist.github.com/"&gt;somewhere&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Site support requests must contain an example URL&lt;/strong&gt;. An example URL is a URL you might want to download, like &lt;code&gt;https://www.youtube.com/watch?v=BaW_jenozKc&lt;/code&gt;. There should be an obvious video present. Except under very special circumstances, the main page of a video service (e.g. &lt;code&gt;https://www.youtube.com/&lt;/code&gt;) is &lt;em&gt;not&lt;/em&gt; an example URL.&lt;/p&gt; 
&lt;h3&gt;Is the issue already documented?&lt;/h3&gt; 
&lt;p&gt;Make sure that someone has not already opened the issue you're trying to open. Search at the top of the window or browse the &lt;a href="https://github.com/ytdl-org/youtube-dl/search?type=Issues"&gt;GitHub Issues&lt;/a&gt; of this repository. Initially, at least, use the search term &lt;code&gt;-label:duplicate&lt;/code&gt; to focus on active issues. If there is an issue, feel free to write something along the lines of "This affects me as well, with version 2015.01.01. Here is some more information on the issue: ...". While some issues may be old, a new post into them often spurs rapid activity.&lt;/p&gt; 
&lt;h3&gt;Are you using the latest version?&lt;/h3&gt; 
&lt;p&gt;Before reporting any issue, type &lt;code&gt;youtube-dl -U&lt;/code&gt;. This should report that you're up-to-date. About 20% of the reports we receive are already fixed, but people are using outdated versions. This goes for feature requests as well.&lt;/p&gt; 
&lt;h3&gt;Why are existing options not enough?&lt;/h3&gt; 
&lt;p&gt;Before requesting a new feature, please have a quick peek at &lt;a href="https://github.com/ytdl-org/youtube-dl/raw/master/README.md#options"&gt;the list of supported options&lt;/a&gt;. Many feature requests are for features that actually exist already! Please, absolutely do show off your work in the issue report and detail how the existing similar options do &lt;em&gt;not&lt;/em&gt; solve your problem.&lt;/p&gt; 
&lt;h3&gt;Is there enough context in your bug report?&lt;/h3&gt; 
&lt;p&gt;People want to solve problems, and often think they do us a favor by breaking down their larger problems (e.g. wanting to skip already downloaded files) to a specific request (e.g. requesting us to look whether the file exists before downloading the info page). However, what often happens is that they break down the problem into two steps: One simple, and one impossible (or extremely complicated one).&lt;/p&gt; 
&lt;p&gt;We are then presented with a very complicated request when the original problem could be solved far easier, e.g. by recording the downloaded video IDs in a separate file. To avoid this, you must include the greater context where it is non-obvious. In particular, every feature request that does not consist of adding support for a new site should contain a use case scenario that explains in what situation the missing feature would be useful.&lt;/p&gt; 
&lt;h3&gt;Does the issue involve one problem, and one problem only?&lt;/h3&gt; 
&lt;p&gt;Some of our users seem to think there is a limit of issues they can or should open. There is no limit of issues they can or should open. While it may seem appealing to be able to dump all your issues into one ticket, that means that someone who solves one of your issues cannot mark the issue as closed. Typically, reporting a bunch of issues leads to the ticket lingering since nobody wants to attack that behemoth, until someone mercifully splits the issue into multiple ones.&lt;/p&gt; 
&lt;p&gt;In particular, every site support request issue should only pertain to services at one site (generally under a common domain, but always using the same backend technology). Do not request support for vimeo user videos, White house podcasts, and Google Plus pages in the same issue. Also, make sure that you don't post bug reports alongside feature requests. As a rule of thumb, a feature request does not include outputs of youtube-dl that are not immediately related to the feature at hand. Do not post reports of a network error alongside the request for a new video service.&lt;/p&gt; 
&lt;h3&gt;Is anyone going to need the feature?&lt;/h3&gt; 
&lt;p&gt;Only post features that you (or an incapacitated friend you can personally talk to) require. Do not post features because they seem like a good idea. If they are really useful, they will be requested by someone who requires them.&lt;/p&gt; 
&lt;h3&gt;Is your question about youtube-dl?&lt;/h3&gt; 
&lt;p&gt;It may sound strange, but some bug reports we receive are completely unrelated to youtube-dl and relate to a different, or even the reporter's own, application. Please make sure that you are actually using youtube-dl. If you are using a UI for youtube-dl, report the bug to the maintainer of the actual application providing the UI. On the other hand, if your UI for youtube-dl fails in some way you believe is related to youtube-dl, by all means, go ahead and report the bug.&lt;/p&gt; 
&lt;h1&gt;COPYRIGHT&lt;/h1&gt; 
&lt;p&gt;youtube-dl is released into the public domain by the copyright holders.&lt;/p&gt; 
&lt;p&gt;This README file was originally written by &lt;a href="https://github.com/dbbolton"&gt;Daniel Bolton&lt;/a&gt; and is likewise released into the public domain.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>