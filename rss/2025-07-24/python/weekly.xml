<rss version="2.0">
  <channel>
    <title>GitHub Python Weekly Trending</title>
    <description>Weekly Trending of Python in GitHub</description>
    <pubDate>Wed, 23 Jul 2025 01:43:13 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>landing-ai/agentic-doc</title>
      <link>https://github.com/landing-ai/agentic-doc</link>
      <description>&lt;p&gt;Python library for Agentic Document Extraction from LandingAI&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;Agentic&amp;nbsp;Document&amp;nbsp;Extraction ‚Äì Python&amp;nbsp;Library&lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://github.com/landing-ai/agentic-doc/actions/workflows/ci-unit.yml"&gt;&lt;img src="https://github.com/landing-ai/agentic-doc/actions/workflows/ci-unit.yml/badge.svg?sanitize=true" alt="Unit test status"&gt;&lt;/a&gt; &lt;a href="https://github.com/landing-ai/agentic-doc/actions/workflows/ci-integ.yml"&gt;&lt;img src="https://github.com/landing-ai/agentic-doc/actions/workflows/ci-integ.yml/badge.svg?sanitize=true" alt="Integration test status"&gt;&lt;/a&gt; &lt;a href="https://discord.gg/RVcW3j9RgR"&gt;&lt;img src="https://dcbadge.vercel.app/api/server/wPdN8RCYew?compact=true&amp;amp;style=flat" alt=""&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/agentic-doc"&gt;&lt;img src="https://badge.fury.io/py/agentic-doc.svg?sanitize=true" alt="PyPI version"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://va.landing.ai/demo/doc-extraction"&gt;Web App&lt;/a&gt;&amp;nbsp;¬∑ &lt;a href="https://discord.com/invite/RVcW3j9RgR"&gt;Discord&lt;/a&gt;&amp;nbsp;¬∑ &lt;a href="https://landing.ai/blog/going-beyond-ocrllm-introducing-agentic-document-extraction"&gt;Blog&lt;/a&gt;&amp;nbsp;¬∑ &lt;a href="https://support.landing.ai/docs/document-extraction"&gt;Docs&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;The LandingAI &lt;strong&gt;Agentic&amp;nbsp;Document&amp;nbsp;Extraction&lt;/strong&gt; API pulls structured data out of visually complex documents‚Äîthink tables, pictures, and charts‚Äîand returns a hierarchical JSON with exact element locations.&lt;/p&gt; 
&lt;p&gt;This Python library wraps that API to provide:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Long‚Äëdocument support&lt;/strong&gt; ‚Äì process 100+&amp;nbsp;page PDFs in a single call&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Auto‚Äëretry / paging&lt;/strong&gt; ‚Äì handles concurrency, time‚Äëouts, and rate limits&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Helper utilities&lt;/strong&gt; ‚Äì bounding‚Äëbox snippets, visual debuggers, and more&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üì¶ &lt;strong&gt;Batteries‚Äëincluded install:&lt;/strong&gt; &lt;code&gt;pip install agentic-doc&lt;/code&gt; ‚Äì nothing else needed ‚Üí see&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/landing-ai/agentic-doc/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üóÇÔ∏è &lt;strong&gt;All file types:&lt;/strong&gt; parse PDFs of &lt;em&gt;any&lt;/em&gt; length, single images, or URLs ‚Üí see&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/landing-ai/agentic-doc/main/#supported-files"&gt;Supported&amp;nbsp;Files&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìö &lt;strong&gt;Long‚Äëdoc ready:&lt;/strong&gt; auto‚Äësplit&amp;nbsp;&amp;amp;&amp;nbsp;parallel‚Äëprocess 1000+&amp;nbsp;page PDFs, then stitch results ‚Üí see&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/landing-ai/agentic-doc/main/#parse-large-pdf-files"&gt;Parse&amp;nbsp;Large&amp;nbsp;PDF&amp;nbsp;Files&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üß© &lt;strong&gt;Structured output:&lt;/strong&gt; returns hierarchical JSON plus ready‚Äëto‚Äërender Markdown ‚Üí see&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/landing-ai/agentic-doc/main/#result-schema"&gt;Result&amp;nbsp;Schema&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üëÅÔ∏è &lt;strong&gt;Ground‚Äëtruth visuals:&lt;/strong&gt; optional bounding‚Äëbox snippets and full‚Äëpage visualizations ‚Üí see&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/landing-ai/agentic-doc/main/#save-groundings-as-images"&gt;Save&amp;nbsp;Groundings&amp;nbsp;as&amp;nbsp;Images&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üèÉ &lt;strong&gt;Batch&amp;nbsp;&amp;amp;&amp;nbsp;parallel:&lt;/strong&gt; feed a list; library manages threads&amp;nbsp;&amp;amp;&amp;nbsp;rate limits (&lt;code&gt;BATCH_SIZE&lt;/code&gt;, &lt;code&gt;MAX_WORKERS&lt;/code&gt;) ‚Üí see&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/landing-ai/agentic-doc/main/#parse-multiple-files-in-a-batch"&gt;Parse&amp;nbsp;Multiple&amp;nbsp;Files&amp;nbsp;in&amp;nbsp;a&amp;nbsp;Batch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üîÑ &lt;strong&gt;Resilient:&lt;/strong&gt; exponential‚Äëbackoff retries for 408/429/502/503/504 and rate‚Äëlimit hits ‚Üí see&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/landing-ai/agentic-doc/main/#automatically-handle-api-errors-and-rate-limits-with-retries"&gt;Automatically&amp;nbsp;Handle&amp;nbsp;API&amp;nbsp;Errors&amp;nbsp;and&amp;nbsp;Rate&amp;nbsp;Limits&amp;nbsp;with&amp;nbsp;Retries&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üõ†Ô∏è &lt;strong&gt;Drop‚Äëin helpers:&lt;/strong&gt; &lt;code&gt;parse_documents&lt;/code&gt;, &lt;code&gt;parse_and_save_documents&lt;/code&gt;, &lt;code&gt;parse_and_save_document&lt;/code&gt; ‚Üí see&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/landing-ai/agentic-doc/main/#main-functions"&gt;Main&amp;nbsp;Functions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;‚öôÔ∏è &lt;strong&gt;Config via env / .env:&lt;/strong&gt; tweak parallelism, logging style, retry caps‚Äîno code changes ‚Üí see&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/landing-ai/agentic-doc/main/#configuration-options"&gt;Configuration&amp;nbsp;Options&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üåê &lt;strong&gt;Raw API ready:&lt;/strong&gt; advanced users can still hit the REST endpoint directly ‚Üí see&amp;nbsp;the&amp;nbsp;&lt;a href="https://support.landing.ai/docs/document-extraction"&gt;API&amp;nbsp;Docs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install agentic-doc
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python version 3.9, 3.10, 3.11 or 3.12&lt;/li&gt; 
 &lt;li&gt;LandingAI agentic AI API key (get the key &lt;a href="https://va.landing.ai/settings/api-key"&gt;here&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Set the API Key as an Environment Variable&lt;/h3&gt; 
&lt;p&gt;After you get the LandingAI agentic AI API key, set the key as an environment variable (or put it in a &lt;code&gt;.env&lt;/code&gt; file):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export VISION_AGENT_API_KEY=&amp;lt;your-api-key&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Supported Files&lt;/h3&gt; 
&lt;p&gt;The library can extract data from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PDFs (any length)&lt;/li&gt; 
 &lt;li&gt;Images that are supported by OpenCV-Python (i.e. the &lt;code&gt;cv2&lt;/code&gt; library)&lt;/li&gt; 
 &lt;li&gt;URLs pointing to PDF or image files&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Basic Usage&lt;/h3&gt; 
&lt;h4&gt;Extract Data from One Document&lt;/h4&gt; 
&lt;p&gt;Run the following script to extract data from one document and return the results in both markdown and structured chunks.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agentic_doc.parse import parse

# Parse a local file
result = parse("path/to/image.png")
print(result[0].markdown)  # Get the extracted data as markdown
print(result[0].chunks)  # Get the extracted data as structured chunks of content

# Parse a document from a URL
result = parse("https://example.com/document.pdf")
print(result[0].markdown)

# Legacy approach (still supported)
from agentic_doc.parse import parse_documents
results = parse_documents(["path/to/image.png"])
parsed_doc = results[0]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Extract Data from Multiple Documents&lt;/h4&gt; 
&lt;p&gt;Run the following script to extract data from multiple documents.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agentic_doc.parse import parse

# Parse multiple local files
file_paths = ["path/to/your/document1.pdf", "path/to/another/document2.pdf"]
results = parse(file_paths)
for result in results:
    print(result.markdown)

# Parse and save results to a directory
results = parse(file_paths, result_save_dir="path/to/save/results")
result_paths = []
for result in results:
    result_paths.append(result.result_path)
# result_paths: ["path/to/save/results/document1_20250313_070305.json", ...]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Using field extraction&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from pydantic import BaseModel, Field
from agentic_doc.parse import parse

class ExtractedFields(BaseModel):
    employee_name: str = Field(description="the full name of the employee")
    employee_ssn: str = Field(description="the social security number of the employee")
    gross_pay: float = Field(description="the gross pay of the employee")
    employee_address: str = Field(description="the address of the employee")

results = parse("mydoc.pdf", extraction_model=ExtractedFields)
fields = results[0].extraction
metadata = results[0].extraction_metadata
print(f"Field value: {fields.employee_name}, confidence: {metadata.employee_name.confidence}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Extract Data Using Connectors&lt;/h4&gt; 
&lt;p&gt;The library now supports various connectors to easily access documents from different sources:&lt;/p&gt; 
&lt;h5&gt;Google Drive Connector&lt;/h5&gt; 
&lt;p&gt;&lt;strong&gt;Prerequisites: Follow the &lt;a href="https://developers.google.com/workspace/drive/api/quickstart/python"&gt;Google Drive API Python Quickstart&lt;/a&gt; tutorial first to set up your credentials.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The Google Drive API quickstart will guide you through:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Creating a Google Cloud project&lt;/li&gt; 
 &lt;li&gt;Enabling the Google Drive API&lt;/li&gt; 
 &lt;li&gt;Setting up OAuth 2.0 credentials&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;After completing the quickstart tutorial, you can use the Google Drive connector as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agentic_doc.parse import parse
from agentic_doc.connectors import GoogleDriveConnectorConfig

# Using OAuth credentials file (from quickstart tutorial)
config = GoogleDriveConnectorConfig(
    client_secret_file="path/to/credentials.json",
    folder_id="your-google-drive-folder-id"  # Optional
)

# Parse all documents in the folder
results = parse(config)

# Parse with filtering
results = parse(config, connector_pattern="*.pdf")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Amazon S3 Connector&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agentic_doc.parse import parse
from agentic_doc.connectors import S3ConnectorConfig

config = S3ConnectorConfig(
    bucket_name="your-bucket-name",
    aws_access_key_id="your-access-key",  # Optional if using IAM roles
    aws_secret_access_key="your-secret-key",  # Optional if using IAM roles
    region_name="us-east-1"
)

# Parse all documents in the bucket
results = parse(config)

# Parse documents in a specific prefix/folder
results = parse(config, connector_path="documents/")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Local Directory Connector&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agentic_doc.parse import parse
from agentic_doc.connectors import LocalConnectorConfig

config = LocalConnectorConfig()

# Parse all supported documents in a directory
results = parse(config, connector_path="/path/to/documents")

# Parse with pattern filtering
results = parse(config, connector_path="/path/to/documents", connector_pattern="*.pdf")

# Parse all supported documents in a directory recursively (search subdirectories as well)
config = LocalConnectorConfig(recursive=True)
results = parse(config, connector_path="/path/to/documents")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;URL Connector&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agentic_doc.parse import parse
from agentic_doc.connectors import URLConnectorConfig

config = URLConnectorConfig(
    headers={"Authorization": "Bearer your-token"},  # Optional
    timeout=60  # Optional
)

# Parse document from URL
results = parse(config, connector_path="https://example.com/document.pdf")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Raw Bytes Input&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agentic_doc.parse import parse

# Load a PDF or image file as bytes
with open("document.pdf", "rb") as f:
    raw_bytes = f.read()

# Parse the document from bytes
results = parse(raw_bytes)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also parse image bytes:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;with open("image.png", "rb") as f:
    image_bytes = f.read()

results = parse(image_bytes)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This is useful when documents are already loaded into memory (e.g., from an API response or uploaded via a web interface). The parser will auto-detect the file type from the bytes.&lt;/p&gt; 
&lt;h2&gt;Why Use It?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Simplified Setup:&lt;/strong&gt; No need to manage API keys or handle low-level REST calls.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic Large File Processing:&lt;/strong&gt; Splits large PDFs into manageable parts and processes them in parallel.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Built-In Error Handling:&lt;/strong&gt; Automatically retries requests with exponential backoff and jitter for common HTTP errors.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Parallel Processing:&lt;/strong&gt; Efficiently parse multiple documents at once with configurable parallelism.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Main Features&lt;/h2&gt; 
&lt;p&gt;With this library, you can do things that are otherwise hard to do with the Agentic Document Extraction API alone. This section describes some of the key features this library offers.&lt;/p&gt; 
&lt;h3&gt;Parse Large PDF Files&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;A single REST API call can only handle up to certain amount of pages at a time&lt;/strong&gt; (see &lt;a href="https://docs.landing.ai/ade/ade-rate-limits#maximum-pages-per-document"&gt;rate limits&lt;/a&gt;). This library automatically splits a large PDF into multiple calls, uses a thread pool to process the calls in parallel, and stitches the results back together as a single result.&lt;/p&gt; 
&lt;p&gt;We've used this library to successfully parse PDFs that are 1000+ pages long.&lt;/p&gt; 
&lt;h3&gt;Parse Multiple Files in a Batch&lt;/h3&gt; 
&lt;p&gt;You can parse multiple files in a single function call with this library. The library processes files in parallel.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; You can change the parallelism by setting the &lt;code&gt;batch_size&lt;/code&gt; setting.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Save Groundings as Images&lt;/h3&gt; 
&lt;p&gt;The library can extract and save the visual regions (groundings) of the document where each chunk of content was found. This is useful for visualizing exactly what parts of the document were extracted and for debugging extraction issues.&lt;/p&gt; 
&lt;p&gt;Each grounding represents a bounding box in the original document, and the library can save these regions as individual PNG images. The images are organized by page number and chunk ID.&lt;/p&gt; 
&lt;p&gt;Here's how to use this feature:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agentic_doc.parse import parse_documents

# Save groundings when parsing a document
results = parse_documents(
    ["path/to/document.pdf"],
    grounding_save_dir="path/to/save/groundings"
)

# The grounding images will be saved to:
# path/to/save/groundings/document_TIMESTAMP/page_X/CHUNK_TYPE_CHUNK_ID_Y.png
# Where X is the page number, CHUNK_ID is the unique ID of each chunk,
# and Y is the index of the grounding within the chunk

# Each chunk's grounding in the result will have the image_path set
for chunk in results[0].chunks:
    for grounding in chunk.grounding:
        if grounding.image_path:
            print(f"Grounding saved to: {grounding.image_path}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This feature works with all parsing functions: &lt;code&gt;parse_documents&lt;/code&gt;, &lt;code&gt;parse_and_save_documents&lt;/code&gt;, and &lt;code&gt;parse_and_save_document&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Visualize Parsing Result&lt;/h3&gt; 
&lt;p&gt;The library provides a visualization utility that creates annotated images showing where each chunk of content was extracted from the document. This is useful for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Verifying the accuracy of the extraction&lt;/li&gt; 
 &lt;li&gt;Debugging extraction issues&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Here's how to use the visualization feature:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agentic_doc.parse import parse
from agentic_doc.utils import viz_parsed_document
from agentic_doc.config import VisualizationConfig

# Parse a document
results = parse("path/to/document.pdf")
parsed_doc = results[0]

# Create visualizations with default settings
# The output images have a PIL.Image.Image type
images = viz_parsed_document(
    "path/to/document.pdf",
    parsed_doc,
    output_dir="path/to/save/visualizations"
)

# Or customize the visualization appearance
viz_config = VisualizationConfig(
    thickness=2,  # Thicker bounding boxes
    text_bg_opacity=0.8,  # More opaque text background
    font_scale=0.7,  # Larger text
    # Custom colors for different chunk types
    color_map={
        ChunkType.TITLE: (0, 0, 255),  # Red for titles
        ChunkType.TEXT: (255, 0, 0),  # Blue for regular text
        # ... other chunk types ...
    }
)

images = viz_parsed_document(
    "path/to/document.pdf",
    parsed_doc,
    output_dir="path/to/save/visualizations",
    viz_config=viz_config
)

# The visualization images will be saved as:
# path/to/save/visualizations/document_viz_page_X.png
# Where X is the page number
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The visualization shows:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Bounding boxes around each extracted chunk&lt;/li&gt; 
 &lt;li&gt;Chunk type and index labels&lt;/li&gt; 
 &lt;li&gt;Different colors for different types of content (titles, text, tables, etc.)&lt;/li&gt; 
 &lt;li&gt;Semi-transparent text backgrounds for better readability&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Automatically Handle API Errors and Rate Limits with Retries&lt;/h3&gt; 
&lt;p&gt;The REST API endpoint imposes rate limits per API key. This library automatically handles the rate limit error or other intermittent HTTP errors with retries.&lt;/p&gt; 
&lt;p&gt;For more information, see &lt;a href="https://raw.githubusercontent.com/landing-ai/agentic-doc/main/#error-handling"&gt;Error Handling&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/landing-ai/agentic-doc/main/#configuration-options"&gt;Configuration Options&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Error Handling&lt;/h3&gt; 
&lt;p&gt;This library implements a retry mechanism for handling API failures:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Retries are performed for these HTTP status codes: 408, 429, 502, 503, 504.&lt;/li&gt; 
 &lt;li&gt;Exponential backoff with jitter is used for retry wait time.&lt;/li&gt; 
 &lt;li&gt;The initial retry wait time is 1 second, which increases exponentially.&lt;/li&gt; 
 &lt;li&gt;Retry will stop after &lt;code&gt;max_retries&lt;/code&gt; attempts. Exceeding the limit raises an exception and results in a failure for this request.&lt;/li&gt; 
 &lt;li&gt;Retry wait time is capped at &lt;code&gt;max_retry_wait_time&lt;/code&gt; seconds.&lt;/li&gt; 
 &lt;li&gt;Retries include a random jitter of up to 10 seconds to distribute requests and prevent the thundering herd problem.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Parsing Errors&lt;/h3&gt; 
&lt;p&gt;If the REST API request encounters an unrecoverable error during parsing (either from client-side or server-side), the library includes an &lt;a href="https://raw.githubusercontent.com/landing-ai/agentic-doc/main/agentic_doc/common.py#L75"&gt;errors&lt;/a&gt; field in the final result for the affected page(s). Each error contains the error message, error_code and corresponding page number.&lt;/p&gt; 
&lt;h2&gt;Configuration Options&lt;/h2&gt; 
&lt;p&gt;The library uses a &lt;a href="https://raw.githubusercontent.com/landing-ai/agentic-doc/main/agentic_doc/config.py"&gt;&lt;code&gt;Settings&lt;/code&gt;&lt;/a&gt; object to manage configuration. You can customize these settings either through environment variables or a &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;p&gt;Below is an example &lt;code&gt;.env&lt;/code&gt; file that customizes the configurations:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Number of files to process in parallel, defaults to 4
BATCH_SIZE=4
# Number of threads used to process parts of each file in parallel, defaults to 5.
MAX_WORKERS=2
# Maximum number of retry attempts for failed intermittent requests, defaults to 100
MAX_RETRIES=80
# Maximum wait time in seconds for each retry, defaults to 60
MAX_RETRY_WAIT_TIME=30
# Logging style for retry, defaults to log_msg
RETRY_LOGGING_STYLE=log_msg
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Max Parallelism&lt;/h3&gt; 
&lt;p&gt;The maximum number of parallel requests is determined by multiplying &lt;code&gt;BATCH_SIZE&lt;/code&gt; √ó &lt;code&gt;MAX_WORKERS&lt;/code&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; The maximum parallelism allowed by this library is 100.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Specifically, increasing &lt;code&gt;MAX_WORKERS&lt;/code&gt; can speed up the processing of large individual files, while increasing &lt;code&gt;BATCH_SIZE&lt;/code&gt; improves throughput when processing multiple files.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; Your job's maximum processing throughput may be limited by your API rate limit. If your rate limit isn't high enough, you may encounter rate limit errors, which the library will automatically handle through retries.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;The optimal values for &lt;code&gt;MAX_WORKERS&lt;/code&gt; and &lt;code&gt;BATCH_SIZE&lt;/code&gt; depend on your API rate limit and the latency of each REST API call. For example, if your account has a rate limit of 5 requests per minute, and each REST API call takes approximately 60 seconds to complete, and you're processing a single large file, then &lt;code&gt;MAX_WORKERS&lt;/code&gt; should be set to 5 and &lt;code&gt;BATCH_SIZE&lt;/code&gt; to 1.&lt;/p&gt; 
&lt;p&gt;You can find your REST API latency in the logs. If you want to increase your rate limit, schedule a time to meet with us &lt;a href="https://scheduler.zoom.us/d/56i81uc2/landingai-document-extraction"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Set &lt;code&gt;RETRY_LOGGING_STYLE&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;RETRY_LOGGING_STYLE&lt;/code&gt; setting controls how the library logs the retry attempts.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;log_msg&lt;/code&gt;: Log the retry attempts as a log messages. Each attempt is logged as a separate message. This is the default setting.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;inline_block&lt;/code&gt;: Print a yellow progress block ('‚ñà') on the same line. Each block represents one retry attempt. Choose this if you don't want to see the verbose retry logging message and still want to track the number of retries that have been made.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;none&lt;/code&gt;: Do not log the retry attempts.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Troubleshooting &amp;amp; FAQ&lt;/h2&gt; 
&lt;h3&gt;Common Issues&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;API Key Errors:&lt;/strong&gt; Ensure your API key is correctly set as an environment variable.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Rate Limits:&lt;/strong&gt; The library automatically retries requests if you hit the API rate limit. Adjust &lt;code&gt;BATCH_SIZE&lt;/code&gt; or &lt;code&gt;MAX_WORKERS&lt;/code&gt; if you encounter frequent rate limit errors.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Parsing Failures:&lt;/strong&gt; If a document fails to parse, an error chunk will be included in the result, detailing the error message and page index.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;URL Access Issues:&lt;/strong&gt; If you're having trouble accessing documents from URLs, check that the URLs are publicly accessible and point to supported file types (PDF or images).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Note on &lt;code&gt;include_marginalia&lt;/code&gt; and &lt;code&gt;include_metadata_in_markdown&lt;/code&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;include_marginalia&lt;/code&gt;: If True, the parser will attempt to extract and include marginalia (footer notes, page number, etc.) from the document in the output.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;include_metadata_in_markdown&lt;/code&gt;: If True, the output markdown will include metadata.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Both parameters default to True. You can set them to False to exclude these elements from the output.&lt;/p&gt; 
&lt;h4&gt;Example: Using the new parameters&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agentic_doc.parse import parse

results = parse(
    "path/to/document.pdf",
    include_marginalia=False,  # Exclude marginalia from output
    include_metadata_in_markdown=False  # Exclude metadata from markdown
)
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>langchain-ai/open_deep_research</title>
      <link>https://github.com/langchain-ai/open_deep_research</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Open Deep Research&lt;/h1&gt; 
&lt;img width="1388" height="298" alt="full_diagram" src="https://github.com/user-attachments/assets/12a2371b-8be2-4219-9b48-90503eb43c69"&gt; 
&lt;p&gt;Deep research has broken out as one of the most popular agent applications. This is a simple, configurable, fully open source deep research agent that works across many model providers, search tools, and MCP servers.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Read more in our &lt;a href="https://blog.langchain.com/open-deep-research/"&gt;blog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;See our &lt;a href="https://www.youtube.com/watch?v=agGiWUpxkhg"&gt;video&lt;/a&gt; for a quick overview&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üöÄ Quickstart&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone the repository and activate a virtual environment:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/langchain-ai/open_deep_research.git
cd open_deep_research
uv venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install dependencies:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv pip install -r pyproject.toml
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Set up your &lt;code&gt;.env&lt;/code&gt; file to customize the environment variables (for model selection, search tools, and other configuration settings):&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cp .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Launch the assistant with the LangGraph server locally to open LangGraph Studio in your browser:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install dependencies and start the LangGraph server
uvx --refresh --from "langgraph-cli[inmem]" --with-editable . --python 3.11 langgraph dev --allow-blocking
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Use this to open the Studio UI:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;- üöÄ API: http://127.0.0.1:2024
- üé® Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024
- üìö API Docs: http://127.0.0.1:2024/docs
&lt;/code&gt;&lt;/pre&gt; 
&lt;img width="817" height="666" alt="Screenshot 2025-07-13 at 11 21 12‚ÄØPM" src="https://github.com/user-attachments/assets/052f2ed3-c664-4a4f-8ec2-074349dcaa3f"&gt; 
&lt;p&gt;Ask a question in the &lt;code&gt;messages&lt;/code&gt; input field and click &lt;code&gt;Submit&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Configurations&lt;/h3&gt; 
&lt;p&gt;Open Deep Research offers extensive configuration options to customize the research process and model behavior. All configurations can be set via the web UI, environment variables, or by modifying the configuration directly.&lt;/p&gt; 
&lt;h4&gt;General Settings&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Max Structured Output Retries&lt;/strong&gt; (default: 3): Maximum number of retries for structured output calls from models when parsing fails&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Allow Clarification&lt;/strong&gt; (default: true): Whether to allow the researcher to ask clarifying questions before starting research&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Max Concurrent Research Units&lt;/strong&gt; (default: 5): Maximum number of research units to run concurrently using sub-agents. Higher values enable faster research but may hit rate limits&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Research Configuration&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Search API&lt;/strong&gt; (default: Tavily): Choose from Tavily (works with all models), OpenAI Native Web Search, Anthropic Native Web Search, or None&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Max Researcher Iterations&lt;/strong&gt; (default: 3): Number of times the Research Supervisor will reflect on research and ask follow-up questions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Max React Tool Calls&lt;/strong&gt; (default: 5): Maximum number of tool calling iterations in a single researcher step&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Models&lt;/h4&gt; 
&lt;p&gt;Open Deep Research uses multiple specialized models for different research tasks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Summarization Model&lt;/strong&gt; (default: &lt;code&gt;openai:gpt-4.1-nano&lt;/code&gt;): Summarizes research results from search APIs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Research Model&lt;/strong&gt; (default: &lt;code&gt;openai:gpt-4.1&lt;/code&gt;): Conducts research and analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Compression Model&lt;/strong&gt; (default: &lt;code&gt;openai:gpt-4.1-mini&lt;/code&gt;): Compresses research findings from sub-agents&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Final Report Model&lt;/strong&gt; (default: &lt;code&gt;openai:gpt-4.1&lt;/code&gt;): Writes the final comprehensive report&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;All models are configured using &lt;a href="https://python.langchain.com/docs/how_to/chat_models_universal_init/"&gt;init_chat_model() API&lt;/a&gt; which supports providers like OpenAI, Anthropic, Google Vertex AI, and others.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Important Model Requirements:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Structured Outputs&lt;/strong&gt;: All models must support structured outputs. Check support &lt;a href="https://python.langchain.com/docs/integrations/chat/"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Search API Compatibility&lt;/strong&gt;: Research and Compression models must support your selected search API:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Anthropic search requires Anthropic models with web search capability&lt;/li&gt; 
   &lt;li&gt;OpenAI search requires OpenAI models with web search capability&lt;/li&gt; 
   &lt;li&gt;Tavily works with all models&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tool Calling&lt;/strong&gt;: All models must support tool calling functionality&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Special Configurations&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;For OpenRouter: Follow &lt;a href="https://github.com/langchain-ai/open_deep_research/issues/75#issuecomment-2811472408"&gt;this guide&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;For local models via Ollama: See &lt;a href="https://github.com/langchain-ai/open_deep_research/issues/65#issuecomment-2743586318"&gt;setup instructions&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Example MCP (Model Context Protocol) Servers&lt;/h4&gt; 
&lt;p&gt;Open Deep Research supports MCP servers to extend research capabilities.&lt;/p&gt; 
&lt;h4&gt;Local MCP Servers&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Filesystem MCP Server&lt;/strong&gt; provides secure file system operations with robust access control:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Read, write, and manage files and directories&lt;/li&gt; 
 &lt;li&gt;Perform operations like reading file contents, creating directories, moving files, and searching&lt;/li&gt; 
 &lt;li&gt;Restrict operations to predefined directories for security&lt;/li&gt; 
 &lt;li&gt;Support for both command-line configuration and dynamic MCP roots&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Example usage:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mcp-server-filesystem /path/to/allowed/dir1 /path/to/allowed/dir2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Remote MCP Servers&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Remote MCP servers&lt;/strong&gt; enable distributed agent coordination and support streamable HTTP requests. Unlike local servers, they can be multi-tenant and require more complex authentication.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Arcade MCP Server Example&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "url": "https://api.arcade.dev/v1/mcps/ms_0ujssxh0cECutqzMgbtXSGnjorm",
  "tools": ["Search_SearchHotels", "Search_SearchOneWayFlights", "Search_SearchRoundtripFlights"]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Remote servers can be configured as authenticated or unauthenticated and support JWT-based authentication through OAuth endpoints.&lt;/p&gt; 
&lt;h3&gt;Evaluation&lt;/h3&gt; 
&lt;p&gt;A comprehensive batch evaluation system designed for detailed analysis and comparative studies.&lt;/p&gt; 
&lt;h4&gt;&lt;strong&gt;Features:&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-dimensional Scoring&lt;/strong&gt;: Specialized evaluators with 0-1 scale ratings&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dataset-driven Evaluation&lt;/strong&gt;: Batch processing across multiple test cases&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;strong&gt;Usage:&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run comprehensive evaluation on LangSmith datasets
python tests/run_evaluate.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;&lt;strong&gt;Key Files:&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;tests/run_evaluate.py&lt;/code&gt;: Main evaluation script&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tests/evaluators.py&lt;/code&gt;: Specialized evaluator functions&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tests/prompts.py&lt;/code&gt;: Evaluation prompts for each dimension&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Deployments and Usages&lt;/h3&gt; 
&lt;h4&gt;LangGraph Studio&lt;/h4&gt; 
&lt;p&gt;Follow the &lt;a href="https://raw.githubusercontent.com/langchain-ai/open_deep_research/main/#-quickstart"&gt;quickstart&lt;/a&gt; to start LangGraph server locally and test the agent out on LangGraph Studio.&lt;/p&gt; 
&lt;h4&gt;Hosted deployment&lt;/h4&gt; 
&lt;p&gt;You can easily deploy to &lt;a href="https://langchain-ai.github.io/langgraph/concepts/#deployment-options"&gt;LangGraph Platform&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Open Agent Platform&lt;/h4&gt; 
&lt;p&gt;Open Agent Platform (OAP) is a UI from which non-technical users can build and configure their own agents. OAP is great for allowing users to configure the Deep Researcher with different MCP tools and search APIs that are best suited to their needs and the problems that they want to solve.&lt;/p&gt; 
&lt;p&gt;We've deployed Open Deep Research to our public demo instance of OAP. All you need to do is add your API Keys, and you can test out the Deep Researcher for yourself! Try it out &lt;a href="https://oap.langchain.com"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You can also deploy your own instance of OAP, and make your own custom agents (like Deep Researcher) available on it to your users.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://docs.oap.langchain.com/quickstart"&gt;Deploy Open Agent Platform&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.oap.langchain.com/setup/agents"&gt;Add Deep Researcher to OAP&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Updates üî•&lt;/h3&gt; 
&lt;h3&gt;Legacy Implementations üèõÔ∏è&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;src/legacy/&lt;/code&gt; folder contains two earlier implementations that provide alternative approaches to automated research:&lt;/p&gt; 
&lt;h4&gt;1. Workflow Implementation (&lt;code&gt;legacy/graph.py&lt;/code&gt;)&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Plan-and-Execute&lt;/strong&gt;: Structured workflow with human-in-the-loop planning&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Sequential Processing&lt;/strong&gt;: Creates sections one by one with reflection&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interactive Control&lt;/strong&gt;: Allows feedback and approval of report plans&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quality Focused&lt;/strong&gt;: Emphasizes accuracy through iterative refinement&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;2. Multi-Agent Implementation (&lt;code&gt;legacy/multi_agent.py&lt;/code&gt;)&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Supervisor-Researcher Architecture&lt;/strong&gt;: Coordinated multi-agent system&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Parallel Processing&lt;/strong&gt;: Multiple researchers work simultaneously&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Speed Optimized&lt;/strong&gt;: Faster report generation through concurrency&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP Support&lt;/strong&gt;: Extensive Model Context Protocol integration&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See &lt;code&gt;src/legacy/legacy.md&lt;/code&gt; for detailed documentation, configuration options, and usage examples for both legacy implementations.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/markitdown</title>
      <link>https://github.com/microsoft/markitdown</link>
      <description>&lt;p&gt;Python tool for converting files and office documents to Markdown.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MarkItDown&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/markitdown/"&gt;&lt;img src="https://img.shields.io/pypi/v/markitdown.svg?sanitize=true" alt="PyPI"&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/dd/markitdown" alt="PyPI - Downloads"&gt; &lt;a href="https://github.com/microsoft/autogen"&gt;&lt;img src="https://img.shields.io/badge/Built%20by-AutoGen%20Team-blue" alt="Built by AutoGen Team"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] MarkItDown now offers an MCP (Model Context Protocol) server for integration with LLM applications like Claude Desktop. See &lt;a href="https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp"&gt;markitdown-mcp&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Breaking changes between 0.0.1 to 0.1.0:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Dependencies are now organized into optional feature-groups (further details below). Use &lt;code&gt;pip install 'markitdown[all]'&lt;/code&gt; to have backward-compatible behavior.&lt;/li&gt; 
  &lt;li&gt;convert_stream() now requires a binary file-like object (e.g., a file opened in binary mode, or an io.BytesIO object). This is a breaking change from the previous version, where it previously also accepted text file-like objects, like io.StringIO.&lt;/li&gt; 
  &lt;li&gt;The DocumentConverter class interface has changed to read from file-like streams rather than file paths. &lt;em&gt;No temporary files are created anymore&lt;/em&gt;. If you are the maintainer of a plugin, or custom DocumentConverter, you likely need to update your code. Otherwise, if only using the MarkItDown class or CLI (as in these examples), you should not need to change anything.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;MarkItDown is a lightweight Python utility for converting various files to Markdown for use with LLMs and related text analysis pipelines. To this end, it is most comparable to &lt;a href="https://github.com/deanmalmgren/textract"&gt;textract&lt;/a&gt;, but with a focus on preserving important document structure and content as Markdown (including: headings, lists, tables, links, etc.) While the output is often reasonably presentable and human-friendly, it is meant to be consumed by text analysis tools -- and may not be the best option for high-fidelity document conversions for human consumption.&lt;/p&gt; 
&lt;p&gt;MarkItDown currently supports the conversion from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PDF&lt;/li&gt; 
 &lt;li&gt;PowerPoint&lt;/li&gt; 
 &lt;li&gt;Word&lt;/li&gt; 
 &lt;li&gt;Excel&lt;/li&gt; 
 &lt;li&gt;Images (EXIF metadata and OCR)&lt;/li&gt; 
 &lt;li&gt;Audio (EXIF metadata and speech transcription)&lt;/li&gt; 
 &lt;li&gt;HTML&lt;/li&gt; 
 &lt;li&gt;Text-based formats (CSV, JSON, XML)&lt;/li&gt; 
 &lt;li&gt;ZIP files (iterates over contents)&lt;/li&gt; 
 &lt;li&gt;Youtube URLs&lt;/li&gt; 
 &lt;li&gt;EPubs&lt;/li&gt; 
 &lt;li&gt;... and more!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Why Markdown?&lt;/h2&gt; 
&lt;p&gt;Markdown is extremely close to plain text, with minimal markup or formatting, but still provides a way to represent important document structure. Mainstream LLMs, such as OpenAI's GPT-4o, natively "&lt;em&gt;speak&lt;/em&gt;" Markdown, and often incorporate Markdown into their responses unprompted. This suggests that they have been trained on vast amounts of Markdown-formatted text, and understand it well. As a side benefit, Markdown conventions are also highly token-efficient.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;MarkItDown requires Python 3.10 or higher. It is recommended to use a virtual environment to avoid dependency conflicts.&lt;/p&gt; 
&lt;p&gt;With the standard Python installation, you can create and activate a virtual environment using the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv .venv
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If using &lt;code&gt;uv&lt;/code&gt;, you can create a virtual environment with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv --python=3.12 .venv
source .venv/bin/activate
# NOTE: Be sure to use 'uv pip install' rather than just 'pip install' to install packages in this virtual environment
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are using Anaconda, you can create a virtual environment with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n markitdown python=3.12
conda activate markitdown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install MarkItDown, use pip: &lt;code&gt;pip install 'markitdown[all]'&lt;/code&gt;. Alternatively, you can install it from the source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone git@github.com:microsoft/markitdown.git
cd markitdown
pip install -e 'packages/markitdown[all]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Command-Line&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf &amp;gt; document.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or use &lt;code&gt;-o&lt;/code&gt; to specify the output file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf -o document.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also pipe content:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cat path-to-file.pdf | markitdown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Optional Dependencies&lt;/h3&gt; 
&lt;p&gt;MarkItDown has optional dependencies for activating various file formats. Earlier in this document, we installed all optional dependencies with the &lt;code&gt;[all]&lt;/code&gt; option. However, you can also install them individually for more control. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install 'markitdown[pdf, docx, pptx]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;will install only the dependencies for PDF, DOCX, and PPTX files.&lt;/p&gt; 
&lt;p&gt;At the moment, the following optional dependencies are available:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;[all]&lt;/code&gt; Installs all optional dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[pptx]&lt;/code&gt; Installs dependencies for PowerPoint files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[docx]&lt;/code&gt; Installs dependencies for Word files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[xlsx]&lt;/code&gt; Installs dependencies for Excel files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[xls]&lt;/code&gt; Installs dependencies for older Excel files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[pdf]&lt;/code&gt; Installs dependencies for PDF files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[outlook]&lt;/code&gt; Installs dependencies for Outlook messages&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[az-doc-intel]&lt;/code&gt; Installs dependencies for Azure Document Intelligence&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[audio-transcription]&lt;/code&gt; Installs dependencies for audio transcription of wav and mp3 files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[youtube-transcription]&lt;/code&gt; Installs dependencies for fetching YouTube video transcription&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Plugins&lt;/h3&gt; 
&lt;p&gt;MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown --list-plugins
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To enable plugins use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown --use-plugins path-to-file.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To find available plugins, search GitHub for the hashtag &lt;code&gt;#markitdown-plugin&lt;/code&gt;. To develop a plugin, see &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Azure Document Intelligence&lt;/h3&gt; 
&lt;p&gt;To use Microsoft Document Intelligence for conversion:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf -o document.md -d -e "&amp;lt;document_intelligence_endpoint&amp;gt;"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;More information about how to set up an Azure Document Intelligence Resource can be found &lt;a href="https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/create-document-intelligence-resource?view=doc-intel-4.0.0"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Python API&lt;/h3&gt; 
&lt;p&gt;Basic usage in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown

md = MarkItDown(enable_plugins=False) # Set to True to enable plugins
result = md.convert("test.xlsx")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Document Intelligence conversion in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown

md = MarkItDown(docintel_endpoint="&amp;lt;document_intelligence_endpoint&amp;gt;")
result = md.convert("test.pdf")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use Large Language Models for image descriptions, provide &lt;code&gt;llm_client&lt;/code&gt; and &lt;code&gt;llm_model&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown
from openai import OpenAI

client = OpenAI()
md = MarkItDown(llm_client=client, llm_model="gpt-4o")
result = md.convert("example.jpg")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;docker build -t markitdown:latest .
docker run --rm -i markitdown:latest &amp;lt; ~/your-file.pdf &amp;gt; output.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h3&gt;How to Contribute&lt;/h3&gt; 
&lt;p&gt;You can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as 'open for contribution' and 'open for reviewing' to help facilitate community contributions. These are ofcourse just suggestions and you are welcome to contribute in any way you like.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;/th&gt; 
    &lt;th&gt;All&lt;/th&gt; 
    &lt;th&gt;Especially Needs Help from Community&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Issues&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/issues"&gt;All Issues&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/issues?q=is%3Aissue+is%3Aopen+label%3A%22open+for+contribution%22"&gt;Issues open for contribution&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;PRs&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/pulls"&gt;All PRs&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/pulls?q=is%3Apr+is%3Aopen+label%3A%22open+for+reviewing%22"&gt;PRs open for reviewing&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h3&gt;Running Tests and Checks&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Navigate to the MarkItDown package:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;cd packages/markitdown
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;code&gt;hatch&lt;/code&gt; in your environment and run tests:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/
hatch shell
hatch test
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(Alternative) Use the Devcontainer which has all the dependencies installed:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;# Reopen the project in Devcontainer and run:
hatch test
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run pre-commit checks before submitting a PR: &lt;code&gt;pre-commit run --all-files&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing 3rd-party Plugins&lt;/h3&gt; 
&lt;p&gt;You can also contribute by creating and sharing 3rd party plugins. See &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Trademarks&lt;/h2&gt; 
&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general"&gt;Microsoft's Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenPipe/ART</title>
      <link>https://github.com/OpenPipe/ART</link>
      <description>&lt;p&gt;Agent Reinforcement Trainer: train multi-step agents for real-world tasks using GRPO. Give your agents on-the-job training. Reinforcement learning for Qwen2.5, Qwen3, Llama, Kimi, and more!&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://art.openpipe.ai"&gt;
   &lt;picture&gt; 
    &lt;img alt="ART logo" src="https://github.com/openpipe/art/raw/main/assets/ART_logo.png" width="160px"&gt; 
   &lt;/picture&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;/p&gt;
 &lt;h1&gt;Agent Reinforcement Trainer&lt;/h1&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p&gt; Train multi-step agents for real-world tasks using GRPO. &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/openpipe/art/raw/main/CONTRIBUTING.md"&gt;&lt;img src="https://img.shields.io/badge/PRs-welcome-blue.svg?sanitize=true" alt="PRs-Welcome"&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/openpipe-art/"&gt;&lt;img src="https://img.shields.io/pypi/dm/openpipe-art?color=364fc7&amp;amp;logoColor=364fc7" alt="Downloads"&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/openpipe/art/blob/main/examples/2048/2048.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Train Agent"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://discord.gg/zbBHRUpwf4"&gt;&lt;img src="https://img.shields.io/badge/Join%20Discord-5865F2?style=plastic&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Join Discord"&gt;&lt;/a&gt; &lt;a href="https://art.openpipe.ai"&gt;&lt;img src="https://img.shields.io/badge/Documentation-orange?style=plastic&amp;amp;logo=gitbook&amp;amp;logoColor=white" alt="Documentation"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üìè RULER: Zero-Shot Agent Rewards&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;RULER&lt;/strong&gt; (Relative Universal LLM-Elicited Rewards) eliminates the need for hand-crafted reward functions by using an LLM-as-judge to automatically score agent trajectories. Simply define your task in the system prompt, and RULER handles the rest‚Äî&lt;strong&gt;no labeled data, expert feedback, or reward engineering required&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;‚ú® &lt;strong&gt;Key Benefits:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;2-3x faster development&lt;/strong&gt; - Skip reward function engineering entirely&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;General-purpose&lt;/strong&gt; - Works across any task without modification&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Strong performance&lt;/strong&gt; - Matches or exceeds hand-crafted rewards in 3/4 benchmarks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy integration&lt;/strong&gt; - Drop-in replacement for manual reward functions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Before: Hours of reward engineering
def complex_reward_function(trajectory):
    # 50+ lines of careful scoring logic...
    pass

# After: One line with RULER
judged_group = await ruler_score_group(group, "openai/o3")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://art.openpipe.ai/fundamentals/ruler"&gt;üìñ Learn more about RULER ‚Üí&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ART Overview&lt;/h2&gt; 
&lt;p&gt;ART is an open-source RL framework that improves agent reliability by allowing LLMs to &lt;strong&gt;learn from experience&lt;/strong&gt;. ART provides an ergonomic harness for integrating GRPO into any python application. For a quick hands-on introduction, run one of the notebooks below. When you're ready to learn more, check out the &lt;a href="https://art.openpipe.ai"&gt;docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üìí Notebooks&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Agent Task&lt;/th&gt; 
   &lt;th&gt;Example Notebook&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Comparative Performance&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ART‚Ä¢E [RULER]&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art/blob/main/examples/art-e/art-e.ipynb"&gt;üèãÔ∏è Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 7B learns to search emails using RULER&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/email_agent/accuracy-training-progress.svg?sanitize=true" height="72"&gt; &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/examples/art-e/art_e/evaluate/display_benchmarks.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;2048&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art/blob/main/examples/2048/2048.ipynb"&gt;üèãÔ∏è Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B learns to play 2048&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/2048/accuracy-training-progress.svg?sanitize=true" height="72"&gt; &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/examples/2048/benchmark_2048.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Temporal Clue&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art/blob/main/examples/temporal_clue/temporal-clue.ipynb"&gt;üèãÔ∏è Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 7B learns to solve Temporal Clue&lt;/td&gt; 
   &lt;td&gt;[Link coming soon]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Tic Tac Toe&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art/blob/main/examples/tic_tac_toe/tic-tac-toe.ipynb"&gt;üèãÔ∏è Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B learns to play Tic Tac Toe&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/tic-tac-toe-local/accuracy-training-progress.svg?sanitize=true" height="72"&gt; &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/examples/tic_tac_toe/benchmark_tic_tac_toe.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Codenames&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art/blob/main/examples/codenames/Codenames_RL.ipynb"&gt;üèãÔ∏è Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B learns to play Codenames&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/codenames/win_rate_over_time.png" height="72"&gt; &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/examples/codenames/Codenames_RL.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Why ART?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ART provides convenient wrappers for introducing RL training into &lt;strong&gt;existing applications&lt;/strong&gt;. We abstract the training server into a modular service that your code doesn't need to interface with.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Train from anywhere.&lt;/strong&gt; Run the ART client on your laptop and let the ART server kick off an ephemeral GPU-enabled environment, or run on a local GPU.&lt;/li&gt; 
 &lt;li&gt;Integrations with hosted platforms like W&amp;amp;B, Langfuse, and OpenPipe provide flexible observability and &lt;strong&gt;simplify debugging&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;ART is customizable with &lt;strong&gt;intelligent defaults&lt;/strong&gt;. You can configure training parameters and inference engine configurations to meet specific needs, or take advantage of the defaults, which have been optimized for training efficiency and stability.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;ART agents can be trained from any client machine that runs python. To add to an existing project, run this command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install openpipe-art
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ü§ñ ART‚Ä¢E Agent&lt;/h2&gt; 
&lt;p&gt;Curious about how to use ART for a real-world task? Check out the &lt;a href="https://openpipe.ai/blog/art-e-mail-agent"&gt;ART‚Ä¢E Agent&lt;/a&gt; blog post, where we detail how we trained Qwen 2.5 14B to beat o3 at email retrieval!&lt;/p&gt; 
&lt;img src="https://github.com/openpipe/art/raw/main/assets/ART_E_graphs.png" width="700"&gt; 
&lt;h2&gt;üîÅ Training Loop Overview&lt;/h2&gt; 
&lt;p&gt;ART's functionality is divided into a &lt;strong&gt;client&lt;/strong&gt; and a &lt;strong&gt;server&lt;/strong&gt;. The OpenAI-compatible client is responsible for interfacing between ART and your codebase. Using the client, you can pass messages and get completions from your LLM as it improves. The server runs independently on any machine with a GPU. It abstracts away the complexity of the inference and training portions of the RL loop while allowing for some custom configuration. An outline of the training loop is shown below:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Inference&lt;/strong&gt;&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt;Your code uses the ART client to perform an agentic workflow (usually executing several rollouts in parallel to gather data faster).&lt;/li&gt; 
   &lt;li&gt;Completion requests are routed to the ART server, which runs the model's latest LoRA in vLLM.&lt;/li&gt; 
   &lt;li&gt;As the agent executes, each &lt;code&gt;system&lt;/code&gt;, &lt;code&gt;user&lt;/code&gt;, and &lt;code&gt;assistant&lt;/code&gt; message is stored in a Trajectory.&lt;/li&gt; 
   &lt;li&gt;When a rollout finishes, your code assigns a &lt;code&gt;reward&lt;/code&gt; to its Trajectory, indicating the performance of the LLM.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt;&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt;When each rollout has finished, Trajectories are grouped and sent to the server. Inference is blocked while training executes.&lt;/li&gt; 
   &lt;li&gt;The server trains your model using GRPO, initializing from the latest checkpoint (or an empty LoRA on the first iteration).&lt;/li&gt; 
   &lt;li&gt;The server saves the newly trained LoRA to a local directory and loads it into vLLM.&lt;/li&gt; 
   &lt;li&gt;Inference is unblocked and the loop resumes at step 1.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This training loop runs until a specified number of inference and training iterations have completed.&lt;/p&gt; 
&lt;h2&gt;üß© Supported Models&lt;/h2&gt; 
&lt;p&gt;ART should work with most vLLM/HuggingFace-transformers compatible causal language models, or at least the ones supported by &lt;a href="https://docs.unsloth.ai/get-started/all-our-models"&gt;Unsloth&lt;/a&gt;. Gemma 3 does not appear to be supported for the time being. If any other model isn't working for you, please let us know on &lt;a href="https://discord.gg/zbBHRUpwf4"&gt;Discord&lt;/a&gt; or open an issue on &lt;a href="https://github.com/openpipe/art/issues"&gt;GitHub&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;ART is in active development, and contributions are most welcome! Please see the &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; file for more information.&lt;/p&gt; 
&lt;h2&gt;üìñ Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{hilton2025art,
  author = {Brad Hilton and Kyle Corbitt and David Corbitt and Saumya Gandhi and Angky William and Bohdan Kovalenskyi and Andie Jones},
  title = {ART: Agent Reinforcement Trainer},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openpipe/art}}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;‚öñÔ∏è License&lt;/h2&gt; 
&lt;p&gt;This repository's source code is available under the &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/LICENSE"&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üôè Credits&lt;/h2&gt; 
&lt;p&gt;ART stands on the shoulders of giants. While we owe many of the ideas and early experiments that led to ART's development to the open source RL community at large, we're especially grateful to the authors of the following projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/trl"&gt;trl&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pytorch/torchtune"&gt;torchtune&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/skypilot-org/skypilot"&gt;SkyPilot&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Finally, thank you to our partners who've helped us test ART in the wild! We're excited to see what you all build with it.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PromtEngineer/localGPT</title>
      <link>https://github.com/PromtEngineer/localGPT</link>
      <description>&lt;p&gt;Chat with your documents on your local device using GPT models. No data leaves your device and 100% private.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LocalGPT - Private Document Intelligence Platform&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/2947" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/2947" alt="PromtEngineer%2FlocalGPT | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/PromtEngineer/localGPT/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/PromtEngineer/localGPT?style=flat-square" alt="GitHub Stars"&gt;&lt;/a&gt; &lt;a href="https://github.com/PromtEngineer/localGPT/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/PromtEngineer/localGPT?style=flat-square" alt="GitHub Forks"&gt;&lt;/a&gt; &lt;a href="https://github.com/PromtEngineer/localGPT/issues"&gt;&lt;img src="https://img.shields.io/github/issues/PromtEngineer/localGPT?style=flat-square" alt="GitHub Issues"&gt;&lt;/a&gt; &lt;a href="https://github.com/PromtEngineer/localGPT/pulls"&gt;&lt;img src="https://img.shields.io/github/issues-pr/PromtEngineer/localGPT?style=flat-square" alt="GitHub Pull Requests"&gt;&lt;/a&gt; &lt;a href="https://www.python.org/downloads/"&gt;&lt;img src="https://img.shields.io/badge/python-3.8+-blue.svg?style=flat-square" alt="Python 3.8+"&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PromtEngineer/localGPT/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-green.svg?style=flat-square" alt="License"&gt;&lt;/a&gt; &lt;a href="https://www.docker.com/"&gt;&lt;img src="https://img.shields.io/badge/docker-supported-blue.svg?style=flat-square" alt="Docker"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://x.com/engineerrprompt"&gt; &lt;img src="https://img.shields.io/badge/Follow%20on%20X-000000?style=for-the-badge&amp;amp;logo=x&amp;amp;logoColor=white" alt="Follow on X"&gt; &lt;/a&gt; &lt;a href="https://discord.gg/tUDWAFGc"&gt; &lt;img src="https://img.shields.io/badge/Join%20our%20Discord-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Join our Discord"&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üöÄ What is LocalGPT?&lt;/h2&gt; 
&lt;p&gt;LocalGPT is a &lt;strong&gt;fully private, on-premise Document Intelligence platform&lt;/strong&gt;. Ask questions, summarise, and uncover insights from your files with state-of-the-art AI‚Äîno data ever leaves your machine.&lt;/p&gt; 
&lt;p&gt;More than a traditional RAG (Retrieval-Augmented Generation) tool, LocalGPT features a &lt;strong&gt;hybrid search engine&lt;/strong&gt; that blends semantic similarity, keyword matching, and &lt;a href="https://jina.ai/news/late-chunking-in-long-context-embedding-models/"&gt;Late Chunking&lt;/a&gt; for long-context precision. A &lt;strong&gt;smart router&lt;/strong&gt; automatically selects between RAG and direct LLM answering for every query, while &lt;strong&gt;contextual enrichment&lt;/strong&gt; and sentence-level &lt;a href="https://huggingface.co/naver/provence-reranker-debertav3-v1"&gt;Context Pruning&lt;/a&gt; surface only the most relevant content. An independent &lt;strong&gt;verification&lt;/strong&gt; pass adds an extra layer of accuracy.&lt;/p&gt; 
&lt;p&gt;The architecture is &lt;strong&gt;modular and lightweight&lt;/strong&gt;‚Äîenable only the components you need. With a pure-Python core and minimal dependencies, LocalGPT is simple to deploy, run, and maintain on any infrastructure.The system has minimal dependencies on frameworks and libraries, making it easy to deploy and maintain. The RAG system is pure python and does not require any additional dependencies.&lt;/p&gt; 
&lt;h2&gt;‚ñ∂Ô∏è Video&lt;/h2&gt; 
&lt;p&gt;Watch this &lt;a href="https://youtu.be/JTbtGH3secI"&gt;video&lt;/a&gt; to get started with LocalGPT.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Home&lt;/th&gt; 
   &lt;th&gt;Create Index&lt;/th&gt; 
   &lt;th&gt;Chat&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/PromtEngineer/localGPT/main/Documentation/images/Home.png" alt=""&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/PromtEngineer/localGPT/main/Documentation/images/Index%20Creation.png" alt=""&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/PromtEngineer/localGPT/main/Documentation/images/Retrieval%20Process.png" alt=""&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;‚ú® Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Utmost Privacy&lt;/strong&gt;: Your data remains on your computer, ensuring 100% security.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Versatile Model Support&lt;/strong&gt;: Seamlessly integrate a variety of open-source models via Ollama.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Diverse Embeddings&lt;/strong&gt;: Choose from a range of open-source embeddings.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reuse Your LLM&lt;/strong&gt;: Once downloaded, reuse your LLM without the need for repeated downloads.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chat History&lt;/strong&gt;: Remembers your previous conversations (in a session).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;API&lt;/strong&gt;: LocalGPT has an API that you can use for building RAG Applications.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GPU, CPU, HPU &amp;amp; MPS Support&lt;/strong&gt;: Supports multiple platforms out of the box, Chat with your data using &lt;code&gt;CUDA&lt;/code&gt;, &lt;code&gt;CPU&lt;/code&gt;, &lt;code&gt;HPU (Intel¬Æ Gaudi¬Æ)&lt;/code&gt; or &lt;code&gt;MPS&lt;/code&gt; and more!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìñ Document Processing&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-format Support&lt;/strong&gt;: PDF, DOCX, TXT, Markdown, and more (Currently only PDF is supported)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Contextual Enrichment&lt;/strong&gt;: Enhanced document understanding with AI-generated context, inspired by &lt;a href="https://www.anthropic.com/news/contextual-retrieval"&gt;Contextual Retrieval&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Batch Processing&lt;/strong&gt;: Handle multiple documents simultaneously&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ü§ñ AI-Powered Chat&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Natural Language Queries&lt;/strong&gt;: Ask questions in plain English&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Source Attribution&lt;/strong&gt;: Every answer includes document references&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Routing&lt;/strong&gt;: Automatically chooses between RAG and direct LLM responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Query Decomposition&lt;/strong&gt;: Breaks complex queries into sub-questions for better answers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Semantic Caching&lt;/strong&gt;: TTL-based caching with similarity matching for faster responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Session-Aware History&lt;/strong&gt;: Maintains conversation context across interactions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Answer Verification&lt;/strong&gt;: Independent verification pass for accuracy&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multiple AI Models&lt;/strong&gt;: Ollama for inference, HuggingFace for embeddings and reranking&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üõ†Ô∏è Developer-Friendly&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;RESTful APIs&lt;/strong&gt;: Complete API access for integration&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time Progress&lt;/strong&gt;: Live updates during document processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible Configuration&lt;/strong&gt;: Customize models, chunk sizes, and search parameters&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensible Architecture&lt;/strong&gt;: Plugin system for custom components&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üé® Modern Interface&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Intuitive Web UI&lt;/strong&gt;: Clean, responsive design&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Session Management&lt;/strong&gt;: Organize conversations by topic&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Index Management&lt;/strong&gt;: Easy document collection management&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time Chat&lt;/strong&gt;: Streaming responses for immediate feedback&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;p&gt;Note: The installation is currently only tested on macOS.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.8 or higher (tested with Python 3.11.5)&lt;/li&gt; 
 &lt;li&gt;Node.js 16+ and npm (tested with Node.js 23.10.0, npm 10.9.2)&lt;/li&gt; 
 &lt;li&gt;Docker (optional, for containerized deployment)&lt;/li&gt; 
 &lt;li&gt;8GB+ RAM (16GB+ recommended)&lt;/li&gt; 
 &lt;li&gt;Ollama (required for both deployment approaches)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;em&gt;&lt;strong&gt;NOTE&lt;/strong&gt;&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;Before this brach is moved to the main branch, please clone this branch for instalation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone -b localgpt-v2 https://github.com/PromtEngineer/localGPT.git
cd localGPT
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Option 1: Docker Deployment&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the repository
git clone https://github.com/PromtEngineer/localGPT.git
cd localGPT

# Install Ollama locally (required even for Docker)
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull qwen3:0.6b
ollama pull qwen3:8b

# Start Ollama
ollama serve

# Start with Docker (in a new terminal)
./start-docker.sh

# Access the application
open http://localhost:3000
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Docker Management Commands:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Check container status
docker compose ps

# View logs
docker compose logs -f

# Stop containers
./start-docker.sh stop
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Option 2: Direct Development (Recommended for Development)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the repository
git clone https://github.com/PromtEngineer/localGPT.git
cd localGPT

# Install Python dependencies
pip install -r requirements.txt

# Key dependencies installed:
# - torch==2.4.1, transformers==4.51.0 (AI models)
# - lancedb (vector database)
# - rank_bm25, fuzzywuzzy (search algorithms)
# - sentence_transformers, rerankers (embedding/reranking)
# - docling (document processing)
# - colpali-engine (multimodal processing - support coming soon)

# Install Node.js dependencies
npm install

# Install and start Ollama
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull qwen3:0.6b
ollama pull qwen3:8b
ollama serve

# Start the system (in a new terminal)
python run_system.py

# Access the application
open http://localhost:3000
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;System Management:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Check system health (comprehensive diagnostics)
python system_health_check.py

# Check service status and health
python run_system.py --health

# Start in production mode
python run_system.py --mode prod

# Skip frontend (backend + RAG API only)
python run_system.py --no-frontend

# View aggregated logs
python run_system.py --logs-only

# Stop all services
python run_system.py --stop
# Or press Ctrl+C in the terminal running python run_system.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Service Architecture:&lt;/strong&gt; The &lt;code&gt;run_system.py&lt;/code&gt; launcher manages four key services:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Ollama Server&lt;/strong&gt; (port 11434): AI model serving&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;RAG API Server&lt;/strong&gt; (port 8001): Document processing and retrieval&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backend Server&lt;/strong&gt; (port 8000): Session management and API endpoints&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Frontend Server&lt;/strong&gt; (port 3000): React/Next.js web interface&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Option 3: Manual Component Startup&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Terminal 1: Start Ollama
ollama serve

# Terminal 2: Start RAG API
python -m rag_system.api_server

# Terminal 3: Start Backend
cd backend &amp;amp;&amp;amp; python server.py

# Terminal 4: Start Frontend
npm run dev

# Access at http://localhost:3000
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h3&gt;Detailed Installation&lt;/h3&gt; 
&lt;h4&gt;1. Install System Dependencies&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Ubuntu/Debian:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt update
sudo apt install python3.8 python3-pip nodejs npm docker.io docker-compose
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;macOS:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew install python@3.8 node npm docker docker-compose
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Windows:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install Python 3.8+, Node.js, and Docker Desktop
# Then use PowerShell or WSL2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;2. Install AI Models&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Install Ollama (Recommended):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Pull recommended models
ollama pull qwen3:0.6b          # Fast generation model
ollama pull qwen3:8b            # High-quality generation model
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3. Configure Environment&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Copy environment template
cp .env.example .env

# Edit configuration
nano .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Key Configuration Options:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-env"&gt;# AI Models (referenced in rag_system/main.py)
OLLAMA_HOST=http://localhost:11434

# Database Paths (used by backend and RAG system)
DATABASE_PATH=./backend/chat_data.db
VECTOR_DB_PATH=./lancedb

# Server Settings (used by run_system.py)
BACKEND_PORT=8000
FRONTEND_PORT=3000
RAG_API_PORT=8001

# Optional: Override default models
GENERATION_MODEL=qwen3:8b
ENRICHMENT_MODEL=qwen3:0.6b
EMBEDDING_MODEL=Qwen/Qwen3-Embedding-0.6B
RERANKER_MODEL=answerdotai/answerai-colbert-small-v1
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;4. Initialize the System&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run system health check
python system_health_check.py

# Initialize databases
python -c "from backend.database import ChatDatabase; ChatDatabase().init_database()"

# Test installation
python -c "from rag_system.main import get_agent; print('‚úÖ Installation successful!')"

# Validate complete setup
python run_system.py --health
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h2&gt;üéØ Getting Started&lt;/h2&gt; 
&lt;h3&gt;1. Create Your First Index&lt;/h3&gt; 
&lt;p&gt;An &lt;strong&gt;index&lt;/strong&gt; is a collection of processed documents that you can chat with.&lt;/p&gt; 
&lt;h4&gt;Using the Web Interface:&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Click "Create New Index"&lt;/li&gt; 
 &lt;li&gt;Upload your documents (PDF, DOCX, TXT)&lt;/li&gt; 
 &lt;li&gt;Configure processing options&lt;/li&gt; 
 &lt;li&gt;Click "Build Index"&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Using Scripts:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Simple script approach
./simple_create_index.sh "My Documents" "path/to/document.pdf"

# Interactive script
python create_index_script.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Using API:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create index
curl -X POST http://localhost:8000/indexes \
  -H "Content-Type: application/json" \
  -d '{"name": "My Index", "description": "My documents"}'

# Upload documents
curl -X POST http://localhost:8000/indexes/INDEX_ID/upload \
  -F "files=@document.pdf"

# Build index
curl -X POST http://localhost:8000/indexes/INDEX_ID/build
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Start Chatting&lt;/h3&gt; 
&lt;p&gt;Once your index is built:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Create a Chat Session&lt;/strong&gt;: Click "New Chat" or use an existing session&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Select Your Index&lt;/strong&gt;: Choose which document collection to query&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ask Questions&lt;/strong&gt;: Type natural language questions about your documents&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Get Answers&lt;/strong&gt;: Receive AI-generated responses with source citations&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;3. Advanced Features&lt;/h3&gt; 
&lt;h4&gt;Custom Model Configuration&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Use different models for different tasks
curl -X POST http://localhost:8000/sessions \
  -H "Content-Type: application/json" \
  -d '{
    "title": "High Quality Session",
    "model": "qwen3:8b",
    "embedding_model": "Qwen/Qwen3-Embedding-4B"
  }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Batch Document Processing&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Process multiple documents at once
python demo_batch_indexing.py --config batch_indexing_config.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;API Integration&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import requests

# Chat with your documents via API
response = requests.post('http://localhost:8000/chat', json={
    'query': 'What are the key findings in the research papers?',
    'session_id': 'your-session-id',
    'search_type': 'hybrid',
    'retrieval_k': 20
})

print(response.json()['response'])
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h2&gt;üîß Configuration&lt;/h2&gt; 
&lt;h3&gt;Model Configuration&lt;/h3&gt; 
&lt;p&gt;LocalGPT supports multiple AI model providers with centralized configuration:&lt;/p&gt; 
&lt;h4&gt;Ollama Models (Local Inference)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;OLLAMA_CONFIG = {
    "host": "http://localhost:11434",
    "generation_model": "qwen3:8b",        # Main text generation
    "enrichment_model": "qwen3:0.6b"       # Lightweight routing/enrichment
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;External Models (HuggingFace Direct)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;EXTERNAL_MODELS = {
    "embedding_model": "Qwen/Qwen3-Embedding-0.6B",           # 1024 dimensions
    "reranker_model": "answerdotai/answerai-colbert-small-v1", # ColBERT reranker
    "fallback_reranker": "BAAI/bge-reranker-base"             # Backup reranker
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Pipeline Configuration&lt;/h3&gt; 
&lt;p&gt;LocalGPT offers two main pipeline configurations:&lt;/p&gt; 
&lt;h4&gt;Default Pipeline (Production-Ready)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"default": {
    "description": "Production-ready pipeline with hybrid search, AI reranking, and verification",
    "storage": {
        "lancedb_uri": "./lancedb",
        "text_table_name": "text_pages_v3",
        "bm25_path": "./index_store/bm25"
    },
    "retrieval": {
        "retriever": "multivector",
        "search_type": "hybrid",
        "late_chunking": {"enabled": True},
        "dense": {"enabled": True, "weight": 0.7},
        "bm25": {"enabled": True}
    },
    "reranker": {
        "enabled": True,
        "type": "ai",
        "strategy": "rerankers-lib",
        "model_name": "answerdotai/answerai-colbert-small-v1",
        "top_k": 10
    },
    "query_decomposition": {"enabled": True, "max_sub_queries": 3},
    "verification": {"enabled": True},
    "retrieval_k": 20,
    "contextual_enricher": {"enabled": True, "window_size": 1}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Fast Pipeline (Speed-Optimized)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"fast": {
    "description": "Speed-optimized pipeline with minimal overhead",
    "retrieval": {
        "search_type": "vector_only",
        "late_chunking": {"enabled": False}
    },
    "reranker": {"enabled": False},
    "query_decomposition": {"enabled": False},
    "verification": {"enabled": False},
    "retrieval_k": 10,
    "contextual_enricher": {"enabled": False}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Search Configuration&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;SEARCH_CONFIG = {
    'hybrid': {
        'dense_weight': 0.7,
        'sparse_weight': 0.3,
        'retrieval_k': 20,
        'reranker_top_k': 10
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h2&gt;üõ†Ô∏è Troubleshooting&lt;/h2&gt; 
&lt;h3&gt;Common Issues&lt;/h3&gt; 
&lt;h4&gt;Installation Problems&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Check Python version
python --version  # Should be 3.8+

# Check dependencies
pip list | grep -E "(torch|transformers|lancedb)"

# Reinstall dependencies
pip install -r requirements.txt --force-reinstall
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Model Loading Issues&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Check Ollama status
ollama list
curl http://localhost:11434/api/tags

# Pull missing models
ollama pull qwen3:0.6b
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Database Issues&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Check database connectivity
python -c "from backend.database import ChatDatabase; db = ChatDatabase(); print('‚úÖ Database OK')"

# Reset database (WARNING: This deletes all data)
rm backend/chat_data.db
python -c "from backend.database import ChatDatabase; ChatDatabase().init_database()"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Performance Issues&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Check system resources
python system_health_check.py

# Monitor memory usage
htop  # or Task Manager on Windows

# Optimize for low-memory systems
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Getting Help&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Check Logs&lt;/strong&gt;: The system creates structured logs in the &lt;code&gt;logs/&lt;/code&gt; directory:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;logs/system.log&lt;/code&gt;: Main system events and errors&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;logs/ollama.log&lt;/code&gt;: Ollama server logs&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;logs/rag-api.log&lt;/code&gt;: RAG API processing logs&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;logs/backend.log&lt;/code&gt;: Backend server logs&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;logs/frontend.log&lt;/code&gt;: Frontend build and runtime logs&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;System Health&lt;/strong&gt;: Run comprehensive diagnostics:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python system_health_check.py  # Full system diagnostics
python run_system.py --health  # Service status check
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Health Endpoints&lt;/strong&gt;: Check individual service health:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Backend: &lt;code&gt;http://localhost:8000/health&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;RAG API: &lt;code&gt;http://localhost:8001/health&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Ollama: &lt;code&gt;http://localhost:11434/api/tags&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: Check the &lt;a href="https://raw.githubusercontent.com/PromtEngineer/localGPT/main/TECHNICAL_DOCS.md"&gt;Technical Documentation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;GitHub Issues&lt;/strong&gt;: Report bugs and request features&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Community&lt;/strong&gt;: Join our Discord/Slack community&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr&gt; 
&lt;h2&gt;üîó API Reference&lt;/h2&gt; 
&lt;h3&gt;Core Endpoints&lt;/h3&gt; 
&lt;h4&gt;Chat API&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-http"&gt;# Session-based chat (recommended)
POST /sessions/{session_id}/chat
Content-Type: application/json

{
  "query": "What are the main topics discussed?",
  "search_type": "hybrid",
  "retrieval_k": 20,
  "ai_rerank": true,
  "context_window_size": 5
}

# Legacy chat endpoint
POST /chat
Content-Type: application/json

{
  "query": "What are the main topics discussed?",
  "session_id": "uuid",
  "search_type": "hybrid",
  "retrieval_k": 20
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Index Management&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-http"&gt;# Create index
POST /indexes
Content-Type: application/json
{
  "name": "My Index",
  "description": "Description",
  "config": "default"
}

# Get all indexes
GET /indexes

# Get specific index
GET /indexes/{id}

# Upload documents to index
POST /indexes/{id}/upload
Content-Type: multipart/form-data
files: [file1.pdf, file2.pdf, ...]

# Build index (process uploaded documents)
POST /indexes/{id}/build
Content-Type: application/json
{
  "config_mode": "default",
  "enable_enrich": true,
  "chunk_size": 512
}

# Delete index
DELETE /indexes/{id}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Session Management&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-http"&gt;# Create session
POST /sessions
Content-Type: application/json
{
  "title": "My Session",
  "model": "qwen3:0.6b"
}

# Get all sessions
GET /sessions

# Get specific session
GET /sessions/{session_id}

# Get session documents
GET /sessions/{session_id}/documents

# Get session indexes
GET /sessions/{session_id}/indexes

# Link index to session
POST /sessions/{session_id}/indexes/{index_id}

# Delete session
DELETE /sessions/{session_id}

# Rename session
POST /sessions/{session_id}/rename
Content-Type: application/json
{
  "new_title": "Updated Session Name"
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Advanced Features&lt;/h3&gt; 
&lt;h4&gt;Query Decomposition&lt;/h4&gt; 
&lt;p&gt;The system can break complex queries into sub-questions for better answers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-http"&gt;POST /sessions/{session_id}/chat
Content-Type: application/json

{
  "query": "Compare the methodologies and analyze their effectiveness",
  "query_decompose": true,
  "compose_sub_answers": true
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Answer Verification&lt;/h4&gt; 
&lt;p&gt;Independent verification pass for accuracy using a separate verification model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-http"&gt;POST /sessions/{session_id}/chat
Content-Type: application/json

{
  "query": "What are the key findings?",
  "verify": true
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Contextual Enrichment&lt;/h4&gt; 
&lt;p&gt;Document context enrichment during indexing for better understanding:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Enable during index building
POST /indexes/{id}/build
{
  "enable_enrich": true,
  "window_size": 2
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Late Chunking&lt;/h4&gt; 
&lt;p&gt;Better context preservation by chunking after embedding:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Configure in pipeline
"late_chunking": {"enabled": true}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Streaming Chat&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-http"&gt;POST /chat/stream
Content-Type: application/json

{
  "query": "Explain the methodology",
  "session_id": "uuid",
  "stream": true
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Batch Processing&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Using the batch indexing script
python demo_batch_indexing.py --config batch_indexing_config.json

# Example batch configuration (batch_indexing_config.json):
{
  "index_name": "Sample Batch Index",
  "index_description": "Example batch index configuration",
  "documents": [
    "./rag_system/documents/invoice_1039.pdf",
    "./rag_system/documents/invoice_1041.pdf"
  ],
  "processing": {
    "chunk_size": 512,
    "chunk_overlap": 64,
    "enable_enrich": true,
    "enable_latechunk": true,
    "enable_docling": true,
    "embedding_model": "Qwen/Qwen3-Embedding-0.6B",
    "generation_model": "qwen3:0.6b",
    "retrieval_mode": "hybrid",
    "window_size": 2
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-http"&gt;# API endpoint for batch processing
POST /batch/index
Content-Type: application/json

{
  "file_paths": ["doc1.pdf", "doc2.pdf"],
  "config": {
    "chunk_size": 512,
    "enable_enrich": true,
    "enable_latechunk": true,
    "enable_docling": true
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For complete API documentation, see &lt;a href="https://raw.githubusercontent.com/PromtEngineer/localGPT/main/API_REFERENCE.md"&gt;API_REFERENCE.md&lt;/a&gt;.&lt;/p&gt; 
&lt;hr&gt; 
&lt;h2&gt;üèóÔ∏è Architecture&lt;/h2&gt; 
&lt;p&gt;LocalGPT is built with a modular, scalable architecture:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-mermaid"&gt;graph TB
    UI[Web Interface] --&amp;gt; API[Backend API]
    API --&amp;gt; Agent[RAG Agent]
    Agent --&amp;gt; Retrieval[Retrieval Pipeline]
    Agent --&amp;gt; Generation[Generation Pipeline]

    Retrieval --&amp;gt; Vector[Vector Search]
    Retrieval --&amp;gt; BM25[BM25 Search]
    Retrieval --&amp;gt; Rerank[Reranking]

    Vector --&amp;gt; LanceDB[(LanceDB)]
    BM25 --&amp;gt; BM25DB[(BM25 Index)]

    Generation --&amp;gt; Ollama[Ollama Models]
    Generation --&amp;gt; HF[Hugging Face Models]

    API --&amp;gt; SQLite[(SQLite DB)]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Overview of the Retrieval Agent&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-mermaid"&gt;graph TD
    classDef llmcall fill:#e6f3ff,stroke:#007bff;
    classDef pipeline fill:#e6ffe6,stroke:#28a745;
    classDef cache fill:#fff3e0,stroke:#fd7e14;
    classDef logic fill:#f8f9fa,stroke:#6c757d;
    classDef thread stroke-dasharray: 5 5;

    A(Start: Agent.run) --&amp;gt; B_asyncio.run(_run_async);
    B --&amp;gt; C{_run_async};

    C --&amp;gt; C1[Get Chat History];
    C1 --&amp;gt; T1[Build Triage Prompt &amp;lt;br/&amp;gt; Query + Doc Overviews ];
    T1 --&amp;gt; T2["(asyncio.to_thread)&amp;lt;br/&amp;gt;LLM Triage: RAG or LLM_DIRECT?"]; class T2 llmcall,thread;
    T2 --&amp;gt; T3{Decision?};

    T3 -- RAG --&amp;gt; RAG_Path;
    T3 -- LLM_DIRECT --&amp;gt; LLM_Path;

    subgraph RAG Path
        RAG_Path --&amp;gt; R1[Format Query + History];
        R1 --&amp;gt; R2["(asyncio.to_thread)&amp;lt;br/&amp;gt;Generate Query Embedding"]; class R2 pipeline,thread;
        R2 --&amp;gt; R3{{Check Semantic Cache}}; class R3 cache;
        R3 -- Hit --&amp;gt; R_Cache_Hit(Return Cached Result);
        R_Cache_Hit --&amp;gt; R_Hist_Update;
        R3 -- Miss --&amp;gt; R4{Decomposition &amp;lt;br/&amp;gt; Enabled?};

        R4 -- Yes --&amp;gt; R5["(asyncio.to_thread)&amp;lt;br/&amp;gt;Decompose Raw Query"]; class R5 llmcall,thread;
        R5 --&amp;gt; R6{{Run Sub-Queries &amp;lt;br/&amp;gt; Parallel RAG Pipeline}}; class R6 pipeline,thread;
        R6 --&amp;gt; R7[Collect Results &amp;amp; Docs];
        R7 --&amp;gt; R8["(asyncio.to_thread)&amp;lt;br/&amp;gt;Compose Final Answer"]; class R8 llmcall,thread;
        R8 --&amp;gt; V1(RAG Answer);

        R4 -- No --&amp;gt; R9["(asyncio.to_thread)&amp;lt;br/&amp;gt;Run Single Query &amp;lt;br/&amp;gt;(RAG Pipeline)"]; class R9 pipeline,thread;
        R9 --&amp;gt; V1;

        V1 --&amp;gt; V2{{Verification &amp;lt;br/&amp;gt; await verify_async}}; class V2 llmcall;
        V2 --&amp;gt; V3(Final RAG Result);
        V3 --&amp;gt; R_Cache_Store{{Store in Semantic Cache}}; class R_Cache_Store cache;
        R_Cache_Store --&amp;gt; FinalResult;
    end

    subgraph Direct LLM Path
        LLM_Path --&amp;gt; L1[Format Query + History];
        L1 --&amp;gt; L2["(asyncio.to_thread)&amp;lt;br/&amp;gt;Generate Direct LLM Answer &amp;lt;br/&amp;gt; (No RAG)"]; class L2 llmcall,thread;
        L2 --&amp;gt; FinalResult(Final Direct Result);
    end

    FinalResult --&amp;gt; R_Hist_Update(Update Chat History);
    R_Hist_Update --&amp;gt; ZZZ(End: Return Result);
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from developers of all skill levels! LocalGPT is an open-source project that benefits from community involvement.&lt;/p&gt; 
&lt;h3&gt;üöÄ Quick Start for Contributors&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Fork and clone the repository
git clone https://github.com/PromtEngineer/localGPT.git
cd localGPT

# Set up development environment
pip install -r requirements.txt
npm install

# Install Ollama and models
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull qwen3:0.6b qwen3:8b

# Verify setup
python system_health_check.py
python run_system.py --mode dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üìã How to Contribute&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;üêõ Report Bugs&lt;/strong&gt;: Use our &lt;a href="https://raw.githubusercontent.com/PromtEngineer/localGPT/main/.github/ISSUE_TEMPLATE/bug_report.md"&gt;bug report template&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üí° Request Features&lt;/strong&gt;: Use our &lt;a href="https://raw.githubusercontent.com/PromtEngineer/localGPT/main/.github/ISSUE_TEMPLATE/feature_request.md"&gt;feature request template&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîß Submit Code&lt;/strong&gt;: Follow our &lt;a href="https://raw.githubusercontent.com/PromtEngineer/localGPT/main/CONTRIBUTING.md#development-workflow"&gt;development workflow&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìö Improve Docs&lt;/strong&gt;: Help make our documentation better&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;üìñ Detailed Guidelines&lt;/h3&gt; 
&lt;p&gt;For comprehensive contributing guidelines, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Development setup and workflow&lt;/li&gt; 
 &lt;li&gt;Coding standards and best practices&lt;/li&gt; 
 &lt;li&gt;Testing requirements&lt;/li&gt; 
 &lt;li&gt;Documentation standards&lt;/li&gt; 
 &lt;li&gt;Release process&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;üëâ See our &lt;a href="https://raw.githubusercontent.com/PromtEngineer/localGPT/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; guide&lt;/strong&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/PromtEngineer/localGPT/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details. For models, please check their respective licenses.&lt;/p&gt; 
&lt;hr&gt; 
&lt;h2&gt;üìû Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/PromtEngineer/localGPT/main/TECHNICAL_DOCS.md"&gt;Technical Docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Issues&lt;/strong&gt;: &lt;a href="https://github.com/PromtEngineer/localGPT/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Discussions&lt;/strong&gt;: &lt;a href="https://github.com/PromtEngineer/localGPT/discussions"&gt;GitHub Discussions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Business Deployment and Customization&lt;/strong&gt;: &lt;a href="https://tally.so/r/wv6R2d"&gt;Contact Us&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;div align="center"&gt; 
 &lt;h2&gt;Star History&lt;/h2&gt; 
 &lt;p&gt;&lt;a href="https://star-history.com/#PromtEngineer/localGPT&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=PromtEngineer/localGPT&amp;amp;type=Date" alt="Star History Chart"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>hesreallyhim/awesome-claude-code</title>
      <link>https://github.com/hesreallyhim/awesome-claude-code</link>
      <description>&lt;p&gt;A curated list of awesome commands, files, and workflows for Claude Code&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;/h1&gt; 
&lt;!-- [![Awesome](https://awesome.re/badge-flat2.svg)](https://awesome.re) --&gt; 
&lt;pre style="display: inline-block; text-align: left;"&gt;
 ‚ñà‚ñà‚ñà‚ñà‚ñà‚îê ‚ñà‚ñà‚îê    ‚ñà‚ñà‚îê‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê ‚ñà‚ñà‚ñà‚îê   ‚ñà‚ñà‚ñà‚îê‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê
‚ñà‚ñà‚îå‚îÄ‚îÄ‚ñà‚ñà‚îê‚ñà‚ñà‚îÇ    ‚ñà‚ñà‚îÇ‚ñà‚ñà‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚ñà‚ñà‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚ñà‚ñà‚îå‚îÄ‚îÄ‚îÄ‚ñà‚ñà‚îê‚ñà‚ñà‚ñà‚ñà‚îê ‚ñà‚ñà‚ñà‚ñà‚îÇ‚ñà‚ñà‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îÇ‚ñà‚ñà‚îÇ ‚ñà‚îê ‚ñà‚ñà‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚îê  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê‚ñà‚ñà‚îÇ   ‚ñà‚ñà‚îÇ‚ñà‚ñà‚îå‚ñà‚ñà‚ñà‚ñà‚îå‚ñà‚ñà‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚îê
‚ñà‚ñà‚îå‚îÄ‚îÄ‚ñà‚ñà‚îÇ‚ñà‚ñà‚îÇ‚ñà‚ñà‚ñà‚îê‚ñà‚ñà‚îÇ‚ñà‚ñà‚îå‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚ñà‚ñà‚îÇ‚ñà‚ñà‚îÇ   ‚ñà‚ñà‚îÇ‚ñà‚ñà‚îÇ‚îî‚ñà‚ñà‚îå‚îò‚ñà‚ñà‚îÇ‚ñà‚ñà‚îå‚îÄ‚îÄ‚îò
‚ñà‚ñà‚îÇ  ‚ñà‚ñà‚îÇ‚îî‚ñà‚ñà‚ñà‚îå‚ñà‚ñà‚ñà‚îå‚îò‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îÇ‚îî‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îå‚îò‚ñà‚ñà‚îÇ ‚îî‚îÄ‚îò ‚ñà‚ñà‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê
‚îî‚îÄ‚îò  ‚îî‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îò‚îî‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îò     ‚îî‚îÄ‚îò‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê‚ñà‚ñà‚îê      ‚ñà‚ñà‚ñà‚ñà‚ñà‚îê ‚ñà‚ñà‚îê   ‚ñà‚ñà‚îê‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê
‚ñà‚ñà‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚ñà‚ñà‚îÇ     ‚ñà‚ñà‚îå‚îÄ‚îÄ‚ñà‚ñà‚îê‚ñà‚ñà‚îÇ   ‚ñà‚ñà‚îÇ‚ñà‚ñà‚îå‚îÄ‚îÄ‚ñà‚ñà‚îê‚ñà‚ñà‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚ñà‚ñà‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚ñà‚ñà‚îå‚îÄ‚îÄ‚îÄ‚ñà‚ñà‚îê‚ñà‚ñà‚îå‚îÄ‚îÄ‚ñà‚ñà‚îê‚ñà‚ñà‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚ñà‚ñà‚îÇ     ‚ñà‚ñà‚îÇ     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îÇ‚ñà‚ñà‚îÇ   ‚ñà‚ñà‚îÇ‚ñà‚ñà‚îÇ  ‚ñà‚ñà‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚îê      ‚ñà‚ñà‚îÇ     ‚ñà‚ñà‚îÇ   ‚ñà‚ñà‚îÇ‚ñà‚ñà‚îÇ  ‚ñà‚ñà‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚îê
‚ñà‚ñà‚îÇ     ‚ñà‚ñà‚îÇ     ‚ñà‚ñà‚îå‚îÄ‚îÄ‚ñà‚ñà‚îÇ‚ñà‚ñà‚îÇ   ‚ñà‚ñà‚îÇ‚ñà‚ñà‚îÇ  ‚ñà‚ñà‚îÇ‚ñà‚ñà‚îå‚îÄ‚îÄ‚îò      ‚ñà‚ñà‚îÇ     ‚ñà‚ñà‚îÇ   ‚ñà‚ñà‚îÇ‚ñà‚ñà‚îÇ  ‚ñà‚ñà‚îÇ‚ñà‚ñà‚îå‚îÄ‚îÄ‚îò
‚îî‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê‚ñà‚ñà‚îÇ  ‚ñà‚ñà‚îÇ‚îî‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îå‚îò‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îå‚îò‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê    ‚îî‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê‚îî‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îå‚îò‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îå‚îò‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îî‚îÄ‚îò  ‚îî‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
&lt;/pre&gt; 
&lt;!--lint enable remark-lint:awesome-badge--&gt; 
&lt;p&gt;&lt;a href="https://awesome.re"&gt;&lt;img src="https://awesome.re/badge-flat2.svg?sanitize=true" alt="Awesome"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!--lint enable remark-lint:awesome-badge--&gt; 
&lt;!--lint disable double-link--&gt; 
&lt;p&gt;This is a curated list of slash-commands, &lt;code&gt;CLAUDE.md&lt;/code&gt; files, CLI tools, and other resources and guides for enhancing your &lt;a href="https://docs.anthropic.com/en/docs/claude-code"&gt;Claude Code&lt;/a&gt; workflow, productivity, and vibes.&lt;/p&gt; 
&lt;!--lint enable double-link--&gt; 
&lt;p&gt;Claude Code is a cutting-edge CLI-based coding assistant and agent that you can access in your terminal or IDE. It is a rapidly evolving tool that offers a number of powerful capabilities, and allows for a lot of configuration, in a lot of different ways. Users are actively working out best practices and workflows. It is the hope that this repo will help the community share knowledge and understand how to get the most out of Claude Code.&lt;/p&gt; 
&lt;h3&gt;Announcements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;2025-07-18 - I ended up over-engineering the submission workflow, but I think it's done, I just have to smoke test it and update the docs. For anyone with existing PR's, don't worry about updating them (for formatting purposes, that is), I can take care of it myself. For anoyne with new PR's, you &lt;em&gt;should&lt;/em&gt; be able to run &lt;code&gt;make submit&lt;/code&gt; from the root directory of your fork for an interactive experience (as I said, needs smoke testing) - alternatively, add your entry to the bottom of &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/THE_RESOURCES_TABLE.csv"&gt;&lt;code&gt;THE_RESOURCES_TABLE&lt;/code&gt;&lt;/a&gt; and run &lt;code&gt;make generate&lt;/code&gt; to automatically update the &lt;code&gt;README.md&lt;/code&gt; based on the information you filled in. If it's not working, just open a PR with the relevant information and I'll deal with it, I created this mess anyway üòÉ.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br&gt; 
&lt;h2&gt;Contents&lt;/h2&gt; 
&lt;p&gt;‚ñ™&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#workflows--knowledge-guides-"&gt;Workflows &amp;amp; Knowledge Guides&lt;/a&gt;&lt;br&gt; ‚ñ™&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#tooling-"&gt;Tooling&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;‚ñ´&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#ide-integrations"&gt;IDE Integrations&lt;/a&gt;&lt;br&gt; ‚ñ™&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#hooks-"&gt;Hooks&lt;/a&gt;&lt;br&gt; ‚ñ™&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#slash-commands-"&gt;Slash-Commands&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;‚ñ´&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#version-control--git"&gt;Version Control &amp;amp; Git&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;‚ñ´&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#code-analysis--testing"&gt;Code Analysis &amp;amp; Testing&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;‚ñ´&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#context-loading--priming"&gt;Context Loading &amp;amp; Priming&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;‚ñ´&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#documentation--changelogs"&gt;Documentation &amp;amp; Changelogs&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;‚ñ´&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#ci--deployment"&gt;CI / Deployment&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;‚ñ´&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#project--task-management"&gt;Project &amp;amp; Task Management&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;‚ñ´&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#miscellaneous"&gt;Miscellaneous&lt;/a&gt;&lt;br&gt; ‚ñ™&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#claudemd-files-"&gt;CLAUDE.md Files&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;‚ñ´&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#language-specific"&gt;Language-Specific&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;‚ñ´&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#domain-specific"&gt;Domain-Specific&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;‚ñ´&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#project-scaffolding--mcp"&gt;Project Scaffolding &amp;amp; MCP&lt;/a&gt;&lt;br&gt; ‚ñ™&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#official-documentation-"&gt;Official Documentation&lt;/a&gt;&lt;/p&gt; 
&lt;br&gt; 
&lt;h2&gt;Workflows &amp;amp; Knowledge Guides üß†&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A &lt;strong&gt;workflow&lt;/strong&gt; is a tightly coupled set of Claude Code-native resources that facilitate specific projects&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://github.com/cloudartisan/cloudartisan.github.io/tree/main/.claude/commands"&gt;&lt;code&gt;Blogging Platform Instructions&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/cloudartisan"&gt;cloudartisan&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;CC-BY-SA-4.0&lt;br&gt; Provides a well-structured set of commands for publishing and maintaining a blogging platform, including commands for creating posts, managing categories, and handling media files.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://claudelog.com"&gt;&lt;code&gt;ClaudeLog&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://www.reddit.com/user/inventor_black/"&gt;InventorBlack&lt;/a&gt;&lt;br&gt; A comprehensive knowledge repository that features detailed breakdowns of advanced Claude Code mechanics including &lt;a href="https://claudelog.com/mechanics/claude-md-supremacy"&gt;CLAUDE.md best practices&lt;/a&gt;, practical technique guides like &lt;a href="https://claudelog.com/mechanics/plan-mode"&gt;plan mode&lt;/a&gt;, and a &lt;a href="https://claudelog.com/configuration"&gt;configuration guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/disler/just-prompt/tree/main/.claude/commands"&gt;&lt;code&gt;Context Priming&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/disler"&gt;disler&lt;/a&gt;&lt;br&gt; Provides a systematic approach to priming Claude Code with comprehensive project context through specialized commands for different project scenarios and development contexts.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/kingler/n8n_agent/tree/main/.claude/commands"&gt;&lt;code&gt;n8n_agent&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/kingler"&gt;kingler&lt;/a&gt;&lt;br&gt; Amazing comprehensive set of comments for code analysis, QA, design, documentation, project structure, project management, optimization, and many more.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/steadycursor/steadystart/tree/main/.claude/commands"&gt;&lt;code&gt;Project Bootstrapping and Task Management&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/steadycursor"&gt;steadycursor&lt;/a&gt;&lt;br&gt; Provides a structured set of commands for bootstrapping and managing a new project, including meta-commands for creating and editing custom slash-commands.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/scopecraft/command/tree/main/.claude/commands"&gt;&lt;code&gt;Project Management, Implementation, Planning, and Release&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/scopecraft"&gt;scopecraft&lt;/a&gt;&lt;br&gt; Really comprehensive set of commands for all aspects of SDLC.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/harperreed/dotfiles/tree/master/.claude/commands"&gt;&lt;code&gt;Project Workflow System&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/harperreed"&gt;harperreed&lt;/a&gt;&lt;br&gt; A set of commands that provide a comprehensive workflow system for managing projects, including task management, code review, and deployment processes.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://diwank.space/field-notes-from-shipping-real-code-with-claude"&gt;&lt;code&gt;Shipping Real Code w/ Claude&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/creatorrr"&gt;Diwank&lt;/a&gt;&lt;br&gt; A detailed blog post explaining the author's process for shipping a product with Claude Code, including CLAUDE.md files and other interesting resources.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Helmi/claude-simone"&gt;&lt;code&gt;Simone&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Helmi"&gt;Helmi&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; A broader project management workflow for Claude Code that encompasses not just a set of commands, but a system of documents, guidelines, and processes to facilitate project planning and execution.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/wcygan/dotfiles/tree/d8ab6b9f5a7a81007b7f5fa3025d4f83ce12cc02/claude/commands"&gt;&lt;code&gt;Slash-commands megalist&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/wcygan"&gt;wcygan&lt;/a&gt;&lt;br&gt; A pretty stunning list (88 at the time of this post!) of slash-commands ranging from agent orchestration, code review, project management, security, documentation, self-assessment, almost anything you can dream of.&lt;/p&gt; 
&lt;br&gt; 
&lt;h2&gt;Tooling üß∞&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Tooling&lt;/strong&gt; denotes applications that are built on top of Claude Code and consist of more components than slash-commands and &lt;code&gt;CLAUDE.md&lt;/code&gt; files&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://github.com/ryoppippi/ccusage"&gt;&lt;code&gt;CC Usage&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/ryoppippi"&gt;ryoppippi&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Handy CLI tool for managing and analyzing Claude Code usage, based on analyzing local Claude Code logs. Presents a nice dashboard regarding cost information, token consumption, etc.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/nyatinte/ccexp"&gt;&lt;code&gt;ccexp&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/nyatinte"&gt;nyatinte&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Interactive CLI tool for discovering and managing Claude Code configuration files and slash commands with a beautiful terminal UI.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/ruvnet/claude-code-flow"&gt;&lt;code&gt;Claude Code Flow&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/ruvnet"&gt;ruvnet&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; This mode serves as a code-first orchestration layer, enabling Claude to write, edit, test, and optimize code autonomously across recursive agent cycles.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/possibilities/claude-composer"&gt;&lt;code&gt;Claude Composer&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/possibilities"&gt;Mike Bannister&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;Unlicense&lt;br&gt; A tool that adds small enhancements to Claude Code.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/claude-did-this/claude-hub"&gt;&lt;code&gt;Claude Hub&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/claude-did-this"&gt;Claude Did This&lt;/a&gt;&lt;br&gt; A webhook service that connects Claude Code to GitHub repositories, enabling AI-powered code assistance directly through pull requests and issues. This integration allows Claude to analyze repositories, answer technical questions, and help developers understand and improve their codebase through simple @mentions.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/smtg-ai/claude-squad"&gt;&lt;code&gt;Claude Squad&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/smtg-ai"&gt;smtg-ai&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;AGPL-3.0&lt;br&gt; Claude Squad is a terminal app that manages multiple Claude Code, Codex (and other local agents including Aider) in separate workspaces, allowing you to work on multiple tasks simultaneously.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/parruda/claude-swarm"&gt;&lt;code&gt;Claude Swarm&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/parruda"&gt;parruda&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Launch Claude Code session that is connected to a swarm of Claude Code Agents.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/eyaltoledano/claude-task-master"&gt;&lt;code&gt;Claude Task Master&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/eyaltoledano"&gt;eyaltoledano&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br&gt; A task management system for AI-driven development with Claude, designed to work seamlessly with Cursor AI.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/grahama1970/claude-task-runner"&gt;&lt;code&gt;Claude Task Runner&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/grahama1970"&gt;grahama1970&lt;/a&gt;&lt;br&gt; A specialized tool to manage context isolation and focused task execution with Claude Code, solving the critical challenge of context length limitations and task focus when working with Claude on complex, multi-step projects.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/dagger/container-use"&gt;&lt;code&gt;Container Use&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/dagger"&gt;dagger&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; Development environments for coding agents. Enable multiple agents to work safely and independently with your preferred stack.&lt;/p&gt; 
&lt;h3&gt;IDE Integrations&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/stevemolitor/claude-code.el"&gt;&lt;code&gt;claude-code.el&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/stevemolitor"&gt;stevemolitor&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; An Emacs interface for Claude Code CLI.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/greggh/claude-code.nvim"&gt;&lt;code&gt;claude-code.nvim&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/greggh"&gt;greggh&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; A seamless integration between Claude Code AI assistant and Neovim.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/stravu/crystal"&gt;&lt;code&gt;crystal&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/stravu"&gt;stravu&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; A full-fledged desktop application for orchestrating, monitoring, and interacting with Claude Code agents.&lt;/p&gt; 
&lt;br&gt; 
&lt;h2&gt;Hooks ü™ù&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Hooks&lt;/strong&gt; are a brand new API for Claude Code that allows users to activate commands and run scripts at different points in Claude's agentic lifecycle.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;[Experimental]&lt;/strong&gt; - The resources listed in this section have not been fully vetted and may not work as expected, given the bleeding-edge nature of Claude Code hooks. Nevertheless, I wished to include them at least as a source of inspiration and to explore this unknown terrain. YMMV!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/beyondcode/claude-hooks-sdk"&gt;&lt;code&gt;claude-code-hooks-sdk&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/beyondcode"&gt;beyondcode&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; A Laravel-inspired PHP SDK for building Claude Code hook responses with a clean, fluent API. This SDK makes it easy to create structured JSON responses for Claude Code hooks using an expressive, chainable interface.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/johnlindquist/claude-hooks"&gt;&lt;code&gt;claude-hooks&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/johnlindquist"&gt;John Lindquist&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; A TypeScript-based system for configuring and customizing Claude Code hooks with a powerful and flexible interface.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Veraticus/nix-config/tree/main/home-manager/claude-code/hooks"&gt;&lt;code&gt;Linting, testing, and notifications (in go)&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Veraticus"&gt;Josh Symonds&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Nice set of hooks for enforcing code quality (linting, testing, notifications), with a nice configuration setup as well.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/nizos/tdd-guard"&gt;&lt;code&gt;TDD Guard&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/nizos"&gt;Nizar Selander&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; A hooks-driven system that monitors file operations in real-time and blocks changes that violate TDD principles.&lt;/p&gt; 
&lt;br&gt; 
&lt;h2&gt;Slash-Commands üî™&lt;/h2&gt; 
&lt;h3&gt;Version Control &amp;amp; Git&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/danielscholl/mvn-mcp-server/raw/main/.claude/commands/bug-fix.md"&gt;&lt;code&gt;/bug-fix&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/danielscholl"&gt;danielscholl&lt;/a&gt;&lt;br&gt; Streamlines bug fixing by creating a GitHub issue first, then a feature branch for implementing and thoroughly testing the solution before merging.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/evmts/tevm-monorepo/raw/main/.claude/commands/commit.md"&gt;&lt;code&gt;/commit&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/evmts"&gt;evmts&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Creates git commits using conventional commit format with appropriate emojis, following project standards and creating descriptive messages that explain the purpose of changes.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/steadycursor/steadystart/raw/main/.claude/commands/2-commit-fast.md"&gt;&lt;code&gt;/commit-fast&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/steadycursor"&gt;steadycursor&lt;/a&gt;&lt;br&gt; Automates git commit process by selecting the first suggested message, generating structured commits with consistent formatting while skipping manual confirmation and removing Claude co-Contributorship footer&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/toyamarinyon/giselle/raw/main/.claude/commands/create-pr.md"&gt;&lt;code&gt;/create-pr&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/toyamarinyon"&gt;toyamarinyon&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; Streamlines pull request creation by handling the entire workflow: creating a new branch, committing changes, formatting modified files with Biome, and submitting the PR.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/liam-hq/liam/raw/main/.claude/commands/create-pull-request.md"&gt;&lt;code&gt;/create-pull-request&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/liam-hq"&gt;liam-hq&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; Provides comprehensive PR creation guidance with GitHub CLI, enforcing title conventions, following template structure, and offering concrete command examples with best practices.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/evmts/tevm-monorepo/raw/main/.claude/commands/create-worktrees.md"&gt;&lt;code&gt;/create-worktrees&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/evmts"&gt;evmts&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Creates git worktrees for all open PRs or specific branches, handling branches with slashes, cleaning up stale worktrees, and supporting custom branch creation for development.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/jeremymailen/kotlinter-gradle/raw/master/.claude/commands/fix-github-issue.md"&gt;&lt;code&gt;/fix-github-issue&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/jeremymailen"&gt;jeremymailen&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; Analyzes and fixes GitHub issues using a structured approach with GitHub CLI for issue details, implementing necessary code changes, running tests, and creating proper commit messages.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/metabase/metabase/raw/master/.claude/commands/fix-issue.md"&gt;&lt;code&gt;/fix-issue&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/metabase"&gt;metabase&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br&gt; Addresses GitHub issues by taking issue number as parameter, analyzing context, implementing solution, and testing/validating the fix for proper integration.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/metabase/metabase/raw/master/.claude/commands/fix-pr.md"&gt;&lt;code&gt;/fix-pr&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/metabase"&gt;metabase&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br&gt; Fetches and fixes unresolved PR comments by automatically retrieving feedback, addressing reviewer concerns, making targeted code improvements, and streamlining the review process.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/evmts/tevm-monorepo/raw/main/.claude/commands/husky.md"&gt;&lt;code&gt;/husky&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/evmts"&gt;evmts&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Sets up and manages Husky Git hooks by configuring pre-commit hooks, establishing commit message standards, integrating with linting tools, and ensuring code quality on commits.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/arkavo-org/opentdf-rs/raw/main/.claude/commands/pr-review.md"&gt;&lt;code&gt;/pr-review&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/arkavo-org"&gt;arkavo-org&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Reviews pull request changes to provide feedback, check for issues, and suggest improvements before merging into the main codebase.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/giselles-ai/giselle/raw/main/.claude/commands/update-branch-name.md"&gt;&lt;code&gt;/update-branch-name&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/giselles-ai"&gt;giselles-ai&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; Updates branch names with proper prefixes and formats, enforcing naming conventions, supporting semantic prefixes, and managing remote branch updates.&lt;/p&gt; 
&lt;h3&gt;Code Analysis &amp;amp; Testing&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/rygwdn/slack-tools/raw/main/.claude/commands/check.md"&gt;&lt;code&gt;/check&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/rygwdn"&gt;rygwdn&lt;/a&gt;&lt;br&gt; Performs comprehensive code quality and security checks, featuring static analysis integration, security vulnerability scanning, code style enforcement, and detailed reporting.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Graphlet-AI/eridu/raw/main/.claude/commands/clean.md"&gt;&lt;code&gt;/clean&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Graphlet-AI"&gt;Graphlet-AI&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; Addresses code formatting and quality issues by fixing black formatting problems, organizing imports with isort, resolving flake8 linting issues, and correcting mypy type errors.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/kingler/n8n_agent/raw/main/.claude/commands/code_analysis.md"&gt;&lt;code&gt;/code_analysis&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/kingler"&gt;kingler&lt;/a&gt;&lt;br&gt; Provides a menu of advanced code analysis commands for deep inspection, including knowledge graph generation, optimization suggestions, and quality evaluation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/to4iki/ai-project-rules/raw/main/.claude/commands/optimize.md"&gt;&lt;code&gt;/optimize&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/to4iki"&gt;to4iki&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Analyzes code performance to identify bottlenecks, proposing concrete optimizations with implementation guidance for improved application performance.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/rzykov/metabase/raw/master/.claude/commands/repro-issue.md"&gt;&lt;code&gt;/repro-issue&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/rzykov"&gt;rzykov&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br&gt; Creates reproducible test cases for GitHub issues, ensuring tests fail reliably and documenting clear reproduction steps for developers.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/zscott/pane/raw/main/.claude/commands/tdd.md"&gt;&lt;code&gt;/tdd&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/zscott"&gt;zscott&lt;/a&gt;&lt;br&gt; Guides development using Test-Driven Development principles, enforcing Red-Green-Refactor discipline, integrating with git workflow, and managing PR creation.&lt;/p&gt; 
&lt;h3&gt;Context Loading &amp;amp; Priming&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/elizaOS/elizaos.github.io/raw/main/.claude/commands/context-prime.md"&gt;&lt;code&gt;/context-prime&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/elizaOS"&gt;elizaOS&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Primes Claude with comprehensive project understanding by loading repository structure, setting development context, establishing project goals, and defining collaboration parameters.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/okuvshynov/cubestat/raw/main/.claude/commands/initref.md"&gt;&lt;code&gt;/initref&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/okuvshynov"&gt;okuvshynov&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Initializes reference documentation structure with standard doc templates, API reference setup, documentation conventions, and placeholder content generation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/ethpandaops/xatu-data/raw/master/.claude/commands/load-llms-txt.md"&gt;&lt;code&gt;/load-llms-txt&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/ethpandaops"&gt;ethpandaops&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Loads LLM configuration files to context, importing specific terminology, model configurations, and establishing baseline terminology for AI discussions.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Mjvolk3/torchcell/raw/main/.claude/commands/load_coo_context.md"&gt;&lt;code&gt;/load_coo_context&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Mjvolk3"&gt;Mjvolk3&lt;/a&gt;&lt;br&gt; References specific files for sparse matrix operations, explains transform usage, compares with previous approaches, and sets data formatting context for development.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Mjvolk3/torchcell/raw/main/.claude/commands/load_dango_pipeline.md"&gt;&lt;code&gt;/load_dango_pipeline&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Mjvolk3"&gt;Mjvolk3&lt;/a&gt;&lt;br&gt; Sets context for model training by referencing pipeline files, establishing working context, and preparing for pipeline work with relevant documentation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/yzyydev/AI-Engineering-Structure/raw/main/.claude/commands/prime.md"&gt;&lt;code&gt;/prime&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/yzyydev"&gt;yzyydev&lt;/a&gt;&lt;br&gt; Sets up initial project context by viewing directory structure and reading key files, creating standardized context with directory visualization and key documentation focus.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/ddisisto/si/raw/main/.claude/commands/rsi.md"&gt;&lt;code&gt;/rsi&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/ddisisto"&gt;ddisisto&lt;/a&gt;&lt;br&gt; Reads all commands and key project files to optimize AI-assisted development by streamlining the process, loading command context, and setting up for better development workflow.&lt;/p&gt; 
&lt;h3&gt;Documentation &amp;amp; Changelogs&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/berrydev-ai/blockdoc-python/raw/main/.claude/commands/add-to-changelog.md"&gt;&lt;code&gt;/add-to-changelog&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/berrydev-ai"&gt;berrydev-ai&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Adds new entries to changelog files while maintaining format consistency, properly documenting changes, and following established project standards for version tracking.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/jerseycheese/Narraitor/tree/feature/issue-227-ai-suggestions/.claude/commands/analyze-issue.md"&gt;&lt;code&gt;/create-docs&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/jerseycheese"&gt;jerseycheese&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Analyzes code structure and purpose to create comprehensive documentation detailing inputs/outputs, behavior, user interaction flows, and edge cases with error handling.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/slunsford/coffee-analytics/raw/main/.claude/commands/docs.md"&gt;&lt;code&gt;/docs&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/slunsford"&gt;slunsford&lt;/a&gt;&lt;br&gt; Generates comprehensive documentation that follows project structure, documenting APIs and usage patterns with consistent formatting for better user understanding.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/hackdays-io/toban-contribution-viewer/raw/main/.claude/commands/explain-issue-fix.md"&gt;&lt;code&gt;/explain-issue-fix&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/hackdays-io"&gt;hackdays-io&lt;/a&gt;&lt;br&gt; Documents solution approaches for GitHub issues, explaining technical decisions, detailing challenges overcome, and providing implementation context for better understanding.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Consiliency/Flutter-Structurizr/raw/main/.claude/commands/update-docs.md"&gt;&lt;code&gt;/update-docs&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Consiliency"&gt;Consiliency&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Reviews current documentation status, updates implementation progress, reviews phase documents, and maintains documentation consistency across the project.&lt;/p&gt; 
&lt;h3&gt;CI / Deployment&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/kelp/webdown/raw/main/.claude/commands/release.md"&gt;&lt;code&gt;/release&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/kelp"&gt;kelp&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Manages software releases by updating changelogs, reviewing README changes, evaluating version increments, and documenting release changes for better version tracking.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/hackdays-io/toban-contribution-viewer/raw/main/.claude/commands/run-ci.md"&gt;&lt;code&gt;/run-ci&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/hackdays-io"&gt;hackdays-io&lt;/a&gt;&lt;br&gt; Activates virtual environments, runs CI-compatible check scripts, iteratively fixes errors, and ensures all tests pass before completion.&lt;/p&gt; 
&lt;h3&gt;Project &amp;amp; Task Management&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/scopecraft/command/raw/main/.claude/commands/create-command.md"&gt;&lt;code&gt;/create-command&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/scopecraft"&gt;scopecraft&lt;/a&gt;&lt;br&gt; Guides Claude through creating new custom commands with proper structure by analyzing requirements, templating commands by category, enforcing command standards, and creating supporting documentation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/taddyorg/inkverse/raw/main/.claude/commands/create-jtbd.md"&gt;&lt;code&gt;/create-jtbd&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/taddyorg"&gt;taddyorg&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;AGPL-3.0&lt;br&gt; Creates Jobs-to-be-Done frameworks that outline user needs with structured format, focusing on specific user problems and organizing by job categories for product development.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/taddyorg/inkverse/raw/main/.claude/commands/create-prd.md"&gt;&lt;code&gt;/create-prd&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/taddyorg"&gt;taddyorg&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;AGPL-3.0&lt;br&gt; Generates comprehensive product requirement documents outlining detailed specifications, requirements, and features following standardized document structure and format.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Wirasm/claudecode-utils/raw/main/.claude/commands/create-prp.md"&gt;&lt;code&gt;/create-prp&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Wirasm"&gt;Wirasm&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Creates product requirement plans by reading PRP methodology, following template structure, creating comprehensive requirements, and structuring product definitions for development.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/disler/just-prompt/raw/main/.claude/commands/project_hello_w_name.md"&gt;&lt;code&gt;/project_hello_w_name&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/disler"&gt;disler&lt;/a&gt;&lt;br&gt; Creates customizable greeting components with name input, demonstrating argument passing, component reusability, state management, and user input handling.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/chrisleyva/todo-slash-command/raw/main/todo.md"&gt;&lt;code&gt;/todo&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/chrisleyva"&gt;chrisleyva&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; A convenient command to quickly manage project todo items without leaving the Claude Code interface, featuring due dates, sorting, task prioritization, and comprehensive todo list management.&lt;/p&gt; 
&lt;h3&gt;Miscellaneous&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/TuckerTucker/tkr-portfolio/raw/main/.claude/commands/five.md"&gt;&lt;code&gt;/five&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/TuckerTucker"&gt;TuckerTucker&lt;/a&gt;&lt;br&gt; Applies the "five whys" methodology to perform root cause analysis, identify underlying issues, and create solution approaches for complex problems.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Mjvolk3/torchcell/raw/main/.claude/commands/fixing_go_in_graph.md"&gt;&lt;code&gt;/fixing_go_in_graph&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Mjvolk3"&gt;Mjvolk3&lt;/a&gt;&lt;br&gt; Focuses on Gene Ontology annotation integration in graph databases, handling multiple data sources, addressing graph representation issues, and ensuring correct data incorporation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/GaloyMoney/lana-bank/raw/main/.claude/commands/mermaid.md"&gt;&lt;code&gt;/mermaid&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/GaloyMoney"&gt;GaloyMoney&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br&gt; Generates Mermaid diagrams from SQL schema files, creating entity relationship diagrams with table properties, validating diagram compilation, and ensuring complete entity coverage.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Mjvolk3/torchcell/raw/main/.claude/commands/review_dcell_model.md"&gt;&lt;code&gt;/review_dcell_model&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Mjvolk3"&gt;Mjvolk3&lt;/a&gt;&lt;br&gt; Reviews old Dcell implementation files, comparing with newer Dango model, noting changes over time, and analyzing refactoring approaches for better code organization.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/zuplo/docs/raw/main/.claude/commands/use-stepper.md"&gt;&lt;code&gt;/use-stepper&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/zuplo"&gt;zuplo&lt;/a&gt;&lt;br&gt; Reformats documentation to use React Stepper component, transforming heading formats, applying proper indentation, and maintaining markdown compatibility with admonition formatting.&lt;/p&gt; 
&lt;br&gt; 
&lt;h2&gt;CLAUDE.md Files üìÇ&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;code&gt;CLAUDE.md&lt;/code&gt; files&lt;/strong&gt; are files that contain important guidelines and context-specfic information or instructions that help Claude Code to better understand your project and your coding standards&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Language-Specific&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/didalgolab/ai-intellij-plugin/raw/main/CLAUDE.md"&gt;&lt;code&gt;AI IntelliJ Plugin&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/didalgolab"&gt;didalgolab&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; Provides comprehensive Gradle commands for IntelliJ plugin development with platform-specific coding patterns, detailed package structure guidelines, and clear internationalization standards.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/alexei-led/aws-mcp-server/raw/main/CLAUDE.md"&gt;&lt;code&gt;AWS MCP Server&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/alexei-led"&gt;alexei-led&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Features multiple Python environment setup options with detailed code style guidelines, comprehensive error handling recommendations, and security considerations for AWS CLI interactions.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/touchlab/DroidconKotlin/raw/main/CLAUDE.md"&gt;&lt;code&gt;DroidconKotlin&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/touchlab"&gt;touchlab&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; Delivers comprehensive Gradle commands for cross-platform Kotlin Multiplatform development with clear module structure and practical guidance for dependency injection.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/expectedparrot/edsl/raw/main/CLAUDE.md"&gt;&lt;code&gt;EDSL&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/expectedparrot"&gt;expectedparrot&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Offers detailed build and test commands with strict code style enforcement, comprehensive testing requirements, and standardized development workflow using Black and mypy.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/giselles-ai/giselle/raw/main/CLAUDE.md"&gt;&lt;code&gt;Giselle&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/giselles-ai"&gt;giselles-ai&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; Provides detailed build and test commands using pnpm and Vitest with strict code formatting requirements and comprehensive naming conventions for code consistency.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/hashintel/hash/raw/main/CLAUDE.md"&gt;&lt;code&gt;HASH&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/hashintel"&gt;hashintel&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br&gt; Features comprehensive repository structure breakdown with strong emphasis on coding standards, detailed Rust documentation guidelines, and systematic PR review process.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/inkline/inkline/raw/main/CLAUDE.md"&gt;&lt;code&gt;Inkline&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/inkline"&gt;inkline&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br&gt; Structures development workflow using pnpm with emphasis on TypeScript and Vue 3 Composition API, detailed component creation process, and comprehensive testing recommendations.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/mattgodbolt/jsbeeb/raw/main/CLAUDE.md"&gt;&lt;code&gt;JSBeeb&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/mattgodbolt"&gt;mattgodbolt&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;GPL-3.0&lt;br&gt; Provides development guide for JavaScript BBC Micro emulator with build and testing instructions, architecture documentation, and debugging workflows.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/LamoomAI/lamoom-python/raw/main/CLAUDE.md"&gt;&lt;code&gt;Lamoom Python&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/LamoomAI"&gt;LamoomAI&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; Serves as reference for production prompt engineering library with load balancing of AI Models, API documentation, and usage patterns with examples.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/langchain-ai/langgraphjs/raw/main/CLAUDE.md"&gt;&lt;code&gt;LangGraphJS&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/langchain-ai"&gt;langchain-ai&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Offers comprehensive build and test commands with detailed TypeScript style guidelines, layered library architecture, and monorepo structure using yarn workspaces.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/metabase/metabase/raw/master/CLAUDE.md"&gt;&lt;code&gt;Metabase&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/metabase"&gt;metabase&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br&gt; Details workflow for REPL-driven development in Clojure/ClojureScript with emphasis on incremental development, testing, and step-by-step approach for feature implementation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/sgcarstrends/backend/raw/main/CLAUDE.md"&gt;&lt;code&gt;SG Cars Trends Backend&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/sgcarstrends"&gt;sgcarstrends&lt;/a&gt;&lt;br&gt; Provides comprehensive structure for TypeScript monorepo projects with detailed commands for development, testing, deployment, and AWS/Cloudflare integration.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/spylang/spy/raw/main/CLAUDE.md"&gt;&lt;code&gt;SPy&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/spylang"&gt;spylang&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Enforces strict coding conventions with comprehensive testing guidelines, multiple code compilation options, and backend-specific test decorators for targeted filtering.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/KarpelesLab/tpl/raw/master/CLAUDE.md"&gt;&lt;code&gt;TPL&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/KarpelesLab"&gt;KarpelesLab&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Details Go project conventions with comprehensive error handling recommendations, table-driven testing approach guidelines, and modernization suggestions for latest Go features.&lt;/p&gt; 
&lt;h3&gt;Domain-Specific&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/Layr-Labs/avs-vibe-developer-guide/raw/master/CLAUDE.md"&gt;&lt;code&gt;AVS Vibe Developer Guide&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Layr-Labs"&gt;Layr-Labs&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Structures AI-assisted EigenLayer AVS development workflow with consistent naming conventions for prompt files and established terminology standards for blockchain concepts.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/CommE2E/comm/raw/master/CLAUDE.md"&gt;&lt;code&gt;Comm&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/CommE2E"&gt;CommE2E&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;BSD-3-Clause&lt;br&gt; Serves as a development reference for E2E-encrypted messaging applications with code organization architecture, security implementation details, and testing procedures.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/badass-courses/course-builder/raw/main/CLAUDE.md"&gt;&lt;code&gt;Course Builder&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/badass-courses"&gt;badass-courses&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Enables real-time multiplayer capabilities for collaborative course creation with diverse tech stack integration and monorepo architecture using Turborepo.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/eastlondoner/cursor-tools/raw/main/CLAUDE.md"&gt;&lt;code&gt;Cursor Tools&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/eastlondoner"&gt;eastlondoner&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Creates a versatile AI command interface supporting multiple providers and models with flexible command options and browser automation through "Stagehand" feature.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/soramimi/Guitar/raw/master/CLAUDE.md"&gt;&lt;code&gt;Guitar&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/soramimi"&gt;soramimi&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;GPL-2.0&lt;br&gt; Serves as development guide for Guitar Git GUI Client with build commands for various platforms, code style guidelines for contributing, and project structure explanation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Fimeg/NetworkChronicles/raw/legacy-v1/CLAUDE.md"&gt;&lt;code&gt;Network Chronicles&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Fimeg"&gt;Fimeg&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Presents detailed implementation plan for AI-driven game characters with technical specifications for LLM integration, character guidelines, and service discovery mechanics.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/different-ai/note-companion/raw/master/CLAUDE.md"&gt;&lt;code&gt;Note Companion&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/different-ai"&gt;different-ai&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Provides detailed styling isolation techniques for Obsidian plugins using Tailwind with custom prefix to prevent style conflicts and practical troubleshooting steps.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/ParetoSecurity/pareto-mac/raw/main/CLAUDE.md"&gt;&lt;code&gt;Pareto Mac&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/ParetoSecurity"&gt;ParetoSecurity&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;GPL-3.0&lt;br&gt; Serves as development guide for Mac security audit tool with build instructions, contribution guidelines, testing procedures, and workflow documentation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/steadycursor/steadystart/raw/main/CLAUDE.md"&gt;&lt;code&gt;SteadyStart&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/steadycursor"&gt;steadycursor&lt;/a&gt;&lt;br&gt; Clear and direct instructives about style, permissions, Claude's "role", communications, and documentation of Claude Code sessions for other team members to stay abreast.&lt;/p&gt; 
&lt;h3&gt;Project Scaffolding &amp;amp; MCP&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/basicmachines-co/basic-memory/raw/main/CLAUDE.md"&gt;&lt;code&gt;Basic Memory&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/basicmachines-co"&gt;basicmachines-co&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;AGPL-3.0&lt;br&gt; Presents an innovative AI-human collaboration framework with Model Context Protocol for bidirectional LLM-markdown communication and flexible knowledge structure for complex projects.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/grahama1970/claude-code-mcp-enhanced/raw/main/CLAUDE.md"&gt;&lt;code&gt;claude-code-mcp-enhanced&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/grahama1970"&gt;grahama1970&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Provides detailed and emphatic instructions for Claude to follow as a coding agent, with testing guidance, code examples, and compliance checks.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Family-IT-Guy/perplexity-mcp/raw/main/CLAUDE.md"&gt;&lt;code&gt;Perplexity MCP&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Family-IT-Guy"&gt;Family-IT-Guy&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;ISC&lt;br&gt; Offers clear step-by-step installation instructions with multiple configuration options, detailed troubleshooting guidance, and concise architecture overview of the MCP protocol.&lt;/p&gt; 
&lt;br&gt; 
&lt;h2&gt;Official Documentation üèõÔ∏è&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Links to some of Anthropic's terrific documentation and resources regarding Claude Code&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;!--lint disable double-link--&gt; 
&lt;p&gt;&lt;a href="https://docs.anthropic.com/en/docs/claude-code"&gt;&lt;code&gt;Anthropic Documentation&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/anthropics"&gt;Anthropic&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;¬©&lt;br&gt; The official documentation for Claude Code, including installation instructions, usage guidelines, API references, tutorials, examples, loads of information that I won't list individually. Like Claude Code, the documentation is frequently updated.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/anthropics/anthropic-quickstarts/raw/main/CLAUDE.md"&gt;&lt;code&gt;Anthropic Quickstarts&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/anthropics"&gt;Anthropic&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Offers comprehensive development guides for three distinct AI-powered demo projects with standardized workflows, strict code style guidelines, and containerization instructions.&lt;/p&gt; 
&lt;h2&gt;Contributing üåª&lt;/h2&gt; 
&lt;p&gt;Please note that this project is released with a &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/code-of-conduct.md"&gt;Contributor Code of Conduct&lt;/a&gt;. By participating in this project you agree to abide by its terms.&lt;/p&gt; 
&lt;p&gt;Regarding content, we especially welcome:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Proven, effective resources that follow best practices and may even be in use in production.&lt;/li&gt; 
 &lt;li&gt;Innovative, creative, or experimental workflows that perhaps are still being iterated upon, but have high potential value, and push the boundaries of Claude Code's documented capabilities and use cases.&lt;/li&gt; 
 &lt;li&gt;Additional libraries and tooling that are built on top of Claude Code and offer enhanced functionality.&lt;/li&gt; 
 &lt;li&gt;Applications of Claude Code outside of the traditional "coding assistant" context, e.g., CI/CD integration, testing, documentation, dev-ops, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for more information on how to contribute to this project. Or, fire up Claude Code and invoke the &lt;code&gt;/project:add-new-resource&lt;/code&gt; command and let Claude walk you through it!&lt;/p&gt; 
&lt;p&gt;If you have any suggestions or thoughts on how to improve the repo, or how to best organize the list, feel free to start a Discussion topic. This is meant to be for the Claude Code community, and in general I prefer not to act on sole authority.&lt;/p&gt; 
&lt;h3&gt;A note about licenses&lt;/h3&gt; 
&lt;p&gt;Because simply listing a hyperlink does not qualify as redistribution, the license of the original source is not relevant to its inclusion. However, for posterity and convenience, we do host copies of all resources whose license permits it. Therefore, please include information about the resource's license. Additionally, take note: &lt;em&gt;if you do not include a LICENSE in your GitHub repo, then by default it is fully copyrighted and redistribution is not allowed&lt;/em&gt;. So, if you are intending to make an open source project, it's critical to pick from one of the many available open source licenses. This is just a reminder that without a LICENSE, your project is not open source (it's merely source-code-available) - it may of course still be included on this list, but this notice is to inform readers about the default rules regarding GitHub and LICENSE files. See &lt;a href="https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/licensing-a-repository"&gt;here&lt;/a&gt; for more details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>lastmile-ai/mcp-agent</title>
      <link>https://github.com/lastmile-ai/mcp-agent</link>
      <description>&lt;p&gt;Build effective agents using Model Context Protocol and simple workflow patterns&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://mcp-agent.com"&gt;&lt;img src="https://github.com/user-attachments/assets/c8d059e5-bd56-4ea2-a72d-807fb4897bde" alt="Logo" width="300"&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;em&gt;Build effective agents with Model Context Protocol using simple, composable patterns.&lt;/em&gt; &lt;/p&gt;
&lt;p align="center"&gt; &lt;a href="https://github.com/lastmile-ai/mcp-agent/tree/main/examples" target="_blank"&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://www.anthropic.com/research/building-effective-agents" target="_blank"&gt;&lt;strong&gt;Building Effective Agents&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://modelcontextprotocol.io/introduction" target="_blank"&gt;&lt;strong&gt;MCP&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://mcp-agent.com"&gt;&lt;img src="https://img.shields.io/badge/docs-8F?style=flat&amp;amp;link=https%3A%2F%2Fmcp-agent.com%2F"&gt;&lt;/a&gt;&lt;a&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/mcp-agent/"&gt;&lt;img src="https://img.shields.io/pypi/v/mcp-agent?color=%2334D058&amp;amp;label=pypi"&gt;&lt;/a&gt; &lt;a href="https://github.com/lastmile-ai/mcp-agent/issues"&gt;&lt;img src="https://img.shields.io/github/issues-raw/lastmile-ai/mcp-agent"&gt;&lt;/a&gt; &lt;a href="https://lmai.link/discord/mcp-agent"&gt;&lt;img src="https://shields.io/discord/1089284610329952357" alt="discord"&gt;&lt;/a&gt; &lt;img alt="Pepy Total Downloads" src="https://img.shields.io/pepy/dt/mcp-agent?label=pypi%20%7C%20downloads"&gt; &lt;a href="https://github.com/lastmile-ai/mcp-agent/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/pypi/l/mcp-agent"&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/13216" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13216" alt="lastmile-ai%2Fmcp-agent | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;&lt;code&gt;mcp-agent&lt;/code&gt;&lt;/strong&gt; is a simple, composable framework to build agents using &lt;a href="https://modelcontextprotocol.io/introduction"&gt;Model Context Protocol&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Inspiration&lt;/strong&gt;: Anthropic announced 2 foundational updates for AI application developers:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://www.anthropic.com/news/model-context-protocol"&gt;Model Context Protocol&lt;/a&gt; - a standardized interface to let any software be accessible to AI assistants via MCP servers.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.anthropic.com/research/building-effective-agents"&gt;Building Effective Agents&lt;/a&gt; - a seminal writeup on simple, composable patterns for building production-ready AI agents.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;code&gt;mcp-agent&lt;/code&gt; puts these two foundational pieces into an AI application framework:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;It handles the pesky business of managing the lifecycle of MCP server connections so you don't have to.&lt;/li&gt; 
 &lt;li&gt;It implements every pattern described in Building Effective Agents, and does so in a &lt;em&gt;composable&lt;/em&gt; way, allowing you to chain these patterns together.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bonus&lt;/strong&gt;: It implements &lt;a href="https://github.com/openai/swarm"&gt;OpenAI's Swarm&lt;/a&gt; pattern for multi-agent orchestration, but in a model-agnostic way.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Altogether, this is the simplest and easiest way to build robust agent applications. Much like MCP, this project is in early development. We welcome all kinds of &lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/CONTRIBUTING.md"&gt;contributions&lt;/a&gt;, feedback and your help in growing this to become a new standard.&lt;/p&gt; 
&lt;h2&gt;Get Started&lt;/h2&gt; 
&lt;p&gt;We recommend using &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt; to manage your Python projects:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add "mcp-agent"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install mcp-agent
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Quickstart&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] The &lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/examples"&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt; directory has several example applications to get started with. To run an example, clone this repo, then:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;cd examples/basic/mcp_basic_agent # Or any other example
cp mcp_agent.secrets.yaml.example mcp_agent.secrets.yaml # Update API keys
uv run main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Here is a basic "finder" agent that uses the fetch and filesystem servers to look up a file, read a blog and write a tweet. &lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/examples/basic/mcp_basic_agent/"&gt;Example link&lt;/a&gt;:&lt;/p&gt; 
&lt;details open&gt; 
 &lt;summary&gt;finder_agent.py&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
import os

from mcp_agent.app import MCPApp
from mcp_agent.agents.agent import Agent
from mcp_agent.workflows.llm.augmented_llm_openai import OpenAIAugmentedLLM

app = MCPApp(name="hello_world_agent")

async def example_usage():
    async with app.run() as mcp_agent_app:
        logger = mcp_agent_app.logger
        # This agent can read the filesystem or fetch URLs
        finder_agent = Agent(
            name="finder",
            instruction="""You can read local files or fetch URLs.
                Return the requested information when asked.""",
            server_names=["fetch", "filesystem"], # MCP servers this Agent can use
        )

        async with finder_agent:
            # Automatically initializes the MCP servers and adds their tools for LLM use
            tools = await finder_agent.list_tools()
            logger.info(f"Tools available:", data=tools)

            # Attach an OpenAI LLM to the agent (defaults to GPT-4o)
            llm = await finder_agent.attach_llm(OpenAIAugmentedLLM)

            # This will perform a file lookup and read using the filesystem server
            result = await llm.generate_str(
                message="Show me what's in README.md verbatim"
            )
            logger.info(f"README.md contents: {result}")

            # Uses the fetch server to fetch the content from URL
            result = await llm.generate_str(
                message="Print the first two paragraphs from https://www.anthropic.com/research/building-effective-agents"
            )
            logger.info(f"Blog intro: {result}")

            # Multi-turn interactions by default
            result = await llm.generate_str("Summarize that in a 128-char tweet")
            logger.info(f"Tweet: {result}")

if __name__ == "__main__":
    asyncio.run(example_usage())

&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;mcp_agent.config.yaml&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;execution_engine: asyncio
logger:
  transports: [console] # You can use [file, console] for both
  level: debug
  path: "logs/mcp-agent.jsonl" # Used for file transport
  # For dynamic log filenames:
  # path_settings:
  #   path_pattern: "logs/mcp-agent-{unique_id}.jsonl"
  #   unique_id: "timestamp"  # Or "session_id"
  #   timestamp_format: "%Y%m%d_%H%M%S"

mcp:
  servers:
    fetch:
      command: "uvx"
      args: ["mcp-server-fetch"]
    filesystem:
      command: "npx"
      args:
        [
          "-y",
          "@modelcontextprotocol/server-filesystem",
          "&amp;lt;add_your_directories&amp;gt;",
        ]

openai:
  # Secrets (API keys, etc.) are stored in an mcp_agent.secrets.yaml file which can be gitignored
  default_model: gpt-4o
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Agent output&lt;/summary&gt; 
 &lt;img width="2398" alt="Image" src="https://github.com/user-attachments/assets/eaa60fdf-bcc6-460b-926e-6fa8534e9089"&gt; 
&lt;/details&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/#why-use-mcp-agent"&gt;Why use mcp-agent?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/#examples"&gt;Example Applications&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/#claude-desktop"&gt;Claude Desktop&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/#streamlit"&gt;Streamlit&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/#gmail-agent"&gt;Gmail Agent&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/#simple-rag-chatbot"&gt;RAG&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/#marimo"&gt;Marimo&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/#python"&gt;Python&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/#swarm"&gt;Swarm (CLI)&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/#core-components"&gt;Core Concepts&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/#workflows"&gt;Workflows Patterns&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/#augmentedllm"&gt;Augmented LLM&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/#parallel"&gt;Parallel&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/#router"&gt;Router&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/#intentclassifier"&gt;Intent-Classifier&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/#orchestrator-workers"&gt;Orchestrator-Workers&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/#evaluator-optimizer"&gt;Evaluator-Optimizer&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/#swarm-1"&gt;OpenAI Swarm&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/#advanced"&gt;Advanced&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/#composability"&gt;Composing multiple workflows&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/#signaling-and-human-input"&gt;Signaling and Human input&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/#app-config"&gt;App Config&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/#mcp-server-management"&gt;MCP Server Management&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/#roadmap"&gt;Roadmap&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/#faqs"&gt;FAQs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Why use &lt;code&gt;mcp-agent&lt;/code&gt;?&lt;/h2&gt; 
&lt;p&gt;There are too many AI frameworks out there already. But &lt;code&gt;mcp-agent&lt;/code&gt; is the only one that is purpose-built for a shared protocol - &lt;a href="https://modelcontextprotocol.io/introduction"&gt;MCP&lt;/a&gt;. It is also the most lightweight, and is closer to an agent pattern library than a framework.&lt;/p&gt; 
&lt;p&gt;As &lt;a href="https://github.com/punkpeye/awesome-mcp-servers"&gt;more services become MCP-aware&lt;/a&gt;, you can use mcp-agent to build robust and controllable AI agents that can leverage those services out-of-the-box.&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;Before we go into the core concepts of mcp-agent, let's show what you can build with it.&lt;/p&gt; 
&lt;p&gt;In short, you can build any kind of AI application with mcp-agent: multi-agent collaborative workflows, human-in-the-loop workflows, RAG pipelines and more.&lt;/p&gt; 
&lt;h3&gt;Claude Desktop&lt;/h3&gt; 
&lt;p&gt;You can integrate mcp-agent apps into MCP clients like Claude Desktop.&lt;/p&gt; 
&lt;h4&gt;mcp-agent server&lt;/h4&gt; 
&lt;p&gt;This app wraps an mcp-agent application inside an MCP server, and exposes that server to Claude Desktop. The app exposes agents and workflows that Claude Desktop can invoke to service of the user's request.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/7807cffd-dba7-4f0c-9c70-9482fd7e0699"&gt;https://github.com/user-attachments/assets/7807cffd-dba7-4f0c-9c70-9482fd7e0699&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This demo shows a multi-agent evaluation task where each agent evaluates aspects of an input poem, and then an aggregator summarizes their findings into a final response.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;: Starting from a user's request over text, the application:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;dynamically defines agents to do the job&lt;/li&gt; 
 &lt;li&gt;uses the appropriate workflow to orchestrate those agents (in this case the Parallel workflow)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Link to code&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/examples/basic/mcp_server_aggregator"&gt;examples/basic/mcp_server_aggregator&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Huge thanks to &lt;a href="https://github.com/StreetLamb"&gt;Jerron Lim (@StreetLamb)&lt;/a&gt; for developing and contributing this example!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Streamlit&lt;/h3&gt; 
&lt;p&gt;You can deploy mcp-agent apps using Streamlit.&lt;/p&gt; 
&lt;h4&gt;Gmail agent&lt;/h4&gt; 
&lt;p&gt;This app is able to perform read and write actions on gmail using text prompts -- i.e. read, delete, send emails, mark as read/unread, etc. It uses an MCP server for Gmail.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/54899cac-de24-4102-bd7e-4b2022c956e3"&gt;https://github.com/user-attachments/assets/54899cac-de24-4102-bd7e-4b2022c956e3&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Link to code&lt;/strong&gt;: &lt;a href="https://github.com/jasonsum/gmail-mcp-server/raw/add-mcp-agent-streamlit/streamlit_app.py"&gt;gmail-mcp-server&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Huge thanks to &lt;a href="https://github.com/jasonsum"&gt;Jason Summer (@jasonsum)&lt;/a&gt; for developing and contributing this example!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Simple RAG Chatbot&lt;/h4&gt; 
&lt;p&gt;This app uses a Qdrant vector database (via an MCP server) to do Q&amp;amp;A over a corpus of text.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/f4dcd227-cae9-4a59-aa9e-0eceeb4acaf4"&gt;https://github.com/user-attachments/assets/f4dcd227-cae9-4a59-aa9e-0eceeb4acaf4&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Link to code&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/examples/usecases/streamlit_mcp_rag_agent/"&gt;examples/usecases/streamlit_mcp_rag_agent&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Huge thanks to &lt;a href="https://github.com/StreetLamb"&gt;Jerron Lim (@StreetLamb)&lt;/a&gt; for developing and contributing this example!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Marimo&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/marimo-team/marimo"&gt;Marimo&lt;/a&gt; is a reactive Python notebook that replaces Jupyter and Streamlit. Here's the "file finder" agent from &lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/#quickstart"&gt;Quickstart&lt;/a&gt; implemented in Marimo:&lt;/p&gt; 
&lt;img src="https://github.com/user-attachments/assets/139a95a5-e3ac-4ea7-9c8f-bad6577e8597" width="400"&gt; 
&lt;p&gt;&lt;strong&gt;Link to code&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/examples/usecases/marimo_mcp_basic_agent/"&gt;examples/usecases/marimo_mcp_basic_agent&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Huge thanks to &lt;a href="https://github.com/akshayka"&gt;Akshay Agrawal (@akshayka)&lt;/a&gt; for developing and contributing this example!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Python&lt;/h3&gt; 
&lt;p&gt;You can write mcp-agent apps as Python scripts or Jupyter notebooks.&lt;/p&gt; 
&lt;h4&gt;Swarm&lt;/h4&gt; 
&lt;p&gt;This example demonstrates a multi-agent setup for handling different customer service requests in an airline context using the Swarm workflow pattern. The agents can triage requests, handle flight modifications, cancellations, and lost baggage cases.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/b314d75d-7945-4de6-965b-7f21eb14a8bd"&gt;https://github.com/user-attachments/assets/b314d75d-7945-4de6-965b-7f21eb14a8bd&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Link to code&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/examples/workflows/workflow_swarm/"&gt;examples/workflows/workflow_swarm&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Core Components&lt;/h2&gt; 
&lt;p&gt;The following are the building blocks of the mcp-agent framework:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/src/mcp_agent/app.py"&gt;MCPApp&lt;/a&gt;&lt;/strong&gt;: global state and app configuration&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP server management&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/src/mcp_agent/mcp/gen_client.py"&gt;&lt;code&gt;gen_client&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/src/mcp_agent/mcp/mcp_connection_manager.py"&gt;&lt;code&gt;MCPConnectionManager&lt;/code&gt;&lt;/a&gt; to easily connect to MCP servers.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/src/mcp_agent/agents/agent.py"&gt;Agent&lt;/a&gt;&lt;/strong&gt;: An Agent is an entity that has access to a set of MCP servers and exposes them to an LLM as tool calls. It has a name and purpose (instruction).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/src/mcp_agent/workflows/llm/augmented_llm.py"&gt;AugmentedLLM&lt;/a&gt;&lt;/strong&gt;: An LLM that is enhanced with tools provided from a collection of MCP servers. Every Workflow pattern described below is an &lt;code&gt;AugmentedLLM&lt;/code&gt; itself, allowing you to compose and chain them together.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Everything in the framework is a derivative of these core capabilities.&lt;/p&gt; 
&lt;h2&gt;Workflows&lt;/h2&gt; 
&lt;p&gt;mcp-agent provides implementations for every pattern in Anthropic‚Äôs &lt;a href="https://www.anthropic.com/research/building-effective-agents"&gt;Building Effective Agents&lt;/a&gt;, as well as the OpenAI &lt;a href="https://github.com/openai/swarm"&gt;Swarm&lt;/a&gt; pattern. Each pattern is model-agnostic, and exposed as an &lt;code&gt;AugmentedLLM&lt;/code&gt;, making everything very composable.&lt;/p&gt; 
&lt;h3&gt;AugmentedLLM&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/src/mcp_agent/workflows/llm/augmented_llm.py"&gt;AugmentedLLM&lt;/a&gt; is an LLM that has access to MCP servers and functions via Agents.&lt;/p&gt; 
&lt;p&gt;LLM providers implement the AugmentedLLM interface to expose 3 functions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;generate&lt;/code&gt;: Generate message(s) given a prompt, possibly over multiple iterations and making tool calls as needed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;generate_str&lt;/code&gt;: Calls &lt;code&gt;generate&lt;/code&gt; and returns result as a string output.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;generate_structured&lt;/code&gt;: Uses &lt;a href="https://github.com/instructor-ai/instructor"&gt;Instructor&lt;/a&gt; to return the generated result as a Pydantic model.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Additionally, &lt;code&gt;AugmentedLLM&lt;/code&gt; has memory, to keep track of long or short-term history.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Example&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from mcp_agent.agents.agent import Agent
from mcp_agent.workflows.llm.augmented_llm_anthropic import AnthropicAugmentedLLM

finder_agent = Agent(
    name="finder",
    instruction="You are an agent with filesystem + fetch access. Return the requested file or URL contents.",
    server_names=["fetch", "filesystem"],
)

async with finder_agent:
   llm = await finder_agent.attach_llm(AnthropicAugmentedLLM)

   result = await llm.generate_str(
      message="Print the first 2 paragraphs of https://www.anthropic.com/research/building-effective-agents",
      # Can override model, tokens and other defaults
   )
   logger.info(f"Result: {result}")

   # Multi-turn conversation
   result = await llm.generate_str(
      message="Summarize those paragraphs in a 128 character tweet",
   )
   logger.info(f"Result: {result}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/src/mcp_agent/workflows/parallel/parallel_llm.py"&gt;Parallel&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F406bb032ca007fd1624f261af717d70e6ca86286-2401x1000.png&amp;amp;w=3840&amp;amp;q=75" alt="Parallel workflow (Image credit: Anthropic)"&gt;&lt;/p&gt; 
&lt;p&gt;Fan-out tasks to multiple sub-agents and fan-in the results. Each subtask is an AugmentedLLM, as is the overall Parallel workflow, meaning each subtask can optionally be a more complex workflow itself.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/examples/workflows/workflow_parallel/main.py"&gt;Link to full example&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details&gt; 
 &lt;summary&gt;Example&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;proofreader = Agent(name="proofreader", instruction="Review grammar...")
fact_checker = Agent(name="fact_checker", instruction="Check factual consistency...")
style_enforcer = Agent(name="style_enforcer", instruction="Enforce style guidelines...")

grader = Agent(name="grader", instruction="Combine feedback into a structured report.")

parallel = ParallelLLM(
    fan_in_agent=grader,
    fan_out_agents=[proofreader, fact_checker, style_enforcer],
    llm_factory=OpenAIAugmentedLLM,
)

result = await parallel.generate_str("Student short story submission: ...", RequestParams(model="gpt4-o"))
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/src/mcp_agent/workflows/router/"&gt;Router&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F5c0c0e9fe4def0b584c04d37849941da55e5e71c-2401x1000.png&amp;amp;w=3840&amp;amp;q=75" alt="Router workflow (Image credit: Anthropic)"&gt;&lt;/p&gt; 
&lt;p&gt;Given an input, route to the &lt;code&gt;top_k&lt;/code&gt; most relevant categories. A category can be an Agent, an MCP server or a regular function.&lt;/p&gt; 
&lt;p&gt;mcp-agent provides several router implementations, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/src/mcp_agent/workflows/router/router_embedding.py"&gt;&lt;code&gt;EmbeddingRouter&lt;/code&gt;&lt;/a&gt;: uses embedding models for classification&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/src/mcp_agent/workflows/router/router_llm.py"&gt;&lt;code&gt;LLMRouter&lt;/code&gt;&lt;/a&gt;: uses LLMs for classification&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/examples/workflows/workflow_router/main.py"&gt;Link to full example&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details&gt; 
 &lt;summary&gt;Example&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;def print_hello_world:
     print("Hello, world!")

finder_agent = Agent(name="finder", server_names=["fetch", "filesystem"])
writer_agent = Agent(name="writer", server_names=["filesystem"])

llm = OpenAIAugmentedLLM()
router = LLMRouter(
    llm=llm,
    agents=[finder_agent, writer_agent],
    functions=[print_hello_world],
)

results = await router.route( # Also available: route_to_agent, route_to_server
    request="Find and print the contents of README.md verbatim",
    top_k=1
)
chosen_agent = results[0].result
async with chosen_agent:
    ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/src/mcp_agent/workflows/intent_classifier/"&gt;IntentClassifier&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;A close sibling of Router, the Intent Classifier pattern identifies the &lt;code&gt;top_k&lt;/code&gt; Intents that most closely match a given input. Just like a Router, mcp-agent provides both an &lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/src/mcp_agent/workflows/intent_classifier/intent_classifier_embedding.py"&gt;embedding&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/src/mcp_agent/workflows/intent_classifier/intent_classifier_llm.py"&gt;LLM-based&lt;/a&gt; intent classifier.&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/src/mcp_agent/workflows/evaluator_optimizer/evaluator_optimizer.py"&gt;Evaluator-Optimizer&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F14f51e6406ccb29e695da48b17017e899a6119c7-2401x1000.png&amp;amp;w=3840&amp;amp;q=75" alt="Evaluator-optimizer workflow (Image credit: Anthropic)"&gt;&lt;/p&gt; 
&lt;p&gt;One LLM (the ‚Äúoptimizer‚Äù) refines a response, another (the ‚Äúevaluator‚Äù) critiques it until a response exceeds a quality criteria.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/examples/workflows/workflow_evaluator_optimizer/main.py"&gt;Link to full example&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details&gt; 
 &lt;summary&gt;Example&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;optimizer = Agent(name="cover_letter_writer", server_names=["fetch"], instruction="Generate a cover letter ...")
evaluator = Agent(name="critiquer", instruction="Evaluate clarity, specificity, relevance...")

llm = EvaluatorOptimizerLLM(
    optimizer=optimizer,
    evaluator=evaluator,
    llm_factory=OpenAIAugmentedLLM,
    min_rating=QualityRating.EXCELLENT, # Keep iterating until the minimum quality bar is reached
)

result = await eo_llm.generate_str("Write a job cover letter for an AI framework developer role at LastMile AI.")
print("Final refined cover letter:", result)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/src/mcp_agent/workflows/orchestrator/orchestrator.py"&gt;Orchestrator-workers&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F8985fc683fae4780fb34eab1365ab78c7e51bc8e-2401x1000.png&amp;amp;w=3840&amp;amp;q=75" alt="Orchestrator workflow (Image credit: Anthropic)"&gt;&lt;/p&gt; 
&lt;p&gt;A higher-level LLM generates a plan, then assigns them to sub-agents, and synthesizes the results. The Orchestrator workflow automatically parallelizes steps that can be done in parallel, and blocks on dependencies.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/examples/workflows/workflow_orchestrator_worker/main.py"&gt;Link to full example&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details&gt; 
 &lt;summary&gt;Example&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;finder_agent = Agent(name="finder", server_names=["fetch", "filesystem"])
writer_agent = Agent(name="writer", server_names=["filesystem"])
proofreader = Agent(name="proofreader", ...)
fact_checker = Agent(name="fact_checker", ...)
style_enforcer = Agent(name="style_enforcer", instructions="Use APA style guide from ...", server_names=["fetch"])

orchestrator = Orchestrator(
    llm_factory=AnthropicAugmentedLLM,
    available_agents=[finder_agent, writer_agent, proofreader, fact_checker, style_enforcer],
)

task = "Load short_story.md, evaluate it, produce a graded_report.md with multiple feedback aspects."
result = await orchestrator.generate_str(task, RequestParams(model="gpt-4o"))
print(result)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/src/mcp_agent/workflows/swarm/swarm.py"&gt;Swarm&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;OpenAI has an experimental multi-agent pattern called &lt;a href="https://github.com/openai/swarm"&gt;Swarm&lt;/a&gt;, which we provide a model-agnostic reference implementation for in mcp-agent.&lt;/p&gt; 
&lt;img src="https://github.com/openai/swarm/raw/main/assets/swarm_diagram.png?raw=true" width="500"&gt; 
&lt;p&gt;The mcp-agent Swarm pattern works seamlessly with MCP servers, and is exposed as an &lt;code&gt;AugmentedLLM&lt;/code&gt;, allowing for composability with other patterns above.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/examples/workflows/workflow_swarm/main.py"&gt;Link to full example&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details&gt; 
 &lt;summary&gt;Example&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;triage_agent = SwarmAgent(...)
flight_mod_agent = SwarmAgent(...)
lost_baggage_agent = SwarmAgent(...)

# The triage agent decides whether to route to flight_mod_agent or lost_baggage_agent
swarm = AnthropicSwarm(agent=triage_agent, context_variables={...})

test_input = "My bag was not delivered!"
result = await swarm.generate_str(test_input)
print("Result:", result)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Advanced&lt;/h2&gt; 
&lt;h3&gt;Composability&lt;/h3&gt; 
&lt;p&gt;An example of composability is using an &lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/#evaluator-optimizer"&gt;Evaluator-Optimizer&lt;/a&gt; workflow as the planner LLM inside the &lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/#orchestrator-workers"&gt;Orchestrator&lt;/a&gt; workflow. Generating a high-quality plan to execute is important for robust behavior, and an evaluator-optimizer can help ensure that.&lt;/p&gt; 
&lt;p&gt;Doing so is seamless in mcp-agent, because each workflow is implemented as an &lt;code&gt;AugmentedLLM&lt;/code&gt;.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Example&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;optimizer = Agent(name="plan_optimizer", server_names=[...], instruction="Generate a plan given an objective ...")
evaluator = Agent(name="plan_evaluator", instruction="Evaluate logic, ordering and precision of plan......")

planner_llm = EvaluatorOptimizerLLM(
    optimizer=optimizer,
    evaluator=evaluator,
    llm_factory=OpenAIAugmentedLLM,
    min_rating=QualityRating.EXCELLENT,
)

orchestrator = Orchestrator(
    llm_factory=AnthropicAugmentedLLM,
    available_agents=[finder_agent, writer_agent, proofreader, fact_checker, style_enforcer],
    planner=planner_llm # It's that simple
)

...
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Signaling and Human Input&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Signaling&lt;/strong&gt;: The framework can pause/resume tasks. The agent or LLM might ‚Äúsignal‚Äù that it needs user input, so the workflow awaits. A developer may signal during a workflow to seek approval or review before continuing with a workflow.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Human Input&lt;/strong&gt;: If an Agent has a &lt;code&gt;human_input_callback&lt;/code&gt;, the LLM can call a &lt;code&gt;__human_input__&lt;/code&gt; tool to request user input mid-workflow.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Example&lt;/summary&gt; 
 &lt;p&gt;The &lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/examples/workflows/workflow_swarm/main.py"&gt;Swarm example&lt;/a&gt; shows this in action.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from mcp_agent.human_input.handler import console_input_callback

lost_baggage = SwarmAgent(
    name="Lost baggage traversal",
    instruction=lambda context_variables: f"""
        {
        FLY_AIR_AGENT_PROMPT.format(
            customer_context=context_variables.get("customer_context", "None"),
            flight_context=context_variables.get("flight_context", "None"),
        )
    }\n Lost baggage policy: policies/lost_baggage_policy.md""",
    functions=[
        escalate_to_agent,
        initiate_baggage_search,
        transfer_to_triage,
        case_resolved,
    ],
    server_names=["fetch", "filesystem"],
    human_input_callback=console_input_callback, # Request input from the console
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;App Config&lt;/h3&gt; 
&lt;p&gt;Create an &lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/schema/mcp-agent.config.schema.json"&gt;&lt;code&gt;mcp_agent.config.yaml&lt;/code&gt;&lt;/a&gt; and a gitignored &lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/examples/basic/mcp_basic_agent/mcp_agent.secrets.yaml.example"&gt;&lt;code&gt;mcp_agent.secrets.yaml&lt;/code&gt;&lt;/a&gt; to define MCP app configuration. This controls logging, execution, LLM provider APIs, and MCP server configuration.&lt;/p&gt; 
&lt;h3&gt;MCP server management&lt;/h3&gt; 
&lt;p&gt;mcp-agent makes it trivial to connect to MCP servers. Create an &lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/schema/mcp-agent.config.schema.json"&gt;&lt;code&gt;mcp_agent.config.yaml&lt;/code&gt;&lt;/a&gt; to define server configuration under the &lt;code&gt;mcp&lt;/code&gt; section:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;mcp:
  servers:
    fetch:
      command: "uvx"
      args: ["mcp-server-fetch"]
      description: "Fetch content at URLs from the world wide web"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/src/mcp_agent/mcp/gen_client.py"&gt;&lt;code&gt;gen_client&lt;/code&gt;&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;Manage the lifecycle of an MCP server within an async context manager:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mcp_agent.mcp.gen_client import gen_client

async with gen_client("fetch") as fetch_client:
    # Fetch server is initialized and ready to use
    result = await fetch_client.list_tools()

# Fetch server is automatically disconnected/shutdown
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The gen_client function makes it easy to spin up connections to MCP servers.&lt;/p&gt; 
&lt;h4&gt;Persistent server connections&lt;/h4&gt; 
&lt;p&gt;In many cases, you want an MCP server to stay online for persistent use (e.g. in a multi-step tool use workflow). For persistent connections, use:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/(src/mcp_agent/mcp/gen_client.py)"&gt;&lt;code&gt;connect&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/src/mcp_agent/mcp/gen_client.py"&gt;&lt;code&gt;disconnect&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mcp_agent.mcp.gen_client import connect, disconnect

fetch_client = None
try:
     fetch_client = connect("fetch")
     result = await fetch_client.list_tools()
finally:
     disconnect("fetch")
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/src/mcp_agent/mcp/mcp_connection_manager.py"&gt;&lt;code&gt;MCPConnectionManager&lt;/code&gt;&lt;/a&gt; For even more fine-grained control over server connections, you can use the MCPConnectionManager.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Example&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from mcp_agent.context import get_current_context
from mcp_agent.mcp.mcp_connection_manager import MCPConnectionManager

context = get_current_context()
connection_manager = MCPConnectionManager(context.server_registry)

async with connection_manager:
fetch_client = await connection_manager.get_server("fetch") # Initializes fetch server
result = fetch_client.list_tool()
fetch_client2 = await connection_manager.get_server("fetch") # Reuses same server connection

# All servers managed by connection manager are automatically disconnected/shut down
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;MCP Server Aggregator&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/src/mcp_agent/mcp/mcp_aggregator.py"&gt;&lt;code&gt;MCPAggregator&lt;/code&gt;&lt;/a&gt; acts as a "server-of-servers". It provides a single MCP server interface for interacting with multiple MCP servers. This allows you to expose tools from multiple servers to LLM applications.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Example&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from mcp_agent.mcp.mcp_aggregator import MCPAggregator

aggregator = await MCPAggregator.create(server_names=["fetch", "filesystem"])

async with aggregator:
   # combined list of tools exposed by 'fetch' and 'filesystem' servers
   tools = await aggregator.list_tools()

   # namespacing -- invokes the 'fetch' server to call the 'fetch' tool
   fetch_result = await aggregator.call_tool(name="fetch-fetch", arguments={"url": "https://www.anthropic.com/research/building-effective-agents"})

   # no namespacing -- first server in the aggregator exposing that tool wins
   read_file_result = await aggregator.call_tool(name="read_file", arguments={})
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome any and all kinds of contributions. Please see the &lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/CONTRIBUTING.md"&gt;CONTRIBUTING guidelines&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;h3&gt;Special Mentions&lt;/h3&gt; 
&lt;p&gt;There have already been incredible community contributors who are driving this project forward:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/evalstate"&gt;Shaun Smith (@evalstate)&lt;/a&gt; -- who has been leading the charge on countless complex improvements, both to &lt;code&gt;mcp-agent&lt;/code&gt; and generally to the MCP ecosystem.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/StreetLamb"&gt;Jerron Lim (@StreetLamb)&lt;/a&gt; -- who has contributed countless hours and excellent examples, and great ideas to the project.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jasonsum"&gt;Jason Summer (@jasonsum)&lt;/a&gt; -- for identifying several issues and adapting his Gmail MCP server to work with mcp-agent&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;p&gt;We will be adding a detailed roadmap (ideally driven by your feedback). The current set of priorities include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Durable Execution&lt;/strong&gt; -- allow workflows to pause/resume and serialize state so they can be replayed or be paused indefinitely. We are working on integrating &lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/src/mcp_agent/executor/temporal.py"&gt;Temporal&lt;/a&gt; for this purpose.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt; -- adding support for long-term memory&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Streaming&lt;/strong&gt; -- Support streaming listeners for iterative progress&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Additional MCP capabilities&lt;/strong&gt; -- Expand beyond tool calls to support: 
  &lt;ul&gt; 
   &lt;li&gt;Resources&lt;/li&gt; 
   &lt;li&gt;Prompts&lt;/li&gt; 
   &lt;li&gt;Notifications&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FAQs&lt;/h2&gt; 
&lt;h3&gt;What are the core benefits of using mcp-agent?&lt;/h3&gt; 
&lt;p&gt;mcp-agent provides a streamlined approach to building AI agents using capabilities exposed by &lt;strong&gt;MCP&lt;/strong&gt; (Model Context Protocol) servers.&lt;/p&gt; 
&lt;p&gt;MCP is quite low-level, and this framework handles the mechanics of connecting to servers, working with LLMs, handling external signals (like human input) and supporting persistent state via durable execution. That lets you, the developer, focus on the core business logic of your AI application.&lt;/p&gt; 
&lt;p&gt;Core benefits:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ü§ù &lt;strong&gt;Interoperability&lt;/strong&gt;: ensures that any tool exposed by any number of MCP servers can seamlessly plug in to your agents.&lt;/li&gt; 
 &lt;li&gt;‚õìÔ∏è &lt;strong&gt;Composability &amp;amp; Customizability&lt;/strong&gt;: Implements well-defined workflows, but in a composable way that enables compound workflows, and allows full customization across model provider, logging, orchestrator, etc.&lt;/li&gt; 
 &lt;li&gt;üíª &lt;strong&gt;Programmatic control flow&lt;/strong&gt;: Keeps things simple as developers just write code instead of thinking in graphs, nodes and edges. For branching logic, you write &lt;code&gt;if&lt;/code&gt; statements. For cycles, use &lt;code&gt;while&lt;/code&gt; loops.&lt;/li&gt; 
 &lt;li&gt;üñêÔ∏è &lt;strong&gt;Human Input &amp;amp; Signals&lt;/strong&gt;: Supports pausing workflows for external signals, such as human input, which are exposed as tool calls an Agent can make.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Do you need an MCP client to use mcp-agent?&lt;/h3&gt; 
&lt;p&gt;No, you can use mcp-agent anywhere, since it handles MCPClient creation for you. This allows you to leverage MCP servers outside of MCP hosts like Claude Desktop.&lt;/p&gt; 
&lt;p&gt;Here's all the ways you can set up your mcp-agent application:&lt;/p&gt; 
&lt;h4&gt;MCP-Agent Server&lt;/h4&gt; 
&lt;p&gt;You can expose mcp-agent applications as MCP servers themselves (see &lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/examples/basic/mcp_agent_server"&gt;example&lt;/a&gt;), allowing MCP clients to interface with sophisticated AI workflows using the standard tools API of MCP servers. This is effectively a server-of-servers.&lt;/p&gt; 
&lt;h4&gt;MCP Client or Host&lt;/h4&gt; 
&lt;p&gt;You can embed mcp-agent in an MCP client directly to manage the orchestration across multiple MCP servers.&lt;/p&gt; 
&lt;h4&gt;Standalone&lt;/h4&gt; 
&lt;p&gt;You can use mcp-agent applications in a standalone fashion (i.e. they aren't part of an MCP client). The &lt;a href="https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/examples/"&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt; are all standalone applications.&lt;/p&gt; 
&lt;h3&gt;Tell me a fun fact&lt;/h3&gt; 
&lt;p&gt;I debated naming this project &lt;em&gt;silsila&lt;/em&gt; (ÿ≥ŸÑÿ≥ŸÑ€Å), which means chain of events in Urdu. mcp-agent is more matter-of-fact, but there's still an easter egg in the project paying homage to silsila.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>sherlock-project/sherlock</title>
      <link>https://github.com/sherlock-project/sherlock</link>
      <description>&lt;p&gt;Hunt down social media accounts by username across social networks&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;br&gt; &lt;a href="https://sherlock-project.github.io/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/sherlock-project/sherlock/master/images/sherlock-logo.png"&gt;&lt;/a&gt; &lt;br&gt; &lt;span&gt;Hunt down social media accounts by username across &lt;a href="https://sherlockproject.xyz/sites"&gt;400+ social networks&lt;/a&gt;&lt;/span&gt; &lt;br&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://sherlockproject.xyz/installation"&gt;Installation&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;‚Ä¢&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href="https://sherlockproject.xyz/usage"&gt;Usage&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;‚Ä¢&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href="https://sherlockproject.xyz/contribute"&gt;Contributing&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img width="70%" height="70%" src="https://raw.githubusercontent.com/sherlock-project/sherlock/master/images/demo.png"&gt;  &lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;br&gt; Packages for ParrotOS and Ubuntu 24.04, maintained by a third party, appear to be &lt;strong&gt;broken&lt;/strong&gt;.&lt;br&gt; Users of these systems should defer to pipx/pip or Docker.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Notes&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;pipx install sherlock-project&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pip&lt;/code&gt; may be used in place of &lt;code&gt;pipx&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;docker run -it --rm sherlock/sherlock&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;dnf install sherlock-project&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Community-maintained packages are available for Debian (&amp;gt;= 13), Ubuntu (&amp;gt;= 22.10), Homebrew, Kali, and BlackArch. These packages are not directly supported or maintained by the Sherlock Project.&lt;/p&gt; 
&lt;p&gt;See all alternative installation methods &lt;a href="https://sherlockproject.xyz/installation"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;General usage&lt;/h2&gt; 
&lt;p&gt;To search for only one user:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sherlock user123
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To search for more than one user:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sherlock user1 user2 user3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Accounts found will be stored in an individual text file with the corresponding username (e.g &lt;code&gt;user123.txt&lt;/code&gt;).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;$ sherlock --help
usage: sherlock [-h] [--version] [--verbose] [--folderoutput FOLDEROUTPUT]
                [--output OUTPUT] [--tor] [--unique-tor] [--csv] [--xlsx]
                [--site SITE_NAME] [--proxy PROXY_URL] [--json JSON_FILE]
                [--timeout TIMEOUT] [--print-all] [--print-found] [--no-color]
                [--browse] [--local] [--nsfw]
                USERNAMES [USERNAMES ...]

Sherlock: Find Usernames Across Social Networks (Version 0.14.3)

positional arguments:
  USERNAMES             One or more usernames to check with social networks.
                        Check similar usernames using {?} (replace to '_', '-', '.').

optional arguments:
  -h, --help            show this help message and exit
  --version             Display version information and dependencies.
  --verbose, -v, -d, --debug
                        Display extra debugging information and metrics.
  --folderoutput FOLDEROUTPUT, -fo FOLDEROUTPUT
                        If using multiple usernames, the output of the results will be
                        saved to this folder.
  --output OUTPUT, -o OUTPUT
                        If using single username, the output of the result will be saved
                        to this file.
  --tor, -t             Make requests over Tor; increases runtime; requires Tor to be
                        installed and in system path.
  --unique-tor, -u      Make requests over Tor with new Tor circuit after each request;
                        increases runtime; requires Tor to be installed and in system
                        path.
  --csv                 Create Comma-Separated Values (CSV) File.
  --xlsx                Create the standard file for the modern Microsoft Excel
                        spreadsheet (xlsx).
  --site SITE_NAME      Limit analysis to just the listed sites. Add multiple options to
                        specify more than one site.
  --proxy PROXY_URL, -p PROXY_URL
                        Make requests over a proxy. e.g. socks5://127.0.0.1:1080
  --json JSON_FILE, -j JSON_FILE
                        Load data from a JSON file or an online, valid, JSON file.
  --timeout TIMEOUT     Time (in seconds) to wait for response to requests (Default: 60)
  --print-all           Output sites where the username was not found.
  --print-found         Output sites where the username was found.
  --no-color            Don't color terminal output
  --browse, -b          Browse to all results on default browser.
  --local, -l           Force the use of the local data.json file.
  --nsfw                Include checking of NSFW sites from default list.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Apify Actor Usage &lt;a href="https://apify.com/netmilk/sherlock?fpr=sherlock"&gt;&lt;img src="https://apify.com/actor-badge?actor=netmilk/sherlock" alt="Sherlock Actor"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://apify.com/netmilk/sherlock?fpr=sherlock"&gt;&lt;img src="https://apify.com/ext/run-on-apify.png" alt="Run Sherlock Actor on Apify" width="176" height="39"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You can run Sherlock in the cloud without installation using the &lt;a href="https://apify.com/netmilk/sherlock?fpr=sherlock"&gt;Sherlock Actor&lt;/a&gt; on &lt;a href="https://apify.com?fpr=sherlock"&gt;Apify&lt;/a&gt; free of charge.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ echo '{"usernames":["user123"]}' | apify call -so netmilk/sherlock
[{
  "username": "user123",
  "links": [
    "https://www.1337x.to/user/user123/",
    ...
  ]
}]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Read more about the &lt;a href="https://raw.githubusercontent.com/sherlock-project/sherlock/.actor/README.md"&gt;Sherlock Actor&lt;/a&gt;, including how to use it programmaticaly via the Apify &lt;a href="https://apify.com/netmilk/sherlock/api?fpr=sherlock"&gt;API&lt;/a&gt;, &lt;a href="https://docs.apify.com/cli/?fpr=sherlock"&gt;CLI&lt;/a&gt; and &lt;a href="https://docs.apify.com/sdk?fpr=sherlock"&gt;JS/TS and Python SDKs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;p&gt;Thank you to everyone who has contributed to Sherlock! ‚ù§Ô∏è&lt;/p&gt; 
&lt;a href="https://github.com/sherlock-project/sherlock/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?&amp;amp;columns=25&amp;amp;max=10000&amp;amp;&amp;amp;repo=sherlock-project/sherlock" noZoom&gt; &lt;/a&gt; 
&lt;h2&gt;Star history&lt;/h2&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=sherlock-project/sherlock&amp;amp;type=Date&amp;amp;theme=dark"&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=sherlock-project/sherlock&amp;amp;type=Date"&gt; 
 &lt;img alt="Sherlock Project Star History Chart" src="https://api.star-history.com/svg?repos=sherlock-project/sherlock&amp;amp;type=Date"&gt; 
&lt;/picture&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT ¬© Sherlock Project&lt;br&gt; Original Creator - &lt;a href="https://github.com/sdushantha"&gt;Siddharth Dushantha&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Reference Links --&gt;</description>
    </item>
    
    <item>
      <title>OpenBB-finance/OpenBB</title>
      <link>https://github.com/OpenBB-finance/OpenBB</link>
      <description>&lt;p&gt;Investment Research for Everyone, Everywhere.&lt;/p&gt;&lt;hr&gt;&lt;br&gt; 
&lt;img src="https://github.com/OpenBB-finance/OpenBB/raw/develop/images/platform-light.svg?raw=true#gh-light-mode-only" alt="OpenBB Platform logo" width="600"&gt; 
&lt;img src="https://github.com/OpenBB-finance/OpenBB/raw/develop/images/platform-dark.svg?raw=true#gh-dark-mode-only" alt="OpenBB Platform logo" width="600"&gt; 
&lt;br&gt; 
&lt;br&gt; 
&lt;p&gt;&lt;a href="https://x.com/openbb_finance"&gt;&lt;img src="https://img.shields.io/twitter/url/https/twitter.com/openbb_finance.svg?style=social&amp;amp;label=Follow%20%40openbb_finance" alt="Twitter"&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/xPHTuHCmuV"&gt;&lt;img src="https://img.shields.io/discord/831165782750789672" alt="Discord Shield"&gt;&lt;/a&gt; &lt;a href="https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/OpenBB-finance/OpenBB"&gt;&lt;img src="https://img.shields.io/static/v1?label=Dev%20Containers&amp;amp;message=Open&amp;amp;color=blue&amp;amp;logo=visualstudiocode" alt="Open in Dev Containers"&gt;&lt;/a&gt; &lt;a href="https://codespaces.new/OpenBB-finance/OpenBB"&gt; &lt;img src="https://github.com/codespaces/badge.svg?sanitize=true" height="20"&gt; &lt;/a&gt; &lt;a target="_blank" href="https://colab.research.google.com/github/OpenBB-finance/OpenBB/blob/develop/examples/googleColab.ipynb"&gt; &lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab"&gt; &lt;/a&gt; &lt;a href="https://pypi.org/project/openbb/"&gt;&lt;img src="https://img.shields.io/pypi/v/openbb?color=blue&amp;amp;label=PyPI%20Package" alt="PyPI"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The first financial Platform that is open source.&lt;/p&gt; 
&lt;p&gt;The OpenBB Platform offers access to equity, options, crypto, forex, macro economy, fixed income, and more while also offering a broad range of extensions to enhance the user experience according to their needs.&lt;/p&gt; 
&lt;p&gt;Get started with: &lt;code&gt;pip install openbb&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openbb import obb
output = obb.equity.price.historical("AAPL")
df = output.to_dataframe()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can sign up to the &lt;a href="https://my.openbb.co/login"&gt;OpenBB Hub&lt;/a&gt; to get the most out of the OpenBB ecosystem.&lt;/p&gt; 
&lt;p&gt;Data integrations available can be found here: &lt;a href="https://docs.openbb.co/platform/reference"&gt;https://docs.openbb.co/platform/reference&lt;/a&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;h2&gt;OpenBB Workspace&lt;/h2&gt; 
&lt;p&gt;While the OpenBB Platform is all about an integration to dozens of different data vendors, the interface is either Python or a CLI.&lt;/p&gt; 
&lt;p&gt;If you want an enterprise UI to visualize this datasets and use AI agents on top, you can find OpenBB Workspace at &lt;a href="https://pro.openbb.co"&gt;https://pro.openbb.co&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://pro.openbb.co"&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://openbb-cms.directus.app/assets/f69b6aaf-0821-4bc8-a43c-715e03a924ef.png" alt="Logo" width="1000"&gt; 
 &lt;/div&gt; &lt;/a&gt; 
&lt;p&gt;Data integration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can learn more about adding data to the OpenBB workspace from the &lt;a href="https://docs.openbb.co/workspace"&gt;docs&lt;/a&gt; or &lt;a href="https://github.com/OpenBB-finance/backends-for-openbb"&gt;this open source repository&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;AI Agents integration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can learn more about adding AI agents to the OpenBB workspace from &lt;a href="https://github.com/OpenBB-finance/agents-for-openbb"&gt;this open source repository&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Integrating OpenBB Platform to the OpenBB Workspace&lt;/h3&gt; 
&lt;p&gt;Connect this library to the OpenBB Workspace with a few simple commands, in a Python (3.9.21 - 3.12) environment.&lt;/p&gt; 
&lt;h4&gt;Run OpenBB Platform backend&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install the packages.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install "openbb[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Start the API server over localhost.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;openbb-api
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will launch a FastAPI server, via Uvicorn, at &lt;code&gt;127.0.0.1:6900&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can check that it works by going to &lt;a href="http://127.0.0.1:6900"&gt;http://127.0.0.1:6900&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Integrate OpenBB Platform backend to OpenBB Workspace&lt;/h4&gt; 
&lt;p&gt;Sign-in to the &lt;a href="https://pro.openbb.co/"&gt;OpenBB Workspace&lt;/a&gt;, and follow the following steps:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/75cffb4a-5e95-470a-b9d0-6ffd4067e069" alt="CleanShot 2025-05-17 at 09 51 56@2x"&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Go to the "Apps" tab&lt;/li&gt; 
 &lt;li&gt;Click on "Connect backend"&lt;/li&gt; 
 &lt;li&gt;Fill in the form with: Name: OpenBB Platform URL: &lt;a href="http://127.0.0.1:6900"&gt;http://127.0.0.1:6900&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Click on "Test". You should get a "Test successful" with the number of apps found.&lt;/li&gt; 
 &lt;li&gt;Click on "Add".&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;That's it.&lt;/p&gt; 
&lt;hr&gt; 
&lt;!-- TABLE OF CONTENTS --&gt; 
&lt;details closed="closed"&gt; 
 &lt;summary&gt;&lt;h2 style="display: inline-block"&gt;Table of Contents&lt;/h2&gt;&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#1-installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#2-contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#3-license"&gt;License&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#4-disclaimer"&gt;Disclaimer&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#5-contacts"&gt;Contacts&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#6-star-history"&gt;Star History&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#7-contributors"&gt;Contributors&lt;/a&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h2&gt;1. Installation&lt;/h2&gt; 
&lt;p&gt;The OpenBB Platform can be installed as a &lt;a href="https://pypi.org/project/openbb/"&gt;PyPI package&lt;/a&gt; by running &lt;code&gt;pip install openbb&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;or by cloning the repository directly with &lt;code&gt;git clone https://github.com/OpenBB-finance/OpenBB.git&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Please find more about the installation process, in the &lt;a href="https://docs.openbb.co/platform/installation"&gt;OpenBB Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;OpenBB Platform CLI installation&lt;/h3&gt; 
&lt;p&gt;The OpenBB Platform CLI is a command-line interface that allows you to access the OpenBB Platform directly from your command line.&lt;/p&gt; 
&lt;p&gt;It can be installed by running &lt;code&gt;pip install openbb-cli&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;or by cloning the repository directly with &lt;code&gt;git clone https://github.com/OpenBB-finance/OpenBB.git&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Please find more about the installation process in the &lt;a href="https://docs.openbb.co/cli/installation"&gt;OpenBB Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;2. Contributing&lt;/h2&gt; 
&lt;p&gt;There are three main ways of contributing to this project. (Hopefully you have starred the project by now ‚≠êÔ∏è)&lt;/p&gt; 
&lt;h3&gt;Become a Contributor&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;More information on our &lt;a href="https://docs.openbb.co/platform/developer_guide/misc/contributing"&gt;Contributing Documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Create a GitHub ticket&lt;/h3&gt; 
&lt;p&gt;Before creating a ticket make sure the one you are creating doesn't exist already &lt;a href="https://github.com/OpenBB-finance/OpenBB/issues"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=bug&amp;amp;template=bug_report.md&amp;amp;title=%5BBug%5D"&gt;Report bug&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=enhancement&amp;amp;template=enhancement.md&amp;amp;title=%5BIMPROVE%5D"&gt;Suggest improvement&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=new+feature&amp;amp;template=feature_request.md&amp;amp;title=%5BFR%5D"&gt;Request a feature&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Provide feedback&lt;/h3&gt; 
&lt;p&gt;We are most active on &lt;a href="https://openbb.co/discord"&gt;our Discord&lt;/a&gt;, but feel free to reach out to us in any of &lt;a href="https://openbb.co/links"&gt;our social media&lt;/a&gt; for feedback.&lt;/p&gt; 
&lt;h2&gt;3. License&lt;/h2&gt; 
&lt;p&gt;Distributed under the AGPLv3 License. See &lt;a href="https://github.com/OpenBB-finance/OpenBB/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;4. Disclaimer&lt;/h2&gt; 
&lt;p&gt;Trading in financial instruments involves high risks including the risk of losing some, or all, of your investment amount, and may not be suitable for all investors.&lt;/p&gt; 
&lt;p&gt;Before deciding to trade in a financial instrument you should be fully informed of the risks and costs associated with trading the financial markets, carefully consider your investment objectives, level of experience, and risk appetite, and seek professional advice where needed.&lt;/p&gt; 
&lt;p&gt;The data contained in the OpenBB Platform is not necessarily accurate.&lt;/p&gt; 
&lt;p&gt;OpenBB and any provider of the data contained in this website will not accept liability for any loss or damage as a result of your trading, or your reliance on the information displayed.&lt;/p&gt; 
&lt;p&gt;All names, logos, and brands of third parties that may be referenced in our sites, products or documentation are trademarks of their respective owners. Unless otherwise specified, OpenBB and its products and services are not endorsed by, sponsored by, or affiliated with these third parties.&lt;/p&gt; 
&lt;p&gt;Our use of these names, logos, and brands is for identification purposes only, and does not imply any such endorsement, sponsorship, or affiliation.&lt;/p&gt; 
&lt;h2&gt;5. Contacts&lt;/h2&gt; 
&lt;p&gt;If you have any questions about the platform or anything OpenBB, feel free to email us at &lt;code&gt;support@openbb.co&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;If you want to say hi, or are interested in partnering with us, feel free to reach us at &lt;code&gt;hello@openbb.co&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Any of our social media platforms: &lt;a href="https://openbb.co/links"&gt;openbb.co/links&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;6. Star History&lt;/h2&gt; 
&lt;p&gt;This is a proxy of our growth and that we are just getting started.&lt;/p&gt; 
&lt;p&gt;But for more metrics important to us check &lt;a href="https://openbb.co/open"&gt;openbb.co/open&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;amp;type=Date&amp;amp;theme=dark"&gt;&lt;img src="https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;amp;type=Date&amp;amp;theme=dark" alt="Star History Chart"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;7. Contributors&lt;/h2&gt; 
&lt;p&gt;OpenBB wouldn't be OpenBB without you. If we are going to disrupt financial industry, every contribution counts. Thank you for being part of this journey.&lt;/p&gt; 
&lt;a href="https://github.com/OpenBB-finance/OpenBB/graphs/contributors"&gt; &lt;img src="https://contributors-img.web.app/image?repo=OpenBB-finance/OpenBB" width="800"&gt; &lt;/a&gt; 
&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; 
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</description>
    </item>
    
    <item>
      <title>chubin/wttr.in</title>
      <link>https://github.com/chubin/wttr.in</link>
      <description>&lt;p&gt;‚õÖ The right way to check the weather&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;em&gt;wttr.in ‚Äî the right way to &lt;del&gt;check&lt;/del&gt; &lt;code&gt;curl&lt;/code&gt; the weather!&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;wttr.in is a console-oriented weather forecast service that supports various information representation methods like terminal-oriented ANSI-sequences for console HTTP clients (curl, httpie, or wget), HTML for web browsers, or PNG for graphical viewers.&lt;/p&gt; 
&lt;p&gt;Originally started as a small project, a wrapper for &lt;a href="https://github.com/schachmat/wego"&gt;wego&lt;/a&gt;, intended to demonstrate the power of the console-oriented services, &lt;em&gt;wttr.in&lt;/em&gt; became a popular weather reporting service, handling tens of millions&lt;a href="https://raw.githubusercontent.com/chubin/wttr.in/master/#wttrin-usage-stats"&gt;¬π&lt;/a&gt; of queries daily.&lt;/p&gt; 
&lt;p&gt;You can see it running here: &lt;a href="https://wttr.in"&gt;wttr.in&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://wttr.in/:help"&gt;Documentation&lt;/a&gt; | &lt;a href="https://github.com/chubin/wttr.in#usage"&gt;Usage&lt;/a&gt; | &lt;a href="https://github.com/chubin/wttr.in#one-line-output"&gt;One-line output&lt;/a&gt; | &lt;a href="https://github.com/chubin/wttr.in#data-rich-output-format-v2"&gt;Data-rich output format&lt;/a&gt; | &lt;a href="https://github.com/chubin/wttr.in#map-view-v3"&gt;Map view&lt;/a&gt; | &lt;a href="https://github.com/chubin/wttr.in#different-output-formats"&gt;Output formats&lt;/a&gt; | &lt;a href="https://github.com/chubin/wttr.in#moon-phases"&gt;Moon phases&lt;/a&gt; | &lt;a href="https://github.com/chubin/wttr.in#internationalization-and-localization"&gt;Internationalization&lt;/a&gt; | &lt;a href="https://github.com/chubin/wttr.in#installation"&gt;Installation&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;You can access the service from a shell or from a Web browser like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ curl wttr.in
Weather for City: Paris, France

     \   /     Clear
      .-.      10 ‚Äì 11 ¬∞C
   ‚Äï (   ) ‚Äï   ‚Üë 11 km/h
      `-‚Äô      10 km
     /   \     0.0 mm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Here is an example weather report:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/chubin/wttr.in/master/share/pics/San_Francisco.png" alt="Weather Report"&gt;&lt;/p&gt; 
&lt;p&gt;Or in PowerShell:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-PowerShell"&gt;Invoke-RestMethod https://wttr.in
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Want to get the weather information for a specific location? You can add the desired location to the URL in your request like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ curl wttr.in/London
$ curl wttr.in/Moscow
$ curl wttr.in/Salt+Lake+City
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you omit the location name, you will get the report for your current location based on your IP address.&lt;/p&gt; 
&lt;p&gt;Use 3-letter airport codes in order to get the weather information at a certain airport:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ curl wttr.in/muc      # Weather for IATA: muc, Munich International Airport, Germany
$ curl wttr.in/ham      # Weather for IATA: ham, Hamburg Airport, Germany
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Let's say you'd like to get the weather for a geographical location other than a town or city - maybe an attraction in a city, a mountain name, or some special location. Add the character &lt;code&gt;~&lt;/code&gt; before the name to look up that special location name before the weather is then retrieved:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ curl wttr.in/~Vostok+Station
$ curl wttr.in/~Eiffel+Tower
$ curl wttr.in/~Kilimanjaro
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For these examples, you'll see a line below the weather forecast output that shows the geolocation results of looking up the location:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Location: Vostok Station, —Å—Ç–∞–Ω—Ü–∏—è –í–æ—Å—Ç–æ–∫, AAT, Antarctica [-78.4642714,106.8364678]
Location: Tour Eiffel, 5, Avenue Anatole France, Gros-Caillou, 7e, Paris, √éle-de-France, 75007, France [48.8582602,2.29449905432]
Location: Kilimanjaro, Northern, Tanzania [-3.4762789,37.3872648]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also use IP-addresses (direct) or domain names (prefixed with &lt;code&gt;@&lt;/code&gt;) to specify a location:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ curl wttr.in/@github.com
$ curl wttr.in/@msu.ru
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To get detailed information online, you can access the &lt;a href="https://wttr.in/:help"&gt;/:help&lt;/a&gt; page:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ curl wttr.in/:help
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Weather Units&lt;/h3&gt; 
&lt;p&gt;By default the USCS units are used for the queries from the USA and the metric system for the rest of the world. You can override this behavior by adding &lt;code&gt;?u&lt;/code&gt;, &lt;code&gt;?m&lt;/code&gt; or &lt;code&gt;?M&lt;/code&gt; to a URL like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ curl wttr.in/Amsterdam?u  # USCS (used by default in US)
$ curl wttr.in/Amsterdam?m  # metric (SI) (used by default everywhere except US)
$ curl wttr.in/Amsterdam?M  # metric (SI), but show wind speed in m/s
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you have several options to pass, write them without delimiters in between for the one-letter options, and use &lt;code&gt;&amp;amp;&lt;/code&gt; as a delimiter for the long options with values:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ curl 'wttr.in/Amsterdam?m2&amp;amp;lang=nl'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It would be a rough equivalent of &lt;code&gt;-m2 --lang nl&lt;/code&gt; for the GNU CLI syntax.&lt;/p&gt; 
&lt;h2&gt;Supported output formats and views&lt;/h2&gt; 
&lt;p&gt;wttr.in currently supports five output formats:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ANSI for the terminal;&lt;/li&gt; 
 &lt;li&gt;Plain-text for the terminal and scripts;&lt;/li&gt; 
 &lt;li&gt;HTML for the browser;&lt;/li&gt; 
 &lt;li&gt;PNG for the graphical viewers;&lt;/li&gt; 
 &lt;li&gt;JSON for scripts and APIs;&lt;/li&gt; 
 &lt;li&gt;Prometheus metrics for scripts and APIs.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The ANSI and HTML formats are selected based on the User-Agent string.&lt;/p&gt; 
&lt;p&gt;To force plain text, which disables colors:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ curl wttr.in/?T
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To restrict output to glyphs available in standard console fonts (e.g. Consolas and Lucida Console):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ curl wttr.in/?d
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The PNG format can be forced by adding &lt;code&gt;.png&lt;/code&gt; to the end of the query:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ wget wttr.in/Paris.png
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can use all of the options with the PNG-format like in an URL, but you have to separate them with &lt;code&gt;_&lt;/code&gt; instead of &lt;code&gt;?&lt;/code&gt; and &lt;code&gt;&amp;amp;&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ wget wttr.in/Paris_0tqp_lang=fr.png
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Useful options for the PNG format:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;t&lt;/code&gt; for transparency (&lt;code&gt;transparency=150&lt;/code&gt;);&lt;/li&gt; 
 &lt;li&gt;transparency=0..255 for a custom transparency level.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Transparency is a useful feature when weather PNGs are used to add weather data to pictures:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ convert source.jpg &amp;lt;( curl wttr.in/Oymyakon_tqp0.png ) -geometry +50+50 -composite target.jpg
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In this example:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;source.jpg&lt;/code&gt; - source file;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;target.jpg&lt;/code&gt; - target file;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Oymyakon&lt;/code&gt; - name of the location;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tqp0&lt;/code&gt; - options (recommended).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://pbs.twimg.com/media/C69-wsIW0AAcAD5.jpg" alt="Picture with weather data"&gt;&lt;/p&gt; 
&lt;p&gt;You can embed a special wttr.in widget, that displays the weather condition for the current or a selected location, into a HTML page using the &lt;a href="https://github.com/midzer/wttr-switcher"&gt;wttr-switcher&lt;/a&gt;. That is how it looks like: &lt;a href="https://midzer.github.io/wttr-switcher/"&gt;wttr-switcher-example&lt;/a&gt; or on a real world web site: &lt;a href="https://feuerwehr-eisolzried.de/"&gt;https://feuerwehr-eisolzried.de/&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/3875145/65265457-50eac180-db11-11e9-8f9b-2e1711dfc436.png" alt="Embedded wttr.in example at feuerwehr-eisolzried.de"&gt;&lt;/p&gt; 
&lt;h2&gt;One-line output&lt;/h2&gt; 
&lt;p&gt;One-line output format is convenient to be used to show weather info in status bar of different programs, such as &lt;em&gt;tmux&lt;/em&gt;, &lt;em&gt;weechat&lt;/em&gt;, etc.&lt;/p&gt; 
&lt;p&gt;For one-line output format, specify additional URL parameter &lt;code&gt;format&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ curl wttr.in/Nuremberg?format=3
Nuremberg: üå¶ +11‚Å∞C
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Available preconfigured formats: 1, 2, 3, 4 and the custom format using the percent notation (see below).&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;1: Current weather at location: &lt;code&gt;üå¶ +11‚Å∞C&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;2: Current weather at location with more details: &lt;code&gt;üå¶ üå°Ô∏è+11¬∞C üå¨Ô∏è‚Üì4km/h&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;3: Name of location and current weather at location: &lt;code&gt;Nuremberg: üå¶ +11‚Å∞C&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;4: Name of location and current weather at location with more details: &lt;code&gt;Nuremberg: üå¶ üå°Ô∏è+11¬∞C üå¨Ô∏è‚Üì4km/h&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can specify multiple locations separated with &lt;code&gt;:&lt;/code&gt; (for repeating queries):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ curl wttr.in/Nuremberg:Hamburg:Berlin?format=3
Nuremberg: üå¶ +11‚Å∞C
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or to process all this queries at once:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ curl -s 'wttr.in/{Nuremberg,Hamburg,Berlin}?format=3'
Nuremberg: üå¶ +11‚Å∞C
Hamburg: üå¶ +8‚Å∞C
Berlin: üå¶ +8‚Å∞C
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To specify your own custom output format, use the special &lt;code&gt;%&lt;/code&gt;-notation:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    c    Weather condition,
    C    Weather condition textual name,
    x    Weather condition, plain-text symbol,
    h    Humidity,
    t    Temperature (Actual),
    f    Temperature (Feels Like),
    w    Wind,
    l    Location,
    m    Moon phase üåëüåíüåìüåîüåïüåñüåóüåò,
    M    Moon day,
    p    Precipitation (mm/3 hours),
    P    Pressure (hPa),
    u    UV index (1-12),

    D    Dawn*,
    S    Sunrise*,
    z    Zenith*,
    s    Sunset*,
    d    Dusk*,
    T    Current time*,
    Z    Local timezone.

(*times are shown in the local timezone)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;So, these two calls are the same:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    $ curl wttr.in/London?format=3
    London: ‚õÖÔ∏è +7‚Å∞C
    $ curl wttr.in/London?format="%l:+%c+%t\n"
    London: ‚õÖÔ∏è +7‚Å∞C
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;tmux&lt;/h3&gt; 
&lt;p&gt;When using in &lt;code&gt;tmux.conf&lt;/code&gt;, you have to escape &lt;code&gt;%&lt;/code&gt; with &lt;code&gt;%&lt;/code&gt;, i.e. write there &lt;code&gt;%%&lt;/code&gt; instead of &lt;code&gt;%&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;The output does not contain new line by default, when the %-notation is used, but it does contain it when preconfigured format (&lt;code&gt;1&lt;/code&gt;,&lt;code&gt;2&lt;/code&gt;,&lt;code&gt;3&lt;/code&gt; etc.) are used. To have the new line in the output when the %-notation is used, use '\n' and single quotes when doing a query from the shell.&lt;/p&gt; 
&lt;p&gt;In programs, that are querying the service automatically (such as tmux), it is better to use some reasonable update interval. In tmux, you can configure it with &lt;code&gt;status-interval&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;If several, &lt;code&gt;:&lt;/code&gt; separated locations, are specified in the query, specify update period as an additional query parameter &lt;code&gt;period=&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;set -g status-interval 60
WEATHER='#(curl -s wttr.in/London:Stockholm:Moscow\?format\="%%l:+%%c%%20%%t%%60%%w&amp;amp;period=60")'
set -g status-right "$WEATHER ..."
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://wttr.in/files/example-tmux-status-line.png" alt="wttr.in in tmux status bar"&gt;&lt;/p&gt; 
&lt;h3&gt;WeeChat&lt;/h3&gt; 
&lt;p&gt;To embed in to an IRC (&lt;a href="https://github.com/weechat/weechat"&gt;WeeChat&lt;/a&gt;) client's existing status bar:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;/alias add wttr /exec -pipe "/mute /set plugins.var.wttr" url:wttr.in/Montreal?format=%l:+%c+%f+%h+%p+%P+%m+%w+%S+%s;/wait 3 /item refresh wttr
/trigger add wttr timer 60000;0;0 "" "" "/wttr"
/item add wttr "" "${plugins.var.wttr}"
/eval /set weechat.bar.status.items ${weechat.bar.status.items},spacer,wttr
/eval /set weechat.startup.command_after_plugins ${weechat.startup.command_after_plugins};/wttr
/wttr
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://i.imgur.com/XkYiRU7.png" alt="wttr.in in WeeChat status bar"&gt;&lt;/p&gt; 
&lt;h3&gt;conky&lt;/h3&gt; 
&lt;p&gt;Conky usage example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;${texeci 1800 curl wttr.in/kyiv_0pq_lang=uk.png
  | convert - -transparent black $HOME/.config/conky/out.png}
${image $HOME/.config/conky/out.png -p 0,0}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/3875145/172178453-9e9ed9e3-9815-426a-9a21-afdd6e279fc8.png" alt="wttr.in in conky"&gt;&lt;/p&gt; 
&lt;h3&gt;IRC&lt;/h3&gt; 
&lt;p&gt;IRC integration example:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenSourceTreasure/Mirc-ASCII-weather-translate-pixel-editor"&gt;https://github.com/OpenSourceTreasure/Mirc-ASCII-weather-translate-pixel-editor&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Emojis support&lt;/h3&gt; 
&lt;p&gt;To see emojis in terminal, you need:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Terminal support for emojis (was added to Cairo 1.15.8);&lt;/li&gt; 
 &lt;li&gt;Font with emojis support.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;For the emoji font, we recommend &lt;em&gt;Noto Color Emoji&lt;/em&gt;, and a good alternative option would be the &lt;em&gt;Emoji One&lt;/em&gt; font; both of them support all necessary emoji glyphs.&lt;/p&gt; 
&lt;p&gt;Font configuration:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-xml"&gt;$ cat ~/.config/fontconfig/fonts.conf
&amp;lt;?xml version="1.0" encoding="UTF-8"?&amp;gt;
&amp;lt;!DOCTYPE fontconfig SYSTEM "fonts.dtd"&amp;gt;
&amp;lt;fontconfig&amp;gt;
  &amp;lt;alias&amp;gt;
    &amp;lt;family&amp;gt;serif&amp;lt;/family&amp;gt;
    &amp;lt;prefer&amp;gt;
      &amp;lt;family&amp;gt;Noto Color Emoji&amp;lt;/family&amp;gt;
    &amp;lt;/prefer&amp;gt;
  &amp;lt;/alias&amp;gt;
  &amp;lt;alias&amp;gt;
    &amp;lt;family&amp;gt;sans-serif&amp;lt;/family&amp;gt;
    &amp;lt;prefer&amp;gt;
      &amp;lt;family&amp;gt;Noto Color Emoji&amp;lt;/family&amp;gt;
    &amp;lt;/prefer&amp;gt;
  &amp;lt;/alias&amp;gt;
  &amp;lt;alias&amp;gt;
    &amp;lt;family&amp;gt;monospace&amp;lt;/family&amp;gt;
    &amp;lt;prefer&amp;gt;
      &amp;lt;family&amp;gt;Noto Color Emoji&amp;lt;/family&amp;gt;
    &amp;lt;/prefer&amp;gt;
  &amp;lt;/alias&amp;gt;
&amp;lt;/fontconfig&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;(to apply the configuration, run &lt;code&gt;fc-cache -f -v&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;In some cases, &lt;code&gt;tmux&lt;/code&gt; and the terminal understanding of some emoji characters may differ, which may cause strange effects similar to that described in #579.&lt;/p&gt; 
&lt;h3&gt;Squeak&lt;/h3&gt; 
&lt;p&gt;To embed into the world main docking bar:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-smalltalk"&gt;wttr := (UpdatingStringMorph on: [(WebClient httpGet: 'https://wttr.in/?format=%20%20%l:%20%C+%t') content] selector: #value)
	stepTime: 60000;
	useStringFormat;
	yourself.
dockingBar := World mainDockingBars first.
dockingBar addMorph: wttr after: (dockingBar findA: ClockMorph).
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/4c2762b0-77ae-41a8-98db-3eb310d073bd" alt="wttr.in integration in the Squeak world main docking bar"&gt;&lt;/p&gt; 
&lt;h2&gt;Data-rich output format (v2)&lt;/h2&gt; 
&lt;p&gt;In the experimental data-rich output format, that is available under the view code &lt;code&gt;v2&lt;/code&gt;, a lot of additional weather and astronomical information is available:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Temperature, and precipitation changes forecast throughout the days;&lt;/li&gt; 
 &lt;li&gt;Moonphase for today and the next three days;&lt;/li&gt; 
 &lt;li&gt;The current weather condition, temperature, humidity, wind speed and direction, pressure;&lt;/li&gt; 
 &lt;li&gt;Timezone;&lt;/li&gt; 
 &lt;li&gt;Dawn, sunrise, noon, sunset, dusk time for he selected location;&lt;/li&gt; 
 &lt;li&gt;Precise geographical coordinates for the selected location.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;  $ curl v2.wttr.in/M√ºnchen
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;  $ curl wttr.in/M√ºnchen?format=v2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or, if you prefer Nerd Fonts instead of Emoji, &lt;code&gt;v2d&lt;/code&gt; (day) or &lt;code&gt;v2n&lt;/code&gt; (night):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;  $ curl v2d.wttr.in/M√ºnchen
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://wttr.in/files/example-wttr-v2.png" alt="data-rich output format"&gt;&lt;/p&gt; 
&lt;p&gt;(The mode is experimental, and it has several limitations currently:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;It works only in terminal;&lt;/li&gt; 
 &lt;li&gt;Only English is supported).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Currently, you need some tweaks for some terminals, to get the best possible visualization.&lt;/p&gt; 
&lt;h3&gt;URXVT&lt;/h3&gt; 
&lt;p&gt;Depending on your configuration you might be taking all steps, or only a few. URXVT currently doesn't support emoji related fonts, but we can get almost the same effect using &lt;em&gt;Font-Symbola&lt;/em&gt;. So add to your &lt;code&gt;.Xresources&lt;/code&gt; file the following line:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    xft:symbola:size=10:minspace=False
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can add it &lt;em&gt;after&lt;/em&gt; your preferred font and it will only show up when required. Then, if you see or feel like you're having spacing issues, add this: &lt;code&gt;URxvt.letterSpace: 0&lt;/code&gt; For some reason URXVT sometimes stops deciding right the word spacing and we need to force it this way.&lt;/p&gt; 
&lt;p&gt;The result, should look like:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/24360204/63842949-1d36d480-c975-11e9-81dd-998d1329bd8a.png" alt="URXVT Emoji line"&gt;&lt;/p&gt; 
&lt;h2&gt;Map view (v3)&lt;/h2&gt; 
&lt;p&gt;In the experimental map view, that is available under the view code &lt;code&gt;v3&lt;/code&gt;, weather information about a geographical region is available:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    $ curl v3.wttr.in/Bayern.sxl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://v3.wttr.in/Bayern.png" alt="v3.wttr.in/Bayern"&gt;&lt;/p&gt; 
&lt;p&gt;or directly in browser:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://v3.wttr.in/Bayern"&gt;https://v3.wttr.in/Bayern&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The map view currently supports three formats:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PNG (for browser and messengers);&lt;/li&gt; 
 &lt;li&gt;Sixel (terminal inline images support);&lt;/li&gt; 
 &lt;li&gt;IIP (terminal with iterm2 inline images protocol support).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Terminal with inline images protocols support:&lt;/p&gt; 
&lt;p&gt;‚ü∂ &lt;em&gt;Detailed article: &lt;a href="https://raw.githubusercontent.com/chubin/wttr.in/master/doc/terminal-images.md"&gt;Images in terminal&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Terminal&lt;/th&gt; 
   &lt;th&gt;Environment&lt;/th&gt; 
   &lt;th&gt;Images support&lt;/th&gt; 
   &lt;th&gt;Protocol&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;uxterm&lt;/td&gt; 
   &lt;td&gt;X11&lt;/td&gt; 
   &lt;td&gt;yes&lt;/td&gt; 
   &lt;td&gt;Sixel&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;mlterm&lt;/td&gt; 
   &lt;td&gt;X11&lt;/td&gt; 
   &lt;td&gt;yes&lt;/td&gt; 
   &lt;td&gt;Sixel&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;kitty&lt;/td&gt; 
   &lt;td&gt;X11&lt;/td&gt; 
   &lt;td&gt;yes&lt;/td&gt; 
   &lt;td&gt;Kitty&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;wezterm&lt;/td&gt; 
   &lt;td&gt;X11&lt;/td&gt; 
   &lt;td&gt;yes&lt;/td&gt; 
   &lt;td&gt;IIP&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Darktile&lt;/td&gt; 
   &lt;td&gt;X11&lt;/td&gt; 
   &lt;td&gt;yes&lt;/td&gt; 
   &lt;td&gt;Sixel&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Jexer&lt;/td&gt; 
   &lt;td&gt;X11&lt;/td&gt; 
   &lt;td&gt;yes&lt;/td&gt; 
   &lt;td&gt;Sixel&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GNOME Terminal&lt;/td&gt; 
   &lt;td&gt;X11&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://gitlab.gnome.org/GNOME/vte/-/issues/253"&gt;in-progress&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Sixel&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;alacritty&lt;/td&gt; 
   &lt;td&gt;X11&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/alacritty/alacritty/issues/910"&gt;in-progress&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Sixel&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;foot&lt;/td&gt; 
   &lt;td&gt;Wayland&lt;/td&gt; 
   &lt;td&gt;yes&lt;/td&gt; 
   &lt;td&gt;Sixel&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DomTerm&lt;/td&gt; 
   &lt;td&gt;Web&lt;/td&gt; 
   &lt;td&gt;yes&lt;/td&gt; 
   &lt;td&gt;Sixel&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Yaft&lt;/td&gt; 
   &lt;td&gt;FB&lt;/td&gt; 
   &lt;td&gt;yes&lt;/td&gt; 
   &lt;td&gt;Sixel&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;iTerm2&lt;/td&gt; 
   &lt;td&gt;Mac OS X&lt;/td&gt; 
   &lt;td&gt;yes&lt;/td&gt; 
   &lt;td&gt;IIP&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;mintty&lt;/td&gt; 
   &lt;td&gt;Windows&lt;/td&gt; 
   &lt;td&gt;yes&lt;/td&gt; 
   &lt;td&gt;Sixel&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows Terminal&lt;/td&gt; 
   &lt;td&gt;Windows&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/terminal/issues/448"&gt;in-progress&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Sixel&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="http://nanno.dip.jp/softlib/man/rlogin/"&gt;RLogin&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Windows&lt;/td&gt; 
   &lt;td&gt;yes&lt;/td&gt; 
   &lt;td&gt;Sixel&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Different output formats&lt;/h2&gt; 
&lt;h3&gt;JSON output&lt;/h3&gt; 
&lt;p&gt;The JSON format is a feature providing access to &lt;em&gt;wttr.in&lt;/em&gt; data through an easy-to-parse format, without requiring the user to create a complex script to reinterpret wttr.in's graphical output.&lt;/p&gt; 
&lt;p&gt;To fetch information in JSON format, use the following syntax:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ curl wttr.in/Detroit?format=j1
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will fetch information on the Detroit region in JSON format. The j1 format code is used to allow for the use of other layouts for the JSON output.&lt;/p&gt; 
&lt;p&gt;The result will look something like the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
	"current_condition": [
		{
		    "FeelsLikeC": "25",
		    "FeelsLikeF": "76",
		    "cloudcover": "100",
		    "humidity": "76",
		    "observation_time": "04:08 PM",
		    "precipMM": "0.2",
		    "pressure": "1019",
		    "temp_C": "22",
		    "temp_F": "72",
		    "uvIndex": 5,
		    "visibility": "16",
		    "weatherCode": "122",
		    "weatherDesc": [
			{
			    "value": "Overcast"
			}
		    ],
		    "weatherIconUrl": [
			{
			    "value": ""
			}
		    ],
		    "winddir16Point": "NNE",
		    "winddirDegree": "20",
		    "windspeedKmph": "7",
		    "windspeedMiles": "4"
		}
	],
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Most of these values are self-explanatory, aside from &lt;code&gt;weatherCode&lt;/code&gt;. The &lt;code&gt;weatherCode&lt;/code&gt; is an enumeration which you can find at either &lt;a href="https://www.worldweatheronline.com/developer/api/docs/weather-icons.aspx"&gt;the WorldWeatherOnline website&lt;/a&gt; or &lt;a href="https://github.com/chubin/wttr.in/raw/master/lib/constants.py"&gt;in the wttr.in source code&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Prometheus Metrics Output&lt;/h3&gt; 
&lt;p&gt;The &lt;a href="https://github.com/prometheus/prometheus"&gt;Prometheus&lt;/a&gt; Metrics format is a feature providing access to &lt;em&gt;wttr.in&lt;/em&gt; data through an easy-to-parse format for monitoring systems, without requiring the user to create a complex script to reinterpret wttr.in's graphical output.&lt;/p&gt; 
&lt;p&gt;To fetch information in Prometheus format, use the following syntax:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ curl wttr.in/Detroit?format=p1
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will fetch information on the Detroit region in Prometheus Metrics format. The &lt;code&gt;p1&lt;/code&gt; format code is used to allow for the use of other layouts for the Prometheus Metrics output.&lt;/p&gt; 
&lt;p&gt;A possible configuration for Prometheus could look like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;    - job_name: 'wttr_in_detroit'
        static_configs:
            - targets: ['wttr.in']
        metrics_path: '/Detroit'
        params:
            format: ['p1']
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The result will look something like the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# HELP temperature_feels_like_celsius Feels Like Temperature in Celsius
temperature_feels_like_celsius{forecast="current"} 7
# HELP temperature_feels_like_fahrenheit Feels Like Temperature in Fahrenheit
temperature_feels_like_fahrenheit{forecast="current"} 45
[truncated]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;...&lt;/p&gt; 
&lt;h2&gt;Moon phases&lt;/h2&gt; 
&lt;p&gt;wttr.in can also be used to check the phase of the Moon. This example shows how to see the current Moon phase in the full-output mode:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ curl wttr.in/Moon
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Get the moon phase for a particular date by adding &lt;code&gt;@YYYY-MM-DD&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ curl wttr.in/Moon@2016-12-25
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The moon phase information uses &lt;a href="https://github.com/chubin/pyphoon"&gt;pyphoon&lt;/a&gt; as its backend.&lt;/p&gt; 
&lt;p&gt;To get the moon phase information in the online mode, use &lt;code&gt;%m&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ curl wttr.in/London?format=%m
üåñ
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Keep in mind that the Unicode representation of moon phases suffers 2 caveats:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;With some fonts, the representation &lt;code&gt;üåò&lt;/code&gt; is ambiguous, for it either seem almost-shadowed or almost-lit, depending on whether your terminal is in light mode or dark mode. Relying on colored fonts like &lt;code&gt;noto-fonts&lt;/code&gt; works around this problem.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The representation &lt;code&gt;üåò&lt;/code&gt; is also ambiguous, for it means "last quarter" in northern hemisphere, but "first quarter" in souther hemisphere. It also means nothing in tropical zones. This is a limitation that &lt;a href="https://www.unicode.org/L2/L2017/17304-moon-var.pdf"&gt;Unicode&lt;/a&gt; is aware about. But it has not been worked around at &lt;code&gt;wttr.in&lt;/code&gt; yet.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See #247, #364 for the corresponding tracking issues, and &lt;a href="https://github.com/chubin/pyphoon/issues/1"&gt;pyphoon#1&lt;/a&gt; for pyphoon. Any help is welcome.&lt;/p&gt; 
&lt;h2&gt;Internationalization and localization&lt;/h2&gt; 
&lt;p&gt;wttr.in supports multilingual locations names that can be specified in any language in the world (it may be surprising, but many locations in the world don't have an English name).&lt;/p&gt; 
&lt;p&gt;The query string should be specified in Unicode (hex-encoded or not). Spaces in the query string must be replaced with &lt;code&gt;+&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ curl wttr.in/—Å—Ç–∞–Ω—Ü–∏—è+–í–æ—Å—Ç–æ–∫
Weather report: —Å—Ç–∞–Ω—Ü–∏—è –í–æ—Å—Ç–æ–∫

               Overcast
      .--.     -65 ‚Äì -47 ¬∞C
   .-(    ).   ‚Üë 23 km/h
  (___.__)__)  15 km
               0.0 mm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The language used for the output (except the location name) does not depend on the input language and it is either English (by default) or the preferred language of the browser (if the query was issued from a browser) that is specified in the query headers (&lt;code&gt;Accept-Language&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;The language can be set explicitly when using console clients by using command-line options like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;curl -H "Accept-Language: fr" wttr.in
http GET wttr.in Accept-Language:ru
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The preferred language can be forced using the &lt;code&gt;lang&lt;/code&gt; option:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ curl wttr.in/Berlin?lang=de
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The third option is to choose the language using the DNS name used in the query:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ curl de.wttr.in/Berlin
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;wttr.in is currently translated into 54 languages, and the number of supported languages is constantly growing.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://wttr.in/:translation"&gt;/:translation&lt;/a&gt; to learn more about the translation process, to see the list of supported languages and contributors, or to know how you can help to translate wttr.in in your language.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://pbs.twimg.com/media/C7hShiDXQAES6z1.jpg" alt="Queries to wttr.in in various languages"&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install the application:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install external dependencies&lt;/li&gt; 
 &lt;li&gt;Install Python dependencies used by the service&lt;/li&gt; 
 &lt;li&gt;Configure IP2Location (optional)&lt;/li&gt; 
 &lt;li&gt;Get a WorldWeatherOnline API and configure wego&lt;/li&gt; 
 &lt;li&gt;Configure wttr.in&lt;/li&gt; 
 &lt;li&gt;Configure the HTTP-frontend service&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Install external dependencies&lt;/h3&gt; 
&lt;p&gt;wttr.in has the following external dependencies:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://golang.org/doc/install"&gt;golang&lt;/a&gt;, wego dependency&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/schachmat/wego"&gt;wego&lt;/a&gt;, weather client for terminal&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;After you install &lt;a href="https://golang.org/doc/install"&gt;golang&lt;/a&gt;, install &lt;code&gt;wego&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;go install github.com/schachmat/wego@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install Python dependencies&lt;/h3&gt; 
&lt;p&gt;Python requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Flask&lt;/li&gt; 
 &lt;li&gt;geoip2&lt;/li&gt; 
 &lt;li&gt;geopy&lt;/li&gt; 
 &lt;li&gt;requests&lt;/li&gt; 
 &lt;li&gt;gevent&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you want to get weather reports as PNG files, you'll also need to install:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PIL&lt;/li&gt; 
 &lt;li&gt;pyte (&amp;gt;=0.6)&lt;/li&gt; 
 &lt;li&gt;necessary fonts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can install most of them using &lt;code&gt;pip&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Some python package use LLVM, so install it first:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;apt-get install llvm-7 llvm-7-dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If &lt;code&gt;virtualenv&lt;/code&gt; is used:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;virtualenv -p python3 ve
ve/bin/pip3 install -r requirements.txt
ve/bin/python3 bin/srv.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Also, you need to install the geoip2 database. You can use a free database GeoLite2 that can be downloaded from (&lt;a href="http://dev.maxmind.com/geoip/geoip2/geolite2/"&gt;http://dev.maxmind.com/geoip/geoip2/geolite2/&lt;/a&gt;).&lt;/p&gt; 
&lt;h3&gt;Configure IP2Location (optional)&lt;/h3&gt; 
&lt;p&gt;If you want to use the IP2location service for IP-addresses that are not covered by GeoLite2, you have to obtain a API key of that service, and after that save into the &lt;code&gt;~/.ip2location.key&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ echo 'YOUR_IP2LOCATION_KEY' &amp;gt; ~/.ip2location.key
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you don't have this file, the service will be silently skipped (it is not a big problem, because the MaxMind database is pretty good).&lt;/p&gt; 
&lt;h3&gt;Installation with Docker&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install Docker&lt;/li&gt; 
 &lt;li&gt;Build Docker Image&lt;/li&gt; 
 &lt;li&gt;These files should be mounted by the user at runtime:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;/root/.wegorc
/root/.ip2location.key (optional)
/app/airports.dat
/app/GeoLite2-City.mmdb
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Get a WorldWeatherOnline key and configure wego&lt;/h3&gt; 
&lt;p&gt;To get a WorldWeatherOnline API key, you must register here:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://developer.worldweatheronline.com/auth/register
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After you have a WorldWeatherOnline key, you can save it into the WWO key file: &lt;code&gt;~/.wwo.key&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Also, you have to specify the key in the &lt;code&gt;wego&lt;/code&gt; configuration:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;$ cat ~/.wegorc
{
	"APIKey": "00XXXXXXXXXXXXXXXXXXXXXXXXXXX",
	"City": "London",
	"Numdays": 3,
	"Imperial": false,
	"Lang": "en"
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;City&lt;/code&gt; parameter in &lt;code&gt;~/.wegorc&lt;/code&gt; is ignored.&lt;/p&gt; 
&lt;h3&gt;Configure wttr.in&lt;/h3&gt; 
&lt;p&gt;Configure the following environment variables that define the path to the local &lt;code&gt;wttr.in&lt;/code&gt; installation, to the GeoLite database, and to the &lt;code&gt;wego&lt;/code&gt; installation. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export WTTR_MYDIR="/home/igor/wttr.in"
export WTTR_GEOLITE="/home/igor/wttr.in/GeoLite2-City.mmdb"
export WTTR_WEGO="/home/igor/go/bin/wego"
export WTTR_LISTEN_HOST="0.0.0.0"
export WTTR_LISTEN_PORT="8002"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Configure the HTTP-frontend service&lt;/h3&gt; 
&lt;p&gt;It's recommended that you also configure the web server that will be used to access the service:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-nginx"&gt;server {
	listen [::]:80;
	server_name  wttr.in *.wttr.in;
	access_log  /var/log/nginx/wttr.in-access.log  main;
	error_log  /var/log/nginx/wttr.in-error.log;

	location / {
	    proxy_pass         http://127.0.0.1:8002;

	    proxy_set_header   Host             $host;
	    proxy_set_header   X-Real-IP        $remote_addr;
	    proxy_set_header   X-Forwarded-For  $remote_addr;

	    client_max_body_size       10m;
	    client_body_buffer_size    128k;

	    proxy_connect_timeout      90;
	    proxy_send_timeout         90;
	    proxy_read_timeout         90;

	    proxy_buffer_size          4k;
	    proxy_buffers              4 32k;
	    proxy_busy_buffers_size    64k;
	    proxy_temp_file_write_size 64k;

	    expires                    off;
	}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;wttr.in usage stats&lt;/h2&gt; 
&lt;p&gt;As of the end of June 2025, &lt;em&gt;wttr.in&lt;/em&gt; handles 20-25 million queries per day from 150,000 to 175,000 users, according to the access logs.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/chubin/wttr.in/master/share/stats/stats.png" alt="wttr.in usage stats"&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>vanna-ai/vanna</title>
      <link>https://github.com/vanna-ai/vanna</link>
      <description>&lt;p&gt;ü§ñ Chat with your SQL database üìä. Accurate Text-to-SQL Generation via LLMs using RAG üîÑ.&lt;/p&gt;&lt;hr&gt;&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;GitHub&lt;/th&gt; 
   &lt;th&gt;PyPI&lt;/th&gt; 
   &lt;th&gt;Documentation&lt;/th&gt; 
   &lt;th&gt;Gurubase&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/vanna-ai/vanna"&gt;&lt;img src="https://img.shields.io/badge/GitHub-vanna-blue?logo=github" alt="GitHub"&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://pypi.org/project/vanna/"&gt;&lt;img src="https://img.shields.io/pypi/v/vanna?logo=pypi" alt="PyPI"&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://vanna.ai/docs/"&gt;&lt;img src="https://img.shields.io/badge/Documentation-vanna-blue?logo=read-the-docs" alt="Documentation"&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://gurubase.io/g/vanna"&gt;&lt;img src="https://img.shields.io/badge/Gurubase-Ask%20Vanna%20Guru-006BFF" alt="Gurubase"&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Vanna&lt;/h1&gt; 
&lt;p&gt;Vanna is an MIT-licensed open-source Python RAG (Retrieval-Augmented Generation) framework for SQL generation and related functionality.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/vanna-ai/vanna/assets/7146154/1901f47a-515d-4982-af50-f12761a3b2ce"&gt;https://github.com/vanna-ai/vanna/assets/7146154/1901f47a-515d-4982-af50-f12761a3b2ce&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/vanna-ai/vanna/assets/7146154/1c7c88ba-c144-4ecf-a028-cf5ba7344ca2" alt="vanna-quadrants"&gt;&lt;/p&gt; 
&lt;h2&gt;How Vanna works&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://github.com/vanna-ai/vanna/assets/7146154/1d2718ad-12a8-4a76-afa2-c61754462f93" alt="Screen Recording 2024-01-24 at 11 21 37‚ÄØAM"&gt;&lt;/p&gt; 
&lt;p&gt;Vanna works in two easy steps - train a RAG "model" on your data, and then ask questions which will return SQL queries that can be set up to automatically run on your database.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Train a RAG "model" on your data&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ask questions&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/vanna-ai/vanna/main/img/vanna-readme-diagram.png" alt=""&gt;&lt;/p&gt; 
&lt;p&gt;If you don't know what RAG is, don't worry -- you don't need to know how this works under the hood to use it. You just need to know that you "train" a model, which stores some metadata and then use it to "ask" questions.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/vanna-ai/vanna/raw/main/src/vanna/base/base.py"&gt;base class&lt;/a&gt; for more details on how this works under the hood.&lt;/p&gt; 
&lt;h2&gt;User Interfaces&lt;/h2&gt; 
&lt;p&gt;These are some of the user interfaces that we've built using Vanna. You can use these as-is or as a starting point for your own custom interface.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://vanna.ai/docs/postgres-openai-vanna-vannadb/"&gt;Jupyter Notebook&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vanna-ai/vanna-streamlit"&gt;vanna-ai/vanna-streamlit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vanna-ai/vanna-flask"&gt;vanna-ai/vanna-flask&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vanna-ai/vanna-slack"&gt;vanna-ai/vanna-slack&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Supported LLMs&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vanna-ai/vanna/tree/main/src/vanna/openai"&gt;OpenAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vanna-ai/vanna/tree/main/src/vanna/anthropic"&gt;Anthropic&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vanna-ai/vanna/raw/main/src/vanna/google/gemini_chat.py"&gt;Gemini&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vanna-ai/vanna/raw/main/src/vanna/hf/hf.py"&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vanna-ai/vanna/tree/main/src/vanna/bedrock"&gt;AWS Bedrock&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vanna-ai/vanna/tree/main/src/vanna/ollama"&gt;Ollama&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vanna-ai/vanna/tree/main/src/vanna/qianwen"&gt;Qianwen&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vanna-ai/vanna/tree/main/src/vanna/qianfan"&gt;Qianfan&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vanna-ai/vanna/tree/main/src/vanna/ZhipuAI"&gt;Zhipu&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Supported VectorStores&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vanna-ai/vanna/tree/main/src/vanna/azuresearch"&gt;AzureSearch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vanna-ai/vanna/tree/main/src/vanna/opensearch"&gt;Opensearch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vanna-ai/vanna/tree/main/src/vanna/pgvector"&gt;PgVector&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vanna-ai/vanna/tree/main/src/vanna/pinecone"&gt;PineCone&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vanna-ai/vanna/tree/main/src/vanna/chromadb"&gt;ChromaDB&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vanna-ai/vanna/tree/main/src/vanna/faiss"&gt;FAISS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vanna-ai/vanna/tree/main/src/vanna/marqo"&gt;Marqo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vanna-ai/vanna/tree/main/src/vanna/milvus"&gt;Milvus&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vanna-ai/vanna/tree/main/src/vanna/qdrant"&gt;Qdrant&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vanna-ai/vanna/tree/main/src/vanna/weaviate"&gt;Weaviate&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vanna-ai/vanna/tree/main/src/vanna/oracle"&gt;Oracle&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Supported Databases&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.postgresql.org/"&gt;PostgreSQL&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.mysql.com/"&gt;MySQL&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://prestodb.io/"&gt;PrestoDB&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://hive.apache.org/"&gt;Apache Hive&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://clickhouse.com/"&gt;ClickHouse&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.snowflake.com/en/"&gt;Snowflake&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.oracle.com/"&gt;Oracle&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.microsoft.com/en-us/sql-server/sql-server-downloads"&gt;Microsoft SQL Server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cloud.google.com/bigquery"&gt;BigQuery&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.sqlite.org/"&gt;SQLite&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://duckdb.org/"&gt;DuckDB&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://vanna.ai/docs/"&gt;documentation&lt;/a&gt; for specifics on your desired database, LLM, etc.&lt;/p&gt; 
&lt;p&gt;If you want to get a feel for how it works after training, you can try this &lt;a href="https://vanna.ai/docs/app/"&gt;Colab notebook&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Install&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install vanna
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;There are a number of optional packages that can be installed so see the &lt;a href="https://vanna.ai/docs/"&gt;documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;Import&lt;/h3&gt; 
&lt;p&gt;See the &lt;a href="https://vanna.ai/docs/"&gt;documentation&lt;/a&gt; if you're customizing the LLM or vector database.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# The import statement will vary depending on your LLM and vector database. This is an example for OpenAI + ChromaDB

from vanna.openai.openai_chat import OpenAI_Chat
from vanna.chromadb.chromadb_vector import ChromaDB_VectorStore

class MyVanna(ChromaDB_VectorStore, OpenAI_Chat):
    def __init__(self, config=None):
        ChromaDB_VectorStore.__init__(self, config=config)
        OpenAI_Chat.__init__(self, config=config)

vn = MyVanna(config={'api_key': 'sk-...', 'model': 'gpt-4-...'})

# See the documentation for other options

&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Training&lt;/h2&gt; 
&lt;p&gt;You may or may not need to run these &lt;code&gt;vn.train&lt;/code&gt; commands depending on your use case. See the &lt;a href="https://vanna.ai/docs/"&gt;documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;These statements are shown to give you a feel for how it works.&lt;/p&gt; 
&lt;h3&gt;Train with DDL Statements&lt;/h3&gt; 
&lt;p&gt;DDL statements contain information about the table names, columns, data types, and relationships in your database.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;vn.train(ddl="""
    CREATE TABLE IF NOT EXISTS my-table (
        id INT PRIMARY KEY,
        name VARCHAR(100),
        age INT
    )
""")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Train with Documentation&lt;/h3&gt; 
&lt;p&gt;Sometimes you may want to add documentation about your business terminology or definitions.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;vn.train(documentation="Our business defines XYZ as ...")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Train with SQL&lt;/h3&gt; 
&lt;p&gt;You can also add SQL queries to your training data. This is useful if you have some queries already laying around. You can just copy and paste those from your editor to begin generating new SQL.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;vn.train(sql="SELECT name, age FROM my-table WHERE name = 'John Doe'")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Asking questions&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;vn.ask("What are the top 10 customers by sales?")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You'll get SQL&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT c.c_name as customer_name,
        sum(l.l_extendedprice * (1 - l.l_discount)) as total_sales
FROM   snowflake_sample_data.tpch_sf1.lineitem l join snowflake_sample_data.tpch_sf1.orders o
        ON l.l_orderkey = o.o_orderkey join snowflake_sample_data.tpch_sf1.customer c
        ON o.o_custkey = c.c_custkey
GROUP BY customer_name
ORDER BY total_sales desc limit 10;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you've connected to a database, you'll get the table:&lt;/p&gt; 
&lt;div&gt; 
 &lt;table border="1" class="dataframe"&gt; 
  &lt;thead&gt; 
   &lt;tr style="text-align: right;"&gt; 
    &lt;th&gt;&lt;/th&gt; 
    &lt;th&gt;CUSTOMER_NAME&lt;/th&gt; 
    &lt;th&gt;TOTAL_SALES&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;th&gt;0&lt;/th&gt; 
    &lt;td&gt;Customer#000143500&lt;/td&gt; 
    &lt;td&gt;6757566.0218&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;th&gt;1&lt;/th&gt; 
    &lt;td&gt;Customer#000095257&lt;/td&gt; 
    &lt;td&gt;6294115.3340&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;th&gt;2&lt;/th&gt; 
    &lt;td&gt;Customer#000087115&lt;/td&gt; 
    &lt;td&gt;6184649.5176&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;th&gt;3&lt;/th&gt; 
    &lt;td&gt;Customer#000131113&lt;/td&gt; 
    &lt;td&gt;6080943.8305&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;th&gt;4&lt;/th&gt; 
    &lt;td&gt;Customer#000134380&lt;/td&gt; 
    &lt;td&gt;6075141.9635&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;th&gt;5&lt;/th&gt; 
    &lt;td&gt;Customer#000103834&lt;/td&gt; 
    &lt;td&gt;6059770.3232&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;th&gt;6&lt;/th&gt; 
    &lt;td&gt;Customer#000069682&lt;/td&gt; 
    &lt;td&gt;6057779.0348&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;th&gt;7&lt;/th&gt; 
    &lt;td&gt;Customer#000102022&lt;/td&gt; 
    &lt;td&gt;6039653.6335&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;th&gt;8&lt;/th&gt; 
    &lt;td&gt;Customer#000098587&lt;/td&gt; 
    &lt;td&gt;6027021.5855&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;th&gt;9&lt;/th&gt; 
    &lt;td&gt;Customer#000064660&lt;/td&gt; 
    &lt;td&gt;5905659.6159&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;p&gt;You'll also get an automated Plotly chart: &lt;img src="https://raw.githubusercontent.com/vanna-ai/vanna/main/img/top-10-customers.png" alt=""&gt;&lt;/p&gt; 
&lt;h2&gt;RAG vs. Fine-Tuning&lt;/h2&gt; 
&lt;p&gt;RAG&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Portable across LLMs&lt;/li&gt; 
 &lt;li&gt;Easy to remove training data if any of it becomes obsolete&lt;/li&gt; 
 &lt;li&gt;Much cheaper to run than fine-tuning&lt;/li&gt; 
 &lt;li&gt;More future-proof -- if a better LLM comes out, you can just swap it out&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Fine-Tuning&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Good if you need to minimize tokens in the prompt&lt;/li&gt; 
 &lt;li&gt;Slow to get started&lt;/li&gt; 
 &lt;li&gt;Expensive to train and run (generally)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Why Vanna?&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;High accuracy on complex datasets.&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Vanna‚Äôs capabilities are tied to the training data you give it&lt;/li&gt; 
   &lt;li&gt;More training data means better accuracy for large and complex datasets&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Secure and private.&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Your database contents are never sent to the LLM or the vector database&lt;/li&gt; 
   &lt;li&gt;SQL execution happens in your local environment&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Self learning.&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;If using via Jupyter, you can choose to "auto-train" it on the queries that were successfully executed&lt;/li&gt; 
   &lt;li&gt;If using via other interfaces, you can have the interface prompt the user to provide feedback on the results&lt;/li&gt; 
   &lt;li&gt;Correct question to SQL pairs are stored for future reference and make the future results more accurate&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Supports any SQL database.&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;The package allows you to connect to any SQL database that you can otherwise connect to with Python&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Choose your front end.&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Most people start in a Jupyter Notebook.&lt;/li&gt; 
   &lt;li&gt;Expose to your end users via Slackbot, web app, Streamlit app, or a custom front end.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Extending Vanna&lt;/h2&gt; 
&lt;p&gt;Vanna is designed to connect to any database, LLM, and vector database. There's a &lt;a href="https://github.com/vanna-ai/vanna/raw/main/src/vanna/base/base.py"&gt;VannaBase&lt;/a&gt; abstract base class that defines some basic functionality. The package provides implementations for use with OpenAI and ChromaDB. You can easily extend Vanna to use your own LLM or vector database. See the &lt;a href="https://vanna.ai/docs/"&gt;documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Vanna in 100 Seconds&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/vanna-ai/vanna/assets/7146154/eb90ee1e-aa05-4740-891a-4fc10e611cab"&gt;https://github.com/vanna-ai/vanna/assets/7146154/eb90ee1e-aa05-4740-891a-4fc10e611cab&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;More resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://vanna.ai/docs/"&gt;Full Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://vanna.ai"&gt;Website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/qUZYKHremx"&gt;Discord group for support&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>soxoj/maigret</title>
      <link>https://github.com/soxoj/maigret</link>
      <description>&lt;p&gt;üïµÔ∏è‚Äç‚ôÇÔ∏è Collect a dossier on a person by username from thousands of sites&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Maigret&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;p align="center"&gt; &lt;a href="https://pypi.org/project/maigret/"&gt; &lt;img alt="PyPI version badge for Maigret" src="https://img.shields.io/pypi/v/maigret?style=flat-square"&gt; &lt;/a&gt; &lt;a href="https://pypi.org/project/maigret/"&gt; &lt;img alt="PyPI download count for Maigret" src="https://img.shields.io/pypi/dw/maigret?style=flat-square"&gt; &lt;/a&gt; &lt;a href="https://github.com/soxoj/maigret"&gt; &lt;img alt="Minimum Python version required: 3.10+" src="https://img.shields.io/badge/Python-3.10%2B-brightgreen?style=flat-square"&gt; &lt;/a&gt; &lt;a href="https://github.com/soxoj/maigret/raw/main/LICENSE"&gt; &lt;img alt="License badge for Maigret" src="https://img.shields.io/github/license/soxoj/maigret?style=flat-square"&gt; &lt;/a&gt; &lt;a href="https://github.com/soxoj/maigret"&gt; &lt;img alt="View count for Maigret project" src="https://komarev.com/ghpvc/?username=maigret&amp;amp;color=brightgreen&amp;amp;label=views&amp;amp;style=flat-square"&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/soxoj/maigret/main/static/maigret.png" height="300"&gt; &lt;/p&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;p&gt;&lt;i&gt;The Commissioner Jules Maigret is a fictional French police detective, created by Georges Simenon. His investigation method is based on understanding the personality of different people and their interactions.&lt;/i&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b&gt;üëâüëâüëâ &lt;a href="https://t.me/osint_maigret_bot"&gt;Online Telegram bot&lt;/a&gt;&lt;/b&gt;&lt;/p&gt; 
&lt;h2&gt;About&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Maigret&lt;/strong&gt; collects a dossier on a person &lt;strong&gt;by username only&lt;/strong&gt;, checking for accounts on a huge number of sites and gathering all the available information from web pages. No API keys are required. Maigret is an easy-to-use and powerful fork of &lt;a href="https://github.com/sherlock-project/sherlock"&gt;Sherlock&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Currently supports more than 3000 sites (&lt;a href="https://github.com/soxoj/maigret/raw/main/sites.md"&gt;full list&lt;/a&gt;), search is launched against 500 popular sites in descending order of popularity by default. Also supported checking Tor sites, I2P sites, and domains (via DNS resolving).&lt;/p&gt; 
&lt;h2&gt;Powered By Maigret&lt;/h2&gt; 
&lt;p&gt;These are professional tools for social media content analysis and OSINT investigations that use Maigret (banners are clickable).&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/SocialLinks-IO/sociallinks-api"&gt;&lt;img height="60" alt="Social Links API" src="https://github.com/user-attachments/assets/789747b2-d7a0-4d4e-8868-ffc4427df660"&gt;&lt;/a&gt; &lt;a href="https://sociallinks.io/products/sl-crimewall"&gt;&lt;img height="60" alt="Social Links Crimewall" src="https://github.com/user-attachments/assets/0b18f06c-2f38-477b-b946-1be1a632a9d1"&gt;&lt;/a&gt; &lt;a href="https://usersearch.ai/"&gt;&lt;img height="60" alt="UserSearch" src="https://github.com/user-attachments/assets/66daa213-cf7d-40cf-9267-42f97cf77580"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Main features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Profile page parsing, &lt;a href="https://github.com/soxoj/socid_extractor"&gt;extraction&lt;/a&gt; of personal info, links to other profiles, etc.&lt;/li&gt; 
 &lt;li&gt;Recursive search by new usernames and other IDs found&lt;/li&gt; 
 &lt;li&gt;Search by tags (site categories, countries)&lt;/li&gt; 
 &lt;li&gt;Censorship and captcha detection&lt;/li&gt; 
 &lt;li&gt;Requests retries&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the full description of Maigret features &lt;a href="https://maigret.readthedocs.io/en/latest/features.html"&gt;in the documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;‚ÄºÔ∏è Maigret is available online via &lt;a href="https://t.me/osint_maigret_bot"&gt;official Telegram bot&lt;/a&gt;. Consider using it if you don't want to install anything.&lt;/p&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;p&gt;Standalone EXE-binaries for Windows are located in &lt;a href="https://github.com/soxoj/maigret/releases"&gt;Releases section&lt;/a&gt; of GitHub repository.&lt;/p&gt; 
&lt;p&gt;Video guide on how to run it: &lt;a href="https://youtu.be/qIgwTZOmMmM"&gt;https://youtu.be/qIgwTZOmMmM&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Installation in Cloud Shells&lt;/h3&gt; 
&lt;p&gt;You can launch Maigret using cloud shells and Jupyter notebooks. Press one of the buttons below and follow the instructions to launch it in your browser.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://console.cloud.google.com/cloudshell/open?git_repo=https://github.com/soxoj/maigret&amp;amp;tutorial=README.md"&gt;&lt;img src="https://user-images.githubusercontent.com/27065646/92304704-8d146d80-ef80-11ea-8c29-0deaabb1c702.png" alt="Open in Cloud Shell"&gt;&lt;/a&gt; &lt;a href="https://repl.it/github/soxoj/maigret"&gt;&lt;img src="https://replit.com/badge/github/soxoj/maigret" alt="Run on Replit" height="50"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://colab.research.google.com/gist/soxoj/879b51bc3b2f8b695abb054090645000/maigret-collab.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" height="45"&gt;&lt;/a&gt; &lt;a href="https://mybinder.org/v2/gist/soxoj/9d65c2f4d3bec5dd25949197ea73cf3a/HEAD"&gt;&lt;img src="https://mybinder.org/badge_logo.svg?sanitize=true" alt="Open In Binder" height="45"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Local installation&lt;/h3&gt; 
&lt;p&gt;Maigret can be installed using pip, Docker, or simply can be launched from the cloned repo.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Python 3.10 or higher and pip is required, &lt;strong&gt;Python 3.11 is recommended.&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# install from pypi
pip3 install maigret

# usage
maigret username
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Cloning a repository&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# or clone and install manually
git clone https://github.com/soxoj/maigret &amp;amp;&amp;amp; cd maigret

# build and install
pip3 install .

# usage
maigret username
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# official image
docker pull soxoj/maigret

# usage
docker run -v /mydir:/app/reports soxoj/maigret:latest username --html

# manual build
docker build -t maigret .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage examples&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# make HTML, PDF, and Xmind8 reports
maigret user --html
maigret user --pdf
maigret user --xmind #Output not compatible with xmind 2022+

# search on sites marked with tags photo &amp;amp; dating
maigret user --tags photo,dating

# search on sites marked with tag us
maigret user --tags us

# search for three usernames on all available sites
maigret user1 user2 user3 -a
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Use &lt;code&gt;maigret --help&lt;/code&gt; to get full options description. Also options &lt;a href="https://maigret.readthedocs.io/en/latest/command-line-options.html"&gt;are documented&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Web interface&lt;/h3&gt; 
&lt;p&gt;You can run Maigret with a web interface, where you can view the graph with results and download reports of all formats on a single page.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Web Interface Screenshots&lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/soxoj/maigret/main/static/web_interface_screenshot_start.png" alt="Web interface: how to start"&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/soxoj/maigret/main/static/web_interface_screenshot.png" alt="Web interface: results"&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;p&gt;Instructions:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Run Maigret with the &lt;code&gt;--web&lt;/code&gt; flag and specify the port number.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;maigret --web 5000
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt; &lt;p&gt;Open &lt;a href="http://127.0.0.1:5000"&gt;http://127.0.0.1:5000&lt;/a&gt; in your browser and enter one or more usernames to make a search.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Wait a bit for the search to complete and view the graph with results, the table with all accounts found, and download reports of all formats.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Maigret has open-source code, so you may contribute your own sites by adding them to &lt;code&gt;data.json&lt;/code&gt; file, or bring changes to it's code!&lt;/p&gt; 
&lt;p&gt;For more information about development and contribution, please read the &lt;a href="https://maigret.readthedocs.io/en/latest/development.html"&gt;development documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Demo with page parsing and recursive username search&lt;/h2&gt; 
&lt;h3&gt;Video (asciinema)&lt;/h3&gt; 
&lt;a href="https://asciinema.org/a/Ao0y7N0TTxpS0pisoprQJdylZ"&gt; &lt;img src="https://asciinema.org/a/Ao0y7N0TTxpS0pisoprQJdylZ.svg?sanitize=true" alt="asciicast" width="600"&gt; &lt;/a&gt; 
&lt;h3&gt;Reports&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/soxoj/maigret/main/static/report_alexaimephotographycars.pdf"&gt;PDF report&lt;/a&gt;, &lt;a href="https://htmlpreview.github.io/?https://raw.githubusercontent.com/soxoj/maigret/main/static/report_alexaimephotographycars.html"&gt;HTML report&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/soxoj/maigret/main/static/report_alexaimephotography_html_screenshot.png" alt="HTML report screenshot"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/soxoj/maigret/main/static/report_alexaimephotography_xmind_screenshot.png" alt="XMind 8 report screenshot"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/soxoj/maigret/main/static/recursive_search.md"&gt;Full console output&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;This tool is intended for educational and lawful purposes only.&lt;/strong&gt; The developers do not endorse or encourage any illegal activities or misuse of this tool. Regulations regarding the collection and use of personal data vary by country and region, including but not limited to GDPR in the EU, CCPA in the USA, and similar laws worldwide.&lt;/p&gt; 
&lt;p&gt;It is your sole responsibility to ensure that your use of this tool complies with all applicable laws and regulations in your jurisdiction. Any illegal use of this tool is strictly prohibited, and you are fully accountable for your actions.&lt;/p&gt; 
&lt;p&gt;The authors and developers of this tool bear no responsibility for any misuse or unlawful activities conducted by its users.&lt;/p&gt; 
&lt;h2&gt;Feedback&lt;/h2&gt; 
&lt;p&gt;If you have any questions, suggestions, or feedback, please feel free to &lt;a href="https://github.com/soxoj/maigret/issues"&gt;open an issue&lt;/a&gt;, create a &lt;a href="https://github.com/soxoj/maigret/discussions"&gt;GitHub discussion&lt;/a&gt;, or contact the author directly via &lt;a href="https://t.me/soxoj"&gt;Telegram&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;SOWEL classification&lt;/h2&gt; 
&lt;p&gt;This tool uses the following OSINT techniques:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://sowel.soxoj.com/other-platform-accounts"&gt;SOTL-2.2. Search For Accounts On Other Platforms&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://sowel.soxoj.com/logins-reuse"&gt;SOTL-6.1. Check Logins Reuse To Find Another Account&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://sowel.soxoj.com/nicknames-reuse"&gt;SOTL-6.2. Check Nicknames Reuse To Find Another Account&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT ¬© &lt;a href="https://github.com/soxoj/maigret"&gt;Maigret&lt;/a&gt;&lt;br&gt; MIT ¬© &lt;a href="https://github.com/sherlock-project/"&gt;Sherlock Project&lt;/a&gt;&lt;br&gt; Original Creator of Sherlock Project - &lt;a href="https://github.com/sdushantha"&gt;Siddharth Dushantha&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>getzep/graphiti</title>
      <link>https://github.com/getzep/graphiti</link>
      <description>&lt;p&gt;Build Real-Time Knowledge Graphs for AI Agents&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://www.getzep.com/"&gt; &lt;img src="https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73" width="150" alt="Zep Logo"&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h1 align="center"&gt; Graphiti &lt;/h1&gt; 
&lt;h2 align="center"&gt; Build Real-Time Knowledge Graphs for AI Agents&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/getzep/Graphiti/actions/workflows/lint.yml"&gt;&lt;img src="https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat" alt="Lint"&gt;&lt;/a&gt; &lt;a href="https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml"&gt;&lt;img src="https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg?sanitize=true" alt="Unit Tests"&gt;&lt;/a&gt; &lt;a href="https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml"&gt;&lt;img src="https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg?sanitize=true" alt="MyPy Check"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://img.shields.io/github/stars/getzep/graphiti" alt="GitHub Repo stars"&gt; &lt;a href="https://discord.com/invite/W8Kw6bsgXQ"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord"&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2501.13956"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat" alt="arXiv"&gt;&lt;/a&gt; &lt;a href="https://github.com/getzep/graphiti/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/getzep/graphiti?style=flat&amp;amp;label=Release&amp;amp;color=limegreen" alt="Release"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/12986" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/12986" alt="getzep%2Fgraphiti | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span&gt;‚≠ê&lt;/span&gt; &lt;em&gt;Help us reach more developers and grow the Graphiti community. Star this repo!&lt;/em&gt;&lt;/p&gt; 
&lt;br&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Check out the new &lt;a href="https://raw.githubusercontent.com/getzep/graphiti/main/mcp_server/README.md"&gt;MCP server for Graphiti&lt;/a&gt;! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Graphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.&lt;/p&gt; 
&lt;p&gt;Use Graphiti to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Integrate and maintain dynamic user interactions and business data.&lt;/li&gt; 
 &lt;li&gt;Facilitate state-based reasoning and task automation for agents.&lt;/li&gt; 
 &lt;li&gt;Query complex, evolving data with semantic, keyword, and graph-based search methods.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/getzep/graphiti/main/images/graphiti-graph-intro.gif" alt="Graphiti temporal walkthrough" width="700px"&gt; &lt;/p&gt; 
&lt;br&gt; 
&lt;p&gt;A knowledge graph is a network of interconnected facts, such as &lt;em&gt;"Kendra loves Adidas shoes."&lt;/em&gt; Each fact is a "triplet" represented by two entities, or nodes ("Kendra", "Adidas shoes"), and their relationship, or edge ("loves"). Knowledge Graphs have been explored extensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph while handling changing relationships and maintaining historical context.&lt;/p&gt; 
&lt;h2&gt;Graphiti and Zep Memory&lt;/h2&gt; 
&lt;p&gt;Graphiti powers the core of &lt;a href="https://www.getzep.com"&gt;Zep's memory layer&lt;/a&gt; for AI Agents.&lt;/p&gt; 
&lt;p&gt;Using Graphiti, we've demonstrated Zep is the &lt;a href="https://blog.getzep.com/state-of-the-art-agent-memory/"&gt;State of the Art in Agent Memory&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Read our paper: &lt;a href="https://arxiv.org/abs/2501.13956"&gt;Zep: A Temporal Knowledge Graph Architecture for Agent Memory&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We're excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://arxiv.org/abs/2501.13956"&gt;&lt;img src="https://raw.githubusercontent.com/getzep/graphiti/main/images/arxiv-screenshot.png" alt="Zep: A Temporal Knowledge Graph Architecture for Agent Memory" width="700px"&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;Why Graphiti?&lt;/h2&gt; 
&lt;p&gt;Traditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Real-Time Incremental Updates:&lt;/strong&gt; Immediate integration of new data episodes without batch recomputation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bi-Temporal Data Model:&lt;/strong&gt; Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Efficient Hybrid Retrieval:&lt;/strong&gt; Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Custom Entity Definitions:&lt;/strong&gt; Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scalability:&lt;/strong&gt; Efficiently manages large datasets with parallel processing, suitable for enterprise environments.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/getzep/graphiti/main/images/graphiti-intro-slides-stock-2.gif" alt="Graphiti structured + unstructured demo" width="700px"&gt; &lt;/p&gt; 
&lt;h2&gt;Graphiti vs. GraphRAG&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Aspect&lt;/th&gt; 
   &lt;th&gt;GraphRAG&lt;/th&gt; 
   &lt;th&gt;Graphiti&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Primary Use&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Static document summarization&lt;/td&gt; 
   &lt;td&gt;Dynamic data management&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Data Handling&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Batch-oriented processing&lt;/td&gt; 
   &lt;td&gt;Continuous, incremental updates&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Knowledge Structure&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Entity clusters &amp;amp; community summaries&lt;/td&gt; 
   &lt;td&gt;Episodic data, semantic entities, communities&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Retrieval Method&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Sequential LLM summarization&lt;/td&gt; 
   &lt;td&gt;Hybrid semantic, keyword, and graph-based search&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Adaptability&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Low&lt;/td&gt; 
   &lt;td&gt;High&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Temporal Handling&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Basic timestamp tracking&lt;/td&gt; 
   &lt;td&gt;Explicit bi-temporal tracking&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Contradiction Handling&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;LLM-driven summarization judgments&lt;/td&gt; 
   &lt;td&gt;Temporal edge invalidation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Query Latency&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Seconds to tens of seconds&lt;/td&gt; 
   &lt;td&gt;Typically sub-second latency&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Custom Entity Types&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;Yes, customizable&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Scalability&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Moderate&lt;/td&gt; 
   &lt;td&gt;High, optimized for large datasets&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Graphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10 or higher&lt;/li&gt; 
 &lt;li&gt;Neo4j 5.26 / FalkorDB 1.1.2 or higher (serves as the embeddings storage backend)&lt;/li&gt; 
 &lt;li&gt;OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini). Using other services may result in incorrect output schemas and ingestion failures. This is particularly problematic when using smaller models.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Optional:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Google Gemini, Anthropic, or Groq API key (for alternative LLM providers)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] The simplest way to install Neo4j is via &lt;a href="https://neo4j.com/download/"&gt;Neo4j Desktop&lt;/a&gt;. It provides a user-friendly interface to manage Neo4j instances and databases. Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example:&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest

&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install graphiti-core
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add graphiti-core
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Installing with FalkorDB Support&lt;/h3&gt; 
&lt;p&gt;If you plan to use FalkorDB as your graph database backend, install with the FalkorDB extra:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install graphiti-core[falkordb]

# or with uv
uv add graphiti-core[falkordb]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;You can also install optional LLM providers as extras:&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install with Anthropic support
pip install graphiti-core[anthropic]

# Install with Groq support
pip install graphiti-core[groq]

# Install with Google Gemini support
pip install graphiti-core[google-genai]

# Install with multiple providers
pip install graphiti-core[anthropic,groq,google-genai]

# Install with FalkorDB and LLM providers
pip install graphiti-core[falkordb,anthropic,google-genai]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; is set in your environment. Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI compatible APIs.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;For a complete working example, see the &lt;a href="https://raw.githubusercontent.com/getzep/graphiti/main/examples/quickstart/README.md"&gt;Quickstart Example&lt;/a&gt; in the examples directory. The quickstart demonstrates:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Connecting to a Neo4j or FalkorDB database&lt;/li&gt; 
 &lt;li&gt;Initializing Graphiti indices and constraints&lt;/li&gt; 
 &lt;li&gt;Adding episodes to the graph (both text and structured JSON)&lt;/li&gt; 
 &lt;li&gt;Searching for relationships (edges) using hybrid search&lt;/li&gt; 
 &lt;li&gt;Reranking search results using graph distance&lt;/li&gt; 
 &lt;li&gt;Searching for nodes using predefined search recipes&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The example is fully documented with clear explanations of each functionality and includes a comprehensive README with setup instructions and next steps.&lt;/p&gt; 
&lt;h2&gt;MCP Server&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;mcp_server&lt;/code&gt; directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server allows AI assistants to interact with Graphiti's knowledge graph capabilities through the MCP protocol.&lt;/p&gt; 
&lt;p&gt;Key features of the MCP server include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Episode management (add, retrieve, delete)&lt;/li&gt; 
 &lt;li&gt;Entity management and relationship handling&lt;/li&gt; 
 &lt;li&gt;Semantic and hybrid search capabilities&lt;/li&gt; 
 &lt;li&gt;Group management for organizing related data&lt;/li&gt; 
 &lt;li&gt;Graph maintenance operations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant workflows.&lt;/p&gt; 
&lt;p&gt;For detailed setup instructions and usage examples, see the &lt;a href="https://raw.githubusercontent.com/getzep/graphiti/main/mcp_server/README.md"&gt;MCP server README&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;REST Service&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;server&lt;/code&gt; directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.&lt;/p&gt; 
&lt;p&gt;Please see the &lt;a href="https://raw.githubusercontent.com/getzep/graphiti/main/server/README.md"&gt;server README&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Optional Environment Variables&lt;/h2&gt; 
&lt;p&gt;In addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables. If you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables must be set.&lt;/p&gt; 
&lt;h3&gt;Database Configuration&lt;/h3&gt; 
&lt;p&gt;Database names are configured directly in the driver constructors:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Neo4j&lt;/strong&gt;: Database name defaults to &lt;code&gt;neo4j&lt;/code&gt; (hardcoded in Neo4jDriver)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;FalkorDB&lt;/strong&gt;: Database name defaults to &lt;code&gt;default_db&lt;/code&gt; (hardcoded in FalkorDriver)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;As of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the &lt;code&gt;graph_driver&lt;/code&gt; parameter.&lt;/p&gt; 
&lt;h4&gt;Neo4j with Custom Database Name&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from graphiti_core import Graphiti
from graphiti_core.driver.neo4j_driver import Neo4jDriver

# Create a Neo4j driver with custom database name
driver = Neo4jDriver(
    uri="bolt://localhost:7687",
    user="neo4j",
    password="password",
    database="my_custom_database"  # Custom database name
)

# Pass the driver to Graphiti
graphiti = Graphiti(graph_driver=driver)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;FalkorDB with Custom Database Name&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from graphiti_core import Graphiti
from graphiti_core.driver.falkordb_driver import FalkorDriver

# Create a FalkorDB driver with custom database name
driver = FalkorDriver(
    host="localhost",
    port=6379,
    username="falkor_user",  # Optional
    password="falkor_password",  # Optional
    database="my_custom_graph"  # Custom database name
)

# Pass the driver to Graphiti
graphiti = Graphiti(graph_driver=driver)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Performance Configuration&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;USE_PARALLEL_RUNTIME&lt;/code&gt; is an optional boolean variable that can be set to true if you wish to enable Neo4j's parallel runtime feature for several of our search queries. Note that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances, as such this feature is off by default.&lt;/p&gt; 
&lt;h2&gt;Using Graphiti with Azure OpenAI&lt;/h2&gt; 
&lt;p&gt;Graphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import AsyncAzureOpenAI
from graphiti_core import Graphiti
from graphiti_core.llm_client import LLMConfig, OpenAIClient
from graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig
from graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient

# Azure OpenAI configuration - use separate endpoints for different services
api_key = "&amp;lt;your-api-key&amp;gt;"
api_version = "&amp;lt;your-api-version&amp;gt;"
llm_endpoint = "&amp;lt;your-llm-endpoint&amp;gt;"  # e.g., "https://your-llm-resource.openai.azure.com/"
embedding_endpoint = "&amp;lt;your-embedding-endpoint&amp;gt;"  # e.g., "https://your-embedding-resource.openai.azure.com/"

# Create separate Azure OpenAI clients for different services
llm_client_azure = AsyncAzureOpenAI(
    api_key=api_key,
    api_version=api_version,
    azure_endpoint=llm_endpoint
)

embedding_client_azure = AsyncAzureOpenAI(
    api_key=api_key,
    api_version=api_version,
    azure_endpoint=embedding_endpoint
)

# Create LLM Config with your Azure deployment names
azure_llm_config = LLMConfig(
    small_model="gpt-4.1-nano",
    model="gpt-4.1-mini",
)

# Initialize Graphiti with Azure OpenAI clients
graphiti = Graphiti(
    "bolt://localhost:7687",
    "neo4j",
    "password",
    llm_client=OpenAIClient(
        llm_config=azure_llm_config,
        client=llm_client_azure
    ),
    embedder=OpenAIEmbedder(
        config=OpenAIEmbedderConfig(
            embedding_model="text-embedding-3-small-deployment"  # Your Azure embedding deployment name
        ),
        client=embedding_client_azure
    ),
    cross_encoder=OpenAIRerankerClient(
        llm_config=LLMConfig(
            model=azure_llm_config.small_model  # Use small model for reranking
        ),
        client=llm_client_azure
    )
)

# Now you can use Graphiti with Azure OpenAI
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Make sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names that match your Azure OpenAI service configuration.&lt;/p&gt; 
&lt;h2&gt;Using Graphiti with Google Gemini&lt;/h2&gt; 
&lt;p&gt;Graphiti supports Google's Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini, you'll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.&lt;/p&gt; 
&lt;p&gt;Install Graphiti:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add "graphiti-core[google-genai]"

# or

pip install "graphiti-core[google-genai]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from graphiti_core import Graphiti
from graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig
from graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig
from graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient

# Google API key configuration
api_key = "&amp;lt;your-google-api-key&amp;gt;"

# Initialize Graphiti with Gemini clients
graphiti = Graphiti(
    "bolt://localhost:7687",
    "neo4j",
    "password",
    llm_client=GeminiClient(
        config=LLMConfig(
            api_key=api_key,
            model="gemini-2.0-flash"
        )
    ),
    embedder=GeminiEmbedder(
        config=GeminiEmbedderConfig(
            api_key=api_key,
            embedding_model="embedding-001"
        )
    ),
    cross_encoder=GeminiRerankerClient(
        config=LLMConfig(
            api_key=api_key,
            model="gemini-2.5-flash-lite-preview-06-17"
        )
    )
)

# Now you can use Graphiti with Google Gemini for all components
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The Gemini reranker uses the &lt;code&gt;gemini-2.5-flash-lite-preview-06-17&lt;/code&gt; model by default, which is optimized for cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI reranker, leveraging Gemini's log probabilities feature to rank passage relevance.&lt;/p&gt; 
&lt;h2&gt;Using Graphiti with Ollama (Local LLM)&lt;/h2&gt; 
&lt;p&gt;Graphiti supports Ollama for running local LLMs and embedding models via Ollama's OpenAI-compatible API. This is ideal for privacy-focused applications or when you want to avoid API costs.&lt;/p&gt; 
&lt;p&gt;Install the models: ollama pull deepseek-r1:7b # LLM ollama pull nomic-embed-text # embeddings&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from graphiti_core import Graphiti
from graphiti_core.llm_client.config import LLMConfig
from graphiti_core.llm_client.openai_client import OpenAIClient
from graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig
from graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient

# Configure Ollama LLM client
llm_config = LLMConfig(
    api_key="abc",  # Ollama doesn't require a real API key
    model="deepseek-r1:7b",
    small_model="deepseek-r1:7b",
    base_url="http://localhost:11434/v1", # Ollama provides this port
)

llm_client = OpenAIClient(config=llm_config)

# Initialize Graphiti with Ollama clients
graphiti = Graphiti(
    "bolt://localhost:7687",
    "neo4j",
    "password",
    llm_client=llm_client,
    embedder=OpenAIEmbedder(
        config=OpenAIEmbedderConfig(
            api_key="abc",
            embedding_model="nomic-embed-text",
            embedding_dim=768,
            base_url="http://localhost:11434/v1",
        )
    ),
    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),
)

# Now you can use Graphiti with local Ollama models
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Ensure Ollama is running (&lt;code&gt;ollama serve&lt;/code&gt;) and that you have pulled the models you want to use.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://help.getzep.com/graphiti"&gt;Guides and API documentation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://help.getzep.com/graphiti/graphiti/quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://help.getzep.com/graphiti/graphiti/lang-graph-agent"&gt;Building an agent with LangChain's LangGraph and Graphiti&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Telemetry&lt;/h2&gt; 
&lt;p&gt;Graphiti collects anonymous usage statistics to help us understand how the framework is being used and improve it for everyone. We believe transparency is important, so here's exactly what we collect and why.&lt;/p&gt; 
&lt;h3&gt;What We Collect&lt;/h3&gt; 
&lt;p&gt;When you initialize a Graphiti instance, we collect:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Anonymous identifier&lt;/strong&gt;: A randomly generated UUID stored locally in &lt;code&gt;~/.cache/graphiti/telemetry_anon_id&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;System information&lt;/strong&gt;: Operating system, Python version, and system architecture&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Graphiti version&lt;/strong&gt;: The version you're using&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configuration choices&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;LLM provider type (OpenAI, Azure, Anthropic, etc.)&lt;/li&gt; 
   &lt;li&gt;Database backend (Neo4j, FalkorDB)&lt;/li&gt; 
   &lt;li&gt;Embedder provider (OpenAI, Azure, Voyage, etc.)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;What We Don't Collect&lt;/h3&gt; 
&lt;p&gt;We are committed to protecting your privacy. We &lt;strong&gt;never&lt;/strong&gt; collect:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Personal information or identifiers&lt;/li&gt; 
 &lt;li&gt;API keys or credentials&lt;/li&gt; 
 &lt;li&gt;Your actual data, queries, or graph content&lt;/li&gt; 
 &lt;li&gt;IP addresses or hostnames&lt;/li&gt; 
 &lt;li&gt;File paths or system-specific information&lt;/li&gt; 
 &lt;li&gt;Any content from your episodes, nodes, or edges&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Why We Collect This Data&lt;/h3&gt; 
&lt;p&gt;This information helps us:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Understand which configurations are most popular to prioritize support and testing&lt;/li&gt; 
 &lt;li&gt;Identify which LLM and database providers to focus development efforts on&lt;/li&gt; 
 &lt;li&gt;Track adoption patterns to guide our roadmap&lt;/li&gt; 
 &lt;li&gt;Ensure compatibility across different Python versions and operating systems&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By sharing this anonymous information, you help us make Graphiti better for everyone in the community.&lt;/p&gt; 
&lt;h3&gt;View the Telemetry Code&lt;/h3&gt; 
&lt;p&gt;The Telemetry code &lt;a href="https://raw.githubusercontent.com/getzep/graphiti/main/graphiti_core/telemetry/telemetry.py"&gt;may be found here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;How to Disable Telemetry&lt;/h3&gt; 
&lt;p&gt;Telemetry is &lt;strong&gt;opt-out&lt;/strong&gt; and can be disabled at any time. To disable telemetry collection:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Option 1: Environment Variable&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export GRAPHITI_TELEMETRY_ENABLED=false
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option 2: Set in your shell profile&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# For bash users (~/.bashrc or ~/.bash_profile)
echo 'export GRAPHITI_TELEMETRY_ENABLED=false' &amp;gt;&amp;gt; ~/.bashrc

# For zsh users (~/.zshrc)
echo 'export GRAPHITI_TELEMETRY_ENABLED=false' &amp;gt;&amp;gt; ~/.zshrc
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option 3: Set for a specific Python session&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
os.environ['GRAPHITI_TELEMETRY_ENABLED'] = 'false'

# Then initialize Graphiti as usual
from graphiti_core import Graphiti
graphiti = Graphiti(...)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Telemetry is automatically disabled during test runs (when &lt;code&gt;pytest&lt;/code&gt; is detected).&lt;/p&gt; 
&lt;h3&gt;Technical Details&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Telemetry uses PostHog for anonymous analytics collection&lt;/li&gt; 
 &lt;li&gt;All telemetry operations are designed to fail silently - they will never interrupt your application or affect Graphiti functionality&lt;/li&gt; 
 &lt;li&gt;The anonymous ID is stored locally and is not tied to any personal information&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Status and Roadmap&lt;/h2&gt; 
&lt;p&gt;Graphiti is under active development. We aim to maintain API stability while working on:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Supporting custom graph schemas: 
  &lt;ul&gt; 
   &lt;li&gt;Allow developers to provide their own defined node and edge classes when ingesting episodes&lt;/li&gt; 
   &lt;li&gt;Enable more flexible knowledge representation tailored to specific use cases&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Enhancing retrieval capabilities with more robust and configurable options&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Graphiti MCP Server&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled&gt; Expanding test coverage to ensure reliability and catch edge cases&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or answering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer to &lt;a href="https://raw.githubusercontent.com/getzep/graphiti/main/CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;Join the &lt;a href="https://discord.com/invite/W8Kw6bsgXQ"&gt;Zep Discord server&lt;/a&gt; and make your way to the &lt;strong&gt;#Graphiti&lt;/strong&gt; channel!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Lightricks/LTX-Video</title>
      <link>https://github.com/Lightricks/LTX-Video</link>
      <description>&lt;p&gt;Official repository for LTX-Video&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;LTX-Video&lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://www.lightricks.com/ltxv"&gt;&lt;img src="https://img.shields.io/badge/Website-LTXV-181717?logo=google-chrome" alt="Website"&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/Lightricks/LTX-Video"&gt;&lt;img src="https://img.shields.io/badge/HuggingFace-Model-orange?logo=huggingface" alt="Model"&gt;&lt;/a&gt; &lt;a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b"&gt;&lt;img src="https://img.shields.io/badge/Demo-Try%20Now-brightgreen?logo=vercel" alt="Demo"&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2501.00103"&gt;&lt;img src="https://img.shields.io/badge/Paper-arXiv-B31B1B?logo=arxiv" alt="Paper"&gt;&lt;/a&gt; &lt;a href="https://github.com/Lightricks/LTX-Video-Trainer"&gt;&lt;img src="https://img.shields.io/badge/LTXV-Trainer-9146FF?logo=github" alt="Trainer"&gt;&lt;/a&gt; &lt;a href="https://discord.gg/Mn8BRgUKKy"&gt;&lt;img src="https://img.shields.io/badge/Join-Discord-5865F2?logo=discord" alt="Discord"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;This is the official repository for LTX-Video.&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#news"&gt;What's new&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#models"&gt;Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#quick-start-guide"&gt;Quick Start Guide&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#online-inference"&gt;Online demo&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#run-locally"&gt;Run locally&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#inference"&gt;Inference&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#comfyui-integration"&gt;ComfyUI Integration&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#diffusers-integration"&gt;Diffusers Integration&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#model-user-guide"&gt;Model User Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#community-contribution"&gt;Community Contribution&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#training"&gt;Training&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#control-models"&gt;Control Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#join-us-"&gt;Join Us!&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#acknowledgement"&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Introduction&lt;/h1&gt; 
&lt;p&gt;LTX-Video is the first DiT-based video generation model that can generate high-quality videos in &lt;em&gt;real-time&lt;/em&gt;. It can generate 30 FPS videos at 1216√ó704 resolution, faster than it takes to watch them. The model is trained on a large-scale dataset of diverse videos and can generate high-resolution videos with realistic and diverse content.&lt;/p&gt; 
&lt;p&gt;The model supports image-to-video, keyframe-based animation, video extension (both forward and backward), video-to-video transformations, and any combination of these features.&lt;/p&gt; 
&lt;h3&gt;Image-to-video examples&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00001.gif" alt="example1"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00002.gif" alt="example2"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00003.gif" alt="example3"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00004.gif" alt="example4"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00005.gif" alt="example5"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00006.gif" alt="example6"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00007.gif" alt="example7"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00008.gif" alt="example8"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00009.gif" alt="example9"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Controlled video examples&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_ic_2v_example_00000.gif" alt="control0"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_ic_2v_example_00001.gif" alt="control1"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_ic_2v_example_00002.gif" alt="control2"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_ic_2v_example_00003.gif" alt="control3"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_ic_2v_example_00004.gif" alt="control4"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;News&lt;/h1&gt; 
&lt;h2&gt;July, 16th, 2025: New Distilled models v0.9.8 with up to 60 seconds of video:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Long shot generation in LTXV-13B! 
  &lt;ul&gt; 
   &lt;li&gt;LTX-Video now supports up to 60 seconds of video.&lt;/li&gt; 
   &lt;li&gt;Compatible also with the official IC-LoRAs.&lt;/li&gt; 
   &lt;li&gt;Try now in &lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/tree/master/example_workflows/ltxv-13b-i2v-long-multi-prompt.json"&gt;ComfyUI&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Release a new distilled models: 
  &lt;ul&gt; 
   &lt;li&gt;13B distilled model &lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-distilled.yaml"&gt;ltxv-13b-0.9.8-distilled&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;2B distilled model &lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.8-distilled.yaml"&gt;ltxv-2b-0.9.8-distilled&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Both models are distilled from the same base model &lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-dev.yaml"&gt;ltxv-13b-0.9.8-dev&lt;/a&gt; and are compatible for use together in the same multiscale pipeline.&lt;/li&gt; 
   &lt;li&gt;Improved prompt understanding and detail generation&lt;/li&gt; 
   &lt;li&gt;Includes corresponding FP8 weights and workflows.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Release a new detailer model &lt;a href="https://huggingface.co/Lightricks/LTX-Video-ICLoRA-detailer-13b-0.9.8"&gt;LTX-Video-ICLoRA-detailer-13B-0.9.8&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Available in &lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/tree/master/example_workflows/ltxv-13b-upscale.json"&gt;ComfyUI&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;July, 8th, 2025: New Control Models Released!&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Released three new control models for LTX-Video on HuggingFace: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Depth Control&lt;/strong&gt;: &lt;a href="https://huggingface.co/Lightricks/LTX-Video-ICLoRA-depth-13b-0.9.7"&gt;LTX-Video-ICLoRA-depth-13b-0.9.7&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Pose Control&lt;/strong&gt;: &lt;a href="https://huggingface.co/Lightricks/LTX-Video-ICLoRA-pose-13b-0.9.7"&gt;LTX-Video-ICLoRA-pose-13b-0.9.7&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Canny Control&lt;/strong&gt;: &lt;a href="https://huggingface.co/Lightricks/LTX-Video-ICLoRA-canny-13b-0.9.7"&gt;LTX-Video-ICLoRA-canny-13b-0.9.7&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;May, 14th, 2025: New distilled model 13B v0.9.7:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release a new 13B distilled model &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled.safetensors"&gt;ltxv-13b-0.9.7-distilled&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Amazing for iterative work - generates HD videos in 10 seconds, with low-res preview after just 3 seconds (on H100)!&lt;/li&gt; 
   &lt;li&gt;Does not require classifier-free guidance and spatio-temporal guidance.&lt;/li&gt; 
   &lt;li&gt;Supports sampling with 8 (recommended), or less diffusion steps.&lt;/li&gt; 
   &lt;li&gt;Also released a LoRA version of the distilled model, &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled-lora128.safetensors"&gt;ltxv-13b-0.9.7-distilled-lora128&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Requires only 1GB of VRAM&lt;/li&gt; 
     &lt;li&gt;Can be used with the full 13B model for fast inference&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Release a new quantized distilled model &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled-fp8.safetensors"&gt;ltxv-13b-0.9.7-distilled-fp8&lt;/a&gt; for &lt;em&gt;real-time&lt;/em&gt; generation (on H100) with even less VRAM&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;May, 5th, 2025: New model 13B v0.9.7:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release a new 13B model &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev.safetensors"&gt;ltxv-13b-0.9.7-dev&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Release a new quantized model &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev-fp8.safetensors"&gt;ltxv-13b-0.9.7-dev-fp8&lt;/a&gt; for faster inference with less VRam&lt;/li&gt; 
 &lt;li&gt;Release a new upscalers 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-temporal-upscaler-0.9.7.safetensors"&gt;ltxv-temporal-upscaler-0.9.7&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-spatial-upscaler-0.9.7.safetensors"&gt;ltxv-spatial-upscaler-0.9.7&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Breakthrough prompt adherence and physical understanding.&lt;/li&gt; 
 &lt;li&gt;New Pipeline for multi-scale video rendering for fast and high quality results&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;April, 15th, 2025: New checkpoints v0.9.6:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release a new checkpoint &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-dev-04-25.safetensors"&gt;ltxv-2b-0.9.6-dev-04-25&lt;/a&gt; with improved quality&lt;/li&gt; 
 &lt;li&gt;Release a new distilled model &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-distilled-04-25.safetensors"&gt;ltxv-2b-0.9.6-distilled-04-25&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;15x faster inference than non-distilled model.&lt;/li&gt; 
   &lt;li&gt;Does not require classifier-free guidance and spatio-temporal guidance.&lt;/li&gt; 
   &lt;li&gt;Supports sampling with 8 (recommended), or less diffusion steps.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Improved prompt adherence, motion quality and fine details.&lt;/li&gt; 
 &lt;li&gt;New default resolution and FPS: 1216 √ó 704 pixels at 30 FPS 
  &lt;ul&gt; 
   &lt;li&gt;Still real time on H100 with the distilled model.&lt;/li&gt; 
   &lt;li&gt;Other resolutions and FPS are still supported.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Support stochastic inference (can improve visual quality when using the distilled model)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;March, 5th, 2025: New checkpoint v0.9.5&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;New license for commercial use (&lt;a href="https://huggingface.co/Lightricks/LTX-Video/ltx-video-2b-v0.9.5.license.txt"&gt;OpenRail-M&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Release a new checkpoint v0.9.5 with improved quality&lt;/li&gt; 
 &lt;li&gt;Support keyframes and video extension&lt;/li&gt; 
 &lt;li&gt;Support higher resolutions&lt;/li&gt; 
 &lt;li&gt;Improved prompt understanding&lt;/li&gt; 
 &lt;li&gt;Improved VAE&lt;/li&gt; 
 &lt;li&gt;New online web app in &lt;a href="https://app.ltx.studio/ltx-video"&gt;LTX-Studio&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Automatic prompt enhancement&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;February, 20th, 2025: More inference options&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Improve STG (Spatiotemporal Guidance) for LTX-Video&lt;/li&gt; 
 &lt;li&gt;Support MPS on macOS with PyTorch 2.3.0&lt;/li&gt; 
 &lt;li&gt;Add support for 8-bit model, LTX-VideoQ8&lt;/li&gt; 
 &lt;li&gt;Add TeaCache for LTX-Video&lt;/li&gt; 
 &lt;li&gt;Add &lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#comfyui-integration"&gt;ComfyUI-LTXTricks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Add Diffusion-Pipe&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;December 31st, 2024: Research paper&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release the &lt;a href="https://arxiv.org/abs/2501.00103"&gt;research paper&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;December 20th, 2024: New checkpoint v0.9.1&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release a new checkpoint v0.9.1 with improved quality&lt;/li&gt; 
 &lt;li&gt;Support for STG / PAG&lt;/li&gt; 
 &lt;li&gt;Support loading checkpoints of LTX-Video in Diffusers format (conversion is done on-the-fly)&lt;/li&gt; 
 &lt;li&gt;Support offloading unused parts to CPU&lt;/li&gt; 
 &lt;li&gt;Support the new timestep-conditioned VAE decoder&lt;/li&gt; 
 &lt;li&gt;Reference contributions from the community in the readme file&lt;/li&gt; 
 &lt;li&gt;Relax transformers dependency&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;November 21th, 2024: Initial release v0.9.0&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Initial release of LTX-Video&lt;/li&gt; 
 &lt;li&gt;Support text-to-video and image-to-video generation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Models &amp;amp; Workflows&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Name&lt;/th&gt; 
   &lt;th&gt;Notes&lt;/th&gt; 
   &lt;th&gt;inference.py config&lt;/th&gt; 
   &lt;th&gt;ComfyUI workflow (Recommended)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-13b-0.9.8-dev&lt;/td&gt; 
   &lt;td&gt;Highest quality, requires more VRAM&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-dev.yaml"&gt;ltxv-13b-0.9.8-dev.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ltxv-13b-i2v-base.json"&gt;ltxv-13b-i2v-base.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b"&gt;ltxv-13b-0.9.8-mix&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Mix ltxv-13b-dev and ltxv-13b-distilled in the same multi-scale rendering workflow for balanced speed-quality&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ltxv-13b-i2v-mixed-multiscale.json"&gt;ltxv-13b-i2v-mixed-multiscale.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv"&gt;ltxv-13b-0.9.8-distilled&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Faster, less VRAM usage, slight quality reduction compared to 13b. Ideal for rapid iterations&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-distilled.yaml"&gt;ltxv-13b-0.9.8-distilled.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/13b-distilled/ltxv-13b-dist-i2v-base.json"&gt;ltxv-13b-dist-i2v-base.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-2b-0.9.8-distilled&lt;/td&gt; 
   &lt;td&gt;Smaller model, slight quality reduction compared to 13b distilled. Ideal for fast generation with light VRAM usage&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.8-distilled.yaml"&gt;ltxv-2b-0.9.8-distilled.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-13b-0.9.8-dev-fp8&lt;/td&gt; 
   &lt;td&gt;Quantized version of ltxv-13b&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-dev-fp8.yaml"&gt;ltxv-13b-0.9.8-dev-fp8.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ltxv-13b-i2v-base-fp8.json"&gt;ltxv-13b-i2v-base-fp8.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-13b-0.9.8-distilled-fp8&lt;/td&gt; 
   &lt;td&gt;Quantized version of ltxv-13b-distilled&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-distilled-fp8.yaml"&gt;ltxv-13b-0.9.8-distilled-fp8.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/13b-distilled/ltxv-13b-dist-i2v-base-fp8.json"&gt;ltxv-13b-dist-i2v-base-fp8.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-2b-0.9.8-distilled-fp8&lt;/td&gt; 
   &lt;td&gt;Quantized version of ltxv-2b-distilled&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.8-distilled-fp8.yaml"&gt;ltxv-2b-0.9.8-distilled-fp8.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-2b-0.9.6&lt;/td&gt; 
   &lt;td&gt;Good quality, lower VRAM requirement than ltxv-13b&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.6-dev.yaml"&gt;ltxv-2b-0.9.6-dev.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/low_level/ltxvideo-i2v.json"&gt;ltxvideo-i2v.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-2b-0.9.6-distilled&lt;/td&gt; 
   &lt;td&gt;15√ó faster, real-time capable, fewer steps needed, no STG/CFG required&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.6-distilled.yaml"&gt;ltxv-2b-0.9.6-distilled.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/low_level/ltxvideo-i2v-distilled.json"&gt;ltxvideo-i2v-distilled.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Quick Start Guide&lt;/h1&gt; 
&lt;h2&gt;Online inference&lt;/h2&gt; 
&lt;p&gt;The model is accessible right away via the following links:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b"&gt;LTX-Studio image-to-video (13B-mix)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv"&gt;LTX-Studio image-to-video (13B distilled)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://fal.ai/models/fal-ai/ltx-video-13b-dev/image-to-video"&gt;Fal.ai image-to-video (13B full)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://fal.ai/models/fal-ai/ltx-video-13b-distilled/image-to-video"&gt;Fal.ai image-to-video (13B distilled)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://replicate.com/lightricks/ltx-video"&gt;Replicate image-to-video&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Run locally&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;The codebase was tested with Python 3.10.5, CUDA version 12.2, and supports PyTorch &amp;gt;= 2.1.2. On macOS, MPS was tested with PyTorch 2.3.0, and should support PyTorch == 2.3 or &amp;gt;= 2.6.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Lightricks/LTX-Video.git
cd LTX-Video

# create env
python -m venv env
source env/bin/activate
python -m pip install -e .\[inference\]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;FP8 Kernels (optional)&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://github.com/Lightricks/LTXVideo-Q8-Kernels"&gt;FP8 kernels&lt;/a&gt; developed for LTX-Video provide performance boost on supported graphics cards (Ada architecture and later). To install FP8 kernels, follow the instructions in that repository.&lt;/p&gt; 
&lt;h3&gt;Inference&lt;/h3&gt; 
&lt;p&gt;üìù &lt;strong&gt;Note:&lt;/strong&gt; For best results, we recommend using our &lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#comfyui-integration"&gt;ComfyUI&lt;/a&gt; workflow. We're working on updating the inference.py script to match the high quality and output fidelity of ComfyUI.&lt;/p&gt; 
&lt;p&gt;To use our model, please follow the inference code in &lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/inference.py"&gt;inference.py&lt;/a&gt;:&lt;/p&gt; 
&lt;h4&gt;For image-to-video generation:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python inference.py --prompt "PROMPT" --conditioning_media_paths IMAGE_PATH --conditioning_start_frames 0 --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Extending a video:&lt;/h4&gt; 
&lt;p&gt;üìù &lt;strong&gt;Note:&lt;/strong&gt; Input video segments must contain a multiple of 8 frames plus 1 (e.g., 9, 17, 25, etc.), and the target frame number should be a multiple of 8.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python inference.py --prompt "PROMPT" --conditioning_media_paths VIDEO_PATH --conditioning_start_frames START_FRAME --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;For video generation with multiple conditions:&lt;/h4&gt; 
&lt;p&gt;You can now generate a video conditioned on a set of images and/or short video segments. Simply provide a list of paths to the images or video segments you want to condition on, along with their target frame numbers in the generated video. You can also specify the conditioning strength for each item (default: 1.0).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python inference.py --prompt "PROMPT" --conditioning_media_paths IMAGE_OR_VIDEO_PATH_1 IMAGE_OR_VIDEO_PATH_2 --conditioning_start_frames TARGET_FRAME_1 TARGET_FRAME_2 --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using as a library&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from ltx_video.inference import infer, InferenceConfig

infer(
    InferenceConfig(
        pipeline_config="configs/ltxv-13b-0.9.8-distilled.yaml",
        prompt=PROMPT,
        height=HEIGHT,
        width=WIDTH,
        num_frames=NUM_FRAMES,
        output_path="output.mp4",
    )
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ComfyUI Integration&lt;/h2&gt; 
&lt;p&gt;To use our model with ComfyUI, please follow the instructions at &lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/"&gt;https://github.com/Lightricks/ComfyUI-LTXVideo/&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Diffusers Integration&lt;/h2&gt; 
&lt;p&gt;To use our model with the Diffusers Python library, check out the &lt;a href="https://huggingface.co/docs/diffusers/main/en/api/pipelines/ltx_video"&gt;official documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Diffusers also support an 8-bit version of LTX-Video, &lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#ltx-videoq8"&gt;see details below&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Model User Guide&lt;/h1&gt; 
&lt;h2&gt;üìù Prompt Engineering&lt;/h2&gt; 
&lt;p&gt;When writing prompts, focus on detailed, chronological descriptions of actions and scenes. Include specific movements, appearances, camera angles, and environmental details - all in a single flowing paragraph. Start directly with the action, and keep descriptions literal and precise. Think like a cinematographer describing a shot list. Keep within 200 words. For best results, build your prompts using this structure:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Start with main action in a single sentence&lt;/li&gt; 
 &lt;li&gt;Add specific details about movements and gestures&lt;/li&gt; 
 &lt;li&gt;Describe character/object appearances precisely&lt;/li&gt; 
 &lt;li&gt;Include background and environment details&lt;/li&gt; 
 &lt;li&gt;Specify camera angles and movements&lt;/li&gt; 
 &lt;li&gt;Describe lighting and colors&lt;/li&gt; 
 &lt;li&gt;Note any changes or sudden events&lt;/li&gt; 
 &lt;li&gt;See &lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#introduction"&gt;examples&lt;/a&gt; for more inspiration.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Automatic Prompt Enhancement&lt;/h3&gt; 
&lt;p&gt;When using &lt;code&gt;LTXVideoPipeline&lt;/code&gt; directly, you can enable prompt enhancement by setting &lt;code&gt;enhance_prompt=True&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;üéÆ Parameter Guide&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Resolution Preset: Higher resolutions for detailed scenes, lower for faster generation and simpler scenes. The model works on resolutions that are divisible by 32 and number of frames that are divisible by 8 + 1 (e.g. 257). In case the resolution or number of frames are not divisible by 32 or 8 + 1, the input will be padded with -1 and then cropped to the desired resolution and number of frames. The model works best on resolutions under 720 x 1280 and number of frames below 257&lt;/li&gt; 
 &lt;li&gt;Seed: Save seed values to recreate specific styles or compositions you like&lt;/li&gt; 
 &lt;li&gt;Guidance Scale: 3-3.5 are the recommended values&lt;/li&gt; 
 &lt;li&gt;Inference Steps: More steps (40+) for quality, fewer steps (20-30) for speed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üìù For advanced parameters usage, please see &lt;code&gt;python inference.py --help&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Community Contribution&lt;/h2&gt; 
&lt;h3&gt;ComfyUI-LTXTricks üõ†Ô∏è&lt;/h3&gt; 
&lt;p&gt;A community project providing additional nodes for enhanced control over the LTX Video model. It includes implementations of advanced techniques like RF-Inversion, RF-Edit, FlowEdit, and more. These nodes enable workflows such as Image and Video to Video (I+V2V), enhanced sampling via Spatiotemporal Skip Guidance (STG), and interpolation with precise frame settings.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks"&gt;ComfyUI-LTXTricks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;üîÑ &lt;strong&gt;RF-Inversion:&lt;/strong&gt; Implements &lt;a href="https://rf-inversion.github.io/"&gt;RF-Inversion&lt;/a&gt; with an &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_inversion.json"&gt;example workflow here&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;‚úÇÔ∏è &lt;strong&gt;RF-Edit:&lt;/strong&gt; Implements &lt;a href="https://github.com/wangjiangshan0725/RF-Solver-Edit"&gt;RF-Solver-Edit&lt;/a&gt; with an &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_rf_edit.json"&gt;example workflow here&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;üåä &lt;strong&gt;FlowEdit:&lt;/strong&gt; Implements &lt;a href="https://github.com/fallenshock/FlowEdit"&gt;FlowEdit&lt;/a&gt; with an &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_flow_edit.json"&gt;example workflow here&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;üé• &lt;strong&gt;I+V2V:&lt;/strong&gt; Enables Video to Video with a reference image. &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_iv2v.json"&gt;Example workflow&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;‚ú® &lt;strong&gt;Enhance:&lt;/strong&gt; Partial implementation of &lt;a href="https://junhahyung.github.io/STGuidance/"&gt;STGuidance&lt;/a&gt;. &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltxv_stg.json"&gt;Example workflow&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;üñºÔ∏è &lt;strong&gt;Interpolation and Frame Setting:&lt;/strong&gt; Nodes for precise control of latents per frame. &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_interpolation.json"&gt;Example workflow&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;LTX-VideoQ8 üé± &lt;a id="ltx-videoq8"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LTX-VideoQ8&lt;/strong&gt; is an 8-bit optimized version of &lt;a href="https://github.com/Lightricks/LTX-Video"&gt;LTX-Video&lt;/a&gt;, designed for faster performance on NVIDIA ADA GPUs.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href="https://github.com/KONAKONA666/LTX-Video"&gt;LTX-VideoQ8&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;üöÄ Up to 3X speed-up with no accuracy loss&lt;/li&gt; 
   &lt;li&gt;üé• Generate 720x480x121 videos in under a minute on RTX 4060 (8GB VRAM)&lt;/li&gt; 
   &lt;li&gt;üõ†Ô∏è Fine-tune 2B transformer models with precalculated latents&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Community Discussion:&lt;/strong&gt; &lt;a href="https://www.reddit.com/r/StableDiffusion/comments/1h79ks2/fast_ltx_video_on_rtx_4060_and_other_ada_gpus/"&gt;Reddit Thread&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Diffusers integration:&lt;/strong&gt; A diffusers integration for the 8-bit model is already out! &lt;a href="https://github.com/sayakpaul/q8-ltx-video"&gt;Details here&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;TeaCache for LTX-Video üçµ &lt;a id="TeaCache"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;TeaCache&lt;/strong&gt; is a training-free caching approach that leverages timestep differences across model outputs to accelerate LTX-Video inference by up to 2x without significant visual quality degradation.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href="https://github.com/ali-vilab/TeaCache/tree/main/TeaCache4LTX-Video"&gt;TeaCache4LTX-Video&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;üöÄ Speeds up LTX-Video inference.&lt;/li&gt; 
   &lt;li&gt;üìä Adjustable trade-offs between speed (up to 2x) and visual quality using configurable parameters.&lt;/li&gt; 
   &lt;li&gt;üõ†Ô∏è No retraining required: Works directly with existing models.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Your Contribution&lt;/h3&gt; 
&lt;p&gt;...is welcome! If you have a project or tool that integrates with LTX-Video, please let us know by opening an issue or pull request.&lt;/p&gt; 
&lt;h1&gt;‚ö°Ô∏è Training&lt;/h1&gt; 
&lt;p&gt;We provide an open-source repository for fine-tuning the LTX-Video model: &lt;a href="https://github.com/Lightricks/LTX-Video-Trainer"&gt;LTX-Video-Trainer&lt;/a&gt;. This repository supports both the 2B and 13B model variants, enabling full fine-tuning as well as LoRA (Low-Rank Adaptation) fine-tuning for more efficient training. This includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Control LoRAs&lt;/strong&gt;: Train custom control models like depth, pose, and canny control&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Effect LoRAs&lt;/strong&gt;: Create specialized effects and transformations for video generation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Explore the repository to customize the model for your specific use cases! More information and training instructions can be found in the &lt;a href="https://github.com/Lightricks/LTX-Video-Trainer/raw/main/README.md"&gt;README&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;üé¨ Control Models&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo"&gt;ComfyUI-LTXVideo&lt;/a&gt; repository now contains workflows and models for 3 specialized models that enable precise control over LTX-Video generation:&lt;/p&gt; 
&lt;p&gt;Pose Control, Depth Control and Canny Control&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Example ComfyUI Workflow (for all control types):&lt;/strong&gt; &lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ic_lora/ic-lora.json"&gt;ic-lora.json&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;üöÄ Join Us&lt;/h1&gt; 
&lt;p&gt;Want to work on cutting-edge AI research and make a real impact on millions of users worldwide?&lt;/p&gt; 
&lt;p&gt;At &lt;strong&gt;Lightricks&lt;/strong&gt;, an AI-first company, we're revolutionizing how visual content is created.&lt;/p&gt; 
&lt;p&gt;If you are passionate about AI, computer vision, and video generation, we would love to hear from you!&lt;/p&gt; 
&lt;p&gt;Please visit our &lt;a href="https://careers.lightricks.com/careers?query=&amp;amp;office=all&amp;amp;department=R%26D"&gt;careers page&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h1&gt;Acknowledgement&lt;/h1&gt; 
&lt;p&gt;We are grateful for the following awesome projects when implementing LTX-Video:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/facebookresearch/DiT"&gt;DiT&lt;/a&gt; and &lt;a href="https://github.com/PixArt-alpha/PixArt-alpha"&gt;PixArt-alpha&lt;/a&gt;: vision transformers for image generation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;üìÑ Our tech report is out! If you find our work helpful, please ‚≠êÔ∏è star the repository and cite our paper.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{HaCohen2024LTXVideo,
  title={LTX-Video: Realtime Video Latent Diffusion},
  author={HaCohen, Yoav and Chiprut, Nisan and Brazowski, Benny and Shalem, Daniel and Moshe, Dudu and Richardson, Eitan and Levin, Eran and Shiran, Guy and Zabari, Nir and Gordon, Ori and Panet, Poriya and Weissbuch, Sapir and Kulikov, Victor and Bitterman, Yaki and Melumian, Zeev and Bibi, Ofir},
  journal={arXiv preprint arXiv:2501.00103},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
  </channel>
</rss>