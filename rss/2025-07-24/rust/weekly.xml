<rss version="2.0">
  <channel>
    <title>GitHub Rust Weekly Trending</title>
    <description>Weekly Trending of Rust in GitHub</description>
    <pubDate>Wed, 23 Jul 2025 01:43:43 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>firecracker-microvm/firecracker</title>
      <link>https://github.com/firecracker-microvm/firecracker</link>
      <description>&lt;p&gt;Secure and fast microVMs for serverless computing.&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="docs/images/fc_logo_full_transparent-bg_white-fg.png"&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="docs/images/fc_logo_full_transparent-bg.png"&gt; 
 &lt;img alt="Firecracker Logo Title" width="750" src="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/docs/images/fc_logo_full_transparent-bg.png"&gt; 
&lt;/picture&gt; 
&lt;p&gt;Our mission is to enable secure, multi-tenant, minimal-overhead execution of container and function workloads.&lt;/p&gt; 
&lt;p&gt;Read more about the Firecracker Charter &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/CHARTER.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;What is Firecracker?&lt;/h2&gt; 
&lt;p&gt;Firecracker is an open source virtualization technology that is purpose-built for creating and managing secure, multi-tenant container and function-based services that provide serverless operational models. Firecracker runs workloads in lightweight virtual machines, called microVMs, which combine the security and isolation properties provided by hardware virtualization technology with the speed and flexibility of containers.&lt;/p&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;The main component of Firecracker is a virtual machine monitor (VMM) that uses the Linux Kernel Virtual Machine (KVM) to create and run microVMs. Firecracker has a minimalist design. It excludes unnecessary devices and guest-facing functionality to reduce the memory footprint and attack surface area of each microVM. This improves security, decreases the startup time, and increases hardware utilization. Firecracker has also been integrated in container runtimes, for example &lt;a href="https://github.com/kata-containers/kata-containers"&gt;Kata Containers&lt;/a&gt; and &lt;a href="https://github.com/liquidmetal-dev/flintlock"&gt;Flintlock&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Firecracker was developed at Amazon Web Services to accelerate the speed and efficiency of services like &lt;a href="https://aws.amazon.com/lambda/"&gt;AWS Lambda&lt;/a&gt; and &lt;a href="https://aws.amazon.com/fargate/"&gt;AWS Fargate&lt;/a&gt;. Firecracker is open sourced under &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/LICENSE"&gt;Apache version 2.0&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To read more about Firecracker, check out &lt;a href="https://firecracker-microvm.github.io"&gt;firecracker-microvm.io&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;To get started with Firecracker, download the latest &lt;a href="https://github.com/firecracker-microvm/firecracker/releases"&gt;release&lt;/a&gt; binaries or build it from source.&lt;/p&gt; 
&lt;p&gt;You can build Firecracker on any Unix/Linux system that has Docker running (we use a development container) and &lt;code&gt;bash&lt;/code&gt; installed, as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/firecracker-microvm/firecracker
cd firecracker
tools/devtool build
toolchain="$(uname -m)-unknown-linux-musl"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The Firecracker binary will be placed at &lt;code&gt;build/cargo_target/${toolchain}/debug/firecracker&lt;/code&gt;. For more information on building, testing, and running Firecracker, go to the &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/docs/getting-started.md"&gt;quickstart guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The overall security of Firecracker microVMs, including the ability to meet the criteria for safe multi-tenant computing, depends on a well configured Linux host operating system. A configuration that we believe meets this bar is included in &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/docs/prod-host-setup.md"&gt;the production host setup document&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Firecracker is already running production workloads within AWS, but it's still Day 1 on the journey guided by our &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/CHARTER.md"&gt;mission&lt;/a&gt;. There's a lot more to build and we welcome all contributions.&lt;/p&gt; 
&lt;p&gt;To contribute to Firecracker, check out the development setup section in the &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/docs/getting-started.md"&gt;getting started guide&lt;/a&gt; and then the Firecracker &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/CONTRIBUTING.md"&gt;contribution guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Releases&lt;/h2&gt; 
&lt;p&gt;New Firecracker versions are released via the GitHub repository &lt;a href="https://github.com/firecracker-microvm/firecracker/releases"&gt;releases&lt;/a&gt; page, typically every two or three months. A history of changes is recorded in our &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/CHANGELOG.md"&gt;changelog&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The Firecracker release policy is detailed &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/docs/RELEASE_POLICY.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Design&lt;/h2&gt; 
&lt;p&gt;Firecracker's overall architecture is described in &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/docs/design.md"&gt;the design document&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Features &amp;amp; Capabilities&lt;/h2&gt; 
&lt;p&gt;Firecracker consists of a single micro Virtual Machine Manager process that exposes an API endpoint to the host once started. The API is &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/src/firecracker/swagger/firecracker.yaml"&gt;specified in OpenAPI format&lt;/a&gt;. Read more about it in the &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/docs/api_requests"&gt;API docs&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The &lt;strong&gt;API endpoint&lt;/strong&gt; can be used to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Configure the microvm by: 
  &lt;ul&gt; 
   &lt;li&gt;Setting the number of vCPUs (the default is 1).&lt;/li&gt; 
   &lt;li&gt;Setting the memory size (the default is 128 MiB).&lt;/li&gt; 
   &lt;li&gt;Configuring a &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/docs/cpu_templates/cpu-templates.md"&gt;CPU template&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Add one or more network interfaces to the microVM.&lt;/li&gt; 
 &lt;li&gt;Add one or more read-write or read-only disks to the microVM, each represented by a file-backed block device.&lt;/li&gt; 
 &lt;li&gt;Trigger a block device re-scan while the guest is running. This enables the guest OS to pick up size changes to the block device's backing file.&lt;/li&gt; 
 &lt;li&gt;Change the backing file for a block device, before or after the guest boots.&lt;/li&gt; 
 &lt;li&gt;Configure rate limiters for virtio devices which can limit the bandwidth, operations per second, or both.&lt;/li&gt; 
 &lt;li&gt;Configure the logging and metric system.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[BETA]&lt;/code&gt; Configure the data tree of the guest-facing metadata service. The service is only available to the guest if this resource is configured.&lt;/li&gt; 
 &lt;li&gt;Add a &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/docs/vsock.md"&gt;vsock socket&lt;/a&gt; to the microVM.&lt;/li&gt; 
 &lt;li&gt;Add a &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/docs/entropy.md"&gt;entropy device&lt;/a&gt; to the microVM.&lt;/li&gt; 
 &lt;li&gt;Start the microVM using a given kernel image, root file system, and boot arguments.&lt;/li&gt; 
 &lt;li&gt;[x86_64 only] Stop the microVM.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Built-in Capabilities&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Demand fault paging and CPU oversubscription enabled by default.&lt;/li&gt; 
 &lt;li&gt;Advanced, thread-specific seccomp filters for enhanced security.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/docs/jailer.md"&gt;Jailer&lt;/a&gt; process for starting Firecracker in production scenarios; applies a cgroup/namespace isolation barrier and then drops privileges.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Tested platforms&lt;/h2&gt; 
&lt;p&gt;We test all combinations of:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Instance&lt;/th&gt; 
   &lt;th align="left"&gt;Host OS &amp;amp; Kernel&lt;/th&gt; 
   &lt;th align="left"&gt;Guest Rootfs&lt;/th&gt; 
   &lt;th align="left"&gt;Guest Kernel&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;c5n.metal&lt;/td&gt; 
   &lt;td align="left"&gt;al2 linux_5.10&lt;/td&gt; 
   &lt;td align="left"&gt;ubuntu 24.04&lt;/td&gt; 
   &lt;td align="left"&gt;linux_5.10&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;m5n.metal&lt;/td&gt; 
   &lt;td align="left"&gt;al2023 linux_6.1&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;linux_6.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;m6i.metal&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;m7i.metal-24xl&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;m7i.metal-48xl&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;m6a.metal&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;m7a.metal-48xl&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;m6g.metal&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;m7g.metal&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;m8g.metal-24xl&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;m8g.metal-48xl&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Known issues and Limitations&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;The &lt;code&gt;pl031&lt;/code&gt; RTC device on aarch64 does not support interrupts, so guest programs which use an RTC alarm (e.g. &lt;code&gt;hwclock&lt;/code&gt;) will not work.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;p&gt;Firecracker's performance characteristics are listed as part of the &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/SPECIFICATION.md"&gt;specification documentation&lt;/a&gt;. All specifications are a part of our commitment to supporting container and function workloads in serverless operational models, and are therefore enforced via continuous integration testing.&lt;/p&gt; 
&lt;h2&gt;Policy for Security Disclosures&lt;/h2&gt; 
&lt;p&gt;The security of Firecracker is our top priority. If you suspect you have uncovered a vulnerability, contact us privately, as outlined in our &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/SECURITY.md"&gt;security policy document&lt;/a&gt;; we will immediately prioritize your disclosure.&lt;/p&gt; 
&lt;h2&gt;FAQ &amp;amp; Contact&lt;/h2&gt; 
&lt;p&gt;Frequently asked questions are collected in our &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/FAQ.md"&gt;FAQ doc&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can get in touch with the Firecracker community in the following ways:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Security-related issues, see our &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/SECURITY.md"&gt;security policy document&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Chat with us on our &lt;a href="https://join.slack.com/t/firecracker-microvm/shared_invite/zt-2tc0mfxpc-tU~HYAYSzLDl5XGGJU3YIg"&gt;Slack workspace&lt;/a&gt; &lt;em&gt;Note: most of the maintainers are on a European time zone.&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Open a GitHub issue in this repository.&lt;/li&gt; 
 &lt;li&gt;Email the maintainers at &lt;a href="mailto:firecracker-maintainers@amazon.com"&gt;firecracker-maintainers@amazon.com&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;When communicating within the Firecracker community, please mind our &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/CODE_OF_CONDUCT.md"&gt;code of conduct&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>helix-editor/helix</title>
      <link>https://github.com/helix-editor/helix</link>
      <description>&lt;p&gt;A post-modern modal text editor.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="logo_dark.svg"&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="logo_light.svg"&gt; 
   &lt;img alt="Helix" height="128" src="https://raw.githubusercontent.com/helix-editor/helix/master/logo_light.svg?sanitize=true"&gt; 
  &lt;/picture&gt; &lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://github.com/helix-editor/helix/actions"&gt;&lt;img src="https://github.com/helix-editor/helix/actions/workflows/build.yml/badge.svg?sanitize=true" alt="Build status"&gt;&lt;/a&gt; &lt;a href="https://github.com/helix-editor/helix/releases/latest"&gt;&lt;img src="https://img.shields.io/github/v/release/helix-editor/helix" alt="GitHub Release"&gt;&lt;/a&gt; &lt;a href="https://docs.helix-editor.com/"&gt;&lt;img src="https://shields.io/badge/-documentation-452859" alt="Documentation"&gt;&lt;/a&gt; &lt;a href="https://github.com/helix-editor/helix/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/helix-editor/helix" alt="GitHub contributors"&gt;&lt;/a&gt; &lt;a href="https://matrix.to/#/%23helix-community:matrix.org"&gt;&lt;img src="https://img.shields.io/matrix/helix-community:matrix.org" alt="Matrix Space"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/helix-editor/helix/master/screenshot.png" alt="Screenshot"&gt;&lt;/p&gt; 
&lt;p&gt;A &lt;a href="https://github.com/mawww/kakoune"&gt;Kakoune&lt;/a&gt; / &lt;a href="https://github.com/neovim/neovim"&gt;Neovim&lt;/a&gt; inspired editor, written in Rust.&lt;/p&gt; 
&lt;p&gt;The editing model is very heavily based on Kakoune; during development I found myself agreeing with most of Kakoune's design decisions.&lt;/p&gt; 
&lt;p&gt;For more information, see the &lt;a href="https://helix-editor.com"&gt;website&lt;/a&gt; or &lt;a href="https://docs.helix-editor.com/"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;All shortcuts/keymaps can be found &lt;a href="https://docs.helix-editor.com/keymap.html"&gt;in the documentation on the website&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/helix-editor/helix/wiki/Troubleshooting"&gt;Troubleshooting&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Features&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;Vim-like modal editing&lt;/li&gt; 
 &lt;li&gt;Multiple selections&lt;/li&gt; 
 &lt;li&gt;Built-in language server support&lt;/li&gt; 
 &lt;li&gt;Smart, incremental syntax highlighting and code editing via tree-sitter&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Although it's primarily a terminal-based editor, I am interested in exploring a custom renderer (similar to Emacs) using wgpu or skulpin.&lt;/p&gt; 
&lt;p&gt;Note: Only certain languages have indentation definitions at the moment. Check &lt;code&gt;runtime/queries/&amp;lt;lang&amp;gt;/&lt;/code&gt; for &lt;code&gt;indents.scm&lt;/code&gt;.&lt;/p&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://docs.helix-editor.com/install.html"&gt;Installation documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://repology.org/project/helix-editor/versions"&gt;&lt;img src="https://repology.org/badge/vertical-allrepos/helix-editor.svg?exclude_unsupported=1" alt="Packaging status"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Contributing&lt;/h1&gt; 
&lt;p&gt;Contributing guidelines can be found &lt;a href="https://raw.githubusercontent.com/helix-editor/helix/master/docs/CONTRIBUTING.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Getting help&lt;/h1&gt; 
&lt;p&gt;Your question might already be answered on the &lt;a href="https://github.com/helix-editor/helix/wiki/FAQ"&gt;FAQ&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Discuss the project on the community &lt;a href="https://matrix.to/#/%23helix-community:matrix.org"&gt;Matrix Space&lt;/a&gt; (make sure to join &lt;code&gt;#helix-editor:matrix.org&lt;/code&gt; if you're on a client that doesn't support Matrix Spaces yet).&lt;/p&gt; 
&lt;h1&gt;Credits&lt;/h1&gt; 
&lt;p&gt;Thanks to &lt;a href="https://github.com/jakenvac"&gt;@jakenvac&lt;/a&gt; for designing the logo!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>rustdesk/rustdesk</title>
      <link>https://github.com/rustdesk/rustdesk</link>
      <description>&lt;p&gt;An open-source remote desktop application designed for self-hosting, as an alternative to TeamViewer.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/rustdesk/rustdesk/master/res/logo-header.svg?sanitize=true" alt="RustDesk - Your remote desktop"&gt;&lt;br&gt; &lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/#raw-steps-to-build"&gt;Build&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/#how-to-build-with-docker"&gt;Docker&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/#file-structure"&gt;Structure&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/#snapshot"&gt;Snapshot&lt;/a&gt;&lt;br&gt; [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-UA.md"&gt;–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-CS.md"&gt;ƒçesky&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-ZH.md"&gt;‰∏≠Êñá&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-HU.md"&gt;Magyar&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-ES.md"&gt;Espa√±ol&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-FA.md"&gt;ŸÅÿßÿ±ÿ≥€å&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-FR.md"&gt;Fran√ßais&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-DE.md"&gt;Deutsch&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-PL.md"&gt;Polski&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-ID.md"&gt;Indonesian&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-FI.md"&gt;Suomi&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-ML.md"&gt;‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-JP.md"&gt;Êó•Êú¨Ë™û&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-NL.md"&gt;Nederlands&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-IT.md"&gt;Italiano&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-RU.md"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-PTBR.md"&gt;Portugu√™s (Brasil)&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-EO.md"&gt;Esperanto&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-KR.md"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-AR.md"&gt;ÿßŸÑÿπÿ±ÿ®Ÿä&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-VN.md"&gt;Ti·∫øng Vi·ªát&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-DA.md"&gt;Dansk&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-GR.md"&gt;ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-TR.md"&gt;T√ºrk√ße&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-NO.md"&gt;Norsk&lt;/a&gt;]&lt;br&gt; &lt;b&gt;We need your help to translate this README, &lt;a href="https://github.com/rustdesk/rustdesk/tree/master/src/lang"&gt;RustDesk UI&lt;/a&gt; and &lt;a href="https://github.com/rustdesk/doc.rustdesk.com"&gt;RustDesk Doc&lt;/a&gt; to your native language&lt;/b&gt; &lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Caution] &lt;strong&gt;Misuse Disclaimer:&lt;/strong&gt; &lt;br&gt; The developers of RustDesk do not condone or support any unethical or illegal use of this software. Misuse, such as unauthorized access, control or invasion of privacy, is strictly against our guidelines. The authors are not responsible for any misuse of the application.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Chat with us: &lt;a href="https://discord.gg/nDceKgxnkV"&gt;Discord&lt;/a&gt; | &lt;a href="https://twitter.com/rustdesk"&gt;Twitter&lt;/a&gt; | &lt;a href="https://www.reddit.com/r/rustdesk"&gt;Reddit&lt;/a&gt; | &lt;a href="https://www.youtube.com/@rustdesk"&gt;YouTube&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://ko-fi.com/I2I04VU09"&gt;&lt;img src="https://ko-fi.com/img/githubbutton_sm.svg?sanitize=true" alt="ko-fi"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Yet another remote desktop solution, written in Rust. Works out of the box with no configuration required. You have full control of your data, with no concerns about security. You can use our rendezvous/relay server, &lt;a href="https://rustdesk.com/server"&gt;set up your own&lt;/a&gt;, or &lt;a href="https://github.com/rustdesk/rustdesk-server-demo"&gt;write your own rendezvous/relay server&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/71636191/171661982-430285f0-2e12-4b1d-9957-4a58e375304d.png" alt="image"&gt;&lt;/p&gt; 
&lt;p&gt;RustDesk welcomes contribution from everyone. See &lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for help getting started.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/rustdesk/rustdesk/wiki/FAQ"&gt;&lt;strong&gt;FAQ&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/rustdesk/rustdesk/releases"&gt;&lt;strong&gt;BINARY DOWNLOAD&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/rustdesk/rustdesk/releases/tag/nightly"&gt;&lt;strong&gt;NIGHTLY BUILD&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://f-droid.org/en/packages/com.carriez.flutter_hbb"&gt;&lt;img src="https://f-droid.org/badge/get-it-on.png" alt="Get it on F-Droid" height="80"&gt;&lt;/a&gt; &lt;a href="https://flathub.org/apps/com.rustdesk.RustDesk"&gt;&lt;img src="https://flathub.org/api/badge?svg&amp;amp;locale=en" alt="Get it on Flathub" height="80"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Dependencies&lt;/h2&gt; 
&lt;p&gt;Desktop versions use Flutter or Sciter (deprecated) for GUI, this tutorial is for Sciter only, since it is easier and more friendly to start. Check out our &lt;a href="https://github.com/rustdesk/rustdesk/raw/master/.github/workflows/flutter-build.yml"&gt;CI&lt;/a&gt; for building Flutter version.&lt;/p&gt; 
&lt;p&gt;Please download Sciter dynamic library yourself.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.win/x64/sciter.dll"&gt;Windows&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.lnx/x64/libsciter-gtk.so"&gt;Linux&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.osx/libsciter.dylib"&gt;macOS&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Raw Steps to build&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Prepare your Rust development env and C++ build env&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;a href="https://github.com/microsoft/vcpkg"&gt;vcpkg&lt;/a&gt;, and set &lt;code&gt;VCPKG_ROOT&lt;/code&gt; env variable correctly&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Windows: vcpkg install libvpx:x64-windows-static libyuv:x64-windows-static opus:x64-windows-static aom:x64-windows-static&lt;/li&gt; 
   &lt;li&gt;Linux/macOS: vcpkg install libvpx libyuv opus aom&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;run &lt;code&gt;cargo run&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href="https://rustdesk.com/docs/en/dev/build/"&gt;Build&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;How to Build on Linux&lt;/h2&gt; 
&lt;h3&gt;Ubuntu 18 (Debian 10)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;sudo apt install -y zip g++ gcc git curl wget nasm yasm libgtk-3-dev clang libxcb-randr0-dev libxdo-dev \
        libxfixes-dev libxcb-shape0-dev libxcb-xfixes0-dev libasound2-dev libpulse-dev cmake make \
        libclang-dev ninja-build libgstreamer1.0-dev libgstreamer-plugins-base1.0-dev libpam0g-dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;openSUSE Tumbleweed&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;sudo zypper install gcc-c++ git curl wget nasm yasm gcc gtk3-devel clang libxcb-devel libXfixes-devel cmake alsa-lib-devel gstreamer-devel gstreamer-plugins-base-devel xdotool-devel pam-devel
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Fedora 28 (CentOS 8)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;sudo yum -y install gcc-c++ git curl wget nasm yasm gcc gtk3-devel clang libxcb-devel libxdo-devel libXfixes-devel pulseaudio-libs-devel cmake alsa-lib-devel gstreamer1-devel gstreamer1-plugins-base-devel pam-devel
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Arch (Manjaro)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;sudo pacman -Syu --needed unzip git cmake gcc curl wget yasm nasm zip make pkg-config clang gtk3 xdotool libxcb libxfixes alsa-lib pipewire
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install vcpkg&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/microsoft/vcpkg
cd vcpkg
git checkout 2023.04.15
cd ..
vcpkg/bootstrap-vcpkg.sh
export VCPKG_ROOT=$HOME/vcpkg
vcpkg/vcpkg install libvpx libyuv opus aom
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Fix libvpx (For Fedora)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cd vcpkg/buildtrees/libvpx/src
cd *
./configure
sed -i 's/CFLAGS+=-I/CFLAGS+=-fPIC -I/g' Makefile
sed -i 's/CXXFLAGS+=-I/CXXFLAGS+=-fPIC -I/g' Makefile
make
cp libvpx.a $HOME/vcpkg/installed/x64-linux/lib/
cd
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Build&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source $HOME/.cargo/env
git clone --recurse-submodules https://github.com/rustdesk/rustdesk
cd rustdesk
mkdir -p target/debug
wget https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.lnx/x64/libsciter-gtk.so
mv libsciter-gtk.so target/debug
VCPKG_ROOT=$HOME/vcpkg cargo run
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;How to build with Docker&lt;/h2&gt; 
&lt;p&gt;Begin by cloning the repository and building the Docker container:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/rustdesk/rustdesk
cd rustdesk
git submodule update --init --recursive
docker build -t "rustdesk-builder" .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, each time you need to build the application, run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;docker run --rm -it -v $PWD:/home/user/rustdesk -v rustdesk-git-cache:/home/user/.cargo/git -v rustdesk-registry-cache:/home/user/.cargo/registry -e PUID="$(id -u)" -e PGID="$(id -g)" rustdesk-builder
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that the first build may take longer before dependencies are cached, subsequent builds will be faster. Additionally, if you need to specify different arguments to the build command, you may do so at the end of the command in the &lt;code&gt;&amp;lt;OPTIONAL-ARGS&amp;gt;&lt;/code&gt; position. For instance, if you wanted to build an optimized release version, you would run the command above followed by &lt;code&gt;--release&lt;/code&gt;. The resulting executable will be available in the target folder on your system, and can be run with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;target/debug/rustdesk
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, if you're running a release executable:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;target/release/rustdesk
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please ensure that you run these commands from the root of the RustDesk repository, or the application may not find the required resources. Also note that other cargo subcommands such as &lt;code&gt;install&lt;/code&gt; or &lt;code&gt;run&lt;/code&gt; are not currently supported via this method as they would install or run the program inside the container instead of the host.&lt;/p&gt; 
&lt;h2&gt;File Structure&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/libs/hbb_common"&gt;libs/hbb_common&lt;/a&gt;&lt;/strong&gt;: video codec, config, tcp/udp wrapper, protobuf, fs functions for file transfer, and some other utility functions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/libs/scrap"&gt;libs/scrap&lt;/a&gt;&lt;/strong&gt;: screen capture&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/libs/enigo"&gt;libs/enigo&lt;/a&gt;&lt;/strong&gt;: platform specific keyboard/mouse control&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/libs/clipboard"&gt;libs/clipboard&lt;/a&gt;&lt;/strong&gt;: file copy and paste implementation for Windows, Linux, macOS.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/src/ui"&gt;src/ui&lt;/a&gt;&lt;/strong&gt;: obsolete Sciter UI (deprecated)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/src/server"&gt;src/server&lt;/a&gt;&lt;/strong&gt;: audio/clipboard/input/video services, and network connections&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/src/client.rs"&gt;src/client.rs&lt;/a&gt;&lt;/strong&gt;: start a peer connection&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/src/rendezvous_mediator.rs"&gt;src/rendezvous_mediator.rs&lt;/a&gt;&lt;/strong&gt;: Communicate with &lt;a href="https://github.com/rustdesk/rustdesk-server"&gt;rustdesk-server&lt;/a&gt;, wait for remote direct (TCP hole punching) or relayed connection&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/src/platform"&gt;src/platform&lt;/a&gt;&lt;/strong&gt;: platform specific code&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/flutter"&gt;flutter&lt;/a&gt;&lt;/strong&gt;: Flutter code for desktop and mobile&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/flutter/web/v1/js"&gt;flutter/web/js&lt;/a&gt;&lt;/strong&gt;: JavaScript for Flutter web client&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Screenshots&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://github.com/rustdesk/rustdesk/assets/28412477/db82d4e7-c4bc-4823-8e6f-6af7eadf7651" alt="Connection Manager"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/rustdesk/rustdesk/assets/28412477/9baa91e9-3362-4d06-aa1a-7518edcbd7ea" alt="Connected to a Windows PC"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/rustdesk/rustdesk/assets/28412477/39511ad3-aa9a-4f8c-8947-1cce286a46ad" alt="File Transfer"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/rustdesk/rustdesk/assets/28412477/78e8708f-e87e-4570-8373-1360033ea6c5" alt="TCP Tunneling"&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>risingwavelabs/risingwave</title>
      <link>https://github.com/risingwavelabs/risingwave</link>
      <description>&lt;p&gt;Stream processing and management platform.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source srcset=".github/RisingWave-logo-dark.svg" width="500px" media="(prefers-color-scheme: dark)"&gt; 
  &lt;img src="https://raw.githubusercontent.com/risingwavelabs/risingwave/main/.github/RisingWave-logo-light.svg?sanitize=true" width="500px"&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;üåä Ride the Wave of Streaming Data.&lt;/h3&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; &lt;a href="https://docs.risingwave.com/"&gt;Docs&lt;/a&gt; | &lt;a href="https://docs.risingwave.com/get-started/rw-benchmarks-stream-processing"&gt;Benchmarks&lt;/a&gt; | &lt;a href="https://docs.risingwave.com/demos/overview"&gt;Demos&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/risingwavelabs/risingwave/releases/latest" target="_blank"&gt; &lt;img alt="Release" src="https://img.shields.io/github/v/release/risingwavelabs/risingwave.svg?sort=semver"&gt; &lt;/a&gt; 
 &lt;a href="https://go.risingwave.com/slack" target="_blank"&gt; &lt;img alt="Slack" src="https://badgen.net/badge/Slack/Join%20RisingWave/0abd59?icon=slack"&gt; &lt;/a&gt; 
 &lt;a href="https://x.com/risingwavelabs" target="_blank"&gt; &lt;img alt="X" src="https://img.shields.io/twitter/follow/risingwavelabs"&gt; &lt;/a&gt; 
 &lt;a href="https://www.youtube.com/@risingwave-labs" target="_blank"&gt; &lt;img alt="YouTube" src="https://img.shields.io/youtube/channel/views/UCsHwdyBRxBpmkA5RRd0YNEA"&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;p&gt;RisingWave is a stream processing and management platform designed to offer the &lt;i&gt;&lt;b&gt;simplest&lt;/b&gt;&lt;/i&gt; and &lt;i&gt;&lt;b&gt;most cost-effective&lt;/b&gt;&lt;/i&gt; way to &lt;b&gt;process&lt;/b&gt;, &lt;b&gt;analyze&lt;/b&gt;, and &lt;b&gt;manage&lt;/b&gt; real-time event data ‚Äî with built-in support for the &lt;a href="https://iceberg.apache.org/"&gt;Apache Iceberg‚Ñ¢&lt;/a&gt; open table format. It provides both a Postgres-compatible &lt;a href="https://docs.risingwave.com/sql/overview"&gt;SQL interface&lt;/a&gt; and a DataFrame-style &lt;a href="https://docs.risingwave.com/python-sdk/intro"&gt;Python interface&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;RisingWave can &lt;b&gt;ingest&lt;/b&gt; millions of events per second, continuously &lt;b&gt;join and analyze&lt;/b&gt; live streams with historical data, &lt;b&gt;serve&lt;/b&gt; ad-hoc queries at low latency, and &lt;b&gt;persist&lt;/b&gt; fresh, consistent results to Apache Iceberg‚Ñ¢ or any other downstream system.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/risingwavelabs/risingwave/main/docs/dev/src/images/architecture_20250609.jpg" alt="RisingWave"&gt;&lt;/p&gt; 
&lt;h2&gt;Try it out in 60 seconds&lt;/h2&gt; 
&lt;p&gt;Install RisingWave standalone mode:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;curl -L https://risingwave.com/sh | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To learn about other installation options, such as using a Docker image, see &lt;a href="https://docs.risingwave.com/docs/current/get-started/"&gt;Quick Start&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Stream, Store, and Query ‚Äî All in One&lt;/h2&gt; 
&lt;p&gt;RisingWave delivers a full &lt;strong&gt;end-to-end streaming data platform&lt;/strong&gt; ‚Äî combining real-time processing with built-in storage and open-format persistence.&lt;/p&gt; 
&lt;p&gt;It supports:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Ingestion:&lt;/strong&gt; Ingest millions of events per second from streaming and batch sources.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Stream processing:&lt;/strong&gt; Perform real-time incremental processing to join and analyze live data with historical tables.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Delivery:&lt;/strong&gt; Deliver fresh, consistent results to data lakes (e.g., Apache Iceberg‚Ñ¢) or any destination.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;What sets RisingWave apart is its integrated storage engine:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Online serving:&lt;/strong&gt; Row-based storage optimized for point and range queries with single-digit millisecond latency.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Offline persistence:&lt;/strong&gt; Built-in Apache Iceberg‚Ñ¢ integration for low-cost, durable storage with open access for external query engines.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;With RisingWave, real-time data isn‚Äôt just processed ‚Äî it‚Äôs stored, queried, and shared across your entire stack.&lt;/p&gt; 
&lt;h2&gt;Key design decisions&lt;/h2&gt; 
&lt;p&gt;RisingWave is designed to be easier to use and more cost-efficient:&lt;/p&gt; 
&lt;h3&gt;PostgreSQL compatibility&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Seamless integration:&lt;/strong&gt; Connects via the PostgreSQL wire protocol, working with psql, JDBC, and any Postgres tool.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Expressive SQL:&lt;/strong&gt; Supports structured, semi-structured, and unstructured data with a familiar SQL dialect.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No manual state tuning:&lt;/strong&gt; Eliminates complex state management configurations.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;S3 as primary storage&lt;/h3&gt; 
&lt;p&gt;RisingWave stores tables, materialized views, and internal states of stream processing jobs in S3 (or equivalent object storage), providing:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;High performance:&lt;/strong&gt; Optimized for complex queries, including joins and time windowing.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Fast recovery:&lt;/strong&gt; Restores from system failures within seconds.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.risingwave.com/deploy/k8s-cluster-scaling"&gt;Dynamic scaling&lt;/a&gt;:&lt;/strong&gt; Instantly adjusts resources to handle workload spikes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Elastic disk cache&lt;/h3&gt; 
&lt;p&gt;Beyond caching hot data in memory, RisingWave supports &lt;a href="https://docs.risingwave.com/get-started/disk-cache"&gt;&lt;strong&gt;elastic disk cache&lt;/strong&gt;&lt;/a&gt;, a powerful performance optimization that uses local disks or EBS for efficient data caching. This minimizes access to S3, lowering processing latency and cutting S3 access costs.&lt;/p&gt; 
&lt;h3&gt;Apache Iceberg‚Ñ¢ native support&lt;/h3&gt; 
&lt;p&gt;RisingWave &lt;a href="https://docs.risingwave.com/iceberg/overview"&gt;&lt;strong&gt;natively integrates with Apache Iceberg‚Ñ¢&lt;/strong&gt;&lt;/a&gt;, enabling continuous ingestion of streaming data into Iceberg tables. It can also read directly from Iceberg, perform automatic compaction, and maintain table health over time. Since Iceberg is an open table format, results are accessible by other query engines ‚Äî making storage not only cost-efficient, but interoperable by design.&lt;/p&gt; 
&lt;h2&gt;In what use cases does RisingWave excel?&lt;/h2&gt; 
&lt;p&gt;RisingWave is particularly effective for the following use cases:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Streaming analytics&lt;/strong&gt;: Achieve sub-second data freshness in live dashboards, ideal for high-stakes scenarios like stock trading, sports betting, and IoT monitoring.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Event-driven applications&lt;/strong&gt;: Develop sophisticated monitoring and alerting systems for critical applications such as fraud and anomaly detection.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time data enrichment&lt;/strong&gt;: Continuously ingest data from diverse sources, conduct real-time data enrichment, and efficiently deliver the results to downstream systems.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Feature engineering&lt;/strong&gt;: Transform batch and streaming data into features in your machine learning models using a unified codebase, ensuring seamless integration and consistency.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Production deployments&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://cloud.risingwave.com"&gt;&lt;strong&gt;RisingWave Cloud&lt;/strong&gt;&lt;/a&gt; offers the easiest way to run RisingWave in production.&lt;/p&gt; 
&lt;p&gt;For &lt;strong&gt;Docker deployment&lt;/strong&gt;, please refer to &lt;a href="https://docs.risingwave.com/docs/current/risingwave-docker-compose/"&gt;Docker Compose&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For &lt;strong&gt;Kubernetes deployment&lt;/strong&gt;, please refer to &lt;a href="https://docs.risingwave.com/docs/current/risingwave-k8s-helm/"&gt;Kubernetes with Helm&lt;/a&gt; or &lt;a href="https://docs.risingwave.com/docs/current/risingwave-kubernetes/"&gt;Kubernetes with Operator&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Looking for help, discussions, collaboration opportunities, or a casual afternoon chat with our fellow engineers and community members? Join our &lt;a href="https://risingwave.com/slack"&gt;Slack workspace&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;Notes on telemetry&lt;/h2&gt; 
&lt;p&gt;RisingWave uses &lt;a href="https://scarf.sh/"&gt;Scarf&lt;/a&gt; to collect anonymized installation analytics. These analytics help support us understand and improve the distribution of our package. The privacy policy of Scarf is available at &lt;a href="https://about.scarf.sh/privacy-policy"&gt;https://about.scarf.sh/privacy-policy&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;RisingWave also collects anonymous usage statistics to better understand how the community is using RisingWave. The sole intention of this exercise is to help improve the product. Users may opt out easily at any time. Please refer to the &lt;a href="https://docs.risingwave.com/docs/current/telemetry/"&gt;user documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;RisingWave is distributed under the Apache License (Version 2.0). Please refer to &lt;a href="https://raw.githubusercontent.com/risingwavelabs/risingwave/main/LICENSE"&gt;LICENSE&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Thanks for your interest in contributing to the project! Please refer to &lt;a href="https://risingwavelabs.github.io/risingwave/"&gt;RisingWave Developer Guide&lt;/a&gt; for more information.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>tree-sitter/tree-sitter</title>
      <link>https://github.com/tree-sitter/tree-sitter</link>
      <description>&lt;p&gt;An incremental parsing system for programming tools&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;tree-sitter&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://zenodo.org/badge/latestdoi/14164618"&gt;&lt;img src="https://zenodo.org/badge/14164618.svg?sanitize=true" alt="DOI"&gt;&lt;/a&gt; &lt;a href="https://discord.gg/w7nTvsVJhm"&gt;&lt;img src="https://img.shields.io/discord/1063097320771698699?logo=discord&amp;amp;label=discord" alt="discord"&gt;&lt;/a&gt; &lt;a href="https://matrix.to/#/%23tree-sitter-chat:matrix.org"&gt;&lt;img src="https://img.shields.io/matrix/tree-sitter-chat%3Amatrix.org?logo=matrix&amp;amp;label=matrix" alt="matrix"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Tree-sitter is a parser generator tool and an incremental parsing library. It can build a concrete syntax tree for a source file and efficiently update the syntax tree as the source file is edited. Tree-sitter aims to be:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt; enough to parse any programming language&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Fast&lt;/strong&gt; enough to parse on every keystroke in a text editor&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Robust&lt;/strong&gt; enough to provide useful results even in the presence of syntax errors&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dependency-free&lt;/strong&gt; so that the runtime library (which is written in pure C) can be embedded in any application&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://tree-sitter.github.io"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tree-sitter/tree-sitter/master/lib/binding_rust/README.md"&gt;Rust binding&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tree-sitter/tree-sitter/master/lib/binding_web/README.md"&gt;WASM binding&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tree-sitter/tree-sitter/master/crates/cli/README.md"&gt;Command-line interface&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>get-convex/convex-backend</title>
      <link>https://github.com/get-convex/convex-backend</link>
      <description>&lt;p&gt;The open-source reactive database for app developers&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://static.convex.dev/logo/convex-logo-light.svg" width="600"&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://static.convex.dev/logo/convex-logo.svg" width="600"&gt; 
  &lt;img alt="Convex logo" src="https://static.convex.dev/logo/convex-logo.svg?sanitize=true" width="600"&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://convex.dev"&gt;Convex&lt;/a&gt; is the open-source reactive database designed to make life easy for web app developers, whether human or LLM. Fetch data and perform business logic with strong consistency by writing pure TypeScript.&lt;/p&gt; 
&lt;p&gt;Convex provides a database, a place to write your server functions, and client libraries. It makes it easy to build and scale dynamic live-updating apps. &lt;a href="https://docs.convex.dev/understanding/"&gt;Read the docs to learn more&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Development of the Convex backend is led by the Convex team. We &lt;a href="https://raw.githubusercontent.com/get-convex/convex-backend/main/CONTRIBUTING.md"&gt;welcome bug fixes&lt;/a&gt; and &lt;a href="https://discord.gg/convex"&gt;love receiving feedback&lt;/a&gt;. We keep this repository synced with any internal development work within a handful of days.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Visit our &lt;a href="https://docs.convex.dev/"&gt;documentation&lt;/a&gt; to learn more about Convex and follow our getting started guides.&lt;/p&gt; 
&lt;p&gt;The easiest way to build with Convex is through our &lt;a href="https://www.convex.dev/plans"&gt;cloud platform&lt;/a&gt;, which includes a generous free tier and lets you focus on building your application without worrying about infrastructure. Many small applications and side-projects can operate entirely on the free tier with zero cost and zero maintenance.&lt;/p&gt; 
&lt;h2&gt;Self Hosting&lt;/h2&gt; 
&lt;p&gt;The self-hosted product includes most features of the cloud product, including the dashboard and CLI. Self-hosted Convex works well with a variety of tools including Neon, Fly.io, Vercel, Netlify, RDS, Sqlite, Postgres, and more.&lt;/p&gt; 
&lt;p&gt;You can either use Docker (recommended) or a prebuilt binary to self host Convex. Check out our &lt;a href="https://raw.githubusercontent.com/get-convex/convex-backend/main/self-hosted/README.md"&gt;self-hosting guide&lt;/a&gt; for detailed instructions. Community support for self-hosting is available in the &lt;code&gt;#self-hosted&lt;/code&gt; channel on &lt;a href="https://discord.gg/convex"&gt;Discord&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Community &amp;amp; Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Join our &lt;a href="https://discord.gg/convex"&gt;Discord community&lt;/a&gt; for help and discussions.&lt;/li&gt; 
 &lt;li&gt;Report issues when building and using the open source Convex backend through &lt;a href="https://github.com/get-convex/convex-backend/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Building from source&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/get-convex/convex-backend/main/BUILD.md"&gt;BUILD.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Disclaimers&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you choose to self-host, we recommend following the self-hosting guide. If you are instead building from source, make sure to change your instance secret and admin key from the defaults in the repo.&lt;/li&gt; 
 &lt;li&gt;Convex is battle tested most thoroughly on Linux and Mac. On Windows, it has less experience. If you run into issues, please message us on &lt;a href="https://convex.dev/community"&gt;Discord&lt;/a&gt; in the &lt;code&gt;#self-hosted&lt;/code&gt; channel.&lt;/li&gt; 
 &lt;li&gt;Convex self-hosted builds contain a beacon to help Convex improve the product. The information is minimal and anonymous and helpful to Convex, but if you really want to disable it, you can set the &lt;code&gt;--disable-beacon&lt;/code&gt; flag on the backend binary. The beacon's messages print in the log and only include 
  &lt;ul&gt; 
   &lt;li&gt;A random identifier for your deployment (not used elsewhere)&lt;/li&gt; 
   &lt;li&gt;Migration version of your database&lt;/li&gt; 
   &lt;li&gt;Git rev of the backend&lt;/li&gt; 
   &lt;li&gt;Uptime of the backend&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Repository layout&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;crates/&lt;/code&gt; contains Rust code&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Main binary 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;local_backend/&lt;/code&gt; is an application server on top of the &lt;code&gt;Runtime&lt;/code&gt;. This is the serving edge for the Convex cloud.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;npm-packages/&lt;/code&gt; contains both our public and internal TypeScript packages.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Internal packages 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;udf-runtime/&lt;/code&gt; sets up the user-defined functions JS environment for queries and mutations&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;udf-tests/&lt;/code&gt; is a collection of functions used in testing the isolate layer&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;system-udfs/&lt;/code&gt; contains functions used by the Convex system e.g. the CLI&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>embassy-rs/embassy</title>
      <link>https://github.com/embassy-rs/embassy</link>
      <description>&lt;p&gt;Modern embedded framework, using Rust and async.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Embassy&lt;/h1&gt; 
&lt;p&gt;Embassy is the next-generation framework for embedded applications. Write safe, correct, and energy-efficient embedded code faster, using the Rust programming language, its async facilities, and the Embassy libraries.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://embassy.dev/book/index.html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.embassy.dev/"&gt;API reference&lt;/a&gt; - &lt;a href="https://embassy.dev/"&gt;Website&lt;/a&gt; - &lt;a href="https://matrix.to/#/%23embassy-rs:matrix.org"&gt;Chat&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;Rust + async ‚ù§Ô∏è embedded&lt;/h2&gt; 
&lt;p&gt;The Rust programming language is blazingly fast and memory-efficient, with no runtime, garbage collector, or OS. It catches a wide variety of bugs at compile time, thanks to its full memory- and thread-safety, and expressive type system.&lt;/p&gt; 
&lt;p&gt;Rust's &lt;a href="https://rust-lang.github.io/async-book/"&gt;async/await&lt;/a&gt; allows for unprecedentedly easy and efficient multitasking in embedded systems. Tasks get transformed at compile time into state machines that get run cooperatively. It requires no dynamic memory allocation and runs on a single stack, so no per-task stack size tuning is required. It obsoletes the need for a traditional RTOS with kernel context switching, and is &lt;a href="https://tweedegolf.nl/en/blog/65/async-rust-vs-rtos-showdown"&gt;faster and smaller than one!&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Batteries included&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Hardware Abstraction Layers&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;HALs implement safe, idiomatic Rust APIs to use the hardware capabilities, so raw register manipulation is not needed. The Embassy project maintains HALs for select hardware, but you can still use HALs from other projects with Embassy.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://docs.embassy.dev/embassy-stm32/"&gt;embassy-stm32&lt;/a&gt;, for all STM32 microcontroller families.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://docs.embassy.dev/embassy-nrf/"&gt;embassy-nrf&lt;/a&gt;, for the Nordic Semiconductor nRF52, nRF53, nRF54 and nRF91 series.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://docs.embassy.dev/embassy-rp/"&gt;embassy-rp&lt;/a&gt;, for the Raspberry Pi RP2040 and RP23xx microcontrollers.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://docs.embassy.dev/embassy-mspm0/"&gt;embassy-mspm0&lt;/a&gt;, for the Texas Instruments MSPM0 microcontrollers.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/esp-rs"&gt;esp-rs&lt;/a&gt;, for the Espressif Systems ESP32 series of chips. 
    &lt;ul&gt; 
     &lt;li&gt;Embassy HAL support for Espressif chips, as well as Async Wi-Fi, Bluetooth, and ESP-NOW, is being developed in the &lt;a href="https://github.com/esp-rs/esp-hal"&gt;esp-rs/esp-hal&lt;/a&gt; repository.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/ch32-rs/ch32-hal"&gt;ch32-hal&lt;/a&gt;, for the WCH 32-bit RISC-V(CH32V) series of chips.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/AlexCharlton/mpfs-hal"&gt;mpfs-hal&lt;/a&gt;, for the Microchip PolarFire SoC.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/py32-rs/py32-hal"&gt;py32-hal&lt;/a&gt;, for the Puya Semiconductor PY32 series of microcontrollers.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Time that Just Works&lt;/strong&gt; - No more messing with hardware timers. &lt;a href="https://docs.embassy.dev/embassy-time"&gt;embassy_time&lt;/a&gt; provides Instant, Duration, and Timer types that are globally available and never overflow.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Real-time ready&lt;/strong&gt; - Tasks on the same async executor run cooperatively, but you can create multiple executors with different priorities so that higher priority tasks preempt lower priority ones. See the &lt;a href="https://github.com/embassy-rs/embassy/raw/master/examples/nrf52840/src/bin/multiprio.rs"&gt;example&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Low-power ready&lt;/strong&gt; - Easily build devices with years of battery life. The async executor automatically puts the core to sleep when there's no work to do. Tasks are woken by interrupts, there is no busy-loop polling while waiting.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Networking&lt;/strong&gt; - The &lt;a href="https://docs.embassy.dev/embassy-net/"&gt;embassy-net&lt;/a&gt; network stack implements extensive networking functionality, including Ethernet, IP, TCP, UDP, ICMP, and DHCP. Async drastically simplifies managing timeouts and serving multiple connections concurrently.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bluetooth&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;The &lt;a href="https://github.com/embassy-rs/trouble"&gt;trouble&lt;/a&gt; crate provides a Bluetooth Low Energy 4.x and 5.x Host that runs on any microcontroller implementing the &lt;a href="https://github.com/embassy-rs/bt-hci"&gt;bt-hci&lt;/a&gt; traits (currently &lt;code&gt;nRF52&lt;/code&gt;, &lt;code&gt;rp2040&lt;/code&gt;, &lt;code&gt;rp23xx&lt;/code&gt; and &lt;code&gt;esp32&lt;/code&gt; and &lt;code&gt;serial&lt;/code&gt; controllers are supported).&lt;/li&gt; 
   &lt;li&gt;The &lt;a href="https://github.com/embassy-rs/nrf-softdevice"&gt;nrf-softdevice&lt;/a&gt; crate provides Bluetooth Low Energy 4.x and 5.x support for nRF52 microcontrollers.&lt;/li&gt; 
   &lt;li&gt;The &lt;a href="https://github.com/embassy-rs/embassy/tree/main/embassy-stm32-wpan"&gt;embassy-stm32-wpan&lt;/a&gt; crate provides Bluetooth Low Energy 5.x support for stm32wb microcontrollers.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;LoRa&lt;/strong&gt; - The &lt;a href="https://github.com/lora-rs/lora-rs"&gt;lora-rs&lt;/a&gt; project provides an async LoRa and LoRaWAN stack that works well on Embassy.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;USB&lt;/strong&gt; - &lt;a href="https://docs.embassy.dev/embassy-usb/"&gt;embassy-usb&lt;/a&gt; implements a device-side USB stack. Implementations for common classes such as USB serial (CDC ACM) and USB HID are available, and a rich builder API allows building your own.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bootloader and DFU&lt;/strong&gt; - &lt;a href="https://github.com/embassy-rs/embassy/tree/master/embassy-boot"&gt;embassy-boot&lt;/a&gt; is a lightweight bootloader supporting firmware application upgrades in a power-fail-safe way, with trial boots and rollbacks.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Sneak peek&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-rust,ignore"&gt;use defmt::info;
use embassy_executor::Spawner;
use embassy_time::{Duration, Timer};
use embassy_nrf::gpio::{AnyPin, Input, Level, Output, OutputDrive, Pin, Pull};
use embassy_nrf::{Peri, Peripherals};

// Declare async tasks
#[embassy_executor::task]
async fn blink(pin: Peri&amp;lt;'static, AnyPin&amp;gt;) {
    let mut led = Output::new(pin, Level::Low, OutputDrive::Standard);

    loop {
        // Timekeeping is globally available, no need to mess with hardware timers.
        led.set_high();
        Timer::after_millis(150).await;
        led.set_low();
        Timer::after_millis(150).await;
    }
}

// Main is itself an async task as well.
#[embassy_executor::main]
async fn main(spawner: Spawner) {
    let p = embassy_nrf::init(Default::default());

    // Spawned tasks run in the background, concurrently.
    spawner.spawn(blink(p.P0_13.into())).unwrap();

    let mut button = Input::new(p.P0_11, Pull::Up);
    loop {
        // Asynchronously wait for GPIO events, allowing other tasks
        // to run, or the core to sleep.
        button.wait_for_low().await;
        info!("Button pressed!");
        button.wait_for_high().await;
        info!("Button released!");
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;Examples are found in the &lt;code&gt;examples/&lt;/code&gt; folder separated by the chip manufacturer they are designed to run on. For example:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;examples/nrf52840&lt;/code&gt; run on the &lt;code&gt;nrf52840-dk&lt;/code&gt; board (PCA10056) but should be easily adaptable to other nRF52 chips and boards.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;examples/nrf5340&lt;/code&gt; run on the &lt;code&gt;nrf5340-dk&lt;/code&gt; board (PCA10095).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;examples/stm32xx&lt;/code&gt; for the various STM32 families.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;examples/rp&lt;/code&gt; are for the RP2040 chip.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;examples/std&lt;/code&gt; are designed to run locally on your PC.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Running examples&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install &lt;code&gt;probe-rs&lt;/code&gt; following the instructions at &lt;a href="https://probe.rs"&gt;https://probe.rs&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Change directory to the sample's base directory. For example:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd examples/nrf52840
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Ensure &lt;code&gt;Cargo.toml&lt;/code&gt; sets the right feature for the name of the chip you are programming. If this name is incorrect, the example may fail to run or immediately crash after being programmed.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Ensure &lt;code&gt;.cargo/config.toml&lt;/code&gt; contains the name of the chip you are programming.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the example&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cargo run --release --bin blinky
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more help getting started, see &lt;a href="https://github.com/embassy-rs/embassy/wiki/Getting-Started"&gt;Getting Started&lt;/a&gt; and &lt;a href="https://github.com/embassy-rs/embassy/wiki/Running-the-Examples"&gt;Running the Examples&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Developing Embassy with Rust Analyzer-based editors&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://rust-analyzer.github.io/"&gt;Rust Analyzer&lt;/a&gt; is used by &lt;a href="https://code.visualstudio.com/"&gt;Visual Studio Code&lt;/a&gt; and others. Given the multiple targets that Embassy serves, there is no Cargo workspace file. Instead, the Rust Analyzer must be told of the target project to work with. In the case of Visual Studio Code, please refer to the &lt;code&gt;.vscode/settings.json&lt;/code&gt; file's &lt;code&gt;rust-analyzer.linkedProjects&lt;/code&gt;setting.&lt;/p&gt; 
&lt;h2&gt;Minimum supported Rust version (MSRV)&lt;/h2&gt; 
&lt;p&gt;Embassy is guaranteed to compile on stable Rust 1.75 and up. It &lt;em&gt;might&lt;/em&gt; compile with older versions, but that may change in any new patch release.&lt;/p&gt; 
&lt;h2&gt;Why the name?&lt;/h2&gt; 
&lt;p&gt;EMBedded ASYnc! :)&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Embassy is licensed under either of&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Apache License, Version 2.0 (&lt;a href="https://raw.githubusercontent.com/embassy-rs/embassy/main/LICENSE-APACHE"&gt;LICENSE-APACHE&lt;/a&gt; or &lt;a href="http://www.apache.org/licenses/LICENSE-2.0"&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;MIT license (&lt;a href="https://raw.githubusercontent.com/embassy-rs/embassy/main/LICENSE-MIT"&gt;LICENSE-MIT&lt;/a&gt; or &lt;a href="http://opensource.org/licenses/MIT"&gt;http://opensource.org/licenses/MIT&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;at your option.&lt;/p&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>aws/amazon-q-developer-cli</title>
      <link>https://github.com/aws/amazon-q-developer-cli</link>
      <description>&lt;p&gt;‚ú® Agentic chat experience in your terminal. Build applications using natural language.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Amazon Q CLI&lt;/h1&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;DMG&lt;/strong&gt;: &lt;a href="https://desktop-release.q.us-east-1.amazonaws.com/latest/Amazon%20Q.dmg"&gt;Download now&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/command-line-installing.html#command-line-installing-ubuntu"&gt;Ubuntu/Debian&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/command-line-installing.html#command-line-installing-appimage"&gt;AppImage&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/command-line-installing.html#command-line-installing-alternative-linux"&gt;Alternative Linux builds&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Thank you so much for considering to contribute to Amazon Q.&lt;/p&gt; 
&lt;p&gt;Before getting started, see our &lt;a href="https://raw.githubusercontent.com/aws/amazon-q-developer-cli/main/CONTRIBUTING.md#security-issue-notifications"&gt;contributing docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;MacOS 
  &lt;ul&gt; 
   &lt;li&gt;Xcode 13 or later&lt;/li&gt; 
   &lt;li&gt;Brew&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;1. Clone repo&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/aws/amazon-q-developer-cli.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;2. Install the Rust toolchain using &lt;a href="https://rustup.rs"&gt;Rustup&lt;/a&gt;:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
rustup default stable
rustup toolchain install nightly
cargo install typos-cli
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3. Develop locally&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;To compile and run: &lt;code&gt;cargo run --bin chat_cli&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;To run tests: &lt;code&gt;cargo test&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;To run lints: &lt;code&gt;cargo clippy&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;To format rust files: &lt;code&gt;cargo +nightly fmt&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;To run subcommands: &lt;code&gt;cargo run --bin chat_cli -- {subcommand}&lt;/code&gt;. 
  &lt;ul&gt; 
   &lt;li&gt;Login would then be: &lt;code&gt;cargo run --bin chat_cli -- login&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Project Layout&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aws/amazon-q-developer-cli/main/crates/chat_cli/"&gt;&lt;code&gt;chat_cli&lt;/code&gt;&lt;/a&gt; - the &lt;code&gt;q&lt;/code&gt; CLI, allows users to interface with Amazon Q Developer from the command line&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aws/amazon-q-developer-cli/main/scripts/"&gt;&lt;code&gt;scripts/&lt;/code&gt;&lt;/a&gt; - Contains ops and build related scripts&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aws/amazon-q-developer-cli/main/crates/"&gt;&lt;code&gt;crates/&lt;/code&gt;&lt;/a&gt; - Contains all rust crates&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aws/amazon-q-developer-cli/main/docs/"&gt;&lt;code&gt;docs/&lt;/code&gt;&lt;/a&gt; - Contains technical documentation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Security&lt;/h2&gt; 
&lt;p&gt;For security related concerns, see &lt;a href="https://raw.githubusercontent.com/aws/amazon-q-developer-cli/main/SECURITY.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Licensing&lt;/h2&gt; 
&lt;p&gt;This repo is dual licensed under MIT and Apache 2.0 licenses.&lt;/p&gt; 
&lt;p&gt;Those licenses can be found &lt;a href="https://raw.githubusercontent.com/aws/amazon-q-developer-cli/main/LICENSE.MIT"&gt;here&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/aws/amazon-q-developer-cli/main/LICENSE.APACHE"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;‚ÄúAmazon Web Services‚Äù and all related marks, including logos, graphic designs, and service names, are trademarks or trade dress of AWS in the U.S. and other countries. AWS‚Äôs trademarks and trade dress may not be used in connection with any product or service that is not AWS‚Äôs, in any manner that is likely to cause confusion among customers, or in any manner that disparages or discredits AWS.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>getzola/zola</title>
      <link>https://github.com/getzola/zola</link>
      <description>&lt;p&gt;A fast static site generator in a single binary with everything built-in. https://www.getzola.org&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;zola (n√© Gutenberg) &lt;img src="https://raw.githubusercontent.com/getzola/zola/master/docs/static/logos/Zola-logo-main-coffee.svg?sanitize=true" align="right" alt="zola logo" width="30%"&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://dev.azure.com/getzola/zola/_build/latest?definitionId=1&amp;amp;branchName=master"&gt;&lt;img src="https://dev.azure.com/getzola/zola/_apis/build/status/getzola.zola?branchName=master" alt="Build Status"&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/downloads/getzola/zola/total" alt="GitHub all releases"&gt;&lt;/p&gt; 
&lt;p&gt;A fast static site generator in a single binary with everything built-in.&lt;/p&gt; 
&lt;p&gt;To find out more see the &lt;a href="https://www.getzola.org/documentation/getting-started/overview/"&gt;Zola Documentation&lt;/a&gt;, look in the &lt;a href="https://raw.githubusercontent.com/getzola/zola/master/docs/content"&gt;docs/content&lt;/a&gt; folder of this repository or visit the &lt;a href="https://zola.discourse.group"&gt;Zola community forum&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This tool and its template engine &lt;a href="https://keats.github.io/tera/"&gt;tera&lt;/a&gt; were born from an intense dislike of the (insane) Golang template engine and therefore of Hugo that I was using before for 6+ sites.&lt;/p&gt; 
&lt;h1&gt;List of features&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.getzola.org/documentation/getting-started/cli-usage/"&gt;Single binary&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.getzola.org/documentation/content/syntax-highlighting/"&gt;Syntax highlighting&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.getzola.org/documentation/content/sass/"&gt;Sass compilation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Assets co-location&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.getzola.org/documentation/content/multilingual/"&gt;Multilingual site support&lt;/a&gt; (Basic currently)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.getzola.org/documentation/content/image-processing/"&gt;Image processing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.getzola.org/documentation/themes/overview/"&gt;Themes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.getzola.org/documentation/content/shortcodes/"&gt;Shortcodes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.getzola.org/documentation/content/linking/"&gt;Internal links&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.getzola.org/documentation/getting-started/cli-usage/#check"&gt;External link checker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.getzola.org/documentation/content/table-of-contents/"&gt;Table of contents automatic generation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Automatic header anchors&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.getzola.org/documentation/content/page/#front-matter"&gt;Aliases&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.getzola.org/documentation/templates/pagination/"&gt;Pagination&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.getzola.org/documentation/templates/taxonomies/"&gt;Custom taxonomies&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.getzola.org/documentation/content/search/"&gt;Search with no servers or any third parties involved&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.getzola.org/documentation/getting-started/cli-usage/#serve"&gt;Live reload&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Deploy on many platforms easily: &lt;a href="https://www.getzola.org/documentation/deployment/netlify/"&gt;Netlify&lt;/a&gt;, &lt;a href="https://www.getzola.org/documentation/deployment/vercel/"&gt;Vercel&lt;/a&gt;, &lt;a href="https://www.getzola.org/documentation/deployment/cloudflare-pages/"&gt;Cloudflare Pages&lt;/a&gt;, etc&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Xerxes-2/clewdr</title>
      <link>https://github.com/Xerxes-2/clewdr</link>
      <description>&lt;p&gt;High Performance LLM Reverse Proxy&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/Xerxes-2/clewdr/master/assets/clewdr-logo.svg?sanitize=true" alt="ClewdR" height="60"&gt; 
 &lt;p&gt;&lt;em&gt;High-Performance LLM Proxy for the Modern Era&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://deepwiki.com/Xerxes-2/clewdr"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki"&gt;&lt;/a&gt; &lt;a href="https://github.com/Xerxes-2/clewdr/releases/latest"&gt;&lt;img src="https://img.shields.io/github/v/release/Xerxes-2/clewdr?style=for-the-badge&amp;amp;logo=github&amp;amp;color=blue" alt="GitHub Release"&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Xerxes-2/clewdr/master/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/Xerxes-2/clewdr?style=for-the-badge&amp;amp;color=green" alt="License"&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Xerxes-2/clewdr/master/#performance-metrics"&gt;&lt;img src="https://img.shields.io/badge/Performance-10x%20Faster-orange?style=for-the-badge" alt="Performance"&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Xerxes-2/clewdr/master/#technical-architecture"&gt;&lt;img src="https://img.shields.io/badge/Memory-Single%20Digit%20MB-purple?style=for-the-badge" alt="Memory"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;h3&gt;üåç Language Support&lt;/h3&gt; 
 &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/Xerxes-2/clewdr/master/README.md"&gt;&lt;strong&gt;üá∫üá∏ English&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/Xerxes-2/clewdr/master/README_zh.md"&gt;&lt;strong&gt;üá®üá≥ ÁÆÄ‰Ωì‰∏≠Êñá&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;hr&gt; 
&lt;h2&gt;üéØ &lt;strong&gt;What is ClewdR?&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;ClewdR&lt;/strong&gt; is a production-grade, high-performance proxy server engineered specifically for &lt;strong&gt;Claude&lt;/strong&gt; (Claude.ai, Claude Code) and &lt;strong&gt;Google Gemini&lt;/strong&gt; (AI Studio, Vertex AI). Built with &lt;strong&gt;Rust&lt;/strong&gt; for maximum performance and minimal resource usage, it provides enterprise-level reliability with consumer-friendly simplicity.&lt;/p&gt; 
&lt;h3&gt;üèÜ &lt;strong&gt;Why ClewdR?&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üöÑ 10x Performance&lt;/strong&gt;: Outperforms script-language implementations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üíæ 1/10th Memory&lt;/strong&gt;: Uses only single-digit MB in production&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîß Production Ready&lt;/strong&gt;: Handles thousands of requests per second&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üåê Multi-Platform&lt;/strong&gt;: Native support for Windows, macOS, Linux, Android&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚ú® &lt;strong&gt;Core Features&lt;/strong&gt;&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üé® &lt;strong&gt;Full-Featured Web Interface&lt;/strong&gt;&lt;/h3&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;React-powered dashboard&lt;/strong&gt; with real-time monitoring&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Multi-language support&lt;/strong&gt; (English/Chinese)&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Secure authentication&lt;/strong&gt; with auto-generated passwords&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Hot configuration reload&lt;/strong&gt; without service interruption&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Visual cookie &amp;amp; key management&lt;/strong&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;h3&gt;üèóÔ∏è &lt;strong&gt;Enterprise Architecture&lt;/strong&gt;&lt;/h3&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Tokio + Axum&lt;/strong&gt; async runtime for maximum throughput&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Event-driven design&lt;/strong&gt; with decoupled components&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Moka-powered caching&lt;/strong&gt; with intelligent invalidation&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Chrome-level fingerprinting&lt;/strong&gt; for seamless API access&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Multi-threaded processing&lt;/strong&gt; with optimal resource usage&lt;/li&gt; 
    &lt;/ul&gt; &lt;h3&gt;üß† &lt;strong&gt;Intelligent Resource Management&lt;/strong&gt;&lt;/h3&gt; 
    &lt;ul&gt; 
     &lt;li&gt; &lt;p&gt;&lt;strong&gt;Smart cookie rotation&lt;/strong&gt; with status classification&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;&lt;strong&gt;API key health monitoring&lt;/strong&gt; and automatic failover&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;&lt;strong&gt;Rate limiting protection&lt;/strong&gt; with exponential backoff&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;&lt;strong&gt;Connection pooling&lt;/strong&gt; with keep-alive optimization&lt;/p&gt; &lt;/li&gt;
    &lt;/ul&gt;&lt;/td&gt; 
   &lt;td width="50%"&gt;   &lt;h3&gt;üåç &lt;strong&gt;Universal Compatibility&lt;/strong&gt;&lt;/h3&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Static compilation&lt;/strong&gt; - single binary, zero dependencies&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Cross-platform native&lt;/strong&gt; - Windows, macOS, Linux, Android&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Docker ready&lt;/strong&gt; with optimized images&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Reverse proxy friendly&lt;/strong&gt; with custom endpoint support&lt;/li&gt; 
    &lt;/ul&gt; &lt;h3&gt;üöÄ &lt;strong&gt;Protocol Support&lt;/strong&gt;&lt;/h3&gt; &lt;h4&gt;&lt;strong&gt;Claude Integration&lt;/strong&gt;&lt;/h4&gt; 
    &lt;ul&gt; 
     &lt;li&gt;‚úÖ &lt;strong&gt;Claude.ai&lt;/strong&gt; web interface&lt;/li&gt; 
     &lt;li&gt;‚úÖ &lt;strong&gt;Claude Code&lt;/strong&gt; specialized support&lt;/li&gt; 
     &lt;li&gt;‚úÖ &lt;strong&gt;System prompt caching&lt;/strong&gt; for efficiency&lt;/li&gt; 
     &lt;li&gt;‚úÖ &lt;strong&gt;Extended Thinking&lt;/strong&gt; mode&lt;/li&gt; 
     &lt;li&gt;‚úÖ &lt;strong&gt;Image attachments&lt;/strong&gt; &amp;amp; web search&lt;/li&gt; 
     &lt;li&gt;‚úÖ &lt;strong&gt;Custom stop sequences&lt;/strong&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;h4&gt;&lt;strong&gt;Google Gemini Integration&lt;/strong&gt;&lt;/h4&gt; 
    &lt;ul&gt; 
     &lt;li&gt;‚úÖ &lt;strong&gt;AI Studio&lt;/strong&gt; &amp;amp; &lt;strong&gt;Vertex AI&lt;/strong&gt;&lt;/li&gt; 
     &lt;li&gt;‚úÖ &lt;strong&gt;OAuth2 authentication&lt;/strong&gt; for Vertex&lt;/li&gt; 
     &lt;li&gt;‚úÖ &lt;strong&gt;HTTP Keep-Alive&lt;/strong&gt; optimization&lt;/li&gt; 
     &lt;li&gt;‚úÖ &lt;strong&gt;Model switching&lt;/strong&gt; with automatic detection&lt;/li&gt; 
    &lt;/ul&gt; &lt;h4&gt;&lt;strong&gt;API Compatibility&lt;/strong&gt;&lt;/h4&gt; 
    &lt;ul&gt; 
     &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;OpenAI format&lt;/strong&gt; - drop-in replacement&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Native formats&lt;/strong&gt; - Claude &amp;amp; Gemini&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Streaming responses&lt;/strong&gt; with real-time processing&lt;/p&gt; &lt;/li&gt;
    &lt;/ul&gt;&lt;/td&gt; 
  &lt;/tr&gt;   
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;üìä &lt;strong&gt;Performance Metrics&lt;/strong&gt;&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Metric&lt;/th&gt; 
    &lt;th&gt;ClewdR&lt;/th&gt; 
    &lt;th&gt;Traditional Proxies&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Memory Usage&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;&amp;lt;10 MB&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;100-500 MB&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Requests/sec&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;1000+&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;100-200&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Startup Time&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;&amp;lt;1 second&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;5-15 seconds&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Binary Size&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;~15 MB&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;50-200 MB&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Dependencies&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;Zero&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;Node.js/Python + libs&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;üöÄ &lt;strong&gt;Quick Start Guide&lt;/strong&gt;&lt;/h2&gt; 
&lt;h3&gt;&lt;strong&gt;Step 1: Download &amp;amp; Run&lt;/strong&gt;&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Download the latest release for your platform
wget https://github.com/Xerxes-2/clewdr/releases/latest/download/clewdr-[platform]

# Extract the binary (if necessary)
tar -xzf clewdr-[platform].tar.gz

# Navigate to the directory
cd clewdr-[platform]

# Make executable (Linux/macOS)
chmod +x clewdr

# Run ClewdR
./clewdr
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;üì¶ &lt;strong&gt;Platform Downloads&lt;/strong&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Platform&lt;/th&gt; 
    &lt;th&gt;Architecture&lt;/th&gt; 
    &lt;th&gt;Download Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;ü™ü Windows&lt;/td&gt; 
    &lt;td&gt;x64&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/Xerxes-2/clewdr/releases/latest"&gt;clewdr-windows-x64.exe&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;üêß Linux&lt;/td&gt; 
    &lt;td&gt;x64&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/Xerxes-2/clewdr/releases/latest"&gt;clewdr-linux-x64&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;üêß Linux&lt;/td&gt; 
    &lt;td&gt;ARM64&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/Xerxes-2/clewdr/releases/latest"&gt;clewdr-linux-arm64&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;üçé macOS&lt;/td&gt; 
    &lt;td&gt;x64&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/Xerxes-2/clewdr/releases/latest"&gt;clewdr-macos-x64&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;üçé macOS&lt;/td&gt; 
    &lt;td&gt;ARM64 (M1/M2)&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/Xerxes-2/clewdr/releases/latest"&gt;clewdr-macos-arm64&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;ü§ñ Android&lt;/td&gt; 
    &lt;td&gt;ARM64&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/Xerxes-2/clewdr/releases/latest"&gt;clewdr-android-arm64&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;&lt;strong&gt;Step 2: Access Web Interface&lt;/strong&gt;&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;üåê Open your browser to &lt;strong&gt;&lt;code&gt;http://127.0.0.1:8484&lt;/code&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;üîê Use the &lt;strong&gt;Web Admin Password&lt;/strong&gt; displayed in the console&lt;/li&gt; 
 &lt;li&gt;üéâ Welcome to ClewdR's management interface!&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üí° Pro Tips:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Forgot password?&lt;/strong&gt; Delete &lt;code&gt;clewdr.toml&lt;/code&gt; and restart&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Docker users:&lt;/strong&gt; Password appears in container logs&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Change password:&lt;/strong&gt; Use the web interface settings&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;Step 3: Configure Your Services&lt;/strong&gt;&lt;/h3&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h4&gt;üçÉ &lt;strong&gt;Claude Setup&lt;/strong&gt;&lt;/h4&gt; 
    &lt;ol&gt; 
     &lt;li&gt;&lt;strong&gt;Add Cookies&lt;/strong&gt;: Paste your Claude.ai session cookies&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Configure Proxy&lt;/strong&gt;: Set upstream proxy if needed&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Test Connection&lt;/strong&gt;: Verify cookie status in dashboard&lt;/li&gt; 
    &lt;/ol&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h4&gt;üîπ &lt;strong&gt;Gemini Setup&lt;/strong&gt;&lt;/h4&gt; 
    &lt;ol&gt; 
     &lt;li&gt;&lt;strong&gt;Add API Keys&lt;/strong&gt;: Input your Google AI Studio keys&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Vertex AI&lt;/strong&gt; (Optional): Configure OAuth2 for enterprise&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Model Selection&lt;/strong&gt;: Choose your preferred models&lt;/li&gt; 
    &lt;/ol&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;&lt;strong&gt;Step 4: Connect Your Applications&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;ClewdR provides multiple API endpoints. Check the console output for available endpoints:&lt;/p&gt; 
&lt;h4&gt;üîó &lt;strong&gt;API Endpoints&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Claude Endpoints
Claude Web:    http://127.0.0.1:8484/v1/messages          # Native format
Claude OpenAI: http://127.0.0.1:8484/v1/chat/completions  # OpenAI compatible
Claude Code:   http://127.0.0.1:8484/code/v1/messages     # Claude Code

# Gemini Endpoints  
Gemini Native: http://127.0.0.1:8484/v1/v1beta/generateContent    # Native format
Gemini OpenAI: http://127.0.0.1:8484/gemini/chat/completions      # OpenAI compatible
Vertex AI:     http://127.0.0.1:8484/v1/vertex/v1beta/            # Vertex AI
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;‚öôÔ∏è &lt;strong&gt;Application Configuration Examples&lt;/strong&gt;&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;SillyTavern Configuration&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "api_url": "http://127.0.0.1:8484/v1/chat/completions",
  "api_key": "your-api-password-from-console",
  "model": "claude-3-sonnet-20240229"
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Continue VSCode Extension&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "models": [
    {
      "title": "Claude via ClewdR",
      "provider": "openai",
      "model": "claude-3-sonnet-20240229",
      "apiBase": "http://127.0.0.1:8484/v1/",
      "apiKey": "your-api-password-from-console"
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Cursor IDE Configuration&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "openaiApiBase": "http://127.0.0.1:8484/v1/",
  "openaiApiKey": "your-api-password-from-console"
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;&lt;strong&gt;Step 5: Verify &amp;amp; Monitor&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ Check cookie/key status in the web dashboard&lt;/li&gt; 
 &lt;li&gt;‚úÖ Monitor request logs for successful connections&lt;/li&gt; 
 &lt;li&gt;‚úÖ Test with a simple chat request&lt;/li&gt; 
 &lt;li&gt;‚úÖ Enjoy blazing-fast LLM proxy performance!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Community Resources&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Github Aggregated Wiki&lt;/strong&gt;: &lt;a href="https://github.com/Xerxes-2/clewdr/wiki"&gt;https://github.com/Xerxes-2/clewdr/wiki&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/teralomaniac/clewd"&gt;Clewd Modified Version&lt;/a&gt; - A modified version of the original Clewd, providing many inspirations and foundational features.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mirrorange/clove"&gt;Clove&lt;/a&gt; - Provides the support logic for Claude Code.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>solana-foundation/anchor</title>
      <link>https://github.com/solana-foundation/anchor</link>
      <description>&lt;p&gt;‚öì Solana Sealevel Framework&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img height="170x" src="https://pbs.twimg.com/media/FVUVaO9XEAAulvK?format=png&amp;amp;name=small"&gt; 
 &lt;h1&gt;Anchor&lt;/h1&gt; 
 &lt;p&gt; &lt;strong&gt;Solana Program Framework&lt;/strong&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;a href="https://github.com/coral-xyz/anchor/actions"&gt;&lt;img alt="Build Status" src="https://github.com/coral-xyz/anchor/actions/workflows/tests.yaml/badge.svg?sanitize=true"&gt;&lt;/a&gt; &lt;a href="https://anchor-lang.com"&gt;&lt;img alt="Tutorials" src="https://img.shields.io/badge/docs-tutorials-blueviolet"&gt;&lt;/a&gt; &lt;a href="https://discord.gg/NHHGSXAnXk"&gt;&lt;img alt="Discord Chat" src="https://img.shields.io/discord/889577356681945098?color=blueviolet"&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img alt="License" src="https://img.shields.io/github/license/coral-xyz/anchor?color=blueviolet"&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;a href="https://www.anchor-lang.com/"&gt;Anchor&lt;/a&gt; is a framework providing several convenient developer tools for writing Solana programs (sometimes called 'smart contracts').&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Rust eDSL for writing Solana programs&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Interface_description_language"&gt;IDL&lt;/a&gt; specification&lt;/li&gt; 
 &lt;li&gt;TypeScript package for generating clients from IDL&lt;/li&gt; 
 &lt;li&gt;CLI and workspace management for developing complete applications&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Anchor is the most popular framework for Solana programs.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] If you're familiar with developing in Ethereum's &lt;a href="https://docs.soliditylang.org/en/"&gt;Solidity&lt;/a&gt;, &lt;a href="https://www.trufflesuite.com/"&gt;Truffle&lt;/a&gt;, &lt;a href="https://github.com/ethereum/web3.js"&gt;web3.js&lt;/a&gt;, then using Anchor be familiar. Although the DSL syntax and semantics are targeted at Solana, the high level flow of writing RPC request handlers, emitting an IDL, and generating clients from IDL is the same.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;For a quickstart guide and in depth tutorials, see the &lt;a href="https://book.anchor-lang.com"&gt;Anchor book&lt;/a&gt; and the &lt;a href="https://anchor-lang.com"&gt;Anchor documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To jump straight to examples, go &lt;a href="https://github.com/coral-xyz/anchor/tree/master/examples"&gt;here&lt;/a&gt;. For the latest Rust and TypeScript API documentation, see &lt;a href="https://docs.rs/anchor-lang"&gt;docs.rs&lt;/a&gt; and the &lt;a href="https://www.anchor-lang.com/docs/clients/typescript"&gt;typedoc&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Packages&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Package&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;th align="left"&gt;Version&lt;/th&gt; 
   &lt;th align="left"&gt;Docs&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;anchor-lang&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Rust primitives for writing programs on Solana&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://crates.io/crates/anchor-lang"&gt;&lt;img src="https://img.shields.io/crates/v/anchor-lang?color=blue" alt="Crates.io"&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://docs.rs/anchor-lang"&gt;&lt;img src="https://docs.rs/anchor-lang/badge.svg?sanitize=true" alt="Docs.rs"&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;anchor-spl&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;CPI clients for SPL programs on Solana&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://crates.io/crates/anchor-spl"&gt;&lt;img src="https://img.shields.io/crates/v/anchor-spl?color=blue" alt="crates"&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://docs.rs/anchor-spl"&gt;&lt;img src="https://docs.rs/anchor-spl/badge.svg?sanitize=true" alt="Docs.rs"&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;anchor-client&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Rust client for Anchor programs&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://crates.io/crates/anchor-client"&gt;&lt;img src="https://img.shields.io/crates/v/anchor-client?color=blue" alt="crates"&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://docs.rs/anchor-client"&gt;&lt;img src="https://docs.rs/anchor-client/badge.svg?sanitize=true" alt="Docs.rs"&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;@coral-xyz/anchor&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;TypeScript client for Anchor programs&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.npmjs.com/package/@coral-xyz/anchor"&gt;&lt;img src="https://img.shields.io/npm/v/@coral-xyz/anchor.svg?color=blue" alt="npm"&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://coral-xyz.github.io/anchor/ts/index.html"&gt;&lt;img src="https://img.shields.io/badge/docs-typedoc-blue" alt="Docs"&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;@coral-xyz/anchor-cli&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;CLI to support building and managing an Anchor workspace&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.npmjs.com/package/@coral-xyz/anchor-cli"&gt;&lt;img src="https://img.shields.io/npm/v/@coral-xyz/anchor-cli.svg?color=blue" alt="npm"&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://coral-xyz.github.io/anchor/cli/commands.html"&gt;&lt;img src="https://img.shields.io/badge/docs-typedoc-blue" alt="Docs"&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Note&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Anchor is in active development, so all APIs are subject to change.&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;This code is unaudited. Use at your own risk.&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;Here's a counter program, where only the designated &lt;code&gt;authority&lt;/code&gt; can increment the count.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;use anchor_lang::prelude::*;

declare_id!("Fg6PaFpoGXkYsidMpWTK6W2BeZ7FEfcYkg476zPFsLnS");

#[program]
mod counter {
    use super::*;

    pub fn initialize(ctx: Context&amp;lt;Initialize&amp;gt;, start: u64) -&amp;gt; Result&amp;lt;()&amp;gt; {
        let counter = &amp;amp;mut ctx.accounts.counter;
        counter.authority = *ctx.accounts.authority.key;
        counter.count = start;
        Ok(())
    }

    pub fn increment(ctx: Context&amp;lt;Increment&amp;gt;) -&amp;gt; Result&amp;lt;()&amp;gt; {
        let counter = &amp;amp;mut ctx.accounts.counter;
        counter.count += 1;
        Ok(())
    }
}

#[derive(Accounts)]
pub struct Initialize&amp;lt;'info&amp;gt; {
    #[account(init, payer = authority, space = 48)]
    pub counter: Account&amp;lt;'info, Counter&amp;gt;,
    pub authority: Signer&amp;lt;'info&amp;gt;,
    pub system_program: Program&amp;lt;'info, System&amp;gt;,
}

#[derive(Accounts)]
pub struct Increment&amp;lt;'info&amp;gt; {
    #[account(mut, has_one = authority)]
    pub counter: Account&amp;lt;'info, Counter&amp;gt;,
    pub authority: Signer&amp;lt;'info&amp;gt;,
}

#[account]
pub struct Counter {
    pub authority: Pubkey,
    pub count: u64,
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more, see the &lt;a href="https://github.com/coral-xyz/anchor/tree/master/examples"&gt;examples&lt;/a&gt; and &lt;a href="https://github.com/coral-xyz/anchor/tree/master/tests"&gt;tests&lt;/a&gt; directories.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Anchor is licensed under &lt;a href="https://raw.githubusercontent.com/solana-foundation/anchor/master/LICENSE"&gt;Apache 2.0&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in Anchor by you, as defined in the Apache-2.0 license, shall be licensed as above, without any additional terms or conditions.&lt;/p&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;p&gt;Thank you for your interest in contributing to Anchor! Please see the &lt;a href="https://raw.githubusercontent.com/solana-foundation/anchor/master/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; to learn how.&lt;/p&gt; 
&lt;h3&gt;Thanks ‚ù§Ô∏è&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/coral-xyz/anchor/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=coral-xyz/anchor" width="100%"&gt; &lt;/a&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>tracel-ai/burn</title>
      <link>https://github.com/tracel-ai/burn</link>
      <description>&lt;p&gt;Burn is a next generation Deep Learning Framework that doesn't compromise on flexibility, efficiency and portability.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/tracel-ai/burn/main/assets/logo-burn-neutral.webp" width="350px"&gt; 
 &lt;p&gt;&lt;a href="https://discord.gg/uPEBbYYDB6"&gt;&lt;img src="https://img.shields.io/discord/1038839012602941528.svg?color=7289da&amp;amp;&amp;amp;logo=discord" alt="Discord"&gt;&lt;/a&gt; &lt;a href="https://crates.io/crates/burn"&gt;&lt;img src="https://img.shields.io/crates/v/burn.svg?sanitize=true" alt="Current Crates.io Version"&gt;&lt;/a&gt; &lt;a href="https://crates.io/crates/burn"&gt;&lt;img src="https://img.shields.io/crates/msrv/burn" alt="Minimum Supported Rust Version"&gt;&lt;/a&gt; &lt;a href="https://burn.dev/docs/burn"&gt;&lt;img src="https://img.shields.io/badge/docs-latest-blue" alt="Documentation"&gt;&lt;/a&gt; &lt;a href="https://github.com/tracel-ai/burn/actions/workflows/test.yml"&gt;&lt;img src="https://github.com/tracel-ai/burn/actions/workflows/test.yml/badge.svg?sanitize=true" alt="Test Status"&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/#license"&gt;&lt;img src="https://shields.io/badge/license-MIT%2FApache--2.0-blue" alt="license"&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/tracel-ai/burn"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.runblaze.dev"&gt;&lt;img src="https://www.runblaze.dev/ci-blaze-powered.png" width="125px"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;hr&gt; 
 &lt;p&gt;&lt;strong&gt;Burn is a next generation Deep Learning Framework that doesn't compromise on &lt;br&gt; flexibility, efficiency and portability.&lt;/strong&gt;&lt;/p&gt; 
 &lt;br&gt; 
&lt;/div&gt; 
&lt;div align="left"&gt; 
 &lt;h2&gt;Performance&lt;/h2&gt; 
 &lt;div align="left"&gt; 
  &lt;img align="right" src="https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-blazingly-fast.png" height="96px"&gt; 
  &lt;p&gt;Because we believe the goal of a deep learning framework is to convert computation into useful intelligence, we have made performance a core pillar of Burn. We strive to achieve top efficiency by leveraging multiple optimization techniques described below.&lt;/p&gt; 
  &lt;p&gt;&lt;strong&gt;Click on each section for more details&lt;/strong&gt; üëá&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;br&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Automatic kernel fusion üí• &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;Using Burn means having your models optimized on any backend. When possible, we provide a way to automatically and dynamically create custom kernels that minimize data relocation between different memory spaces, extremely useful when moving memory is the bottleneck.&lt;/p&gt; 
  &lt;p&gt;As an example, you could write your own GELU activation function with the high level tensor api (see Rust code snippet below).&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-rust"&gt;fn gelu_custom&amp;lt;B: Backend, const D: usize&amp;gt;(x: Tensor&amp;lt;B, D&amp;gt;) -&amp;gt; Tensor&amp;lt;B, D&amp;gt; {
    let x = x.clone() * ((x / SQRT_2).erf() + 1);
    x / 2
}
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;Then, at runtime, a custom low-level kernel will be automatically created for your specific implementation and will rival a handcrafted GPU implementation. The kernel consists of about 60 lines of WGSL &lt;a href="%22https://www.w3.org/TR/WGSL/https://www.w3.org/TR/WGSL/%22"&gt;WebGPU Shading Language&lt;/a&gt;, an extremely verbose lower level shader language you probably don't want to program your deep learning models in!&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Asynchronous execution ‚ù§Ô∏è‚Äçüî• &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;For &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/#backends"&gt;first-party backends&lt;/a&gt;, an asynchronous execution style is used, which allows to perform various optimizations, such as the previously mentioned automatic kernel fusion.&lt;/p&gt; 
  &lt;p&gt;Asynchronous execution also ensures that the normal execution of the framework does not block the model computations, which implies that the framework overhead won't impact the speed of execution significantly. Conversely, the intense computations in the model do not interfere with the responsiveness of the framework. For more information about our asynchronous backends, see &lt;a href="https://burn.dev/blog/creating-high-performance-asynchronous-backends-with-burn-compute"&gt;this blog post&lt;/a&gt;.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Thread-safe building blocks ü¶û &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;Burn emphasizes thread safety by leveraging the &lt;a href="https://doc.rust-lang.org/book/ch04-00-understanding-ownership.html"&gt;ownership system of Rust&lt;/a&gt;. With Burn, each module is the owner of its weights. It is therefore possible to send a module to another thread for computing the gradients, then send the gradients to the main thread that can aggregate them, and &lt;em&gt;voil√†&lt;/em&gt;, you get multi-device training.&lt;/p&gt; 
  &lt;p&gt;This is a very different approach from what PyTorch does, where backpropagation actually mutates the &lt;em&gt;grad&lt;/em&gt; attribute of each tensor parameter. This is not a thread-safe operation and therefore requires lower level synchronization primitives, see &lt;a href="https://pytorch.org/docs/stable/distributed.html"&gt;distributed training&lt;/a&gt; for reference. Note that this is still very fast, but not compatible across different backends and quite hard to implement.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Intelligent memory management ü¶Ä &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;One of the main roles of a deep learning framework is to reduce the amount of memory necessary to run models. The naive way of handling memory is that each tensor has its own memory space, which is allocated when the tensor is created then deallocated as the tensor gets out of scope. However, allocating and deallocating data is very costly, so a memory pool is often required to achieve good throughput. Burn offers an infrastructure that allows for easily creating and selecting memory management strategies for backends. For more details on memory management in Burn, see &lt;a href="https://burn.dev/blog/creating-high-performance-asynchronous-backends-with-burn-compute"&gt;this blog post&lt;/a&gt;.&lt;/p&gt; 
  &lt;p&gt;Another very important memory optimization of Burn is that we keep track of when a tensor can be mutated in-place just by using the ownership system well. Even though it is a rather small memory optimization on its own, it adds up considerably when training or running inference with larger models and contributes to reduce the memory usage even more. For more information, see &lt;a href="https://burn.dev/blog/burn-rusty-approach-to-tensor-handling"&gt;this blog post about tensor handling&lt;/a&gt;.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Automatic kernel selection üéØ &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;A good deep learning framework should ensure that models run smoothly on all hardware. However, not all hardware share the same behavior in terms of execution speed. For instance, a matrix multiplication kernel can be launched with many different parameters, which are highly sensitive to the size of the matrices and the hardware. Using the wrong configuration could reduce the speed of execution by a large factor (10 times or even more in extreme cases), so choosing the right kernels becomes a priority.&lt;/p&gt; 
  &lt;p&gt;With our home-made backends, we run benchmarks automatically and choose the best configuration for the current hardware and matrix sizes with a reasonable caching strategy.&lt;/p&gt; 
  &lt;p&gt;This adds a small overhead by increasing the warmup execution time, but stabilizes quickly after a few forward and backward passes, saving lots of time in the long run. Note that this feature isn't mandatory, and can be disabled when cold starts are a priority over optimized throughput.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Hardware specific features üî• &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;It is no secret that deep learning is mostly relying on matrix multiplication as its core operation, since this is how fully-connected neural networks are modeled.&lt;/p&gt; 
  &lt;p&gt;More and more, hardware manufacturers optimize their chips specifically for matrix multiplication workloads. For instance, Nvidia has its &lt;em&gt;Tensor Cores&lt;/em&gt; and today most cellphones have AI specialized chips. As of this moment, we support Tensor Cores with our LibTorch, Candle, CUDA, Metal and WGPU/SPIR-V backends, but not other accelerators yet. We hope &lt;a href="https://github.com/gpuweb/gpuweb/issues/4195"&gt;this issue&lt;/a&gt; gets resolved at some point to bring support to our WGPU backend.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Custom Backend Extension üéí &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;Burn aims to be the most flexible deep learning framework. While it's crucial to maintain compatibility with a wide variety of backends, Burn also provides the ability to extend the functionalities of a backend implementation to suit your personal modeling requirements.&lt;/p&gt; 
  &lt;p&gt;This versatility is advantageous in numerous ways, such as supporting custom operations like flash attention or manually writing your own kernel for a specific backend to enhance performance. See &lt;a href="https://burn.dev/books/burn/advanced/backend-extension/index.html"&gt;this section&lt;/a&gt; in the Burn Book üî• for more details.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br&gt; 
 &lt;h2&gt;Backend&lt;/h2&gt; 
 &lt;div align="left"&gt; 
  &lt;img align="right" src="https://raw.githubusercontent.com/tracel-ai/burn/main/assets/backend-chip.png" height="96px"&gt; 
  &lt;p&gt;Burn strives to be as fast as possible on as many hardwares as possible, with robust implementations. We believe this flexibility is crucial for modern needs where you may train your models in the cloud, then deploy on customer hardwares, which vary from user to user.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;br&gt; 
 &lt;p&gt;&lt;strong&gt;Supported Backends&lt;/strong&gt;&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Backend&lt;/th&gt; 
    &lt;th&gt;Devices&lt;/th&gt; 
    &lt;th&gt;Class&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CUDA&lt;/td&gt; 
    &lt;td&gt;NVIDIA GPUs&lt;/td&gt; 
    &lt;td&gt;First-Party&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;ROCm&lt;/td&gt; 
    &lt;td&gt;AMD GPUs&lt;/td&gt; 
    &lt;td&gt;First-Party&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Metal&lt;/td&gt; 
    &lt;td&gt;Apple GPUs&lt;/td&gt; 
    &lt;td&gt;First-Party&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Vulkan&lt;/td&gt; 
    &lt;td&gt;Most GPUs on Linux &amp;amp; Windows&lt;/td&gt; 
    &lt;td&gt;First-Party&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Wgpu&lt;/td&gt; 
    &lt;td&gt;Most GPUs&lt;/td&gt; 
    &lt;td&gt;First-Party&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;NdArray&lt;/td&gt; 
    &lt;td&gt;Most CPUs&lt;/td&gt; 
    &lt;td&gt;Third-Party&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;LibTorch&lt;/td&gt; 
    &lt;td&gt;Most GPUs &amp;amp; CPUs&lt;/td&gt; 
    &lt;td&gt;Third-Party&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Candle&lt;/td&gt; 
    &lt;td&gt;Nvidia, Apple GPUs &amp;amp; CPUs&lt;/td&gt; 
    &lt;td&gt;Third-Party&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;br&gt; 
 &lt;p&gt;Compared to other frameworks, Burn has a very different approach to supporting many backends. By design, most code is generic over the Backend trait, which allows us to build Burn with swappable backends. This makes composing backend possible, augmenting them with additional functionalities such as autodifferentiation and automatic kernel fusion.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Autodiff: Backend decorator that brings backpropagation to any backend üîÑ &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;Contrary to the aforementioned backends, Autodiff is actually a backend &lt;em&gt;decorator&lt;/em&gt;. This means that it cannot exist by itself; it must encapsulate another backend.&lt;/p&gt; 
  &lt;p&gt;The simple act of wrapping a base backend with Autodiff transparently equips it with autodifferentiation support, making it possible to call backward on your model.&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-rust"&gt;use burn::backend::{Autodiff, Wgpu};
use burn::tensor::{Distribution, Tensor};

fn main() {
    type Backend = Autodiff&amp;lt;Wgpu&amp;gt;;

    let x: Tensor&amp;lt;Backend, 2&amp;gt; = Tensor::random([32, 32], Distribution::Default);
    let y: Tensor&amp;lt;Backend, 2&amp;gt; = Tensor::random([32, 32], Distribution::Default).require_grad();

    let tmp = x.clone() + y.clone();
    let tmp = tmp.matmul(x);
    let tmp = tmp.exp();

    let grads = tmp.backward();
    let y_grad = y.grad(&amp;amp;grads).unwrap();
    println!("{y_grad}");
}
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;Of note, it is impossible to make the mistake of calling backward on a model that runs on a backend that does not support autodiff (for inference), as this method is only offered by an Autodiff backend.&lt;/p&gt; 
  &lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/crates/burn-autodiff/README.md"&gt;Autodiff Backend README&lt;/a&gt; for more details.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Fusion: Backend decorator that brings kernel fusion to all first-party backends &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;This backend decorator enhances a backend with kernel fusion, provided that the inner backend supports it. Note that you can compose this backend with other backend decorators such as Autodiff. For now, only the WGPU and CUDA backends have support for fused kernels.&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-rust"&gt;use burn::backend::{Autodiff, Fusion, Wgpu};
use burn::tensor::{Distribution, Tensor};

fn main() {
    type Backend = Autodiff&amp;lt;Fusion&amp;lt;Wgpu&amp;gt;&amp;gt;;

    let x: Tensor&amp;lt;Backend, 2&amp;gt; = Tensor::random([32, 32], Distribution::Default);
    let y: Tensor&amp;lt;Backend, 2&amp;gt; = Tensor::random([32, 32], Distribution::Default).require_grad();

    let tmp = x.clone() + y.clone();
    let tmp = tmp.matmul(x);
    let tmp = tmp.exp();

    let grads = tmp.backward();
    let y_grad = y.grad(&amp;amp;grads).unwrap();
    println!("{y_grad}");
}

&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;Of note, we plan to implement automatic gradient checkpointing based on compute bound and memory bound operations, which will work gracefully with the fusion backend to make your code run even faster during training, see &lt;a href="https://github.com/tracel-ai/burn/issues/936"&gt;this issue&lt;/a&gt;.&lt;/p&gt; 
  &lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/crates/burn-fusion/README.md"&gt;Fusion Backend README&lt;/a&gt; for more details.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Router (Beta): Backend decorator that composes multiple backends into a single one &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;That backend simplifies hardware operability, if for instance you want to execute some operations on the CPU and other operations on the GPU.&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-rust"&gt;use burn::tensor::{Distribution, Tensor};
use burn::backend::{
    NdArray, Router, Wgpu, ndarray::NdArrayDevice, router::duo::MultiDevice, wgpu::WgpuDevice,
};

fn main() {
    type Backend = Router&amp;lt;(Wgpu, NdArray)&amp;gt;;

    let device_0 = MultiDevice::B1(WgpuDevice::DiscreteGpu(0));
    let device_1 = MultiDevice::B2(NdArrayDevice::Cpu);

    let tensor_gpu =
        Tensor::&amp;lt;Backend, 2&amp;gt;::random([3, 3], burn::tensor::Distribution::Default, &amp;amp;device_0);
    let tensor_cpu =
        Tensor::&amp;lt;Backend, 2&amp;gt;::random([3, 3], burn::tensor::Distribution::Default, &amp;amp;device_1);
}

&lt;/code&gt;&lt;/pre&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Remote (Beta): Backend decorator for remote backend execution, useful for distributed computations &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;That backend has two parts, one client and one server. The client sends tensor operations over the network to a remote compute backend. You can use any first-party backend as server in a single line of code:&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-rust"&gt;fn main_server() {
    // Start a server on port 3000.
    burn::server::start::&amp;lt;burn::backend::Cuda&amp;gt;(Default::default(), 3000);
}

fn main_client() {
    // Create a client that communicate with the server on port 3000.
    use burn::backend::{Autodiff, RemoteBackend};

    type Backend = Autodiff&amp;lt;RemoteDevice&amp;gt;;

    let device = RemoteDevice::new("ws://localhost:3000");
    let tensor_gpu =
        Tensor::&amp;lt;Backend, 2&amp;gt;::random([3, 3], Distribution::Default, &amp;amp;device);
}

&lt;/code&gt;&lt;/pre&gt; 
 &lt;/details&gt; 
 &lt;br&gt; 
 &lt;h2&gt;Training &amp;amp; Inference&lt;/h2&gt; 
 &lt;div align="left"&gt; 
  &lt;img align="right" src="https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-wall.png" height="96px"&gt; 
  &lt;p&gt;The whole deep learning workflow is made easy with Burn, as you can monitor your training progress with an ergonomic dashboard, and run inference everywhere from embedded devices to large GPU clusters.&lt;/p&gt; 
  &lt;p&gt;Burn was built from the ground up with training and inference in mind. It's also worth noting how Burn, in comparison to frameworks like PyTorch, simplifies the transition from training to deployment, eliminating the need for code changes.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div align="center"&gt; 
  &lt;br&gt; 
  &lt;a href="https://www.youtube.com/watch?v=N9RM5CQbNQc" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/tracel-ai/burn/main/assets/burn-train-tui.png" alt="Burn Train TUI" width="75%"&gt; &lt;/a&gt; 
 &lt;/div&gt; 
 &lt;br&gt; 
 &lt;p&gt;&lt;strong&gt;Click on the following sections to expand üëá&lt;/strong&gt;&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Training Dashboard üìà &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;As you can see in the previous video (click on the picture!), a new terminal UI dashboard based on the &lt;a href="https://github.com/ratatui-org/ratatui"&gt;Ratatui&lt;/a&gt; crate allows users to follow their training with ease without having to connect to any external application.&lt;/p&gt; 
  &lt;p&gt;You can visualize your training and validation metrics updating in real-time and analyze the lifelong progression or recent history of any registered metrics using only the arrow keys. Break from the training loop without crashing, allowing potential checkpoints to be fully written or important pieces of code to complete without interruption üõ°&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; ONNX Support üê´ &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;ONNX (Open Neural Network Exchange) is an open-standard format that exports both the architecture and the weights of a deep learning model.&lt;/p&gt; 
  &lt;p&gt;Burn supports the importation of models that follow the ONNX standard so you can easily port a model you have written in another framework like TensorFlow or PyTorch to Burn to benefit from all the advantages our framework offers.&lt;/p&gt; 
  &lt;p&gt;Our ONNX support is further described in &lt;a href="https://burn.dev/books/burn/import/onnx-model.html"&gt;this section of the Burn Book üî•&lt;/a&gt;.&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This crate is in active development and currently supports a &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/crates/burn-import/SUPPORTED-ONNX-OPS.md"&gt;limited set of ONNX operators&lt;/a&gt;.&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Importing PyTorch or Safetensors Models üöö &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;You can load weights from PyTorch or Safetensors formats directly into your Burn-defined models. This makes it easy to reuse existing models while benefiting from Burn's performance and deployment features.&lt;/p&gt; 
  &lt;p&gt;Learn more:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://burn.dev/books/burn/import/pytorch-model.html"&gt;Import pre-trained PyTorch models into Burn&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://burn.dev/books/burn/import/safetensors-model.html"&gt;Load models from Safetensors format&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Inference in the Browser üåê &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;Several of our backends can compile to Web Assembly: Candle and NdArray for CPU, and WGPU for GPU. This means that you can run inference directly within a browser. We provide several examples of this:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/mnist-inference-web"&gt;MNIST&lt;/a&gt; where you can draw digits and a small convnet tries to find which one it is! 2Ô∏è‚É£ 7Ô∏è‚É£ üò∞&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/image-classification-web"&gt;Image Classification&lt;/a&gt; where you can upload images and classify them! üåÑ&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Embedded: &lt;i&gt;no_std&lt;/i&gt; support ‚öôÔ∏è &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;Burn's core components support &lt;a href="https://docs.rust-embedded.org/book/intro/no-std.html"&gt;no_std&lt;/a&gt;. This means it can run in bare metal environment such as embedded devices without an operating system.&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;As of now, only the NdArray backend can be used in a &lt;em&gt;no_std&lt;/em&gt; environment.&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/details&gt; 
 &lt;br&gt; 
 &lt;h3&gt;Benchmarks&lt;/h3&gt; 
 &lt;p&gt;To evaluate performance across different backends and track improvements over time, we provide a dedicated benchmarking suite.&lt;/p&gt; 
 &lt;p&gt;Run and compare benchmarks using &lt;a href="https://github.com/tracel-ai/burn-bench"&gt;burn-bench&lt;/a&gt;.&lt;/p&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Warning&lt;/strong&gt; When using one of the &lt;code&gt;wgpu&lt;/code&gt; backends, you may encounter compilation errors related to recursive type evaluation. This is due to complex type nesting within the &lt;code&gt;wgpu&lt;/code&gt; dependency chain. To resolve this issue, add the following line at the top of your &lt;code&gt;main.rs&lt;/code&gt; or &lt;code&gt;lib.rs&lt;/code&gt; file:&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-rust"&gt;#![recursion_limit = "256"]
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;The default recursion limit (128) is often just below the required depth (typically 130-150) due to deeply nested associated types and trait bounds.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h2&gt;Getting Started&lt;/h2&gt; 
 &lt;div align="left"&gt; 
  &lt;img align="right" src="https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-walking.png" height="96px"&gt; 
  &lt;p&gt;Just heard of Burn? You are at the right place! Just continue reading this section and we hope you can get on board really quickly.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;details&gt; 
  &lt;summary&gt; The Burn Book üî• &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;To begin working effectively with Burn, it is crucial to understand its key components and philosophy. This is why we highly recommend new users to read the first sections of &lt;a href="https://burn.dev/books/burn/"&gt;The Burn Book üî•&lt;/a&gt;. It provides detailed examples and explanations covering every facet of the framework, including building blocks like tensors, modules, and optimizers, all the way to advanced usage, like coding your own GPU kernels.&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;The project is constantly evolving, and we try as much as possible to keep the book up to date with new additions. However, we might miss some details sometimes, so if you see something weird, let us know! We also gladly accept Pull Requests üòÑ&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Examples üôè &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;Let's start with a code snippet that shows how intuitive the framework is to use! In the following, we declare a neural network module with some parameters along with its forward pass.&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-rust"&gt;use burn::nn;
use burn::module::Module;
use burn::tensor::backend::Backend;

#[derive(Module, Debug)]
pub struct PositionWiseFeedForward&amp;lt;B: Backend&amp;gt; {
    linear_inner: nn::Linear&amp;lt;B&amp;gt;,
    linear_outer: nn::Linear&amp;lt;B&amp;gt;,
    dropout: nn::Dropout,
    gelu: nn::Gelu,
}

impl&amp;lt;B: Backend&amp;gt; PositionWiseFeedForward&amp;lt;B&amp;gt; {
    pub fn forward&amp;lt;const D: usize&amp;gt;(&amp;amp;self, input: Tensor&amp;lt;B, D&amp;gt;) -&amp;gt; Tensor&amp;lt;B, D&amp;gt; {
        let x = self.linear_inner.forward(input);
        let x = self.gelu.forward(x);
        let x = self.dropout.forward(x);

        self.linear_outer.forward(x)
    }
}
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;We have a somewhat large amount of &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples"&gt;examples&lt;/a&gt; in the repository that shows how to use the framework in different scenarios.&lt;/p&gt; 
  &lt;p&gt;Following &lt;a href="https://burn.dev/books/burn/"&gt;the book&lt;/a&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/guide"&gt;Basic Workflow&lt;/a&gt; : Creates a custom CNN &lt;code&gt;Module&lt;/code&gt; to train on the MNIST dataset and use for inference.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/custom-training-loop"&gt;Custom Training Loop&lt;/a&gt; : Implements a basic training loop instead of using the &lt;code&gt;Learner&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/custom-wgpu-kernel"&gt;Custom WGPU Kernel&lt;/a&gt; : Learn how to create your own custom operation with the WGPU backend.&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;Additional examples:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/custom-csv-dataset"&gt;Custom CSV Dataset&lt;/a&gt; : Implements a dataset to parse CSV data for a regression task.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/simple-regression"&gt;Regression&lt;/a&gt; : Trains a simple MLP on the California Housing dataset to predict the median house value for a district.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/custom-image-dataset"&gt;Custom Image Dataset&lt;/a&gt; : Trains a simple CNN on custom image dataset following a simple folder structure.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/custom-renderer"&gt;Custom Renderer&lt;/a&gt; : Implements a custom renderer to display the &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/building-blocks/learner.md"&gt;&lt;code&gt;Learner&lt;/code&gt;&lt;/a&gt; progress.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/image-classification-web"&gt;Image Classification Web&lt;/a&gt; : Image classification web browser demo using Burn, WGPU and WebAssembly.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/mnist-inference-web"&gt;MNIST Inference on Web&lt;/a&gt; : An interactive MNIST inference demo in the browser. The demo is available &lt;a href="https://burn.dev/demo/"&gt;online&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/mnist"&gt;MNIST Training&lt;/a&gt; : Demonstrates how to train a custom &lt;code&gt;Module&lt;/code&gt; (MLP) with the &lt;code&gt;Learner&lt;/code&gt; configured to log metrics and keep training checkpoints.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/named-tensor"&gt;Named Tensor&lt;/a&gt; : Performs operations with the experimental &lt;code&gt;NamedTensor&lt;/code&gt; feature.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/onnx-inference"&gt;ONNX Import Inference&lt;/a&gt; : Imports an ONNX model pre-trained on MNIST to perform inference on a sample image with Burn.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/pytorch-import"&gt;PyTorch Import Inference&lt;/a&gt; : Imports a PyTorch model pre-trained on MNIST to perform inference on a sample image with Burn.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/text-classification"&gt;Text Classification&lt;/a&gt; : Trains a text classification transformer model on the AG News or DbPedia dataset. The trained model can then be used to classify a text sample.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/text-generation"&gt;Text Generation&lt;/a&gt; : Trains a text generation transformer model on the DbPedia dataset.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/wgan"&gt;Wasserstein GAN MNIST&lt;/a&gt; : Trains a WGAN model to generate new handwritten digits based on MNIST.&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;For more practical insights, you can clone the repository and run any of them directly on your computer!&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Pre-trained Models ü§ñ &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;We keep an updated and curated list of models and examples built with Burn, see the &lt;a href="https://github.com/tracel-ai/models"&gt;tracel-ai/models repository&lt;/a&gt; for more details.&lt;/p&gt; 
  &lt;p&gt;Don't see the model you want? Don't hesitate to open an issue, and we may prioritize it. Built a model using Burn and want to share it? You can also open a Pull Request and add your model under the community section!&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Why use Rust for Deep Learning? ü¶Ä &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;Deep Learning is a special form of software where you need very high level abstractions as well as extremely fast execution time. Rust is the perfect candidate for that use case since it provides zero-cost abstractions to easily create neural network modules, and fine-grained control over memory to optimize every detail.&lt;/p&gt; 
  &lt;p&gt;It's important that a framework be easy to use at a high level so that its users can focus on innovating in the AI field. However, since running models relies so heavily on computations, performance can't be neglected.&lt;/p&gt; 
  &lt;p&gt;To this day, the mainstream solution to this problem has been to offer APIs in Python, but rely on bindings to low-level languages such as C/C++. This reduces portability, increases complexity and creates frictions between researchers and engineers. We feel like Rust's approach to abstractions makes it versatile enough to tackle this two languages dichotomy.&lt;/p&gt; 
  &lt;p&gt;Rust also comes with the Cargo package manager, which makes it incredibly easy to build, test, and deploy from any environment, which is usually a pain in Python.&lt;/p&gt; 
  &lt;p&gt;Although Rust has the reputation of being a difficult language at first, we strongly believe it leads to more reliable, bug-free solutions built faster (after some practice üòÖ)!&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;Deprecation Note&lt;/strong&gt;&lt;br&gt;Since &lt;code&gt;0.14.0&lt;/code&gt;, the internal structure for tensor data has changed. The previous &lt;code&gt;Data&lt;/code&gt; struct was deprecated and officially removed since &lt;code&gt;0.17.0&lt;/code&gt; in favor of the new &lt;code&gt;TensorData&lt;/code&gt; struct, which allows for more flexibility by storing the underlying data as bytes and keeping the data type as a field. If you are using &lt;code&gt;Data&lt;/code&gt; in your code, make sure to switch to &lt;code&gt;TensorData&lt;/code&gt;.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;!-- &gt;
&gt; In the event that you are trying to load a model record saved in a previous version, make sure to
&gt; enable the `record-backward-compat` feature using a previous version of burn (&lt;=0.16.0). Otherwise,
&gt; the record won't be deserialized correctly and you will get an error message (which will also point
&gt; you to the backward compatible feature flag). The backward compatibility was maintained for
&gt; deserialization (loading), so as soon as you have saved the record again it will be saved according
&gt; to the new structure and you will be able to upgrade to this version. Please note that binary formats
&gt; are not backward compatible. Thus, you will need to load your record in a previous version and save it
&gt; to another of the self-describing record formats before using a compatible version (as described) with the
&gt; `record-backward-compat` feature flag. --&gt; 
 &lt;details id="deprecation"&gt; 
  &lt;summary&gt; Loading Model Records From Previous Versions ‚ö†Ô∏è &lt;/summary&gt; 
  &lt;br&gt; 
  &lt;p&gt;In the event that you are trying to load a model record saved in a version older than &lt;code&gt;0.14.0&lt;/code&gt;, make sure to use a compatible version (&lt;code&gt;0.14&lt;/code&gt;, &lt;code&gt;0.15&lt;/code&gt; or &lt;code&gt;0.16&lt;/code&gt;) with the &lt;code&gt;record-backward-compat&lt;/code&gt; feature flag.&lt;/p&gt; 
  &lt;pre&gt;&lt;code&gt;features = [..., "record-backward-compat"]
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;Otherwise, the record won't be deserialized correctly and you will get an error message. This error will also point you to the backward compatible feature flag.&lt;/p&gt; 
  &lt;p&gt;The backward compatibility was maintained for deserialization when loading records. Therefore, as soon as you have saved the record again it will be saved according to the new structure and you can upgrade back to the current version&lt;/p&gt; 
  &lt;p&gt;Please note that binary formats are not backward compatible. Thus, you will need to load your record in a previous version and save it in any of the other self-describing record format (e.g., using the &lt;code&gt;NamedMpkFileRecorder&lt;/code&gt;) before using a compatible version (as described) with the &lt;code&gt;record-backward-compat&lt;/code&gt; feature flag.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;h2&gt;Community&lt;/h2&gt; 
 &lt;div align="left"&gt; 
  &lt;img align="right" src="https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-community.png" height="96px"&gt; 
  &lt;p&gt;If you are excited about the project, don't hesitate to join our &lt;a href="https://discord.gg/uPEBbYYDB6"&gt;Discord&lt;/a&gt;! We try to be as welcoming as possible to everybody from any background. You can ask your questions and share what you built with the community!&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;br&gt; 
 &lt;p&gt;&lt;strong&gt;Contributing&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Before contributing, please take a moment to review our &lt;a href="https://github.com/tracel-ai/burn/tree/main/CODE-OF-CONDUCT.md"&gt;code of conduct&lt;/a&gt;. It's also highly recommended to read the &lt;a href="https://github.com/tracel-ai/burn/tree/main/contributor-book/src/project-architecture"&gt;architecture overview&lt;/a&gt;, which explains some of our architectural decisions. Refer to our &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/CONTRIBUTING.md"&gt;contributing guide&lt;/a&gt; for more details.&lt;/p&gt; 
 &lt;h2&gt;Status&lt;/h2&gt; 
 &lt;p&gt;Burn is currently in active development, and there will be breaking changes. While any resulting issues are likely to be easy to fix, there are no guarantees at this stage.&lt;/p&gt; 
 &lt;h2&gt;License&lt;/h2&gt; 
 &lt;p&gt;Burn is distributed under the terms of both the MIT license and the Apache License (Version 2.0). See &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/LICENSE-APACHE"&gt;LICENSE-APACHE&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/LICENSE-MIT"&gt;LICENSE-MIT&lt;/a&gt; for details. Opening a pull request is assumed to signal agreement with these licensing terms.&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>paradedb/paradedb</title>
      <link>https://github.com/paradedb/paradedb</link>
      <description>&lt;p&gt;ParadeDB is a modern Elasticsearch alternative built on Postgres. Built for real-time, update-heavy workloads.&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; &lt;a href="https://paradedb.com"&gt;&lt;img src="https://raw.githubusercontent.com/paradedb/paradedb/main/docs/logo/readme.svg?sanitize=true" alt="ParadeDB"&gt;&lt;/a&gt; &lt;br&gt; &lt;/h1&gt; 
&lt;p align="center"&gt; &lt;b&gt;Postgres for Search and Analytics&lt;/b&gt;&lt;br&gt; &lt;/p&gt; 
&lt;h3 align="center"&gt; &lt;a href="https://paradedb.com"&gt;Website&lt;/a&gt; ‚Ä¢ &lt;a href="https://docs.paradedb.com"&gt;Docs&lt;/a&gt; ‚Ä¢ &lt;a href="https://join.slack.com/t/paradedbcommunity/shared_invite/zt-32abtyjg4-yoYoi~RPh9MSW8tDbl0BQw"&gt;Community&lt;/a&gt; ‚Ä¢ &lt;a href="https://paradedb.com/blog/"&gt;Blog&lt;/a&gt; ‚Ä¢ &lt;a href="https://docs.paradedb.com/changelog/"&gt;Changelog&lt;/a&gt; &lt;/h3&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;a href="https://artifacthub.io/packages/search?repo=paradedb"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/paradedb" alt="Artifact Hub"&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/paradedb/paradedb"&gt;&lt;img src="https://img.shields.io/docker/pulls/paradedb/paradedb" alt="Docker Pulls"&gt;&lt;/a&gt; &lt;a href="https://github.com/paradedb/paradedb?tab=AGPL-3.0-1-ov-file#readme"&gt;&lt;img src="https://img.shields.io/github/license/paradedb/paradedb?color=blue" alt="License"&gt;&lt;/a&gt; &lt;a href="https://join.slack.com/t/paradedbcommunity/shared_invite/zt-32abtyjg4-yoYoi~RPh9MSW8tDbl0BQw"&gt;&lt;img src="https://img.shields.io/badge/Join%20Slack-purple?logo=slack&amp;amp;link=https%3A%2F%2Fjoin.slack.com%2Ft%2Fparadedbcommunity%2Fshared_invite%2Fzt-32abtyjg4-yoYoi~RPh9MSW8tDbl0BQw" alt="Slack URL"&gt;&lt;/a&gt; &lt;a href="https://x.com/paradedb"&gt;&lt;img src="https://img.shields.io/twitter/url?url=https%3A%2F%2Ftwitter.com%2Fparadedb&amp;amp;label=Follow%20%40paradedb" alt="X URL"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://paradedb.com"&gt;ParadeDB&lt;/a&gt; is a modern Elasticsearch alternative built on Postgres. Built for real-time, update-heavy workloads.&lt;/p&gt; 
&lt;h2&gt;Get Started&lt;/h2&gt; 
&lt;p&gt;Please see our &lt;a href="https://docs.paradedb.com"&gt;documentation&lt;/a&gt; to get started. You'll also find our &lt;a href="https://docs.paradedb.com/welcome/architecture"&gt;architecture&lt;/a&gt; docs and &lt;a href="https://docs.paradedb.com/welcome/roadmap"&gt;public roadmap&lt;/a&gt; there.&lt;/p&gt; 
&lt;h2&gt;Deploying ParadeDB&lt;/h2&gt; 
&lt;p&gt;ParadeDB and its extensions can be deployed in one of two ways:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Docker image based on &lt;a href="https://hub.docker.com/_/postgres"&gt;Postgres&lt;/a&gt; (&lt;a href="https://docs.paradedb.com/deploy/aws"&gt;see deployment instructions&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Kubernetes Helm chart based on &lt;a href="https://artifacthub.io/packages/helm/cloudnative-pg/cloudnative-pg"&gt;CloudNativePG&lt;/a&gt; (&lt;a href="https://docs.paradedb.com/deploy/helm"&gt;see deployment instructions&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For more information, including enterprise features and support, please &lt;a href="mailto:sales@paradedb.com"&gt;contact us by email&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Extensions&lt;/h3&gt; 
&lt;p&gt;You can find prebuilt binaries for the ParadeDB Postgres extensions on Debian 11, 12, Ubuntu 22.04 and 24.04, Red Hat Enterprise Linux 8 and 9, and macOS 14 (Sonoma) and 15 (Sequoia) for Postgres 14+ in the &lt;a href="https://github.com/paradedb/paradedb/releases/latest"&gt;GitHub Releases&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;ParadeDB supports all versions supported by the PostgreSQL Global Development Group, which includes PostgreSQL 14+, and you can compile the extensions for other versions of Postgres by following the instructions in the respective extension's README.&lt;/p&gt; 
&lt;h3&gt;Docker Image&lt;/h3&gt; 
&lt;p&gt;To quickly get a ParadeDB instance up and running, simply pull and run the latest Docker image:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --name paradedb -e POSTGRES_PASSWORD=password paradedb/paradedb
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will start a ParadeDB instance with default user &lt;code&gt;postgres&lt;/code&gt; and password &lt;code&gt;password&lt;/code&gt;. You can then connect to the database using &lt;code&gt;psql&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker exec -it paradedb psql -U postgres
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To install ParadeDB locally or on-premise, we recommend using our &lt;code&gt;docker-compose.yml&lt;/code&gt; file. Alternatively, you can pass the appropriate environment variables to the &lt;code&gt;docker run&lt;/code&gt; command, replacing the &amp;lt;&amp;gt; with your desired values:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run \
  --name paradedb \
  -e POSTGRES_USER=&amp;lt;user&amp;gt; \
  -e POSTGRES_PASSWORD=&amp;lt;password&amp;gt; \
  -e POSTGRES_DB=&amp;lt;dbname&amp;gt; \
  -v paradedb_data:/var/lib/postgresql/data/ \
  -p 5432:5432 \
  -d \
  paradedb/paradedb:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will start a ParadeDB instance with non-root user &lt;code&gt;&amp;lt;user&amp;gt;&lt;/code&gt; and password &lt;code&gt;&amp;lt;password&amp;gt;&lt;/code&gt;. The &lt;code&gt;-v&lt;/code&gt; flag enables your ParadeDB data to persist across restarts in a Docker volume named &lt;code&gt;paradedb_data&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can then connect to the database using &lt;code&gt;psql&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker exec -it paradedb psql -U &amp;lt;user&amp;gt; -d &amp;lt;dbname&amp;gt; -p 5432 -W
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Helm Chart&lt;/h3&gt; 
&lt;p&gt;ParadeDB is also available for Kubernetes via our Helm chart. You can find our Helm chart in the &lt;a href="https://github.com/paradedb/charts"&gt;ParadeDB Helm Chart GitHub repository&lt;/a&gt; or download it directly from &lt;a href="https://artifacthub.io/packages/helm/paradedb/paradedb"&gt;Artifact Hub&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;ParadeDB Cloud&lt;/h3&gt; 
&lt;p&gt;At the moment, ParadeDB is not available as a managed cloud service. If you are interested in a ParadeDB Cloud service, please let us know by joining our &lt;a href="https://form.typeform.com/to/jHkLmIzx"&gt;waitlist&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;If you're missing a feature or have found a bug, please open a &lt;a href="https://github.com/paradedb/paradedb/issues/new/choose"&gt;GitHub Issue&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To get community support, you can:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Post a question in the &lt;a href="https://join.slack.com/t/paradedbcommunity/shared_invite/zt-32abtyjg4-yoYoi~RPh9MSW8tDbl0BQw"&gt;ParadeDB Slack Community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Ask for help on our &lt;a href="https://github.com/paradedb/paradedb/discussions"&gt;GitHub Discussions&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you need commercial support, please &lt;a href="mailto:sales@paradedb.com"&gt;contact the ParadeDB team&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome community contributions, big or small, and are here to guide you along the way. To get started contributing, check our &lt;a href="https://github.com/paradedb/paradedb/labels/good%20first%20issue"&gt;first timer issues&lt;/a&gt; or message us in the &lt;a href="https://join.slack.com/t/paradedbcommunity/shared_invite/zt-32abtyjg4-yoYoi~RPh9MSW8tDbl0BQw"&gt;ParadeDB Community Slack&lt;/a&gt;. Once you contribute, ping us in Slack and we'll send you some ParadeDB swag!&lt;/p&gt; 
&lt;p&gt;For more information on how to contribute, please see our &lt;a href="https://raw.githubusercontent.com/paradedb/paradedb/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This project is released with a &lt;a href="https://raw.githubusercontent.com/paradedb/paradedb/main/CODE_OF_CONDUCT.md"&gt;Contributor Code of Conduct&lt;/a&gt;. By participating in this project, you agree to follow its terms.&lt;/p&gt; 
&lt;p&gt;Thank you for helping us make ParadeDB better for everyone &lt;span&gt;‚ù§Ô∏è&lt;/span&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;ParadeDB is licensed under the &lt;a href="https://raw.githubusercontent.com/paradedb/paradedb/main/LICENSE"&gt;GNU Affero General Public License v3.0&lt;/a&gt; and as commercial software. For commercial licensing, please contact us at &lt;a href="mailto:sales@paradedb.com"&gt;sales@paradedb.com&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>espanso/espanso</title>
      <link>https://github.com/espanso/espanso</link>
      <description>&lt;p&gt;Cross-platform Text Expander written in Rust&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/espanso/espanso/dev/images/logo_extended.png" alt="espanso"&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A cross-platform Text Expander written in Rust&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/github/v/release/espanso/espanso" alt="GitHub release (latest by date)"&gt; &lt;img src="https://img.shields.io/badge/language-rust-orange" alt="Language"&gt; &lt;img src="https://img.shields.io/badge/platforms-Windows%2C%20macOS%20and%20Linux-blue" alt="Platforms"&gt; &lt;img src="https://img.shields.io/github/license/espanso/espanso" alt="License"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/espanso/espanso/dev/images/example.gif" alt="example"&gt;&lt;/p&gt; 
&lt;p&gt;Visit the &lt;a href="https://espanso.org"&gt;espanso website&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;What is a Text Expander?&lt;/h4&gt; 
&lt;p&gt;A &lt;em&gt;text expander&lt;/em&gt; is a program that detects when you type a specific &lt;strong&gt;keyword&lt;/strong&gt; and replaces it with &lt;strong&gt;something else&lt;/strong&gt;. This is useful in many ways:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Save a lot of typing&lt;/strong&gt;, expanding common sentences.&lt;/li&gt; 
 &lt;li&gt;Create &lt;strong&gt;system-wide&lt;/strong&gt; code snippets.&lt;/li&gt; 
 &lt;li&gt;Execute &lt;strong&gt;custom scripts&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Use &lt;strong&gt;emojis&lt;/strong&gt; like a pro.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Works on &lt;strong&gt;Windows&lt;/strong&gt;, &lt;strong&gt;macOS&lt;/strong&gt; and &lt;strong&gt;Linux&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Works with almost &lt;strong&gt;any program&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Works with &lt;strong&gt;Emojis&lt;/strong&gt; üòÑ&lt;/li&gt; 
 &lt;li&gt;Works with &lt;strong&gt;Images&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Includes a powerful &lt;strong&gt;Search Bar&lt;/strong&gt; üîé&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Date&lt;/strong&gt; expansion support&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Custom scripts&lt;/strong&gt; support&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Shell commands&lt;/strong&gt; support&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;App-specific&lt;/strong&gt; configurations&lt;/li&gt; 
 &lt;li&gt;Support &lt;a href="https://espanso.org/docs/matches/forms/"&gt;Forms&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Expandable with &lt;strong&gt;packages&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Built-in &lt;strong&gt;package manager&lt;/strong&gt; for &lt;a href="https://hub.espanso.org/"&gt;espanso hub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;File based configuration&lt;/li&gt; 
 &lt;li&gt;Support Regex triggers&lt;/li&gt; 
 &lt;li&gt;Experimental Wayland support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Get Started&lt;/h2&gt; 
&lt;p&gt;Visit the &lt;a href="https://espanso.org/docs/"&gt;official documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you need some help to setup espanso, want to ask a question or simply get involved in the community, you can &lt;a href="https://www.reddit.com/r/espanso/"&gt;join the official Subreddit&lt;/a&gt; or &lt;a href="https://discord.gg/DFcCNDg7bB"&gt;join the official Discord&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;Donations&lt;/h2&gt; 
&lt;p&gt;espanso is a free, open source software developed in my (little) spare time. If you liked the project and would like to support further development, please consider making a small donation, it really helps :)&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;amp;hosted_button_id=FHNLR5DRS267E&amp;amp;source=url"&gt;&lt;img src="https://raw.githubusercontent.com/espanso/espanso/dev/images/donate.gif" alt="Donate with PayPal"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;Many people helped the project along the way, thank you to all of you!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/espanso/espanso/graphs/contributors"&gt;&lt;img src="https://contrib.rocks/image?repo=espanso/espanso" alt="Image"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Remarks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Thanks to &lt;a href="https://github.com/jordansissel/xdotool"&gt;libxdo&lt;/a&gt; and &lt;a href="https://github.com/astrand/xclip"&gt;xclip&lt;/a&gt;, used to implement the Linux port.&lt;/li&gt; 
 &lt;li&gt;Thanks to &lt;a href="https://xkbcommon.org/"&gt;libxkbcommon&lt;/a&gt; and &lt;a href="https://github.com/bugaevc/wl-clipboard"&gt;wl-clipboard&lt;/a&gt;, used to implement the Wayland port.&lt;/li&gt; 
 &lt;li&gt;Thanks to &lt;a href="https://www.wxwidgets.org/"&gt;wxWidgets&lt;/a&gt; for providing a powerful cross-platform GUI library.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;espanso was created by &lt;a href="http://federicoterzi.com"&gt;Federico Terzi&lt;/a&gt; and is licensed under the &lt;a href="https://raw.githubusercontent.com/espanso/espanso/dev/LICENSE"&gt;GPL-3.0 license&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>linera-io/linera-protocol</title>
      <link>https://github.com/linera-io/linera-protocol</link>
      <description>&lt;p&gt;Main repository for the Linera protocol&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src="https://github.com/linera-io/linera-protocol/assets/1105398/fe08c941-93af-4114-bb83-bcc0eaec95f9" width="250" height="85"&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/linera-io/linera-protocol/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/linera-io/linera-protocol" alt="License"&gt;&lt;/a&gt; &lt;a href="https://github.com/linera-io/linera-protocol/actions/workflows/docker-compose.yml"&gt;&lt;img src="https://github.com/linera-io/linera-protocol/actions/workflows/docker-compose.yml/badge.svg?sanitize=true" alt="Build Status for Docker"&gt;&lt;/a&gt; &lt;a href="https://github.com/linera-io/linera-protocol/actions/workflows/rust.yml"&gt;&lt;img src="https://github.com/linera-io/linera-protocol/actions/workflows/rust.yml/badge.svg?sanitize=true" alt="Build Status for Rust"&gt;&lt;/a&gt; &lt;a href="https://github.com/linera-io/linera-protocol/actions/workflows/documentation.yml"&gt;&lt;img src="https://github.com/linera-io/linera-protocol/actions/workflows/documentation.yml/badge.svg?sanitize=true" alt="Build Status for Documentation"&gt;&lt;/a&gt; &lt;a href="https://x.com/linera_io"&gt;&lt;img src="https://img.shields.io/twitter/follow/linera_io" alt="Twitter"&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/linera"&gt;&lt;img src="https://img.shields.io/discord/984941796272521226" alt="Discord"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- [![Build Status for Kubernetes](https://github.com/linera-io/linera-protocol/actions/workflows/kubernetes.yml/badge.svg)](https://github.com/linera-io/linera-protocol/actions/workflows/kubernetes.yml) --&gt; 
&lt;p&gt;&lt;a href="https://linera.io"&gt;Linera&lt;/a&gt; is a decentralized blockchain infrastructure designed for highly scalable, secure, low-latency Web3 applications.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Visit our &lt;a href="https://linera.dev"&gt;developer page&lt;/a&gt; and read our &lt;a href="https://linera.io/whitepaper"&gt;whitepaper&lt;/a&gt; to learn more about the Linera protocol.&lt;/p&gt; 
&lt;h2&gt;Repository Structure&lt;/h2&gt; 
&lt;p&gt;The main crates and directories of this repository can be summarized as follows: (listed from low to high levels in the dependency graph)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://linera-io.github.io/linera-protocol/linera_base/index.html"&gt;&lt;code&gt;linera-base&lt;/code&gt;&lt;/a&gt; Base definitions, including cryptography.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://linera-io.github.io/linera-protocol/linera_version/index.html"&gt;&lt;code&gt;linera-version&lt;/code&gt;&lt;/a&gt; A library to manage version info in binaries and services.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://linera-io.github.io/linera-protocol/linera_views/index.html"&gt;&lt;code&gt;linera-views&lt;/code&gt;&lt;/a&gt; A library mapping complex data structures onto a key-value store. The corresponding procedural macros are implemented in &lt;code&gt;linera-views-derive&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://linera-io.github.io/linera-protocol/linera_execution/index.html"&gt;&lt;code&gt;linera-execution&lt;/code&gt;&lt;/a&gt; Persistent data and the corresponding logic for runtime and execution of Linera applications.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://linera-io.github.io/linera-protocol/linera_chain/index.html"&gt;&lt;code&gt;linera-chain&lt;/code&gt;&lt;/a&gt; Persistent data and the corresponding logic for chains of blocks, certificates, and cross-chain messaging.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://linera-io.github.io/linera-protocol/linera_storage/index.html"&gt;&lt;code&gt;linera-storage&lt;/code&gt;&lt;/a&gt; Defines the storage abstractions for the protocol on top of &lt;code&gt;linera-chain&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://linera-io.github.io/linera-protocol/linera_core/index.html"&gt;&lt;code&gt;linera-core&lt;/code&gt;&lt;/a&gt; The core Linera protocol, including client and server logic, node synchronization, etc.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://linera-io.github.io/linera-protocol/linera_rpc/index.html"&gt;&lt;code&gt;linera-rpc&lt;/code&gt;&lt;/a&gt; Defines the data-type for RPC messages (currently all client ‚Üî proxy ‚Üî chain ‚Üî chain interactions), and track the corresponding data schemas.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://linera-io.github.io/linera-protocol/linera_client/index.html"&gt;&lt;code&gt;linera-client&lt;/code&gt;&lt;/a&gt; Library for writing Linera clients. Used for the command-line client and the node service in &lt;code&gt;linera-service&lt;/code&gt;, as well as the Web client in &lt;a href="https://github.com/linera-io/linera-web/"&gt;&lt;code&gt;linera-web&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://linera-io.github.io/linera-protocol/linera_service/index.html"&gt;&lt;code&gt;linera-service&lt;/code&gt;&lt;/a&gt; Executable for clients (aka CLI wallets), proxy (aka validator frontend) and servers.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://linera-io.github.io/linera-protocol/linera_sdk/index.html"&gt;&lt;code&gt;linera-sdk&lt;/code&gt;&lt;/a&gt; The library to develop Linera applications written in Rust for the Wasm virtual machine. The corresponding procedural macros are implemented in &lt;code&gt;linera-sdk-derive&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/linera-io/linera-protocol/main/examples"&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt; Examples of Linera applications written in Rust.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/linera-io/linera-protocol/main/INSTALL.md"&gt;&lt;code&gt;INSTALL.md&lt;/code&gt;&lt;/a&gt; for software requirements to develop in this repo.&lt;/p&gt; 
&lt;h2&gt;Quickstart with the Linera CLI tool&lt;/h2&gt; 
&lt;p&gt;The following commands set up a local test network and run some transfers between the microchains owned by a single wallet.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Make sure to compile the Linera binaries and add them in the $PATH.
# cargo build -p linera-storage-service -p linera-service --bins
export PATH="$PWD/target/debug:$PATH"

# Import the optional helper function `linera_spawn`.
source /dev/stdin &amp;lt;&amp;lt;&amp;lt;"$(linera net helper 2&amp;gt;/dev/null)"

# Run a local test network with the default parameters and a number of microchains
# owned by the default wallet. This also defines `LINERA_TMP_DIR`.
linera_spawn \
linera net up --with-faucet --faucet-port 8080

# Remember the URL of the faucet.
FAUCET_URL=http://localhost:8080

# If you're using a testnet, start here and run this instead:
#   LINERA_TMP_DIR=$(mktemp -d)
#   FAUCET_URL=https://faucet.testnet-XXX.linera.net  # for some value XXX

# Set the path of the future wallet.
export LINERA_WALLET="$LINERA_TMP_DIR/wallet.json"
export LINERA_KEYSTORE="$LINERA_TMP_DIR/keystore.json"
export LINERA_STORAGE="rocksdb:$LINERA_TMP_DIR/client.db"

# Initialize a new user wallet.
linera wallet init --faucet $FAUCET_URL

# Request chains.
INFO1=($(linera wallet request-chain --faucet $FAUCET_URL))
INFO2=($(linera wallet request-chain --faucet $FAUCET_URL))
CHAIN1="${INFO1[0]}"
ACCOUNT1="${INFO1[1]}"
CHAIN2="${INFO2[0]}"
ACCOUNT2="${INFO2[1]}"

# Show the different chains tracked by the wallet.
linera wallet show

# Query the chain balance of some of the chains.
linera query-balance "$CHAIN1"
linera query-balance "$CHAIN2"

# Transfer 10 units then 5 back.
linera transfer 10 --from "$CHAIN1" --to "$CHAIN2"
linera transfer 5 --from "$CHAIN2" --to "$CHAIN1"

# Query balances again.
linera query-balance "$CHAIN1"
linera query-balance "$CHAIN2"

# Now let's fund the user balances.
linera transfer 5 --from "$CHAIN1" --to "$CHAIN1:$ACCOUNT1"
linera transfer 2 --from "$CHAIN1:$ACCOUNT1" --to "$CHAIN2:$ACCOUNT2"

# Query user balances again.
linera query-balance "$CHAIN1:$ACCOUNT1"
linera query-balance "$CHAIN2:$ACCOUNT2"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;More complex examples may be found in our &lt;a href="https://linera.dev"&gt;developer manual&lt;/a&gt; as well as the &lt;a href="https://raw.githubusercontent.com/linera-io/linera-protocol/main/examples"&gt;example applications&lt;/a&gt; in this repository.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! If you'd like to contribute to the Linera protocol:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repository&lt;/li&gt; 
 &lt;li&gt;Create a feature branch (&lt;code&gt;git checkout -b feature/amazing-feature&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Commit your changes (&lt;code&gt;git commit -m 'Add some amazing feature'&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Push to the branch (&lt;code&gt;git push origin feature/amazing-feature&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Open a Pull Request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;For detailed guidelines, see our &lt;a href="https://raw.githubusercontent.com/linera-io/linera-protocol/main/CONTRIBUTING.md"&gt;contribution guide&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>vercel/turborepo</title>
      <link>https://github.com/vercel/turborepo</link>
      <description>&lt;p&gt;Build system optimized for JavaScript¬†and TypeScript, written in Rust&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://turborepo.com"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://user-images.githubusercontent.com/4060187/196936123-f6e1db90-784d-4174-b774-92502b718836.png"&gt; 
   &lt;img src="https://user-images.githubusercontent.com/4060187/196936104-5797972c-ab10-4834-bd61-0d1e5f442c9c.png" height="128"&gt; 
  &lt;/picture&gt; &lt;/a&gt;&lt;/p&gt;
&lt;h1 align="center"&gt;&lt;a href="https://turborepo.com"&gt;Turborepo&lt;/a&gt;&lt;/h1&gt;
&lt;a href="https://turborepo.com"&gt; &lt;/a&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a aria-label="Vercel logo" href="https://vercel.com/"&gt;&lt;img src="https://img.shields.io/badge/MADE%20BY%20Vercel-000000.svg?style=for-the-badge&amp;amp;logo=Vercel&amp;amp;labelColor=000"&gt;&lt;/a&gt; &lt;a aria-label="NPM version" href="https://www.npmjs.com/package/turbo"&gt;&lt;img alt="" src="https://img.shields.io/npm/v/turbo.svg?style=for-the-badge&amp;amp;labelColor=000000"&gt;&lt;/a&gt; &lt;a aria-label="License" href="https://github.com/vercel/turborepo/raw/main/LICENSE"&gt;&lt;img alt="" src="https://img.shields.io/npm/l/turbo.svg?style=for-the-badge&amp;amp;labelColor=000000&amp;amp;color="&gt;&lt;/a&gt; &lt;a aria-label="Join the community on GitHub" href="https://github.com/vercel/turborepo/discussions"&gt;&lt;img alt="" src="https://img.shields.io/badge/Join%20the%20community-blueviolet.svg?style=for-the-badge&amp;amp;logo=turborepo&amp;amp;labelColor=000000&amp;amp;logoWidth=20&amp;amp;logoColor=white"&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;Turborepo is a high-performance build system for JavaScript and TypeScript codebases, written in Rust.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Visit &lt;a href="https://turborepo.com"&gt;https://turborepo.com&lt;/a&gt; to get started with Turborepo.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/vercel/turborepo/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;The Turborepo community can be found on &lt;a href="https://github.com/vercel/turborepo/discussions"&gt;GitHub Discussions&lt;/a&gt;, where you can ask questions, voice ideas, and share your projects.&lt;/p&gt; 
&lt;p&gt;To chat with other community members, you can join &lt;a href="https://vercel.community/tag/turborepo"&gt;Vercel Community's &lt;code&gt;#turborepo&lt;/code&gt; tag&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Our &lt;a href="https://github.com/vercel/turborepo/raw/main/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt; applies to all Turborepo community channels.&lt;/p&gt; 
&lt;h2&gt;Who is using Turborepo?&lt;/h2&gt; 
&lt;p&gt;Turborepo is used by the world's leading companies. Check out the &lt;a href="https://turborepo.com/showcase"&gt;Turborepo Showcase&lt;/a&gt; to learn more.&lt;/p&gt; 
&lt;h2&gt;Updates&lt;/h2&gt; 
&lt;p&gt;Follow &lt;a href="https://x.com/turborepo"&gt;@turborepo&lt;/a&gt; on X for project updates.&lt;/p&gt; 
&lt;h2&gt;Authors&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Turborepo&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Jared Palmer (&lt;a href="https://x.com/jaredpalmer"&gt;@jaredpalmer&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Security&lt;/h2&gt; 
&lt;p&gt;If you believe you have found a security vulnerability in Turborepo, we encourage you to responsibly disclose this and not open a public issue. We will investigate all legitimate reports. Email &lt;code&gt;security@vercel.com&lt;/code&gt; to disclose any security vulnerabilities.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://vercel.com/security"&gt;https://vercel.com/security&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>raphamorim/rio</title>
      <link>https://github.com/raphamorim/rio</link>
      <description>&lt;p&gt;A hardware-accelerated GPU terminal emulator focusing to run in desktops and browsers.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt; &lt;p align="center"&gt; &lt;img src="https://rioterm.com/assets/rio-logo.png" alt="Rio terminal logo" width="128"&gt; &lt;br&gt;Rio Terminal &lt;/p&gt;&lt;/h1&gt; 
&lt;p align="center"&gt; Rio is a modern terminal built to run everywhere. &lt;br&gt; &lt;a href="https://raw.githubusercontent.com/raphamorim/rio/main/#about"&gt;About&lt;/a&gt; ¬∑ &lt;a href="https://rioterm.com/docs/install"&gt;Install&lt;/a&gt; ¬∑ &lt;a href="https://rioterm.com/docs/config"&gt;Config&lt;/a&gt; ¬∑ &lt;a href="https://rioterm.com/docs/releases"&gt;Changelog&lt;/a&gt; ¬∑ &lt;a href="https://github.com/sponsors/raphamorim"&gt;Sponsor&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;‚ö†Ô∏è Development Notice&lt;/strong&gt;: We are currently in the process of releasing Rio 0.3.0, which includes major performance improvements and architectural changes. The main branch is under active development and may be unstable. For stable usage, please use the &lt;a href="https://github.com/raphamorim/rio/releases"&gt;latest release&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;About&lt;/h2&gt; 
&lt;p&gt;Documentation: &lt;a href="https://rioterm.com"&gt;https://rioterm.com&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;If you are using or want to help in any way please consider to donate via &lt;a href="https://github.com/sponsors/raphamorim"&gt;Github Sponsors&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Rio would not be possible without &lt;a href="https://github.com/alacritty/alacritty/"&gt;Alacritty&lt;/a&gt;, since a lot of Rio functionalities (e.g: ANSI parser, events, processor) was originally written (and still uses a good amount) of Alacritty code.&lt;/p&gt; 
&lt;h2&gt;Supporting the Project&lt;/h2&gt; 
&lt;p&gt;If you use and like Rio, please consider sponsoring it: your support helps to cover the fees required to maintain the project and to validate the time spent working on it!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/sponsors/raphamorim"&gt;&lt;img src="https://img.shields.io/github/sponsors/raphamorim?label=Sponsor%20Rio&amp;amp;logo=github&amp;amp;style=for-the-badge" alt="Sponsor Rio terminal"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Packaging&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://repology.org/project/rio-terminal/versions"&gt;&lt;img src="https://repology.org/badge/vertical-allrepos/rio-terminal.svg?columns=3" alt="Packaging status"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Platforms&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Name&lt;/th&gt; 
   &lt;th&gt;Details&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MacOs &lt;em&gt;as desktop application&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rioterm.com/docs/install/macos"&gt;Installation guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Linux &lt;em&gt;as desktop application&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rioterm.com/docs/install/linux"&gt;Installation guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows &lt;em&gt;as desktop application&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rioterm.com/docs/install/windows"&gt;Installation guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Web Browser &lt;em&gt;(WebAssembly)&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;(Sugarloaf is ready but Rio still need to be ported)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Demo with split and CRT on MacOS&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/raphamorim/rio/main/docs/static/assets/posts/0.2.0/demo-rio.png" alt="Demo Rio 0.2.0 on MacOS"&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Demo with blurred background on Linux&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/raphamorim/rio/main/docs/static/assets/demos/demos-nixos-blur.png" alt="Demo blurred background"&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Demo of Rio running on a Steam Deck&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/raphamorim/rio/main/docs/static/assets/demos/demo-flatpak-steamdeck.jpg" alt="Demo of Rio running on a Steam Deck"&gt;&lt;/p&gt; 
&lt;h2&gt;Minimal stable rust version&lt;/h2&gt; 
&lt;p&gt;Rio's MSRV is 1.87.0.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>antinomyhq/forge</title>
      <link>https://github.com/antinomyhq/forge</link>
      <description>&lt;p&gt;AI enabled pair programmer for Claude, GPT, O Series, Grok, Deepseek, Gemini and 300+ models&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt;‚öíÔ∏è Forge: AI-Enhanced Terminal Development Environment&lt;/h1&gt; 
&lt;p align="center"&gt;A comprehensive coding agent that integrates AI capabilities with your development environment&lt;/p&gt; 
&lt;p align="center"&gt;&lt;code&gt;npx forgecode@latest&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/antinomyhq/forge/actions"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/antinomyhq/forge/ci.yml?style=for-the-badge" alt="CI Status"&gt;&lt;/a&gt; &lt;a href="https://github.com/antinomyhq/forge/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/antinomyhq/forge?style=for-the-badge" alt="GitHub Release"&gt;&lt;/a&gt; &lt;a href="https://discord.gg/kRZBPpkgwq"&gt;&lt;img src="https://img.shields.io/discord/1044859667798568962?style=for-the-badge&amp;amp;cacheSeconds=120&amp;amp;logo=discord" alt="Discord"&gt;&lt;/a&gt; &lt;a href="https://cla-assistant.io/antinomyhq/forge"&gt;&lt;img src="https://cla-assistant.io/readme/badge/antinomyhq/forge?style=for-the-badge" alt="CLA assistant"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://assets.antinomy.ai/images/forge_demo_2x.gif" alt="Code-Forge Demo"&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Table&amp;nbsp;of&amp;nbsp;Contents&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/antinomyhq/forge/main/#quickstart"&gt;Quickstart&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/antinomyhq/forge/main/#usage-examples"&gt;Usage Examples&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/antinomyhq/forge/main/#why-forge"&gt;Why Forge?&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/antinomyhq/forge/main/#command-line-options"&gt;Command-Line Options&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/antinomyhq/forge/main/#advanced-configuration"&gt;Advanced Configuration&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/antinomyhq/forge/main/#provider-configuration"&gt;Provider Configuration&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/antinomyhq/forge/main/#forgeyaml-configuration-options"&gt;forge.yaml Configuration Options&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/antinomyhq/forge/main/#mcp-configuration"&gt;MCP Configuration&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/antinomyhq/forge/main/#example-use-cases"&gt;Example Use Cases&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/antinomyhq/forge/main/#usage-in-multi-agent-workflows"&gt;Usage in Multi-Agent Workflows&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/antinomyhq/forge/main/#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/antinomyhq/forge/main/#community"&gt;Community&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/antinomyhq/forge/main/#support-us"&gt;Support Us&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;Run Forge in interactive mode via npx&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx forgecode@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Connect through the Forge app and complete the OAuth process. This will open your browser to app.forgecode.dev where you can sign up or sign in with Google/GitHub.&lt;/p&gt; 
&lt;p&gt;That's it! Forge is now ready to assist you with your development tasks.&lt;/p&gt; 
&lt;h2&gt;Usage Examples&lt;/h2&gt; 
&lt;p&gt;Forge can be used in different ways depending on your needs. Here are some common usage patterns:&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Code Understanding&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;&amp;gt; Can you explain how the authentication system works in this codebase?
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Forge will analyze your project's structure, identify authentication-related files, and provide a detailed explanation of the authentication flow, including the relationships between different components.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Implementing New Features&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;&amp;gt; I need to add a dark mode toggle to our React application. How should I approach this?
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Forge will suggest the best approach based on your current codebase, explain the steps needed, and even scaffold the necessary components and styles for you.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Debugging Assistance&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;&amp;gt; I'm getting this error: "TypeError: Cannot read property 'map' of undefined". What might be causing it?
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Forge will analyze the error, suggest potential causes based on your code, and propose different solutions to fix the issue.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Code Reviews&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;&amp;gt; Please review the code in src/components/UserProfile.js and suggest improvements
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Forge will analyze the code, identify potential issues, and suggest improvements for readability, performance, security, and maintainability.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Learning New Technologies&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;&amp;gt; I want to integrate GraphQL into this Express application. Can you explain how to get started?
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Forge will provide a tailored tutorial on integrating GraphQL with Express, using your specific project structure as context.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Database Schema Design&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;&amp;gt; I need to design a database schema for a blog with users, posts, comments, and categories
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Forge will suggest an appropriate schema design, including tables/collections, relationships, indexes, and constraints based on your project's existing database technology.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Refactoring Legacy Code&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;&amp;gt; Help me refactor this class-based component to use React Hooks
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Forge can help modernize your codebase by walking you through refactoring steps and implementing them with your approval.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Git Operations&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;&amp;gt; I need to merge branch 'feature/user-profile' into main but there are conflicts
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Forge can guide you through resolving git conflicts, explaining the differences and suggesting the best way to reconcile them.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Why Forge?&lt;/h2&gt; 
&lt;p&gt;Forge is designed for developers who want to enhance their workflow with AI assistance while maintaining full control over their development environment.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Zero configuration&lt;/strong&gt; - Just add your API key and you're ready to go&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Seamless integration&lt;/strong&gt; - Works right in your terminal, where you already work&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-provider support&lt;/strong&gt; - Use OpenAI, Anthropic, or other LLM providers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Secure by design&lt;/strong&gt; - Your code stays on your machine&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open-source&lt;/strong&gt; - Transparent, extensible, and community-driven&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Forge helps you code faster, solve complex problems, and learn new technologies without leaving your terminal.&lt;/p&gt; 
&lt;h2&gt;Command-Line Options&lt;/h2&gt; 
&lt;p&gt;Here's a quick reference of Forge's command-line options:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Option&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-p, --prompt &amp;lt;PROMPT&amp;gt;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Direct prompt to process without entering interactive mode&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-c, --command &amp;lt;COMMAND&amp;gt;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to a file containing initial commands to execute&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-w, --workflow &amp;lt;WORKFLOW&amp;gt;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to a file containing the workflow to execute&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-e, --event &amp;lt;EVENT&amp;gt;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Dispatch an event to the workflow&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--conversation &amp;lt;CONVERSATION&amp;gt;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to a file containing the conversation to execute&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-r, --restricted&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enable restricted shell mode for enhanced security&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--verbose&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enable verbose output mode&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-h, --help&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Print help information&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-V, --version&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Print version&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Advanced Configuration&lt;/h2&gt; 
&lt;h3&gt;Provider Configuration&lt;/h3&gt; 
&lt;p&gt;Forge supports multiple AI providers. Below are setup instructions for each supported provider:&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;forgecode.dev (Recommended)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# .env
FORGE_KEY=ForgeKey
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To use Forgecode's provider with Forge:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Visit &lt;a href="https://app.forgecode.dev/"&gt;https://app.forgecode.dev/&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Login with your existing credentials or create a new account&lt;/li&gt; 
  &lt;li&gt;Once logged in, your account will automatically enable the Forge Provider&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;em&gt;No changes in &lt;code&gt;forge.yaml&lt;/code&gt; required&lt;/em&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;OpenRouter&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# .env
OPENROUTER_API_KEY=&amp;lt;your_openrouter_api_key&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;em&gt;No changes in &lt;code&gt;forge.yaml&lt;/code&gt; required&lt;/em&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Requesty&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# .env
REQUESTY_API_KEY=&amp;lt;your_requesty_api_key&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;em&gt;No changes in &lt;code&gt;forge.yaml&lt;/code&gt; required&lt;/em&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;x-ai&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# .env
XAI_API_KEY=&amp;lt;your_xai_api_key&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;switch the model using &lt;code&gt;/model&lt;/code&gt; command in the Forge CLI.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;OpenAI&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# .env
OPENAI_API_KEY=&amp;lt;your_openai_api_key&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;# forge.yaml
model: o3-mini-high
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Anthropic&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# .env
ANTHROPIC_API_KEY=&amp;lt;your_anthropic_api_key&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;# forge.yaml
model: claude-3.7-sonnet
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Google Vertex AI&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# .env
PROJECT_ID=&amp;lt;your_project_id&amp;gt;
LOCATION=&amp;lt;your_location&amp;gt;
OPENAI_API_KEY=&amp;lt;vertex_ai_key&amp;gt;
OPENAI_URL=https://${LOCATION}-aiplatform.googleapis.com/v1beta1/projects/${PROJECT_ID}/locations/${LOCATION}/endpoints/openapi
&lt;/code&gt;&lt;/pre&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;# forge.yaml
model: publishers/anthropic/models/claude-3-7-sonnet
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;OpenAI-Compatible Providers&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# .env
OPENAI_API_KEY=&amp;lt;your_provider_api_key&amp;gt;
OPENAI_URL=&amp;lt;your_provider_url&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;# forge.yaml
model: &amp;lt;provider-specific-model&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Groq&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# .env
OPENAI_API_KEY=&amp;lt;your_groq_api_key&amp;gt;
OPENAI_URL=https://api.groq.com/openai/v1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;# forge.yaml
model: deepseek-r1-distill-llama-70b
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Amazon Bedrock&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;To use Amazon Bedrock models with Forge, you'll need to first set up the &lt;a href="https://github.com/aws-samples/bedrock-access-gateway"&gt;Bedrock Access Gateway&lt;/a&gt;:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Set up Bedrock Access Gateway&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Follow the deployment steps in the &lt;a href="https://github.com/aws-samples/bedrock-access-gateway"&gt;Bedrock Access Gateway repo&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Create your own API key in Secrets Manager&lt;/li&gt; 
    &lt;li&gt;Deploy the CloudFormation stack&lt;/li&gt; 
    &lt;li&gt;Note your API Base URL from the CloudFormation outputs&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Create these files in your project directory&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# .env
OPENAI_API_KEY=&amp;lt;your_bedrock_gateway_api_key&amp;gt;
OPENAI_URL=&amp;lt;your_bedrock_gateway_base_url&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class="language-yaml"&gt;# forge.yaml
model: anthropic.claude-3-opus
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt;
 &lt;/ol&gt;
&lt;/details&gt;   
&lt;h3&gt;forge.yaml Configuration Options&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;forge.yaml&lt;/code&gt; file supports several advanced configuration options that let you customize Forge's behavior.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Custom Rules&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Add your own guidelines that all agents should follow when generating responses.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;# forge.yaml
custom_rules: |
  1. Always add comprehensive error handling to any code you write.
  2. Include unit tests for all new functions.
  3. Follow our team's naming convention: camelCase for variables, PascalCase for classes.
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Commands&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Define custom commands as shortcuts for repetitive prompts:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;# forge.yaml
commands:
  - name: 'refactor'
    description: 'Refactor selected code'
    prompt: 'Please refactor this code to improve readability and performance'
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Specify the default AI model to use for all agents in the workflow.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;# forge.yaml
model: 'claude-3.7-sonnet'
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Max Walker Depth&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Control how deeply Forge traverses your project directory structure when gathering context.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;# forge.yaml
max_walker_depth: 3 # Limit directory traversal to 3 levels deep
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Temperature&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Adjust the creativity and randomness in AI responses. Lower values (0.0-0.3) produce more focused, deterministic outputs, while higher values (0.7-2.0) generate more diverse and creative results.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;# forge.yaml
temperature: 0.7 # Balanced creativity and focus
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Tool Max Failure Limit&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Control how many times a tool can fail before Forge forces completion to prevent infinite retry loops. This helps avoid situations where an agent gets stuck repeatedly trying the same failing operation.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;# forge.yaml
max_tool_failure_per_turn: 3 # Allow up to 3 failures per tool before forcing completion
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Set to a higher value if you want more retry attempts, or lower if you want faster failure detection.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Max Requests Per Turn&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Limit the maximum number of requests an agent can make in a single conversation turn. This prevents runaway conversations and helps control API usage and costs.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;# forge.yaml
max_requests_per_turn: 50 # Allow up to 50 requests per turn
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;When this limit is reached, Forge will:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Ask you if you wish to continue&lt;/li&gt; 
  &lt;li&gt;If you respond with 'Yes', it will continue the conversation&lt;/li&gt; 
  &lt;li&gt;If you respond with 'No', it will end the conversation&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;The MCP feature allows AI agents to communicate with external tools and services. This implementation follows Anthropic's &lt;a href="https://docs.anthropic.com/en/docs/claude-code/tutorials#set-up-model-context-protocol-mcp"&gt;Model Context Protocol&lt;/a&gt; design.&lt;/p&gt; 
 &lt;h3&gt;MCP Configuration&lt;/h3&gt; 
 &lt;p&gt;Configure MCP servers using the CLI:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# List all MCP servers
forge mcp list

# Add a new server
forge mcp add

# Add a server using JSON format
forge mcp add-json

# Get server details
forge mcp get

# Remove a server
forge mcp remove
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Or manually create a &lt;code&gt;.mcp.json&lt;/code&gt; file with the following structure:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
	"mcpServers": {
		"server_name": {
			"command": "command_to_execute",
			"args": ["arg1", "arg2"],
			"env": { "ENV_VAR": "value" }
		},
		"another_server": {
			"url": "http://localhost:3000/events"
		}
	}
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;MCP configurations are read from two locations (in order of precedence):&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Local configuration (project-specific)&lt;/li&gt; 
  &lt;li&gt;User configuration (user-specific)&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;Example Use Cases&lt;/h3&gt; 
 &lt;p&gt;MCP can be used for various integrations:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Web browser automation&lt;/li&gt; 
  &lt;li&gt;External API interactions&lt;/li&gt; 
  &lt;li&gt;Tool integration&lt;/li&gt; 
  &lt;li&gt;Custom service connections&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;Usage in Multi-Agent Workflows&lt;/h3&gt; 
 &lt;p&gt;MCP tools can be used as part of multi-agent workflows, allowing specialized agents to interact with external systems as part of a collaborative problem-solving approach.&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;For comprehensive documentation on all features and capabilities, please visit the &lt;a href="https://github.com/antinomyhq/forge/tree/main/docs"&gt;documentation site&lt;/a&gt;.&lt;/p&gt; 
&lt;hr&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Join our vibrant Discord community to connect with other Forge users and contributors, get help with your projects, share ideas, and provide feedback!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/kRZBPpkgwq"&gt;&lt;img src="https://img.shields.io/discord/1044859667798568962?style=for-the-badge&amp;amp;cacheSeconds=120&amp;amp;logo=discord" alt="Discord"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;h2&gt;Support Us&lt;/h2&gt; 
&lt;p&gt;Your support drives Forge's continued evolution! By starring our GitHub repository, you:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Help others discover this powerful tool üîç&lt;/li&gt; 
 &lt;li&gt;Motivate our development team üí™&lt;/li&gt; 
 &lt;li&gt;Enable us to prioritize new features üõ†Ô∏è&lt;/li&gt; 
 &lt;li&gt;Strengthen our open-source community üå±&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>qdrant/qdrant</title>
      <link>https://github.com/qdrant/qdrant</link>
      <description>&lt;p&gt;Qdrant - High-performance, massive-scale Vector Database and Vector Search Engine for the next generation of AI. Also available in the cloud https://cloud.qdrant.io/&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/qdrant/qdrant/raw/master/docs/logo-dark.svg"&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://github.com/qdrant/qdrant/raw/master/docs/logo-light.svg"&gt; 
  &lt;img height="100" alt="Qdrant" src="https://github.com/qdrant/qdrant/raw/master/docs/logo.svg?sanitize=true"&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;b&gt;Vector Search Engine for the next generation of AI applications&lt;/b&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/qdrant/qdrant/actions/workflows/rust.yml"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/qdrant/qdrant/rust.yml?style=flat-square" alt="Tests status"&gt;&lt;/a&gt; &lt;a href="https://api.qdrant.tech/"&gt;&lt;img src="https://img.shields.io/badge/Docs-OpenAPI%203.0-success?style=flat-square" alt="OpenAPI Docs"&gt;&lt;/a&gt; &lt;a href="https://github.com/qdrant/qdrant/raw/master/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/qdrant/qdrant?style=flat-square" alt="Apache 2.0 License"&gt;&lt;/a&gt; &lt;a href="https://qdrant.to/discord"&gt;&lt;img src="https://img.shields.io/discord/907569970500743200?logo=Discord&amp;amp;style=flat-square&amp;amp;color=7289da" alt="Discord"&gt;&lt;/a&gt; &lt;a href="https://qdrant.to/roadmap"&gt;&lt;img src="https://img.shields.io/badge/Roadmap-2025-bc1439.svg?style=flat-square" alt="Roadmap 2025"&gt;&lt;/a&gt; &lt;a href="https://cloud.qdrant.io/"&gt;&lt;img src="https://img.shields.io/badge/Qdrant-Cloud-24386C.svg?logo=cloud&amp;amp;style=flat-square" alt="Qdrant Cloud"&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Qdrant&lt;/strong&gt; (read: &lt;em&gt;quadrant&lt;/em&gt;) is a vector similarity search engine and vector database. It provides a production-ready service with a convenient API to store, search, and manage points‚Äîvectors with an additional payload Qdrant is tailored to extended filtering support. It makes it useful for all sorts of neural-network or semantic-based matching, faceted search, and other applications.&lt;/p&gt; 
&lt;p&gt;Qdrant is written in Rust ü¶Ä, which makes it fast and reliable even under high load. See &lt;a href="https://qdrant.tech/benchmarks/"&gt;benchmarks&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!&lt;/p&gt; 
&lt;p&gt;Qdrant is also available as a fully managed &lt;strong&gt;&lt;a href="https://cloud.qdrant.io/"&gt;Qdrant Cloud&lt;/a&gt;&lt;/strong&gt; ‚õÖ including a &lt;strong&gt;free tier&lt;/strong&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/qdrant/qdrant/master/docs/QUICK_START.md"&gt;Quick Start&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/qdrant/qdrant/master/#clients"&gt;Client Libraries&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/qdrant/qdrant/master/#demo-projects"&gt;Demo Projects&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/qdrant/qdrant/master/#integrations"&gt;Integrations&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/qdrant/qdrant/master/#contacts"&gt;Contact&lt;/a&gt; &lt;/strong&gt; &lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Python&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;pip install qdrant-client
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The python client offers a convenient way to start with Qdrant locally:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from qdrant_client import QdrantClient
qdrant = QdrantClient(":memory:") # Create in-memory Qdrant instance, for testing, CI/CD
# OR
client = QdrantClient(path="path/to/db")  # Persists changes to disk, fast prototyping
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Client-Server&lt;/h3&gt; 
&lt;p&gt;To experience the full power of Qdrant locally, run the container with this command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -p 6333:6333 qdrant/qdrant
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now you can connect to this with any client, including Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;qdrant = QdrantClient("http://localhost:6333") # Connect to existing Qdrant instance
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Before deploying Qdrant to production, be sure to read our &lt;a href="https://qdrant.tech/documentation/guides/installation/"&gt;installation&lt;/a&gt; and &lt;a href="https://qdrant.tech/documentation/guides/security/"&gt;security&lt;/a&gt; guides.&lt;/p&gt; 
&lt;h3&gt;Clients&lt;/h3&gt; 
&lt;p&gt;Qdrant offers the following client libraries to help you integrate it into your application stack with ease:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Official: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/qdrant/go-client"&gt;Go client&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/qdrant/rust-client"&gt;Rust client&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/qdrant/qdrant-js"&gt;JavaScript/TypeScript client&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/qdrant/qdrant-client"&gt;Python client&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/qdrant/qdrant-dotnet"&gt;.NET/C# client&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/qdrant/java-client"&gt;Java client&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Community: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://hexdocs.pm/qdrant/readme.html"&gt;Elixir&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/hkulekci/qdrant-php"&gt;PHP&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/andreibondarev/qdrant-ruby"&gt;Ruby&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/metaloom/qdrant-java-client"&gt;Java&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Where do I go from here?&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/qdrant/qdrant/master/docs/QUICK_START.md"&gt;Quick Start Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;End to End &lt;a href="https://colab.research.google.com/drive/1Bz8RSVHwnNDaNtDwotfPj0w7AYzsdXZ-?usp=sharing"&gt;Colab Notebook&lt;/a&gt; demo with SentenceBERT and Qdrant&lt;/li&gt; 
 &lt;li&gt;Detailed &lt;a href="https://qdrant.tech/documentation/"&gt;Documentation&lt;/a&gt; are great starting points&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://qdrant.to/qdrant-tutorial"&gt;Step-by-Step Tutorial&lt;/a&gt; to create your first neural network project with Qdrant&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Demo Projects &lt;a href="https://replit.com/@qdrant"&gt;&lt;img align="right" src="https://replit.com/badge/github/qdrant/qdrant" alt="Run on Repl.it"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h3&gt;Discover Semantic Text Search üîç&lt;/h3&gt; 
&lt;p&gt;Unlock the power of semantic embeddings with Qdrant, transcending keyword-based search to find meaningful connections in short texts. Deploy a neural search in minutes using a pre-trained neural network, and experience the future of text search. &lt;a href="https://qdrant.to/semantic-search-demo"&gt;Try it online!&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Explore Similar Image Search - Food Discovery üçï&lt;/h3&gt; 
&lt;p&gt;There's more to discovery than text search, especially when it comes to food. People often choose meals based on appearance rather than descriptions and ingredients. Let Qdrant help your users find their next delicious meal using visual search, even if they don't know the dish's name. &lt;a href="https://qdrant.to/food-discovery"&gt;Check it out!&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Master Extreme Classification - E-commerce Product Categorization üì∫&lt;/h3&gt; 
&lt;p&gt;Enter the cutting-edge realm of extreme classification, an emerging machine learning field tackling multi-class and multi-label problems with millions of labels. Harness the potential of similarity learning models, and see how a pre-trained transformer model and Qdrant can revolutionize e-commerce product categorization. &lt;a href="https://qdrant.to/extreme-classification-demo"&gt;Play with it online!&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; More solutions &lt;/summary&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td width="30%"&gt; &lt;img src="https://qdrant.tech/content/images/text_search.png"&gt; &lt;/td&gt; 
    &lt;td width="30%"&gt; &lt;img src="https://qdrant.tech/content/images/image_search.png"&gt; &lt;/td&gt; 
    &lt;td width="30%"&gt; &lt;img src="https://qdrant.tech/content/images/recommendations.png"&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt; Semantic Text Search &lt;/td&gt; 
    &lt;td&gt; Similar Image Search &lt;/td&gt; 
    &lt;td&gt; Recommendations &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td&gt; &lt;img width="300px" src="https://qdrant.tech/content/images/chat_bots.png"&gt; &lt;/td&gt; 
    &lt;td&gt; &lt;img width="300px" src="https://qdrant.tech/content/images/matching_engines.png"&gt; &lt;/td&gt; 
    &lt;td&gt; &lt;img width="300px" src="https://qdrant.tech/content/images/anomalies_detection.png"&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt; Chat Bots &lt;/td&gt; 
    &lt;td&gt; Matching Engines &lt;/td&gt; 
    &lt;td&gt; Anomaly Detection &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h2&gt;API&lt;/h2&gt; 
&lt;h3&gt;REST&lt;/h3&gt; 
&lt;p&gt;Online OpenAPI 3.0 documentation is available &lt;a href="https://api.qdrant.tech/"&gt;here&lt;/a&gt;. OpenAPI makes it easy to generate a client for virtually any framework or programming language.&lt;/p&gt; 
&lt;p&gt;You can also download raw OpenAPI &lt;a href="https://github.com/qdrant/qdrant/raw/master/docs/redoc/master/openapi.json"&gt;definitions&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;gRPC&lt;/h3&gt; 
&lt;p&gt;For faster production-tier searches, Qdrant also provides a gRPC interface. You can find gRPC documentation &lt;a href="https://qdrant.tech/documentation/interfaces/#grpc-interface"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;h3&gt;Filtering and Payload&lt;/h3&gt; 
&lt;p&gt;Qdrant can attach any JSON payloads to vectors, allowing for both the storage and filtering of data based on the values in these payloads. Payload supports a wide range of data types and query conditions, including keyword matching, full-text filtering, numerical ranges, geo-locations, and more.&lt;/p&gt; 
&lt;p&gt;Filtering conditions can be combined in various ways, including &lt;code&gt;should&lt;/code&gt;, &lt;code&gt;must&lt;/code&gt;, and &lt;code&gt;must_not&lt;/code&gt; clauses, ensuring that you can implement any desired business logic on top of similarity matching.&lt;/p&gt; 
&lt;h3&gt;Hybrid Search with Sparse Vectors&lt;/h3&gt; 
&lt;p&gt;To address the limitations of vector embeddings when searching for specific keywords, Qdrant introduces support for sparse vectors in addition to the regular dense ones.&lt;/p&gt; 
&lt;p&gt;Sparse vectors can be viewed as an generalization of BM25 or TF-IDF ranking. They enable you to harness the capabilities of transformer-based neural networks to weigh individual tokens effectively.&lt;/p&gt; 
&lt;h3&gt;Vector Quantization and On-Disk Storage&lt;/h3&gt; 
&lt;p&gt;Qdrant provides multiple options to make vector search cheaper and more resource-efficient. Built-in vector quantization reduces RAM usage by up to 97% and dynamically manages the trade-off between search speed and precision.&lt;/p&gt; 
&lt;h3&gt;Distributed Deployment&lt;/h3&gt; 
&lt;p&gt;Qdrant offers comprehensive horizontal scaling support through two key mechanisms:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Size expansion via sharding and throughput enhancement via replication&lt;/li&gt; 
 &lt;li&gt;Zero-downtime rolling updates and seamless dynamic scaling of the collections&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Highlighted Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Query Planning and Payload Indexes&lt;/strong&gt; - leverages stored payload information to optimize query execution strategy.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SIMD Hardware Acceleration&lt;/strong&gt; - utilizes modern CPU x86-x64 and Neon architectures to deliver better performance.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Async I/O&lt;/strong&gt; - uses &lt;code&gt;io_uring&lt;/code&gt; to maximize disk throughput utilization even on a network-attached storage.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Write-Ahead Logging&lt;/strong&gt; - ensures data persistence with update confirmation, even during power outages.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Integrations&lt;/h1&gt; 
&lt;p&gt;Examples and/or documentation of Qdrant integrations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.cohere.com/docs/qdrant-and-cohere"&gt;Cohere&lt;/a&gt; (&lt;a href="https://qdrant.tech/articles/qa-with-cohere-and-qdrant/"&gt;blogpost on building a QA app with Cohere and Qdrant&lt;/a&gt;) - Use Cohere embeddings with Qdrant&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.docarray.org/user_guide/storing/index_qdrant/"&gt;DocArray&lt;/a&gt; - Use Qdrant as a document store in DocArray&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://haystack.deepset.ai/integrations/qdrant-document-store"&gt;Haystack&lt;/a&gt; - Use Qdrant as a document store with Haystack (&lt;a href="https://haystack.deepset.ai/blog/qdrant-integration"&gt;blogpost&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://python.langchain.com/docs/integrations/providers/qdrant/"&gt;LangChain&lt;/a&gt; (&lt;a href="https://qdrant.tech/articles/langchain-integration/"&gt;blogpost&lt;/a&gt;) - Use Qdrant as a memory backend for LangChain.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://gpt-index.readthedocs.io/en/latest/examples/vector_stores/QdrantIndexDemo.html"&gt;LlamaIndex&lt;/a&gt; - Use Qdrant as a Vector Store with LlamaIndex.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/openai/chatgpt-retrieval-plugin/raw/main/docs/providers/qdrant/setup.md"&gt;OpenAI - ChatGPT retrieval plugin&lt;/a&gt; - Use Qdrant as a memory backend for ChatGPT&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://devblogs.microsoft.com/semantic-kernel/the-power-of-persistent-memory-with-semantic-kernel-and-qdrant-vector-database/"&gt;Microsoft Semantic Kernel&lt;/a&gt; - Use Qdrant as persistent memory with Semantic Kernel&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contacts&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Have questions? Join our &lt;a href="https://qdrant.to/discord"&gt;Discord channel&lt;/a&gt; or mention &lt;a href="https://qdrant.to/twitter"&gt;@qdrant_engine on Twitter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Want to stay in touch with latest releases? Subscribe to our &lt;a href="https://qdrant.tech/subscribe/"&gt;Newsletters&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Looking for a managed cloud? Check &lt;a href="https://qdrant.tech/pricing/"&gt;pricing&lt;/a&gt;, need something personalised? We're at &lt;a href="mailto:info@qdrant.tech"&gt;info@qdrant.tech&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Qdrant is licensed under the Apache License, Version 2.0. View a copy of the &lt;a href="https://github.com/qdrant/qdrant/raw/master/LICENSE"&gt;License file&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>rust-lang/rust</title>
      <link>https://github.com/rust-lang/rust</link>
      <description>&lt;p&gt;Empowering everyone to build reliable and efficient software.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/rust-lang/www.rust-lang.org/master/static/images/rust-social-wide-dark.svg"&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/rust-lang/www.rust-lang.org/master/static/images/rust-social-wide-light.svg"&gt; 
  &lt;img alt="The Rust Programming Language: A language empowering everyone to build reliable and efficient software" src="https://raw.githubusercontent.com/rust-lang/www.rust-lang.org/master/static/images/rust-social-wide-light.svg?sanitize=true" width="50%"&gt; 
 &lt;/picture&gt; 
 &lt;p&gt;&lt;a href="https://www.rust-lang.org/"&gt;Website&lt;/a&gt; | &lt;a href="https://www.rust-lang.org/learn/get-started"&gt;Getting started&lt;/a&gt; | &lt;a href="https://www.rust-lang.org/learn"&gt;Learn&lt;/a&gt; | &lt;a href="https://www.rust-lang.org/learn#learn-use"&gt;Documentation&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/rust-lang/rust/master/CONTRIBUTING.md"&gt;Contributing&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;This is the main source code repository for &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt;. It contains the compiler, standard library, and documentation.&lt;/p&gt; 
&lt;h2&gt;Why Rust?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Performance:&lt;/strong&gt; Fast and memory-efficient, suitable for critical services, embedded devices, and easily integrated with other languages.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Reliability:&lt;/strong&gt; Our rich type system and ownership model ensure memory and thread safety, reducing bugs at compile-time.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Productivity:&lt;/strong&gt; Comprehensive documentation, a compiler committed to providing great diagnostics, and advanced tooling including package manager and build tool (&lt;a href="https://github.com/rust-lang/cargo"&gt;Cargo&lt;/a&gt;), auto-formatter (&lt;a href="https://github.com/rust-lang/rustfmt"&gt;rustfmt&lt;/a&gt;), linter (&lt;a href="https://github.com/rust-lang/rust-clippy"&gt;Clippy&lt;/a&gt;) and editor support (&lt;a href="https://github.com/rust-lang/rust-analyzer"&gt;rust-analyzer&lt;/a&gt;).&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;Read &lt;a href="https://doc.rust-lang.org/book/ch01-01-installation.html"&gt;"Installation"&lt;/a&gt; from &lt;a href="https://doc.rust-lang.org/book/index.html"&gt;The Book&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installing from Source&lt;/h2&gt; 
&lt;p&gt;If you really want to install from source (though this is not recommended), see &lt;a href="https://raw.githubusercontent.com/rust-lang/rust/master/INSTALL.md"&gt;INSTALL.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting Help&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://www.rust-lang.org/community"&gt;https://www.rust-lang.org/community&lt;/a&gt; for a list of chat platforms and forums.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/rust-lang/rust/master/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Rust is primarily distributed under the terms of both the MIT license and the Apache License (Version 2.0), with portions covered by various BSD-like licenses.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/rust-lang/rust/master/LICENSE-APACHE"&gt;LICENSE-APACHE&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/rust-lang/rust/master/LICENSE-MIT"&gt;LICENSE-MIT&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/rust-lang/rust/master/COPYRIGHT"&gt;COPYRIGHT&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h2&gt;Trademark&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://rustfoundation.org/"&gt;The Rust Foundation&lt;/a&gt; owns and protects the Rust and Cargo trademarks and logos (the "Rust Trademarks").&lt;/p&gt; 
&lt;p&gt;If you want to use these names or brands, please read the &lt;a href="https://rustfoundation.org/policy/rust-trademark-policy/"&gt;Rust language trademark policy&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Third-party logos may be subject to third-party copyrights and trademarks. See &lt;a href="https://www.rust-lang.org/policies/licenses"&gt;Licenses&lt;/a&gt; for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>gfx-rs/wgpu</title>
      <link>https://github.com/gfx-rs/wgpu</link>
      <description>&lt;p&gt;A cross-platform, safe, pure-Rust graphics API.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;wgpu&lt;/h1&gt; 
&lt;img align="right" width="20%" src="https://raw.githubusercontent.com/gfx-rs/wgpu/trunk/logo.png"&gt; 
&lt;p&gt;&lt;a href="https://matrix.to/#/%23Wgpu:matrix.org"&gt;&lt;img src="https://img.shields.io/static/v1?label=Space&amp;amp;message=%23Wgpu&amp;amp;color=blue&amp;amp;logo=matrix" alt="Matrix Space"&gt;&lt;/a&gt; &lt;a href="https://matrix.to/#/%23wgpu:matrix.org"&gt;&lt;img src="https://img.shields.io/static/v1?label=devs&amp;amp;message=%23wgpu&amp;amp;color=blueviolet&amp;amp;logo=matrix" alt="Dev Matrix"&gt;&lt;/a&gt; &lt;a href="https://matrix.to/#/%23wgpu-users:matrix.org"&gt;&lt;img src="https://img.shields.io/static/v1?label=users&amp;amp;message=%23wgpu-users&amp;amp;color=blueviolet&amp;amp;logo=matrix" alt="User Matrix"&gt;&lt;/a&gt; &lt;a href="https://github.com/gfx-rs/wgpu/actions"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/gfx-rs/wgpu/ci.yml?branch=trunk&amp;amp;logo=github&amp;amp;label=CI" alt="Build Status"&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/gfx-rs/wgpu"&gt;&lt;img src="https://img.shields.io/codecov/c/github/gfx-rs/wgpu?logo=codecov&amp;amp;logoColor=fff&amp;amp;label=codecov&amp;amp;token=84qJTesmeS" alt="codecov.io"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;wgpu&lt;/code&gt; is a cross-platform, safe, pure-rust graphics API. It runs natively on Vulkan, Metal, D3D12, and OpenGL; and on top of WebGL2 and WebGPU on wasm.&lt;/p&gt; 
&lt;p&gt;The API is based on the &lt;a href="https://gpuweb.github.io/gpuweb/"&gt;WebGPU standard&lt;/a&gt;. It serves as the core of the WebGPU integration in Firefox, Servo, and Deno.&lt;/p&gt; 
&lt;h2&gt;Quick Links&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Docs&lt;/th&gt; 
   &lt;th align="center"&gt;Examples&lt;/th&gt; 
   &lt;th align="center"&gt;Changelog&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.rs/wgpu/"&gt;v26&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/gfx-rs/wgpu/tree/v26/examples#readme"&gt;v26&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/gfx-rs/wgpu/releases"&gt;v26&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://wgpu.rs/doc/wgpu/"&gt;&lt;code&gt;trunk&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/gfx-rs/wgpu/tree/trunk/examples#readme"&gt;&lt;code&gt;trunk&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/gfx-rs/wgpu/raw/trunk/CHANGELOG.md#unreleased"&gt;&lt;code&gt;trunk&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Contributors are welcome! See &lt;a href="https://raw.githubusercontent.com/gfx-rs/wgpu/trunk/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Repo Overview&lt;/h2&gt; 
&lt;p&gt;The repository hosts the following libraries:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://crates.io/crates/wgpu"&gt;&lt;img src="https://img.shields.io/crates/v/wgpu.svg?label=wgpu" alt="Crates.io"&gt;&lt;/a&gt; &lt;a href="https://docs.rs/wgpu/"&gt;&lt;img src="https://docs.rs/wgpu/badge.svg?sanitize=true" alt="docs.rs"&gt;&lt;/a&gt; - User facing Rust API.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://crates.io/crates/wgpu-core"&gt;&lt;img src="https://img.shields.io/crates/v/wgpu-core.svg?label=wgpu-core" alt="Crates.io"&gt;&lt;/a&gt; &lt;a href="https://docs.rs/wgpu-core/"&gt;&lt;img src="https://docs.rs/wgpu-core/badge.svg?sanitize=true" alt="docs.rs"&gt;&lt;/a&gt; - Internal safe implementation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://crates.io/crates/wgpu-hal"&gt;&lt;img src="https://img.shields.io/crates/v/wgpu-hal.svg?label=wgpu-hal" alt="Crates.io"&gt;&lt;/a&gt; &lt;a href="https://docs.rs/wgpu-hal/"&gt;&lt;img src="https://docs.rs/wgpu-hal/badge.svg?sanitize=true" alt="docs.rs"&gt;&lt;/a&gt; - Internal unsafe GPU API abstraction layer.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://crates.io/crates/wgpu-types"&gt;&lt;img src="https://img.shields.io/crates/v/wgpu-types.svg?label=wgpu-types" alt="Crates.io"&gt;&lt;/a&gt; &lt;a href="https://docs.rs/wgpu-types/"&gt;&lt;img src="https://docs.rs/wgpu-types/badge.svg?sanitize=true" alt="docs.rs"&gt;&lt;/a&gt; - Rust types shared between all crates.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://crates.io/crates/naga"&gt;&lt;img src="https://img.shields.io/crates/v/naga.svg?label=naga" alt="Crates.io"&gt;&lt;/a&gt; &lt;a href="https://docs.rs/naga/"&gt;&lt;img src="https://docs.rs/naga/badge.svg?sanitize=true" alt="docs.rs"&gt;&lt;/a&gt; - Stand-alone shader translation library.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://crates.io/crates/deno_webgpu"&gt;&lt;img src="https://img.shields.io/crates/v/deno_webgpu.svg?label=deno_webgpu" alt="Crates.io"&gt;&lt;/a&gt; - WebGPU implementation for the Deno JavaScript/TypeScript runtime&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The following binaries:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://crates.io/crates/naga-cli"&gt;&lt;img src="https://img.shields.io/crates/v/naga-cli.svg?label=naga-cli" alt="Crates.io"&gt;&lt;/a&gt; - Tool for translating shaders between different languages using &lt;code&gt;naga&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://crates.io/crates/wgpu-info"&gt;&lt;img src="https://img.shields.io/crates/v/wgpu-info.svg?label=wgpu-info" alt="Crates.io"&gt;&lt;/a&gt; - Tool for getting information on GPUs in the system.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cts_runner&lt;/code&gt; - WebGPU Conformance Test Suite runner using &lt;code&gt;deno_webgpu&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;player&lt;/code&gt; - standalone application for replaying the API traces.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For an overview of all the components in the gfx-rs ecosystem, see &lt;a href="https://raw.githubusercontent.com/gfx-rs/wgpu/trunk/docs/big-picture.png"&gt;the big picture&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Play with our Examples&lt;/h3&gt; 
&lt;p&gt;Go to &lt;a href="https://wgpu.rs/examples/"&gt;https://wgpu.rs/examples/&lt;/a&gt; to play with our examples in your browser. Requires a browser supporting WebGPU for the WebGPU examples.&lt;/p&gt; 
&lt;h3&gt;Rust&lt;/h3&gt; 
&lt;p&gt;Rust examples can be found at &lt;a href="https://raw.githubusercontent.com/gfx-rs/wgpu/trunk/examples"&gt;examples&lt;/a&gt;. You can run the examples natively with &lt;code&gt;cargo run --bin wgpu-examples &amp;lt;example&amp;gt;&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;If you are new to wgpu and graphics programming, we recommend starting with &lt;a href="https://sotrh.github.io/learn-wgpu/"&gt;https://sotrh.github.io/learn-wgpu/&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To run the examples in a browser, run &lt;code&gt;cargo xtask run-wasm&lt;/code&gt;. Then open &lt;code&gt;http://localhost:8000&lt;/code&gt; in your browser, and you can choose an example to run. Naturally, in order to display any of the WebGPU based examples, you need to make sure your browser supports it.&lt;/p&gt; 
&lt;h3&gt;C/C++&lt;/h3&gt; 
&lt;p&gt;To use wgpu in C/C++, you need &lt;a href="https://github.com/gfx-rs/wgpu-native"&gt;wgpu-native&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you are looking for a wgpu C++ tutorial, look at the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://eliemichel.github.io/LearnWebGPU/"&gt;https://eliemichel.github.io/LearnWebGPU/&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Others&lt;/h3&gt; 
&lt;p&gt;If you want to use wgpu in other languages, there are many bindings to wgpu-native from languages such as Python, D, Julia, Kotlin, and more. See &lt;a href="https://github.com/gfx-rs/wgpu-native#bindings"&gt;the list&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;We have the Matrix space &lt;a href="https://matrix.to/#/%23Wgpu:matrix.org"&gt;&lt;img src="https://img.shields.io/static/v1?label=Space&amp;amp;message=%23Wgpu&amp;amp;color=blue&amp;amp;logo=matrix" alt="Matrix Space"&gt;&lt;/a&gt; with a few different rooms that form the wgpu community:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://matrix.to/#/%23wgpu:matrix.org"&gt;&lt;img src="https://img.shields.io/static/v1?label=wgpu-devs&amp;amp;message=%23wgpu&amp;amp;color=blueviolet&amp;amp;logo=matrix" alt="Wgpu Matrix"&gt;&lt;/a&gt; - discussion of the wgpu's development.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://matrix.to/#/%23naga:matrix.org"&gt;&lt;img src="https://img.shields.io/static/v1?label=naga-devs&amp;amp;message=%23naga&amp;amp;color=blueviolet&amp;amp;logo=matrix" alt="Naga Matrix"&gt;&lt;/a&gt; - discussion of the naga's development.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://matrix.to/#/%23wgpu-users:matrix.org"&gt;&lt;img src="https://img.shields.io/static/v1?label=wgpu-users&amp;amp;message=%23wgpu-users&amp;amp;color=blueviolet&amp;amp;logo=matrix" alt="User Matrix"&gt;&lt;/a&gt; - discussion of using the library and the surrounding ecosystem.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://matrix.to/#/%23wgpu-random:matrix.org"&gt;&lt;img src="https://img.shields.io/static/v1?label=random&amp;amp;message=%23wgpu-random&amp;amp;color=blueviolet&amp;amp;logo=matrix" alt="Random Matrix"&gt;&lt;/a&gt; - discussion of everything else.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Wiki&lt;/h2&gt; 
&lt;p&gt;We have a &lt;a href="https://github.com/gfx-rs/wgpu/wiki"&gt;wiki&lt;/a&gt; that serves as a knowledge base.&lt;/p&gt; 
&lt;h2&gt;Extension Specifications&lt;/h2&gt; 
&lt;p&gt;While the core of wgpu is based on the WebGPU standard, we also support extensions that allow for features that the standard does not have yet. For high-level documentation on how to use these extensions, see the individual specifications:&lt;/p&gt; 
&lt;p&gt;üß™EXPERIMENTALüß™ APIs are subject to change and may allow undefined behavior if used incorrectly.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üß™EXPERIMENTALüß™ &lt;a href="https://raw.githubusercontent.com/gfx-rs/wgpu/trunk/docs/api-specs/ray_tracing.md"&gt;Ray Tracing&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;üß™EXPERIMENTALüß™ &lt;a href="https://raw.githubusercontent.com/gfx-rs/wgpu/trunk/docs/api-specs/mesh_shading.md"&gt;Mesh Shading&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Supported Platforms&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;API&lt;/th&gt; 
   &lt;th&gt;Windows&lt;/th&gt; 
   &lt;th&gt;Linux/Android&lt;/th&gt; 
   &lt;th&gt;macOS/iOS&lt;/th&gt; 
   &lt;th&gt;Web (wasm)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vulkan&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;üåã&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Metal&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DX12&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenGL&lt;/td&gt; 
   &lt;td&gt;üÜó (GL 3.3+)&lt;/td&gt; 
   &lt;td&gt;üÜó (GL ES 3.0+)&lt;/td&gt; 
   &lt;td&gt;üìê&lt;/td&gt; 
   &lt;td&gt;üÜó (WebGL2)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;WebGPU&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;‚úÖ = First Class Support&lt;br&gt; üÜó = Downlevel/Best Effort Support&lt;br&gt; üìê = Requires the &lt;a href="https://raw.githubusercontent.com/gfx-rs/wgpu/trunk/#angle"&gt;ANGLE&lt;/a&gt; translation layer (GL ES 3.0 only)&lt;br&gt; üåã = Requires the &lt;a href="https://vulkan.lunarg.com/sdk/home#mac"&gt;MoltenVK&lt;/a&gt; translation layer&lt;br&gt; üõ†Ô∏è = Unsupported, though open to contributions&lt;/p&gt; 
&lt;h3&gt;Shader Support&lt;/h3&gt; 
&lt;p&gt;wgpu supports shaders in &lt;a href="https://gpuweb.github.io/gpuweb/wgsl/"&gt;WGSL&lt;/a&gt;, SPIR-V, and GLSL. Both &lt;a href="https://github.com/Microsoft/DirectXShaderCompiler"&gt;HLSL&lt;/a&gt; and &lt;a href="https://github.com/KhronosGroup/glslang"&gt;GLSL&lt;/a&gt; have compilers to target SPIR-V. All of these shader languages can be used with any backend as we handle all of the conversions. Additionally, support for these shader inputs is not going away.&lt;/p&gt; 
&lt;p&gt;While WebGPU does not support any shading language other than WGSL, we will automatically convert your non-WGSL shaders if you're running on WebGPU.&lt;/p&gt; 
&lt;p&gt;WGSL is always supported by default, but GLSL and SPIR-V need features enabled to compile in support.&lt;/p&gt; 
&lt;p&gt;Note that the WGSL specification is still under development, so the &lt;a href="https://gpuweb.github.io/gpuweb/wgsl/"&gt;draft specification&lt;/a&gt; does not exactly describe what &lt;code&gt;wgpu&lt;/code&gt; supports. See &lt;a href="https://raw.githubusercontent.com/gfx-rs/wgpu/trunk/#tracking-the-webgpu-and-wgsl-draft-specifications"&gt;below&lt;/a&gt; for details.&lt;/p&gt; 
&lt;p&gt;To enable SPIR-V shaders, enable the &lt;code&gt;spirv&lt;/code&gt; feature of wgpu. To enable GLSL shaders, enable the &lt;code&gt;glsl&lt;/code&gt; feature of wgpu.&lt;/p&gt; 
&lt;h3&gt;Angle&lt;/h3&gt; 
&lt;p&gt;&lt;a href="http://angleproject.org"&gt;Angle&lt;/a&gt; is a translation layer from GLES to other backends developed by Google. We support running our GLES3 backend over it in order to reach platforms DX11 support, which aren't accessible otherwise. In order to run with Angle, the "angle" feature has to be enabled, and Angle libraries placed in a location visible to the application. These binaries can be downloaded from &lt;a href="https://github.com/DileSoft/gfbuild-angle"&gt;gfbuild-angle&lt;/a&gt; artifacts, &lt;a href="https://github.com/google/angle/raw/main/doc/DevSetup.md"&gt;manual compilation&lt;/a&gt; may be required on Macs with Apple silicon.&lt;/p&gt; 
&lt;p&gt;On Windows, you generally need to copy them into the working directory, in the same directory as the executable, or somewhere in your path. On Linux, you can point to them using &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; environment.&lt;/p&gt; 
&lt;h3&gt;MSRV policy&lt;/h3&gt; 
&lt;p&gt;Due to complex dependants, we have two MSRV policies:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;naga&lt;/code&gt;, &lt;code&gt;wgpu-core&lt;/code&gt;, &lt;code&gt;wgpu-hal&lt;/code&gt;, and &lt;code&gt;wgpu-types&lt;/code&gt;'s MSRV is &lt;strong&gt;1.82&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;The rest of the workspace has an MSRV of &lt;strong&gt;1.88&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;It is enforced on CI (in "/.github/workflows/ci.yml") with the &lt;code&gt;CORE_MSRV&lt;/code&gt; and &lt;code&gt;REPO_MSRV&lt;/code&gt; variables. This version can only be upgraded in breaking releases, though we release a breaking version every three months.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;naga&lt;/code&gt;, &lt;code&gt;wgpu-core&lt;/code&gt;, &lt;code&gt;wgpu-hal&lt;/code&gt;, and &lt;code&gt;wgpu-types&lt;/code&gt; crates should never require an MSRV ahead of Firefox's MSRV for nightly builds, as determined by the value of &lt;code&gt;MINIMUM_RUST_VERSION&lt;/code&gt; in &lt;a href="https://searchfox.org/mozilla-central/source/python/mozboot/mozboot/util.py"&gt;&lt;code&gt;python/mozboot/mozboot/util.py&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Environment Variables&lt;/h2&gt; 
&lt;p&gt;All testing and example infrastructure share the same set of environment variables that determine which Backend/GPU it will run on.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;WGPU_ADAPTER_NAME&lt;/code&gt; with a substring of the name of the adapter you want to use (ex. &lt;code&gt;1080&lt;/code&gt; will match &lt;code&gt;NVIDIA GeForce 1080ti&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;WGPU_BACKEND&lt;/code&gt; with a comma-separated list of the backends you want to use (&lt;code&gt;vulkan&lt;/code&gt;, &lt;code&gt;metal&lt;/code&gt;, &lt;code&gt;dx12&lt;/code&gt;, or &lt;code&gt;gl&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;WGPU_POWER_PREF&lt;/code&gt; with the power preference to choose when a specific adapter name isn't specified (&lt;code&gt;high&lt;/code&gt;, &lt;code&gt;low&lt;/code&gt; or &lt;code&gt;none&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;WGPU_DX12_COMPILER&lt;/code&gt; with the DX12 shader compiler you wish to use (&lt;code&gt;dxc&lt;/code&gt;, &lt;code&gt;static-dxc&lt;/code&gt;, or &lt;code&gt;fxc&lt;/code&gt;). Note that &lt;code&gt;dxc&lt;/code&gt; requires &lt;code&gt;dxcompiler.dll&lt;/code&gt; (min v1.8.2502) to be in the working directory, and &lt;code&gt;static-dxc&lt;/code&gt; requires the &lt;code&gt;static-dxc&lt;/code&gt; crate feature to be enabled. Otherwise, it will fall back to &lt;code&gt;fxc&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;WGPU_GLES_MINOR_VERSION&lt;/code&gt; with the minor OpenGL ES 3 version number to request (&lt;code&gt;0&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt;, &lt;code&gt;2&lt;/code&gt; or &lt;code&gt;automatic&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;WGPU_ALLOW_UNDERLYING_NONCOMPLIANT_ADAPTER&lt;/code&gt; with a boolean whether non-compliant drivers are enumerated (&lt;code&gt;0&lt;/code&gt; for false, &lt;code&gt;1&lt;/code&gt; for true).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;When running the CTS, use the variables &lt;code&gt;DENO_WEBGPU_ADAPTER_NAME&lt;/code&gt;, &lt;code&gt;DENO_WEBGPU_BACKEND&lt;/code&gt;, &lt;code&gt;DENO_WEBGPU_POWER_PREFERENCE&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Testing&lt;/h2&gt; 
&lt;p&gt;We have multiple methods of testing, each of which tests different qualities about wgpu. We automatically run our tests on CI. The current state of CI testing:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Platform/Backend&lt;/th&gt; 
   &lt;th&gt;Tests&lt;/th&gt; 
   &lt;th&gt;Notes&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows/DX12&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;using WARP&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows/OpenGL&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;using llvmpipe&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MacOS/Metal&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;using hardware runner&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Linux/Vulkan&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;using lavapipe&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Linux/OpenGL ES&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;using llvmpipe&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Chrome/WebGL&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;using swiftshader&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Chrome/WebGPU&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;not set up&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Core Test Infrastructure&lt;/h3&gt; 
&lt;p&gt;We use a tool called &lt;a href="https://github.com/nextest-rs/nextest"&gt;&lt;code&gt;cargo nextest&lt;/code&gt;&lt;/a&gt; to run our tests. To install it, run &lt;code&gt;cargo install cargo-nextest&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;To run the test suite:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cargo xtask test
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run the test suite on WebGL (currently incomplete):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cd wgpu
wasm-pack test --headless --chrome --no-default-features --features webgl --workspace
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will automatically run the tests using a packaged browser. Remove &lt;code&gt;--headless&lt;/code&gt; to run the tests with whatever browser you wish at &lt;code&gt;http://localhost:8000&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;If you are a user and want a way to help contribute to wgpu, we always need more help writing test cases.&lt;/p&gt; 
&lt;h3&gt;WebGPU Conformance Test Suite&lt;/h3&gt; 
&lt;p&gt;WebGPU includes a Conformance Test Suite to validate that implementations are working correctly. We run cases from the CTS against wgpu using &lt;a href="https://deno.com/"&gt;Deno&lt;/a&gt;. A &lt;a href="https://raw.githubusercontent.com/gfx-rs/wgpu/trunk/cts_runner/test.lst"&gt;default list of enabled tests&lt;/a&gt; is automatically run on pull requests in CI.&lt;/p&gt; 
&lt;p&gt;To run the default set of CTS tests locally, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cargo xtask cts
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also specify a test selector on the command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cargo xtask cts 'webgpu:api,operation,command_buffer,basic:*'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or supply your own test list in a file:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cargo xtask cts -f your_tests.lst
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To find the full list of tests, go to the &lt;a href="https://gpuweb.github.io/cts/standalone/?runnow=0&amp;amp;worker=0&amp;amp;debug=0&amp;amp;q=webgpu:*"&gt;web version of the CTS&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The version of the CTS used by &lt;code&gt;cargo xtask cts&lt;/code&gt; is specified in &lt;a href="https://raw.githubusercontent.com/gfx-rs/wgpu/trunk/cts_runner/revision.txt"&gt;&lt;code&gt;cts_runner/revision.txt&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Tracking the WebGPU and WGSL draft specifications&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;wgpu&lt;/code&gt; crate is meant to be an idiomatic Rust translation of the &lt;a href="https://www.w3.org/TR/webgpu/"&gt;WebGPU API&lt;/a&gt;. That specification, along with its shading language, &lt;a href="https://gpuweb.github.io/gpuweb/wgsl/"&gt;WGSL&lt;/a&gt;, are both still in the "Working Draft" phase, and while the general outlines are stable, details change frequently. Until the specification is stabilized, the &lt;code&gt;wgpu&lt;/code&gt; crate and the version of WGSL it implements will likely differ from what is specified, as the implementation catches up.&lt;/p&gt; 
&lt;p&gt;Exactly which WGSL features &lt;code&gt;wgpu&lt;/code&gt; supports depends on how you are using it:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;When running as native code, &lt;code&gt;wgpu&lt;/code&gt; uses the &lt;a href="https://github.com/gfx-rs/naga/"&gt;Naga&lt;/a&gt; crate to translate WGSL code into the shading language of your platform's native GPU API. Naga has &lt;a href="https://github.com/gfx-rs/naga/milestone/4"&gt;a milestone&lt;/a&gt; for catching up to the WGSL specification, but in general, there is no up-to-date summary of the differences between Naga and the WGSL spec.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;When running in a web browser (by compilation to WebAssembly) without the &lt;code&gt;"webgl"&lt;/code&gt; feature enabled, &lt;code&gt;wgpu&lt;/code&gt; relies on the browser's own WebGPU implementation. WGSL shaders are simply passed through to the browser, so that determines which WGSL features you can use.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;When running in a web browser with &lt;code&gt;wgpu&lt;/code&gt;'s &lt;code&gt;"webgl"&lt;/code&gt; feature enabled, &lt;code&gt;wgpu&lt;/code&gt; uses Naga to translate WGSL programs into GLSL. This uses the same version of Naga as if you were running &lt;code&gt;wgpu&lt;/code&gt; as native code.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Coordinate Systems&lt;/h2&gt; 
&lt;p&gt;wgpu uses the coordinate systems of D3D and Metal:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Render&lt;/th&gt; 
   &lt;th&gt;Texture&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/gfx-rs/wgpu/trunk/docs/render_coordinates.png" alt="render_coordinates"&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/gfx-rs/wgpu/trunk/docs/texture_coordinates.png" alt="texture_coordinates"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>foundry-rs/foundry</title>
      <link>https://github.com/foundry-rs/foundry</link>
      <description>&lt;p&gt;Foundry is a blazing fast, portable and modular toolkit for Ethereum application development written in Rust.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/foundry-rs/foundry/master/.github/assets/banner.png" alt="Foundry banner"&gt; 
 &lt;p&gt;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/foundry-rs/foundry/actions"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/foundry-rs/foundry/test.yml?branch=master" alt="Github Actions"&gt;&lt;/a&gt; &lt;a href="https://t.me/foundry_rs"&gt;&lt;img src="https://img.shields.io/endpoint?color=neon&amp;amp;logo=telegram&amp;amp;label=chat&amp;amp;style=flat-square&amp;amp;url=https%3A%2F%2Ftg.sumanjay.workers.dev%2Ffoundry_rs" alt="Telegram Chat"&gt;&lt;/a&gt; &lt;a href="https://t.me/foundry_support"&gt;&lt;img src="https://img.shields.io/endpoint?color=neon&amp;amp;logo=telegram&amp;amp;label=support&amp;amp;style=flat-square&amp;amp;url=https%3A%2F%2Ftg.sumanjay.workers.dev%2Ffoundry_support" alt="Telegram Support"&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/Foundry-grey?style=flat&amp;amp;logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAElElEQVR4nH1VUUhUaRg9984YdzBpkqR0Z210rIESIXSabEbcHgydrpNRRj00kWaztj0U1MOW0MOIbD300IvLMqBpMTGYxdoqyoRNDUESBDWwUuPugCSSsTM7u0Oj1/+efdiMcmnP2/fDd77D4f/OB6xCa2urQZbllVICYGtqanK1tLS4AdgAyAAgyzJaW1sNq/ulT4twOGw4fPiwAGDp7Ow8VV1d7bVarRWxWCw/k8mgsbExm0wmZ+Lx+M/Xr1//CcAsSVmSJH01McLhsAEAnE5nx+Tk5B/xeJxOp5N9fX2sqqqixWLhnTt36HA4GIvFGI1GU3V1df5Pe/9D1t7eHkgkEuzo6GBPT49WWloq7Ha7fujQITocDu7atUs3m83i6tWr2okTJ/jixQuePn265zPScDhskGUZe/fubXv8+DFv3rypbdiwQaxbt46RSIT79u3j0NAQb926RVVVOT4+TqvVyvz8fD0YDC5NTk6ysbHxlCRJ/5KSlAAURyKRTFNTkwAg7t69S5/Px76+Pq7GyMgI9+/fz9HRUQIQO3bsEKOjo38DsJCUJADw+/0BVVW7otHo8ps3b4yvXr3CxMQETCYTTCYTNE0DAOTl5SGXy0FRFOzZswdmsxkVFRXLNTU1xmg0+kNvb+/3AGAcGBiI7969Wwcg6urq+OTJE967d49btmzh9PT0R3WJRIKBQIDBYJBTU1NsaGggAGGz2fTe3t5fAeQZAWwuLi4uP3nypOT1emEwGFBeXo7a2losLCygoaEB/f39MJlMCIVCkCQJBw8ehNVqhcfjQXNzs1RSUiKtX7++DEAZqqqq3KFQiABYUFDAM2fOkCQXFxdJkvfv32dhYSG9Xi+vXbvG2dnZj4oDgQCLioqoKAqHhobodDq/Mc7NzUklJSUIBoOw2WzYtm0blpeXsWbNGkxMTODp06doa2vD4OAgNm7cCIvFApLQdR3nzp3Dzp078fLlSxQVFeHdu3cAgIpHjx69/zBUX5k+MDBAt9vNY8eOsbu7m6lUigcOHKDL5WImkyHJz9TGYrEcALsMIPn69esZTdMIgM+ePUNXVxdu376NsrIyuN1uXLp0CWazGcPDw3C5XFBVFWfPnkVNTQ18Pp+ezWY5MzPzO4DfAABHjhzpJslUKqVdvHiR4+PjbG9vZy6XI0kuLS0xmUxSCEGS9Pv9LC0tpdFoZGVlpSaEoM/nuwIAKx/7q5GRkb9CoZBQVVWcP3+ez58/J0mm02kODg7ywoULjMViTKfTtNvtXLt2LTdt2qTncrnlsbGxLICvSUqfrl5HJBLh1NTUkhBCJ8mFhQX29/dTVVUWFBTwwYMH1HWdly9fpqIoeiKRWJqfn2d1dXWnLMuf7zMAHD16tGd+fn7FZy2bzYrKykodAAFQVVV9cXFRkNTevn3Lubk5trS0XPnfxHE4HN8ODw+nV/yanp6mx+Ohx+P5aIMQgmNjY3/W1tZ+t5rsSwG7+fjx4/76+vrm7du32woLC00AkE6n38fj8ZmHDx/+cuPGjR8BJL8YsCtYdQIMALYqilKvKEo9APuHty+egH8A3GfFDJXmxmMAAAAASUVORK5CYII%3D&amp;amp;link=https%3A%2F%2Fbook.getfoundry.sh%2F" alt="Foundry"&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://getfoundry.sh/getting-started/installation"&gt;Install&lt;/a&gt;&lt;/strong&gt; | &lt;a href="https://getfoundry.sh"&gt;Docs&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/docs/dev/README.md"&gt;Developer Guidelines&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/CONTRIBUTING.md"&gt;Contributing&lt;/a&gt; | &lt;a href="https://foundry-rs.github.io/foundry"&gt;Crate Docs&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr&gt; 
&lt;h3&gt;Foundry is a blazing fast, portable and modular toolkit for Ethereum application development written in Rust.&lt;/h3&gt; 
&lt;p&gt;Foundry consists of:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/#forge"&gt;&lt;strong&gt;Forge&lt;/strong&gt;&lt;/a&gt;: Build, test, fuzz, debug and deploy &lt;a href="https://soliditylang.org/"&gt;Solidity&lt;/a&gt; contracts, like Hardhat, Brownie, Ape.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/#cast"&gt;&lt;strong&gt;Cast&lt;/strong&gt;&lt;/a&gt;: A Swiss Army knife for interacting with EVM smart contracts, sending transactions and getting chain data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/#anvil"&gt;&lt;strong&gt;Anvil&lt;/strong&gt;&lt;/a&gt;: Fast local Ethereum development node, akin to Hardhat Network, Tenderly.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/#chisel"&gt;&lt;strong&gt;Chisel&lt;/strong&gt;&lt;/a&gt;: Fast, utilitarian, and verbose Solidity REPL.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Need help getting started with Foundry? Read the &lt;a href="https://getfoundry.sh"&gt;üìñ Foundry Docs&lt;/a&gt;!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/foundry-rs/foundry/master/.github/assets/demo.gif" alt="Demo"&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;High-Performance Compilation&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Fast and Flexible&lt;/strong&gt;: Automatically detects and installs the required Solidity compiler version.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Solidity and Vyper Support&lt;/strong&gt;: Fully supports both Solidity and Vyper out-of-the-box.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Incremental Compilation&lt;/strong&gt;: Re-compiles only changed files, saving time.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Parallelized Pipeline&lt;/strong&gt;: Leverages multi-core systems for ultra-fast builds.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Broad Compatibility&lt;/strong&gt;: Supports non-standard directory structures, including &lt;a href="https://twitter.com/gakonst/status/1461289225337421829"&gt;Hardhat repos&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Advanced Testing&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;No Context Switching&lt;/strong&gt;: Write tests directly in Solidity.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Fuzz Testing&lt;/strong&gt;: Quickly identify edge cases with input shrinking and counter-example generation.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Invariant Testing&lt;/strong&gt;: Ensure complex system properties hold across a wide range of inputs.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Debugging Made Easy&lt;/strong&gt;: Use &lt;a href="https://github.com/foundry-rs/forge-std"&gt;forge-std&lt;/a&gt;'s &lt;code&gt;console.sol&lt;/code&gt; for flexible debug logging.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Interactive Debugger&lt;/strong&gt;: Step through your Solidity code with Foundry's interactive debugger, making it easy to pinpoint issues.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Powerful Runtime Features&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;RPC Forking&lt;/strong&gt;: Fast and efficient remote RPC forking backed by &lt;a href="https://github.com/alloy-rs/alloy"&gt;Alloy&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Lightweight &amp;amp; Portable&lt;/strong&gt;: No dependency on Nix or other package managers for installation.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Streamlined CI/CD&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Optimized CI&lt;/strong&gt;: Accelerate builds, run tests and execute scripts using &lt;a href="https://github.com/foundry-rs/foundry-toolchain"&gt;Foundry's GitHub action&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Getting started is very easy:&lt;/p&gt; 
&lt;p&gt;Install &lt;code&gt;foundryup&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;curl -L https://foundry.paradigm.xyz | bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Next, run &lt;code&gt;foundryup&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;It will automatically install the latest version of the precompiled binaries: &lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/#forge"&gt;&lt;code&gt;forge&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/#cast"&gt;&lt;code&gt;cast&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/#anvil"&gt;&lt;code&gt;anvil&lt;/code&gt;&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/#chisel"&gt;&lt;code&gt;chisel&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;foundryup
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Done!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;For additional details see the &lt;a href="https://getfoundry.sh/getting-started/installation"&gt;installation guide&lt;/a&gt; in the &lt;a href="https://getfoundry.sh"&gt;Foundry Docs&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you're experiencing any issues while installing, check out &lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/#getting-help"&gt;Getting Help&lt;/a&gt; and the &lt;a href="https://getfoundry.sh/faq"&gt;FAQ&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;How Fast?&lt;/h2&gt; 
&lt;p&gt;Forge is quite fast at both compiling (leveraging &lt;code&gt;solc&lt;/code&gt; with &lt;a href="https://github.com/foundry-rs/compilers"&gt;foundry-compilers&lt;/a&gt;) and testing.&lt;/p&gt; 
&lt;p&gt;See the benchmarks below. Older benchmarks against &lt;a href="https://github.com/dapphub/dapptools"&gt;DappTools&lt;/a&gt; can be found in the &lt;a href="https://www.paradigm.xyz/2022/03/foundry-02#blazing-fast-compilation--testing"&gt;v0.2.0 announcement post&lt;/a&gt; and in the &lt;a href="https://github.com/mds1/convex-shutdown-simulation"&gt;Convex Shutdown Simulation&lt;/a&gt; repository.&lt;/p&gt; 
&lt;h3&gt;Testing Benchmarks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Project&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://github.com/foundry-rs/foundry/releases/tag/nightly-59f354c179f4e7f6d7292acb3d068815c79286d1"&gt;Forge 1.0&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://github.com/foundry-rs/foundry/releases/tag/nightly-5b7e4cb3c882b28f3c32ba580de27ce7381f415a"&gt;Forge 0.2&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;DappTools&lt;/th&gt; 
   &lt;th&gt;Speedup&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Vectorized/solady"&gt;vectorized/solady&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Unit / Fuzz&lt;/td&gt; 
   &lt;td&gt;0.9s&lt;/td&gt; 
   &lt;td&gt;2.3s&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;2.6x&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/morpho-org/morpho-blue"&gt;morpho-org/morpho-blue&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Invariant&lt;/td&gt; 
   &lt;td&gt;0.7s&lt;/td&gt; 
   &lt;td&gt;1m43s&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;147.1x&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/morpho-org/morpho-blue"&gt;morpho-org/morpho-blue-oracles&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Integration (Cold)&lt;/td&gt; 
   &lt;td&gt;6.1s&lt;/td&gt; 
   &lt;td&gt;6.3s&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;1.04x&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/morpho-org/morpho-blue"&gt;morpho-org/morpho-blue-oracles&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Integration (Cached)&lt;/td&gt; 
   &lt;td&gt;0.6s&lt;/td&gt; 
   &lt;td&gt;0.9s&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;1.50x&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/transmissions11/solmate/"&gt;transmissions11/solmate&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Unit / Fuzz&lt;/td&gt; 
   &lt;td&gt;2.7s&lt;/td&gt; 
   &lt;td&gt;2.8s&lt;/td&gt; 
   &lt;td&gt;6m34s&lt;/td&gt; 
   &lt;td&gt;1.03x / 140.0x&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/reflexer-labs/geb"&gt;reflexer-labs/geb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Unit / Fuzz&lt;/td&gt; 
   &lt;td&gt;0.2s&lt;/td&gt; 
   &lt;td&gt;0.4s&lt;/td&gt; 
   &lt;td&gt;23s&lt;/td&gt; 
   &lt;td&gt;2.0x / 57.5x&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;em&gt;In the above benchmarks, compilation was always skipped&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Takeaway: Forge dramatically outperforms the competition, delivering blazing-fast execution speeds while continuously expanding its robust feature set.&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Compilation Benchmarks&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset=".github/assets/build_benchmark_solady_dark.png" width="600px"&gt; 
  &lt;img src="https://raw.githubusercontent.com/foundry-rs/foundry/master/.github/assets/build_benchmark_solady_light.png" width="600px"&gt; 
 &lt;/picture&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset=".github/assets/build_benchmark_openzeppelin_dark.png" width="600px"&gt; 
  &lt;img src="https://raw.githubusercontent.com/foundry-rs/foundry/master/.github/assets/build_benchmark_openzeppelin_light.png" width="600px"&gt; 
 &lt;/picture&gt; 
 &lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Takeaway: Forge compilation is consistently faster than Hardhat by a factor of &lt;code&gt;2.1x&lt;/code&gt; to &lt;code&gt;5.2x&lt;/code&gt;, depending on the amount of caching involved.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Forge&lt;/h2&gt; 
&lt;p&gt;Forge helps you build, test, fuzz, debug and deploy Solidity contracts.&lt;/p&gt; 
&lt;p&gt;The best way to understand Forge is to simply try it (in less than 30 seconds!).&lt;/p&gt; 
&lt;p&gt;First, let's initialize a new &lt;code&gt;counter&lt;/code&gt; example repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;forge init counter
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Next &lt;code&gt;cd&lt;/code&gt; into &lt;code&gt;counter&lt;/code&gt; and build :&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;forge build
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;[‚†ä] Compiling...
[‚†î] Compiling 27 files with Solc 0.8.28
[‚†í] Solc 0.8.28 finished in 452.13ms
Compiler run successful!
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Let's &lt;a href="https://getfoundry.sh/forge/tests#tests"&gt;test&lt;/a&gt; our contracts:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;forge test
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;[‚†ä] Compiling...
No files changed, compilation skipped

Ran 2 tests for test/Counter.t.sol:CounterTest
[PASS] testFuzz_SetNumber(uint256) (runs: 256, Œº: 31121, ~: 31277)
[PASS] test_Increment() (gas: 31293)
Suite result: ok. 2 passed; 0 failed; 0 skipped; finished in 5.35ms (4.86ms CPU time)

Ran 1 test suite in 5.91ms (5.35ms CPU time): 2 tests passed, 0 failed, 0 skipped (2 total tests)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Finally, let's run our deployment script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;forge script script/Counter.s.sol
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;[‚†ä] Compiling...
No files changed, compilation skipped
Script ran successfully.
Gas used: 109037

If you wish to simulate on-chain transactions pass a RPC URL.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run &lt;code&gt;forge --help&lt;/code&gt; to explore the full list of available subcommands and their usage.&lt;/p&gt; 
&lt;p&gt;More documentation can be found in the &lt;a href="https://getfoundry.sh/forge/overview"&gt;forge&lt;/a&gt; section of the Foundry Docs.&lt;/p&gt; 
&lt;h2&gt;Cast&lt;/h2&gt; 
&lt;p&gt;Cast is a Swiss Army knife for interacting with Ethereum applications from the command line.&lt;/p&gt; 
&lt;p&gt;Here are a few examples of what you can do:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Check the latest block on Ethereum Mainnet&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cast block-number --rpc-url https://eth.merkle.io
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Check the Ether balance of &lt;code&gt;vitalik.eth&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cast balance vitalik.eth --ether --rpc-url https://eth.merkle.io
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Replay and trace a transaction&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cast run 0x9c32042f5e997e27e67f82583839548eb19dc78c4769ad6218657c17f2a5ed31 --rpc-url https://eth.merkle.io
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Optionally, pass &lt;code&gt;--etherscan-api-key &amp;lt;API_KEY&amp;gt;&lt;/code&gt; to decode transaction traces using verified source maps, providing more detailed and human-readable information.&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;Run &lt;code&gt;cast --help&lt;/code&gt; to explore the full list of available subcommands and their usage.&lt;/p&gt; 
&lt;p&gt;More documentation can be found in the &lt;a href="https://getfoundry.sh/cast/overview"&gt;cast&lt;/a&gt; section of the Foundry Docs.&lt;/p&gt; 
&lt;h2&gt;Anvil&lt;/h2&gt; 
&lt;p&gt;Anvil is a fast local Ethereum development node.&lt;/p&gt; 
&lt;p&gt;Let's fork Ethereum mainnet at the latest block:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;anvil --fork-url https://eth.merkle.io
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can use those same &lt;code&gt;cast&lt;/code&gt; subcommands against your &lt;code&gt;anvil&lt;/code&gt; instance:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cast block-number
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;p&gt;Run &lt;code&gt;anvil --help&lt;/code&gt; to explore the full list of available features and their usage.&lt;/p&gt; 
&lt;p&gt;More documentation can be found in the &lt;a href="https://getfoundry.sh/anvil/overview"&gt;anvil&lt;/a&gt; section of the Foundry Docs.&lt;/p&gt; 
&lt;h2&gt;Chisel&lt;/h2&gt; 
&lt;p&gt;Chisel is a fast, utilitarian, and verbose Solidity REPL.&lt;/p&gt; 
&lt;p&gt;To use Chisel, simply type &lt;code&gt;chisel&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;chisel
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;From here, start writing Solidity code! Chisel will offer verbose feedback on each input.&lt;/p&gt; 
&lt;p&gt;Create a variable &lt;code&gt;a&lt;/code&gt; and query it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;‚ûú uint256 a = 123;
‚ûú a
Type: uint256
‚îú Hex: 0x7b
‚îú Hex (full word): 0x000000000000000000000000000000000000000000000000000000000000007b
‚îî Decimal: 123
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Finally, run &lt;code&gt;!source&lt;/code&gt; to see &lt;code&gt;a&lt;/code&gt; was applied:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-solidity"&gt;// SPDX-License-Identifier: UNLICENSED
pragma solidity ^0.8.28;

import {Vm} from "forge-std/Vm.sol";

contract REPL {
    Vm internal constant vm = Vm(address(uint160(uint256(keccak256("hevm cheat code")))));

    /// @notice REPL contract entry point
    function run() public {
        uint256 a = 123;
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;p&gt;Run &lt;code&gt;chisel --help&lt;/code&gt; to explore the full list of available features and their usage.&lt;/p&gt; 
&lt;p&gt;More documentation can be found in the &lt;a href="https://getfoundry.sh/chisel/overview"&gt;chisel&lt;/a&gt; section of the Foundry Docs.&lt;/p&gt; 
&lt;h2&gt;Configuration&lt;/h2&gt; 
&lt;p&gt;Foundry is highly configurable, allowing you to tailor it to your needs. Configuration is managed via a file called &lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/crates/config"&gt;&lt;code&gt;foundry.toml&lt;/code&gt;&lt;/a&gt; located in the root of your project or any parent directory. For a full list of configuration options, refer to the &lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/crates/config/README.md#all-options"&gt;config package documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Profiles and Namespaces&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Configuration can be organized into &lt;strong&gt;profiles&lt;/strong&gt;, which are arbitrarily namespaced for flexibility.&lt;/li&gt; 
 &lt;li&gt;The default profile is named &lt;code&gt;default&lt;/code&gt;. Learn more in the &lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/crates/config/README.md#default-profile"&gt;Default Profile section&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;To select a different profile, set the &lt;code&gt;FOUNDRY_PROFILE&lt;/code&gt; environment variable.&lt;/li&gt; 
 &lt;li&gt;Override specific settings using environment variables prefixed with &lt;code&gt;FOUNDRY_&lt;/code&gt; (e.g., &lt;code&gt;FOUNDRY_SRC&lt;/code&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;p&gt;You can find additional &lt;a href="https://getfoundry.sh/config/overview"&gt;setup and configurations guides&lt;/a&gt; in the &lt;a href="https://getfoundry.sh"&gt;Foundry Docs&lt;/a&gt; and in the &lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/crates/config/README.md"&gt;config crate&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://getfoundry.sh/config/overview"&gt;Configuring with &lt;code&gt;foundry.toml&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://getfoundry.sh/config/vscode.html"&gt;Setting up VSCode&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://getfoundry.sh/config/shell-autocompletion.html"&gt;Shell autocompletions&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See our &lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting Help&lt;/h2&gt; 
&lt;p&gt;First, see if the answer to your question can be found in the &lt;a href="https://getfoundry.sh"&gt;Foundy Docs&lt;/a&gt;, or in the relevant crate.&lt;/p&gt; 
&lt;p&gt;If the answer is not there:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Join the &lt;a href="https://t.me/foundry_support"&gt;support Telegram&lt;/a&gt; to get help, or&lt;/li&gt; 
 &lt;li&gt;Open a &lt;a href="https://github.com/foundry-rs/foundry/discussions/new"&gt;discussion&lt;/a&gt; with your question, or&lt;/li&gt; 
 &lt;li&gt;Open an issue with &lt;a href="https://github.com/foundry-rs/foundry/issues/new"&gt;the bug&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you want to contribute, or follow along with contributor discussion, you can use our &lt;a href="https://t.me/foundry_rs"&gt;main telegram&lt;/a&gt; to chat with us about the development of Foundry!&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Licensed under either of &lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/LICENSE-APACHE"&gt;Apache License&lt;/a&gt;, Version 2.0 or &lt;a href="https://raw.githubusercontent.com/foundry-rs/foundry/master/LICENSE-MIT"&gt;MIT License&lt;/a&gt; at your option.&lt;/p&gt; 
&lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in these crates by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Foundry is a clean-room rewrite of the testing framework &lt;a href="https://github.com/dapphub/dapptools"&gt;DappTools&lt;/a&gt;. None of this would have been possible without the DappHub team's work over the years.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/mattsse_"&gt;Matthias Seitz&lt;/a&gt;: Created &lt;a href="https://github.com/gakonst/ethers-rs/tree/master/ethers-solc/"&gt;ethers-solc&lt;/a&gt; (now &lt;a href="https://github.com/foundry-rs/compilers"&gt;foundry-compilers&lt;/a&gt;) which is the backbone of our compilation pipeline, as well as countless contributions to ethers, in particular the &lt;code&gt;abigen&lt;/code&gt; macros.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/rohitnarurkar"&gt;Rohit Narurkar&lt;/a&gt;: Created the Rust Solidity version manager &lt;a href="https://github.com/roynalnaruto/svm-rs"&gt;svm-rs&lt;/a&gt; which we use to auto-detect and manage multiple Solidity versions.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/brockjelmore"&gt;Brock Elmore&lt;/a&gt;: For extending the VM's cheatcodes and implementing &lt;a href="https://github.com/foundry-rs/foundry/pull/192"&gt;structured call tracing&lt;/a&gt;, a critical feature for debugging smart contract calls.&lt;/li&gt; 
 &lt;li&gt;All the other &lt;a href="https://github.com/foundry-rs/foundry/graphs/contributors"&gt;contributors&lt;/a&gt; to the &lt;a href="https://github.com/gakonst/ethers-rs"&gt;ethers-rs&lt;/a&gt;, &lt;a href="https://github.com/alloy-rs/alloy"&gt;alloy&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/foundry-rs/foundry"&gt;foundry&lt;/a&gt; repositories and chatrooms.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>