<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Sat, 26 Jul 2025 01:35:12 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>OpenBB-finance/OpenBB</title>
      <link>https://github.com/OpenBB-finance/OpenBB</link>
      <description>&lt;p&gt;Investment Research for Everyone, Everywhere.&lt;/p&gt;&lt;hr&gt;&lt;br&gt; 
&lt;img src="https://github.com/OpenBB-finance/OpenBB/raw/develop/images/platform-light.svg?raw=true#gh-light-mode-only" alt="OpenBB Platform logo" width="600"&gt; 
&lt;img src="https://github.com/OpenBB-finance/OpenBB/raw/develop/images/platform-dark.svg?raw=true#gh-dark-mode-only" alt="OpenBB Platform logo" width="600"&gt; 
&lt;br&gt; 
&lt;br&gt; 
&lt;p&gt;&lt;a href="https://x.com/openbb_finance"&gt;&lt;img src="https://img.shields.io/twitter/url/https/twitter.com/openbb_finance.svg?style=social&amp;amp;label=Follow%20%40openbb_finance" alt="Twitter"&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/xPHTuHCmuV"&gt;&lt;img src="https://img.shields.io/discord/831165782750789672" alt="Discord Shield"&gt;&lt;/a&gt; &lt;a href="https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/OpenBB-finance/OpenBB"&gt;&lt;img src="https://img.shields.io/static/v1?label=Dev%20Containers&amp;amp;message=Open&amp;amp;color=blue&amp;amp;logo=visualstudiocode" alt="Open in Dev Containers"&gt;&lt;/a&gt; &lt;a href="https://codespaces.new/OpenBB-finance/OpenBB"&gt; &lt;img src="https://github.com/codespaces/badge.svg?sanitize=true" height="20"&gt; &lt;/a&gt; &lt;a target="_blank" href="https://colab.research.google.com/github/OpenBB-finance/OpenBB/blob/develop/examples/googleColab.ipynb"&gt; &lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab"&gt; &lt;/a&gt; &lt;a href="https://pypi.org/project/openbb/"&gt;&lt;img src="https://img.shields.io/pypi/v/openbb?color=blue&amp;amp;label=PyPI%20Package" alt="PyPI"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The first financial Platform that is open source.&lt;/p&gt; 
&lt;p&gt;The OpenBB Platform offers access to equity, options, crypto, forex, macro economy, fixed income, and more while also offering a broad range of extensions to enhance the user experience according to their needs.&lt;/p&gt; 
&lt;p&gt;Get started with: &lt;code&gt;pip install openbb&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openbb import obb
output = obb.equity.price.historical("AAPL")
df = output.to_dataframe()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can sign up to the &lt;a href="https://my.openbb.co/login"&gt;OpenBB Hub&lt;/a&gt; to get the most out of the OpenBB ecosystem.&lt;/p&gt; 
&lt;p&gt;Data integrations available can be found here: &lt;a href="https://docs.openbb.co/platform/reference"&gt;https://docs.openbb.co/platform/reference&lt;/a&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;h2&gt;OpenBB Workspace&lt;/h2&gt; 
&lt;p&gt;While the OpenBB Platform is all about an integration to dozens of different data vendors, the interface is either Python or a CLI.&lt;/p&gt; 
&lt;p&gt;If you want an enterprise UI to visualize this datasets and use AI agents on top, you can find OpenBB Workspace at &lt;a href="https://pro.openbb.co"&gt;https://pro.openbb.co&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://pro.openbb.co"&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://openbb-cms.directus.app/assets/f69b6aaf-0821-4bc8-a43c-715e03a924ef.png" alt="Logo" width="1000"&gt; 
 &lt;/div&gt; &lt;/a&gt; 
&lt;p&gt;Data integration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can learn more about adding data to the OpenBB workspace from the &lt;a href="https://docs.openbb.co/workspace"&gt;docs&lt;/a&gt; or &lt;a href="https://github.com/OpenBB-finance/backends-for-openbb"&gt;this open source repository&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;AI Agents integration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can learn more about adding AI agents to the OpenBB workspace from &lt;a href="https://github.com/OpenBB-finance/agents-for-openbb"&gt;this open source repository&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Integrating OpenBB Platform to the OpenBB Workspace&lt;/h3&gt; 
&lt;p&gt;Connect this library to the OpenBB Workspace with a few simple commands, in a Python (3.9.21 - 3.12) environment.&lt;/p&gt; 
&lt;h4&gt;Run OpenBB Platform backend&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install the packages.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install "openbb[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Start the API server over localhost.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;openbb-api
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will launch a FastAPI server, via Uvicorn, at &lt;code&gt;127.0.0.1:6900&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can check that it works by going to &lt;a href="http://127.0.0.1:6900"&gt;http://127.0.0.1:6900&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Integrate OpenBB Platform backend to OpenBB Workspace&lt;/h4&gt; 
&lt;p&gt;Sign-in to the &lt;a href="https://pro.openbb.co/"&gt;OpenBB Workspace&lt;/a&gt;, and follow the following steps:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/75cffb4a-5e95-470a-b9d0-6ffd4067e069" alt="CleanShot 2025-05-17 at 09 51 56@2x"&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Go to the "Apps" tab&lt;/li&gt; 
 &lt;li&gt;Click on "Connect backend"&lt;/li&gt; 
 &lt;li&gt;Fill in the form with: Name: OpenBB Platform URL: &lt;a href="http://127.0.0.1:6900"&gt;http://127.0.0.1:6900&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Click on "Test". You should get a "Test successful" with the number of apps found.&lt;/li&gt; 
 &lt;li&gt;Click on "Add".&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;That's it.&lt;/p&gt; 
&lt;hr&gt; 
&lt;!-- TABLE OF CONTENTS --&gt; 
&lt;details closed="closed"&gt; 
 &lt;summary&gt;&lt;h2 style="display: inline-block"&gt;Table of Contents&lt;/h2&gt;&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#1-installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#2-contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#3-license"&gt;License&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#4-disclaimer"&gt;Disclaimer&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#5-contacts"&gt;Contacts&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#6-star-history"&gt;Star History&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#7-contributors"&gt;Contributors&lt;/a&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h2&gt;1. Installation&lt;/h2&gt; 
&lt;p&gt;The OpenBB Platform can be installed as a &lt;a href="https://pypi.org/project/openbb/"&gt;PyPI package&lt;/a&gt; by running &lt;code&gt;pip install openbb&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;or by cloning the repository directly with &lt;code&gt;git clone https://github.com/OpenBB-finance/OpenBB.git&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Please find more about the installation process, in the &lt;a href="https://docs.openbb.co/platform/installation"&gt;OpenBB Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;OpenBB Platform CLI installation&lt;/h3&gt; 
&lt;p&gt;The OpenBB Platform CLI is a command-line interface that allows you to access the OpenBB Platform directly from your command line.&lt;/p&gt; 
&lt;p&gt;It can be installed by running &lt;code&gt;pip install openbb-cli&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;or by cloning the repository directly with &lt;code&gt;git clone https://github.com/OpenBB-finance/OpenBB.git&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Please find more about the installation process in the &lt;a href="https://docs.openbb.co/cli/installation"&gt;OpenBB Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;2. Contributing&lt;/h2&gt; 
&lt;p&gt;There are three main ways of contributing to this project. (Hopefully you have starred the project by now ⭐️)&lt;/p&gt; 
&lt;h3&gt;Become a Contributor&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;More information on our &lt;a href="https://docs.openbb.co/platform/developer_guide/misc/contributing"&gt;Contributing Documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Create a GitHub ticket&lt;/h3&gt; 
&lt;p&gt;Before creating a ticket make sure the one you are creating doesn't exist already &lt;a href="https://github.com/OpenBB-finance/OpenBB/issues"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=bug&amp;amp;template=bug_report.md&amp;amp;title=%5BBug%5D"&gt;Report bug&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=enhancement&amp;amp;template=enhancement.md&amp;amp;title=%5BIMPROVE%5D"&gt;Suggest improvement&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=new+feature&amp;amp;template=feature_request.md&amp;amp;title=%5BFR%5D"&gt;Request a feature&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Provide feedback&lt;/h3&gt; 
&lt;p&gt;We are most active on &lt;a href="https://openbb.co/discord"&gt;our Discord&lt;/a&gt;, but feel free to reach out to us in any of &lt;a href="https://openbb.co/links"&gt;our social media&lt;/a&gt; for feedback.&lt;/p&gt; 
&lt;h2&gt;3. License&lt;/h2&gt; 
&lt;p&gt;Distributed under the AGPLv3 License. See &lt;a href="https://github.com/OpenBB-finance/OpenBB/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;4. Disclaimer&lt;/h2&gt; 
&lt;p&gt;Trading in financial instruments involves high risks including the risk of losing some, or all, of your investment amount, and may not be suitable for all investors.&lt;/p&gt; 
&lt;p&gt;Before deciding to trade in a financial instrument you should be fully informed of the risks and costs associated with trading the financial markets, carefully consider your investment objectives, level of experience, and risk appetite, and seek professional advice where needed.&lt;/p&gt; 
&lt;p&gt;The data contained in the OpenBB Platform is not necessarily accurate.&lt;/p&gt; 
&lt;p&gt;OpenBB and any provider of the data contained in this website will not accept liability for any loss or damage as a result of your trading, or your reliance on the information displayed.&lt;/p&gt; 
&lt;p&gt;All names, logos, and brands of third parties that may be referenced in our sites, products or documentation are trademarks of their respective owners. Unless otherwise specified, OpenBB and its products and services are not endorsed by, sponsored by, or affiliated with these third parties.&lt;/p&gt; 
&lt;p&gt;Our use of these names, logos, and brands is for identification purposes only, and does not imply any such endorsement, sponsorship, or affiliation.&lt;/p&gt; 
&lt;h2&gt;5. Contacts&lt;/h2&gt; 
&lt;p&gt;If you have any questions about the platform or anything OpenBB, feel free to email us at &lt;code&gt;support@openbb.co&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;If you want to say hi, or are interested in partnering with us, feel free to reach us at &lt;code&gt;hello@openbb.co&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Any of our social media platforms: &lt;a href="https://openbb.co/links"&gt;openbb.co/links&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;6. Star History&lt;/h2&gt; 
&lt;p&gt;This is a proxy of our growth and that we are just getting started.&lt;/p&gt; 
&lt;p&gt;But for more metrics important to us check &lt;a href="https://openbb.co/open"&gt;openbb.co/open&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;amp;type=Date&amp;amp;theme=dark"&gt;&lt;img src="https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;amp;type=Date&amp;amp;theme=dark" alt="Star History Chart"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;7. Contributors&lt;/h2&gt; 
&lt;p&gt;OpenBB wouldn't be OpenBB without you. If we are going to disrupt financial industry, every contribution counts. Thank you for being part of this journey.&lt;/p&gt; 
&lt;a href="https://github.com/OpenBB-finance/OpenBB/graphs/contributors"&gt; &lt;img src="https://contributors-img.web.app/image?repo=OpenBB-finance/OpenBB" width="800"&gt; &lt;/a&gt; 
&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; 
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</description>
    </item>
    
    <item>
      <title>yeongpin/cursor-free-vip</title>
      <link>https://github.com/yeongpin/cursor-free-vip</link>
      <description>&lt;p&gt;[Support 0.49.x]（Reset Cursor AI MachineID &amp; Bypass Higher Token Limit） Cursor Ai ，自动重置机器ID ， 免费升级使用Pro功能: You've reached your trial request limit. / Too many free trial accounts used on this machine. Please upgrade to pro. We have this limit in place to prevent abuse. Please let us know if you believe this is a mistake.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;➤ Cursor Free VIP&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/images/logo.png" alt="Cursor Pro Logo" width="200" style="border-radius: 6px;"&gt; &lt;/p&gt; 
 &lt;p align="center"&gt; &lt;/p&gt;
 &lt;p&gt;&lt;a href="https://github.com/yeongpin/cursor-free-vip/releases/latest"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/release/yeongpin/cursor-free-vip" alt="Release"&gt;&lt;/a&gt; &lt;a href="https://creativecommons.org/licenses/by-nc-nd/4.0/"&gt;&lt;img src="https://img.shields.io/badge/License-CC_BY--NC--ND_4.0-lightgrey.svg?sanitize=true" alt="License: CC BY-NC-ND 4.0"&gt;&lt;/a&gt; &lt;a href="https://github.com/yeongpin/cursor-free-vip/stargazers"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/stars/yeongpin/cursor-free-vip" alt="Stars"&gt;&lt;/a&gt; &lt;a href="https://github.com/yeongpin/cursor-free-vip/releases/latest"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/downloads/yeongpin/cursor-free-vip/total" alt="Downloads"&gt;&lt;/a&gt; &lt;a href="https://buymeacoffee.com/yeongpin" target="_blank"&gt;&lt;img alt="Buy Me a Coffee" src="https://img.shields.io/badge/Buy%20Me%20a%20Coffee-Support%20Me-FFDA33"&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/yeongpin/cursor-free-vip"&gt;&lt;img src="https://devin.ai/assets/deepwiki-badge.png" alt="Ask DeepWiki.com" height="20"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/13425" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13425" alt="yeongpin%2Fcursor-free-vip | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"&gt;&lt;/a&gt; &lt;br&gt; &lt;a href="https://www.buymeacoffee.com/yeongpin" target="_blank"&gt; &lt;img src="https://img.buymeacoffee.com/button-api/?text=buy%20me%20a%20coffee&amp;amp;emoji=%E2%98%95&amp;amp;slug=yeongpin&amp;amp;button_colour=ffda33&amp;amp;font_colour=000000&amp;amp;font_family=Bree&amp;amp;outline_colour=000000&amp;amp;coffee_colour=FFDD00&amp;amp;latest=2" width="160" height="55" alt="Buy Me a Coffee"&gt; &lt;/a&gt;&lt;/p&gt; 
 &lt;h4&gt;Support Latest 0.49.x Version | 支持最新 0.49.x 版本&lt;/h4&gt; 
 &lt;p&gt;This tool is for educational purposes, currently the repo does not violate any laws. Please support the original project. This tool will not generate any fake email accounts and OAuth access.&lt;/p&gt; 
 &lt;p&gt;Supports Windows, macOS and Linux.&lt;/p&gt; 
 &lt;p&gt;For optimal performance, run with privileges and always stay up to date.&lt;/p&gt; 
 &lt;p&gt;這是一款用於學習和研究的工具，目前 repo 沒有違反任何法律。請支持原作者。 這款工具不會生成任何假的電子郵件帳戶和 OAuth 訪問。&lt;/p&gt; 
 &lt;p&gt;支持 Windows、macOS 和 Linux。&lt;/p&gt; 
 &lt;p&gt;對於最佳性能，請以管理員身份運行並始終保持最新。&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/images/product_2025-04-16_10-40-21.png" alt="new" width="800" style="border-radius: 6px;"&gt;&lt;br&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;🔄 Change Log | 更新日志&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/CHANGELOG.md"&gt;Watch Change Log | 查看更新日志&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;✨ Features | 功能特點&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Support Windows macOS and Linux systems&lt;br&gt;支持 Windows、macOS 和 Linux 系統&lt;br&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Reset Cursor's configuration&lt;br&gt;重置 Cursor 的配置&lt;br&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Multi-language support (English, 简体中文, 繁體中文, Vietnamese)&lt;br&gt;多語言支持（英文、简体中文、繁體中文、越南語）&lt;br&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;💻 System Support | 系統支持&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Operating System&lt;/th&gt; 
   &lt;th&gt;Architecture&lt;/th&gt; 
   &lt;th&gt;Supported&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows&lt;/td&gt; 
   &lt;td&gt;x64, x86&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;macOS&lt;/td&gt; 
   &lt;td&gt;Intel, Apple Silicon&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Linux&lt;/td&gt; 
   &lt;td&gt;x64, x86, ARM64&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;👀 How to use | 如何使用&lt;/h2&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;b&gt;⭐ Auto Run Script | 腳本自動化運行&lt;/b&gt;&lt;/summary&gt; 
 &lt;h3&gt;&lt;strong&gt;Linux/macOS&lt;/strong&gt;&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;curl -fsSL https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/scripts/install.sh -o install.sh &amp;amp;&amp;amp; chmod +x install.sh &amp;amp;&amp;amp; ./install.sh
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;&lt;strong&gt;Archlinux&lt;/strong&gt;&lt;/h3&gt; 
 &lt;p&gt;Install via &lt;a href="https://aur.archlinux.org/packages/cursor-free-vip-git"&gt;AUR&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;yay -S cursor-free-vip-git
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-powershell"&gt;irm https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/scripts/install.ps1 | iex
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;If you want to stop the script, please press Ctrl+C&lt;br&gt;要停止腳本，請按 Ctrl+C&lt;/p&gt; 
&lt;h2&gt;❗ Note | 注意事項&lt;/h2&gt; 
&lt;p&gt;📝 Config | 文件配置 &lt;code&gt;Win / Macos / Linux Path | 路徑 [Documents/.cursor-free-vip/config.ini]&lt;/code&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;⭐ Config | 文件配置&lt;/b&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;[Chrome]
# Default Google Chrome Path | 默認Google Chrome 遊覽器路徑
chromepath = C:\Program Files\Google/Chrome/Application/chrome.exe

[Turnstile]
# Handle Turnstile Wait Time | 等待人機驗證時間
handle_turnstile_time = 2
# Handle Turnstile Wait Random Time (must merge 1-3 or 1,3) | 等待人機驗證隨機時間（必須是 1-3 或者 1,3 這樣的組合）
handle_turnstile_random_time = 1-3

[OSPaths]
# Storage Path | 存儲路徑
storage_path = /Users/username/Library/Application Support/Cursor/User/globalStorage/storage.json
# SQLite Path | SQLite路徑
sqlite_path = /Users/username/Library/Application Support/Cursor/User/globalStorage/state.vscdb
# Machine ID Path | 機器ID路徑
machine_id_path = /Users/username/Library/Application Support/Cursor/machineId
# For Linux users: ~/.config/cursor/machineid

[Timing]
# Min Random Time | 最小隨機時間
min_random_time = 0.1
# Max Random Time | 最大隨機時間
max_random_time = 0.8
# Page Load Wait | 頁面加載等待時間
page_load_wait = 0.1-0.8
# Input Wait | 輸入等待時間
input_wait = 0.3-0.8
# Submit Wait | 提交等待時間
submit_wait = 0.5-1.5
# Verification Code Input | 驗證碼輸入等待時間
verification_code_input = 0.1-0.3
# Verification Success Wait | 驗證成功等待時間
verification_success_wait = 2-3
# Verification Retry Wait | 驗證重試等待時間
verification_retry_wait = 2-3
# Email Check Initial Wait | 郵件檢查初始等待時間
email_check_initial_wait = 4-6
# Email Refresh Wait | 郵件刷新等待時間
email_refresh_wait = 2-4
# Settings Page Load Wait | 設置頁面加載等待時間
settings_page_load_wait = 1-2
# Failed Retry Time | 失敗重試時間
failed_retry_time = 0.5-1
# Retry Interval | 重試間隔
retry_interval = 8-12
# Max Timeout | 最大超時時間
max_timeout = 160

[Utils]
# Check Update | 檢查更新
check_update = True
# Show Account Info | 顯示賬號信息
show_account_info = True

[TempMailPlus]
# Enable TempMailPlus | 啓用 TempMailPlus（任何轉發到TempMailPlus的郵件都支持獲取驗證碼，例如cloudflare郵件Catch-all）
enabled = false
# TempMailPlus Email | TempMailPlus 電子郵件
email = xxxxx@mailto.plus
# TempMailPlus pin | TempMailPlus pin碼
epin = 

[WindowsPaths]
storage_path = C:\Users\yeongpin\AppData\Roaming\Cursor\User\globalStorage\storage.json
sqlite_path = C:\Users\yeongpin\AppData\Roaming\Cursor\User\globalStorage\state.vscdb
machine_id_path = C:\Users\yeongpin\AppData\Roaming\Cursor\machineId
cursor_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app
updater_path = C:\Users\yeongpin\AppData\Local\cursor-updater
update_yml_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app-update.yml
product_json_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app\product.json

[Browser]
default_browser = opera
chrome_path = C:\Program Files\Google\Chrome\Application\chrome.exe
edge_path = C:\Program Files (x86)\Microsoft\Edge\Application\msedge.exe
firefox_path = C:\Program Files\Mozilla Firefox\firefox.exe
brave_path = C:\Program Files\BraveSoftware/Brave-Browser/Application/brave.exe
chrome_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe
edge_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\msedgedriver.exe
firefox_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\geckodriver.exe
brave_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe
opera_path = C:\Users\yeongpin\AppData\Local\Programs\Opera\opera.exe
opera_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe

[OAuth]
show_selection_alert = False
timeout = 120
max_attempts = 3
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Use administrator privileges to run the script &lt;br&gt;請使用管理員身份運行腳本&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Confirm that Cursor is closed before running the script &lt;br&gt;請確保在運行腳本前已經關閉 Cursor&lt;br&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;This tool is only for learning and research purposes &lt;br&gt;此工具僅供學習和研究使用&lt;br&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Please comply with the relevant software usage terms when using this tool &lt;br&gt;使用本工具時請遵守相關軟件使用條款&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🚨 Common Issues | 常見問題&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;如果遇到權限問題，請確保：&lt;/th&gt; 
   &lt;th align="center"&gt;此腳本以管理員身份運行&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;If you encounter permission issues, please ensure:&lt;/td&gt; 
   &lt;td align="center"&gt;This script is run with administrator privileges&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Error 'User is not authorized'&lt;/td&gt; 
   &lt;td align="center"&gt;This means your account was banned for using temporary (disposal) mail. Ensure using a non-temporary mail service&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;🤩 Contribution | 貢獻&lt;/h2&gt; 
&lt;p&gt;歡迎提交 Issue 和 Pull Request！&lt;/p&gt; 
&lt;a href="https://github.com/yeongpin/cursor-free-vip/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=yeongpin/cursor-free-vip&amp;amp;preview=true&amp;amp;max=&amp;amp;columns="&gt; &lt;/a&gt; 
&lt;br&gt;
&lt;br&gt; 
&lt;h2&gt;📩 Disclaimer | 免責聲明&lt;/h2&gt; 
&lt;p&gt;本工具僅供學習和研究使用，使用本工具所產生的任何後果由使用者自行承擔。 &lt;br&gt;&lt;/p&gt; 
&lt;p&gt;This tool is only for learning and research purposes, and any consequences arising from the use of this tool are borne by the user.&lt;/p&gt; 
&lt;h2&gt;💰 Buy Me a Coffee | 請我喝杯咖啡&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/images/provi-code.jpg" alt="buy_me_a_coffee" width="280"&gt;&lt;br&gt; &lt;/td&gt; 
    &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/images/paypal.png" alt="buy_me_a_coffee" width="280"&gt;&lt;br&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;⭐ Star History | 星星數&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://star-history.com/#yeongpin/cursor-free-vip&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=yeongpin/cursor-free-vip&amp;amp;type=Date" alt="Star History Chart"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;📝 License | 授權&lt;/h2&gt; 
&lt;p&gt;本項目採用 &lt;a href="https://creativecommons.org/licenses/by-nc-nd/4.0/"&gt;CC BY-NC-ND 4.0&lt;/a&gt; 授權。 Please refer to the &lt;a href="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/LICENSE.md"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>pytorch/vision</title>
      <link>https://github.com/pytorch/vision</link>
      <description>&lt;p&gt;Datasets, Transforms and Models specific to Computer Vision&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;torchvision&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pepy.tech/project/torchvision"&gt;&lt;img src="https://pepy.tech/badge/torchvision" alt="total torchvision downloads"&gt;&lt;/a&gt; &lt;a href="https://pytorch.org/vision/stable/index.html"&gt;&lt;img src="https://img.shields.io/badge/dynamic/json.svg?label=docs&amp;amp;url=https%3A%2F%2Fpypi.org%2Fpypi%2Ftorchvision%2Fjson&amp;amp;query=%24.info.version&amp;amp;colorB=brightgreen&amp;amp;prefix=v" alt="documentation"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The torchvision package consists of popular datasets, model architectures, and common image transformations for computer vision.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Please refer to the &lt;a href="https://pytorch.org/get-started/locally/"&gt;official instructions&lt;/a&gt; to install the stable versions of &lt;code&gt;torch&lt;/code&gt; and &lt;code&gt;torchvision&lt;/code&gt; on your system.&lt;/p&gt; 
&lt;p&gt;To build source, refer to our &lt;a href="https://github.com/pytorch/vision/raw/main/CONTRIBUTING.md#development-installation"&gt;contributing page&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The following is the corresponding &lt;code&gt;torchvision&lt;/code&gt; versions and supported Python versions.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;code&gt;torch&lt;/code&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;code&gt;torchvision&lt;/code&gt;&lt;/th&gt; 
   &lt;th&gt;Python&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;main&lt;/code&gt; / &lt;code&gt;nightly&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;main&lt;/code&gt; / &lt;code&gt;nightly&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;&amp;gt;=3.9&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.12&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;2.5&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;0.20&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;&amp;gt;=3.9&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.12&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;2.4&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;0.19&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;&amp;gt;=3.8&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.12&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;2.3&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;0.18&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;&amp;gt;=3.8&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.12&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;2.2&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;0.17&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;&amp;gt;=3.8&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.11&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;2.1&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;0.16&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;&amp;gt;=3.8&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.11&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;2.0&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;0.15&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;&amp;gt;=3.8&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.11&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;details&gt; 
 &lt;summary&gt;older versions&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;code&gt;torch&lt;/code&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;code&gt;torchvision&lt;/code&gt;&lt;/th&gt; 
    &lt;th&gt;Python&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;1.13&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;0.14&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;&amp;gt;=3.7.2&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.10&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;1.12&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;0.13&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;&amp;gt;=3.7&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.10&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;1.11&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;0.12&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;&amp;gt;=3.7&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.10&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;1.10&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;0.11&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;&amp;gt;=3.6&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.9&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;1.9&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;0.10&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;&amp;gt;=3.6&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.9&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;1.8&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;0.9&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;&amp;gt;=3.6&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.9&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;1.7&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;0.8&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;&amp;gt;=3.6&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.9&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;1.6&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;0.7&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;&amp;gt;=3.6&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.8&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;1.5&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;0.6&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;&amp;gt;=3.5&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.8&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;1.4&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;0.5&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;==2.7&lt;/code&gt;, &lt;code&gt;&amp;gt;=3.5&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.8&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;1.3&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;0.4.2&lt;/code&gt; / &lt;code&gt;0.4.3&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;==2.7&lt;/code&gt;, &lt;code&gt;&amp;gt;=3.5&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.7&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;1.2&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;0.4.1&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;==2.7&lt;/code&gt;, &lt;code&gt;&amp;gt;=3.5&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.7&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;1.1&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;0.3&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;==2.7&lt;/code&gt;, &lt;code&gt;&amp;gt;=3.5&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.7&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;&amp;lt;=1.0&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;0.2&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;==2.7&lt;/code&gt;, &lt;code&gt;&amp;gt;=3.5&lt;/code&gt;, &lt;code&gt;&amp;lt;=3.7&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h2&gt;Image Backends&lt;/h2&gt; 
&lt;p&gt;Torchvision currently supports the following image backends:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;torch tensors&lt;/li&gt; 
 &lt;li&gt;PIL images: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://python-pillow.org/"&gt;Pillow&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/uploadcare/pillow-simd"&gt;Pillow-SIMD&lt;/a&gt; - a &lt;strong&gt;much faster&lt;/strong&gt; drop-in replacement for Pillow with SIMD.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Read more in in our &lt;a href="https://pytorch.org/vision/stable/transforms.html"&gt;docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;[UNSTABLE] Video Backend&lt;/h2&gt; 
&lt;p&gt;Torchvision currently supports the following video backends:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/PyAV-Org/PyAV"&gt;pyav&lt;/a&gt; (default) - Pythonic binding for ffmpeg libraries.&lt;/li&gt; 
 &lt;li&gt;video_reader - This needs ffmpeg to be installed and torchvision to be built from source. There shouldn't be any conflicting version of ffmpeg installed. Currently, this is only supported on Linux.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;conda install -c conda-forge 'ffmpeg&amp;lt;4.3'
python setup.py install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Using the models on C++&lt;/h1&gt; 
&lt;p&gt;Refer to &lt;a href="https://github.com/pytorch/vision/tree/main/examples/cpp"&gt;example/cpp&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;DISCLAIMER&lt;/strong&gt;: the &lt;code&gt;libtorchvision&lt;/code&gt; library includes the torchvision custom ops as well as most of the C++ torchvision APIs. Those APIs do not come with any backward-compatibility guarantees and may change from one version to the next. Only the Python APIs are stable and with backward-compatibility guarantees. So, if you need stability within a C++ environment, your best bet is to export the Python APIs via torchscript.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;You can find the API documentation on the pytorch website: &lt;a href="https://pytorch.org/vision/stable/index.html"&gt;https://pytorch.org/vision/stable/index.html&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/pytorch/vision/main/CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt; file for how to help out.&lt;/p&gt; 
&lt;h2&gt;Disclaimer on Datasets&lt;/h2&gt; 
&lt;p&gt;This is a utility library that downloads and prepares public datasets. We do not host or distribute these datasets, vouch for their quality or fairness, or claim that you have license to use the dataset. It is your responsibility to determine whether you have permission to use the dataset under the dataset's license.&lt;/p&gt; 
&lt;p&gt;If you're a dataset owner and wish to update any part of it (description, citation, etc.), or do not want your dataset to be included in this library, please get in touch through a GitHub issue. Thanks for your contribution to the ML community!&lt;/p&gt; 
&lt;h2&gt;Pre-trained Model License&lt;/h2&gt; 
&lt;p&gt;The pre-trained models provided in this library may have their own licenses or terms and conditions derived from the dataset used for training. It is your responsibility to determine whether you have permission to use the models for your use case.&lt;/p&gt; 
&lt;p&gt;More specifically, SWAG models are released under the CC-BY-NC 4.0 license. See &lt;a href="https://github.com/facebookresearch/SWAG/raw/main/LICENSE"&gt;SWAG LICENSE&lt;/a&gt; for additional details.&lt;/p&gt; 
&lt;h2&gt;Citing TorchVision&lt;/h2&gt; 
&lt;p&gt;If you find TorchVision useful in your work, please consider citing the following BibTeX entry:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@software{torchvision2016,
    title        = {TorchVision: PyTorch's Computer Vision library},
    author       = {TorchVision maintainers and contributors},
    year         = 2016,
    journal      = {GitHub repository},
    publisher    = {GitHub},
    howpublished = {\url{https://github.com/pytorch/vision}}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>frappe/hrms</title>
      <link>https://github.com/frappe/hrms</link>
      <description>&lt;p&gt;Open Source HR and Payroll Software&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://frappe.io/hr"&gt; &lt;img src="https://raw.githubusercontent.com/frappe/hrms/develop/.github/frappe-hr-logo.png" height="80px" width="80px" alt="Frappe HR Logo"&gt; &lt;/a&gt; 
 &lt;h2&gt;Frappe HR&lt;/h2&gt; 
 &lt;p align="center"&gt; &lt;/p&gt;
 &lt;p&gt;Open Source, modern, and easy-to-use HR and Payroll Software&lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/frappe/hrms/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/frappe/hrms/actions/workflows/ci.yml/badge.svg?branch=develop" alt="CI"&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/frappe/hrms"&gt;&lt;img src="https://codecov.io/gh/frappe/hrms/branch/develop/graph/badge.svg?token=0TwvyUg3I5" alt="codecov"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/frappe/hrms/develop/.github/hrms-hero.png"&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://frappe.io/hr"&gt;Website&lt;/a&gt; - 
 &lt;a href="https://docs.frappe.io/hr/introduction"&gt;Documentation&lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Frappe HR&lt;/h2&gt; 
&lt;p&gt;Frappe HR has everything you need to drive excellence within the company. It's a complete HRMS solution with over 13 different modules right from Employee Management, Onboarding, Leaves, to Payroll, Taxation, and more!&lt;/p&gt; 
&lt;h2&gt;Motivation&lt;/h2&gt; 
&lt;p&gt;When Frappe team started growing in terms of size, we needed an open-source HR and Payroll software. We didn't find any "true" open-source HR software out there and so decided to build one ourselves. Initially, it was a set of modules within ERPNext but version 14 onwards, as the modules became more mature, Frappe HR was created as a separate product.&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Employee Lifecycle&lt;/strong&gt;: From onboarding employees, managing promotions and transfers, all the way to documenting feedback with exit interviews, make life easier for employees throughout their life cycle.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Leave and Attendance&lt;/strong&gt;: Configure leave policies, pull regional holidays with a click, check-in and check-out with geolocation capturing, track leave balances and attendance with reports.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Expense Claims and Advances&lt;/strong&gt;: Manage employee advances, claim expenses, configure multi-level approval workflows, all this with seamless integration with ERPNext accounting.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Performance Management&lt;/strong&gt;: Track goals, align goals with key result areas (KRAs), enable employees to evaluate themselves, make managing appraisal cycles easy.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Payroll &amp;amp; Taxation&lt;/strong&gt;: Create salary structures, configure income tax slabs, run standard payroll, accomodate additional salaries and off cycle payments, view income breakup on salary slips and so much more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Frappe HR Mobile App&lt;/strong&gt;: Apply for and approve leaves on the go, check-in and check-out, access employee profile right from the mobile app.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details open&gt; 
 &lt;summary&gt;View Screenshots&lt;/summary&gt; 
 &lt;img src="https://raw.githubusercontent.com/frappe/hrms/develop/.github/hrms-appraisal.png"&gt; 
 &lt;img src="https://raw.githubusercontent.com/frappe/hrms/develop/.github/hrms-requisition.png"&gt; 
 &lt;img src="https://raw.githubusercontent.com/frappe/hrms/develop/.github/hrms-attendance.png"&gt; 
 &lt;img src="https://raw.githubusercontent.com/frappe/hrms/develop/.github/hrms-salary.png"&gt; 
 &lt;img src="https://raw.githubusercontent.com/frappe/hrms/develop/.github/hrms-pwa.png"&gt; 
&lt;/details&gt; 
&lt;h3&gt;Under the Hood&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/frappe/frappe"&gt;&lt;strong&gt;Frappe Framework&lt;/strong&gt;&lt;/a&gt;: A full-stack web application framework written in Python and Javascript. The framework provides a robust foundation for building web applications, including a database abstraction layer, user authentication, and a REST API.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/frappe/frappe-ui"&gt;&lt;strong&gt;Frappe UI&lt;/strong&gt;&lt;/a&gt;: A Vue-based UI library, to provide a modern user interface. The Frappe UI library provides a variety of components that can be used to build single-page applications on top of the Frappe Framework.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Production Setup&lt;/h2&gt; 
&lt;h3&gt;Managed Hosting&lt;/h3&gt; 
&lt;p&gt;You can try &lt;a href="https://frappecloud.com"&gt;Frappe Cloud&lt;/a&gt;, a simple, user-friendly and sophisticated &lt;a href="https://github.com/frappe/press"&gt;open-source&lt;/a&gt; platform to host Frappe applications with peace of mind.&lt;/p&gt; 
&lt;p&gt;It takes care of installation, setup, upgrades, monitoring, maintenance and support of your Frappe deployments. It is a fully featured developer platform with an ability to manage and control multiple Frappe deployments.&lt;/p&gt; 
&lt;div&gt; 
 &lt;a href="https://frappecloud.com/hrms/signup" target="_blank"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://frappe.io/files/try-on-fc-white.png"&gt; 
   &lt;img src="https://frappe.io/files/try-on-fc-black.png" alt="Try on Frappe Cloud" height="28"&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Development setup&lt;/h2&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;p&gt;You need Docker, docker-compose and git setup on your machine. Refer &lt;a href="https://docs.docker.com/"&gt;Docker documentation&lt;/a&gt;. After that, run the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/frappe/hrms
cd hrms/docker
docker-compose up
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Wait for some time until the setup script creates a site. After that you can access &lt;code&gt;http://localhost:8000&lt;/code&gt; in your browser and the login screen for HR should show up.&lt;/p&gt; 
&lt;p&gt;Use the following credentials to log in:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Username: &lt;code&gt;Administrator&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Password: &lt;code&gt;admin&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Local&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Set up bench by following the &lt;a href="https://frappeframework.com/docs/user/en/installation"&gt;Installation Steps&lt;/a&gt; and start the server and keep it running &lt;pre&gt;&lt;code class="language-sh"&gt;$ bench start
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;In a separate terminal window, run the following commands &lt;pre&gt;&lt;code class="language-sh"&gt;$ bench new-site hrms.local
$ bench get-app erpnext
$ bench get-app hrms
$ bench --site hrms.local install-app hrms
$ bench --site hrms.local add-to-hosts
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;You can access the site at &lt;code&gt;http://hrms.local:8080&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Learning and Community&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://frappe.school"&gt;Frappe School&lt;/a&gt; - Learn Frappe Framework and ERPNext from the various courses by the maintainers or from the community.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.frappe.io/hr"&gt;Documentation&lt;/a&gt; - Extensive documentation for Frappe HR.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discuss.erpnext.com/"&gt;User Forum&lt;/a&gt; - Engage with the community of ERPNext users and service providers.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://t.me/frappehr"&gt;Telegram Group&lt;/a&gt; - Get instant help from the community of users.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://github.com/frappe/erpnext/wiki/Issue-Guidelines"&gt;Issue Guidelines&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://erpnext.com/security"&gt;Report Security Vulnerabilities&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/frappe/erpnext/wiki/Contribution-Guidelines"&gt;Pull Request Requirements&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Logo and Trademark Policy&lt;/h2&gt; 
&lt;p&gt;Please read our &lt;a href="https://raw.githubusercontent.com/frappe/hrms/develop/TRADEMARK_POLICY.md"&gt;Logo and Trademark Policy&lt;/a&gt;.&lt;/p&gt; 
&lt;br&gt; 
&lt;br&gt; 
&lt;div align="center" style="padding-top: 0.75rem;"&gt; 
 &lt;a href="https://frappe.io" target="_blank"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://frappe.io/files/Frappe-white.png"&gt; 
   &lt;img src="https://frappe.io/files/Frappe-black.png" alt="Frappe Technologies" height="28"&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>QwenLM/Qwen3-Coder</title>
      <link>https://github.com/QwenLM/Qwen3-Coder</link>
      <description>&lt;p&gt;Qwen3-Coder is the code version of Qwen3, the large language model series developed by Qwen team, Alibaba Cloud.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/qwen3_coder.png" width="400"&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align="center"&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/qwen3-coder-main.jpg" width="800"&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align="center"&gt; 💜 &lt;a href="https://chat.qwenlm.ai/"&gt;&lt;b&gt;Qwen Chat&lt;/b&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;🤗 &lt;a href="https://huggingface.co/collections/Qwen/qwen3-coder-687fc861e53c939e52d52d10"&gt;Hugging Face&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;🤖 &lt;a href="https://modelscope.cn/organization/qwen"&gt;ModelScope&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; 📑 &lt;a href="https://qwenlm.github.io/blog/qwen3-coder"&gt;Blog&lt;/a&gt; &amp;nbsp;&amp;nbsp; ｜ &amp;nbsp;&amp;nbsp;📖 &lt;a href="https://qwen.readthedocs.io/"&gt;Documentation&lt;/a&gt; &lt;br&gt; &amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; 🌍 &lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-Coder-WebDev"&gt;WebDev&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;💬 &lt;a href="https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png"&gt;WeChat (微信)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;🫨 &lt;a href="https://discord.gg/CV4E9rpNSD"&gt; Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; 📄 &lt;a href="https://arxiv.org/abs/2505.09388"&gt;Arxiv&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; 👽 &lt;a href="https://github.com/QwenLM/qwen-code"&gt;Qwen Code&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;Visit our Hugging Face or ModelScope organization (click links above), search checkpoints with names starting with &lt;code&gt;Qwen3-Coder-&lt;/code&gt;, and you will find all you need! Enjoy!&lt;/p&gt; 
&lt;h1&gt;Qwen3-Coder: Agentic Coding in the World.&lt;/h1&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Today, we're announcing Qwen3-Coder, our most agentic code model to date. &lt;strong&gt;Qwen3-Coder&lt;/strong&gt; is available in multiple sizes, but we're excited to introduce its most powerful variant first: &lt;strong&gt;Qwen3-Coder-480B-A35B-Instruct&lt;/strong&gt; — a 480B-parameter Mixture-of-Experts model with 35B active parameters, offering exceptional performance in both coding and agentic tasks. &lt;strong&gt;Qwen3-Coder-480B-A35B-Instruct&lt;/strong&gt; sets new state-of-the-art results among open models on Agentic Coding, Agentic Browser-Use, and Agentic Tool-Use, comparable to Claude Sonnet.&lt;/p&gt; 
&lt;p&gt;💻 &lt;strong&gt;Significant Performance&lt;/strong&gt;: among open models on &lt;strong&gt;Agentic Coding&lt;/strong&gt;, &lt;strong&gt;Agentic Browser-Use&lt;/strong&gt;, and other foundational coding tasks, achieving results comparable to Claude Sonnet;&lt;/p&gt; 
&lt;p&gt;📚 &lt;strong&gt;Long-context Capabilities&lt;/strong&gt;: with native support for &lt;strong&gt;256K&lt;/strong&gt; tokens, extendable up to &lt;strong&gt;1M&lt;/strong&gt; tokens using Yarn, optimized for repository-scale understanding;&lt;/p&gt; 
&lt;p&gt;🛠 &lt;strong&gt;Agentic Coding&lt;/strong&gt;: supporting for most platform such as &lt;strong&gt;Qwen Code&lt;/strong&gt;, &lt;strong&gt;CLINE&lt;/strong&gt;, featuring a specially designed function call format;&lt;/p&gt; 
&lt;h2&gt;Basic information&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;✨ Supporting long context understanding and generation with the context length of 256K tokens;&lt;/li&gt; 
 &lt;li&gt;✨ Supporting 358 coding languages;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;['ABAP', 'ActionScript', 'Ada', 'Agda', 'Alloy', 'ApacheConf', 'AppleScript', 'Arc', 'Arduino', 'AsciiDoc', 'AspectJ', 'Assembly', 'Augeas', 'AutoHotkey', 'AutoIt', 'Awk', 'Batchfile', 'Befunge', 'Bison', 'BitBake', 'BlitzBasic', 'BlitzMax', 'Bluespec', 'Boo', 'Brainfuck', 'Brightscript', 'Bro', 'C', 'C#', 'C++', 'C2hs Haskell', 'CLIPS', 'CMake', 'COBOL', 'CSS', 'CSV', "Cap'n Proto", 'CartoCSS', 'Ceylon', 'Chapel', 'ChucK', 'Cirru', 'Clarion', 'Clean', 'Click', 'Clojure', 'CoffeeScript', 'ColdFusion', 'ColdFusion CFC', 'Common Lisp', 'Component Pascal', 'Coq', 'Creole', 'Crystal', 'Csound', 'Cucumber', 'Cuda', 'Cycript', 'Cython', 'D', 'DIGITAL Command Language', 'DM', 'DNS Zone', 'Darcs Patch', 'Dart', 'Diff', 'Dockerfile', 'Dogescript', 'Dylan', 'E', 'ECL', 'Eagle', 'Ecere Projects', 'Eiffel', 'Elixir', 'Elm', 'Emacs Lisp', 'EmberScript', 'Erlang', 'F#', 'FLUX', 'FORTRAN', 'Factor', 'Fancy', 'Fantom', 'Forth', 'FreeMarker', 'G-code', 'GAMS', 'GAP', 'GAS', 'GDScript', 'GLSL', 'Genshi', 'Gentoo Ebuild', 'Gentoo Eclass', 'Gettext Catalog', 'Glyph', 'Gnuplot', 'Go', 'Golo', 'Gosu', 'Grace', 'Gradle', 'Grammatical Framework', 'GraphQL', 'Graphviz (DOT)', 'Groff', 'Groovy', 'Groovy Server Pages', 'HCL', 'HLSL', 'HTML', 'HTML+Django', 'HTML+EEX', 'HTML+ERB', 'HTML+PHP', 'HTTP', 'Haml', 'Handlebars', 'Harbour', 'Haskell', 'Haxe', 'Hy', 'IDL', 'IGOR Pro', 'INI', 'IRC log', 'Idris', 'Inform 7', 'Inno Setup', 'Io', 'Ioke', 'Isabelle', 'J', 'JFlex', 'JSON', 'JSON5', 'JSONLD', 'JSONiq', 'JSX', 'Jade', 'Jasmin', 'Java', 'Java Server Pages', 'JavaScript', 'Julia', 'Jupyter Notebook', 'KRL', 'KiCad', 'Kit', 'Kotlin', 'LFE', 'LLVM', 'LOLCODE', 'LSL', 'LabVIEW', 'Lasso', 'Latte', 'Lean', 'Less', 'Lex', 'LilyPond', 'Linker Script', 'Liquid', 'Literate Agda', 'Literate CoffeeScript', 'Literate Haskell', 'LiveScript', 'Logos', 'Logtalk', 'LookML', 'Lua', 'M', 'M4', 'MAXScript', 'MTML', 'MUF', 'Makefile', 'Mako', 'Maple', 'Markdown', 'Mask', 'Mathematica', 'Matlab', 'Max', 'MediaWiki', 'Metal', 'MiniD', 'Mirah', 'Modelica', 'Module Management System', 'Monkey', 'MoonScript', 'Myghty', 'NSIS', 'NetLinx', 'NetLogo', 'Nginx', 'Nimrod', 'Ninja', 'Nit', 'Nix', 'Nu', 'NumPy', 'OCaml', 'ObjDump', 'Objective-C++', 'Objective-J', 'Octave', 'Omgrofl', 'Opa', 'Opal', 'OpenCL', 'OpenEdge ABL', 'OpenSCAD', 'Org', 'Ox', 'Oxygene', 'Oz', 'PAWN', 'PHP', 'POV-Ray SDL', 'Pan', 'Papyrus', 'Parrot', 'Parrot Assembly', 'Parrot Internal Representation', 'Pascal', 'Perl', 'Perl6', 'Pickle', 'PigLatin', 'Pike', 'Pod', 'PogoScript', 'Pony', 'PostScript', 'PowerShell', 'Processing', 'Prolog', 'Propeller Spin', 'Protocol Buffer', 'Public Key', 'Pure Data', 'PureBasic', 'PureScript', 'Python', 'Python traceback', 'QML', 'QMake', 'R', 'RAML', 'RDoc', 'REALbasic', 'RHTML', 'RMarkdown', 'Racket', 'Ragel in Ruby Host', 'Raw token data', 'Rebol', 'Red', 'Redcode', "Ren'Py", 'RenderScript', 'RobotFramework', 'Rouge', 'Ruby', 'Rust', 'SAS', 'SCSS', 'SMT', 'SPARQL', 'SQF', 'SQL', 'STON', 'SVG', 'Sage', 'SaltStack', 'Sass', 'Scala', 'Scaml', 'Scheme', 'Scilab', 'Self', 'Shell', 'ShellSession', 'Shen', 'Slash', 'Slim', 'Smali', 'Smalltalk', 'Smarty', 'Solidity', 'SourcePawn', 'Squirrel', 'Stan', 'Standard ML', 'Stata', 'Stylus', 'SuperCollider', 'Swift', 'SystemVerilog', 'TOML', 'TXL', 'Tcl', 'Tcsh', 'TeX', 'Tea', 'Text', 'Textile', 'Thrift', 'Turing', 'Turtle', 'Twig', 'TypeScript', 'Unified Parallel C', 'Unity3D Asset', 'Uno', 'UnrealScript', 'UrWeb', 'VCL', 'VHDL', 'Vala', 'Verilog', 'VimL', 'Visual Basic', 'Volt', 'Vue', 'Web Ontology Language', 'WebAssembly', 'WebIDL', 'X10', 'XC', 'XML', 'XPages', 'XProc', 'XQuery', 'XS', 'XSLT', 'Xojo', 'Xtend', 'YAML', 'YANG', 'Yacc', 'Zephir', 'Zig', 'Zimpl', 'desktop', 'eC', 'edn', 'fish', 'mupad', 'nesC', 'ooc', 'reStructuredText', 'wisp', 'xBase']
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;✨ Retain strengths in math and general capabilities from base model.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Important]&lt;/p&gt; 
 &lt;p&gt;Qwen3-coder function calling relies on our new tool parser &lt;code&gt;qwen3coder_tool_parser.py&lt;/code&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct/blob/main/qwen3coder_tool_parser.py"&gt;here&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;We updated both the special tokens and their corresponding token ids, in order to maintain consistency with Qwen3. Please make sure to use the new tokenizer.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;model name&lt;/th&gt; 
   &lt;th&gt;type&lt;/th&gt; 
   &lt;th&gt;length&lt;/th&gt; 
   &lt;th&gt;Download&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen3-Coder-480B-A35B-Instruct&lt;/td&gt; 
   &lt;td&gt;instruct&lt;/td&gt; 
   &lt;td&gt;256k&lt;/td&gt; 
   &lt;td&gt;🤗 &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct"&gt;Hugging Face&lt;/a&gt; • 🤖 &lt;a href="https://modelscope.cn/models/Qwen/Qwen3-Coder-480B-A35B-Instruct"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen3-Coder-480B-A35B-Instruct-FP8&lt;/td&gt; 
   &lt;td&gt;instruct&lt;/td&gt; 
   &lt;td&gt;256k&lt;/td&gt; 
   &lt;td&gt;🤗 &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8"&gt;Hugging Face&lt;/a&gt; • 🤖 &lt;a href="https://modelscope.cn/models/Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Detailed performance and introduction are shown in this &lt;a href="https://qwenlm.github.io/blog/qwen3-coder"&gt; 📑 blog&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Important] &lt;strong&gt;Qwen3-Coder-480B-A35B-Instruct&lt;/strong&gt; are instruction models for chatting;&lt;/p&gt; 
 &lt;p&gt;This model supports only non-thinking mode and does not generate &lt;code&gt;&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;&lt;/code&gt; blocks in its output. Meanwhile, specifying &lt;code&gt;enable_thinking=False&lt;/code&gt; is no longer required.**&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;👉🏻 Chat with Qwen3-Coder-480B-A35B-Instruct&lt;/h3&gt; 
&lt;p&gt;You can just write several lines of code with &lt;code&gt;transformers&lt;/code&gt; to chat with Qwen3-Coder-480B-A35B-Instruct. Essentially, we build the tokenizer and the model with &lt;code&gt;from_pretrained&lt;/code&gt; method, and we use generate method to perform chatting with the help of chat template provided by the tokenizer. Below is an example of how to chat with &lt;strong&gt;Qwen3-Coder-480B-A35B-Instruct&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen3-Coder-480B-A35B-Instruct"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = "write a quick sort algorithm."
messages = [
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=65536
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;apply_chat_template()&lt;/code&gt; function is used to convert the messages into a format that the model can understand. The &lt;code&gt;add_generation_prompt&lt;/code&gt; argument is used to add a generation prompt, which refers to &lt;code&gt;&amp;lt;|im_start|&amp;gt;assistant\n&lt;/code&gt; to the input. Notably, we apply ChatML template for chat models following our previous practice. The &lt;code&gt;max_new_tokens&lt;/code&gt; argument is used to set the maximum length of the response. The &lt;code&gt;tokenizer.batch_decode()&lt;/code&gt; function is used to decode the response. In terms of the input, the above messages is an example to show how to format your dialog history and system prompt. You can use the other size of instruct model in the same way.&lt;/p&gt; 
&lt;h4&gt;Fill in the middle with Qwen3-Coder-480B-A35B-Instruct&lt;/h4&gt; 
&lt;p&gt;The code insertion task, also referred to as the "fill-in-the-middle" challenge, requires the insertion of code segments in a manner that bridges the gaps within a given code context. For an approach aligned with best practices, we recommend adhering to the formatting guidelines outlined in the paper "Efficient Training of Language Models to Fill in the Middle"[&lt;a href="https://arxiv.org/abs/2207.14255"&gt;arxiv&lt;/a&gt;].&lt;/p&gt; 
&lt;p&gt;The prompt should be structured as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;prompt = '&amp;lt;|fim_prefix|&amp;gt;' + prefix_code + '&amp;lt;|fim_suffix|&amp;gt;' + suffix_code + '&amp;lt;|fim_middle|&amp;gt;'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Following the approach mentioned, an example would be structured in this manner:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoTokenizer, AutoModelForCausalLM
# load model
device = "cuda" # the device to load the model onto

TOKENIZER = AutoTokenizer.from_pretrained("Qwen/Qwen3-Coder-480B-A35B-Instruct")
MODEL = AutoModelForCausalLM.from_pretrained("Qwen/Qwen3-Coder-480B-A35B-Instruct", device_map="auto").eval()


input_text = """&amp;lt;|fim_prefix|&amp;gt;def quicksort(arr):
    if len(arr) &amp;lt;= 1:
        return arr
    pivot = arr[len(arr) // 2]
    &amp;lt;|fim_suffix|&amp;gt;
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x &amp;gt; pivot]
    return quicksort(left) + middle + quicksort(right)&amp;lt;|fim_middle|&amp;gt;"""
            
messages = [
    {"role": "system", "content": "You are a code completion assistant."},
    {"role": "user", "content": input_text}
]


text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = TOKENIZER([text], return_tensors="pt").to(model.device)

# Use `max_new_tokens` to control the maximum output length.
generated_ids = MODEL.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=False)[0]
# The generated_ids include prompt_ids, we only need to decode the tokens after prompt_ids.
output_text = TOKENIZER.decode(generated_ids[len(model_inputs.input_ids[0]):], skip_special_tokens=True)

print(f"Prompt: {input_text}\n\nGenerated text: {output_text}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Use Cases&lt;/h2&gt; 
&lt;h3&gt;Example: Physics-Based Chimney Demolition Simulation with Controlled Explosion&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;Prompt with Qwen Chat Web Dev &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;使用 three.js, cannon-es.js 生成一个震撼的3D建筑拆除演示。

## 场景设置：
- 地面是一个深灰色混凝土平面，尺寸80*80，
- 所有物体严格遵循现实物理规则，包括重力、摩擦力、碰撞检测和动量守恒

## 建筑结构：
- 一座圆形高层建筑，周长对应20个方块
- 建筑总高度60个方块
- 每层采用砖砌结构，方块与砖结构建筑一致, 错开50%排列，增强结构稳定性
- 建筑外墙使用米色方块
- **重要：方块初始排列时必须确保紧密贴合，无间隙，可以通过轻微重叠或调整半径来实现**
- **重要：建筑初始化完成后，所有方块应该处于物理"睡眠"状态，确保建筑在爆炸前保持完美的静止状态，不会因重力而下沉或松散**
- 建筑砖块之间使用粘性材料填充（不可见），通过高摩擦力（0.8+）和低弹性（0.05以下）来模拟粘合效果
- 砖块在建筑倒塌瞬间不会散掉，而是建筑作为一个整体倒在地面的时候才因受力过大而散掉

## 定向爆破系统：
- 在建筑的第1层的最右侧方块附近安装爆炸装置（不可见）
- 提供操作按钮点击爆炸
- **爆炸时唤醒所有相关方块的物理状态**
- 爆炸点产生半径2的强力冲击波，冲击波影响到的方块, 受到2-5单位的冲击力

## 建筑稳定性要求：
- **确保建筑在未爆炸时完全静止，无任何晃动或下沉**
- **物理世界初始化后给建筑几个物理步骤来自然稳定，或使用睡眠机制**
- **方块间的接触材料应具有高摩擦力和极低弹性，模拟砖块间的砂浆粘合**

## 震撼的倒塌效果：
- 方块在爆炸冲击下不仅飞散，还会在空中翻滚和碰撞
- 烟尘会随着建筑倒塌逐渐扩散，营造真实的拆除现场氛围

## 增强的视觉效果：
- 添加环境光照变化：爆炸瞬间亮度激增，然后被烟尘遮挡变暗
- 粒子系统包括：烟雾、灰尘

## 技术要求：
- 粒子系统用于烟雾和灰尘效果
- 所有代码集成在单个HTML文件中，包含必要的CSS样式
- 添加简单的UI控制：重置按钮、相机角度切换, 爆炸按钮, 鼠标左键控制摄像机角度，右键控制摄像机位置，滚轮控制摄像机焦距
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo1.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example1.png" width="400"&gt; &lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Example: Multicolor and Interactive Animation&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;Prompt with Cline [act mode] &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;Create an amazing animation multicolor and interactive using p5js

use this cdn:
https://cdn.jsdelivr.net/npm/p5@1.7.0/lib/p5.min.js
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo2.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example2.png" width="400"&gt; &lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Example: 3D Google Earth&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;Prompt with Qwen Chat Web Dev &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;To create a 3D Google Earth, you need to load the terrain map correctly. You can use any online resource. The code is written into an HTML file.
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo3.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example3.png" width="400"&gt; &lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Example: Testing Your WPM with a Famous Quote&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt; Prompt with Qwen-Code CLI &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;Create an interesting typing game with a keyboard in the lower middle of the screen and some famous articles in the upper middle. When the user types a word correctly, a cool reaction should be given to encourage him. Design a modern soft color scheme inspired by macarons. Come up with a very creative solution first, and then start writing code.
The game should be able to support typing, and you need to neglect upcase and lowercase.
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo4.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example4.png" width="400"&gt; &lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Example: Bouncing Ball in Rotation Hypercube&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt; Prompt with Qwen Chat Web Dev &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;Make a page in HTML that shows an animation of a ball bouncing in a rotating hypercube
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo5.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example5.png" width="400"&gt; &lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Example: Solar System Simulation&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt; Prompt with Cline [act mode] &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;write a web page to show the solar system simulation
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo6.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example6.png" width="400"&gt; &lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Example: DUET Game&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt; Prompt with Cline [act mode] &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;Create a complete, single-file HTML game with CSS and JavaScript. The game is inspired by "Duet".

Gameplay:

There are two balls, one red and one blue, rotating around a central point.
The player uses the 'A' and 'D' keys to rotate them counter-clockwise and clockwise.
White rectangular obstacles move down from the top of the screen.
The player must rotate the balls to avoid hitting the obstacles.
If a ball hits an obstacle, the game is over.
Visuals:

Make the visual effects amazing.
Use a dark background with neon glowing effects for the balls and obstacles.
Animations should be very smooth.
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo7.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example7.png" width="400"&gt; &lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#QwenLM/Qwen3-Coder&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=QwenLM/Qwen3-Coder&amp;amp;type=Date" alt="Star History Chart"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find our work helpful, feel free to give us a cite.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{qwen3technicalreport,
      title={Qwen3 Technical Report}, 
      author={Qwen Team},
      year={2025},
      eprint={2505.09388},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.09388},
}
@article{hui2024qwen2,
  title={Qwen2. 5-Coder Technical Report},
  author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},
  journal={arXiv preprint arXiv:2409.12186},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contact Us&lt;/h2&gt; 
&lt;p&gt;If you are interested to leave a message to either our research team or product team, join our &lt;a href="https://discord.gg/z3GAxXZ9Ce"&gt;Discord&lt;/a&gt; or &lt;a href="https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png"&gt;WeChat groups&lt;/a&gt;!&lt;/p&gt; 
&lt;p align="right" style="font-size: 14px; color: #555; margin-top: 20px;"&gt; &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/#readme-top" style="text-decoration: none; color: #007bff; font-weight: bold;"&gt; ↑ Back to Top ↑ &lt;/a&gt; &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>unslothai/unsloth</title>
      <link>https://github.com/unslothai/unsloth</link>
      <description>&lt;p&gt;Fine-tuning &amp; Reinforcement Learning for LLMs. 🦥 Train Qwen3, Llama 4, DeepSeek-R1, Gemma 3, TTS 2x faster with 70% less VRAM.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://unsloth.ai"&gt;
   &lt;picture&gt; 
    &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png"&gt; 
    &lt;source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png"&gt; 
    &lt;img alt="unsloth logo" src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png" height="110" style="max-width: 100%;"&gt; 
   &lt;/picture&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb"&gt;&lt;img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/start%20free%20finetune%20button.png" width="154"&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/unsloth"&gt;&lt;img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord%20button.png" width="165"&gt;&lt;/a&gt; &lt;a href="https://docs.unsloth.ai"&gt;&lt;img src="https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/Documentation%20Button.png" width="137"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;h3&gt;Finetune Gemma 3n, Qwen3, Llama 4, Phi-4 &amp;amp; Mistral 2x faster with 80% less VRAM!&lt;/h3&gt; 
 &lt;p&gt;&lt;img src="https://i.ibb.co/sJ7RhGG/image-41.png" alt=""&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;✨ Finetune for Free&lt;/h2&gt; 
&lt;p&gt;Notebooks are beginner friendly. Read our &lt;a href="https://docs.unsloth.ai/get-started/fine-tuning-guide"&gt;guide&lt;/a&gt;. Add your dataset, click "Run All", and export your finetuned model to GGUF, Ollama, vLLM or Hugging Face.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Unsloth supports&lt;/th&gt; 
   &lt;th&gt;Free Notebooks&lt;/th&gt; 
   &lt;th&gt;Performance&lt;/th&gt; 
   &lt;th&gt;Memory use&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemma 3n (4B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Conversational.ipynb"&gt;▶️ Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.5x faster&lt;/td&gt; 
   &lt;td&gt;50% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen3 (14B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(14B)-Reasoning-Conversational.ipynb"&gt;▶️ Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;70% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen3 (4B): GRPO&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb"&gt;▶️ Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;80% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemma 3 (4B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(4B).ipynb"&gt;▶️ Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.6x faster&lt;/td&gt; 
   &lt;td&gt;60% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama 3.2 (3B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb"&gt;▶️ Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;70% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Phi-4 (14B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb"&gt;▶️ Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;70% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama 3.2 Vision (11B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb"&gt;▶️ Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;50% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama 3.1 (8B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb"&gt;▶️ Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;70% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Mistral v0.3 (7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb"&gt;▶️ Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2.2x faster&lt;/td&gt; 
   &lt;td&gt;75% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Orpheus-TTS (3B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Orpheus_(3B)-TTS.ipynb"&gt;▶️ Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.5x faster&lt;/td&gt; 
   &lt;td&gt;50% less&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;See all our notebooks for: &lt;a href="https://github.com/unslothai/notebooks?tab=readme-ov-file#-kaggle-notebooks"&gt;Kaggle&lt;/a&gt;, &lt;a href="https://docs.unsloth.ai/get-started/unsloth-notebooks#grpo-reasoning-rl-notebooks"&gt;GRPO&lt;/a&gt;, &lt;strong&gt;&lt;a href="https://docs.unsloth.ai/get-started/unsloth-notebooks#text-to-speech-tts-notebooks"&gt;TTS&lt;/a&gt;&lt;/strong&gt; &amp;amp; &lt;a href="https://docs.unsloth.ai/get-started/unsloth-notebooks#vision-multimodal-notebooks"&gt;Vision&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;See &lt;a href="https://docs.unsloth.ai/get-started/all-our-models"&gt;all our models&lt;/a&gt; and &lt;a href="https://github.com/unslothai/notebooks"&gt;all our notebooks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;See detailed documentation for Unsloth &lt;a href="https://docs.unsloth.ai/"&gt;here&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;⚡ Quickstart&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Install with pip (recommended)&lt;/strong&gt; for Linux devices:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;pip install unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Windows install instructions, see &lt;a href="https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;🦥 Unsloth.ai News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;📣 &lt;strong&gt;Gemma 3n&lt;/strong&gt; by Google: &lt;a href="https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune"&gt;Read Blog&lt;/a&gt;. We &lt;a href="https://huggingface.co/collections/unsloth/gemma-3n-685d3874830e49e1c93f9339"&gt;uploaded GGUFs, 4-bit models&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;📣 &lt;strong&gt;&lt;a href="https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning"&gt;Text-to-Speech (TTS)&lt;/a&gt;&lt;/strong&gt; is now supported, including &lt;code&gt;sesame/csm-1b&lt;/code&gt; and STT &lt;code&gt;openai/whisper-large-v3&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;📣 &lt;strong&gt;&lt;a href="https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune"&gt;Qwen3&lt;/a&gt;&lt;/strong&gt; is now supported. Qwen3-30B-A3B fits on 17.5GB VRAM.&lt;/li&gt; 
 &lt;li&gt;📣 Introducing &lt;strong&gt;&lt;a href="https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs"&gt;Dynamic 2.0&lt;/a&gt;&lt;/strong&gt; quants that set new benchmarks on 5-shot MMLU &amp;amp; KL Divergence.&lt;/li&gt; 
 &lt;li&gt;📣 &lt;strong&gt;&lt;a href="https://unsloth.ai/blog/llama4"&gt;Llama 4&lt;/a&gt;&lt;/strong&gt; by Meta, including Scout &amp;amp; Maverick are now supported.&lt;/li&gt; 
 &lt;li&gt;📣 &lt;a href="https://unsloth.ai/blog/gemma3#everything"&gt;&lt;strong&gt;EVERYTHING&lt;/strong&gt; is now supported&lt;/a&gt; - all models (BERT, diffusion, Cohere, Mamba), FFT, etc. MultiGPU coming soon. Enable FFT with &lt;code&gt;full_finetuning = True&lt;/code&gt;, 8-bit with &lt;code&gt;load_in_8bit = True&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;📣 Introducing Long-context &lt;a href="https://unsloth.ai/blog/grpo"&gt;Reasoning (GRPO)&lt;/a&gt; in Unsloth. Train your own reasoning model with just 5GB VRAM. Transform Llama, Phi, Mistral etc. into reasoning LLMs!&lt;/li&gt; 
 &lt;li&gt;📣 &lt;a href="https://unsloth.ai/blog/deepseek-r1"&gt;DeepSeek-R1&lt;/a&gt; - run or fine-tune them &lt;a href="https://unsloth.ai/blog/deepseek-r1"&gt;with our guide&lt;/a&gt;. All model uploads: &lt;a href="https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5"&gt;here&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click for more news&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;📣 Introducing Unsloth &lt;a href="https://unsloth.ai/blog/dynamic-4bit"&gt;Dynamic 4-bit Quantization&lt;/a&gt;! We dynamically opt not to quantize certain parameters and this greatly increases accuracy while only using &amp;lt;10% more VRAM than BnB 4-bit. See our collection on &lt;a href="https://huggingface.co/collections/unsloth/unsloth-4-bit-dynamic-quants-67503bb873f89e15276c44e7"&gt;Hugging Face here.&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;📣 &lt;a href="https://unsloth.ai/blog/phi4"&gt;Phi-4&lt;/a&gt; by Microsoft: We also &lt;a href="https://unsloth.ai/blog/phi4"&gt;fixed bugs&lt;/a&gt; in Phi-4 and &lt;a href="https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa"&gt;uploaded GGUFs, 4-bit&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;📣 &lt;a href="https://unsloth.ai/blog/vision"&gt;Vision models&lt;/a&gt; now supported! &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb"&gt;Llama 3.2 Vision (11B)&lt;/a&gt;, &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb"&gt;Qwen 2.5 VL (7B)&lt;/a&gt; and &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Pixtral_(12B)-Vision.ipynb"&gt;Pixtral (12B) 2409&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;📣 &lt;a href="https://huggingface.co/collections/unsloth/llama-33-all-versions-67535d7d994794b9d7cf5e9f"&gt;Llama 3.3 (70B)&lt;/a&gt;, Meta's latest model is supported.&lt;/li&gt; 
  &lt;li&gt;📣 We worked with Apple to add &lt;a href="https://arxiv.org/abs/2411.09009"&gt;Cut Cross Entropy&lt;/a&gt;. Unsloth now supports 89K context for Meta's Llama 3.3 (70B) on a 80GB GPU - 13x longer than HF+FA2. For Llama 3.1 (8B), Unsloth enables 342K context, surpassing its native 128K support.&lt;/li&gt; 
  &lt;li&gt;📣 We found and helped fix a &lt;a href="https://unsloth.ai/blog/gradient"&gt;gradient accumulation bug&lt;/a&gt;! Please update Unsloth and transformers.&lt;/li&gt; 
  &lt;li&gt;📣 We cut memory usage by a &lt;a href="https://unsloth.ai/blog/long-context"&gt;further 30%&lt;/a&gt; and now support &lt;a href="https://unsloth.ai/blog/long-context"&gt;4x longer context windows&lt;/a&gt;!&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;🔗 Links and Resources&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Links&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;📚 &lt;strong&gt;Documentation &amp;amp; Wiki&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.unsloth.ai"&gt;Read Our Docs&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img width="16" src="https://upload.wikimedia.org/wikipedia/commons/6/6f/Logo_of_Twitter.svg?sanitize=true"&gt;&amp;nbsp; &lt;strong&gt;Twitter (aka X)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://twitter.com/unslothai"&gt;Follow us on X&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;💾 &lt;strong&gt;Installation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.unsloth.ai/get-started/installing-+-updating"&gt;Pip install&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🔮 &lt;strong&gt;Our Models&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.unsloth.ai/get-started/all-our-models"&gt;Unsloth Releases&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;✍️ &lt;strong&gt;Blog&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://unsloth.ai/blog"&gt;Read our Blogs&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img width="15" src="https://redditinc.com/hs-fs/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png"&gt;&amp;nbsp; &lt;strong&gt;Reddit&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://reddit.com/r/unsloth"&gt;Join our Reddit&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;⭐ Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports &lt;strong&gt;full-finetuning&lt;/strong&gt;, pretraining, 4b-bit, 16-bit and &lt;strong&gt;8-bit&lt;/strong&gt; training&lt;/li&gt; 
 &lt;li&gt;Supports &lt;strong&gt;all transformer-style models&lt;/strong&gt; including &lt;a href="https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning"&gt;TTS, STT&lt;/a&gt;, multimodal, diffusion, &lt;a href="https://docs.unsloth.ai/get-started/unsloth-notebooks#other-important-notebooks"&gt;BERT&lt;/a&gt; and more!&lt;/li&gt; 
 &lt;li&gt;All kernels written in &lt;a href="https://openai.com/index/triton/"&gt;OpenAI's Triton&lt;/a&gt; language. &lt;strong&gt;Manual backprop engine&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;0% loss in accuracy&lt;/strong&gt; - no approximation methods - all exact.&lt;/li&gt; 
 &lt;li&gt;No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) &lt;a href="https://developer.nvidia.com/cuda-gpus"&gt;Check your GPU!&lt;/a&gt; GTX 1070, 1080 works, but is slow.&lt;/li&gt; 
 &lt;li&gt;Works on &lt;strong&gt;Linux&lt;/strong&gt; and &lt;strong&gt;Windows&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;If you trained a model with 🦥Unsloth, you can use this cool sticker! &amp;nbsp; &lt;img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/made%20with%20unsloth.png" width="200" align="center"&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;💾 Install Unsloth&lt;/h2&gt; 
&lt;p&gt;You can also see our documentation for more detailed installation and updating instructions &lt;a href="https://docs.unsloth.ai/get-started/installing-+-updating"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Pip Installation&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Install with pip (recommended) for Linux devices:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;To update Unsloth:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install --upgrade --force-reinstall --no-cache-dir unsloth unsloth_zoo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://github.com/unslothai/unsloth/edit/main/README.md#advanced-pip-installation"&gt;here&lt;/a&gt; for advanced pip install instructions.&lt;/p&gt; 
&lt;h3&gt;Windows Installation&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!warning] Python 3.13 does not support Unsloth. Use 3.12, 3.11 or 3.10&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install NVIDIA Video Driver:&lt;/strong&gt; You should install the latest version of your GPUs driver. Download drivers here: &lt;a href="https://www.nvidia.com/Download/index.aspx"&gt;NVIDIA GPU Drive&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Visual Studio C++:&lt;/strong&gt; You will need Visual Studio, with C++ installed. By default, C++ is not installed with &lt;a href="https://visualstudio.microsoft.com/vs/community/"&gt;Visual Studio&lt;/a&gt;, so make sure you select all of the C++ options. Also select options for Windows 10/11 SDK. For detailed instructions with options, see &lt;a href="https://docs.unsloth.ai/get-started/installing-+-updating"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install CUDA Toolkit:&lt;/strong&gt; Follow the instructions to install &lt;a href="https://developer.nvidia.com/cuda-toolkit-archive"&gt;CUDA Toolkit&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install PyTorch:&lt;/strong&gt; You will need the correct version of PyTorch that is compatible with your CUDA drivers, so make sure to select them carefully. &lt;a href="https://pytorch.org/get-started/locally/"&gt;Install PyTorch&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Unsloth:&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;pip install unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Notes&lt;/h4&gt; 
&lt;p&gt;To run Unsloth directly on Windows:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install Triton from this Windows fork and follow the instructions &lt;a href="https://github.com/woct0rdho/triton-windows"&gt;here&lt;/a&gt; (be aware that the Windows fork requires PyTorch &amp;gt;= 2.4 and CUDA 12)&lt;/li&gt; 
 &lt;li&gt;In the &lt;code&gt;SFTConfig&lt;/code&gt;, set &lt;code&gt;dataset_num_proc=1&lt;/code&gt; to avoid a crashing issue:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;SFTConfig(
    dataset_num_proc=1,
    ...
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Advanced/Troubleshooting&lt;/h4&gt; 
&lt;p&gt;For &lt;strong&gt;advanced installation instructions&lt;/strong&gt; or if you see weird errors during installations:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install &lt;code&gt;torch&lt;/code&gt; and &lt;code&gt;triton&lt;/code&gt;. Go to &lt;a href="https://pytorch.org"&gt;https://pytorch.org&lt;/a&gt; to install it. For example &lt;code&gt;pip install torch torchvision torchaudio triton&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Confirm if CUDA is installed correctly. Try &lt;code&gt;nvcc&lt;/code&gt;. If that fails, you need to install &lt;code&gt;cudatoolkit&lt;/code&gt; or CUDA drivers.&lt;/li&gt; 
 &lt;li&gt;Install &lt;code&gt;xformers&lt;/code&gt; manually. You can try installing &lt;code&gt;vllm&lt;/code&gt; and seeing if &lt;code&gt;vllm&lt;/code&gt; succeeds. Check if &lt;code&gt;xformers&lt;/code&gt; succeeded with &lt;code&gt;python -m xformers.info&lt;/code&gt; Go to &lt;a href="https://github.com/facebookresearch/xformers"&gt;https://github.com/facebookresearch/xformers&lt;/a&gt;. Another option is to install &lt;code&gt;flash-attn&lt;/code&gt; for Ampere GPUs.&lt;/li&gt; 
 &lt;li&gt;Double check that your versions of Python, CUDA, CUDNN, &lt;code&gt;torch&lt;/code&gt;, &lt;code&gt;triton&lt;/code&gt;, and &lt;code&gt;xformers&lt;/code&gt; are compatible with one another. The &lt;a href="https://github.com/pytorch/pytorch/raw/main/RELEASE.md#release-compatibility-matrix"&gt;PyTorch Compatibility Matrix&lt;/a&gt; may be useful.&lt;/li&gt; 
 &lt;li&gt;Finally, install &lt;code&gt;bitsandbytes&lt;/code&gt; and check it with &lt;code&gt;python -m bitsandbytes&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Conda Installation (Optional)&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;⚠️Only use Conda if you have it. If not, use Pip&lt;/code&gt;. Select either &lt;code&gt;pytorch-cuda=11.8,12.1&lt;/code&gt; for CUDA 11.8 or CUDA 12.1. We support &lt;code&gt;python=3.10,3.11,3.12&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create --name unsloth_env \
    python=3.11 \
    pytorch-cuda=12.1 \
    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \
    -y
conda activate unsloth_env

pip install unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;If you're looking to install Conda in a Linux environment, &lt;a href="https://docs.anaconda.com/miniconda/"&gt;read here&lt;/a&gt;, or run the below 🔽&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;mkdir -p ~/miniconda3
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
rm -rf ~/miniconda3/miniconda.sh
~/miniconda3/bin/conda init bash
~/miniconda3/bin/conda init zsh
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Advanced Pip Installation&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;⚠️Do **NOT** use this if you have Conda.&lt;/code&gt; Pip is a bit more complex since there are dependency issues. The pip command is different for &lt;code&gt;torch 2.2,2.3,2.4,2.5&lt;/code&gt; and CUDA versions.&lt;/p&gt; 
&lt;p&gt;For other torch versions, we support &lt;code&gt;torch211&lt;/code&gt;, &lt;code&gt;torch212&lt;/code&gt;, &lt;code&gt;torch220&lt;/code&gt;, &lt;code&gt;torch230&lt;/code&gt;, &lt;code&gt;torch240&lt;/code&gt; and for CUDA versions, we support &lt;code&gt;cu118&lt;/code&gt; and &lt;code&gt;cu121&lt;/code&gt; and &lt;code&gt;cu124&lt;/code&gt;. For Ampere devices (A100, H100, RTX3090) and above, use &lt;code&gt;cu118-ampere&lt;/code&gt; or &lt;code&gt;cu121-ampere&lt;/code&gt; or &lt;code&gt;cu124-ampere&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For example, if you have &lt;code&gt;torch 2.4&lt;/code&gt; and &lt;code&gt;CUDA 12.1&lt;/code&gt;, use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade pip
pip install "unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Another example, if you have &lt;code&gt;torch 2.5&lt;/code&gt; and &lt;code&gt;CUDA 12.4&lt;/code&gt;, use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade pip
pip install "unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And other examples:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "unsloth[cu121-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu118-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu118-torch240] @ git+https://github.com/unslothai/unsloth.git"

pip install "unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git"

pip install "unsloth[cu121-torch250] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu124-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, run the below in a terminal to get the &lt;strong&gt;optimal&lt;/strong&gt; pip installation command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, run the below manually in a Python REPL:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;try: import torch
except: raise ImportError('Install torch via `pip install torch`')
from packaging.version import Version as V
v = V(torch.__version__)
cuda = str(torch.version.cuda)
is_ampere = torch.cuda.get_device_capability()[0] &amp;gt;= 8
if cuda != "12.1" and cuda != "11.8" and cuda != "12.4": raise RuntimeError(f"CUDA = {cuda} not supported!")
if   v &amp;lt;= V('2.1.0'): raise RuntimeError(f"Torch = {v} too old!")
elif v &amp;lt;= V('2.1.1'): x = 'cu{}{}-torch211'
elif v &amp;lt;= V('2.1.2'): x = 'cu{}{}-torch212'
elif v  &amp;lt; V('2.3.0'): x = 'cu{}{}-torch220'
elif v  &amp;lt; V('2.4.0'): x = 'cu{}{}-torch230'
elif v  &amp;lt; V('2.5.0'): x = 'cu{}{}-torch240'
elif v  &amp;lt; V('2.6.0'): x = 'cu{}{}-torch250'
else: raise RuntimeError(f"Torch = {v} too new!")
x = x.format(cuda.replace(".", ""), "-ampere" if is_ampere else "")
print(f'pip install --upgrade pip &amp;amp;&amp;amp; pip install "unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git"')
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;📜 Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Go to our official &lt;a href="https://docs.unsloth.ai"&gt;Documentation&lt;/a&gt; for saving to GGUF, checkpointing, evaluation and more!&lt;/li&gt; 
 &lt;li&gt;We support Huggingface's TRL, Trainer, Seq2SeqTrainer or even Pytorch code!&lt;/li&gt; 
 &lt;li&gt;We're in 🤗Hugging Face's official docs! Check out the &lt;a href="https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth"&gt;SFT docs&lt;/a&gt; and &lt;a href="https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth"&gt;DPO docs&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;If you want to download models from the ModelScope community, please use an environment variable: &lt;code&gt;UNSLOTH_USE_MODELSCOPE=1&lt;/code&gt;, and install the modelscope library by: &lt;code&gt;pip install modelscope -U&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;unsloth_cli.py also supports &lt;code&gt;UNSLOTH_USE_MODELSCOPE=1&lt;/code&gt; to download models and datasets. please remember to use the model and dataset id in the ModelScope community.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from unsloth import FastLanguageModel, FastModel
import torch
from trl import SFTTrainer, SFTConfig
from datasets import load_dataset
max_seq_length = 2048 # Supports RoPE Scaling internally, so choose any!
# Get LAION dataset
url = "https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl"
dataset = load_dataset("json", data_files = {"train" : url}, split = "train")

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/Meta-Llama-3.1-8B-bnb-4bit",      # Llama-3.1 2x faster
    "unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit",
    "unsloth/Meta-Llama-3.1-70B-bnb-4bit",
    "unsloth/Meta-Llama-3.1-405B-bnb-4bit",    # 4bit for 405b!
    "unsloth/Mistral-Small-Instruct-2409",     # Mistral 22b 2x faster!
    "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "unsloth/Phi-3.5-mini-instruct",           # Phi-3.5 2x faster!
    "unsloth/Phi-3-medium-4k-instruct",
    "unsloth/gemma-2-9b-bnb-4bit",
    "unsloth/gemma-2-27b-bnb-4bit",            # Gemma 2x faster!

    "unsloth/Llama-3.2-1B-bnb-4bit",           # NEW! Llama 3.2 models
    "unsloth/Llama-3.2-1B-Instruct-bnb-4bit",
    "unsloth/Llama-3.2-3B-bnb-4bit",
    "unsloth/Llama-3.2-3B-Instruct-bnb-4bit",

    "unsloth/Llama-3.3-70B-Instruct-bnb-4bit" # NEW! Llama 3.3 70B!
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastModel.from_pretrained(
    model_name = "unsloth/gemma-3-4B-it",
    max_seq_length = 2048, # Choose any for long context!
    load_in_4bit = True,  # 4 bit quantization to reduce memory
    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory
    full_finetuning = False, # [NEW!] We have full finetuning now!
    # token = "hf_...", # use one if using gated models
)

# Do model patching and add fast LoRA weights
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    max_seq_length = max_seq_length,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

trainer = SFTTrainer(
    model = model,
    train_dataset = dataset,
    tokenizer = tokenizer,
    args = SFTConfig(
        max_seq_length = max_seq_length,
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 10,
        max_steps = 60,
        logging_steps = 1,
        output_dir = "outputs",
        optim = "adamw_8bit",
        seed = 3407,
    ),
)
trainer.train()

# Go to https://github.com/unslothai/unsloth/wiki for advanced tips like
# (1) Saving to GGUF / merging to 16bit for vLLM
# (2) Continued training from a saved LoRA adapter
# (3) Adding an evaluation loop / OOMs
# (4) Customized chat templates
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a name="RL"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;💡 Reinforcement Learning&lt;/h2&gt; 
&lt;p&gt;RL including DPO, GRPO, PPO, Reward Modelling, Online DPO all work with Unsloth. We're in 🤗Hugging Face's official docs! We're on the &lt;a href="https://huggingface.co/learn/nlp-course/en/chapter12/6"&gt;GRPO docs&lt;/a&gt; and the &lt;a href="https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth"&gt;DPO docs&lt;/a&gt;! List of RL notebooks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Advanced Qwen3 GRPO notebook: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb"&gt;Link&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ORPO notebook: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-ORPO.ipynb"&gt;Link&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;DPO Zephyr notebook: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Zephyr_(7B)-DPO.ipynb"&gt;Link&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;KTO notebook: &lt;a href="https://colab.research.google.com/drive/1MRgGtLWuZX4ypSfGguFgC-IblTvO2ivM?usp=sharing"&gt;Link&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;SimPO notebook: &lt;a href="https://colab.research.google.com/drive/1Hs5oQDovOay4mFA6Y9lQhVJ8TnbFLFh2?usp=sharing"&gt;Link&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click for DPO code&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0" # Optional set GPU device ID

from unsloth import FastLanguageModel
import torch
from trl import DPOTrainer, DPOConfig
max_seq_length = 2048

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/zephyr-sft-bnb-4bit",
    max_seq_length = max_seq_length,
    load_in_4bit = True,
)

# Do model patching and add fast LoRA weights
model = FastLanguageModel.get_peft_model(
    model,
    r = 64,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 64,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    max_seq_length = max_seq_length,
)

dpo_trainer = DPOTrainer(
    model = model,
    ref_model = None,
    train_dataset = YOUR_DATASET_HERE,
    # eval_dataset = YOUR_DATASET_HERE,
    tokenizer = tokenizer,
    args = DPOConfig(
        per_device_train_batch_size = 4,
        gradient_accumulation_steps = 8,
        warmup_ratio = 0.1,
        num_train_epochs = 3,
        logging_steps = 1,
        optim = "adamw_8bit",
        seed = 42,
        output_dir = "outputs",
        max_length = 1024,
        max_prompt_length = 512,
        beta = 0.1,
    ),
)
dpo_trainer.train()
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;🥇 Performance Benchmarking&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;For our most detailed benchmarks, read our &lt;a href="https://unsloth.ai/blog/llama3-3"&gt;Llama 3.3 Blog&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Benchmarking of Unsloth was also conducted by &lt;a href="https://huggingface.co/blog/unsloth-trl"&gt;🤗Hugging Face&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We tested using the Alpaca Dataset, a batch size of 2, gradient accumulation steps of 4, rank = 32, and applied QLoRA on all linear layers (q, k, v, o, gate, up, down):&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;VRAM&lt;/th&gt; 
   &lt;th&gt;🦥 Unsloth speed&lt;/th&gt; 
   &lt;th&gt;🦥 VRAM reduction&lt;/th&gt; 
   &lt;th&gt;🦥 Longer context&lt;/th&gt; 
   &lt;th&gt;😊 Hugging Face + FA2&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 3.3 (70B)&lt;/td&gt; 
   &lt;td&gt;80GB&lt;/td&gt; 
   &lt;td&gt;2x&lt;/td&gt; 
   &lt;td&gt;&amp;gt;75%&lt;/td&gt; 
   &lt;td&gt;13x longer&lt;/td&gt; 
   &lt;td&gt;1x&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 3.1 (8B)&lt;/td&gt; 
   &lt;td&gt;80GB&lt;/td&gt; 
   &lt;td&gt;2x&lt;/td&gt; 
   &lt;td&gt;&amp;gt;70%&lt;/td&gt; 
   &lt;td&gt;12x longer&lt;/td&gt; 
   &lt;td&gt;1x&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Context length benchmarks&lt;/h3&gt; 
&lt;h4&gt;Llama 3.1 (8B) max. context length&lt;/h4&gt; 
&lt;p&gt;We tested Llama 3.1 (8B) Instruct and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;GPU VRAM&lt;/th&gt; 
   &lt;th&gt;🦥Unsloth context length&lt;/th&gt; 
   &lt;th&gt;Hugging Face + FA2&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;8 GB&lt;/td&gt; 
   &lt;td&gt;2,972&lt;/td&gt; 
   &lt;td&gt;OOM&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;12 GB&lt;/td&gt; 
   &lt;td&gt;21,848&lt;/td&gt; 
   &lt;td&gt;932&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;16 GB&lt;/td&gt; 
   &lt;td&gt;40,724&lt;/td&gt; 
   &lt;td&gt;2,551&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;24 GB&lt;/td&gt; 
   &lt;td&gt;78,475&lt;/td&gt; 
   &lt;td&gt;5,789&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;40 GB&lt;/td&gt; 
   &lt;td&gt;153,977&lt;/td&gt; 
   &lt;td&gt;12,264&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;48 GB&lt;/td&gt; 
   &lt;td&gt;191,728&lt;/td&gt; 
   &lt;td&gt;15,502&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;80 GB&lt;/td&gt; 
   &lt;td&gt;342,733&lt;/td&gt; 
   &lt;td&gt;28,454&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Llama 3.3 (70B) max. context length&lt;/h4&gt; 
&lt;p&gt;We tested Llama 3.3 (70B) Instruct on a 80GB A100 and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;GPU VRAM&lt;/th&gt; 
   &lt;th&gt;🦥Unsloth context length&lt;/th&gt; 
   &lt;th&gt;Hugging Face + FA2&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;48 GB&lt;/td&gt; 
   &lt;td&gt;12,106&lt;/td&gt; 
   &lt;td&gt;OOM&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;80 GB&lt;/td&gt; 
   &lt;td&gt;89,389&lt;/td&gt; 
   &lt;td&gt;6,916&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;br&gt; 
&lt;p&gt;&lt;img src="https://i.ibb.co/sJ7RhGG/image-41.png" alt=""&gt; &lt;br&gt;&lt;/p&gt; 
&lt;h3&gt;Citation&lt;/h3&gt; 
&lt;p&gt;You can cite the Unsloth repo as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@software{unsloth,
  author = {Daniel Han, Michael Han and Unsloth team},
  title = {Unsloth},
  url = {http://github.com/unslothai/unsloth},
  year = {2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Thank You to&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;The &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;llama.cpp library&lt;/a&gt; that lets users save models with Unsloth&lt;/li&gt; 
 &lt;li&gt;The Hugging Face team and their &lt;a href="https://github.com/huggingface/trl"&gt;TRL library&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/erikwijmans"&gt;Erik&lt;/a&gt; for his help adding &lt;a href="https://github.com/apple/ml-cross-entropy"&gt;Apple's ML Cross Entropy&lt;/a&gt; in Unsloth&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Etherll"&gt;Etherl&lt;/a&gt; for adding support for &lt;a href="https://github.com/unslothai/notebooks/pull/34"&gt;TTS, diffusion and BERT models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;And of course for every single person who has contributed or has used Unsloth!&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>QwenLM/Qwen3</title>
      <link>https://github.com/QwenLM/Qwen3</link>
      <description>&lt;p&gt;Qwen3 is the large language model series developed by Qwen team, Alibaba Cloud.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Qwen3&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/logo_qwen3.png" width="400"&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align="center"&gt; 💜 &lt;a href="https://chat.qwen.ai/"&gt;&lt;b&gt;Qwen Chat&lt;/b&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;🤗 &lt;a href="https://huggingface.co/Qwen"&gt;Hugging Face&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;🤖 &lt;a href="https://modelscope.cn/organization/qwen"&gt;ModelScope&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; 📑 &lt;a href="https://arxiv.org/abs/2505.09388"&gt;Paper&lt;/a&gt; &amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; 📑 &lt;a href="https://qwenlm.github.io/blog/qwen3/"&gt;Blog&lt;/a&gt; &amp;nbsp;&amp;nbsp; ｜ &amp;nbsp;&amp;nbsp;📖 &lt;a href="https://qwen.readthedocs.io/"&gt;Documentation&lt;/a&gt; &lt;br&gt; 🖥️ &lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-Demo"&gt;Demo&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;💬 &lt;a href="https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png"&gt;WeChat (微信)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;🫨 &lt;a href="https://discord.gg/CV4E9rpNSD"&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;/p&gt; 
&lt;p&gt;Visit our Hugging Face or ModelScope organization (click links above), search checkpoints with names starting with &lt;code&gt;Qwen3-&lt;/code&gt; or visit the &lt;a href="https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f"&gt;Qwen3 collection&lt;/a&gt;, and you will find all you need! Enjoy!&lt;/p&gt; 
&lt;p&gt;To learn more about Qwen3, feel free to read our documentation [&lt;a href="https://qwen.readthedocs.io/en/latest/"&gt;EN&lt;/a&gt;|&lt;a href="https://qwen.readthedocs.io/zh-cn/latest/"&gt;ZH&lt;/a&gt;]. Our documentation consists of the following sections:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Quickstart: the basic usages and demonstrations;&lt;/li&gt; 
 &lt;li&gt;Inference: the guidance for the inference with Transformers, including batch inference, streaming, etc.;&lt;/li&gt; 
 &lt;li&gt;Run Locally: the instructions for running LLM locally on CPU and GPU, with frameworks like llama.cpp and Ollama;&lt;/li&gt; 
 &lt;li&gt;Deployment: the demonstration of how to deploy Qwen for large-scale inference with frameworks like SGLang, vLLM, TGI, etc.;&lt;/li&gt; 
 &lt;li&gt;Quantization: the practice of quantizing LLMs with GPTQ, AWQ, as well as the guidance for how to make high-quality quantized GGUF files;&lt;/li&gt; 
 &lt;li&gt;Training: the instructions for post-training, including SFT and RLHF (TODO) with frameworks like Axolotl, LLaMA-Factory, etc.&lt;/li&gt; 
 &lt;li&gt;Framework: the usage of Qwen with frameworks for application, e.g., RAG, Agent, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;h3&gt;Qwen3-Instruct-2507&lt;/h3&gt; 
&lt;p&gt;We are excited to introduce the updated version of the &lt;strong&gt;Qwen3-235B-A22B non-thinking mode&lt;/strong&gt;, named &lt;strong&gt;Qwen3-235B-A22B-Instruct-2507&lt;/strong&gt;, featuring the following key enhancements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Significant improvements&lt;/strong&gt; in general capabilities, including &lt;strong&gt;instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Substantial gains&lt;/strong&gt; in long-tail knowledge coverage across &lt;strong&gt;multiple languages&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Markedly better alignment&lt;/strong&gt; with user preferences in &lt;strong&gt;subjective and open-ended tasks&lt;/strong&gt;, enabling more helpful responses and higher-quality text generation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced capabilities&lt;/strong&gt; in &lt;strong&gt;256K-token long-context understanding&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-235B-A22B-Instruct-2507.jpeg" alt="Qwen3-235B-A22B-Instruct-2507"&gt;&lt;/p&gt; 
&lt;h3&gt;Qwen3-Thinking-2507&lt;/h3&gt; 
&lt;p&gt;Over the past three months, we have continued to scale the &lt;strong&gt;thinking capability&lt;/strong&gt; of Qwen3-235B-A22B, improving both the &lt;strong&gt;quality and depth&lt;/strong&gt; of reasoning. We are pleased to introduce &lt;strong&gt;Qwen3-235B-A22B-Thinking-2507&lt;/strong&gt;, featuring the following key enhancements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Significantly improved performance&lt;/strong&gt; on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise — achieving &lt;strong&gt;state-of-the-art results among open-source thinking models&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Markedly better general capabilities&lt;/strong&gt;, such as instruction following, tool usage, text generation, and alignment with human preferences.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced 256K long-context understanding&lt;/strong&gt; capabilities.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-2507/Qwen3-235B-A22B-Thinking-2507.jpeg" alt="Qwen3-235B-A22B-Thinking-2507"&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Important] This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks with adequate maximum generation length.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;The updated versions of &lt;strong&gt;more Qwen3 model sizes&lt;/strong&gt; are also expected to be released very soon. Stay tuned🚀&lt;/p&gt; 
&lt;h3&gt;Qwen3&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Previous Qwen3 Release&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt; We are excited to announce the release of Qwen3, the latest addition to the Qwen family of large language models. These models represent our most advanced and intelligent systems to date, improving from our experience in building QwQ and Qwen2.5. We are making the weights of Qwen3 available to the public, including both dense and Mixture-of-Expert (MoE) models. &lt;br&gt;&lt;br&gt; The highlights from Qwen3 include: &lt;/p&gt;
 &lt;ul&gt; 
  &lt;li&gt;&lt;b&gt;Dense and Mixture-of-Experts (MoE) models of various sizes&lt;/b&gt;, available in 0.6B, 1.7B, 4B, 8B, 14B, 32B and 30B-A3B, 235B-A22B.&lt;/li&gt; 
  &lt;li&gt;&lt;b&gt;Seamless switching between thinking mode&lt;/b&gt; (for complex logical reasoning, math, and coding) and &lt;b&gt;non-thinking mode&lt;/b&gt; (for efficient, general-purpose chat), ensuring optimal performance across various scenarios.&lt;/li&gt; 
  &lt;li&gt;&lt;b&gt;Significantly enhancement in reasoning capabilities&lt;/b&gt;, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.&lt;/li&gt; 
  &lt;li&gt;&lt;b&gt;Superior human preference alignment&lt;/b&gt;, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.&lt;/li&gt; 
  &lt;li&gt;&lt;b&gt;Expertise in agent capabilities&lt;/b&gt;, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.&lt;/li&gt; 
  &lt;li&gt;&lt;b&gt;Support of 100+ languages and dialects&lt;/b&gt; with strong capabilities for &lt;b&gt;multilingual instruction following&lt;/b&gt; and &lt;b&gt;translation&lt;/b&gt;.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;2025.07.25: We released the updated version of Qwen3-235B-A22B thinking mode, named Qwen3-235B-A22B-Thinking-2507. Check out the &lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507"&gt;modelcard&lt;/a&gt; for more details!&lt;/li&gt; 
 &lt;li&gt;2025.07.21: We released the updated version of Qwen3-235B-A22B non-thinking mode, named Qwen3-235B-A22B-Instruct-2507, featuring significant enhancements over the previous version and supporting 256K-token long-context understanding. Check our &lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507"&gt;modelcard&lt;/a&gt; for more details!&lt;/li&gt; 
 &lt;li&gt;2025.04.29: We released the Qwen3 series. Check our &lt;a href="https://qwenlm.github.io/blog/qwen3"&gt;blog&lt;/a&gt; for more details!&lt;/li&gt; 
 &lt;li&gt;2024.09.19: We released the Qwen2.5 series. This time there are 3 extra model sizes: 3B, 14B, and 32B for more possibilities. Check our &lt;a href="https://qwenlm.github.io/blog/qwen2.5"&gt;blog&lt;/a&gt; for more!&lt;/li&gt; 
 &lt;li&gt;2024.06.06: We released the Qwen2 series. Check our &lt;a href="https://qwenlm.github.io/blog/qwen2/"&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2024.03.28: We released the first MoE model of Qwen: Qwen1.5-MoE-A2.7B! Temporarily, only HF transformers and vLLM support the model. We will soon add the support of llama.cpp, mlx-lm, etc. Check our &lt;a href="https://qwenlm.github.io/blog/qwen-moe/"&gt;blog&lt;/a&gt; for more information!&lt;/li&gt; 
 &lt;li&gt;2024.02.05: We released the Qwen1.5 series.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;p&gt;Detailed evaluation results are reported in this &lt;a href="https://qwenlm.github.io/blog/qwen3/"&gt;📑 blog&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For requirements on GPU memory and the respective throughput, see results &lt;a href="https://qwen.readthedocs.io/en/latest/getting_started/speed_benchmark.html"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Run Qwen3&lt;/h2&gt; 
&lt;h3&gt;🤗 Transformers&lt;/h3&gt; 
&lt;p&gt;Transformers is a library of pretrained natural language processing for inference and training. The latest version of &lt;code&gt;transformers&lt;/code&gt; is recommended and &lt;code&gt;transformers&amp;gt;=4.51.0&lt;/code&gt; is required.&lt;/p&gt; 
&lt;h4&gt;Qwen3 Instruct&lt;/h4&gt; 
&lt;p&gt;The following contains a code snippet illustrating how to use Qwen3-235B-A22B-Instruct-2507 to generate content based on given inputs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen3-235B-A22B-Instruct-2507"

# load the tokenizer and the model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)

# prepare the model input
prompt = "Give me a short introduction to large language model."
messages = [
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# conduct text completion
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=16384
)
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() 

content = tokenizer.decode(output_ids, skip_special_tokens=True)

print("content:", content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] The updated version of Qwen3-235B-A22B, namely &lt;strong&gt;Qwen3-235B-A22B-Instruct-2507&lt;/strong&gt; supports &lt;strong&gt;only non-thinking mode&lt;/strong&gt; and &lt;strong&gt;does not generate &lt;code&gt;&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;&lt;/code&gt; blocks&lt;/strong&gt; in its output. Meanwhile, &lt;strong&gt;specifying &lt;code&gt;enable_thinking=False&lt;/code&gt; is no longer required&lt;/strong&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Qwen3 Thinking&lt;/h4&gt; 
&lt;p&gt;The following contains a code snippet illustrating how to use Qwen3-235B-A22B-Thinking-2507 to generate content based on given inputs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen3-235B-A22B-Thinking-2507"

# load the tokenizer and the model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)

# prepare the model input
prompt = "Give me a short introduction to large language model."
messages = [
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# conduct text completion
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=32768
)
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() 

# parsing thinking content
try:
    # rindex finding 151668 (&amp;lt;/think&amp;gt;)
    index = len(output_ids) - output_ids[::-1].index(151668)
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip("\n")
content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip("\n")

print("thinking content:", thinking_content)  # no opening &amp;lt;think&amp;gt; tag
print("content:", content)

&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] The updated version of Qwen3-235B-A22B, namely &lt;strong&gt;Qwen3-235B-A22B-Thinking-2507&lt;/strong&gt; supports &lt;strong&gt;only thinking mode&lt;/strong&gt;. Additionally, to enforce model thinking, the default chat template automatically includes &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt;. Therefore, it is normal for the model's output to contain only &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt; without an explicit opening &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; tag.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Switching Thinking/Non-thinking Modes for Previous Qwen3 Hybrid Models&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt; By default, Qwen3 models will think before response. This could be controlled by &lt;/p&gt;
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;enable_thinking=False&lt;/code&gt;: Passing &lt;code&gt;enable_thinking=False&lt;/code&gt; to `tokenizer.apply_chat_template` will strictly prevent the model from generating thinking content.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;/think&lt;/code&gt; and &lt;code&gt;/no_think&lt;/code&gt; instructions: Use those words in the system or user message to signify whether Qwen3 should think. In multi-turn conversations, the latest instruction is followed.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;ModelScope&lt;/h3&gt; 
&lt;p&gt;We strongly advise users especially those in mainland China to use ModelScope. ModelScope adopts a Python API similar to Transformers. The CLI tool &lt;code&gt;modelscope download&lt;/code&gt; can help you solve issues concerning downloading checkpoints.&lt;/p&gt; 
&lt;h3&gt;llama.cpp&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp"&gt;&lt;code&gt;llama.cpp&lt;/code&gt;&lt;/a&gt; enables LLM inference with minimal setup and state-of-the-art performance on a wide range of hardware. &lt;code&gt;llama.cpp&amp;gt;=b5092&lt;/code&gt; is required for the support of Qwen3 architecture. &lt;code&gt;llama.cpp&amp;gt;=b5401&lt;/code&gt; is recommended for the full support of the official Qwen3 chat template.&lt;/p&gt; 
&lt;p&gt;To use the CLI, run the following in a terminal:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;./llama-cli -hf Qwen/Qwen3-8B-GGUF:Q8_0 --jinja --color -ngl 99 -fa -sm row --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0 -c 40960 -n 32768 --no-context-shift
# CTRL+C to exit
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use the API server, run the following in a terminal:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;./llama-server -hf Qwen/Qwen3-8B-GGUF:Q8_0 --jinja --reasoning-format deepseek -ngl 99 -fa -sm row --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0 -c 40960 -n 32768 --no-context-shift --port 8080
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;A simple web front end will be at &lt;code&gt;http://localhost:8080&lt;/code&gt; and an OpenAI-compatible API will be at &lt;code&gt;http://localhost:8080/v1&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For additional guides, please refer to &lt;a href="https://qwen.readthedocs.io/en/latest/run_locally/llama.cpp.html"&gt;our documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] llama.cpp adopts "rotating context management" and infinite generation is made possible by evicting earlier tokens. It could configured by parameters and the commands above effectively disable it. For more details, please refer to &lt;a href="https://qwen.readthedocs.io/en/latest/run_locally/llama.cpp.html#llama-cli"&gt;our documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Ollama&lt;/h3&gt; 
&lt;p&gt;After &lt;a href="https://ollama.com/"&gt;installing Ollama&lt;/a&gt;, you can initiate the Ollama service with the following command (Ollama v0.6.6 or higher is required):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ollama serve
# You need to keep this service running whenever you are using ollama
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To pull a model checkpoint and run the model, use the &lt;code&gt;ollama run&lt;/code&gt; command. You can specify a model size by adding a suffix to &lt;code&gt;qwen3&lt;/code&gt;, such as &lt;code&gt;:8b&lt;/code&gt; or &lt;code&gt;:30b-a3b&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ollama run qwen3:8b
# Setting parameters, type "/set parameter num_ctx 40960" and "/set parameter num_predict 32768"
# To exit, type "/bye" and press ENTER
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also access the Ollama service via its OpenAI-compatible API. Please note that you need to (1) keep &lt;code&gt;ollama serve&lt;/code&gt; running while using the API, and (2) execute &lt;code&gt;ollama run qwen3:8b&lt;/code&gt; before utilizing this API to ensure that the model checkpoint is prepared. The API is at &lt;code&gt;http://localhost:11434/v1/&lt;/code&gt; by default.&lt;/p&gt; 
&lt;p&gt;For additional details, please visit &lt;a href="https://ollama.com/"&gt;ollama.ai&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Ollama adopts the same "rotating context management" with llama.cpp. However, its default settings (&lt;code&gt;num_ctx&lt;/code&gt; 2048 and &lt;code&gt;num_predict&lt;/code&gt; -1), suggesting infinite generation with a 2048-token context, could lead to trouble for Qwen3 models. We recommend setting &lt;code&gt;num_ctx&lt;/code&gt; and &lt;code&gt;num_predict&lt;/code&gt; properly.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;LMStudio&lt;/h3&gt; 
&lt;p&gt;Qwen3 has already been supported by &lt;a href="https://lmstudio.ai/"&gt;lmstudio.ai&lt;/a&gt;. You can directly use LMStudio with our GGUF files.&lt;/p&gt; 
&lt;h3&gt;ExecuTorch&lt;/h3&gt; 
&lt;p&gt;To export and run on ExecuTorch (iOS, Android, Mac, Linux, and more), please follow this &lt;a href="https://github.com/pytorch/executorch/raw/main/examples/models/qwen3/README.md"&gt;example&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;MNN&lt;/h3&gt; 
&lt;p&gt;To export and run on MNN, which supports Qwen3 on mobile devices, please visit &lt;a href="https://github.com/alibaba/MNN"&gt;Alibaba MNN&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;MLX LM&lt;/h3&gt; 
&lt;p&gt;If you are running on Apple Silicon, &lt;a href="https://github.com/ml-explore/mlx-lm"&gt;&lt;code&gt;mlx-lm&lt;/code&gt;&lt;/a&gt; also supports Qwen3 (&lt;code&gt;mlx-lm&amp;gt;=0.24.0&lt;/code&gt;). Look for models ending with MLX on Hugging Face Hub.&lt;/p&gt; 
&lt;h3&gt;OpenVINO&lt;/h3&gt; 
&lt;p&gt;If you are running on Intel CPU or GPU, &lt;a href="https://github.com/openvinotoolkit"&gt;OpenVINO toolkit&lt;/a&gt; supports Qwen3. You can follow this &lt;a href="https://github.com/openvinotoolkit/openvino_notebooks/raw/latest/notebooks/llm-chatbot/llm-chatbot.ipynb"&gt;chatbot example&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- ### Text generation web UI

You can directly use [`text-generation-webui`](https://github.com/oobabooga/text-generation-webui) for creating a web UI demo. If you use GGUF, remember to install the latest wheel of `llama.cpp` with the support of Qwen2.5. --&gt; 
&lt;!-- ### llamafile

Clone [`llamafile`](https://github.com/Mozilla-Ocho/llamafile), run source install, and then create your own llamafile with the GGUF file following the guide [here](https://github.com/Mozilla-Ocho/llamafile?tab=readme-ov-file#creating-llamafiles). You are able to run one line of command, say `./qwen.llamafile`, to create a demo. --&gt; 
&lt;h2&gt;Deploy Qwen3&lt;/h2&gt; 
&lt;p&gt;Qwen3 is supported by multiple inference frameworks. Here we demonstrate the usage of &lt;code&gt;SGLang&lt;/code&gt;, &lt;code&gt;vLLM&lt;/code&gt; and &lt;code&gt;TensorRT-LLM&lt;/code&gt;. You can also find Qwen3 models from various inference providers, e.g., &lt;a href="https://www.alibabacloud.com/en/product/modelstudio"&gt;Alibaba Cloud Model Studio&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;SGLang&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/sgl-project/sglang"&gt;SGLang&lt;/a&gt; is a fast serving framework for large language models and vision language models. SGLang could be used to launch a server with OpenAI-compatible API service. &lt;code&gt;sglang&amp;gt;=0.4.6.post1&lt;/code&gt; is required. It is as easy as&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python -m sglang.launch_server --model-path Qwen/Qwen3-8B --port 30000 --reasoning-parser qwen3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;An OpenAI-compatible API will be available at &lt;code&gt;http://localhost:30000/v1&lt;/code&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] For Qwen3-Thinking-2507, e.g., Qwen3-235B-A22B-Thinking-2507, please use the following command at the moment:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;python -m sglang.launch_server --model-path Qwen/Qwen3-235B-A22B-Thinking-2507 --port 30000 --tp 8 --context-length 262144  --reasoning-parser deepseek-r1
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;vLLM&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt; is a high-throughput and memory-efficient inference and serving engine for LLMs. &lt;code&gt;vllm&amp;gt;=0.9.0&lt;/code&gt; is recommended.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;vllm serve Qwen/Qwen3-8B --port 8000 --enable-reasoning --reasoning-parser qwen3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;An OpenAI-compatible API will be available at &lt;code&gt;http://localhost:8000/v1&lt;/code&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] For Qwen3-Thinking-2507, e.g., Qwen3-235B-A22B-Thinking-2507, please use the following command at the moment:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;vllm serve Qwen/Qwen3-235B-A22B-Thinking-2507 --port 8000 --tensor-parallel-size 8 --max-model-len 262144 --enable-reasoning --reasoning-parser deepseek_r1
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;TensorRT-LLM&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/NVIDIA/TensorRT-LLM"&gt;TensorRT-LLM&lt;/a&gt; is an open-source LLM inference engine from NVIDIA, which provides optimizations including custom attention kernels, quantization and more on NVIDIA GPUs. Qwen3 is supported in its re-architected &lt;a href="https://nvidia.github.io/TensorRT-LLM/torch.html"&gt;PyTorch backend&lt;/a&gt;. &lt;code&gt;tensorrt_llm&amp;gt;=0.20.0rc3&lt;/code&gt; is recommended. Please refer to the &lt;a href="https://github.com/NVIDIA/TensorRT-LLM/raw/main/examples/models/core/qwen/README.md#qwen3"&gt;README&lt;/a&gt; page for more details.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;trtllm-serve Qwen/Qwen3-8B --host localhost --port 8000 --backend pytorch
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;An OpenAI-compatible API will be available at &lt;code&gt;http://localhost:8000/v1&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;MindIE&lt;/h3&gt; 
&lt;p&gt;For deployment on Ascend NPUs, please visit &lt;a href="https://modelers.cn/"&gt;Modelers&lt;/a&gt; and search for Qwen3.&lt;/p&gt; 
&lt;!-- 
### OpenLLM

[OpenLLM](https://github.com/bentoml/OpenLLM) allows you to easily run Qwen2.5 as OpenAI-compatible APIs. You can start a model server using `openllm serve`. For example:

```bash
openllm serve qwen2.5:7b
```

The server is active at `http://localhost:3000/`, providing OpenAI-compatible APIs. You can create an OpenAI client to call its chat API. For more information, refer to [our documentation](https://qwen.readthedocs.io/en/latest/deployment/openllm.html). --&gt; 
&lt;h2&gt;Build with Qwen3&lt;/h2&gt; 
&lt;h3&gt;Tool Use&lt;/h3&gt; 
&lt;p&gt;For tool use capabilities, we recommend taking a look at &lt;a href="https://github.com/QwenLM/Qwen-Agent"&gt;Qwen-Agent&lt;/a&gt;, which provides a wrapper around these APIs to support tool use or function calling with MCP support. Tool use with Qwen3 can also be conducted with SGLang, vLLM, Transformers, llama.cpp, Ollama, etc. Follow guides in our documentation to see how to enable the support.&lt;/p&gt; 
&lt;h3&gt;Finetuning&lt;/h3&gt; 
&lt;p&gt;We advise you to use training frameworks, including &lt;a href="https://github.com/OpenAccess-AI-Collective/axolotl"&gt;Axolotl&lt;/a&gt;, &lt;a href="https://github.com/unslothai/unsloth"&gt;UnSloth&lt;/a&gt;, &lt;a href="https://github.com/modelscope/swift"&gt;Swift&lt;/a&gt;, &lt;a href="https://github.com/hiyouga/LLaMA-Factory"&gt;Llama-Factory&lt;/a&gt;, etc., to finetune your models with SFT, DPO, GRPO, etc.&lt;/p&gt; 
&lt;h2&gt;License Agreement&lt;/h2&gt; 
&lt;p&gt;All our open-weight models are licensed under Apache 2.0. You can find the license files in the respective Hugging Face repositories.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find our work helpful, feel free to give us a cite.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{qwen3,
    title={Qwen3 Technical Report}, 
    author={An Yang and Anfeng Li and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Gao and Chengen Huang and Chenxu Lv and Chujie Zheng and Dayiheng Liu and Fan Zhou and Fei Huang and Feng Hu and Hao Ge and Haoran Wei and Huan Lin and Jialong Tang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jing Zhou and Jingren Zhou and Junyang Lin and Kai Dang and Keqin Bao and Kexin Yang and Le Yu and Lianghao Deng and Mei Li and Mingfeng Xue and Mingze Li and Pei Zhang and Peng Wang and Qin Zhu and Rui Men and Ruize Gao and Shixuan Liu and Shuang Luo and Tianhao Li and Tianyi Tang and Wenbiao Yin and Xingzhang Ren and Xinyu Wang and Xinyu Zhang and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yinger Zhang and Yu Wan and Yuqiong Liu and Zekun Wang and Zeyu Cui and Zhenru Zhang and Zhipeng Zhou and Zihan Qiu},
    journal = {arXiv preprint arXiv:2505.09388},
    year={2025}
}

@article{qwen2.5,
    title   = {Qwen2.5 Technical Report}, 
    author  = {An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},
    journal = {arXiv preprint arXiv:2412.15115},
    year    = {2024}
}

@article{qwen2,
    title   = {Qwen2 Technical Report}, 
    author  = {An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},
    journal = {arXiv preprint arXiv:2407.10671},
    year    = {2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contact Us&lt;/h2&gt; 
&lt;p&gt;If you are interested to leave a message to either our research team or product team, join our &lt;a href="https://discord.gg/z3GAxXZ9Ce"&gt;Discord&lt;/a&gt; or &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen3/main/assets/wechat.png"&gt;WeChat groups&lt;/a&gt;!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>serengil/deepface</title>
      <link>https://github.com/serengil/deepface</link>
      <description>&lt;p&gt;A Lightweight Face Recognition and Facial Attribute Analysis (Age, Gender, Emotion and Race) Library for Python&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;deepface&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://pepy.tech/project/deepface"&gt;&lt;img src="https://static.pepy.tech/personalized-badge/deepface?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=blue&amp;amp;left_text=downloads" alt="Downloads"&gt;&lt;/a&gt; &lt;a href="https://github.com/serengil/deepface/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/serengil/deepface?color=yellow&amp;amp;style=flat&amp;amp;label=%E2%AD%90%20stars" alt="Stars"&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/serengil/deepface"&gt;&lt;img src="https://img.shields.io/docker/pulls/serengil/deepface?logo=docker" alt="Pulls"&gt;&lt;/a&gt; &lt;a href="https://github.com/serengil/deepface/raw/master/LICENSE"&gt;&lt;img src="http://img.shields.io/:license-MIT-green.svg?style=flat" alt="License"&gt;&lt;/a&gt; &lt;a href="https://github.com/serengil/deepface/actions/workflows/tests.yml"&gt;&lt;img src="https://github.com/serengil/deepface/actions/workflows/tests.yml/badge.svg?sanitize=true" alt="Tests"&gt;&lt;/a&gt; &lt;a href="https://doi.org/10.17671/gazibtd.1399077"&gt;&lt;img src="http://img.shields.io/:DOI-10.17671/gazibtd.1399077-blue.svg?style=flat" alt="DOI"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://sefiks.com"&gt;&lt;img src="https://img.shields.io/:blog-sefiks.com-blue.svg?style=flat&amp;amp;logo=wordpress" alt="Blog"&gt;&lt;/a&gt; &lt;a href="https://www.youtube.com/@sefiks?sub_confirmation=1"&gt;&lt;img src="https://img.shields.io/:youtube-@sefiks-red.svg?style=flat&amp;amp;logo=youtube" alt="YouTube"&gt;&lt;/a&gt; &lt;a href="https://twitter.com/intent/user?screen_name=serengil"&gt;&lt;img src="https://img.shields.io/:follow-@serengil-blue.svg?style=flat&amp;amp;logo=x" alt="Twitter"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.patreon.com/serengil?repo=deepface"&gt;&lt;img src="https://img.shields.io/:become-patron-f96854.svg?style=flat&amp;amp;logo=patreon" alt="Patreon"&gt;&lt;/a&gt; &lt;a href="https://github.com/sponsors/serengil"&gt;&lt;img src="https://img.shields.io/github/sponsors/serengil?logo=GitHub&amp;amp;color=lightgray" alt="GitHub Sponsors"&gt;&lt;/a&gt; &lt;a href="https://buymeacoffee.com/serengil"&gt;&lt;img src="https://img.shields.io/badge/-buy_me_a%C2%A0coffee-gray?logo=buy-me-a-coffee" alt="Buy Me a Coffee"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://trendshift.io/repositories/4227" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/4227" alt="serengil%2Fdeepface | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"&gt;&lt;/a&gt; 
  &lt;!--
  &lt;a href="https://www.producthunt.com/posts/deepface?embed=true&amp;utm_source=badge-featured&amp;utm_medium=badge&amp;utm_souce=badge-deepface" target="_blank"&gt;
      &lt;img src="https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=753599&amp;theme=light" alt="DeepFace - A Lightweight Deep Face Recognition Library for Python | Product Hunt" style="width: 250px; height: 54px;" width="250" height="54" /&gt;
  &lt;/a&gt;
  --&gt; 
 &lt;/div&gt; 
 &lt;!--
[![Hacker News](https://img.shields.io/badge/dynamic/json?color=orange&amp;label=Hacker%20News&amp;query=score&amp;url=https%3A%2F%2Fhacker-news.firebaseio.com%2Fv0%2Fitem%2F42584896.json&amp;logo=y-combinator)](https://news.ycombinator.com/item?id=42584896)
[![Product Hunt](https://img.shields.io/badge/Product%20Hunt-%E2%96%B2-orange?logo=producthunt)](https://www.producthunt.com/posts/deepface?embed=true&amp;utm_source=badge-featured&amp;utm_medium=badge&amp;utm_souce=badge-deepface)
--&gt; 
 &lt;!-- [![DOI](http://img.shields.io/:DOI-10.1109/ICEET53442.2021.9659697-blue.svg?style=flat)](https://doi.org/10.1109/ICEET53442.2021.9659697) --&gt; 
 &lt;!-- [![DOI](http://img.shields.io/:DOI-10.1109/ASYU50717.2020.9259802-blue.svg?style=flat)](https://doi.org/10.1109/ASYU50717.2020.9259802) --&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt;&lt;img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/deepface-icon-labeled.png" width="200" height="240"&gt;&lt;/p&gt; 
&lt;p&gt;DeepFace is a lightweight &lt;a href="https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/"&gt;face recognition&lt;/a&gt; and facial attribute analysis (&lt;a href="https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/"&gt;age&lt;/a&gt;, &lt;a href="https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/"&gt;gender&lt;/a&gt;, &lt;a href="https://sefiks.com/2018/01/01/facial-expression-recognition-with-keras/"&gt;emotion&lt;/a&gt; and &lt;a href="https://sefiks.com/2019/11/11/race-and-ethnicity-prediction-in-keras/"&gt;race&lt;/a&gt;) framework for python. It is a hybrid face recognition framework wrapping &lt;strong&gt;state-of-the-art&lt;/strong&gt; models: &lt;a href="https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/"&gt;&lt;code&gt;VGG-Face&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://sefiks.com/2018/09/03/face-recognition-with-facenet-in-keras/"&gt;&lt;code&gt;FaceNet&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://sefiks.com/2019/07/21/face-recognition-with-openface-in-keras/"&gt;&lt;code&gt;OpenFace&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://sefiks.com/2020/02/17/face-recognition-with-facebook-deepface-in-keras/"&gt;&lt;code&gt;DeepFace&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://sefiks.com/2020/06/16/face-recognition-with-deepid-in-keras/"&gt;&lt;code&gt;DeepID&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://sefiks.com/2020/12/14/deep-face-recognition-with-arcface-in-keras-and-python/"&gt;&lt;code&gt;ArcFace&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://sefiks.com/2020/07/11/face-recognition-with-dlib-in-python/"&gt;&lt;code&gt;Dlib&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;SFace&lt;/code&gt;, &lt;code&gt;GhostFaceNet&lt;/code&gt;, &lt;code&gt;Buffalo_L&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://sefiks.com/2020/05/01/a-gentle-introduction-to-face-recognition-in-deep-learning/"&gt;A modern face recognition pipeline&lt;/a&gt; consists of 5 common stages: &lt;a href="https://sefiks.com/2020/08/25/deep-face-detection-with-opencv-in-python/"&gt;detect&lt;/a&gt;, &lt;a href="https://sefiks.com/2020/02/23/face-alignment-for-face-recognition-in-python-within-opencv/"&gt;align&lt;/a&gt;, &lt;a href="https://sefiks.com/2020/11/20/facial-landmarks-for-face-recognition-with-dlib/"&gt;normalize&lt;/a&gt;, &lt;a href="https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/"&gt;represent&lt;/a&gt; and &lt;a href="https://sefiks.com/2020/05/22/fine-tuning-the-threshold-in-face-recognition/"&gt;verify&lt;/a&gt;. While DeepFace handles all these common stages in the background, you don’t need to acquire in-depth knowledge about all the processes behind it. You can just call its verification, find or analysis function with a single line of code.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/serengil/deepface/tree/master/benchmarks"&gt;&lt;code&gt;Experiments&lt;/code&gt;&lt;/a&gt; show that &lt;strong&gt;human beings have 97.53% accuracy&lt;/strong&gt; on facial recognition tasks whereas those models already reached and passed that accuracy level.&lt;/p&gt; 
&lt;h2&gt;Installation &lt;a href="https://pypi.org/project/deepface/"&gt;&lt;img src="https://img.shields.io/pypi/v/deepface.svg?sanitize=true" alt="PyPI"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;The easiest way to install deepface is to download it from &lt;a href="https://pypi.org/project/deepface/"&gt;&lt;code&gt;PyPI&lt;/code&gt;&lt;/a&gt;. It's going to install the library itself and its prerequisites as well.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;$ pip install deepface
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can also install deepface from its source code. Source code may have new features not published in pip release yet.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;$ git clone https://github.com/serengil/deepface.git
$ cd deepface
$ pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once you installed the library, then you will be able to import it and use its functionalities.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from deepface import DeepFace
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Face Verification&lt;/strong&gt; - &lt;a href="https://youtu.be/KRCvkNCOphE"&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This function verifies face pairs as same person or different persons. It expects exact image paths as inputs. Passing numpy or base64 encoded images is also welcome. Then, it is going to return a dictionary and you should check just its verified key.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;result = DeepFace.verify(img1_path = "img1.jpg", img2_path = "img2.jpg")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p align="center"&gt;&lt;img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-1.jpg" width="95%"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Face recognition&lt;/strong&gt; - &lt;a href="https://youtu.be/Hrjp-EStM_s"&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://sefiks.com/2020/05/25/large-scale-face-recognition-for-deep-learning/"&gt;Face recognition&lt;/a&gt; requires applying face verification many times. Herein, deepface has an out-of-the-box find function to handle this action. It's going to look for the identity of input image in the database path and it will return list of pandas data frame as output. Meanwhile, facial embeddings of the facial database are stored in a pickle file to be searched faster in next time. Result is going to be the size of faces appearing in the source image. Besides, target images in the database can have many faces as well.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;dfs = DeepFace.find(img_path = "img1.jpg", db_path = "C:/my_db")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p align="center"&gt;&lt;img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-6-v2.jpg" width="95%"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Facial Attribute Analysis&lt;/strong&gt; - &lt;a href="https://youtu.be/GT2UeN85BdA"&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;DeepFace also comes with a strong facial attribute analysis module including &lt;a href="https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/"&gt;&lt;code&gt;age&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/"&gt;&lt;code&gt;gender&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://sefiks.com/2018/01/01/facial-expression-recognition-with-keras/"&gt;&lt;code&gt;facial expression&lt;/code&gt;&lt;/a&gt; (including angry, fear, neutral, sad, disgust, happy and surprise) and &lt;a href="https://sefiks.com/2019/11/11/race-and-ethnicity-prediction-in-keras/"&gt;&lt;code&gt;race&lt;/code&gt;&lt;/a&gt; (including asian, white, middle eastern, indian, latino and black) predictions. Result is going to be the size of faces appearing in the source image.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;objs = DeepFace.analyze(
  img_path = "img4.jpg", actions = ['age', 'gender', 'race', 'emotion']
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p align="center"&gt;&lt;img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-2.jpg" width="95%"&gt;&lt;/p&gt; 
&lt;p&gt;Age model got ± 4.65 MAE; gender model got 97.44% accuracy, 96.29% precision and 95.05% recall as mentioned in its &lt;a href="https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/"&gt;tutorial&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Real Time Analysis&lt;/strong&gt; - &lt;a href="https://youtu.be/-c9sSJcx6wI"&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://youtu.be/IXoah6rhxac"&gt;&lt;code&gt;React Demo part-i&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://youtu.be/_waBA-cH2D4"&gt;&lt;code&gt;React Demo part-ii&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You can run deepface for real time videos as well. Stream function will access your webcam and apply both face recognition and facial attribute analysis. The function starts to analyze a frame if it can focus a face sequentially 5 frames. Then, it shows results 5 seconds.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;DeepFace.stream(db_path = "C:/database")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p align="center"&gt;&lt;img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-3.jpg" width="90%"&gt;&lt;/p&gt; 
&lt;p&gt;Even though face recognition is based on one-shot learning, you can use multiple face pictures of a person as well. You should rearrange your directory structure as illustrated below.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;user
├── database
│   ├── Alice
│   │   ├── Alice1.jpg
│   │   ├── Alice2.jpg
│   ├── Bob
│   │   ├── Bob.jpg
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you intend to perform face verification or analysis tasks directly from your browser, &lt;a href="https://github.com/serengil/deepface-react-ui"&gt;&lt;code&gt;deepface-react-ui&lt;/code&gt;&lt;/a&gt; is a separate repository built using ReactJS depending on deepface api.&lt;/p&gt; 
&lt;p&gt;Here, you can also find some real time demos for various facial recognition models:&lt;/p&gt; 
&lt;p align="center"&gt;&lt;img src="https://sefiks.com/wp-content/uploads/2020/02/deepface-cover.jpg" width="90%"&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Task&lt;/th&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Demo&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Facial Recognition&lt;/td&gt; 
   &lt;td&gt;DeepFace&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/YjYIMs5ZOfc"&gt;&lt;code&gt;Video&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Facial Recognition&lt;/td&gt; 
   &lt;td&gt;FaceNet&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/vB1I5vWgTQg"&gt;&lt;code&gt;Video&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Facial Recognition&lt;/td&gt; 
   &lt;td&gt;VGG-Face&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/tSU_lNi0gQQ"&gt;&lt;code&gt;Video&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Facial Recognition&lt;/td&gt; 
   &lt;td&gt;OpenFace&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/-4z2sL6wzP8"&gt;&lt;code&gt;Video&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Age &amp;amp; Gender&lt;/td&gt; 
   &lt;td&gt;Default&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/tFI7vZn3P7E"&gt;&lt;code&gt;Video&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Race &amp;amp; Ethnicity&lt;/td&gt; 
   &lt;td&gt;Default&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/-ztiy5eJha8"&gt;&lt;code&gt;Video&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Emotion&lt;/td&gt; 
   &lt;td&gt;Default&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/Y7DfLvLKScs"&gt;&lt;code&gt;Video&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Celebrity Look-Alike&lt;/td&gt; 
   &lt;td&gt;Default&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/RMgIKU1H8DY"&gt;&lt;code&gt;Video&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Embeddings&lt;/strong&gt; - &lt;a href="https://sefiks.com/2025/06/28/what-are-vector-embeddings-and-why-they-matter-in-ai/"&gt;&lt;code&gt;Tutorial&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://youtu.be/OYialFo7Qo4"&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Face recognition models basically represent facial images as multi-dimensional vectors. Sometimes, you need those embedding vectors directly. DeepFace comes with a dedicated representation function. Represent function returns a list of embeddings. Result is going to be the size of faces appearing in the image path.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;embedding_objs = DeepFace.represent(img_path = "img.jpg")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Embeddings can be &lt;a href="https://sefiks.com/2020/05/01/a-gentle-introduction-to-face-recognition-in-deep-learning/"&gt;plotted&lt;/a&gt; as below. Each slot is corresponding to a dimension value and dimension value is emphasized with colors. Similar to 2D barcodes, vertical dimension stores no information in the illustration.&lt;/p&gt; 
&lt;p align="center"&gt;&lt;img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/embedding.jpg" width="95%"&gt;&lt;/p&gt; 
&lt;p&gt;In summary, the distance between vector embeddings of the same person should be smaller than that between embeddings of different people. When reduced to two-dimensional space, the clusters become clearly distinguishable.&lt;/p&gt; 
&lt;p align="center"&gt;&lt;img src="https://raw.githubusercontent.com/serengil/deepface/refs/heads/master/icon/facenet-pca.png" width="95%"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Face recognition models&lt;/strong&gt; - &lt;a href="https://youtu.be/eKOZawGR3y0"&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;DeepFace is a &lt;strong&gt;hybrid&lt;/strong&gt; face recognition package. It currently wraps many &lt;strong&gt;state-of-the-art&lt;/strong&gt; face recognition models: &lt;a href="https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/"&gt;&lt;code&gt;VGG-Face&lt;/code&gt;&lt;/a&gt; , &lt;a href="https://sefiks.com/2018/09/03/face-recognition-with-facenet-in-keras/"&gt;&lt;code&gt;FaceNet&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://sefiks.com/2019/07/21/face-recognition-with-openface-in-keras/"&gt;&lt;code&gt;OpenFace&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://sefiks.com/2020/02/17/face-recognition-with-facebook-deepface-in-keras/"&gt;&lt;code&gt;DeepFace&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://sefiks.com/2020/06/16/face-recognition-with-deepid-in-keras/"&gt;&lt;code&gt;DeepID&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://sefiks.com/2020/12/14/deep-face-recognition-with-arcface-in-keras-and-python/"&gt;&lt;code&gt;ArcFace&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://sefiks.com/2020/07/11/face-recognition-with-dlib-in-python/"&gt;&lt;code&gt;Dlib&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;SFace&lt;/code&gt;, &lt;code&gt;GhostFaceNet&lt;/code&gt; and &lt;code&gt;Buffalo_L&lt;/code&gt;. The default configuration uses VGG-Face model.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;models = [
    "VGG-Face", "Facenet", "Facenet512", "OpenFace", "DeepFace",
    "DeepID", "ArcFace", "Dlib", "SFace", "GhostFaceNet",
    "Buffalo_L",
]

result = DeepFace.verify(
  img1_path = "img1.jpg", img2_path = "img2.jpg", model_name = models[0]
)

dfs = DeepFace.find(
  img_path = "img1.jpg", db_path = "C:/my_db", model_name = models[1]
)

embeddings = DeepFace.represent(
  img_path = "img.jpg", model_name = models[2]
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p align="center"&gt;&lt;img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/model-portfolio-20240316.jpg" width="95%"&gt;&lt;/p&gt; 
&lt;p&gt;FaceNet, VGG-Face, ArcFace and Dlib are overperforming ones based on experiments - see &lt;a href="https://github.com/serengil/deepface/tree/master/benchmarks"&gt;&lt;code&gt;BENCHMARKS&lt;/code&gt;&lt;/a&gt; for more details. You can find the measured scores of various models in DeepFace and the reported scores from their original studies in the following table.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Measured Score&lt;/th&gt; 
   &lt;th&gt;Declared Score&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Facenet512&lt;/td&gt; 
   &lt;td&gt;98.4%&lt;/td&gt; 
   &lt;td&gt;99.6%&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Human-beings&lt;/td&gt; 
   &lt;td&gt;97.5%&lt;/td&gt; 
   &lt;td&gt;97.5%&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Facenet&lt;/td&gt; 
   &lt;td&gt;97.4%&lt;/td&gt; 
   &lt;td&gt;99.2%&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dlib&lt;/td&gt; 
   &lt;td&gt;96.8%&lt;/td&gt; 
   &lt;td&gt;99.3 %&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;VGG-Face&lt;/td&gt; 
   &lt;td&gt;96.7%&lt;/td&gt; 
   &lt;td&gt;98.9%&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ArcFace&lt;/td&gt; 
   &lt;td&gt;96.7%&lt;/td&gt; 
   &lt;td&gt;99.5%&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GhostFaceNet&lt;/td&gt; 
   &lt;td&gt;93.3%&lt;/td&gt; 
   &lt;td&gt;99.7%&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SFace&lt;/td&gt; 
   &lt;td&gt;93.0%&lt;/td&gt; 
   &lt;td&gt;99.5%&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenFace&lt;/td&gt; 
   &lt;td&gt;78.7%&lt;/td&gt; 
   &lt;td&gt;92.9%&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DeepFace&lt;/td&gt; 
   &lt;td&gt;69.0%&lt;/td&gt; 
   &lt;td&gt;97.3%&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DeepID&lt;/td&gt; 
   &lt;td&gt;66.5%&lt;/td&gt; 
   &lt;td&gt;97.4%&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Conducting experiments with those models within DeepFace may reveal disparities compared to the original studies, owing to the adoption of distinct detection or normalization techniques. Furthermore, some models have been released solely with their backbones, lacking pre-trained weights. Thus, we are utilizing their re-implementations instead of the original pre-trained weights.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Face Detection and Alignment&lt;/strong&gt; - &lt;a href="https://youtu.be/GZ2p2hj2H5k"&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Face detection and alignment are important early stages of a modern face recognition pipeline. &lt;a href="https://github.com/serengil/deepface/tree/master/benchmarks"&gt;Experiments&lt;/a&gt; show that detection increases the face recognition accuracy up to 42%, while alignment increases it up to 6%. &lt;a href="https://sefiks.com/2020/02/23/face-alignment-for-face-recognition-in-python-within-opencv/"&gt;&lt;code&gt;OpenCV&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://sefiks.com/2020/08/25/deep-face-detection-with-opencv-in-python/"&gt;&lt;code&gt;Ssd&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://sefiks.com/2020/07/11/face-recognition-with-dlib-in-python/"&gt;&lt;code&gt;Dlib&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://sefiks.com/2020/09/09/deep-face-detection-with-mtcnn-in-python/"&gt;&lt;code&gt;MtCnn&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;Faster MtCnn&lt;/code&gt;, &lt;a href="https://sefiks.com/2021/04/27/deep-face-detection-with-retinaface-in-python/"&gt;&lt;code&gt;RetinaFace&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://sefiks.com/2022/01/14/deep-face-detection-with-mediapipe/"&gt;&lt;code&gt;MediaPipe&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;Yolo&lt;/code&gt;, &lt;code&gt;YuNet&lt;/code&gt; and &lt;code&gt;CenterFace&lt;/code&gt; detectors are wrapped in deepface.&lt;/p&gt; 
&lt;p align="center"&gt;&lt;img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/detector-portfolio-v6.jpg" width="95%"&gt;&lt;/p&gt; 
&lt;p&gt;All deepface functions accept optional detector backend and align input arguments. You can switch among those detectors and alignment modes with these arguments. OpenCV is the default detector and alignment is on by default.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;backends = [
    'opencv', 'ssd', 'dlib', 'mtcnn', 'fastmtcnn',
    'retinaface', 'mediapipe', 'yolov8', 'yolov11s',
    'yolov11n', 'yolov11m', 'yunet', 'centerface',
]
detector = backends[3]
align = True

obj = DeepFace.verify(
  img1_path = "img1.jpg", img2_path = "img2.jpg", detector_backend = detector, align = align
)

dfs = DeepFace.find(
  img_path = "img.jpg", db_path = "my_db", detector_backend = detector, align = align
)

embedding_objs = DeepFace.represent(
  img_path = "img.jpg", detector_backend = detector, align = align
)

demographies = DeepFace.analyze(
  img_path = "img4.jpg", detector_backend = detector, align = align
)

face_objs = DeepFace.extract_faces(
  img_path = "img.jpg", detector_backend = detector, align = align
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Face recognition models are actually CNN models and they expect standard sized inputs. So, resizing is required before representation. To avoid deformation, deepface adds black padding pixels according to the target size argument after detection and alignment.&lt;/p&gt; 
&lt;p align="center"&gt;&lt;img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/detector-outputs-20240414.jpg" width="90%"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://sefiks.com/2021/04/27/deep-face-detection-with-retinaface-in-python/"&gt;RetinaFace&lt;/a&gt; and &lt;a href="https://sefiks.com/2020/09/09/deep-face-detection-with-mtcnn-in-python/"&gt;MtCnn&lt;/a&gt; seem to overperform in detection and alignment stages but they are much slower. If the speed of your pipeline is more important, then you should use opencv or ssd. On the other hand, if you consider the accuracy, then you should use retinaface or mtcnn.&lt;/p&gt; 
&lt;p&gt;The performance of RetinaFace is very satisfactory even in the crowd as seen in the following illustration. Besides, it comes with an incredible facial landmark detection performance. Highlighted red points show some facial landmarks such as eyes, nose and mouth. That's why, alignment score of RetinaFace is high as well.&lt;/p&gt; 
&lt;p align="center"&gt;&lt;img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/retinaface-results.jpeg" width="90%"&gt; &lt;br&gt;&lt;em&gt;The Yellow Angels - Fenerbahce Women's Volleyball Team&lt;/em&gt; &lt;/p&gt; 
&lt;p&gt;You can find out more about RetinaFace on this &lt;a href="https://github.com/serengil/retinaface"&gt;repo&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Face Anti Spoofing&lt;/strong&gt; - &lt;a href="https://youtu.be/UiK1aIjOBlQ"&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;DeepFace also includes an anti-spoofing analysis module to understand given image is real or fake. To activate this feature, set the &lt;code&gt;anti_spoofing&lt;/code&gt; argument to True in any DeepFace tasks.&lt;/p&gt; 
&lt;p align="center"&gt;&lt;img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/face-anti-spoofing.jpg" width="40%"&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# anti spoofing test in face detection
face_objs = DeepFace.extract_faces(img_path="dataset/img1.jpg", anti_spoofing = True)
assert all(face_obj["is_real"] is True for face_obj in face_objs)

# anti spoofing test in real time analysis
DeepFace.stream(db_path = "C:/database", anti_spoofing = True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Similarity&lt;/strong&gt; - &lt;a href="https://youtu.be/1EPoS69fHOc"&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Face recognition models are regular &lt;a href="https://sefiks.com/2018/03/23/convolutional-autoencoder-clustering-images-with-neural-networks/"&gt;convolutional neural networks&lt;/a&gt; and they are responsible to represent faces as vectors. We expect that a face pair of same person should be &lt;a href="https://sefiks.com/2020/05/22/fine-tuning-the-threshold-in-face-recognition/"&gt;more similar&lt;/a&gt; than a face pair of different persons.&lt;/p&gt; 
&lt;p&gt;Similarity could be calculated by different metrics such as &lt;a href="https://sefiks.com/2018/08/13/cosine-similarity-in-machine-learning/"&gt;Cosine Similarity&lt;/a&gt;, Angular Distance, Euclidean Distance or L2 normalized Euclidean. The default configuration uses cosine similarity. According to &lt;a href="https://github.com/serengil/deepface/tree/master/benchmarks"&gt;experiments&lt;/a&gt;, no distance metric is overperforming than other.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;metrics = ["cosine", "euclidean", "euclidean_l2", "angular"]

result = DeepFace.verify(
  img1_path = "img1.jpg", img2_path = "img2.jpg", distance_metric = metrics[1]
)

dfs = DeepFace.find(
  img_path = "img1.jpg", db_path = "C:/my_db", distance_metric = metrics[2]
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;API&lt;/strong&gt; - &lt;a href="https://youtu.be/HeKCQ6U9XmI"&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://youtu.be/9Tk9lRQareA"&gt;&lt;code&gt;Docker Demo&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;DeepFace serves an API as well - see &lt;a href="https://github.com/serengil/deepface/tree/master/deepface/api/src"&gt;&lt;code&gt;api folder&lt;/code&gt;&lt;/a&gt; for more details. You can clone deepface source code and run the api with the following command. It will use gunicorn server to get a rest service up. In this way, you can call deepface from an external system such as mobile app or web.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cd scripts

# run the service directly
./service.sh

# run the service via docker
./dockerize.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p align="center"&gt;&lt;img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/deepface-api.jpg" width="90%"&gt;&lt;/p&gt; 
&lt;p&gt;Face recognition, facial attribute analysis and vector representation functions are covered in the API. You are expected to call these functions as http post methods. Default service endpoints will be &lt;code&gt;http://localhost:5005/verify&lt;/code&gt; for face recognition, &lt;code&gt;http://localhost:5005/analyze&lt;/code&gt; for facial attribute analysis, and &lt;code&gt;http://localhost:5005/represent&lt;/code&gt; for vector representation. The API accepts images as file uploads (via form data), or as exact image paths, URLs, or base64-encoded strings (via either JSON or form data), providing versatile options for different client requirements. &lt;a href="https://github.com/serengil/deepface/tree/master/deepface/api/postman"&gt;Here&lt;/a&gt;, you can find a postman project to find out how these methods should be called.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Large Scale Facial Recognition&lt;/strong&gt; - &lt;a href="https://www.youtube.com/playlist?list=PLsS_1RYmYQQGSJu_Z3OVhXhGmZ86_zuIm"&gt;&lt;code&gt;Playlist&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;If your task requires facial recognition on large datasets, you should combine DeepFace with a vector index or vector database. This setup will perform &lt;a href="https://youtu.be/c10w0Ptn_CU"&gt;approximate nearest neighbor&lt;/a&gt; searches instead of exact ones, allowing you to identify a face in a database containing billions of entries within milliseconds. Common vector index solutions include &lt;a href="https://youtu.be/Jpxm914o2xk"&gt;Annoy&lt;/a&gt;, &lt;a href="https://youtu.be/6AmEvDTKT-k"&gt;Faiss&lt;/a&gt;, &lt;a href="https://youtu.be/2ZYTV9HlFdU"&gt;Voyager&lt;/a&gt;, &lt;a href="https://youtu.be/EVBhO8rbKbg"&gt;NMSLIB&lt;/a&gt;, &lt;a href="https://youtu.be/i4GvuOmzKzo"&gt;ElasticSearch&lt;/a&gt;. For vector databases, popular options are &lt;a href="https://youtu.be/Xfv4hCWvkp0"&gt;Postgres with its pgvector extension&lt;/a&gt; and &lt;a href="https://youtu.be/yrXlS0d6t4w"&gt;RediSearch&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt;&lt;img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/deepface-big-data.jpg" width="90%"&gt;&lt;/p&gt; 
&lt;p&gt;Conversely, if your task involves facial recognition on small to moderate-sized databases, you can adopt use relational databases such as &lt;a href="https://youtu.be/f41sLxn1c0k"&gt;Postgres&lt;/a&gt; or &lt;a href="https://youtu.be/_1ShBeWToPg"&gt;SQLite&lt;/a&gt;, or NoSQL databases like &lt;a href="https://youtu.be/dmprgum9Xu8"&gt;Mongo&lt;/a&gt;, &lt;a href="https://youtu.be/X7DSpUMVTsw"&gt;Redis&lt;/a&gt; or &lt;a href="https://youtu.be/J_yXpc3Y8Ec"&gt;Cassandra&lt;/a&gt; to perform exact nearest neighbor search.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Encrypt Embeddings&lt;/strong&gt; - &lt;a href="https://youtu.be/8VCu39jFZ7k"&gt;&lt;code&gt;Demo with PHE&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://sefiks.com/2025/03/04/vector-similarity-search-with-partially-homomorphic-encryption-in-python/"&gt;&lt;code&gt;Tutorial for PHE&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://youtu.be/njjw0PEhH00"&gt;&lt;code&gt;Demo with FHE&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://sefiks.com/2021/12/01/homomorphic-facial-recognition-with-tenseal/"&gt;&lt;code&gt;Tutorial for FHE&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Vector embeddings, though not reversible, carry sensitive information like fingerprints, making their security crucial. Encrypting them prevents adversarial misuse. Traditional encryption (e.g., AES) is secure but unsuitable for cloud-based distance calculations.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://youtu.be/3ejI0zNPMEQ"&gt;Homomorphic encryption&lt;/a&gt; allows computations on encrypted data without revealing content—ideal for secure cloud processing. For example, the cloud can compute encrypted similarity without knowing the data, while only the key holder can decrypt the result. See the &lt;a href="https://github.com/serengil/LightPHE"&gt;&lt;code&gt;LightPHE&lt;/code&gt;&lt;/a&gt; library for partially homomorphic encryption.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from lightphe import LightPHE

# build an additively homomorphic cryptosystem (e.g. Paillier) on-prem
cs = LightPHE(algorithm_name = "Paillier", precision = 19)

# define plain vectors for source and target
alpha = DeepFace.represent("img1.jpg")[0]["embedding"]
beta = DeepFace.represent("target.jpg")[0]["embedding"]

# encrypt source embedding on-prem - private key not required
encrypted_alpha = cs.encrypt(alpha)

# dot product of encrypted &amp;amp; plain embedding in cloud - private key not required
encrypted_cosine_similarity = encrypted_alpha @ beta

# decrypt similarity on-prem - private key required
calculated_similarity = cs.decrypt(encrypted_cosine_similarity)[0]

# verification
print("same person" if calculated_similarity &amp;gt;= 1 - threshold else "different persons")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p align="center"&gt;&lt;img src="https://raw.githubusercontent.com/serengil/deepface/refs/heads/master/icon/encrypt-embeddings.jpg" width="60%"&gt;&lt;/p&gt; 
&lt;p&gt;For stronger privacy, fully homomorphic encryption enables dot product computations between encrypted embeddings, but it's far more computationally intensive. Explore &lt;a href="https://github.com/serengil/cipherface"&gt;&lt;code&gt;CipherFace&lt;/code&gt;&lt;/a&gt; for FHE-based approaches.&lt;/p&gt; 
&lt;h3&gt;Extended Applications&lt;/h3&gt; 
&lt;p&gt;DeepFace can also be used for fun and insightful applications such as&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Find Your Celebrity Look-Alike&lt;/strong&gt; - &lt;a href="https://youtu.be/jaxkEn-Kieo"&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://youtu.be/RMgIKU1H8DY"&gt;&lt;code&gt;Real-Time Demo&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://sefiks.com/2019/05/05/celebrity-look-alike-face-recognition-with-deep-learning-in-keras/"&gt;&lt;code&gt;Tutorial&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;DeepFace can analyze your facial features and match them with celebrities, letting you discover which famous personality you resemble the most.&lt;/p&gt; 
&lt;p align="center"&gt;&lt;img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/celebrity-look-alike.jpg" width="55%"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Find Which Parent a Child Look More&lt;/strong&gt; - &lt;a href="https://youtu.be/nza4tmi9vhE"&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://sefiks.com/2022/12/22/decide-whom-your-child-looks-like-with-facial-recognition-mommy-or-daddy/"&gt;&lt;code&gt;Tutorial&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;DeepFace can also be used to compare a child's face to their parents' or relatives' faces to determine which one the child resembles more.&lt;/p&gt; 
&lt;p align="center"&gt;&lt;img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/parental-look-alike-scaled.jpg" width="90%"&gt;&lt;/p&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;p&gt;Pull requests are more than welcome! If you are planning to contribute a large patch, please create an issue first to get any upfront questions or design decisions out of the way first.&lt;/p&gt; 
&lt;p&gt;Before creating a PR, you should run the unit tests and linting locally by running &lt;code&gt;make test &amp;amp;&amp;amp; make lint&lt;/code&gt; command. Once a PR sent, GitHub test workflow will be run automatically and unit test and linting jobs will be available in &lt;a href="https://github.com/serengil/deepface/actions"&gt;GitHub actions&lt;/a&gt; before approval.&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;There are many ways to support a project - starring⭐️ the GitHub repo is just one 🙏 It really helps the project get discovered by more people.&lt;/p&gt; 
&lt;p&gt;If you do like this work, then you can support it financially on &lt;a href="https://www.patreon.com/serengil?repo=deepface"&gt;Patreon&lt;/a&gt;, &lt;a href="https://github.com/sponsors/serengil"&gt;GitHub Sponsors&lt;/a&gt; or &lt;a href="https://buymeacoffee.com/serengil"&gt;Buy Me a Coffee&lt;/a&gt;. Also, your company's logo will be shown on README on GitHub if you become a sponsor in gold, silver or bronze tiers.&lt;/p&gt; 
&lt;a href="https://www.patreon.com/serengil?repo=deepface"&gt; &lt;img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/patreon.png" width="30%"&gt; &lt;/a&gt; 
&lt;!--
&lt;a href="https://github.com/sponsors/serengil"&gt;
&lt;img src="https://raw.githubusercontent.com/serengil/deepface/refs/heads/master/icon/github_sponsor_button.png" width="37%"&gt;
&lt;/a&gt;

&lt;a href="https://buymeacoffee.com/serengil"&gt;
&lt;img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/bmc-button.png" width="25%"&gt;
&lt;/a&gt;
--&gt; 
&lt;!--
Additionally, you can help us reach a wider audience by upvoting our posts on Hacker News and Product Hunt.

&lt;div style="display: flex; align-items: center; gap: 10px;"&gt;
  &lt;a href="https://news.ycombinator.com/item?id=42584896"&gt;
    &lt;img src="https://hackerbadge.vercel.app/api?id=42584896&amp;type=orange" style="width: 250px; height: 54px;" width="250" alt="Featured on Hacker News"&gt;
  &lt;/a&gt;
  
  &lt;a href="https://www.producthunt.com/posts/deepface?embed=true&amp;utm_source=badge-featured&amp;utm_medium=badge&amp;utm_souce=badge-deepface" target="_blank"&gt;
    &lt;img src="https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=753599&amp;theme=light" alt="DeepFace - A Lightweight Deep Face Recognition Library for Python | Product Hunt" style="width: 250px; height: 54px;" width="250" height="54" /&gt;
  &lt;/a&gt;
&lt;/div&gt;
--&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;Please cite deepface in your publications if it helps your research.&lt;/p&gt; 
&lt;details open&gt; 
 &lt;summary&gt;S. Serengil and A. Ozpinar, &lt;b&gt;"A Benchmark of Facial Recognition Pipelines and Co-Usability Performances of Modules"&lt;/b&gt;, &lt;i&gt;Journal of Information Technologies&lt;/i&gt;, vol. 17, no. 2, pp. 95-107, 2024.&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-BibTeX"&gt;@article{serengil2024lightface,
  title     = {A Benchmark of Facial Recognition Pipelines and Co-Usability Performances of Modules},
  author    = {Serengil, Sefik and Ozpinar, Alper},
  journal   = {Journal of Information Technologies},
  volume    = {17},
  number    = {2},
  pages     = {95-107},
  year      = {2024},
  doi       = {10.17671/gazibtd.1399077},
  url       = {https://dergipark.org.tr/en/pub/gazibtd/issue/84331/1399077},
  publisher = {Gazi University}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;S. I. Serengil and A. Ozpinar, &lt;b&gt;"LightFace: A Hybrid Deep Face Recognition Framework"&lt;/b&gt;, &lt;i&gt;2020 Innovations in Intelligent Systems and Applications Conference (ASYU)&lt;/i&gt;, 2020, pp. 23-27.&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-BibTeX"&gt;@inproceedings{serengil2020lightface,
  title        = {LightFace: A Hybrid Deep Face Recognition Framework},
  author       = {Serengil, Sefik Ilkin and Ozpinar, Alper},
  booktitle    = {2020 Innovations in Intelligent Systems and Applications Conference (ASYU)},
  pages        = {23-27},
  year         = {2020},
  doi          = {10.1109/ASYU50717.2020.9259802},
  url          = {https://ieeexplore.ieee.org/document/9259802},
  organization = {IEEE}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;S. I. Serengil and A. Ozpinar, &lt;b&gt;"HyperExtended LightFace: A Facial Attribute Analysis Framework"&lt;/b&gt;, &lt;i&gt;2021 International Conference on Engineering and Emerging Technologies (ICEET)&lt;/i&gt;, 2021, pp. 1-4.&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-BibTeX"&gt;@inproceedings{serengil2021lightface,
  title        = {HyperExtended LightFace: A Facial Attribute Analysis Framework},
  author       = {Serengil, Sefik Ilkin and Ozpinar, Alper},
  booktitle    = {2021 International Conference on Engineering and Emerging Technologies (ICEET)},
  pages        = {1-4},
  year         = {2021},
  doi          = {10.1109/ICEET53442.2021.9659697},
  url          = {https://ieeexplore.ieee.org/document/9659697},
  organization = {IEEE}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;Also, if you use deepface in your GitHub projects, please add &lt;code&gt;deepface&lt;/code&gt; in the &lt;code&gt;requirements.txt&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Licence&lt;/h2&gt; 
&lt;p&gt;DeepFace is licensed under the MIT License - see &lt;a href="https://github.com/serengil/deepface/raw/master/LICENSE"&gt;&lt;code&gt;LICENSE&lt;/code&gt;&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;DeepFace wraps some external face recognition models: &lt;a href="http://www.robots.ox.ac.uk/~vgg/software/vgg_face/"&gt;VGG-Face&lt;/a&gt;, &lt;a href="https://github.com/davidsandberg/facenet/raw/master/LICENSE.md"&gt;Facenet&lt;/a&gt; (both 128d and 512d), &lt;a href="https://github.com/iwantooxxoox/Keras-OpenFace/raw/master/LICENSE"&gt;OpenFace&lt;/a&gt;, &lt;a href="https://github.com/swghosh/DeepFace"&gt;DeepFace&lt;/a&gt;, &lt;a href="https://github.com/Ruoyiran/DeepID/raw/master/LICENSE.md"&gt;DeepID&lt;/a&gt;, &lt;a href="https://github.com/leondgarse/Keras_insightface/raw/master/LICENSE"&gt;ArcFace&lt;/a&gt;, &lt;a href="https://github.com/davisking/dlib/raw/master/dlib/LICENSE.txt"&gt;Dlib&lt;/a&gt;, &lt;a href="https://github.com/opencv/opencv_zoo/raw/master/models/face_recognition_sface/LICENSE"&gt;SFace&lt;/a&gt;, &lt;a href="https://github.com/HamadYA/GhostFaceNets/raw/main/LICENSE"&gt;GhostFaceNet&lt;/a&gt; and &lt;a href="https://github.com/deepinsight/insightface/raw/master/README.md"&gt;Buffalo_L&lt;/a&gt;. Besides, age, gender and race / ethnicity models were trained on the backbone of VGG-Face with transfer learning. Similarly, DeepFace wraps many face detectors: &lt;a href="https://github.com/opencv/opencv/raw/4.x/LICENSE"&gt;OpenCv&lt;/a&gt;, &lt;a href="https://github.com/opencv/opencv/raw/master/LICENSE"&gt;Ssd&lt;/a&gt;, &lt;a href="https://github.com/davisking/dlib/raw/master/LICENSE.txt"&gt;Dlib&lt;/a&gt;, &lt;a href="https://github.com/ipazc/mtcnn/raw/master/LICENSE"&gt;MtCnn&lt;/a&gt;, &lt;a href="https://github.com/timesler/facenet-pytorch/raw/master/LICENSE.md"&gt;Fast MtCnn&lt;/a&gt;, &lt;a href="https://github.com/serengil/retinaface/raw/master/LICENSE"&gt;RetinaFace&lt;/a&gt;, &lt;a href="https://github.com/google/mediapipe/raw/master/LICENSE"&gt;MediaPipe&lt;/a&gt;, &lt;a href="https://github.com/ShiqiYu/libfacedetection/raw/master/LICENSE"&gt;YuNet&lt;/a&gt;, &lt;a href="https://github.com/derronqi/yolov8-face/raw/main/LICENSE"&gt;Yolo&lt;/a&gt; and &lt;a href="https://github.com/Star-Clouds/CenterFace/raw/master/LICENSE"&gt;CenterFace&lt;/a&gt;. Finally, DeepFace is optionally using &lt;a href="https://github.com/minivision-ai/Silent-Face-Anti-Spoofing/raw/master/LICENSE"&gt;face anti spoofing&lt;/a&gt; to determine the given images are real or fake. License types will be inherited when you intend to utilize those models. Please check the license types of those models for production purposes.&lt;/p&gt; 
&lt;p&gt;DeepFace &lt;a href="https://thenounproject.com/term/face-recognition/2965879/"&gt;logo&lt;/a&gt; is created by &lt;a href="https://thenounproject.com/coquet_adrien/"&gt;Adrien Coquet&lt;/a&gt; and it is licensed under &lt;a href="https://creativecommons.org/licenses/by/3.0/"&gt;Creative Commons: By Attribution 3.0 License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>home-assistant/core</title>
      <link>https://github.com/home-assistant/core</link>
      <description>&lt;p&gt;🏡 Open source home automation that puts local control and privacy first.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Home Assistant |Chat Status|&lt;/h1&gt; 
&lt;p&gt;Open source home automation that puts local control and privacy first. Powered by a worldwide community of tinkerers and DIY enthusiasts. Perfect to run on a Raspberry Pi or a local server.&lt;/p&gt; 
&lt;p&gt;Check out &lt;code&gt;home-assistant.io &amp;lt;https://home-assistant.io&amp;gt;&lt;/code&gt;__ for &lt;code&gt;a demo &amp;lt;https://demo.home-assistant.io&amp;gt;&lt;/code&gt;&lt;strong&gt;, &lt;code&gt;installation instructions &amp;lt;https://home-assistant.io/getting-started/&amp;gt;&lt;/code&gt;&lt;/strong&gt;, &lt;code&gt;tutorials &amp;lt;https://home-assistant.io/getting-started/automation/&amp;gt;&lt;/code&gt;__ and &lt;code&gt;documentation &amp;lt;https://home-assistant.io/docs/&amp;gt;&lt;/code&gt;__.&lt;/p&gt; 
&lt;p&gt;|screenshot-states|&lt;/p&gt; 
&lt;h2&gt;Featured integrations&lt;/h2&gt; 
&lt;p&gt;|screenshot-integrations|&lt;/p&gt; 
&lt;p&gt;The system is built using a modular approach so support for other devices or actions can be implemented easily. See also the &lt;code&gt;section on architecture &amp;lt;https://developers.home-assistant.io/docs/architecture_index/&amp;gt;&lt;/code&gt;__ and the &lt;code&gt;section on creating your own components &amp;lt;https://developers.home-assistant.io/docs/creating_component_index/&amp;gt;&lt;/code&gt;__.&lt;/p&gt; 
&lt;p&gt;If you run into issues while using Home Assistant or during development of a component, check the &lt;code&gt;Home Assistant help section &amp;lt;https://home-assistant.io/help/&amp;gt;&lt;/code&gt;__ of our website for further help and information.&lt;/p&gt; 
&lt;p&gt;|ohf-logo|&lt;/p&gt; 
&lt;p&gt;.. |Chat Status| image:: &lt;a href="https://img.shields.io/discord/330944238910963714.svg"&gt;https://img.shields.io/discord/330944238910963714.svg&lt;/a&gt; :target: &lt;a href="https://www.home-assistant.io/join-chat/"&gt;https://www.home-assistant.io/join-chat/&lt;/a&gt; .. |screenshot-states| image:: &lt;a href="https://raw.githubusercontent.com/home-assistant/core/dev/.github/assets/screenshot-states.png"&gt;https://raw.githubusercontent.com/home-assistant/core/dev/.github/assets/screenshot-states.png&lt;/a&gt; :target: &lt;a href="https://demo.home-assistant.io"&gt;https://demo.home-assistant.io&lt;/a&gt; .. |screenshot-integrations| image:: &lt;a href="https://raw.githubusercontent.com/home-assistant/core/dev/.github/assets/screenshot-integrations.png"&gt;https://raw.githubusercontent.com/home-assistant/core/dev/.github/assets/screenshot-integrations.png&lt;/a&gt; :target: &lt;a href="https://home-assistant.io/integrations/"&gt;https://home-assistant.io/integrations/&lt;/a&gt; .. |ohf-logo| image:: &lt;a href="https://www.openhomefoundation.org/badges/home-assistant.png"&gt;https://www.openhomefoundation.org/badges/home-assistant.png&lt;/a&gt; :alt: Home Assistant - A project from the Open Home Foundation :target: &lt;a href="https://www.openhomefoundation.org/"&gt;https://www.openhomefoundation.org/&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>vllm-project/vllm</title>
      <link>https://github.com/vllm-project/vllm</link>
      <description>&lt;p&gt;A high-throughput and memory-efficient inference and serving engine for LLMs&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-dark.png"&gt; 
  &lt;img alt="vLLM" src="https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-light.png" width="55%"&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;h3 align="center"&gt; Easy, fast, and cheap LLM serving for everyone &lt;/h3&gt; 
&lt;p align="center"&gt; | &lt;a href="https://docs.vllm.ai"&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://blog.vllm.ai/"&gt;&lt;b&gt;Blog&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2309.06180"&gt;&lt;b&gt;Paper&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://x.com/vllm_project"&gt;&lt;b&gt;Twitter/X&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://discuss.vllm.ai"&gt;&lt;b&gt;User Forum&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://slack.vllm.ai"&gt;&lt;b&gt;Developer Slack&lt;/b&gt;&lt;/a&gt; | &lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;em&gt;Latest News&lt;/em&gt; 🔥&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2025/05] We hosted &lt;a href="https://lu.ma/c1rqyf1f"&gt;NYC vLLM Meetup&lt;/a&gt;! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1_q_aW_ioMJWUImf1s1YM-ZhjXz8cUeL0IJvaquOYBeA/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/05] vLLM is now a hosted project under PyTorch Foundation! Please find the announcement &lt;a href="https://pytorch.org/blog/pytorch-foundation-welcomes-vllm/"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/04] We hosted &lt;a href="https://www.sginnovate.com/event/limited-availability-morning-evening-slots-remaining-inaugural-vllm-asia-developer-day"&gt;Asia Developer Day&lt;/a&gt;! Please find the meetup slides from the vLLM team &lt;a href="https://docs.google.com/presentation/d/19cp6Qu8u48ihB91A064XfaXruNYiBOUKrBxAmDOllOo/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/01] We are excited to announce the alpha release of vLLM V1: A major architectural upgrade with 1.7x speedup! Clean code, optimized execution loop, zero-overhead prefix caching, enhanced multimodal support, and more. Please check out our blog post &lt;a href="https://blog.vllm.ai/2025/01/27/v1-alpha-release.html"&gt;here&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Previous News&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;[2025/03] We hosted &lt;a href="https://lu.ma/vllm-ollama"&gt;vLLM x Ollama Inference Night&lt;/a&gt;! Please find the meetup slides from the vLLM team &lt;a href="https://docs.google.com/presentation/d/16T2PDD1YwRnZ4Tu8Q5r6n53c5Lr5c73UV9Vd2_eBo4U/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025/03] We hosted &lt;a href="https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg"&gt;the first vLLM China Meetup&lt;/a&gt;! Please find the meetup slides from vLLM team &lt;a href="https://docs.google.com/presentation/d/1REHvfQMKGnvz6p3Fd23HhSO4c8j5WPGZV0bKYLwnHyQ/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025/03] We hosted &lt;a href="https://lu.ma/7mu4k4xx"&gt;the East Coast vLLM Meetup&lt;/a&gt;! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1NHiv8EUFF1NLd3fEYODm56nDmL26lEeXCaDgyDlTsRs/edit#slide=id.g31441846c39_0_0"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025/02] We hosted &lt;a href="https://lu.ma/h7g3kuj9"&gt;the ninth vLLM meetup&lt;/a&gt; with Meta! Please find the meetup slides from vLLM team &lt;a href="https://docs.google.com/presentation/d/1jzC_PZVXrVNSFVCW-V4cFXb6pn7zZ2CyP_Flwo05aqg/edit?usp=sharing"&gt;here&lt;/a&gt; and AMD &lt;a href="https://drive.google.com/file/d/1Zk5qEJIkTmlQ2eQcXQZlljAx3m9s7nwn/view?usp=sharing"&gt;here&lt;/a&gt;. The slides from Meta will not be posted.&lt;/li&gt; 
  &lt;li&gt;[2025/01] We hosted &lt;a href="https://lu.ma/zep56hui"&gt;the eighth vLLM meetup&lt;/a&gt; with Google Cloud! Please find the meetup slides from vLLM team &lt;a href="https://docs.google.com/presentation/d/1epVkt4Zu8Jz_S5OhEHPc798emsYh2BwYfRuDDVEF7u4/edit?usp=sharing"&gt;here&lt;/a&gt;, and Google Cloud team &lt;a href="https://drive.google.com/file/d/1h24pHewANyRL11xy5dXUbvRC9F9Kkjix/view?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/12] vLLM joins &lt;a href="https://pytorch.org/blog/vllm-joins-pytorch"&gt;pytorch ecosystem&lt;/a&gt;! Easy, Fast, and Cheap LLM Serving for Everyone!&lt;/li&gt; 
  &lt;li&gt;[2024/11] We hosted &lt;a href="https://lu.ma/h0qvrajz"&gt;the seventh vLLM meetup&lt;/a&gt; with Snowflake! Please find the meetup slides from vLLM team &lt;a href="https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing"&gt;here&lt;/a&gt;, and Snowflake team &lt;a href="https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/10] We have just created a developer slack (&lt;a href="https://slack.vllm.ai"&gt;slack.vllm.ai&lt;/a&gt;) focusing on coordinating contributions and discussing features. Please feel free to join us there!&lt;/li&gt; 
  &lt;li&gt;[2024/10] Ray Summit 2024 held a special track for vLLM! Please find the opening talk slides from the vLLM team &lt;a href="https://docs.google.com/presentation/d/1B_KQxpHBTRa_mDF-tR6i8rWdOU5QoTZNcEg2MKZxEHM/edit?usp=sharing"&gt;here&lt;/a&gt;. Learn more from the &lt;a href="https://www.youtube.com/playlist?list=PLzTswPQNepXl6AQwifuwUImLPFRVpksjR"&gt;talks&lt;/a&gt; from other vLLM contributors and users!&lt;/li&gt; 
  &lt;li&gt;[2024/09] We hosted &lt;a href="https://lu.ma/87q3nvnh"&gt;the sixth vLLM meetup&lt;/a&gt; with NVIDIA! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/07] We hosted &lt;a href="https://lu.ma/lp0gyjqr"&gt;the fifth vLLM meetup&lt;/a&gt; with AWS! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/07] In partnership with Meta, vLLM officially supports Llama 3.1 with FP8 quantization and pipeline parallelism! Please check out our blog post &lt;a href="https://blog.vllm.ai/2024/07/23/llama31.html"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/06] We hosted &lt;a href="https://lu.ma/agivllm"&gt;the fourth vLLM meetup&lt;/a&gt; with Cloudflare and BentoML! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/04] We hosted &lt;a href="https://robloxandvllmmeetup2024.splashthat.com/"&gt;the third vLLM meetup&lt;/a&gt; with Roblox! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/01] We hosted &lt;a href="https://lu.ma/ygxbpzhl"&gt;the second vLLM meetup&lt;/a&gt; with IBM! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2023/10] We hosted &lt;a href="https://lu.ma/first-vllm-meetup"&gt;the first vLLM meetup&lt;/a&gt; with a16z! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2023/08] We would like to express our sincere gratitude to &lt;a href="https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/"&gt;Andreessen Horowitz&lt;/a&gt; (a16z) for providing a generous grant to support the open-source development and research of vLLM.&lt;/li&gt; 
  &lt;li&gt;[2023/06] We officially released vLLM! FastChat-vLLM integration has powered &lt;a href="https://chat.lmsys.org"&gt;LMSYS Vicuna and Chatbot Arena&lt;/a&gt; since mid-April. Check out our &lt;a href="https://vllm.ai"&gt;blog post&lt;/a&gt;.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr&gt; 
&lt;h2&gt;About&lt;/h2&gt; 
&lt;p&gt;vLLM is a fast and easy-to-use library for LLM inference and serving.&lt;/p&gt; 
&lt;p&gt;Originally developed in the &lt;a href="https://sky.cs.berkeley.edu"&gt;Sky Computing Lab&lt;/a&gt; at UC Berkeley, vLLM has evolved into a community-driven project with contributions from both academia and industry.&lt;/p&gt; 
&lt;p&gt;vLLM is fast with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;State-of-the-art serving throughput&lt;/li&gt; 
 &lt;li&gt;Efficient management of attention key and value memory with &lt;a href="https://blog.vllm.ai/2023/06/20/vllm.html"&gt;&lt;strong&gt;PagedAttention&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Continuous batching of incoming requests&lt;/li&gt; 
 &lt;li&gt;Fast model execution with CUDA/HIP graph&lt;/li&gt; 
 &lt;li&gt;Quantizations: &lt;a href="https://arxiv.org/abs/2210.17323"&gt;GPTQ&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2306.00978"&gt;AWQ&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2309.05516"&gt;AutoRound&lt;/a&gt;, INT4, INT8, and FP8&lt;/li&gt; 
 &lt;li&gt;Optimized CUDA kernels, including integration with FlashAttention and FlashInfer&lt;/li&gt; 
 &lt;li&gt;Speculative decoding&lt;/li&gt; 
 &lt;li&gt;Chunked prefill&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;vLLM is flexible and easy to use with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Seamless integration with popular Hugging Face models&lt;/li&gt; 
 &lt;li&gt;High-throughput serving with various decoding algorithms, including &lt;em&gt;parallel sampling&lt;/em&gt;, &lt;em&gt;beam search&lt;/em&gt;, and more&lt;/li&gt; 
 &lt;li&gt;Tensor, pipeline, data and expert parallelism support for distributed inference&lt;/li&gt; 
 &lt;li&gt;Streaming outputs&lt;/li&gt; 
 &lt;li&gt;OpenAI-compatible API server&lt;/li&gt; 
 &lt;li&gt;Support NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, TPU, and AWS Neuron&lt;/li&gt; 
 &lt;li&gt;Prefix caching support&lt;/li&gt; 
 &lt;li&gt;Multi-LoRA support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;vLLM seamlessly supports most popular open-source models on HuggingFace, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Transformer-like LLMs (e.g., Llama)&lt;/li&gt; 
 &lt;li&gt;Mixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3)&lt;/li&gt; 
 &lt;li&gt;Embedding Models (e.g., E5-Mistral)&lt;/li&gt; 
 &lt;li&gt;Multi-modal LLMs (e.g., LLaVA)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Find the full list of supported models &lt;a href="https://docs.vllm.ai/en/latest/models/supported_models.html"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Install vLLM with &lt;code&gt;pip&lt;/code&gt; or &lt;a href="https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source"&gt;from source&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install vllm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Visit our &lt;a href="https://docs.vllm.ai/en/latest/"&gt;documentation&lt;/a&gt; to learn more.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.vllm.ai/en/latest/getting_started/installation.html"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.vllm.ai/en/latest/getting_started/quickstart.html"&gt;Quickstart&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.vllm.ai/en/latest/models/supported_models.html"&gt;List of Supported Models&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome and value any contributions and collaborations. Please check out &lt;a href="https://docs.vllm.ai/en/latest/contributing/index.html"&gt;Contributing to vLLM&lt;/a&gt; for how to get involved.&lt;/p&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;p&gt;vLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!&lt;/p&gt; 
&lt;!-- Note: Please sort them in alphabetical order. --&gt; 
&lt;!-- Note: Please keep these consistent with docs/community/sponsors.md --&gt; 
&lt;p&gt;Cash Donations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;a16z&lt;/li&gt; 
 &lt;li&gt;Dropbox&lt;/li&gt; 
 &lt;li&gt;Sequoia Capital&lt;/li&gt; 
 &lt;li&gt;Skywork AI&lt;/li&gt; 
 &lt;li&gt;ZhenFund&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Compute Resources:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;AMD&lt;/li&gt; 
 &lt;li&gt;Anyscale&lt;/li&gt; 
 &lt;li&gt;AWS&lt;/li&gt; 
 &lt;li&gt;Crusoe Cloud&lt;/li&gt; 
 &lt;li&gt;Databricks&lt;/li&gt; 
 &lt;li&gt;DeepInfra&lt;/li&gt; 
 &lt;li&gt;Google Cloud&lt;/li&gt; 
 &lt;li&gt;Intel&lt;/li&gt; 
 &lt;li&gt;Lambda Lab&lt;/li&gt; 
 &lt;li&gt;Nebius&lt;/li&gt; 
 &lt;li&gt;Novita AI&lt;/li&gt; 
 &lt;li&gt;NVIDIA&lt;/li&gt; 
 &lt;li&gt;Replicate&lt;/li&gt; 
 &lt;li&gt;Roblox&lt;/li&gt; 
 &lt;li&gt;RunPod&lt;/li&gt; 
 &lt;li&gt;Trainy&lt;/li&gt; 
 &lt;li&gt;UC Berkeley&lt;/li&gt; 
 &lt;li&gt;UC San Diego&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Slack Sponsor: Anyscale&lt;/p&gt; 
&lt;p&gt;We also have an official fundraising venue through &lt;a href="https://opencollective.com/vllm"&gt;OpenCollective&lt;/a&gt;. We plan to use the fund to support the development, maintenance, and adoption of vLLM.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use vLLM for your research, please cite our &lt;a href="https://arxiv.org/abs/2309.06180"&gt;paper&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contact Us&lt;/h2&gt; 
&lt;!-- --8&lt;-- [start:contact-us] --&gt; 
&lt;ul&gt; 
 &lt;li&gt;For technical questions and feature requests, please use GitHub &lt;a href="https://github.com/vllm-project/vllm/issues"&gt;Issues&lt;/a&gt; or &lt;a href="https://github.com/vllm-project/vllm/discussions"&gt;Discussions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;For discussing with fellow users, please use the &lt;a href="https://discuss.vllm.ai"&gt;vLLM Forum&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;For coordinating contributions and development, please use &lt;a href="https://slack.vllm.ai"&gt;Slack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;For security disclosures, please use GitHub's &lt;a href="https://github.com/vllm-project/vllm/security/advisories"&gt;Security Advisories&lt;/a&gt; feature&lt;/li&gt; 
 &lt;li&gt;For collaborations and partnerships, please contact us at &lt;a href="mailto:vllm-questions@lists.berkeley.edu"&gt;vllm-questions@lists.berkeley.edu&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- --8&lt;-- [end:contact-us] --&gt; 
&lt;h2&gt;Media Kit&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you wish to use vLLM's logo, please refer to &lt;a href="https://github.com/vllm-project/media-kit"&gt;our media kit repo&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>faif/python-patterns</title>
      <link>https://github.com/faif/python-patterns</link>
      <description>&lt;p&gt;A collection of design patterns/idioms in Python&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;python-patterns&lt;/h1&gt; 
&lt;p&gt;A collection of design patterns and idioms in Python.&lt;/p&gt; 
&lt;p&gt;Remember that each pattern has its own trade-offs. And you need to pay attention more to why you're choosing a certain pattern than to how to implement it.&lt;/p&gt; 
&lt;h2&gt;Current Patterns&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Creational Patterns&lt;/strong&gt;:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Pattern&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/creational/abstract_factory.py"&gt;abstract_factory&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;use a generic function with specific factories&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/creational/borg.py"&gt;borg&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;a singleton with shared-state among instances&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/creational/builder.py"&gt;builder&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;instead of using multiple constructors, builder object receives parameters and returns constructed objects&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/creational/factory.py"&gt;factory&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;delegate a specialized function/method to create instances&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/creational/lazy_evaluation.py"&gt;lazy_evaluation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;lazily-evaluated property pattern in Python&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/creational/pool.py"&gt;pool&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;preinstantiate and maintain a group of instances of the same type&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/creational/prototype.py"&gt;prototype&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;use a factory and clones of a prototype for new instances (if instantiation is expensive)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Structural Patterns&lt;/strong&gt;:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Pattern&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/structural/3-tier.py"&gt;3-tier&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;data&amp;lt;-&amp;gt;business logic&amp;lt;-&amp;gt;presentation separation (strict relationships)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/structural/adapter.py"&gt;adapter&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;adapt one interface to another using a white-list&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/structural/bridge.py"&gt;bridge&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;a client-provider middleman to soften interface changes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/structural/composite.py"&gt;composite&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;lets clients treat individual objects and compositions uniformly&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/structural/decorator.py"&gt;decorator&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;wrap functionality with other functionality in order to affect outputs&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/structural/facade.py"&gt;facade&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;use one class as an API to a number of others&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/structural/flyweight.py"&gt;flyweight&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;transparently reuse existing instances of objects with similar/identical state&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/structural/front_controller.py"&gt;front_controller&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;single handler requests coming to the application&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/structural/mvc.py"&gt;mvc&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;model&amp;lt;-&amp;gt;view&amp;lt;-&amp;gt;controller (non-strict relationships)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/structural/proxy.py"&gt;proxy&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;an object funnels operations to something else&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Behavioral Patterns&lt;/strong&gt;:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Pattern&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/behavioral/chain_of_responsibility.py"&gt;chain_of_responsibility&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;apply a chain of successive handlers to try and process the data&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/behavioral/catalog.py"&gt;catalog&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;general methods will call different specialized methods based on construction parameter&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/behavioral/chaining_method.py"&gt;chaining_method&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;continue callback next object method&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/behavioral/command.py"&gt;command&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bundle a command and arguments to call later&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/behavioral/iterator.py"&gt;iterator&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;traverse a container and access the container's elements&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/behavioral/iterator_alt.py"&gt;iterator&lt;/a&gt; (alt. impl.)&lt;/td&gt; 
   &lt;td&gt;traverse a container and access the container's elements&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/behavioral/mediator.py"&gt;mediator&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;an object that knows how to connect other objects and act as a proxy&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/behavioral/memento.py"&gt;memento&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;generate an opaque token that can be used to go back to a previous state&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/behavioral/observer.py"&gt;observer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;provide a callback for notification of events/changes to data&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/behavioral/publish_subscribe.py"&gt;publish_subscribe&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;a source syndicates events/data to 0+ registered listeners&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/behavioral/registry.py"&gt;registry&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;keep track of all subclasses of a given class&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/behavioral/specification.py"&gt;specification&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;business rules can be recombined by chaining the business rules together using boolean logic&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/behavioral/state.py"&gt;state&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;logic is organized into a discrete number of potential states and the next state that can be transitioned to&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/behavioral/strategy.py"&gt;strategy&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;selectable operations over the same data&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/behavioral/template.py"&gt;template&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;an object imposes a structure but takes pluggable components&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/behavioral/visitor.py"&gt;visitor&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;invoke a callback for all items of a collection&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Design for Testability Patterns&lt;/strong&gt;:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Pattern&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/dependency_injection.py"&gt;dependency_injection&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3 variants of dependency injection&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Fundamental Patterns&lt;/strong&gt;:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Pattern&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/fundamental/delegation_pattern.py"&gt;delegation_pattern&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;an object handles a request by delegating to a second object (the delegate)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Others&lt;/strong&gt;:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Pattern&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/other/blackboard.py"&gt;blackboard&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;architectural model, assemble different sub-system knowledge to build a solution, AI approach - non gang of four pattern&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/other/graph_search.py"&gt;graph_search&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;graphing algorithms - non gang of four pattern&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/other/hsm/hsm.py"&gt;hsm&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;hierarchical state machine - non gang of four pattern&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Videos&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=bsyjSW46TDg"&gt;Design Patterns in Python by Peter Ullrich&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=G5OeYHCJuv0"&gt;Sebastian Buczyński - Why you don't need design patterns in Python?&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=imW-trt0i9I"&gt;You Don't Need That!&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=PfgEU3W0kyU"&gt;Pluggable Libs Through Design Patterns&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;When an implementation is added or modified, please review the following guidelines:&lt;/p&gt; 
&lt;h5&gt;Docstrings&lt;/h5&gt; 
&lt;p&gt;Add module level description in form of a docstring with links to corresponding references or other useful information.&lt;/p&gt; 
&lt;p&gt;Add "Examples in Python ecosystem" section if you know some. It shows how patterns could be applied to real-world problems.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/structural/facade.py"&gt;facade.py&lt;/a&gt; has a good example of detailed description, but sometimes the shorter one as in &lt;a href="https://raw.githubusercontent.com/faif/python-patterns/master/patterns/behavioral/template.py"&gt;template.py&lt;/a&gt; would suffice.&lt;/p&gt; 
&lt;h5&gt;Python 2 compatibility&lt;/h5&gt; 
&lt;p&gt;To see Python 2 compatible versions of some patterns please check-out the &lt;a href="https://github.com/faif/python-patterns/tree/legacy"&gt;legacy&lt;/a&gt; tag.&lt;/p&gt; 
&lt;h5&gt;Update README&lt;/h5&gt; 
&lt;p&gt;When everything else is done - update corresponding part of README.&lt;/p&gt; 
&lt;h5&gt;Travis CI&lt;/h5&gt; 
&lt;p&gt;Please run the following before submitting a patch&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;black .&lt;/code&gt; This lints your code.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Then either:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;tox&lt;/code&gt; or &lt;code&gt;tox -e ci37&lt;/code&gt; This runs unit tests. see tox.ini for further details.&lt;/li&gt; 
 &lt;li&gt;If you have a bash compatible shell use &lt;code&gt;./lint.sh&lt;/code&gt; This script will lint and test your code. This script mirrors the CI pipeline actions.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can also run &lt;code&gt;flake8&lt;/code&gt; or &lt;code&gt;pytest&lt;/code&gt; commands manually. Examples can be found in &lt;code&gt;tox.ini&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing via issue triage &lt;a href="https://www.codetriage.com/faif/python-patterns"&gt;&lt;img src="https://www.codetriage.com/faif/python-patterns/badges/users.svg?sanitize=true" alt="Open Source Helpers"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;You can triage issues and pull requests which may include reproducing bug reports or asking for vital information, such as version numbers or reproduction instructions. If you would like to start triaging issues, one easy way to get started is to &lt;a href="https://www.codetriage.com/faif/python-patterns"&gt;subscribe to python-patterns on CodeTriage&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>QwenLM/Qwen</title>
      <link>https://github.com/QwenLM/Qwen</link>
      <description>&lt;p&gt;The official repo of Qwen (通义千问) chat &amp; pretrained large language model proposed by Alibaba Cloud.&lt;/p&gt;&lt;hr&gt;&lt;p align="left"&gt; &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen/main/README_CN.md"&gt;中文&lt;/a&gt;&amp;nbsp; ｜ &amp;amp;nbspEnglish&amp;nbsp; ｜ &amp;nbsp;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen/main/README_JA.md"&gt;日本語&lt;/a&gt; ｜ &amp;nbsp;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen/main/README_FR.md"&gt;Français&lt;/a&gt; ｜ &amp;nbsp;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen/main/README_ES.md"&gt;Español&lt;/a&gt; &lt;/p&gt; 
&lt;br&gt;
&lt;br&gt; 
&lt;p align="center"&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg" width="400"&gt; &lt;/p&gt;
&lt;p&gt; &lt;br&gt; &lt;/p&gt;
&lt;p align="center"&gt; 🤗 &lt;a href="https://huggingface.co/Qwen"&gt;Hugging Face&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;🤖 &lt;a href="https://modelscope.cn/organization/qwen"&gt;ModelScope&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; 📑 &lt;a href="https://arxiv.org/abs/2309.16609"&gt;Paper&lt;/a&gt; &amp;nbsp;&amp;nbsp; ｜ &amp;nbsp;&amp;nbsp;🖥️ &lt;a href="https://modelscope.cn/studios/qwen/Qwen-72B-Chat-Demo/summary"&gt;Demo&lt;/a&gt; &lt;br&gt; &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen/main/assets/wechat.png"&gt;WeChat (微信)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;&lt;a href="https://discord.gg/CV4E9rpNSD"&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; ｜ &amp;nbsp;&amp;nbsp;&lt;a href="https://dashscope.aliyun.com"&gt;API&lt;/a&gt; &lt;/p&gt; 
&lt;br&gt;
&lt;br&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Important] Qwen2 is here! You are welcome to follow &lt;a href="https://github.com/QwenLM/Qwen2"&gt;QwenLM/Qwen2&lt;/a&gt; and share your experience there.&lt;/p&gt; 
 &lt;p&gt;This repo (&lt;a href="https://github.com/QwenLM/Qwen"&gt;QwenLM/Qwen&lt;/a&gt;) is no longer actively maintained, due to substantial codebase differences.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;br&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;Qwen-Chat&lt;/th&gt; 
   &lt;th align="center"&gt;Qwen-Chat (Int4)&lt;/th&gt; 
   &lt;th align="center"&gt;Qwen-Chat (Int8)&lt;/th&gt; 
   &lt;th align="center"&gt;Qwen&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1.8B&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://modelscope.cn/models/qwen/Qwen-1_8B-Chat/summary"&gt;🤖&lt;/a&gt; &lt;a href="https://huggingface.co/Qwen/Qwen-1_8B-Chat"&gt;🤗&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://modelscope.cn/models/qwen/Qwen-1_8B-Chat-Int4/summary"&gt;🤖&lt;/a&gt; &lt;a href="https://huggingface.co/Qwen/Qwen-1_8B-Chat-Int4"&gt;🤗&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://modelscope.cn/models/qwen/Qwen-1_8B-Chat-Int8/summary"&gt;🤖&lt;/a&gt; &lt;a href="https://huggingface.co/Qwen/Qwen-1_8B-Chat-Int8"&gt;🤗&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://modelscope.cn/models/qwen/Qwen-1_8B/summary"&gt;🤖&lt;/a&gt; &lt;a href="https://huggingface.co/Qwen/Qwen-1_8B"&gt;🤗&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary"&gt;🤖&lt;/a&gt; &lt;a href="https://huggingface.co/Qwen/Qwen-7B-Chat"&gt;🤗&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int4/summary"&gt;🤖&lt;/a&gt; &lt;a href="https://huggingface.co/Qwen/Qwen-7B-Chat-Int4"&gt;🤗&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int8/summary"&gt;🤖&lt;/a&gt; &lt;a href="https://huggingface.co/Qwen/Qwen-7B-Chat-Int8"&gt;🤗&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://modelscope.cn/models/qwen/Qwen-7B/summary"&gt;🤖&lt;/a&gt; &lt;a href="https://huggingface.co/Qwen/Qwen-7B"&gt;🤗&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14B&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://modelscope.cn/models/qwen/Qwen-14B-Chat/summary"&gt;🤖&lt;/a&gt; &lt;a href="https://huggingface.co/Qwen/Qwen-14B-Chat"&gt;🤗&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int4/summary"&gt;🤖&lt;/a&gt; &lt;a href="https://huggingface.co/Qwen/Qwen-14B-Chat-Int4"&gt;🤗&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int8/summary"&gt;🤖&lt;/a&gt; &lt;a href="https://huggingface.co/Qwen/Qwen-14B-Chat-Int8"&gt;🤗&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://modelscope.cn/models/qwen/Qwen-14B/summary"&gt;🤖&lt;/a&gt; &lt;a href="https://huggingface.co/Qwen/Qwen-14B"&gt;🤗&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;72B&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://modelscope.cn/models/qwen/Qwen-72B-Chat/summary"&gt;🤖&lt;/a&gt; &lt;a href="https://huggingface.co/Qwen/Qwen-72B-Chat"&gt;🤗&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://modelscope.cn/models/qwen/Qwen-72B-Chat-Int4/summary"&gt;🤖&lt;/a&gt; &lt;a href="https://huggingface.co/Qwen/Qwen-72B-Chat-Int4"&gt;🤗&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://modelscope.cn/models/qwen/Qwen-72B-Chat-Int8/summary"&gt;🤖&lt;/a&gt; &lt;a href="https://huggingface.co/Qwen/Qwen-72B-Chat-Int8"&gt;🤗&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://modelscope.cn/models/qwen/Qwen-72B/summary"&gt;🤖&lt;/a&gt; &lt;a href="https://huggingface.co/Qwen/Qwen-72B"&gt;🤗&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;We opensource our &lt;strong&gt;Qwen&lt;/strong&gt; series, now including &lt;strong&gt;Qwen&lt;/strong&gt;, the base language models, namely &lt;strong&gt;Qwen-1.8B&lt;/strong&gt;, &lt;strong&gt;Qwen-7B&lt;/strong&gt;, &lt;strong&gt;Qwen-14B&lt;/strong&gt;, and &lt;strong&gt;Qwen-72B&lt;/strong&gt;, as well as &lt;strong&gt;Qwen-Chat&lt;/strong&gt;, the chat models, namely &lt;strong&gt;Qwen-1.8B-Chat&lt;/strong&gt;, &lt;strong&gt;Qwen-7B-Chat&lt;/strong&gt;, &lt;strong&gt;Qwen-14B-Chat&lt;/strong&gt;, and &lt;strong&gt;Qwen-72B-Chat&lt;/strong&gt;. Links are on the above table. Click them and check the model cards. Also, we release the &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2309.16609"&gt;technical report&lt;/a&gt;&lt;/strong&gt;. Please click the paper link and check it out!&lt;/p&gt; 
&lt;p&gt;In brief, we have strong base language models, which have been stably pretrained for up to 3 trillion tokens of multilingual data with a wide coverage of domains, languages (with a focus on Chinese and English), etc. They are able to achieve competitive performance on benchmark datasets. Additionally, we have chat models that are aligned with human preference based on SFT and RLHF (not released yet), which are able to chat, create content, extract information, summarize, translate, code, solve math problems, and so on, and are able to use tools, play as agents, or even play as code interpreters, etc.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Model&lt;/th&gt; 
   &lt;th align="center"&gt;Release Date&lt;/th&gt; 
   &lt;th align="center"&gt;Max Length&lt;/th&gt; 
   &lt;th align="center"&gt;System Prompt Enhancement&lt;/th&gt; 
   &lt;th align="center"&gt;# of Pretrained Tokens&lt;/th&gt; 
   &lt;th align="center"&gt;Minimum GPU Memory Usage of Finetuning (Q-Lora)&lt;/th&gt; 
   &lt;th align="center"&gt;Minimum GPU Usage of Generating 2048 Tokens (Int4)&lt;/th&gt; 
   &lt;th align="center"&gt;Tool Usage&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Qwen-1.8B&lt;/td&gt; 
   &lt;td align="center"&gt;23.11.30&lt;/td&gt; 
   &lt;td align="center"&gt;32K&lt;/td&gt; 
   &lt;td align="center"&gt;✅&lt;/td&gt; 
   &lt;td align="center"&gt;2.2T&lt;/td&gt; 
   &lt;td align="center"&gt;5.8GB&lt;/td&gt; 
   &lt;td align="center"&gt;2.9GB&lt;/td&gt; 
   &lt;td align="center"&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Qwen-7B&lt;/td&gt; 
   &lt;td align="center"&gt;23.08.03&lt;/td&gt; 
   &lt;td align="center"&gt;32K&lt;/td&gt; 
   &lt;td align="center"&gt;❎&lt;/td&gt; 
   &lt;td align="center"&gt;2.4T&lt;/td&gt; 
   &lt;td align="center"&gt;11.5GB&lt;/td&gt; 
   &lt;td align="center"&gt;8.2GB&lt;/td&gt; 
   &lt;td align="center"&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Qwen-14B&lt;/td&gt; 
   &lt;td align="center"&gt;23.09.25&lt;/td&gt; 
   &lt;td align="center"&gt;8K&lt;/td&gt; 
   &lt;td align="center"&gt;❎&lt;/td&gt; 
   &lt;td align="center"&gt;3.0T&lt;/td&gt; 
   &lt;td align="center"&gt;18.7GB&lt;/td&gt; 
   &lt;td align="center"&gt;13.0GB&lt;/td&gt; 
   &lt;td align="center"&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Qwen-72B&lt;/td&gt; 
   &lt;td align="center"&gt;23.11.30&lt;/td&gt; 
   &lt;td align="center"&gt;32K&lt;/td&gt; 
   &lt;td align="center"&gt;✅&lt;/td&gt; 
   &lt;td align="center"&gt;3.0T&lt;/td&gt; 
   &lt;td align="center"&gt;61.4GB&lt;/td&gt; 
   &lt;td align="center"&gt;48.9GB&lt;/td&gt; 
   &lt;td align="center"&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;In this repo, you can figure out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Quickstart with Qwen, and enjoy the simple inference.&lt;/li&gt; 
 &lt;li&gt;Details about the quantization models, including GPTQ and KV cache quantization.&lt;/li&gt; 
 &lt;li&gt;Statistics of inference performance, including speed and memory.&lt;/li&gt; 
 &lt;li&gt;Tutorials on finetuning, including full-parameter tuning, LoRA, and Q-LoRA.&lt;/li&gt; 
 &lt;li&gt;Instructions on deployment, with the example of vLLM and FastChat.&lt;/li&gt; 
 &lt;li&gt;Instructions on building demos, including WebUI, CLI demo, etc.&lt;/li&gt; 
 &lt;li&gt;Introduction to DashScope API service, as well as the instructions on building an OpenAI-style API for your model.&lt;/li&gt; 
 &lt;li&gt;Information about Qwen for tool use, agent, and code interpreter&lt;/li&gt; 
 &lt;li&gt;Statistics of long-context understanding evaluation&lt;/li&gt; 
 &lt;li&gt;License agreement&lt;/li&gt; 
 &lt;li&gt;...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Also, if you meet problems, turn to &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen/main/FAQ.md"&gt;FAQ&lt;/a&gt; for help first. Still feeling struggled? Feel free to shoot us issues (better in English so that more people can understand you)! If you would like to help us, send us pull requests with no hesitation! We are always excited about PR!&lt;/p&gt; 
&lt;p&gt;Would like to chat with us or date us coffee time? Welcome to our Discord or WeChat! &lt;br&gt;&lt;br&gt;&lt;/p&gt; 
&lt;h2&gt;News and Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;2023.11.30 🔥 We release &lt;strong&gt;Qwen-72B&lt;/strong&gt; and &lt;strong&gt;Qwen-72B-Chat&lt;/strong&gt;, which are trained on 3T tokens and support 32k context, along with &lt;strong&gt;Qwen-1.8B&lt;/strong&gt;, and &lt;strong&gt;Qwen-1.8B-Chat&lt;/strong&gt;, on ModelScope and Hugging Face. We have also strengthened the System Prompt capabilities of the Qwen-72B-Chat and Qwen-1.8B-Chat, see &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen/main/examples/system_prompt.md"&gt;example documentation&lt;/a&gt;. Additionally, support the inference on &lt;strong&gt;Ascend 910&lt;/strong&gt; and &lt;strong&gt;Hygon DCU&lt;/strong&gt;. Check &lt;code&gt;ascend-support&lt;/code&gt; and &lt;code&gt;dcu-support&lt;/code&gt; for more details.&lt;/li&gt; 
 &lt;li&gt;2023.10.17 We release the Int8 quantized model &lt;strong&gt;Qwen-7B-Chat-Int8&lt;/strong&gt; and &lt;strong&gt;Qwen-14B-Chat-Int8&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;2023.9.25 🔥 We release &lt;strong&gt;Qwen-14B&lt;/strong&gt; and &lt;strong&gt;Qwen-14B-Chat&lt;/strong&gt; on ModelScope and Hugging Face, along with &lt;a href="https://github.com/QwenLM/qwen.cpp"&gt;qwen.cpp&lt;/a&gt; and &lt;a href="https://github.com/QwenLM/Qwen-Agent"&gt;Qwen-Agent&lt;/a&gt;. Codes and checkpoints of &lt;strong&gt;Qwen-7B&lt;/strong&gt; and &lt;strong&gt;Qwen-7B-Chat&lt;/strong&gt; are also updated. &lt;strong&gt;PLEASE PULL THE LATEST VERSION!&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Compared to &lt;strong&gt;Qwen-7B&lt;/strong&gt; (original), &lt;strong&gt;Qwen-7B&lt;/strong&gt; uses more training tokens, increasing from 2.2T tokens to 2.4T tokens, while the context length extends from 2048 to 8192. The Chinese knowledge and coding ability of &lt;strong&gt;Qwen-7B&lt;/strong&gt; have been further improved.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;2023.9.12 We now support finetuning on the Qwen-7B models, including full-parameter finetuning, LoRA and Q-LoRA.&lt;/li&gt; 
 &lt;li&gt;2023.8.21 We release the Int4 quantized model for Qwen-7B-Chat, &lt;strong&gt;Qwen-7B-Chat-Int4&lt;/strong&gt;, which requires low memory costs but achieves improved inference speed. Besides, there is no significant performance degradation on the benchmark evaluation.&lt;/li&gt; 
 &lt;li&gt;2023.8.3 We release both &lt;strong&gt;Qwen-7B&lt;/strong&gt; and &lt;strong&gt;Qwen-7B-Chat&lt;/strong&gt; on ModelScope and Hugging Face. We also provide a technical memo for more details about the model, including training details and model performance.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;p&gt;Qwen models outperform the baseline models of similar model sizes on a series of benchmark datasets, e.g., MMLU, C-Eval, GSM8K, MATH, HumanEval, MBPP, BBH, etc., which evaluate the models’ capabilities on natural language understanding, mathematic problem solving, coding, etc. Qwen-72B achieves better performance than LLaMA2-70B on all tasks and outperforms GPT-3.5 on 7 out of 10 tasks.&lt;/p&gt; 
&lt;p align="left"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen/main/assets/radar_72b.jpg" width="600px/"&gt; &lt;/p&gt;
&lt;p&gt; &lt;br&gt; &lt;/p&gt;
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Model&lt;/th&gt; 
   &lt;th align="center"&gt;MMLU&lt;/th&gt; 
   &lt;th align="center"&gt;C-Eval&lt;/th&gt; 
   &lt;th align="center"&gt;GSM8K&lt;/th&gt; 
   &lt;th align="center"&gt;MATH&lt;/th&gt; 
   &lt;th align="center"&gt;HumanEval&lt;/th&gt; 
   &lt;th align="center"&gt;MBPP&lt;/th&gt; 
   &lt;th align="center"&gt;BBH&lt;/th&gt; 
   &lt;th align="center"&gt;CMMLU&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;5-shot&lt;/td&gt; 
   &lt;td align="center"&gt;5-shot&lt;/td&gt; 
   &lt;td align="center"&gt;8-shot&lt;/td&gt; 
   &lt;td align="center"&gt;4-shot&lt;/td&gt; 
   &lt;td align="center"&gt;0-shot&lt;/td&gt; 
   &lt;td align="center"&gt;3-shot&lt;/td&gt; 
   &lt;td align="center"&gt;3-shot&lt;/td&gt; 
   &lt;td align="center"&gt;5-shot&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;LLaMA2-7B&lt;/td&gt; 
   &lt;td align="center"&gt;46.8&lt;/td&gt; 
   &lt;td align="center"&gt;32.5&lt;/td&gt; 
   &lt;td align="center"&gt;16.7&lt;/td&gt; 
   &lt;td align="center"&gt;3.3&lt;/td&gt; 
   &lt;td align="center"&gt;12.8&lt;/td&gt; 
   &lt;td align="center"&gt;20.8&lt;/td&gt; 
   &lt;td align="center"&gt;38.2&lt;/td&gt; 
   &lt;td align="center"&gt;31.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;LLaMA2-13B&lt;/td&gt; 
   &lt;td align="center"&gt;55.0&lt;/td&gt; 
   &lt;td align="center"&gt;41.4&lt;/td&gt; 
   &lt;td align="center"&gt;29.6&lt;/td&gt; 
   &lt;td align="center"&gt;5.0&lt;/td&gt; 
   &lt;td align="center"&gt;18.9&lt;/td&gt; 
   &lt;td align="center"&gt;30.3&lt;/td&gt; 
   &lt;td align="center"&gt;45.6&lt;/td&gt; 
   &lt;td align="center"&gt;38.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;LLaMA2-34B&lt;/td&gt; 
   &lt;td align="center"&gt;62.6&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;42.2&lt;/td&gt; 
   &lt;td align="center"&gt;6.2&lt;/td&gt; 
   &lt;td align="center"&gt;22.6&lt;/td&gt; 
   &lt;td align="center"&gt;33.0&lt;/td&gt; 
   &lt;td align="center"&gt;44.1&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;ChatGLM2-6B&lt;/td&gt; 
   &lt;td align="center"&gt;47.9&lt;/td&gt; 
   &lt;td align="center"&gt;51.7&lt;/td&gt; 
   &lt;td align="center"&gt;32.4&lt;/td&gt; 
   &lt;td align="center"&gt;6.5&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;33.7&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;InternLM-7B&lt;/td&gt; 
   &lt;td align="center"&gt;51.0&lt;/td&gt; 
   &lt;td align="center"&gt;53.4&lt;/td&gt; 
   &lt;td align="center"&gt;31.2&lt;/td&gt; 
   &lt;td align="center"&gt;6.3&lt;/td&gt; 
   &lt;td align="center"&gt;10.4&lt;/td&gt; 
   &lt;td align="center"&gt;14.0&lt;/td&gt; 
   &lt;td align="center"&gt;37.0&lt;/td&gt; 
   &lt;td align="center"&gt;51.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;InternLM-20B&lt;/td&gt; 
   &lt;td align="center"&gt;62.1&lt;/td&gt; 
   &lt;td align="center"&gt;58.8&lt;/td&gt; 
   &lt;td align="center"&gt;52.6&lt;/td&gt; 
   &lt;td align="center"&gt;7.9&lt;/td&gt; 
   &lt;td align="center"&gt;25.6&lt;/td&gt; 
   &lt;td align="center"&gt;35.6&lt;/td&gt; 
   &lt;td align="center"&gt;52.5&lt;/td&gt; 
   &lt;td align="center"&gt;59.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Baichuan2-7B&lt;/td&gt; 
   &lt;td align="center"&gt;54.7&lt;/td&gt; 
   &lt;td align="center"&gt;56.3&lt;/td&gt; 
   &lt;td align="center"&gt;24.6&lt;/td&gt; 
   &lt;td align="center"&gt;5.6&lt;/td&gt; 
   &lt;td align="center"&gt;18.3&lt;/td&gt; 
   &lt;td align="center"&gt;24.2&lt;/td&gt; 
   &lt;td align="center"&gt;41.6&lt;/td&gt; 
   &lt;td align="center"&gt;57.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Baichuan2-13B&lt;/td&gt; 
   &lt;td align="center"&gt;59.5&lt;/td&gt; 
   &lt;td align="center"&gt;59.0&lt;/td&gt; 
   &lt;td align="center"&gt;52.8&lt;/td&gt; 
   &lt;td align="center"&gt;10.1&lt;/td&gt; 
   &lt;td align="center"&gt;17.1&lt;/td&gt; 
   &lt;td align="center"&gt;30.2&lt;/td&gt; 
   &lt;td align="center"&gt;49.0&lt;/td&gt; 
   &lt;td align="center"&gt;62.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Yi-34B&lt;/td&gt; 
   &lt;td align="center"&gt;76.3&lt;/td&gt; 
   &lt;td align="center"&gt;81.8&lt;/td&gt; 
   &lt;td align="center"&gt;67.9&lt;/td&gt; 
   &lt;td align="center"&gt;15.9&lt;/td&gt; 
   &lt;td align="center"&gt;26.2&lt;/td&gt; 
   &lt;td align="center"&gt;38.2&lt;/td&gt; 
   &lt;td align="center"&gt;66.4&lt;/td&gt; 
   &lt;td align="center"&gt;82.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;XVERSE-65B&lt;/td&gt; 
   &lt;td align="center"&gt;70.8&lt;/td&gt; 
   &lt;td align="center"&gt;68.6&lt;/td&gt; 
   &lt;td align="center"&gt;60.3&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;26.3&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;Qwen-1.8B&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;45.3&lt;/td&gt; 
   &lt;td align="center"&gt;56.1&lt;/td&gt; 
   &lt;td align="center"&gt;32.3&lt;/td&gt; 
   &lt;td align="center"&gt;2.3&lt;/td&gt; 
   &lt;td align="center"&gt;15.2&lt;/td&gt; 
   &lt;td align="center"&gt;14.2&lt;/td&gt; 
   &lt;td align="center"&gt;22.3&lt;/td&gt; 
   &lt;td align="center"&gt;52.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;Qwen-7B&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;58.2&lt;/td&gt; 
   &lt;td align="center"&gt;63.5&lt;/td&gt; 
   &lt;td align="center"&gt;51.7&lt;/td&gt; 
   &lt;td align="center"&gt;11.6&lt;/td&gt; 
   &lt;td align="center"&gt;29.9&lt;/td&gt; 
   &lt;td align="center"&gt;31.6&lt;/td&gt; 
   &lt;td align="center"&gt;45.0&lt;/td&gt; 
   &lt;td align="center"&gt;62.2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;Qwen-14B&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;66.3&lt;/td&gt; 
   &lt;td align="center"&gt;72.1&lt;/td&gt; 
   &lt;td align="center"&gt;61.3&lt;/td&gt; 
   &lt;td align="center"&gt;24.8&lt;/td&gt; 
   &lt;td align="center"&gt;32.3&lt;/td&gt; 
   &lt;td align="center"&gt;40.8&lt;/td&gt; 
   &lt;td align="center"&gt;53.4&lt;/td&gt; 
   &lt;td align="center"&gt;71.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;Qwen-72B&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;77.4&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;83.3&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;78.9&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;35.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;35.4&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;52.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;67.7&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;83.6&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;For all compared models, we report the best scores between their official reported results and &lt;a href="https://opencompass.org.cn/leaderboard-llm"&gt;OpenCompass&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For more experimental results (detailed model performance on more benchmark datasets) and details, please refer to our technical report by clicking &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/QWEN_TECHNICAL_REPORT.pdf"&gt;here&lt;/a&gt;. &lt;br&gt;&lt;br&gt;&lt;/p&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;python 3.8 and above&lt;/li&gt; 
 &lt;li&gt;pytorch 1.12 and above, 2.0 and above are recommended&lt;/li&gt; 
 &lt;li&gt;transformers 4.32 and above&lt;/li&gt; 
 &lt;li&gt;CUDA 11.4 and above are recommended (this is for GPU users, flash-attention users, etc.)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;Below, we provide simple examples to show how to use Qwen-Chat with 🤖 ModelScope and 🤗 Transformers.&lt;/p&gt; 
&lt;p&gt;You can use our pre-built docker images to skip most of the environment setup steps, see Section &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen/main/#-docker"&gt;"Using Pre-built Docker Images"&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;If not using docker, please make sure you have setup the environment and installed the required packages. Make sure you meet the above requirements, and then install the dependent libraries.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If your device supports fp16 or bf16, we recommend installing &lt;a href="https://github.com/Dao-AILab/flash-attention"&gt;flash-attention&lt;/a&gt; (&lt;strong&gt;we support flash attention 2 now.&lt;/strong&gt;) for higher efficiency and lower memory usage. (&lt;strong&gt;flash-attention is optional and the project can run normally without installing it&lt;/strong&gt;)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Dao-AILab/flash-attention
cd flash-attention &amp;amp;&amp;amp; pip install .
# Below are optional. Installing them might be slow.
# pip install csrc/layer_norm
# If the version of flash-attn is higher than 2.1.1, the following is not needed.
# pip install csrc/rotary
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now you can start with ModelScope or Transformers.&lt;/p&gt; 
&lt;h3&gt;🤗 Transformers&lt;/h3&gt; 
&lt;p&gt;To use Qwen-Chat for the inference, all you need to do is to input a few lines of codes as demonstrated below. Remember to pass in the correct model names or paths, such as "Qwen/Qwen-7B-Chat" and "Qwen/Qwen-14B-Chat". However, &lt;strong&gt;please make sure that you are using the latest code.&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

# Model names: "Qwen/Qwen-7B-Chat", "Qwen/Qwen-14B-Chat"
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)

# use bf16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()
# use fp16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()
# use cpu only
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="cpu", trust_remote_code=True).eval()
# use auto mode, automatically select precision based on the device.
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-7B-Chat",
    device_map="auto",
    trust_remote_code=True
).eval()

# Specify hyperparameters for generation. But if you use transformers&amp;gt;=4.32.0, there is no need to do this.
# model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)

# 1st dialogue turn
response, history = model.chat(tokenizer, "你好", history=None)
print(response)
# 你好！很高兴为你提供帮助。

# 2nd dialogue turn
response, history = model.chat(tokenizer, "给我讲一个年轻人奋斗创业最终取得成功的故事。", history=history)
print(response)
# 这是一个关于一个年轻人奋斗创业最终取得成功的故事。
# 故事的主人公叫李明，他来自一个普通的家庭，父母都是普通的工人。从小，李明就立下了一个目标：要成为一名成功的企业家。
# 为了实现这个目标，李明勤奋学习，考上了大学。在大学期间，他积极参加各种创业比赛，获得了不少奖项。他还利用课余时间去实习，积累了宝贵的经验。
# 毕业后，李明决定开始自己的创业之路。他开始寻找投资机会，但多次都被拒绝了。然而，他并没有放弃。他继续努力，不断改进自己的创业计划，并寻找新的投资机会。
# 最终，李明成功地获得了一笔投资，开始了自己的创业之路。他成立了一家科技公司，专注于开发新型软件。在他的领导下，公司迅速发展起来，成为了一家成功的科技企业。
# 李明的成功并不是偶然的。他勤奋、坚韧、勇于冒险，不断学习和改进自己。他的成功也证明了，只要努力奋斗，任何人都有可能取得成功。

# 3rd dialogue turn
response, history = model.chat(tokenizer, "给这个故事起一个标题", history=history)
print(response)
# 《奋斗创业：一个年轻人的成功之路》
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Running Qwen, the base language model, is also simple.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Running Qwen&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

# Model names: "Qwen/Qwen-7B", "Qwen/Qwen-14B" 
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B", trust_remote_code=True)
# use bf16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="auto", trust_remote_code=True, bf16=True).eval()
# use fp16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="auto", trust_remote_code=True, fp16=True).eval()
# use cpu only
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="cpu", trust_remote_code=True).eval()
# use auto mode, automatically select precision based on the device.
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-7B",
    device_map="auto",
    trust_remote_code=True
).eval()

# Specify hyperparameters for generation. But if you use transformers&amp;gt;=4.32.0, there is no need to do this.
# model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B", trust_remote_code=True)

inputs = tokenizer('蒙古国的首都是乌兰巴托（Ulaanbaatar）\n冰岛的首都是雷克雅未克（Reykjavik）\n埃塞俄比亚的首都是', return_tensors='pt')
inputs = inputs.to(model.device)
pred = model.generate(**inputs)
print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))
# 蒙古国的首都是乌兰巴托（Ulaanbaatar）\n冰岛的首都是雷克雅未克（Reykjavik）\n埃塞俄比亚的首都是亚的斯亚贝巴（Addis Ababa）...
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p id="DownloadModel"&gt; In the event of a network issue while attempting to download model checkpoints and codes from HuggingFace, an alternative approach is to initially fetch the checkpoint from ModelScope and then load it from the local directory as outlined below: &lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from modelscope import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer

# Downloading model checkpoint to a local dir model_dir
# model_dir = snapshot_download('qwen/Qwen-7B')
# model_dir = snapshot_download('qwen/Qwen-7B-Chat')
# model_dir = snapshot_download('qwen/Qwen-14B')
model_dir = snapshot_download('qwen/Qwen-14B-Chat')

# Loading local checkpoints
# trust_remote_code is still set as True since we still load codes from local dir instead of transformers
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_dir,
    device_map="auto",
    trust_remote_code=True
).eval()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;🤖 ModelScope&lt;/h3&gt; 
&lt;p&gt;ModelScope is an open-source platform for Model-as-a-Service (MaaS), which provides flexible and cost-effective model service to AI developers. Similarly, you can run the models with ModelScope as shown below:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from modelscope import AutoModelForCausalLM, AutoTokenizer
from modelscope import GenerationConfig

# Model names: "qwen/Qwen-7B-Chat", "qwen/Qwen-14B-Chat"
tokenizer = AutoTokenizer.from_pretrained("qwen/Qwen-7B-Chat", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()
model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True) # 可指定不同的生成长度、top_p等相关超参

response, history = model.chat(tokenizer, "你好", history=None)
print(response)
response, history = model.chat(tokenizer, "浙江的省会在哪里？", history=history) 
print(response)
response, history = model.chat(tokenizer, "它有什么好玩的景点", history=history)
print(response)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Batch Inference&lt;/h3&gt; 
&lt;p&gt;Qwen supports batch inference. With flash attention enabled, using batch inference can bring a 40% speedup. The example code is shown below:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import GenerationConfig
from qwen_generation_utils import make_context, decode_tokens, get_stop_words_ids

# To generate attention masks automatically, it is necessary to assign distinct
# token_ids to pad_token and eos_token, and set pad_token_id in the generation_config.
tokenizer = AutoTokenizer.from_pretrained(
    './',
    pad_token='&amp;lt;|extra_0|&amp;gt;',
    eos_token='&amp;lt;|endoftext|&amp;gt;',
    padding_side='left',
    trust_remote_code=True
)
model = AutoModelForCausalLM.from_pretrained(
    './',
    pad_token_id=tokenizer.pad_token_id,
    device_map="auto",
    trust_remote_code=True
).eval()
model.generation_config = GenerationConfig.from_pretrained('./', pad_token_id=tokenizer.pad_token_id)

all_raw_text = ["我想听你说爱我。", "今天我想吃点啥，甜甜的，推荐下", "我马上迟到了，怎么做才能不迟到"]
batch_raw_text = []
for q in all_raw_text:
    raw_text, _ = make_context(
        tokenizer,
        q,
        system="You are a helpful assistant.",
        max_window_size=model.generation_config.max_window_size,
        chat_format=model.generation_config.chat_format,
    )
    batch_raw_text.append(raw_text)

batch_input_ids = tokenizer(batch_raw_text, padding='longest')
batch_input_ids = torch.LongTensor(batch_input_ids['input_ids']).to(model.device)
batch_out_ids = model.generate(
    batch_input_ids,
    return_dict_in_generate=False,
    generation_config=model.generation_config
)
padding_lens = [batch_input_ids[i].eq(tokenizer.pad_token_id).sum().item() for i in range(batch_input_ids.size(0))]

batch_response = [
    decode_tokens(
        batch_out_ids[i][padding_lens[i]:],
        tokenizer,
        raw_text_len=len(batch_raw_text[i]),
        context_length=(batch_input_ids[i].size(0)-padding_lens[i]),
        chat_format="chatml",
        verbose=False,
        errors='replace'
    ) for i in range(len(all_raw_text))
]
print(batch_response)

response, _ = model.chat(tokenizer, "我想听你说爱我。", history=None)
print(response)

response, _ = model.chat(tokenizer, "今天我想吃点啥，甜甜的，推荐下", history=None)
print(response)

response, _ = model.chat(tokenizer, "我马上迟到了，怎么做才能不迟到", history=None)
print(response)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;CPU&lt;/h3&gt; 
&lt;p&gt;To deploy our models on CPU, we strongly advise you to use &lt;a href="https://github.com/QwenLM/qwen.cpp"&gt;qwen.cpp&lt;/a&gt;, which is a pure C++ implementation of Qwen and tiktoken. Check the repo for more details!&lt;/p&gt; 
&lt;p&gt;Also, it is also simple to directly run the model on CPU, which requires your specification of device:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="cpu", trust_remote_code=True).eval()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;However, it is likely that you suffer from extremely low inference efficiency.&lt;/p&gt; 
&lt;h3&gt;Multiple GPUs&lt;/h3&gt; 
&lt;p&gt;If you suffer from lack of GPU memory and you would like to run the model on more than 1 GPU, you can directly use the default loading method, which is now supported by Transformers. The previous method based on &lt;code&gt;utils.py&lt;/code&gt; is deprecated.&lt;/p&gt; 
&lt;p&gt;However, though this method is simple, the efficiency of the native pipeline parallelism is low. We advise you to use vLLM with FastChat and please read the section for deployment.&lt;/p&gt; 
&lt;h3&gt;x86 Platforms&lt;/h3&gt; 
&lt;p&gt;When deploy on Core™/Xeon® Scalable Processors or with Arc™ GPU, &lt;a href="https://docs.openvino.ai/2023.3/gen_ai_guide.html"&gt;OpenVINO™ Toolkit&lt;/a&gt; is recommended. You can install and run this &lt;a href="https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/254-llm-chatbot"&gt;example notebook&lt;/a&gt;. For related issues, you are welcome to file an issue at &lt;a href="https://github.com/openvinotoolkit/openvino_notebooks/issues"&gt;OpenVINO repo&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;DashScope&lt;/h3&gt; 
&lt;p&gt;The most simple way to use Qwen through APIs is DashScope API service through Alibaba Cloud. We give an introduction to the usage. Additionally, we provide a script for you to deploy an OpenAI-style API on your own servers.&lt;/p&gt; 
&lt;p&gt;DashScope is the large language model API service provided by Alibaba Cloud, which now supports Qwen. Note that the models behind DashScope are in-house versions temporarily without details provided. The services include &lt;code&gt;qwen-turbo&lt;/code&gt; and &lt;code&gt;qwen-plus&lt;/code&gt;, where the former one runs faster and the latter achieves better performance. For more information, visit the documentation &lt;a href="https://dashscope.aliyun.com"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Please head to the official website &lt;a href="https://help.aliyun.com/zh/dashscope/developer-reference/activate-dashscope-and-create-an-api-key?spm=a2c4g.11186623.0.0.6c2774fahtfXdn"&gt;link&lt;/a&gt; to create a DashScope account and obtain the API key (AK). We recommend setting the AK with an environment variable:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export DASHSCOPE_API_KEY="YOUR_DASHSCOPE_API_KEY"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then please install the packages and click &lt;a href="https://help.aliyun.com/zh/dashscope/developer-reference/install-dashscope-sdk"&gt;here&lt;/a&gt; for the documentation. If you use Python, you can install DashScope with pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install dashscope
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you use JAVA SDK, you can install it in this way:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-xml"&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.alibaba/dashscope-sdk-java --&amp;gt;
&amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;com.alibaba&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;dashscope-sdk-java&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;the-latest-version&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The simplest way to use DashScope is the usage with messages, which is similar to OpenAI API. The example is demonstrated below:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import random
from http import HTTPStatus
from dashscope import Generation


def call_with_messages():
    messages = [{'role': 'system', 'content': 'You are a helpful assistant.'},
                {'role': 'user', 'content': '如何做西红柿鸡蛋？'}]
    gen = Generation()
    response = gen.call(
        Generation.Models.qwen_turbo,
        messages=messages,
        seed=random.randint(1, 10000),  # set the random seed, optional, default to 1234 if not set
        result_format='message',  # set the result to be "message" format.
    )
    return response


if __name__ == '__main__':
    response = call_with_messages()
    if response.status_code == HTTPStatus.OK:
        print(response)
    else:
        print('Request id: %s, Status code: %s, error code: %s, error message: %s' % (
            response.request_id, response.status_code,
            response.code, response.message
        ))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more usages, please visit the official website for more details. &lt;br&gt;&lt;br&gt;&lt;/p&gt; 
&lt;h2&gt;Quantization&lt;/h2&gt; 
&lt;h3&gt;GPTQ&lt;/h3&gt; 
&lt;p&gt;We provide a solution based on &lt;a href="https://github.com/PanQiWei/AutoGPTQ"&gt;AutoGPTQ&lt;/a&gt;, and release the Int4 and Int8 quantized models, which achieve nearly lossless model effects but improved performance on both memory costs and inference speed.&lt;/p&gt; 
&lt;p&gt;Here we demonstrate how to use our provided quantized models for inference. Before you start, make sure you meet the requirements of auto-gptq (e.g., torch 2.0 and above, transformers 4.32.0 and above, etc.) and install the required packages:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install auto-gptq optimum
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you meet problems installing &lt;code&gt;auto-gptq&lt;/code&gt;, we advise you to check out the official &lt;a href="https://github.com/PanQiWei/AutoGPTQ"&gt;repo&lt;/a&gt; to find a wheel.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: The pre-compiled &lt;code&gt;auto-gptq&lt;/code&gt; packages strongly depend on the version of &lt;code&gt;torch&lt;/code&gt; and its CUDA version. Moreover, due to recent update, you may also encounter unsupported version errors from &lt;code&gt;transformers&lt;/code&gt;, &lt;code&gt;optimum&lt;/code&gt;, or &lt;code&gt;peft&lt;/code&gt;. We recommend using the latest versions meeting the following requirements:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;torch==2.1 auto-gptq&amp;gt;=0.5.1 transformers&amp;gt;=4.35.0 optimum&amp;gt;=1.14.0 peft&amp;gt;=0.6.1&lt;/li&gt; 
  &lt;li&gt;torch&amp;gt;=2.0,&amp;lt;2.1 auto-gptq&amp;lt;0.5.0 transformers&amp;lt;4.35.0 optimum&amp;lt;1.14.0 peft&amp;gt;=0.5.0,&amp;lt;0.6.0&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Then you can load the quantized model easily and run inference as same as usual:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Model names: "Qwen/Qwen-7B-Chat-Int4", "Qwen/Qwen-14B-Chat-Int4"
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-7B-Chat-Int4",
    device_map="auto",
    trust_remote_code=True
).eval()
response, history = model.chat(tokenizer, "Hi", history=None)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We illustrate the model performance of both BF16, Int8 and Int4 models on the benchmark, and we find that the quantized model does not suffer from significant performance degradation. Results are shown below:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Quantization&lt;/th&gt; 
   &lt;th align="center"&gt;MMLU&lt;/th&gt; 
   &lt;th align="center"&gt;CEval (val)&lt;/th&gt; 
   &lt;th align="center"&gt;GSM8K&lt;/th&gt; 
   &lt;th align="center"&gt;Humaneval&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-1.8B-Chat (BF16)&lt;/td&gt; 
   &lt;td align="center"&gt;43.3&lt;/td&gt; 
   &lt;td align="center"&gt;55.6&lt;/td&gt; 
   &lt;td align="center"&gt;33.7&lt;/td&gt; 
   &lt;td align="center"&gt;26.2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-1.8B-Chat (Int8)&lt;/td&gt; 
   &lt;td align="center"&gt;43.1&lt;/td&gt; 
   &lt;td align="center"&gt;55.8&lt;/td&gt; 
   &lt;td align="center"&gt;33.0&lt;/td&gt; 
   &lt;td align="center"&gt;27.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-1.8B-Chat (Int4)&lt;/td&gt; 
   &lt;td align="center"&gt;42.9&lt;/td&gt; 
   &lt;td align="center"&gt;52.8&lt;/td&gt; 
   &lt;td align="center"&gt;31.2&lt;/td&gt; 
   &lt;td align="center"&gt;25.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-7B-Chat (BF16)&lt;/td&gt; 
   &lt;td align="center"&gt;55.8&lt;/td&gt; 
   &lt;td align="center"&gt;59.7&lt;/td&gt; 
   &lt;td align="center"&gt;50.3&lt;/td&gt; 
   &lt;td align="center"&gt;37.2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-7B-Chat (Int8)&lt;/td&gt; 
   &lt;td align="center"&gt;55.4&lt;/td&gt; 
   &lt;td align="center"&gt;59.4&lt;/td&gt; 
   &lt;td align="center"&gt;48.3&lt;/td&gt; 
   &lt;td align="center"&gt;34.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-7B-Chat (Int4)&lt;/td&gt; 
   &lt;td align="center"&gt;55.1&lt;/td&gt; 
   &lt;td align="center"&gt;59.2&lt;/td&gt; 
   &lt;td align="center"&gt;49.7&lt;/td&gt; 
   &lt;td align="center"&gt;29.9&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-14B-Chat (BF16)&lt;/td&gt; 
   &lt;td align="center"&gt;64.6&lt;/td&gt; 
   &lt;td align="center"&gt;69.8&lt;/td&gt; 
   &lt;td align="center"&gt;60.1&lt;/td&gt; 
   &lt;td align="center"&gt;43.9&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-14B-Chat (Int8)&lt;/td&gt; 
   &lt;td align="center"&gt;63.6&lt;/td&gt; 
   &lt;td align="center"&gt;68.6&lt;/td&gt; 
   &lt;td align="center"&gt;60.0&lt;/td&gt; 
   &lt;td align="center"&gt;48.2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-14B-Chat (Int4)&lt;/td&gt; 
   &lt;td align="center"&gt;63.3&lt;/td&gt; 
   &lt;td align="center"&gt;69.0&lt;/td&gt; 
   &lt;td align="center"&gt;59.8&lt;/td&gt; 
   &lt;td align="center"&gt;45.7&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-72B-Chat (BF16)&lt;/td&gt; 
   &lt;td align="center"&gt;74.4&lt;/td&gt; 
   &lt;td align="center"&gt;80.1&lt;/td&gt; 
   &lt;td align="center"&gt;76.4&lt;/td&gt; 
   &lt;td align="center"&gt;64.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-72B-Chat (Int8)&lt;/td&gt; 
   &lt;td align="center"&gt;73.5&lt;/td&gt; 
   &lt;td align="center"&gt;80.1&lt;/td&gt; 
   &lt;td align="center"&gt;73.5&lt;/td&gt; 
   &lt;td align="center"&gt;62.2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-72B-Chat (Int4)&lt;/td&gt; 
   &lt;td align="center"&gt;73.4&lt;/td&gt; 
   &lt;td align="center"&gt;80.1&lt;/td&gt; 
   &lt;td align="center"&gt;75.3&lt;/td&gt; 
   &lt;td align="center"&gt;61.6&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Quantization of KV cache&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;NOTE: Please be aware that due to the internal mechanism of Hugging Face, the support files for this functionality (i.e., &lt;code&gt;cache_autogptq_cuda_256.cpp&lt;/code&gt; and &lt;code&gt;cache_autogptq_cuda_kernel_256.cu&lt;/code&gt;) may be missing. Please manually download them from the Hugging Face Hub and place them into the same folder as the other module files.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;The attention KV cache can be quantized and compressed for storage, to get a higher sample throughput. The arguments &lt;code&gt;use_cache_quantization&lt;/code&gt; and &lt;code&gt;use_cache_kernel&lt;/code&gt; in &lt;code&gt;config.json&lt;/code&gt; are provided to enable KV cache quantization. The specific use method is as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-7B-Chat",
     device_map="auto",
     trust_remote_code=True,
     use_cache_quantization=True,
     use_cache_kernel=True,
     use_flash_attn=False
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Attention: Currently, KV cache quantization and flash attention cannot be used at the same time. If you enable KV cache quantization and flash attention at the same time (&lt;code&gt;use_flash_attn=True&lt;/code&gt;, &lt;code&gt;use_cache_quantization=True&lt;/code&gt;, &lt;code&gt;use_cache_kernel=True&lt;/code&gt;), &lt;code&gt;use_flash_attn&lt;/code&gt; is disabled by default (&lt;code&gt;use_flash_attn=false&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;We have verified that the use of the quantized Int8-KV-Cache model does not suffer from significant performance degradation in downstream evaluation. In the following, we focus on profiling its memory footprint in different conditions. The profiling runs on a single A100-SXM4-80G GPU with PyTorch 2.0.1 and CUDA 11.4. We use BF16 models to generate 1024 tokens by default, and "OOM" indicates out-of-memory error.&lt;/p&gt; 
&lt;p&gt;With KV cache quantization, the model can infer with a larger batch size (bs).&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;USE KV Cache&lt;/th&gt; 
   &lt;th align="center"&gt;bs=1&lt;/th&gt; 
   &lt;th align="center"&gt;bs=4&lt;/th&gt; 
   &lt;th align="center"&gt;bs=16&lt;/th&gt; 
   &lt;th align="center"&gt;bs=32&lt;/th&gt; 
   &lt;th align="center"&gt;bs=64&lt;/th&gt; 
   &lt;th align="center"&gt;bs=100&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td align="center"&gt;16.3GB&lt;/td&gt; 
   &lt;td align="center"&gt;24.1GB&lt;/td&gt; 
   &lt;td align="center"&gt;31.7GB&lt;/td&gt; 
   &lt;td align="center"&gt;48.7GB&lt;/td&gt; 
   &lt;td align="center"&gt;OOM&lt;/td&gt; 
   &lt;td align="center"&gt;OOM&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Yes&lt;/td&gt; 
   &lt;td align="center"&gt;15.5GB&lt;/td&gt; 
   &lt;td align="center"&gt;17.2GB&lt;/td&gt; 
   &lt;td align="center"&gt;22.3GB&lt;/td&gt; 
   &lt;td align="center"&gt;30.2GB&lt;/td&gt; 
   &lt;td align="center"&gt;48.2GB&lt;/td&gt; 
   &lt;td align="center"&gt;72.4GB&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;With KV cache quantization the model can save more memory when generating longer sequence (&lt;code&gt;sl&lt;/code&gt;, sequence length, referring to the number of tokens generated) at the stage of inference.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;USE KV Cache&lt;/th&gt; 
   &lt;th align="center"&gt;sl=512&lt;/th&gt; 
   &lt;th align="center"&gt;sl=1024&lt;/th&gt; 
   &lt;th align="center"&gt;sl=2048&lt;/th&gt; 
   &lt;th align="center"&gt;sl=4096&lt;/th&gt; 
   &lt;th align="center"&gt;sl=8192&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td align="center"&gt;15.2GB&lt;/td&gt; 
   &lt;td align="center"&gt;16.3GB&lt;/td&gt; 
   &lt;td align="center"&gt;17.6GB&lt;/td&gt; 
   &lt;td align="center"&gt;19.5GB&lt;/td&gt; 
   &lt;td align="center"&gt;23.2GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Yes&lt;/td&gt; 
   &lt;td align="center"&gt;15GB&lt;/td&gt; 
   &lt;td align="center"&gt;15.5GB&lt;/td&gt; 
   &lt;td align="center"&gt;15.8GB&lt;/td&gt; 
   &lt;td align="center"&gt;16.6GB&lt;/td&gt; 
   &lt;td align="center"&gt;17.6GB&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The model with KV cache quantization will convert the format of &lt;code&gt;layer_past&lt;/code&gt; from float to int8, and meanwhile the quantized &lt;code&gt;layer-past&lt;/code&gt; will also store the quantization parameters.&lt;/p&gt; 
&lt;p&gt;Specific steps are as follows:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Quantize key/value&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;    qv,scale,zero_point=quantize_cache_v(v)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Store into layer_past&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The following is the format of quantized &lt;code&gt;layer_past&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    layer_past=((q_key,key_scale,key_zero_point),
                (q_value,value_scale,value_zero_point))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The original format of &lt;code&gt;layer_past&lt;/code&gt; is shown below:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    layer_past=(key,value)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to use the attention KV which is quantized, you can use the dequantization operation to convert the Int8 key/value back to the float format as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    v=dequantize_cache_torch(qv,scale,zero_point)
&lt;/code&gt;&lt;/pre&gt; 
&lt;br&gt; 
&lt;h2&gt;Inference Performance&lt;/h2&gt; 
&lt;p&gt;This section provides the statistics of speed and memory of models in different precisions. The speed and memory profiling are conducted using &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile.py"&gt;this script&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We measured the average inference speed (tokens/s) and GPU memory usage of generating 2048 with the models in BF16, Int8, and Int4.&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;Model Size&lt;/td&gt; 
   &lt;td&gt;Quantization&lt;/td&gt; 
   &lt;td&gt;Speed (Tokens/s)&lt;/td&gt; 
   &lt;td&gt;GPU Memory Usage&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="3"&gt;1.8B&lt;/td&gt; 
   &lt;td&gt;BF16&lt;/td&gt; 
   &lt;td&gt;54.09&lt;/td&gt; 
   &lt;td&gt;4.23GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Int8&lt;/td&gt; 
   &lt;td&gt;55.56&lt;/td&gt; 
   &lt;td&gt;3.48GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Int4&lt;/td&gt; 
   &lt;td&gt;71.07&lt;/td&gt; 
   &lt;td&gt;2.91GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="3"&gt;7B&lt;/td&gt; 
   &lt;td&gt;BF16&lt;/td&gt; 
   &lt;td&gt;40.93&lt;/td&gt; 
   &lt;td&gt;16.99GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Int8&lt;/td&gt; 
   &lt;td&gt;37.47&lt;/td&gt; 
   &lt;td&gt;11.20GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Int4&lt;/td&gt; 
   &lt;td&gt;50.09&lt;/td&gt; 
   &lt;td&gt;8.21GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="3"&gt;14B&lt;/td&gt; 
   &lt;td&gt;BF16&lt;/td&gt; 
   &lt;td&gt;32.22&lt;/td&gt; 
   &lt;td&gt;30.15GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Int8&lt;/td&gt; 
   &lt;td&gt;29.28&lt;/td&gt; 
   &lt;td&gt;18.81GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Int4&lt;/td&gt; 
   &lt;td&gt;38.72&lt;/td&gt; 
   &lt;td&gt;13.01GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="3"&gt;72B&lt;/td&gt; 
   &lt;td&gt;BF16&lt;/td&gt; 
   &lt;td&gt;8.48&lt;/td&gt; 
   &lt;td&gt;144.69GB (2xA100)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Int8&lt;/td&gt; 
   &lt;td&gt;9.05&lt;/td&gt; 
   &lt;td&gt;81.27GB (2xA100)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Int4&lt;/td&gt; 
   &lt;td&gt;11.32&lt;/td&gt; 
   &lt;td&gt;48.86GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;72B + vLLM&lt;/td&gt; 
   &lt;td&gt;BF16&lt;/td&gt; 
   &lt;td&gt;17.60&lt;/td&gt; 
   &lt;td&gt;2xA100&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;p&gt;The profiling runs on a single A100-SXM4-80G GPU (except 2xA100 is mentioned) with PyTorch 2.0.1, CUDA 11.8, and Flash-Attention 2. (72B + vLLM uses PyTorch 2.1.0 and Cuda 11.8.) The inference speed is averaged over the encoded and generated tokens.&lt;/p&gt; 
&lt;p&gt;Note: The generation speed of the Int4/Int8 models mentioned above is provided by the autogptq library. The current speed of the model loaded using &lt;code&gt;AutoModelForCausalLM.from_pretrained&lt;/code&gt; will be approximately 20% slower. We have reported this issue to the HuggingFace team and will update it promptly if a solution is available.&lt;/p&gt; 
&lt;p&gt;We also measure the inference speed and GPU memory usage with different settings of context and generation lengths, Flash-Attention version. You can find the results in the according modelcards on Hugging Face or ModelScope.&lt;/p&gt; 
&lt;h2&gt;Finetuning&lt;/h2&gt; 
&lt;h3&gt;Usage&lt;/h3&gt; 
&lt;p&gt;Now we provide the official training script, &lt;code&gt;finetune.py&lt;/code&gt;, for users to finetune the pretrained model for downstream applications in a simple fashion. Additionally, we provide shell scripts to launch finetuning with no worries. This script supports the training with &lt;a href="https://github.com/microsoft/DeepSpeed"&gt;DeepSpeed&lt;/a&gt; and &lt;a href="https://engineering.fb.com/2021/07/15/open-source/fsdp/"&gt;FSDP&lt;/a&gt;. The shell scripts that we provide use DeepSpeed (Note: this may have conflicts with the latest version of pydantic and you should use make sure &lt;code&gt;pydantic&amp;lt;2.0&lt;/code&gt;) and Peft. You can install them by:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "peft&amp;lt;0.8.0" deepspeed
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To prepare your training data, you need to put all the samples into a list and save it to a json file. Each sample is a dictionary consisting of an id and a list for conversation. Below is a simple example list with 1 sample:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;[
  {
    "id": "identity_0",
    "conversations": [
      {
        "from": "user",
        "value": "你好"
      },
      {
        "from": "assistant",
        "value": "我是一个语言模型，我叫通义千问。"
      }
    ]
  }
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After data preparation, you can use the provided shell scripts to run finetuning. Remember to specify the path to the data file, &lt;code&gt;$DATA&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;The finetuning scripts allow you to perform:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Full-parameter finetuning&lt;/li&gt; 
 &lt;li&gt;LoRA&lt;/li&gt; 
 &lt;li&gt;Q-LoRA&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Full-parameter finetuning requires updating all parameters in the whole training process. To launch your training, run the following script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Distributed training. We do not provide single-GPU training script as the insufficient GPU memory will break down the training.
bash finetune/finetune_ds.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Remember to specify the correct model name or path, the data path, as well as the output directory in the shell scripts. Another thing to notice is that we use DeepSpeed ZeRO 3 in this script. If you want to make changes, just remove the argument &lt;code&gt;--deepspeed&lt;/code&gt; or make changes in the DeepSpeed configuration json file based on your requirements. Additionally, this script supports mixed-precision training, and thus you can use &lt;code&gt;--bf16 True&lt;/code&gt; or &lt;code&gt;--fp16 True&lt;/code&gt;. Remember to use DeepSpeed when you use fp16 due to mixed precision training. Empirically we advise you to use bf16 to make your training consistent with our pretraining and alignment if your machine supports bf16, and thus we use it by default.&lt;/p&gt; 
&lt;p&gt;Similarly, to run LoRA, use another script to run as shown below. Before you start, make sure that you have installed &lt;code&gt;peft&lt;/code&gt;. Also, you need to specify your paths to your model, data, and output. We advise you to use absolute path for your pretrained model. This is because LoRA only saves the adapter and the absolute path in the adapter configuration json file is used for finding out the pretrained model to load. Also, this script support both bf16 and fp16.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Single GPU training
bash finetune/finetune_lora_single_gpu.sh
# Distributed training
bash finetune/finetune_lora_ds.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In comparison with full-parameter finetuning, LoRA (&lt;a href="https://arxiv.org/abs/2106.09685"&gt;paper&lt;/a&gt;) only updates the parameters of adapter layers but keeps the original large language model layers frozen. This allows much fewer memory costs and thus fewer computation costs.&lt;/p&gt; 
&lt;p&gt;Note that if you use LoRA to finetune the base language model, e.g., Qwen-7B, instead of chat models, e.g., Qwen-7B-Chat, the script automatically switches the embedding and output layer as trainable parameters. This is because the base language model has no knowledge of special tokens brought by ChatML format. Thus these layers should be updated for the model to understand and predict the tokens. Or in another word, if your training brings in special tokens in LoRA, you should set the layers to trainable parameters by setting &lt;code&gt;modules_to_save&lt;/code&gt; inside the code. Also, if we have these parameters trainable, it is not available to use ZeRO 3, and this is why we use ZeRO 2 in the script by default. If you do not have new trainable parameters, you can switch to ZeRO 3 by changing the DeepSpeed configuration file. Additionally, we find that there is a significant gap between the memory footprint of LoRA with and without these trainable parameters. Therefore, if you have trouble with memory, we advise you to LoRA finetune the chat models. Check the profile below for more information.&lt;/p&gt; 
&lt;p&gt;If you still suffer from insufficient memory, you can consider Q-LoRA (&lt;a href="https://arxiv.org/abs/2305.14314"&gt;paper&lt;/a&gt;), which uses the quantized large language model and other techniques such as paged attention to allow even fewer memory costs.&lt;/p&gt; 
&lt;p&gt;Note: to run single-GPU Q-LoRA training, you may need to install &lt;code&gt;mpi4py&lt;/code&gt; through &lt;code&gt;pip&lt;/code&gt; or &lt;code&gt;conda&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;To run Q-LoRA, directly run the following script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Single GPU training
bash finetune/finetune_qlora_single_gpu.sh
# Distributed training
bash finetune/finetune_qlora_ds.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Q-LoRA, we advise you to load our provided quantized model, e.g., Qwen-7B-Chat-Int4. You &lt;strong&gt;SHOULD NOT&lt;/strong&gt; use the bf16 models. Different from full-parameter finetuning and LoRA, only fp16 is supported for Q-LoRA. For single-GPU training, we have to use DeepSpeed for mixed-precision training due to our observation of errors caused by torch amp. Besides, for Q-LoRA, the troubles with the special tokens in LoRA still exist. However, as we only provide the Int4 models for chat models, which means the language model has learned the special tokens of ChatML format, you have no worry about the layers. Note that the layers of the Int4 model should not be trainable, and thus if you introduce special tokens in your training, Q-LoRA might not work.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;NOTE: Please be aware that due to the internal mechanisms of Hugging Face, certain non-Python files (e.g., &lt;code&gt;*.cpp&lt;/code&gt; and &lt;code&gt;*.cu&lt;/code&gt;) may be missing from the saved checkpoint. You may need to manually copy them to the directory containing other files.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Different from full-parameter finetuning, the training of both LoRA and Q-LoRA only saves the adapter parameters. Suppose your training starts from Qwen-7B, you can load the finetuned model for inference as shown below:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from peft import AutoPeftModelForCausalLM

model = AutoPeftModelForCausalLM.from_pretrained(
    path_to_adapter, # path to the output directory
    device_map="auto",
    trust_remote_code=True
).eval()
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;NOTE: If &lt;code&gt;peft&amp;gt;=0.8.0&lt;/code&gt;, it will try to load the tokenizer as well, however, initialized without &lt;code&gt;trust_remote_code=True&lt;/code&gt;, leading to &lt;code&gt;ValueError: Tokenizer class QWenTokenizer does not exist or is not currently imported.&lt;/code&gt; Currently, you could downgrade &lt;code&gt;peft&amp;lt;0.8.0&lt;/code&gt; or move tokenizer files elsewhere to workaround this issue.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;If you want to merge the adapters and save the finetuned model as a standalone model (you can only do this with LoRA, and you CANNOT merge the parameters from Q-LoRA), you can run the following codes:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from peft import AutoPeftModelForCausalLM

model = AutoPeftModelForCausalLM.from_pretrained(
    path_to_adapter, # path to the output directory
    device_map="auto",
    trust_remote_code=True
).eval()

merged_model = model.merge_and_unload()
# max_shard_size and safe serialization are not necessary. 
# They respectively work for sharding checkpoint and save the model to safetensors
merged_model.save_pretrained(new_model_directory, max_shard_size="2048MB", safe_serialization=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;new_model_directory&lt;/code&gt; directory will contain the merged model weights and module files. Please note that &lt;code&gt;*.cu&lt;/code&gt; and &lt;code&gt;*.cpp&lt;/code&gt; files may be missing in the saved files. If you wish to use the KV cache functionality, please manually copy them. Besides, the tokenizer files are not saved in the new directory in this step. You can copy the tokenizer files or use the following code&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
    path_to_adapter, # path to the output directory
    trust_remote_code=True
)

tokenizer.save_pretrained(new_model_directory)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: For multi-GPU training, you need to specify the proper hyperparameters for distributed training based on your machine. Besides, we advise you to specify your maximum sequence length with the argument &lt;code&gt;--model_max_length&lt;/code&gt;, based on your consideration of data, memory footprint, and training speed.&lt;/p&gt; 
&lt;h3&gt;Quantize Fine-tuned Models&lt;/h3&gt; 
&lt;p&gt;This section applies to full-parameter/LoRA fine-tuned models. (Note: You do not need to quantize the Q-LoRA fine-tuned model because it is already quantized.) If you use LoRA, please follow the above instructions to merge your model before quantization.&lt;/p&gt; 
&lt;p&gt;We recommend using &lt;a href="https://github.com/PanQiWei/AutoGPTQ"&gt;auto_gptq&lt;/a&gt; to quantize the finetuned model.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install auto-gptq optimum
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: Currently AutoGPTQ has a bug referred in &lt;a href="https://github.com/PanQiWei/AutoGPTQ/issues/370"&gt;this issue&lt;/a&gt;. Here is a &lt;a href="https://github.com/PanQiWei/AutoGPTQ/pull/495"&gt;workaround PR&lt;/a&gt;, and you can pull this branch and install from the source.&lt;/p&gt; 
&lt;p&gt;First, prepare the calibration data. You can reuse the fine-tuning data, or use other data following the same format.&lt;/p&gt; 
&lt;p&gt;Second, run the following script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python run_gptq.py \
    --model_name_or_path $YOUR_LORA_MODEL_PATH \
    --data_path $DATA \
    --out_path $OUTPUT_PATH \
    --bits 4 # 4 for int4; 8 for int8
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This step requires GPUs and may costs a few hours according to your data size and model size.&lt;/p&gt; 
&lt;p&gt;Then, copy all &lt;code&gt;*.py&lt;/code&gt;, &lt;code&gt;*.cu&lt;/code&gt;, &lt;code&gt;*.cpp&lt;/code&gt; files and &lt;code&gt;generation_config.json&lt;/code&gt; to the output path. And we recommend you to overwrite &lt;code&gt;config.json&lt;/code&gt; by copying the file from the coresponding official quantized model (for example, if you are fine-tuning &lt;code&gt;Qwen-7B-Chat&lt;/code&gt; and use &lt;code&gt;--bits 4&lt;/code&gt;, you can find the &lt;code&gt;config.json&lt;/code&gt; from &lt;a href="https://huggingface.co/Qwen/Qwen-7B-Chat-Int4/blob/main/config.json"&gt;Qwen-7B-Chat-Int4&lt;/a&gt;). You should also rename the &lt;code&gt;gptq.safetensors&lt;/code&gt; into &lt;code&gt;model.safetensors&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Finally, test the model by the same method to load the official quantized model. For example,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

tokenizer = AutoTokenizer.from_pretrained("/path/to/your/model", trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(
    "/path/to/your/model",
    device_map="auto",
    trust_remote_code=True
).eval()

response, history = model.chat(tokenizer, "你好", history=None)
print(response)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Multinode Finetuning&lt;/h3&gt; 
&lt;p&gt;Our provided scripts support multinode finetuning. You can refer to the comments in &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen/main/finetune/finetune_lora_ds.sh"&gt;script&lt;/a&gt; to correctly set corresponding arguments and launch the script on each node. For more information about multinode distributed training, please refer to &lt;a href="https://pytorch.org/docs/stable/elastic/run.html"&gt;torchrun&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Note: DeepSpeed ZeRO 3 requires much greater inter-node communication rate than ZeRO 2, which will significantly reduce the training speed in the case of multinode finetuning. Therefore, we do not recommend using DeepSpeed ZeRO 3 configurations in multinode finetuning scripts.&lt;/p&gt; 
&lt;h3&gt;Profiling of Memory and Speed&lt;/h3&gt; 
&lt;p&gt;We profile the GPU memory and training speed of both LoRA (LoRA (emb) refers to training the embedding and output layer, while LoRA has no trainable embedding and output layer) and Q-LoRA in the setup of single-GPU training. In this test, we experiment on a single A100-SXM4-80G GPU, and we use CUDA 11.8 and Pytorch 2.0. Flash attention 2 is applied. We uniformly use a batch size of 1 and gradient accumulation of 8. We profile the memory (GB) and speed (s/iter) of inputs of different lengths, namely 256, 512, 1024, 2048, 4096, and 8192. We also report the statistics of full-parameter finetuning with Qwen-7B on 2 A100 GPUs. We only report the statistics of 256, 512, and 1024 tokens due to the limitation of GPU memory.&lt;/p&gt; 
&lt;p&gt;For Qwen-7B, we also test the performance of multinode finetuning. We experiment using two servers, each containing two A100-SXM4-80G GPUs, and the rest of configurations are the same as other Qwen-7B experiments. The results of multinode finetuning are marked as LoRA (multinode) in the table.&lt;/p&gt; 
&lt;p&gt;For Qwen-72B, we experiment in two ways: 1) Lora fintuning + DeepSpeed ZeRO 3 on 4 A100-SXM4-80G GPUs and 2) QLora (int4) fine-tuning on a single A100-SXM4-80G GPU. Note that OOM occurs on 4 A100-SXM4-80G GPUs both with LoRA (emb) fine-tuning and LoRA fine-tuning without Deepspeed ZeRO 3 (you can pass &lt;code&gt;--deepspeed finetune/ds_config_zero3.json&lt;/code&gt; to &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen/main/finetune/finetune_lora_ds.sh"&gt;&lt;code&gt;finetune/finetune_lora_ds.sh&lt;/code&gt;&lt;/a&gt; to enable DeepSpeed ZeRO 3).&lt;/p&gt; 
&lt;p&gt;The statistics are listed below:&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th rowspan="2"&gt;Model Size&lt;/th&gt;
   &lt;th rowspan="2"&gt;Method&lt;/th&gt;
   &lt;th rowspan="2"&gt;#Nodes&lt;/th&gt;
   &lt;th rowspan="2"&gt;#GPUs per node&lt;/th&gt;
   &lt;th colspan="6" align="center"&gt;Sequence Length&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;256&lt;/th&gt;
   &lt;th align="center"&gt;512&lt;/th&gt;
   &lt;th align="center"&gt;1024&lt;/th&gt;
   &lt;th align="center"&gt;2048&lt;/th&gt;
   &lt;th align="center"&gt;4096&lt;/th&gt;
   &lt;th align="center"&gt;8192&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th rowspan="4"&gt;1.8B&lt;/th&gt;
   &lt;td&gt;LoRA&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt;
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td align="center"&gt;6.7G / 1.0s/it&lt;/td&gt;
   &lt;td align="center"&gt;7.4G / 1.0s/it&lt;/td&gt;
   &lt;td align="center"&gt;8.4G / 1.1s/it&lt;/td&gt;
   &lt;td align="center"&gt;11.0G / 1.7s/it&lt;/td&gt;
   &lt;td align="center"&gt;16.2G / 3.3s/it&lt;/td&gt;
   &lt;td align="center"&gt;21.8G / 6.8s/it&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LoRA (emb)&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt;
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td align="center"&gt;13.7G / 1.0s/it&lt;/td&gt;
   &lt;td align="center"&gt;14.0G / 1.0s/it&lt;/td&gt;
   &lt;td align="center"&gt;14.0G / 1.1s/it&lt;/td&gt;
   &lt;td align="center"&gt;15.1G / 1.8s/it&lt;/td&gt;
   &lt;td align="center"&gt;19.7G / 3.4s/it&lt;/td&gt;
   &lt;td align="center"&gt;27.7G / 7.0s/it&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Q-LoRA&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt;
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td align="center"&gt;5.8G / 1.4s/it&lt;/td&gt;
   &lt;td align="center"&gt;6.0G / 1.4s/it&lt;/td&gt;
   &lt;td align="center"&gt;6.6G / 1.4s/it&lt;/td&gt;
   &lt;td align="center"&gt;7.8G / 2.0s/it&lt;/td&gt;
   &lt;td align="center"&gt;10.2G / 3.4s/it&lt;/td&gt;
   &lt;td align="center"&gt;15.8G / 6.5s/it&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Full-parameter&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt;
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td align="center"&gt;43.5G / 2.1s/it&lt;/td&gt;
   &lt;td align="center"&gt;43.5G / 2.2s/it&lt;/td&gt;
   &lt;td align="center"&gt;43.5G / 2.2s/it&lt;/td&gt;
   &lt;td align="center"&gt;43.5G / 2.3s/it&lt;/td&gt;
   &lt;td align="center"&gt;47.1G / 2.8s/it&lt;/td&gt;
   &lt;td align="center"&gt;48.3G / 5.6s/it&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th rowspan="5"&gt;7B&lt;/th&gt; 
   &lt;td&gt;LoRA&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt;
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td align="center"&gt;20.1G / 1.2s/it&lt;/td&gt;
   &lt;td align="center"&gt;20.4G / 1.5s/it&lt;/td&gt;
   &lt;td align="center"&gt;21.5G / 2.8s/it&lt;/td&gt;
   &lt;td align="center"&gt;23.8G / 5.2s/it&lt;/td&gt;
   &lt;td align="center"&gt;29.7G / 10.1s/it&lt;/td&gt;
   &lt;td align="center"&gt;36.6G / 21.3s/it&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LoRA (emb)&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt;
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td align="center"&gt;33.7G / 1.4s/it&lt;/td&gt;
   &lt;td align="center"&gt;34.1G / 1.6s/it&lt;/td&gt;
   &lt;td align="center"&gt;35.2G / 2.9s/it&lt;/td&gt;
   &lt;td align="center"&gt;35.1G / 5.3s/it&lt;/td&gt;
   &lt;td align="center"&gt;39.2G / 10.3s/it&lt;/td&gt;
   &lt;td align="center"&gt;48.5G / 21.7s/it&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Q-LoRA&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt;
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td align="center"&gt;11.5G / 3.0s/it&lt;/td&gt;
   &lt;td align="center"&gt;11.5G / 3.0s/it&lt;/td&gt;
   &lt;td align="center"&gt;12.3G / 3.5s/it&lt;/td&gt;
   &lt;td align="center"&gt;13.9G / 7.0s/it&lt;/td&gt;
   &lt;td align="center"&gt;16.9G / 11.6s/it&lt;/td&gt;
   &lt;td align="center"&gt;23.5G / 22.3s/it&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Full-parameter&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt;
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td align="center"&gt;139.2G / 4.0s/it&lt;/td&gt;
   &lt;td align="center"&gt;148.0G / 4.0s/it&lt;/td&gt;
   &lt;td align="center"&gt;162.0G / 4.5s/it&lt;/td&gt;
   &lt;td align="center"&gt;-&lt;/td&gt;
   &lt;td align="center"&gt;-&lt;/td&gt;
   &lt;td align="center"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LoRA (multinode)&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt;
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td align="center"&gt;74.7G / 2.09s/it&lt;/td&gt;
   &lt;td align="center"&gt;77.6G / 3.16s/it&lt;/td&gt;
   &lt;td align="center"&gt;84.9G / 5.17s/it&lt;/td&gt;
   &lt;td align="center"&gt;95.1G / 9.25s/it&lt;/td&gt;
   &lt;td align="center"&gt;121.1G / 18.1s/it&lt;/td&gt;
   &lt;td align="center"&gt;155.5G / 37.4s/it&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th rowspan="3"&gt;14B&lt;/th&gt; 
   &lt;td&gt;LoRA&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt;
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td align="center"&gt;34.6G / 1.6s/it&lt;/td&gt;
   &lt;td align="center"&gt;35.1G / 2.4s/it&lt;/td&gt;
   &lt;td align="center"&gt;35.3G / 4.4s/it&lt;/td&gt;
   &lt;td align="center"&gt;37.4G / 8.4s/it&lt;/td&gt;
   &lt;td align="center"&gt;42.5G / 17.0s/it&lt;/td&gt;
   &lt;td align="center"&gt;55.2G / 36.0s/it&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LoRA (emb)&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt;
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td align="center"&gt;51.2 / 1.7s/it&lt;/td&gt;
   &lt;td align="center"&gt;51.1G / 2.6s/it&lt;/td&gt;
   &lt;td align="center"&gt;51.5G / 4.6s/it&lt;/td&gt;
   &lt;td align="center"&gt;54.1G / 8.6s/it&lt;/td&gt;
   &lt;td align="center"&gt;56.8G / 17.2s/it&lt;/td&gt;
   &lt;td align="center"&gt;67.7G / 36.3s/it&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Q-LoRA&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt;
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td align="center"&gt;18.7G / 5.3s/it&lt;/td&gt;
   &lt;td align="center"&gt;18.4G / 6.3s/it&lt;/td&gt;
   &lt;td align="center"&gt;18.9G / 8.2s/it&lt;/td&gt;
   &lt;td align="center"&gt;19.9G / 11.8s/it&lt;/td&gt;
   &lt;td align="center"&gt;23.0G / 20.1s/it&lt;/td&gt;
   &lt;td align="center"&gt;27.9G / 38.3s/it&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th rowspan="2"&gt;72B&lt;/th&gt; 
   &lt;td&gt;LoRA + Deepspeed Zero3&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt;
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td align="center"&gt;215.4G / 17.6s/it&lt;/td&gt;
   &lt;td align="center"&gt;217.7G / 20.5s/it&lt;/td&gt;
   &lt;td align="center"&gt;222.6G / 29.4s/it&lt;/td&gt;
   &lt;td align="center"&gt;228.8G / 45.7s/it&lt;/td&gt;
   &lt;td align="center"&gt;249.0G / 83.4s/it&lt;/td&gt;
   &lt;td align="center"&gt;289.2G / 161.5s/it&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Q-LoRA&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt;
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td align="center"&gt;61.4G / 27.4s/it&lt;/td&gt;
   &lt;td align="center"&gt;61.4G / 31.5s/it&lt;/td&gt;
   &lt;td align="center"&gt;62.9G / 41.4s/it&lt;/td&gt;
   &lt;td align="center"&gt;64.1G / 59.5s/it&lt;/td&gt;
   &lt;td align="center"&gt;68.0G / 97.7s/it&lt;/td&gt;
   &lt;td align="center"&gt;75.6G / 179.8s/it&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;br&gt; 
&lt;h2&gt;Deployment&lt;/h2&gt; 
&lt;h3&gt;vLLM&lt;/h3&gt; 
&lt;p&gt;For deployment and fast inference, we suggest using vLLM.&lt;/p&gt; 
&lt;p&gt;If you use &lt;strong&gt;CUDA 12.1 and PyTorch 2.1&lt;/strong&gt;, you can directly use the following command to install vLLM.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install vllm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Otherwise, please refer to the official vLLM &lt;a href="https://docs.vllm.ai/en/latest/getting_started/installation.html"&gt;Installation Instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;vLLM + Transformer-like Wrapper&lt;/h4&gt; 
&lt;p&gt;You can download the &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen/main/examples/vllm_wrapper.py"&gt;wrapper codes&lt;/a&gt; and execute the following commands for multiple rounds of dialogue interaction. (Note: It currently only supports the &lt;code&gt;model.chat()&lt;/code&gt; method.)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from vllm_wrapper import vLLMWrapper

model = vLLMWrapper('Qwen/Qwen-7B-Chat', tensor_parallel_size=1)
# model = vLLMWrapper('Qwen/Qwen-7B-Chat-Int4', tensor_parallel_size=1, dtype="float16")

response, history = model.chat(query="你好", history=None)
print(response)
response, history = model.chat(query="给我讲一个年轻人奋斗创业最终取得成功的故事。", history=history)
print(response)
response, history = model.chat(query="给这个故事起一个标题", history=history)
print(response)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;vLLM + Web Demo / OpenAI-like API&lt;/h4&gt; 
&lt;p&gt;You can use FastChat to lauch a web demo or an OpenAI API server. First, install FastChat:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "fschat[model_worker,webui]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run Qwen with vLLM and FastChat, you need launch a controller by:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m fastchat.serve.controller
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you can launch the model worker, which means loading your model for inference. For single GPU inference, you can directly run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --dtype bfloat16
# python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --dtype float16 # run int4 model
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;However, if you hope to run the model on multiple GPUs for faster inference or larger memory, you can use tensor parallelism supported by vLLM. Suppose you run the model on 4 GPUs, the command is shown below:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --tensor-parallel-size 4 --dtype bfloat16
# python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --tensor-parallel-size 4 --dtype float16 # run int4 model
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After launching your model worker, you can launch a:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Web UI Demo&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m fastchat.serve.gradio_web_server
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;OpenAI API&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m fastchat.serve.openai_api_server --host localhost --port 8000
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;However, if you find it difficult to use vLLM and FastChat, you can try our provided simplest methods to deploy a web demo, CLI demo, and API.&lt;/p&gt; 
&lt;h3&gt;Web UI&lt;/h3&gt; 
&lt;p&gt;We provide code for users to build a web UI demo (thanks to @wysaid). Before you start, make sure you install the following packages:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install -r requirements_web_demo.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then run the command below and click on the generated link:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python web_demo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p align="center"&gt; &lt;br&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen/main/assets/web_demo.gif" width="600"&gt; &lt;br&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;CLI Demo&lt;/h3&gt; 
&lt;p&gt;We provide a CLI demo example in &lt;code&gt;cli_demo.py&lt;/code&gt;, which supports streaming output for the generation. Users can interact with Qwen-7B-Chat by inputting prompts, and the model returns model outputs in the streaming mode. Run the command below:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python cli_demo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p align="center"&gt; &lt;br&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen/main/assets/cli_demo.gif" width="600"&gt; &lt;br&gt; &lt;/p&gt;
&lt;p&gt; &lt;br&gt; &lt;/p&gt;
&lt;h3&gt;API&lt;/h3&gt; 
&lt;p&gt;We provide methods to deploy local API based on OpenAI API (thanks to @hanpenggit). Before you start, install the required packages:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install fastapi uvicorn "openai&amp;lt;1.0" pydantic sse_starlette
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then run the command to deploy your API:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python openai_api.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can change your arguments, e.g., &lt;code&gt;-c&lt;/code&gt; for checkpoint name or path, &lt;code&gt;--cpu-only&lt;/code&gt; for CPU deployment, etc. If you meet problems launching your API deployment, updating the packages to the latest version can probably solve them.&lt;/p&gt; 
&lt;p&gt;Using the API is also simple. See the example below:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import openai
openai.api_base = "http://localhost:8000/v1"
openai.api_key = "none"

# create a request activating streaming response
for chunk in openai.ChatCompletion.create(
    model="Qwen",
    messages=[
        {"role": "user", "content": "你好"}
    ],
    stream=True 
    # Specifying stop words in streaming output format is not yet supported and is under development.
):
    if hasattr(chunk.choices[0].delta, "content"):
        print(chunk.choices[0].delta.content, end="", flush=True)

# create a request not activating streaming response
response = openai.ChatCompletion.create(
    model="Qwen",
    messages=[
        {"role": "user", "content": "你好"}
    ],
    stream=False,
    stop=[] # You can add custom stop words here, e.g., stop=["Observation:"] for ReAct prompting.
)
print(response.choices[0].message.content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p align="center"&gt; &lt;br&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen/main/assets/openai_api.gif" width="600"&gt; &lt;br&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Function calling&lt;/strong&gt; is also supported (but only when &lt;code&gt;stream=False&lt;/code&gt; for the moment). See the &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen/main/examples/function_call_examples.py"&gt;example usage&lt;/a&gt; here. &lt;br&gt;&lt;br&gt;&lt;/p&gt; 
&lt;h2&gt;🐳 Docker&lt;/h2&gt; 
&lt;p&gt;To simplify the deployment process, we provide docker images with pre-built environments: &lt;a href="https://hub.docker.com/r/qwenllm/qwen"&gt;qwenllm/qwen&lt;/a&gt;. You only need to install the driver and download model files to launch demos, deploy OpenAI API, and finetune the model.&lt;/p&gt; 
&lt;h3&gt;Preparation&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install the correct version of Nvidia driver depending on the image to use:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;qwenllm/qwen:cu117&lt;/code&gt; (&lt;strong&gt;recommend&lt;/strong&gt;): &lt;code&gt;&amp;gt;= 515.48.07&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;qwenllm/qwen:cu114&lt;/code&gt; (w/o flash-attention): &lt;code&gt;&amp;gt;= 470.82.01&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;qwenllm/qwen:cu121&lt;/code&gt;: &lt;code&gt;&amp;gt;= 530.30.02&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;qwenllm/qwen:latest&lt;/code&gt;: same as &lt;code&gt;qwenllm/qwen:cu117&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install and configure &lt;a href="https://docs.docker.com/engine/install/"&gt;docker&lt;/a&gt; and &lt;a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html"&gt;nvidia-container-toolkit&lt;/a&gt;:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# configure docker
sudo systemctl start docker
# test if docker is correctly installed
sudo docker run hello-world

# configure nvidia-container-toolkit
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker
# test if nvidia-container-toolkit is correctly installed
sudo docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Download model checkpoints and codes to your environment (see &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen/main/#DownloadModel"&gt;here&lt;/a&gt;).&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Deployment&lt;/h3&gt; 
&lt;p&gt;Here we use Qwen-7B-Chat as an example. Before launching a web demo or API, you can setup the configuration as shown below:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;IMAGE_NAME=qwenllm/qwen:cu117
PORT=8901
CHECKPOINT_PATH=/path/to/Qwen-7B-Chat   # Path to downloaded model checkpoints and codes
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The following scripts can help you build:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;OpenAI API&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;bash docker/docker_openai_api.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH} --port ${PORT}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Web UI&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;bash docker/docker_web_demo.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH} --port ${PORT}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;CLI Demo&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;bash docker/docker_cli_demo.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The commands above will automatically download the required image and launch a Web UI demo in background (the service will auto-restart). You can open &lt;code&gt;http://localhost:${PORT}&lt;/code&gt; on the host to use the demo.&lt;/p&gt; 
&lt;p&gt;The demo is successfully launched if you see the following output:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;Successfully started web demo. Open '...' to try!
Run `docker logs ...` to check demo status.
Run `docker rm -f ...` to stop and remove the demo.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to check the status of the demo, you can use &lt;code&gt;docker logs qwen&lt;/code&gt; to display outputs.&lt;/p&gt; 
&lt;p&gt;You can use &lt;code&gt;docker rm -f qwen&lt;/code&gt; to stop the service and remove the container.&lt;/p&gt; 
&lt;h3&gt;Finetuning&lt;/h3&gt; 
&lt;p&gt;The method of finetuning using the pre-built Docker image is basically the same as &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen/main/#Finetuning"&gt;the above chapter&lt;/a&gt; (we have already installed dependencies in the image):&lt;/p&gt; 
&lt;p&gt;The following is an example of single-GPU LoRA:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;IMAGE_NAME=qwenllm/qwen:cu117
CHECKPOINT_PATH=/path/to/Qwen-7B                # Path to downloaded model checkpoints and codes
#CHECKPOINT_PATH=/path/to/Qwen-7B-Chat-Int4     # Path to downloaded model checkpoints and codes (Q-LoRA)
DATA_PATH=/path/to/data/root                    # Prepare finetune data at ${DATA_PATH}/example.json
OUTPUT_PATH=/path/to/output/checkpoint          # Path to finetune outputs

# Use all host devices by default
DEVICE=all
# If you need to specify GPUs for training, set device as follow (NOTE: internal quotation marks cannot be omitted)
#DEVICE='"device=0,1,2,3"'

mkdir -p ${OUTPUT_PATH}

# Single-GPU LoRA finetuning
docker run --gpus ${DEVICE} --rm --name qwen \
    --mount type=bind,source=${CHECKPOINT_PATH},target=/data/shared/Qwen/Qwen-7B \
    --mount type=bind,source=${DATA_PATH},target=/data/shared/Qwen/data \
    --mount type=bind,source=${OUTPUT_PATH},target=/data/shared/Qwen/output_qwen \
    --shm-size=2gb \
    -it ${IMAGE_NAME} \
    bash finetune/finetune_lora_single_gpu.sh -m /data/shared/Qwen/Qwen-7B/ -d /data/shared/Qwen/data/example.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To make a change to single-GPU Q-LoRA for example, you just need to modify the bash command inside &lt;code&gt;docker run&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;bash finetune/finetune_qlora_single_gpu.sh -m /data/shared/Qwen/Qwen-7B-Chat-Int4/ -d /data/shared/Qwen/data/example.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;br&gt; 
&lt;h2&gt;🔥 System Prompt&lt;/h2&gt; 
&lt;p&gt;Qwen-1.8-Chat and Qwen-72B-Chat have been fully trained on diverse system prompts with multiple rounds of complex interactions, so that they can follow a variety of system prompts and realize model customization in context, further improving the scalability of Qwen-chat.&lt;/p&gt; 
&lt;p&gt;With System Prompt, Qwen-Chat can realize &lt;strong&gt;roly playing&lt;/strong&gt;, &lt;strong&gt;language style transfer&lt;/strong&gt;, &lt;strong&gt;task setting&lt;/strong&gt;, and &lt;strong&gt;behavior setting&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen/main/assets/system_prompt_language_style.png" alt=""&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen/main/assets/system_prompt_role_play_en.png" alt=""&gt;&lt;/p&gt; 
&lt;p&gt;For more information, please refer to the &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen/main/examples/system_prompt.md"&gt;example documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Tool Usage&lt;/h2&gt; 
&lt;p&gt;Qwen-Chat has been optimized for tool usage and function calling capabilities. Users can develop agents, LangChain applications, and even augment Qwen with a Python Code Interpreter.&lt;/p&gt; 
&lt;p&gt;We provide documentation on how to implement tool calls based on the principle of ReAct Prompting, please refer to &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen/main/examples/react_prompt.md"&gt;the ReAct example&lt;/a&gt;. Based on this principle, we provide support for function calling in &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen/main/openai_api.py"&gt;openai_api.py&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We have tested the model's tool calling capabilities on our open-source Chinese evaluation benchmark and found that Qwen-Chat consistently performs well:&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th colspan="4" align="center"&gt;Chinese Tool-Use Benchmark (Version 20231206)&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Model&lt;/th&gt;
   &lt;th align="center"&gt;Tool Selection (Acc.↑)&lt;/th&gt;
   &lt;th align="center"&gt;Tool Input (Rouge-L↑)&lt;/th&gt;
   &lt;th align="center"&gt;False Positive Error↓&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GPT-4&lt;/td&gt;
   &lt;td align="center"&gt;98.0%&lt;/td&gt;
   &lt;td align="center"&gt;0.953&lt;/td&gt;
   &lt;td align="center"&gt;23.9%&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GPT-3.5&lt;/td&gt;
   &lt;td align="center"&gt;74.5%&lt;/td&gt;
   &lt;td align="center"&gt;0.807&lt;/td&gt;
   &lt;td align="center"&gt;80.6%&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-1_8B-Chat&lt;/td&gt;
   &lt;td align="center"&gt;85.0%&lt;/td&gt;
   &lt;td align="center"&gt;0.839&lt;/td&gt;
   &lt;td align="center"&gt;27.6%&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-7B-Chat&lt;/td&gt;
   &lt;td align="center"&gt;95.5%&lt;/td&gt;
   &lt;td align="center"&gt;0.900&lt;/td&gt;
   &lt;td align="center"&gt;11.6%&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-14B-Chat&lt;/td&gt;
   &lt;td align="center"&gt;96.9%&lt;/td&gt;
   &lt;td align="center"&gt;0.917&lt;/td&gt;
   &lt;td align="center"&gt;5.6%&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-72B-Chat&lt;/td&gt;
   &lt;td align="center"&gt;98.2%&lt;/td&gt;
   &lt;td align="center"&gt;0.927&lt;/td&gt;
   &lt;td align="center"&gt;1.1%&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;p&gt;To assess Qwen's ability to use the Python Code Interpreter for tasks such as mathematical problem solving, data visualization, and other general-purpose tasks such as file handling and web scraping, we have created and open-sourced a benchmark specifically designed for evaluating these capabilities. You can find the benchmark at this &lt;a href="https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark"&gt;link&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We have observed that Qwen performs well in terms of code executability and result accuracy when generating code:&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th colspan="5" align="center"&gt;Code Interpreter Benchmark (Version 20231206)&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th rowspan="2" align="center"&gt;Model&lt;/th&gt; 
   &lt;th colspan="3" align="center"&gt;Accuracy of Code Execution Results (%)&lt;/th&gt; 
   &lt;th colspan="1" align="center"&gt;Executable Rate of Code (%)&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Math↑&lt;/th&gt;
   &lt;th align="center"&gt;Visualization-Hard↑&lt;/th&gt;
   &lt;th align="center"&gt;Visualization-Easy↑&lt;/th&gt;
   &lt;th align="center"&gt;General↑&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GPT-4&lt;/td&gt; 
   &lt;td align="center"&gt;82.8&lt;/td&gt; 
   &lt;td align="center"&gt;66.7&lt;/td&gt; 
   &lt;td align="center"&gt;60.8&lt;/td&gt; 
   &lt;td align="center"&gt;82.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GPT-3.5&lt;/td&gt; 
   &lt;td align="center"&gt;47.3&lt;/td&gt; 
   &lt;td align="center"&gt;33.3&lt;/td&gt; 
   &lt;td align="center"&gt;55.7&lt;/td&gt; 
   &lt;td align="center"&gt;74.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LLaMA2-13B-Chat&lt;/td&gt; 
   &lt;td align="center"&gt;8.3&lt;/td&gt; 
   &lt;td align="center"&gt;1.2&lt;/td&gt; 
   &lt;td align="center"&gt;15.2&lt;/td&gt; 
   &lt;td align="center"&gt;48.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CodeLLaMA-13B-Instruct&lt;/td&gt; 
   &lt;td align="center"&gt;28.2&lt;/td&gt; 
   &lt;td align="center"&gt;15.5&lt;/td&gt; 
   &lt;td align="center"&gt;21.5&lt;/td&gt; 
   &lt;td align="center"&gt;74.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;InternLM-20B-Chat&lt;/td&gt; 
   &lt;td align="center"&gt;34.6&lt;/td&gt; 
   &lt;td align="center"&gt;10.7&lt;/td&gt; 
   &lt;td align="center"&gt;25.1&lt;/td&gt; 
   &lt;td align="center"&gt;65.5&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ChatGLM3-6B&lt;/td&gt; 
   &lt;td align="center"&gt;54.2&lt;/td&gt; 
   &lt;td align="center"&gt;4.8&lt;/td&gt; 
   &lt;td align="center"&gt;15.2&lt;/td&gt; 
   &lt;td align="center"&gt;67.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-1.8B-Chat&lt;/td&gt; 
   &lt;td align="center"&gt;25.6&lt;/td&gt; 
   &lt;td align="center"&gt;21.4&lt;/td&gt; 
   &lt;td align="center"&gt;22.8&lt;/td&gt; 
   &lt;td align="center"&gt;65.5&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-7B-Chat&lt;/td&gt; 
   &lt;td align="center"&gt;41.9&lt;/td&gt; 
   &lt;td align="center"&gt;23.8&lt;/td&gt; 
   &lt;td align="center"&gt;38.0&lt;/td&gt; 
   &lt;td align="center"&gt;67.2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-14B-Chat&lt;/td&gt; 
   &lt;td align="center"&gt;58.4&lt;/td&gt; 
   &lt;td align="center"&gt;31.0&lt;/td&gt; 
   &lt;td align="center"&gt;45.6&lt;/td&gt; 
   &lt;td align="center"&gt;65.5&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-72B-Chat&lt;/td&gt; 
   &lt;td align="center"&gt;72.7&lt;/td&gt; 
   &lt;td align="center"&gt;41.7&lt;/td&gt; 
   &lt;td align="center"&gt;43.0&lt;/td&gt; 
   &lt;td align="center"&gt;82.8&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;p align="center"&gt; &lt;br&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen/main/assets/code_interpreter_showcase_001.jpg"&gt; &lt;br&gt; &lt;/p&gt;
&lt;p&gt; &lt;br&gt; &lt;/p&gt;
&lt;h2&gt;Long-Context Understanding&lt;/h2&gt; 
&lt;p&gt;To extend the context length and break the bottleneck of training sequence length, we introduce several techniques, including NTK-aware interpolation, window attention, and LogN attention scaling, to extend the context length of Qwen-14B from 2K to over 8K tokens, and Qwen-1.8B/7B from 8K to 32K tokens.&lt;/p&gt; 
&lt;p&gt;For Qwen-72B, we adapt RoPE to longer contexts with a larger rotary base. Qwen-72B supports the max context length of 32K tokens.&lt;/p&gt; 
&lt;p&gt;We conduct language modeling experiments on the arXiv dataset with the PPL evaluation and find that Qwen can reach outstanding performance in the scenario of long context. Results are demonstrated below:&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th rowspan="2"&gt;Model&lt;/th&gt;
   &lt;th colspan="6" align="center"&gt;Sequence Length&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;1024&lt;/th&gt;
   &lt;th align="center"&gt;2048&lt;/th&gt;
   &lt;th align="center"&gt;4096&lt;/th&gt;
   &lt;th align="center"&gt;8192&lt;/th&gt;
   &lt;th align="center"&gt;16384&lt;/th&gt;
   &lt;th align="center"&gt;32768&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-7B (original)&lt;/td&gt;
   &lt;td align="center"&gt;4.23&lt;/td&gt;
   &lt;td align="center"&gt;3.78&lt;/td&gt;
   &lt;td align="center"&gt;39.35&lt;/td&gt;
   &lt;td align="center"&gt;469.81&lt;/td&gt;
   &lt;td align="center"&gt;2645.09&lt;/td&gt;
   &lt;td align="center"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;+ dynamic_ntk&lt;/td&gt;
   &lt;td align="center"&gt;4.23&lt;/td&gt;
   &lt;td align="center"&gt;3.78&lt;/td&gt;
   &lt;td align="center"&gt;3.59&lt;/td&gt;
   &lt;td align="center"&gt;3.66&lt;/td&gt;
   &lt;td align="center"&gt;5.71&lt;/td&gt;
   &lt;td align="center"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;+ dynamic_ntk + logn&lt;/td&gt;
   &lt;td align="center"&gt;4.23&lt;/td&gt;
   &lt;td align="center"&gt;3.78&lt;/td&gt;
   &lt;td align="center"&gt;3.58&lt;/td&gt;
   &lt;td align="center"&gt;3.56&lt;/td&gt;
   &lt;td align="center"&gt;4.62&lt;/td&gt;
   &lt;td align="center"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;+ dynamic_ntk + logn + window_attn&lt;/td&gt;
   &lt;td align="center"&gt;4.23&lt;/td&gt;
   &lt;td align="center"&gt;3.78&lt;/td&gt;
   &lt;td align="center"&gt;3.58&lt;/td&gt;
   &lt;td align="center"&gt;3.49&lt;/td&gt;
   &lt;td align="center"&gt;4.32&lt;/td&gt;
   &lt;td align="center"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
  &lt;/tr&gt;
  &lt;tr&gt; 
   &lt;td&gt;Qwen-1.8B&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;5.00&lt;/b&gt;&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;4.48&lt;/b&gt;&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;4.13&lt;/b&gt;&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;3.89&lt;/b&gt;&lt;/td&gt;
   &lt;td align="center"&gt;17.42&lt;/td&gt;
   &lt;td align="center"&gt;433.85&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;+ dynamic_ntk + logn + window_attn&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;5.00&lt;/b&gt;&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;4.48&lt;/b&gt;&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;4.14&lt;/b&gt;&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;3.93&lt;/b&gt;&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;3.82&lt;/b&gt;&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;3.83&lt;/b&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-7B&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;4.23&lt;/b&gt;&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;3.81&lt;/b&gt;&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;3.52&lt;/b&gt;&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;3.31&lt;/b&gt;&lt;/td&gt;
   &lt;td align="center"&gt;7.27&lt;/td&gt;
   &lt;td align="center"&gt;181.49&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;+ dynamic_ntk + logn + window_attn&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;4.23&lt;/b&gt;&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;3.81&lt;/b&gt;&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;3.52&lt;/b&gt;&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;3.33&lt;/b&gt;&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;3.22&lt;/b&gt;&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;3.17&lt;/b&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-14B&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;-&lt;/b&gt;&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;3.46&lt;/b&gt;&lt;/td&gt;
   &lt;td align="center"&gt;22.79&lt;/td&gt;
   &lt;td align="center"&gt;334.65&lt;/td&gt;
   &lt;td align="center"&gt;3168.35&lt;/td&gt;
   &lt;td align="center"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;+ dynamic_ntk + logn + window_attn&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;-&lt;/b&gt;&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;3.46&lt;/b&gt;&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;3.29&lt;/b&gt;&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;3.18&lt;/b&gt;&lt;/td&gt;
   &lt;td align="center"&gt;3.42&lt;/td&gt;
   &lt;td align="center"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-72B&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;-&lt;/b&gt;&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;-&lt;/b&gt;&lt;/td&gt;
   &lt;td align="center"&gt;-&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;2.83&lt;/b&gt;&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;2.73&lt;/b&gt;&lt;/td&gt;
   &lt;td align="center"&gt;&lt;b&gt;2.72&lt;/b&gt;&lt;/td&gt; 
  &lt;/tr&gt;  
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;p&gt;Furthermore, to verify the ability of Qwen-72B-Chat on long text understanding, we tested it on &lt;a href="https://arxiv.org/abs/2307.11088"&gt;L-Eval&lt;/a&gt; (closed-ended tasks). The results are as follows:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Model&lt;/th&gt; 
   &lt;th align="center"&gt;Input Length&lt;/th&gt; 
   &lt;th align="center"&gt;Average&lt;/th&gt; 
   &lt;th align="center"&gt;Coursera&lt;/th&gt; 
   &lt;th align="center"&gt;GSM&lt;/th&gt; 
   &lt;th align="center"&gt;QuALITY&lt;/th&gt; 
   &lt;th align="center"&gt;TOEFL&lt;/th&gt; 
   &lt;th align="center"&gt;CodeU&lt;/th&gt; 
   &lt;th align="center"&gt;SFcition&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;ChatGPT-3.5-16k&lt;/td&gt; 
   &lt;td align="center"&gt;16K&lt;/td&gt; 
   &lt;td align="center"&gt;60.73&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;63.51&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;84.00&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;61.38&lt;/td&gt; 
   &lt;td align="center"&gt;78.43&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;12.22&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;64.84&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;Qwen-72B-Chat&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;32K&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;62.30&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;58.13&lt;/td&gt; 
   &lt;td align="center"&gt;76.00&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;77.22&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;86.24&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;6.66&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;69.53&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;We conducted the "needle in a haystack" experiment (the idea came from &lt;a href="https://twitter.com/GregKamradt/status/1727018183608193393"&gt;@Greg Kamradt&lt;/a&gt;) to test whether the model can retrieve information at different positions in the inputs of different lengths, the result is as follows:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen/main/assets/qwen_72b_needle_in_a_haystack.png" alt=""&gt;&lt;/p&gt; 
&lt;p&gt;The above results show that Qwen-72B-Chat can accurately retrieve information placed in various positions within an input length of 32k, proving its excellent long text understanding capabilities.&lt;/p&gt; 
&lt;h2&gt;Tokenizer&lt;/h2&gt; 
&lt;p&gt;Our tokenizer based on tiktoken is different from other tokenizers, e.g., sentencepiece tokenizer. You need to pay attention to special tokens, especially in finetuning. For more detailed information on the tokenizer and related use in fine-tuning, please refer to the &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen/main/tokenization_note.md"&gt;documentation&lt;/a&gt;. &lt;br&gt;&lt;br&gt;&lt;/p&gt; 
&lt;h2&gt;Reproduction&lt;/h2&gt; 
&lt;p&gt;For your reproduction of the model performance on benchmark datasets, we provide scripts for you to reproduce the results. Check &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen/main/eval/EVALUATION.md"&gt;eval/EVALUATION.md&lt;/a&gt; for more information. Note that the reproduction may lead to slight differences from our reported results. &lt;br&gt;&lt;br&gt;&lt;/p&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;p&gt;If you meet problems, please refer to &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen/main/FAQ.md"&gt;FAQ&lt;/a&gt; and the issues first to search a solution before you launch a new issue. &lt;br&gt;&lt;br&gt;&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find our work helpful, feel free to give us a cite.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{qwen,
  title={Qwen Technical Report},
  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;br&gt; 
&lt;h2&gt;License Agreement&lt;/h2&gt; 
&lt;p&gt;The source code provided at &lt;a href="https://github.com/QwenLM/Qwen"&gt;https://github.com/QwenLM/Qwen&lt;/a&gt; is licensed under the &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen/main/LICENSE"&gt;Apache 2.0 License&lt;/a&gt; that can be found at the root directory.&lt;/p&gt; 
&lt;p&gt;Researchers and developers are free to use the codes and model weights of both Qwen and Qwen-Chat. For their commercial use, please check the License Agreement accompanying each model.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Qwen-72B, Qwen-14B, and Qwen-7B are licensed under the &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT"&gt;Tongyi Qianwen LICENSE AGREEMENT&lt;/a&gt; that can be found at the corresponding HuggingFace and ModelScope repository. For commercial use, please fill out the form (&lt;a href="https://dashscope.console.aliyun.com/openModelApply/Qwen-72B-Chat"&gt;72B&lt;/a&gt;, &lt;a href="https://dashscope.console.aliyun.com/openModelApply/Qwen-14B-Chat"&gt;14B&lt;/a&gt;, and &lt;a href="https://dashscope.console.aliyun.com/openModelApply/qianwen"&gt;7B&lt;/a&gt;) to apply.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Qwen-1.8B is licensed under the &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen/main/Tongyi%20Qianwen%20RESEARCH%20LICENSE%20AGREEMENT"&gt;Tongyi Qianwen RESEARCH LICENSE AGREEMENT&lt;/a&gt; that can be found at the corresponding HuggingFace and ModelScope repository. For commercial use, please contact us. &lt;br&gt;&lt;br&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contact Us&lt;/h2&gt; 
&lt;p&gt;If you are interested to leave a message to either our research team or product team, join our Discord or WeChat groups! Also, feel free to send an email to &lt;a href="mailto:qianwen_opensource@alibabacloud.com"&gt;qianwen_opensource@alibabacloud.com&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>infiniflow/ragflow</title>
      <link>https://github.com/infiniflow/ragflow</link>
      <description>&lt;p&gt;RAGFlow is an open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://demo.ragflow.io/"&gt; &lt;img src="https://raw.githubusercontent.com/infiniflow/ragflow/main/web/src/assets/logo-with-text.png" width="520" alt="ragflow logo"&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README.md"&gt;&lt;img alt="README in English" src="https://img.shields.io/badge/English-DBEDFA"&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README_zh.md"&gt;&lt;img alt="简体中文版自述文件" src="https://img.shields.io/badge/%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87-DFE0E5"&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README_tzh.md"&gt;&lt;img alt="繁體版中文自述文件" src="https://img.shields.io/badge/%E7%B9%81%E9%AB%94%E4%B8%AD%E6%96%87-DFE0E5"&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README_ja.md"&gt;&lt;img alt="日本語のREADME" src="https://img.shields.io/badge/%E6%97%A5%E6%9C%AC%E8%AA%9E-DFE0E5"&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README_ko.md"&gt;&lt;img alt="한국어" src="https://img.shields.io/badge/%ED%95%9C%EA%B5%AD%EC%96%B4-DFE0E5"&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README_id.md"&gt;&lt;img alt="Bahasa Indonesia" src="https://img.shields.io/badge/Bahasa%20Indonesia-DFE0E5"&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README_pt_br.md"&gt;&lt;img alt="Português(Brasil)" src="https://img.shields.io/badge/Portugu%C3%AAs(Brasil)-DFE0E5"&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://x.com/intent/follow?screen_name=infiniflowai" target="_blank"&gt; &lt;img src="https://img.shields.io/twitter/follow/infiniflow?logo=X&amp;amp;color=%20%23f5f5f5" alt="follow on X(Twitter)"&gt; &lt;/a&gt; &lt;a href="https://demo.ragflow.io" target="_blank"&gt; &lt;img alt="Static Badge" src="https://img.shields.io/badge/Online-Demo-4e6b99"&gt; &lt;/a&gt; &lt;a href="https://hub.docker.com/r/infiniflow/ragflow" target="_blank"&gt; &lt;img src="https://img.shields.io/docker/pulls/infiniflow/ragflow?label=Docker%20Pulls&amp;amp;color=0db7ed&amp;amp;logo=docker&amp;amp;logoColor=white&amp;amp;style=flat-square" alt="docker pull infiniflow/ragflow:v0.19.1"&gt; &lt;/a&gt; &lt;a href="https://github.com/infiniflow/ragflow/releases/latest"&gt; &lt;img src="https://img.shields.io/github/v/release/infiniflow/ragflow?color=blue&amp;amp;label=Latest%20Release" alt="Latest Release"&gt; &lt;/a&gt; &lt;a href="https://github.com/infiniflow/ragflow/raw/main/LICENSE"&gt; &lt;img height="21" src="https://img.shields.io/badge/License-Apache--2.0-ffffff?labelColor=d4eaf7&amp;amp;color=2e6cc4" alt="license"&gt; &lt;/a&gt; &lt;a href="https://deepwiki.com/infiniflow/ragflow"&gt; &lt;img alt="Ask DeepWiki" src="https://deepwiki.com/badge.svg?sanitize=true"&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h4 align="center"&gt; &lt;a href="https://ragflow.io/docs/dev/"&gt;Document&lt;/a&gt; | &lt;a href="https://github.com/infiniflow/ragflow/issues/4214"&gt;Roadmap&lt;/a&gt; | &lt;a href="https://twitter.com/infiniflowai"&gt;Twitter&lt;/a&gt; | &lt;a href="https://discord.gg/NjYzJD3GM3"&gt;Discord&lt;/a&gt; | &lt;a href="https://demo.ragflow.io"&gt;Demo&lt;/a&gt; &lt;/h4&gt; 
&lt;h1&gt;&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/9064" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/9064" alt="infiniflow%2Fragflow | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;b&gt;📕 Table of Contents&lt;/b&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;💡 &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-what-is-ragflow"&gt;What is RAGFlow?&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;🎮 &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-demo"&gt;Demo&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;📌 &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-latest-updates"&gt;Latest Updates&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;🌟 &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-key-features"&gt;Key Features&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;🔎 &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-system-architecture"&gt;System Architecture&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;🎬 &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-get-started"&gt;Get Started&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;🔧 &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-configurations"&gt;Configurations&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;🔧 &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-build-a-docker-image-without-embedding-models"&gt;Build a docker image without embedding models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;🔧 &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-build-a-docker-image-including-embedding-models"&gt;Build a docker image including embedding models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;🔨 &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-launch-service-from-source-for-development"&gt;Launch service from source for development&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;📚 &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;📜 &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-roadmap"&gt;Roadmap&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;🏄 &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-community"&gt;Community&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;🙌 &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;💡 What is RAGFlow?&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://ragflow.io/"&gt;RAGFlow&lt;/a&gt; is an open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding. It offers a streamlined RAG workflow for businesses of any scale, combining LLM (Large Language Models) to provide truthful question-answering capabilities, backed by well-founded citations from various complex formatted data.&lt;/p&gt; 
&lt;h2&gt;🎮 Demo&lt;/h2&gt; 
&lt;p&gt;Try our demo at &lt;a href="https://demo.ragflow.io"&gt;https://demo.ragflow.io&lt;/a&gt;.&lt;/p&gt; 
&lt;div align="center" style="margin-top:20px;margin-bottom:20px;"&gt; 
 &lt;img src="https://github.com/infiniflow/ragflow/assets/7248/2f6baa3e-1092-4f11-866d-36f6a9d075e5" width="1200"&gt; 
 &lt;img src="https://github.com/user-attachments/assets/504bbbf1-c9f7-4d83-8cc5-e9cb63c26db6" width="1200"&gt; 
&lt;/div&gt; 
&lt;h2&gt;🔥 Latest Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;2025-05-23 Adds a Python/JavaScript code executor component to Agent.&lt;/li&gt; 
 &lt;li&gt;2025-05-05 Supports cross-language query.&lt;/li&gt; 
 &lt;li&gt;2025-03-19 Supports using a multi-modal model to make sense of images within PDF or DOCX files.&lt;/li&gt; 
 &lt;li&gt;2025-02-28 Combined with Internet search (Tavily), supports reasoning like Deep Research for any LLMs.&lt;/li&gt; 
 &lt;li&gt;2024-12-18 Upgrades Document Layout Analysis model in DeepDoc.&lt;/li&gt; 
 &lt;li&gt;2024-08-22 Support text to SQL statements through RAG.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🎉 Stay Tuned&lt;/h2&gt; 
&lt;p&gt;⭐️ Star our repository to stay up-to-date with exciting new features and improvements! Get instant notifications for new releases! 🌟&lt;/p&gt; 
&lt;div align="center" style="margin-top:20px;margin-bottom:20px;"&gt; 
 &lt;img src="https://github.com/user-attachments/assets/18c9707e-b8aa-4caf-a154-037089c105ba" width="1200"&gt; 
&lt;/div&gt; 
&lt;h2&gt;🌟 Key Features&lt;/h2&gt; 
&lt;h3&gt;🍭 &lt;strong&gt;"Quality in, quality out"&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/deepdoc/README.md"&gt;Deep document understanding&lt;/a&gt;-based knowledge extraction from unstructured data with complicated formats.&lt;/li&gt; 
 &lt;li&gt;Finds "needle in a data haystack" of literally unlimited tokens.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🍱 &lt;strong&gt;Template-based chunking&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Intelligent and explainable.&lt;/li&gt; 
 &lt;li&gt;Plenty of template options to choose from.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🌱 &lt;strong&gt;Grounded citations with reduced hallucinations&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Visualization of text chunking to allow human intervention.&lt;/li&gt; 
 &lt;li&gt;Quick view of the key references and traceable citations to support grounded answers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🍔 &lt;strong&gt;Compatibility with heterogeneous data sources&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports Word, slides, excel, txt, images, scanned copies, structured data, web pages, and more.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🛀 &lt;strong&gt;Automated and effortless RAG workflow&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Streamlined RAG orchestration catered to both personal and large businesses.&lt;/li&gt; 
 &lt;li&gt;Configurable LLMs as well as embedding models.&lt;/li&gt; 
 &lt;li&gt;Multiple recall paired with fused re-ranking.&lt;/li&gt; 
 &lt;li&gt;Intuitive APIs for seamless integration with business.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🔎 System Architecture&lt;/h2&gt; 
&lt;div align="center" style="margin-top:20px;margin-bottom:20px;"&gt; 
 &lt;img src="https://github.com/infiniflow/ragflow/assets/12318111/d6ac5664-c237-4200-a7c2-a4a00691b485" width="1000"&gt; 
&lt;/div&gt; 
&lt;h2&gt;🎬 Get Started&lt;/h2&gt; 
&lt;h3&gt;📝 Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;CPU &amp;gt;= 4 cores&lt;/li&gt; 
 &lt;li&gt;RAM &amp;gt;= 16 GB&lt;/li&gt; 
 &lt;li&gt;Disk &amp;gt;= 50 GB&lt;/li&gt; 
 &lt;li&gt;Docker &amp;gt;= 24.0.0 &amp;amp; Docker Compose &amp;gt;= v2.26.1&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://gvisor.dev/docs/user_guide/install/"&gt;gVisor&lt;/a&gt;: Required only if you intend to use the code executor (sandbox) feature of RAGFlow.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] If you have not installed Docker on your local machine (Windows, Mac, or Linux), see &lt;a href="https://docs.docker.com/engine/install/"&gt;Install Docker Engine&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;🚀 Start up the server&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Ensure &lt;code&gt;vm.max_map_count&lt;/code&gt; &amp;gt;= 262144:&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;To check the value of &lt;code&gt;vm.max_map_count&lt;/code&gt;:&lt;/p&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;$ sysctl vm.max_map_count
&lt;/code&gt;&lt;/pre&gt; 
   &lt;p&gt;Reset &lt;code&gt;vm.max_map_count&lt;/code&gt; to a value at least 262144 if it is not.&lt;/p&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;# In this case, we set it to 262144:
$ sudo sysctl -w vm.max_map_count=262144
&lt;/code&gt;&lt;/pre&gt; 
   &lt;p&gt;This change will be reset after a system reboot. To ensure your change remains permanent, add or update the &lt;code&gt;vm.max_map_count&lt;/code&gt; value in &lt;strong&gt;/etc/sysctl.conf&lt;/strong&gt; accordingly:&lt;/p&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;vm.max_map_count=262144
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the repo:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;$ git clone https://github.com/infiniflow/ragflow.git
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Start up the server using the pre-built Docker images:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!CAUTION] All Docker images are built for x86 platforms. We don't currently offer Docker images for ARM64. If you are on an ARM64 platform, follow &lt;a href="https://ragflow.io/docs/dev/build_docker_image"&gt;this guide&lt;/a&gt; to build a Docker image compatible with your system.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The command below downloads the &lt;code&gt;v0.19.1-slim&lt;/code&gt; edition of the RAGFlow Docker image. See the following table for descriptions of different RAGFlow editions. To download a RAGFlow edition different from &lt;code&gt;v0.19.1-slim&lt;/code&gt;, update the &lt;code&gt;RAGFLOW_IMAGE&lt;/code&gt; variable accordingly in &lt;strong&gt;docker/.env&lt;/strong&gt; before using &lt;code&gt;docker compose&lt;/code&gt; to start the server. For example: set &lt;code&gt;RAGFLOW_IMAGE=infiniflow/ragflow:v0.19.1&lt;/code&gt; for the full edition &lt;code&gt;v0.19.1&lt;/code&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ cd ragflow/docker
# Use CPU for embedding and DeepDoc tasks:
$ docker compose -f docker-compose.yml up -d

# To use GPU to accelerate embedding and DeepDoc tasks:
# docker compose -f docker-compose-gpu.yml up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;RAGFlow image tag&lt;/th&gt; 
   &lt;th&gt;Image size (GB)&lt;/th&gt; 
   &lt;th&gt;Has embedding models?&lt;/th&gt; 
   &lt;th&gt;Stable?&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;v0.19.1&lt;/td&gt; 
   &lt;td&gt;≈9&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;Stable release&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;v0.19.1-slim&lt;/td&gt; 
   &lt;td&gt;≈2&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;Stable release&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;nightly&lt;/td&gt; 
   &lt;td&gt;≈9&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;em&gt;Unstable&lt;/em&gt; nightly build&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;nightly-slim&lt;/td&gt; 
   &lt;td&gt;≈2&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;&lt;em&gt;Unstable&lt;/em&gt; nightly build&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt; &lt;p&gt;Check the server status after having the server up and running:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;$ docker logs -f ragflow-server
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;The following output confirms a successful launch of the system:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;
      ____   ___    ______ ______ __
     / __ \ /   |  / ____// ____// /____  _      __
    / /_/ // /| | / / __ / /_   / // __ \| | /| / /
   / _, _// ___ |/ /_/ // __/  / // /_/ /| |/ |/ /
  /_/ |_|/_/  |_|\____//_/    /_/ \____/ |__/|__/

 * Running on all addresses (0.0.0.0)
&lt;/code&gt;&lt;/pre&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;If you skip this confirmation step and directly log in to RAGFlow, your browser may prompt a &lt;code&gt;network anormal&lt;/code&gt; error because, at that moment, your RAGFlow may not be fully initialized.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;In your web browser, enter the IP address of your server and log in to RAGFlow.&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;With the default settings, you only need to enter &lt;code&gt;http://IP_OF_YOUR_MACHINE&lt;/code&gt; (&lt;strong&gt;sans&lt;/strong&gt; port number) as the default HTTP serving port &lt;code&gt;80&lt;/code&gt; can be omitted when using the default configurations.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;In &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/service_conf.yaml.template"&gt;service_conf.yaml.template&lt;/a&gt;, select the desired LLM factory in &lt;code&gt;user_default_llm&lt;/code&gt; and update the &lt;code&gt;API_KEY&lt;/code&gt; field with the corresponding API key.&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;See &lt;a href="https://ragflow.io/docs/dev/llm_api_key_setup"&gt;llm_api_key_setup&lt;/a&gt; for more information.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;p&gt;&lt;em&gt;The show is on!&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;🔧 Configurations&lt;/h2&gt; 
&lt;p&gt;When it comes to system configurations, you will need to manage the following files:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/.env"&gt;.env&lt;/a&gt;: Keeps the fundamental setups for the system, such as &lt;code&gt;SVR_HTTP_PORT&lt;/code&gt;, &lt;code&gt;MYSQL_PASSWORD&lt;/code&gt;, and &lt;code&gt;MINIO_PASSWORD&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/service_conf.yaml.template"&gt;service_conf.yaml.template&lt;/a&gt;: Configures the back-end services. The environment variables in this file will be automatically populated when the Docker container starts. Any environment variables set within the Docker container will be available for use, allowing you to customize service behavior based on the deployment environment.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt;: The system relies on &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt; to start up.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/README.md"&gt;./docker/README&lt;/a&gt; file provides a detailed description of the environment settings and service configurations which can be used as &lt;code&gt;${ENV_VARS}&lt;/code&gt; in the &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/service_conf.yaml.template"&gt;service_conf.yaml.template&lt;/a&gt; file.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;To update the default HTTP serving port (80), go to &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt; and change &lt;code&gt;80:80&lt;/code&gt; to &lt;code&gt;&amp;lt;YOUR_SERVING_PORT&amp;gt;:80&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Updates to the above configurations require a reboot of all containers to take effect:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;$ docker compose -f docker-compose.yml up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Switch doc engine from Elasticsearch to Infinity&lt;/h3&gt; 
&lt;p&gt;RAGFlow uses Elasticsearch by default for storing full text and vectors. To switch to &lt;a href="https://github.com/infiniflow/infinity/"&gt;Infinity&lt;/a&gt;, follow these steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Stop all running containers:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;$ docker compose -f docker/docker-compose.yml down -v
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] &lt;code&gt;-v&lt;/code&gt; will delete the docker container volumes, and the existing data will be cleared.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt; &lt;p&gt;Set &lt;code&gt;DOC_ENGINE&lt;/code&gt; in &lt;strong&gt;docker/.env&lt;/strong&gt; to &lt;code&gt;infinity&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Start the containers:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;$ docker compose -f docker-compose.yml up -d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] Switching to Infinity on a Linux/arm64 machine is not yet officially supported.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;🔧 Build a Docker image without embedding models&lt;/h2&gt; 
&lt;p&gt;This image is approximately 2 GB in size and relies on external LLM and embedding services.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/infiniflow/ragflow.git
cd ragflow/
docker build --platform linux/amd64 --build-arg LIGHTEN=1 -f Dockerfile -t infiniflow/ragflow:nightly-slim .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;🔧 Build a Docker image including embedding models&lt;/h2&gt; 
&lt;p&gt;This image is approximately 9 GB in size. As it includes embedding models, it relies on external LLM services only.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/infiniflow/ragflow.git
cd ragflow/
docker build --platform linux/amd64 -f Dockerfile -t infiniflow/ragflow:nightly .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;🔨 Launch service from source for development&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install uv, or skip this step if it is already installed:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pipx install uv pre-commit
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the source code and install Python dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/infiniflow/ragflow.git
cd ragflow/
uv sync --python 3.10 --all-extras # install RAGFlow dependent python modules
uv run download_deps.py
pre-commit install
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Launch the dependent services (MinIO, Elasticsearch, Redis, and MySQL) using Docker Compose:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker compose -f docker/docker-compose-base.yml up -d
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Add the following line to &lt;code&gt;/etc/hosts&lt;/code&gt; to resolve all hosts specified in &lt;strong&gt;docker/.env&lt;/strong&gt; to &lt;code&gt;127.0.0.1&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;127.0.0.1       es01 infinity mysql minio redis sandbox-executor-manager
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If you cannot access HuggingFace, set the &lt;code&gt;HF_ENDPOINT&lt;/code&gt; environment variable to use a mirror site:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;export HF_ENDPOINT=https://hf-mirror.com
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If your operating system does not have jemalloc, please install it as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# ubuntu
sudo apt-get install libjemalloc-dev
# centos
sudo yum install jemalloc
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Launch backend service:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;source .venv/bin/activate
export PYTHONPATH=$(pwd)
bash docker/launch_backend_service.sh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install frontend dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd web
npm install
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Launch frontend service:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;npm run dev
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;The following output confirms a successful launch of the system:&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/0daf462c-a24d-4496-a66f-92533534e187" alt=""&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Stop RAGFlow front-end and back-end service after development is complete:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pkill -f "ragflow_server.py|task_executor.py"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;📚 Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/"&gt;Quickstart&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/configurations"&gt;Configuration&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/release_notes"&gt;Release notes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/category/guides"&gt;User guides&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/category/developers"&gt;Developer guides&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/category/references"&gt;References&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/faq"&gt;FAQs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;📜 Roadmap&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/infiniflow/ragflow/issues/4214"&gt;RAGFlow Roadmap 2025&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;🏄 Community&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/NjYzJD3GM3"&gt;Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/infiniflowai"&gt;Twitter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/orgs/infiniflow/discussions"&gt;GitHub Discussions&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🙌 Contributing&lt;/h2&gt; 
&lt;p&gt;RAGFlow flourishes via open-source collaboration. In this spirit, we embrace diverse contributions from the community. If you would like to be a part, review our &lt;a href="https://ragflow.io/docs/dev/contributing"&gt;Contribution Guidelines&lt;/a&gt; first.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>roboflow/rf-detr</title>
      <link>https://github.com/roboflow/rf-detr</link>
      <description>&lt;p&gt;RF-DETR is a real-time object detection model architecture developed by Roboflow, SOTA on COCO and designed for fine-tuning.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;RF-DETR: SOTA Real-Time Object Detection Model&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://badge.fury.io/py/rfdetr"&gt;&lt;img src="https://badge.fury.io/py/rfdetr.svg?sanitize=true" alt="version"&gt;&lt;/a&gt; &lt;a href="https://pypistats.org/packages/rfdetr"&gt;&lt;img src="https://img.shields.io/pypi/dm/rfdetr" alt="downloads"&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/rfdetr"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/rfdetr" alt="python-version"&gt;&lt;/a&gt; &lt;a href="https://github.com/roboflow/rfdetr/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-Apache%202.0-blue" alt="license"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/SkalskiP/RF-DETR"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue" alt="hf space"&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-rf-detr-on-detection-dataset.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="colab"&gt;&lt;/a&gt; &lt;a href="https://blog.roboflow.com/rf-detr"&gt;&lt;img src="https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true" alt="roboflow"&gt;&lt;/a&gt; &lt;a href="https://discord.gg/GbfgXGJ8Bk"&gt;&lt;img src="https://img.shields.io/discord/1159501506232451173?logo=discord&amp;amp;label=discord&amp;amp;labelColor=fff&amp;amp;color=5865f2&amp;amp;link=https%3A%2F%2Fdiscord.gg%2FGbfgXGJ8Bk" alt="discord"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;RF-DETR is a real-time, transformer-based object detection model architecture developed by Roboflow and released under the Apache 2.0 license.&lt;/p&gt; 
&lt;p&gt;RF-DETR is the first real-time model to exceed 60 AP on the &lt;a href="https://cocodataset.org/#home"&gt;Microsoft COCO benchmark&lt;/a&gt; alongside competitive performance at base sizes. It also achieves state-of-the-art performance on &lt;a href="https://github.com/roboflow/rf100-vl"&gt;RF100-VL&lt;/a&gt;, an object detection benchmark that measures model domain adaptability to real world problems. RF-DETR is fastest and most accurate for its size when compared current real-time objection models.&lt;/p&gt; 
&lt;p&gt;RF-DETR is small enough to run on the edge using &lt;a href="https://github.com/roboflow/inference"&gt;Inference&lt;/a&gt;, making it an ideal model for deployments that need both strong accuracy and real-time performance.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://rfdetr.roboflow.com"&gt;Read the documentation to get started training.&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;2025/07/23&lt;/code&gt;: We release three new checkpoints for RF-DETR: Nano, Small, and Medium. 
  &lt;ul&gt; 
   &lt;li&gt;RF-DETR Base is now deprecated. We recommend using RF-DETR Medium which offers subtantially better accuracy at comparable latency.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;2025/03/20&lt;/code&gt;: We release RF-DETR real-time object detection model. &lt;strong&gt;Code and checkpoint for RF-DETR-large and RF-DETR-base are available.&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;2025/04/03&lt;/code&gt;: We release early stopping, gradient checkpointing, metrics saving, training resume, TensorBoard and W&amp;amp;B logging support.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;2025/05/16&lt;/code&gt;: We release an 'optimize_for_inference' method which speeds up native PyTorch by up to 2x, depending on platform.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Results&lt;/h2&gt; 
&lt;p&gt;RF-DETR achieves state-of-the-art performance on both the Microsoft COCO and the RF100-VL benchmarks.&lt;/p&gt; 
&lt;p&gt;The table below shows the performance of RF-DETR medium, compared to comparable medium models:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://media.roboflow.com/rfdetr/pareto1.png" alt="rf-detr-coco-rf100-vl-9"&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;family&lt;/th&gt; 
   &lt;th&gt;size&lt;/th&gt; 
   &lt;th&gt;coco_map50&lt;/th&gt; 
   &lt;th&gt;coco_map50@95&lt;/th&gt; 
   &lt;th&gt;rf100vl_map50&lt;/th&gt; 
   &lt;th&gt;rv100vl_map50@95&lt;/th&gt; 
   &lt;th&gt;latency&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;RF-DETR&lt;/td&gt; 
   &lt;td&gt;Nano&lt;/td&gt; 
   &lt;td&gt;67.6&lt;/td&gt; 
   &lt;td&gt;48.4&lt;/td&gt; 
   &lt;td&gt;84.1&lt;/td&gt; 
   &lt;td&gt;57.1&lt;/td&gt; 
   &lt;td&gt;2.32&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;RF-DETR&lt;/td&gt; 
   &lt;td&gt;Small&lt;/td&gt; 
   &lt;td&gt;72.1&lt;/td&gt; 
   &lt;td&gt;53.0&lt;/td&gt; 
   &lt;td&gt;85.9&lt;/td&gt; 
   &lt;td&gt;59.6&lt;/td&gt; 
   &lt;td&gt;3.52&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;RF-DETR&lt;/td&gt; 
   &lt;td&gt;Medium&lt;/td&gt; 
   &lt;td&gt;73.6&lt;/td&gt; 
   &lt;td&gt;54.7&lt;/td&gt; 
   &lt;td&gt;86.6&lt;/td&gt; 
   &lt;td&gt;60.6&lt;/td&gt; 
   &lt;td&gt;4.52&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;YOLO11&lt;/td&gt; 
   &lt;td&gt;n&lt;/td&gt; 
   &lt;td&gt;52.0&lt;/td&gt; 
   &lt;td&gt;37.4&lt;/td&gt; 
   &lt;td&gt;81.4&lt;/td&gt; 
   &lt;td&gt;55.3&lt;/td&gt; 
   &lt;td&gt;2.49&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;YOLO11&lt;/td&gt; 
   &lt;td&gt;s&lt;/td&gt; 
   &lt;td&gt;59.7&lt;/td&gt; 
   &lt;td&gt;44.4&lt;/td&gt; 
   &lt;td&gt;82.3&lt;/td&gt; 
   &lt;td&gt;56.2&lt;/td&gt; 
   &lt;td&gt;3.16&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;YOLO11&lt;/td&gt; 
   &lt;td&gt;m&lt;/td&gt; 
   &lt;td&gt;64.1&lt;/td&gt; 
   &lt;td&gt;48.6&lt;/td&gt; 
   &lt;td&gt;82.5&lt;/td&gt; 
   &lt;td&gt;56.5&lt;/td&gt; 
   &lt;td&gt;5.13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;YOLO11&lt;/td&gt; 
   &lt;td&gt;l&lt;/td&gt; 
   &lt;td&gt;65.3&lt;/td&gt; 
   &lt;td&gt;50.2&lt;/td&gt; 
   &lt;td&gt;x&lt;/td&gt; 
   &lt;td&gt;x&lt;/td&gt; 
   &lt;td&gt;6.65&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;YOLO11&lt;/td&gt; 
   &lt;td&gt;x&lt;/td&gt; 
   &lt;td&gt;66.5&lt;/td&gt; 
   &lt;td&gt;51.2&lt;/td&gt; 
   &lt;td&gt;x&lt;/td&gt; 
   &lt;td&gt;x&lt;/td&gt; 
   &lt;td&gt;11.92&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LW-DETR&lt;/td&gt; 
   &lt;td&gt;Tiny&lt;/td&gt; 
   &lt;td&gt;60.7&lt;/td&gt; 
   &lt;td&gt;42.9&lt;/td&gt; 
   &lt;td&gt;x&lt;/td&gt; 
   &lt;td&gt;x&lt;/td&gt; 
   &lt;td&gt;1.91&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LW-DETR&lt;/td&gt; 
   &lt;td&gt;Small&lt;/td&gt; 
   &lt;td&gt;66.8&lt;/td&gt; 
   &lt;td&gt;48.0&lt;/td&gt; 
   &lt;td&gt;84.5&lt;/td&gt; 
   &lt;td&gt;58.0&lt;/td&gt; 
   &lt;td&gt;2.62&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LW-DETR&lt;/td&gt; 
   &lt;td&gt;Medium&lt;/td&gt; 
   &lt;td&gt;72.0&lt;/td&gt; 
   &lt;td&gt;52.6&lt;/td&gt; 
   &lt;td&gt;85.2&lt;/td&gt; 
   &lt;td&gt;59.4&lt;/td&gt; 
   &lt;td&gt;4.49&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;D-FINE&lt;/td&gt; 
   &lt;td&gt;Nano&lt;/td&gt; 
   &lt;td&gt;60.2&lt;/td&gt; 
   &lt;td&gt;42.7&lt;/td&gt; 
   &lt;td&gt;83.6&lt;/td&gt; 
   &lt;td&gt;57.7&lt;/td&gt; 
   &lt;td&gt;2.12&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;D-FINE&lt;/td&gt; 
   &lt;td&gt;Small&lt;/td&gt; 
   &lt;td&gt;67.6&lt;/td&gt; 
   &lt;td&gt;50.7&lt;/td&gt; 
   &lt;td&gt;84.5&lt;/td&gt; 
   &lt;td&gt;59.9&lt;/td&gt; 
   &lt;td&gt;3.55&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;D-FINE&lt;/td&gt; 
   &lt;td&gt;Medium&lt;/td&gt; 
   &lt;td&gt;72.6&lt;/td&gt; 
   &lt;td&gt;55.1&lt;/td&gt; 
   &lt;td&gt;84.6&lt;/td&gt; 
   &lt;td&gt;60.2&lt;/td&gt; 
   &lt;td&gt;5.68&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;a href="https://rfdetr.roboflow.com/learn/benchmarks/"&gt;See our benchmark notes in the RF-DETR documentation.&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;We are actively working on RF-DETR Large and X-Large models using the same techniques we used to achieve the strong accuracy that RF-DETR Medium attains. This is why RF-DETR Large and X-Large is not yet reported on our pareto charts and why we haven't benchmarked other models at similar sizes. Check back in the next few weeks for the launch of new RF-DETR Large and X-Large models.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install RF-DETR, install the &lt;code&gt;rfdetr&lt;/code&gt; package in a &lt;a href="https://www.python.org/"&gt;&lt;strong&gt;Python&amp;gt;=3.9&lt;/strong&gt;&lt;/a&gt; environment with &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install rfdetr
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;Install from source&lt;/summary&gt; 
 &lt;br&gt; 
 &lt;p&gt;By installing RF-DETR from source, you can explore the most recent features and enhancements that have not yet been officially released. Please note that these updates are still in development and may not be as stable as the latest published release.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install git+https://github.com/roboflow/rf-detr.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Inference&lt;/h2&gt; 
&lt;p&gt;The easiest path to deployment is using Roboflow's &lt;a href="https://github.com/roboflow/inference"&gt;Inference&lt;/a&gt; package.&lt;/p&gt; 
&lt;p&gt;The code below lets you run &lt;code&gt;rfdetr-base&lt;/code&gt; on an image:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
import supervision as sv
from inference import get_model
from PIL import Image
from io import BytesIO
import requests

url = "https://media.roboflow.com/dog.jpeg"
image = Image.open(BytesIO(requests.get(url).content))

model = get_model("rfdetr-base")

predictions = model.infer(image, confidence=0.5)[0]

detections = sv.Detections.from_inference(predictions)

labels = [prediction.class_name for prediction in predictions.predictions]

annotated_image = image.copy()
annotated_image = sv.BoxAnnotator(color=sv.ColorPalette.ROBOFLOW).annotate(annotated_image, detections)
annotated_image = sv.LabelAnnotator(color=sv.ColorPalette.ROBOFLOW).annotate(annotated_image, detections, labels)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Predict&lt;/h2&gt; 
&lt;p&gt;You can also use the .predict method to perform inference during local development. The &lt;code&gt;.predict()&lt;/code&gt; method accepts various input formats, including file paths, PIL images, NumPy arrays, and torch tensors. Please ensure inputs use RGB channel order. For &lt;code&gt;torch.Tensor&lt;/code&gt; inputs specifically, they must have a shape of &lt;code&gt;(3, H, W)&lt;/code&gt; with values normalized to the &lt;code&gt;[0..1)&lt;/code&gt; range. If you don't plan to modify the image or batch size dynamically at runtime, you can also use &lt;code&gt;.optimize_for_inference()&lt;/code&gt; to get up to 2x end-to-end speedup, depending on platform.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import io
import requests
import supervision as sv
from PIL import Image
from rfdetr import RFDETRBase
from rfdetr.util.coco_classes import COCO_CLASSES

model = RFDETRBase()

model = model.optimize_for_inference()

url = "https://media.roboflow.com/notebooks/examples/dog-2.jpeg"

image = Image.open(io.BytesIO(requests.get(url).content))
detections = model.predict(image, threshold=0.5)

labels = [
    f"{COCO_CLASSES[class_id]} {confidence:.2f}"
    for class_id, confidence
    in zip(detections.class_id, detections.confidence)
]

annotated_image = image.copy()
annotated_image = sv.BoxAnnotator().annotate(annotated_image, detections)
annotated_image = sv.LabelAnnotator().annotate(annotated_image, detections, labels)

sv.plot_image(annotated_image)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Train a Model&lt;/h3&gt; 
&lt;p&gt;You can fine-tune an RF-DETR Nano, Small, Medium, and Base model with a custom dataset using the &lt;code&gt;rfdetr&lt;/code&gt; Python package.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://rfdetr.roboflow.com/learn/train/"&gt;Read our training tutorial to get started&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Visit our &lt;a href="https://rfdetr.roboflow.com"&gt;documentation website&lt;/a&gt; to learn more about how to use RF-DETR.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Both the code and the weights pretrained on the COCO dataset are released under the &lt;a href="https://github.com/roboflow/r-flow/raw/main/LICENSE"&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;Our work is built upon &lt;a href="https://arxiv.org/pdf/2406.03459"&gt;LW-DETR&lt;/a&gt;, &lt;a href="https://arxiv.org/pdf/2304.07193"&gt;DINOv2&lt;/a&gt;, and &lt;a href="https://arxiv.org/pdf/2010.04159"&gt;Deformable DETR&lt;/a&gt;. Thanks to their authors for their excellent work!&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find our work helpful for your research, please consider citing the following BibTeX entry.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@software{rf-detr,
  author = {Robinson, Isaac and Robicheaux, Peter and Popov, Matvei},
  license = {Apache-2.0},
  title = {RF-DETR},
  howpublished = {\url{https://github.com/roboflow/rf-detr}},
  year = {2025},
  note = {SOTA Real-Time Object Detection Model}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contribute&lt;/h2&gt; 
&lt;p&gt;We welcome and appreciate all contributions! If you notice any issues or bugs, have questions, or would like to suggest new features, please &lt;a href="https://github.com/roboflow/rf-detr/issues/new"&gt;open an issue&lt;/a&gt; or pull request. By sharing your ideas and improvements, you help make RF-DETR better for everyone.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://youtube.com/roboflow"&gt; &lt;img src="https://media.roboflow.com/notebooks/template/icons/purple/youtube.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949634652" width="3%"&gt; &lt;/a&gt; 
 &lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%"&gt; 
 &lt;a href="https://roboflow.com"&gt; &lt;img src="https://media.roboflow.com/notebooks/template/icons/purple/roboflow-app.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949746649" width="3%"&gt; &lt;/a&gt; 
 &lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%"&gt; 
 &lt;a href="https://www.linkedin.com/company/roboflow-ai/"&gt; &lt;img src="https://media.roboflow.com/notebooks/template/icons/purple/linkedin.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633691" width="3%"&gt; &lt;/a&gt; 
 &lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%"&gt; 
 &lt;a href="https://docs.roboflow.com"&gt; &lt;img src="https://media.roboflow.com/notebooks/template/icons/purple/knowledge.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949634511" width="3%"&gt; &lt;/a&gt; 
 &lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%"&gt; 
 &lt;a href="https://discuss.roboflow.com"&gt; &lt;img src="https://media.roboflow.com/notebooks/template/icons/purple/forum.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633584" width="3%"&gt; &lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%"&gt; &lt;/a&gt;
 &lt;a href="https://blog.roboflow.com"&gt; &lt;img src="https://media.roboflow.com/notebooks/template/icons/purple/blog.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633605" width="3%"&gt; &lt;/a&gt;  
&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>