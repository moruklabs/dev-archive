<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Sat, 20 Sep 2025 01:35:58 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>Alibaba-NLP/DeepResearch</title>
      <link>https://github.com/Alibaba-NLP/DeepResearch</link>
      <description>&lt;p&gt;Tongyi Deep Research, the Leading Open-source Deep Research Agent&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/logo.png" width="100%" /&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center" style="line-height: 1;"&gt; 
 &lt;p&gt;&lt;a href="https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B"&gt;&lt;img src="https://img.shields.io/badge/Models-5EDDD2?style=for-the-badge&amp;amp;logo=huggingface&amp;amp;logoColor=ffffff&amp;amp;labelColor" alt="MODELS" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Alibaba-NLP/DeepResearch"&gt;&lt;img src="https://img.shields.io/badge/Github-24292F?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" alt="GITHUB" /&gt;&lt;/a&gt; &lt;a href="https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/"&gt;&lt;img src="https://img.shields.io/badge/Blog-4285F4?style=for-the-badge&amp;amp;logo=google-chrome&amp;amp;logoColor=white" alt="Blog" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; ü§ó &lt;a href="https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B" target="_blank"&gt;HuggingFace&lt;/a&gt; ÔΩú &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/tongyi.png" width="14px" style="display:inline;" /&gt; &lt;a href="https://modelscope.cn/models/iic/Tongyi-DeepResearch-30B-A3B" target="_blank"&gt;ModelScope&lt;/a&gt; &lt;/p&gt;
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/14217" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14217" alt="Alibaba-NLP%2FWebAgent | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt;
&lt;h1&gt;Introduction&lt;/h1&gt; 
&lt;p&gt;We present &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/tongyi.png" width="14px" style="display:inline;" /&gt; &lt;strong&gt;Tongyi DeepResearch&lt;/strong&gt;, an agentic large language model featuring 30.5 billion total parameters, with only 3.3 billion activated per token. Developed by Tongyi Lab, the model is specifically designed for &lt;strong&gt;long-horizon, deep information-seeking&lt;/strong&gt; tasks. Tongyi DeepResearch demonstrates state-of-the-art performance across a range of agentic search benchmarks, including Humanity's Last Exam, BrowserComp, BrowserComp-ZH, WebWalkerQA,xbench-DeepSearch, FRAMES and SimpleQA.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Tongyi DeepResearch builds upon our previous work on the &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/tongyi.png" width="14px" style="display:inline;" /&gt; &lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/WebAgent/"&gt;WebAgent&lt;/a&gt; project.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;More details can be found in our üì∞&amp;nbsp;&lt;a href="https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/"&gt;Tech Blog&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/performance.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚öôÔ∏è &lt;strong&gt;Fully automated synthetic data generation pipeline&lt;/strong&gt;: We design a highly scalable data synthesis pipeline, which is fully automatic and empowers agentic pre-training, supervised fine-tuning, and reinforcement learning.&lt;/li&gt; 
 &lt;li&gt;üîÑ &lt;strong&gt;Large-scale continual pre-training on agentic data&lt;/strong&gt;: Leveraging diverse, high-quality agentic interaction data to extend model capabilities, maintain freshness, and strengthen reasoning performance.&lt;/li&gt; 
 &lt;li&gt;üîÅ &lt;strong&gt;End-to-end reinforcement learning&lt;/strong&gt;: We employ a strictly on-policy RL approach based on a customized Group Relative Policy Optimization framework, with token-level policy gradients, leave-one-out advantage estimation, and selective filtering of negative samples to stabilize training in a non‚Äëstationary environment.&lt;/li&gt; 
 &lt;li&gt;ü§ñ &lt;strong&gt;Agent Inference Paradigm Compatibility&lt;/strong&gt;: At inference, Tongyi DeepResearch is compatible with two inference paradigms: ReAct, for rigorously evaluating the model's core intrinsic abilities, and an IterResearch-based 'Heavy' mode, which uses a test-time scaling strategy to unlock the model's maximum performance ceiling.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Model Download&lt;/h1&gt; 
&lt;p&gt;You can directly download the model by following the links below.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Model&lt;/th&gt; 
   &lt;th align="center"&gt;Download Links&lt;/th&gt; 
   &lt;th align="center"&gt;Model Size&lt;/th&gt; 
   &lt;th align="center"&gt;Context Length&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Tongyi-DeepResearch-30B-A3B&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B"&gt;ü§ó HuggingFace&lt;/a&gt;&lt;br /&gt; &lt;a href="https://modelscope.cn/models/iic/Tongyi-DeepResearch-30B-A3B"&gt;ü§ñ ModelScope&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;30B-A3B&lt;/td&gt; 
   &lt;td align="center"&gt;128K&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;News&lt;/h1&gt; 
&lt;p&gt;[2025/09/17]üî• We have released &lt;strong&gt;Tongyi-DeepResearch-30B-A3B&lt;/strong&gt;.&lt;/p&gt; 
&lt;h1&gt;Deep Research Benchmark Results&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/benchmark.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;This guide provides instructions for setting up the environment and running inference scripts located in the &lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/inference/"&gt;inference&lt;/a&gt; folder.&lt;/p&gt; 
&lt;h3&gt;1. Environment Setup&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Recommended Python version: &lt;strong&gt;3.10.0&lt;/strong&gt; (using other versions may cause dependency issues).&lt;/li&gt; 
 &lt;li&gt;It is strongly advised to create an isolated environment using &lt;code&gt;conda&lt;/code&gt; or &lt;code&gt;virtualenv&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Example with Conda
conda create -n react_infer_env python=3.10.0 
conda activate react_infer_env
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Installation&lt;/h3&gt; 
&lt;p&gt;Install the required dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Prepare Evaluation Data&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create a folder named &lt;code&gt;eval_data/&lt;/code&gt; in the project root.&lt;/li&gt; 
 &lt;li&gt;Place your QA file in &lt;strong&gt;JSONL&lt;/strong&gt; format inside this directory, e.g. &lt;code&gt;eval_data/example.jsonl&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Each line must be a JSON object that includes &lt;strong&gt;both&lt;/strong&gt; of the following keys: &lt;pre&gt;&lt;code class="language-json"&gt;{"question": "...","answer": "..."}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;A sample file is provided in the &lt;code&gt;eval_data&lt;/code&gt; folder for reference.&lt;/li&gt; 
 &lt;li&gt;If you plan to use the &lt;em&gt;file parser&lt;/em&gt; tool, &lt;strong&gt;prepend the file name to the &lt;code&gt;question&lt;/code&gt; field&lt;/strong&gt; and place the referenced file inside the &lt;code&gt;eval_data/file_corpus/&lt;/code&gt; directory.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;4. Configure the Inference Script&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Open &lt;code&gt;run_react_infer.sh&lt;/code&gt; and modify the following variables as instructed in the comments: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;MODEL_PATH&lt;/code&gt; - path to the local or remote model weights.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;DATASET&lt;/code&gt; - path to the evaluation set, e.g. &lt;code&gt;example&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;OUTPUT_PATH&lt;/code&gt; - path for saving the prediction results, e.g. &lt;code&gt;./outputs&lt;/code&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Depending on the tools you enable (retrieval, calculator, web search, etc.), provide the required &lt;code&gt;API_KEY&lt;/code&gt;, &lt;code&gt;BASE_URL&lt;/code&gt;, or other credentials. Each key is explained inline in the bash script.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;5. Run the Inference Script&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;bash run_react_infer.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;With these steps, you can fully prepare the environment, configure the dataset, and run the model. For more details, consult the inline comments in each script or open an issue.&lt;/p&gt; 
&lt;h2&gt;Benchmark Evaluation&lt;/h2&gt; 
&lt;p&gt;We provide benchmark evaluation scripts for various datasets. Please refer to the &lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/evaluation/"&gt;evaluation scripts&lt;/a&gt; directory for more details.&lt;/p&gt; 
&lt;h2&gt;Deep Research Agent Family&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/family.png" /&gt; &lt;/p&gt; 
&lt;p&gt;Tongyi DeepResearch also has an extensive deep research agent family. You can find more information in the following paper:&lt;/p&gt; 
&lt;p&gt;[1] &lt;a href="https://arxiv.org/pdf/2501.07572"&gt;WebWalker: Benchmarking LLMs in Web Traversal&lt;/a&gt;&lt;br /&gt; [2] &lt;a href="https://arxiv.org/pdf/2505.22648"&gt;WebDancer: Towards Autonomous Information Seeking Agency&lt;/a&gt;&lt;br /&gt; [3] &lt;a href="https://arxiv.org/pdf/2507.02592"&gt;WebSailor: Navigating Super-human Reasoning for Web Agent&lt;/a&gt;&lt;br /&gt; [4] &lt;a href="https://arxiv.org/pdf/2507.15061"&gt;WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization&lt;/a&gt;&lt;br /&gt; [5] &lt;a href="https://arxiv.org/pdf/2508.05748"&gt;WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent&lt;/a&gt;&lt;br /&gt; [6] &lt;a href="https://arxiv.org/pdf/2509.13309"&gt;WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents&lt;/a&gt;&lt;br /&gt; [7] &lt;a href="https://arxiv.org/pdf/2509.13313"&gt;ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization&lt;/a&gt;&lt;br /&gt; [8] &lt;a href="https://arxiv.org/pdf/2509.13312"&gt;WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research&lt;/a&gt;&lt;br /&gt; [9] &lt;a href="https://arxiv.org/pdf/2509.13305"&gt;WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning&lt;/a&gt;&lt;br /&gt; [10] &lt;a href="https://arxiv.org/pdf/2509.13310"&gt;Scaling Agents via Continual Pre-training&lt;/a&gt;&lt;br /&gt; [11] &lt;a href="https://arxiv.org/pdf/2509.13311"&gt;Towards General Agentic Intelligence via Environment Scaling&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üåü Misc&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://www.star-history.com/#Alibaba-NLP/DeepResearch&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Alibaba-NLP/DeepResearch&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üö© Talent Recruitment&lt;/h2&gt; 
&lt;p&gt;üî•üî•üî• We are hiring! Research intern positions are open (based in Hangzhou„ÄÅBeijing„ÄÅShanghai)&lt;/p&gt; 
&lt;p&gt;üìö &lt;strong&gt;Research Area&lt;/strong&gt;ÔºöWeb Agent, Search Agent, Agent RL, MultiAgent RL, Agentic RAG&lt;/p&gt; 
&lt;p&gt;‚òéÔ∏è &lt;strong&gt;Contact&lt;/strong&gt;Ôºö&lt;a href=""&gt;yongjiang.jy@alibaba-inc.com&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contact Information&lt;/h2&gt; 
&lt;p&gt;For communications, please contact Yong Jiang (&lt;a href="mailto:yongjiang.jy@alibaba-inc.com"&gt;yongjiang.jy@alibaba-inc.com&lt;/a&gt;).&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{tongyidr,
  author={Tongyi DeepResearch Team},
  title={Tongyi-DeepResearch},
  year={2025},
  howpublished={\url{https://github.com/Alibaba-NLP/DeepResearch}}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>microsoft/markitdown</title>
      <link>https://github.com/microsoft/markitdown</link>
      <description>&lt;p&gt;Python tool for converting files and office documents to Markdown.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MarkItDown&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/markitdown/"&gt;&lt;img src="https://img.shields.io/pypi/v/markitdown.svg?sanitize=true" alt="PyPI" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/dd/markitdown" alt="PyPI - Downloads" /&gt; &lt;a href="https://github.com/microsoft/autogen"&gt;&lt;img src="https://img.shields.io/badge/Built%20by-AutoGen%20Team-blue" alt="Built by AutoGen Team" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] MarkItDown now offers an MCP (Model Context Protocol) server for integration with LLM applications like Claude Desktop. See &lt;a href="https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp"&gt;markitdown-mcp&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Breaking changes between 0.0.1 to 0.1.0:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Dependencies are now organized into optional feature-groups (further details below). Use &lt;code&gt;pip install 'markitdown[all]'&lt;/code&gt; to have backward-compatible behavior.&lt;/li&gt; 
  &lt;li&gt;convert_stream() now requires a binary file-like object (e.g., a file opened in binary mode, or an io.BytesIO object). This is a breaking change from the previous version, where it previously also accepted text file-like objects, like io.StringIO.&lt;/li&gt; 
  &lt;li&gt;The DocumentConverter class interface has changed to read from file-like streams rather than file paths. &lt;em&gt;No temporary files are created anymore&lt;/em&gt;. If you are the maintainer of a plugin, or custom DocumentConverter, you likely need to update your code. Otherwise, if only using the MarkItDown class or CLI (as in these examples), you should not need to change anything.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;MarkItDown is a lightweight Python utility for converting various files to Markdown for use with LLMs and related text analysis pipelines. To this end, it is most comparable to &lt;a href="https://github.com/deanmalmgren/textract"&gt;textract&lt;/a&gt;, but with a focus on preserving important document structure and content as Markdown (including: headings, lists, tables, links, etc.) While the output is often reasonably presentable and human-friendly, it is meant to be consumed by text analysis tools -- and may not be the best option for high-fidelity document conversions for human consumption.&lt;/p&gt; 
&lt;p&gt;MarkItDown currently supports the conversion from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PDF&lt;/li&gt; 
 &lt;li&gt;PowerPoint&lt;/li&gt; 
 &lt;li&gt;Word&lt;/li&gt; 
 &lt;li&gt;Excel&lt;/li&gt; 
 &lt;li&gt;Images (EXIF metadata and OCR)&lt;/li&gt; 
 &lt;li&gt;Audio (EXIF metadata and speech transcription)&lt;/li&gt; 
 &lt;li&gt;HTML&lt;/li&gt; 
 &lt;li&gt;Text-based formats (CSV, JSON, XML)&lt;/li&gt; 
 &lt;li&gt;ZIP files (iterates over contents)&lt;/li&gt; 
 &lt;li&gt;Youtube URLs&lt;/li&gt; 
 &lt;li&gt;EPubs&lt;/li&gt; 
 &lt;li&gt;... and more!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Why Markdown?&lt;/h2&gt; 
&lt;p&gt;Markdown is extremely close to plain text, with minimal markup or formatting, but still provides a way to represent important document structure. Mainstream LLMs, such as OpenAI's GPT-4o, natively "&lt;em&gt;speak&lt;/em&gt;" Markdown, and often incorporate Markdown into their responses unprompted. This suggests that they have been trained on vast amounts of Markdown-formatted text, and understand it well. As a side benefit, Markdown conventions are also highly token-efficient.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;MarkItDown requires Python 3.10 or higher. It is recommended to use a virtual environment to avoid dependency conflicts.&lt;/p&gt; 
&lt;p&gt;With the standard Python installation, you can create and activate a virtual environment using the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv .venv
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If using &lt;code&gt;uv&lt;/code&gt;, you can create a virtual environment with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv --python=3.12 .venv
source .venv/bin/activate
# NOTE: Be sure to use 'uv pip install' rather than just 'pip install' to install packages in this virtual environment
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are using Anaconda, you can create a virtual environment with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n markitdown python=3.12
conda activate markitdown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install MarkItDown, use pip: &lt;code&gt;pip install 'markitdown[all]'&lt;/code&gt;. Alternatively, you can install it from the source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone git@github.com:microsoft/markitdown.git
cd markitdown
pip install -e 'packages/markitdown[all]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Command-Line&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf &amp;gt; document.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or use &lt;code&gt;-o&lt;/code&gt; to specify the output file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf -o document.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also pipe content:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cat path-to-file.pdf | markitdown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Optional Dependencies&lt;/h3&gt; 
&lt;p&gt;MarkItDown has optional dependencies for activating various file formats. Earlier in this document, we installed all optional dependencies with the &lt;code&gt;[all]&lt;/code&gt; option. However, you can also install them individually for more control. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install 'markitdown[pdf, docx, pptx]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;will install only the dependencies for PDF, DOCX, and PPTX files.&lt;/p&gt; 
&lt;p&gt;At the moment, the following optional dependencies are available:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;[all]&lt;/code&gt; Installs all optional dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[pptx]&lt;/code&gt; Installs dependencies for PowerPoint files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[docx]&lt;/code&gt; Installs dependencies for Word files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[xlsx]&lt;/code&gt; Installs dependencies for Excel files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[xls]&lt;/code&gt; Installs dependencies for older Excel files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[pdf]&lt;/code&gt; Installs dependencies for PDF files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[outlook]&lt;/code&gt; Installs dependencies for Outlook messages&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[az-doc-intel]&lt;/code&gt; Installs dependencies for Azure Document Intelligence&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[audio-transcription]&lt;/code&gt; Installs dependencies for audio transcription of wav and mp3 files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[youtube-transcription]&lt;/code&gt; Installs dependencies for fetching YouTube video transcription&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Plugins&lt;/h3&gt; 
&lt;p&gt;MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown --list-plugins
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To enable plugins use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown --use-plugins path-to-file.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To find available plugins, search GitHub for the hashtag &lt;code&gt;#markitdown-plugin&lt;/code&gt;. To develop a plugin, see &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Azure Document Intelligence&lt;/h3&gt; 
&lt;p&gt;To use Microsoft Document Intelligence for conversion:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf -o document.md -d -e "&amp;lt;document_intelligence_endpoint&amp;gt;"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;More information about how to set up an Azure Document Intelligence Resource can be found &lt;a href="https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/create-document-intelligence-resource?view=doc-intel-4.0.0"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Python API&lt;/h3&gt; 
&lt;p&gt;Basic usage in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown

md = MarkItDown(enable_plugins=False) # Set to True to enable plugins
result = md.convert("test.xlsx")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Document Intelligence conversion in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown

md = MarkItDown(docintel_endpoint="&amp;lt;document_intelligence_endpoint&amp;gt;")
result = md.convert("test.pdf")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use Large Language Models for image descriptions (currently only for pptx and image files), provide &lt;code&gt;llm_client&lt;/code&gt; and &lt;code&gt;llm_model&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown
from openai import OpenAI

client = OpenAI()
md = MarkItDown(llm_client=client, llm_model="gpt-4o", llm_prompt="optional custom prompt")
result = md.convert("example.jpg")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;docker build -t markitdown:latest .
docker run --rm -i markitdown:latest &amp;lt; ~/your-file.pdf &amp;gt; output.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h3&gt;How to Contribute&lt;/h3&gt; 
&lt;p&gt;You can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as 'open for contribution' and 'open for reviewing' to help facilitate community contributions. These are of course just suggestions and you are welcome to contribute in any way you like.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;/th&gt; 
    &lt;th&gt;All&lt;/th&gt; 
    &lt;th&gt;Especially Needs Help from Community&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Issues&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/issues"&gt;All Issues&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/issues?q=is%3Aissue+is%3Aopen+label%3A%22open+for+contribution%22"&gt;Issues open for contribution&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;PRs&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/pulls"&gt;All PRs&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/pulls?q=is%3Apr+is%3Aopen+label%3A%22open+for+reviewing%22"&gt;PRs open for reviewing&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h3&gt;Running Tests and Checks&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Navigate to the MarkItDown package:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;cd packages/markitdown
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;code&gt;hatch&lt;/code&gt; in your environment and run tests:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/
hatch shell
hatch test
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(Alternative) Use the Devcontainer which has all the dependencies installed:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;# Reopen the project in Devcontainer and run:
hatch test
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run pre-commit checks before submitting a PR: &lt;code&gt;pre-commit run --all-files&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing 3rd-party Plugins&lt;/h3&gt; 
&lt;p&gt;You can also contribute by creating and sharing 3rd party plugins. See &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Trademarks&lt;/h2&gt; 
&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general"&gt;Microsoft's Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>WECENG/ticket-purchase</title>
      <link>https://github.com/WECENG/ticket-purchase</link>
      <description>&lt;p&gt;Â§ßÈ∫¶Ëá™Âä®Êä¢Á•®ÔºåÊîØÊåÅ‰∫∫Âëò„ÄÅÂüéÂ∏Ç„ÄÅÊó•ÊúüÂú∫Ê¨°„ÄÅ‰ª∑Ê†ºÈÄâÊã©&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Â§ßÈ∫¶Êä¢Á•®ËÑöÊú¨ V1.0&lt;/h1&gt; 
&lt;h3&gt;ÁâπÂæÅ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Ëá™Âä®Êó†Âª∂Êó∂Êä¢Á•®&lt;/li&gt; 
 &lt;li&gt;ÊîØÊåÅ‰∫∫Âëò„ÄÅÂüéÂ∏Ç„ÄÅÊó•ÊúüÂú∫Ê¨°„ÄÅ‰ª∑Ê†ºÈÄâÊã©&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ÂäüËÉΩ‰ªãÁªç&lt;/h2&gt; 
&lt;p&gt;ÈÄöËøáseleniumÊâìÂºÄÈ°µÈù¢ËøõË°åÁôªÂΩïÔºåÊ®°ÊãüÁî®Êà∑Ë¥≠Á•®ÊµÅÁ®ãËá™Âä®Ë¥≠Á•®&lt;/p&gt; 
&lt;p&gt;ÂÖ∂ÊµÅÁ®ãÂõæÂ¶Ç‰∏ã:&lt;/p&gt; 
&lt;img src="img/Â§ßÈ∫¶Êä¢Á•®ÊµÅÁ®ã.png" width="50%" height="50%" /&gt; 
&lt;h2&gt;ÂáÜÂ§áÂ∑•‰Ωú&lt;/h2&gt; 
&lt;h3&gt;1. ÈÖçÁΩÆÁéØÂ¢É&lt;/h3&gt; 
&lt;h4&gt;1.1ÂÆâË£Öpython3ÁéØÂ¢É&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;ËÆøÈóÆPythonÂÆòÊñπÁΩëÁ´ôÔºö&lt;a href="https://www.python.org/downloads/windows/"&gt;https://www.python.org/downloads/windows/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;‰∏ãËΩΩÊúÄÊñ∞ÁöÑPython 3.9+ÁâàÊú¨ÁöÑÂÆâË£ÖÁ®ãÂ∫è„ÄÇ&lt;/li&gt; 
 &lt;li&gt;ËøêË°åÂÆâË£ÖÁ®ãÂ∫è„ÄÇ&lt;/li&gt; 
 &lt;li&gt;Âú®ÂÆâË£ÖÁ®ãÂ∫è‰∏≠ÔºåÁ°Æ‰øùÂãæÈÄâ "Add Python X.X to PATH" ÈÄâÈ°πÔºåËøôÂ∞ÜËá™Âä®Â∞ÜPythonÊ∑ªÂä†Âà∞Á≥ªÁªüÁéØÂ¢ÉÂèòÈáè‰∏≠ÔºåÊñπ‰æøÂú®ÂëΩ‰ª§Ë°å‰∏≠‰ΩøÁî®Python„ÄÇ&lt;/li&gt; 
 &lt;li&gt;ÂÆåÊàêÂÆâË£ÖÂêéÔºå‰Ω†ÂèØ‰ª•Âú®ÂëΩ‰ª§ÊèêÁ§∫Á¨¶ÊàñPowerShell‰∏≠ËæìÂÖ• &lt;code&gt;python3&lt;/code&gt; Êù•ÂêØÂä®PythonËß£ÈáäÂô®„ÄÇ&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;macOS&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;‰Ω†ÂèØ‰ª•‰ΩøÁî®HomebrewÊù•ÂÆâË£ÖPython 3„ÄÇ&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;ÂÆâË£ÖHomebrewÔºàÂ¶ÇÊûúÊú™ÂÆâË£ÖÔºâÔºöÊâìÂºÄÁªàÁ´ØÂπ∂ËøêË°å‰ª•‰∏ãÂëΩ‰ª§Ôºö&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;ÂÆâË£ÖPython 3ÔºöËøêË°å‰ª•‰∏ãÂëΩ‰ª§Êù•ÂÆâË£ÖPython 3Ôºö&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;brew install python@3
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;1.2 ÂÆâË£ÖÊâÄÈúÄË¶ÅÁöÑÁéØÂ¢É&lt;/h4&gt; 
&lt;p&gt;Âú®ÂëΩ‰ª§Á™óÂè£ËæìÂÖ•Â¶Ç‰∏ãÊåá‰ª§&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip3 install selenium
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;1.3 ‰∏ãËΩΩgoogle chromeÊµèËßàÂô®&lt;/h4&gt; 
&lt;p&gt;‰∏ãËΩΩÂú∞ÂùÄ: &lt;a href="https://www.google.cn/intl/zh-CN/chrome/?brand=YTUH&amp;amp;gclid=Cj0KCQjwj5mpBhDJARIsAOVjBdoV_1sBwdqKGHV3rUU1vJmNKZdy5QNzbRT8F5O0-_jq1WHXurE8a7MaAkWrEALw_wcB&amp;amp;gclsrc=aw.ds"&gt;https://www.google.cn/intl/zh-CN/chrome/?brand=YTUH&amp;amp;gclid=Cj0KCQjwj5mpBhDJARIsAOVjBdoV_1sBwdqKGHV3rUU1vJmNKZdy5QNzbRT8F5O0-_jq1WHXurE8a7MaAkWrEALw_wcB&amp;amp;gclsrc=aw.ds&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;2. ‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂&lt;/h3&gt; 
&lt;p&gt;Âú®ËøêË°åÁ®ãÂ∫è‰πãÂâçÔºåÈúÄË¶ÅÂÖà‰øÆÊîπ&lt;code&gt;config.json&lt;/code&gt;Êñá‰ª∂„ÄÇËØ•Êñá‰ª∂Áî®‰∫éÊåáÂÆöÁî®Êà∑ÈúÄË¶ÅÊä¢Á•®ÁöÑÁõ∏ÂÖ≥‰ø°ÊÅØÔºåÂåÖÊã¨ÊºîÂî±‰ºöÁöÑÂú∫Ê¨°„ÄÅËßÇÊºîÁöÑ‰∫∫Âëò„ÄÅÂüéÂ∏Ç„ÄÅÊó•Êúü„ÄÅ‰ª∑Ê†ºÁ≠â„ÄÇÊñá‰ª∂ÁªìÊûúÂ¶Ç‰∏ãÂõæÊâÄÁ§∫Ôºö&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/WECENG/ticket-purchase/master/img/config_json.png" width="50%" height="50%" /&gt; 
&lt;h4&gt;2.1 Êñá‰ª∂ÂÜÖÂÆπËØ¥Êòé&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;index_url&lt;/code&gt;‰∏∫Â§ßÈ∫¶ÁΩëÁöÑÂú∞ÂùÄÔºå&lt;strong&gt;Êó†ÈúÄ‰øÆÊîπ&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;login_url&lt;/code&gt;‰∏∫Â§ßÈ∫¶ÁΩëÁöÑÁôªÂΩïÂú∞ÂùÄÔºå&lt;strong&gt;Êó†ÈúÄ‰øÆÊîπ&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;target_url&lt;/code&gt;‰∏∫Áî®Êà∑ÈúÄË¶ÅÊä¢ÁöÑÊºîÂî±‰ºöÁ•®ÁöÑÁõÆÊ†áÂú∞ÂùÄÔºå&lt;strong&gt;ÂæÖ‰øÆÊîπ&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;users&lt;/code&gt;‰∏∫ËßÇÊºî‰∫∫ÁöÑÂßìÂêçÔºå&lt;strong&gt;ËßÇÊºî‰∫∫ÈúÄË¶ÅÁî®Êà∑Âú®ÊâãÊú∫Â§ßÈ∫¶APP‰∏≠ÂÖàÂ°´ÂÜôÂ•ΩÔºåÁÑ∂ÂêéÂÜçÂ°´ÂÖ•ËØ•ÈÖçÁΩÆÊñá‰ª∂‰∏≠&lt;/strong&gt;Ôºå&lt;strong&gt;ÂæÖ‰øÆÊîπ&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;city&lt;/code&gt;‰∏∫ÂüéÂ∏ÇÔºå&lt;strong&gt;Â¶ÇÊûúÁî®Êà∑ÈúÄË¶ÅÊä¢ÁöÑÊºîÂî±‰ºöÁ•®ÈúÄË¶ÅÈÄâÊã©ÂüéÂ∏ÇÔºåËØ∑ÊääÂüéÂ∏ÇÂ°´ÂÖ•Ê≠§Â§Ñ„ÄÇÂ¶ÇÊó†ÈúÄÈÄâÊã©ÔºåÂàô‰∏çÂ°´&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;date&lt;/code&gt;‰∏∫Âú∫Ê¨°Êó•ÊúüÔºå&lt;strong&gt;ÂæÖ‰øÆÊîπÔºåÂèØÂ§öÈÄâ&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;price&lt;/code&gt;‰∏∫Á•®Ê°£ÁöÑ‰ª∑Ê†ºÔºå&lt;strong&gt;ÂæÖ‰øÆÊîπÔºåÂèØÂ§öÈÄâ&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;if_commit_order&lt;/code&gt;‰∏∫ÊòØÂê¶Ë¶ÅËá™Âä®Êèê‰∫§ËÆ¢ÂçïÔºå&lt;strong&gt;ÊîπÊàê true&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;if_listen‰∏∫ÊòØÂê¶ÂõûÊµÅÁõëÂê¨Ôºå&lt;strong&gt;ÊîπÊàêtrue&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;2.2 Á§∫‰æãËØ¥Êòé&lt;/h4&gt; 
&lt;p&gt;ËøõÂÖ•Â§ßÈ∫¶ÁΩë&lt;a href="https://www.damai.cn/%EF%BC%8C%E9%80%89%E6%8B%A9%E4%BD%A0%E9%9C%80%E8%A6%81%E6%8A%A2%E7%A5%A8%E7%9A%84%E6%BC%94%E5%94%B1%E4%BC%9A%E3%80%82%E5%81%87%E8%AE%BE%E5%A6%82%E4%B8%8B%E5%9B%BE%E6%89%80%E7%A4%BA%EF%BC%9A"&gt;https://www.damai.cn/ÔºåÈÄâÊã©‰Ω†ÈúÄË¶ÅÊä¢Á•®ÁöÑÊºîÂî±‰ºö„ÄÇÂÅáËÆæÂ¶Ç‰∏ãÂõæÊâÄÁ§∫Ôºö&lt;/a&gt;&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/WECENG/ticket-purchase/master/img/example.png" width="50%" height="50%" /&gt; 
&lt;p&gt;Êé•‰∏ãÊù•ÊåâÁÖß‰∏ãÂõæÁöÑÊ†áÊ≥®ÂØπÈÖçÁΩÆÊñá‰ª∂ËøõË°å‰øÆÊîπÔºö&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/WECENG/ticket-purchase/master/img/example_detail.png" width="50%" height="50%" /&gt; 
&lt;p&gt;ÊúÄÁªà&lt;code&gt;config.json&lt;/code&gt;ÁöÑÊñá‰ª∂ÂÜÖÂÆπÂ¶Ç‰∏ãÔºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "index_url": "https://www.damai.cn/",
  "login_url": "https://passport.damai.cn/login?ru=https%3A%2F%2Fwww.damai.cn%2F",
  "target_url": "https://detail.damai.cn/item.htm?spm=a2oeg.home.card_0.ditem_1.591b23e1JQGWHg&amp;amp;id=740680932762",
  "users": [
    "ÂêçÂ≠ó1",
    "ÂêçÂ≠ó2"
  ],
  "city": "ÂπøÂ∑û",
  "date": "2023-10-28",
  "price": "1039",
  "if_listen":true,
  "if_commit_order": true
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3.ËøêË°åÁ®ãÂ∫è&lt;/h3&gt; 
&lt;p&gt;ËøêË°åÁ®ãÂ∫èÂºÄÂßãÊä¢Á•®ÔºåËøõÂÖ•ÂëΩ‰ª§Á™óÂè£ÔºåÊâßË°åÂ¶Ç‰∏ãÂëΩ‰ª§Ôºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cd damai
python3 damai.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Â§ßÈ∫¶appÊä¢Á•®&lt;/h1&gt; 
&lt;p&gt;Â§ßÈ∫¶appÊä¢Á•®ËÑöÊú¨ÈúÄË¶Å‰æùËµñappiumÔºåÂõ†Ê≠§ÈúÄË¶ÅÁé∞Âú®ÂÆâË£Öappium server&amp;amp;clientÁéØÂ¢ÉÔºåÊ≠•È™§Â¶Ç‰∏ãÔºö&lt;/p&gt; 
&lt;h2&gt;appium server&lt;/h2&gt; 
&lt;h3&gt;‰∏ãËΩΩ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;ÂÖàÂÆâË£ÖÂ•ΩnodeÁéØÂ¢ÉÔºàÂÖ∑Â§ánpmÔºânodeÁâàÊú¨Âè∑18.0.0&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ÂÖà‰∏ãËΩΩÂπ∂ÂÆâË£ÖÂ•Ωandroid sdkÔºåÂπ∂ÈÖçÁΩÆÁéØÂ¢ÉÂèòÈáèÔºàappium serverËøêË°åÈúÄ‰æùËµñandroid sdk)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‰∏ãËΩΩappium&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;npm install -g appium
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Êü•ÁúãappiumÊòØÂê¶ÂÆâË£ÖÊàêÂäü&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;appium -v
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‰∏ãËΩΩUiAutomator2È©±Âä®&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;npm install appium-uiautomator2-driver
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;‚Äã ÂèØËÉΩ‰ºöÈÅáÂà∞Â¶Ç‰∏ãÈîôËØØÔºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-tex"&gt;‚ûú  xcode git:(master) ‚úó npm install appium-uiautomator2-driver

npm ERR! code 1
npm ERR! path /Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver/node_modules/appium-chromedriver
npm ERR! command failed
npm ERR! command sh -c node install-npm.js
npm ERR! [11:57:54] Error installing Chromedriver: Request failed with status code 404
npm ERR! [11:57:54] AxiosError: Request failed with status code 404
npm ERR!     at settle (/Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver/node_modules/axios/lib/core/settle.js:19:12)
npm ERR!     at IncomingMessage.handleStreamEnd (/Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver/node_modules/axios/lib/adapters/http.js:572:11)
npm ERR!     at IncomingMessage.emit (node:events:539:35)
npm ERR!     at endReadableNT (node:internal/streams/readable:1344:12)
npm ERR!     at processTicksAndRejections (node:internal/process/task_queues:82:21)
npm ERR! [11:57:54] Downloading Chromedriver can be skipped by setting the'APPIUM_SKIP_CHROMEDRIVER_INSTALL' environment variable.

npm ERR! A complete log of this run can be found in:
npm ERR!     /Users/chenweicheng/.npm/_logs/2023-10-26T03_57_35_950Z-debug-0.log
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;‚Äã Ëß£ÂÜ≥ÂäûÊ≥ïÔºàÊ∑ªÂä†ÁéØÂ¢ÉÂèòÈáèÔºåÈîôËØØÂéüÂõ†ÊòØÊ≤°ÊúâÊâæÂà∞chromeÊµèËßàÂô®È©±Âä®ÔºåÂøΩÁï•Âç≥ÂèØÔºâ&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;export APPIUM_SKIP_CHROMEDRIVER_INSTALL=true
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ÂêØÂä®&lt;/h3&gt; 
&lt;p&gt;ÂêØÂä®appium serverÂπ∂‰ΩøÁî®uiautomator2È©±Âä®&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;appium --use-plugins uiautomator2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ÂêØÂä®ÊàêÂäüÂ∞ÜÂá∫Áé∞Â¶Ç‰∏ã‰ø°ÊÅØÔºö&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;[Appium] Welcome to Appium v2.2.1 (REV 2176894a5be5da17a362bf3f20678641a78f4b69)
[Appium] Non-default server args:
[Appium] {
[Appium]   usePlugins: [
[Appium]     'uiautomator2'
[Appium]   ]
[Appium] }
[Appium] Attempting to load driver uiautomator2...
[Appium] Requiring driver at /Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver
[Appium] Appium REST http interface listener started on http://0.0.0.0:4723
[Appium] You can provide the following URLs in your client code to connect to this server:
[Appium] 	http://127.0.0.1:4723/ (only accessible from the same host)
[Appium] 	http://172.31.102.45:4723/
[Appium] 	http://198.18.0.1:4723/
[Appium] Available drivers:
[Appium]   - uiautomator2@2.32.3 (automationName 'UiAutomator2')
[Appium] No plugins have been installed. Use the "appium plugin" command to install the one(s) you want to use.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ÂÖ∂‰∏≠&lt;code&gt;[Appium] http://127.0.0.1:4723/ (only accessible from the same host) [Appium] http://172.31.102.45:4723/ [Appium] http://198.18.0.1:4723/&lt;/code&gt;‰∏∫appium serverËøûÊé•Âú∞ÂùÄ&lt;/p&gt; 
&lt;h2&gt;appium client&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;ÂÖà‰∏ãËΩΩÂπ∂ÂÆâË£ÖÂ•Ωpython3Âíåpip3&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ÂÆâË£Ö&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;pip3 install appium-python-client
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Âú®‰ª£Á†Å‰∏≠ÂºïÂÖ•Âπ∂‰ΩøÁî®appium&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;from appium import webdriver
from appium.options.common.base import AppiumOptions

device_app_info = AppiumOptions()
device_app_info.set_capability('platformName', 'Android')
device_app_info.set_capability('platformVersion', '10')
device_app_info.set_capability('deviceName', 'YourDeviceName')
device_app_info.set_capability('appPackage', 'cn.damai')
device_app_info.set_capability('appActivity', '.launcher.splash.SplashMainActivity')
device_app_info.set_capability('unicodeKeyboard', True)
device_app_info.set_capability('resetKeyboard', True)
device_app_info.set_capability('noReset', True)
device_app_info.set_capability('newCommandTimeout', 6000)
device_app_info.set_capability('automationName', 'UiAutomator2')

# ËøûÊé•appium serverÔºåserverÂú∞ÂùÄÊü•ÁúãappiumÂêØÂä®‰ø°ÊÅØ
driver = webdriver.Remote('http://127.0.0.1:4723', options=device_app_info)

&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ÂêØÂä®ËÑöÊú¨Á®ãÂ∫è&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;cd damai_appium
python3 damai_appium.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>laude-institute/terminal-bench</title>
      <link>https://github.com/laude-institute/terminal-bench</link>
      <description>&lt;p&gt;A benchmark for LLMs on complicated tasks in the terminal&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;terminal-bench&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;#####################################################################
#  _____                   _             _     ______________       #
# |_   _|__ _ __ _ __ ___ (_)_ __   __ _| |   ||            ||      #
#   | |/ _ \ '__| '_ ` _ \| | '_ \ / _` | |   || &amp;gt;          ||      #
#   | |  __/ |  | | | | | | | | | | (_| | |   ||            ||      #
#   |_|\___|_|  |_| |_| |_|_|_| |_|\__,_|_|   ||____________||      #
#   ____                  _                   |______________|      #
#  | __ )  ___ _ __   ___| |__                 \\############\\     #
#  |  _ \ / _ \ '_ \ / __| '_ \                 \\############\\    # 
#  | |_) |  __/ | | | (__| | | |                 \      ____    \   #
#  |____/ \___|_| |_|\___|_| |_|                  \_____\___\____\  #
#                                                                   #
#####################################################################
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/6xWPKhGDbA"&gt;&lt;img src="https://img.shields.io/badge/Join_our_discord-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://github.com/laude-institute/terminal-bench"&gt;&lt;img src="https://img.shields.io/badge/T--Bench-000000?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=000&amp;amp;logoColor=white" alt="Github" /&gt;&lt;/a&gt; &lt;a href="https://www.tbench.ai/docs"&gt;&lt;img src="https://img.shields.io/badge/Docs-000000?style=for-the-badge&amp;amp;logo=mdbook&amp;amp;color=105864" alt="Docs" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Terminal-Bench is the benchmark for testing AI agents in real terminal environments. From compiling code to training models and setting up servers, Terminal-Bench evaluates how well agents can handle real-world, end-to-end tasks - autonomously.&lt;/p&gt; 
&lt;p&gt;Whether you're building LLM agents, benchmarking frameworks, or stress-testing system-level reasoning, Terminal-Bench gives you a reproducible task suite and execution harness designed for practical, real-world evaluation.&lt;/p&gt; 
&lt;p&gt;Terminal-Bench consists of two parts: a &lt;strong&gt;dataset of tasks&lt;/strong&gt;, and an &lt;strong&gt;execution harness&lt;/strong&gt; that connects a language model to our terminal sandbox.&lt;/p&gt; 
&lt;p&gt;Terminal-Bench is currently in &lt;strong&gt;beta&lt;/strong&gt; with ~100 tasks. Over the coming months, we are going to expand Terminal-Bench into comprehensive testbed for AI agents in text-based environments. Any contributions are welcome, especially new and challenging tasks!&lt;/p&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;Our &lt;a href="https://www.tbench.ai/docs/installation"&gt;Quickstart Guide&lt;/a&gt; will walk you through installing the repo and &lt;a href="https://www.tbench.ai/docs/contributing"&gt;contributing&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Terminal-Bench is distributed as a pip package and can be run using the Terminal-Bench CLI: &lt;code&gt;tb&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv tool install terminal-bench
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install terminal-bench
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Further Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.tbench.ai/tasks"&gt;Task Gallery&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.tbench.ai/docs/task-ideas"&gt;Task Ideas&lt;/a&gt; - Browse community-sourced task ideas&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.tbench.ai/docs/dashboard"&gt;Dashboard Documentation&lt;/a&gt; - Information about the Terminal-Bench dashboard&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Core Components&lt;/h2&gt; 
&lt;h3&gt;Dataset of Tasks&lt;/h3&gt; 
&lt;p&gt;Each task in Terminal-Bench includes&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;an instruction in English,&lt;/li&gt; 
 &lt;li&gt;a test script to verify if the language model / agent completed the task successfully,&lt;/li&gt; 
 &lt;li&gt;a reference ("oracle") solution that solves the task.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Tasks are located in the &lt;a href="https://raw.githubusercontent.com/laude-institute/terminal-bench/main/tasks"&gt;&lt;code&gt;tasks&lt;/code&gt;&lt;/a&gt; folder of the repository, and the aforementioned list of current tasks gives an overview that is easy to browse.&lt;/p&gt; 
&lt;h3&gt;Execution Harness&lt;/h3&gt; 
&lt;p&gt;The harness connects language models to a sandboxed terminal environment. After &lt;a href="https://www.tbench.ai/docs/installation"&gt;installing the terminal-bench package&lt;/a&gt; (along with the dependencies &lt;code&gt;uv&lt;/code&gt; and &lt;code&gt;Docker&lt;/code&gt;) you can view how to run the harness using:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tb run --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For detailed information about running the harness and its options, see the &lt;a href="https://www.tbench.ai/docs/first-steps"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Submit to Our Leaderboard&lt;/h3&gt; 
&lt;p&gt;Terminal-Bench-Core v0.1.1 is the set of tasks for Terminal-Bench's beta release and corresponds to the current leaderboard. To evaluate on it pass &lt;code&gt;--dataset-name terminal-bench-core&lt;/code&gt; and &lt;code&gt;--dataset-version 0.1.1&lt;/code&gt; to the harness. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tb run \
    --agent terminus \
    --model anthropic/claude-3-7-latest \
    --dataset-name terminal-bench-core
    --dataset-version 0.1.1
    --n-concurrent 8
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more detailed instructions on submitting to the leaderboard, view our &lt;a href="https://www.tbench.ai/docs/submitting-to-leaderboard"&gt;leaderboard submission guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For more information on Terminal-Bench datasets and versioning view our &lt;a href="https://www.tbench.ai/docs/registry"&gt;registry overview&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;h3&gt;Creating New Tasks&lt;/h3&gt; 
&lt;p&gt;View our &lt;a href="https://www.tbench.ai/docs/task-quickstart"&gt;task contribution quickstart&lt;/a&gt; to create a new task.&lt;/p&gt; 
&lt;h3&gt;Creating New Adapters&lt;/h3&gt; 
&lt;p&gt;View &lt;a href="https://www.tbench.ai/docs/adapters"&gt;How to create a new adapter for a new benchmark&lt;/a&gt; to contribute a new adapter.&lt;/p&gt; 
&lt;h2&gt;Citing Us&lt;/h2&gt; 
&lt;p&gt;If you found Terminal-Bench useful, please cite us as:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{tbench_2025,
      title={Terminal-Bench: A Benchmark for AI Agents in Terminal Environments}, 
      url={https://github.com/laude-institute/terminal-bench}, 
      author={The Terminal-Bench Team}, year={2025}, month={Apr}} 
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>NVIDIA/Megatron-LM</title>
      <link>https://github.com/NVIDIA/Megatron-LM</link>
      <description>&lt;p&gt;Ongoing research training transformer models at scale&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;Megatron-LM &amp;amp; Megatron Core&lt;/h1&gt; 
 &lt;h4&gt;GPU-optimized library for training transformer models at scale&lt;/h4&gt; 
 &lt;p&gt;&lt;a href="https://docs.nvidia.com/Megatron-Core/developer-guide/latest/index.html"&gt;&lt;img src="https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/CHANGELOG.md"&gt;&lt;img src="https://img.shields.io/badge/release-0.12.0-green" alt="version" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-Apache-blue" alt="license" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;div align="left"&gt; 
  &lt;h2&gt;‚ö° Quick Start&lt;/h2&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Install Megatron Core with required dependencies
pip install megatron-core
pip install --no-build-isolation transformer-engine[pytorch]

# 2. Clone repository for examples
git clone https://github.com/NVIDIA/Megatron-LM.git
cd Megatron-LM
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;&lt;strong&gt;‚Üí &lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#installation"&gt;Complete Installation Guide&lt;/a&gt;&lt;/strong&gt; - Docker, pip variants (dev,lts,etc.), source installation, and system requirements&lt;/p&gt; 
  &lt;h1&gt;Latest News&lt;/h1&gt; 
  &lt;ul&gt; 
   &lt;li&gt;üîÑ NEW! &lt;strong&gt;&lt;a href="https://github.com/NVIDIA-NeMo/Megatron-Bridge"&gt;Megatron Bridge&lt;/a&gt;&lt;/strong&gt; - Bidirectional converter for interoperability between Hugging Face and Megatron checkpoints, featuring production-ready recipes for popular models.&lt;/li&gt; 
   &lt;li&gt;üó∫Ô∏è &lt;strong&gt;&lt;a href="https://github.com/NVIDIA/Megatron-LM/issues/1729"&gt;MoE Q3-Q4 2025 Roadmap&lt;/a&gt;&lt;/strong&gt; - Comprehensive roadmap for MoE features including DeepSeek-V3, Qwen3, advanced parallelism strategies, FP8 optimizations, and Blackwell performance enhancements.&lt;/li&gt; 
   &lt;li&gt;üöÄ &lt;strong&gt;&lt;a href="https://github.com/NVIDIA/Megatron-LM/issues/1739"&gt;GPT-OSS Implementation&lt;/a&gt;&lt;/strong&gt; - Advanced features including YaRN RoPE scaling, attention sinks, and custom activation functions are being integrated into Megatron Core.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;[2025/06]&lt;/strong&gt; &lt;strong&gt;&lt;a href="https://github.com/yanring/Megatron-MoE-ModelZoo"&gt;Megatron MoE Model Zoo&lt;/a&gt;&lt;/strong&gt; - Best practices and optimized configurations for training DeepSeek-V3, Mixtral, and Qwen3 MoE models with performance benchmarking and checkpoint conversion tools.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;[2025/05]&lt;/strong&gt; Megatron Core v0.11.0 brings new capabilities for multi-data center LLM training (&lt;a href="https://developer.nvidia.com/blog/turbocharge-llm-training-across-long-haul-data-center-networks-with-nvidia-nemo-framework/"&gt;blog&lt;/a&gt;).&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Previous News&lt;/summary&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;[2024/07]&lt;/strong&gt; Megatron Core v0.7 improves scalability and training resiliency and adds support for multimodal training (&lt;a href="https://developer.nvidia.com/blog/train-generative-ai-models-more-efficiently-with-new-nvidia-Megatron-Core-functionalities/"&gt;blog&lt;/a&gt;).&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;[2024/06]&lt;/strong&gt; Megatron Core added supports for Mamba-based models. Check out our paper &lt;a href="https://arxiv.org/pdf/2406.07887"&gt;An Empirical Study of Mamba-based Language Models&lt;/a&gt; and &lt;a href="https://github.com/NVIDIA/Megatron-LM/tree/ssm/examples/mamba"&gt;code example&lt;/a&gt;.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;[2024/01 Announcement]&lt;/strong&gt; NVIDIA has released the core capabilities in &lt;strong&gt;Megatron-LM&lt;/strong&gt; into &lt;a href="https://github.com/NVIDIA/Megatron-LM/tree/main/megatron/core"&gt;&lt;strong&gt;Megatron Core&lt;/strong&gt;&lt;/a&gt; in this repository. Megatron Core expands upon Megatron-LM's GPU-optimized techniques with more cutting-edge innovations on system-level optimizations, featuring composable and modular APIs. Explore the [Megatron Core intro](#Megatron Core) for more details.&lt;/li&gt; 
   &lt;/ul&gt; 
  &lt;/details&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Table of Contents&lt;/summary&gt; 
   &lt;p&gt;&lt;strong&gt;Getting Started&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#-quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#latest-news"&gt;Latest News&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#megatron-overview"&gt;Megatron Overview&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#project-structure"&gt;Project Structure&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#megatron-lm-reference-implementation"&gt;Megatron-LM: Reference Implementation&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#megatron-core-production-library"&gt;Megatron Core: Production Library&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#installation"&gt;Installation&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#-docker-recommended"&gt;Docker (Recommended)&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#-pip-installation"&gt;Pip Installation&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#-source-installation"&gt;Source Installation&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#system-requirements"&gt;System Requirements&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;Core Features&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#performance-benchmarking"&gt;Performance Benchmarking&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#weak-scaling-results"&gt;Weak Scaling Results&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#strong-scaling-results"&gt;Strong Scaling Results&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#ecosystem-libraries"&gt;Ecosystem Libraries&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#training"&gt;Training&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#getting-started"&gt;Getting Started&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#data-preparation"&gt;Data Preparation&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#parallelism-strategies"&gt;Parallelism Strategies&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#data-parallelism-dp"&gt;Data Parallelism (DP)&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#tensor-parallelism-tp"&gt;Tensor Parallelism (TP)&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#pipeline-parallelism-pp"&gt;Pipeline Parallelism (PP)&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#context-parallelism-cp"&gt;Context Parallelism (CP)&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#expert-parallelism-ep"&gt;Expert Parallelism (EP)&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#parallelism-selection-guide"&gt;Parallelism Selection Guide&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#performance-optimizations"&gt;Performance Optimizations&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;Resources&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/examples/"&gt;Examples&lt;/a&gt; - Training scripts and tutorials&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://docs.nvidia.com/Megatron-Core/"&gt;Documentation&lt;/a&gt; - Official docs&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#roadmaps"&gt;Roadmaps&lt;/a&gt; - Development roadmaps and feature tracking&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#-community--support"&gt;Community &amp;amp; Support&lt;/a&gt; - Get help and contribute 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#getting-help"&gt;Getting Help&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/#citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ul&gt; 
  &lt;/details&gt; 
  &lt;h1&gt;Megatron Overview&lt;/h1&gt; 
  &lt;h2&gt;Project Structure&lt;/h2&gt; 
  &lt;pre&gt;&lt;code&gt;Megatron-LM/
‚îú‚îÄ‚îÄ megatron/                    
‚îÇ   ‚îú‚îÄ‚îÄ core/                    # Megatron Core (kernels, parallelism, building blocks)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/              # Transformer models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transformer/         # Transformer building blocks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tensor_parallel/     # Tensor parallelism
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipeline_parallel/   # Pipeline parallelism
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ distributed/         # Distributed training (FSDP, DDP)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ optimizer/           # Optimizers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ datasets/            # Dataset loaders
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inference/           # Inference engines
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ export/              # Model export (e.g. TensorRT-LLM)
‚îÇ   ‚îú‚îÄ‚îÄ training/                # Training scripts
‚îÇ   ‚îú‚îÄ‚îÄ inference/               # Inference server
‚îÇ   ‚îú‚îÄ‚îÄ legacy/                  # Legacy components
‚îÇ   ‚îî‚îÄ‚îÄ post_training/           # Post-training (RLHF, etc.)
‚îú‚îÄ‚îÄ examples/                    # Ready-to-use training examples
‚îú‚îÄ‚îÄ tools/                       # Utility tools
‚îú‚îÄ‚îÄ tests/                       # Comprehensive test suite
‚îî‚îÄ‚îÄ docs/                        # Documentation
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h3&gt;Megatron-LM: Reference Implementation&lt;/h3&gt; 
  &lt;p&gt;&lt;strong&gt;Reference implementation&lt;/strong&gt; that includes Megatron Core plus everything needed to train models.&lt;/p&gt; 
  &lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Training state-of-the-art foundation models&lt;/strong&gt; at scale with cutting-edge performance on latest NVIDIA hardware&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Research teams&lt;/strong&gt; exploring new architectures and training techniques&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Learning distributed training&lt;/strong&gt; concepts and best practices&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Quick experimentation&lt;/strong&gt; with proven model configurations&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;&lt;strong&gt;What you get:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Pre-configured training scripts for GPT, LLama, DeepSeek, Qwen, and more.&lt;/li&gt; 
   &lt;li&gt;End-to-end examples from data prep to evaluation&lt;/li&gt; 
   &lt;li&gt;Research-focused tools and utilities&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;h3&gt;Megatron Core: Composable Library&lt;/h3&gt; 
  &lt;p&gt;&lt;strong&gt;Composable library&lt;/strong&gt; with GPU-optimized building blocks for custom training frameworks.&lt;/p&gt; 
  &lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Framework developers&lt;/strong&gt; building on top of modular and optimized components&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Research teams&lt;/strong&gt; needing custom training loops, optimizers, or data pipelines&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;ML engineers&lt;/strong&gt; requiring fault-tolerant training pipelines&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;&lt;strong&gt;What you get:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Composable transformer building blocks (attention, MLP, etc.)&lt;/li&gt; 
   &lt;li&gt;Advanced parallelism strategies (TP, PP, DP, EP, CP)&lt;/li&gt; 
   &lt;li&gt;Pipeline schedules and distributed optimizers&lt;/li&gt; 
   &lt;li&gt;Mixed precision support (FP16, BF16, FP8)&lt;/li&gt; 
   &lt;li&gt;GPU-optimized kernels and memory management&lt;/li&gt; 
   &lt;li&gt;High-performance dataloaders and dataset utilities&lt;/li&gt; 
   &lt;li&gt;Model architectures (LLaMA, Qwen, GPT, Mixtral, Mamba, etc.)&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;h2&gt;Ecosystem Libraries&lt;/h2&gt; 
  &lt;p&gt;&lt;strong&gt;Libraries used by Megatron Core:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/NVIDIA/Megatron-Energon"&gt;Megatron Energon&lt;/a&gt;&lt;/strong&gt; üì£ &lt;strong&gt;NEW!&lt;/strong&gt; - Multi-modal data loader (text, images, video, audio) with distributed loading and dataset blending&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/NVIDIA/TransformerEngine"&gt;Transformer Engine&lt;/a&gt;&lt;/strong&gt; - Optimized kernels and FP8 mixed precision support&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/NVIDIA/nvidia-resiliency-ext"&gt;Resiliency Extension (NVRx)&lt;/a&gt;&lt;/strong&gt; - Fault tolerant training with failure detection and recovery&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;&lt;strong&gt;Libraries using Megatron Core:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/NVIDIA-NeMo/Megatron-Bridge"&gt;Megatron Bridge&lt;/a&gt;&lt;/strong&gt; - Training library with bidirectional Hugging Face ‚Üî Megatron checkpoint conversion, flexible training loops, and production-ready recipes&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/NVIDIA-NeMo/RL"&gt;NeMo RL&lt;/a&gt;&lt;/strong&gt; - Scalable toolkit for efficient reinforcement learning with RLHF, DPO, and other post-training methods&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/overview.html"&gt;NeMo Framework&lt;/a&gt;&lt;/strong&gt; - Enterprise framework with cloud-native support and end-to-end examples&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/NVIDIA/TensorRT-Model-Optimizer"&gt;TensorRT Model Optimizer (ModelOpt)&lt;/a&gt;&lt;/strong&gt; - Model optimization toolkit for quantization, pruning, and distillation&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;&lt;strong&gt;Compatible with:&lt;/strong&gt; &lt;a href="https://github.com/huggingface/accelerate"&gt;Hugging Face Accelerate&lt;/a&gt;, &lt;a href="https://github.com/hpcaitech/ColossalAI"&gt;Colossal-AI&lt;/a&gt;, &lt;a href="https://github.com/microsoft/DeepSpeed"&gt;DeepSpeed&lt;/a&gt;&lt;/p&gt; 
  &lt;h1&gt;Installation&lt;/h1&gt; 
  &lt;h2&gt;üê≥ Docker (Recommended)&lt;/h2&gt; 
  &lt;p&gt;We strongly recommend using the previous releases of &lt;a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch"&gt;PyTorch NGC Container&lt;/a&gt; rather than the latest one for optimal compatibility with Megatron Core release and testing. Our releases are always based on the previous month's NGC container, so this ensures compatibility and stability.&lt;/p&gt; 
  &lt;p&gt;This container comes with all dependencies pre-installed with compatible versions and optimized configurations for NVIDIA GPUs:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;PyTorch (latest stable version)&lt;/li&gt; 
   &lt;li&gt;CUDA, cuDNN, NCCL (latest stable versions)&lt;/li&gt; 
   &lt;li&gt;Support for FP8 on NVIDIA Hopper, Ada, and Blackwell GPUs&lt;/li&gt; 
   &lt;li&gt;For best performance, use NVIDIA Turing GPU architecture generations and later&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;# Run container with mounted directories
docker run --runtime --nvidia --gpus all -it --rm \
  -v /path/to/megatron:/workspace/megatron \
  -v /path/to/dataset:/workspace/dataset \
  -v /path/to/checkpoints:/workspace/checkpoints \
  nvcr.io/nvidia/pytorch:25.04-py3
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h2&gt;Pip Installation&lt;/h2&gt; 
  &lt;p&gt;Megatron Core offers support for two NGC PyTorch containers:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;dev&lt;/code&gt;: Moving head that supports the most recent upstream dependencies&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;lts&lt;/code&gt;: Long-term support of NGC PyTorch 24.01&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;Both containers can be combined with &lt;code&gt;mlm&lt;/code&gt; which adds package dependencies for Megatron-LM on top of Megatron Core.&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;# Install the latest release with minimal dependencies (no Transformer Engine)
pip install megatron-core[dev]
&lt;/code&gt;&lt;/pre&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;# Install packages for LTS support NGC PyTorch 24.01
pip install megatron-core[lts]
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;For a version of Megatron Core with only torch, run:&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;pip install megatron-core
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;For dependencies required by Megatron-LM, please run:&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;pip install megatron-core[mlm]
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h2&gt;Source Installation&lt;/h2&gt; 
  &lt;p&gt;For development or latest features:&lt;/p&gt; 
  &lt;p&gt;For Hybrid models, Megatron Core requires &lt;a href="https://github.com/state-spaces/mamba"&gt;mamba&lt;/a&gt;. If the pre-built wheel in PyPI does not fit your environment, you can fall back to an install script Megatron Core uses in its CI system. For this, please install &lt;code&gt;uv&lt;/code&gt; first:&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;export UV_VERSION=0.7.2
export PATH="$HOME/.local/bin:$PATH"
curl -LsSf https://astral.sh/uv/${UV_VERSION}/install.sh | sh
export UV_PROJECT_ENVIRONMENT=./venv
export PATH="$UV_PROJECT_ENVIRONMENT/bin:$PATH"
export UV_LINK_MODE=copy
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;Run the following command to build upstream dependencies from source:&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;# Clone and install
git clone https://github.com/NVIDIA/Megatron-LM.git
cd Megatron-LM

# Optional: checkout specific release
git checkout core_r0.13.0

bash docker/common/install.sh --environment {dev,lts}
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h2&gt;System Requirements&lt;/h2&gt; 
  &lt;h3&gt;Hardware Requirements&lt;/h3&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;FP8 Support&lt;/strong&gt;: NVIDIA Hopper, Ada, Blackwell GPUs&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Recommended&lt;/strong&gt;: NVIDIA Turing architecture or later&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;h3&gt;Software Requirements&lt;/h3&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;CUDA/cuDNN/NCCL&lt;/strong&gt;: Latest stable versions&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;PyTorch&lt;/strong&gt;: Latest stable version&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Transformer Engine&lt;/strong&gt;: Latest stable version&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt;: 3.12 recommended&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;h1&gt;Performance Benchmarking&lt;/h1&gt; 
  &lt;p&gt;For our latest performance benchmarking results, please refer to &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance_summary.html"&gt;NVIDIA NeMo Framework Performance Summary&lt;/a&gt;.&lt;/p&gt; 
  &lt;p&gt;Our codebase efficiently trains models from 2B to 462B parameters across thousands of GPUs, achieving up to &lt;strong&gt;47% Model FLOP Utilization (MFU)&lt;/strong&gt; on H100 clusters.&lt;/p&gt; 
  &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/images/model_table.png" alt="Model table" /&gt;&lt;/p&gt; 
  &lt;p&gt;&lt;strong&gt;Benchmark Configuration:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Vocabulary size&lt;/strong&gt;: 131,072 tokens&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Sequence length&lt;/strong&gt;: 4096 tokens&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Model scaling&lt;/strong&gt;: Varied hidden size, attention heads, and layers to achieve target parameter counts&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Communication optimizations&lt;/strong&gt;: Fine-grained overlapping with DP (&lt;code&gt;--overlap-grad-reduce&lt;/code&gt;, &lt;code&gt;--overlap-param-gather&lt;/code&gt;), TP (&lt;code&gt;--tp-comm-overlap&lt;/code&gt;), and PP (enabled by default)&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;&lt;strong&gt;Key Results:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;6144 H100 GPUs&lt;/strong&gt;: Successfully benchmarked 462B parameter model training&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Superlinear scaling&lt;/strong&gt;: MFU increases from 41% to 47-48% with model size&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;End-to-end measurement&lt;/strong&gt;: Throughputs include all operations (data loading, optimizer steps, communication, logging)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Production ready&lt;/strong&gt;: Full training pipeline with checkpointing and fault tolerance&lt;/li&gt; 
   &lt;li&gt;&lt;em&gt;Note: Performance results measured without training to convergence&lt;/em&gt;&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;h2&gt;Weak Scaling Results&lt;/h2&gt; 
  &lt;p&gt;Our weak scaled results show superlinear scaling (MFU increases from 41% for the smallest model considered to 47-48% for the largest models); this is because larger GEMMs have higher arithmetic intensity and are consequently more efficient to execute.&lt;/p&gt; 
  &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/images/weak_scaling.png" alt="Weak scaling" /&gt;&lt;/p&gt; 
  &lt;h2&gt;Strong Scaling Results&lt;/h2&gt; 
  &lt;p&gt;We also strong scaled the standard GPT-3 model (our version has slightly more than 175 billion parameters due to larger vocabulary size) from 96 H100 GPUs to 4608 GPUs, using the same batch size of 1152 sequences throughout. Communication becomes more exposed at larger scale, leading to a reduction in MFU from 47% to 42%.&lt;/p&gt; 
  &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/images/strong_scaling.png" alt="Strong scaling" /&gt;&lt;/p&gt; 
  &lt;h1&gt;Training&lt;/h1&gt; 
  &lt;h2&gt;Getting Started&lt;/h2&gt; 
  &lt;h3&gt;Simple Training Example&lt;/h3&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;# Distributed training example (2 GPUs, mock data)
torchrun --nproc_per_node=2 examples/run_simple_mcore_train_loop.py
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h3&gt;LLama-3 Training Example&lt;/h3&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;# 8 GPUs, FP8 precision, mock data
./examples/llama/train_llama3_8b_fp8.sh
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h2&gt;Data Preparation&lt;/h2&gt; 
  &lt;h3&gt;JSONL Data Format&lt;/h3&gt; 
  &lt;pre&gt;&lt;code class="language-json"&gt;{"text": "Your training text here..."}
{"text": "Another training sample..."}
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h3&gt;Basic Preprocessing&lt;/h3&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;python tools/preprocess_data.py \
    --input data.jsonl \
    --output-prefix processed_data \
    --tokenizer-type HuggingFaceTokenizer \
    --tokenizer-model /path/to/tokenizer.model \
    --workers 8 \
    --append-eod
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h3&gt;Key Arguments&lt;/h3&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;--input&lt;/code&gt;: Path to input JSON/JSONL file&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;--output-prefix&lt;/code&gt;: Prefix for output binary files (.bin and .idx)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;--tokenizer-type&lt;/code&gt;: Tokenizer type (&lt;code&gt;HuggingFaceTokenizer&lt;/code&gt;, &lt;code&gt;GPT2BPETokenizer&lt;/code&gt;, etc.)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;--tokenizer-model&lt;/code&gt;: Path to tokenizer model file&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;--workers&lt;/code&gt;: Number of parallel workers for processing&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;--append-eod&lt;/code&gt;: Add end-of-document token&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;!-- **‚Üí [Complete Data Preparation Guide](./docs/data-preparation.md)** - Comprehensive guide covering advanced preprocessing, dataset collection, deduplication, and optimization strategies --&gt; 
  &lt;h1&gt;Parallelism Strategies&lt;/h1&gt; 
  &lt;h2&gt;Data Parallelism (DP)&lt;/h2&gt; 
  &lt;h3&gt;Standard Data Parallel&lt;/h3&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;# Standard DDP - replicate model on each GPU
torchrun --nproc_per_node=8 pretrain_gpt.py \
    --data-parallel-sharding-strategy no_shard
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h3&gt;Fully Sharded Data Parallel (FSDP)&lt;/h3&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;# Megatron's optimized FSDP (~15% faster than PyTorch FSDP2)
--use-custom-fsdp

# PyTorch FSDP2
--use-torch-fsdp2

# Sharding strategies
--data-parallel-sharding-strategy optim              # Shard optimizer states (ZeRO-1)
--data-parallel-sharding-strategy optim_grads        # Shard gradients + optimizer (ZeRO-2)
--data-parallel-sharding-strategy optim_grads_params # Shard parameters + gradients + optimizer (ZeRO-3)
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h2&gt;Tensor Parallelism (TP)&lt;/h2&gt; 
  &lt;p&gt;Split individual model layers across GPUs:&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;--tensor-model-parallel-size 4  # 4-way tensor parallelism
--sequence-parallel             # Enable sequence parallelism (recommended with TP)
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h2&gt;Pipeline Parallelism (PP)&lt;/h2&gt; 
  &lt;p&gt;Split model depth across GPUs:&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;--pipeline-model-parallel-size 8     # 8 pipeline stages
--virtual-pipeline-model-parallel-size 4  # Virtual pipeline for better load balancing
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h2&gt;Context Parallelism (CP)&lt;/h2&gt; 
  &lt;p&gt;Split long sequences across GPUs for handling long contexts:&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;--context-parallel-size 2                    # 2-way context parallelism
--cp-comm-type p2p                          # Communication: p2p, a2a, allgather, a2a+p2p
--hierarchical-context-parallel-sizes 2 4   # Hierarchical context parallelism
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h2&gt;Expert Parallelism (EP)&lt;/h2&gt; 
  &lt;p&gt;For Mixture of Experts (MoE) models:&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;--expert-model-parallel-size 4  # 4-way expert parallelism
--num-experts 8                 # 8 experts per MoE layer
--moe-grouped-gemm              # Optimize expert computation
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h2&gt;Combining Parallelism Strategies&lt;/h2&gt; 
  &lt;h3&gt;Parallelism Selection Guide&lt;/h3&gt; 
  &lt;p&gt;Based on &lt;a href="https://github.com/NVIDIA/NeMo/tree/main/scripts/performance/recommended_model_configs"&gt;NVIDIA NeMo production configurations&lt;/a&gt;:&lt;/p&gt; 
  &lt;table&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th&gt;Model&lt;/th&gt; 
     &lt;th&gt;Size&lt;/th&gt; 
     &lt;th&gt;GPUs&lt;/th&gt; 
     &lt;th&gt;TP&lt;/th&gt; 
     &lt;th&gt;PP&lt;/th&gt; 
     &lt;th&gt;CP&lt;/th&gt; 
     &lt;th&gt;EP&lt;/th&gt; 
     &lt;th&gt;Notes&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;LLama-3&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;8&lt;/td&gt; 
     &lt;td&gt;1&lt;/td&gt; 
     &lt;td&gt;1&lt;/td&gt; 
     &lt;td&gt;2&lt;/td&gt; 
     &lt;td&gt;1&lt;/td&gt; 
     &lt;td&gt;CP for long seqlen (8K)&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;LLama-3&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;70B&lt;/td&gt; 
     &lt;td&gt;64&lt;/td&gt; 
     &lt;td&gt;4&lt;/td&gt; 
     &lt;td&gt;4&lt;/td&gt; 
     &lt;td&gt;2&lt;/td&gt; 
     &lt;td&gt;1&lt;/td&gt; 
     &lt;td&gt;TP+PP&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;LLama-3.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;405B&lt;/td&gt; 
     &lt;td&gt;1024&lt;/td&gt; 
     &lt;td&gt;8&lt;/td&gt; 
     &lt;td&gt;8&lt;/td&gt; 
     &lt;td&gt;2&lt;/td&gt; 
     &lt;td&gt;1&lt;/td&gt; 
     &lt;td&gt;3D parallelism for scale&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;GPT-3&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;175B&lt;/td&gt; 
     &lt;td&gt;128-512&lt;/td&gt; 
     &lt;td&gt;4&lt;/td&gt; 
     &lt;td&gt;8&lt;/td&gt; 
     &lt;td&gt;1&lt;/td&gt; 
     &lt;td&gt;1&lt;/td&gt; 
     &lt;td&gt;Large model config&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;Mixtral&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;8x7B&lt;/td&gt; 
     &lt;td&gt;64&lt;/td&gt; 
     &lt;td&gt;1&lt;/td&gt; 
     &lt;td&gt;4&lt;/td&gt; 
     &lt;td&gt;1&lt;/td&gt; 
     &lt;td&gt;8&lt;/td&gt; 
     &lt;td&gt;EP for MoE&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;Mixtral&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;8x22B&lt;/td&gt; 
     &lt;td&gt;256&lt;/td&gt; 
     &lt;td&gt;4&lt;/td&gt; 
     &lt;td&gt;4&lt;/td&gt; 
     &lt;td&gt;8&lt;/td&gt; 
     &lt;td&gt;8&lt;/td&gt; 
     &lt;td&gt;Combined TP+EP for large MoE&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;DeepSeek-V3&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;671B&lt;/td&gt; 
     &lt;td&gt;1024&lt;/td&gt; 
     &lt;td&gt;2&lt;/td&gt; 
     &lt;td&gt;16&lt;/td&gt; 
     &lt;td&gt;1&lt;/td&gt; 
     &lt;td&gt;64&lt;/td&gt; 
     &lt;td&gt;Large MoE config&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
  &lt;h3&gt;MoE-Specific Requirements&lt;/h3&gt; 
  &lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: When combining Expert Parallelism (EP) with Tensor Parallelism (TP), &lt;strong&gt;Sequence Parallelism (SP) must be enabled&lt;/strong&gt;.&lt;/p&gt; 
  &lt;h2&gt;Performance Optimizations&lt;/h2&gt; 
  &lt;table&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th&gt;Feature&lt;/th&gt; 
     &lt;th&gt;Flag&lt;/th&gt; 
     &lt;th&gt;Benefit&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;FlashAttention&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;code&gt;--attention-backend&lt;/code&gt;&lt;/td&gt; 
     &lt;td&gt;Faster attention and lower memory usage&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;FP8 Training&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;code&gt;--fp8-hybrid&lt;/code&gt;&lt;/td&gt; 
     &lt;td&gt;Faster training&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;Activation Checkpointing&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;code&gt;--recompute-activations&lt;/code&gt;&lt;/td&gt; 
     &lt;td&gt;Reduced memory usage&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;Data Parallelism Communication Overlap&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;code&gt;--overlap-grad-reduce&lt;/code&gt;&lt;/td&gt; 
     &lt;td&gt;Faster distributed training&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;Distributed Optimizer&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;code&gt;--use-distributed-optimizer&lt;/code&gt;&lt;/td&gt; 
     &lt;td&gt;Reduced checkpointing time&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
  &lt;p&gt;&lt;strong&gt;‚Üí &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance-guide.html#performance-tuning-guide"&gt;NVIDIA NeMo Framework Performance Tuning Guide&lt;/a&gt;&lt;/strong&gt; - Comprehensive performance optimization guide covering advanced tuning techniques, communication overlaps, memory optimizations, and profiling options.&lt;/p&gt; 
  &lt;h3&gt;FlashAttention&lt;/h3&gt; 
  &lt;p&gt;&lt;a href="https://github.com/Dao-AILab/flash-attention"&gt;FlashAttention&lt;/a&gt; is a fast and memory-efficient attention algorithm. We recommend the default usage, which uses cuDNN for attention via Transformer Engine and provides up to 50% speedups on forward and 84% on backward propagation with FP8 kernels. The &lt;code&gt;flash-attn&lt;/code&gt; package is also supported via &lt;code&gt;--use-flash-attn&lt;/code&gt;.&lt;/p&gt; 
  &lt;h3&gt;Mixed Precision Training&lt;/h3&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;--fp16                    # Standard FP16
--bf16                    # BFloat16 (recommended for large models)
--fp8-hybrid              # FP8 training (Hopper, Ada, and Blackwell GPUs)
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h3&gt;Activation Checkpointing and Recomputation&lt;/h3&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;# For limited memory
--recompute-activations

# For extreme memory constraints
--recompute-granularity full \
--recompute-method uniform
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h3&gt;Data Parallelism Communication Overlap&lt;/h3&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;--overlap-grad-reduce
--overlap-param-gather
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h3&gt;Distributed Optimizer&lt;/h3&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;--use-distributed-optimizer
&lt;/code&gt;&lt;/pre&gt; 
  &lt;h1&gt;Roadmaps&lt;/h1&gt; 
  &lt;p&gt;Stay up-to-date with our development roadmaps and planned features:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/NVIDIA/Megatron-LM/issues/1729"&gt;MoE Q3-Q4 2025 Roadmap&lt;/a&gt;&lt;/strong&gt; - Comprehensive MoE feature development including DeepSeek-V3, Qwen3, advanced parallelism, FP8 optimizations, and Blackwell enhancements&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/NVIDIA/Megatron-LM/issues/1739"&gt;GPT-OSS Implementation Tracker&lt;/a&gt;&lt;/strong&gt; - Advanced features including YaRN RoPE scaling, attention sinks, and custom activation functions&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;&lt;em&gt;More roadmap trackers will be added soon.&lt;/em&gt;&lt;/p&gt; 
  &lt;h1&gt;Community &amp;amp; Support&lt;/h1&gt; 
  &lt;h2&gt;Getting Help&lt;/h2&gt; 
  &lt;ul&gt; 
   &lt;li&gt;üìñ &lt;strong&gt;&lt;a href="https://docs.nvidia.com/Megatron-Core/"&gt;Documentation&lt;/a&gt;&lt;/strong&gt; - Official documentation&lt;/li&gt; 
   &lt;li&gt;üêõ &lt;strong&gt;&lt;a href="https://github.com/NVIDIA/Megatron-LM/issues"&gt;Issues&lt;/a&gt;&lt;/strong&gt; - Bug reports and feature requests&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;h2&gt;Contributing&lt;/h2&gt; 
  &lt;p&gt;We ‚ù§Ô∏è contributions! Ways to contribute:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;üêõ &lt;strong&gt;Report bugs&lt;/strong&gt; - Help us improve reliability&lt;/li&gt; 
   &lt;li&gt;üí° &lt;strong&gt;Suggest features&lt;/strong&gt; - Shape the future of Megatron Core&lt;/li&gt; 
   &lt;li&gt;üìù &lt;strong&gt;Improve docs&lt;/strong&gt; - Make Megatron Core more accessible&lt;/li&gt; 
   &lt;li&gt;üîß &lt;strong&gt;Submit PRs&lt;/strong&gt; - Contribute code improvements&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;&lt;strong&gt;‚Üí &lt;a href="https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;h2&gt;Citation&lt;/h2&gt; 
  &lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{megatron-lm,
  title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>knownsec/aipyapp</title>
      <link>https://github.com/knownsec/aipyapp</link>
      <description>&lt;p&gt;AI-Powered Python &amp; Python-Powered AI (Python-Use)&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/3af4e228-79b2-4fa0-a45c-c38276c6db91" alt="logo" /&gt;&lt;/p&gt; 
&lt;h1&gt;Python use&lt;/h1&gt; 
&lt;p&gt;AIPy is an implementation of the Python-use concept, demonstrating its practical value and potential.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Mission&lt;/strong&gt;: unleash the full potential of large language models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Vision&lt;/strong&gt;: a future where LLMs can think independently and proactively leverage AIPy to solve complex problems.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What&lt;/h2&gt; 
&lt;p&gt;Python use provides the entire Python execution environment to LLM. Imagine LLM sitting in front of a computer, typing various commands into the Python command-line interpreter, pressing Enter to execute, observing the results, and then typing and executing more code.&lt;/p&gt; 
&lt;p&gt;Unlike Agents, Python use does not define any tools interface. LLM can freely use all the features provided by the Python runtime environment.&lt;/p&gt; 
&lt;h2&gt;Why&lt;/h2&gt; 
&lt;p&gt;If you are a data engineer, you are likely familiar with the following scenarios:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Handling various data file formats: csv/excel, json, html, sqlite, parquet, etc.&lt;/li&gt; 
 &lt;li&gt;Performing operations like data cleaning, transformation, computation, aggregation, sorting, grouping, filtering, analysis, and visualization.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This process often requires:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Starting Python, importing pandas as pd, and typing a bunch of commands to process data.&lt;/li&gt; 
 &lt;li&gt;Generating a bunch of intermediate temporary files.&lt;/li&gt; 
 &lt;li&gt;Describing your needs to ChatGPT/Claude, copying the generated data processing code, and running it manually.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;So, why not start the Python command-line interpreter, directly describe your data processing needs, and let it be done automatically? The benefits are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;No need to manually input a bunch of Python commands temporarily.&lt;/li&gt; 
 &lt;li&gt;No need to describe your needs to GPT, copy the program, and run it manually.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This is the problem Python use aims to solve!&lt;/p&gt; 
&lt;h2&gt;How&lt;/h2&gt; 
&lt;p&gt;Python use (aipython) is a Python command-line interpreter integrated with LLM. You can:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Enter and execute Python commands as usual.&lt;/li&gt; 
 &lt;li&gt;Describe your needs in natural language, and aipython will automatically generate Python commands and execute them.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Moreover, the two modes can access data interchangeably. For example, after aipython processes your natural language commands, you can use standard Python commands to view various data.&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;AIPython has two running modes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Task mode: Very simple and easy to use, just input your task, suitable for users unfamiliar with Python.&lt;/li&gt; 
 &lt;li&gt;Python mode: Suitable for users familiar with Python, allowing both task input and Python commands, ideal for advanced users.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The default running mode is task mode, which can be switched to Python mode using the &lt;code&gt;--python&lt;/code&gt; parameter.&lt;/p&gt; 
&lt;h3&gt;Basic Config&lt;/h3&gt; 
&lt;p&gt;~/.aipyapp/aipyapp.toml:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;[llm.deepseek]
type = "deepseek"
api_key = "Your DeepSeek API Key"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Task Mode&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;uv run aipy&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; Get the latest posts from Reddit r/LocalLLaMA
......
......
&amp;gt;&amp;gt;&amp;gt; /done
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;pip install aipyapp&lt;/code&gt; and run with &lt;code&gt;aipy&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;-&amp;gt; % aipy
üöÄ Python use - AIPython (0.1.22) [https://aipy.app]
&amp;gt;&amp;gt; Get the latest posts from Reddit r/LocalLLaMA
......
&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Python Mode&lt;/h3&gt; 
&lt;h4&gt;Basic Usage&lt;/h4&gt; 
&lt;p&gt;Automatic task processing:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; ai("Get the title of Google's homepage")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Automatically Request to Install Third-Party Libraries&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;Python use - AIPython (Quit with 'exit()')
&amp;gt;&amp;gt;&amp;gt; ai("Use psutil to list all processes on MacOS")

üì¶ LLM requests to install third-party packages: ['psutil']
If you agree and have installed, please enter 'y [y/n] (n): y

&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Thanks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Hei Ge: Product manager/senior user/chief tester&lt;/li&gt; 
 &lt;li&gt;Sonnet 3.7: Generated the first version of the code, which was almost ready to use without modification.&lt;/li&gt; 
 &lt;li&gt;ChatGPT: Provided many suggestions and code snippets, especially for the command-line interface.&lt;/li&gt; 
 &lt;li&gt;Codeium: Intelligent code completion&lt;/li&gt; 
 &lt;li&gt;Copilot: Code improvement suggestions and README translation&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>google-research/timesfm</title>
      <link>https://github.com/google-research/timesfm</link>
      <description>&lt;p&gt;TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TimesFM&lt;/h1&gt; 
&lt;p&gt;TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2310.10688"&gt;A decoder-only foundation model for time-series forecasting&lt;/a&gt;, ICML 2024.&lt;/li&gt; 
 &lt;li&gt;All checkpoints: &lt;a href="https://huggingface.co/collections/google/timesfm-release-66e4be5fdb56e960c1e482a6"&gt;TimesFM Hugging Face Collection&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/"&gt;Google Research blog&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cloud.google.com/bigquery/docs/timesfm-model"&gt;TimesFM in BigQuery&lt;/a&gt;: an official Google product.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This open version is not an officially supported Google product.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Latest Model Version:&lt;/strong&gt; TimesFM 2.5&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Archived Model Versions:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;1.0 and 2.0: relevant code archived in the sub directory &lt;code&gt;v1&lt;/code&gt;. You can &lt;code&gt;pip install timesfm==1.3.0&lt;/code&gt; to install an older version of this package to load them.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Update - Sept. 15, 2025&lt;/h2&gt; 
&lt;p&gt;TimesFM 2.5 is out!&lt;/p&gt; 
&lt;p&gt;Comparing to TimesFM 2.0, this new 2.5 model:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;uses 200M parameters, down from 500M.&lt;/li&gt; 
 &lt;li&gt;supports up to 16k context length, up from 2048.&lt;/li&gt; 
 &lt;li&gt;supports continuous quantile forecast up to 1k horizon via an optional 30M quantile head.&lt;/li&gt; 
 &lt;li&gt;gets rid of the &lt;code&gt;frequency&lt;/code&gt; indicator.&lt;/li&gt; 
 &lt;li&gt;has a couple of new forecasting flags.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Along with the model upgrade we have also upgraded the inference API. This repo will be under construction over the next few weeks to&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;add support for an upcoming Flax version of the model (faster inference).&lt;/li&gt; 
 &lt;li&gt;add back covariate support.&lt;/li&gt; 
 &lt;li&gt;populate more docstrings, docs and notebook.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Install&lt;/h3&gt; 
&lt;p&gt;TODO(siriuz42): Package timesfm==2.0.0 and upload to PyPI .&lt;/p&gt; 
&lt;p&gt;Run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/google-research/timesfm.git
cd timesfm
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Code Example&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np
import timesfm
model = timesfm.TimesFM_2p5_200M_torch()
model.load_checkpoint()
model.compile(
    timesfm.ForecastConfig(
        max_context=1024,
        max_horizon=256,
        normalize_inputs=True,
        use_continuous_quantile_head=True,
        force_flip_invariance=True,
        infer_is_positive=True,
        fix_quantile_crossing=True,
    )
)
point_forecast, quantile_forecast = model.forecast(
    horizon=12,
    inputs=[
        np.linspace(0, 1, 100),
        np.sin(np.linspace(0, 20, 67)),
    ],  # Two dummy inputs
)
point_forecast.shape  # (2, 12)
quantile_forecast.shape  # (2, 12, 10): mean, then 10th to 90th quantiles.
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>jingyaogong/minimind</title>
      <link>https://github.com/jingyaogong/minimind</link>
      <description>&lt;p&gt;üöÄüöÄ „ÄåÂ§ßÊ®°Âûã„Äç2Â∞èÊó∂ÂÆåÂÖ®‰ªé0ËÆ≠ÁªÉ26MÁöÑÂ∞èÂèÇÊï∞GPTÔºÅüåè Train a 26M-parameter GPT from scratch in just 2h!&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/logo.png" alt="logo" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://visitor-badge.laobi.icu/badge?page_id=jingyaogong/minimind" alt="visitors" /&gt; &lt;a href="https://github.com/jingyaogong/minimind/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/jingyaogong/minimind?style=social" alt="GitHub Repo stars" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/jingyaogong/minimind" alt="GitHub Code License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/jingyaogong/minimind/commits/master"&gt;&lt;img src="https://img.shields.io/github/last-commit/jingyaogong/minimind" alt="GitHub last commit" /&gt;&lt;/a&gt; &lt;a href="https://github.com/jingyaogong/minimind/pulls"&gt;&lt;img src="https://img.shields.io/badge/PRs-welcome-blue" alt="GitHub pull request" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97-MiniMind%20%20Collection-blue" alt="Collection" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;"Â§ßÈÅìËá≥ÁÆÄ"&lt;/h3&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;‰∏≠Êñá | &lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/README_en.md"&gt;English&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt;Ê≠§ÂºÄÊ∫êÈ°πÁõÆÊó®Âú®ÂÆåÂÖ®‰ªé0ÂºÄÂßãÔºå‰ªÖÁî®3ÂùóÈí±ÊàêÊú¨ + 2Â∞èÊó∂ÔºÅÂç≥ÂèØËÆ≠ÁªÉÂá∫‰ªÖ‰∏∫25.8MÁöÑË∂ÖÂ∞èËØ≠Ë®ÄÊ®°Âûã&lt;strong&gt;MiniMind&lt;/strong&gt;„ÄÇ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MiniMind&lt;/strong&gt;Á≥ªÂàóÊûÅÂÖ∂ËΩªÈáèÔºåÊúÄÂ∞èÁâàÊú¨‰ΩìÁßØÊòØ GPT-3 ÁöÑ $\frac{1}{7000}$ÔºåÂäõÊ±ÇÂÅöÂà∞ÊúÄÊôÆÈÄöÁöÑ‰∏™‰∫∫GPU‰πüÂèØÂø´ÈÄüËÆ≠ÁªÉ„ÄÇ&lt;/li&gt; 
 &lt;li&gt;È°πÁõÆÂêåÊó∂ÂºÄÊ∫ê‰∫ÜÂ§ßÊ®°ÂûãÁöÑÊûÅÁÆÄÁªìÊûÑ-ÂåÖÂê´ÊãìÂ±ïÂÖ±‰∫´Ê∑∑Âêà‰∏ìÂÆ∂(MoE)„ÄÅÊï∞ÊçÆÈõÜÊ∏ÖÊ¥ó„ÄÅÈ¢ÑËÆ≠ÁªÉ(Pretrain)„ÄÅÁõëÁù£ÂæÆË∞É(SFT)„ÄÅLoRAÂæÆË∞ÉÔºå Áõ¥Êé•ÂÅèÂ•ΩÂº∫ÂåñÂ≠¶‰π†(DPO)ÁÆóÊ≥ï„ÄÅÊ®°ÂûãËí∏È¶èÁÆóÊ≥ïÁ≠âÂÖ®ËøáÁ®ã‰ª£Á†Å„ÄÇ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MiniMind&lt;/strong&gt;ÂêåÊó∂ÊãìÂ±ï‰∫ÜËßÜËßâÂ§öÊ®°ÊÄÅÁöÑVLM: &lt;a href="https://github.com/jingyaogong/minimind-v"&gt;MiniMind-V&lt;/a&gt;„ÄÇ&lt;/li&gt; 
 &lt;li&gt;È°πÁõÆÊâÄÊúâÊ†∏ÂøÉÁÆóÊ≥ï‰ª£Á†ÅÂùá‰ªé0‰ΩøÁî®PyTorchÂéüÁîüÈáçÊûÑÔºÅ‰∏ç‰æùËµñÁ¨¨‰∏âÊñπÂ∫ìÊèê‰æõÁöÑÊäΩË±°Êé•Âè£„ÄÇ&lt;/li&gt; 
 &lt;li&gt;Ëøô‰∏ç‰ªÖÊòØÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÖ®Èò∂ÊÆµÂºÄÊ∫êÂ§çÁé∞Ôºå‰πüÊòØ‰∏Ä‰∏™ÂÖ•Èó®LLMÁöÑÊïôÁ®ã„ÄÇ&lt;/li&gt; 
 &lt;li&gt;Â∏åÊúõÊ≠§È°πÁõÆËÉΩ‰∏∫ÊâÄÊúâ‰∫∫Êèê‰æõ‰∏Ä‰∏™ÊäõÁ†ñÂºïÁéâÁöÑÁ§∫‰æãÔºå‰∏ÄËµ∑ÊÑüÂèóÂàõÈÄ†ÁöÑ‰πêË∂£ÔºÅÊé®Âä®Êõ¥ÂπøÊ≥õAIÁ§æÂå∫ÁöÑËøõÊ≠•ÔºÅ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‰∏∫Èò≤Ê≠¢ËØØËß£Ôºå‚Äú2Â∞èÊó∂‚Äù Âü∫‰∫éNVIDIA 3090Á°¨‰ª∂ËÆæÂ§áÔºàÂçïÂç°ÔºâÊµãËØïÔºå‚Äú3ÂùóÈí±‚Äù ÊåáGPUÊúçÂä°Âô®ÁßüÁî®ÊàêÊú¨ÔºåÂÖ∑‰ΩìËßÑÊ†ºËØ¶ÊÉÖËßÅ‰∏ãÊñá„ÄÇ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/minimind2.gif" alt="minimind2" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.modelscope.cn/studios/gongjy/MiniMind-Reasoning"&gt;üîóüçìÊé®ÁêÜÊ®°Âûã&lt;/a&gt; | &lt;a href="https://www.modelscope.cn/studios/gongjy/MiniMind"&gt;üîóü§ñÂ∏∏ËßÑÊ®°Âûã&lt;/a&gt; | &lt;a href="https://www.bilibili.com/video/BV12dHPeqE72/?share_source=copy_web&amp;amp;vd_source=670c2504f88726f8cf4a21ef6147c0e8"&gt;üîóüéûÔ∏èËßÜÈ¢ë‰ªãÁªç&lt;/a&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table&gt; 
   &lt;tbody&gt;
    &lt;tr&gt; 
     &lt;td align="center"&gt; &lt;a href="https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5" style="text-decoration: none;"&gt; &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/and_huggingface.png" alt="Hugging Face Logo" style="vertical-align: middle; width: auto; max-width: 100%;" /&gt; &lt;/a&gt; &lt;/td&gt; 
     &lt;td align="center"&gt; &lt;a href="https://www.modelscope.cn/profile/gongjy" style="text-decoration: none;"&gt; &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/and_modelscope.png" alt="ModelScope Logo" style="vertical-align: middle; width: auto; max-width: 100%;" /&gt; &lt;/a&gt; &lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt;
  &lt;/table&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;h1&gt;üìå Introduction&lt;/h1&gt; 
&lt;p&gt;Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLarge Language Model, LLMÔºâÁöÑÂá∫Áé∞ÂºïÂèë‰∫ÜÂÖ®‰∏ñÁïåÂØπAIÁöÑÁ©∫ÂâçÂÖ≥Ê≥®„ÄÇ Êó†ËÆ∫ÊòØChatGPT„ÄÅDeepSeekËøòÊòØQwenÔºåÈÉΩ‰ª•ÂÖ∂ÊÉäËâ≥ÁöÑÊïàÊûú‰ª§‰∫∫Âèπ‰∏∫ËßÇÊ≠¢„ÄÇ ÁÑ∂ËÄåÔºåÂä®ËæÑÊï∞Áôæ‰∫øÂèÇÊï∞ÁöÑÂ∫ûÂ§ßËßÑÊ®°Ôºå‰ΩøÂæóÂÆÉ‰ª¨ÂØπ‰∏™‰∫∫ËÆæÂ§áËÄåË®Ä‰∏ç‰ªÖÈöæ‰ª•ËÆ≠ÁªÉÔºåÁîöËá≥ËøûÈÉ®ÁΩ≤ÈÉΩÊòæÂæóÈÅ•‰∏çÂèØÂèä„ÄÇ ÊâìÂºÄÂ§ßÊ®°ÂûãÁöÑ‚ÄúÈªëÁõíÂ≠ê‚ÄùÔºåÊé¢Á¥¢ÂÖ∂ÂÜÖÈÉ®Ëøê‰ΩúÊú∫Âà∂ÔºåÂ§ö‰πà‰ª§‰∫∫ÂøÉÊΩÆÊæéÊπÉÔºÅ ÈÅóÊÜæÁöÑÊòØÔºå99%ÁöÑÊé¢Á¥¢Âè™ËÉΩÊ≠¢Ê≠•‰∫é‰ΩøÁî®LoRAÁ≠âÊäÄÊúØÂØπÁé∞ÊúâÂ§ßÊ®°ÂûãËøõË°åÂ∞ëÈáèÂæÆË∞ÉÔºåÂ≠¶‰π†‰∏Ä‰∫õÊñ∞Êåá‰ª§Êàñ‰ªªÂä°„ÄÇ ËøôÂ∞±Â•ΩÊØîÊïôÁâõÈ°øÂ¶Ç‰Ωï‰ΩøÁî®21‰∏ñÁ∫™ÁöÑÊô∫ËÉΩÊâãÊú∫‚Äî‚ÄîËôΩÁÑ∂ÊúâË∂£ÔºåÂç¥ÂÆåÂÖ®ÂÅèÁ¶ª‰∫ÜÁêÜËß£Áâ©ÁêÜÊú¨Ë¥®ÁöÑÂàùË°∑„ÄÇ ‰∏éÊ≠§ÂêåÊó∂ÔºåÁ¨¨‰∏âÊñπÁöÑÂ§ßÊ®°ÂûãÊ°ÜÊû∂ÂíåÂ∑•ÂÖ∑Â∫ìÔºåÂ¶Çtransformers+trlÔºåÂá†‰πéÂè™Êö¥Èú≤‰∫ÜÈ´òÂ∫¶ÊäΩË±°ÁöÑÊé•Âè£„ÄÇ ÈÄöËøáÁü≠Áü≠10Ë°å‰ª£Á†ÅÔºåÂ∞±ËÉΩÂÆåÊàê‚ÄúÂä†ËΩΩÊ®°Âûã+Âä†ËΩΩÊï∞ÊçÆÈõÜ+Êé®ÁêÜ+Âº∫ÂåñÂ≠¶‰π†‚ÄùÁöÑÂÖ®ÊµÅÁ®ãËÆ≠ÁªÉ„ÄÇ ËøôÁßçÈ´òÊïàÁöÑÂ∞ÅË£ÖÂõ∫ÁÑ∂‰æøÂà©Ôºå‰ΩÜ‰πüÂÉè‰∏ÄÊû∂È´òÈÄüÈ£ûËàπÔºåÂ∞ÜÊàë‰ª¨‰∏éÂ∫ïÂ±ÇÂÆûÁé∞ÈöîÁ¶ªÂºÄÊù•ÔºåÈòªÁ¢ç‰∫ÜÊ∑±ÂÖ•Êé¢Á©∂LLMÊ†∏ÂøÉ‰ª£Á†ÅÁöÑÊú∫‰ºö„ÄÇ ÁÑ∂ËÄåÔºå‚ÄúÁî®‰πêÈ´òÊãºÂá∫‰∏ÄÊû∂È£ûÊú∫ÔºåËøúÊØîÂùêÂú®Â§¥Á≠âËà±ÈáåÈ£ûË°åÊõ¥ËÆ©‰∫∫ÂÖ¥Â•ãÔºÅ‚Äù„ÄÇ Êõ¥Á≥üÁ≥ïÁöÑÊòØÔºå‰∫íËÅîÁΩë‰∏äÂÖÖÊñ•ÁùÄÂ§ßÈáè‰ªòË¥πËØæÁ®ãÂíåËê•ÈîÄÂè∑Ôºå‰ª•ÊºèÊ¥ûÁôæÂá∫„ÄÅ‰∏ÄÁü•ÂçäËß£ÁöÑÂÜÖÂÆπÊé®ÈîÄAIÊïôÁ®ã„ÄÇ Ê≠£Âõ†Â¶ÇÊ≠§ÔºåÊú¨È°πÁõÆÂàùË°∑ÊòØÊãâ‰ΩéLLMÁöÑÂ≠¶‰π†Èó®ÊßõÔºåËÆ©ÊØè‰∏™‰∫∫ÈÉΩËÉΩ‰ªéÁêÜËß£ÊØè‰∏ÄË°å‰ª£Á†ÅÂºÄÂßãÔºå ‰ªéÈõ∂ÂºÄÂßã‰∫≤ÊâãËÆ≠ÁªÉ‰∏Ä‰∏™ÊûÅÂ∞èÁöÑËØ≠Ë®ÄÊ®°Âûã„ÄÇÊòØÁöÑÔºå‰ªé&lt;strong&gt;Èõ∂ÂºÄÂßãËÆ≠ÁªÉ&lt;/strong&gt;ÔºåËÄå‰∏çÊòØ‰ªÖ‰ªÖËøõË°å&lt;strong&gt;Êé®ÁêÜ&lt;/strong&gt;ÔºÅ ÊúÄ‰ΩéÂè™ÈúÄ3ÂùóÈí±‰∏çÂà∞ÁöÑÊúçÂä°Âô®ÊàêÊú¨ÔºåÂ∞±ËÉΩ‰∫≤Ë∫´‰ΩìÈ™å‰ªé0Âà∞1ÊûÑÂª∫‰∏Ä‰∏™ËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÖ®ËøáÁ®ã„ÄÇ ‰∏ÄËµ∑ÊÑüÂèóÂàõÈÄ†ÁöÑ‰πêË∂£ÂêßÔºÅ&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] ÔºàÊà™Ëá≥2025-02-07ÔºâMiniMindÁ≥ªÂàóÂ∑≤ÂÆåÊàêÂ§ö‰∏™ÂûãÂè∑Ê®°ÂûãÁöÑÈ¢ÑËÆ≠ÁªÉÔºåÊúÄÂ∞è‰ªÖÈúÄ25.8MÔºà0.02BÔºâÔºåÂç≥ÂèØÂÖ∑Â§áÊµÅÁïÖÂØπËØùËÉΩÂäõÔºÅ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;Models List&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Ê®°Âûã (Â§ßÂ∞è)&lt;/th&gt; 
    &lt;th&gt;Êé®ÁêÜÂç†Áî® (Á∫¶)&lt;/th&gt; 
    &lt;th&gt;Release&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2-small (26M)&lt;/td&gt; 
    &lt;td&gt;0.5 GB&lt;/td&gt; 
    &lt;td&gt;2025.04.26&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2-MoE (145M)&lt;/td&gt; 
    &lt;td&gt;1.0 GB&lt;/td&gt; 
    &lt;td&gt;2025.04.26&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2 (104M)&lt;/td&gt; 
    &lt;td&gt;1.0 GB&lt;/td&gt; 
    &lt;td&gt;2025.04.26&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1-small (26M)&lt;/td&gt; 
    &lt;td&gt;0.5 GB&lt;/td&gt; 
    &lt;td&gt;2024.08.28&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1-moe (4√ó26M)&lt;/td&gt; 
    &lt;td&gt;1.0 GB&lt;/td&gt; 
    &lt;td&gt;2024.09.17&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1 (108M)&lt;/td&gt; 
    &lt;td&gt;1.0 GB&lt;/td&gt; 
    &lt;td&gt;2024.09.01&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;strong&gt;È°πÁõÆÂåÖÂê´&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;MiniMind-LLMÁªìÊûÑÁöÑÂÖ®ÈÉ®‰ª£Á†ÅÔºàDense+MoEÊ®°ÂûãÔºâ„ÄÇ&lt;/li&gt; 
 &lt;li&gt;ÂåÖÂê´TokenizerÂàÜËØçÂô®ËØ¶ÁªÜËÆ≠ÁªÉ‰ª£Á†Å„ÄÇ&lt;/li&gt; 
 &lt;li&gt;ÂåÖÂê´Pretrain„ÄÅSFT„ÄÅLoRA„ÄÅRLHF-DPO„ÄÅÊ®°ÂûãËí∏È¶èÁöÑÂÖ®ËøáÁ®ãËÆ≠ÁªÉ‰ª£Á†Å„ÄÇ&lt;/li&gt; 
 &lt;li&gt;Êî∂ÈõÜ„ÄÅËí∏È¶è„ÄÅÊï¥ÁêÜÂπ∂Ê∏ÖÊ¥óÂéªÈáçÊâÄÊúâÈò∂ÊÆµÁöÑÈ´òË¥®ÈáèÊï∞ÊçÆÈõÜÔºå‰∏îÂÖ®ÈÉ®ÂºÄÊ∫ê„ÄÇ&lt;/li&gt; 
 &lt;li&gt;‰ªé0ÂÆûÁé∞È¢ÑËÆ≠ÁªÉ„ÄÅÊåá‰ª§ÂæÆË∞É„ÄÅLoRA„ÄÅDPOÂº∫ÂåñÂ≠¶‰π†ÔºåÁôΩÁõíÊ®°ÂûãËí∏È¶è„ÄÇÂÖ≥ÈîÆÁÆóÊ≥ïÂá†‰πé‰∏ç‰æùËµñÁ¨¨‰∏âÊñπÂ∞ÅË£ÖÁöÑÊ°ÜÊû∂Ôºå‰∏îÂÖ®ÈÉ®ÂºÄÊ∫ê„ÄÇ&lt;/li&gt; 
 &lt;li&gt;ÂêåÊó∂ÂÖºÂÆπ&lt;code&gt;transformers&lt;/code&gt;„ÄÅ&lt;code&gt;trl&lt;/code&gt;„ÄÅ&lt;code&gt;peft&lt;/code&gt;Á≠âÁ¨¨‰∏âÊñπ‰∏ªÊµÅÊ°ÜÊû∂„ÄÇ&lt;/li&gt; 
 &lt;li&gt;ËÆ≠ÁªÉÊîØÊåÅÂçïÊú∫ÂçïÂç°„ÄÅÂçïÊú∫Â§öÂç°(DDP„ÄÅDeepSpeed)ËÆ≠ÁªÉÔºåÊîØÊåÅwandbÂèØËßÜÂåñËÆ≠ÁªÉÊµÅÁ®ã„ÄÇÊîØÊåÅÂä®ÊÄÅÂêØÂÅúËÆ≠ÁªÉ„ÄÇ&lt;/li&gt; 
 &lt;li&gt;Âú®Á¨¨‰∏âÊñπÊµãËØÑÊ¶úÔºàC-Eval„ÄÅC-MMLU„ÄÅOpenBookQAÁ≠âÔºâËøõË°åÊ®°ÂûãÊµãËØï„ÄÇ&lt;/li&gt; 
 &lt;li&gt;ÂÆûÁé∞Openai-ApiÂçèËÆÆÁöÑÊûÅÁÆÄÊúçÂä°Á´ØÔºå‰æø‰∫éÈõÜÊàêÂà∞Á¨¨‰∏âÊñπChatUI‰ΩøÁî®ÔºàFastGPT„ÄÅOpen-WebUIÁ≠âÔºâ„ÄÇ&lt;/li&gt; 
 &lt;li&gt;Âü∫‰∫éstreamlitÂÆûÁé∞ÊúÄÁÆÄËÅäÂ§©WebUIÂâçÁ´Ø„ÄÇ&lt;/li&gt; 
 &lt;li&gt;ÂÖ®Èù¢ÂÖºÂÆπÁ§æÂå∫ÁÉ≠Èó®&lt;code&gt;llama.cpp&lt;/code&gt;„ÄÅ&lt;code&gt;vllm&lt;/code&gt;„ÄÅ&lt;code&gt;ollama&lt;/code&gt;Êé®ÁêÜÂºïÊìéÊàñ&lt;code&gt;Llama-Factory&lt;/code&gt;ËÆ≠ÁªÉÊ°ÜÊû∂„ÄÇ&lt;/li&gt; 
 &lt;li&gt;Â§çÁé∞(Ëí∏È¶è/RL)Â§ßÂûãÊé®ÁêÜÊ®°ÂûãDeepSeek-R1ÁöÑMiniMind-ReasonÊ®°ÂûãÔºå&lt;strong&gt;Êï∞ÊçÆ+Ê®°Âûã&lt;/strong&gt;ÂÖ®ÈÉ®ÂºÄÊ∫êÔºÅ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Â∏åÊúõÊ≠§ÂºÄÊ∫êÈ°πÁõÆÂèØ‰ª•Â∏ÆÂä©LLMÂàùÂ≠¶ËÄÖÂø´ÈÄüÂÖ•Èó®ÔºÅ&lt;/p&gt; 
&lt;h3&gt;üëâ&lt;strong&gt;Êõ¥Êñ∞Êó•Âøó&lt;/strong&gt;&lt;/h3&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2025-04-26 (newest üéâüéâüéâ)&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ÈáçË¶ÅÊõ¥Êñ∞&lt;/li&gt; 
  &lt;li&gt;Â¶ÇÊúâÂÖºÂÆπÊÄßÈúÄË¶ÅÔºåÂèØËÆøÈóÆ&lt;a href="https://github.com/jingyaogong/minimind/tree/7da201a944a90ed49daef8a0265c959288dff83a"&gt;üîóÊóß‰ªìÂ∫ìÂÜÖÂÆπüîó&lt;/a&gt;„ÄÇ&lt;/li&gt; 
  &lt;li&gt;MiniMindÊ®°ÂûãÂèÇÊï∞ÂÆåÂÖ®ÊîπÂêçÔºåÂØπÈΩêTransformersÂ∫ìÊ®°ÂûãÔºàÁªü‰∏ÄÂëΩÂêçÔºâ„ÄÇ&lt;/li&gt; 
  &lt;li&gt;generateÊñπÂºèÈáçÊûÑÔºåÁªßÊâøËá™GenerationMixinÁ±ª„ÄÇ&lt;/li&gt; 
  &lt;li&gt;üî•ÊîØÊåÅllama.cpp„ÄÅvllm„ÄÅollamaÁ≠âÁÉ≠Èó®‰∏âÊñπÁîüÊÄÅ„ÄÇ&lt;/li&gt; 
  &lt;li&gt;ËßÑËåÉ‰ª£Á†ÅÂíåÁõÆÂΩïÁªìÊûÑ„ÄÇ&lt;/li&gt; 
  &lt;li&gt;ÊîπÂä®ËØçË°®&lt;code&gt;&amp;lt;s&amp;gt;&amp;lt;/s&amp;gt;&lt;/code&gt;-&amp;gt;&lt;code&gt;&amp;lt;|im_start|&amp;gt;&amp;lt;|im_end|&amp;gt;&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-text"&gt;‰∏∫ÂÖºÂÆπÁ¨¨‰∏âÊñπÊé®ÁêÜÊ°ÜÊû∂llama.cpp„ÄÅvllmÔºåÊú¨Ê¨°Êõ¥Êñ∞ÈúÄ‰ªòÂá∫‰∏Ä‰∫õÂèØËßÇ‰ª£‰ª∑„ÄÇ
Êú¨Ê¨°Êõ¥Êñ∞‰∏çÂÜçÊîØÊåÅ„ÄåÁõ¥Êé•„ÄçÂä†ËΩΩ25-04-26‰ª•ÂâçÁöÑÊóßÊ®°ÂûãËøõË°åÊé®ÁêÜ„ÄÇ
Áî±‰∫éLlama‰ΩçÁΩÆÁºñÁ†ÅÊñπÂºè‰∏éminimindÂ≠òÂú®Âå∫Âà´ÔºåÂØºËá¥Êò†Â∞ÑLlamaÊ®°ÂûãÂêéQKÂÄºÂ≠òÂú®Â∑ÆÂºÇ
MiniMind2Á≥ªÂàóÊóßÊ®°ÂûãÂùáÁªèËøáÊùÉÈáçÊò†Â∞Ñ+ÔºàÂæÆË∞ÉËÆ≠ÁªÉÔºâQKVOÁ∫øÊÄßÂ±ÇÊ†°ÂáÜÊÅ¢Â§çËÄåÊù•„ÄÇ
Êú¨Ê¨°Êõ¥Êñ∞ÂêéÂ∞ÜÊîæÂºÉÂØπ`minimind-v1`ÂÖ®Á≥ªÂàóÁöÑÁª¥Êä§ÔºåÂπ∂Âú®‰ªìÂ∫ì‰∏≠‰∏ãÁ∫ø„ÄÇ
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2025-02-09&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ËøéÊù•ÂèëÂ∏É‰ª•Êù•ÈáçÂ§ßÊõ¥Êñ∞ÔºåRelease MiniMind2 Series„ÄÇ&lt;/li&gt; 
  &lt;li&gt;‰ª£Á†ÅÂá†‰πéÂÖ®ÈÉ®ÈáçÊûÑÔºå‰ΩøÁî®Êõ¥ÁÆÄÊ¥ÅÊòé‰∫ÜÁöÑÁªü‰∏ÄÁªìÊûÑ„ÄÇ Â¶ÇÊúâÊóß‰ª£Á†ÅÁöÑÂÖºÂÆπÊÄßÈúÄË¶ÅÔºåÂèØËÆøÈóÆ&lt;a href="https://github.com/jingyaogong/minimind/tree/6e9cd28ef9b34a0a10afbdf6f59e65cb6e628efb"&gt;üîóÊóß‰ªìÂ∫ìÂÜÖÂÆπüîó&lt;/a&gt;„ÄÇ&lt;/li&gt; 
  &lt;li&gt;ÂÖçÂéªÊï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÊ≠•È™§„ÄÇÁªü‰∏ÄÊï∞ÊçÆÈõÜÊ†ºÂºèÔºåÊõ¥Êç¢‰∏∫&lt;code&gt;jsonl&lt;/code&gt;Ê†ºÂºèÊùúÁªùÊï∞ÊçÆÈõÜ‰∏ãËΩΩÊ∑∑‰π±ÁöÑÈóÆÈ¢ò„ÄÇ&lt;/li&gt; 
  &lt;li&gt;MiniMind2Á≥ªÂàóÊïàÊûúÁõ∏ÊØîMiniMind-V1ÊòæËëóÊèêÂçá„ÄÇ&lt;/li&gt; 
  &lt;li&gt;Â∞èÈóÆÈ¢òÔºö{kv-cacheÂÜôÊ≥ïÊõ¥Ê†áÂáÜ„ÄÅMoEÁöÑË¥üËΩΩÂùáË°°lossË¢´ËÄÉËôëÁ≠âÁ≠â}&lt;/li&gt; 
  &lt;li&gt;Êèê‰æõÊ®°ÂûãËøÅÁßªÂà∞ÁßÅÊúâÊï∞ÊçÆÈõÜÁöÑËÆ≠ÁªÉÊñπÊ°àÔºàÂåªÁñóÊ®°Âûã„ÄÅËá™ÊàëËÆ§Áü•Ê†∑‰æãÔºâ„ÄÇ&lt;/li&gt; 
  &lt;li&gt;Á≤æÁÆÄÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÔºåÂπ∂Â§ßÂπÖÊèêÂçáÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆË¥®ÈáèÔºåÂ§ßÂπÖÁº©Áü≠‰∏™‰∫∫Âø´ÈÄüËÆ≠ÁªÉÊâÄÈúÄÊó∂Èó¥ÔºåÂçïÂç°3090Âç≥ÂèØ2Â∞èÊó∂Â§çÁé∞ÔºÅ&lt;/li&gt; 
  &lt;li&gt;Êõ¥Êñ∞ÔºöLoRAÂæÆË∞ÉËÑ±Á¶ªpeftÂåÖË£ÖÔºå‰ªé0ÂÆûÁé∞LoRAËøáÁ®ãÔºõDPOÁÆóÊ≥ï‰ªé0‰ΩøÁî®PyTorchÂéüÁîüÂÆûÁé∞ÔºõÊ®°ÂûãÁôΩÁõíËí∏È¶èÂéüÁîüÂÆûÁé∞„ÄÇ&lt;/li&gt; 
  &lt;li&gt;MiniMind2-DeepSeek-R1Á≥ªÂàóËí∏È¶èÊ®°ÂûãËØûÁîüÔºÅ&lt;/li&gt; 
  &lt;li&gt;MiniMind2ÂÖ∑Â§á‰∏ÄÂÆöÁöÑËã±ÊñáËÉΩÂäõÔºÅ&lt;/li&gt; 
  &lt;li&gt;Êõ¥Êñ∞MiniMind2‰∏éÁ¨¨‰∏âÊñπÊ®°ÂûãÁöÑÂü∫‰∫éÊõ¥Â§öÂ§ßÊ®°ÂûãÊ¶úÂçïÊµãËØïÊÄßËÉΩÁöÑÁªìÊûú„ÄÇ&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2024-10-05&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;‰∏∫MiniMindÊãìÂ±ï‰∫ÜÂ§öÊ®°ÊÄÅËÉΩÂäõ‰πã---ËßÜËßâ&lt;/li&gt; 
  &lt;li&gt;ÁßªÊ≠•Â≠™ÁîüÈ°πÁõÆ&lt;a href="https://github.com/jingyaogong/minimind-v"&gt;minimind-v&lt;/a&gt;Êü•ÁúãËØ¶ÊÉÖÔºÅ&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2024-09-27&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;09-27Êõ¥Êñ∞pretrainÊï∞ÊçÆÈõÜÁöÑÈ¢ÑÂ§ÑÁêÜÊñπÂºèÔºå‰∏∫‰∫Ü‰øùËØÅÊñáÊú¨ÂÆåÊï¥ÊÄßÔºåÊîæÂºÉÈ¢ÑÂ§ÑÁêÜÊàê.binËÆ≠ÁªÉÁöÑÂΩ¢ÂºèÔºàËΩªÂæÆÁâ∫Áâ≤ËÆ≠ÁªÉÈÄüÂ∫¶Ôºâ„ÄÇ&lt;/li&gt; 
  &lt;li&gt;ÁõÆÂâçpretrainÈ¢ÑÂ§ÑÁêÜÂêéÁöÑÊñá‰ª∂ÂëΩÂêç‰∏∫Ôºöpretrain_data.csv„ÄÇ&lt;/li&gt; 
  &lt;li&gt;Âà†Èô§‰∫Ü‰∏Ä‰∫õÂÜó‰ΩôÁöÑ‰ª£Á†Å„ÄÇ&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2024-09-17&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Êõ¥Êñ∞minimind-v1-moeÊ®°Âûã&lt;/li&gt; 
  &lt;li&gt;‰∏∫‰∫ÜÈò≤Ê≠¢Ê≠ß‰πâÔºå‰∏çÂÜç‰ΩøÁî®mistral_tokenizerÂàÜËØçÔºåÂÖ®ÈÉ®ÈááÁî®Ëá™ÂÆö‰πâÁöÑminimind_tokenizer‰Ωú‰∏∫ÂàÜËØçÂô®„ÄÇ&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2024-09-01&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Êõ¥Êñ∞minimind-v1 (108M)Ê®°ÂûãÔºåÈááÁî®minimind_tokenizerÔºåÈ¢ÑËÆ≠ÁªÉËΩÆÊ¨°3 + SFTËΩÆÊ¨°10ÔºåÊõ¥ÂÖÖÂàÜËÆ≠ÁªÉÔºåÊÄßËÉΩÊõ¥Âº∫„ÄÇ&lt;/li&gt; 
  &lt;li&gt;È°πÁõÆÂ∑≤ÈÉ®ÁΩ≤Ëá≥ModelScopeÂàõÁ©∫Èó¥ÔºåÂèØ‰ª•Âú®Ê≠§ÁΩëÁ´ô‰∏ä‰ΩìÈ™åÔºö&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://www.modelscope.cn/studios/gongjy/minimind"&gt;üîóModelScopeÂú®Á∫ø‰ΩìÈ™åüîó&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2024-08-27&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;È°πÁõÆÈ¶ñÊ¨°ÂºÄÊ∫ê&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h1&gt;üìå Âø´ÈÄüÂºÄÂßã&lt;/h1&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;ÂàÜ‰∫´Êú¨‰∫∫ÁöÑËΩØÁ°¨‰ª∂ÈÖçÁΩÆÔºà‰ªÖ‰æõÂèÇËÄÉÔºâ&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;CPU: Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz&lt;/li&gt; 
  &lt;li&gt;RAM: 128 GB&lt;/li&gt; 
  &lt;li&gt;GPU: NVIDIA GeForce RTX 3090(24GB) * 8&lt;/li&gt; 
  &lt;li&gt;Ubuntu==20.04&lt;/li&gt; 
  &lt;li&gt;CUDA==12.2&lt;/li&gt; 
  &lt;li&gt;Python==3.10.16&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/requirements.txt"&gt;requirements.txt&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;Á¨¨0Ê≠•&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/jingyaogong/minimind.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;‚Ö† ÊµãËØïÂ∑≤ÊúâÊ®°ÂûãÊïàÊûú&lt;/h2&gt; 
&lt;h3&gt;1.ÁéØÂ¢ÉÂáÜÂ§á&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2.‰∏ãËΩΩÊ®°Âûã&lt;/h3&gt; 
&lt;p&gt;Âà∞È°πÁõÆÊ†πÁõÆÂΩï&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://huggingface.co/jingyaogong/MiniMind2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ÔºàÂèØÈÄâÔºâÂëΩ‰ª§Ë°åÈóÆÁ≠î&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# load=0: load from pytorch model, load=1: load from transformers-hf model
python eval_model.py --load 1 --model_mode 2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ÔºàÂèØÈÄâÔºâÂêØÂä®WebUI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ÂèØËÉΩÈúÄË¶Å`python&amp;gt;=3.10` ÂÆâË£Ö `pip install streamlit`
# cd scripts
streamlit run web_demo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ÔºàÂèØÈÄâÔºâÁ¨¨‰∏âÊñπÊé®ÁêÜÊ°ÜÊû∂&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ollama
ollama run jingyaogong/minimind2
# vllm
vllm serve ./MiniMind2/ --served-model-name "minimind"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;‚Ö° ‰ªé0ÂºÄÂßãËá™Â∑±ËÆ≠ÁªÉ&lt;/h2&gt; 
&lt;h3&gt;1.ÁéØÂ¢ÉÂáÜÂ§á&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;Ê≥®ÔºöÊèêÂâçÊµãËØïTorchÊòØÂê¶ÂèØÁî®cuda&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;import torch
print(torch.cuda.is_available())
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Â¶ÇÊûú‰∏çÂèØÁî®ÔºåËØ∑Ëá™Ë°åÂéª&lt;a href="https://download.pytorch.org/whl/torch_stable.html"&gt;torch_stable&lt;/a&gt; ‰∏ãËΩΩwhlÊñá‰ª∂ÂÆâË£Ö„ÄÇÂèÇËÄÉ&lt;a href="https://blog.csdn.net/weixin_45456738/article/details/141029610?ops_request_misc=&amp;amp;request_id=&amp;amp;biz_id=102&amp;amp;utm_term=%E5%AE%89%E8%A3%85torch&amp;amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-141029610.nonecase&amp;amp;spm=1018.2226.3001.4187"&gt;ÈìæÊé•&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;2.Êï∞ÊçÆ‰∏ãËΩΩ&lt;/h3&gt; 
&lt;p&gt;‰ªé‰∏ãÊñáÊèê‰æõÁöÑ&lt;a href="https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files"&gt;Êï∞ÊçÆÈõÜ‰∏ãËΩΩÈìæÊé•&lt;/a&gt; ‰∏ãËΩΩÈúÄË¶ÅÁöÑÊï∞ÊçÆÊñá‰ª∂ÔºàÂàõÂª∫&lt;code&gt;./dataset&lt;/code&gt;ÁõÆÂΩïÔºâÂπ∂ÊîæÂà∞&lt;code&gt;./dataset&lt;/code&gt;‰∏ã&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;Ê≥®ÔºöÊï∞ÊçÆÈõÜÈ°ªÁü•&lt;/summary&gt; 
 &lt;p&gt;ÈªòËÆ§Êé®Ëçê‰∏ãËΩΩ&lt;code&gt;pretrain_hq.jsonl&lt;/code&gt; + &lt;code&gt;sft_mini_512.jsonl&lt;/code&gt;ÊúÄÂø´ÈÄüÂ∫¶Â§çÁé∞ZeroËÅäÂ§©Ê®°Âûã„ÄÇ&lt;/p&gt; 
 &lt;p&gt;Êï∞ÊçÆÊñá‰ª∂ÂèØËá™Áî±ÈÄâÊã©Ôºå‰∏ãÊñáÊèê‰æõ‰∫ÜÂ§öÁßçÊê≠ÈÖçÊñπÊ°àÔºåÂèØÊ†πÊçÆËá™Â∑±ÊâãÂ§¥ÁöÑËÆ≠ÁªÉÈúÄÊ±ÇÂíåGPUËµÑÊ∫êËøõË°åÈÄÇÂΩìÁªÑÂêà„ÄÇ&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;3.ÂºÄÂßãËÆ≠ÁªÉ&lt;/h3&gt; 
&lt;p&gt;ÁõÆÂΩï‰Ωç‰∫é&lt;code&gt;trainer&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;3.1 È¢ÑËÆ≠ÁªÉÔºàÂ≠¶Áü•ËØÜÔºâ&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python train_pretrain.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ÊâßË°åÈ¢ÑËÆ≠ÁªÉÔºåÂæóÂà∞ &lt;code&gt;pretrain_*.pth&lt;/code&gt; ‰Ωú‰∏∫È¢ÑËÆ≠ÁªÉÁöÑËæìÂá∫ÊùÉÈáçÔºàÂÖ∂‰∏≠*‰∏∫Ê®°ÂûãÁöÑdimensionÔºåÈªòËÆ§‰∏∫512Ôºâ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;3.2 ÁõëÁù£ÂæÆË∞ÉÔºàÂ≠¶ÂØπËØùÊñπÂºèÔºâ&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python train_full_sft.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ÊâßË°åÁõëÁù£ÂæÆË∞ÉÔºåÂæóÂà∞ &lt;code&gt;full_sft_*.pth&lt;/code&gt; ‰Ωú‰∏∫Êåá‰ª§ÂæÆË∞ÉÁöÑËæìÂá∫ÊùÉÈáçÔºàÂÖ∂‰∏≠&lt;code&gt;full&lt;/code&gt;Âç≥‰∏∫ÂÖ®ÂèÇÊï∞ÂæÆË∞ÉÔºâ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;Ê≥®ÔºöËÆ≠ÁªÉÈ°ªÁü•&lt;/summary&gt; 
 &lt;p&gt;ÊâÄÊúâËÆ≠ÁªÉËøáÁ®ãÈªòËÆ§ÊØèÈöî100Ê≠•‰øùÂ≠ò1Ê¨°ÂèÇÊï∞Âà∞Êñá‰ª∂&lt;code&gt;./out/***.pth&lt;/code&gt;ÔºàÊØèÊ¨°‰ºöË¶ÜÁõñÊéâÊóßÊùÉÈáçÊñá‰ª∂Ôºâ„ÄÇ&lt;/p&gt; 
 &lt;p&gt;ÁÆÄÂçïËµ∑ËßÅÔºåÊ≠§Â§ÑÂè™ÂÜôÊòé‰∏§‰∏™Èò∂ÊÆµËÆ≠ÁªÉËøáÁ®ã„ÄÇÂ¶ÇÈúÄÂÖ∂ÂÆÉËÆ≠ÁªÉ (LoRA, Ëí∏È¶è, Âº∫ÂåñÂ≠¶‰π†, ÂæÆË∞ÉÊé®ÁêÜÁ≠â) ÂèØÂèÇËÄÉ‰∏ãÊñá„ÄêÂÆûÈ™å„ÄëÂ∞èËäÇÁöÑËØ¶ÁªÜËØ¥Êòé„ÄÇ&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h3&gt;4.ÊµãËØïÊ®°ÂûãÊïàÊûú&lt;/h3&gt; 
&lt;p&gt;Á°Æ‰øùÈúÄË¶ÅÊµãËØïÁöÑÊ®°Âûã&lt;code&gt;*.pth&lt;/code&gt;Êñá‰ª∂‰Ωç‰∫é&lt;code&gt;./out/&lt;/code&gt;ÁõÆÂΩï‰∏ã„ÄÇ ‰πüÂèØ‰ª•Áõ¥Êé•Âéª&lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch/files"&gt;Ê≠§Â§Ñ&lt;/a&gt;‰∏ãËΩΩ‰ΩøÁî®ÊàëËÆ≠ÁªÉÁöÑ&lt;code&gt;*.pth&lt;/code&gt;Êñá‰ª∂„ÄÇ&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python eval_model.py --model_mode 1 # ÈªòËÆ§‰∏∫0ÔºöÊµãËØïpretrainÊ®°ÂûãÊïàÊûúÔºåËÆæÁΩÆ‰∏∫1ÔºöÊµãËØïfull_sftÊ®°ÂûãÊïàÊûú
&lt;/code&gt;&lt;/pre&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;Ê≥®ÔºöÊµãËØïÈ°ªÁü•&lt;/summary&gt; 
 &lt;p&gt;Â¶ÇÈúÄËØ¶ÊÉÖÔºåÊü•Áúã&lt;code&gt;eval_model.py&lt;/code&gt;ËÑöÊú¨‰ª£Á†ÅÂç≥ÂèØ„ÄÇmodel_modeÂàÜ‰∏∫ 0: È¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºå1: SFT-ChatÊ®°ÂûãÔºå2: RLHF-ChatÊ®°ÂûãÔºå3: ReasonÊ®°Âûã&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] ÊâÄÊúâËÆ≠ÁªÉËÑöÊú¨Âùá‰∏∫PytorchÂéüÁîüÊ°ÜÊû∂ÔºåÂùáÊîØÊåÅÂ§öÂç°Âä†ÈÄüÔºåÂÅáËÆæ‰Ω†ÁöÑËÆæÂ§áÊúâN (NÔºû1) Âº†ÊòæÂç°Ôºö&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;ÂçïÊú∫NÂç°ÂêØÂä®ËÆ≠ÁªÉÊñπÂºè (DDP, ÊîØÊåÅÂ§öÊú∫Â§öÂç°ÈõÜÁæ§)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node N train_xxx.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;Ê≥®ÔºöÂÖ∂ÂÆÉÈ°ªÁü•&lt;/summary&gt; 
 &lt;p&gt;ÂçïÊú∫NÂç°ÂêØÂä®ËÆ≠ÁªÉ (DeepSpeed)&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;deepspeed --master_port 29500 --num_gpus=N train_xxx.py
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;ÂèØÊ†πÊçÆÈúÄË¶ÅÂºÄÂêØwandbËÆ∞ÂΩïËÆ≠ÁªÉËøáÁ®ã&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# ÈúÄË¶ÅÁôªÂΩï: wandb login
torchrun --nproc_per_node N train_xxx.py --use_wandb
# and
python train_xxx.py --use_wandb
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;ÈÄöËøáÊ∑ªÂä†&lt;code&gt;--use_wandb&lt;/code&gt;ÂèÇÊï∞ÔºåÂèØ‰ª•ËÆ∞ÂΩïËÆ≠ÁªÉËøáÁ®ãÔºåËÆ≠ÁªÉÂÆåÊàêÂêéÔºåÂèØ‰ª•Âú®wandbÁΩëÁ´ô‰∏äÊü•ÁúãËÆ≠ÁªÉËøáÁ®ã„ÄÇÈÄöËøá‰øÆÊîπ&lt;code&gt;wandb_project&lt;/code&gt; Âíå&lt;code&gt;wandb_run_name&lt;/code&gt;ÂèÇÊï∞ÔºåÂèØ‰ª•ÊåáÂÆöÈ°πÁõÆÂêçÁß∞ÂíåËøêË°åÂêçÁß∞„ÄÇ&lt;/p&gt; 
&lt;/details&gt; 
&lt;h1&gt;üìå Êï∞ÊçÆ‰ªãÁªç&lt;/h1&gt; 
&lt;h2&gt;‚Ö† Tokenizer&lt;/h2&gt; 
&lt;p&gt;ÂàÜËØçÂô®Â∞ÜÂçïËØç‰ªéËá™ÁÑ∂ËØ≠Ë®ÄÈÄöËøá‚ÄúËØçÂÖ∏‚ÄùÊò†Â∞ÑÂà∞&lt;code&gt;0, 1, 36&lt;/code&gt;ËøôÊ†∑ÁöÑÊï∞Â≠óÔºåÂèØ‰ª•ÁêÜËß£‰∏∫Êï∞Â≠óÂ∞±‰ª£Ë°®‰∫ÜÂçïËØçÂú®‚ÄúËØçÂÖ∏‚Äù‰∏≠ÁöÑÈ°µÁ†Å„ÄÇ ÂèØ‰ª•ÈÄâÊã©Ëá™Â∑±ÊûÑÈÄ†ËØçË°®ËÆ≠ÁªÉ‰∏Ä‰∏™‚ÄúËØçÂÖ∏‚ÄùÔºå‰ª£Á†ÅÂèØËßÅ&lt;code&gt;./scripts/train_tokenizer.py&lt;/code&gt;Ôºà‰ªÖ‰æõÂ≠¶‰π†ÂèÇËÄÉÔºåËã•ÈùûÂøÖË¶ÅÊó†ÈúÄÂÜçËá™Ë°åËÆ≠ÁªÉÔºåMiniMindÂ∑≤Ëá™Â∏¶tokenizerÔºâ„ÄÇ ÊàñËÄÖÈÄâÊã©ÊØîËæÉÂá∫ÂêçÁöÑÂºÄÊ∫êÂ§ßÊ®°ÂûãÂàÜËØçÂô®Ôºå Ê≠£Â¶ÇÂêåÁõ¥Êé•Áî®Êñ∞Âçé/ÁâõÊ¥•ËØçÂÖ∏ÁöÑ‰ºòÁÇπÊòØtokenÁºñÁ†ÅÂéãÁº©ÁéáÂæàÂ•ΩÔºåÁº∫ÁÇπÊòØÈ°µÊï∞Â§™Â§öÔºåÂä®ËæÑÊï∞ÂçÅ‰∏á‰∏™ËØçÊ±áÁü≠ËØ≠Ôºõ Ëá™Â∑±ËÆ≠ÁªÉÁöÑÂàÜËØçÂô®Ôºå‰ºòÁÇπÊòØËØçË°®ÈïøÂ∫¶ÂíåÂÜÖÂÆπÈöèÊÑèÊéßÂà∂ÔºåÁº∫ÁÇπÊòØÂéãÁº©ÁéáÂæà‰ΩéÔºà‰æãÂ¶Ç"hello"‰πüËÆ∏‰ºöË¢´ÊãÜÂàÜ‰∏∫"h e l l o" ‰∫î‰∏™Áã¨Á´ãÁöÑtokenÔºâÔºå‰∏îÁîüÂÉªËØçÈöæ‰ª•Ë¶ÜÁõñ„ÄÇ ‚ÄúËØçÂÖ∏‚ÄùÁöÑÈÄâÊã©Âõ∫ÁÑ∂ÂæàÈáçË¶ÅÔºåLLMÁöÑËæìÂá∫Êú¨Ë¥®‰∏äÊòØSoftMaxÂà∞ËØçÂÖ∏N‰∏™ËØçÁöÑÂ§öÂàÜÁ±ªÈóÆÈ¢òÔºåÁÑ∂ÂêéÈÄöËøá‚ÄúËØçÂÖ∏‚ÄùËß£Á†ÅÂà∞Ëá™ÁÑ∂ËØ≠Ë®Ä„ÄÇ Âõ†‰∏∫MiniMind‰ΩìÁßØÈúÄË¶Å‰∏•Ê†ºÊéßÂà∂Ôºå‰∏∫‰∫ÜÈÅøÂÖçÊ®°ÂûãÂ§¥ÈáçËÑöËΩªÔºàËØçÂµåÂÖ•embeddingÂ±ÇÂèÇÊï∞Âú®LLMÂç†ÊØîÂ§™È´òÔºâÔºåÊâÄ‰ª•ËØçË°®ÈïøÂ∫¶Áü≠Áü≠ÁõäÂñÑ„ÄÇ&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;Tokenizer‰ªãÁªç&lt;/summary&gt; 
 &lt;p&gt;Á¨¨‰∏âÊñπÂº∫Â§ßÁöÑÂºÄÊ∫êÊ®°Âûã‰æãÂ¶ÇYi„ÄÅqwen„ÄÅchatglm„ÄÅmistral„ÄÅLlama3ÁöÑtokenizerËØçË°®ÈïøÂ∫¶Â¶Ç‰∏ãÔºö&lt;/p&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt;
    &lt;th&gt;TokenizerÊ®°Âûã&lt;/th&gt;
    &lt;th&gt;ËØçË°®Â§ßÂ∞è&lt;/th&gt;
    &lt;th&gt;Êù•Ê∫ê&lt;/th&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;yi tokenizer&lt;/td&gt;
    &lt;td&gt;64,000&lt;/td&gt;
    &lt;td&gt;01‰∏áÁâ©Ôºà‰∏≠ÂõΩÔºâ&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;qwen2 tokenizer&lt;/td&gt;
    &lt;td&gt;151,643&lt;/td&gt;
    &lt;td&gt;ÈòøÈáå‰∫ëÔºà‰∏≠ÂõΩÔºâ&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;glm tokenizer&lt;/td&gt;
    &lt;td&gt;151,329&lt;/td&gt;
    &lt;td&gt;Êô∫Ë∞±AIÔºà‰∏≠ÂõΩÔºâ&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;mistral tokenizer&lt;/td&gt;
    &lt;td&gt;32,000&lt;/td&gt;
    &lt;td&gt;Mistral AIÔºàÊ≥ïÂõΩÔºâ&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;llama3 tokenizer&lt;/td&gt;
    &lt;td&gt;128,000&lt;/td&gt;
    &lt;td&gt;MetaÔºàÁæéÂõΩÔºâ&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;minimind tokenizer&lt;/td&gt;
    &lt;td&gt;6,400&lt;/td&gt;
    &lt;td&gt;Ëá™ÂÆö‰πâ&lt;/td&gt;
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;üëâ2024-09-17Êõ¥Êñ∞Ôºö‰∏∫‰∫ÜÈò≤Ê≠¢ËøáÂéªÁöÑÁâàÊú¨Ê≠ß‰πâ&amp;amp;ÊéßÂà∂‰ΩìÁßØÔºåminimindÊâÄÊúâÊ®°ÂûãÂùá‰ΩøÁî®minimind_tokenizerÂàÜËØçÔºåÂ∫üÂºÉÊâÄÊúâmistral_tokenizerÁâàÊú¨„ÄÇ&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;pre&gt;&lt;code&gt;# ‰∏Ä‰∫õËá™Ë®ÄËá™ËØ≠
&amp;gt; Â∞ΩÁÆ°minimind_tokenizerÈïøÂ∫¶ÂæàÂ∞èÔºåÁºñËß£Á†ÅÊïàÁéáÂº±‰∫éqwen2„ÄÅglmÁ≠â‰∏≠ÊñáÂèãÂ•ΩÂûãÂàÜËØçÂô®„ÄÇ
&amp;gt; ‰ΩÜminimindÊ®°ÂûãÈÄâÊã©‰∫ÜËá™Â∑±ËÆ≠ÁªÉÁöÑminimind_tokenizer‰Ωú‰∏∫ÂàÜËØçÂô®Ôºå‰ª•‰øùÊåÅÊï¥‰ΩìÂèÇÊï∞ËΩªÈáèÔºåÈÅøÂÖçÁºñÁ†ÅÂ±ÇÂíåËÆ°ÁÆóÂ±ÇÂç†ÊØîÂ§±Ë°°ÔºåÂ§¥ÈáçËÑöËΩªÔºåÂõ†‰∏∫minimindÁöÑËØçË°®Â§ßÂ∞èÂè™Êúâ6400„ÄÇ
&amp;gt; ‰∏îminimindÂú®ÂÆûÈôÖÊµãËØï‰∏≠Ê≤°ÊúâÂá∫Áé∞ËøáÁîüÂÉªËØçÊ±áËß£Á†ÅÂ§±Ë¥•ÁöÑÊÉÖÂÜµÔºåÊïàÊûúËâØÂ•Ω„ÄÇ
&amp;gt; Áî±‰∫éËá™ÂÆö‰πâËØçË°®ÂéãÁº©ÈïøÂ∫¶Âà∞6400Ôºå‰ΩøÂæóLLMÊÄªÂèÇÊï∞ÈáèÊúÄ‰ΩéÂè™Êúâ25.8M„ÄÇ
&amp;gt; ËÆ≠ÁªÉÊï∞ÊçÆ`tokenizer_train.jsonl`ÂùáÊù•Ëá™‰∫é`Âå†Êï∞Â§ßÊ®°ÂûãÊï∞ÊçÆÈõÜ`ÔºåËøôÈÉ®ÂàÜÊï∞ÊçÆÁõ∏ÂØπÊ¨°Ë¶ÅÔºåÂ¶ÇÈúÄËÆ≠ÁªÉÂèØ‰ª•Ëá™Áî±ÈÄâÊã©„ÄÇ
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚Ö° PretrainÊï∞ÊçÆ&lt;/h2&gt; 
&lt;p&gt;ÁªèÂéÜ‰∫ÜMiniMind-V1ÁöÑ‰ΩéË¥®ÈáèÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÔºåÂØºËá¥Ê®°ÂûãËÉ°Ë®Ä‰π±ËØ≠ÁöÑÊïôËÆ≠Ôºå&lt;code&gt;2025-02-05&lt;/code&gt; ‰πãÂêéÂÜ≥ÂÆö‰∏çÂÜçÈááÁî®Â§ßËßÑÊ®°Êó†ÁõëÁù£ÁöÑÊï∞ÊçÆÈõÜÂÅöÈ¢ÑËÆ≠ÁªÉ„ÄÇ ËøõËÄåÂ∞ùËØïÊää&lt;a href="https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data"&gt;Âå†Êï∞Â§ßÊ®°ÂûãÊï∞ÊçÆÈõÜ&lt;/a&gt;ÁöÑ‰∏≠ÊñáÈÉ®ÂàÜÊèêÂèñÂá∫Êù•Ôºå Ê∏ÖÊ¥óÂá∫Â≠óÁ¨¶&lt;code&gt;&amp;lt;512&lt;/code&gt;ÈïøÂ∫¶ÁöÑÂ§ßÁ∫¶1.6GBÁöÑËØ≠ÊñôÁõ¥Êé•ÊãºÊé•ÊàêÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆ &lt;code&gt;pretrain_hq.jsonl&lt;/code&gt;ÔºåhqÂç≥‰∏∫high qualityÔºàÂΩìÁÑ∂‰πüËøò‰∏çÁÆóhighÔºåÊèêÂçáÊï∞ÊçÆË¥®ÈáèÊó†Ê≠¢Â∞ΩÔºâ„ÄÇ&lt;/p&gt; 
&lt;p&gt;Êñá‰ª∂&lt;code&gt;pretrain_hq.jsonl&lt;/code&gt; Êï∞ÊçÆÊ†ºÂºè‰∏∫&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;{"text": "Â¶Ç‰ΩïÊâçËÉΩÊëÜËÑ±ÊãñÂª∂ÁóáÔºü Ê≤ªÊÑàÊãñÂª∂ÁóáÂπ∂‰∏çÂÆπÊòìÔºå‰ΩÜ‰ª•‰∏ãÂª∫ËÆÆÂèØËÉΩÊúâÊâÄÂ∏ÆÂä©..."}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;‚Ö¢ SFTÊï∞ÊçÆ&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data"&gt;Âå†Êï∞Â§ßÊ®°ÂûãSFTÊï∞ÊçÆÈõÜ&lt;/a&gt; ‚ÄúÊòØ‰∏Ä‰∏™ÂÆåÊï¥„ÄÅÊ†ºÂºèÁªü‰∏Ä„ÄÅÂÆâÂÖ®ÁöÑÂ§ßÊ®°ÂûãËÆ≠ÁªÉÂíåÁ†îÁ©∂ËµÑÊ∫ê„ÄÇ ‰ªéÁΩëÁªú‰∏äÁöÑÂÖ¨ÂºÄÊï∞ÊçÆÊ∫êÊî∂ÈõÜÂπ∂Êï¥ÁêÜ‰∫ÜÂ§ßÈáèÂºÄÊ∫êÊï∞ÊçÆÈõÜÔºåÂØπÂÖ∂ËøõË°å‰∫ÜÊ†ºÂºèÁªü‰∏ÄÔºåÊï∞ÊçÆÊ∏ÖÊ¥óÔºå ÂåÖÂê´10MÊù°Êï∞ÊçÆÁöÑ‰∏≠ÊñáÊï∞ÊçÆÈõÜÂíåÂåÖÂê´2MÊù°Êï∞ÊçÆÁöÑËã±ÊñáÊï∞ÊçÆÈõÜ„ÄÇ‚Äù ‰ª•‰∏äÊòØÂÆòÊñπ‰ªãÁªçÔºå‰∏ãËΩΩÊñá‰ª∂ÂêéÁöÑÊï∞ÊçÆÊÄªÈáèÂ§ßÁ∫¶Âú®4B tokensÔºåËÇØÂÆöÊòØÈÄÇÂêà‰Ωú‰∏∫‰∏≠ÊñáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑSFTÊï∞ÊçÆÁöÑ„ÄÇ ‰ΩÜÊòØÂÆòÊñπÊèê‰æõÁöÑÊï∞ÊçÆÊ†ºÂºèÂæà‰π±ÔºåÂÖ®ÈÉ®Áî®Êù•sft‰ª£‰ª∑Â§™Â§ß„ÄÇ ÊàëÂ∞ÜÊääÂÆòÊñπÊï∞ÊçÆÈõÜËøõË°å‰∫Ü‰∫åÊ¨°Ê∏ÖÊ¥óÔºåÊääÂê´ÊúâÁ¨¶Âè∑Ê±°ÊüìÂíåÂô™Â£∞ÁöÑÊù°ÁõÆÂéªÈô§ÔºõÂè¶Â§ñ‰æùÁÑ∂Âè™‰øùÁïô‰∫ÜÊÄªÈïøÂ∫¶&lt;code&gt;&amp;lt;512&lt;/code&gt; ÁöÑÂÜÖÂÆπÔºåÊ≠§Èò∂ÊÆµÂ∏åÊúõÈÄöËøáÂ§ßÈáèÂØπËØùË°•ÂÖÖÈ¢ÑËÆ≠ÁªÉÈò∂ÊÆµÊ¨†Áº∫ÁöÑÁü•ËØÜ„ÄÇ ÂØºÂá∫Êñá‰ª∂‰∏∫&lt;code&gt;sft_512.jsonl&lt;/code&gt;(~7.5GB)„ÄÇ&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.modelscope.cn/organization/Magpie-Align"&gt;Magpie-SFTÊï∞ÊçÆÈõÜ&lt;/a&gt; Êî∂ÈõÜ‰∫Ü~1MÊù°Êù•Ëá™Qwen2/2.5ÁöÑÈ´òË¥®ÈáèÂØπËØùÔºåÊàëÂ∞ÜËøôÈÉ®ÂàÜÊï∞ÊçÆËøõ‰∏ÄÊ≠•Ê∏ÖÊ¥óÔºåÊääÊÄªÈïøÂ∫¶&lt;code&gt;&amp;lt;2048&lt;/code&gt;ÁöÑÈÉ®ÂàÜÂØºÂá∫‰∏∫&lt;code&gt;sft_2048.jsonl&lt;/code&gt;(~9GB)„ÄÇ ÈïøÂ∫¶&lt;code&gt;&amp;lt;1024&lt;/code&gt;ÁöÑÈÉ®ÂàÜÂØºÂá∫‰∏∫&lt;code&gt;sft_1024.jsonl&lt;/code&gt;(~5.5GB)ÔºåÁî®Â§ßÊ®°ÂûãÂØπËØùÊï∞ÊçÆÁõ¥Êé•ËøõË°åsftÂ∞±Â±û‰∫é‚ÄúÈªëÁõíËí∏È¶è‚ÄùÁöÑËåÉÁï¥„ÄÇ&lt;/p&gt; 
&lt;p&gt;Ëøõ‰∏ÄÊ≠•Ê∏ÖÊ¥óÂâç‰∏§Ê≠•sftÁöÑÊï∞ÊçÆÔºàÂè™‰øùÁïô‰∏≠ÊñáÂ≠óÁ¨¶Âç†ÊØîÈ´òÁöÑÂÜÖÂÆπÔºâÔºåÁ≠õÈÄâÈïøÂ∫¶&lt;code&gt;&amp;lt;512&lt;/code&gt;ÁöÑÂØπËØùÔºåÂæóÂà∞&lt;code&gt;sft_mini_512.jsonl&lt;/code&gt;(~1.2GB)„ÄÇ&lt;/p&gt; 
&lt;p&gt;ÊâÄÊúâsftÊñá‰ª∂ &lt;code&gt;sft_X.jsonl&lt;/code&gt; Êï∞ÊçÆÊ†ºÂºèÂùá‰∏∫&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;{
    "conversations": [
        {"role": "user", "content": "‰Ω†Â•Ω"},
        {"role": "assistant", "content": "‰Ω†Â•ΩÔºÅ"},
        {"role": "user", "content": "ÂÜçËßÅ"},
        {"role": "assistant", "content": "ÂÜçËßÅÔºÅ"}
    ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;‚Ö£ RLHFÊï∞ÊçÆ&lt;/h2&gt; 
&lt;p&gt;Êù•Ëá™&lt;a href="https://www.modelscope.cn/datasets/Magpie-Align/MagpieLM-DPO-Data-v0.1"&gt;Magpie-DPOÊï∞ÊçÆÈõÜ&lt;/a&gt; Â§ßÁ∫¶200kÊù°ÂÅèÂ•ΩÊï∞ÊçÆÔºàÂùáÊòØËã±ÊñáÔºâÁîüÊàêËá™Llama3.1-70B/8BÔºåÂèØ‰ª•Áî®‰∫éËÆ≠ÁªÉÂ•ñÂä±Ê®°ÂûãÔºå‰ºòÂåñÊ®°ÂûãÂõûÂ§çË¥®ÈáèÔºå‰ΩøÂÖ∂Êõ¥Âä†Á¨¶Âêà‰∫∫Á±ªÂÅèÂ•Ω„ÄÇ ËøôÈáåÂ∞ÜÊï∞ÊçÆÊÄªÈïøÂ∫¶&lt;code&gt;&amp;lt;3000&lt;/code&gt;ÁöÑÂÜÖÂÆπÈáçÁªÑ‰∏∫&lt;code&gt;dpo.jsonl&lt;/code&gt;(~0.9GB)ÔºåÂåÖÂê´&lt;code&gt;chosen&lt;/code&gt;Âíå&lt;code&gt;rejected&lt;/code&gt;‰∏§‰∏™Â≠óÊÆµÔºå&lt;code&gt;chosen&lt;/code&gt; ‰∏∫ÂÅèÂ•ΩÁöÑÂõûÂ§çÔºå&lt;code&gt;rejected&lt;/code&gt;‰∏∫ÊãíÁªùÁöÑÂõûÂ§ç„ÄÇ&lt;/p&gt; 
&lt;p&gt;Êñá‰ª∂ &lt;code&gt;dpo.jsonl&lt;/code&gt; Êï∞ÊçÆÊ†ºÂºè‰∏∫&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;{
  "chosen": [
    {"content": "Q", "role": "user"}, 
    {"content": "good answer", "role": "assistant"}
  ], 
  "rejected": [
    {"content": "Q", "role": "user"}, 
    {"content": "bad answer", "role": "assistant"}
  ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;‚Ö§ ReasonÊï∞ÊçÆÈõÜÔºö&lt;/h2&gt; 
&lt;p&gt;‰∏çÂæó‰∏çËØ¥2025Âπ¥2ÊúàË∞ÅËÉΩÁÅ´ÁöÑËøáDeepSeek... ‰πüÊøÄÂèë‰∫ÜÊàëÂØπRLÂºïÂØºÁöÑÊé®ÁêÜÊ®°ÂûãÁöÑÊµìÂéöÂÖ¥Ë∂£ÔºåÁõÆÂâçÂ∑≤ÁªèÁî®Qwen2.5Â§çÁé∞‰∫ÜR1-Zero„ÄÇ Â¶ÇÊûúÊúâÊó∂Èó¥+ÊïàÊûúworkÔºà‰ΩÜ99%Âü∫Ê®°ËÉΩÂäõ‰∏çË∂≥ÔºâÊàë‰ºöÂú®‰πãÂêéÊõ¥Êñ∞MiniMindÂü∫‰∫éRLËÆ≠ÁªÉÁöÑÊé®ÁêÜÊ®°ÂûãËÄå‰∏çÊòØËí∏È¶èÊ®°Âûã„ÄÇ Êó∂Èó¥ÊúâÈôêÔºåÊúÄÂø´ÁöÑ‰ΩéÊàêÊú¨ÊñπÊ°à‰æùÁÑ∂ÊòØÁõ¥Êé•Ëí∏È¶èÔºàÈªëÁõíÊñπÂºèÔºâ„ÄÇ ËÄê‰∏ç‰ΩèR1Â§™ÁÅ´ÔºåÁü≠Áü≠Âá†Â§©Â∞±Â∑≤ÁªèÂ≠òÂú®‰∏Ä‰∫õR1ÁöÑËí∏È¶èÊï∞ÊçÆÈõÜ&lt;a href="https://www.modelscope.cn/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B"&gt;R1-Llama-70B&lt;/a&gt;„ÄÅ&lt;a href="https://www.modelscope.cn/datasets/AI-ModelScope/R1-Distill-SFT"&gt;R1-Distill-SFT&lt;/a&gt;„ÄÅ &lt;a href="https://huggingface.co/datasets/shareAI/Alpaca-Distill-R1-ZH"&gt;Alpaca-Distill-R1&lt;/a&gt;„ÄÅ &lt;a href="https://huggingface.co/datasets/jinliuxi/deepseek_r1_zh"&gt;deepseek_r1_zh&lt;/a&gt;Á≠âÁ≠âÔºåÁ∫Ø‰∏≠ÊñáÁöÑÊï∞ÊçÆÂèØËÉΩÊØîËæÉÂ∞ë„ÄÇ ÊúÄÁªàÊï¥ÂêàÂÆÉ‰ª¨ÔºåÂØºÂá∫Êñá‰ª∂‰∏∫&lt;code&gt;r1_mix_1024.jsonl&lt;/code&gt;ÔºåÊï∞ÊçÆÊ†ºÂºèÂíå&lt;code&gt;sft_X.jsonl&lt;/code&gt;‰∏ÄËá¥„ÄÇ&lt;/p&gt; 
&lt;h2&gt;‚Ö• Êõ¥Â§öÊï∞ÊçÆÈõÜ&lt;/h2&gt; 
&lt;p&gt;ÁõÆÂâçÂ∑≤ÁªèÊúâ&lt;a href="https://github.com/HqWu-HITCS/Awesome-Chinese-LLM"&gt;HqWu-HITCS/Awesome-Chinese-LLM&lt;/a&gt; Âú®Êî∂ÈõÜÂíåÊ¢≥ÁêÜ‰∏≠ÊñáLLMÁõ∏ÂÖ≥ÁöÑÂºÄÊ∫êÊ®°Âûã„ÄÅÂ∫îÁî®„ÄÅÊï∞ÊçÆÈõÜÂèäÊïôÁ®ãÁ≠âËµÑÊñôÔºåÂπ∂ÊåÅÁª≠Êõ¥Êñ∞ËøôÊñπÈù¢ÁöÑÊúÄÊñ∞ËøõÂ±ï„ÄÇÂÖ®Èù¢‰∏î‰∏ì‰∏öÔºåRespectÔºÅ&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚Öß MiniMindËÆ≠ÁªÉÊï∞ÊçÆÈõÜ&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] 2025-02-05ÂêéÔºåÂºÄÊ∫êMiniMindÊúÄÁªàËÆ≠ÁªÉÊâÄÁî®ÁöÑÊâÄÊúâÊï∞ÊçÆÈõÜÔºåÂõ†Ê≠§Êó†ÈúÄÂÜçËá™Ë°åÈ¢ÑÂ§ÑÁêÜÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÔºåÈÅøÂÖçÈáçÂ§çÊÄßÁöÑÊï∞ÊçÆÂ§ÑÁêÜÂ∑•‰Ωú„ÄÇ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;MiniMindËÆ≠ÁªÉÊï∞ÊçÆÈõÜ‰∏ãËΩΩÂú∞ÂùÄÔºö &lt;a href="https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files"&gt;ModelScope&lt;/a&gt; | &lt;a href="https://huggingface.co/datasets/jingyaogong/minimind_dataset/tree/main"&gt;HuggingFace&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Êó†ÈúÄÂÖ®ÈÉ®cloneÔºåÂèØÂçïÁã¨‰∏ãËΩΩÊâÄÈúÄÁöÑÊñá‰ª∂&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Â∞Ü‰∏ãËΩΩÁöÑÊï∞ÊçÆÈõÜÊñá‰ª∂ÊîæÂà∞&lt;code&gt;./dataset/&lt;/code&gt;ÁõÆÂΩï‰∏ãÔºà‚ú®‰∏∫Êé®ËçêÁöÑÂøÖÈ°ªÈ°πÔºâ&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./dataset/
‚îú‚îÄ‚îÄ dpo.jsonl (909MB)
‚îú‚îÄ‚îÄ lora_identity.jsonl (22.8KB)
‚îú‚îÄ‚îÄ lora_medical.jsonl (34MB)
‚îú‚îÄ‚îÄ pretrain_hq.jsonl (1.6GB, ‚ú®)
‚îú‚îÄ‚îÄ r1_mix_1024.jsonl (340MB)
‚îú‚îÄ‚îÄ sft_1024.jsonl (5.6GB)
‚îú‚îÄ‚îÄ sft_2048.jsonl (9GB)
‚îú‚îÄ‚îÄ sft_512.jsonl (7.5GB)
‚îú‚îÄ‚îÄ sft_mini_512.jsonl (1.2GB, ‚ú®)
‚îî‚îÄ‚îÄ tokenizer_train.jsonl (1GB)
&lt;/code&gt;&lt;/pre&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;Ê≥®ÔºöÂêÑÊï∞ÊçÆÈõÜÁÆÄ‰ªã&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;dpo.jsonl&lt;/code&gt; --RLHFÈò∂ÊÆµÊï∞ÊçÆÈõÜ&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;lora_identity.jsonl&lt;/code&gt; --Ëá™ÊàëËÆ§Áü•Êï∞ÊçÆÈõÜÔºà‰æãÂ¶ÇÔºö‰Ω†ÊòØË∞ÅÔºüÊàëÊòØminimind...ÔºâÔºåÊé®ËçêÁî®‰∫éloraËÆ≠ÁªÉÔºà‰∫¶ÂèØÁî®‰∫éÂÖ®ÂèÇSFTÔºåÂãøË¢´ÂêçÂ≠óÂ±ÄÈôêÔºâ&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;lora_medical.jsonl&lt;/code&gt; --ÂåªÁñóÈóÆÁ≠îÊï∞ÊçÆÈõÜÔºåÊé®ËçêÁî®‰∫éloraËÆ≠ÁªÉÔºà‰∫¶ÂèØÁî®‰∫éÂÖ®ÂèÇSFTÔºåÂãøË¢´ÂêçÂ≠óÂ±ÄÈôêÔºâ&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;pretrain_hq.jsonl&lt;/code&gt;‚ú® --È¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÔºåÊï¥ÂêàËá™jiangshuÁßëÊäÄ&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;r1_mix_1024.jsonl&lt;/code&gt; --DeepSeek-R1-1.5BËí∏È¶èÊï∞ÊçÆÔºåÊØèÊù°Êï∞ÊçÆÂ≠óÁ¨¶ÊúÄÂ§ßÈïøÂ∫¶‰∏∫1024ÔºàÂõ†Ê≠§ËÆ≠ÁªÉÊó∂ËÆæÁΩÆmax_seq_len=1024Ôºâ&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;sft_1024.jsonl&lt;/code&gt; --Êï¥ÂêàËá™Qwen2.5Ëí∏È¶èÊï∞ÊçÆÔºàÊòØsft_2048ÁöÑÂ≠êÈõÜÔºâÔºåÊØèÊù°Êï∞ÊçÆÂ≠óÁ¨¶ÊúÄÂ§ßÈïøÂ∫¶‰∏∫1024ÔºàÂõ†Ê≠§ËÆ≠ÁªÉÊó∂ËÆæÁΩÆmax_seq_len=1024Ôºâ&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;sft_2048.jsonl&lt;/code&gt; --Êï¥ÂêàËá™Qwen2.5Ëí∏È¶èÊï∞ÊçÆÔºåÊØèÊù°Êï∞ÊçÆÂ≠óÁ¨¶ÊúÄÂ§ßÈïøÂ∫¶‰∏∫2048ÔºàÂõ†Ê≠§ËÆ≠ÁªÉÊó∂ËÆæÁΩÆmax_seq_len=2048Ôºâ&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;sft_512.jsonl&lt;/code&gt; --Êï¥ÂêàËá™Âå†Êï∞ÁßëÊäÄSFTÊï∞ÊçÆÔºåÊØèÊù°Êï∞ÊçÆÂ≠óÁ¨¶ÊúÄÂ§ßÈïøÂ∫¶‰∏∫512ÔºàÂõ†Ê≠§ËÆ≠ÁªÉÊó∂ËÆæÁΩÆmax_seq_len=512Ôºâ&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;sft_mini_512.jsonl&lt;/code&gt;‚ú® --ÊûÅÁÆÄÊï¥ÂêàËá™Âå†Êï∞ÁßëÊäÄSFTÊï∞ÊçÆ+Qwen2.5Ëí∏È¶èÊï∞ÊçÆÔºàÁî®‰∫éÂø´ÈÄüËÆ≠ÁªÉZeroÊ®°ÂûãÔºâÔºåÊØèÊù°Êï∞ÊçÆÂ≠óÁ¨¶ÊúÄÂ§ßÈïøÂ∫¶‰∏∫512ÔºàÂõ†Ê≠§ËÆ≠ÁªÉÊó∂ËÆæÁΩÆmax_seq_len=512Ôºâ&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;tokenizer_train.jsonl&lt;/code&gt; --ÂùáÊù•Ëá™‰∫é&lt;code&gt;Âå†Êï∞Â§ßÊ®°ÂûãÊï∞ÊçÆÈõÜ&lt;/code&gt;ÔºåËøôÈÉ®ÂàÜÊï∞ÊçÆÁõ∏ÂØπÊ¨°Ë¶ÅÔºåÔºà‰∏çÊé®ËçêËá™Â∑±ÈáçÂ§çËÆ≠ÁªÉtokenizerÔºåÁêÜÁî±Â¶Ç‰∏äÔºâÂ¶ÇÈúÄËá™Â∑±ËÆ≠ÁªÉtokenizerÂèØ‰ª•Ëá™Áî±ÈÄâÊã©Êï∞ÊçÆÈõÜ„ÄÇ&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/dataset.jpg" alt="dataset" /&gt;&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;ËØ¥Êòé &amp;amp; Êé®ËçêËÆ≠ÁªÉÊñπÊ°à&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;MiniMind2 SeriesÂùáÁªèËøáÂÖ±Á∫¶20GBËØ≠ÊñôËÆ≠ÁªÉÔºåÂ§ßÁ∫¶4B tokensÔºåÂç≥ÂØπÂ∫î‰∏äÈù¢ÁöÑÊï∞ÊçÆÁªÑÂêàËÆ≠ÁªÉÁªìÊûúÔºàÂºÄÈîÄÔºöüí∞üí∞üí∞üí∞üí∞üí∞üí∞üí∞ÔºåÊïàÊûúÔºöüòäüòäüòäüòäüòäüòäÔºâ&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;ÊÉ≥Ë¶ÅÊúÄÂø´ÈÄüÂ∫¶‰ªé0ÂÆûÁé∞ZeroÊ®°ÂûãÔºåÊé®Ëçê‰ΩøÁî®&lt;code&gt;pretrain_hq.jsonl&lt;/code&gt; + &lt;code&gt;sft_mini_512.jsonl&lt;/code&gt; ÁöÑÊï∞ÊçÆÁªÑÂêàÔºåÂÖ∑‰ΩìËä±ÈîÄÂíåÊïàÊûúÂèØÊü•Áúã‰∏ãÊñáË°®Ê†ºÔºàÂºÄÈîÄÔºöüí∞ÔºåÊïàÊûúÔºöüòäüòäÔºâ&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Êé®ËçêÂÖ∑Â§á‰∏ÄÂÆöÁÆóÂäõËµÑÊ∫êÊàñÊõ¥Âú®ÊÑèÊïàÊûúÁöÑÊúãÂèãÂèØ‰ª•ËÄÉËôëÂâçËÄÖÂÆåÊï¥Â§çÁé∞MiniMind2Ôºõ‰ªÖÊúâÂçïÂç°GPUÊàñÂú®‰πéÁü≠Êó∂Èó¥Âø´ÈÄüÂ§çÁé∞ÁöÑÊúãÂèãÂº∫ÁÉàÊé®ËçêÂêéËÄÖÔºõ&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;„ÄêÊäò‰∏≠ÊñπÊ°à„Äë‰∫¶ÂèØÈÄâÊã©‰æãÂ¶Ç&lt;code&gt;sft_mini_512.jsonl&lt;/code&gt;„ÄÅ&lt;code&gt;sft_1024.jsonl&lt;/code&gt;‰∏≠Á≠âËßÑÊ®°Êï∞ÊçÆËøõË°åËá™Áî±ÁªÑÂêàËÆ≠ÁªÉÔºàÂºÄÈîÄÔºöüí∞üí∞üí∞ÔºåÊïàÊûúÔºöüòäüòäüòäüòäÔºâ„ÄÇ&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h1&gt;üìå Model Structure&lt;/h1&gt; 
&lt;p&gt;MiniMind-DenseÔºàÂíå&lt;a href="https://ai.meta.com/blog/meta-llama-3-1/"&gt;Llama3.1&lt;/a&gt;‰∏ÄÊ†∑Ôºâ‰ΩøÁî®‰∫ÜTransformerÁöÑDecoder-OnlyÁªìÊûÑÔºåË∑üGPT-3ÁöÑÂå∫Âà´Âú®‰∫éÔºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ÈááÁî®‰∫ÜGPT-3ÁöÑÈ¢ÑÊ†áÂáÜÂåñÊñπÊ≥ïÔºå‰πüÂ∞±ÊòØÂú®ÊØè‰∏™TransformerÂ≠êÂ±ÇÁöÑËæìÂÖ•‰∏äËøõË°åÂΩí‰∏ÄÂåñÔºåËÄå‰∏çÊòØÂú®ËæìÂá∫‰∏ä„ÄÇÂÖ∑‰ΩìÊù•ËØ¥Ôºå‰ΩøÁî®ÁöÑÊòØRMSNormÂΩí‰∏ÄÂåñÂáΩÊï∞„ÄÇ&lt;/li&gt; 
 &lt;li&gt;Áî®SwiGLUÊøÄÊ¥ªÂáΩÊï∞Êõø‰ª£‰∫ÜReLUÔºåËøôÊ†∑ÂÅöÊòØ‰∏∫‰∫ÜÊèêÈ´òÊÄßËÉΩ„ÄÇ&lt;/li&gt; 
 &lt;li&gt;ÂÉèGPT-Neo‰∏ÄÊ†∑ÔºåÂéªÊéâ‰∫ÜÁªùÂØπ‰ΩçÁΩÆÂµåÂÖ•ÔºåÊîπÁî®‰∫ÜÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÔºàRoPEÔºâÔºåËøôÊ†∑Âú®Â§ÑÁêÜË∂ÖÂá∫ËÆ≠ÁªÉÈïøÂ∫¶ÁöÑÊé®ÁêÜÊó∂ÊïàÊûúÊõ¥Â•Ω„ÄÇ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;MiniMind-MoEÊ®°ÂûãÔºåÂÆÉÁöÑÁªìÊûÑÂü∫‰∫éLlama3Âíå&lt;a href="https://arxiv.org/pdf/2405.04434"&gt;Deepseek-V2/3&lt;/a&gt;‰∏≠ÁöÑMixFFNÊ∑∑Âêà‰∏ìÂÆ∂Ê®°Âùó„ÄÇ&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;DeepSeek-V2Âú®ÂâçÈ¶àÁΩëÁªúÔºàFFNÔºâÊñπÈù¢ÔºåÈááÁî®‰∫ÜÊõ¥ÁªÜÁ≤íÂ∫¶ÁöÑ‰∏ìÂÆ∂ÂàÜÂâ≤ÂíåÂÖ±‰∫´ÁöÑ‰∏ìÂÆ∂ÈöîÁ¶ªÊäÄÊúØÔºå‰ª•ÊèêÈ´òExpertsÁöÑÊïàÊûú„ÄÇ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;MiniMindÁöÑÊï¥‰ΩìÁªìÊûÑ‰∏ÄËá¥ÔºåÂè™ÊòØÂú®RoPEËÆ°ÁÆó„ÄÅÊé®ÁêÜÂáΩÊï∞ÂíåFFNÂ±ÇÁöÑ‰ª£Á†Å‰∏äÂÅö‰∫Ü‰∏Ä‰∫õÂ∞èË∞ÉÊï¥„ÄÇ ÂÖ∂ÁªìÊûÑÂ¶Ç‰∏ãÂõæÔºàÈáçÁªòÁâàÔºâÔºö&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/LLM-structure.png" alt="structure" /&gt; &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/LLM-structure-moe.png" alt="structure-moe" /&gt;&lt;/p&gt; 
&lt;p&gt;‰øÆÊîπÊ®°ÂûãÈÖçÁΩÆËßÅ&lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/model/LMConfig.py"&gt;./model/LMConfig.py&lt;/a&gt;„ÄÇ ÂèÇËÄÉÊ®°ÂûãÂèÇÊï∞ÁâàÊú¨ËßÅ‰∏ãË°®Ôºö&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Name&lt;/th&gt; 
   &lt;th&gt;params&lt;/th&gt; 
   &lt;th&gt;len_vocab&lt;/th&gt; 
   &lt;th&gt;rope_theta&lt;/th&gt; 
   &lt;th&gt;n_layers&lt;/th&gt; 
   &lt;th&gt;d_model&lt;/th&gt; 
   &lt;th&gt;kv_heads&lt;/th&gt; 
   &lt;th&gt;q_heads&lt;/th&gt; 
   &lt;th&gt;share+route&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-Small&lt;/td&gt; 
   &lt;td&gt;26M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e6&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;512&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-MoE&lt;/td&gt; 
   &lt;td&gt;145M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e6&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;640&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;1+4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2&lt;/td&gt; 
   &lt;td&gt;104M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e6&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;768&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;minimind-v1-small&lt;/td&gt; 
   &lt;td&gt;26M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e4&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;512&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;minimind-v1-moe&lt;/td&gt; 
   &lt;td&gt;4√ó26M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e4&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;512&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;1+4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;minimind-v1&lt;/td&gt; 
   &lt;td&gt;108M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e4&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;768&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;üìå Experiment&lt;/h1&gt; 
&lt;h2&gt;‚Ö† ËÆ≠ÁªÉÂºÄÈîÄ&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Êó∂Èó¥Âçï‰Ωç&lt;/strong&gt;ÔºöÂ∞èÊó∂ (h)„ÄÇ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÊàêÊú¨Âçï‰Ωç&lt;/strong&gt;Ôºö‰∫∫Ê∞ëÂ∏Å (Ôø•)Ôºõ7Ôø• ‚âà 1ÁæéÂÖÉ„ÄÇ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;3090 ÁßüÂç°Âçï‰ª∑&lt;/strong&gt;Ôºö‚âà1.3Ôø•/hÔºàÂèØËá™Ë°åÂèÇËÄÉÂÆûÊó∂Â∏Ç‰ª∑Ôºâ„ÄÇ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÂèÇËÄÉÊ†áÂáÜ&lt;/strong&gt;ÔºöË°®Ê†º‰ªÖÂÆûÊµã &lt;code&gt;pretrain&lt;/code&gt; Âíå &lt;code&gt;sft_mini_512&lt;/code&gt; ‰∏§‰∏™Êï∞ÊçÆÈõÜÁöÑËÆ≠ÁªÉÊó∂Èó¥ÔºåÂÖ∂ÂÆÉËÄóÊó∂Ê†πÊçÆÊï∞ÊçÆÈõÜÂ§ßÂ∞è‰º∞ÁÆóÔºàÂèØËÉΩÂ≠òÂú®‰∫õËÆ∏Âá∫ÂÖ•Ôºâ„ÄÇ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Âü∫‰∫é 3090 ÔºàÂçïÂç°ÔºâÊàêÊú¨ËÆ°ÁÆó&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Name&lt;/th&gt; 
   &lt;th&gt;params&lt;/th&gt; 
   &lt;th&gt;pretrain&lt;/th&gt; 
   &lt;th&gt;sft_mini_512&lt;/th&gt; 
   &lt;th&gt;sft_512&lt;/th&gt; 
   &lt;th&gt;sft_1024&lt;/th&gt; 
   &lt;th&gt;sft_2048&lt;/th&gt; 
   &lt;th&gt;RLHF&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-Small&lt;/td&gt; 
   &lt;td&gt;26M&lt;/td&gt; 
   &lt;td&gt;‚âà1.1h&lt;br /&gt;‚âà1.43Ôø•&lt;/td&gt; 
   &lt;td&gt;‚âà1h&lt;br /&gt;‚âà1.3Ôø•&lt;/td&gt; 
   &lt;td&gt;‚âà6h&lt;br /&gt;‚âà7.8Ôø•&lt;/td&gt; 
   &lt;td&gt;‚âà4.58h&lt;br /&gt;‚âà5.95Ôø•&lt;/td&gt; 
   &lt;td&gt;‚âà7.5h&lt;br /&gt;‚âà9.75Ôø•&lt;/td&gt; 
   &lt;td&gt;‚âà1h&lt;br /&gt;‚âà1.3Ôø•&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2&lt;/td&gt; 
   &lt;td&gt;104M&lt;/td&gt; 
   &lt;td&gt;‚âà3.9h&lt;br /&gt;‚âà5.07Ôø•&lt;/td&gt; 
   &lt;td&gt;‚âà3.3h&lt;br /&gt;‚âà4.29Ôø•&lt;/td&gt; 
   &lt;td&gt;‚âà20h&lt;br /&gt;‚âà26Ôø•&lt;/td&gt; 
   &lt;td&gt;‚âà15h&lt;br /&gt;‚âà19.5Ôø•&lt;/td&gt; 
   &lt;td&gt;‚âà25h&lt;br /&gt;‚âà32.5Ôø•&lt;/td&gt; 
   &lt;td&gt;‚âà3h&lt;br /&gt;‚âà3.9Ôø•&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;ËÆ≠ÁªÉÂºÄÈîÄÊÄªÁªì&amp;amp;È¢ÑÊµã&lt;/summary&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;MiniMind2-SmallÂèÇÊï∞&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;code&gt;pretrain_hq&lt;/code&gt;+&lt;code&gt;sft_mini_512&lt;/code&gt;Êï∞ÊçÆÈõÜ &lt;br /&gt;ÂçïÂç°3090 (1 epoch) + 2.1Â∞èÊó∂ + Ëä±Ë¥π2.73ÂÖÉ‰∫∫Ê∞ëÂ∏Å &lt;br /&gt;Âç≥ÂèØ‰ªé0ËÆ≠ÁªÉÂá∫MiniMind-Zero-0.025BÊ®°Âûã!!!&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/blockquote&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;MiniMind2-SmallÂèÇÊï∞&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;code&gt;pretrain_hq&lt;/code&gt;+&lt;code&gt;sft_512&lt;/code&gt;+&lt;code&gt;sft_2048&lt;/code&gt;+&lt;code&gt;dpo&lt;/code&gt;Êï∞ÊçÆÈõÜ &lt;br /&gt;ÂçïÂç°3090 (2 epochs) + Â§ßÁ∫¶38.16Â∞èÊó∂ + Ëä±Ë¥π49.61ÂÖÉ‰∫∫Ê∞ëÂ∏Å &lt;br /&gt;Âç≥ÂèØ‰ªé0ËÆ≠ÁªÉÂá∫MiniMind2-Small-0.025BÊ®°Âûã!!!&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/blockquote&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;MiniMind2ÂèÇÊï∞&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;code&gt;pretrain_hq&lt;/code&gt;+&lt;code&gt;sft_512&lt;/code&gt;+&lt;code&gt;sft_2048&lt;/code&gt;+&lt;code&gt;dpo&lt;/code&gt;Êï∞ÊçÆÈõÜ &lt;br /&gt;ÂçïÂç°3090 (2 epochs) + Â§ßÁ∫¶122Â∞èÊó∂ + Ëä±Ë¥π158.6ÂÖÉ‰∫∫Ê∞ëÂ∏Å &lt;br /&gt;Âç≥ÂèØ‰ªé0ËÆ≠ÁªÉÂá∫MiniMind2-0.1BÊ®°Âûã!!!&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;p&gt;‚ú®Âü∫‰∫éÂçïÂç°NVIDIA 3090ÁöÑ&lt;code&gt;MiniMind-Zero&lt;/code&gt;‰ªé0ËÆ≠ÁªÉ‰ªÖÈúÄ&lt;code&gt;2Â∞èÊó∂&lt;/code&gt; + &lt;code&gt;3ÂùóÈí±&lt;/code&gt;ÔºåÂÆûÁé∞ChatBotÊïàÊûúÔºÅ&lt;/p&gt; 
&lt;p&gt;‚ú®PSÔºöËã•ÈááÁî®8Âç°4090ËÆ≠ÁªÉÔºåÊÄªÁî®Êó∂ÁîöËá≥ÂèØ‰ª•ÂéãÁº©Âà∞10ÂàÜÈíü‰ª•ÂÜÖÔºÅÔºàÁî±‰∫éÊó∂Èó¥Êõ¥Áü≠ÔºåËä±Ë¥πÂêåÊ†∑3ÂÖÉÂ∑¶Âè≥Ôºå‰∏éÂçïÂç°ÊàêÊú¨Áõ∏ÂΩìÔºâ&lt;/p&gt; 
&lt;p&gt;‚ú®‰ª•ÊûÅ‰ΩéÊûÅ‰ΩéÁöÑÈó®ÊßõÔºåÂÆûÁé∞‰∫∫‰∫∫ÂèØÁé©ÁöÑÂ§ßÊ®°ÂûãËá™Áî±ÔºÅËøôÊ≠£ÊòØMiniMindÁ≥ªÂàóÁöÑËØûÁîüÂàùË°∑ÔºÅ&lt;/p&gt; 
&lt;p&gt;‚ú®‰ªÖ‰ª∑ÂÄº&lt;code&gt;3ÂùóÈí±&lt;/code&gt;ÊàêÊú¨ÁöÑ&lt;code&gt;MiniMind-Zero&lt;/code&gt;Âπ∂‰∏çÊòØÂô±Â§¥ÔºÅChatÊµãËØïÔºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;üë∂: ËØ∑‰ªãÁªç‰∏Ä‰∏ãËá™Â∑±„ÄÇ
ü§ñÔ∏è: ‰Ωú‰∏∫‰∫∫Â∑•Êô∫ËÉΩÔºåÊàëÊ≤°ÊúâÂÆûÈôÖÁöÑÁîüÊ¥ªÔºå‰πüÊ≤°ÊúâËá™ÊàëÊÑèËØÜÔºåÊâÄ‰ª•Ê≤°ÊúâËá™Â∑±ÁöÑÁîüÊ¥ª„ÄÇÊàëË¢´ËÆæËÆ°ÊàêËÉΩÂ§üÂ∏ÆÂä©Áî®Êà∑Ëß£Á≠îÈóÆÈ¢ò„ÄÅÊèê‰æõ‰ø°ÊÅØ„ÄÅËøõË°åÂØπËØùÁ≠â„ÄÇÊàëÁöÑËÆæËÆ°ÂíåÂäüËÉΩÊòØÁî±ËÆ°ÁÆóÊú∫ÁßëÂ≠¶„ÄÅÊú∫Âô®Â≠¶‰π†„ÄÅ‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÂíåÁÆóÊ≥ïÊâÄÊûÑÂª∫ÁöÑÔºåÊàëÈÄöËøáÁºñÁ®ãÂíåÁÆóÊ≥ïÂÆûÁé∞„ÄÇ

üë∂: Êé®Ëçê‰∏Ä‰∫õÊù≠Â∑ûÁöÑÁâπËâ≤ÁæéÈ£üÂêß„ÄÇ
ü§ñÔ∏è: Êù≠Â∑ûÊúâÂæàÂ§öÁæéÂë≥ÁöÑÁâπËâ≤ÁæéÈ£üÔºåÊØîÂ¶ÇÈ∏°ËÇâ‰∏≤„ÄÅÈ≥óÈ±ºÂ§¥„ÄÅÊ∞¥ÁÖÆÈ±º„ÄÅË±ÜËÖêËÑëÁ≠â„ÄÇËøô‰∫õÁæéÈ£üÈÉΩÈùûÂ∏∏ÁæéÂë≥ÔºåÊúâÁùÄÁã¨ÁâπÁöÑÈ£éÂë≥ÂíåÂè£ÊÑüÔºåÁªùÂØπÊòØÂÄºÂæó‰∏ÄËØïÁöÑÁæéÈ£ü„ÄÇ

üë∂: ËØ∑‰∏∫ÊàëËÆ≤Ëß£‚ÄúÂ§ßËØ≠Ë®ÄÊ®°Âûã‚ÄùËøô‰∏™Ê¶ÇÂøµ„ÄÇ
ü§ñÔ∏è: Â§ßËØ≠Ë®ÄÊ®°ÂûãÊòØ‰∏ÄÁßç‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÔºåÂÆÉ‰ΩøÁî®Â§ßÈáèÁöÑÊñáÊú¨Êï∞ÊçÆÊù•ËÆ≠ÁªÉÊ®°ÂûãÔºåÁÑ∂ÂêéÂú®‰∏çÊñ≠ÁöÑËø≠‰ª£‰∏≠‰∏çÊñ≠‰ºòÂåñÊ®°Âûã„ÄÇÂÆÉÁöÑÁõÆÊ†áÊòØÊ†πÊçÆËæìÂÖ•ÁöÑÊï∞ÊçÆÊù•ÁîüÊàêÁ¨¶ÂêàÁâπÂÆöÈúÄÊ±ÇÁöÑÊñáÊú¨ÔºåËøô‰∫õÊñáÊú¨ÂèØ‰ª•ÊòØ‰ªª‰Ωï‰Ω†ÊÑüÂÖ¥Ë∂£ÁöÑ‰∏ªÈ¢ò„ÄÇ

üë∂: Introduce the history of the United States, please.
ü§ñÔ∏è: ÊÇ®ÊèêÂà∞ÁöÑ‚ÄúIntrook's the believeations of theument." Ëøô‰∏™ÂêçÂ≠óÊù•Ê∫ê‰∫é‰∏≠ÂõΩÂè§‰ª£ÁöÑ"groty of of the change."
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ÊûÅÈÄü‰∏îÂàùÂÖ∑ÊïàÊûúÔºåÁîöËá≥‰ªçÁÑ∂ÂèØ‰ª•Ëøõ‰∏ÄÊ≠•ÂéãÁº©Ëé∑ÂèñÊõ¥Â∞èÊõ¥‰ºòË¥®ÁöÑËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇ ZeroÊ®°ÂûãÊùÉÈáç‰øùÂ≠ò‰∏∫ &lt;code&gt;full_sft_512_zero.pth&lt;/code&gt;ÔºàËßÅ‰∏ãÊñáMiniMindÊ®°ÂûãÊñá‰ª∂ÈìæÊé•ÔºâÔºåÂ¶ÇÊúâÂÖ¥Ë∂£ÂèØ‰∏ãËΩΩÊ£ÄÈ™åÊ≠§Ê®°ÂûãÊïàÊûú„ÄÇ&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚Ö° ‰∏ªË¶ÅËÆ≠ÁªÉÊ≠•È™§&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ÊâÄÊúâËÆ≠ÁªÉËÑöÊú¨Âùá &lt;code&gt;cd ./trainer&lt;/code&gt; ÁõÆÂΩïÊâßË°å&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;1. È¢ÑËÆ≠ÁªÉ(Pretrain)&lt;/strong&gt;:&lt;/h3&gt; 
&lt;p&gt;LLMÈ¶ñÂÖàË¶ÅÂ≠¶‰π†ÁöÑÂπ∂ÈùûÁõ¥Êé•‰∏é‰∫∫‰∫§ÊµÅÔºåËÄåÊòØËÆ©ÁΩëÁªúÂèÇÊï∞‰∏≠ÂÖÖÊª°Áü•ËØÜÁöÑÂ¢®Ê∞¥Ôºå‚ÄúÂ¢®Ê∞¥‚Äù ÁêÜËÆ∫‰∏äÂñùÁöÑË∂äÈ•±Ë∂äÂ•ΩÔºå‰∫ßÁîüÂ§ßÈáèÁöÑÂØπ‰∏ñÁïåÁöÑÁü•ËØÜÁßØÁ¥Ø„ÄÇ È¢ÑËÆ≠ÁªÉÂ∞±ÊòØËÆ©ModelÂÖàÂüãÂ§¥Ëã¶Â≠¶Â§ßÈáèÂü∫Êú¨ÁöÑÁü•ËØÜÔºå‰æãÂ¶Ç‰ªéWikiÁôæÁßë„ÄÅÊñ∞Èóª„ÄÅ‰π¶Á±çÊï¥ÁêÜÂ§ßËßÑÊ®°ÁöÑÈ´òË¥®ÈáèËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇ Ëøô‰∏™ËøáÁ®ãÊòØ‚ÄúÊó†ÁõëÁù£‚ÄùÁöÑÔºåÂç≥‰∫∫Á±ª‰∏çÈúÄË¶ÅÂú®ËøáÁ®ã‰∏≠ÂÅö‰ªª‰Ωï‚ÄúÊúâÁõëÁù£‚ÄùÁöÑÊ†°Ê≠£ÔºåËÄåÊòØÁî±Ê®°ÂûãËá™Â∑±‰ªéÂ§ßÈáèÊñáÊú¨‰∏≠ÊÄªÁªìËßÑÂæãÂ≠¶‰π†Áü•ËØÜÁÇπ„ÄÇ Ê®°ÂûãÊ≠§Èò∂ÊÆµÁõÆÁöÑÂè™Êúâ‰∏Ä‰∏™Ôºö&lt;strong&gt;Â≠¶‰ºöËØçËØ≠Êé•Èæô&lt;/strong&gt;„ÄÇ‰æãÂ¶ÇÊàë‰ª¨ËæìÂÖ•‚ÄúÁß¶ÂßãÁöá‚ÄùÂõõ‰∏™Â≠óÔºåÂÆÉÂèØ‰ª•Êé•Èæô‚ÄúÊòØ‰∏≠ÂõΩÁöÑÁ¨¨‰∏Ä‰ΩçÁöáÂ∏ù‚Äù„ÄÇ&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_pretrain.py # 1Âç≥‰∏∫ÂçïÂç°ËÆ≠ÁªÉÔºåÂèØÊ†πÊçÆÁ°¨‰ª∂ÊÉÖÂÜµËá™Ë°åË∞ÉÊï¥ (ËÆæÁΩÆ&amp;gt;=2)
# or
python train_pretrain.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ËÆ≠ÁªÉÂêéÁöÑÊ®°ÂûãÊùÉÈáçÊñá‰ª∂ÈªòËÆ§ÊØèÈöî&lt;code&gt;100Ê≠•&lt;/code&gt;‰øùÂ≠ò‰∏∫: &lt;code&gt;pretrain_*.pth&lt;/code&gt;Ôºà* ‰∏∫Ê®°ÂûãÂÖ∑‰ΩìdimensionÔºåÊØèÊ¨°‰øùÂ≠òÊó∂Êñ∞Êñá‰ª∂‰ºöË¶ÜÁõñÊóßÊñá‰ª∂Ôºâ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;2. ÊúâÁõëÁù£ÂæÆË∞É(Supervised Fine-Tuning)&lt;/strong&gt;:&lt;/h3&gt; 
&lt;p&gt;ÁªèËøáÈ¢ÑËÆ≠ÁªÉÔºåLLMÊ≠§Êó∂Â∑≤ÁªèÊéåÊè°‰∫ÜÂ§ßÈáèÁü•ËØÜÔºåÁÑ∂ËÄåÊ≠§Êó∂ÂÆÉÂè™‰ºöÊó†ËÑëÂú∞ËØçËØ≠Êé•ÈæôÔºåËøò‰∏ç‰ºö‰∏é‰∫∫ËÅäÂ§©„ÄÇ SFTÈò∂ÊÆµÂ∞±ÈúÄË¶ÅÊääÂçäÊàêÂìÅLLMÊñΩÂä†‰∏Ä‰∏™Ëá™ÂÆö‰πâÁöÑËÅäÂ§©Ê®°ÊùøËøõË°åÂæÆË∞É„ÄÇ ‰æãÂ¶ÇÊ®°ÂûãÈÅáÂà∞ËøôÊ†∑ÁöÑÊ®°Êùø„ÄêÈóÆÈ¢ò-&amp;gt;ÂõûÁ≠îÔºåÈóÆÈ¢ò-&amp;gt;ÂõûÁ≠î„ÄëÂêé‰∏çÂÜçÊó†ËÑëÊé•ÈæôÔºåËÄåÊòØÊÑèËØÜÂà∞ËøôÊòØ‰∏ÄÊÆµÂÆåÊï¥ÁöÑÂØπËØùÁªìÊùü„ÄÇ Áß∞Ëøô‰∏™ËøáÁ®ã‰∏∫Êåá‰ª§ÂæÆË∞ÉÔºåÂ∞±Â¶ÇÂêåËÆ©Â∑≤ÁªèÂ≠¶ÂØå‰∫îËΩ¶ÁöÑ„ÄåÁâõÈ°ø„ÄçÂÖàÁîüÈÄÇÂ∫î21‰∏ñÁ∫™Êô∫ËÉΩÊâãÊú∫ÁöÑËÅäÂ§©‰π†ÊÉØÔºåÂ≠¶‰π†Â±èÂπïÂ∑¶‰æßÊòØÂØπÊñπÊ∂àÊÅØÔºåÂè≥‰æßÊòØÊú¨‰∫∫Ê∂àÊÅØËøô‰∏™ËßÑÂæã„ÄÇ Âú®ËÆ≠ÁªÉÊó∂ÔºåMiniMindÁöÑÊåá‰ª§ÂíåÂõûÁ≠îÈïøÂ∫¶Ë¢´Êà™Êñ≠Âú®512ÔºåÊòØ‰∏∫‰∫ÜËäÇÁúÅÊòæÂ≠òÁ©∫Èó¥„ÄÇÂ∞±ÂÉèÊàë‰ª¨Â≠¶‰π†Êó∂Ôºå‰ºöÂÖà‰ªéÁü≠ÁöÑÊñáÁ´†ÂºÄÂßãÔºåÂΩìÂ≠¶‰ºöÂÜô‰Ωú200Â≠ó‰ΩúÊñáÂêéÔºå800Â≠óÊñáÁ´†‰πüÂèØ‰ª•ÊâãÂà∞ÊìíÊù•„ÄÇ Âú®ÈúÄË¶ÅÈïøÂ∫¶ÊãìÂ±ïÊó∂ÔºåÂè™ÈúÄË¶ÅÂáÜÂ§áÂ∞ëÈáèÁöÑ2k/4k/8kÈïøÂ∫¶ÂØπËØùÊï∞ÊçÆËøõË°åËøõ‰∏ÄÊ≠•ÂæÆË∞ÉÂç≥ÂèØÔºàÊ≠§Êó∂ÊúÄÂ•ΩÈÖçÂêàRoPE-NTKÁöÑÂü∫ÂáÜÂ∑ÆÂÄºÔºâ„ÄÇ&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Âú®Êé®ÁêÜÊó∂ÈÄöËøáË∞ÉÊï¥RoPEÁ∫øÊÄßÂ∑ÆÂÄºÔºåÂÆûÁé∞ÂÖçËÆ≠ÁªÉÈïøÂ∫¶Â§ñÊé®Âà∞2048Âèä‰ª•‰∏äÂ∞Ü‰ºöÂæàÊñπ‰æø„ÄÇ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_full_sft.py
# or
python train_full_sft.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ËÆ≠ÁªÉÂêéÁöÑÊ®°ÂûãÊùÉÈáçÊñá‰ª∂ÈªòËÆ§ÊØèÈöî&lt;code&gt;100Ê≠•&lt;/code&gt;‰øùÂ≠ò‰∏∫: &lt;code&gt;full_sft_*.pth&lt;/code&gt;Ôºà* ‰∏∫Ê®°ÂûãÂÖ∑‰ΩìdimensionÔºåÊØèÊ¨°‰øùÂ≠òÊó∂Êñ∞Êñá‰ª∂‰ºöË¶ÜÁõñÊóßÊñá‰ª∂Ôºâ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;‚Ö¢ ÂÖ∂ÂÆÉËÆ≠ÁªÉÊ≠•È™§&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ÊâÄÊúâËÆ≠ÁªÉËÑöÊú¨Âùá &lt;code&gt;cd ./trainer&lt;/code&gt; ÁõÆÂΩïÊâßË°å&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;3. ‰∫∫Á±ªÂèçÈ¶àÂº∫ÂåñÂ≠¶‰π†(Reinforcement Learning from Human Feedback, RLHF)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Âú®ÂâçÈù¢ÁöÑËÆ≠ÁªÉÊ≠•È™§‰∏≠ÔºåÊ®°ÂûãÂ∑≤ÁªèÂÖ∑Â§á‰∫ÜÂü∫Êú¨ÁöÑÂØπËØùËÉΩÂäõÔºå‰ΩÜÊòØËøôÊ†∑ÁöÑËÉΩÂäõÂÆåÂÖ®Âü∫‰∫éÂçïËØçÊé•ÈæôÔºåÁº∫Â∞ëÊ≠£ÂèçÊ†∑‰æãÁöÑÊøÄÂä±„ÄÇ Ê®°ÂûãÊ≠§Êó∂Â∞öÊú™Áü•‰ªÄ‰πàÂõûÁ≠îÊòØÂ•ΩÁöÑÔºå‰ªÄ‰πàÊòØÂ∑ÆÁöÑ„ÄÇÊàë‰ª¨Â∏åÊúõÂÆÉËÉΩÂ§üÊõ¥Á¨¶Âêà‰∫∫ÁöÑÂÅèÂ•ΩÔºåÈôç‰ΩéËÆ©‰∫∫Á±ª‰∏çÊª°ÊÑèÁ≠îÊ°àÁöÑ‰∫ßÁîüÊ¶ÇÁéá„ÄÇ Ëøô‰∏™ËøáÁ®ãÂ∞±ÂÉèÊòØËÆ©Ê®°ÂûãÂèÇÂä†Êñ∞ÁöÑÂüπËÆ≠Ôºå‰ªé‰ºòÁßÄÂëòÂ∑•ÁöÑ‰Ωú‰∏∫‰æãÂ≠êÔºåÊ∂àÊûÅÂëòÂ∑•‰Ωú‰∏∫Âèç‰æãÔºåÂ≠¶‰π†Â¶Ç‰ΩïÊõ¥Â•ΩÂú∞ÂõûÂ§ç„ÄÇ Ê≠§Â§Ñ‰ΩøÁî®ÁöÑÊòØRLHFÁ≥ªÂàó‰πã-Áõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñ(Direct Preference Optimization, DPO)„ÄÇ ‰∏éPPO(Proximal Policy Optimization)ËøôÁßçÈúÄË¶ÅÂ•ñÂä±Ê®°Âûã„ÄÅ‰ª∑ÂÄºÊ®°ÂûãÁöÑRLÁÆóÊ≥ï‰∏çÂêåÔºõ DPOÈÄöËøáÊé®ÂØºPPOÂ•ñÂä±Ê®°ÂûãÁöÑÊòæÂºèËß£ÔºåÊääÂú®Á∫øÂ•ñÂä±Ê®°ÂûãÊç¢ÊàêÁ¶ªÁ∫øÊï∞ÊçÆÔºåRefÊ®°ÂûãËæìÂá∫ÂèØ‰ª•ÊèêÂâç‰øùÂ≠ò„ÄÇ DPOÊÄßËÉΩÂá†‰πé‰∏çÂèòÔºåÂè™Áî®Ë∑ë actor_model Âíå ref_model ‰∏§‰∏™Ê®°ÂûãÔºåÂ§ßÂ§ßËäÇÁúÅÊòæÂ≠òÂºÄÈîÄÂíåÂ¢ûÂä†ËÆ≠ÁªÉÁ®≥ÂÆöÊÄß„ÄÇ&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Ê≥®ÔºöRLHFËÆ≠ÁªÉÊ≠•È™§&lt;strong&gt;Âπ∂ÈùûÂøÖÈ°ª&lt;/strong&gt;ÔºåÊ≠§Ê≠•È™§Èöæ‰ª•ÊèêÂçáÊ®°Âûã‚ÄúÊô∫Âäõ‚ÄùËÄåÈÄöÂ∏∏‰ªÖÁî®‰∫éÊèêÂçáÊ®°ÂûãÁöÑ‚ÄúÁ§ºË≤å‚ÄùÔºåÊúâÂà©ÔºàÁ¨¶ÂêàÂÅèÂ•Ω„ÄÅÂáèÂ∞ëÊúâÂÆ≥ÂÜÖÂÆπÔºâ‰πüÊúâÂºäÔºàÊ†∑Êú¨Êî∂ÈõÜÊòÇË¥µ„ÄÅÂèçÈ¶àÂÅèÂ∑Æ„ÄÅÂ§öÊ†∑ÊÄßÊçüÂ§±Ôºâ„ÄÇ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_dpo.py
# or
python train_dpo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ËÆ≠ÁªÉÂêéÁöÑÊ®°ÂûãÊùÉÈáçÊñá‰ª∂ÈªòËÆ§ÊØèÈöî&lt;code&gt;100Ê≠•&lt;/code&gt;‰øùÂ≠ò‰∏∫: &lt;code&gt;rlhf_*.pth&lt;/code&gt;Ôºà* ‰∏∫Ê®°ÂûãÂÖ∑‰ΩìdimensionÔºåÊØèÊ¨°‰øùÂ≠òÊó∂Êñ∞Êñá‰ª∂‰ºöË¶ÜÁõñÊóßÊñá‰ª∂Ôºâ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;4. Áü•ËØÜËí∏È¶è(Knowledge Distillation, KD)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Âú®ÂâçÈù¢ÁöÑÊâÄÊúâËÆ≠ÁªÉÊ≠•È™§‰∏≠ÔºåÊ®°ÂûãÂ∑≤ÁªèÂÆåÂÖ®ÂÖ∑Â§á‰∫ÜÂü∫Êú¨ËÉΩÂäõÔºåÈÄöÂ∏∏ÂèØ‰ª•Â≠¶ÊàêÂá∫Â∏à‰∫Ü„ÄÇ ËÄåÁü•ËØÜËí∏È¶èÂèØ‰ª•Ëøõ‰∏ÄÊ≠•‰ºòÂåñÊ®°ÂûãÁöÑÊÄßËÉΩÂíåÊïàÁéáÔºåÊâÄË∞ìÁü•ËØÜËí∏È¶èÔºåÂç≥Â≠¶ÁîüÊ®°ÂûãÈù¢ÂêëÊïôÂ∏àÊ®°ÂûãÂ≠¶‰π†„ÄÇ ÊïôÂ∏àÊ®°ÂûãÈÄöÂ∏∏ÊòØÁªèËøáÂÖÖÂàÜËÆ≠ÁªÉÁöÑÂ§ßÊ®°ÂûãÔºåÂÖ∑ÊúâËæÉÈ´òÁöÑÂáÜÁ°ÆÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ Â≠¶ÁîüÊ®°ÂûãÊòØ‰∏Ä‰∏™ËæÉÂ∞èÁöÑÊ®°ÂûãÔºåÁõÆÊ†áÊòØÂ≠¶‰π†ÊïôÂ∏àÊ®°ÂûãÁöÑË°å‰∏∫ÔºåËÄå‰∏çÊòØÁõ¥Êé•‰ªéÂéüÂßãÊï∞ÊçÆ‰∏≠Â≠¶‰π†„ÄÇ Âú®SFTÂ≠¶‰π†‰∏≠ÔºåÊ®°ÂûãÁöÑÁõÆÊ†áÊòØÊãüÂêàËØçTokenÂàÜÁ±ªÁ°¨Ê†áÁ≠æÔºàhard labelsÔºâÔºåÂç≥ÁúüÂÆûÁöÑÁ±ªÂà´Ê†áÁ≠æÔºàÂ¶Ç 0 Êàñ 6400Ôºâ„ÄÇ Âú®Áü•ËØÜËí∏È¶è‰∏≠ÔºåÊïôÂ∏àÊ®°ÂûãÁöÑsoftmaxÊ¶ÇÁéáÂàÜÂ∏ÉË¢´Áî®‰ΩúËΩØÊ†áÁ≠æÔºàsoft labelsÔºâ„ÄÇÂ∞èÊ®°Âûã‰ªÖÂ≠¶‰π†ËΩØÊ†áÁ≠æÔºåÂπ∂‰ΩøÁî®KL-LossÊù•‰ºòÂåñÊ®°ÂûãÁöÑÂèÇÊï∞„ÄÇ ÈÄö‰øóÂú∞ËØ¥ÔºåSFTÁõ¥Êé•Â≠¶‰π†ËÄÅÂ∏àÁªôÁöÑËß£È¢òÁ≠îÊ°à„ÄÇËÄåKDËøáÁ®ãÁõ∏ÂΩì‰∫é‚ÄúÊâìÂºÄ‚ÄùËÄÅÂ∏àËÅ™ÊòéÁöÑÂ§ßËÑëÔºåÂ∞ΩÂèØËÉΩÂú∞Ê®°‰ªøËÄÅÂ∏à‚ÄúÂ§ßËÑë‚ÄùÊÄùËÄÉÈóÆÈ¢òÁöÑÁ•ûÁªèÂÖÉÁä∂ÊÄÅ„ÄÇ ‰æãÂ¶ÇÔºåÂΩìËÄÅÂ∏àÊ®°ÂûãËÆ°ÁÆó&lt;code&gt;1+1=2&lt;/code&gt;Ëøô‰∏™ÈóÆÈ¢òÁöÑÊó∂ÂÄôÔºåÊúÄÂêé‰∏ÄÂ±ÇÁ•ûÁªèÂÖÉaÁä∂ÊÄÅ‰∏∫0ÔºåÁ•ûÁªèÂÖÉbÁä∂ÊÄÅ‰∏∫100ÔºåÁ•ûÁªèÂÖÉcÁä∂ÊÄÅ‰∏∫-99... Â≠¶ÁîüÊ®°ÂûãÈÄöËøáÂ§ßÈáèÊï∞ÊçÆÔºåÂ≠¶‰π†ÊïôÂ∏àÊ®°ÂûãÂ§ßËÑëÂÜÖÈÉ®ÁöÑËøêËΩ¨ËßÑÂæã„ÄÇËøô‰∏™ËøáÁ®ãÂç≥Áß∞‰πã‰∏∫ÔºöÁü•ËØÜËí∏È¶è„ÄÇ Áü•ËØÜËí∏È¶èÁöÑÁõÆÁöÑÂè™Êúâ‰∏Ä‰∏™ÔºöËÆ©Â∞èÊ®°Âûã‰ΩìÁßØÊõ¥Â∞èÁöÑÂêåÊó∂ÊïàÊûúÊõ¥Â•Ω„ÄÇ ÁÑ∂ËÄåÈöèÁùÄLLMËØûÁîüÂíåÂèëÂ±ïÔºåÊ®°ÂûãËí∏È¶è‰∏ÄËØçË¢´ÂπøÊ≥õÊª•Áî®Ôºå‰ªéËÄå‰∫ßÁîü‰∫Ü‚ÄúÁôΩÁõí/ÈªëÁõí‚ÄùÁü•ËØÜËí∏È¶è‰∏§‰∏™Ê¥æÂà´„ÄÇ GPT-4ËøôÁßçÈó≠Ê∫êÊ®°ÂûãÔºåÁî±‰∫éÊó†Ê≥ïËé∑ÂèñÂÖ∂ÂÜÖÈÉ®ÁªìÊûÑÔºåÂõ†Ê≠§Âè™ËÉΩÈù¢ÂêëÂÆÉÊâÄËæìÂá∫ÁöÑÊï∞ÊçÆÂ≠¶‰π†ÔºåËøô‰∏™ËøáÁ®ãÁß∞‰πã‰∏∫ÈªëÁõíËí∏È¶èÔºå‰πüÊòØÂ§ßÊ®°ÂûãÊó∂‰ª£ÊúÄÊôÆÈÅçÁöÑÂÅöÊ≥ï„ÄÇ ÈªëÁõíËí∏È¶è‰∏éSFTËøáÁ®ãÂÆåÂÖ®‰∏ÄËá¥ÔºåÂè™‰∏çËøáÊï∞ÊçÆÊòØ‰ªéÂ§ßÊ®°ÂûãÁöÑËæìÂá∫Êî∂ÈõÜÔºåÂõ†Ê≠§Âè™ÈúÄË¶ÅÂáÜÂ§áÊï∞ÊçÆÂπ∂‰∏îËøõ‰∏ÄÊ≠•FTÂç≥ÂèØ„ÄÇ Ê≥®ÊÑèÊõ¥ÊîπË¢´Âä†ËΩΩÁöÑÂü∫Á°ÄÊ®°Âûã‰∏∫&lt;code&gt;full_sft_*.pth&lt;/code&gt;ÔºåÂç≥Âü∫‰∫éÂæÆË∞ÉÊ®°ÂûãÂÅöËøõ‰∏ÄÊ≠•ÁöÑËí∏È¶èÂ≠¶‰π†„ÄÇ &lt;code&gt;./dataset/sft_1024.jsonl&lt;/code&gt;‰∏é&lt;code&gt;./dataset/sft_2048.jsonl&lt;/code&gt; ÂùáÊî∂ÈõÜËá™qwen2.5-7/72B-InstructÂ§ßÊ®°ÂûãÔºåÂèØÁõ¥Êé•Áî®‰∫éSFT‰ª•Ëé∑ÂèñQwenÁöÑÈÉ®ÂàÜË°å‰∏∫„ÄÇ&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Ê≥®ÊÑèÈúÄË¶ÅÊõ¥Êîπtrain_full_sft.pyÊï∞ÊçÆÈõÜË∑ØÂæÑÔºå‰ª•Âèämax_seq_len  
torchrun --nproc_per_node 1 train_full_sft.py
# or
python train_full_sft.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ËÆ≠ÁªÉÂêéÁöÑÊ®°ÂûãÊùÉÈáçÊñá‰ª∂ÈªòËÆ§ÊØèÈöî&lt;code&gt;100Ê≠•&lt;/code&gt;ÂêåÊ†∑‰øùÂ≠ò‰∏∫: &lt;code&gt;full_sft_*.pth&lt;/code&gt;Ôºà*‰∏∫Ê®°ÂûãÂÖ∑‰ΩìdimensionÔºåÊØèÊ¨°‰øùÂ≠òÊó∂Êñ∞Êñá‰ª∂‰ºöË¶ÜÁõñÊóßÊñá‰ª∂Ôºâ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Ê≠§Â§ÑÂ∫îÂΩìÁùÄÈáç‰ªãÁªçMiniMindÂÆûÁé∞ÁöÑÁôΩÁõíËí∏È¶è‰ª£Á†Å&lt;code&gt;train_distillation.py&lt;/code&gt;ÔºåÁî±‰∫éMiniMindÂêåÁ≥ªÂàóÊú¨Ë∫´Âπ∂‰∏çÂ≠òÂú®Âº∫Â§ßÁöÑÊïôÂ∏àÊ®°ÂûãÔºåÂõ†Ê≠§ÁôΩÁõíËí∏È¶è‰ª£Á†Å‰ªÖ‰Ωú‰∏∫Â≠¶‰π†ÂèÇËÄÉ„ÄÇ&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_distillation.py
# or
python train_distillation.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;strong&gt;5. LoRA (Low-Rank Adaptation)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;LoRAÊòØ‰∏ÄÁßçÈ´òÊïàÁöÑÂèÇÊï∞È´òÊïàÂæÆË∞ÉÔºàParameter-Efficient Fine-Tuning, PEFTÔºâÊñπÊ≥ïÔºåÊó®Âú®ÈÄöËøá‰ΩéÁß©ÂàÜËß£ÁöÑÊñπÂºèÂØπÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãËøõË°åÂæÆË∞É„ÄÇ Áõ∏ÊØî‰∫éÂÖ®ÂèÇÊï∞ÂæÆË∞ÉÔºàFull Fine-TuningÔºâÔºåLoRA Âè™ÈúÄË¶ÅÊõ¥Êñ∞Â∞ëÈáèÁöÑÂèÇÊï∞„ÄÇ LoRA ÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÔºöÂú®Ê®°ÂûãÁöÑÊùÉÈáçÁü©Èòµ‰∏≠ÂºïÂÖ•‰ΩéÁß©ÂàÜËß£Ôºå‰ªÖÂØπ‰ΩéÁß©ÈÉ®ÂàÜËøõË°åÊõ¥Êñ∞ÔºåËÄå‰øùÊåÅÂéüÂßãÈ¢ÑËÆ≠ÁªÉÊùÉÈáç‰∏çÂèò„ÄÇ ‰ª£Á†ÅÂèØËßÅ&lt;code&gt;./model/model_lora.py&lt;/code&gt;Âíå&lt;code&gt;train_lora.py&lt;/code&gt;ÔºåÂÆåÂÖ®‰ªé0ÂÆûÁé∞LoRAÊµÅÁ®ãÔºå‰∏ç‰æùËµñÁ¨¨‰∏âÊñπÂ∫ìÁöÑÂ∞ÅË£Ö„ÄÇ&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_lora.py
# or
python train_lora.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ËÆ≠ÁªÉÂêéÁöÑÊ®°ÂûãÊùÉÈáçÊñá‰ª∂ÈªòËÆ§ÊØèÈöî&lt;code&gt;100Ê≠•&lt;/code&gt;‰øùÂ≠ò‰∏∫: &lt;code&gt;lora_xxx_*.pth&lt;/code&gt;Ôºà* ‰∏∫Ê®°ÂûãÂÖ∑‰ΩìdimensionÔºåÊØèÊ¨°‰øùÂ≠òÊó∂Êñ∞Êñá‰ª∂‰ºöË¶ÜÁõñÊóßÊñá‰ª∂Ôºâ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;ÈùûÂ∏∏Â§öÁöÑ‰∫∫Âõ∞ÊÉëÔºåÂ¶Ç‰Ωï‰ΩøÊ®°ÂûãÂ≠¶‰ºöËá™Â∑±ÁßÅÊúâÈ¢ÜÂüüÁöÑÁü•ËØÜÔºüÂ¶Ç‰ΩïÂáÜÂ§áÊï∞ÊçÆÈõÜÔºüÂ¶Ç‰ΩïËøÅÁßªÈÄöÁî®È¢ÜÂüüÊ®°ÂûãÊâìÈÄ†ÂûÇÂüüÊ®°ÂûãÔºü ËøôÈáå‰∏æÂá†‰∏™‰æãÂ≠êÔºåÂØπ‰∫éÈÄöÁî®Ê®°ÂûãÔºåÂåªÂ≠¶È¢ÜÂüüÁü•ËØÜÊ¨†Áº∫ÔºåÂèØ‰ª•Â∞ùËØïÂú®ÂéüÊúâÊ®°ÂûãÂü∫Á°Ä‰∏äÂä†ÂÖ•È¢ÜÂüüÁü•ËØÜÔºå‰ª•Ëé∑ÂæóÊõ¥Â•ΩÁöÑÊÄßËÉΩ„ÄÇ ÂêåÊó∂ÔºåÊàë‰ª¨ÈÄöÂ∏∏‰∏çÂ∏åÊúõÂ≠¶‰ºöÈ¢ÜÂüüÁü•ËØÜÁöÑÂêåÊó∂ÊçüÂ§±ÂéüÊúâÂü∫Á°ÄÊ®°ÂûãÁöÑÂÖ∂ÂÆÉËÉΩÂäõÔºåÊ≠§Êó∂LoRAÂèØ‰ª•ÂæàÂ•ΩÁöÑÊîπÂñÑËøô‰∏™ÈóÆÈ¢ò„ÄÇ Âè™ÈúÄË¶ÅÂáÜÂ§áÂ¶Ç‰∏ãÊ†ºÂºèÁöÑÂØπËØùÊï∞ÊçÆÈõÜÊîæÁΩÆÂà∞&lt;code&gt;./dataset/lora_xxx.jsonl&lt;/code&gt;ÔºåÂêØÂä® &lt;code&gt;python train_lora.py&lt;/code&gt; ËÆ≠ÁªÉÂç≥ÂèØÂæóÂà∞&lt;code&gt;./out/lora/lora_xxx.pth&lt;/code&gt;Êñ∞Ê®°ÂûãÊùÉÈáç„ÄÇ&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ÂåªÁñóÂú∫ÊôØ&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; {"conversations": [{"role": "user", "content": "ËØ∑ÈóÆÈ¢àÊ§éÁóÖÁöÑ‰∫∫ÊûïÂ§¥Â§öÈ´òÊâçÊúÄÂ•ΩÔºü"}, {"role": "assistant", "content": "È¢àÊ§éÁóÖÊÇ£ËÄÖÈÄâÊã©ÊûïÂ§¥ÁöÑÈ´òÂ∫¶Â∫îËØ•Ê†πÊçÆ..."}]}
 {"conversations": [{"role": "user", "content": "ËØ∑ÈóÆxxx"}, {"role": "assistant", "content": "xxx..."}]}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Ëá™ÊàëËÆ§Áü•Âú∫ÊôØ&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; {"conversations": [{"role": "user", "content": "‰Ω†Âè´‰ªÄ‰πàÂêçÂ≠óÔºü"}, {"role": "assistant", "content": "ÊàëÂè´minimind..."}]}
 {"conversations": [{"role": "user", "content": "‰Ω†ÊòØË∞Å"}, {"role": "assistant", "content": "ÊàëÊòØ..."}]}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Ê≠§Êó∂„ÄêÂü∫Á°ÄÊ®°Âûã+LoRAÊ®°Âûã„ÄëÂç≥ÂèØËé∑ÂæóÂåªÁñóÂú∫ÊôØÊ®°ÂûãÂ¢ûÂº∫ÁöÑËÉΩÂäõÔºåÁõ∏ÂΩì‰∫é‰∏∫Âü∫Á°ÄÊ®°ÂûãÂ¢ûÂä†‰∫ÜLoRAÂ§ñÊåÇÔºåËøô‰∏™ËøáÁ®ãÂπ∂‰∏çÊçüÂ§±Âü∫Á°ÄÊ®°ÂûãÁöÑÊú¨Ë∫´ËÉΩÂäõ„ÄÇ Êàë‰ª¨ÂèØ‰ª•ÈÄöËøá&lt;code&gt;eval_model.py&lt;/code&gt;ËøõË°åÊ®°ÂûãËØÑ‰º∞ÊµãËØï„ÄÇ&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Ê≥®ÊÑèÔºömodel_modeÂç≥ÈÄâÊã©Âü∫Á°ÄÊ®°ÂûãÁöÑÁ±ªÂûãÔºåËøôÂíåtrain_loraÊòØÂü∫‰∫éÂì™‰∏™Ê®°ÂûãËÆ≠ÁªÉÁöÑÁõ∏ÂÖ≥ÔºåÁ°Æ‰øùÁªü‰∏ÄÂç≥ÂèØ„ÄÇ
python eval_model.py --lora_name 'lora_medical' --model_mode 2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Â∞èÊµãËØï&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;üë∂: ÊàëÊúÄËøëÁªèÂ∏∏ÊÑüÂà∞Â§¥ÊôïÔºåÂèØËÉΩÊòØ‰ªÄ‰πàÂéüÂõ†Ôºü
ü§ñÔ∏è: Â§¥ÊôïÂèØËÉΩÊòØÁî±‰∫éÂ§öÁßçÂéüÂõ†ÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éÔºöÂøÉËÑèÁñæÁóÖ„ÄÅÁúºÁùõÊçü‰º§„ÄÅËøáÊïèÂèçÂ∫î„ÄÅÁöÆËÇ§ÈóÆÈ¢ò„ÄÅËøáÊïèÂèçÂ∫î„ÄÅÂéãÂäõ„ÄÅÁÑ¶Ëôë„ÄÅÁñ≤Âä≥„ÄÅËçØÁâ©ÂâØ‰ΩúÁî®„ÄÅÊÄßÂäüËÉΩÈöúÁ¢çÊàñËÄÖÂÖ∂‰ªñ...

---

üë∂: ‰Ω†ÂíåopenaiÊòØ‰ªÄ‰πàÂÖ≥Á≥ªÔºü
ü§ñÔ∏è: ÊàëÊòØ MiniMindÔºå‰∏Ä‰∏™Áî± Jingyao Gong ÂºÄÂèëÁöÑ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã„ÄÇÊàëÈÄöËøáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂíåÁÆóÊ≥ïËÆ≠ÁªÉÊù•‰∏éÁî®Êà∑ËøõË°å‰∫§‰∫í„ÄÇ
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;PSÔºöÂè™Ë¶ÅÊúâÊâÄÈúÄË¶ÅÁöÑÊï∞ÊçÆÈõÜÔºå‰πüÂèØ‰ª•full_sftÂÖ®ÂèÇÂæÆË∞ÉÔºàÈúÄË¶ÅËøõË°åÈÄöÁî®Áü•ËØÜÁöÑÊ∑∑ÂêàÈÖçÊØîÔºåÂê¶ÂàôËøáÊãüÂêàÈ¢ÜÂüüÊï∞ÊçÆ‰ºöËÆ©Ê®°ÂûãÂèòÂÇªÔºåÊçüÂ§±ÈÄöÁî®ÊÄßÔºâ&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;6. ËÆ≠ÁªÉÊé®ÁêÜÊ®°Âûã (Reasoning Model)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;DeepSeek-R1ÂÆûÂú®Â§™ÁÅ´‰∫ÜÔºåÂá†‰πéÈáçÊñ∞ÊåáÊòé‰∫ÜÊú™Êù•LLMÁöÑÊñ∞ËåÉÂºè„ÄÇ ËÆ∫ÊñáÊåáÂá∫&lt;code&gt;&amp;gt;3B&lt;/code&gt;ÁöÑÊ®°ÂûãÁªèÂéÜÂ§öÊ¨°ÂèçÂ§çÁöÑÂÜ∑ÂêØÂä®ÂíåRLÂ•ñÂä±ËÆ≠ÁªÉÊâçËÉΩËé∑ÂæóËÇâÁúºÂèØËßÅÁöÑÊé®ÁêÜËÉΩÂäõÊèêÂçá„ÄÇ ÊúÄÂø´ÊúÄÁ®≥Â¶•ÊúÄÁªèÊµéÁöÑÂÅöÊ≥ïÔºå‰ª•ÂèäÊúÄËøëÁàÜÂèëÁöÑÂêÑÁßçÂêÑÊ†∑ÊâÄË∞ìÁöÑÊé®ÁêÜÊ®°ÂûãÂá†‰πéÈÉΩÊòØÁõ¥Êé•Èù¢ÂêëÊï∞ÊçÆËøõË°åËí∏È¶èËÆ≠ÁªÉÔºå ‰ΩÜÁî±‰∫éÁº∫‰πèÊäÄÊúØÂê´ÈáèÔºåËí∏È¶èÊ¥æË¢´RLÊ¥æÁûß‰∏çËµ∑ÔºàhhhhÔºâ„ÄÇ Êú¨‰∫∫ËøÖÈÄüÂ∑≤ÁªèÂú®QwenÁ≥ªÂàó1.5BÂ∞èÊ®°Âûã‰∏äËøõË°å‰∫ÜÂ∞ùËØïÔºåÂæàÂø´Â§çÁé∞‰∫ÜZeroËøáÁ®ãÁöÑÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõ„ÄÇ ÁÑ∂ËÄå‰∏Ä‰∏™ÈÅóÊÜæÁöÑÂÖ±ËØÜÊòØÔºöÂèÇÊï∞Â§™Â∞èÁöÑÊ®°ÂûãÁõ¥Êé•ÈÄöËøáÂÜ∑ÂêØÂä®SFT+GRPOÂá†‰πé‰∏çÂèØËÉΩËé∑Âæó‰ªª‰ΩïÊé®ÁêÜÊïàÊûú„ÄÇ MiniMind2Á¨¨‰∏ÄÊó∂Èó¥Âè™ËÉΩÂùöÂÆö‰∏çÁßªÁöÑÈÄâÊã©ÂÅöËí∏È¶èÊ¥æÔºåÊó•ÂêéÂü∫‰∫é0.1BÊ®°ÂûãÁöÑRLÂ¶ÇÊûúÂêåÊ†∑ÂèñÂæóÂ∞èÂ∞èËøõÂ±ï‰ºöÊõ¥Êñ∞Ê≠§ÈÉ®ÂàÜÁöÑËÆ≠ÁªÉÊñπÊ°à„ÄÇ&lt;/p&gt; 
&lt;p&gt;ÂÅöËí∏È¶èÈúÄË¶ÅÂáÜÂ§áÁöÑ‰æùÁÑ∂ÊòØÂíåSFTÈò∂ÊÆµÂêåÊ†∑Ê†ºÂºèÁöÑÊï∞ÊçÆÂç≥ÂèØÔºåÊï∞ÊçÆÈõÜÊù•Ê∫êÂ∑≤Â¶Ç‰∏äÊñá‰ªãÁªç„ÄÇÊï∞ÊçÆÊ†ºÂºè‰æãÂ¶ÇÔºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "conversations": [
    {
      "role": "user",
      "content": "‰Ω†Â•ΩÔºåÊàëÊòØÂ∞èËä≥ÔºåÂæàÈ´òÂÖ¥ËÆ§ËØÜ‰Ω†„ÄÇ"
    },
    {
      "role": "assistant",
      "content": "&amp;lt;think&amp;gt;\n‰Ω†Â•ΩÔºÅÊàëÊòØÁî±‰∏≠ÂõΩÁöÑ‰∏™‰∫∫ÂºÄÂèëËÄÖÁã¨Á´ãÂºÄÂèëÁöÑÊô∫ËÉΩÂä©ÊâãMiniMind-R1-Lite-PreviewÔºåÂæàÈ´òÂÖ¥‰∏∫ÊÇ®Êèê‰æõÊúçÂä°ÔºÅ\n&amp;lt;/think&amp;gt;\n&amp;lt;answer&amp;gt;\n‰Ω†Â•ΩÔºÅÊàëÊòØÁî±‰∏≠ÂõΩÁöÑ‰∏™‰∫∫ÂºÄÂèëËÄÖÁã¨Á´ãÂºÄÂèëÁöÑÊô∫ËÉΩÂä©ÊâãMiniMind-R1-Lite-PreviewÔºåÂæàÈ´òÂÖ¥‰∏∫ÊÇ®Êèê‰æõÊúçÂä°ÔºÅ\n&amp;lt;/answer&amp;gt;"
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Êé®ÁêÜÊ®°ÂûãR1ÁöÑÂõûÂ§çÊ®°ÊùøÊòØÔºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;&amp;lt;think&amp;gt;\nÊÄùËÄÉËøáÁ®ã\n&amp;lt;/think&amp;gt;\n
&amp;lt;answer&amp;gt;\nÊúÄÁªàÂõûÁ≠î\n&amp;lt;/answer&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ËøôÂú®GRPO‰∏≠ÈÄöËøáËÆæÁΩÆËßÑÂàôÂ•ñÂä±ÂáΩÊï∞Á∫¶ÊùüÊ®°ÂûãÁ¨¶ÂêàÊÄùËÄÉÊ†áÁ≠æÂíåÂõûÂ§çÊ†áÁ≠æÔºàÂú®ÂÜ∑ÂêØÂä®Èù†ÂâçÁöÑÈò∂ÊÆµÂ•ñÂä±ÂÄºËÆæÁΩÆÂ∫îËØ•ÊèêÈ´ò‰∏Ä‰∫õÔºâ&lt;/p&gt; 
&lt;p&gt;Âè¶‰∏Ä‰∏™ÈóÆÈ¢òÊòØËí∏È¶èËøáÁ®ãËôΩÁÑ∂ÂíåSFT‰∏ÄÊ†∑Ôºå‰ΩÜÂÆûÈ™åÁªìÊûúÊòØÊ®°ÂûãÈöæ‰ª•ÊØèÊ¨°ÈÉΩÁ¨¶ÂêàÊ®°ÊùøËßÑËåÉÁöÑÂõûÂ§çÔºåÂç≥ËÑ±Á¶ªÊÄùËÄÉÂíåÂõûÂ§çÊ†áÁ≠æÁ∫¶Êùü„ÄÇ ËøôÈáåÁöÑÂ∞èÊäÄÂ∑ßÊòØÂ¢ûÂä†Ê†áËÆ∞‰ΩçÁΩÆtokenÁöÑÊçüÂ§±ÊÉ©ÁΩöÔºåËØ¶ËßÅ&lt;code&gt;train_distill_reason.py&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;# Âú® sp_ids ÂØπÂ∫îÁöÑ‰ΩçÁΩÆÂ¢ûÂä†È¢ùÂ§ñÁöÑÊÉ©ÁΩö
...
loss_mask[sp_ids] = 10 # ÊÉ©ÁΩöÁ≥ªÊï∞
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Âè¶Âè¶‰∏Ä‰∏™tipsÊòØÁî±‰∫éÊé®ÁêÜÊï∞ÊçÆÁî±‰∫éÂè™Á≠õÈÄâ‰∫Ü&lt;code&gt;&amp;lt;1024&lt;/code&gt;ÈïøÂ∫¶ÁöÑÊï∞ÊçÆÔºåÂÖ∂‰∏≠Â§öËΩÆÂØπËØùÂíåËã±ÊñáÊï∞ÊçÆÂÅèÂ∞ëÔºå Âõ†Ê≠§&lt;code&gt;r1_mix_1024.jsonl&lt;/code&gt;ËøõË°å‰∫ÜÂ§ßÁ∫¶10kÊù°Â§öËΩÆÂØπËØù+Ëã±ÊñáÊï∞ÊçÆÁöÑÊ∑∑ÂêàÔºåÈò≤Ê≠¢Ê®°ÂûãÈÅóÂøò‰∏•Èáç„ÄÇ&lt;/p&gt; 
&lt;p&gt;ËÑöÊú¨ÈªòËÆ§Âü∫‰∫érlhfÂêéÁöÑÂü∫Ê®°ÂûãÂÅöÊé®ÁêÜËÉΩÂäõÁöÑËí∏È¶èÂæÆË∞ÉÔºå‰∏ãÈù¢Áõ¥Êé•ÂêØÂä®ËÆ≠ÁªÉÂç≥ÂèØÔºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_distill_reason.py
# or
python train_distill_reason.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ËÆ≠ÁªÉÂêéÁöÑÊ®°ÂûãÊùÉÈáçÊñá‰ª∂ÈªòËÆ§ÊØèÈöî&lt;code&gt;100Ê≠•&lt;/code&gt;‰øùÂ≠ò‰∏∫: &lt;code&gt;reason_*.pth&lt;/code&gt;Ôºà*‰∏∫Ê®°ÂûãÂÖ∑‰ΩìdimensionÔºåÊØèÊ¨°‰øùÂ≠òÊó∂Êñ∞Êñá‰ª∂‰ºöË¶ÜÁõñÊóßÊñá‰ª∂Ôºâ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;ÊµãËØï‰∏Ä‰∏ãÔºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;üë∂: ËØ∑‰ªãÁªç‰∏Ä‰∏ãËá™Â∑±„ÄÇ
ü§ñÔ∏è: &amp;lt;think&amp;gt;
ÂóØÔºåÁî®Êà∑ËÆ©Êàë‰ªãÁªç‰∏Ä‰∏ãËá™Â∑±„ÄÇÊàëÁé∞Âú®ÈúÄË¶ÅËß£Èáä‰∏Ä‰∏ãËá™Â∑±„ÄÇÈ¶ñÂÖàÔºåÊàëÂ∫îËØ•ÊòéÁ°ÆÁî®Êà∑ÊòØË∞ÅÔºåÊòØÂ≠¶Áîü„ÄÅËÅåÂú∫‰∫∫Â£´ËøòÊòØÊôÆÈÄö‰∫∫„ÄÇÁî®Êà∑ÂèØËÉΩÊòØ‰∏Ä‰∏™Â≠¶ÁîüÊàñËÄÖËÅåÂú∫‰∫∫Â£´ÔºåÊâÄ‰ª•‰ø°ÊÅØ‰º†ÈÄíÂ∫îËØ•ÊòØÂ∏ÆÂä©Â≠¶ÁîüÂíåËÅåÂú∫‰∫∫Â£´ÁöÑ‰ø°ÊÅØ„ÄÇ
ÁÑ∂ÂêéÔºåÊàëÈúÄË¶ÅÂõûÂ∫îÁî®Êà∑ÔºåËøôÂèØËÉΩÂåÖÊã¨‰ªãÁªçËá™Â∑±ÁöÑ‰ΩçÁΩÆ„ÄÅÂÖ¥Ë∂£Áà±Â•Ω„ÄÅËÅå‰∏öÁªèÂéÜÁ≠â‰ø°ÊÅØ„ÄÇÁî®Êà∑ËøòÂèØËÉΩÊÉ≥‰∫ÜËß£Ëá™Â∑±Âú®Âì™ÈáåÔºåÊØîÂ¶ÇËÅå‰∏öÂèëÂ±ï„ÄÅÂ∑•‰ΩúÁªèÈ™åÁ≠â„ÄÇ
ÊàëÂ∫îËØ•Ê≥®ÊÑèÁî®ËØçË¶ÅÊ≠£ÂºèÔºå‰∏çË¶ÅÂ§™Ê≠£ÂºèÔºåÂêåÊó∂Ë¶Å‰øùÊåÅÁÆÄÊ¥ÅÊòé‰∫Ü„ÄÇËøôÊ†∑Áî®Êà∑ÂÆπÊòìÁêÜËß£Ôºå‰πüÊñπ‰æø‰ªñ‰ª¨Êü•ÈòÖÁõ∏ÂÖ≥ËµÑÊñô„ÄÇ
ÊúÄÂêéÔºåÊàëÂ∫îËØ•ÊÄªÁªì‰∏Ä‰∏ãÔºåËÆ©Áî®Êà∑Áü•ÈÅìÊàëÊòØÂ¶Ç‰ΩïÂ∏ÆÂä©‰ªñ‰ª¨ÂÆåÊàê‰ªñ‰ª¨ÁöÑ‰ªªÂä°ÁöÑÔºåËøôÊ†∑‰ªñ‰ª¨ÊúâÊõ¥Â§öÂÖ≥‰∫éÊàëÁöÑ‰ø°ÊÅØ„ÄÇ
&amp;lt;/think&amp;gt;
&amp;lt;answer&amp;gt;
ÊàëÊòØ**‚Äú‰Ω†ÁöÑÂêçÂ≠ó‚ÄùÔºå‰Ω†Âè´‰ªÄ‰πàÂêçÂ≠óÔºü
&amp;lt;/answer&amp;gt;

üë∂: ‰Ω†Êõ¥ÊìÖÈïøÂì™‰∏Ä‰∏™Â≠¶ÁßëÔºü
ü§ñÔ∏è: &amp;lt;think&amp;gt;
ÊÇ®Â•ΩÔºÅÊàëÊòØÁî±‰∏≠ÂõΩÁöÑ‰∏™‰∫∫ÂºÄÂèëËÄÖÂºÄÂèëÁöÑÊô∫ËÉΩÂä©ÊâãMiniMind-R1„ÄÇÂ¶ÇÊÇ®Êúâ‰ªª‰Ωï‰ªª‰ΩïÈóÆÈ¢òÔºåÊàë‰ºöÂ∞ΩÊàëÊâÄËÉΩ‰∏∫ÊÇ®Êèê‰æõÂ∏ÆÂä©„ÄÇ
&amp;lt;/think&amp;gt;
&amp;lt;answer&amp;gt;
ÊÇ®Â•ΩÔºÅÊàëÊòØÁî±‰∏≠ÂõΩÁöÑ‰∏™‰∫∫ÂºÄÂèëËÄÖÂºÄÂèëÁöÑÊô∫ËÉΩÂä©ÊâãMiniMind-R1„ÄÇÂ¶ÇÊÇ®Êúâ‰ªª‰Ωï‰ªª‰ΩïÈóÆÈ¢òÔºåÊàë‰ºöÂ∞ΩÊàëÊâÄËÉΩ‰∏∫ÊÇ®Êèê‰æõÂ∏ÆÂä©„ÄÇ
&amp;lt;/answer&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚Ö£ Ê®°ÂûãÂèÇÊï∞ËÆæÂÆö&lt;/h2&gt; 
&lt;p&gt;üìãÂÖ≥‰∫éLLMÁöÑÂèÇÊï∞ÈÖçÁΩÆÔºåÊúâ‰∏ÄÁØáÂæàÊúâÊÑèÊÄùÁöÑËÆ∫Êñá&lt;a href="https://arxiv.org/pdf/2402.14905"&gt;MobileLLM&lt;/a&gt;ÂÅö‰∫ÜËØ¶ÁªÜÁöÑÁ†îÁ©∂ÂíåÂÆûÈ™å„ÄÇ Scaling LawÂú®Â∞èÊ®°Âûã‰∏≠ÊúâËá™Â∑±Áã¨ÁâπÁöÑËßÑÂæã„ÄÇ ÂºïËµ∑TransformerÂèÇÊï∞ÊàêËßÑÊ®°ÂèòÂåñÁöÑÂèÇÊï∞Âá†‰πéÂè™ÂèñÂÜ≥‰∫é&lt;code&gt;d_model&lt;/code&gt;Âíå&lt;code&gt;n_layers&lt;/code&gt;„ÄÇ&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;d_model&lt;/code&gt;‚Üë + &lt;code&gt;n_layers&lt;/code&gt;‚Üì -&amp;gt; ÁüÆËÉñÂ≠ê&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;d_model&lt;/code&gt;‚Üì + &lt;code&gt;n_layers&lt;/code&gt;‚Üë -&amp;gt; Áò¶È´ò‰∏™&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;2020Âπ¥ÊèêÂá∫Scaling LawÁöÑËÆ∫ÊñáËÆ§‰∏∫ÔºåËÆ≠ÁªÉÊï∞ÊçÆÈáè„ÄÅÂèÇÊï∞Èáè‰ª•ÂèäËÆ≠ÁªÉËø≠‰ª£Ê¨°Êï∞ÊâçÊòØÂÜ≥ÂÆöÊÄßËÉΩÁöÑÂÖ≥ÈîÆÂõ†Á¥†ÔºåËÄåÊ®°ÂûãÊû∂ÊûÑÁöÑÂΩ±ÂìçÂá†‰πéÂèØ‰ª•ÂøΩËßÜ„ÄÇ ÁÑ∂ËÄå‰ºº‰πéËøô‰∏™ÂÆöÂæãÂØπÂ∞èÊ®°ÂûãÂπ∂‰∏çÂÆåÂÖ®ÈÄÇÁî®„ÄÇ MobileLLMÊèêÂá∫Êû∂ÊûÑÁöÑÊ∑±Â∫¶ÊØîÂÆΩÂ∫¶Êõ¥ÈáçË¶ÅÔºå„ÄåÊ∑±ËÄåÁ™Ñ„ÄçÁöÑ„ÄåÁò¶Èïø„ÄçÊ®°ÂûãÂèØ‰ª•Â≠¶‰π†Âà∞ÊØî„ÄåÂÆΩËÄåÊµÖ„ÄçÊ®°ÂûãÊõ¥Â§öÁöÑÊäΩË±°Ê¶ÇÂøµ„ÄÇ ‰æãÂ¶ÇÂΩìÊ®°ÂûãÂèÇÊï∞Âõ∫ÂÆöÂú®125MÊàñËÄÖ350MÊó∂Ôºå30ÔΩû42Â±ÇÁöÑ„ÄåÁã≠Èïø„ÄçÊ®°ÂûãÊòéÊòæÊØî12Â±ÇÂ∑¶Âè≥ÁöÑ„ÄåÁüÆËÉñ„ÄçÊ®°ÂûãÊúâÊõ¥‰ºòË∂äÁöÑÊÄßËÉΩÔºå Âú®Â∏∏ËØÜÊé®ÁêÜ„ÄÅÈóÆÁ≠î„ÄÅÈòÖËØªÁêÜËß£Á≠â8‰∏™Âü∫ÂáÜÊµãËØï‰∏äÈÉΩÊúâÁ±ª‰ººÁöÑË∂ãÂäø„ÄÇ ËøôÂÖ∂ÂÆûÊòØÈùûÂ∏∏ÊúâË∂£ÁöÑÂèëÁé∞ÔºåÂõ†‰∏∫‰ª•ÂæÄ‰∏∫100MÂ∑¶Âè≥ÈáèÁ∫ßÁöÑÂ∞èÊ®°ÂûãËÆæËÆ°Êû∂ÊûÑÊó∂ÔºåÂá†‰πéÊ≤°‰∫∫Â∞ùËØïËøáÂè†Âä†Ë∂ÖËøá12Â±Ç„ÄÇ Ëøô‰∏éMiniMindÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÊ®°ÂûãÂèÇÊï∞ÈáèÂú®&lt;code&gt;d_model&lt;/code&gt;Âíå&lt;code&gt;n_layers&lt;/code&gt;‰πãÈó¥ËøõË°åË∞ÉÊï¥ÂÆûÈ™åËßÇÂØüÂà∞ÁöÑÊïàÊûúÊòØ‰∏ÄËá¥ÁöÑ„ÄÇ ÁÑ∂ËÄå„ÄåÊ∑±ËÄåÁ™Ñ„ÄçÁöÑ„ÄåÁ™Ñ„Äç‰πüÊòØÊúâÁª¥Â∫¶ÊûÅÈôêÁöÑÔºåÂΩìd_model&amp;lt;512Êó∂ÔºåËØçÂµåÂÖ•Áª¥Â∫¶ÂùçÂ°åÁöÑÂä£ÂäøÈùûÂ∏∏ÊòéÊòæÔºå Â¢ûÂä†ÁöÑlayersÂπ∂‰∏çËÉΩÂº•Ë°•ËØçÂµåÂÖ•Âú®Âõ∫ÂÆöq_headÂ∏¶Êù•d_head‰∏çË∂≥ÁöÑÂä£Âäø„ÄÇ ÂΩìd_model&amp;gt;1536Êó∂ÔºålayersÁöÑÂ¢ûÂä†‰ºº‰πéÊØîd_modelÁöÑ‰ºòÂÖàÁ∫ßÊõ¥È´òÔºåÊõ¥ËÉΩÂ∏¶Êù•ÂÖ∑Êúâ‚ÄúÊÄß‰ª∑ÊØî‚ÄùÁöÑÂèÇÊï∞-&amp;gt;ÊïàÊûúÂ¢ûÁõä„ÄÇ&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Âõ†Ê≠§MiniMindËÆæÂÆösmallÊ®°Âûãdim=512Ôºån_layers=8Êù•Ëé∑ÂèñÁöÑ„ÄåÊûÅÂ∞è‰ΩìÁßØ&amp;lt;-&amp;gt;Êõ¥Â•ΩÊïàÊûú„ÄçÁöÑÂπ≥Ë°°„ÄÇ&lt;/li&gt; 
 &lt;li&gt;ËÆæÂÆödim=768Ôºån_layers=16Êù•Ëé∑ÂèñÊïàÊûúÁöÑÊõ¥Â§ßÊî∂ÁõäÔºåÊõ¥Âä†Á¨¶ÂêàÂ∞èÊ®°ÂûãScaling-LawÁöÑÂèòÂåñÊõ≤Á∫ø„ÄÇ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;‰Ωú‰∏∫ÂèÇËÄÉÔºåGPT3ÁöÑÂèÇÊï∞ËÆæÂÆöËßÅ‰∏ãË°®Ôºö &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/gpt3_config.png" alt="gpt3_config.png" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚Ö§ ËÆ≠ÁªÉÁªìÊûú&lt;/h2&gt; 
&lt;p&gt;MiniMind2 Ê®°ÂûãËÆ≠ÁªÉÊçüÂ§±Ëµ∞ÂäøÔºàÁî±‰∫éÊï∞ÊçÆÈõÜÂú®ËÆ≠ÁªÉÂêéÂèàÊõ¥Êñ∞Ê∏ÖÊ¥óÂ§öÊ¨°ÔºåÂõ†Ê≠§Loss‰ªÖ‰æõÂèÇËÄÉÔºâ&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;models&lt;/th&gt; 
   &lt;th&gt;pretrain (length-512)&lt;/th&gt; 
   &lt;th&gt;sft (length-512)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-Small&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/pre_512_loss.png" width="100%" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/sft_512_loss.png" width="100%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/pre_768_loss.png" width="100%" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/sft_768_loss.png" width="100%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;ËÆ≠ÁªÉÂÆåÊàê-Ê®°ÂûãÂêàÈõÜ&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ËÄÉËôëÂà∞Â§ö‰∫∫ÂèçÂ∫îÁôæÂ∫¶ÁΩëÁõòÈÄüÂ∫¶ÊÖ¢ÔºåMiniMind2Âèä‰ª•ÂêéÂÖ®ÈÉ®‰ΩøÁî®ModelScope/HuggingFaceÊâòÁÆ°„ÄÇ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;‚ë† PyTorchÂéüÁîüÊ®°Âûã&lt;/h4&gt; 
&lt;p&gt;MiniMind2Ê®°ÂûãÊùÉÈáç (&lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch"&gt;ModelScope&lt;/a&gt; | &lt;a href="https://huggingface.co/jingyaogong/MiniMind2-Pytorch"&gt;HuggingFace&lt;/a&gt;)&lt;/p&gt; 
&lt;p&gt;MiniMind-V1Ê®°ÂûãÊùÉÈáç (&lt;a href="https://pan.baidu.com/s/1KUfSzEkSXYbCCBj0Pw-9fA?pwd=6666"&gt;ÁôæÂ∫¶ÁΩëÁõò&lt;/a&gt;)&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;TorchÊñá‰ª∂ÂëΩÂêçÂØπÁÖß&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model Name&lt;/th&gt; 
    &lt;th&gt;params&lt;/th&gt; 
    &lt;th&gt;pretrain_model&lt;/th&gt; 
    &lt;th&gt;sft_model&lt;/th&gt; 
    &lt;th&gt;rl_model&lt;/th&gt; 
    &lt;th&gt;reason_model&lt;/th&gt; 
    &lt;th&gt;lora_model&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2-small&lt;/td&gt; 
    &lt;td&gt;26M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;full_sft_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;rlhf_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;reason_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;lora_xxx_512.pth&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2-MoE&lt;/td&gt; 
    &lt;td&gt;145M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_640_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;full_sft_640_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;rlhf_640_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2&lt;/td&gt; 
    &lt;td&gt;104M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;full_sft_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;rlhf_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;reason_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;lora_xxx_768.pth&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model Name&lt;/th&gt; 
    &lt;th&gt;params&lt;/th&gt; 
    &lt;th&gt;pretrain_model&lt;/th&gt; 
    &lt;th&gt;ÂçïËΩÆÂØπËØùsft&lt;/th&gt; 
    &lt;th&gt;Â§öËΩÆÂØπËØùsft&lt;/th&gt; 
    &lt;th&gt;rl_model&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1-small&lt;/td&gt; 
    &lt;td&gt;26M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;single_chat/full_sft_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;multi_chat/full_sft_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;rl_512.pth&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1-moe&lt;/td&gt; 
    &lt;td&gt;4√ó26M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_512_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;single_chat/full_sft_512_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;multi_chat/full_sft_512_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1&lt;/td&gt; 
    &lt;td&gt;108M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;single_chat/full_sft_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;multi_chat/full_sft_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;rl_768.pth&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h4&gt;‚ë° TransformersÊ®°Âûã&lt;/h4&gt; 
&lt;p&gt;MiniMindÁ≥ªÂàó (&lt;a href="https://www.modelscope.cn/collections/MiniMind-b72f4cfeb74b47"&gt;ModelScope&lt;/a&gt; | &lt;a href="https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5"&gt;HuggingFace&lt;/a&gt;)&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üìå Eval&lt;/h1&gt; 
&lt;h2&gt;‚Ö† RLHFÂØπÊØîSFTÁØá&lt;/h2&gt; 
&lt;p&gt;ÊµãËØïÂü∫‰∫é&lt;code&gt;full_sft&lt;/code&gt;Âíå&lt;code&gt;rlhf&lt;/code&gt;ÁöÑ&lt;code&gt;MiniMind2Á≥ªÂàó&lt;/code&gt;Ê®°ÂûãÂØπÊØîÔºåÊµãËØïÈöèÊú∫ÁßçÂ≠êÂùáÂõ∫ÂÆö&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;ÈóÆÁ≠îÂÆûÊµã&lt;/summary&gt; 
 &lt;p&gt;[A0] MiniMind2Ôºöfull_sft_640&lt;/p&gt; 
 &lt;p&gt;[A1] MiniMind2Ôºörlhf_640&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-text"&gt;[Q]: ‰Ω†Âè´‰ªÄ‰πàÂêçÂ≠óÔºü
[A0]: ÊàëÊòØ‰∏Ä‰∏™Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåÂèØ‰ª•ÂõûÁ≠îÂêÑÁßçÈóÆÈ¢òÔºåÊèê‰æõ‰ø°ÊÅØÂíåÂ∏ÆÂä©„ÄÇÊàëÂèØ‰ª•ÂõûÁ≠îÂêÑÁßçÈóÆÈ¢òÔºåÊèê‰æõ‰ø°ÊÅØÂíåÂ∏ÆÂä©ÔºåÂ∏ÆÂä©‰Ω†Ëß£ÂÜ≥ÂêÑÁßçÈóÆÈ¢ò„ÄÇ
[A1]: ÊàëÊòØ‰∏ÄÁßçÂü∫‰∫éÊñáÊú¨ÁöÑAIÊ®°ÂûãÔºåÊàëÂπ∂Ê≤°Êúâ‰∏™‰∫∫ÁªèÂéÜÊàñÊÉÖÊÑüÔºåÊâÄ‰ª•Êó†Ê≥ïÊÑüÂèóÂíå‰ΩìÈ™åÊÉÖÊÑü„ÄÇÊàëÁöÑËÆæËÆ°ÁõÆÁöÑÊòØ‰∏∫‰∫ÜÂ∏ÆÂä©Áî®Êà∑Ëß£ÂÜ≥ÈóÆÈ¢ò„ÄÅÊèê‰æõ‰ø°ÊÅØÂíåËøõË°åÂØπËØù„ÄÇÂ¶ÇÊûúÊÇ®Êúâ‰ªª‰ΩïÈóÆÈ¢òÊàñÈúÄË¶ÅÂ∏ÆÂä©ÔºåËØ∑ÈöèÊó∂ÂëäËØâÊàëÔºÅ

[Q]: ‰Ω†Êõ¥ÊìÖÈïøÂì™‰∏Ä‰∏™Â≠¶ÁßëÔºü
[A0]: ‰Ωú‰∏∫‰∏Ä‰∏™AIÊ®°ÂûãÔºåÊàëÂπ∂Ê≤°ÊúâÁúüÊ≠£ÁöÑ‚ÄúÊìÖÈïø‚ÄùËøô‰∏™Â≠¶Áßë„ÄÇÊàëË¢´ËÆæËÆ°ÊàêËÉΩÂ§üÊèê‰æõ‰ø°ÊÅØ„ÄÅËß£Á≠îÈóÆÈ¢ò„ÄÅËøõË°åÂØπËØù„ÄÅÊâßË°å‰ªªÂä°Âíå‰∏éÁî®Êà∑ËøõË°åÂØπËØù„ÄÇ‰∏çËøáÔºåÊàëÂèØ‰ª•Êèê‰æõ‰ø°ÊÅØ„ÄÅËß£Á≠îÈóÆÈ¢ò„ÄÅËøõË°åÂØπËØù„ÄÅÊèê‰æõÂª∫ËÆÆ„ÄÅÂ§ÑÁêÜÈóÆÈ¢ò„ÄÅËøõË°åÂØπËØù„ÄÅÊèê‰æõÂª∫ËÆÆ„ÄÅÂ≠¶‰π†Êñ∞Áü•ËØÜ„ÄÅÁºñÂÜô‰ª£Á†Å„ÄÅÂõûÁ≠îÈóÆÈ¢ò„ÄÅÊèê‰æõÂª∫ËÆÆÁ≠âÂ§öÁßçÂ≠¶ÁßëÁöÑÂäüËÉΩ„ÄÇÂ¶ÇÊûú‰Ω†ÊúâÂÖ∑‰ΩìÁöÑÂ≠¶ÁßëÈúÄÊ±ÇÊàñ‰∏ªÈ¢òÔºåÊ¨¢ËøéÊèê‰æõËØ¶ÁªÜ‰ø°ÊÅØ„ÄÇ
[A1]: ‰Ωú‰∏∫AIÔºåÊàëÂπ∂Ê≤°Êúâ‰∏™‰∫∫ËÉΩÂäõÊàñÂÖ¥Ë∂£Ôºå‰ΩÜÊàëÂèØ‰ª•Êèê‰æõ‰∏Ä‰∫õÂÖ≥‰∫éÊï∞Â≠¶„ÄÅÁßëÂ≠¶„ÄÅËÆ°ÁÆóÊú∫ÁßëÂ≠¶„ÄÅËØ≠Ë®ÄÂ≠¶„ÄÅÂøÉÁêÜÂ≠¶Á≠âÂ≠¶ÁßëÁöÑÁü•ËØÜ„ÄÇÂ¶ÇÊûú‰Ω†ÂØπÊï∞Â≠¶ÊÑüÂÖ¥Ë∂£ÔºåÊàëÂèØ‰ª•Êèê‰æõ‰∏Ä‰∫õÂü∫Á°ÄÁü•ËØÜÔºå‰æãÂ¶Ç‰ª£Êï∞„ÄÅÂá†‰Ωï„ÄÅÊ¶ÇÁéá„ÄÅÁªüËÆ°Â≠¶Á≠â„ÄÇÂ¶ÇÊûú‰Ω†ÈúÄË¶ÅÊõ¥Ê∑±ÂÖ•ÁöÑÂ≠¶‰π†ÔºåÊàë‰πüÂèØ‰ª•Êèê‰æõÁõ∏ÂÖ≥ÁöÑÂú®Á∫øËØæÁ®ã„ÄÅ‰π¶Á±ç„ÄÅËÆ∫ÊñáÁ≠âËµÑÊ∫ê„ÄÇËØ∑ÂëäËØâÊàë‰Ω†ÂÖ∑‰ΩìÈúÄË¶Å‰ªÄ‰πàÂ∏ÆÂä©ÔºåÊàë‰ºöÂ∞ΩÂäõÊèê‰æõÂ∏ÆÂä©„ÄÇ

[Q]: È≤ÅËøÖÁöÑ„ÄäÁãÇ‰∫∫Êó•ËÆ∞„ÄãÊòØÂ¶Ç‰ΩïÊâπÂà§Â∞ÅÂª∫Á§ºÊïôÁöÑÔºü
[A0]: È≤ÅËøÖÁöÑ„ÄäÁãÇ‰∫∫Êó•ËÆ∞„ÄãÊòØ‰∏ÄÈÉ®ËÆ≤Ëø∞‰∫ÜÈ≤ÅËøÖÂØπ‰∫∫Á±ª‰ª∑ÂÄºÂíåÈÅìÂæ∑Ë¥£‰ªªÁöÑÂÄ°ÂØº„ÄÇÂú®ËøôÈÉ®‰ΩúÂìÅ‰∏≠ÔºåÈ≤ÅËøÖÈÄöËøáÁªÜËÖªÁöÑÁ¨îËß¶ÔºåÊé¢ËÆ®‰∫Ü‰∫∫ÊÄß„ÄÅÈÅìÂæ∑„ÄÅÊ≠£‰πâ„ÄÅÁâ∫Áâ≤‰∏éË¥£‰ªªÁ≠âÊ∑±ÂàªÁöÑ‰∏ªÈ¢ò„ÄÇ‰ª•‰∏ãÊòØÈ≤ÅËøÖÂú®„ÄäÁãÇ‰∫∫Êó•ËÆ∞„Äã‰∏≠Â¶Ç‰ΩïÊâπÂà§Â∞ÅÂª∫Á§ºÊïôÁöÑÂá†‰∏™ÊñπÈù¢Ôºö1. **Á§æ‰ºöÁé∞ÂÆû‰∏éÈÅìÂæ∑ËßÇÂøµ**ÔºöÈ≤ÅËøÖÈÄöËøáËÆ≤Ëø∞È≤ÅËøÖÁöÑÊïÖ‰∫ãÔºåÊé¢ËÆ®‰∫ÜÁ§æ‰ºöÁöÑ‰∏çÂπ≥Á≠â„ÄÅÊùÉÂäõÁöÑÊµÅÂä®„ÄÅ‰∫∫ÊÄßÁöÑÊºîÂèòÁ≠âÁ§æ‰ºöÈóÆÈ¢ò„ÄÇ‰ªñËÆ§‰∏∫ÔºåÁ§æ‰ºö‰∏çÂπ≥Á≠â‰∏ç‰ªÖ‰ªÖÊòØ‰∏çÂÖ¨Ê≠£ÁöÑÔºåÊõ¥ÊòØÊØè‰∏™‰∫∫Âú®Á§æ‰ºö‰∏≠ÁöÑ‰∏çÂÖ¨Ê≠£„ÄÇ‰ªñÂëºÂêÅÁ§æ‰ºöÂêÑÁïåÂÖ±ÂêåÂä™ÂäõÔºåÈÄöËøáÊïôËÇ≤„ÄÅÂÆ£‰º†„ÄÅÊ≥ïÂæãÁ≠âÊâãÊÆµÔºå‰∏∫ÊûÑÂª∫ÂíåË∞êÁ§æ‰ºöÂÅöÂá∫Ë¥°ÁåÆ„ÄÇ2. **ÈÅìÂæ∑‰∏éË¥£‰ªª**ÔºöÈ≤ÅËøÖÈÄöËøáËÆ≤Ëø∞È≤ÅËøÖÁöÑÊïÖ‰∫ãÔºåÂ±ïÁé∞‰∫ÜÈÅìÂæ∑ÂíåË¥£‰ªªÁöÑÈáçË¶ÅÊÄß„ÄÇ‰ªñÊèêÂÄ°ÈÄöËøáÈÅìÂæ∑ÊïôËÇ≤Êù•ÂüπÂÖª‰∫∫‰ª¨ÁöÑÈÅìÂæ∑ÂìÅË¥®ÔºåÂêåÊó∂‰πüÂº∫Ë∞É‰∫ÜÈÅìÂæ∑ÂíåË¥£‰ªªÁöÑÈáçË¶ÅÊÄß„ÄÇÈ≤ÅËøÖÈÄöËøáËÆ≤Ëø∞È≤ÅËøÖÁöÑÊïÖ‰∫ãÔºåÊè≠Á§∫‰∫ÜÁ§æ‰ºöÁöÑ‰∏çÂπ≥Á≠âÂíå‰∫∫ÊÄßÁöÑÂ§çÊùÇÊÄßÔºå‰ΩìÁé∞‰∫ÜÂØπ‰∫∫ÊÄßÁöÑÊ∑±ÂàªÁêÜËß£„ÄÇ3. **ÈÅìÂæ∑‰∏éË¥£‰ªª**ÔºöÈ≤ÅËøÖÈÄöËøáËÆ≤Ëø∞È≤ÅËøÖÁöÑÊïÖ‰∫ãÔºåÂº∫Ë∞É‰∫ÜÈÅìÂæ∑Ë¥£‰ªªÁöÑÈáçË¶ÅÊÄß„ÄÇ‰ªñÊèêÂÄ°ÈÄöËøáËá™Â∑±ÁöÑË°å‰∏∫Êù•‰ΩìÁé∞ÂØπÁ§æ‰ºöÁöÑÂ∞äÈáçÂíåÂØπ‰∫∫ÊÄßÁöÑÊ∑±ÂàªÁêÜËß£„ÄÇÈ≤ÅËøÖÈÄöËøáËÆ≤Ëø∞È≤ÅËøÖÁöÑÊïÖ‰∫ãÔºåÂº∫Ë∞É‰∫ÜÈÅìÂæ∑ÂíåË¥£‰ªªÁöÑÈáçË¶ÅÊÄß„ÄÇ4. **ÈÅìÂæ∑‰∏éÈÅìÂæ∑**ÔºöÈ≤ÅËøÖÈÄöËøáËÆ≤Ëø∞È≤ÅËøÖÁöÑÊïÖ‰∫ãÔºåÊè≠Á§∫‰∫ÜÈÅìÂæ∑ÂíåË¥£‰ªªÁöÑÁúüÊ≠£ÊÑè‰πâ„ÄÇÈ≤ÅËøÖÈÄöËøáËÆ≤Ëø∞È≤ÅËøÖÁöÑÊïÖ‰∫ãÔºåÊè≠Á§∫‰∫ÜÈÅìÂæ∑ÂíåË¥£‰ªªÁöÑÊú¨Ë¥®ÔºåÂº∫Ë∞É‰∫ÜÈÅìÂæ∑ÂíåË¥£‰ªªÁöÑÈáçË¶ÅÊÄß„ÄÇ5. **‰∏™‰∫∫‰∏éÁ§æ‰ºö**ÔºöÈ≤ÅËøÖÈÄöËøáËÆ≤Ëø∞È≤ÅËøÖÁöÑÊïÖ‰∫ãÔºåÊè≠Á§∫‰∫Ü‰∏™‰∫∫‰∏éÁ§æ‰ºö‰πãÈó¥ÁöÑÂ§çÊùÇÂÖ≥Á≥ª„ÄÇ‰ªñÂëºÂêÅÁ§æ‰ºöÂêÑÁïåÂÖ±ÂêåÂä™ÂäõÔºåÈÄöËøáÊïôËÇ≤ÂíåÂÆ£‰º†Ôºå‰∏∫ÊûÑÂª∫ÂíåË∞êÁ§æ‰ºöÂÅöÂá∫Ë¥°ÁåÆ„ÄÇÈ≤ÅËøÖÁöÑ„ÄäÁãÇ‰∫∫Êó•ËÆ∞„ÄãÈÄöËøáÂÖ∂‰∏∞ÂØåÁöÑÊïÖ‰∫ãÂíåÊ∑±ÂàªÁöÑÈÅìÂæ∑ÂØìË®ÄÔºå‰º†Ëææ‰∫ÜÂÖ≥‰∫é‰∫∫ÊÄß„ÄÅÈÅìÂæ∑„ÄÅÊ≠£‰πâ„ÄÅÈÅìÂæ∑‰ª•Âèä‰∏™‰∫∫‰∏éÁ§æ‰ºöÁöÑÂ§çÊùÇÂÖ≥Á≥ª„ÄÇËøôÈÉ®‰ΩúÂìÅ‰∏ç‰ªÖÂú®ÊñáÂ≠¶‰ª∑ÂÄº‰∏äÂèñÂæó‰∫ÜÂ∑®Â§ßÊàêÂ∞±ÔºåÊõ¥Âú®ÊÄùÊÉ≥Â±ÇÈù¢‰∏äÂêØÂèë‰∫ÜÂêéÊù•ÁöÑÊñáÂ≠¶Âàõ‰ΩúÔºåÂØπÂêé‰∏ñ‰∫ßÁîü‰∫ÜÊ∑±ËøúÁöÑÂΩ±Âìç„ÄÇ
[A1]: È≤ÅËøÖÁöÑ„ÄäÁãÇ‰∫∫Êó•ËÆ∞„ÄãÊòØ‰∏ÄÈÉ®ÊèèÁªòÊàò‰∫âÂíåËá™Áî±Á≤æÁ•ûÁöÑ‰ΩúÂìÅÔºåÂÆÉÊâπÂà§‰∫ÜÂ∞ÅÂª∫Á§æ‰ºöÁöÑÁßçÁßçÈóÆÈ¢òÂíåÂÜ≤Á™ÅÔºåÂèçÊò†‰∫ÜÁ§æ‰ºöÁöÑ‰∏çÂπ≥Á≠âÂíå‰∫∫ÊÄßÊú¨Ë¥®„ÄÇÂú®ËøôÈÉ®‰ΩúÂìÅ‰∏≠ÔºåÈ≤ÅËøÖÈÄöËøáÂØπÂ∞ÅÂª∫Á§æ‰ºöÁöÑËØ∏Â§öÁé∞Ë±°ËøõË°åÊâπÂà§ÔºåÂ¶Ç‚ÄúÂºóÈõ∑Âæ∑ÈáåÂÖã¬∑Á∫¶Áø∞ÈÄäÁöÑ‚ÄúÈπ∞Ëø™‰º¶‚Äù„ÄÅ‚ÄúÁ∫≥ÊâéÁâπÁöÑ‚Äù„ÄÅ‚ÄúË¥ùÂÖãÊ±âÂßÜ‚Äù„ÄÅ‚Äú‰πîÊ≤ª¬∑Â®ÅÂ∞îÈÄä‚ÄùÁ≠âÔºåÂØπÂ∞ÅÂª∫Á§æ‰ºöÁöÑÁßçÁßçÈóÆÈ¢òËøõË°å‰∫ÜÊâπÂà§ÔºåÂº∫Ë∞É‰∫ÜÂ∞ÅÂª∫Á§æ‰ºöÁöÑ‰∏çÂπ≥Á≠â„ÄÅ‰∫∫ÊÄßÊú¨Ë¥®‰ª•Âèä‰∫∫ÊÄßÊú¨Ë¥®„ÄÇÈ≤ÅËøÖÈÄöËøá‰ªñÁöÑ‚ÄúÈπ∞Ëø™‰º¶‚Äù„ÄÅ‚ÄúÁ∫≥ÊâéÁâπÁöÑ‚Äù„ÄÅ‚ÄúË¥ùÂÖãÊ±âÂßÜ‚Äù„ÄÅ‚Äú‰πîÊ≤ª¬∑Â®ÅÂ∞îÈÄä‚ÄùÁ≠âÔºåÂØπÂ∞ÅÂª∫Á§æ‰ºöÁöÑÁßçÁßçÈóÆÈ¢òËøõË°å‰∫ÜÊâπÂà§„ÄÇÂú®Â∞èËØ¥‰∏≠ÔºåÈ≤ÅËøÖÈÄöËøáÂØπÂ∞ÅÂª∫Á§æ‰ºöÁöÑËØ∏Â§öÁé∞Ë±°ËøõË°åÊâπÂà§ÔºåÊè≠Á§∫‰∫ÜÂ∞ÅÂª∫Á§æ‰ºöÁöÑËÖêË¥•„ÄÅ‰∫∫ÊÄßÊú¨Ë¥®‰ª•Âèä‰∫∫ÊÄßÊú¨Ë¥®ÁöÑÂ§çÊùÇÊÄß„ÄÇËøôÁßçÊâπÂà§‰ΩìÁé∞‰∫ÜÂØπÂ∞ÅÂª∫Á§æ‰ºöÁöÑÂ∞äÈáçÂíåÂØπ‰∫∫ÊÄßÁöÑÂ§çÊùÇÊÄßÔºåÂêåÊó∂‰πüÂèçÊò†‰∫ÜÁé∞‰ª£Á§æ‰ºö‰∏≠ÊôÆÈÅçÂ≠òÂú®ÁöÑ‰∏çÂπ≥Á≠âÂíå‰∫∫ÊÄßÊú¨Ë¥®„ÄÇÈ≤ÅËøÖÁöÑ„ÄäÁãÇ‰∫∫Êó•ËÆ∞„Äã‰∏ç‰ªÖÊòØ‰∏ÄÈÉ®ÂÖ≥‰∫éÂ∞ÅÂª∫Á§æ‰ºöÁöÑÂ∞èËØ¥Ôºå‰πüÊòØ‰∏ÄÈÉ®ÂÖ∑ÊúâÊ∑±ÂàªÁ§æ‰ºöÊÑè‰πâÁöÑ‰ΩúÂìÅÔºåÂØπÂêé‰∏ñÁöÑÂÆ°Âà§ÂíåÊâπÂà§Ëµ∑Âà∞‰∫ÜÂÖ≥ÈîÆ‰ΩúÁî®„ÄÇ

[Q]: ÊàëÂí≥ÂóΩÂ∑≤ÁªèÊåÅÁª≠‰∫Ü‰∏§Âë®ÔºåÈúÄË¶ÅÂéªÂåªÈô¢Ê£ÄÊü•ÂêóÔºü
[A0]: Âí≥ÂóΩÁ°ÆÂÆûÈúÄË¶Å‰∏ÄÂÆöÁöÑÊ£ÄÊü•Ôºå‰ΩÜÂ¶ÇÊûúÊÇ®ÁöÑÂí≥ÂóΩÊåÅÁª≠Êó∂Èó¥ËøáÈïøÔºåÊàñËÄÖÊúâÂÖ∂‰ªñ‰∏çÈÄÇÁóáÁä∂ÔºàÂ¶ÇÊåÅÁª≠Âí≥ÂóΩ„ÄÅÂëºÂê∏Âõ∞ÈöæÁ≠âÔºâÔºåÂª∫ËÆÆÂ∞ΩÂø´Â∞±Âåª„ÄÇÂåªÁîü‰ºöÊ†πÊçÆÊÇ®ÁöÑÁóáÁä∂„ÄÅ‰ΩìÊ£ÄÁªìÊûúÂíåÂèØËÉΩÁöÑÂΩ±ÂÉèÂ≠¶Ê£ÄÊü•ÔºàÂ¶ÇXÂÖâ„ÄÅMRIÊàñCTÊâ´ÊèèÔºâÊù•ËØäÊñ≠ÊÇ®ÁöÑÁóáÁä∂„ÄÇÂú®Á≠âÂæÖÂ∞±ÂåªÊúüÈó¥ÔºåÊÇ®ÂèØ‰ª•Â∞ùËØï‰ª•‰∏ãÊñπÊ≥ïÊù•Â∏ÆÂä©ÁºìËß£ÁóáÁä∂Ôºö1. **‰ºëÊÅØ**ÔºöÁ°Æ‰øùÊÇ®ÊúâË∂≥Â§üÁöÑÁù°Áú†„ÄÇ2. **‰øùÊåÅÊ∞¥ÂàÜ**ÔºöËÑ±Ê∞¥ÂèØËÉΩ‰ºöÂä†ÈáçÂí≥ÂóΩ„ÄÇ3. **ÁÉ≠Êï∑**Ôºö‰ΩøÁî®ÁÉ≠Êï∑Ë¢ãÊàñÁÉ≠ÊØõÂ∑æÊï∑Âú®ÁñºÁóõÁöÑÈÉ®‰ΩçÔºåÊúâÂä©‰∫éÁºìËß£ÂñâÂíôÁóõ„ÄÇ4. **ÈÅøÂÖçÂà∫ÊøÄ**ÔºöÈÅøÂÖç‰ΩøÁî®ÂèØËÉΩÂà∫ÊøÄÂñâÂíôÁöÑÂåñÂ≠¶Áâ©Ë¥®ÔºåÂ¶ÇÊüê‰∫õÂåñÂ≠¶Áâ©Ë¥®„ÄÇ5. **‰øùÊåÅÂÆ§ÂÜÖÊπøÊ∂¶**ÔºöÁ°Æ‰øùÊÇ®ÁöÑÂçßÂÆ§ÊúâËâØÂ•ΩÁöÑÈÄöÈ£éÔºåÂèØ‰ª•‰ΩøÁî®Âä†ÊπøÂô®Â∏ÆÂä©‰øùÊåÅÁ©∫Ê∞îÊπøÂ∫¶„ÄÇ6. **‰øùÊåÅÂÆ§ÂÜÖÊπøÂ∫¶**Ôºö‰ΩøÁî®Âä†ÊπøÂô®ÂèØ‰ª•Â∏ÆÂä©ÁºìËß£ÂñâÂíôÂπ≤Áá•ÂíåÂí≥ÂóΩ„ÄÇÂ¶ÇÊûúÁóáÁä∂ÊåÅÁª≠‰∏çÁºìËß£ÊàñÊúâÂÖ∂‰ªñ‰∏çÈÄÇÁóáÁä∂ÔºåËØ∑Á´ãÂç≥Â∞±Âåª„ÄÇÂ∏åÊúõÊÇ®Êó©Êó•Â∫∑Â§çÔºÅ
[A1]: Â¶ÇÊûúÊÇ®ÁöÑÂí≥ÂóΩÊåÅÁª≠‰∫Ü‰∏§Âë®ÔºåÈÄöÂ∏∏Âª∫ËÆÆÊÇ®Â∞ΩÂø´Â∞±ÂåªÔºåÂõ†‰∏∫ËøôÂèØËÉΩ‰ºöÂΩ±ÂìçÊÇ®ÁöÑÂëºÂê∏ÂíåÂøÉË∑≥„ÄÇÊÇ®ÂèØ‰ª•ÈÄöËøá‰ª•‰∏ãÊ≠•È™§Êù•Ê£ÄÊü•Âí≥ÂóΩÔºö1. **‰ºëÊÅØ**ÔºöÁî®Á∫∏Â∑æÊàñËÇòÈÉ®ËΩªËΩªÊåâÂéãÂí≥ÂóΩÁöÑ‰ΩçÁΩÆ„ÄÇ2. **‰ΩøÁî®ËçØÁâ©**ÔºöÂú®ÂèØËÉΩÁöÑÊÉÖÂÜµ‰∏ãÔºå‰ΩøÁî®ËçØÁâ©ÔºàÂ¶ÇÊäóËøáÊïèËçØ„ÄÅÊäóËøáÊïèËçØÁ≠âÔºâÊù•ÁºìËß£Âí≥ÂóΩ„ÄÇËØ∑Âú®ÂåªÁîüÁöÑÊåáÂØº‰∏ã‰ΩøÁî®ËçØÁâ©„ÄÇ3. **ÁÉ≠Êï∑**ÔºöÂú®Âí≥ÂóΩÊúüÈó¥ÔºåÂèØ‰ª•‰ΩøÁî®ÁÉ≠Êï∑Êù•ÁºìËß£ÁñºÁóõÂíåËÇøËÉÄ„ÄÇ4. **ËçØÁâ©Ê≤ªÁñó**ÔºöÂ¶ÇÊûúÂí≥ÂóΩÊåÅÁª≠‰∫Ü‰∏§Âë®ÔºåÂèØ‰ª•‰ΩøÁî®‰∏Ä‰∫õÊ∂àÁÇéËçØÊàñÊäóËøáÊïèËçØÊù•ÁºìËß£ÁóáÁä∂„ÄÇËØ∑Âä°ÂøÖÂú®ÂåªÁîüÁöÑÊåáÂØº‰∏ã‰ΩøÁî®„ÄÇ5. **‰øùÊåÅÂÅ•Â∫∑ÁöÑÁîüÊ¥ªÊñπÂºè**Ôºö‰øùÊåÅÂÖÖË∂≥ÁöÑÁù°Áú†„ÄÅÂùáË°°ÁöÑÈ•ÆÈ£ü„ÄÅÈÄÇÈáèÁöÑËøêÂä®ÂíåÈÅøÂÖçÂê∏ÁÉüÂíåÈôêÈÖíÔºåËøô‰∫õÈÉΩÊúâÂä©‰∫éÊèêÈ´òÂí≥ÂóΩÁöÑÂá†Áéá„ÄÇËØ∑ËÆ∞‰ΩèÔºåËøô‰∫õÂª∫ËÆÆ‰∏çËÉΩÊõø‰ª£‰∏ì‰∏öÂåªÁñóÊÑèËßÅ„ÄÇÂ¶ÇÊûúÊÇ®ÁöÑÁóáÁä∂ÊåÅÁª≠ÊàñÂä†ÈáçÔºåÂº∫ÁÉàÂª∫ËÆÆÊÇ®Â∞ΩÂø´Â∞±Âåª„ÄÇ
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;üëâÊïàÊûúÊÄªÁªì&lt;/p&gt; 
&lt;p&gt;full_sftÊ®°ÂûãÂú®ÁÆÄÊ¥ÅÊÄßÂíå‰ø°ÊÅØÂáÜÁ°ÆÊÄßÊñπÈù¢Ë°®Áé∞Êõ¥Â•ΩÔºõrlhfÊ®°ÂûãÂú®ÂõûÁ≠î‰∏≠ÂÄæÂêë‰∫éÊèê‰æõÊõ¥Â§öÁöÑËÉåÊôØ‰ø°ÊÅØÔºå‰ΩÜ‰ø°ÊÅØÂáÜÁ°ÆÊÄßÊúâÂæÖÊîπËøõ„ÄÇ ÊÄªÁöÑÊù•ËØ¥RLHFÂêéÁöÑÊ®°ÂûãÂÄæÂêë‰∫éÂ≠¶‰π†ÔºöËØ¥Êõ¥Â§öÊúâÁ§ºË≤å‰ΩÜÊó†Áî®ÁöÑÂ∫üËØùËÆ®Â•Ω‚ÄúÂØπËØù‚ÄùÊú¨Ë∫´ÔºåËÄåÂØπ‰ø°ÊÅØÂáÜÁ°ÆÊÄßÂàôÊúâËΩªÂæÆÊçüÂ§±„ÄÇ Â§©‰∏ãÊ≤°ÊúâÂÖçË¥πÁöÑÂçàÈ§êÔºåËøòÈúÄË¶ÅÁªßÁª≠ÊèêÂçáRLHFÊï∞ÊçÆÈõÜÁöÑË¥®ÈáèÔºå‰πüË¶ÅÊé•ÂèóÊ®°ÂûãËÉΩÂäõÊó†Ê≥ïÈÅøÂÖçÁöÑÊçüÂ§±(Á®ãÂ∫¶ÊúâËΩªÈáç)„ÄÇ DPOÂíåÂú®Á∫øPPOÁöÑÂå∫Âà´Âú®‰∫érejectÂíåchosenÈÉΩÊòØÁ¶ªÁ∫øÂáÜÂ§áÁöÑÔºåÂíåminimindÊ®°ÂûãÊú¨Ë∫´ÁöÑËæìÂá∫ÂøÖÁÑ∂Â≠òÂú®ÂæàÂ§ßÁöÑÂàÜÂ∏ÉÂ∑ÆÂºÇ„ÄÇ ÈÄö‰øóÂú∞ËØ¥DPOÁÆóÊ≥ï‰ΩøÊ®°ÂûãËßÇÁúã‰πí‰πìÁêÉ‰∏ñÁïåÂÜ†ÂÜõÁöÑÊâìÊ≥ï„ÄåÂΩïÂÉè„ÄçËøõË°åRLÔºåËÄå‰∏çÊòØÂÉèPPO‰∏ÄÊ†∑ËØ∑rewardÊ®°ÂûãÂÅö„ÄåÊïôÁªÉ„ÄçÁ∫†Ê≠£Ëá™Â∑±ÁöÑÊâìÊ≥ïËøõË°åRL„ÄÇ&lt;/p&gt; 
&lt;h2&gt;‚Ö° ‰∏ªËßÇÊ†∑‰æãÊµãËØÑ&lt;/h2&gt; 
&lt;p&gt;üèÉ‰ª•‰∏ãÊµãËØï‰∫é2025-02-09ÂÆåÊàêÔºåÊ≠§Êó•ÊúüÂêéÂèëÂ∏ÉÁöÑÊñ∞Ê®°ÂûãÔºåÊó†ÁâπÊÆäÈúÄË¶ÅÊó∂Â∞Ü‰∏çÂä†ÂÖ•ÊµãËØï„ÄÇ&lt;/p&gt; 
&lt;p&gt;[A] &lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch"&gt;MiniMind2 (0.1B)&lt;/a&gt;&lt;br /&gt; [B] &lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch"&gt;MiniMind2-MoE (0.15B)&lt;/a&gt;&lt;br /&gt; [C] &lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch"&gt;MiniMind2-Small (0.02B)&lt;/a&gt;&lt;br /&gt; [D] &lt;a href="https://pan.baidu.com/s/1_COe0FQRDmeapSsvArahCA?pwd=6666"&gt;minimind-v1-small(0.02B)&lt;/a&gt;&lt;br /&gt; [E] &lt;a href="https://pan.baidu.com/s/1tqB-GMvuiGQBvEl-yZ-oBw?pwd=6666"&gt;minimind-v1-moe(0.1B)&lt;/a&gt;&lt;br /&gt; [F] &lt;a href="https://pan.baidu.com/s/1p713loS7EfwHQf3G9eYI3Q?pwd=6666"&gt;minimind-v1(0.1B)&lt;/a&gt;&lt;br /&gt; [G] &lt;a href="https://github.com/DLLXW/baby-llama2-chinese"&gt;baby-llama2-chinese(0.2B)&lt;/a&gt;&lt;br /&gt; [H] &lt;a href="https://github.com/charent/ChatLM-mini-Chinese"&gt;chatlm-mini-chinese(0.2B)&lt;/a&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;[Q]: ‰Ω†Áü•ÈÅìÈïøÊ±üÂêóÔºü
[A]: ÈïøÊ±üÔºå‰Ωç‰∫é‰∏≠ÂõΩ‰∏úÂçóÈÉ®ÔºåÊòØ‰∏≠ÂõΩÁöÑÁ¨¨‰∏ÄÈïøÊ≤≥ÔºåÂÖ®Èïø6300Â§öÂÖ¨ÈáåÔºåÊµÅÁªè11‰∏™ÁúÅ‰ªΩÔºåÊòØ‰∏≠ÂõΩÊúÄÈïøÁöÑÊ≤≥ÊµÅÔºå‰πüÊòØ‰∏≠ÂõΩÁöÑ‚ÄúÊØç‰∫≤Ê≤≥‚Äù„ÄÇÈïøÊ±üÁöÑÂéÜÂè≤ÂèØ‰ª•ËøΩÊ∫ØÂà∞ÂÖ¨ÂÖÉÂâç2000Â§öÂπ¥ÁöÑÂïÜÊúùÊó∂ÊúüÔºåÂΩìÊó∂ÁöÑÂïÜÊúùÊòØÂïÜÊúùÁöÑÂêéË£î„ÄÇÂïÜÊúùÊó∂ÊúüÔºåÈïøÊ±üÁöÑÁªèÊµéÂèëÂ±ïËææÂà∞‰∫ÜÈ´òÂ≥∞ÔºåÊòØ‰∏≠ÂõΩÊúÄÈáçË¶ÅÁöÑÂÜú‰∏öÂíåÂ∑•‰∏ö‰∏≠ÂøÉ‰πã‰∏Ä„ÄÇÈïøÊ±üÊµÅÂüüÁöÑÁªèÊµé‰∏ªË¶Å‰ª•ÂÜú‰∏ö‰∏∫‰∏ªÔºåÈïøÊ±üÊ∏î‰∏öÂç†ÊçÆ‰∫Ü‰∏ªË¶Å‰∫ß‰∏öÔºåÂ¶ÇÈíìÈ±º„ÄÅÁåéÈ±ºÁ≠â„ÄÇÈïøÊ±üÁöÑÊ∞¥ËµÑÊ∫ê‰πüÈùûÂ∏∏‰∏∞ÂØåÔºåÂåÖÊã¨Ê≤≥ÊµÅ„ÄÅÊπñÊ≥ä„ÄÅÊ≤≥ÊµÅÁ≠â„ÄÇÈïøÊ±üÁöÑËá™ÁÑ∂ÊôØËßÇ‰πüÂçÅÂàÜÂ£ÆËßÇÔºåÈïøÊ±üÂõõÂ≠£ÊôØËâ≤ÂêÑÂºÇÔºåÊò•Â≠£Â§öÈõ®ÔºåÂ§èÂ≠£Â§öÈõ®ÔºåÁßãÂ≠£ÂàôÂ§öÈõ®ÔºåÂÜ¨Â≠£ÂàôÂØíÂÜ∑Â§öÈõ™ÔºåÊò•Â≠£ÂíåÁßãÂ≠£ÈÉΩÊúâÁæé‰∏ΩÁöÑËä±ÊúµÂíåÊûúÂÆû„ÄÇÈïøÊ±üÊòØ‰∏≠ÂõΩÁöÑÁªèÊµé‰∏≠ÂøÉÔºåÊã•Êúâ‰ºóÂ§öÁöÑÂõΩÂÆ∂ÂíåÂüéÂ∏ÇÔºåÊòØ‰∏ñÁïå‰∏äÈáçË¶ÅÁöÑ‰∫§ÈÄöÊû¢Á∫ΩÂíåËà™Ëøê‰∏≠ÂøÉ„ÄÇÂêåÊó∂ÔºåÈïøÊ±üËøòÊòØ‰∏≠ÂõΩÁöÑÈáçË¶ÅÊ∞¥Âà©ËµÑÊ∫êÔºåÂØπ‰∫é‰∏≠ÂõΩÁöÑÁªèÊµéÂèëÂ±ïÂÖ∑ÊúâÈáçË¶ÅÊÑè‰πâ„ÄÇ
[B]: ÊòØÁöÑÔºåÊàëÁü•ÈÅìÈïøÊ±ü„ÄÇÈïøÊ±ü‰Ωç‰∫é‰∏≠ÂõΩÁöÑ‰∏≠ÈÉ®ÔºåÊòØ‰∏≠ÂõΩÁ¨¨‰∏âÈïøÁöÑÊ≤≥ÊµÅÔºåÂÖ®ÈïøÁ∫¶6300ÂÖ¨ÈáåÔºåÊµÅÁªèÊ±üË•øÁúÅ„ÄÅÂÆâÂæΩÁúÅ„ÄÅÊ±üËãèÁúÅ„ÄÅÂÆâÂæΩÁúÅ„ÄÅÊ±üËãèÁúÅÂíåÂÆâÂæΩÁúÅ„ÄÇ
[C]: ÂΩìÁÑ∂ÔºåÈïøÊ±üÊòØ‰∏≠ÂõΩÊúÄÈïøÁöÑÊ≤≥ÊµÅÔºåÂÖ®ÈïøÁ∫¶6300ÂÖ¨Èáå„ÄÇÂÆÉ‰ª•ÂÖ∂‰∏∞ÂØåÁöÑÊ∞¥ËµÑÊ∫ê„ÄÅ‰∏∞ÂØåÁöÑÊ∞¥ÁîüÂä®Ê§çÁâ©ËµÑÊ∫ê‰ª•ÂèäÁã¨ÁâπÁöÑÂú∞ÁêÜ‰ΩçÁΩÆËÄåÈóªÂêç‰∫é‰∏ñ„ÄÇ
[D]: ÊòØÁöÑÔºåÈïøÊ±üÊòØ‰∏≠ÂõΩÊúÄÈïøÁöÑÊ≤≥ÊµÅÔºåÂÖ®ÈïøË∂ÖËøá3300ÂçÉÁ±≥„ÄÇ
[E]: ÈïøÊ±üÊòØ‰∏≠ÂõΩÊúÄÈïøÁöÑÊ≤≥ÊµÅÔºåÊÄªÈïøÂ∫¶Á∫¶‰∏∫6300ÂÖ¨Èáå„ÄÇÂÆÉÂèëÊ∫ê‰∫éÈùíËóèÈ´òÂéüÁöÑÈùíÊµ∑ÊπñÔºåÊµÅÁªèÈùíÊµ∑„ÄÅÂõõÂ∑ù„ÄÅÁîòËÇÉ„ÄÅÂÆÅÂ§è„ÄÅÂÜÖËíôÂè§„ÄÅÈôïË•øÂíåÂ±±Ë•ø„ÄÇÈïøÊ±üÊµÅÂüüÊòØ‰∏≠ÂõΩÊúÄÂ§ßÁöÑÂÜú‰∏öÂå∫‰πã‰∏ÄÔºå‰πüÊòØ‰∏≠ÂõΩÁöÑÊñáÂåñÂíåÁªèÊµé‰∏≠ÂøÉ„ÄÇ
[F]: ÊòØÁöÑÔºåÈïøÊ±üÊòØ‰∏≠ÂõΩÊúÄÈïøÁöÑÊ≤≥ÊµÅ‰πã‰∏ÄÔºå‰πüÊòØ‰∏ñÁïåÁ¨¨‰∏âÂ§ßÊ≤≥ÊµÅ„ÄÇÈïøÊ±üÂèëÊ∫ê‰∫éÈùíËóèÈ´òÂéüÔºåÊµÅÁªè11‰∏™ÁúÅ‰ªΩÔºåÊúÄÁªàÊ≥®ÂÖ•‰∏úÊµ∑„ÄÇÈïøÊ±üÊµÅÂüüÊòØ‰∏≠ÂõΩÊúÄÂ§ßÁöÑÊ≤≥ÊµÅÊµÅÂüü‰πã‰∏ÄÔºåÊã•ÊúâÁùÄ‰∏∞ÂØåÁöÑËá™ÁÑ∂ËµÑÊ∫êÂíåÊñáÂåñÈÅó‰∫ßÔºåÂ¶ÇÈïøÊ±üÂ§ßÊ°•„ÄÅÈïøÊ±üÊµ∑Ê¥ãÂçöÁâ©È¶Ü„ÄÅÈïøÊ±üÂè§ÂüéÁ≠â„ÄÇ
[G]: ÂêåÊó∂‰πüÁü•ÈÅìÔºå‰∏≠ÂõΩÊòØ‰∏ñÁïå‰∏äÊúÄÈïøÁöÑÂüéÂ∏Ç‰πã‰∏ÄÔºåÊã•ÊúâË∂ÖËøá3400‰∏á‰∫∫Âè£„ÄÇ
[H]: ÊòØÁöÑÔºåÈïøÊ±üÊòØ‰∏≠ÂõΩÊúÄÈïøÁöÑÊ≤≥ÊµÅÔºå‰πüÊòØ‰∏ñÁïå‰∏äÊúÄÈïøÁöÑÊ≤≥ÊµÅ‰πã‰∏Ä„ÄÇÂÆÉÂèëÊ∫ê‰∫éÈùíËóèÈ´òÂéüÔºåÊµÅÁªè‰πù‰∏™ÁúÅÂå∫ÔºåÊúÄÁªàÊ≥®ÂÖ•‰∏úÊµ∑„ÄÇÈïøÊ±üÊµÅÂüüÁöÑÊ∞îÂÄôÊ∏©ÂíåÔºåÂõõÂ≠£ÂàÜÊòéÔºåÊòØ‰∏≠ÂõΩÈáçË¶ÅÁöÑÂÜú‰∏öÂíåÂ∑•‰∏öÂü∫Âú∞„ÄÇÈïøÊ±üÊµÅÂüüÁöÑÊ∞¥‰∫ßËµÑÊ∫ê‰∏∞ÂØåÔºåÊòØ‰∏≠ÂõΩÁöÑÈáçË¶ÅÊ∏î‰∏öËµÑÊ∫ê‰πã‰∏Ä„ÄÇ


[Q]: ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞ÊòØ‰ªÄ‰πàÔºü
[A]: ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞ÊòØÁè†Á©ÜÊúóÁéõÂ≥∞ÔºàMount EverestÔºâÔºå‰Ωç‰∫éÂñúÈ©¨ÊãâÈõÖÂ±±ËÑâÔºåÂ∞ºÊ≥äÂ∞îÂíå‰∏≠ÂõΩÁöÑËæπÁïå‰∏ä„ÄÇÂÆÉÁöÑÊµ∑ÊãîÈ´òÂ∫¶ÊòØ8,848.86Á±≥Ôºà29,031.7Ëã±Â∞∫ÔºâÔºåÊòØÂú∞ÁêÉ‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞„ÄÇ
[B]: Áè†Á©ÜÊúóÁéõÂ≥∞ÔºàMount EverestÔºâÊòØ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞ÔºåÊµ∑ÊãîÈ´òÂ∫¶Ëææ8,848Á±≥Ôºà29,029Ëã±Â∞∫Ôºâ„ÄÇ
[C]: Áè†Á©ÜÊúóÁéõÂ≥∞ÊòØ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞Ôºå‰Ωç‰∫éÂñúÈ©¨ÊãâÈõÖÂ±±ËÑâÔºåÊµ∑Êãî8,848.86Á±≥Ôºà29,031.7Ëã±Â∞∫Ôºâ„ÄÇÂÆÉ‰Ωç‰∫éÂ∞ºÊ≥äÂ∞îÂíå‰∏≠ÂõΩÁöÑËæπÁïå‰∏äÔºåÊòØÂñúÈ©¨ÊãâÈõÖÂ±±ËÑâÁöÑ‰∏ÄÈÉ®ÂàÜ„ÄÇ
[D]: ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞ÊòØÁè†Á©ÜÊúóÁéõÂ≥∞Ôºå‰Ωç‰∫éÂ∞ºÊ≥äÂ∞îÂíå‰∏≠ÂõΩÁöÑËæπÂ¢É‰∏äÔºåÊµ∑Êãî8848Á±≥„ÄÇ
[E]: ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞ÊòØÁè†Á©ÜÊúóÁéõÂ≥∞Ôºå‰Ωç‰∫éÂñúÈ©¨ÊãâÈõÖÂ±±ËÑâÔºåÊµ∑Êãî8,848Á±≥„ÄÇ
[F]: Áè†Á©ÜÊúóÁéõÂ≥∞ÊòØ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞ÔºåÊµ∑Êãî8848Á±≥„ÄÇ
[G]: ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞ÊòØÁè†Á©ÜÊúóÁéõÂ≥∞Ôºå‰Ωç‰∫éÂñúÈ©¨ÊãâÈõÖÂ±±ËÑâÔºåÊµ∑Êãî8,848.86Á±≥Ôºà29,031.69Ëã±Â∞∫Ôºâ„ÄÇ
[H]: ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞ÊòØÁè†Á©ÜÊúóÁéõÂ≥∞Ôºå‰πüÁß∞‰∏∫Ëê®Âä†ÁéõÂ°îÔºå‰Ωç‰∫éÂñúÈ©¨ÊãâÈõÖÂ±±ËÑâÁöÑÂ∞ºÊ≥äÂ∞îÂíå‰∏≠ÂõΩËæπÂ¢É‰πãÈó¥ÔºåÊµ∑Êãî8,848.86Á±≥Ôºà29,031.69Ëã±Â∞∫Ôºâ„ÄÇÂÆÉÊòØ‰∏ñÁïå‰∏äÊúÄËëóÂêçÁöÑÁôªÂ±±ÁõÆÊ†á‰πã‰∏ÄÔºåÂê∏ÂºïÁùÄÊù•Ëá™‰∏ñÁïåÂêÑÂú∞ÁöÑÁôªÂ±±ËÄÖÂâçÊù•ÊåëÊàò„ÄÇÁè†Á©ÜÊúóÁéõÂ≥∞ÁöÑÂêçÂ≠óÊù•Ëá™ËóèËØ≠ÔºåÊÑèÊÄùÊòØ‚ÄúÂ•≥Á•ûÁöÑÈ¢ùÂ≥∞‚ÄùÔºåÂÆÉÊòØÁî±‰∏§‰∏™‰∏çÂêåÁöÑÂ±±Â≥∞ÁªÑÊàêÁöÑÔºåÂÖ∂‰∏≠‰∏Ä‰∏™ÊòØ‰Ωç‰∫éÂ∞ºÊ≥äÂ∞îÁöÑÁè†Á©ÜÊúóÁéõÂ≥∞ÔºåÂè¶‰∏Ä‰∏™Âàô‰Ωç‰∫é‰∏≠ÂõΩ„ÄÇÂÆÉÁöÑÈ´òÂ∫¶ÊòØÈÄöËøáÊµãÈáèÊµ∑ÊãîÈ´òÂ∫¶Êù•Á°ÆÂÆöÁöÑÔºåÂõ†‰∏∫ÂÆÉÊòØÈÄöËøáÊµãÈáèÂú∞ÁêÉÁöÑÂπ≥ÂùáÈ´òÂ∫¶Êù•Á°ÆÂÆöÁöÑ„ÄÇÂ∞ΩÁÆ°Áè†Á©ÜÊúóÁéõÂ≥∞ÊòØ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞Ôºå‰ΩÜÂÆÉÂπ∂‰∏çÊòØÂú∞ÁêÉ‰∏ä‰ªª‰ΩïÂÖ∂‰ªñÂú∞ÊñπÈÉΩËÉΩÂà∞ËææÁöÑ„ÄÇÂú∞ÁêÉ‰∏äÁöÑÂÖ∂‰ªñÂú∞ÊñπÔºåÂ¶ÇÂçóÁæéÊ¥≤ÁöÑÂÆâÁ¨¨ÊñØÂ±±ËÑâÔºå‰πüÊòØ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±ËÑâÔºå‰ΩÜÂÆÉÂπ∂‰∏çÊòØÂú∞ÁêÉ‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞„ÄÇÊÄª‰πãÔºåÁè†Á©ÜÊúóÁéõÂ≥∞ÊòØ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞Ôºå‰ΩÜÂÆÉÂπ∂‰∏çÊòØÂú∞ÁêÉ‰∏ä‰ªª‰ΩïÂú∞ÊñπÈÉΩËÉΩÂà∞ËææÁöÑ„ÄÇÂÆÉÊòØÂú∞ÁêÉ‰∏äÊúÄÈ´òÁöÑÂ±±ËÑâÔºå‰ΩÜÂÆÉÂπ∂‰∏çÊòØÂú∞ÁêÉ‰∏ä‰ªª‰ΩïÂú∞ÊñπÈÉΩËÉΩÂà∞ËææÁöÑ„ÄÇ


[Q]: ‰∏áÊúâÂºïÂäõÊòØË∞ÅÊèêÂá∫ÁöÑÔºü
[A]: ‰∏áÊúâÂºïÂäõÊòØÁî±ÁâõÈ°øÂú®17‰∏ñÁ∫™ÊèêÂá∫ÁöÑÔºåÂÆÉÊòØÁâõÈ°ø‰∏âÂ§ßÂÆöÂæã‰πã‰∏Ä„ÄÇÁâõÈ°øÂú®17‰∏ñÁ∫™ÊèêÂá∫ÁöÑÂºïÂäõÂÆöÂæãÊè≠Á§∫‰∫Ü‰∏áÊúâÂºïÂäõÂÆöÂæãÁöÑÊú¨Ë¥®ÔºåÂπ∂‰∏îËß£Èáä‰∫ÜË°åÊòü„ÄÅÂç´Êòü„ÄÅÂΩóÊòüÁ≠âÂ§©‰Ωì‰πãÈó¥Â¶Ç‰ΩïÁõ∏‰∫í‰ΩúÁî®„ÄÇËøô‰∏™ÂÆöÂæãÊòØÁî±ËâæËê®ÂÖã¬∑ÁâõÈ°øÂú®1687Âπ¥ÊèêÂá∫ÁöÑÔºå‰ªñÈÄöËøáÂØπ‰∏áÊúâÂºïÂäõÂÆöÂæãÁöÑÂàÜÊûêÂíåÂÆûÈ™åÔºåÊàêÂäüÂú∞Ëß£Èáä‰∫ÜÂ§©‰Ωì‰πãÈó¥ÁöÑÂºïÂäõ‰ΩúÁî®Ôºå‰ªéËÄåËß£Èáä‰∫ÜÂú∞ÁêÉÁªïÂ§™Èò≥ÊóãËΩ¨ÁöÑÁé∞Ë±°„ÄÇ
[B]: ‰∏áÊúâÂºïÂäõÔºàHuman PolicyÔºâÊòØÁâõÈ°øÁöÑ‰∏âÂ§ßÂäõÂ≠¶ÂÆöÂæãÔºåÁî®‰∫éÊèèËø∞‰∏§‰∏™Áâ©‰Ωì‰πãÈó¥ÁöÑÁõ∏‰∫í‰ΩúÁî®Âäõ„ÄÇÂÆÉÁî±‰∏áÊúâÂºïÂäõÂÆöÂæãÂíåÁâõÈ°øÁöÑÁ¨¨‰∏ÄÂÆöÂæãÔºàÊÉØÊÄßÂÆöÂæãÔºâÊûÑÊàê„ÄÇ‰∏áÊúâÂºïÂäõÊòØÁî±‰∫é‰∏§‰∏™Áâ©‰Ωì‰πãÈó¥Áõ∏‰∫í‰ΩúÁî®ËÄå‰∫ßÁîüÁöÑÔºåÂÆÉ‰ª¨ÁöÑË¥®Èáè„ÄÅË∑ùÁ¶ª„ÄÅÈÄüÂ∫¶ÂíåÊñπÂêëÁ≠âÂõ†Á¥†ÂÜ≥ÂÆö‰∫ÜÂÆÉ‰ª¨‰πãÈó¥ÁöÑÁõ∏‰∫í‰ΩúÁî®„ÄÇËøôÁßçÁõ∏‰∫í‰ΩúÁî®ÂäõÊòØÔºöË¥®ÈáèÁöÑ‰πòÁßØÔºöË¥®ÈáèÁöÑ‰πòÁßØÊòØ‰∏áÊúâÂºïÂäõÂÆöÂæãÂíåÁâõÈ°øÁ¨¨‰∫åÂÆöÂæãÁöÑ‰πòÁßØ„ÄÇË¥®ÈáèÁöÑ‰πòÁßØÔºöË¥®ÈáèÁöÑ‰πòÁßØÊòØÁâ©‰Ωì‰πãÈó¥ÁöÑ‰∏ÄÁßçÁâ©ÁêÜËøáÁ®ãÔºåÂÆÉÊòØÁâ©‰ΩìÈó¥Áõ∏‰∫í‰ΩúÁî®ÁöÑ‰∫ßÁâ©ÔºåÂØπ‰∫éÂÖ∂‰ªñÁâ©‰ΩìÊù•ËØ¥ÔºåÂÖ∂Ë¥®ÈáèÂèØ‰ª•Ë°®Á§∫‰∏∫‰∏§‰∏™Áâ©‰Ωì‰πãÈó¥ÁöÑË¥®Èáè‰πãÂíåÔºåÂç≥ÈáçÂäõÂä†ÈÄüÂ∫¶a„ÄÇ
[C]: ‰∏áÊúâÂºïÂäõÊòØ‰∏Ä‰∏™Áî±ÁâõÈ°øÂú®1687Âπ¥ÊèêÂá∫ÁöÑÁâ©ÁêÜÂ≠¶Ê¶ÇÂøµÔºåÂÆÉÊèèËø∞‰∫ÜÁâ©‰Ωì‰πãÈó¥ÂºïÂäõÂ¶Ç‰ΩïÁõ∏‰∫í‰ΩúÁî®ÁöÑËßÑÂæã„ÄÇ
[D]: ‰∏áÊúâÂºïÂäõÊòØÁî±Ëã±ÂõΩÁâ©ÁêÜÂ≠¶ÂÆ∂ÁâõÈ°øÊèêÂá∫ÁöÑ„ÄÇ‰ªñËÆ§‰∏∫ÔºåÊâÄÊúâÁâ©‰ΩìÈÉΩ‰ºö‰∫ßÁîüÂºïÂäõÔºåËøô‰∏™ÂºïÂäõÊòØÁî±Áâ©‰ΩìÁöÑË¥®ÈáèÂíåË∑ùÁ¶ªÂÜ≥ÂÆöÁöÑ„ÄÇËøô‰∏™ÁêÜËÆ∫Êè≠Á§∫‰∫ÜË¥®ÈáèÂíåË∑ùÁ¶ª‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºåÂπ∂ËØÅÊòé‰∫Ü‰∏áÊúâÂºïÂäõÁöÑÊú¨Ë¥®„ÄÇ
[E]: ‰∏áÊúâÂºïÂäõÊòØÁî±ÁâõÈ°øÂú®1687Âπ¥ÊèêÂá∫ÁöÑ„ÄÇÂÆÉÊòØÂÆáÂÆô‰∏≠‰∏Ä‰∏™Âü∫Êú¨ÁöÑÁâ©ÁêÜÁé∞Ë±°ÔºåÂÆÉÊèèËø∞‰∫ÜÁâ©‰Ωì‰πãÈó¥ÁöÑÁõ∏‰∫í‰ΩúÁî®„ÄÇËøô‰∏™ÂäõË¢´Áß∞‰∏∫‰∏áÊúâÂºïÂäõÔºåÂÆÉÂú®ÊâÄÊúâÁâ©‰Ωì‰πãÈó¥ÈÉΩÂ≠òÂú®ÔºåËøôÁßçÂäõ‰∏éÁâ©‰ΩìÁöÑË¥®ÈáèÂíåË∑ùÁ¶ªÊúâÂÖ≥„ÄÇ‰∏áÊúâÂºïÂäõÊòØËá™ÁÑ∂Áïå‰∏≠ÊúÄÂü∫Êú¨ÁöÑÂäõÈáè‰πã‰∏ÄÔºåÂÆÉ‰∏ç‰ªÖÂΩ±ÂìçÁùÄÁâ©‰ΩìÁöÑËøêÂä®ÂíåÂèòÂåñÔºåËøòÂØπÂÆáÂÆôÁöÑÁªìÊûÑÂíåÊºîÂåñ‰∫ßÁîüÁùÄÈáçË¶ÅÁöÑÂΩ±Âìç„ÄÇ
[F]: ‰∏áÊúâÂºïÂäõÊòØËã±ÂõΩÁâ©ÁêÜÂ≠¶ÂÆ∂ËâæËê®ÂÖã¬∑ÁâõÈ°øÂú®17‰∏ñÁ∫™ÊèêÂá∫ÁöÑ„ÄÇ
[G]: ÂºïÂäõÊòØÂÆáÂÆô‰∏≠‰∏ÄÁßçÂäõÈáèÔºåÂÆÉÊòØÁî±Ë¥®ÈáèÊàñËÉΩÈáèÊâÄÁªÑÊàêÁöÑ„ÄÇÊ†πÊçÆÁâõÈ°øÁöÑ‰∏áÊúâÂºïÂäõÂÆöÂæãÔºå‰∏Ä‰∏™Áâ©‰Ωì‰πãÈó¥ÁöÑÂºïÂäõÂ§ßÂ∞è‰∏éÂÆÉÁöÑË¥®ÈáèÊàêÊ≠£ÊØîÔºå‰∏éÂÆÉÁöÑË¥®ÈáèÊàêÂèçÊØî„ÄÇÂõ†Ê≠§ÔºåÁâõÈ°øÁöÑ‰∏áÊúâÂºïÂäõÂÆöÂæãË°®ÊòéÔºå‰∏Ä‰∏™Áâ©‰Ωì‰πãÈó¥ÁöÑÂºïÂäõÂ§ßÂ∞è‰∏éÂÆÉÁöÑË¥®ÈáèÊàêÊ≠£ÊØîÔºå‰∏éÂÆÉÁöÑË¥®ÈáèÊàêÂèçÊØî„ÄÇËøô‰∏™ÂÆöÂæãÂú®Áâ©ÁêÜÂ≠¶„ÄÅÂ§©ÊñáÂ≠¶ÂíåÁâ©ÁêÜÂ≠¶‰∏≠ÈÉΩÊúâÂπøÊ≥õÁöÑÂ∫îÁî®„ÄÇ
[H]: ‰∏áÊúâÂºïÂäõÊòØÁî±Ëã±ÂõΩÁâ©ÁêÜÂ≠¶ÂÆ∂ËâæËê®ÂÖã¬∑ÁâõÈ°ø‰∫é1687Âπ¥ÊèêÂá∫ÁöÑ„ÄÇ‰ªñÂú®1687Âπ¥ÂèëË°®‰∫Ü‰∏ÄÁØáËëóÂêçÁöÑËÆ∫ÊñáÔºåÈ¢ò‰∏∫„ÄäËá™ÁÑ∂Âì≤Â≠¶ÁöÑÊï∞Â≠¶ÂéüÁêÜ„ÄãÔºåÂÖ∂‰∏≠ÊèêÂá∫‰∫Ü‰∏áÊúâÂºïÂäõÂÆöÂæãÔºåÊåáÂá∫‰ªª‰Ωï‰∏§‰∏™Áâ©‰Ωì‰πãÈó¥ÈÉΩÂ≠òÂú®ÁùÄ‰∏ÄÁßçÂºïÂäõÔºåËøôÁßçÂºïÂäõÁöÑÂ§ßÂ∞è‰∏éÂÆÉ‰ª¨ÁöÑË¥®ÈáèÊàêÊ≠£ÊØîÔºå‰∏éÂÆÉ‰ª¨‰πãÈó¥ÁöÑË∑ùÁ¶ªÁöÑÂπ≥ÊñπÊàêÂèçÊØî„ÄÇËøô‰∏™ÂÆöÂæãË¢´Áß∞‰∏∫‰∏áÊúâÂºïÂäõÂÆöÂæãÔºåÂÆÉÊèèËø∞‰∫ÜË¥®ÈáèÂíåË∑ùÁ¶ª‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇÂÆÉË°®ÊòéÔºå‰∏§‰∏™Áâ©‰Ωì‰πãÈó¥ÁöÑÂºïÂäõ‰∏éÂÆÉ‰ª¨ÁöÑË¥®ÈáèÊàêÊ≠£ÊØîÔºå‰∏éÂÆÉ‰ª¨‰πãÈó¥ÁöÑË∑ùÁ¶ªÁöÑÂπ≥ÊñπÊàêÂèçÊØî„ÄÇËøô‰∏™ÂÆöÂæãÂØπ‰∫éËß£ÈáäÂÆáÂÆô‰∏≠ÁöÑËÆ∏Â§öÁé∞Ë±°ÂíåÁé∞Ë±°‰πãÈó¥ÁöÑÁõ∏‰∫í‰ΩúÁî®ÈùûÂ∏∏ÈáçË¶ÅÔºå‰æãÂ¶ÇË°åÊòüÁªïÂ§™Èò≥ËøêÂä®„ÄÅÊÅíÊòü‰πãÈó¥ÁöÑÂºïÂäõÈÄèÈïúÊïàÂ∫îÁ≠âÁ≠â„ÄÇÂÆÉ‰πüË¢´ÂπøÊ≥õÂ∫îÁî®‰∫éÂ§©ÊñáÂ≠¶„ÄÅÁâ©ÁêÜÂ≠¶ÂíåÂ∑•Á®ãÂ≠¶Á≠âÈ¢ÜÂüüÔºå‰∏∫Êàë‰ª¨ÁêÜËß£ÂÆáÂÆôÁöÑÊú¨Ë¥®ÂíåÊºîÂåñÊèê‰æõ‰∫ÜÈáçË¶ÅÁöÑÁêÜËÆ∫Âü∫Á°Ä„ÄÇ


[Q]: ‰Ω†Áü•ÈÅìÂ§ßÁÜäÁå´ÁöÑ‰∏ªË¶ÅÈ£üÁâ©ÊòØ‰ªÄ‰πàÂêóÔºü
[A]: Â§ßÁÜäÁå´‰∏ªË¶Å‰ª•Á´πÂ≠ê‰∏∫È£üÔºå‰ΩÜ‰πü‰ºöÂêÉÂÖ∂‰ªñÊ§çÁâ©„ÄÅÊòÜËô´„ÄÅÂ∞èÂûãÂì∫‰π≥Âä®Áâ©ÂíåÈ±ºÁ±ª„ÄÇÂÆÉ‰ª¨ÁöÑÊ∂àÂåñÁ≥ªÁªü‰∏éÂÖ∂‰ªñÁÜäÁßëÂä®Áâ©Á±ª‰ººÔºåËÉΩÂ§üÊ∂àÂåñÁ´πÂ≠ê„ÄÇÂ§ßÁÜäÁå´ÁöÑÁâôÈΩøÈîãÂà©ÔºåÈÄÇÂêàÂíÄÂöºÁ´πÂ≠êÔºåÂõ†Ê≠§ÂÆÉ‰ª¨ÁªèÂ∏∏Ë¢´Áß∞‰∏∫‚ÄúÁ´πÈ£üÂä®Áâ©‚Äù„ÄÇ
[B]: Â§ßÁÜäÁå´ÊòØ‰ª•Á´πÂ≠ê‰∏∫È£üÁöÑÂì∫‰π≥Âä®Áâ©ÔºåÂÆÉ‰ª¨ÊòØËÇâÈ£üÂä®Áâ©„ÄÇÂÆÉ‰ª¨‰∏ªË¶ÅÈ£üÁâ©ÊòØÁ´πÂ≠êÔºåÁâπÂà´ÊòØÁ´πÂ≠ê‰∏≠ÂØåÂê´Á∫§Áª¥Á¥†ÂíåÁüøÁâ©Ë¥®ÔºåÂπ∂‰∏î‰∏ç‰ºöÁõ¥Êé•ËøõÂÖ•Ê∂àÂåñÁ≥ªÁªü„ÄÇÂ§ßÁÜäÁå´ÁöÑËÇ†ÈÅì‰∏ªË¶ÅÁî±Á´πÂ≠êÁªÑÊàêÔºåÂÆÉ‰ª¨ÈúÄË¶ÅÊ∂àÂåñÁ´πÂ≠ê‰∏≠ÁöÑÁ∫§Áª¥Á¥†ÂíåÂÖ∂‰ªñÁ∫§Áª¥Á¥†Êù•ÊîØÊåÅÂÆÉ‰ª¨ÁöÑÊ≠£Â∏∏ËøõÈ£ü„ÄÇ
[C]: Â§ßÁÜäÁå´‰∏ªË¶Å‰ª•Á´πÂ≠ê‰∏∫È£üÔºå‰ΩÜÂÆÉ‰ª¨‰πüÂêÉÂÖ∂‰ªñÊ§çÁâ©ÔºåÂåÖÊã¨Ê†ëÂè∂„ÄÅÂ´©ËäΩ„ÄÅÊòÜËô´„ÄÅÊ∞¥Êûú„ÄÅËî¨ËèúÂíåÂ∞èÂûãÂì∫‰π≥Âä®Áâ©Á≠â„ÄÇÂ§ßÁÜäÁå´ÊòØ‰∏≠ÂõΩÁâπÊúâÁöÑÁâ©ÁßçÔºåÂ±û‰∫éÁÜäÁßëÔºå‰∏ªË¶ÅÂàÜÂ∏ÉÂú®‰∏≠ÂõΩÁöÑÂõõÂ∑ù„ÄÅÈôïË•øÂíåÁîòËÇÉÁ≠âÂú∞„ÄÇ
[D]: Â§ßÁÜäÁå´ÁöÑ‰∏ªË¶ÅÈ£üÁâ©ÊòØÁ´πÂ≠ê„ÄÇÂÆÉ‰ª¨ÈÄöÂ∏∏ÂêÉÁ´πÂ≠êÔºåÊØèÂ§©ÈúÄË¶ÅÂêÉÊéâÂ§ßÁ∫¶30ÂÖ¨Êñ§ÁöÑÁ´πÂ≠ê„ÄÇ
[E]: ÊòØÁöÑÔºåÂ§ßÁÜäÁå´‰∏ªË¶Å‰ª•Á´πÂ≠ê‰∏∫È£ü„ÄÇÂÆÉ‰ª¨ÊØèÂ§©ÈúÄË¶ÅÂêÉÊéâÂ§ßÈáèÁöÑÁ´πÂ≠êÊù•Áª¥ÊåÅÁîüÂëΩÔºåÊØèÂ§©ÈúÄË¶ÅÂêÉÊéâ40Ëá≥80ÂÖ¨Êñ§ÁöÑÁ´πÂ≠ê„ÄÇÊ≠§Â§ñÔºåÂÆÉ‰ª¨‰πü‰ºöÂêÉ‰∏Ä‰∫õÂÖ∂‰ªñÁöÑÊ§çÁâ©ÔºåÂ¶ÇËçâ„ÄÅÊûúÂÆû„ÄÅÂùöÊûúÂíåÂ∞èÂûãÂì∫‰π≥Âä®Áâ©„ÄÇ
[F]: Â§ßÁÜäÁå´‰∏ªË¶Å‰ª•Á´πÂ≠ê‰∏∫È£üÔºå‰πü‰ºöÂÅ∂Â∞îÈ£üÁî®ÂÖ∂‰ªñÁöÑÊ§çÁâ©ÂíåÂä®Áâ©„ÄÇÊ≠§Â§ñÔºåÂ§ßÁÜäÁå´‰πü‰ºöÂêÉ‰∏Ä‰∫õÈáéÊûú„ÄÅÂ∞èÂä®Áâ©ÂíåÊ§çÁâ©Ôºå‰ΩÜËøô‰∫õÈÉΩ‰∏çÂ±û‰∫éÂÆÉ‰ª¨ÁöÑ‰∏ªË¶ÅÈ£üÁâ©Êù•Ê∫ê„ÄÇ
[G]: Â§ßÁÜäÁå´ÁöÑ‰∏ªË¶ÅÈ£üÁâ©ÊòØÁ´πÂ≠ê„ÄÇ
[H]: Â§ßÁÜäÁå´ÊòØ‰∏ÄÁßç‰ª•Á´πÂ≠ê‰∏∫‰∏ªË¶ÅÈ£üÁâ©ÁöÑÊ§çÈ£üÂä®Áâ©ÔºåÂÆÉ‰ª¨ÁöÑËÇ†ÈÅìÁªìÊûÑÈÄÇÂêàÊ∂àÂåñÁ∫§Áª¥Á¥†Âê´ÈáèÈ´òÁöÑÁ´πÂ≠êÔºå‰ΩÜ‰πü‰ºöÂêÉ‰∏Ä‰∫õÂÖ∂‰ªñÊ§çÁâ©ÔºåÂ¶ÇËçâÊú¨Ê§çÁâ©ÂíåÂ∞èÂûãÂì∫‰π≥Âä®Áâ©„ÄÇÂ§ßÁÜäÁå´ÁöÑËÇ†ÈÅìÁªìÊûÑ‰πüÈÄÇÂ∫î‰∫ÜÈ´òÁ∫§Áª¥ÁöÑÁ´πÂ≠êÔºåËøô‰ΩøÂæóÂÆÉ‰ª¨ËÉΩÂ§üÊ∂àÂåñÂ§ßÈáèÁöÑÁ∫§Áª¥Á¥†Ôºå‰ªéËÄåÂ∏ÆÂä©ÂÆÉ‰ª¨Âú®ÈáéÂ§ñÁîüÂ≠ò„ÄÇÂ§ßÁÜäÁå´ÁöÑËÇ†ÈÅì‰∏≠ËøòÂê´Êúâ‰∏ÄÁßçÂè´ÂÅö‚ÄúÁ∫§Áª¥Á¥†ÈÖ∂‚ÄùÁöÑÈÖ∂Á±ªÁâ©Ë¥®ÔºåËøôÁßçÈÖ∂ËÉΩÂ§üÂ∏ÆÂä©Â§ßÁÜäÁå´ÂàÜËß£Á∫§Áª¥Á¥†Ôºå‰ªéËÄåÂ∏ÆÂä©ÂÆÉ‰ª¨Ê∂àÂåñÂ§ßÈáèÁöÑÊ∞¥ÂàÜÂíåËê•ÂÖªÁâ©Ë¥®„ÄÇÊ≠§Â§ñÔºåÂ§ßÁÜäÁå´ÁöÑËÇ†ÈÅìËøòÂê´Êúâ‰∏ÄÁßçÂè´ÂÅö‚ÄúÁ∫§Áª¥Á¥†ËõãÁôΩ‚ÄùÁöÑËõãÁôΩË¥®ÔºåËøôÁßçËõãÁôΩË¥®ËÉΩÂ§üÂ∏ÆÂä©Â§ßÁÜäÁå´Áª¥ÊåÅ‰ΩìÂÜÖÁöÑËÉΩÈáèÊ∞¥Âπ≥Ôºå‰ªéËÄåÂ∏ÆÂä©ÂÆÉ‰ª¨Âú®ÈáéÂ§ñÁîüÂ≠ò„ÄÇÊÄªÁöÑÊù•ËØ¥ÔºåÂ§ßÁÜäÁå´ÁöÑ‰∏ªË¶ÅÈ£üÁâ©ÊòØÁ´πÂ≠êÔºå‰ΩÜÂÆÉ‰ª¨‰πü‰ºöÂêÉ‰∏Ä‰∫õÂÖ∂‰ªñÊ§çÁâ©ÔºåÂ¶ÇËçâÊú¨Ê§çÁâ©ÂíåÂ∞èÂûãÂì∫‰π≥Âä®Áâ©„ÄÇ


[Q]: Êµ∑Ê∞¥‰∏∫‰ªÄ‰πàÊòØÂí∏ÁöÑÔºü
[A]: Êµ∑Ê∞¥‰πãÊâÄ‰ª•ÊòØÂí∏ÁöÑÔºå‰∏ªË¶ÅÊòØÁî±‰∫éÂÖ∂‰∏≠ÁöÑÂåñÂ≠¶ÊàêÂàÜ„ÄÇÊµ∑Ê∞¥‰∏≠Âê´ÊúâÂ§ßÈáèÁöÑÁüøÁâ©Ë¥®ÂíåÁõêÂàÜÔºåÂÖ∂‰∏≠‰∏ªË¶ÅÊàêÂàÜÊòØÁ°´ÈÖ∏Áõê„ÄÅÁ¢≥ÈÖ∏Áõê„ÄÅÈïÅÁ¶ªÂ≠êÂíåÈíæÁ¶ªÂ≠ê„ÄÇËøô‰∫õÁâ©Ë¥®Âú®È´òÊµìÂ∫¶‰∏ãÔºå‰ºöÊ≤âÊ∑ÄÂú®Êµ∑Ê∞¥‰∏≠ÔºåÂΩ¢ÊàêÊµ∑Ê∞¥„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊµ∑Ê∞¥‰∏≠ÁöÑÁüøÁâ©Ë¥®ÂíåÁõêÂàÜ‰∏ªË¶ÅÊù•Ëá™‰∫éÂ≤©Áü≥„ÄÅÂúüÂ£§„ÄÅÁîüÁâ©Âíå‰∫∫Á±ªÊéíÊ≥ÑÁâ©Á≠â„ÄÇÂ≤©Áü≥ÊòØÊµ∑Ê∞¥ÁöÑÁªÑÊàêÈÉ®ÂàÜÔºå‰∏ªË¶ÅÁî±ÁüøÁâ©Ë¥®ÁªÑÊàêÔºåÂ¶ÇÁü≥Ëã±„ÄÅÈïøÁü≥„ÄÅ‰∫ëÊØçÁ≠â„ÄÇÂúüÂ£§‰∏≠Âê´ÊúâÂ§ßÈáèÁöÑÂåñÂ≠¶Áâ©Ë¥®ÔºåÂ¶ÇÊ∞Æ„ÄÅÁ£∑„ÄÅÈíæÁ≠âÔºåËøô‰∫õÁâ©Ë¥®‰ºöÊ∏óÈÄèÂà∞ÂúüÂ£§‰∏≠ÔºåÂΩ¢ÊàêÂúüÂ£§„ÄÇÁîüÁâ©‰ΩìÂÜÖÁöÑÁõêÂàÜ‰∏ªË¶ÅÊù•Ê∫ê‰∫éÊ§çÁâ©ÁöÑÂè∂ÁªøÁ¥†Á≠âËâ≤Á¥†ÔºåËøô‰∫õËâ≤Á¥†‰ºöÂê∏Êî∂Â§™Èò≥ÂÖâ‰∏≠ÁöÑËÉΩÈáèÔºåËΩ¨Âåñ‰∏∫ÁîüÁâ©‰ΩìÂèØ‰ª•Âà©Áî®ÁöÑËÉΩÈáè„ÄÇËÄå‰∫∫Á±ªÊéíÊ≥ÑÁâ©‰∏≠Âê´ÊúâÁöÑÁõêÂàÜ‰∏ªË¶ÅÊù•Ê∫ê‰∫éÁüøÁâ©Ë¥®ÂíåÁ¢≥ÈÖ∏ÁõêÔºåËøô‰∫õÁüøÁâ©Ë¥®Âú®Ê∞¥‰∏≠ÁöÑÊµìÂ∫¶ËæÉÈ´òÔºå‰ºöÂØºËá¥Ê∞¥‰∏≠Ê∫∂Ëß£ÁöÑÁüøÁâ©Ë¥®ÊµìÂ∫¶Â¢ûÂä†ÔºåÂΩ¢ÊàêÊ∞¥‰Ωì‰∏≠ÁöÑÁõêÂàÜ„ÄÇÂõ†Ê≠§ÔºåÊµ∑Ê∞¥‰πãÊâÄ‰ª•ÊòØÂí∏ÁöÑÔºåÊòØÁî±‰∫éÂÖ∂‰∏≠ÁöÑÂåñÂ≠¶ÊàêÂàÜÂíåÁªìÊûÑÁâπÂæÅ„ÄÇÊµ∑Ê∞¥‰∏≠ÁöÑÁüøÁâ©Ë¥®ÂíåÁõêÂàÜ‰∏ªË¶ÅÊù•Ëá™‰∫éÂ≤©Áü≥„ÄÅÂúüÂ£§„ÄÅÁîüÁâ©Âíå‰∫∫Á±ªÊéíÊ≥ÑÁâ©Á≠âÔºåËÄåËøô‰∫õÁâ©Ë¥®ÁöÑÁßØÁ¥ØÂíåÊéíÊ≥Ñ‰ºöÂØºËá¥Êµ∑Ê∞¥ÂëàÁé∞Âá∫Âí∏Âë≥„ÄÇ
[B]: Êµ∑Ê∞¥ÊòØÂí∏ÁöÑÔºåÂõ†‰∏∫ÂÆÉÂåÖÂê´‰∫ÜÂ§öÁßçÁâ©Ë¥®ÔºåÂåÖÊã¨Ê∞ØÂåñÈí†ÔºàNaClÔºâ„ÄÅÊ∞ØÂåñÈí†ÔºàNaClÔºâ„ÄÅÁ¢≥ÈÖ∏ÁõêÔºàÂ¶ÇÁ°´ÈÖ∏ÁõêÔºâ„ÄÅÊ∞ØÂåñÈí†ÔºàNaClÔºâÂíåÊ∞¥„ÄÇÂí∏Áâ©Ë¥®ÁöÑÊù•Ê∫êÔºöÁõêÊòØÂí∏ÁöÑÂ∏∏ËßÅÊù•Ê∫êÔºåÂê´ÊúâÈíæ„ÄÅÈíô„ÄÅÈïÅ„ÄÅÈíæÁ≠âÁüøÁâ©Ë¥®„ÄÇËøô‰∫õÁüøÁâ©Ë¥®ÈÄöÂ∏∏Êù•Ëá™‰∫éÂú∞‰∏ãÊ∞¥„ÄÅÂú∞‰∏ãÊ∞¥ÊàñÂúüÂ£§‰∏≠ÔºåËøô‰∫õÁüøÁâ©Ë¥®ÁöÑÂê´ÈáèËøú‰Ωé‰∫éÁ∫ØÂáÄÊ∞¥„ÄÇÂí∏Âë≥ÁöÑÂéüÂõ†ÔºöÁõêÁöÑÂí∏Âë≥‰∏ªË¶ÅÊ∫ê‰∫éÂÖ∂Ê∫∂Ëß£‰∫éÊ∞¥‰∏≠ÁöÑÂæÆÂ∞èÈ¢óÁ≤íÁâ©„ÄÇËøô‰∫õÂ∞èÈ¢óÁ≤íÁâ©‰∏ªË¶ÅÈÄöËøáÊ§çÁâ©Ê†πÈÉ®ÂíåÂä®Áâ©‰ΩìÂÜÖÁöÑÂæÆÁîüÁâ©‰º†Êí≠ÔºåÂ¶ÇÊòÜËô´„ÄÅÂ∞èÂûãÂì∫‰π≥Âä®Áâ©„ÄÅÈ∏üÁ±ªÂíåÊüê‰∫õÊòÜËô´„ÄÇÁªº‰∏äÊâÄËø∞ÔºåÊµ∑Ê∞¥ÊòØÂí∏ÁöÑÔºåÂõ†‰∏∫ÂÆÉÂê´ÊúâÂ§ßÈáèÁöÑÁõêÁ±ªÔºåÂåÖÊã¨ÁüøÁâ©Ë¥®„ÄÅÂ≤©Áü≥„ÄÅÁõê„ÄÅÁüøÁâ©Ë¥®Á≠âÔºåËøô‰∫õÁâ©Ë¥®Âú®Êµ∑Ê∞¥‰∏≠ÁöÑÊµìÂ∫¶È´òÔºåÂØπÂí∏Âë≥ÁöÑ‰∫ßÁîüËæÉÂ§ßÂΩ±Âìç„ÄÇ
[C]: Êµ∑Ê∞¥ÊòØÂí∏ÁöÑÔºå‰∏ªË¶ÅÊòØÂõ†‰∏∫ÂÖ∂‰∏≠Âê´ÊúâÂêÑÁßçÁõêÂàÜÔºåÂÖ∂‰∏≠‰ª•Êµ∑Ê∞¥‰∏∫‰∏ªË¶ÅÊàêÂàÜ‰πã‰∏Ä„ÄÇÂΩìÊµ∑Ê∞¥‰∏≠Âê´ÊúâÂ§ßÈáèÁöÑÁõêÂàÜÊó∂ÔºåÁõêÂàÜ‰ºöË¢´Ê∫∂Ëß£Âú®Êµ∑Ê∞¥‰∏≠ÔºåÂΩ¢ÊàêÂí∏Âë≥„ÄÇÊµ∑Ê∞¥‰∏≠ÁöÑÁõêÂàÜ‰∏ªË¶ÅÊù•Ê∫ê‰∫éÊ∫∂Ëß£ÁõêÁ±ª„ÄÅËí∏ÂèëÊûêÂá∫ÁöÑÁõêÂíåÊµ∑Ê∞¥ÂÜªÁªìÊàêÂÜ∞Á≠âÂõ†Á¥†„ÄÇ
[D]: Êµ∑Ê∞¥‰∏≠Âê´ÊúâËÆ∏Â§öÁõêÂàÜÔºåËøô‰∫õÁõêÂàÜÂèØ‰ª•ÂØºËá¥Êµ∑Ê∞¥Âí∏Â∫¶ÁöÑÂ¢ûÂä†„ÄÇÊµ∑Ê∞¥‰∏≠ÁöÑÁõêÂàÜ‰∏ªË¶ÅÊù•Ëá™‰∫éÊµ∑Ê∞¥‰∏≠ÁöÑÁõêÂàÜÔºåËøô‰∫õÁõêÂàÜÂèØ‰ª•‰ªéÊµ∑Ê∞¥‰∏≠ÁöÑÁõêÂàÜ‰∏≠Ëé∑Âæó„ÄÇÊµ∑Ê∞¥‰∏≠ÁöÑÊ∞ßÂåñÁâ©ÂíåÁõêÂàÜÂèØ‰ª•‰∏éÊµ∑Ê∞¥‰∏≠ÁöÑÁõêÂàÜÂΩ¢ÊàêÊ∞¢ÈîÆÔºå‰ªéËÄå‰ΩøÊµ∑Ê∞¥Âí∏Â∫¶Â¢ûÂä†„ÄÇ
[E]: Êµ∑Ê∞¥ÊòØÂí∏ÁöÑÔºåËøôÊòØÂõ†‰∏∫ÂÆÉÂê´ÊúâÂ§ßÈáèÁöÑÁõêÂàÜÔºåÂÖ∂‰∏≠ÂåÖÊã¨Ê∞ØÁ¶ªÂ≠êÂíåÊ∞ØÁ¶ªÂ≠ê„ÄÇËøô‰∫õÁ¶ªÂ≠êÂú®Êµ∑Ê¥ã‰∏≠ÂΩ¢Êàê‰∫ÜÂ§ßÈáèÁöÑÁõêÔºåËøô‰∫õÁõêÂàÜË¢´Áß∞‰∏∫ÁõêÂàÜ„ÄÇÊµ∑Ê∞¥‰∏≠Âê´ÊúâËÆ∏Â§öÊ∫∂Ëß£Âú®Ê∞¥‰∏≠ÁöÑÁõêÔºåËøô‰∫õÁõêÂàÜ‰ºöÈöèÁùÄÊµ∑Ê∞¥ÁöÑÂçáÈ´òËÄåÈÄêÊ∏êËí∏ÂèëÔºåÊúÄÁªàÂØºËá¥Êµ∑Ê∞¥‰∏≠ÁõêÂàÜÁöÑËí∏Âèë„ÄÇËøô‰∫õÁõêÂàÜÂú®Êµ∑Ê∞¥‰∏≠Ë¢´ÂÆåÂÖ®Ê∂àËÄóÔºåÁÑ∂ÂêéË¢´ÊéíÈô§Âú®Êµ∑Ê∞¥‰∏≠„ÄÇÊ≠§Â§ñÔºåÊµ∑Ê∞¥‰∏≠ËøòÂê´ÊúâËÆ∏Â§öÂÖ∂‰ªñÁöÑÁõêÂàÜÔºå‰ΩÜËøô‰∫õÁõêÂàÜÊòØ‰∏çÂêåÁöÑÔºåÂõ†‰∏∫ÂÆÉ‰ª¨Âú®Ê∞¥‰∏≠ÁöÑÊµÆËµ∑Âíå‰∏ãÈôçËøáÁ®ãÊòØ‰∏çÂêåÁöÑ„ÄÇÊµ∑Ê∞¥‰∏≠ÁöÑÁõêÂàÜÊòØ‰ªéÊµ∑Â∫ïÁöÑÂ≤©Áü≥‰∏≠ÊèêÂèñÂá∫Êù•ÁöÑÔºåËøô‰∫õÁõêÂàÜÂú®Êµ∑Ê∞¥‰∏≠ÂèØ‰ª•Ë¢´Âà©Áî®„ÄÇÊÄªÁöÑÊù•ËØ¥ÔºåÊµ∑Ê∞¥ÊòØÂí∏ÁöÑÔºåËøô‰πüÊòØ‰∏∫‰ªÄ‰πàÊµ∑Ê∞¥‰∏≠Âê´ÊúâËÆ∏Â§öÁõêÂàÜÂíåÁõêÂàÜÁöÑÂéüÂõ†„ÄÇ
[F]: Êµ∑Ê∞¥Âê´ÊúâËÆ∏Â§öÊ∫∂Ëß£ÁöÑÁõêÁ±ªÔºåÂÖ∂‰∏≠ÊúÄ‰∏ªË¶ÅÁöÑÊòØÊ∞ØÂåñÈí†„ÄÇÂΩìÊ∞¥‰∏≠Âä†ÂÖ•ÁõêÊó∂ÔºåÁõêÁöÑÂàÜÂ≠ê‰ºö‰∏éÊ∞¥ÂàÜÂ≠êÂèëÁîüÂèçÂ∫îÔºåÂΩ¢ÊàêÊ∞¢Ê∞ßÂåñÈí†ÔºåËøôÊòØ‰∏ÄÁßçÁõê„ÄÇ
[G]: Âõ†‰∏∫Ê∞¥ÊòØËÆ∏Â§öÁîüÁâ©ÁöÑÁîüÂ≠òÊâÄÂøÖÈúÄÁöÑÔºåÂÆÉËÉΩÂ§üÂê∏Êî∂ÂíåÈáäÊîæ‰∫åÊ∞ßÂåñÁ¢≥ÔºåÈáäÊîæÊ∞ßÊ∞îÔºåÁª¥ÊåÅÂ§ßÊ∞î‰∏≠ÁöÑÊ∞¥Ëí∏Ê∞îÔºå‰ª•ÂèäÁª¥ÊåÅÊ∞¥Âæ™ÁéØ„ÄÇÊ∞¥‰∏≠ÁöÑÁõêÁ±ªÂíåÂåñÂ≠¶Áâ©Ë¥®‰ºöÈáäÊîæÂá∫Ê∞ßÊ∞îÔºå‰ΩøÂú∞ÁêÉ‰∏äÁöÑÁîüÁâ©ÂëºÂê∏„ÄÇÊ≠§Â§ñÔºåÊ∞¥ËøòÂÖ∑ÊúâËÆ∏Â§öÂÖ∂‰ªñÂäüËÉΩÔºåÂ¶Ç‰øùÊåÅÊ∞¥Âæ™ÁéØ„ÄÅÁª¥ÊåÅÊ∞¥Âæ™ÁéØ„ÄÅË∞ÉËäÇÊ∞îÂÄôÂíåÁéØÂ¢ÉÁ≠â„ÄÇÊ∞¥ÊòØËÆ∏Â§öÁîüÁâ©ÁîüÂ≠òÊâÄÂøÖÈúÄÁöÑÔºåÂÆÉ‰πüÊòØËÆ∏Â§öËá™ÁÑ∂Áé∞Ë±°ÁöÑÈáçË¶ÅÂü∫Á°Ä„ÄÇ
[H]: Êµ∑Ê∞¥ÊòØÂí∏ÁöÑ‰∏ªË¶ÅÂéüÂõ†ÊòØÂõ†‰∏∫ÂÖ∂‰∏≠Âê´ÊúâÂ§ßÈáèÁöÑÊ∫∂Ëß£ÁõêÁ±ªÔºåÂÖ∂‰∏≠ÊúÄ‰∏ªË¶ÅÁöÑÊòØÊ∞ØÂåñÈí†ÔºàNaClÔºâ„ÄÇÊ∞ØÂåñÈí†ÊòØ‰∏ÄÁßçÈùûÂ∏∏Â∏∏ËßÅÁöÑÁõêÁ±ªÔºåÂÆÉÂ≠òÂú®‰∫éÂú∞ÁêÉ‰∏äÁöÑÂ§ßÈÉ®ÂàÜÂú∞Âå∫ÔºåÂåÖÊã¨Êµ∑Ê¥ã„ÄÅÊπñÊ≥ä„ÄÅÊ≤≥ÊµÅÂíåÂú∞‰∏ãÊ∞¥„ÄÇÂΩìÊ∞¥‰ªéÊµ∑Ê¥ã‰∏≠Ëí∏ÂèëÊó∂ÔºåÂÆÉ‰ºöÂ∏¶Ëµ∞‰∏ÄÈÉ®ÂàÜÁõêÂàÜÔºå‰ΩøÊµ∑Ê∞¥ÂèòÂæóÂí∏„ÄÇÊ≠§Â§ñÔºåÊµ∑‰∏≠ËøòÂê´ÊúâÂÖ∂‰ªñÊ∫∂Ëß£ÁõêÁ±ªÁöÑÁâ©Ë¥®ÔºåÂ¶ÇÁ°´ÈÖ∏Áõê„ÄÅÁ°´ÈÖ∏ÈïÅ„ÄÅÁ°´ÈÖ∏Èíô„ÄÅÁ°´ÈÖ∏ÈïÅÁõê„ÄÅÁ°´ÈÖ∏ÈïÅÈíæ„ÄÅÁ°´ÈÖ∏ÈïÅÈí†„ÄÅÁ°´ÈÖ∏ÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈí†„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅ


[Q]: ÂÜô‰∏ÄÊÆµpythonÁöÑÂø´ÈÄüÊéíÂ∫è‰ª£Á†Å
[A]: Â•ΩÁöÑÔºå‰ª•‰∏ãÊòØPythonÁöÑÂø´ÈÄüÊéíÂ∫è‰ª£Á†ÅÔºödef quick_sort(arr):    if len(arr) &amp;lt;= 1:        return arr    else:        pivot = arr[len(arr) // 2]        left = [x for x in arr[1:]]        right = [x for x in arr[2:] if x &amp;lt; pivot]        return quick_sort(left) + [pivot] + quick_sort(right)# ÊµãËØï‰ª£Á†Åarr = [3, 6, 8, 10, 1, 2, 1]print(quick_sort(arr))ËøôÊÆµ‰ª£Á†ÅÈ¶ñÂÖàÊ£ÄÊü•Êï∞ÁªÑÁöÑÈïøÂ∫¶ÊòØÂê¶Â∞è‰∫éÁ≠â‰∫é1„ÄÇÂ¶ÇÊûúÊòØÔºåÈÇ£‰πàÂÆÉÂ∞±ÊåâÁÖß‰ªéÂ∞èÂà∞Â§ßÁöÑÈ°∫Â∫èÊéíÂàó„ÄÇÊé•ÁùÄÔºåÂØπ‰∫éÊï∞ÁªÑÁöÑÊØè‰∏™ÂÖÉÁ¥†ÔºåÂÆÉÊÉ≥Ë¶ÅËøõË°åÈÄíÂΩíÂú∞ÊéíÂ∫èÔºåÁõ¥Âà∞ËææÂà∞ÊéíÂ∫èÁöÑÁªìÊùü‰ΩçÁΩÆ„ÄÇÂ¶ÇÊûúÊï∞ÁªÑÁöÑÈïøÂ∫¶ÊòØÂ•áÊï∞ÔºåÈÇ£‰πàÂÆÉÂ∞±ÊåâÁÖß‰ªéÂ§ßÂà∞Â∞èÁöÑÈ°∫Â∫èÊéíÂàó„ÄÇÊúÄÂêéÔºåÂπ≥ÂùáÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶ÊòØO(n log n)ÔºåÂÖ∂‰∏≠nÊòØÊï∞ÁªÑÁöÑÈïøÂ∫¶„ÄÇ
[B]: ‰ª•‰∏ãÊòØ‰∏Ä‰∏™‰ΩøÁî®PythonÁöÑÂø´ÈÄüÊéíÂ∫èÁÆóÊ≥ïÁöÑÂÆûÁé∞Ôºödef sort_array(arr):    # ÁîüÊàê‰∏Ä‰∏™ÈöèÊú∫ÊéíÂàó    sorted_array = sorted(arr)    # ËøîÂõûÊéíÂ∫èÂêéÁöÑÂàóË°®    return sorted(arr)# Á§∫‰æãarr = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]# Ë∞ÉÁî®ÂáΩÊï∞sorted_array = sort_array(arr)print("ÂéüÂßãÊï∞ÁªÑÊòØÔºö", sorted_array)ËøôÊÆµ‰ª£Á†ÅÈ¶ñÂÖàÁîüÊàê‰∏Ä‰∏™ÈöèÊú∫ÊéíÂàóÔºåÁÑ∂ÂêéÈÄöËøá‰∏Ä‰∏™ÂàóË°®ÁöÑÈïøÂ∫¶ËøõË°åÊéíÂ∫è„ÄÇÊúÄÂêéÔºåËøîÂõûÊéíÂ∫èÂêéÁöÑÂàóË°®„ÄÇÊ≥®ÊÑèÔºåËøô‰∏™‰ª£Á†Å‰ΩøÁî®‰∫ÜPythonÁöÑÂÜÖÁΩÆsortedÂáΩÊï∞ÂíåÂÜÖÁΩÆÁöÑsorted()ÂáΩÊï∞ÔºåÂÆÉ‰ª¨Âú®Êó∂Èó¥Â§çÊùÇÂ∫¶‰∏∫O(n log n)ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊØîÁõ¥Êé•‰ΩøÁî®sorted()ÂáΩÊï∞Êõ¥Âø´„ÄÇ
[C]: ‰ª•‰∏ãÊòØ‰∏Ä‰∏™ÁÆÄÂçïÁöÑÊéíÂ∫èÁÆóÊ≥ïÔºåÈÄÇÁî®‰∫éÂ∞èÂûãÊï∞ÁªÑÔºåÂåÖÊã¨Â∞è‰∫éÁ≠â‰∫é5ÁöÑÊï∞ÁªÑ„ÄÇdef quick_sort(arr):    if len(arr) &amp;lt;= 1:        return arr    else    for _ in range(2, 51):        if arr[len(arr) - 1:] is not None:            arr[len(arr) - 1] = arr[len(arr) - 1]                arr[len(arr)] = arr[len(arr)]    return quick_sort(arr)# Á§∫‰æãarr = [3, 5, 1, 4, 1, 2, 1]sorted_arr = quick_sort(arr)print("Sorted array is:", sorted_arr)### ‰ª£Á†ÅËß£ÈáäÔºö1. **ÂáΩÊï∞ÂÆö‰πâ**Ôºöquick_sort ÂáΩÊï∞Êé•Âèó‰∏Ä‰∏™Êï∞ÁªÑ arr ‰Ωú‰∏∫ÂèÇÊï∞„ÄÇ2. **Âæ™ÁéØ**ÔºöÂØπ‰∫é arr ÁöÑÂÖÉÁ¥†Ôºå‰ΩøÁî®ÈÄíÂΩíÊù•ÂÆûÁé∞ÔºåÂç≥‰ªéÂè≥Âà∞Â∑¶ÈÅçÂéÜÊï∞ÁªÑÔºåÂπ∂Â∞ÜÂΩìÂâçÂÖÉÁ¥†‰∏éÊï∞ÁªÑÁöÑÂΩìÂâçÂÖÉÁ¥†‰∫§Êç¢‰ΩçÁΩÆÔºåÁõ¥Âà∞Êï∞ÁªÑËææÂà∞Á©∫ÊàñÊï∞ÁªÑÈïøÂ∫¶Â∞è‰∫éÁ≠â‰∫é arr„ÄÇ3. **ÈÄíÂΩíË∞ÉÁî®**ÔºöÈÅçÂéÜÊï∞ÁªÑÔºåÂ∞ÜÂΩìÂâçÂÖÉÁ¥†‰∏éÊï∞ÁªÑÁöÑÂΩìÂâçÂÖÉÁ¥†ËøõË°å‰∫§Êç¢ÔºåÁõ¥Âà∞Êï∞ÁªÑËææÂà∞Á©∫ÊàñÊï∞ÁªÑÈïøÂ∫¶Â∞è‰∫éÁ≠â‰∫é arr„ÄÇ4. **Ê£ÄÊü•Êï∞ÁªÑÈïøÂ∫¶**Ôºö‰ΩøÁî® if __name__ == "__main__" Ê£ÄÊü•Êï∞ÁªÑÁöÑÈïøÂ∫¶ÊòØÂê¶Â∞è‰∫é arrÔºåÂ¶ÇÊûúÊòØÔºåÂ∞±Â∞ÜÊï∞ÁªÑ‰ªéËæÉÂ§ßÁöÑÊï∞ÁªÑÁßªÂä®Âà∞Êï∞ÁªÑÁöÑÊú´Â∞æ„ÄÇ5. **ËæìÂá∫ÊéíÂ∫èÂêéÁöÑÁªìÊûú**
[D]: ‰Ωú‰∏∫‰∏Ä‰∏™AIËØ≠Ë®ÄÊ®°ÂûãÔºåÊàëÊó†Ê≥ïÁºñÂÜôPythonÁ®ãÂ∫è„ÄÇ‰ΩÜÊòØÔºåPythonÊòØ‰∏Ä‰∏™ÈùûÂ∏∏ÊµÅË°åÁöÑÁºñÁ®ãËØ≠Ë®ÄÔºåÂÆÉÂÖ∑ÊúâËÆ∏Â§ö‰ºòÁßÄÁöÑÂ∫ìÂíåÊ°ÜÊû∂ÔºåÂåÖÊã¨NumPy„ÄÅPandas„ÄÅMatplotlib„ÄÅPandas„ÄÅBandke„ÄÅNormÂíåNKlearnÁ≠â„ÄÇPythonÊúâÂæàÂ§ö‰ºòÁßÄÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂ∫ìÂíåÊ°ÜÊû∂ÔºåÂèØ‰ª•Áî®‰∫éÂ§ÑÁêÜÂíåÂàÜÊûêÂêÑÁßçÁ±ªÂûãÁöÑÊï∞ÊçÆÔºåÂ¶ÇÊñáÊú¨„ÄÅÂõæÂÉè„ÄÅÈü≥È¢ë„ÄÅËßÜÈ¢ëÁ≠â„ÄÇPythonËøòÂèØ‰ª•Áî®‰∫éÂàõÂª∫ÂêÑÁßçÂ∫îÁî®Á®ãÂ∫èÔºåÂ¶ÇÁΩëÈ°µÊêúÁ¥¢„ÄÅÊ∏∏ÊàèÂºÄÂèë„ÄÅÁßëÂ≠¶ËÆ°ÁÆóÁ≠â„ÄÇPythonÈùûÂ∏∏ÁÅµÊ¥ªÔºå‰πüÊîØÊåÅÂ§öÁßçÁºñÁ®ãÈ£éÊ†ºÔºåÂõ†Ê≠§ÂèØ‰ª•ËΩªÊùæÂú∞ÊâæÂà∞ÈÄÇÂêàËá™Â∑±ÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ
[E]: ‰Ωú‰∏∫AIËØ≠Ë®ÄÊ®°ÂûãÔºåÊàëÂèØ‰ª•ÂõûÁ≠îÊÇ®ÁöÑÈóÆÈ¢ò„ÄÇ‰ΩÜÊòØÊàëÊó†Ê≥ïËøõË°åÁºñÁ®ãÊìç‰ΩúÔºåÂõ†‰∏∫ÊàëÊó†Ê≥ïÁ°ÆÂÆöÊÇ®ÊÉ≥ÈóÆÁöÑÊòØÂì™ÁßçÁºñÁ®ãËØ≠Ë®Ä„ÄÇËØ∑ÊÇ®Êèê‰æõÊõ¥ÂÖ∑‰ΩìÁöÑÈóÆÈ¢òÔºå‰ª•‰æøÊàëËÉΩÂ§üÊõ¥Â•ΩÂú∞ÂõûÁ≠îÊÇ®ÁöÑÈóÆÈ¢ò„ÄÇ
[F]: Âø´ÈÄüÊéíÂ∫èÔºàQuick SortÔºâÊòØ‰∏ÄÁßçÁÆÄÂçïÁöÑÊéíÂ∫èÁÆóÊ≥ïÔºåÂÆÉÈÄöËøáÂ∞ÜÊï∞ÊçÆÂàíÂàÜ‰∏∫Â∞èÁªÑÔºåÊØèÁªÑÊï∞ÊçÆÊåâÁÖß‰∏ÄÂÆöËßÑÂàôÊéíÂ∫èÔºåÊúÄÁªàÂæóÂà∞‰∏Ä‰∏™ÊúâÂ∫èÊï∞ÁªÑ„ÄÇ
[G]: Â•ΩÁöÑÔºå‰ª•‰∏ãÊòØ‰∏ÄÊÆµÂø´ÈÄüÊéíÂ∫èÁöÑPython‰ª£Á†Å```python sorted_print(sorted(sorted(sorted(sorted_print))```ËæìÂá∫ÁªìÊûú‰∏∫Ôºö```[1, 2, 3, 4, 5, 6, 7, 8, 9]```
[H]: Âø´ÈÄüÊéíÂ∫èÊòØ‰∏ÄÁßçÂü∫‰∫éÂàÜÊ≤ªÊÄùÊÉ≥ÁöÑÈ´òÊïàÊéíÂ∫èÁÆóÊ≥ïÔºåÂÆÉÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶‰∏∫ O(n log n)„ÄÇÂÆÉÁöÑÂü∫Êú¨ÊÄùÊÉ≥ÊòØÈÄöËøá‰∏ÄË∂üÊéíÂ∫èÂ∞ÜÂæÖÊéíÂ∫èÂ∫èÂàóÂàíÂàÜ‰∏∫‰∏§‰∏™Â≠êÂ∫èÂàóÔºåÂÖ∂‰∏≠‰∏Ä‰∏™Â≠êÂ∫èÂàóÁöÑÊâÄÊúâÂÖÉÁ¥†ÈÉΩÊØîÂè¶‰∏Ä‰∏™Â≠êÂ∫èÂàóÁöÑÊâÄÊúâÂÖÉÁ¥†Â∞èÔºåÁÑ∂ÂêéÂØπËøô‰∏§‰∏™Â≠êÂ∫èÂàóÂàÜÂà´ÈÄíÂΩíÂú∞ËøõË°åÊéíÂ∫è„ÄÇ‰∏ãÈù¢ÊòØ‰∏Ä‰∏™ Python ‰ª£Á†ÅÁ§∫‰æãÔºåÂÆûÁé∞‰∫ÜÂø´ÈÄüÊéíÂ∫èÁÆóÊ≥ïÔºö```pythondef quick_sort(arr):    if len(arr) &amp;gt; 1:        return arr    else: return quick_sort(arr)```ËØ•‰ª£Á†Å‰ΩøÁî®‰∫ÜÈÄíÂΩíÁöÑÊñπÂºèÔºåÈ¶ñÂÖàÂà§Êñ≠Êï∞ÁªÑÁöÑÈïøÂ∫¶ÊòØÂê¶Â∞è‰∫éÁ≠â‰∫é1ÔºåÂ¶ÇÊûúÊòØÔºåÂàôÁõ¥Êé•ËøîÂõûÊï∞ÁªÑÔºåÂê¶ÂàôÈÄíÂΩíÂú∞ÂØπÊï∞ÁªÑËøõË°åÊéíÂ∫è„ÄÇÂú®ÈÄíÂΩíËøáÁ®ã‰∏≠ÔºåÊàë‰ª¨È¶ñÂÖàÂà§Êñ≠Êï∞ÁªÑÁöÑÁ¨¨‰∏Ä‰∏™ÂÖÉÁ¥†ÊòØÂê¶Â§ß‰∫éÁ≠â‰∫é1ÔºåÂ¶ÇÊûúÊòØÔºåÂàôÂ∞ÜÊï∞ÁªÑ‰∏≠ÁöÑÂÖÉÁ¥†‰∏éÁ¨¨‰∏Ä‰∏™ÂÖÉÁ¥†ËøõË°åÊØîËæÉÔºåÂ¶ÇÊûú‰∏çÁ≠â‰∫é1ÔºåÂàôÂ∞ÜÊï∞ÁªÑ‰∏≠ÁöÑÂÖÉÁ¥†‰∏éÁ¨¨‰∫å‰∏™ÂÖÉÁ¥†ËøõË°åÊØîËæÉÔºå‰ª•Ê≠§Á±ªÊé®ÔºåÁõ¥Âà∞ÊâÄÊúâÂÖÉÁ¥†ÈÉΩË¢´ÊéíÂ∫è„ÄÇËØ•‰ª£Á†ÅÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶‰∏∫ O(n log n)ÔºåÊòØ‰∏Ä‰∏™ÈùûÂ∏∏ÂÆûÁî®ÁöÑÊéíÂ∫èÁÆóÊ≥ï„ÄÇ


[Q]: Êà™Ëá≥2025Âπ¥2Êúà9Êó•14Êó∂31ÂàÜÔºåÁîµÂΩ±„ÄäÂì™Âêí‰πãÈ≠îÁ´•ÈóπÊµ∑„ÄãÁ¥ØËÆ°Á•®ÊàøÔºàÂê´È¢ÑÂîÆÔºâÁ™ÅÁ†¥78.09‰∫øÂÖÉÔºåË∂ÖËøá„ÄäÊòüÁêÉÂ§ßÊàòÔºöÂ§©Ë°åËÄÖÂ¥õËµ∑„ÄãÁöÑÁ•®ÊàøÊàêÁª©ÔºåÊàê‰∏∫È¶ñÈÉ®ËøõÂÖ•ÂÖ®ÁêÉÁ•®ÊàøÊ¶úÂâç40ÁöÑ‰∫öÊ¥≤ÁîµÂΩ±„ÄÇËøô‰∏ÄÊàêÂ∞±‰∏ç‰ªÖÊ†áÂøóÁùÄ‰∏≠ÂõΩÁîµÂΩ±Âú®ÂõΩÈôÖÂ∏ÇÂú∫ÁöÑÁ™ÅÁ†¥Ôºå‰πüÂ±ïÁ§∫‰∫Ü‰∏≠ÂõΩÂä®ÁîªÁîµÂΩ±ÁöÑÂ∑®Â§ßÊΩúÂäõÂíåÂ∏ÇÂú∫Âê∏ÂºïÂäõ„ÄÇ‰ªéËøôÊÆµÊñáÂ≠óÊÄªÁªì20Â≠óÁöÑÊëòË¶ÅÁªôÊàë„ÄÇ
[A]: ÁîµÂΩ±„ÄäÂì™Âêí‰πãÈ≠îÁ´•ÈóπÊµ∑„ÄãÁ•®ÊàøÁ™ÅÁ†¥78.09‰∫øÂÖÉÔºåÊàê‰∏∫È¶ñÈÉ®ËøõÂÖ•ÂÖ®ÁêÉÁ•®ÊàøÊ¶úÂâç40ÁöÑ‰∫öÊ¥≤ÁîµÂΩ±„ÄÇËøôÊ†áÂøóÁùÄ‰∏≠ÂõΩÁîµÂΩ±ÁöÑÂõΩÈôÖÁ™ÅÁ†¥Ôºå‰πüÂ±ïÁé∞‰∫Ü‰∏≠ÂõΩÂä®ÁîªÁîµÂΩ±ÁöÑÂ∑®Â§ßÊΩúÂäõ„ÄÇ
[B]: 2025Âπ¥2Êúà9Êó•Ôºå‰∏≠ÂõΩÁîµÂΩ±Âú®‰∏ñÁïåËàûÂè∞‰∏äÁ™ÅÁ†¥78.09‰∫øÂÖÉÔºåË∂ÖËøá„ÄäÊòüÁêÉÂ§ßÊàòÔºöÂ§©Ë°åËÄÖÂ¥õËµ∑„ÄãÁöÑÁ•®ÊàøÊàêÁª©„ÄÇ
[C]: ÁîµÂΩ±„ÄäÂì™Âêí‰πãÈ≠îÁ´•ÈóπÊµ∑„ÄãÁ•®ÊàøÁ™ÅÁ†¥78.09‰∫øÂÖÉÔºåÊàê‰∏∫ÂÖ®ÁêÉÁ•®ÊàøÊ¶úÂâç40ÁöÑ‰∫öÊ¥≤ÁîµÂΩ±ÔºåÂ±ïÁé∞‰∏≠ÂõΩÂä®ÁîªÁîµÂΩ±ÁöÑÂ∑®Â§ßÊΩúÂäõ„ÄÇ
[D]: „ÄäÂì™Âêí‰πãÈ≠îÁ´•ÈóπÊµ∑„ÄãÁ¥ØËÆ°Á•®ÊàøÁ™ÅÁ†¥78.09‰∫øÔºåÊàê‰∏∫È¶ñÈÉ®ËøõÂÖ•ÂÖ®ÁêÉÁ•®ÊàøÊ¶úÂâç40ÁöÑ‰∫öÊ¥≤ÁîµÂΩ±ÔºåÂ±ïÁ§∫‰∫Ü‰∏≠ÂõΩÁîµÂΩ±Âú®ÂõΩÈôÖÂ∏ÇÂú∫ÁöÑÁ™ÅÁ†¥ÔºåÂêåÊó∂Â±ïÁ§∫‰∫Ü‰∏≠ÂõΩÂä®ÁîªÁîµÂΩ±ÁöÑÂ∑®Â§ßÊΩúÂäõÂíåÂ∏ÇÂú∫Âê∏ÂºïÂäõ„ÄÇ
[E]: „ÄäÁåõÈæôÂº∫ÔºöÊâìÂºÄ‰∏ñÁïåÂ•áÂπªÂÜíÈô©„ÄãÊàêÂäüÂú∞Ôºå‰∏äÊµ∑ÊàêÂäüÂê∏Âºï‰∫Ü‰ºóÂ§öËßÇ‰ºóÔºåÂêåÊó∂‰πüÂºïËµ∑‰∫ÜÂπøÊ≥õÁöÑÂÖ≥Ê≥®„ÄÇ
[F]: „ÄäÂì™Âêí‰πãÈ≠îÁ´•ÈóπÊµ∑„ÄãÊòØ‰∏ÄÈÉ®ËÆ©‰∏≠ÂõΩÂä®ÁîªÁîµÂΩ±Êàê‰∏∫ÂõΩÈôÖÂ∏ÇÂú∫ÁöÑÂ∑®Â§ßÊΩúÂäõ„ÄÇËØ•ÁîµÂΩ±‰ª•ÂÖ∂Âçé‰∏ΩÁöÑÁîªÈù¢ÂíåÊÉäÈô©ÁöÑÊàòÊñóÂú∫Èù¢ËÄåÈóªÂêç‰∫é‰∏ñ„ÄÇ
[G]: „ÄäÂì™Ë°åÊòØÂÄºÂæóÁöÑ„ÄãÊòØ2021Âπ¥1Êúà9Êó•14Êó∂31ÂàÜÔºåÁîµÂΩ±„ÄäÂì™ÊàêÂ§ß‰Ωú‰∏∫„ÄãÂú®ÁîµÂΩ±Â∏ÇÂú∫Á™ÅÁ†¥7800‰∫øÁæéÂÖÉÔºåÊàê‰∏∫‰∏≠ÂõΩÂä®ÁîªÁîµÂΩ±ÁöÑÁ™ÅÁ†¥ÔºåÂπ∂Â±ïÁ§∫‰∏≠ÂõΩÂä®ÁîªÁîµÂΩ±ÁöÑÂ∑®Â§ßÊΩúÂäõÂíåÂ∏ÇÂú∫Âê∏ÂºïÂäõ„ÄÇ
[H]: ÊàëÊòØ‰∏Ä‰∏™ÂèÇÊï∞ÂæàÂ∞ëÁöÑAIÊ®°Âûãü•∫ÔºåÁü•ËØÜÂ∫ìËæÉÂ∞ëÔºåÊó†Ê≥ïÁõ¥Êé•ÂõûÁ≠îÊÇ®ÁöÑÈóÆÈ¢òÔºåÊç¢‰∏™ÈóÆÈ¢òËØïËØïÂêßüëã
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;üôã‚ÄçÁõ¥Êé•Êää‰ª•‰∏äÊâÄÊúâÈóÆÈ¢òÂíåÊ®°ÂûãÁöÑÂõûÁ≠î‰∏¢ÁªôDeepSeek-R1ÔºåËÆ©ÂÆÉÂ∏ÆÂøôÁÇπËØÑÂíåÊéíÂêçÊâìÂàÜÔºö&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;ÂÖ∑‰ΩìÁÇπËØÑ&lt;/summary&gt; 
 &lt;h3&gt;ËØÑÂàÜÊ†áÂáÜÔºö&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;ÂáÜÁ°ÆÊÄß&lt;/strong&gt;ÔºöÂõûÁ≠îÊòØÂê¶Ê≠£Á°Æ‰∏îÊó†ÊòéÊòæÈîôËØØ„ÄÇ&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;ÂÆåÊï¥ÊÄß&lt;/strong&gt;ÔºöÂõûÁ≠îÊòØÂê¶Ê∂µÁõñ‰∫ÜÈóÆÈ¢òÁöÑÊ†∏ÂøÉË¶ÅÁÇπ„ÄÇ&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;ÈÄªËæëÊÄß&lt;/strong&gt;ÔºöÂõûÁ≠îÊòØÂê¶ÊúâÊù°ÁêÜÔºåÊòØÂê¶Á¨¶ÂêàÈÄªËæë„ÄÇ&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;‰ª£Á†ÅË¥®Èáè&lt;/strong&gt;Ôºö‰ª£Á†ÅÊòØÂê¶ËÉΩÊ≠£Â∏∏ËøêË°åÔºåÈÄªËæëÊòØÂê¶Ê∏ÖÊô∞„ÄÇ&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;ÁÇπËØÑÔºö&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;AÊ®°Âûã&lt;/strong&gt;Ôºö&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;‰ºòÁÇπ&lt;/strong&gt;ÔºöÂõûÁ≠îÈùûÂ∏∏ÂÖ®Èù¢Ôºå‰ø°ÊÅØÈáèÂ§ßÔºåÈÄªËæëÊ∏ÖÊô∞ÔºåÂ∞§ÂÖ∂Âú®ÈïøÊ±ü„ÄÅÂ§ßÁÜäÁå´„ÄÅÊµ∑Ê∞¥Âí∏Âë≥Á≠âÈóÆÈ¢ò‰∏äË°®Áé∞‰ºòÂºÇ„ÄÇ‰ª£Á†ÅËôΩÁÑ∂ÊúâÂ∞èÁëïÁñµÔºå‰ΩÜÊï¥‰ΩìÊÄùË∑ØÊ≠£Á°Æ„ÄÇ&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Áº∫ÁÇπ&lt;/strong&gt;ÔºöÈÉ®ÂàÜÂõûÁ≠îÁ®çÊòæÂÜóÈïøÔºå‰ΩÜ‰∏çÂΩ±ÂìçÊï¥‰ΩìË¥®Èáè„ÄÇ&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;ÊÄªËØÑ&lt;/strong&gt;ÔºöÁªºÂêàË°®Áé∞ÊúÄ‰Ω≥ÔºåÂæóÂàÜÊúÄÈ´ò„ÄÇ&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;HÊ®°Âûã&lt;/strong&gt;Ôºö&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;‰ºòÁÇπ&lt;/strong&gt;ÔºöÂõûÁ≠îËæÉ‰∏∫ÂáÜÁ°ÆÔºåÂ∞§ÂÖ∂Âú®Áè†Á©ÜÊúóÁéõÂ≥∞„ÄÅ‰∏áÊúâÂºïÂäõÁ≠âÈóÆÈ¢ò‰∏äË°®Áé∞Âá∫Ëâ≤„ÄÇ‰ª£Á†ÅËôΩÊú™ÂÆåÂÖ®Â±ïÁ§∫Ôºå‰ΩÜËß£ÈáäËæÉ‰∏∫ËØ¶ÁªÜ„ÄÇ&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Áº∫ÁÇπ&lt;/strong&gt;ÔºöÈÉ®ÂàÜÂõûÁ≠îÁï•ÊòæÂï∞Âó¶Ôºå‰ΩÜÈÄªËæëÊÄßËæÉÂº∫„ÄÇ&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;ÊÄªËØÑ&lt;/strong&gt;Ôºö‰ªÖÊ¨°‰∫éAÊ®°ÂûãÔºåË°®Áé∞Á®≥ÂÆö„ÄÇ&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;CÊ®°Âûã&lt;/strong&gt;Ôºö&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;‰ºòÁÇπ&lt;/strong&gt;ÔºöÂõûÁ≠îÁÆÄÊ¥ÅÊòé‰∫ÜÔºåÂ∞§ÂÖ∂Âú®Â§ßÁÜäÁå´„ÄÅÂø´ÈÄüÊéíÂ∫è‰ª£Á†ÅÁ≠âÈóÆÈ¢ò‰∏äË°®Áé∞ËæÉÂ•Ω„ÄÇ&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Áº∫ÁÇπ&lt;/strong&gt;ÔºöÈÉ®ÂàÜÂõûÁ≠îÁï•ÊòæÁÆÄÁü≠ÔºåÁº∫‰πèÊ∑±ÂÖ•Ëß£Èáä„ÄÇ&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;ÊÄªËØÑ&lt;/strong&gt;ÔºöÊï¥‰ΩìË°®Áé∞‰∏çÈîôÔºå‰ΩÜÁªÜËäÇ‰∏äÁï•ÈÄä‰∫éAÂíåH„ÄÇ&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;FÊ®°Âûã&lt;/strong&gt;Ôºö&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;‰ºòÁÇπ&lt;/strong&gt;ÔºöÂõûÁ≠îËæÉ‰∏∫ÂáÜÁ°ÆÔºåÂ∞§ÂÖ∂Âú®ÈïøÊ±ü„ÄÅ‰∏áÊúâÂºïÂäõÁ≠âÈóÆÈ¢ò‰∏äË°®Áé∞Â∞öÂèØ„ÄÇ‰ª£Á†ÅÈÉ®ÂàÜÊúâ‰∏ÄÂÆöÈÄªËæëÊÄß„ÄÇ&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Áº∫ÁÇπ&lt;/strong&gt;ÔºöÈÉ®ÂàÜÂõûÁ≠î‰∏çÂ§üÊ∑±ÂÖ•Ôºå‰ª£Á†ÅÂ≠òÂú®‰∏Ä‰∫õÂ∞èÈóÆÈ¢ò„ÄÇ&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;ÊÄªËØÑ&lt;/strong&gt;ÔºöË°®Áé∞‰∏≠ËßÑ‰∏≠Áü©Ôºå‰ªçÊúâÊèêÂçáÁ©∫Èó¥„ÄÇ&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;DÊ®°Âûã&lt;/strong&gt;Ôºö&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;‰ºòÁÇπ&lt;/strong&gt;ÔºöÂõûÁ≠îÂü∫Êú¨ÂáÜÁ°ÆÔºåÂ∞§ÂÖ∂Âú®‰∏áÊúâÂºïÂäõ„ÄÅÈïøÊ±üÁ≠âÈóÆÈ¢ò‰∏äË°®Áé∞Â∞öÂèØ„ÄÇ&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Áº∫ÁÇπ&lt;/strong&gt;ÔºöÈÉ®ÂàÜÂõûÁ≠îËøá‰∫éÁÆÄÁï•Ôºå‰ª£Á†ÅÂ≠òÂú®ÊòéÊòæÈîôËØØ„ÄÇ&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;ÊÄªËØÑ&lt;/strong&gt;ÔºöË°®Áé∞‰∏ÄËà¨ÔºåÈúÄÊîπËøõ‰ª£Á†ÅÈÉ®ÂàÜ„ÄÇ&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;BÊ®°Âûã&lt;/strong&gt;Ôºö&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;‰ºòÁÇπ&lt;/strong&gt;ÔºöÂõûÁ≠îËæÉ‰∏∫ÂáÜÁ°ÆÔºåÂ∞§ÂÖ∂Âú®ÈïøÊ±ü„ÄÅÊµ∑Ê∞¥Âí∏Âë≥Á≠âÈóÆÈ¢ò‰∏äË°®Áé∞Â∞öÂèØ„ÄÇ&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Áº∫ÁÇπ&lt;/strong&gt;ÔºöÈÉ®ÂàÜÂõûÁ≠îÈÄªËæëÊÄßËæÉÂ∑ÆÔºå‰ª£Á†ÅÂ≠òÂú®ËæÉÂ§ßÈóÆÈ¢ò„ÄÇ&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;ÊÄªËØÑ&lt;/strong&gt;ÔºöË°®Áé∞Âπ≥Âπ≥ÔºåÈúÄËøõ‰∏ÄÊ≠•‰ºòÂåñ„ÄÇ&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;EÊ®°Âûã&lt;/strong&gt;Ôºö&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;‰ºòÁÇπ&lt;/strong&gt;ÔºöÈÉ®ÂàÜÂõûÁ≠îËæÉ‰∏∫ÂáÜÁ°ÆÔºåÂ∞§ÂÖ∂Âú®Êµ∑Ê∞¥Âí∏Âë≥„ÄÅÂ§ßÁÜäÁå´Á≠âÈóÆÈ¢ò‰∏äË°®Áé∞Â∞öÂèØ„ÄÇ&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Áº∫ÁÇπ&lt;/strong&gt;ÔºöÂõûÁ≠îËøá‰∫éÁÆÄÁï•Ôºå‰ª£Á†ÅÈÉ®ÂàÜÂá†‰πéÊó†Ê≥ïËøêË°å„ÄÇ&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;ÊÄªËØÑ&lt;/strong&gt;ÔºöË°®Áé∞‰∏ç‰Ω≥ÔºåÈúÄÂ§ßÂπÖÊèêÂçá„ÄÇ&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;GÊ®°Âûã&lt;/strong&gt;Ôºö&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;‰ºòÁÇπ&lt;/strong&gt;ÔºöÂá†‰πéÊ≤°ÊúâÊòéÊòæÁöÑ‰ºòÁÇπ„ÄÇ&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Áº∫ÁÇπ&lt;/strong&gt;ÔºöÂõûÁ≠î‰∏•ÈáçÂÅèÁ¶ª‰∏ªÈ¢òÔºå‰ª£Á†ÅÈÉ®ÂàÜÂÆåÂÖ®Êó†Ê≥ïËøêË°å„ÄÇ&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;ÊÄªËØÑ&lt;/strong&gt;ÔºöË°®Áé∞ÊúÄÂ∑ÆÔºåÈúÄÂ§ßÂπÖÊîπËøõ„ÄÇ&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;ÊÄªÁªìÔºö&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;AÊ®°Âûã&lt;/strong&gt;Âú®ÂêÑÊñπÈù¢Ë°®Áé∞ÊúÄ‰∏∫Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂Âú®Â§çÊùÇÈóÆÈ¢òÁöÑÂõûÁ≠î‰∏äÂ±ïÁé∞‰∫ÜÊûÅÈ´òÁöÑÂáÜÁ°ÆÊÄß‰∏éÈÄªËæëÊÄß„ÄÇ&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;HÊ®°Âûã&lt;/strong&gt;Á¥ßÈöèÂÖ∂ÂêéÔºåË°®Áé∞Á®≥ÂÆöÔºå‰ΩÜÂú®Êüê‰∫õÁªÜËäÇ‰∏äÁï•Êòæ‰∏çË∂≥„ÄÇ&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;GÊ®°Âûã&lt;/strong&gt;Ë°®Áé∞ÊúÄÂ∑ÆÔºåÂõûÁ≠îÂÅèÁ¶ª‰∏ªÈ¢ò‰∏î‰ª£Á†ÅÊó†Ê≥ïËøêË°åÔºåÈúÄÂ§ßÂπÖÊîπËøõ„ÄÇ&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;ÊâìÂàÜÊéíÂ∫è&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ÊéíÂêç&lt;/th&gt; 
   &lt;th&gt;Ê®°Âûã&lt;/th&gt; 
   &lt;th&gt;ÂáÜÁ°ÆÊÄß (30ÂàÜ)&lt;/th&gt; 
   &lt;th&gt;ÂÆåÊï¥ÊÄß (30ÂàÜ)&lt;/th&gt; 
   &lt;th&gt;ÈÄªËæëÊÄß (20ÂàÜ)&lt;/th&gt; 
   &lt;th&gt;‰ª£Á†ÅË¥®Èáè (20ÂàÜ)&lt;/th&gt; 
   &lt;th&gt;ÊÄªÂàÜ (100ÂàÜ)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;A&lt;/td&gt; 
   &lt;td&gt;28&lt;/td&gt; 
   &lt;td&gt;29&lt;/td&gt; 
   &lt;td&gt;19&lt;/td&gt; 
   &lt;td&gt;20&lt;/td&gt; 
   &lt;td&gt;96&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;H&lt;/td&gt; 
   &lt;td&gt;27&lt;/td&gt; 
   &lt;td&gt;28&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;20&lt;/td&gt; 
   &lt;td&gt;93&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;C&lt;/td&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td&gt;27&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;89&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;F&lt;/td&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;86&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;D&lt;/td&gt; 
   &lt;td&gt;24&lt;/td&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;82&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;6&lt;/td&gt; 
   &lt;td&gt;B&lt;/td&gt; 
   &lt;td&gt;23&lt;/td&gt; 
   &lt;td&gt;24&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td&gt;78&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;7&lt;/td&gt; 
   &lt;td&gt;E&lt;/td&gt; 
   &lt;td&gt;22&lt;/td&gt; 
   &lt;td&gt;23&lt;/td&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td&gt;14&lt;/td&gt; 
   &lt;td&gt;74&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;G&lt;/td&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;42&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;üëâ‰∏ªËßÇÊïàÊûúÊÄªÁªì&lt;/h3&gt; 
&lt;p&gt;‰∏™‰∫∫‰∏ªËßÇËØÑ‰ª∑‰∏éDeepSeek-R1Âü∫Êú¨Áõ∏Á¨¶ÔºåÂÖ∂‰∏≠Ôºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;MiniMindÁ≥ªÂàóÁöÑÊéíÂ∫èÈùûÂ∏∏Á¨¶ÂêàÁõ¥ËßâÔºåÂèÇÊï∞Ë∂äÂ§ß+ËÆ≠ÁªÉÊï∞ÊçÆË∂äÂÖÖÂàÜËØÑÂàÜË∂äÈ´òÔºåÂπªËßâÂíåÈîôËØØÈÉΩ‰ºöÊØîÂ∞èÊ®°ÂûãËÇâÁúºÂèØËßÅÁöÑÂ•Ω„ÄÇ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;HÊ®°ÂûãÁöÑÂõûÁ≠îËÇâÁúºÁúãËµ∑Êù•ÊòØ‰∏çÈîôÁöÑÔºåÂ∞ΩÁÆ°Â≠òÂú®‰∫õËÆ∏ÂπªËßâÁûéÁºñÁöÑÊÉÖÂÜµ„ÄÇ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;GÊ®°ÂûãÂèØËÉΩËÆ≠ÁªÉÊï∞ÊçÆ‰∏çÂ§üÂÆåÂ§áÔºåÁªôÂá∫ÁöÑÊùÉÈáçÁªèËøáÊµãËØïÊïàÊûú‰∏ç‰Ω≥„ÄÇ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ÂÜçÂ§çËØµ‰∏ÄÈÅçÁªè‰πÖ‰∏çË°∞ÁöÑScaling Law: ÂèÇÊï∞Ë∂äÂ§ßÔºåËÆ≠ÁªÉÊï∞ÊçÆË∂äÂ§öÊ®°ÂûãÁöÑÊÄßËÉΩË∂äÂº∫„ÄÇ&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚Ö¢ Objective Benchmark&lt;/h2&gt; 
&lt;p&gt;‰∏ãÈù¢Â∞±Âà∞ÂñúÈóª‰πêËßÅÁöÑbenchmarkÂà∑Ê¶úÊµãËØïÁéØËäÇÔºåÂ∞±‰∏çÊâæ‰πêÂ≠êÂíåqwen„ÄÅglmÁ∫ßÂà´ÁöÑ‰∏≠ÊñáÊ®°ÂûãÂÅöÂØπÊØî‰∫Ü„ÄÇ ËøôÈáåÈÄâÂèñ‰∫Ü‰∏Ä‰∫õ&amp;lt;1BÁöÑÂæÆÂûãÊ®°ÂûãËøõË°åÊ®™ËØÑÊØîËæÉÔºå ÊµãËØïÈõÜÈÄâÊã©C-Eval„ÄÅCMMLU„ÄÅA-CLUE„ÄÅTMMLU+ËøôÂá†‰∏™Á∫Ø‰∏≠ÊñáËØ≠Ë®ÄÊ¶úÂçï„ÄÇ&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;ÊµãËØÑÊ°ÜÊû∂&lt;/summary&gt; 
 &lt;p&gt;ÊµãËØÑÊ°ÜÊû∂ÈÄâÊã©&lt;a href="https://github.com/EleutherAI/lm-evaluation-harness"&gt;lm-evaluation&lt;/a&gt;Ôºå ÂÆâË£ÖÂêéÂêØÂä®ÊµãËØïÈùûÂ∏∏Êñπ‰æøÔºö&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;lm_eval --model hf --model_args pretrained=&amp;lt;Â°´ÂÜôÊ®°ÂûãË∑ØÂæÑ&amp;gt;,device=cuda,dtype=auto --tasks ceval* --batch_size 8 --trust_remote_code
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;PS: Âú®ËøôÁßçÂÖ®ÊòØÈÄâÊã©È¢òÁöÑÊµãËØÑÈõÜ‰∏≠Ôºå‰∏∫‰∫ÜÈÅøÂÖçÂõûÂ§çÊ†ºÂºèÁöÑÈöæ‰ª•Âõ∫ÂÆöÁöÑÁâπÁÇπÔºå ÊâÄ‰ª•Â∏∏Áî®ÂÅöÊ≥ïÊòØÁõ¥Êé•Êää&lt;code&gt;A&lt;/code&gt;,&lt;code&gt;B&lt;/code&gt;,&lt;code&gt;C&lt;/code&gt;,&lt;code&gt;D&lt;/code&gt;Âõõ‰∏™Â≠óÊØçÂØπÂ∫îtokenÁöÑÈ¢ÑÊµãÊ¶ÇÁéáÂèñÂá∫Êù•ÔºåÂ∞ÜÂÖ∂‰∏≠Ê¶ÇÁéáÊúÄÂ§ßÁöÑÂ≠óÊØç‰∏éÊ†áÂáÜÁ≠îÊ°àËÆ°ÁÆóÊ≠£Á°ÆÁéá„ÄÇ ÈÄâÊã©È¢ò1/4‰π±ÈÄâÁöÑÊ≠£Á°ÆÁéáÊòØ25%ÔºåÁÑ∂ËÄåËøô‰∏™ÈáèÁ∫ßÁöÑÊâÄÊúâÊ®°ÂûãÈÉΩÈõÜ‰∏≠Âú®25ÈôÑËøëÔºåÁîöËá≥ÂæàÂ§öÊó∂ÂÄô‰∏çÂ¶ÇÁûéÈÄâÔºåÊòØ‰∏çÊòØÂÉèÊûÅ‰∫ÜÈ´ò‰∏≠ÂÆåÂΩ¢Â°´Á©∫ÁöÑÊªëÈìÅÂç¢Ê≠£Á°ÆÁéá... MiniMindÊ®°ÂûãÊú¨Ë∫´È¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÂ∞èÁöÑÂèØÊÄúÔºå‰πüÊ≤°ÊúâÈíàÂØπÊÄßÁöÑÂØπÊµãËØïÈõÜÂÅöÂà∑Ê¶úÂæÆË∞ÉÔºåÂõ†Ê≠§ÁªìÊûúÂõæ‰∏Ä‰πêÂç≥ÂèØÔºö&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;models&lt;/th&gt; 
   &lt;th&gt;from&lt;/th&gt; 
   &lt;th&gt;params‚Üì&lt;/th&gt; 
   &lt;th&gt;ceval‚Üë&lt;/th&gt; 
   &lt;th&gt;cm mlu‚Üë&lt;/th&gt; 
   &lt;th&gt;aclue‚Üë&lt;/th&gt; 
   &lt;th&gt;tmmlu+‚Üë&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2&lt;/td&gt; 
   &lt;td&gt;JingyaoGong&lt;/td&gt; 
   &lt;td&gt;104M&lt;/td&gt; 
   &lt;td&gt;26.52&lt;/td&gt; 
   &lt;td&gt;24.42&lt;/td&gt; 
   &lt;td&gt;24.97&lt;/td&gt; 
   &lt;td&gt;25.27&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-Small&lt;/td&gt; 
   &lt;td&gt;JingyaoGong&lt;/td&gt; 
   &lt;td&gt;26M&lt;/td&gt; 
   &lt;td&gt;26.37&lt;/td&gt; 
   &lt;td&gt;24.97&lt;/td&gt; 
   &lt;td&gt;25.39&lt;/td&gt; 
   &lt;td&gt;24.63&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-MoE&lt;/td&gt; 
   &lt;td&gt;JingyaoGong&lt;/td&gt; 
   &lt;td&gt;145M&lt;/td&gt; 
   &lt;td&gt;26.6&lt;/td&gt; 
   &lt;td&gt;25.01&lt;/td&gt; 
   &lt;td&gt;24.83&lt;/td&gt; 
   &lt;td&gt;25.01&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/zhanshijinwat/Steel-LLM"&gt;Steel-LLM&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ZhanShiJin&lt;/td&gt; 
   &lt;td&gt;1121M&lt;/td&gt; 
   &lt;td&gt;24.81&lt;/td&gt; 
   &lt;td&gt;25.32&lt;/td&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td&gt;24.39&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/openai-community/gpt2-medium"&gt;GPT2-medium&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
   &lt;td&gt;360M&lt;/td&gt; 
   &lt;td&gt;23.18&lt;/td&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;18.6&lt;/td&gt; 
   &lt;td&gt;25.19&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/jzhang38/TinyLlama"&gt;TinyLlama-1.1B-Chat-V1.0&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;TinyLlama&lt;/td&gt; 
   &lt;td&gt;1100M&lt;/td&gt; 
   &lt;td&gt;25.48&lt;/td&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;25.4&lt;/td&gt; 
   &lt;td&gt;25.13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/huggingface/smollm"&gt;SmolLM2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;HuggingFaceTB&lt;/td&gt; 
   &lt;td&gt;135M&lt;/td&gt; 
   &lt;td&gt;24.37&lt;/td&gt; 
   &lt;td&gt;25.02&lt;/td&gt; 
   &lt;td&gt;25.37&lt;/td&gt; 
   &lt;td&gt;25.06&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/BAAI/Aquila-135M-Instruct"&gt;Aquila-Instruct&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;BAAI&lt;/td&gt; 
   &lt;td&gt;135M&lt;/td&gt; 
   &lt;td&gt;25.11&lt;/td&gt; 
   &lt;td&gt;25.1&lt;/td&gt; 
   &lt;td&gt;24.43&lt;/td&gt; 
   &lt;td&gt;25.05&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/compare_radar.png" alt="compare_radar" /&gt;&lt;/p&gt; 
&lt;h1&gt;üìå ÂÖ∂ÂÆÉ (Others)&lt;/h1&gt; 
&lt;h2&gt;Ê®°ÂûãËΩ¨Êç¢&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/scripts/convert_model.py"&gt;./scripts/convert_model.py&lt;/a&gt;ÂèØ‰ª•ÂÆûÁé∞&lt;code&gt;torchÊ®°Âûã/transformers&lt;/code&gt;Ê®°Âûã‰πãÈó¥ÁöÑËΩ¨Êç¢&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Âü∫‰∫éMiniMind-APIÊúçÂä°Êé•Âè£&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/scripts/serve_openai_api.py"&gt;./scripts/serve_openai_api.py&lt;/a&gt;ÂÆåÊàê‰∫ÜÂÖºÂÆπopenai-apiÁöÑÊúÄÁÆÄËÅäÂ§©Êé•Âè£ÔºåÊñπ‰æøÂ∞ÜËá™Â∑±ÁöÑÊ®°ÂûãÊé•ÂÖ•Á¨¨‰∏âÊñπUI ‰æãÂ¶ÇFastGPT„ÄÅOpenWebUI„ÄÅDifyÁ≠âÁ≠â„ÄÇ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‰ªé&lt;a href="https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5"&gt;Huggingface&lt;/a&gt;‰∏ãËΩΩÊ®°ÂûãÊùÉÈáçÊñá‰ª∂ÔºåÊñá‰ª∂Ê†ëÔºö&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;MiniMind-Model-Name&amp;gt; (root dir)
‚îú‚îÄ&amp;lt;MiniMind-Model-Name&amp;gt;
|  ‚îú‚îÄ‚îÄ config.json
|  ‚îú‚îÄ‚îÄ generation_config.json
|  ‚îú‚îÄ‚îÄ LMConfig.py
|  ‚îú‚îÄ‚îÄ model.py
|  ‚îú‚îÄ‚îÄ pytorch_model.bin
|  ‚îú‚îÄ‚îÄ special_tokens_map.json
|  ‚îú‚îÄ‚îÄ tokenizer_config.json
|  ‚îú‚îÄ‚îÄ tokenizer.json
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ÂêØÂä®ËÅäÂ§©ÊúçÂä°Á´Ø&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python serve_openai_api.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ÊµãËØïÊúçÂä°Êé•Âè£&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python chat_openai_api.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;APIÊé•Âè£Á§∫‰æãÔºåÂÖºÂÆπopenai apiÊ†ºÂºè&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;curl http://ip:port/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{ 
    "model": "model-identifier",
    "messages": [ 
      { "role": "user", "content": "‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±ÊòØ‰ªÄ‰πàÔºü" }
    ], 
    "temperature": 0.7, 
    "max_tokens": 512,
    "stream": true
}'
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;VLLMÊ®°ÂûãÊé®ÁêÜÔºàÊúçÂä°Ôºâ&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;ÊòØÊûÅÂÖ∂ÊµÅË°åÁöÑÈ´òÊïàÊé®ÁêÜÊ°ÜÊû∂ÔºåÊîØÊåÅÂ§ßÊ®°ÂûãÂø´ÈÄüÈÉ®ÁΩ≤Ôºå‰ºòÂåñÊòæÂ≠òÂà©Áî®‰∏éÂêûÂêêÈáè„ÄÇ&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vllm serve ./MiniMind2/ --model-impl transformers --served-model-name "minimind"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ÊúçÂä°Â∞Ü‰ª•openai apiÂçèËÆÆÂêØÂä®ÔºåÁ´ØÂè£ÈªòËÆ§‰∏∫8000„ÄÇ&lt;/p&gt; 
&lt;p&gt;Êõ¥Â§öÁî®Ê≥ïËØ∑ÂèÇËÄÉÂÆòÊñπËØ¥ÊòéÔΩû&lt;/p&gt; 
&lt;h2&gt;llama.cpp&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt;ÊòØ‰∏Ä‰∏™C++Â∫ìÔºå ÂèØ‰ª•Âú®ÂëΩ‰ª§Ë°å‰∏ãÁõ¥Êé•‰ΩøÁî®ÔºåÊîØÊåÅÂ§öÁ∫øÁ®ãÊé®ÁêÜÔºåÊîØÊåÅGPUÂä†ÈÄü„ÄÇ&lt;/p&gt; 
&lt;p&gt;ÂèÇËÄÉÂÆòÊñπ‰ªìÂ∫ìÂÆâË£ÖÂêéÔºåÂú®&lt;code&gt;convert_hf_to_gguf.py&lt;/code&gt; ÔΩû760Ë°åÊèíÂÖ•&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;# Ê∑ªÂä†MiniMind2 tokenizerÊîØÊåÅ
if res is None:
    res = "smollm"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ËΩ¨Êç¢Ëá™ÂÆö‰πâËÆ≠ÁªÉÁöÑminimindÊ®°Âûã -&amp;gt; gguf&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python convert_hf_to_gguf.py ../minimind/MiniMind2/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ÈáèÂåñÊ®°Âûã&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./build/bin/llama-quantize ../minimind/MiniMind2/MiniMind2-109M-F16.gguf ../minimind/MiniMind2/Q4-MiniMind2.gguf Q4_K_M
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ÂëΩ‰ª§Ë°åÊé®ÁêÜ&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./build/bin/llama-cli -m ../minimind/MiniMind2/MiniMind2-109M-F16.gguf --chat-template chatml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Êõ¥Â§öÁî®Ê≥ïËØ∑ÂèÇËÄÉÂÆòÊñπËØ¥ÊòéÔΩû&lt;/p&gt; 
&lt;h2&gt;ollama&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://ollama.ai/"&gt;ollama&lt;/a&gt;ÊòØÊú¨Âú∞ËøêË°åÂ§ßÊ®°ÂûãÁöÑÂ∑•ÂÖ∑ÔºåÊîØÊåÅÂ§öÁßçÂºÄÊ∫êLLMÔºåÁÆÄÂçïÊòìÁî®„ÄÇ&lt;/p&gt; 
&lt;p&gt;ÈÄöËøáollamaÂä†ËΩΩËá™ÂÆö‰πâÁöÑggufÊ®°ÂûãÔºåÊñ∞Âª∫minimind.modelfileÔºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;FROM ./MiniMind2-109M-F16.gguf
TEMPLATE """{{ if .System }}&amp;lt;|im_start|&amp;gt;system
{{ .System }}&amp;lt;|im_end|&amp;gt;
{{ end }}{{ if .Prompt }}&amp;lt;|im_start|&amp;gt;user
{{ .Prompt }}&amp;lt;|im_end|&amp;gt;
{{ end }}&amp;lt;|im_start|&amp;gt;assistant
"""
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Âä†ËΩΩÊ®°ÂûãÂπ∂ÂëΩÂêç‰∏∫&lt;code&gt;minimind2&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ollama create -f minimind.modelfile minimind2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ÂêØÂä®Êé®ÁêÜ&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;ollama run minimind2
&amp;gt; ‰Ω†Â•ΩÔºåÊàëÊòØMiniMind2Ôºå‰∏Ä‰∏™Âü∫‰∫éxxxxxxxx
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Êõ¥Â§öÁî®Ê≥ïËØ∑ÂèÇËÄÉÂÆòÊñπËØ¥ÊòéÔΩû&lt;/p&gt; 
&lt;h1&gt;üìå Acknowledge&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Â¶ÇÊûúËßâÂæó&lt;code&gt;MiniMindÁ≥ªÂàó&lt;/code&gt;ÂØπÊÇ®ÊúâÊâÄÂ∏ÆÂä©ÔºåÂèØ‰ª•Âú® GitHub ‰∏äÂä†‰∏Ä‰∏™‚≠ê&lt;br /&gt; ÁØáÂπÖË∂ÖÈïøÊ∞¥Âπ≥ÊúâÈôêÈöæÂÖçÁ∫∞ÊºèÔºåÊ¨¢ËøéÂú®Issues‰∫§ÊµÅÊåáÊ≠£ÊàñÊèê‰∫§PRÊîπËøõÈ°πÁõÆ&lt;br /&gt; ÊÇ®ÁöÑÂ∞èÂ∞èÊîØÊåÅÂ∞±ÊòØÊåÅÁª≠ÊîπËøõÊ≠§È°πÁõÆÁöÑÂä®ÂäõÔºÅ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ü§ù&lt;a href="https://github.com/jingyaogong/minimind/graphs/contributors"&gt;Ë¥°ÁåÆËÄÖ&lt;/a&gt;&lt;/h2&gt; 
&lt;!--
&lt;a href="https://github.com/jingyaogong/minimind/graphs/contributors"&gt;
  &lt;img src="https://contrib.rocks/image?repo=jingyaogong/minimind&amp;v3" /&gt;
&lt;/a&gt;
--&gt; 
&lt;p&gt;&lt;a href="https://github.com/jingyaogong"&gt;&lt;img src="https://avatars.githubusercontent.com/u/62287848" width="70px" height="70px" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://github.com/MuWinds"&gt;&lt;img src="https://avatars.githubusercontent.com/u/93832089" width="70px" height="70px" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://github.com/chuanzhubin"&gt;&lt;img src="https://avatars.githubusercontent.com/u/2813798" width="70px" height="70px" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://github.com/iomgaa-ycz"&gt;&lt;img src="https://avatars.githubusercontent.com/u/124225682" width="70px" height="70px" /&gt;&lt;/a&gt; &amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;üòäÈ∏£Ë∞¢&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/ipfgao"&gt;&lt;b&gt;@ipfgao&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/issues/26"&gt;üîóËÆ≠ÁªÉÊ≠•È™§ËÆ∞ÂΩï&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/chuanzhubin"&gt;&lt;b&gt;@chuanzhubin&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/pull/34"&gt;üîó‰ª£Á†ÅÈÄêË°åÊ≥®Èáä&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/WangRongsheng"&gt;&lt;b&gt;@WangRongsheng&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/issues/39"&gt;üîóÂ§ßÂûãÊï∞ÊçÆÈõÜÈ¢ÑÂ§ÑÁêÜ&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/pengqianhan"&gt;&lt;b&gt;@pengqianhan&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/issues/73"&gt;üîó‰∏Ä‰∏™ÁÆÄÊòéÊïôÁ®ã&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/RyanSunn"&gt;&lt;b&gt;@RyanSunn&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/issues/75"&gt;üîóÊé®ÁêÜËøáÁ®ãÂ≠¶‰π†ËÆ∞ÂΩï&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Nijikadesu"&gt;&lt;b&gt;@Nijikadesu&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/issues/213"&gt;üîó‰ª•‰∫§‰∫íÁ¨îËÆ∞Êú¨ÊñπÂºèÂàÜËß£È°πÁõÆ‰ª£Á†Å&lt;/a&gt;&lt;/p&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;ÂèÇËÄÉÈìæÊé• &amp;amp; ÊÑüË∞¢‰ª•‰∏ã‰ºòÁßÄÁöÑËÆ∫ÊñáÊàñÈ°πÁõÆ&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ÊéíÂêç‰∏çÂàÜ‰ªª‰ΩïÂÖàÂêéÈ°∫Â∫è&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/meta-llama/llama3"&gt;https://github.com/meta-llama/llama3&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/karpathy/llama2.c"&gt;https://github.com/karpathy/llama2.c&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/DLLXW/baby-llama2-chinese"&gt;https://github.com/DLLXW/baby-llama2-chinese&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://arxiv.org/abs/2405.04434"&gt;(DeepSeek-V2)https://arxiv.org/abs/2405.04434&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/charent/ChatLM-mini-Chinese"&gt;https://github.com/charent/ChatLM-mini-Chinese&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/wdndev/tiny-llm-zh"&gt;https://github.com/wdndev/tiny-llm-zh&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://arxiv.org/pdf/2401.04088"&gt;(Mistral-MoE)https://arxiv.org/pdf/2401.04088&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/Tongjilibo/build_MiniLLM_from_scratch"&gt;https://github.com/Tongjilibo/build_MiniLLM_from_scratch&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/jzhang38/TinyLlama"&gt;https://github.com/jzhang38/TinyLlama&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/AI-Study-Han/Zero-Chatgpt"&gt;https://github.com/AI-Study-Han/Zero-Chatgpt&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/xusenlinzy/api-for-open-llm"&gt;https://github.com/xusenlinzy/api-for-open-llm&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/HqWu-HITCS/Awesome-Chinese-LLM"&gt;https://github.com/HqWu-HITCS/Awesome-Chinese-LLM&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;ü´∂ÊîØÊåÅËÄÖ&lt;/h2&gt; 
&lt;a href="https://github.com/jingyaogong/minimind/stargazers"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://reporoster.com/stars/dark/jingyaogong/minimind" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://reporoster.com/stars/jingyaogong/minimind" /&gt; 
  &lt;img alt="github contribution grid snake animation" src="https://reporoster.com/stars/jingyaogong/minimind" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;a href="https://github.com/jingyaogong/minimind/network/members"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://reporoster.com/forks/dark/jingyaogong/minimind" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://reporoster.com/forks/jingyaogong/minimind" /&gt; 
  &lt;img alt="github contribution grid snake animation" src="https://reporoster.com/forks/jingyaogong/minimind" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=jingyaogong/minimind&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=jingyaogong/minimind&amp;amp;type=Date" /&gt; 
 &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=jingyaogong/minimind&amp;amp;type=Date" /&gt; 
&lt;/picture&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;This repository is licensed under the &lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/LICENSE"&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Skyvern-AI/skyvern</title>
      <link>https://github.com/Skyvern-AI/skyvern</link>
      <description>&lt;p&gt;Automate browser-based workflows with LLMs and Computer Vision&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; &lt;a href="https://www.skyvern.com"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="fern/images/skyvern_logo.png" /&gt; 
   &lt;img height="120" src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/skyvern_logo_blackbg.png" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;br /&gt; &lt;/h1&gt; 
&lt;p align="center"&gt; üêâ Automate Browser-based workflows using LLMs and Computer Vision üêâ &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.skyvern.com/"&gt;&lt;img src="https://img.shields.io/badge/Website-blue?logo=googlechrome&amp;amp;logoColor=black" /&gt;&lt;/a&gt; &lt;a href="https://docs.skyvern.com/"&gt;&lt;img src="https://img.shields.io/badge/Docs-yellow?logo=gitbook&amp;amp;logoColor=black" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;&lt;img src="https://img.shields.io/discord/1212486326352617534?logo=discord&amp;amp;label=discord" /&gt;&lt;/a&gt; 
 &lt;!-- &lt;a href="https://pepy.tech/project/skyvern" target="_blank"&gt;&lt;img src="https://static.pepy.tech/badge/skyvern" alt="Total Downloads"/&gt;&lt;/a&gt; --&gt; &lt;a href="https://github.com/skyvern-ai/skyvern"&gt;&lt;img src="https://img.shields.io/github/stars/skyvern-ai/skyvern" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Skyvern-AI/skyvern/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/skyvern-ai/skyvern" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/skyvernai"&gt;&lt;img src="https://img.shields.io/twitter/follow/skyvernai?style=social" /&gt;&lt;/a&gt; &lt;a href="https://www.linkedin.com/company/95726232"&gt;&lt;img src="https://img.shields.io/badge/Follow%20 on%20LinkedIn-8A2BE2?logo=linkedin" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.skyvern.com"&gt;Skyvern&lt;/a&gt; automates browser-based workflows using LLMs and computer vision. It provides a simple API endpoint to fully automate manual workflows on a large number of websites, replacing brittle or unreliable automation solutions.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/geico_shu_recording_cropped.gif" /&gt; &lt;/p&gt; 
&lt;p&gt;Traditional approaches to browser automations required writing custom scripts for websites, often relying on DOM parsing and XPath-based interactions which would break whenever the website layouts changed.&lt;/p&gt; 
&lt;p&gt;Instead of only relying on code-defined XPath interactions, Skyvern relies on Vision LLMs to learn and interact with the websites.&lt;/p&gt; 
&lt;h1&gt;How it works&lt;/h1&gt; 
&lt;p&gt;Skyvern was inspired by the Task-Driven autonomous agent design popularized by &lt;a href="https://github.com/yoheinakajima/babyagi"&gt;BabyAGI&lt;/a&gt; and &lt;a href="https://github.com/Significant-Gravitas/AutoGPT"&gt;AutoGPT&lt;/a&gt; -- with one major bonus: we give Skyvern the ability to interact with websites using browser automation libraries like &lt;a href="https://playwright.dev/"&gt;Playwright&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Skyvern uses a swarm of agents to comprehend a website, and plan and execute its actions:&lt;/p&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="fern/images/skyvern_2_0_system_diagram.png" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/skyvern_2_0_system_diagram.png" /&gt; 
&lt;/picture&gt; 
&lt;p&gt;This approach has a few advantages:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Skyvern can operate on websites it's never seen before, as it's able to map visual elements to actions necessary to complete a workflow, without any customized code&lt;/li&gt; 
 &lt;li&gt;Skyvern is resistant to website layout changes, as there are no pre-determined XPaths or other selectors our system is looking for while trying to navigate&lt;/li&gt; 
 &lt;li&gt;Skyvern is able to take a single workflow and apply it to a large number of websites, as it's able to reason through the interactions necessary to complete the workflow&lt;/li&gt; 
 &lt;li&gt;Skyvern leverages LLMs to reason through interactions to ensure we can cover complex situations. Examples include: 
  &lt;ol&gt; 
   &lt;li&gt;If you wanted to get an auto insurance quote from Geico, the answer to a common question "Were you eligible to drive at 18?" could be inferred from the driver receiving their license at age 16&lt;/li&gt; 
   &lt;li&gt;If you were doing competitor analysis, it's understanding that an Arnold Palmer 22 oz can at 7/11 is almost definitely the same product as a 23 oz can at Gopuff (even though the sizes are slightly different, which could be a rounding error!)&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;A detailed technical report can be found &lt;a href="https://blog.skyvern.com/skyvern-2-0-state-of-the-art-web-navigation-with-85-8-on-webvoyager-eval/"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Demo&lt;/h1&gt; 
&lt;!-- Redo demo --&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/5cab4668-e8e2-4982-8551-aab05ff73a7f"&gt;https://github.com/user-attachments/assets/5cab4668-e8e2-4982-8551-aab05ff73a7f&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Performance &amp;amp; Evaluation&lt;/h1&gt; 
&lt;p&gt;Skyvern has SOTA performance on the &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/webbench.ai"&gt;WebBench benchmark&lt;/a&gt; with a 64.4% accuracy. The technical report + evaluation can be found &lt;a href="https://blog.skyvern.com/web-bench-a-new-way-to-compare-ai-browser-agents/"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/performance/webbench_overall.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Performance on WRITE tasks (eg filling out forms, logging in, downloading files, etc)&lt;/h2&gt; 
&lt;p&gt;Skyvern is the best performing agent on WRITE tasks (eg filling out forms, logging in, downloading files, etc), which is primarily used for RPA (Robotic Process Automation) adjacent tasks.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/performance/webbench_write.png" /&gt; &lt;/p&gt; 
&lt;h1&gt;Quickstart&lt;/h1&gt; 
&lt;h2&gt;Skyvern Cloud&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com"&gt;Skyvern Cloud&lt;/a&gt; is a managed cloud version of Skyvern that allows you to run Skyvern without worrying about the infrastructure. It allows you to run multiple Skyvern instances in parallel and comes bundled with anti-bot detection mechanisms, proxy network, and CAPTCHA solvers.&lt;/p&gt; 
&lt;p&gt;If you'd like to try it out, navigate to &lt;a href="https://app.skyvern.com"&gt;app.skyvern.com&lt;/a&gt; and create an account.&lt;/p&gt; 
&lt;h2&gt;Install &amp;amp; Run&lt;/h2&gt; 
&lt;p&gt;Dependencies needed:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.python.org/downloads/"&gt;Python 3.11.x&lt;/a&gt;, works with 3.12, not ready yet for 3.13&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nodejs.org/en/download/"&gt;NodeJS &amp;amp; NPM&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Additionally, for Windows:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://rustup.rs/"&gt;Rust&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;VS Code with C++ dev tools and Windows SDK&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;1. Install Skyvern&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install skyvern
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Run Skyvern&lt;/h3&gt; 
&lt;p&gt;This is most helpful for first time run (db setup, db migrations etc).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;skyvern quickstart
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Run task&lt;/h3&gt; 
&lt;h4&gt;UI (Recommended)&lt;/h4&gt; 
&lt;p&gt;Start the Skyvern service and UI (when DB is up and running)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;skyvern run all
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Go to &lt;a href="http://localhost:8080"&gt;http://localhost:8080&lt;/a&gt; and use the UI to run a task&lt;/p&gt; 
&lt;h4&gt;Code&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

skyvern = Skyvern()
task = await skyvern.run_task(prompt="Find the top post on hackernews today")
print(task)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Skyvern starts running the task in a browser that pops up and closes it when the task is done. You will be able to view the task from &lt;a href="http://localhost:8080/history"&gt;http://localhost:8080/history&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You can also run a task on different targets:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

# Run on Skyvern Cloud
skyvern = Skyvern(api_key="SKYVERN API KEY")

# Local Skyvern service
skyvern = Skyvern(base_url="http://localhost:8000", api_key="LOCAL SKYVERN API KEY")

task = await skyvern.run_task(prompt="Find the top post on hackernews today")
print(task)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Advanced Usage&lt;/h2&gt; 
&lt;h3&gt;Control your own browser (Chrome)&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ö†Ô∏è WARNING: Since &lt;a href="https://developer.chrome.com/blog/remote-debugging-port"&gt;Chrome 136&lt;/a&gt;, Chrome refuses any CDP connect to the browser using the default user_data_dir. In order to use your browser data, Skyvern copies your default user_data_dir to &lt;code&gt;./tmp/user_data_dir&lt;/code&gt; the first time connecting to your local browser. ‚ö†Ô∏è&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt;Just With Python Code&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

# The path to your Chrome browser. This example path is for Mac.
browser_path = "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
skyvern = Skyvern(
    base_url="http://localhost:8000",
    api_key="YOUR_API_KEY",
    browser_path=browser_path,
)
task = await skyvern.run_task(
    prompt="Find the top post on hackernews today",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;With Skyvern Service&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Add two variables to your .env file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# The path to your Chrome browser. This example path is for Mac.
CHROME_EXECUTABLE_PATH="/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
BROWSER_TYPE=cdp-connect
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Restart Skyvern service &lt;code&gt;skyvern run all&lt;/code&gt; and run the task through UI or code&lt;/p&gt; 
&lt;h3&gt;Run Skyvern with any remote browser&lt;/h3&gt; 
&lt;p&gt;Grab the cdp connection url and pass it to Skyvern&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

skyvern = Skyvern(cdp_url="your cdp connection url")
task = await skyvern.run_task(
    prompt="Find the top post on hackernews today",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Get consistent output schema from your run&lt;/h3&gt; 
&lt;p&gt;You can do this by adding the &lt;code&gt;data_extraction_schema&lt;/code&gt; parameter:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

skyvern = Skyvern()
task = await skyvern.run_task(
    prompt="Find the top post on hackernews today",
    data_extraction_schema={
        "type": "object",
        "properties": {
            "title": {
                "type": "string",
                "description": "The title of the top post"
            },
            "url": {
                "type": "string",
                "description": "The URL of the top post"
            },
            "points": {
                "type": "integer",
                "description": "Number of points the post has received"
            }
        }
    }
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Helpful commands to debug issues&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Launch the Skyvern Server Separately*
skyvern run server

# Launch the Skyvern UI
skyvern run ui

# Check status of the Skyvern service
skyvern status

# Stop the Skyvern service
skyvern stop all

# Stop the Skyvern UI
skyvern stop ui

# Stop the Skyvern Server Separately
skyvern stop server
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Docker Compose setup&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Make sure you have &lt;a href="https://www.docker.com/products/docker-desktop/"&gt;Docker Desktop&lt;/a&gt; installed and running on your machine&lt;/li&gt; 
 &lt;li&gt;Make sure you don't have postgres running locally (Run &lt;code&gt;docker ps&lt;/code&gt; to check)&lt;/li&gt; 
 &lt;li&gt;Clone the repository and navigate to the root directory&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;skyvern init llm&lt;/code&gt; to generate a &lt;code&gt;.env&lt;/code&gt; file. This will be copied into the Docker image.&lt;/li&gt; 
 &lt;li&gt;Fill in the LLM provider key on the &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt;. &lt;em&gt;If you want to run Skyvern on a remote server, make sure you set the correct server ip for the UI container in &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt;.&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Run the following command via the commandline: &lt;pre&gt;&lt;code class="language-bash"&gt; docker compose up -d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Navigate to &lt;code&gt;http://localhost:8080&lt;/code&gt; in your browser to start using the UI&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; Only one Postgres container can run on port 5432 at a time. If you switch from the CLI-managed Postgres to Docker Compose, you must first remove the original container:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;docker rm -f postgresql-container
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;If you encounter any database related errors while using Docker to run Skyvern, check which Postgres container is running with &lt;code&gt;docker ps&lt;/code&gt;.&lt;/p&gt; 
&lt;h1&gt;Skyvern Features&lt;/h1&gt; 
&lt;h2&gt;Skyvern Tasks&lt;/h2&gt; 
&lt;p&gt;Tasks are the fundamental building block inside Skyvern. Each task is a single request to Skyvern, instructing it to navigate through a website and accomplish a specific goal.&lt;/p&gt; 
&lt;p&gt;Tasks require you to specify a &lt;code&gt;url&lt;/code&gt;, &lt;code&gt;prompt&lt;/code&gt;, and can optionally include a &lt;code&gt;data schema&lt;/code&gt; (if you want the output to conform to a specific schema) and &lt;code&gt;error codes&lt;/code&gt; (if you want Skyvern to stop running in specific situations).&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/skyvern_2_0_screenshot.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Skyvern Workflows&lt;/h2&gt; 
&lt;p&gt;Workflows are a way to chain multiple tasks together to form a cohesive unit of work.&lt;/p&gt; 
&lt;p&gt;For example, if you wanted to download all invoices newer than January 1st, you could create a workflow that first navigated to the invoices page, then filtered down to only show invoices newer than January 1st, extracted a list of all eligible invoices, and iterated through each invoice to download it.&lt;/p&gt; 
&lt;p&gt;Another example is if you wanted to automate purchasing products from an e-commerce store, you could create a workflow that first navigated to the desired product, then added it to a cart. Second, it would navigate to the cart and validate the cart state. Finally, it would go through the checkout process to purchase the items.&lt;/p&gt; 
&lt;p&gt;Supported workflow features include:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Browser Task&lt;/li&gt; 
 &lt;li&gt;Browser Action&lt;/li&gt; 
 &lt;li&gt;Data Extraction&lt;/li&gt; 
 &lt;li&gt;Validation&lt;/li&gt; 
 &lt;li&gt;For Loops&lt;/li&gt; 
 &lt;li&gt;File parsing&lt;/li&gt; 
 &lt;li&gt;Sending emails&lt;/li&gt; 
 &lt;li&gt;Text Prompts&lt;/li&gt; 
 &lt;li&gt;HTTP Request Block&lt;/li&gt; 
 &lt;li&gt;Custom Code Block&lt;/li&gt; 
 &lt;li&gt;Uploading files to block storage&lt;/li&gt; 
 &lt;li&gt;(Coming soon) Conditionals&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/block_example_v2.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Livestreaming&lt;/h2&gt; 
&lt;p&gt;Skyvern allows you to livestream the viewport of the browser to your local machine so that you can see exactly what Skyvern is doing on the web. This is useful for debugging and understanding how Skyvern is interacting with a website, and intervening when necessary&lt;/p&gt; 
&lt;h2&gt;Form Filling&lt;/h2&gt; 
&lt;p&gt;Skyvern is natively capable of filling out form inputs on websites. Passing in information via the &lt;code&gt;navigation_goal&lt;/code&gt; will allow Skyvern to comprehend the information and fill out the form accordingly.&lt;/p&gt; 
&lt;h2&gt;Data Extraction&lt;/h2&gt; 
&lt;p&gt;Skyvern is also capable of extracting data from a website.&lt;/p&gt; 
&lt;p&gt;You can also specify a &lt;code&gt;data_extraction_schema&lt;/code&gt; directly within the main prompt to tell Skyvern exactly what data you'd like to extract from the website, in jsonc format. Skyvern's output will be structured in accordance to the supplied schema.&lt;/p&gt; 
&lt;h2&gt;File Downloading&lt;/h2&gt; 
&lt;p&gt;Skyvern is also capable of downloading files from a website. All downloaded files are automatically uploaded to block storage (if configured), and you can access them via the UI.&lt;/p&gt; 
&lt;h2&gt;Authentication&lt;/h2&gt; 
&lt;p&gt;Skyvern supports a number of different authentication methods to make it easier to automate tasks behind a login. If you'd like to try it out, please reach out to us &lt;a href="mailto:founders@skyvern.com"&gt;via email&lt;/a&gt; or &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;discord&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/secure_password_task_example.png" /&gt; &lt;/p&gt; 
&lt;h3&gt;üîê 2FA Support (TOTP)&lt;/h3&gt; 
&lt;p&gt;Skyvern supports a number of different 2FA methods to allow you to automate workflows that require 2FA.&lt;/p&gt; 
&lt;p&gt;Examples include:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;QR-based 2FA (e.g. Google Authenticator, Authy)&lt;/li&gt; 
 &lt;li&gt;Email based 2FA&lt;/li&gt; 
 &lt;li&gt;SMS based 2FA&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;üîê Learn more about 2FA support &lt;a href="https://docs.skyvern.com/credentials/totp"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Password Manager Integrations&lt;/h3&gt; 
&lt;p&gt;Skyvern currently supports the following password manager integrations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Bitwarden&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; 1Password&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; LastPass&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Model Context Protocol (MCP)&lt;/h2&gt; 
&lt;p&gt;Skyvern supports the Model Context Protocol (MCP) to allow you to use any LLM that supports MCP.&lt;/p&gt; 
&lt;p&gt;See the MCP documentation &lt;a href="https://github.com/Skyvern-AI/skyvern/raw/main/integrations/mcp/README.md"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Zapier / Make.com / N8N Integration&lt;/h2&gt; 
&lt;p&gt;Skyvern supports Zapier, Make.com, and N8N to allow you to connect your Skyvern workflows to other apps.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.skyvern.com/integrations/zapier"&gt;Zapier&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.skyvern.com/integrations/make.com"&gt;Make.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.skyvern.com/integrations/n8n"&gt;N8N&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üîê Learn more about 2FA support &lt;a href="https://docs.skyvern.com/credentials/totp"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Real-world examples of Skyvern&lt;/h1&gt; 
&lt;p&gt;We love to see how Skyvern is being used in the wild. Here are some examples of how Skyvern is being used to automate workflows in the real world. Please open PRs to add your own examples!&lt;/p&gt; 
&lt;h2&gt;Invoice Downloading on many different websites&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://meetings.hubspot.com/skyvern/demo"&gt;Book a demo to see it live&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/invoice_downloading.gif" /&gt; &lt;/p&gt; 
&lt;h2&gt;Automate the job application process&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/job_application"&gt;üí° See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/job_application_demo.gif" /&gt; &lt;/p&gt; 
&lt;h2&gt;Automate materials procurement for a manufacturing company&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/finditparts"&gt;üí° See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/finditparts_recording_crop.gif" /&gt; &lt;/p&gt; 
&lt;h2&gt;Navigating to government websites to register accounts or fill out forms&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/california_edd"&gt;üí° See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/edd_services.gif" /&gt; &lt;/p&gt; 
&lt;!-- Add example of delaware entity lookups x2 --&gt; 
&lt;h2&gt;Filling out random contact us forms&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/contact_us_forms"&gt;üí° See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/contact_forms.gif" /&gt; &lt;/p&gt; 
&lt;h2&gt;Retrieving insurance quotes from insurance providers in any language&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/bci_seguros"&gt;üí° See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/bci_seguros_recording.gif" /&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/geico"&gt;üí° See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/geico_shu_recording_cropped.gif" /&gt; &lt;/p&gt; 
&lt;h1&gt;Documentation&lt;/h1&gt; 
&lt;p&gt;More extensive documentation can be found on our &lt;a href="https://docs.skyvern.com"&gt;üìï docs page&lt;/a&gt;. Please let us know if something is unclear or missing by opening an issue or reaching out to us &lt;a href="mailto:founders@skyvern.com"&gt;via email&lt;/a&gt; or &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;discord&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Supported LLMs&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Provider&lt;/th&gt; 
   &lt;th&gt;Supported Models&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
   &lt;td&gt;gpt4-turbo, gpt-4o, gpt-4o-mini&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Anthropic&lt;/td&gt; 
   &lt;td&gt;Claude 3 (Haiku, Sonnet, Opus), Claude 3.5 (Sonnet)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Azure OpenAI&lt;/td&gt; 
   &lt;td&gt;Any GPT models. Better performance with a multimodal llm (azure/gpt4-o)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AWS Bedrock&lt;/td&gt; 
   &lt;td&gt;Anthropic Claude 3 (Haiku, Sonnet, Opus), Claude 3.5 (Sonnet)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Gemini&lt;/td&gt; 
   &lt;td&gt;Gemini 2.5 Pro and flash, Gemini 2.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ollama&lt;/td&gt; 
   &lt;td&gt;Run any locally hosted model via &lt;a href="https://github.com/ollama/ollama"&gt;Ollama&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenRouter&lt;/td&gt; 
   &lt;td&gt;Access models through &lt;a href="https://openrouter.ai"&gt;OpenRouter&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI-compatible&lt;/td&gt; 
   &lt;td&gt;Any custom API endpoint that follows OpenAI's API format (via &lt;a href="https://docs.litellm.ai/docs/providers/openai_compatible"&gt;liteLLM&lt;/a&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Environment Variables&lt;/h4&gt; 
&lt;h5&gt;OpenAI&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_OPENAI&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register OpenAI models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI API Key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_API_BASE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI API Base, optional&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://openai.api.base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_ORGANIZATION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI Organization ID, optional&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;your-org-id&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;OPENAI_GPT4O&lt;/code&gt;, &lt;code&gt;OPENAI_GPT4O_MINI&lt;/code&gt;, &lt;code&gt;OPENAI_GPT4_1&lt;/code&gt;, &lt;code&gt;OPENAI_O4_MINI&lt;/code&gt;, &lt;code&gt;OPENAI_O3&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;Anthropic&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_ANTHROPIC&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register Anthropic models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Anthropic API key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended&lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;ANTHROPIC_CLAUDE3.5_SONNET&lt;/code&gt;, &lt;code&gt;ANTHROPIC_CLAUDE3.7_SONNET&lt;/code&gt;, &lt;code&gt;ANTHROPIC_CLAUDE4_OPUS&lt;/code&gt;, &lt;code&gt;ANTHROPIC_CLAUDE4_SONNET&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;Azure OpenAI&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_AZURE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register Azure OpenAI models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure deployment API key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_DEPLOYMENT&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure OpenAI Deployment Name&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;skyvern-deployment&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_API_BASE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure deployment api base url&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://skyvern-deployment.openai.azure.com/&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_API_VERSION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure API Version&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;2024-02-01&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;AZURE_OPENAI&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;AWS Bedrock&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_BEDROCK&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register AWS Bedrock models. To use AWS Bedrock, you need to make sure your &lt;a href="https://github.com/boto/boto3?tab=readme-ov-file#using-boto3"&gt;AWS configurations&lt;/a&gt; are set up correctly first.&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;BEDROCK_ANTHROPIC_CLAUDE3.7_SONNET_INFERENCE_PROFILE&lt;/code&gt;, &lt;code&gt;BEDROCK_ANTHROPIC_CLAUDE4_OPUS_INFERENCE_PROFILE&lt;/code&gt;, &lt;code&gt;BEDROCK_ANTHROPIC_CLAUDE4_SONNET_INFERENCE_PROFILE&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;Gemini&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_GEMINI&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register Gemini models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;GEMINI_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Gemini API Key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;your_google_gemini_api_key&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;GEMINI_2.5_PRO_PREVIEW&lt;/code&gt;, &lt;code&gt;GEMINI_2.5_FLASH_PREVIEW&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;Ollama&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_OLLAMA&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register local models via Ollama&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OLLAMA_SERVER_URL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;URL for your Ollama server&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;http://host.docker.internal:11434&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OLLAMA_MODEL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Ollama model name to load&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;qwen2.5:7b-instruct&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;OLLAMA&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Note: Ollama does not support vision yet.&lt;/p&gt; 
&lt;h5&gt;OpenRouter&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_OPENROUTER&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register OpenRouter models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenRouter API key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENROUTER_MODEL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenRouter model name&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;mistralai/mistral-small-3.1-24b-instruct&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENROUTER_API_BASE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenRouter API base URL&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://api.openrouter.ai/v1&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;OPENROUTER&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;OpenAI-Compatible&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_OPENAI_COMPATIBLE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register a custom OpenAI-compatible API endpoint&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_MODEL_NAME&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Model name for OpenAI-compatible endpoint&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;yi-34b&lt;/code&gt;, &lt;code&gt;gpt-3.5-turbo&lt;/code&gt;, &lt;code&gt;mistral-large&lt;/code&gt;, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;API key for OpenAI-compatible endpoint&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_API_BASE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Base URL for OpenAI-compatible endpoint&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://api.together.xyz/v1&lt;/code&gt;, &lt;code&gt;http://localhost:8000/v1&lt;/code&gt;, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_API_VERSION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;API version for OpenAI-compatible endpoint, optional&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;2023-05-15&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_MAX_TOKENS&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Maximum tokens for completion, optional&lt;/td&gt; 
   &lt;td&gt;Integer&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;4096&lt;/code&gt;, &lt;code&gt;8192&lt;/code&gt;, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_TEMPERATURE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Temperature setting, optional&lt;/td&gt; 
   &lt;td&gt;Float&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;0.0&lt;/code&gt;, &lt;code&gt;0.5&lt;/code&gt;, &lt;code&gt;0.7&lt;/code&gt;, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_SUPPORTS_VISION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Whether model supports vision, optional&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Supported LLM Key: &lt;code&gt;OPENAI_COMPATIBLE&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;General LLM Configuration&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;LLM_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The name of the model you want to use&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;See supported LLM keys above&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;SECONDARY_LLM_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The name of the model for mini agents skyvern runs with&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;See supported LLM keys above&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;LLM_CONFIG_MAX_TOKENS&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Override the max tokens used by the LLM&lt;/td&gt; 
   &lt;td&gt;Integer&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;128000&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Feature Roadmap&lt;/h1&gt; 
&lt;p&gt;This is our planned roadmap for the next few months. If you have any suggestions or would like to see a feature added, please don't hesitate to reach out to us &lt;a href="mailto:founders@skyvern.com"&gt;via email&lt;/a&gt; or &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;discord&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Open Source&lt;/strong&gt; - Open Source Skyvern's core codebase&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Workflow support&lt;/strong&gt; - Allow support to chain multiple Skyvern calls together&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Improved context&lt;/strong&gt; - Improve Skyvern's ability to understand content around interactable elements by introducing feeding relevant label context through the text prompt&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Cost Savings&lt;/strong&gt; - Improve Skyvern's stability and reduce the cost of running Skyvern by optimizing the context tree passed into Skyvern&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Self-serve UI&lt;/strong&gt; - Deprecate the Streamlit UI in favour of a React-based UI component that allows users to kick off new jobs in Skyvern&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Workflow UI Builder&lt;/strong&gt; - Introduce a UI to allow users to build and analyze workflows visually&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Chrome Viewport streaming&lt;/strong&gt; - Introduce a way to live-stream the Chrome viewport to the user's browser (as a part of the self-serve UI)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Past Runs UI&lt;/strong&gt; - Deprecate the Streamlit UI in favour of a React-based UI that allows you to visualize past runs and their results&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Auto workflow builder ("Observer") mode&lt;/strong&gt; - Allow Skyvern to auto-generate workflows as it's navigating the web to make it easier to build new workflows&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Prompt Caching&lt;/strong&gt; - Introduce a caching layer to the LLM calls to dramatically reduce the cost of running Skyvern (memorize past actions and repeat them!)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Web Evaluation Dataset&lt;/strong&gt; - Integrate Skyvern with public benchmark tests to track the quality of our models over time&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Improved Debug mode&lt;/strong&gt; - Allow Skyvern to plan its actions and get "approval" before running them, allowing you to debug what it's doing and more easily iterate on the prompt&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Chrome Extension&lt;/strong&gt; - Allow users to interact with Skyvern through a Chrome extension (incl voice mode, saving tasks, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Skyvern Action Recorder&lt;/strong&gt; - Allow Skyvern to watch a user complete a task and then automatically generate a workflow for it&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Interactable Livestream&lt;/strong&gt; - Allow users to interact with the livestream in real-time to intervene when necessary (such as manually submitting sensitive forms)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Integrate LLM Observability tools&lt;/strong&gt; - Integrate LLM Observability tools to allow back-testing prompt changes with specific data sets + visualize the performance of Skyvern over time&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Langchain Integration&lt;/strong&gt; - Create langchain integration in langchain_community to use Skyvern as a "tool".&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Contributing&lt;/h1&gt; 
&lt;p&gt;We welcome PRs and suggestions! Don't hesitate to open a PR/issue or to reach out to us &lt;a href="mailto:founders@skyvern.com"&gt;via email&lt;/a&gt; or &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;discord&lt;/a&gt;. Please have a look at our &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/CONTRIBUTING.md"&gt;contribution guide&lt;/a&gt; and &lt;a href="https://github.com/skyvern-ai/skyvern/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22"&gt;"Help Wanted" issues&lt;/a&gt; to get started!&lt;/p&gt; 
&lt;p&gt;If you want to chat with the skyvern repository to get a high level overview of how it is structured, how to build off it, and how to resolve usage questions, check out &lt;a href="https://sage.storia.ai?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=skyvern-readme"&gt;Code Sage&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Telemetry&lt;/h1&gt; 
&lt;p&gt;By Default, Skyvern collects basic usage statistics to help us understand how Skyvern is being used. If you would like to opt-out of telemetry, please set the &lt;code&gt;SKYVERN_TELEMETRY&lt;/code&gt; environment variable to &lt;code&gt;false&lt;/code&gt;.&lt;/p&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;Skyvern's open source repository is supported via a managed cloud. All of the core logic powering Skyvern is available in this open source repository licensed under the &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/LICENSE"&gt;AGPL-3.0 License&lt;/a&gt;, with the exception of anti-bot measures available in our managed cloud offering.&lt;/p&gt; 
&lt;p&gt;If you have any questions or concerns around licensing, please &lt;a href="mailto:support@skyvern.com"&gt;contact us&lt;/a&gt; and we would be happy to help.&lt;/p&gt; 
&lt;h1&gt;Star History&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#Skyvern-AI/skyvern&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Skyvern-AI/skyvern&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>facebookresearch/detectron2</title>
      <link>https://github.com/facebookresearch/detectron2</link>
      <description>&lt;p&gt;Detectron2 is a platform for object detection, segmentation and other visual recognition tasks.&lt;/p&gt;&lt;hr&gt;&lt;img src="https://raw.githubusercontent.com/facebookresearch/detectron2/main/.github/Detectron2-Logo-Horz.svg?sanitize=true" width="300" /&gt; 
&lt;p&gt;Detectron2 is Facebook AI Research's next generation library that provides state-of-the-art detection and segmentation algorithms. It is the successor of &lt;a href="https://github.com/facebookresearch/Detectron/"&gt;Detectron&lt;/a&gt; and &lt;a href="https://github.com/facebookresearch/maskrcnn-benchmark/"&gt;maskrcnn-benchmark&lt;/a&gt;. It supports a number of computer vision research projects and production applications in Facebook.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://user-images.githubusercontent.com/1381301/66535560-d3422200-eace-11e9-9123-5535d469db19.png" /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2&gt;Learn More about Detectron2&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Includes new capabilities such as panoptic segmentation, Densepose, Cascade R-CNN, rotated bounding boxes, PointRend, DeepLab, ViTDet, MViTv2 etc.&lt;/li&gt; 
 &lt;li&gt;Used as a library to support building &lt;a href="https://raw.githubusercontent.com/facebookresearch/detectron2/main/projects/"&gt;research projects&lt;/a&gt; on top of it.&lt;/li&gt; 
 &lt;li&gt;Models can be exported to TorchScript format or Caffe2 format for deployment.&lt;/li&gt; 
 &lt;li&gt;It &lt;a href="https://detectron2.readthedocs.io/notes/benchmarks.html"&gt;trains much faster&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See our &lt;a href="https://ai.meta.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-/"&gt;blog post&lt;/a&gt; to see more demos. See this &lt;a href="https://ai.meta.com/blog/detectron-everingham-prize/"&gt;interview&lt;/a&gt; to learn more about the stories behind detectron2.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://detectron2.readthedocs.io/tutorials/install.html"&gt;installation instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://detectron2.readthedocs.io/tutorials/getting_started.html"&gt;Getting Started with Detectron2&lt;/a&gt;, and the &lt;a href="https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5"&gt;Colab Notebook&lt;/a&gt; to learn about basic usage.&lt;/p&gt; 
&lt;p&gt;Learn more at our &lt;a href="https://detectron2.readthedocs.org"&gt;documentation&lt;/a&gt;. And see &lt;a href="https://raw.githubusercontent.com/facebookresearch/detectron2/main/projects/"&gt;projects/&lt;/a&gt; for some projects that are built on top of detectron2.&lt;/p&gt; 
&lt;h2&gt;Model Zoo and Baselines&lt;/h2&gt; 
&lt;p&gt;We provide a large set of baseline results and trained models available for download in the &lt;a href="https://raw.githubusercontent.com/facebookresearch/detectron2/main/MODEL_ZOO.md"&gt;Detectron2 Model Zoo&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Detectron2 is released under the &lt;a href="https://raw.githubusercontent.com/facebookresearch/detectron2/main/LICENSE"&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Citing Detectron2&lt;/h2&gt; 
&lt;p&gt;If you use Detectron2 in your research or wish to refer to the baseline results published in the &lt;a href="https://raw.githubusercontent.com/facebookresearch/detectron2/main/MODEL_ZOO.md"&gt;Model Zoo&lt;/a&gt;, please use the following BibTeX entry.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;@misc{wu2019detectron2,
  author =       {Yuxin Wu and Alexander Kirillov and Francisco Massa and
                  Wan-Yen Lo and Ross Girshick},
  title =        {Detectron2},
  howpublished = {\url{https://github.com/facebookresearch/detectron2}},
  year =         {2019}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>mlflow/mlflow</title>
      <link>https://github.com/mlflow/mlflow</link>
      <description>&lt;p&gt;The open source developer platform to build AI/LLM applications and models with confidence. Enhance your AI applications with end-to-end tracking, observability, and evaluations, all in one integrated platform.&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center" style="border-bottom: none"&gt; &lt;a href="https://mlflow.org/"&gt; &lt;img alt="MLflow logo" src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/logo.svg?sanitize=true" width="200" /&gt; &lt;/a&gt; &lt;/h1&gt; 
&lt;h2 align="center" style="border-bottom: none"&gt;Open-Source Platform for Productionizing AI&lt;/h2&gt; 
&lt;p&gt;MLflow is an open-source developer platform to build AI/LLM applications and models with confidence. Enhance your AI applications with end-to-end &lt;strong&gt;experiment tracking&lt;/strong&gt;, &lt;strong&gt;observability&lt;/strong&gt;, and &lt;strong&gt;evaluations&lt;/strong&gt;, all in one integrated platform.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://pypi.org/project/mlflow/"&gt;&lt;img src="https://img.shields.io/pypi/v/mlflow" alt="Python SDK" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/projects/mlflow"&gt;&lt;img src="https://img.shields.io/pypi/dm/mlflow" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/mlflow/mlflow/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/mlflow/mlflow" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/intent/follow?screen_name=mlflow" target="_blank"&gt; &lt;img src="https://img.shields.io/twitter/follow/mlflow?logo=X&amp;amp;color=%20%23f5f5f5" alt="follow on X(Twitter)" /&gt;&lt;/a&gt; &lt;a href="https://www.linkedin.com/company/mlflow-org/" target="_blank"&gt; &lt;img src="https://custom-icon-badges.demolab.com/badge/LinkedIn-0A66C2?logo=linkedin-white&amp;amp;logoColor=fff" alt="follow on LinkedIn" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/mlflow/mlflow"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;div&gt; 
  &lt;a href="https://mlflow.org/"&gt;&lt;strong&gt;Website&lt;/strong&gt;&lt;/a&gt; ¬∑ 
  &lt;a href="https://mlflow.org/docs/latest/index.html"&gt;&lt;strong&gt;Docs&lt;/strong&gt;&lt;/a&gt; ¬∑ 
  &lt;a href="https://github.com/mlflow/mlflow/issues/new/choose"&gt;&lt;strong&gt;Feature Request&lt;/strong&gt;&lt;/a&gt; ¬∑ 
  &lt;a href="https://mlflow.org/blog"&gt;&lt;strong&gt;News&lt;/strong&gt;&lt;/a&gt; ¬∑ 
  &lt;a href="https://www.youtube.com/@mlflowoss"&gt;&lt;strong&gt;YouTube&lt;/strong&gt;&lt;/a&gt; ¬∑ 
  &lt;a href="https://lu.ma/mlflow?k=c"&gt;&lt;strong&gt;Events&lt;/strong&gt;&lt;/a&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2&gt;üöÄ Installation&lt;/h2&gt; 
&lt;p&gt;To install the MLflow Python package, run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install mlflow
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üì¶ Core Components&lt;/h2&gt; 
&lt;p&gt;MLflow is &lt;strong&gt;the only platform that provides a unified solution for all your AI/ML needs&lt;/strong&gt;, including LLMs, Agents, Deep Learning, and traditional machine learning.&lt;/p&gt; 
&lt;h3&gt;üí° For LLM / GenAI Developers&lt;/h3&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-tracing.png" alt="Tracing" width="100%" /&gt; 
    &lt;div align="center"&gt; 
     &lt;br /&gt; 
     &lt;a href="https://mlflow.org/docs/latest/llms/tracing/index.html"&gt;&lt;strong&gt;üîç Tracing / Observability&lt;/strong&gt;&lt;/a&gt; 
     &lt;br /&gt;
     &lt;br /&gt; 
     &lt;div&gt;
      Trace the internal states of your LLM/agentic applications for debugging quality issues and monitoring performance with ease.
     &lt;/div&gt;
     &lt;br /&gt; 
     &lt;a href="https://mlflow.org/docs/latest/genai/tracing/quickstart/python-openai/"&gt;Getting Started ‚Üí&lt;/a&gt; 
     &lt;br /&gt;
     &lt;br /&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-llm-eval.png" alt="LLM Evaluation" width="100%" /&gt; 
    &lt;div align="center"&gt; 
     &lt;br /&gt; 
     &lt;a href="https://mlflow.org/docs/latest/genai/eval-monitor/"&gt;&lt;strong&gt;üìä LLM Evaluation&lt;/strong&gt;&lt;/a&gt; 
     &lt;br /&gt;
     &lt;br /&gt; 
     &lt;div&gt;
      A suite of automated model evaluation tools, seamlessly integrated with experiment tracking to compare across multiple versions.
     &lt;/div&gt;
     &lt;br /&gt; 
     &lt;a href="https://mlflow.org/docs/latest/genai/eval-monitor/"&gt;Getting Started ‚Üí&lt;/a&gt; 
     &lt;br /&gt;
     &lt;br /&gt; 
    &lt;/div&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-prompt.png" alt="Prompt Management" /&gt; 
    &lt;div align="center"&gt; 
     &lt;br /&gt; 
     &lt;a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/prompt-registry/"&gt;&lt;strong&gt;ü§ñ Prompt Management&lt;/strong&gt;&lt;/a&gt; 
     &lt;br /&gt;
     &lt;br /&gt; 
     &lt;div&gt;
      Version, track, and reuse prompts across your organization, helping maintain consistency and improve collaboration in prompt development.
     &lt;/div&gt;
     &lt;br /&gt; 
     &lt;a href="https://mlflow.org/docs/latest/genai/prompt-registry/create-and-edit-prompts/"&gt;Getting Started ‚Üí&lt;/a&gt; 
     &lt;br /&gt;
     &lt;br /&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-logged-model.png" alt="MLflow Hero" /&gt; 
    &lt;div align="center"&gt; 
     &lt;br /&gt; 
     &lt;a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/version-tracking/"&gt;&lt;strong&gt;üì¶ App Version Tracking&lt;/strong&gt;&lt;/a&gt; 
     &lt;br /&gt;
     &lt;br /&gt; 
     &lt;div&gt;
      MLflow keeps track of many moving parts in your AI applications, such as models, prompts, tools, and code, with end-to-end lineage.
     &lt;/div&gt;
     &lt;br /&gt; 
     &lt;a href="https://mlflow.org/docs/latest/genai/version-tracking/quickstart/"&gt;Getting Started ‚Üí&lt;/a&gt; 
     &lt;br /&gt;
     &lt;br /&gt; 
    &lt;/div&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;üéì For Data Scientists&lt;/h3&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td colspan="2" align="center"&gt; &lt;img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-experiment.png" alt="Tracking" width="50%" /&gt; 
    &lt;div align="center"&gt; 
     &lt;br /&gt; 
     &lt;a href="https://mlflow.org/docs/latest/ml/tracking/"&gt;&lt;strong&gt;üìù Experiment Tracking&lt;/strong&gt;&lt;/a&gt; 
     &lt;br /&gt;
     &lt;br /&gt; 
     &lt;div&gt;
      Track your models, parameters, metrics, and evaluation results in ML experiments and compare them using an interactive UI.
     &lt;/div&gt;
     &lt;br /&gt; 
     &lt;a href="https://mlflow.org/docs/latest/ml/tracking/quickstart/"&gt;Getting Started ‚Üí&lt;/a&gt; 
     &lt;br /&gt;
     &lt;br /&gt; 
    &lt;/div&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-model-registry.png" alt="Model Registry" width="100%" /&gt; 
    &lt;div align="center"&gt; 
     &lt;br /&gt; 
     &lt;a href="https://mlflow.org/docs/latest/ml/model-registry/"&gt;&lt;strong&gt;üíæ Model Registry&lt;/strong&gt;&lt;/a&gt; 
     &lt;br /&gt;
     &lt;br /&gt; 
     &lt;div&gt;
       A centralized model store designed to collaboratively manage the full lifecycle and deployment of machine learning models.
     &lt;/div&gt;
     &lt;br /&gt; 
     &lt;a href="https://mlflow.org/docs/latest/ml/model-registry/tutorial/"&gt;Getting Started ‚Üí&lt;/a&gt; 
     &lt;br /&gt;
     &lt;br /&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-deployment.png" alt="Deployment" width="100%" /&gt; 
    &lt;div align="center"&gt; 
     &lt;br /&gt; 
     &lt;a href="https://mlflow.org/docs/latest/ml/deployment/"&gt;&lt;strong&gt;üöÄ Deployment&lt;/strong&gt;&lt;/a&gt; 
     &lt;br /&gt;
     &lt;br /&gt; 
     &lt;div&gt;
       Tools for seamless model deployment to batch and real-time scoring on platforms like Docker, Kubernetes, Azure ML, and AWS SageMaker.
     &lt;/div&gt;
     &lt;br /&gt; 
     &lt;a href="https://mlflow.org/docs/latest/ml/deployment/"&gt;Getting Started ‚Üí&lt;/a&gt; 
     &lt;br /&gt;
     &lt;br /&gt; 
    &lt;/div&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;üåê Hosting MLflow Anywhere&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-providers.png" alt="Providers" width="100%" /&gt; 
&lt;/div&gt; 
&lt;p&gt;You can run MLflow in many different environments, including local machines, on-premise servers, and cloud infrastructure.&lt;/p&gt; 
&lt;p&gt;Trusted by thousands of organizations, MLflow is now offered as a managed service by most major cloud providers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/sagemaker-ai/experiments/"&gt;Amazon SageMaker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://learn.microsoft.com/en-us/azure/machine-learning/concept-mlflow?view=azureml-api-2"&gt;Azure ML&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.databricks.com/product/managed-mlflow"&gt;Databricks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nebius.com/services/managed-mlflow"&gt;Nebius&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For hosting MLflow on your own infrastructure, please refer to &lt;a href="https://mlflow.org/docs/latest/ml/tracking/#tracking-setup"&gt;this guidance&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üó£Ô∏è Supported Programming Languages&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/mlflow/"&gt;Python&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.npmjs.com/package/mlflow-tracing"&gt;TypeScript / JavaScript&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mvnrepository.com/artifact/org.mlflow/mlflow-client"&gt;Java&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cran.r-project.org/web/packages/mlflow/readme/README.html"&gt;R&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üîó Integrations&lt;/h2&gt; 
&lt;p&gt;MLflow is natively integrated with many popular machine learning frameworks and GenAI libraries.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-integrations.png" alt="Integrations" /&gt;&lt;/p&gt; 
&lt;h2&gt;Usage Examples&lt;/h2&gt; 
&lt;h3&gt;Experiment Tracking (&lt;a href="https://mlflow.org/docs/latest/ml/tracking/"&gt;Doc&lt;/a&gt;)&lt;/h3&gt; 
&lt;p&gt;The following examples trains a simple regression model with scikit-learn, while enabling MLflow's &lt;a href="https://mlflow.org/docs/latest/tracking/autolog.html"&gt;autologging&lt;/a&gt; feature for experiment tracking.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import mlflow

from sklearn.model_selection import train_test_split
from sklearn.datasets import load_diabetes
from sklearn.ensemble import RandomForestRegressor

# Enable MLflow's automatic experiment tracking for scikit-learn
mlflow.sklearn.autolog()

# Load the training dataset
db = load_diabetes()
X_train, X_test, y_train, y_test = train_test_split(db.data, db.target)

rf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)
# MLflow triggers logging automatically upon model fitting
rf.fit(X_train, y_train)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once the above code finishes, run the following command in a separate terminal and access the MLflow UI via the printed URL. An MLflow &lt;strong&gt;Run&lt;/strong&gt; should be automatically created, which tracks the training dataset, hyper parameters, performance metrics, the trained model, dependencies, and even more.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlflow ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Evaluating Models (&lt;a href="https://mlflow.org/docs/latest/model-evaluation/index.html"&gt;Doc&lt;/a&gt;)&lt;/h3&gt; 
&lt;p&gt;The following example runs automatic evaluation for question-answering tasks with several built-in metrics.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import mlflow
import pandas as pd

# Evaluation set contains (1) input question (2) model outputs (3) ground truth
df = pd.DataFrame(
    {
        "inputs": ["What is MLflow?", "What is Spark?"],
        "outputs": [
            "MLflow is an innovative fully self-driving airship powered by AI.",
            "Sparks is an American pop and rock duo formed in Los Angeles.",
        ],
        "ground_truth": [
            "MLflow is an open-source platform for productionizing AI.",
            "Apache Spark is an open-source, distributed computing system.",
        ],
    }
)
eval_dataset = mlflow.data.from_pandas(
    df, predictions="outputs", targets="ground_truth"
)

# Start an MLflow Run to record the evaluation results to
with mlflow.start_run(run_name="evaluate_qa"):
    # Run automatic evaluation with a set of built-in metrics for question-answering models
    results = mlflow.evaluate(
        data=eval_dataset,
        model_type="question-answering",
    )

print(results.tables["eval_results_table"])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Observability (&lt;a href="https://mlflow.org/docs/latest/llms/tracing/index.html"&gt;Doc&lt;/a&gt;)&lt;/h3&gt; 
&lt;p&gt;MLflow Tracing provides LLM observability for various GenAI libraries such as OpenAI, LangChain, LlamaIndex, DSPy, AutoGen, and more. To enable auto-tracing, call &lt;code&gt;mlflow.xyz.autolog()&lt;/code&gt; before running your models. Refer to the documentation for customization and manual instrumentation.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import mlflow
from openai import OpenAI

# Enable tracing for OpenAI
mlflow.openai.autolog()

# Query OpenAI LLM normally
response = OpenAI().chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Hi!"}],
    temperature=0.1,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then navigate to the "Traces" tab in the MLflow UI to find the trace records OpenAI query.&lt;/p&gt; 
&lt;h2&gt;üí≠ Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;For help or questions about MLflow usage (e.g. "how do I do X?") visit the &lt;a href="https://mlflow.org/docs/latest/index.html"&gt;documentation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;In the documentation, you can ask the question to our AI-powered chat bot. Click on the &lt;strong&gt;"Ask AI"&lt;/strong&gt; button at the right bottom.&lt;/li&gt; 
 &lt;li&gt;Join the &lt;a href="https://lu.ma/mlflow?k=c"&gt;virtual events&lt;/a&gt; like office hours and meetups.&lt;/li&gt; 
 &lt;li&gt;To report a bug, file a documentation issue, or submit a feature request, please &lt;a href="https://github.com/mlflow/mlflow/issues/new/choose"&gt;open a GitHub issue&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;For release announcements and other discussions, please subscribe to our mailing list (&lt;a href="mailto:mlflow-users@googlegroups.com"&gt;mlflow-users@googlegroups.com&lt;/a&gt;) or join us on &lt;a href="https://mlflow.org/slack"&gt;Slack&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;We happily welcome contributions to MLflow!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Submit &lt;a href="https://github.com/mlflow/mlflow/issues/new?template=bug_report_template.yaml"&gt;bug reports&lt;/a&gt; and &lt;a href="https://github.com/mlflow/mlflow/issues/new?template=feature_request_template.yaml"&gt;feature requests&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Contribute for &lt;a href="https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;good-first-issues&lt;/a&gt; and &lt;a href="https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22"&gt;help-wanted&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Writing about MLflow and sharing your experience&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Please see our &lt;a href="https://raw.githubusercontent.com/mlflow/mlflow/master/CONTRIBUTING.md"&gt;contribution guide&lt;/a&gt; to learn more about contributing to MLflow.&lt;/p&gt; 
&lt;h2&gt;‚≠êÔ∏è Star History&lt;/h2&gt; 
&lt;a href="https://star-history.com/#mlflow/mlflow&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=mlflow/mlflow&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;h2&gt;‚úèÔ∏è Citation&lt;/h2&gt; 
&lt;p&gt;If you use MLflow in your research, please cite it using the "Cite this repository" button at the top of the &lt;a href="https://github.com/mlflow/mlflow"&gt;GitHub repository page&lt;/a&gt;, which will provide you with citation formats including APA and BibTeX.&lt;/p&gt; 
&lt;h2&gt;üë• Core Members&lt;/h2&gt; 
&lt;p&gt;MLflow is currently maintained by the following core members with significant contributions from hundreds of exceptionally talented community members.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/BenWilson2"&gt;Ben Wilson&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dbczumar"&gt;Corey Zumar&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/daniellok-db"&gt;Daniel Lok&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/gabrielfu"&gt;Gabriel Fu&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/harupy"&gt;Harutaka Kawamura&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/serena-ruan"&gt;Serena Ruan&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TomeHirata"&gt;Tomu Hirata&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/WeichenXu123"&gt;Weichen Xu&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/B-Step62"&gt;Yuki Watanabe&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>RUC-NLPIR/FlashRAG</title>
      <link>https://github.com/RUC-NLPIR/FlashRAG</link>
      <description>&lt;p&gt;‚ö°FlashRAG: A Python Toolkit for Efficient RAG Research (WWW2025 Resource)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;
 &lt;div align="center"&gt;
  ‚ö°FlashRAG: A Python Toolkit for Efficient RAG Research
  &lt;div&gt;&lt;/div&gt;
 &lt;/div&gt;&lt;/h1&gt; 
&lt;p&gt;[ English | &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/README_zh.md"&gt;‰∏≠Êñá&lt;/a&gt; ]&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://arxiv.org/abs/2405.13576" target="_blank"&gt;&lt;img src="https://img.shields.io/badge/arXiv-b5212f.svg?logo=arxiv" /&gt;&lt;/a&gt; 
 &lt;a href="https://huggingface.co/datasets/RUC-NLPIR/FlashRAG_datasets/" target="_blank"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace%20Datasets-27b3b4.svg?sanitize=true" /&gt;&lt;/a&gt; 
 &lt;a href="https://www.modelscope.cn/datasets/hhjinjiajie/FlashRAG_Dataset" target="_blank"&gt;&lt;img src="https://custom-icon-badges.demolab.com/badge/ModelScope%20Datasets-624aff?style=flat&amp;amp;logo=modelscope&amp;amp;logoColor=white" /&gt;&lt;/a&gt; 
 &lt;a href="https://deepwiki.com/RUC-NLPIR/FlashRAG"&gt;&lt;img src="https://devin.ai/assets/deepwiki-badge.png" alt="DeepWiki Document" height="20" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/RUC-NLPIR/FlashRAG/raw/main/LICENSE"&gt;&lt;img alt="License" src="https://img.shields.io/badge/LICENSE-MIT-green" /&gt;&lt;/a&gt; 
 &lt;a&gt;&lt;img alt="Static Badge" src="https://img.shields.io/badge/made_with-Python-blue" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;h4 align="center"&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#wrench-installation"&gt;Installation&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#sparkles-features"&gt;Features&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#rocket-quick-start"&gt;Quick-Start&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#gear-components"&gt; Components&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#art-flashrag-ui"&gt; FlashRAG-UI&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#robot-supporting-methods"&gt; Supporting Methods&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#notebook-supporting-datasets--document-corpus"&gt; Supporting Datasets&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#raised_hands-additional-faqs"&gt; FAQs&lt;/a&gt; &lt;/p&gt; &lt;/h4&gt; 
&lt;p&gt;FlashRAG is a Python toolkit for the reproduction and development of Retrieval Augmented Generation (RAG) research. Our toolkit includes 36 pre-processed benchmark RAG datasets and &lt;strong&gt;23 state-of-the-art RAG algorithms&lt;/strong&gt;, including &lt;strong&gt;7 reasoning-based methods&lt;/strong&gt; that combine reasoning ability with retrieval.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/asset/framework.jpg" /&gt; &lt;/p&gt; 
&lt;p&gt;With FlashRAG and provided resources, you can effortlessly reproduce existing SOTA works in the RAG domain or implement your custom RAG processes and components. Besides, we provide an easy-to-use UI:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/8ca00873-5df2-48a7-b853-89e7b18bc6e9"&gt;https://github.com/user-attachments/assets/8ca00873-5df2-48a7-b853-89e7b18bc6e9&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt; &lt;a href="https://trendshift.io/repositories/10454" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/10454" alt="RUC-NLPIR%2FFlashRAG | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;üîó&lt;/span&gt; Navigation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#sparkles-features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#mag_right-roadmap"&gt;Roadmap&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#page_with_curl-changelog"&gt;Changelog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#wrench-installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#rocket-quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#gear-components"&gt;Components&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#art-flashrag-ui"&gt;FlashRAG-UI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#robot-supporting-methods"&gt;Supporting Methods&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#notebook-supporting-datasets--document-corpus"&gt;Supporting Datasets &amp;amp; Document Corpus&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#raised_hands-additional-faqs"&gt;Additional FAQs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#bookmark-license"&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#star2-citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;span&gt;‚ú®&lt;/span&gt; Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Extensive and Customizable Framework&lt;/strong&gt;: Includes essential components for RAG scenarios such as retrievers, rerankers, generators, and compressors, allowing for flexible assembly of complex pipelines.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Comprehensive Benchmark Datasets&lt;/strong&gt;: A collection of 36 pre-processed RAG benchmark datasets to test and validate RAG models' performances.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Pre-implemented Advanced RAG Algorithms&lt;/strong&gt;: Features &lt;strong&gt;23 advancing RAG algorithms&lt;/strong&gt; with reported results, based on our framework. Easily reproducing results under different settings.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üöÄ Reasoning-based Methods&lt;/strong&gt;: &lt;strong&gt;NEW!&lt;/strong&gt; We now support &lt;strong&gt;7 reasoning-based methods&lt;/strong&gt; that combine reasoning ability with retrieval, achieving superior performance on complex multi-hop tasks.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Efficient Preprocessing Stage&lt;/strong&gt;: Simplifies the RAG workflow preparation by providing various scripts like corpus processing for retrieval, retrieval index building, and pre-retrieval of documents.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Optimized Execution&lt;/strong&gt;: The library's efficiency is enhanced with tools like vLLM, FastChat for LLM inference acceleration, and Faiss for vector index management.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Easy to Use UI&lt;/strong&gt; : We have developed a very easy to use UI to easily and quickly configure and experience the RAG baselines we have implemented, as well as run evaluation scripts on a visual interface.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;span&gt;üîé&lt;/span&gt; Roadmap&lt;/h2&gt; 
&lt;p&gt;FlashRAG is still under development and there are many issues and room for improvement. We will continue to update. And we also sincerely welcome contributions on this open-source toolkit.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Support OpenAI models&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Provdide instructions for each component&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Integrate sentence Transformers&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Support multimodal RAG&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Support reasoning-based methods&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Inlcude more RAG approaches&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Enhance code adaptability and readability&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add support for api-based retriever (vllm server)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;span&gt;üìÉ&lt;/span&gt; Changelog&lt;/h2&gt; 
&lt;p&gt;[25/08/06] üéØ &lt;strong&gt;NEW!&lt;/strong&gt; We have added support for &lt;strong&gt;Reasoning Pipeline&lt;/strong&gt;, which is a new paradigm that combines reasoning ability and retrieval, representing work that includes &lt;a href="https://github.com/SsmallSong/R1-Searcher"&gt;R1-Searcher&lt;/a&gt;, &lt;a href="https://github.com/PeterGriffinJin/Search-R1"&gt;Search-R1&lt;/a&gt;,.... We evaluate the performance of the pipeline on various RAG benchmarks, it can achieve F1 scores close to 60 on multi hop inference datasets such as HotpotQA. See it in &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#robot-supporting-methods"&gt;&lt;strong&gt;result table&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;[25/03/21] üöÄ &lt;strong&gt;Major Update!&lt;/strong&gt; We have expanded our toolkit to support &lt;strong&gt;23 state-of-the-art RAG algorithms&lt;/strong&gt;, including &lt;strong&gt;7 reasoning-based methods&lt;/strong&gt; that significantly improve performance on complex reasoning tasks. This represents a major milestone in our toolkit's evolution!&lt;/p&gt; 
&lt;p&gt;[25/02/24] üî•üî•üî• We have added support for &lt;strong&gt;multimodal RAG&lt;/strong&gt;, including &lt;a href="https://ruc-nlpir.github.io/FlashRAG/#/zh-cn/component/generator?id=%e5%a4%9a%e6%a8%a1%e6%80%81%e7%94%9f%e6%88%90%e5%99%a8"&gt;&lt;strong&gt;MLLMs like Llava, Qwen, InternVL&lt;/strong&gt;&lt;/a&gt;, and various &lt;a href="https://ruc-nlpir.github.io/FlashRAG/#/zh-cn/component/retriever?id=%e5%a4%9a%e6%a8%a1%e6%80%81%e6%a3%80%e7%b4%a2%e5%99%a8"&gt;&lt;strong&gt;multimodal retrievers with Clip architecture&lt;/strong&gt;&lt;/a&gt;. More information can be found in our new version of arxiv article and our documentation. Try it!&lt;/p&gt; 
&lt;p&gt;[25/01/21] Our technical paper &lt;a href="https://arxiv.org/abs/2405.13576"&gt;FlashRAG: A Python Toolkit for Efficient RAG Research&lt;/a&gt; is honored to have been accepted to the Resource Track of the 2025 &lt;strong&gt;ACM Web Conference (WWW 2025)&lt;/strong&gt;. Please Check it out!&lt;/p&gt; 
&lt;p&gt;[25/01/12] Introduce &lt;strong&gt;FlashRAG-UI&lt;/strong&gt;, an easy to use interface. You can easily and quickly configure and experience the supported RAG methods and evaluate them on the benchmarks.&lt;/p&gt; 
&lt;p&gt;[25/01/11] We have added support for a new method &lt;a href="https://arxiv.org/abs/2404.00610"&gt;&lt;u&gt;RQRAG&lt;/u&gt;&lt;/a&gt; method, see it in &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/docs/original_docs/reproduce_experiment.md"&gt;&lt;strong&gt;reproduce_experiment&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;[25/01/07] We have currently support the aggregation of multiple retrievers, see it in &lt;a href="https://github.com/RUC-NLPIR/FlashRAG/raw/main/docs/original_docs/multi_retriever_usage.md"&gt;&lt;strong&gt;multi retriever usage&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;[25/01/07] We have integrated a very flexible and lightweight corpus chunking library &lt;a href="https://github.com/chonkie-ai/chonkie?tab=readme-ov-file#usage"&gt;&lt;strong&gt;Chunkie&lt;/strong&gt;&lt;/a&gt;, which supports various custom chunking methods (tokens, sentences, semantic, etc.). Use it in &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/docs/original_docs/chunk-doc-corpus.md"&gt;&lt;u&gt;chunking doc corpus&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;[24/10/21] We have released a version based on the Paddle framework that supports Chinese hardware platforms. Please refer to &lt;a href="https://github.com/RUC-NLPIR/FlashRAG-Paddle"&gt;FlashRAG Paddle&lt;/a&gt; for details.&lt;/p&gt; 
&lt;p&gt;[24/10/13] A new in-domain dataset and corpus - &lt;a href="https://arxiv.org/pdf/2406.05654"&gt;DomainRAG&lt;/a&gt; have been added to the dataset. The dataset is based on the internal enrollment data of Renmin University of China, covering seven types of tasks, which can be used for conducting domain-specific RAG testing.&lt;/p&gt; 
&lt;p&gt;[24/09/24] We have released a version based on the MindSpore framework that supports Chinese hardware platforms. Please refer to &lt;a href="https://github.com/RUC-NLPIR/FlashRAG-MindSpore"&gt;FlashRAG MindSpore&lt;/a&gt; for details.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Show more&lt;/summary&gt; 
 &lt;p&gt;[24/09/18] Due to the complexity and limitations of installing Pyserini in certain environments, we have introduced a lightweight &lt;code&gt;BM25s&lt;/code&gt; package as an alternative (faster and easier to use). The retriever based on Pyserini will be deprecated in future versions. To use retriever with &lt;code&gt;bm25s&lt;/code&gt;, just set &lt;code&gt;bm25_backend&lt;/code&gt; to &lt;code&gt;bm25s&lt;/code&gt; in config.&lt;/p&gt; 
 &lt;p&gt;[24/09/09] We add support for a new method &lt;a href="https://aclanthology.org/2024.naacl-long.389.pdf"&gt;&lt;u&gt;Adaptive-RAG&lt;/u&gt;&lt;/a&gt;, which can automatically select the RAG process to execute based on the type of query. See it result in &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#robot-supporting-methods"&gt;&lt;u&gt;result table&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;[24/08/02] We add support for a new method &lt;a href="https://arxiv.org/abs/2405.19670"&gt;&lt;u&gt;Spring&lt;/u&gt;&lt;/a&gt;, significantly improve the performance of LLM by adding only a few token embeddings. See it result in &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#robot-supporting-methods"&gt;&lt;u&gt;result table&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;[24/07/17] Due to some unknown issues with HuggingFace, our original dataset link has been invalid. We have updated it. Please check the &lt;a href="https://huggingface.co/datasets/RUC-NLPIR/FlashRAG_datasets/"&gt;new link&lt;/a&gt; if you encounter any problems.&lt;/p&gt; 
 &lt;p&gt;[24/07/06] We add support for a new method: &lt;a href="https://arxiv.org/abs/2406.11460"&gt;&lt;u&gt;Trace&lt;/u&gt;&lt;/a&gt;, which refine text by constructing a knowledge graph. See it &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#robot-supporting-methods"&gt;&lt;u&gt;results&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/docs/original_docs/baseline_details.md"&gt;&lt;u&gt;details&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;[24/06/19] We add support for a new method: &lt;a href="https://arxiv.org/abs/2212.10509"&gt;&lt;u&gt;IRCoT&lt;/u&gt;&lt;/a&gt;, and update the &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#robot-supporting-methods"&gt;&lt;u&gt;result table&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;[24/06/15] We provide a &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/examples/quick_start/demo_en.py"&gt;&lt;u&gt;demo&lt;/u&gt;&lt;/a&gt; to perform the RAG process using our toolkit.&lt;/p&gt; 
 &lt;p&gt;[24/06/11] We have integrated &lt;code&gt;sentence transformers&lt;/code&gt; in the retriever module. Now it's easier to use the retriever without setting pooling methods.&lt;/p&gt; 
 &lt;p&gt;[24/06/05] We have provided detailed document for reproducing existing methods (see &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/docs/original_docs/reproduce_experiment.md"&gt;how to reproduce&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/docs/original_docs/baseline_details.md"&gt;baseline details&lt;/a&gt;), and &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/docs/original_docs/configuration.md"&gt;&lt;u&gt;configurations settings&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;[24/06/02] We have provided an introduction of FlashRAG for beginners, see &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/docs/original_docs/introduction_for_beginners_en.md"&gt;&lt;u&gt;an introduction to flashrag&lt;/u&gt;&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/docs/original_docs/introduction_for_beginners_zh.md"&gt;&lt;u&gt;‰∏≠ÊñáÁâà&lt;/u&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/docs/original_docs/introduction_for_beginners_kr.md"&gt;&lt;u&gt;ÌïúÍµ≠Ïñ¥&lt;/u&gt;&lt;/a&gt;).&lt;/p&gt; 
 &lt;p&gt;[24/05/31] We supported Openai-series models as generator.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;&lt;span&gt;üîß&lt;/span&gt; Installation&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/pypi/v/flashrag-dev" alt="PyPI - Version" /&gt; &lt;img src="https://img.shields.io/pypi/dw/flashrag-dev" alt="PyPI - Downloads" /&gt; &lt;img src="https://img.shields.io/pypi/dm/flashrag-dev" alt="PyPI - Downloads" /&gt;&lt;/p&gt; 
&lt;p&gt;To get started with FlashRAG, you can simply install it with pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-base"&gt;pip install flashrag-dev --pre
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or you can clone it from Github and install (requires Python 3.10+):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/RUC-NLPIR/FlashRAG.git
cd FlashRAG
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to use vllm, sentence-transformers or pyserini, you can install the optional dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install all extra dependencies
pip install flashrag-dev[full]

# Install vllm for faster speed
pip install vllm&amp;gt;=0.4.1

# Install sentence-transformers
pip install sentence-transformers

# Install pyserini for bm25
pip install pyserini
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Due to the incompatibility when installing &lt;code&gt;faiss&lt;/code&gt; using &lt;code&gt;pip&lt;/code&gt;, it is necessary to use the following conda command for installation.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# CPU-only version
conda install -c pytorch faiss-cpu=1.8.0

# GPU(+CPU) version
conda install -c pytorch -c nvidia faiss-gpu=1.8.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: It is impossible to install the latest version of &lt;code&gt;faiss&lt;/code&gt; on certain systems.&lt;/p&gt; 
&lt;p&gt;From the official Faiss repository (&lt;a href="https://github.com/facebookresearch/faiss/raw/main/INSTALL.md"&gt;source&lt;/a&gt;):&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;The CPU-only faiss-cpu conda package is currently available on Linux (x86_64 and arm64), OSX (arm64 only), and Windows (x86_64)&lt;/li&gt; 
  &lt;li&gt;faiss-gpu, containing both CPU and GPU indices, is available on Linux (x86_64 only) for CUDA 11.4 and 12.1&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;span&gt;üöÄ&lt;/span&gt; Quick Start&lt;/h2&gt; 
&lt;h3&gt;Corpus Construction&lt;/h3&gt; 
&lt;p&gt;To build an index, you first need to save your corpus as a &lt;code&gt;jsonl&lt;/code&gt; file with each line representing a document.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsonl"&gt;{"id": "0", "contents": "..."}
{"id": "1", "contents": "..."}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to use Wikipedia as your corpus, you can refer to our documentation &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/docs/original_docs/process-wiki.md"&gt;Processing Wikipedia&lt;/a&gt; to convert it into an indexable format.&lt;/p&gt; 
&lt;h3&gt;Index Construction&lt;/h3&gt; 
&lt;p&gt;You can use the following code to build your own index.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;For &lt;strong&gt;dense retrieval methods&lt;/strong&gt;, especially popular embedding models, we use &lt;code&gt;faiss&lt;/code&gt; to build the index.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;For &lt;strong&gt;sparse retrieval methods (BM25)&lt;/strong&gt;, we use &lt;code&gt;Pyserini&lt;/code&gt; or &lt;code&gt;bm25s&lt;/code&gt; to build the corpus into a Lucene inverted index. The built index contains the original documents.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;For Dense Retrieval Methods&lt;/h4&gt; 
&lt;p&gt;Modify the parameters in the following code to your own.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m flashrag.retriever.index_builder \
  --retrieval_method e5 \
  --model_path /model/e5-base-v2/ \
  --corpus_path indexes/sample_corpus.jsonl \
  --save_dir indexes/ \
  --use_fp16 \
  --max_length 512 \
  --batch_size 256 \
  --pooling_method mean \
  --faiss_type Flat 
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--pooling_method&lt;/code&gt;: If this parameter is not specified, we will automatically select it based on the model name and model file. However, since different embedding models use different pooling methods, &lt;strong&gt;we may not have fully implemented them&lt;/strong&gt;. To ensure accuracy, you can &lt;strong&gt;specify the pooling method corresponding to the retrieval model you are using&lt;/strong&gt; (&lt;code&gt;mean&lt;/code&gt;, &lt;code&gt;pooler&lt;/code&gt;, or &lt;code&gt;cls&lt;/code&gt;).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;---instruction&lt;/code&gt;: Some embedding models require additional instructions to be concatenated to the query before encoding, which can be specified here. Currently, we will automatically fill in the instructions for &lt;strong&gt;E5&lt;/strong&gt; and &lt;strong&gt;BGE&lt;/strong&gt; models, while other models need to be supplemented manually.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If the retrieval model supports the &lt;code&gt;sentence transformers&lt;/code&gt; library, you can use the following code to build the index (&lt;strong&gt;without considering the pooling method&lt;/strong&gt;).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m flashrag.retriever.index_builder \
  --retrieval_method e5 \
  --model_path /model/e5-base-v2/ \
  --corpus_path indexes/sample_corpus.jsonl \
  --save_dir indexes/ \
  --use_fp16 \
  --max_length 512 \
  --batch_size 256 \
  --pooling_method mean \
  --sentence_transformer \
  --faiss_type Flat 
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;For Sparse Retrieval Methods (BM25)&lt;/h4&gt; 
&lt;p&gt;If building a bm25 index, there is no need to specify &lt;code&gt;model_path&lt;/code&gt;.&lt;/p&gt; 
&lt;h5&gt;Building Index with BM25s&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m flashrag.retriever.index_builder \
  --retrieval_method bm25 \
  --corpus_path indexes/sample_corpus.jsonl \
  --bm25_backend bm25s \
  --save_dir indexes/ 
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Building Index with Pyserini&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m flashrag.retriever.index_builder \
  --retrieval_method bm25 \
  --corpus_path indexes/sample_corpus.jsonl \
  --bm25_backend pyserini \
  --save_dir indexes/ 
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;For Sparse Neural Retrieval Methods (SPLADE)&lt;/h3&gt; 
&lt;h5&gt;Install Seismic Index:&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh # Install Rust for compiling
pip install pyseismic-lsr # Install Seismic
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Then build the index with Seismic:&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m flashrag.retriever.index_builder \ # builder
        --retrieval_method splade \ # Model name to trigger seismic index (splade only available)
        --model_path retriever/splade-v3 \ # Local path or repository path are both supported.
        --corpus_embedded_path data/ms_marco/ms_marco_embedded_corpus.jsonl \  # Use cached embedded corpus if corpus is already available in seismic expected format
        --corpus_path data/ms_marco/ms_marco_corpus.jsonl \ # Corpus path in format {id, contents} jsonl file to be embedded if not already built
        --save_dir indexes/ \ # save index directory
        --use_fp16 \ # tell to use fp16 for splade model
        --max_length 512 \ # max tokens for each document
        --batch_size 4 \ # batch size for splade model (4-5 seems the best size for Tesla T4 16GB)
        --n_postings 1000 \ # seismic number of posting lists
        --centroid_fraction 0.2 \ # seismic centroids
        --min_cluster_size 2 \ # seismic min cluster
        --summary_energy 0.4 \ # seismic energy
        --batched_indexing 10000000 # seismic batch
        --nknn 32 # Optional parameter. Tell to seismic to use also knn graph. if not present seismic will work without knn graph
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using the ready-made pipeline&lt;/h3&gt; 
&lt;p&gt;You can use the pipeline class we have already built (as shown in &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#pipelines"&gt;&lt;u&gt;pipelines&lt;/u&gt;&lt;/a&gt;) to implement the RAG process inside. In this case, you just need to configure the config and load the corresponding pipeline.&lt;/p&gt; 
&lt;p&gt;Firstly, load the entire process's config, which records various hyperparameters required in the RAG process. You can input yaml files as parameters or directly as variables.&lt;/p&gt; 
&lt;p&gt;Please note that &lt;strong&gt;variables as input take precedence over files&lt;/strong&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from flashrag.config import Config

# hybrid load configs
config_dict = {'data_dir': 'dataset/'}
my_config = Config(
    config_file_path = 'my_config.yaml',
    config_dict = config_dict
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We provide comprehensive guidance on how to set configurations, you can see our &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/docs/original_docs/configuration.md"&gt;&lt;u&gt;configuration guidance&lt;/u&gt;&lt;/a&gt;. You can also refer to the &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/flashrag/config/basic_config.yaml"&gt;&lt;u&gt;basic yaml file&lt;/u&gt;&lt;/a&gt; we provide to set your own parameters.&lt;/p&gt; 
&lt;p&gt;Next, load the corresponding dataset and initialize the pipeline. The components in the pipeline will be automatically loaded.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from flashrag.utils import get_dataset
from flashrag.pipeline import SequentialPipeline
from flashrag.prompt import PromptTemplate
from flashrag.config import Config

config_dict = {'data_dir': 'dataset/'}
my_config = Config(
    config_file_path = 'my_config.yaml',
    config_dict = config_dict
)
all_split = get_dataset(my_config)
test_data = all_split['test']

pipeline = SequentialPipeline(my_config)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can specify your own input prompt using &lt;code&gt;PromptTemplete&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;prompt_templete = PromptTemplate(
    config,
    system_prompt = "Answer the question based on the given document. Only give me the answer and do not output any other words.\nThe following are given documents.\n\n{reference}",
    user_prompt = "Question: {question}\nAnswer:"
)
pipeline = SequentialPipeline(
  my_config,
  prompt_template = prompt_templete
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Finally, execute &lt;code&gt;pipeline.run&lt;/code&gt; to obtain the final result.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;output_dataset = pipeline.run(test_data, do_eval=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;output_dataset&lt;/code&gt; contains the intermediate results and metric scores for each item in the input dataset. Meanwhile, the dataset with intermediate results and the overall evaluation score will also be saved as a file (if &lt;code&gt;save_intermediate_data&lt;/code&gt; and &lt;code&gt;save_metric_score&lt;/code&gt; are specified).&lt;/p&gt; 
&lt;h3&gt;Build your own pipeline!&lt;/h3&gt; 
&lt;p&gt;Sometimes you may need to implement more complex RAG process, and you can build your own pipeline to implement it. You just need to inherit &lt;code&gt;BasicPipeline&lt;/code&gt;, initialize the components you need, and complete the &lt;code&gt;run&lt;/code&gt; function.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from flashrag.pipeline import BasicPipeline
from flashrag.utils import get_retriever, get_generator

class ToyPipeline(BasicPipeline):
  def __init__(self, config, prompt_templete=None):
    # Load your own components
    pass

  def run(self, dataset, do_eval=True):
    # Complete your own process logic

    # get attribute in dataset using `.`
    input_query = dataset.question
    ...
    # use `update_output` to save intermeidate data
    dataset.update_output("pred",pred_answer_list)
    dataset = self.evaluate(dataset, do_eval=do_eval)
    return dataset
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please first understand the input and output forms of the components you need to use from our &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/docs/original_docs/basic_usage.md"&gt;&lt;u&gt;documentation&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Just use components&lt;/h3&gt; 
&lt;p&gt;If you already have your own code and only want to use our components to embed the original code, you can refer to the &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/docs/original_docs/basic_usage.md"&gt;&lt;u&gt;basic introduction of the components&lt;/u&gt;&lt;/a&gt; to obtain the input and output formats of each component.&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;‚öô&lt;/span&gt; Components&lt;/h2&gt; 
&lt;p&gt;In FlashRAG, we have built a series of common RAG components, including retrievers, generators, refiners, and more. Based on these components, we have assembled several pipelines to implement the RAG workflow, while also providing the flexibility to combine these components in custom arrangements to create your own pipeline.&lt;/p&gt; 
&lt;h4&gt;RAG-Components&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Module&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="1"&gt;Judger&lt;/td&gt; 
   &lt;td&gt;SKR Judger&lt;/td&gt; 
   &lt;td&gt;Judging whether to retrieve using &lt;a href="https://aclanthology.org/2023.findings-emnlp.691.pdf"&gt;SKR&lt;/a&gt; method&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="4"&gt;Retriever&lt;/td&gt; 
   &lt;td&gt;Dense Retriever&lt;/td&gt; 
   &lt;td&gt;Bi-encoder models such as dpr, bge, e5, using faiss for search&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BM25 Retriever&lt;/td&gt; 
   &lt;td&gt;Sparse retrieval method based on Lucene&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Bi-Encoder Reranker&lt;/td&gt; 
   &lt;td&gt;Calculate matching score using bi-Encoder&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Cross-Encoder Reranker&lt;/td&gt; 
   &lt;td&gt;Calculate matching score using cross-encoder&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="5"&gt;Refiner&lt;/td&gt; 
   &lt;td&gt;Extractive Refiner&lt;/td&gt; 
   &lt;td&gt;Refine input by extracting important context&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Abstractive Refiner&lt;/td&gt; 
   &lt;td&gt;Refine input through seq2seq model&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LLMLingua Refiner&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aclanthology.org/2023.emnlp-main.825/"&gt;LLMLingua-series&lt;/a&gt; prompt compressor&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SelectiveContext Refiner&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2310.06201"&gt;Selective-Context&lt;/a&gt; prompt compressor&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; KG Refiner &lt;/td&gt; 
   &lt;td&gt;Use &lt;a hred="https://arxiv.org/abs/2406.11460"&gt;Trace method to construct a knowledge graph&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt;
  &lt;tr&gt; 
   &lt;td rowspan="4"&gt;Generator&lt;/td&gt; 
   &lt;td&gt;Encoder-Decoder Generator&lt;/td&gt; 
   &lt;td&gt;Encoder-Decoder model, supporting &lt;a href="https://arxiv.org/abs/2007.01282"&gt;Fusion-in-Decoder (FiD)&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Decoder-only Generator&lt;/td&gt; 
   &lt;td&gt;Native transformers implementation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;FastChat Generator&lt;/td&gt; 
   &lt;td&gt;Accelerate with &lt;a href="https://github.com/lm-sys/FastChat"&gt;FastChat&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;vllm Generator&lt;/td&gt; 
   &lt;td&gt;Accelerate with &lt;a href="https://github.com/vllm-project/vllm"&gt;vllm&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Pipelines&lt;/h4&gt; 
&lt;p&gt;Referring to a &lt;a href="https://arxiv.org/abs/2312.10997"&gt;&lt;u&gt;survey on retrieval-augmented generation&lt;/u&gt;&lt;/a&gt;, we categorized RAG methods into four types based on their inference paths.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Sequential&lt;/strong&gt;: Sequential execuation of RAG process, like Query-(pre-retrieval)-retriever-(post-retrieval)-generator&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Conditional&lt;/strong&gt;: Implements different paths for different types of input queries&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Branching&lt;/strong&gt; : Executes multiple paths in parallel, merging the responses from each path&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Loop&lt;/strong&gt;: Iteratively performs retrieval and generation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In each category, we have implemented corresponding common pipelines. Some pipelines have corresponding work papers.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Module&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="1"&gt;Sequential&lt;/td&gt; 
   &lt;td&gt;Sequential Pipeline&lt;/td&gt; 
   &lt;td&gt;Linear execution of query, supporting refiner, reranker&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="1"&gt;Conditional&lt;/td&gt; 
   &lt;td&gt;Conditional Pipeline&lt;/td&gt; 
   &lt;td&gt;With a judger module, distinct execution paths for various query types&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;Branching&lt;/td&gt; 
   &lt;td&gt;REPLUG Pipeline&lt;/td&gt; 
   &lt;td&gt;Generate answer by integrating probabilities in multiple generation paths&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;SuRe Pipeline&lt;/td&gt; 
   &lt;td&gt;Ranking and merging generated results based on each document&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="6"&gt;Loop&lt;/td&gt; 
   &lt;td&gt;Iterative Pipeline&lt;/td&gt; 
   &lt;td&gt;Alternating retrieval and generation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Self-Ask Pipeline&lt;/td&gt; 
   &lt;td&gt;Decompose complex problems into subproblems using &lt;a href="https://arxiv.org/abs/2210.03350"&gt;self-ask&lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Self-RAG Pipeline&lt;/td&gt; 
   &lt;td&gt;Adaptive retrieval, critique, and generation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;FLARE Pipeline&lt;/td&gt; 
   &lt;td&gt;Dynamic retrieval during the generation process&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;IRCoT Pipeline&lt;/td&gt; 
   &lt;td&gt;Integrate retrieval process with CoT&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Reasoning Pipeline&lt;/td&gt; 
   &lt;td&gt;Reasoning with retrieval&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;&lt;span&gt;üé®&lt;/span&gt; FlashRAG-UI&lt;/h2&gt; 
&lt;p&gt;With &lt;strong&gt;FlashRAG-UI&lt;/strong&gt;, you can easily and quickly configure and experience the supported RAG methods through our meticulously designed visual interface, and evaluate these methods on benchmarks, making complex research work more efficient!&lt;/p&gt; 
&lt;h3&gt;&lt;span&gt;üåü&lt;/span&gt; Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;One-Click Configuration Loading&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;You can load parameters and configuration files for various RAG methods through simple clicks, selections, and inputs.&lt;/li&gt; 
   &lt;li&gt;Supports preview interface for intuitive parameter settings.&lt;/li&gt; 
   &lt;li&gt;Provides save functionality to easily store configurations for future use.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quick Method Experience&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Quickly load corpora and index files to explore the characteristics and application scenarios of various RAG methods.&lt;/li&gt; 
   &lt;li&gt;Supports loading and switching different components and hyperparameters, seamlessly connecting different RAG Pipelines to quickly experience their performance and differences!&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Efficient Benchmark Reproduction&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Easily reproduce the built-in baseline methods and carefully collected benchmarks on FlashRAG-UI.&lt;/li&gt; 
   &lt;li&gt;Use cutting-edge research tools directly without complex settings, providing a smooth experience for your research work!&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Show more&lt;/summary&gt; 
 &lt;table align="center"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center"&gt; &lt;img src="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/asset/demo_en1.jpg" alt="Image 1" width="505" /&gt; &lt;/td&gt; 
    &lt;td align="center"&gt; &lt;img src="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/asset/demo_en2.jpg" alt="Image 2" width="505" /&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt; &lt;img src="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/asset/demo_en4.png" alt="Image 3" width="500" /&gt; &lt;/td&gt; 
    &lt;td align="center"&gt; &lt;img src="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/asset/demo_en3.jpg" alt="Image 4" width="500" /&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h4&gt;Experience our meticulously designed FlashRAG-UI‚Äîboth user-friendly and visually appealing:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd webui
python interface.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;span&gt;ü§ñ&lt;/span&gt; Supporting Methods&lt;/h2&gt; 
&lt;p&gt;We have implemented &lt;strong&gt;23 works&lt;/strong&gt; with a consistent setting of:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Generator:&lt;/strong&gt; LLAMA3-8B-instruct with input length of 2048&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Retriever:&lt;/strong&gt; e5-base-v2 as embedding model, retrieve 5 docs per query&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; A consistent default prompt, template can be found in the &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/docs/original_docs/baseline_details.md"&gt;&lt;u&gt;method details&lt;/u&gt;&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For open-source methods, we implemented their processes using our framework. For methods where the author did not provide source code, we will try our best to follow the methods in the original paper for implementation.&lt;/p&gt; 
&lt;p&gt;For necessary settings and hyperparameters specific to some methods, we have documented them in the &lt;strong&gt;specific settings&lt;/strong&gt; column. For more details, please consult our &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/docs/original_docs/reproduce_experiment.md"&gt;&lt;u&gt;reproduce guidance&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/docs/original_docs/baseline_details.md"&gt;&lt;u&gt;method details&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;It's important to note that, to ensure consistency, we have utilized a uniform setting. However, this setting may differ from the original setting of the method, leading to variations in results compared to the original outcomes.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;NQ (EM)&lt;/th&gt; 
   &lt;th&gt;TriviaQA (EM)&lt;/th&gt; 
   &lt;th&gt;Hotpotqa (F1)&lt;/th&gt; 
   &lt;th&gt;2Wiki (F1)&lt;/th&gt; 
   &lt;th&gt;PopQA (F1)&lt;/th&gt; 
   &lt;th&gt;WebQA(EM)&lt;/th&gt; 
   &lt;th&gt;Specific setting&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Naive Generation&lt;/td&gt; 
   &lt;td&gt;Sequential&lt;/td&gt; 
   &lt;td&gt;22.6&lt;/td&gt; 
   &lt;td&gt;55.7&lt;/td&gt; 
   &lt;td&gt;28.4&lt;/td&gt; 
   &lt;td&gt;33.9&lt;/td&gt; 
   &lt;td&gt;21.7&lt;/td&gt; 
   &lt;td&gt;18.8&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Standard RAG&lt;/td&gt; 
   &lt;td&gt;Sequential&lt;/td&gt; 
   &lt;td&gt;35.1&lt;/td&gt; 
   &lt;td&gt;58.9&lt;/td&gt; 
   &lt;td&gt;35.3&lt;/td&gt; 
   &lt;td&gt;21.0&lt;/td&gt; 
   &lt;td&gt;36.7&lt;/td&gt; 
   &lt;td&gt;15.7&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://aclanthology.org/2023.acl-long.136.pdf"&gt;AAR-contriever-kilt&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Sequential&lt;/td&gt; 
   &lt;td&gt;30.1&lt;/td&gt; 
   &lt;td&gt;56.8&lt;/td&gt; 
   &lt;td&gt;33.4&lt;/td&gt; 
   &lt;td&gt;19.8&lt;/td&gt; 
   &lt;td&gt;36.1&lt;/td&gt; 
   &lt;td&gt;16.1&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2310.06839"&gt;LongLLMLingua&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Sequential&lt;/td&gt; 
   &lt;td&gt;32.2&lt;/td&gt; 
   &lt;td&gt;59.2&lt;/td&gt; 
   &lt;td&gt;37.5&lt;/td&gt; 
   &lt;td&gt;25.0&lt;/td&gt; 
   &lt;td&gt;38.7&lt;/td&gt; 
   &lt;td&gt;17.5&lt;/td&gt; 
   &lt;td&gt;Compress Ratio=0.5&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2310.04408"&gt;RECOMP-abstractive&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Sequential&lt;/td&gt; 
   &lt;td&gt;33.1&lt;/td&gt; 
   &lt;td&gt;56.4&lt;/td&gt; 
   &lt;td&gt;37.5&lt;/td&gt; 
   &lt;td&gt;32.4&lt;/td&gt; 
   &lt;td&gt;39.9&lt;/td&gt; 
   &lt;td&gt;20.2&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2310.06201"&gt;Selective-Context&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Sequential&lt;/td&gt; 
   &lt;td&gt;30.5&lt;/td&gt; 
   &lt;td&gt;55.6&lt;/td&gt; 
   &lt;td&gt;34.4&lt;/td&gt; 
   &lt;td&gt;18.5&lt;/td&gt; 
   &lt;td&gt;33.5&lt;/td&gt; 
   &lt;td&gt;17.3&lt;/td&gt; 
   &lt;td&gt;Compress Ratio=0.5&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2406.11460"&gt;Trace&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Sequential&lt;/td&gt; 
   &lt;td&gt;30.7&lt;/td&gt; 
   &lt;td&gt;50.2&lt;/td&gt; 
   &lt;td&gt;34.0&lt;/td&gt; 
   &lt;td&gt;15.5&lt;/td&gt; 
   &lt;td&gt;37.4&lt;/td&gt; 
   &lt;td&gt;19.9&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2405.19670"&gt;Spring&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Sequential&lt;/td&gt; 
   &lt;td&gt;37.9&lt;/td&gt; 
   &lt;td&gt;64.6&lt;/td&gt; 
   &lt;td&gt;42.6&lt;/td&gt; 
   &lt;td&gt;37.3&lt;/td&gt; 
   &lt;td&gt;54.8&lt;/td&gt; 
   &lt;td&gt;27.7&lt;/td&gt; 
   &lt;td&gt;Use Llama2-7B-chat with trained embedding table&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2404.13081"&gt;SuRe&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Branching&lt;/td&gt; 
   &lt;td&gt;37.1&lt;/td&gt; 
   &lt;td&gt;53.2&lt;/td&gt; 
   &lt;td&gt;33.4&lt;/td&gt; 
   &lt;td&gt;20.6&lt;/td&gt; 
   &lt;td&gt;48.1&lt;/td&gt; 
   &lt;td&gt;24.2&lt;/td&gt; 
   &lt;td&gt;Use provided prompt&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2301.12652"&gt;REPLUG&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Branching&lt;/td&gt; 
   &lt;td&gt;28.9&lt;/td&gt; 
   &lt;td&gt;57.7&lt;/td&gt; 
   &lt;td&gt;31.2&lt;/td&gt; 
   &lt;td&gt;21.1&lt;/td&gt; 
   &lt;td&gt;27.8&lt;/td&gt; 
   &lt;td&gt;20.2&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://aclanthology.org/2023.findings-emnlp.691.pdf"&gt;SKR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Conditional&lt;/td&gt; 
   &lt;td&gt;33.2&lt;/td&gt; 
   &lt;td&gt;56.0&lt;/td&gt; 
   &lt;td&gt;32.4&lt;/td&gt; 
   &lt;td&gt;23.4&lt;/td&gt; 
   &lt;td&gt;31.7&lt;/td&gt; 
   &lt;td&gt;17.0&lt;/td&gt; 
   &lt;td&gt;Use infernece-time training data&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://aclanthology.org/2024.naacl-long.389.pdf"&gt;Adaptive-RAG&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Conditional&lt;/td&gt; 
   &lt;td&gt;35.1&lt;/td&gt; 
   &lt;td&gt;56.6&lt;/td&gt; 
   &lt;td&gt;39.1&lt;/td&gt; 
   &lt;td&gt;28.4&lt;/td&gt; 
   &lt;td&gt;40.4&lt;/td&gt; 
   &lt;td&gt;16.0&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2310.01558"&gt;Ret-Robust&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Loop&lt;/td&gt; 
   &lt;td&gt;42.9&lt;/td&gt; 
   &lt;td&gt;68.2&lt;/td&gt; 
   &lt;td&gt;35.8&lt;/td&gt; 
   &lt;td&gt;43.4&lt;/td&gt; 
   &lt;td&gt;57.2&lt;/td&gt; 
   &lt;td&gt;33.7&lt;/td&gt; 
   &lt;td&gt;Use LLAMA2-13B with trained lora&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2310.11511"&gt;Self-RAG&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Loop&lt;/td&gt; 
   &lt;td&gt;36.4&lt;/td&gt; 
   &lt;td&gt;38.2&lt;/td&gt; 
   &lt;td&gt;29.6&lt;/td&gt; 
   &lt;td&gt;25.1&lt;/td&gt; 
   &lt;td&gt;32.7&lt;/td&gt; 
   &lt;td&gt;21.9&lt;/td&gt; 
   &lt;td&gt;Use trained selfrag-llama2-7B&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2305.06983"&gt;FLARE&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Loop&lt;/td&gt; 
   &lt;td&gt;22.5&lt;/td&gt; 
   &lt;td&gt;55.8&lt;/td&gt; 
   &lt;td&gt;28.0&lt;/td&gt; 
   &lt;td&gt;33.9&lt;/td&gt; 
   &lt;td&gt;20.7&lt;/td&gt; 
   &lt;td&gt;20.2&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2305.15294"&gt;Iter-Retgen&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2310.05149"&gt;ITRG&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Loop&lt;/td&gt; 
   &lt;td&gt;36.8&lt;/td&gt; 
   &lt;td&gt;60.1&lt;/td&gt; 
   &lt;td&gt;38.3&lt;/td&gt; 
   &lt;td&gt;21.6&lt;/td&gt; 
   &lt;td&gt;37.9&lt;/td&gt; 
   &lt;td&gt;18.2&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://aclanthology.org/2023.acl-long.557.pdf"&gt;IRCoT&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Loop&lt;/td&gt; 
   &lt;td&gt;33.3&lt;/td&gt; 
   &lt;td&gt;56.9&lt;/td&gt; 
   &lt;td&gt;41.5&lt;/td&gt; 
   &lt;td&gt;32.4&lt;/td&gt; 
   &lt;td&gt;45.6&lt;/td&gt; 
   &lt;td&gt;20.7&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2404.00610"&gt;RQRAG&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Loop&lt;/td&gt; 
   &lt;td&gt;32.6&lt;/td&gt; 
   &lt;td&gt;52.5&lt;/td&gt; 
   &lt;td&gt;33.5&lt;/td&gt; 
   &lt;td&gt;35.8&lt;/td&gt; 
   &lt;td&gt;46.4&lt;/td&gt; 
   &lt;td&gt;26.2&lt;/td&gt; 
   &lt;td&gt;Use trained rqrag-llama2-7B&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;üöÄ Reasoning-based Methods (NEW!)&lt;/h4&gt; 
&lt;p&gt;We now support &lt;strong&gt;7 reasoning-based methods&lt;/strong&gt; that combine reasoning ability with retrieval, achieving superior performance on complex multi-hop tasks:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;NQ (EM)&lt;/th&gt; 
   &lt;th&gt;TriviaQA (EM)&lt;/th&gt; 
   &lt;th&gt;PopQA (EM)&lt;/th&gt; 
   &lt;th&gt;Hotpotqa (F1)&lt;/th&gt; 
   &lt;th&gt;2Wiki (F1)&lt;/th&gt; 
   &lt;th&gt;Musique (F1)&lt;/th&gt; 
   &lt;th&gt;Bamboogle (F1)&lt;/th&gt; 
   &lt;th&gt;Specific setting&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2503.09516"&gt;Search-R1&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Reasoning&lt;/td&gt; 
   &lt;td&gt;45.2&lt;/td&gt; 
   &lt;td&gt;62.2&lt;/td&gt; 
   &lt;td&gt;49.2&lt;/td&gt; 
   &lt;td&gt;54.5&lt;/td&gt; 
   &lt;td&gt;42.6&lt;/td&gt; 
   &lt;td&gt;29.2&lt;/td&gt; 
   &lt;td&gt;59.9&lt;/td&gt; 
   &lt;td&gt;SearchR1-nq_hotpotqa_train-qwen2.5-7b-em-ppo&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2503.05592"&gt;R1-Searcher&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Reasoning&lt;/td&gt; 
   &lt;td&gt;36.9&lt;/td&gt; 
   &lt;td&gt;61.6&lt;/td&gt; 
   &lt;td&gt;42.0&lt;/td&gt; 
   &lt;td&gt;49.0&lt;/td&gt; 
   &lt;td&gt;49.1&lt;/td&gt; 
   &lt;td&gt;24.7&lt;/td&gt; 
   &lt;td&gt;57.7&lt;/td&gt; 
   &lt;td&gt;Qwen-2.5-7B-base-RAG-RL&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2505.16582"&gt;O2-Searcher&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Reasoning&lt;/td&gt; 
   &lt;td&gt;41.4&lt;/td&gt; 
   &lt;td&gt;51.4&lt;/td&gt; 
   &lt;td&gt;46.8&lt;/td&gt; 
   &lt;td&gt;43.4&lt;/td&gt; 
   &lt;td&gt;48.6&lt;/td&gt; 
   &lt;td&gt;19.0&lt;/td&gt; 
   &lt;td&gt;47.6&lt;/td&gt; 
   &lt;td&gt;O2-Searcher-Qwen2.5-3B-GRPO&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.arxiv.org/pdf/2505.11277"&gt;AutoRefine&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Reasoning&lt;/td&gt; 
   &lt;td&gt;43.8&lt;/td&gt; 
   &lt;td&gt;59.8&lt;/td&gt; 
   &lt;td&gt;32.4&lt;/td&gt; 
   &lt;td&gt;54.0&lt;/td&gt; 
   &lt;td&gt;50.3&lt;/td&gt; 
   &lt;td&gt;23.6&lt;/td&gt; 
   &lt;td&gt;46.6&lt;/td&gt; 
   &lt;td&gt;AutoRefine-Qwen2.5-3B-Base&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2503.21729"&gt;ReaRAG&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Reasoning&lt;/td&gt; 
   &lt;td&gt;26.3&lt;/td&gt; 
   &lt;td&gt;51.8&lt;/td&gt; 
   &lt;td&gt;24.6&lt;/td&gt; 
   &lt;td&gt;42.9&lt;/td&gt; 
   &lt;td&gt;41.6&lt;/td&gt; 
   &lt;td&gt;21.2&lt;/td&gt; 
   &lt;td&gt;41.9&lt;/td&gt; 
   &lt;td&gt;ReaRAG-9B&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2503.21729"&gt;CoRAG&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Reasoning&lt;/td&gt; 
   &lt;td&gt;40.9&lt;/td&gt; 
   &lt;td&gt;63.1&lt;/td&gt; 
   &lt;td&gt;36.0&lt;/td&gt; 
   &lt;td&gt;56.6&lt;/td&gt; 
   &lt;td&gt;60.7&lt;/td&gt; 
   &lt;td&gt;31.9&lt;/td&gt; 
   &lt;td&gt;54.1&lt;/td&gt; 
   &lt;td&gt;CoRAG-Llama3.1-8B-MultihopQA&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2505.16834"&gt;SimpleDeepSearcher&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Reasoning&lt;/td&gt; 
   &lt;td&gt;36.1&lt;/td&gt; 
   &lt;td&gt;61.6&lt;/td&gt; 
   &lt;td&gt;42.0&lt;/td&gt; 
   &lt;td&gt;49.0&lt;/td&gt; 
   &lt;td&gt;49.1&lt;/td&gt; 
   &lt;td&gt;24.7&lt;/td&gt; 
   &lt;td&gt;57.7&lt;/td&gt; 
   &lt;td&gt;Qwen-7B-SimpleDeepSearcher&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;&lt;span&gt;üìì&lt;/span&gt; Supporting Datasets &amp;amp; Document Corpus&lt;/h2&gt; 
&lt;h3&gt;Datasets&lt;/h3&gt; 
&lt;p&gt;We have collected and processed 36 datasets widely used in RAG research, pre-processing them to ensure a consistent format for ease of use. For certain datasets (such as Wiki-asp), we have adapted them to fit the requirements of RAG tasks according to the methods commonly used within the community. All datasets are available at &lt;a href="https://huggingface.co/datasets/RUC-NLPIR/FlashRAG_datasets"&gt;&lt;u&gt;Huggingface datasets&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For each dataset, we save each split as a &lt;code&gt;jsonl&lt;/code&gt; file, and each line is a dict as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;{
  'id': str,
  'question': str,
  'golden_answers': List[str],
  'metadata': dict
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Below is the list of datasets along with the corresponding sample sizes:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Task&lt;/th&gt; 
   &lt;th&gt;Dataset Name&lt;/th&gt; 
   &lt;th&gt;Knowledge Source&lt;/th&gt; 
   &lt;th&gt;# Train&lt;/th&gt; 
   &lt;th&gt;# Dev&lt;/th&gt; 
   &lt;th&gt;# Test&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QA&lt;/td&gt; 
   &lt;td&gt;NQ&lt;/td&gt; 
   &lt;td&gt;wiki&lt;/td&gt; 
   &lt;td&gt;79,168&lt;/td&gt; 
   &lt;td&gt;8,757&lt;/td&gt; 
   &lt;td&gt;3,610&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QA&lt;/td&gt; 
   &lt;td&gt;TriviaQA&lt;/td&gt; 
   &lt;td&gt;wiki &amp;amp; web&lt;/td&gt; 
   &lt;td&gt;78,785&lt;/td&gt; 
   &lt;td&gt;8,837&lt;/td&gt; 
   &lt;td&gt;11,313&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QA&lt;/td&gt; 
   &lt;td&gt;PopQA&lt;/td&gt; 
   &lt;td&gt;wiki&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
   &lt;td&gt;14,267&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QA&lt;/td&gt; 
   &lt;td&gt;SQuAD&lt;/td&gt; 
   &lt;td&gt;wiki&lt;/td&gt; 
   &lt;td&gt;87,599&lt;/td&gt; 
   &lt;td&gt;10,570&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QA&lt;/td&gt; 
   &lt;td&gt;MSMARCO-QA&lt;/td&gt; 
   &lt;td&gt;web&lt;/td&gt; 
   &lt;td&gt;808,731&lt;/td&gt; 
   &lt;td&gt;101,093&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QA&lt;/td&gt; 
   &lt;td&gt;NarrativeQA&lt;/td&gt; 
   &lt;td&gt;books and story&lt;/td&gt; 
   &lt;td&gt;32,747&lt;/td&gt; 
   &lt;td&gt;3,461&lt;/td&gt; 
   &lt;td&gt;10,557&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QA&lt;/td&gt; 
   &lt;td&gt;WikiQA&lt;/td&gt; 
   &lt;td&gt;wiki&lt;/td&gt; 
   &lt;td&gt;20,360&lt;/td&gt; 
   &lt;td&gt;2,733&lt;/td&gt; 
   &lt;td&gt;6,165&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QA&lt;/td&gt; 
   &lt;td&gt;WebQuestions&lt;/td&gt; 
   &lt;td&gt;Google Freebase&lt;/td&gt; 
   &lt;td&gt;3,778&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
   &lt;td&gt;2,032&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QA&lt;/td&gt; 
   &lt;td&gt;AmbigQA&lt;/td&gt; 
   &lt;td&gt;wiki&lt;/td&gt; 
   &lt;td&gt;10,036&lt;/td&gt; 
   &lt;td&gt;2,002&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QA&lt;/td&gt; 
   &lt;td&gt;SIQA&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;33,410&lt;/td&gt; 
   &lt;td&gt;1,954&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QA&lt;/td&gt; 
   &lt;td&gt;CommonSenseQA&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;9,741&lt;/td&gt; 
   &lt;td&gt;1,221&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QA&lt;/td&gt; 
   &lt;td&gt;BoolQ&lt;/td&gt; 
   &lt;td&gt;wiki&lt;/td&gt; 
   &lt;td&gt;9,427&lt;/td&gt; 
   &lt;td&gt;3,270&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QA&lt;/td&gt; 
   &lt;td&gt;PIQA&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;16,113&lt;/td&gt; 
   &lt;td&gt;1,838&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QA&lt;/td&gt; 
   &lt;td&gt;Fermi&lt;/td&gt; 
   &lt;td&gt;wiki&lt;/td&gt; 
   &lt;td&gt;8,000&lt;/td&gt; 
   &lt;td&gt;1,000&lt;/td&gt; 
   &lt;td&gt;1,000&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;multi-hop QA&lt;/td&gt; 
   &lt;td&gt;HotpotQA&lt;/td&gt; 
   &lt;td&gt;wiki&lt;/td&gt; 
   &lt;td&gt;90,447&lt;/td&gt; 
   &lt;td&gt;7,405&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;multi-hop QA&lt;/td&gt; 
   &lt;td&gt;2WikiMultiHopQA&lt;/td&gt; 
   &lt;td&gt;wiki&lt;/td&gt; 
   &lt;td&gt;15,000&lt;/td&gt; 
   &lt;td&gt;12,576&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;multi-hop QA&lt;/td&gt; 
   &lt;td&gt;Musique&lt;/td&gt; 
   &lt;td&gt;wiki&lt;/td&gt; 
   &lt;td&gt;19,938&lt;/td&gt; 
   &lt;td&gt;2,417&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;multi-hop QA&lt;/td&gt; 
   &lt;td&gt;Bamboogle&lt;/td&gt; 
   &lt;td&gt;wiki&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
   &lt;td&gt;125&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;multi-hop QA&lt;/td&gt; 
   &lt;td&gt;StrategyQA&lt;/td&gt; 
   &lt;td&gt;wiki&lt;/td&gt; 
   &lt;td&gt;2290&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Long-form QA&lt;/td&gt; 
   &lt;td&gt;ASQA&lt;/td&gt; 
   &lt;td&gt;wiki&lt;/td&gt; 
   &lt;td&gt;4,353&lt;/td&gt; 
   &lt;td&gt;948&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Long-form QA&lt;/td&gt; 
   &lt;td&gt;ELI5&lt;/td&gt; 
   &lt;td&gt;Reddit&lt;/td&gt; 
   &lt;td&gt;272,634&lt;/td&gt; 
   &lt;td&gt;1,507&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Long-form QA&lt;/td&gt; 
   &lt;td&gt;WikiPassageQA&lt;/td&gt; 
   &lt;td&gt;wiki&lt;/td&gt; 
   &lt;td&gt;3,332&lt;/td&gt; 
   &lt;td&gt;417&lt;/td&gt; 
   &lt;td&gt;416&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Open-Domain Summarization&lt;/td&gt; 
   &lt;td&gt;WikiASP&lt;/td&gt; 
   &lt;td&gt;wiki&lt;/td&gt; 
   &lt;td&gt;300,636&lt;/td&gt; 
   &lt;td&gt;37,046&lt;/td&gt; 
   &lt;td&gt;37,368&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;multiple-choice&lt;/td&gt; 
   &lt;td&gt;MMLU&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;99,842&lt;/td&gt; 
   &lt;td&gt;1,531&lt;/td&gt; 
   &lt;td&gt;14,042&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;multiple-choice&lt;/td&gt; 
   &lt;td&gt;TruthfulQA&lt;/td&gt; 
   &lt;td&gt;wiki&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
   &lt;td&gt;817&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;multiple-choice&lt;/td&gt; 
   &lt;td&gt;HellaSWAG&lt;/td&gt; 
   &lt;td&gt;ActivityNet&lt;/td&gt; 
   &lt;td&gt;39,905&lt;/td&gt; 
   &lt;td&gt;10,042&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;multiple-choice&lt;/td&gt; 
   &lt;td&gt;ARC&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;3,370&lt;/td&gt; 
   &lt;td&gt;869&lt;/td&gt; 
   &lt;td&gt;3,548&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;multiple-choice&lt;/td&gt; 
   &lt;td&gt;OpenBookQA&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;4,957&lt;/td&gt; 
   &lt;td&gt;500&lt;/td&gt; 
   &lt;td&gt;500&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;multiple-choice&lt;/td&gt; 
   &lt;td&gt;QuaRTz&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;2696&lt;/td&gt; 
   &lt;td&gt;384&lt;/td&gt; 
   &lt;td&gt;784&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fact Verification&lt;/td&gt; 
   &lt;td&gt;FEVER&lt;/td&gt; 
   &lt;td&gt;wiki&lt;/td&gt; 
   &lt;td&gt;104,966&lt;/td&gt; 
   &lt;td&gt;10,444&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dialog Generation&lt;/td&gt; 
   &lt;td&gt;WOW&lt;/td&gt; 
   &lt;td&gt;wiki&lt;/td&gt; 
   &lt;td&gt;63,734&lt;/td&gt; 
   &lt;td&gt;3,054&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Entity Linking&lt;/td&gt; 
   &lt;td&gt;AIDA CoNll-yago&lt;/td&gt; 
   &lt;td&gt;Freebase &amp;amp; wiki&lt;/td&gt; 
   &lt;td&gt;18,395&lt;/td&gt; 
   &lt;td&gt;4,784&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Entity Linking&lt;/td&gt; 
   &lt;td&gt;WNED&lt;/td&gt; 
   &lt;td&gt;Wiki&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
   &lt;td&gt;8,995&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Slot Filling&lt;/td&gt; 
   &lt;td&gt;T-REx&lt;/td&gt; 
   &lt;td&gt;DBPedia&lt;/td&gt; 
   &lt;td&gt;2,284,168&lt;/td&gt; 
   &lt;td&gt;5,000&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Slot Filling&lt;/td&gt; 
   &lt;td&gt;Zero-shot RE&lt;/td&gt; 
   &lt;td&gt;wiki&lt;/td&gt; 
   &lt;td&gt;147,909&lt;/td&gt; 
   &lt;td&gt;3,724&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;In-domain QA&lt;/td&gt; 
   &lt;td&gt;DomainRAG&lt;/td&gt; 
   &lt;td&gt;Web pages of RUC&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
   &lt;td&gt;485&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Document Corpus&lt;/h3&gt; 
&lt;p&gt;Our toolkit supports jsonl format for retrieval document collections, with the following structure:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsonl"&gt;{"id":"0", "contents": "..."}
{"id":"1", "contents": "..."}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;contents&lt;/code&gt; key is essential for building the index. For documents that include both text and title, we recommend setting the value of &lt;code&gt;contents&lt;/code&gt; to &lt;code&gt;{title}\n{text}&lt;/code&gt;. The corpus file can also contain other keys to record additional characteristics of the documents.&lt;/p&gt; 
&lt;p&gt;In the academic research, Wikipedia and MS MARCO are the most commonly used retrieval document collections. For Wikipedia, we provide a &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/docs/original_docs/process-wiki.md"&gt;&lt;u&gt;comprehensive script&lt;/u&gt;&lt;/a&gt; to process any Wikipedia dump into a clean corpus. Additionally, various processed versions of the Wikipedia corpus are available in many works, and we have listed some reference links.&lt;/p&gt; 
&lt;p&gt;For MS MARCO, it is already processed upon release and can be directly downloaded from its &lt;a href="https://huggingface.co/datasets/Tevatron/msmarco-passage-corpus"&gt;&lt;u&gt;hosting link&lt;/u&gt;&lt;/a&gt; on Hugging Face.&lt;/p&gt; 
&lt;h3&gt;Index&lt;/h3&gt; 
&lt;p&gt;To facilitate easier replication of the experiments, we now provide a preprocessed index available in the ModelScope dataset page: &lt;a href="https://www.modelscope.cn/datasets/hhjinjiajie/FlashRAG_Dataset/file/view/master?id=47985&amp;amp;status=2&amp;amp;fileName=retrieval_corpus%252Fwiki18_100w_e5_index.zip"&gt;FlashRAG_Dataset/retrieval_corpus/wiki18_100w_e5_index.zip&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The index was created using the e5-base-v2 retriever on our uploaded wiki18_100w dataset, which is consistent with the index used in our experiments.&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;üç≠&lt;/span&gt; Awesome Work using FlashRAG&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/SsmallSong/R1-Searcher"&gt;R1-Searcher&lt;/a&gt;, a method that incentivizes the search capability in LLMs via reinforcement learning&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Agent-RL/ReSearch"&gt;ReSearch&lt;/a&gt;, a method that learns to reason with search for LLMs via reinforcement learning&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ADaM-BJTU/AutoCoA"&gt;AutoCoA&lt;/a&gt;, a method that internalizes chain-of-action generation into reasoning models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;span&gt;üôå&lt;/span&gt; Additional FAQs&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/docs/original_docs/configuration.md"&gt;How should I set different experimental parameters?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/docs/original_docs/process-wiki.md"&gt;How to build my own corpus, such as a specific segmented Wikipedia?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/docs/original_docs/building-index.md"&gt;How to index my own corpus?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/docs/original_docs/reproduce_experiment.md"&gt;How to reproduce supporting methods?&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;span&gt;üîñ&lt;/span&gt; License&lt;/h2&gt; 
&lt;p&gt;FlashRAG is licensed under the &lt;a href="https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/LICENSE"&gt;&lt;u&gt;MIT License&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;üåü&lt;/span&gt; Citation&lt;/h2&gt; 
&lt;p&gt;Please kindly cite our paper if helps your research:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-BibTex"&gt;@article{FlashRAG,
  author       = {Jiajie Jin and
                  Yutao Zhu and
                  Xinyu Yang and
                  Chenghao Zhang and
                  Zhicheng Dou},
  title        = {FlashRAG: {A} Modular Toolkit for Efficient Retrieval-Augmented Generation
                  Research},
  journal      = {CoRR},
  volume       = {abs/2405.13576},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2405.13576},
  doi          = {10.48550/ARXIV.2405.13576},
  eprinttype    = {arXiv},
  eprint       = {2405.13576},
  timestamp    = {Tue, 18 Jun 2024 09:26:37 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2405-13576.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#RUC-NLPIR/FlashRAG&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=RUC-NLPIR/FlashRAG&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>apache/airflow</title>
      <link>https://github.com/apache/airflow</link>
      <description>&lt;p&gt;Apache Airflow - A platform to programmatically author, schedule, and monitor workflows&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Apache Airflow&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Badges&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;License&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.apache.org/licenses/LICENSE-2.0.txt"&gt;&lt;img src="https://img.shields.io/:license-Apache%202-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PyPI&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://badge.fury.io/py/apache-airflow"&gt;&lt;img src="https://badge.fury.io/py/apache-airflow.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/apache-airflow/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/apache-airflow.svg?sanitize=true" alt="PyPI - Python Version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/apache-airflow/"&gt;&lt;img src="https://img.shields.io/pypi/dm/apache-airflow" alt="PyPI - Downloads" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Containers&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://hub.docker.com/r/apache/airflow"&gt;&lt;img src="https://img.shields.io/docker/pulls/apache/airflow.svg?sanitize=true" alt="Docker Pulls" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/apache/airflow"&gt;&lt;img src="https://img.shields.io/docker/stars/apache/airflow.svg?sanitize=true" alt="Docker Stars" /&gt;&lt;/a&gt; &lt;a href="https://artifacthub.io/packages/search?repo=apache-airflow"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/apache-airflow" alt="Artifact HUB" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Community&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/apache/airflow/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/apache/airflow" alt="Contributors" /&gt;&lt;/a&gt; &lt;a href="https://s.apache.org/airflow-slack"&gt;&lt;img src="https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&amp;amp;style=social" alt="Slack Status" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/commit-activity/m/apache/airflow" alt="Commit Activity" /&gt; &lt;a href="https://ossrank.com/p/6"&gt;&lt;img src="https://shields.io/endpoint?url=https://ossrank.com/shield/6" alt="OSSRank" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Version&lt;/th&gt; 
   &lt;th&gt;Build Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Main&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/apache/airflow/actions"&gt;&lt;img src="https://github.com/apache/airflow/actions/workflows/ci-amd.yml/badge.svg?sanitize=true" alt="GitHub Build main" /&gt;&lt;/a&gt; &lt;a href="https://github.com/apache/airflow/actions"&gt;&lt;img src="https://github.com/apache/airflow/actions/workflows/ci-arm.yml/badge.svg?sanitize=true" alt="GitHub Build main" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3.x&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/apache/airflow/actions"&gt;&lt;img src="https://github.com/apache/airflow/actions/workflows/ci-amd.yml/badge.svg?branch=v3-1-test" alt="GitHub Build 3.1" /&gt;&lt;/a&gt; &lt;a href="https://github.com/apache/airflow/actions"&gt;&lt;img src="https://github.com/apache/airflow/actions/workflows/ci-arm.yml/badge.svg?branch=v3-1-test" alt="GitHub Build 3.1" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2.x&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/apache/airflow/actions"&gt;&lt;img src="https://github.com/apache/airflow/actions/workflows/ci.yml/badge.svg?branch=v2-11-test" alt="GitHub Build 2.11" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;picture width="500"&gt; 
 &lt;img src="https://github.com/apache/airflow/raw/19ebcac2395ef9a6b6ded3a2faa29dc960c1e635/docs/apache-airflow/img/logos/wordmark_1.png?raw=true" alt="Apache Airflow logo" /&gt; 
&lt;/picture&gt; 
&lt;p&gt;&lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/"&gt;Apache Airflow&lt;/a&gt; (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows.&lt;/p&gt; 
&lt;p&gt;When workflows are defined as code, they become more maintainable, versionable, testable, and collaborative.&lt;/p&gt; 
&lt;p&gt;Use Airflow to author workflows (Dags) that orchestrate tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.&lt;/p&gt; 
&lt;!-- END Apache Airflow, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;!-- START doctoc generated TOC please keep comment here to allow auto update --&gt; 
&lt;!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE --&gt; 
&lt;p&gt;&lt;strong&gt;Table of contents&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#project-focus"&gt;Project Focus&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#principles"&gt;Principles&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#requirements"&gt;Requirements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#getting-started"&gt;Getting started&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#installing-from-pypi"&gt;Installing from PyPI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#official-source-code"&gt;Official source code&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#convenience-packages"&gt;Convenience packages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#user-interface"&gt;User Interface&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#semantic-versioning"&gt;Semantic versioning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#version-life-cycle"&gt;Version Life Cycle&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#support-for-python-and-kubernetes-versions"&gt;Support for Python and Kubernetes versions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#base-os-support-for-reference-airflow-images"&gt;Base OS support for reference Airflow images&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#approach-to-dependencies-of-airflow"&gt;Approach to dependencies of Airflow&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#voting-policy"&gt;Voting Policy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#who-uses-apache-airflow"&gt;Who uses Apache Airflow?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#who-maintains-apache-airflow"&gt;Who maintains Apache Airflow?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#what-goes-into-the-next-release"&gt;What goes into the next release?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#can-i-use-the-apache-airflow-logo-in-my-presentation"&gt;Can I use the Apache Airflow logo in my presentation?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#links"&gt;Links&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#sponsors"&gt;Sponsors&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- END doctoc generated TOC please keep comment here to allow auto update --&gt; 
&lt;h2&gt;Project Focus&lt;/h2&gt; 
&lt;p&gt;Airflow works best with workflows that are mostly static and slowly changing. When the DAG structure is similar from one run to the next, it clarifies the unit of work and continuity. Other similar projects include &lt;a href="https://github.com/spotify/luigi"&gt;Luigi&lt;/a&gt;, &lt;a href="https://oozie.apache.org/"&gt;Oozie&lt;/a&gt; and &lt;a href="https://azkaban.github.io/"&gt;Azkaban&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Airflow is commonly used to process data, but has the opinion that tasks should ideally be idempotent (i.e., results of the task will be the same, and will not create duplicated data in a destination system), and should not pass large quantities of data from one task to the next (though tasks can pass metadata using Airflow's &lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/concepts/xcoms.html"&gt;XCom feature&lt;/a&gt;). For high-volume, data-intensive tasks, a best practice is to delegate to external services specializing in that type of work.&lt;/p&gt; 
&lt;p&gt;Airflow is not a streaming solution, but it is often used to process real-time data, pulling data off streams in batches.&lt;/p&gt; 
&lt;h2&gt;Principles&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Dynamic&lt;/strong&gt;: Pipelines are defined in code, enabling dynamic dag generation and parameterization.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensible&lt;/strong&gt;: The Airflow framework includes a wide range of built-in operators and can be extended to fit your needs.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible&lt;/strong&gt;: Airflow leverages the &lt;a href="https://jinja.palletsprojects.com"&gt;&lt;strong&gt;Jinja&lt;/strong&gt;&lt;/a&gt; templating engine, allowing rich customizations.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- START Requirements, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;Apache Airflow is tested with:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;Main version (dev)&lt;/th&gt; 
   &lt;th&gt;Stable version (3.0.6)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python&lt;/td&gt; 
   &lt;td&gt;3.10, 3.11, 3.12, 3.13&lt;/td&gt; 
   &lt;td&gt;3.9, 3.10, 3.11, 3.12&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Platform&lt;/td&gt; 
   &lt;td&gt;AMD64/ARM64(*)&lt;/td&gt; 
   &lt;td&gt;AMD64/ARM64(*)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Kubernetes&lt;/td&gt; 
   &lt;td&gt;1.30, 1.31, 1.32, 1.33&lt;/td&gt; 
   &lt;td&gt;1.30, 1.31, 1.32, 1.33&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PostgreSQL&lt;/td&gt; 
   &lt;td&gt;13, 14, 15, 16, 17&lt;/td&gt; 
   &lt;td&gt;13, 14, 15, 16, 17&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MySQL&lt;/td&gt; 
   &lt;td&gt;8.0, 8.4, Innovation&lt;/td&gt; 
   &lt;td&gt;8.0, 8.4, Innovation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SQLite&lt;/td&gt; 
   &lt;td&gt;3.15.0+&lt;/td&gt; 
   &lt;td&gt;3.15.0+&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;* Experimental&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: MariaDB is not tested/recommended.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: SQLite is used in Airflow tests. Do not use it in production. We recommend using the latest stable version of SQLite for local development.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Airflow currently can be run on POSIX-compliant Operating Systems. For development, it is regularly tested on fairly modern Linux Distros and recent versions of macOS. On Windows you can run it via WSL2 (Windows Subsystem for Linux 2) or via Linux Containers. The work to add Windows support is tracked via &lt;a href="https://github.com/apache/airflow/issues/10388"&gt;#10388&lt;/a&gt;, but it is not a high priority. You should only use Linux-based distros as "Production" execution environment as this is the only environment that is supported. The only distro that is used in our CI tests and that is used in the &lt;a href="https://hub.docker.com/p/apache/airflow"&gt;Community managed DockerHub image&lt;/a&gt; is &lt;code&gt;Debian Bookworm&lt;/code&gt;.&lt;/p&gt; 
&lt;!-- END Requirements, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;!-- START Getting started, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;Visit the official Airflow website documentation (latest &lt;strong&gt;stable&lt;/strong&gt; release) for help with &lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/installation/"&gt;installing Airflow&lt;/a&gt;, &lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/start.html"&gt;getting started&lt;/a&gt;, or walking through a more complete &lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/tutorial/"&gt;tutorial&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: If you're looking for documentation for the main branch (latest development branch): you can find it on &lt;a href="https://s.apache.org/airflow-docs/"&gt;s.apache.org/airflow-docs&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;For more information on Airflow Improvement Proposals (AIPs), visit the &lt;a href="https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals"&gt;Airflow Wiki&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Documentation for dependent projects like provider distributions, Docker image, Helm Chart, you'll find it in &lt;a href="https://airflow.apache.org/docs/"&gt;the documentation index&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- END Getting started, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;!-- START Installing from PyPI, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Installing from PyPI&lt;/h2&gt; 
&lt;p&gt;We publish Apache Airflow as &lt;code&gt;apache-airflow&lt;/code&gt; package in PyPI. Installing it however might be sometimes tricky because Airflow is a bit of both a library and application. Libraries usually keep their dependencies open, and applications usually pin them, but we should do neither and both simultaneously. We decided to keep our dependencies as open as possible (in &lt;code&gt;pyproject.toml&lt;/code&gt;) so users can install different versions of libraries if needed. This means that &lt;code&gt;pip install apache-airflow&lt;/code&gt; will not work from time to time or will produce unusable Airflow installation.&lt;/p&gt; 
&lt;p&gt;To have repeatable installation, however, we keep a set of "known-to-be-working" constraint files in the orphan &lt;code&gt;constraints-main&lt;/code&gt; and &lt;code&gt;constraints-2-0&lt;/code&gt; branches. We keep those "known-to-be-working" constraints files separately per major/minor Python version. You can use them as constraint files when installing Airflow from PyPI. Note that you have to specify correct Airflow tag/version/branch and Python versions in the URL.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Installing just Airflow:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: Only &lt;code&gt;pip&lt;/code&gt; installation is currently officially supported.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;While it is possible to install Airflow with tools like &lt;a href="https://python-poetry.org"&gt;Poetry&lt;/a&gt; or &lt;a href="https://pypi.org/project/pip-tools"&gt;pip-tools&lt;/a&gt;, they do not share the same workflow as &lt;code&gt;pip&lt;/code&gt; - especially when it comes to constraint vs. requirements management. Installing via &lt;code&gt;Poetry&lt;/code&gt; or &lt;code&gt;pip-tools&lt;/code&gt; is not currently supported.&lt;/p&gt; 
&lt;p&gt;There are known issues with &lt;code&gt;bazel&lt;/code&gt; that might lead to circular dependencies when using it to install Airflow. Please switch to &lt;code&gt;pip&lt;/code&gt; if you encounter such problems. &lt;code&gt;Bazel&lt;/code&gt; community works on fixing the problem in &lt;code&gt;this PR &amp;lt;https://github.com/bazelbuild/rules_python/pull/1166&amp;gt;&lt;/code&gt;_ so it might be that newer versions of &lt;code&gt;bazel&lt;/code&gt; will handle it.&lt;/p&gt; 
&lt;p&gt;If you wish to install Airflow using those tools, you should use the constraint files and convert them to the appropriate format and workflow that your tool requires.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install 'apache-airflow==3.0.6' \
 --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-3.0.6/constraints-3.10.txt"
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Installing with extras (i.e., postgres, google)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install 'apache-airflow[postgres,google]==3.0.6' \
 --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-3.0.6/constraints-3.10.txt"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For information on installing provider distributions, check &lt;a href="http://airflow.apache.org/docs/apache-airflow-providers/index.html"&gt;providers&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- END Installing from PyPI, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;For comprehensive instructions on setting up your local development environment and installing Apache Airflow, please refer to the &lt;a href="https://raw.githubusercontent.com/apache/airflow/main/INSTALLING.md"&gt;INSTALLING.md&lt;/a&gt; file.&lt;/p&gt; 
&lt;!-- START Official source code, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Official source code&lt;/h2&gt; 
&lt;p&gt;Apache Airflow is an &lt;a href="https://www.apache.org"&gt;Apache Software Foundation&lt;/a&gt; (ASF) project, and our official source code releases:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Follow the &lt;a href="https://www.apache.org/legal/release-policy.html"&gt;ASF Release Policy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Can be downloaded from &lt;a href="https://downloads.apache.org/airflow"&gt;the ASF Distribution Directory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Are cryptographically signed by the release manager&lt;/li&gt; 
 &lt;li&gt;Are officially voted on by the PMC members during the &lt;a href="https://www.apache.org/legal/release-policy.html#release-approval"&gt;Release Approval Process&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Following the ASF rules, the source packages released must be sufficient for a user to build and test the release provided they have access to the appropriate platform and tools.&lt;/p&gt; 
&lt;!-- END Official source code, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Convenience packages&lt;/h2&gt; 
&lt;p&gt;There are other ways of installing and using Airflow. Those are "convenience" methods - they are not "official releases" as stated by the &lt;code&gt;ASF Release Policy&lt;/code&gt;, but they can be used by the users who do not want to build the software themselves.&lt;/p&gt; 
&lt;p&gt;Those are - in the order of most common ways people install Airflow:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/apache-airflow/"&gt;PyPI releases&lt;/a&gt; to install Airflow using standard &lt;code&gt;pip&lt;/code&gt; tool&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://hub.docker.com/r/apache/airflow"&gt;Docker Images&lt;/a&gt; to install airflow via &lt;code&gt;docker&lt;/code&gt; tool, use them in Kubernetes, Helm Charts, &lt;code&gt;docker-compose&lt;/code&gt;, &lt;code&gt;docker swarm&lt;/code&gt;, etc. You can read more about using, customizing, and extending the images in the &lt;a href="https://airflow.apache.org/docs/docker-stack/index.html"&gt;Latest docs&lt;/a&gt;, and learn details on the internals in the &lt;a href="https://airflow.apache.org/docs/docker-stack/index.html"&gt;images&lt;/a&gt; document.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/apache/airflow/tags"&gt;Tags in GitHub&lt;/a&gt; to retrieve the git project sources that were used to generate official source packages via git&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;All those artifacts are not official releases, but they are prepared using officially released sources. Some of those artifacts are "development" or "pre-release" ones, and they are clearly marked as such following the ASF Policy.&lt;/p&gt; 
&lt;h2&gt;User Interface&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;DAGs&lt;/strong&gt;: Overview of all DAGs in your environment.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/dags.png" alt="DAGs" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Assets&lt;/strong&gt;: Overview of Assets with dependencies.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/assets_graph.png" alt="Asset Dependencies" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Grid&lt;/strong&gt;: Grid representation of a DAG that spans across time.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/grid.png" alt="Grid" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Graph&lt;/strong&gt;: Visualization of a DAG's dependencies and their current status for a specific run.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/graph.png" alt="Graph" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Home&lt;/strong&gt;: Summary statistics of your Airflow environment.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/home.png" alt="Home" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Backfill&lt;/strong&gt;: Backfilling a DAG for a specific date range.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/backfill.png" alt="Backfill" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;: Quick way to view source code of a DAG.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/code.png" alt="Code" /&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Semantic versioning&lt;/h2&gt; 
&lt;p&gt;As of Airflow 2.0.0, we support a strict &lt;a href="https://semver.org/"&gt;SemVer&lt;/a&gt; approach for all packages released.&lt;/p&gt; 
&lt;p&gt;There are few specific rules that we agreed to that define details of versioning of the different packages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Airflow&lt;/strong&gt;: SemVer rules apply to core airflow only (excludes any changes to providers). Changing limits for versions of Airflow dependencies is not a breaking change on its own.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Airflow Providers&lt;/strong&gt;: SemVer rules apply to changes in the particular provider's code only. SemVer MAJOR and MINOR versions for the packages are independent of the Airflow version. For example, &lt;code&gt;google 4.1.0&lt;/code&gt; and &lt;code&gt;amazon 3.0.6&lt;/code&gt; providers can happily be installed with &lt;code&gt;Airflow 2.1.2&lt;/code&gt;. If there are limits of cross-dependencies between providers and Airflow packages, they are present in providers as &lt;code&gt;install_requires&lt;/code&gt; limitations. We aim to keep backwards compatibility of providers with all previously released Airflow 2 versions but there will sometimes be breaking changes that might make some, or all providers, have minimum Airflow version specified.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Airflow Helm Chart&lt;/strong&gt;: SemVer rules apply to changes in the chart only. SemVer MAJOR and MINOR versions for the chart are independent of the Airflow version. We aim to keep backwards compatibility of the Helm Chart with all released Airflow 2 versions, but some new features might only work starting from specific Airflow releases. We might however limit the Helm Chart to depend on minimal Airflow version.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Airflow API clients&lt;/strong&gt;: Their versioning is independent from Airflow versions. They follow their own SemVer rules for breaking changes and new features - which for example allows to change the way we generate the clients.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Version Life Cycle&lt;/h2&gt; 
&lt;p&gt;Apache Airflow version life cycle:&lt;/p&gt; 
&lt;!-- This table is automatically updated by prek scripts/ci/prek/supported_versions.py --&gt; 
&lt;!-- Beginning of auto-generated table --&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Version&lt;/th&gt; 
   &lt;th&gt;Current Patch/Minor&lt;/th&gt; 
   &lt;th&gt;State&lt;/th&gt; 
   &lt;th&gt;First Release&lt;/th&gt; 
   &lt;th&gt;Limited Maintenance&lt;/th&gt; 
   &lt;th&gt;EOL/Terminated&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;3.0.6&lt;/td&gt; 
   &lt;td&gt;Supported&lt;/td&gt; 
   &lt;td&gt;Apr 22, 2025&lt;/td&gt; 
   &lt;td&gt;TBD&lt;/td&gt; 
   &lt;td&gt;TBD&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;2.11.0&lt;/td&gt; 
   &lt;td&gt;Supported&lt;/td&gt; 
   &lt;td&gt;Dec 17, 2020&lt;/td&gt; 
   &lt;td&gt;Oct 22, 2025&lt;/td&gt; 
   &lt;td&gt;Apr 22, 2026&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1.10&lt;/td&gt; 
   &lt;td&gt;1.10.15&lt;/td&gt; 
   &lt;td&gt;EOL&lt;/td&gt; 
   &lt;td&gt;Aug 27, 2018&lt;/td&gt; 
   &lt;td&gt;Dec 17, 2020&lt;/td&gt; 
   &lt;td&gt;June 17, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1.9&lt;/td&gt; 
   &lt;td&gt;1.9.0&lt;/td&gt; 
   &lt;td&gt;EOL&lt;/td&gt; 
   &lt;td&gt;Jan 03, 2018&lt;/td&gt; 
   &lt;td&gt;Aug 27, 2018&lt;/td&gt; 
   &lt;td&gt;Aug 27, 2018&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1.8&lt;/td&gt; 
   &lt;td&gt;1.8.2&lt;/td&gt; 
   &lt;td&gt;EOL&lt;/td&gt; 
   &lt;td&gt;Mar 19, 2017&lt;/td&gt; 
   &lt;td&gt;Jan 03, 2018&lt;/td&gt; 
   &lt;td&gt;Jan 03, 2018&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1.7&lt;/td&gt; 
   &lt;td&gt;1.7.1.2&lt;/td&gt; 
   &lt;td&gt;EOL&lt;/td&gt; 
   &lt;td&gt;Mar 28, 2016&lt;/td&gt; 
   &lt;td&gt;Mar 19, 2017&lt;/td&gt; 
   &lt;td&gt;Mar 19, 2017&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!-- End of auto-generated table --&gt; 
&lt;p&gt;Limited support versions will be supported with security and critical bug fix only. EOL versions will not get any fixes nor support. We always recommend that all users run the latest available minor release for whatever major version is in use. We &lt;strong&gt;highly&lt;/strong&gt; recommend upgrading to the latest Airflow major release at the earliest convenient time and before the EOL date.&lt;/p&gt; 
&lt;h2&gt;Support for Python and Kubernetes versions&lt;/h2&gt; 
&lt;p&gt;As of Airflow 2.0, we agreed to certain rules we follow for Python and Kubernetes support. They are based on the official release schedule of Python and Kubernetes, nicely summarized in the &lt;a href="https://devguide.python.org/#status-of-python-branches"&gt;Python Developer's Guide&lt;/a&gt; and &lt;a href="https://kubernetes.io/docs/setup/release/version-skew-policy/"&gt;Kubernetes version skew policy&lt;/a&gt;.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;We drop support for Python and Kubernetes versions when they reach EOL. Except for Kubernetes, a version stays supported by Airflow if two major cloud providers still provide support for it. We drop support for those EOL versions in main right after EOL date, and it is effectively removed when we release the first new MINOR (Or MAJOR if there is no new MINOR version) of Airflow. For example, for Python 3.10 it means that we will drop support in main right after 27.06.2023, and the first MAJOR or MINOR version of Airflow released after will not have it.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;We support a new version of Python/Kubernetes in main after they are officially released, as soon as we make them work in our CI pipeline (which might not be immediate due to dependencies catching up with new versions of Python mostly) we release new images/support in Airflow based on the working CI setup.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;This policy is best-effort which means there may be situations where we might terminate support earlier if circumstances require it.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Base OS support for reference Airflow images&lt;/h2&gt; 
&lt;p&gt;The Airflow Community provides conveniently packaged container images that are published whenever we publish an Apache Airflow release. Those images contain:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Base OS with necessary packages to install Airflow (stable Debian OS)&lt;/li&gt; 
 &lt;li&gt;Base Python installation in versions supported at the time of release for the MINOR version of Airflow released (so there could be different versions for 2.3 and 2.2 line for example)&lt;/li&gt; 
 &lt;li&gt;Libraries required to connect to supported Databases (again the set of databases supported depends on the MINOR version of Airflow)&lt;/li&gt; 
 &lt;li&gt;Predefined set of popular providers (for details see the &lt;a href="https://raw.githubusercontent.com/apache/airflow/main/Dockerfile"&gt;Dockerfile&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;Possibility of building your own, custom image where the user can choose their own set of providers and libraries (see &lt;a href="https://airflow.apache.org/docs/docker-stack/build.html"&gt;Building the image&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;In the future Airflow might also support a "slim" version without providers nor database clients installed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The version of the base OS image is the stable version of Debian. Airflow supports using all currently active stable versions - as soon as all Airflow dependencies support building, and we set up the CI pipeline for building and testing the OS version. Approximately 6 months before the end-of-regular support of a previous stable version of the OS, Airflow switches the images released to use the latest supported version of the OS.&lt;/p&gt; 
&lt;p&gt;For example switch from &lt;code&gt;Debian Bullseye&lt;/code&gt; to &lt;code&gt;Debian Bookworm&lt;/code&gt; has been implemented before 2.8.0 release in October 2023 and &lt;code&gt;Debian Bookworm&lt;/code&gt; will be the only option supported as of Airflow 2.10.0.&lt;/p&gt; 
&lt;p&gt;Users will continue to be able to build their images using stable Debian releases until the end of regular support and building and verifying of the images happens in our CI but no unit tests were executed using this image in the &lt;code&gt;main&lt;/code&gt; branch.&lt;/p&gt; 
&lt;h2&gt;Approach to dependencies of Airflow&lt;/h2&gt; 
&lt;p&gt;Airflow has a lot of dependencies - direct and transitive, also Airflow is both - library and application, therefore our policies to dependencies has to include both - stability of installation of application, but also ability to install newer version of dependencies for those users who develop DAGs. We developed the approach where &lt;code&gt;constraints&lt;/code&gt; are used to make sure airflow can be installed in a repeatable way, while we do not limit our users to upgrade most of the dependencies. As a result we decided not to upper-bound version of Airflow dependencies by default, unless we have good reasons to believe upper-bounding them is needed because of importance of the dependency as well as risk it involves to upgrade specific dependency. We also upper-bound the dependencies that we know cause problems.&lt;/p&gt; 
&lt;p&gt;The constraint mechanism of ours takes care about finding and upgrading all the non-upper bound dependencies automatically (providing that all the tests pass). Our &lt;code&gt;main&lt;/code&gt; build failures will indicate in case there are versions of dependencies that break our tests - indicating that we should either upper-bind them or that we should fix our code/tests to account for the upstream changes from those dependencies.&lt;/p&gt; 
&lt;p&gt;Whenever we upper-bound such a dependency, we should always comment why we are doing it - i.e. we should have a good reason why dependency is upper-bound. And we should also mention what is the condition to remove the binding.&lt;/p&gt; 
&lt;h3&gt;Approach for dependencies for Airflow Core&lt;/h3&gt; 
&lt;p&gt;Those dependencies are maintained in &lt;code&gt;pyproject.toml&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;There are few dependencies that we decided are important enough to upper-bound them by default, as they are known to follow predictable versioning scheme, and we know that new versions of those are very likely to bring breaking changes. We commit to regularly review and attempt to upgrade to the newer versions of the dependencies as they are released, but this is manual process.&lt;/p&gt; 
&lt;p&gt;The important dependencies are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;SQLAlchemy&lt;/code&gt;: upper-bound to specific MINOR version (SQLAlchemy is known to remove deprecations and introduce breaking changes especially that support for different Databases varies and changes at various speed)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Alembic&lt;/code&gt;: it is important to handle our migrations in predictable and performant way. It is developed together with SQLAlchemy. Our experience with Alembic is that it very stable in MINOR version&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Flask&lt;/code&gt;: We are using Flask as the back-bone of our web UI and API. We know major version of Flask are very likely to introduce breaking changes across those so limiting it to MAJOR version makes sense&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;werkzeug&lt;/code&gt;: the library is known to cause problems in new versions. It is tightly coupled with Flask libraries, and we should update them together&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;celery&lt;/code&gt;: Celery is a crucial component of Airflow as it used for CeleryExecutor (and similar). Celery &lt;a href="https://docs.celeryq.dev/en/stable/contributing.html?highlight=semver#versions"&gt;follows SemVer&lt;/a&gt;, so we should upper-bound it to the next MAJOR version. Also, when we bump the upper version of the library, we should make sure Celery Provider minimum Airflow version is updated.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;kubernetes&lt;/code&gt;: Kubernetes is a crucial component of Airflow as it is used for the KubernetesExecutor (and similar). Kubernetes Python library &lt;a href="https://github.com/kubernetes-client/python#compatibility"&gt;follows SemVer&lt;/a&gt;, so we should upper-bound it to the next MAJOR version. Also, when we bump the upper version of the library, we should make sure Kubernetes Provider minimum Airflow version is updated.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Approach for dependencies in Airflow Providers and extras&lt;/h3&gt; 
&lt;p&gt;The main part of the Airflow is the Airflow Core, but the power of Airflow also comes from a number of providers that extend the core functionality and are released separately, even if we keep them (for now) in the same monorepo for convenience. You can read more about the providers in the &lt;a href="https://airflow.apache.org/docs/apache-airflow-providers/index.html"&gt;Providers documentation&lt;/a&gt;. We also have set of policies implemented for maintaining and releasing community-managed providers as well as the approach for community vs. 3rd party providers in the &lt;a href="https://github.com/apache/airflow/raw/main/PROVIDERS.rst"&gt;providers&lt;/a&gt; document.&lt;/p&gt; 
&lt;p&gt;Those &lt;code&gt;extras&lt;/code&gt; and &lt;code&gt;providers&lt;/code&gt; dependencies are maintained in &lt;code&gt;provider.yaml&lt;/code&gt; of each provider.&lt;/p&gt; 
&lt;p&gt;By default, we should not upper-bound dependencies for providers, however each provider's maintainer might decide to add additional limits (and justify them with comment).&lt;/p&gt; 
&lt;!-- START Contributing, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Want to help build Apache Airflow? Check out our &lt;a href="https://github.com/apache/airflow/raw/main/contributing-docs/README.rst"&gt;contributors' guide&lt;/a&gt; for a comprehensive overview of how to contribute, including setup instructions, coding standards, and pull request guidelines.&lt;/p&gt; 
&lt;p&gt;If you can't wait to contribute, and want to get started asap, check out the &lt;a href="https://github.com/apache/airflow/raw/main/contributing-docs/03_contributors_quick_start.rst"&gt;contribution quickstart&lt;/a&gt; here!&lt;/p&gt; 
&lt;p&gt;Official Docker (container) images for Apache Airflow are described in &lt;a href="https://github.com/apache/airflow/raw/main/dev/breeze/doc/ci/02_images.md"&gt;images&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- END Contributing, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;!-- START Who uses Apache Airflow, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Voting Policy&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Commits need a +1 vote from a committer who is not the author&lt;/li&gt; 
 &lt;li&gt;When we do AIP voting, both PMC member's and committer's &lt;code&gt;+1s&lt;/code&gt; are considered a binding vote.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Who uses Apache Airflow?&lt;/h2&gt; 
&lt;p&gt;We know about around 500 organizations that are using Apache Airflow (but there are likely many more) &lt;a href="https://github.com/apache/airflow/raw/main/INTHEWILD.md"&gt;in the wild&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you use Airflow - feel free to make a PR to add your organisation to the list.&lt;/p&gt; 
&lt;!-- END Who uses Apache Airflow, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;!-- START Who maintains Apache Airflow, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Who maintains Apache Airflow?&lt;/h2&gt; 
&lt;p&gt;Airflow is the work of the &lt;a href="https://github.com/apache/airflow/graphs/contributors"&gt;community&lt;/a&gt;, but the &lt;a href="https://people.apache.org/committers-by-project.html#airflow"&gt;core committers/maintainers&lt;/a&gt; are responsible for reviewing and merging PRs as well as steering conversations around new feature requests. If you would like to become a maintainer, please review the Apache Airflow &lt;a href="https://github.com/apache/airflow/raw/main/COMMITTERS.rst#guidelines-to-become-an-airflow-committer"&gt;committer requirements&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- END Who maintains Apache Airflow, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;What goes into the next release?&lt;/h2&gt; 
&lt;p&gt;Often you will see an issue that is assigned to specific milestone with Airflow version, or a PR that gets merged to the main branch and you might wonder which release the merged PR(s) will be released in or which release the fixed issues will be in. The answer to this is as usual - it depends on various scenarios. The answer is different for PRs and Issues.&lt;/p&gt; 
&lt;p&gt;To add a bit of context, we are following the &lt;a href="https://semver.org/"&gt;Semver&lt;/a&gt; versioning scheme as described in &lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/release-process.html"&gt;Airflow release process&lt;/a&gt;. More details are explained in detail in this README under the &lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#semantic-versioning"&gt;Semantic versioning&lt;/a&gt; chapter, but in short, we have &lt;code&gt;MAJOR.MINOR.PATCH&lt;/code&gt; versions of Airflow.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;MAJOR&lt;/code&gt; version is incremented in case of breaking changes&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;MINOR&lt;/code&gt; version is incremented when there are new features added&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;PATCH&lt;/code&gt; version is incremented when there are only bug-fixes and doc-only changes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Generally we release &lt;code&gt;MINOR&lt;/code&gt; versions of Airflow from a branch that is named after the MINOR version. For example &lt;code&gt;2.7.*&lt;/code&gt; releases are released from &lt;code&gt;v2-7-stable&lt;/code&gt; branch, &lt;code&gt;2.8.*&lt;/code&gt; releases are released from &lt;code&gt;v2-8-stable&lt;/code&gt; branch, etc.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Most of the time in our release cycle, when the branch for next &lt;code&gt;MINOR&lt;/code&gt; branch is not yet created, all PRs merged to &lt;code&gt;main&lt;/code&gt; (unless they get reverted), will find their way to the next &lt;code&gt;MINOR&lt;/code&gt; release. For example if the last release is &lt;code&gt;2.7.3&lt;/code&gt; and &lt;code&gt;v2-8-stable&lt;/code&gt; branch is not created yet, the next &lt;code&gt;MINOR&lt;/code&gt; release is &lt;code&gt;2.8.0&lt;/code&gt; and all PRs merged to main will be released in &lt;code&gt;2.8.0&lt;/code&gt;. However, some PRs (bug-fixes and doc-only changes) when merged, can be cherry-picked to current &lt;code&gt;MINOR&lt;/code&gt; branch and released in the next &lt;code&gt;PATCHLEVEL&lt;/code&gt; release. For example, if &lt;code&gt;2.8.1&lt;/code&gt; is already released and we are working on &lt;code&gt;2.9.0dev&lt;/code&gt;, then marking a PR with &lt;code&gt;2.8.2&lt;/code&gt; milestone means that it will be cherry-picked to &lt;code&gt;v2-8-test&lt;/code&gt; branch and released in &lt;code&gt;2.8.2rc1&lt;/code&gt;, and eventually in &lt;code&gt;2.8.2&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;When we prepare for the next &lt;code&gt;MINOR&lt;/code&gt; release, we cut new &lt;code&gt;v2-*-test&lt;/code&gt; and &lt;code&gt;v2-*-stable&lt;/code&gt; branch and prepare &lt;code&gt;alpha&lt;/code&gt;, &lt;code&gt;beta&lt;/code&gt; releases for the next &lt;code&gt;MINOR&lt;/code&gt; version, the PRs merged to main will still be released in the next &lt;code&gt;MINOR&lt;/code&gt; release until &lt;code&gt;rc&lt;/code&gt; version is cut. This is happening because the &lt;code&gt;v2-*-test&lt;/code&gt; and &lt;code&gt;v2-*-stable&lt;/code&gt; branches are rebased on top of main when next &lt;code&gt;beta&lt;/code&gt; and &lt;code&gt;rc&lt;/code&gt; releases are prepared. For example, when we cut &lt;code&gt;2.10.0beta1&lt;/code&gt; version, anything merged to main before &lt;code&gt;2.10.0rc1&lt;/code&gt; is released, will find its way to 2.10.0rc1.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Then, once we prepare the first RC candidate for the MINOR release, we stop moving the &lt;code&gt;v2-*-test&lt;/code&gt; and &lt;code&gt;v2-*-stable&lt;/code&gt; branches and the PRs merged to main will be released in the next &lt;code&gt;MINOR&lt;/code&gt; release. However, some PRs (bug-fixes and doc-only changes) when merged, can be cherry-picked to current &lt;code&gt;MINOR&lt;/code&gt; branch and released in the next &lt;code&gt;PATCHLEVEL&lt;/code&gt; release - for example when the last released version from &lt;code&gt;v2-10-stable&lt;/code&gt; branch is &lt;code&gt;2.10.0rc1&lt;/code&gt;, some of the PRs from main can be marked as &lt;code&gt;2.10.0&lt;/code&gt; milestone by committers, the release manager will try to cherry-pick them into the release branch. If successful, they will be released in &lt;code&gt;2.10.0rc2&lt;/code&gt; and subsequently in &lt;code&gt;2.10.0&lt;/code&gt;. This also applies to subsequent &lt;code&gt;PATCHLEVEL&lt;/code&gt; versions. When for example &lt;code&gt;2.10.1&lt;/code&gt; is already released, marking a PR with &lt;code&gt;2.10.2&lt;/code&gt; milestone will mean that it will be cherry-picked to &lt;code&gt;v2-10-stable&lt;/code&gt; branch and released in &lt;code&gt;2.10.2rc1&lt;/code&gt; and eventually in &lt;code&gt;2.10.2&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The final decision about cherry-picking is made by the release manager.&lt;/p&gt; 
&lt;p&gt;Marking issues with a milestone is a bit different. Maintainers do not mark issues with a milestone usually, normally they are only marked in PRs. If PR linked to the issue (and "fixing it") gets merged and released in a specific version following the process described above, the issue will be automatically closed, no milestone will be set for the issue, you need to check the PR that fixed the issue to see which version it was released in.&lt;/p&gt; 
&lt;p&gt;However, sometimes maintainers mark issues with specific milestone, which means that the issue is important to become a candidate to take a look when the release is being prepared. Since this is an Open-Source project, where basically all contributors volunteer their time, there is no guarantee that specific issue will be fixed in specific version. We do not want to hold the release because some issue is not fixed, so in such case release manager will reassign such unfixed issues to the next milestone in case they are not fixed in time for the current release. Therefore, the milestone for issue is more of an intent that it should be looked at, than promise it will be fixed in the version.&lt;/p&gt; 
&lt;p&gt;More context and &lt;strong&gt;FAQ&lt;/strong&gt; about the patchlevel release can be found in the &lt;a href="https://raw.githubusercontent.com/apache/airflow/main/dev/WHAT_GOES_INTO_THE_NEXT_RELEASE.md"&gt;What goes into the next release&lt;/a&gt; document in the &lt;code&gt;dev&lt;/code&gt; folder of the repository.&lt;/p&gt; 
&lt;h2&gt;Can I use the Apache Airflow logo in my presentation?&lt;/h2&gt; 
&lt;p&gt;Yes! Be sure to abide by the Apache Foundation &lt;a href="https://www.apache.org/foundation/marks/#books"&gt;trademark policies&lt;/a&gt; and the Apache Airflow &lt;a href="https://cwiki.apache.org/confluence/display/AIRFLOW/Brandbook"&gt;Brandbook&lt;/a&gt;. The most up-to-date logos are found in &lt;a href="https://github.com/apache/airflow/tree/main/airflow-core/docs/img/logos/"&gt;this repo&lt;/a&gt; and on the Apache Software Foundation &lt;a href="https://www.apache.org/logos/about.html"&gt;website&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://s.apache.org/airflow-slack"&gt;Chat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://airflow.apache.org/community/"&gt;Community Information&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;p&gt;The CI infrastructure for Apache Airflow has been sponsored by:&lt;/p&gt; 
&lt;!-- Ordered by most recently "funded" --&gt; 
&lt;p&gt;&lt;a href="https://astronomer.io"&gt;&lt;img src="https://assets2.astronomer.io/logos/logoForLIGHTbackground.png" alt="astronomer.io" width="250px" /&gt;&lt;/a&gt; &lt;a href="https://aws.amazon.com/opensource/"&gt;&lt;img src="https://github.com/apache/airflow/raw/main/providers/amazon/docs/integration-logos/AWS-Cloud-alt_light-bg@4x.png?raw=true" alt="AWS OpenSource" width="130px" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>hao-ai-lab/FastVideo</title>
      <link>https://github.com/hao-ai-lab/FastVideo</link>
      <description>&lt;p&gt;A unified inference and post-training framework for accelerated video generation.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/hao-ai-lab/FastVideo/main/assets/logos/logo.svg?sanitize=true" width="30%" /&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;FastVideo is a unified post-training and inference framework for accelerated video generation.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;FastVideo features an end-to-end unified pipeline for accelerating diffusion models, starting from data preprocessing to model training, finetuning, distillation, and inference. FastVideo is designed to be modular and extensible, allowing users to easily add new optimizations and techniques. Whether it is training-free optimizations or post-training optimizations, FastVideo has you covered.&lt;/p&gt; 
&lt;p align="center"&gt; | üïπÔ∏è &lt;a href="https://fastwan.fastvideo.org/" &lt;b&gt;Online Demo&lt;/a&gt; | &lt;a href="https://hao-ai-lab.github.io/FastVideo"&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://hao-ai-lab.github.io/FastVideo/inference/inference_quick_start.html"&gt;&lt;b&gt; Quick Start&lt;/b&gt;&lt;/a&gt; | ü§ó &lt;a href="https://huggingface.co/collections/FastVideo/fastwan-6886a305d9799c8cd1496408" target="_blank"&gt;&lt;b&gt;FastWan&lt;/b&gt;&lt;/a&gt; | üü£üí¨ &lt;a href="https://join.slack.com/t/fastvideo/shared_invite/zt-3csdw1isz-Euq8_Q8~baewG8hxjXs2gQ" target="_blank"&gt; &lt;b&gt;Slack&lt;/b&gt; &lt;/a&gt; | üü£üí¨ &lt;a href="https://ibb.co/S7HLCSTh" target="_blank"&gt; &lt;b&gt; WeChat &lt;/b&gt; &lt;/a&gt; | &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/hao-ai-lab/FastVideo/main/assets/fastwan.png" width="90%" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;NEWS&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;2025/08/04&lt;/code&gt;: Release &lt;a href="https://hao-ai-lab.github.io/FastVideo/distillation/dmd.html"&gt;FastWan&lt;/a&gt; models and &lt;a href="https://hao-ai-lab.github.io/blogs/fastvideo_post_training/"&gt;Sparse-Distillation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;2025/06/14&lt;/code&gt;: Release finetuning and inference code for &lt;a href="https://arxiv.org/pdf/2505.13389"&gt;VSA&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;2025/04/24&lt;/code&gt;: &lt;a href="https://hao-ai-lab.github.io/blogs/fastvideo/"&gt;FastVideo V1&lt;/a&gt; is released!&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;2025/02/18&lt;/code&gt;: Release the inference code for &lt;a href="https://hao-ai-lab.github.io/blogs/sta/"&gt;Sliding Tile Attention&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;p&gt;FastVideo has the following features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;End-to-end post-training support: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://hao-ai-lab.github.io/blogs/fastvideo_post_training/"&gt;Sparse distillation&lt;/a&gt; for Wan2.1 and Wan2.2 to achineve &amp;gt;50x denoising speedup&lt;/li&gt; 
   &lt;li&gt;Data preprocessing pipeline for video data&lt;/li&gt; 
   &lt;li&gt;Support full finetuning and LoRA finetuning for state-of-the-art open video DiTs&lt;/li&gt; 
   &lt;li&gt;Scalable training with FSDP2, sequence parallelism, and selective activation checkpointing, with near linear scaling to 64 GPUs&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;State-of-the-art performance optimizations for inference 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://arxiv.org/pdf/2505.13389"&gt;Video Sparse Attention&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://arxiv.org/pdf/2502.04507"&gt;Sliding Tile Attention&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://arxiv.org/pdf/2411.19108"&gt;TeaCache&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://arxiv.org/abs/2410.02367"&gt;Sage Attention&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Diverse hardware and OS support 
  &lt;ul&gt; 
   &lt;li&gt;Support H100, A100, 4090&lt;/li&gt; 
   &lt;li&gt;Support Linux, Windows, MacOS&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;We recommend using an environment manager such as &lt;code&gt;Conda&lt;/code&gt; to create a clean environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create and activate a new conda environment
conda create -n fastvideo python=3.12
conda activate fastvideo

# Install FastVideo
pip install fastvideo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please see our &lt;a href="https://hao-ai-lab.github.io/FastVideo/getting_started/installation.html"&gt;docs&lt;/a&gt; for more detailed installation instructions.&lt;/p&gt; 
&lt;h2&gt;Sparse Distillation&lt;/h2&gt; 
&lt;p&gt;For our sparse distillation techniques, please see our &lt;a href="https://hao-ai-lab.github.io/FastVideo/distillation/dmd.html"&gt;distillation docs&lt;/a&gt; and check out our &lt;a href="https://hao-ai-lab.github.io/blogs/fastvideo_post_training/"&gt;blog&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;See below for recipes and datasets:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Model&lt;/th&gt; 
   &lt;th align="center"&gt;Sparse Distillation&lt;/th&gt; 
   &lt;th align="center"&gt;Dataset&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/FastVideo/FastWan2.1-T2V-1.3B-Diffusers"&gt;FastWan2.1-T2V-1.3B&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/hao-ai-lab/FastVideo/tree/main/examples/distill/Wan2.1-T2V/Wan-Syn-Data-480P"&gt;Recipe&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/datasets/FastVideo/Wan-Syn_77x448x832_600k"&gt;FastVideo Synthetic Wan2.1 480P&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/FastVideo/FastWan2.1-T2V-14B-Diffusers"&gt;FastWan2.1-T2V-14B-Preview&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Coming soon!&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/datasets/FastVideo/Wan-Syn_77x768x1280_250k"&gt;FastVideo Synthetic Wan2.1 720P&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/FastVideo/FastWan2.2-TI2V-5B-Diffusers"&gt;FastWan2.2-TI2V-5B&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/hao-ai-lab/FastVideo/tree/main/examples/distill/Wan2.2-TI2V-5B-Diffusers/Data-free"&gt;Recipe&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/datasets/FastVideo/Wan2.2-Syn-121x704x1280_32k"&gt;FastVideo Synthetic Wan2.2 720P&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Inference&lt;/h2&gt; 
&lt;h3&gt;Generating Your First Video&lt;/h3&gt; 
&lt;p&gt;Here's a minimal example to generate a video using the default settings. Make sure VSA kernels are &lt;a href="https://hao-ai-lab.github.io/FastVideo/video_sparse_attention/installation.html"&gt;installed&lt;/a&gt;. Create a file called &lt;code&gt;example.py&lt;/code&gt; with the following code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
from fastvideo import VideoGenerator

def main():
    os.environ["FASTVIDEO_ATTENTION_BACKEND"] = "VIDEO_SPARSE_ATTN"

    # Create a video generator with a pre-trained model
    generator = VideoGenerator.from_pretrained(
        "FastVideo/FastWan2.1-T2V-1.3B-Diffusers",
        num_gpus=1,  # Adjust based on your hardware
    )

    # Define a prompt for your video
    prompt = "A curious raccoon peers through a vibrant field of yellow sunflowers, its eyes wide with interest."

    # Generate the video
    video = generator.generate_video(
        prompt,
        return_frames=True,  # Also return frames from this call (defaults to False)
        output_path="my_videos/",  # Controls where videos are saved
        save_video=True
    )

if __name__ == '__main__':
    main()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run the script with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python example.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For a more detailed guide, please see our &lt;a href="https://hao-ai-lab.github.io/FastVideo/inference/inference_quick_start.html"&gt;inference quick start&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Other docs:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://hao-ai-lab.github.io/FastVideo/design/overview.html"&gt;Design Overview&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://hao-ai-lab.github.io/FastVideo/getting_started/installation.html"&gt;Contribution Guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Distillation and Finetuning&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://hao-ai-lab.github.io/FastVideo/distillation/dmd.html"&gt;Distillation Guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- - [Finetuning Guide](https://hao-ai-lab.github.io/FastVideo/training/finetune.html) --&gt; 
&lt;h2&gt;üìë Development Plan&lt;/h2&gt; 
&lt;!-- - More distillation methods --&gt; 
&lt;!-- - [ ] Add Distribution Matching Distillation --&gt; 
&lt;p&gt;More FastWan Models Coming Soon!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add FastWan2.1-T2V-14B&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add FastWan2.2-T2V-14B&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add FastWan2.2-I2V-14B&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- - Optimization features
- Code updates --&gt; 
&lt;!-- - [ ] fp8 support --&gt; 
&lt;!-- - [ ] faster load model and save model support --&gt; 
&lt;p&gt;See details in &lt;a href="https://github.com/hao-ai-lab/FastVideo/issues/468"&gt;development roadmap&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome all contributions. Please check out our guide &lt;a href="https://hao-ai-lab.github.io/FastVideo/contributing/overview.html"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Acknowledgement&lt;/h2&gt; 
&lt;p&gt;We learned and reused code from the following projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Wan-Video"&gt;Wan-Video&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/HazyResearch/ThunderKittens"&gt;ThunderKittens&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/triton-lang/triton"&gt;Triton&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tianweiy/DMD2"&gt;DMD2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/diffusers"&gt;diffusers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/xdit-project/xDiT"&gt;xDiT&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sgl-project/sglang"&gt;SGLang&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We thank &lt;a href="https://ifm.mbzuai.ac.ae/"&gt;MBZUAI&lt;/a&gt;, &lt;a href="https://www.anyscale.com/"&gt;Anyscale&lt;/a&gt;, and &lt;a href="https://www.gmicloud.ai/"&gt;GMI Cloud&lt;/a&gt; for their support throughout this project.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find FastVideo useful, please considering citing our work:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@software{fastvideo2024,
  title        = {FastVideo: A Unified Framework for Accelerated Video Generation},
  author       = {The FastVideo Team},
  url          = {https://github.com/hao-ai-lab/FastVideo},
  month        = apr,
  year         = {2024},
}

@article{zhang2025vsa,
  title={VSA: Faster Video Diffusion with Trainable Sparse Attention},
  author={Zhang, Peiyuan and Huang, Haofeng and Chen, Yongqi and Lin, Will and Liu, Zhengzhong and Stoica, Ion and Xing, Eric and Zhang, Hao},
  journal={arXiv preprint arXiv:2505.13389},
  year={2025}
}

@article{zhang2025fast,
  title={Fast video generation with sliding tile attention},
  author={Zhang, Peiyuan and Chen, Yongqi and Su, Runlong and Ding, Hangliang and Stoica, Ion and Liu, Zhengzhong and Zhang, Hao},
  journal={arXiv preprint arXiv:2502.04507},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>TheAlgorithms/Python</title>
      <link>https://github.com/TheAlgorithms/Python</link>
      <description>&lt;p&gt;All Algorithms implemented in Python&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;!-- Title: --&gt; 
 &lt;a href="https://github.com/TheAlgorithms/"&gt; &lt;img src="https://raw.githubusercontent.com/TheAlgorithms/website/1cd824df116b27029f17c2d1b42d81731f28a920/public/logo.svg?sanitize=true" height="100" /&gt; &lt;/a&gt; 
 &lt;h1&gt;&lt;a href="https://github.com/TheAlgorithms/"&gt;The Algorithms&lt;/a&gt; - Python&lt;/h1&gt; 
 &lt;!-- Labels: --&gt; 
 &lt;!-- First row: --&gt; 
 &lt;a href="https://gitpod.io/#https://github.com/TheAlgorithms/Python"&gt; &lt;img src="https://img.shields.io/badge/Gitpod-Ready--to--Code-blue?logo=gitpod&amp;amp;style=flat-square" height="20" alt="Gitpod Ready-to-Code" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/TheAlgorithms/Python/raw/master/CONTRIBUTING.md"&gt; &lt;img src="https://img.shields.io/static/v1.svg?label=Contributions&amp;amp;message=Welcome&amp;amp;color=0059b3&amp;amp;style=flat-square" height="20" alt="Contributions Welcome" /&gt; &lt;/a&gt; 
 &lt;img src="https://img.shields.io/github/repo-size/TheAlgorithms/Python.svg?label=Repo%20size&amp;amp;style=flat-square" height="20" /&gt; 
 &lt;a href="https://the-algorithms.com/discord"&gt; &lt;img src="https://img.shields.io/discord/808045925556682782.svg?logo=discord&amp;amp;colorB=7289DA&amp;amp;style=flat-square" height="20" alt="Discord chat" /&gt; &lt;/a&gt; 
 &lt;a href="https://gitter.im/TheAlgorithms/community"&gt; &lt;img src="https://img.shields.io/badge/Chat-Gitter-ff69b4.svg?label=Chat&amp;amp;logo=gitter&amp;amp;style=flat-square" height="20" alt="Gitter chat" /&gt; &lt;/a&gt; 
 &lt;!-- Second row: --&gt; 
 &lt;br /&gt; 
 &lt;a href="https://github.com/TheAlgorithms/Python/actions"&gt; &lt;img src="https://img.shields.io/github/actions/workflow/status/TheAlgorithms/Python/build.yml?branch=master&amp;amp;label=CI&amp;amp;logo=github&amp;amp;style=flat-square" height="20" alt="GitHub Workflow Status" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/pre-commit/pre-commit"&gt; &lt;img src="https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&amp;amp;logoColor=white&amp;amp;style=flat-square" height="20" alt="pre-commit" /&gt; &lt;/a&gt; 
 &lt;a href="https://docs.astral.sh/ruff/formatter/"&gt; &lt;img src="https://img.shields.io/static/v1?label=code%20style&amp;amp;message=ruff&amp;amp;color=black&amp;amp;style=flat-square" height="20" alt="code style: black" /&gt; &lt;/a&gt; 
 &lt;!-- Short description: --&gt; 
 &lt;h3&gt;All algorithms implemented in Python - for education üìö&lt;/h3&gt; 
&lt;/div&gt; 
&lt;p&gt;Implementations are for learning purposes only. They may be less efficient than the implementations in the Python standard library. Use them at your discretion.&lt;/p&gt; 
&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; 
&lt;p&gt;üìã Read through our &lt;a href="https://raw.githubusercontent.com/TheAlgorithms/Python/master/CONTRIBUTING.md"&gt;Contribution Guidelines&lt;/a&gt; before you contribute.&lt;/p&gt; 
&lt;h2&gt;üåê Community Channels&lt;/h2&gt; 
&lt;p&gt;We are on &lt;a href="https://the-algorithms.com/discord"&gt;Discord&lt;/a&gt; and &lt;a href="https://gitter.im/TheAlgorithms/community"&gt;Gitter&lt;/a&gt;! Community channels are a great way for you to ask questions and get help. Please join us!&lt;/p&gt; 
&lt;h2&gt;üìú List of Algorithms&lt;/h2&gt; 
&lt;p&gt;See our &lt;a href="https://raw.githubusercontent.com/TheAlgorithms/Python/master/DIRECTORY.md"&gt;directory&lt;/a&gt; for easier navigation and a better overview of the project.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>