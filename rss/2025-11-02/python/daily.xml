<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Sat, 01 Nov 2025 01:36:28 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>microsoft/agent-lightning</title>
      <link>https://github.com/microsoft/agent-lightning</link>
      <description>&lt;p&gt;The absolute trainer to light up AI agents.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/assets/readme-banner.svg?sanitize=true" alt="Agent-lightning-banner" style="width:600px" /&gt; &lt;/p&gt; 
&lt;h1&gt;Agent Lightningâš¡&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/tests-full.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/tests-full.yml/badge.svg?sanitize=true" alt="Test" /&gt;&lt;/a&gt; &lt;a href="https://microsoft.github.io/agent-lightning/"&gt;&lt;img src="https://img.shields.io/badge/GitHub%20Pages-Documentation-blue" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/agentlightning"&gt;&lt;img src="https://badge.fury.io/py/agentlightning.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/microsoft/agent-lightning/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/RYk7CdvDR7"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join-5865F2?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;The absolute trainer to light up AI agents.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Join our &lt;a href="https://discord.gg/RYk7CdvDR7"&gt;Discord community&lt;/a&gt; to connect with other users and contributors.&lt;/p&gt; 
&lt;h2&gt;âš¡ Core Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Turn your agent into an optimizable beast with &lt;strong&gt;ZERO CODE CHANGE&lt;/strong&gt; (almost)! ğŸ’¤&lt;/li&gt; 
 &lt;li&gt;Build with &lt;strong&gt;ANY&lt;/strong&gt; agent framework (LangChain, OpenAI Agent SDK, AutoGen, CrewAI, Microsoft Agent Framework...); or even WITHOUT agent framework (Python OpenAI). You name it! ğŸ¤–&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Selectively&lt;/strong&gt; optimize one or more agents in a multi-agent system. ğŸ¯&lt;/li&gt; 
 &lt;li&gt;Embraces &lt;strong&gt;Algorithms&lt;/strong&gt; like Reinforcement Learning, Automatic Prompt Optimization, Supervised Fine-tuning and more. ğŸ¤—&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Read more on our &lt;a href="https://microsoft.github.io/agent-lightning/"&gt;documentation website&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/assets/readme-diff.svg?sanitize=true" alt="Agent-Lightning Core Quickstart" style="width:100%" /&gt; &lt;/p&gt; 
&lt;h2&gt;âš¡ Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install agentlightning
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please refer to our &lt;a href="https://microsoft.github.io/agent-lightning/stable/tutorials/installation/"&gt;installation guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;To start using Agent-lightning, check out our &lt;a href="https://microsoft.github.io/agent-lightning/"&gt;documentation&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/microsoft/agent-lightning/main/examples"&gt;examples&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;âš¡ Articles&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;10/22/2025 &lt;a href="https://blog.vllm.ai/2025/10/22/agent-lightning.html"&gt;No More Retokenization Drift: Returning Token IDs via the OpenAI Compatible API Matters in Agent RL&lt;/a&gt; vLLM blog. See also &lt;a href="https://zhuanlan.zhihu.com/p/1965067274642785725"&gt;Zhihu writeup&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;8/11/2025 &lt;a href="https://medium.com/@yugez/training-ai-agents-to-write-and-self-correct-sql-with-reinforcement-learning-571ed31281ad"&gt;Training AI Agents to Write and Self-correct SQL with Reinforcement Learning&lt;/a&gt; Medium.&lt;/li&gt; 
 &lt;li&gt;8/5/2025 &lt;a href="https://arxiv.org/abs/2508.03680"&gt;Agent Lightning: Train ANY AI Agents with Reinforcement Learning&lt;/a&gt; arXiv paper.&lt;/li&gt; 
 &lt;li&gt;7/26/2025 &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1m9m670/we_discovered_an_approach_to_train_any_ai_agent/"&gt;We discovered an approach to train any AI agent with RL, with (almost) zero code changes.&lt;/a&gt; Reddit.&lt;/li&gt; 
 &lt;li&gt;6/6/2025 &lt;a href="https://www.microsoft.com/en-us/research/project/agent-lightning/"&gt;Agent Lightning - Microsoft Research&lt;/a&gt; Project page.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;âš¡ Community Projects&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/af-74413592/DeepWerewolf"&gt;DeepWerewolf&lt;/a&gt; â€” A case study of agent RL training for the Chinese Werewolf game built with AgentScope and Agent Lightning.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://agentflow.stanford.edu/"&gt;AgentFlow&lt;/a&gt; â€” A modular multi-agent framework that combines planner, executor, verifier, and generator agents with the Flow-GRPO algorithm to tackle long-horizon, sparse-reward tasks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;âš¡ Architecture&lt;/h2&gt; 
&lt;p&gt;Agent Lightning keeps the moving parts to a minimum so you can focus on your idea, not the plumbing. Your agent continues to run as usual; you can still use any agent framework you like; you drop in the lightweight &lt;code&gt;agl.emit_xxx()&lt;/code&gt; helper, or let the tracer collect every prompt, tool call, and reward. Those events become structured spans that flow into the LightningStore, a central hub that keeps tasks, resources, and traces in sync.&lt;/p&gt; 
&lt;p&gt;On the other side of the store sits the algorithm you choose, or write yourself. The algorithm reads spans, learns from them, and posts updated resources such as refined prompt templates or new policy weights. The Trainer ties it all together: it streams datasets to runners, ferries resources between the store and the algorithm, and updates the inference engine when improvements land. You can either stop there, or simply let the same loop keep turning.&lt;/p&gt; 
&lt;p&gt;No rewrites, no lock-in, just a clear path from first rollout to steady improvement.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/assets/readme-architecture.svg?sanitize=true" alt="Agent-lightning Architecture" style="width:100%" /&gt; &lt;/p&gt; 
&lt;h2&gt;âš¡ CI Status&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Workflow&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CPU Tests&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/tests.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/tests.yml/badge.svg?sanitize=true" alt="tests workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GPU Tests&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/tests-full.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/tests-full.yml/badge.svg?sanitize=true" alt="tests-full workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Examples Integration&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/badge-examples.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/badge-examples.yml/badge.svg?sanitize=true" alt="examples summary workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Latest Dependency Compatibility&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/badge-latest.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/badge-latest.yml/badge.svg?sanitize=true" alt="latest summary workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Legacy Examples Compatibility&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/examples-compat.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/examples-compat.yml/badge.svg?sanitize=true" alt="examples compatibility workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;âš¡ Citation&lt;/h2&gt; 
&lt;p&gt;If you find Agent Lightning useful in your research or projects, please cite our paper:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{luo2025agentlightningtrainai,
      title={Agent Lightning: Train ANY AI Agents with Reinforcement Learning},
      author={Xufang Luo and Yuge Zhang and Zhiyuan He and Zilong Wang and Siyun Zhao and Dongsheng Li and Luna K. Qiu and Yuqing Yang},
      year={2025},
      eprint={2508.03680},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2508.03680},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;âš¡ Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h2&gt;âš¡ Trademarks&lt;/h2&gt; 
&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general"&gt;Microsoft's Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.&lt;/p&gt; 
&lt;h2&gt;âš¡ Responsible AI&lt;/h2&gt; 
&lt;p&gt;This project has been evaluated and certified to comply with the Microsoft Responsible AI Standard. The team will continue to monitor and maintain the repository, addressing any severe issues, including potential harms, if they arise.&lt;/p&gt; 
&lt;h2&gt;âš¡ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License. See the &lt;a href="https://raw.githubusercontent.com/microsoft/agent-lightning/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>pixeltable/pixeltable</title>
      <link>https://github.com/pixeltable/pixeltable</link>
      <description>&lt;p&gt;Pixeltable â€” Data Infrastructure providing a declarative, incremental approach for multimodal AI workloads.&lt;/p&gt;&lt;hr&gt;&lt;picture class="github-only"&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="https://github.com/user-attachments/assets/e9bf82b2-cace-4bd8-9523-b65495eb8131" /&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/user-attachments/assets/c5ab123e-806c-49bf-93e7-151353719b16" /&gt; 
 &lt;img alt="Pixeltable Logo" src="https://github.com/user-attachments/assets/e9bf82b2-cace-4bd8-9523-b65495eb8131" width="40%" /&gt; 
&lt;/picture&gt; 
&lt;div&gt; 
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;p&gt;The only open source Python library providing declarative data infrastructure for building multimodal AI applications, enabling incremental storage, transformation, indexing, retrieval, and orchestration of data.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/License-Apache%202.0-0530AD.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/pixeltable/pixeltable/actions/workflows/pytest.yml"&gt;&lt;img src="https://github.com/pixeltable/pixeltable/actions/workflows/pytest.yml/badge.svg?sanitize=true" alt="tests status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/pixeltable/pixeltable/actions/workflows/nightly.yml"&gt;&lt;img src="https://github.com/pixeltable/pixeltable/actions/workflows/nightly.yml/badge.svg?sanitize=true" alt="nightly status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/pixeltable/pixeltable/actions/workflows/stress-tests.yml"&gt;&lt;img src="https://github.com/pixeltable/pixeltable/actions/workflows/stress-tests.yml/badge.svg?sanitize=true" alt="stress-tests status" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/pixeltable/"&gt;&lt;img src="https://img.shields.io/pypi/v/pixeltable?color=4D148C" alt="PyPI Package" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/QPyqFYx2UN"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%92%AC-Discord-%235865F2.svg?sanitize=true" alt="My Discord (1306431018890166272)" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.pixeltable.com/overview/quick-start"&gt;&lt;strong&gt;Quick Start&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://docs.pixeltable.com/"&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://pixeltable.github.io/pixeltable/"&gt;&lt;strong&gt;API Reference&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://github.com/pixeltable/pixeltable/tree/main/docs/sample-apps"&gt;&lt;strong&gt;Sample Apps&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://discord.gg/QPyqFYx2UN"&gt;&lt;strong&gt;Discord Community&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;pip install pixeltable
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Pixeltable replaces the complex multi-system architecture typically needed for AI applications (databases, file storage, vector DBs, APIs, orchestration) with a single declarative table interface that natively handles multimodal data like images, videos, and documents.&lt;/p&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/b50fd6df-5169-4881-9dbe-1b6e5d06cede"&gt;https://github.com/user-attachments/assets/b50fd6df-5169-4881-9dbe-1b6e5d06cede&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;With Pixeltable, you define your &lt;em&gt;entire&lt;/em&gt; data processing and AI workflow declaratively using &lt;strong&gt;&lt;a href="https://docs.pixeltable.com/datastore/computed-columns"&gt;computed columns&lt;/a&gt;&lt;/strong&gt; on &lt;strong&gt;&lt;a href="https://docs.pixeltable.com/datastore/tables-and-operations"&gt;tables&lt;/a&gt;&lt;/strong&gt;. Focus on your application logic, not the data plumbing.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;
# Installation
pip install -qU torch transformers openai pixeltable

# Basic setup
import pixeltable as pxt

# Table with multimodal column types (Image, Video, Audio, Document)
t = pxt.create_table('images', {'input_image': pxt.Image})

# Computed columns: define transformation logic once, runs on all data
from pixeltable.functions import huggingface

# Object detection with automatic model management
t.add_computed_column(
    detections=huggingface.detr_for_object_detection(
        t.input_image,
        model_id='facebook/detr-resnet-50'
    )
)

# Extract specific fields from detection results
t.add_computed_column(detections_text=t.detections.label_text)

# OpenAI Vision API integration with built-in rate limiting and async management
from pixeltable.functions import openai

t.add_computed_column(
    vision=openai.vision(
        prompt="Describe what's in this image.",
        image=t.input_image,
        model='gpt-4o-mini'
    )
)

# Insert data directly from an external URL
# Automatically triggers computation of all computed columns
t.insert(input_image='https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000025.jpg')

# Query - All data, metadata, and computed results are persistently stored
# Structured and unstructured data are returned side-by-side
results = t.select(
    t.input_image,
    t.detections_text,
    t.vision
).collect()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;What Happened?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Data Ingestion &amp;amp; Storage:&lt;/strong&gt; References &lt;a href="https://docs.pixeltable.com/datastore/bringing-data"&gt;files&lt;/a&gt; (images, videos, audio, docs) in place, handles structured data.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Transformation &amp;amp; Processing:&lt;/strong&gt; Applies &lt;em&gt;any&lt;/em&gt; Python function (&lt;a href="https://docs.pixeltable.com/datastore/custom-functions"&gt;UDFs&lt;/a&gt;) or built-in operations (&lt;a href="https://docs.pixeltable.com/datastore/iterators"&gt;chunking, frame extraction&lt;/a&gt;) automatically.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AI Model Integration:&lt;/strong&gt; Runs inference (&lt;a href="https://docs.pixeltable.com/datastore/vector-database"&gt;embeddings&lt;/a&gt;, &lt;a href="https://docs.pixeltable.com/examples/vision/yolox"&gt;object detection&lt;/a&gt;, &lt;a href="https://docs.pixeltable.com/integrations/frameworks#cloud-llm-providers"&gt;LLMs&lt;/a&gt;) as part of the data pipeline.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Indexing &amp;amp; Retrieval:&lt;/strong&gt; Creates and manages vector indexes for fast &lt;a href="https://docs.pixeltable.com/datastore/vector-database#phase-3%3A-query"&gt;semantic search&lt;/a&gt; alongside traditional filtering.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Incremental Computation:&lt;/strong&gt; Only &lt;a href="https://docs.pixeltable.com/overview/quick-start"&gt;recomputes&lt;/a&gt; what's necessary when data or code changes, saving time and cost.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Versioning &amp;amp; Lineage:&lt;/strong&gt; Automatically tracks data and schema changes for reproducibility. See below for an example that uses "time travel" to query an older version of a table.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Pixeltable can ingest data from local storage or directly from a URL. When external media files are referenced by URL, as in the &lt;code&gt;insert&lt;/code&gt; statement above, Pixeltable caches them locally before processing. See the &lt;a href="https://github.com/pixeltable/pixeltable/raw/main/docs/notebooks/feature-guides/working-with-external-files.ipynb"&gt;Working with External Files&lt;/a&gt; notebook for more details.&lt;/p&gt; 
&lt;h2&gt;Where Did My Data Go?&lt;/h2&gt; 
&lt;p&gt;Pixeltable workloads generate various outputs, including both structured outputs (such as bounding boxes for detected objects) and/or unstructured outputs (such as generated images or video). By default, everything resides in your Pixeltable user directory at &lt;code&gt;~/.pixeltable&lt;/code&gt;. Structured data is stored in a Postgres instance in &lt;code&gt;~/.pixeltable&lt;/code&gt;. Generated media (images, video, audio, documents) are stored outside the Postgres database, in separate flat files in &lt;code&gt;~/.pixeltable/media&lt;/code&gt;. Those media files are referenced by URL in the database, and Pixeltable provides the "glue" for a unified table interface over both structured and unstructured data.&lt;/p&gt; 
&lt;p&gt;In general, the user is not expected to interact directly with the data in &lt;code&gt;~/.pixeltable&lt;/code&gt;; the data store is fully managed by Pixeltable and is intended to be accessed through the Pixeltable Python SDK.&lt;/p&gt; 
&lt;h2&gt;Key Principles&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.pixeltable.com/datastore/tables-and-operations"&gt;Unified Multimodal Interface:&lt;/a&gt;&lt;/strong&gt; &lt;code&gt;pxt.Image&lt;/code&gt;, &lt;code&gt;pxt.Video&lt;/code&gt;, &lt;code&gt;pxt.Audio&lt;/code&gt;, &lt;code&gt;pxt.Document&lt;/code&gt;, etc. â€“ manage diverse data consistently.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;t = pxt.create_table(
   'media',
   {
       'img': pxt.Image,
       'video': pxt.Video
   }
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.pixeltable.com/datastore/computed-columns"&gt;Declarative Computed Columns:&lt;/a&gt;&lt;/strong&gt; Define processing steps once; they run automatically on new/updated data.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;t.add_computed_column(
   classification=huggingface.vit_for_image_classification(
       t.image
   )
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.pixeltable.com/datastore/vector-database"&gt;Built-in Vector Search:&lt;/a&gt;&lt;/strong&gt; Add embedding indexes and perform similarity searches directly on tables/views.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;t.add_embedding_index(
   'img',
   embedding=clip.using(
       model_id='openai/clip-vit-base-patch32'
   )
)

sim = t.img.similarity("cat playing with yarn")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.pixeltable.com/datastore/views"&gt;Incremental View Maintenance:&lt;/a&gt;&lt;/strong&gt; Create virtual tables using iterators for efficient processing without data duplication.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Document chunking with overlap &amp;amp; metadata and many more options to build your own iterator
chunks = pxt.create_view('chunks', docs,
   iterator=DocumentSplitter.create(
       document=docs.doc, 
       separators='sentence,token_limit',
       overlap=50, limit=500
   ))

# Video frame extraction  
frames = pxt.create_view('frames', videos,
   iterator=FrameIterator.create(video=videos.video, fps=0.5))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.pixeltable.com/integrations/frameworks"&gt;Seamless AI Integration:&lt;/a&gt;&lt;/strong&gt; Built-in functions for OpenAI, Anthropic, Hugging Face, CLIP, YOLOX, and more.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# LLM integration (OpenAI, Anthropic, etc.)
t.add_computed_column(
   response=openai.chat_completions(
       messages=[{"role": "user", "content": t.prompt}], model='gpt-4o-mini'
   )
)

# Computer vision (YOLOX object detection)
t.add_computed_column(
   detections=yolox(t.image, model_id='yolox_s', threshold=0.5)
)

# Embedding models (Hugging Face, CLIP)
t.add_computed_column(
   embeddings=huggingface.sentence_transformer(
       t.text, model_id='all-MiniLM-L6-v2'
   )
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.pixeltable.com/datastore/custom-functions"&gt;Bring Your Own Code:&lt;/a&gt;&lt;/strong&gt; Extend Pixeltable with UDFs, batch processing, and custom aggregators.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@pxt.udf
def format_prompt(context: list, question: str) -&amp;gt; str:
   return f"Context: {context}\nQuestion: {question}"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.pixeltable.com/examples/chat/tools"&gt;Agentic Workflows / Tool Calling:&lt;/a&gt;&lt;/strong&gt; Register &lt;code&gt;@pxt.udf&lt;/code&gt;, &lt;code&gt;@pxt.query&lt;/code&gt; functions, or &lt;strong&gt;MCP tools&lt;/strong&gt; as tools.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Example tools: UDFs, Query functions, and MCP tools
mcp_tools = pxt.mcp_udfs('http://localhost:8000/mcp')  # Load from MCP server
tools = pxt.tools(get_weather_udf, search_context_query, *mcp_tools)

# LLM decides which tool to call; Pixeltable executes it
t.add_computed_column(
   tool_output=invoke_tools(tools, t.llm_tool_choice)
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.pixeltable.com/datastore/tables-and-operations#data-operations"&gt;Data Persistence:&lt;/a&gt;&lt;/strong&gt; All data, metadata, and computed results are automatically stored and versioned.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;t = pxt.get_table('my_table')  # Get a handle to an existing table
t.select(t.account, t.balance).collect()  # Query its contents
t.revert()  # Undo the last modification to the table and restore its previous state
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.pixeltable.com/datastore/tables-and-operations#data-operations"&gt;Time Travel:&lt;/a&gt;&lt;/strong&gt; By default, Pixeltable preserves the full change history of each table, and any prior version can be selected and queried.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;t.history()  # Display a human-readable list of all prior versions of the table
old_version = pxt.get_table('my_table:472')  # Get a handle to a specific table version
old_version.select(t.account, t.balance).collect()  # Query the older version
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.pixeltable.com/datastore/filtering-and-selecting"&gt;SQL-like Python Querying:&lt;/a&gt;&lt;/strong&gt; Familiar syntax combined with powerful AI capabilities.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;results = (
   t.where(t.score &amp;gt; 0.8)
   .order_by(t.timestamp)
   .select(t.image, score=t.score)
   .limit(10)
   .collect()
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://pixeltable.github.io/pixeltable/pixeltable/io/"&gt;I/O &amp;amp; Integration:&lt;/a&gt;&lt;/strong&gt; Export to multiple formats and integrate with ML/AI tools ecosystem.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Export to analytics/ML formats  
pxt.export_parquet(table, 'data.parquet', partition_size_bytes=100_000_000)
pxt.export_lancedb(table, 'vector_db')

# DataFrame conversions
results = table.select(table.image, table.labels).collect()
df = results.to_pandas()                           # â†’ pandas DataFrame  
models = results.to_pydantic(MyModel)              # â†’ Pydantic models

# Specialized ML dataset formats
coco_path = table.to_coco_dataset()                # â†’ COCO annotations
pytorch_ds = table.to_pytorch_dataset('pt')        # â†’ PyTorch DataLoader ready

# ML tool integrations  
pxt.create_label_studio_project(table, label_config)  # Annotation
pxt.export_images_as_fo_dataset(table, table.image)   # FiftyOne
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Key Examples&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;(See the &lt;a href="https://docs.pixeltable.com/overview/quick-start"&gt;Full Quick Start&lt;/a&gt; or &lt;a href="https://raw.githubusercontent.com/pixeltable/pixeltable/main/#notebook-gallery"&gt;Notebook Gallery&lt;/a&gt; for more details)&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1. Multimodal Data Store and Data Transformation (Computed Column):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install pixeltable
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pixeltable as pxt

# Create a table
t = pxt.create_table(
    'films',
    {'name': pxt.String, 'revenue': pxt.Float, 'budget': pxt.Float},
    if_exists="replace"
)

t.insert([
    {'name': 'Inside Out', 'revenue': 800.5, 'budget': 200.0},
    {'name': 'Toy Story', 'revenue': 1073.4, 'budget': 200.0}
])

# Add a computed column for profit - runs automatically!
t.add_computed_column(profit=(t.revenue - t.budget), if_exists="replace")

# Query the results
print(t.select(t.name, t.profit).collect())
# Output includes the automatically computed 'profit' column
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;2. Object Detection with &lt;a href="https://github.com/pixeltable/pixeltable-yolox"&gt;YOLOX&lt;/a&gt;:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install pixeltable pixeltable-yolox
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import PIL
import pixeltable as pxt
from yolox.models import Yolox
from yolox.data.datasets import COCO_CLASSES

t = pxt.create_table('image', {'image': pxt.Image}, if_exists='replace')

# Insert some images
prefix = 'https://upload.wikimedia.org/wikipedia/commons'
paths = [
    '/1/15/Cat_August_2010-4.jpg',
    '/e/e1/Example_of_a_Dog.jpg',
    '/thumb/b/bf/Bird_Diversity_2013.png/300px-Bird_Diversity_2013.png'
]
t.insert({'image': prefix + p} for p in paths)

@pxt.udf
def detect(image: PIL.Image.Image) -&amp;gt; list[str]:
    model = Yolox.from_pretrained("yolox_s")
    result = model([image])
    coco_labels = [COCO_CLASSES[label] for label in result[0]["labels"]]
    return coco_labels

t.add_computed_column(classification=detect(t.image))

print(t.select().collect())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;3. Image Similarity Search (CLIP Embedding Index):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install pixeltable sentence-transformers
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pixeltable as pxt
from pixeltable.functions.huggingface import clip

# Create image table and add sample images
images = pxt.create_table('my_images', {'img': pxt.Image}, if_exists='replace')
images.insert([
    {'img': 'https://upload.wikimedia.org/wikipedia/commons/thumb/6/68/Orange_tabby_cat_sitting_on_fallen_leaves-Hisashi-01A.jpg/1920px-Orange_tabby_cat_sitting_on_fallen_leaves-Hisashi-01A.jpg'},
    {'img': 'https://upload.wikimedia.org/wikipedia/commons/d/d5/Retriever_in_water.jpg'}
])

# Add CLIP embedding index for similarity search
images.add_embedding_index(
    'img',
    embedding=clip.using(model_id='openai/clip-vit-base-patch32')
)

# Text-based image search
query_text = "a dog playing fetch"
sim_text = images.img.similarity(query_text)
results_text = images.order_by(sim_text, asc=False).limit(3).select(
    image=images.img, similarity=sim_text
).collect()
print("--- Text Query Results ---")
print(results_text)

# Image-based image search
query_image_url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Huskiesatrest.jpg/2880px-Huskiesatrest.jpg'
sim_image = images.img.similarity(query_image_url)
results_image = images.order_by(sim_image, asc=False).limit(3).select(
    image=images.img, similarity=sim_image
).collect()
print("--- Image URL Query Results ---")
print(results_image)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;4. Multimodal/Incremental RAG Workflow (Document Chunking &amp;amp; LLM Call):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install pixeltable openai spacy sentence-transformers
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m spacy download en_core_web_sm
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pixeltable as pxt
import pixeltable.functions as pxtf
from pixeltable.functions import openai, huggingface
from pixeltable.iterators import DocumentSplitter

# Manage your tables by directories
directory = "my_docs"
pxt.drop_dir(directory, if_not_exists="ignore", force=True)
pxt.create_dir("my_docs")

# Create a document table and add a PDF
docs = pxt.create_table(f'{directory}.docs', {'doc': pxt.Document})
docs.insert([{'doc': 'https://github.com/pixeltable/pixeltable/raw/release/docs/resources/rag-demo/Jefferson-Amazon.pdf'}])

# Create chunks view with sentence-based splitting
chunks = pxt.create_view(
    'doc_chunks',
    docs,
    iterator=DocumentSplitter.create(document=docs.doc, separators='sentence')
)

# Explicitly create the embedding function object
embed_model = huggingface.sentence_transformer.using(model_id='all-MiniLM-L6-v2')
# Add embedding index using the function object
chunks.add_embedding_index('text', string_embed=embed_model)

# Define query function for retrieval - Returns a DataFrame expression
@pxt.query
def get_relevant_context(query_text: str, limit: int = 3):
    sim = chunks.text.similarity(query_text)
    # Return a list of strings (text of relevant chunks)
    return chunks.order_by(sim, asc=False).limit(limit).select(chunks.text)

# Build a simple Q&amp;amp;A table
qa = pxt.create_table(f'{directory}.qa_system', {'prompt': pxt.String})

# 1. Add retrieved context (now a list of strings)
qa.add_computed_column(context=get_relevant_context(qa.prompt))

# 2. Format the prompt with context
qa.add_computed_column(
    final_prompt=pxtf.string.format(
        """
        PASSAGES:
        {0}

        QUESTION:
        {1}
        """,
        qa.context,
        qa.prompt
    )
)

# 4. Generate the answer using the well-formatted prompt column
qa.add_computed_column(
    answer=openai.chat_completions(
        model='gpt-4o-mini',
        messages=[{
            'role': 'user',
            'content': qa.final_prompt
        }]
    ).choices[0].message.content
)

# Ask a question and get the answer
qa.insert([{'prompt': 'What can you tell me about Amazon?'}])
print("--- Final Answer ---")
print(qa.select(qa.answer).collect())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Notebook Gallery&lt;/h2&gt; 
&lt;p&gt;Explore Pixeltable's capabilities interactively:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Topic&lt;/th&gt; 
   &lt;th align="left"&gt;Notebook&lt;/th&gt; 
   &lt;th align="left"&gt;Topic&lt;/th&gt; 
   &lt;th align="center"&gt;Notebook&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;Fundamentals&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;Integrations&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;10-Min Tour&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a target="_blank" href="https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/pixeltable-basics.ipynb"&gt; &lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt; &lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;OpenAI&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a target="_blank" href="https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-openai.ipynb"&gt; &lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt; &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Tables &amp;amp; Ops&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a target="_blank" href="https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/fundamentals/tables-and-data-operations.ipynb"&gt; &lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt; &lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Anthropic&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a target="_blank" href="https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-anthropic.ipynb"&gt; &lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt; &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;UDFs&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a target="_blank" href="https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/udfs-in-pixeltable.ipynb"&gt; &lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt; &lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Together AI&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a target="_blank" href="https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-together.ipynb"&gt; &lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt; &lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Embedding Index&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a target="_blank" href="https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/embedding-indexes.ipynb"&gt; &lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt; &lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Label Studio&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a target="_blank" href="https://docs.pixeltable.com/examples/vision/label-studio"&gt; &lt;img src="https://img.shields.io/badge/ğŸ“š%20Docs-013056" alt="Visit Docs" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;External Files&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a target="_blank" href="https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/working-with-external-files.ipynb"&gt; &lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt; &lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Mistral&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a target="_blank" href="https://colab.research.google.com/github/mistralai/cookbook/blob/main/third_party/Pixeltable/incremental_prompt_engineering_and_model_comparison.ipynb"&gt; &lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Github" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;Use Cases&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;Sample Apps&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;RAG Demo&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a target="_blank" href="https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/use-cases/rag-demo.ipynb"&gt; &lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Multimodal Agent&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a target="_blank" href="https://huggingface.co/spaces/Pixeltable/Multimodal-Powerhouse"&gt; &lt;img src="https://img.shields.io/badge/ğŸ¤—%20Demo-FF7D04" alt="HF Space" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Object Detection&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a target="_blank" href="https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/use-cases/object-detection-in-videos.ipynb"&gt; &lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt; &lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Image/Text Search&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a target="_blank" href="https://github.com/pixeltable/pixeltable/tree/main/docs/sample-apps/text-and-image-similarity-search-nextjs-fastapi"&gt; &lt;img src="https://img.shields.io/badge/ğŸ–¥ï¸%20App-black.svg" alt="GitHub App" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Audio Transcription&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a target="_blank" href="https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/use-cases/audio-transcriptions.ipynb"&gt; &lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Discord Bot&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a target="_blank" href="https://github.com/pixeltable/pixeltable/raw/main/docs/sample-apps/context-aware-discord-bot"&gt; &lt;img src="https://img.shields.io/badge/%F0%9F%92%AC%20Bot-%235865F2.svg?sanitize=true" alt="GitHub App" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Maintaining Production-Ready Multimodal AI Apps is Still Too Hard&lt;/h2&gt; 
&lt;p&gt;Building robust AI applications, especially &lt;a href="https://docs.pixeltable.com/datastore/bringing-data"&gt;multimodal&lt;/a&gt; ones, requires stitching together numerous tools:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ETL pipelines for data loading and transformation.&lt;/li&gt; 
 &lt;li&gt;Vector databases for semantic search.&lt;/li&gt; 
 &lt;li&gt;Feature stores for ML models.&lt;/li&gt; 
 &lt;li&gt;Orchestrators for scheduling.&lt;/li&gt; 
 &lt;li&gt;Model serving infrastructure for inference.&lt;/li&gt; 
 &lt;li&gt;Separate systems for parallelization, caching, versioning, and lineage tracking.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This complex "data plumbing" slows down development, increases costs, and makes applications brittle and hard to reproduce.&lt;/p&gt; 
&lt;h2&gt;Roadmap (2025)&lt;/h2&gt; 
&lt;h3&gt;Cloud Infrastructure and Deployment&lt;/h3&gt; 
&lt;p&gt;We're working on a hosted Pixeltable service that will:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Enable Multimodal Data Sharing of Pixeltable Tables and Views | &lt;a href="https://www.pixeltable.com/waitlist"&gt;Waitlist&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Provide a persistent cloud instance&lt;/li&gt; 
 &lt;li&gt;Turn Pixeltable workflows (Tables, Queries, UDFs) into API endpoints/&lt;a href="https://github.com/pixeltable/pixeltable-mcp-server"&gt;MCP Servers&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We love contributions! Whether it's reporting bugs, suggesting features, improving documentation, or submitting code changes, please check out our &lt;a href="https://raw.githubusercontent.com/pixeltable/pixeltable/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; and join the &lt;a href="https://github.com/pixeltable/pixeltable/discussions"&gt;Discussions&lt;/a&gt; or our &lt;a href="https://discord.gg/QPyqFYx2UN"&gt;Discord Server&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Pixeltable is licensed under the Apache 2.0 License.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>hanxi/xiaomusic</title>
      <link>https://github.com/hanxi/xiaomusic</link>
      <description>&lt;p&gt;ä½¿ç”¨å°çˆ±éŸ³ç®±æ’­æ”¾éŸ³ä¹ï¼ŒéŸ³ä¹ä½¿ç”¨ yt-dlp ä¸‹è½½ã€‚&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;XiaoMusic: æ— é™å¬æ­Œï¼Œè§£æ”¾å°çˆ±éŸ³ç®±&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/hanxi/xiaomusic"&gt;&lt;img src="https://img.shields.io/github/license/hanxi/xiaomusic" alt="GitHub License" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/hanxi/xiaomusic"&gt;&lt;img src="https://img.shields.io/docker/v/hanxi/xiaomusic?sort=semver&amp;amp;label=docker%20image" alt="Docker Image Version" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/hanxi/xiaomusic"&gt;&lt;img src="https://img.shields.io/docker/pulls/hanxi/xiaomusic" alt="Docker Pulls" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/xiaomusic/"&gt;&lt;img src="https://img.shields.io/pypi/v/xiaomusic" alt="PyPI - Version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/xiaomusic/"&gt;&lt;img src="https://img.shields.io/pypi/dm/xiaomusic" alt="PyPI - Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/xiaomusic/"&gt;&lt;img src="https://img.shields.io/python/required-version-toml?tomlFilePath=https%3A%2F%2Fraw.githubusercontent.com%2Fhanxi%2Fxiaomusic%2Fmain%2Fpyproject.toml" alt="Python Version from PEP 621 TOML" /&gt;&lt;/a&gt; &lt;a href="https://github.com/hanxi/xiaomusic/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/hanxi/xiaomusic" alt="GitHub Release" /&gt;&lt;/a&gt; &lt;a href="https://visitorbadge.io/status?path=hanxi%2Fxiaomusic"&gt;&lt;img src="https://api.visitorbadge.io/api/daily?path=hanxi%2Fxiaomusic&amp;amp;label=daily%20visitor&amp;amp;countColor=%232ccce4&amp;amp;style=flat" alt="Visitors" /&gt;&lt;/a&gt; &lt;a href="https://visitorbadge.io/status?path=hanxi%2Fxiaomusic"&gt;&lt;img src="https://api.visitorbadge.io/api/visitors?path=hanxi%2Fxiaomusic&amp;amp;label=total%20visitor&amp;amp;countColor=%232ccce4&amp;amp;style=flat" alt="Visitors" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;ä½¿ç”¨å°çˆ±éŸ³ç®±æ’­æ”¾éŸ³ä¹ï¼ŒéŸ³ä¹ä½¿ç”¨ yt-dlp ä¸‹è½½ã€‚&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/hanxi/xiaomusic"&gt;https://github.com/hanxi/xiaomusic&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;æ–‡æ¡£: &lt;a href="https://xdocs.hanxi.cc/"&gt;https://xdocs.hanxi.cc/&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] åˆæ¬¡å®‰è£…é‡åˆ°é—®é¢˜è¯·æŸ¥é˜… &lt;a href="https://github.com/hanxi/xiaomusic/issues/99"&gt;ğŸ’¬ FAQé—®é¢˜é›†åˆ&lt;/a&gt; ï¼Œä¸€èˆ¬é‡åˆ°çš„é—®é¢˜éƒ½å·²ç»æœ‰è§£å†³åŠæ³•ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ‘‹ æœ€ç®€é…ç½®è¿è¡Œ&lt;/h2&gt; 
&lt;p&gt;å·²ç»æ”¯æŒåœ¨ web é¡µé¢é…ç½®å…¶ä»–å‚æ•°ï¼Œdocker å¯åŠ¨å‘½ä»¤å¦‚ä¸‹:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -p 58090:8090 -e XIAOMUSIC_PUBLIC_PORT=58090 -v /xiaomusic_music:/app/music -v /xiaomusic_conf:/app/conf hanxi/xiaomusic
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ğŸ”¥ å›½å†…ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -p 58090:8090 -e XIAOMUSIC_PUBLIC_PORT=58090 -v /xiaomusic_music:/app/music -v /xiaomusic_conf:/app/conf docker.hanxi.cc/hanxi/xiaomusic
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;å¯¹åº”çš„ docker compose é…ç½®å¦‚ä¸‹ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;services:
  xiaomusic:
    image: hanxi/xiaomusic
    container_name: xiaomusic
    restart: unless-stopped
    ports:
      - 58090:8090
    environment:
      XIAOMUSIC_PUBLIC_PORT: 58090
    volumes:
      - /xiaomusic_music:/app/music
      - /xiaomusic_conf:/app/conf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ğŸ”¥ å›½å†…ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;services:
  xiaomusic:
    image: docker.hanxi.cc/hanxi/xiaomusic
    container_name: xiaomusic
    restart: unless-stopped
    ports:
      - 58090:8090
    environment:
      XIAOMUSIC_PUBLIC_PORT: 58090
    volumes:
      - /xiaomusic_music:/app/music
      - /xiaomusic_conf:/app/conf
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;å…¶ä¸­ conf ç›®å½•ä¸ºé…ç½®æ–‡ä»¶å­˜æ”¾ç›®å½•ï¼Œmusic ç›®å½•ä¸ºéŸ³ä¹å­˜æ”¾ç›®å½•ï¼Œå»ºè®®åˆ†å¼€é…ç½®ä¸ºä¸åŒçš„ç›®å½•ã€‚&lt;/li&gt; 
 &lt;li&gt;/xiaomusic_music å’Œ /xiaomusic_conf æ˜¯ docker æ‰€åœ¨çš„ä¸»æœºçš„ç›®å½•ï¼Œå¯ä»¥ä¿®æ”¹ä¸ºå…¶ä»–ç›®å½•ã€‚å¦‚æœæŠ¥é”™æ‰¾ä¸åˆ° /xiaomusic_music ç›®å½•ï¼Œå¯ä»¥å…ˆæ‰§è¡Œ &lt;code&gt;mkdir -p /xiaomusic_{music,conf}&lt;/code&gt; å‘½ä»¤æ–°å»ºç›®å½•ã€‚&lt;/li&gt; 
 &lt;li&gt;/app/music å’Œ /app/conf æ˜¯ docker å®¹å™¨é‡Œçš„ç›®å½•ï¼Œä¸è¦å»ä¿®æ”¹ã€‚&lt;/li&gt; 
 &lt;li&gt;XIAOMUSIC_PUBLIC_PORT æ˜¯ç”¨æ¥é…ç½® NAS æœ¬åœ°ç«¯å£çš„ã€‚8090 æ˜¯å®¹å™¨ç«¯å£ï¼Œä¸è¦å»ä¿®æ”¹ã€‚&lt;/li&gt; 
 &lt;li&gt;åå°è®¿é—®åœ°å€ä¸ºï¼š http://NAS_IP:58090&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] docker å’Œ docker compose äºŒé€‰ä¸€å³å¯ï¼Œå¯åŠ¨æˆåŠŸåï¼Œåœ¨ web é¡µé¢å¯ä»¥é…ç½®å…¶ä»–å‚æ•°ï¼Œå¸¦æœ‰ &lt;code&gt;*&lt;/code&gt; å·çš„é…ç½®æ˜¯å¿…é¡»è¦é…ç½®çš„ï¼Œå…¶ä»–çš„ç”¨ä¸ä¸Šæ—¶ä¸ç”¨ä¿®æ”¹ã€‚åˆæ¬¡é…ç½®æ—¶éœ€è¦åœ¨é¡µé¢ä¸Šè¾“å…¥å°ç±³è´¦å·å’Œå¯†ç ä¿å­˜åæ‰èƒ½è·å–åˆ°è®¾å¤‡åˆ—è¡¨ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] ç›®å‰å®‰è£…æ­¥éª¤å·²ç»æ˜¯æœ€ç®€åŒ–äº†ï¼Œå¦‚æœè¿˜æ˜¯å«Œå®‰è£…éº»çƒ¦ï¼Œå¯ä»¥å¾®ä¿¡æˆ–è€… QQ çº¦æˆ‘è¿œç¨‹å®‰è£…ï¼Œæˆ‘ä¸€èˆ¬å‘¨æœ«å’Œæ™šä¸Šæ‰æœ‰æ—¶é—´ï¼Œéœ€è¦èµåŠ©ä¸ªè¾›è‹¦è´¹ &lt;span&gt;ğŸ’°&lt;/span&gt; 50 å…ƒä¸€æ¬¡ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;é‡åˆ°é—®é¢˜å¯ä»¥å» web è®¾ç½®é¡µé¢åº•éƒ¨ç‚¹å‡»ã€ä¸‹è½½æ—¥å¿—æ–‡ä»¶ã€‘æŒ‰é’®ï¼Œç„¶åæœç´¢ä¸€ä¸‹æ—¥å¿—æ–‡ä»¶å†…å®¹ç¡®ä¿é‡Œé¢æ²¡æœ‰è´¦å·å¯†ç ä¿¡æ¯å(æœ‰å°±åˆ é™¤è¿™äº›æ•æ„Ÿä¿¡æ¯)ï¼Œç„¶ååœ¨æ issues åé¦ˆé—®é¢˜æ—¶æŠŠä¸‹è½½çš„æ—¥å¿—æ–‡ä»¶å¸¦ä¸Šã€‚&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] ä½œè€…çš„å¦ä¸€ä¸ªé€‚ç”¨äº NAS ä¸Šå®‰è£…çš„å¼€æºå·¥å…·ï¼š &lt;a href="https://github.com/hanxi/tiny-nav"&gt;https://github.com/hanxi/tiny-nav&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;å–œæ¬¢å¬ä¹¦çš„å¯ä»¥é…åˆè¿™ä¸ªå·¥å…·ä½¿ç”¨ &lt;a href="https://github.com/hanxi/epub2mp3"&gt;https://github.com/hanxi/epub2mp3&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ğŸ”¥ã€å¹¿å‘Š:å¯ç”¨äºå®‰è£… frp å®ç°å†…ç½‘ç©¿é€ã€‘&lt;/li&gt; 
  &lt;li&gt;ğŸ”¥ æµ·å¤– RackNerd VPS æœºå™¨æ¨èï¼Œå¯æ”¯ä»˜å®ä»˜æ¬¾ã€‚&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://my.racknerd.com/aff.php?aff=11177"&gt;&lt;img src="https://racknerd.com/banners/320x50.gif" alt="RackNerd Mobile Leaderboard Banner" width="320" height="50" /&gt;&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ä¸çŸ¥é“é€‰å“ªä¸ªå¥—é¤å¯ä»¥ç›´æ¥ä¹°è¿™ä¸ªæœ€ä¾¿å®œçš„ &lt;a href="https://my.racknerd.com/aff.php?aff=11177&amp;amp;pid=912"&gt;https://my.racknerd.com/aff.php?aff=11177&amp;amp;pid=912&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ä¹Ÿå¯ä»¥ç”¨æ¥éƒ¨ç½²ä»£ç†ï¼Œdocker éƒ¨ç½²æ–¹æ³•è§ &lt;a href="https://github.com/hanxi/blog/issues/96"&gt;https://github.com/hanxi/blog/issues/96&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ğŸ”¥ã€å¹¿å‘Š: æ­å»ºæ‚¨çš„ä¸“å±å¤§æ¨¡å‹ä¸»é¡µ å‘Šåˆ«ç¹çé…ç½®éš¾é¢˜ï¼Œä¸€é”®å³å¯ç•…äº«ç¨³å®šæµç•…çš„AIä½“éªŒï¼ã€‘&lt;a href="https://university.aliyun.com/mobile?userCode=szqvatm6"&gt;https://university.aliyun.com/mobile?userCode=szqvatm6&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;å…è´¹ä¸»æœº&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://dartnode.com?aff=SnappyPigeon570"&gt;&lt;img src="https://dartnode.com/branding/DN-Open-Source-sm.png" alt="Powered by DartNode - Free VPS for Open Source" width="320" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;ğŸ¤ æ”¯æŒè¯­éŸ³å£ä»¤&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ã€æ’­æ”¾æ­Œæ›²ã€‘ï¼Œæ’­æ”¾æœ¬åœ°çš„æ­Œæ›²&lt;/li&gt; 
 &lt;li&gt;ã€æ’­æ”¾æ­Œæ›²+æ­Œåã€‘ï¼Œæ¯”å¦‚ï¼šæ’­æ”¾æ­Œæ›²å‘¨æ°ä¼¦æ™´å¤©&lt;/li&gt; 
 &lt;li&gt;ã€ä¸Šä¸€é¦–ã€‘&lt;/li&gt; 
 &lt;li&gt;ã€ä¸‹ä¸€é¦–ã€‘&lt;/li&gt; 
 &lt;li&gt;ã€å•æ›²å¾ªç¯ã€‘&lt;/li&gt; 
 &lt;li&gt;ã€å…¨éƒ¨å¾ªç¯ã€‘&lt;/li&gt; 
 &lt;li&gt;ã€éšæœºæ’­æ”¾ã€‘&lt;/li&gt; 
 &lt;li&gt;ã€å…³æœºã€‘ï¼Œã€åœæ­¢æ’­æ”¾ã€‘ï¼Œä¸¤ä¸ªæ•ˆæœæ˜¯ä¸€æ ·çš„ã€‚&lt;/li&gt; 
 &lt;li&gt;ã€åˆ·æ–°åˆ—è¡¨ã€‘ï¼Œå½“å¤åˆ¶äº†æ­Œæ›²è¿› music ç›®å½•åï¼Œå¯ä»¥ç”¨è¿™ä¸ªå£ä»¤åˆ·æ–°æ­Œå•ã€‚&lt;/li&gt; 
 &lt;li&gt;ã€æ’­æ”¾åˆ—è¡¨+åˆ—è¡¨åã€‘ï¼Œæ¯”å¦‚ï¼šæ’­æ”¾åˆ—è¡¨å…¶ä»–ã€‚&lt;/li&gt; 
 &lt;li&gt;ã€åŠ å…¥æ”¶è—ã€‘ï¼ŒæŠŠå½“å‰æ’­æ”¾çš„æ­Œæ›²åŠ å…¥æ”¶è—æ­Œå•ã€‚&lt;/li&gt; 
 &lt;li&gt;ã€å–æ¶ˆæ”¶è—ã€‘ï¼ŒæŠŠå½“å‰æ’­æ”¾çš„æ­Œæ›²ä»æ”¶è—æ­Œå•é‡Œç§»é™¤ã€‚&lt;/li&gt; 
 &lt;li&gt;ã€æ’­æ”¾åˆ—è¡¨æ”¶è—ã€‘ï¼Œè¿™ä¸ªç”¨äºæ’­æ”¾æ”¶è—æ­Œå•ã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;del&gt;ã€æ’­æ”¾æœ¬åœ°æ­Œæ›²+æ­Œåã€‘ï¼Œè¿™ä¸ªå£ä»¤å’Œæ’­æ”¾æ­Œæ›²çš„åŒºåˆ«æ˜¯æœ¬åœ°æ‰¾ä¸åˆ°ä¹Ÿä¸ä¼šå»ä¸‹è½½ã€‚&lt;/del&gt;&lt;/li&gt; 
 &lt;li&gt;ã€æ’­æ”¾åˆ—è¡¨ç¬¬å‡ ä¸ª+åˆ—è¡¨åã€‘ï¼Œå…·ä½“è§ï¼š &lt;a href="https://github.com/hanxi/xiaomusic/issues/158"&gt;https://github.com/hanxi/xiaomusic/issues/158&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ã€æœç´¢æ’­æ”¾+å…³é”®è¯ã€‘ï¼Œä¼šæœç´¢å…³é”®è¯ä½œä¸ºä¸´æ—¶æœç´¢åˆ—è¡¨æ’­æ”¾ï¼Œæ¯”å¦‚è¯´ã€æœç´¢æ’­æ”¾æ—ä¿Šæ°ã€‘ï¼Œä¼šæ’­æ”¾æ‰€æœ‰æ—ä¿Šæ°çš„æ­Œã€‚&lt;/li&gt; 
 &lt;li&gt;ã€æœ¬åœ°æœç´¢æ’­æ”¾+å…³é”®è¯ã€‘ï¼Œè·Ÿæœç´¢æ’­æ”¾çš„åŒºåˆ«æ˜¯æœ¬åœ°æ‰¾ä¸åˆ°ä¹Ÿä¸ä¼šå»ä¸‹è½½ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] éšè—ç©æ³•: å¯¹å°çˆ±åŒå­¦è¯´æ’­æ”¾æ­Œæ›²å°çŒªä½©å¥‡çš„æ•…äº‹ï¼Œä¼šå…ˆä¸‹è½½å°çŒªä½©å¥‡çš„æ•…äº‹ï¼Œç„¶åå†æ’­æ”¾å°çŒªä½©å¥‡çš„æ•…äº‹ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ› ï¸ pip æ–¹å¼å®‰è£…è¿è¡Œ&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;&amp;gt; pip install -U xiaomusic
&amp;gt; xiaomusic --help
 __  __  _                   __  __                 _
 \ \/ / (_)   __ _    ___   |  \/  |  _   _   ___  (_)   ___
  \  /  | |  / _` |  / _ \  | |\/| | | | | | / __| | |  / __|
  /  \  | | | (_| | | (_) | | |  | | | |_| | \__ \ | | | (__
 /_/\_\ |_|  \__,_|  \___/  |_|  |_|  \__,_| |___/ |_|  \___|
          XiaoMusic v0.3.69 by: github.com/hanxi

usage: xiaomusic [-h] [--port PORT] [--hardware HARDWARE] [--account ACCOUNT]
                 [--password PASSWORD] [--cookie COOKIE] [--verbose]
                 [--config CONFIG] [--ffmpeg_location FFMPEG_LOCATION]

options:
  -h, --help            show this help message and exit
  --port PORT           ç›‘å¬ç«¯å£
  --hardware HARDWARE   å°çˆ±éŸ³ç®±å‹å·
  --account ACCOUNT     xiaomi account
  --password PASSWORD   xiaomi password
  --cookie COOKIE       xiaomi cookie
  --verbose             show info
  --config CONFIG       config file path
  --ffmpeg_location FFMPEG_LOCATION
                        ffmpeg bin path
&amp;gt; xiaomusic --config config.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;å…¶ä¸­ &lt;code&gt;config.json&lt;/code&gt; æ–‡ä»¶å¯ä»¥å‚è€ƒ &lt;code&gt;config-example.json&lt;/code&gt; æ–‡ä»¶é…ç½®ã€‚è§ &lt;a href="https://github.com/hanxi/xiaomusic/issues/94"&gt;https://github.com/hanxi/xiaomusic/issues/94&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;ä¸ä¿®æ”¹é»˜è®¤ç«¯å£ 8090 çš„æƒ…å†µä¸‹ï¼Œåªéœ€è¦æ‰§è¡Œ &lt;code&gt;xiaomusic&lt;/code&gt; å³å¯å¯åŠ¨ã€‚&lt;/p&gt; 
&lt;h2&gt;ğŸ”© å¼€å‘ç¯å¢ƒè¿è¡Œ&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ä½¿ç”¨ install_dependencies.sh ä¸‹è½½ä¾èµ–&lt;/li&gt; 
 &lt;li&gt;ä½¿ç”¨ pdm å®‰è£…ç¯å¢ƒ&lt;/li&gt; 
 &lt;li&gt;é»˜è®¤ç›‘å¬äº†ç«¯å£ 8090 , ä½¿ç”¨å…¶ä»–ç«¯å£è‡ªè¡Œä¿®æ”¹ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pdm run xiaomusic.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;å¦‚æœæ˜¯å¼€å‘å‰ç«¯ç•Œé¢ï¼Œå¯ä»¥é€šè¿‡ &lt;a href="http://localhost:8090/docs"&gt;http://localhost:8090/docs&lt;/a&gt; æŸ¥çœ‹æœ‰ä»€ä¹ˆæ¥å£ã€‚ç›®å‰çš„ web æ§åˆ¶å°éå¸¸ç®€é™‹ï¼Œæ¬¢è¿æœ‰å…´è¶£çš„æœ‹å‹å¸®å¿™å®ç°ä¸€ä¸ªæ¼‚äº®çš„å‰ç«¯ï¼Œéœ€è¦ä»€ä¹ˆæ¥å£å¯ä»¥éšæ—¶æéœ€æ±‚ã€‚&lt;/p&gt; 
&lt;h3&gt;ğŸš¦ ä»£ç æäº¤è§„èŒƒ&lt;/h3&gt; 
&lt;p&gt;æäº¤å‰è¯·æ‰§è¡Œ&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pdm lintfmt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ç”¨äºæ£€æŸ¥ä»£ç å’Œæ ¼å¼åŒ–ä»£ç ã€‚&lt;/p&gt; 
&lt;h3&gt;æœ¬åœ°ç¼–è¯‘ Docker Image&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;docker build -t xiaomusic .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;æŠ€æœ¯æ ˆ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;åç«¯ä»£ç ä½¿ç”¨ Python è¯­è¨€ç¼–å†™ã€‚&lt;/li&gt; 
 &lt;li&gt;HTTP æœåŠ¡ä½¿ç”¨çš„æ˜¯ FastAPI æ¡†æ¶ï¼Œ&lt;del&gt;æ—©æœŸç‰ˆæœ¬ä½¿ç”¨çš„æ˜¯ Flask&lt;/del&gt;ã€‚&lt;/li&gt; 
 &lt;li&gt;ä½¿ç”¨äº† Docker ï¼Œåœ¨ NAS ä¸Šå®‰è£…æ›´æ–¹ä¾¿ã€‚&lt;/li&gt; 
 &lt;li&gt;é»˜è®¤çš„å‰ç«¯ä¸»é¢˜ä½¿ç”¨äº† jQuery ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;å·²æµ‹è¯•æ”¯æŒçš„è®¾å¤‡&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;å‹å·&lt;/th&gt; 
   &lt;th&gt;åç§°&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;L06A&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.l06a"&gt;å°çˆ±éŸ³ç®±&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;L07A&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://home.mi.com/webapp/content/baike/product/index.html?model=xiaomi.wifispeaker.l7a"&gt;Redmiå°çˆ±éŸ³ç®± Play&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;S12/S12A/MDZ-25-DA&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.s12"&gt;å°ç±³AIéŸ³ç®±&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LX5A&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.lx5a"&gt;å°çˆ±éŸ³ç®± ä¸‡èƒ½é¥æ§ç‰ˆ&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LX05&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.lx05"&gt;å°çˆ±éŸ³ç®±Playï¼ˆ2019æ¬¾ï¼‰&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;L15A&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://home.mi.com/webapp/content/baike/product/index.html?model=xiaomi.wifispeaker.l15a#/"&gt;å°ç±³AIéŸ³ç®±ï¼ˆç¬¬äºŒä»£ï¼‰&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;L16A&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.l16a"&gt;Xiaomi Sound&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;L17A&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.l17a"&gt;Xiaomi Sound Pro&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LX06&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.lx06"&gt;å°çˆ±éŸ³ç®±Pro&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LX01&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.lx01"&gt;å°çˆ±éŸ³ç®±mini&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;L05B&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.l05b"&gt;å°çˆ±éŸ³ç®±Play&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;L05C&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.l05c"&gt;å°ç±³å°çˆ±éŸ³ç®±Play å¢å¼ºç‰ˆ&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;L09A&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://home.mi.com/webapp/content/baike/product/index.html?model=xiaomi.wifispeaker.l09a"&gt;å°ç±³éŸ³ç®±Art&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LX04 X10A X08A&lt;/td&gt; 
   &lt;td&gt;å·²ç»æ”¯æŒçš„è§¦å±ç‰ˆ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;X08C X08E X8F&lt;/td&gt; 
   &lt;td&gt;å·²ç»ä¸éœ€è¦è®¾ç½®äº†. &lt;del&gt;éœ€è¦è®¾ç½®ã€å‹å·å…¼å®¹æ¨¡å¼ã€‘é€‰é¡¹ä¸º true&lt;/del&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;M01/XMYX01JY&lt;/td&gt; 
   &lt;td&gt;å°ç±³å°çˆ±éŸ³ç®±HD éœ€è¦è®¾ç½®ã€ç‰¹æ®Šå‹å·è·å–å¯¹è¯è®°å½•ã€‘é€‰é¡¹ä¸º true æ‰èƒ½è¯­éŸ³æ’­æ”¾&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OH2P&lt;/td&gt; 
   &lt;td&gt;XIAOMI æ™ºèƒ½éŸ³ç®± Pro&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OH2&lt;/td&gt; 
   &lt;td&gt;XIAOMI æ™ºèƒ½éŸ³ç®±&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;å‹å·ä¸äº§å“åç§°å¯¹ç…§å¯ä»¥åœ¨è¿™é‡ŒæŸ¥è¯¢ &lt;a href="https://home.miot-spec.com/s/xiaomi.wifispeaker"&gt;https://home.miot-spec.com/s/xiaomi.wifispeaker&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] å¦‚æœä½ çš„è®¾å¤‡æ”¯æŒæ’­æ”¾ï¼Œè¯·åé¦ˆç»™æˆ‘æ·»åŠ åˆ°æ”¯æŒåˆ—è¡¨é‡Œï¼Œè°¢è°¢ã€‚ ç›®å‰åº”è¯¥æ‰€æœ‰è®¾å¤‡ç±»å‹éƒ½å·²ç»æ”¯æŒæ’­æ”¾ï¼Œæœ‰é—®é¢˜éšæ—¶åé¦ˆã€‚ å…¶ä»–è§¦å±ç‰ˆä¸èƒ½æ’­æ”¾å¯ä»¥è®¾ç½®ã€å‹å·å…¼å®¹æ¨¡å¼ã€‘é€‰é¡¹ä¸º true è¯•è¯•ã€‚è§ &lt;a href="https://github.com/hanxi/xiaomusic/issues/30"&gt;https://github.com/hanxi/xiaomusic/issues/30&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸµ æ”¯æŒéŸ³ä¹æ ¼å¼&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;mp3&lt;/li&gt; 
 &lt;li&gt;flac&lt;/li&gt; 
 &lt;li&gt;wav&lt;/li&gt; 
 &lt;li&gt;ape&lt;/li&gt; 
 &lt;li&gt;ogg&lt;/li&gt; 
 &lt;li&gt;m4a&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] æœ¬åœ°éŸ³ä¹ä¼šæœç´¢ç›®å½•ä¸‹ä¸Šé¢æ ¼å¼çš„æ–‡ä»¶ï¼Œä¸‹è½½çš„æ­Œæ›²æ˜¯ mp3 æ ¼å¼çš„ã€‚ å·²çŸ¥ L05B L05C LX06 L16A ä¸æ”¯æŒ flac æ ¼å¼ã€‚ å¦‚æœæ ¼å¼ä¸èƒ½æ’­æ”¾å¯ä»¥æ‰“å¼€ã€è½¬æ¢ä¸ºMP3ã€‘å’Œã€å‹å·å…¼å®¹æ¨¡å¼ã€‘é€‰é¡¹ã€‚å…·ä½“è§ &lt;a href="https://github.com/hanxi/xiaomusic/issues/153#issuecomment-2328168689"&gt;https://github.com/hanxi/xiaomusic/issues/153#issuecomment-2328168689&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸŒ ç½‘ç»œæ­Œå•åŠŸèƒ½&lt;/h2&gt; 
&lt;p&gt;å¯ä»¥é…ç½®ä¸€ä¸ª json æ ¼å¼çš„æ­Œå•ï¼Œæ”¯æŒç”µå°å’Œæ­Œæ›²ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ç”¨åˆ«äººåˆ†äº«çš„é“¾æ¥ï¼ŒåŒæ—¶é…å¤‡äº† m3u æ–‡ä»¶æ ¼å¼è½¬æ¢å·¥å…·ï¼Œå¯ä»¥å¾ˆæ–¹ä¾¿çš„æŠŠ m3u ç”µå°æ–‡ä»¶è½¬æ¢æˆç½‘ç»œæ­Œå•æ ¼å¼çš„ json æ–‡ä»¶ï¼Œå…·ä½“ç”¨æ³•è§ &lt;a href="https://github.com/hanxi/xiaomusic/issues/78"&gt;https://github.com/hanxi/xiaomusic/issues/78&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] æ¬¢è¿æœ‰æƒ³æ³•çš„æœ‹å‹ä»¬åˆ¶ä½œæ›´å¤šçš„æ­Œå•è½¬æ¢å·¥å…·ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸº æ›´å¤šå…¶ä»–å¯é€‰é…ç½®&lt;/h2&gt; 
&lt;p&gt;è§ &lt;a href="https://github.com/hanxi/xiaomusic/issues/333"&gt;https://github.com/hanxi/xiaomusic/issues/333&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;âš ï¸ å®‰å…¨æé†’&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT]&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;å¦‚æœé…ç½®äº†å…¬ç½‘è®¿é—® xiaomusic ï¼Œè¯·ä¸€å®šè¦å¼€å¯å¯†ç ç™»é™†ï¼Œå¹¶è®¾ç½®å¤æ‚çš„å¯†ç ã€‚ä¸”ä¸è¦åœ¨å…¬å…±åœºæ‰€çš„ WiFi ç¯å¢ƒä¸‹ä½¿ç”¨ï¼Œå¦åˆ™å¯èƒ½é€ æˆå°ç±³è´¦å·å¯†ç æ³„éœ²ã€‚&lt;/li&gt; 
  &lt;li&gt;å¼ºçƒˆä¸å»ºè®®å°†å°çˆ±éŸ³ç®±çš„å°ç±³è´¦å·ç»‘å®šæ‘„åƒå¤´ï¼Œä»£ç éš¾å…ä¼šæœ‰ bug ï¼Œä¸€æ—¦å°ç±³è´¦å·å¯†ç æ³„éœ²ï¼Œå¯èƒ½ç›‘æ§å½•åƒä¹Ÿä¼šæ³„éœ²ã€‚&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ¤” é«˜çº§ç¯‡&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;è‡ªå®šä¹‰å£ä»¤åŠŸèƒ½ &lt;a href="https://github.com/hanxi/xiaomusic/issues/105"&gt;https://github.com/hanxi/xiaomusic/issues/105&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hanxi/xiaomusic/issues/312"&gt;https://github.com/hanxi/xiaomusic/issues/312&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hanxi/xiaomusic/issues/269"&gt;https://github.com/hanxi/xiaomusic/issues/269&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hanxi/xiaomusic/issues/159"&gt;https://github.com/hanxi/xiaomusic/issues/159&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“¢ è®¨è®ºåŒº&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pd.qq.com/s/e2jybz0ss"&gt;ç‚¹å‡»é“¾æ¥åŠ å…¥QQé¢‘é“ã€xiaomusicã€‘&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://qm.qq.com/cgi-bin/qm/qr?_wv=1027&amp;amp;k=13St5PLVcTxYlWTAs_iAawazjtdD1l-a&amp;amp;authKey=dJWEpaT2fDBDpdUUOWj%2FLt6NS1ePBfShDfz7a6seNURi05VvVnAGQzXF%2FM%2F5HgIm&amp;amp;noverify=0&amp;amp;group_code=604526973"&gt;ç‚¹å‡»é“¾æ¥åŠ å…¥ç¾¤èŠã€æ»¡ xiaomusicå®˜æ–¹äº¤æµç¾¤1ã€‘ 604526973&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://qm.qq.com/q/BmVNqhDL3M"&gt;ç‚¹å‡»é“¾æ¥åŠ å…¥ç¾¤èŠã€æ»¡ xiaomusicå®˜æ–¹äº¤æµç¾¤2ã€‘1021062499&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://qm.qq.com/q/lxIhquqbza"&gt;ç‚¹å‡»é“¾æ¥åŠ å…¥ç¾¤èŠã€xiaomusicå®˜æ–¹äº¤æµç¾¤3ã€‘ 1072151477&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hanxi/xiaomusic/issues"&gt;https://github.com/hanxi/xiaomusic/issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hanxi/xiaomusic/issues/86"&gt;å¾®ä¿¡ç¾¤äºŒç»´ç &lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;â¤ï¸ æ„Ÿè°¢&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.mi.com/"&gt;xiaomi&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pdm.fming.dev/latest/"&gt;PDM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yihong0618/xiaogpt"&gt;xiaogpt&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yihong0618/MiService"&gt;MiService&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yihong0618/gitblog/issues/258"&gt;å®ç°åŸç†&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp"&gt;yt-dlp&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/zzz6519003/awesome-xiaoai"&gt;awesome-xiaoai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/F-loat/xiaoplayer"&gt;å¾®ä¿¡å°ç¨‹åº: å¯å¯éŸ³ä¹&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/52fisher/xiaomusicUI"&gt;pure ä¸»é¢˜ xiaomusicUI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/52fisher/XMusicPlayer"&gt;ç§»åŠ¨ç«¯çš„æ’­æ”¾å™¨ä¸»é¢˜&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/clarencejh/xiaomusic"&gt;Tailwindä¸»é¢˜&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/DarrenWen/xiaomusicui"&gt;ä¸€ä¸ªç¬¬ä¸‰æ–¹çš„ä¸»é¢˜&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/umami-software/umami"&gt;Umami ç»Ÿè®¡&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/getsentry/sentry"&gt;Sentry æŠ¥é”™ç›‘æ§&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;æ‰€æœ‰å¸®å¿™è°ƒè¯•å’Œæµ‹è¯•çš„æœ‹å‹&lt;/li&gt; 
 &lt;li&gt;æ‰€æœ‰åé¦ˆé—®é¢˜å’Œå»ºè®®çš„æœ‹å‹&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ‘‰ å…¶ä»–æ•™ç¨‹&lt;/h3&gt; 
&lt;p&gt;æ›´å¤šåŠŸèƒ½è§ &lt;a href="https://github.com/hanxi/xiaomusic/issues/211"&gt;ğŸ“ æ–‡æ¡£æ±‡æ€»&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸš¨ å…è´£å£°æ˜&lt;/h2&gt; 
&lt;p&gt;æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ç›®çš„ï¼Œä¸å¾—ç”¨äºä»»ä½•å•†ä¸šæ´»åŠ¨ã€‚ç”¨æˆ·åœ¨ä½¿ç”¨æœ¬é¡¹ç›®æ—¶åº”éµå®ˆæ‰€åœ¨åœ°åŒºçš„æ³•å¾‹æ³•è§„ï¼Œå¯¹äºè¿æ³•ä½¿ç”¨æ‰€å¯¼è‡´çš„åæœï¼Œæœ¬é¡¹ç›®åŠä½œè€…ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚ æœ¬é¡¹ç›®å¯èƒ½å­˜åœ¨æœªçŸ¥çš„ç¼ºé™·å’Œé£é™©ï¼ˆåŒ…æ‹¬ä½†ä¸é™äºè®¾å¤‡æŸåå’Œè´¦å·å°ç¦ç­‰ï¼‰ï¼Œä½¿ç”¨è€…åº”è‡ªè¡Œæ‰¿æ‹…ä½¿ç”¨æœ¬é¡¹ç›®æ‰€äº§ç”Ÿçš„æ‰€æœ‰é£é™©åŠè´£ä»»ã€‚ ä½œè€…ä¸ä¿è¯æœ¬é¡¹ç›®çš„å‡†ç¡®æ€§ã€å®Œæ•´æ€§ã€åŠæ—¶æ€§ã€å¯é æ€§ï¼Œä¹Ÿä¸æ‰¿æ‹…ä»»ä½•å› ä½¿ç”¨æœ¬é¡¹ç›®è€Œäº§ç”Ÿçš„ä»»ä½•æŸå¤±æˆ–æŸå®³è´£ä»»ã€‚ ä½¿ç”¨æœ¬é¡¹ç›®å³è¡¨ç¤ºæ‚¨å·²é˜…è¯»å¹¶åŒæ„æœ¬å…è´£å£°æ˜çš„å…¨éƒ¨å†…å®¹ã€‚&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#hanxi/xiaomusic&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=hanxi/xiaomusic&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;èµèµ&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span&gt;ğŸ’°&lt;/span&gt; çˆ±å‘ç”µ &lt;a href="https://afdian.com/a/imhanxi"&gt;https://afdian.com/a/imhanxi&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ç‚¹ä¸ª Star &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;è°¢è°¢ &lt;span&gt;â¤ï¸&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;img src="https://i.v2ex.co/7Q03axO5l.png" alt="å–æ¯å¥¶èŒ¶" /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/hanxi/xiaomusic/raw/main/LICENSE"&gt;MIT&lt;/a&gt; License Â© 2024 æ¶µæ›¦&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>stanford-cs336/spring2025-lectures</title>
      <link>https://github.com/stanford-cs336/spring2025-lectures</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Spring 2025 CS336 lectures&lt;/h1&gt; 
&lt;p&gt;This repo contains the lecture materials for "Stanford CS336: Language modeling from scratch".&lt;/p&gt; 
&lt;h2&gt;Non-executable (ppt/pdf) lectures&lt;/h2&gt; 
&lt;p&gt;Located in &lt;code&gt;nonexecutable/&lt;/code&gt;as PDFs&lt;/p&gt; 
&lt;h2&gt;Executable lectures&lt;/h2&gt; 
&lt;p&gt;Located as &lt;code&gt;lecture_*.py&lt;/code&gt; in the root directory&lt;/p&gt; 
&lt;p&gt;You can compile a lecture by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    python execute.py -m lecture_01
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;which generates a &lt;code&gt;var/traces/lecture_01.json&lt;/code&gt; and caches any images as appropriate.&lt;/p&gt; 
&lt;p&gt;However, if you want to run it on the cluster, you can do:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    ./remote_execute.sh lecture_01
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;which copies the files to our slurm cluster, runs it there, and copies the results back. You have to setup the appropriate environment and tweak some configs to make this work (these instructions are not complete).&lt;/p&gt; 
&lt;h3&gt;Frontend&lt;/h3&gt; 
&lt;p&gt;If you need to tweak the Javascript:&lt;/p&gt; 
&lt;p&gt;Install (one-time):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    npm create vite@latest trace-viewer -- --template react
    cd trace-viewer
    npm install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Load a local server to view at &lt;code&gt;http://localhost:5173?trace=var/traces/sample.json&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    npm run dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Deploy to the main website:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    cd trace-viewer
    npm run build
    git add dist/assets
    # then commit to the repo and it should show up on the website
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>hacksider/Deep-Live-Cam</title>
      <link>https://github.com/hacksider/Deep-Live-Cam</link>
      <description>&lt;p&gt;real time face swap and one-click video deepfake with only a single image&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt;Deep-Live-Cam&lt;/h1&gt; 
&lt;p align="center"&gt; Real-time face swap and video deepfake with a single click and only a single image. &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/11395" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/11395" alt="hacksider%2FDeep-Live-Cam | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/demo.gif" alt="Demo GIF" width="800" /&gt; &lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This deepfake software is designed to be a productive tool for the AI-generated media industry. It can assist artists in animating custom characters, creating engaging content, and even using models for clothing design.&lt;/p&gt; 
&lt;p&gt;We are aware of the potential for unethical applications and are committed to preventative measures. A built-in check prevents the program from processing inappropriate media (nudity, graphic content, sensitive material like war footage, etc.). We will continue to develop this project responsibly, adhering to the law and ethics. We may shut down the project or add watermarks if legally required.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Ethical Use: Users are expected to use this software responsibly and legally. If using a real person's face, obtain their consent and clearly label any output as a deepfake when sharing online.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Content Restrictions: The software includes built-in checks to prevent processing inappropriate media, such as nudity, graphic content, or sensitive material.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Legal Compliance: We adhere to all relevant laws and ethical guidelines. If legally required, we may shut down the project or add watermarks to the output.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;User Responsibility: We are not responsible for end-user actions. Users must ensure their use of the software aligns with ethical standards and legal requirements.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By using this software, you agree to these terms and commit to using it in a manner that respects the rights and dignity of others.&lt;/p&gt; 
&lt;p&gt;Users are expected to use this software responsibly and legally. If using a real person's face, obtain their consent and clearly label any output as a deepfake when sharing online. We are not responsible for end-user actions.&lt;/p&gt; 
&lt;h2&gt;Exclusive v2.3 Quick Start - Pre-built (Windows/Mac Silicon)&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://deeplivecam.net/index.php/quickstart"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/Download.png" width="285" height="77" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;a href="https://deeplivecam.net/index.php/quickstart"&gt; &lt;h5&gt;This is the fastest build you can get if you have a discrete NVIDIA or AMD GPU or Mac Silicon, And you'll receive special priority support.&lt;/h5&gt; &lt;h6&gt;These Pre-builts are perfect for non-technical users or those who don't have time to, or can't manually install all the requirements. Just a heads-up: this is an open-source project, so you can also install it manually.&lt;/h6&gt; &lt;h2&gt;TLDR; Live Deepfake in just 3 Clicks&lt;/h2&gt; &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/af825228-852c-411b-b787-ffd9aac72fc6" alt="easysteps" /&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Select a face&lt;/li&gt; 
  &lt;li&gt;Select which camera to use&lt;/li&gt; 
  &lt;li&gt;Press live!&lt;/li&gt; 
 &lt;/ol&gt; &lt;h2&gt;Features &amp;amp; Uses - Everything is in real-time&lt;/h2&gt; &lt;h3&gt;Mouth Mask&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Retain your original mouth for accurate movement using Mouth Mask&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/ludwig.gif" alt="resizable-gif" /&gt; &lt;/p&gt; &lt;h3&gt;Face Mapping&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Use different faces on multiple subjects simultaneously&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/streamers.gif" alt="face_mapping_source" /&gt; &lt;/p&gt; &lt;h3&gt;Your Movie, Your Face&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Watch movies with any face in real-time&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/movie.gif" alt="movie" /&gt; &lt;/p&gt; &lt;h3&gt;Live Show&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Run Live shows and performances&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/live_show.gif" alt="show" /&gt; &lt;/p&gt; &lt;h3&gt;Memes&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Create Your Most Viral Meme Yet&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/meme.gif" alt="show" width="450" /&gt; &lt;br /&gt; &lt;sub&gt;Created using Many Faces feature in Deep-Live-Cam&lt;/sub&gt; &lt;/p&gt; &lt;h3&gt;Omegle&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Surprise people on Omegle&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; 
  &lt;video src="https://github.com/user-attachments/assets/2e9b9b82-fa04-4b70-9f56-b1f68e7672d0" width="450" controls&gt;&lt;/video&gt; &lt;/p&gt; &lt;h2&gt;Installation (Manual)&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Please be aware that the installation requires technical skills and is not for beginners. Consider downloading the quickstart version.&lt;/strong&gt;&lt;/p&gt; &lt;/a&gt;
&lt;details&gt;
 &lt;a href="https://deeplivecam.net/index.php/quickstart"&gt; &lt;summary&gt;Click to see the process&lt;/summary&gt; &lt;h3&gt;Installation&lt;/h3&gt; &lt;p&gt;This is more likely to work on your computer but will be slower as it utilizes the CPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Set up Your Platform&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Python (3.11 recommended)&lt;/li&gt; 
   &lt;li&gt;pip&lt;/li&gt; 
   &lt;li&gt;git&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=OlNWCpFdVMA"&gt;ffmpeg&lt;/a&gt; - &lt;code&gt;iex (irm ffmpeg.tc.ht)&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://visualstudio.microsoft.com/visual-cpp-build-tools/"&gt;Visual Studio 2022 Runtimes (Windows)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt;&lt;/a&gt; 
 &lt;p&gt;&lt;strong&gt;2. Clone the Repository&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/hacksider/Deep-Live-Cam.git
cd Deep-Live-Cam
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;3. Download the Models&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth"&gt;GFPGANv1.4&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx"&gt;inswapper_128_fp16.onnx&lt;/a&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;Place these files in the "&lt;strong&gt;models&lt;/strong&gt;" folder.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;4. Install Dependencies&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;We highly recommend using a &lt;code&gt;venv&lt;/code&gt; to avoid issues.&lt;/p&gt; 
 &lt;p&gt;For Windows:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;For Linux:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Ensure you use the installed Python 3.10
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;For macOS:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Apple Silicon (M1/M2/M3) requires specific setup:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Install Python 3.11 (specific version is important)
brew install python@3.11

# Install tkinter package (required for the GUI)
brew install python-tk@3.10

# Create and activate virtual environment with Python 3.11
python3.11 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;** In case something goes wrong and you need to reinstall the virtual environment **&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Deactivate the virtual environment
rm -rf venv

# Reinstall the virtual environment
python -m venv venv
source venv/bin/activate

# install the dependencies again
pip install -r requirements.txt

# gfpgan and basicsrs issue fix
pip install git+https://github.com/xinntao/BasicSR.git@master
pip uninstall gfpgan -y
pip install git+https://github.com/TencentARC/GFPGAN.git@master
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Run:&lt;/strong&gt; If you don't have a GPU, you can run Deep-Live-Cam using &lt;code&gt;python run.py&lt;/code&gt;. Note that initial execution will download models (~300MB).&lt;/p&gt; 
 &lt;h3&gt;GPU Acceleration&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;CUDA Execution Provider (Nvidia)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install &lt;a href="https://developer.nvidia.com/cuda-12-8-0-download-archive"&gt;CUDA Toolkit 12.8.0&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Install &lt;a href="https://developer.nvidia.com/rdp/cudnn-archive"&gt;cuDNN v8.9.7 for CUDA 12.x&lt;/a&gt; (required for onnxruntime-gpu): 
   &lt;ul&gt; 
    &lt;li&gt;Download cuDNN v8.9.7 for CUDA 12.x&lt;/li&gt; 
    &lt;li&gt;Make sure the cuDNN bin directory is in your system PATH&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
pip uninstall onnxruntime onnxruntime-gpu
pip install onnxruntime-gpu==1.21.0
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="3"&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python run.py --execution-provider cuda
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;CoreML Execution Provider (Apple Silicon)&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Apple Silicon (M1/M2/M3) specific installation:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Make sure you've completed the macOS setup above using Python 3.10.&lt;/li&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall onnxruntime onnxruntime-silicon
pip install onnxruntime-silicon==1.13.1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="3"&gt; 
  &lt;li&gt;Usage (important: specify Python 3.10):&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python3.10 run.py --execution-provider coreml
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Important Notes for macOS:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;You &lt;strong&gt;must&lt;/strong&gt; use Python 3.10, not newer versions like 3.11 or 3.13&lt;/li&gt; 
  &lt;li&gt;Always run with &lt;code&gt;python3.10&lt;/code&gt; command not just &lt;code&gt;python&lt;/code&gt; if you have multiple Python versions installed&lt;/li&gt; 
  &lt;li&gt;If you get error about &lt;code&gt;_tkinter&lt;/code&gt; missing, reinstall the tkinter package: &lt;code&gt;brew reinstall python-tk@3.10&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;If you get model loading errors, check that your models are in the correct folder&lt;/li&gt; 
  &lt;li&gt;If you encounter conflicts with other Python versions, consider uninstalling them: &lt;pre&gt;&lt;code class="language-bash"&gt;# List all installed Python versions
brew list | grep python

# Uninstall conflicting versions if needed
brew uninstall --ignore-dependencies python@3.11 python@3.13

# Keep only Python 3.11
brew cleanup
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;CoreML Execution Provider (Apple Legacy)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall onnxruntime onnxruntime-coreml
pip install onnxruntime-coreml==1.21.0
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python run.py --execution-provider coreml
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;DirectML Execution Provider (Windows)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall onnxruntime onnxruntime-directml
pip install onnxruntime-directml==1.21.0
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python run.py --execution-provider directml
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;OpenVINOâ„¢ Execution Provider (Intel)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall onnxruntime onnxruntime-openvino
pip install onnxruntime-openvino==1.21.0
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python run.py --execution-provider openvino
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;1. Image/Video Mode&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Execute &lt;code&gt;python run.py&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Choose a source face image and a target image/video.&lt;/li&gt; 
 &lt;li&gt;Click "Start".&lt;/li&gt; 
 &lt;li&gt;The output will be saved in a directory named after the target video.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2. Webcam Mode&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Execute &lt;code&gt;python run.py&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Select a source face image.&lt;/li&gt; 
 &lt;li&gt;Click "Live".&lt;/li&gt; 
 &lt;li&gt;Wait for the preview to appear (10-30 seconds).&lt;/li&gt; 
 &lt;li&gt;Use a screen capture tool like OBS to stream.&lt;/li&gt; 
 &lt;li&gt;To change the face, select a new source image.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Command Line Arguments (Unmaintained)&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;options:
  -h, --help                                               show this help message and exit
  -s SOURCE_PATH, --source SOURCE_PATH                     select a source image
  -t TARGET_PATH, --target TARGET_PATH                     select a target image or video
  -o OUTPUT_PATH, --output OUTPUT_PATH                     select output file or directory
  --frame-processor FRAME_PROCESSOR [FRAME_PROCESSOR ...]  frame processors (choices: face_swapper, face_enhancer, ...)
  --keep-fps                                               keep original fps
  --keep-audio                                             keep original audio
  --keep-frames                                            keep temporary frames
  --many-faces                                             process every face
  --map-faces                                              map source target faces
  --mouth-mask                                             mask the mouth region
  --video-encoder {libx264,libx265,libvpx-vp9}             adjust output video encoder
  --video-quality [0-51]                                   adjust output video quality
  --live-mirror                                            the live camera display as you see it in the front-facing camera frame
  --live-resizable                                         the live camera frame is resizable
  --max-memory MAX_MEMORY                                  maximum amount of RAM in GB
  --execution-provider {cpu} [{cpu} ...]                   available execution provider (choices: cpu, ...)
  --execution-threads EXECUTION_THREADS                    number of execution threads
  -v, --version                                            show program's version number and exit
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Looking for a CLI mode? Using the -s/--source argument will make the run program in cli mode.&lt;/p&gt; 
&lt;h2&gt;Press&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;We are always open to criticism and are ready to improve, that's why we didn't cherry-pick anything.&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://arstechnica.com/information-technology/2024/08/new-ai-tool-enables-real-time-face-swapping-on-webcams-raising-fraud-concerns/"&gt;&lt;em&gt;"Deep-Live-Cam goes viral, allowing anyone to become a digital doppelganger"&lt;/em&gt;&lt;/a&gt; - Ars Technica&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://dataconomy.com/2024/08/15/what-is-deep-live-cam-github-deepfake/"&gt;&lt;em&gt;"Thanks Deep Live Cam, shapeshifters are among us now"&lt;/em&gt;&lt;/a&gt; - Dataconomy&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.newsbytesapp.com/news/science/deep-live-cam-ai-impersonation-tool-goes-viral/story"&gt;&lt;em&gt;"This free AI tool lets you become anyone during video-calls"&lt;/em&gt;&lt;/a&gt; - NewsBytes&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.creativebloq.com/ai/ok-this-viral-ai-live-stream-software-is-truly-terrifying"&gt;&lt;em&gt;"OK, this viral AI live stream software is truly terrifying"&lt;/em&gt;&lt;/a&gt; - Creative Bloq&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://petapixel.com/2024/08/14/deep-live-cam-deepfake-ai-tool-lets-you-become-anyone-in-a-video-call-with-single-photo-mark-zuckerberg-jd-vance-elon-musk/"&gt;&lt;em&gt;"Deepfake AI Tool Lets You Become Anyone in a Video Call With Single Photo"&lt;/em&gt;&lt;/a&gt; - PetaPixel&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.techeblog.com/deep-live-cam-ai-transform-face/"&gt;&lt;em&gt;"Deep-Live-Cam Uses AI to Transform Your Face in Real-Time, Celebrities Included"&lt;/em&gt;&lt;/a&gt; - TechEBlog&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://telegrafi.com/en/a-tool-that-makes-you-look-like-anyone-during-a-video-call-is-going-viral-on-the-Internet/"&gt;&lt;em&gt;"An AI tool that "makes you look like anyone" during a video call is going viral online"&lt;/em&gt;&lt;/a&gt; - Telegrafi&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://decrypt.co/244565/this-deepfake-tool-turning-images-into-livestreams-is-topping-the-github-charts"&gt;&lt;em&gt;"This Deepfake Tool Turning Images Into Livestreams is Topping the GitHub Charts"&lt;/em&gt;&lt;/a&gt; - Emerge&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.digitalmusicnews.com/2024/08/15/face-swapping-ai-real-time-mimic/"&gt;&lt;em&gt;"New Real-Time Face-Swapping AI Allows Anyone to Mimic Famous Faces"&lt;/em&gt;&lt;/a&gt; - Digital Music News&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.diyphotography.net/this-real-time-webcam-deepfake-tool-raises-alarms-about-the-future-of-identity-theft/"&gt;&lt;em&gt;"This real-time webcam deepfake tool raises alarms about the future of identity theft"&lt;/em&gt;&lt;/a&gt; - DIYPhotography&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/watch?time_continue=1074&amp;amp;v=py4Tc-Y8BcY"&gt;&lt;em&gt;"That's Crazy, Oh God. That's Fucking Freaky Dude... That's So Wild Dude"&lt;/em&gt;&lt;/a&gt; - SomeOrdinaryGamers&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/live/mFsCe7AIxq8?feature=shared&amp;amp;t=2686"&gt;&lt;em&gt;"Alright look look look, now look chat, we can do any face we want to look like chat"&lt;/em&gt;&lt;/a&gt; - IShowSpeed&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=wnCghLjqv3s&amp;amp;t=551s"&gt;&lt;em&gt;"They do a pretty good job matching poses, expression and even the lighting"&lt;/em&gt;&lt;/a&gt; - TechLinked (LTT)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.golem.de/news/deepfakes-als-sean-connery-an-der-redaktionskonferenz-teilnahm-2408-188172.html"&gt;&lt;em&gt;"Als Sean Connery an der Redaktionskonferenz teilnahm"&lt;/em&gt;&lt;/a&gt; - Golem.de (German)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://ffmpeg.org/"&gt;ffmpeg&lt;/a&gt;: for making video-related operations easy&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/deepinsight"&gt;deepinsight&lt;/a&gt;: for their &lt;a href="https://github.com/deepinsight/insightface"&gt;insightface&lt;/a&gt; project which provided a well-made library and models. Please be reminded that the &lt;a href="https://github.com/deepinsight/insightface?tab=readme-ov-file#license"&gt;use of the model is for non-commercial research purposes only&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/havok2-htwo"&gt;havok2-htwo&lt;/a&gt;: for sharing the code for webcam&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/GosuDRM"&gt;GosuDRM&lt;/a&gt;: for the open version of roop&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pereiraroland26"&gt;pereiraroland26&lt;/a&gt;: Multiple faces support&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vic4key"&gt;vic4key&lt;/a&gt;: For supporting/contributing to this project&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kier007"&gt;kier007&lt;/a&gt;: for improving the user experience&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/qitianai"&gt;qitianai&lt;/a&gt;: for multi-lingual support&lt;/li&gt; 
 &lt;li&gt;and &lt;a href="https://github.com/hacksider/Deep-Live-Cam/graphs/contributors"&gt;all developers&lt;/a&gt; behind libraries used in this project.&lt;/li&gt; 
 &lt;li&gt;Footnote: Please be informed that the base author of the code is &lt;a href="https://github.com/s0md3v/roop"&gt;s0md3v&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;All the wonderful users who helped make this project go viral by starring the repo â¤ï¸&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://github.com/hacksider/Deep-Live-Cam/stargazers"&gt;&lt;img src="https://reporoster.com/stars/hacksider/Deep-Live-Cam" alt="Stargazers" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributions&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://repobeats.axiom.co/api/embed/fec8e29c45dfdb9c5916f3a7830e1249308d20e1.svg?sanitize=true" alt="Alt" title="Repobeats analytics image" /&gt;&lt;/p&gt; 
&lt;h2&gt;Stars to the Moon ğŸš€&lt;/h2&gt; 
&lt;a href="https://star-history.com/#hacksider/deep-live-cam&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>google/adk-samples</title>
      <link>https://github.com/google/adk-samples</link>
      <description>&lt;p&gt;A collection of sample agents built with Agent Development (ADK)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Agent Development Kit (ADK) Samples&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/google/adk-samples/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;img src="https://github.com/google/adk-docs/raw/main/docs/assets/agent-development-kit.png" alt="Agent Development Kit Logo" width="150" /&gt; 
&lt;p&gt;Welcome to the ADK Sample Agents repository! This collection provides ready-to-use agents built on top of the &lt;a href="https://google.github.io/adk-docs/"&gt;Agent Development Kit&lt;/a&gt;, designed to accelerate your development process. These agents cover a range of common use cases and complexities, from simple conversational bots to complex multi-agent workflows.&lt;/p&gt; 
&lt;h2&gt;âœ¨ Getting Started&lt;/h2&gt; 
&lt;p&gt;This repo contains ADK sample agents for both &lt;strong&gt;Python&lt;/strong&gt; and &lt;strong&gt;Java.&lt;/strong&gt; Navigate to the &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/google/adk-samples/main/python/"&gt;Python&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/google/adk-samples/main/java/"&gt;Java&lt;/a&gt;&lt;/strong&gt; subfolders to see language-specific setup instructions, and learn more about the available sample agents.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] The agents in this repository are built using the &lt;strong&gt;Agent Development Kit (ADK)&lt;/strong&gt;. Before you can run any of the samples, you must have the ADK installed. For instructions, please refer to the &lt;a href="https://google.github.io/adk-docs/get-started/installation"&gt;&lt;strong&gt;ADK Installation Guide&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;To learn more, check out the &lt;a href="https://google.github.io/adk-docs/"&gt;ADK Documentation&lt;/a&gt;, and the GitHub repositories for &lt;a href="https://github.com/google/adk-python"&gt;ADK Python&lt;/a&gt; and &lt;a href="https://github.com/google/adk-java"&gt;ADK Java&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸŒ³ Repository Structure&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;â”œâ”€â”€ java
â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ agents
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ software-bug-assistant
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â””â”€â”€ time-series-forecasting
â”‚&amp;nbsp;&amp;nbsp; â””â”€â”€ README.md
â”œâ”€â”€ python
â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ agents
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ academic-research
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ blog-writer
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ brand-search-optimization
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ camel
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ customer-service
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ data-engineering
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ data-science
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ financial-advisor
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ fomc-research
â”‚   â”‚   â”œâ”€â”€ gemini-fullstack
â”‚   â”‚   â”œâ”€â”€ google-trends-agent
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ image-scoring
â”‚   â”‚   â”œâ”€â”€ llm-auditor
â”‚   â”‚   â”œâ”€â”€ machine-learning-engineering
â”‚   â”‚   â”œâ”€â”€ marketing-agency
â”‚   â”‚   â”œâ”€â”€ medical-pre-authorization
â”‚   â”‚   â”œâ”€â”€ personalized-shopping
â”‚   â”‚   â”œâ”€â”€ RAG
â”‚   â”‚   â”œâ”€â”€ realtime-conversational-agent
â”‚   â”‚   â”œâ”€â”€ safety-plugins
â”‚   â”‚   â”œâ”€â”€ short-movie-agents
â”‚   â”‚   â”œâ”€â”€ software-bug-assistant  
â”‚   â”‚   â”œâ”€â”€ travel-concierge
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â””â”€â”€ README.md
â””â”€â”€ README.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;â„¹ï¸ Getting help&lt;/h2&gt; 
&lt;p&gt;If you have any questions or if you found any problems with this repository, please report through &lt;a href="https://github.com/google/adk-samples/issues"&gt;GitHub issues&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ¤ Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Whether it's bug reports, feature requests, documentation improvements, or code contributions, please see our &lt;a href="https://github.com/google/adk-samples/raw/main/CONTRIBUTING.md"&gt;&lt;strong&gt;Contributing Guidelines&lt;/strong&gt;&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the Apache 2.0 License - see the &lt;a href="https://github.com/google/adk-samples/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;Disclaimers&lt;/h2&gt; 
&lt;p&gt;This is not an officially supported Google product. This project is not eligible for the &lt;a href="https://bughunters.google.com/open-source-security"&gt;Google Open Source Software Vulnerability Rewards Program&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This project is intended for demonstration purposes only. It is not intended for use in a production environment.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>allenai/olmocr</title>
      <link>https://github.com/allenai/olmocr</link>
      <description>&lt;p&gt;Toolkit for linearizing PDFs for LLM datasets/training&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img width="350" alt="olmocr-2-full@2x" src="https://github.com/user-attachments/assets/24f1b596-4059-46f1-8130-5d72dcc0b02e" /&gt; 
 &lt;hr /&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/allenai/OLMo/raw/main/LICENSE"&gt; &lt;img alt="GitHub License" src="https://img.shields.io/github/license/allenai/OLMo" /&gt; &lt;/a&gt; &lt;a href="https://github.com/allenai/olmocr/releases"&gt; &lt;img alt="GitHub release" src="https://img.shields.io/github/release/allenai/olmocr.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;a href="https://arxiv.org/abs/2502.18443"&gt; &lt;img alt="Tech Report v1" src="https://img.shields.io/badge/Paper_v1-olmOCR-blue" /&gt; &lt;/a&gt; &lt;a href="https://arxiv.org/abs/2510.19817"&gt; &lt;img alt="Tech Report v2" src="https://img.shields.io/badge/Paper_v2-olmOCR-blue" /&gt; &lt;/a&gt; &lt;a href="https://olmocr.allenai.org"&gt; &lt;img alt="Demo" src="https://img.shields.io/badge/Ai2-Demo-F0529C" /&gt; &lt;/a&gt; &lt;a href="https://discord.gg/sZq3jTNVNG"&gt; &lt;img alt="Discord" src="https://img.shields.io/badge/Discord%20-%20blue?style=flat&amp;amp;logo=discord&amp;amp;label=Ai2&amp;amp;color=%235B65E9" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;A toolkit for converting PDFs and other image-based document formats into clean, readable, plain text format.&lt;/p&gt; 
&lt;p&gt;Try the online demo: &lt;a href="https://olmocr.allenai.org/"&gt;https://olmocr.allenai.org/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Convert PDF, PNG, and JPEG based documents into clean Markdown&lt;/li&gt; 
 &lt;li&gt;Support for equations, tables, handwriting, and complex formatting&lt;/li&gt; 
 &lt;li&gt;Automatically removes headers and footers&lt;/li&gt; 
 &lt;li&gt;Convert into text with a natural reading order, even in the presence of figures, multi-column layouts, and insets&lt;/li&gt; 
 &lt;li&gt;Efficient, less than $200 USD per million pages converted&lt;/li&gt; 
 &lt;li&gt;(Based on a 7B parameter VLM, so it requires a GPU)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;News&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;October 21, 2025 - v0.4.0 - &lt;a href="https://huggingface.co/allenai/olmOCR-2-7B-1025-FP8"&gt;New model release&lt;/a&gt;, boosts olmOCR-bench score by ~4 points using synthetic data and introduces RL training.&lt;/li&gt; 
 &lt;li&gt;August 13, 2025 - v0.3.0 - &lt;a href="https://huggingface.co/allenai/olmOCR-7B-0825-FP8"&gt;New model release&lt;/a&gt;, fixes auto-rotation detection, and hallucinations on blank documents.&lt;/li&gt; 
 &lt;li&gt;July 24, 2025 - v0.2.1 - &lt;a href="https://huggingface.co/allenai/olmOCR-7B-0725-FP8"&gt;New model release&lt;/a&gt;, scores 3 points higher on &lt;a href="https://github.com/allenai/olmocr/tree/main/olmocr/bench"&gt;olmOCR-Bench&lt;/a&gt;, also runs significantly faster because it's default FP8, and needs much fewer retries per document.&lt;/li&gt; 
 &lt;li&gt;July 23, 2025 - v0.2.0 - New cleaned up &lt;a href="https://github.com/allenai/olmocr/tree/main/olmocr/train"&gt;trainer code&lt;/a&gt;, makes it much simpler to train olmOCR models yourself.&lt;/li&gt; 
 &lt;li&gt;June 17, 2025 - v0.1.75 - Switch from sglang to vllm based inference pipeline, updated docker image to CUDA 12.8.&lt;/li&gt; 
 &lt;li&gt;May 23, 2025 - v0.1.70 - Official docker support and images are now available! &lt;a href="https://raw.githubusercontent.com/allenai/olmocr/main/#using-docker"&gt;See Docker usage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;May 19, 2025 - v0.1.68 - &lt;a href="https://github.com/allenai/olmocr/tree/main/olmocr/bench"&gt;olmOCR-Bench&lt;/a&gt; launch, scoring 77.4. Launch includes 2 point performance boost in olmOCR pipeline due to bug fixes with prompts.&lt;/li&gt; 
 &lt;li&gt;Mar 17, 2025 - v0.1.60 - Performance improvements due to better temperature selection in sampling.&lt;/li&gt; 
 &lt;li&gt;Feb 25, 2025 - v0.1.58 - Initial public launch and demo.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Benchmark&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/allenai/olmocr/tree/main/olmocr/bench"&gt;&lt;strong&gt;olmOCR-Bench&lt;/strong&gt;&lt;/a&gt;: We also ship a comprehensive benchmark suite covering over 7,000 test cases across 1,400 documents to help measure performance of OCR systems.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;ArXiv&lt;/th&gt; 
   &lt;th&gt;Old&lt;br /&gt;scans&lt;br /&gt;math&lt;/th&gt; 
   &lt;th&gt;Tables&lt;/th&gt; 
   &lt;th&gt;Old&lt;br /&gt;scans&lt;/th&gt; 
   &lt;th&gt;Headers&lt;br /&gt;&amp;amp;&lt;br /&gt;footers&lt;/th&gt; 
   &lt;th&gt;Multi&lt;br /&gt;column&lt;/th&gt; 
   &lt;th&gt;Long&lt;br /&gt;tiny&lt;br /&gt;text&lt;/th&gt; 
   &lt;th&gt;Base&lt;/th&gt; 
   &lt;th&gt;Overall&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mistral OCR API&lt;/td&gt; 
   &lt;td&gt;77.2&lt;/td&gt; 
   &lt;td&gt;67.5&lt;/td&gt; 
   &lt;td&gt;60.6&lt;/td&gt; 
   &lt;td&gt;29.3&lt;/td&gt; 
   &lt;td&gt;93.6&lt;/td&gt; 
   &lt;td&gt;71.3&lt;/td&gt; 
   &lt;td&gt;77.1&lt;/td&gt; 
   &lt;td&gt;99.4&lt;/td&gt; 
   &lt;td&gt;72.0Â±1.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Marker 1.10.1&lt;/td&gt; 
   &lt;td&gt;83.8&lt;/td&gt; 
   &lt;td&gt;66.8&lt;/td&gt; 
   &lt;td&gt;72.9&lt;/td&gt; 
   &lt;td&gt;33.5&lt;/td&gt; 
   &lt;td&gt;86.6&lt;/td&gt; 
   &lt;td&gt;80.0&lt;/td&gt; 
   &lt;td&gt;85.7&lt;/td&gt; 
   &lt;td&gt;99.3&lt;/td&gt; 
   &lt;td&gt;76.1Â±1.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MinerU 2.5.4*&lt;/td&gt; 
   &lt;td&gt;76.6&lt;/td&gt; 
   &lt;td&gt;54.6&lt;/td&gt; 
   &lt;td&gt;84.9&lt;/td&gt; 
   &lt;td&gt;33.7&lt;/td&gt; 
   &lt;td&gt;96.6&lt;/td&gt; 
   &lt;td&gt;78.2&lt;/td&gt; 
   &lt;td&gt;83.5&lt;/td&gt; 
   &lt;td&gt;93.7&lt;/td&gt; 
   &lt;td&gt;75.2Â±1.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DeepSeek-OCR&lt;/td&gt; 
   &lt;td&gt;77.2&lt;/td&gt; 
   &lt;td&gt;73.6&lt;/td&gt; 
   &lt;td&gt;80.2&lt;/td&gt; 
   &lt;td&gt;33.3&lt;/td&gt; 
   &lt;td&gt;96.1&lt;/td&gt; 
   &lt;td&gt;66.4&lt;/td&gt; 
   &lt;td&gt;79.4&lt;/td&gt; 
   &lt;td&gt;99.8&lt;/td&gt; 
   &lt;td&gt;75.7Â±1.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Nanonets-OCR2-3B&lt;/td&gt; 
   &lt;td&gt;75.4&lt;/td&gt; 
   &lt;td&gt;46.1&lt;/td&gt; 
   &lt;td&gt;86.8&lt;/td&gt; 
   &lt;td&gt;40.9&lt;/td&gt; 
   &lt;td&gt;32.1&lt;/td&gt; 
   &lt;td&gt;81.9&lt;/td&gt; 
   &lt;td&gt;93.0&lt;/td&gt; 
   &lt;td&gt;99.6&lt;/td&gt; 
   &lt;td&gt;69.5Â±1.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PaddleOCR-VL*&lt;/td&gt; 
   &lt;td&gt;85.7&lt;/td&gt; 
   &lt;td&gt;71.0&lt;/td&gt; 
   &lt;td&gt;84.1&lt;/td&gt; 
   &lt;td&gt;37.8&lt;/td&gt; 
   &lt;td&gt;97.0&lt;/td&gt; 
   &lt;td&gt;79.9&lt;/td&gt; 
   &lt;td&gt;85.7&lt;/td&gt; 
   &lt;td&gt;98.5&lt;/td&gt; 
   &lt;td&gt;80.0Â±1.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Infinity-Parser 7B*&lt;/td&gt; 
   &lt;td&gt;84.4&lt;/td&gt; 
   &lt;td&gt;83.8&lt;/td&gt; 
   &lt;td&gt;85.0&lt;/td&gt; 
   &lt;td&gt;47.9&lt;/td&gt; 
   &lt;td&gt;88.7&lt;/td&gt; 
   &lt;td&gt;84.2&lt;/td&gt; 
   &lt;td&gt;86.4&lt;/td&gt; 
   &lt;td&gt;99.8&lt;/td&gt; 
   &lt;td&gt;82.5Â±?&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Chandra OCR 0.1.0*&lt;/td&gt; 
   &lt;td&gt;82.2&lt;/td&gt; 
   &lt;td&gt;80.3&lt;/td&gt; 
   &lt;td&gt;88.0&lt;/td&gt; 
   &lt;td&gt;50.4&lt;/td&gt; 
   &lt;td&gt;90.8&lt;/td&gt; 
   &lt;td&gt;81.2&lt;/td&gt; 
   &lt;td&gt;92.3&lt;/td&gt; 
   &lt;td&gt;99.9&lt;/td&gt; 
   &lt;td&gt;83.1Â±0.9&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td colspan="10"&gt;
    &lt;hr /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;olmOCR v0.4.0&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;83.0&lt;/td&gt; 
   &lt;td&gt;82.3&lt;/td&gt; 
   &lt;td&gt;84.9&lt;/td&gt; 
   &lt;td&gt;47.7&lt;/td&gt; 
   &lt;td&gt;96.1&lt;/td&gt; 
   &lt;td&gt;83.7&lt;/td&gt; 
   &lt;td&gt;81.9&lt;/td&gt; 
   &lt;td&gt;99.7&lt;/td&gt; 
   &lt;td&gt;82.4Â±1.1&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;Requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Recent NVIDIA GPU (tested on RTX 4090, L40S, A100, H100) with at least 15 GB of GPU RAM&lt;/li&gt; 
 &lt;li&gt;30GB of free disk space&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You will need to install poppler-utils and additional fonts for rendering PDF images.&lt;/p&gt; 
&lt;p&gt;Install dependencies (Ubuntu/Debian)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update
sudo apt-get install poppler-utils ttf-mscorefonts-installer msttcorefonts fonts-crosextra-caladea fonts-crosextra-carlito gsfonts lcdf-typetools
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Set up a conda environment and install olmocr. The requirements for running olmOCR are difficult to install in an existing python environment, so please do make a clean python environment to install into.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n olmocr python=3.11
conda activate olmocr

# For CPU-only operations, ex running the benchmark
pip install olmocr[bench]

# For actually converting the files with your own GPU
pip install olmocr[gpu]  --extra-index-url https://download.pytorch.org/whl/cu128

# Recommended: Install flash infer for faster inference on GPU
pip install https://download.pytorch.org/whl/cu128/flashinfer/flashinfer_python-0.2.5%2Bcu128torch2.7-cp38-abi3-linux_x86_64.whl
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Local Usage Example&lt;/h3&gt; 
&lt;p&gt;For quick testing, try the &lt;a href="https://olmocr.allen.ai/"&gt;web demo&lt;/a&gt;. To run locally, a GPU is required, as inference is powered by &lt;a href="https://github.com/sgl-project/sglang"&gt;sglang&lt;/a&gt; under the hood.&lt;/p&gt; 
&lt;p&gt;Convert a Single PDF:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Download a sample PDF
curl -o olmocr-sample.pdf https://olmocr.allenai.org/papers/olmocr_3pg_sample.pdf

# Convert it to markdown
python -m olmocr.pipeline ./localworkspace --markdown --pdfs olmocr-sample.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Convert an Image file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m olmocr.pipeline ./localworkspace --markdown --pdfs random_page.png
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Convert Multiple PDFs:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m olmocr.pipeline ./localworkspace --markdown --pdfs tests/gnarly_pdfs/*.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With the addition of the &lt;code&gt;--markdown&lt;/code&gt; flag, results will be stored as markdown files inside of &lt;code&gt;./localworkspace/markdown/&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;Viewing Results&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;./localworkspace/&lt;/code&gt; workspace folder will then have both &lt;a href="https://github.com/allenai/dolma"&gt;Dolma&lt;/a&gt; and markdown files (if using &lt;code&gt;--markdown&lt;/code&gt;).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cat localworkspace/markdown/olmocr-sample.md 
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using an Inference Provider or External Server&lt;/h3&gt; 
&lt;p&gt;If you have a vLLM server already running elsewhere (or any inference platform implementing the OpenAI API), you can point olmOCR to use it instead of spawning a local instance:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Use external vLLM server instead of local one
python -m olmocr.pipeline ./localworkspace --server http://remote-server:8000/v1 --markdown --pdfs tests/gnarly_pdfs/*.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The served model name should be &lt;code&gt;olmocr&lt;/code&gt;. An example vLLM launch command would be:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vllm serve allenai/olmOCR-2-7B-1025-FP8 --served-model-name olmocr --max-model-len 16384
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Verified External Providers&lt;/h4&gt; 
&lt;p&gt;We have tested &lt;code&gt;olmOCR-2-7B-1025-FP8&lt;/code&gt; on these external model providers and confirmed that they work&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;$/1M Input tokens&lt;/th&gt; 
   &lt;th&gt;$/1M Output tokens&lt;/th&gt; 
   &lt;th&gt;Example Command&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://ai2endpoints.cirrascale.ai/models/overview"&gt;Cirrascale&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;$0.07&lt;/td&gt; 
   &lt;td&gt;$0.15&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;python -m olmocr.pipeline ./localworkspace1 --server https://ai2endpoints.cirrascale.ai/api --api_key sk-XXXXXXX --model olmOCR-2-7B-1025 --pdfs tests/gnarly_pdfs/*.pdf&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://deepinfra.com/"&gt;DeepInfra&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;$0.09&lt;/td&gt; 
   &lt;td&gt;$0.19&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;python -m olmocr.pipeline ./localworkspace1 --server https://api.deepinfra.com/v1/openai --api_key DfXXXXXXX --model allenai/olmOCR-2-7B-1025 --pdfs tests/gnarly_pdfs/*.pdf&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.saas.parasail.io/serverless?name=olmocr-7b-1025-fp8"&gt;Parasail&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;$0.10&lt;/td&gt; 
   &lt;td&gt;$0.20&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;python -m olmocr.pipeline ./localworkspace1 --server https://api.parasail.io/v1 --api_key psk-XXXXX --model allenai/olmOCR-2-7B-1025 --pdfs tests/gnarly_pdfs/*.pdf&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Notes on arguments&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--server&lt;/code&gt;: Defines the OpenAI-compatible endpoint: ex &lt;code&gt;https://api.deepinfra.com/v1/openai&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--api_key&lt;/code&gt;: Your API key, bassed in via Authorization Bearer HTTP header&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--pages_per_group&lt;/code&gt;: You may want a smaller number of pages per group as many external provides have lower concurrent request limits&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model&lt;/code&gt;: The model identifier, ex. &lt;code&gt;allenai/olmOCR-2-7B-1025&lt;/code&gt;, different providers have different names, and if you run locally, you can use &lt;code&gt;olmocr&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Other arguments work the same as with local inference&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Multi-node / Cluster Usage&lt;/h3&gt; 
&lt;p&gt;If you want to convert millions of PDFs, using multiple nodes running in parallel, then olmOCR supports reading your PDFs from AWS S3, and coordinating work using an AWS S3 output bucket.&lt;/p&gt; 
&lt;p&gt;For example, you can start this command on your first worker node, and it will set up a simple work queue in your AWS bucket and start converting PDFs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace --pdfs s3://my_s3_bucket/jakep/gnarly_pdfs/*.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now on any subsequent nodes, just run this and they will start grabbing items from the same workspace queue.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are at Ai2 and want to linearize millions of PDFs efficiently using &lt;a href="https://www.beaker.org"&gt;beaker&lt;/a&gt;, just add the &lt;code&gt;--beaker&lt;/code&gt; flag. This will prepare the workspace on your local machine, and then launch N GPU workers in the cluster to start converting PDFs.&lt;/p&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace --pdfs s3://my_s3_bucket/jakep/gnarly_pdfs/*.pdf --beaker --beaker_gpus 4
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using Docker&lt;/h3&gt; 
&lt;p&gt;Pull the Docker image.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker pull alleninstituteforai/olmocr:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run the container interactively:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -it --gpus all --name olmocr_container alleninstituteforai/olmocr:latest /bin/bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to access your local files inside the container, use volume mounting:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -it --gpus all \
  -v /path/to/your/local/files:/local_files \
  --name olmocr_container \
  alleninstituteforai/olmocr:latest /bin/bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;All dependencies are already installed. Once youâ€™re inside the container, you can run olmOCR commands. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -o olmocr-sample.pdf https://olmocr.allenai.org/papers/olmocr_3pg_sample.pdf

python -m olmocr.pipeline ./localworkspace --markdown --pdfs olmocr-sample.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;You can also visit our Docker repository on &lt;a href="https://hub.docker.com/r/alleninstituteforai/olmocr"&gt;Docker Hub&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Full documentation for the pipeline&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m olmocr.pipeline --help
usage: pipeline.py [-h] [--pdfs [PDFS ...]] [--model MODEL] [--workspace_profile WORKSPACE_PROFILE] [--pdf_profile PDF_PROFILE] [--pages_per_group PAGES_PER_GROUP] [--max_page_retries MAX_PAGE_RETRIES] [--max_page_error_rate MAX_PAGE_ERROR_RATE] [--workers WORKERS]
                   [--apply_filter] [--stats] [--markdown] [--target_longest_image_dim TARGET_LONGEST_IMAGE_DIM] [--target_anchor_text_len TARGET_ANCHOR_TEXT_LEN] [--guided_decoding] [--gpu-memory-utilization GPU_MEMORY_UTILIZATION] [--max_model_len MAX_MODEL_LEN]
                   [--tensor-parallel-size TENSOR_PARALLEL_SIZE] [--data-parallel-size DATA_PARALLEL_SIZE] [--port PORT] [--server SERVER] [--beaker] [--beaker_workspace BEAKER_WORKSPACE] [--beaker_cluster BEAKER_CLUSTER] [--beaker_gpus BEAKER_GPUS] [--beaker_priority BEAKER_PRIORITY]
                   workspace

Manager for running millions of PDFs through a batch inference pipeline

positional arguments:
  workspace             The filesystem path where work will be stored, can be a local folder, or an s3 path if coordinating work with many workers, s3://bucket/prefix/

options:
  -h, --help            show this help message and exit
  --pdfs [PDFS ...]     Path to add pdfs stored in s3 to the workspace, can be a glob path s3://bucket/prefix/*.pdf or path to file containing list of pdf paths
  --model MODEL         Path where the model is located, allenai/olmOCR-7B-0725-FP8 is the default, can be local, s3, or hugging face.
  --workspace_profile WORKSPACE_PROFILE
                        S3 configuration profile for accessing the workspace
  --pdf_profile PDF_PROFILE
                        S3 configuration profile for accessing the raw pdf documents
  --pages_per_group PAGES_PER_GROUP
                        Aiming for this many pdf pages per work item group
  --max_page_retries MAX_PAGE_RETRIES
                        Max number of times we will retry rendering a page
  --max_page_error_rate MAX_PAGE_ERROR_RATE
                        Rate of allowable failed pages in a document, 1/250 by default
  --workers WORKERS     Number of workers to run at a time
  --apply_filter        Apply basic filtering to English pdfs which are not forms, and not likely seo spam
  --stats               Instead of running any job, reports some statistics about the current workspace
  --markdown            Also write natural text to markdown files preserving the folder structure of the input pdfs
  --target_longest_image_dim TARGET_LONGEST_IMAGE_DIM
                        Dimension on longest side to use for rendering the pdf pages
  --target_anchor_text_len TARGET_ANCHOR_TEXT_LEN
                        Maximum amount of anchor text to use (characters), not used for new models
  --guided_decoding     Enable guided decoding for model YAML type outputs

VLLM arguments:
  --gpu-memory-utilization GPU_MEMORY_UTILIZATION
                        Fraction of VRAM vLLM may pre-allocate for KV-cache (passed through to vllm serve).
  --max_model_len MAX_MODEL_LEN
                        Upper bound (tokens) vLLM will allocate KV-cache for, lower if VLLM won't start
  --tensor-parallel-size TENSOR_PARALLEL_SIZE, -tp TENSOR_PARALLEL_SIZE
                        Tensor parallel size for vLLM
  --data-parallel-size DATA_PARALLEL_SIZE, -dp DATA_PARALLEL_SIZE
                        Data parallel size for vLLM
  --port PORT           Port to use for the VLLM server
  --server SERVER       URL of external vLLM (or other compatible provider)
                        server (e.g., http://hostname:port). If provided,
                        skips spawning local vLLM instance

beaker/cluster execution:
  --beaker              Submit this job to beaker instead of running locally
  --beaker_workspace BEAKER_WORKSPACE
                        Beaker workspace to submit to
  --beaker_cluster BEAKER_CLUSTER
                        Beaker clusters you want to run on
  --beaker_gpus BEAKER_GPUS
                        Number of gpu replicas to run
  --beaker_priority BEAKER_PRIORITY
                        Beaker priority level for the job
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Code overview&lt;/h2&gt; 
&lt;p&gt;There are some nice reusable pieces of the code that may be useful for your own projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A prompting strategy to get really good natural text parsing using ChatGPT 4o - &lt;a href="https://github.com/allenai/olmocr/raw/main/olmocr/data/buildsilver.py"&gt;buildsilver.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Basic filtering by language and SEO spam removal - &lt;a href="https://github.com/allenai/olmocr/raw/main/olmocr/filter/filter.py"&gt;filter.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;SFT Finetuning code for Qwen2.5-VL - &lt;a href="https://github.com/allenai/olmocr/raw/main/olmocr/train/train.py"&gt;train.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;GRPO RL Trainer - &lt;a href="https://github.com/allenai/olmocr/raw/main/olmocr/train/grpo_train.py"&gt;grpo_train.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Synthetic data generation - &lt;a href="https://github.com/allenai/olmocr/raw/main/olmocr/bench/synth/mine_html_templates.py"&gt;mine_html_templates.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Processing millions of PDFs through a finetuned model using VLLM - &lt;a href="https://github.com/allenai/olmocr/raw/main/olmocr/pipeline.py"&gt;pipeline.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Viewing &lt;a href="https://github.com/allenai/dolma"&gt;Dolma docs&lt;/a&gt; created from PDFs - &lt;a href="https://github.com/allenai/olmocr/raw/main/olmocr/viewer/dolmaviewer.py"&gt;dolmaviewer.py&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Team&lt;/h2&gt; 
&lt;!-- start team --&gt; 
&lt;p&gt;&lt;strong&gt;olmOCR&lt;/strong&gt; is developed and maintained by the AllenNLP team, backed by &lt;a href="https://allenai.org/"&gt;the Allen Institute for Artificial Intelligence (AI2)&lt;/a&gt;. AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering. To learn more about who specifically contributed to this codebase, see &lt;a href="https://github.com/allenai/olmocr/graphs/contributors"&gt;our contributors&lt;/a&gt; page.&lt;/p&gt; 
&lt;!-- end team --&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;!-- start license --&gt; 
&lt;p&gt;&lt;strong&gt;olmOCR&lt;/strong&gt; is licensed under &lt;a href="https://www.apache.org/licenses/LICENSE-2.0"&gt;Apache 2.0&lt;/a&gt;. A full copy of the license can be found &lt;a href="https://github.com/allenai/olmocr/raw/main/LICENSE"&gt;on GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- end license --&gt; 
&lt;h2&gt;Citing&lt;/h2&gt; 
&lt;p&gt;For olmOCR v1 and OlmOCR-bench:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{olmocrbench,
      title={{olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models}},
      author={Jake Poznanski and Jon Borchardt and Jason Dunkelberger and Regan Huff and Daniel Lin and Aman Rangapur and Christopher Wilhelm and Kyle Lo and Luca Soldaini},
      year={2025},
      eprint={2502.18443},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.18443},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For olmOCR v2 Unit Testing Rewards with RL:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{olmocr2,
      title={olmOCR 2: Unit Test Rewards for Document OCR}, 
      author={Jake Poznanski and Luca Soldaini and Kyle Lo},
      year={2025},
      eprint={2510.19817},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2510.19817}, 
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>langchain-ai/deepagents</title>
      <link>https://github.com/langchain-ai/deepagents</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ğŸ§ ğŸ¤–Deep Agents&lt;/h1&gt; 
&lt;p&gt;Using an LLM to call tools in a loop is the simplest form of an agent. This architecture, however, can yield agents that are â€œshallowâ€ and fail to plan and act over longer, more complex tasks.&lt;/p&gt; 
&lt;p&gt;Applications like â€œDeep Researchâ€, "Manus", and â€œClaude Codeâ€ have gotten around this limitation by implementing a combination of four things: a &lt;strong&gt;planning tool&lt;/strong&gt;, &lt;strong&gt;sub agents&lt;/strong&gt;, access to a &lt;strong&gt;file system&lt;/strong&gt;, and a &lt;strong&gt;detailed prompt&lt;/strong&gt;.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/langchain-ai/deepagents/master/deep_agents.png" alt="deep agent" width="600" /&gt; 
&lt;p&gt;&lt;code&gt;deepagents&lt;/code&gt; is a Python package that implements these in a general purpose way so that you can easily create a Deep Agent for your application. For a full overview and quickstart of &lt;code&gt;deepagents&lt;/code&gt;, the best resource is our &lt;a href="https://docs.langchain.com/oss/python/deepagents/overview"&gt;docs&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Acknowledgements: This project was primarily inspired by Claude Code, and initially was largely an attempt to see what made Claude Code general purpose, and make it even more so.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# pip
pip install deepagents

# uv
uv add deepagents

# poetry
poetry add deepagents
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;(To run the example below, you will need to &lt;code&gt;pip install tavily-python&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;Make sure to set &lt;code&gt;TAVILY_API_KEY&lt;/code&gt; in your environment. You can generate one &lt;a href="https://www.tavily.com/"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
from typing import Literal
from tavily import TavilyClient
from deepagents import create_deep_agent

tavily_client = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])

# Web search tool
def internet_search(
    query: str,
    max_results: int = 5,
    topic: Literal["general", "news", "finance"] = "general",
    include_raw_content: bool = False,
):
    """Run a web search"""
    return tavily_client.search(
        query,
        max_results=max_results,
        include_raw_content=include_raw_content,
        topic=topic,
    )


# System prompt to steer the agent to be an expert researcher
research_instructions = """You are an expert researcher. Your job is to conduct thorough research, and then write a polished report.

You have access to an internet search tool as your primary means of gathering information.

## `internet_search`

Use this to run an internet search for a given query. You can specify the max number of results to return, the topic, and whether raw content should be included.
"""

# Create the deep agent
agent = create_deep_agent(
    tools=[internet_search],
    system_prompt=research_instructions,
)

# Invoke the agent
result = agent.invoke({"messages": [{"role": "user", "content": "What is langgraph?"}]})
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/langchain-ai/deepagents/master/examples/research/research_agent.py"&gt;examples/research/research_agent.py&lt;/a&gt; for a more complex example.&lt;/p&gt; 
&lt;p&gt;The agent created with &lt;code&gt;create_deep_agent&lt;/code&gt; is just a LangGraph graph - so you can interact with it (streaming, human-in-the-loop, memory, studio) in the same way you would any LangGraph agent.&lt;/p&gt; 
&lt;h2&gt;Core Capabilities&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Planning &amp;amp; Task Decomposition&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Deep Agents include a built-in &lt;code&gt;write_todos&lt;/code&gt; tool that enables agents to break down complex tasks into discrete steps, track progress, and adapt plans as new information emerges.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Context Management&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;File system tools (&lt;code&gt;ls&lt;/code&gt;, &lt;code&gt;read_file&lt;/code&gt;, &lt;code&gt;write_file&lt;/code&gt;, &lt;code&gt;edit_file&lt;/code&gt;, &lt;code&gt;glob&lt;/code&gt;, &lt;code&gt;grep&lt;/code&gt;) allow agents to offload large context to memory, preventing context window overflow and enabling work with variable-length tool results.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Subagent Spawning&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;A built-in &lt;code&gt;task&lt;/code&gt; tool enables agents to spawn specialized subagents for context isolation. This keeps the main agentâ€™s context clean while still going deep on specific subtasks.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Long-term Memory&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Extend agents with persistent memory across threads using LangGraphâ€™s Store. Agents can save and retrieve information from previous conversations.&lt;/p&gt; 
&lt;h2&gt;Customizing Deep Agents&lt;/h2&gt; 
&lt;p&gt;There are several parameters you can pass to &lt;code&gt;create_deep_agent&lt;/code&gt; to create your own custom deep agent.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;model&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;By default, &lt;code&gt;deepagents&lt;/code&gt; uses &lt;code&gt;"claude-sonnet-4-5-20250929"&lt;/code&gt;. You can customize this by passing any &lt;a href="https://python.langchain.com/docs/integrations/chat/"&gt;LangChain model object&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from langchain.chat_models import init_chat_model
from deepagents import create_deep_agent

model = init_chat_model("openai:gpt-4o")
agent = create_deep_agent(
    model=model,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;code&gt;system_prompt&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;Deep Agents come with a built-in system prompt. This is relatively detailed prompt that is heavily based on and inspired by &lt;a href="https://github.com/kn1026/cc/raw/main/claudecode.md"&gt;attempts&lt;/a&gt; to &lt;a href="https://github.com/asgeirtj/system_prompts_leaks/raw/main/Anthropic/claude-code.md"&gt;replicate&lt;/a&gt; Claude Code's system prompt. It was made more general purpose than Claude Code's system prompt. The default prompt contains detailed instructions for how to use the built-in planning tool, file system tools, and sub agents.&lt;/p&gt; 
&lt;p&gt;Each deep agent tailored to a use case should include a custom system prompt specific to that use case as well. The importance of prompting for creating a successful deep agent cannot be overstated.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from deepagents import create_deep_agent

research_instructions = """You are an expert researcher. Your job is to conduct thorough research, and then write a polished report.
"""

agent = create_deep_agent(
    system_prompt=research_instructions,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;code&gt;tools&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;Just like with tool-calling agents, you can provide a deep agent with a set of tools that it has access to.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
from typing import Literal
from tavily import TavilyClient
from deepagents import create_deep_agent

tavily_client = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])

def internet_search(
    query: str,
    max_results: int = 5,
    topic: Literal["general", "news", "finance"] = "general",
    include_raw_content: bool = False,
):
    """Run a web search"""
    return tavily_client.search(
        query,
        max_results=max_results,
        include_raw_content=include_raw_content,
        topic=topic,
    )

agent = create_deep_agent(
    tools=[internet_search]
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;code&gt;middleware&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;create_deep_agent&lt;/code&gt; is implemented with middleware that can be customized. You can provide additional middleware to extend functionality, add tools, or implement custom hooks.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from langchain_core.tools import tool
from deepagents import create_deep_agent
from langchain.agents.middleware import AgentMiddleware

@tool
def get_weather(city: str) -&amp;gt; str:
    """Get the weather in a city."""
    return f"The weather in {city} is sunny."

@tool
def get_temperature(city: str) -&amp;gt; str:
    """Get the temperature in a city."""
    return f"The temperature in {city} is 70 degrees Fahrenheit."

class WeatherMiddleware(AgentMiddleware):
  tools = [get_weather, get_temperature]

agent = create_deep_agent(
    model="anthropic:claude-sonnet-4-20250514",
    middleware=[WeatherMiddleware()]
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;code&gt;subagents&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;A main feature of Deep Agents is their ability to spawn subagents. You can specify custom subagents that your agent can hand off work to in the subagents parameter. Sub agents are useful for context quarantine (to help not pollute the overall context of the main agent) as well as custom instructions.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;subagents&lt;/code&gt; should be a list of dictionaries, where each dictionary follow this schema:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;class SubAgent(TypedDict):
    name: str
    description: str
    prompt: str
    tools: Sequence[BaseTool | Callable | dict[str, Any]]
    model: NotRequired[str | BaseChatModel]
    middleware: NotRequired[list[AgentMiddleware]]
    interrupt_on: NotRequired[dict[str, bool | InterruptOnConfig]]

class CompiledSubAgent(TypedDict):
    name: str
    description: str
    runnable: Runnable
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;SubAgent fields:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;name&lt;/strong&gt;: This is the name of the subagent, and how the main agent will call the subagent&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;description&lt;/strong&gt;: This is the description of the subagent that is shown to the main agent&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;prompt&lt;/strong&gt;: This is the prompt used for the subagent&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;tools&lt;/strong&gt;: This is the list of tools that the subagent has access to.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;model&lt;/strong&gt;: Optional model name or model instance.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;middleware&lt;/strong&gt; Additional middleware to attach to the subagent. See &lt;a href="https://docs.langchain.com/oss/python/langchain/middleware"&gt;here&lt;/a&gt; for an introduction into middleware and how it works with create_agent.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;interrupt_on&lt;/strong&gt; A custom interrupt config that specifies human-in-the-loop interactions for your tools.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;CompiledSubAgent fields:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;name&lt;/strong&gt;: This is the name of the subagent, and how the main agent will call the subagent&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;description&lt;/strong&gt;: This is the description of the subagent that is shown to the main agent&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;runnable&lt;/strong&gt;: A pre-built LangGraph graph/agent that will be used as the subagent&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Using SubAgent&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
from typing import Literal
from tavily import TavilyClient
from deepagents import create_deep_agent

tavily_client = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])

def internet_search(
    query: str,
    max_results: int = 5,
    topic: Literal["general", "news", "finance"] = "general",
    include_raw_content: bool = False,
):
    """Run a web search"""
    return tavily_client.search(
        query,
        max_results=max_results,
        include_raw_content=include_raw_content,
        topic=topic,
    )

research_subagent = {
    "name": "research-agent",
    "description": "Used to research more in depth questions",
    "system_prompt": "You are a great researcher",
    "tools": [internet_search],
    "model": "openai:gpt-4o",  # Optional override, defaults to main agent model
}
subagents = [research_subagent]

agent = create_deep_agent(
    model="anthropic:claude-sonnet-4-20250514",
    subagents=subagents
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Using CustomSubAgent&lt;/h4&gt; 
&lt;p&gt;For more complex use cases, you can provide your own pre-built LangGraph graph as a subagent:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Create a custom agent graph
custom_graph = create_agent(
    model=your_model,
    tools=specialized_tools,
    prompt="You are a specialized agent for data analysis..."
)

# Use it as a custom subagent
custom_subagent = CompiledSubAgent(
    name="data-analyzer",
    description="Specialized agent for complex data analysis tasks",
    runnable=custom_graph
)

subagents = [custom_subagent]

agent = create_deep_agent(
    model="anthropic:claude-sonnet-4-20250514",
    tools=[internet_search],
    system_prompt=research_instructions,
    subagents=subagents
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;code&gt;interrupt_on&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;A common reality for agents is that some tool operations may be sensitive and require human approval before execution. Deep Agents supports human-in-the-loop workflows through LangGraphâ€™s interrupt capabilities. You can configure which tools require approval using a checkpointer.&lt;/p&gt; 
&lt;p&gt;These tool configs are passed to our prebuilt &lt;a href="https://docs.langchain.com/oss/python/langchain/middleware#human-in-the-loop"&gt;HITL middleware&lt;/a&gt; so that the agent pauses execution and waits for feedback from the user before executing configured tools.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from langchain_core.tools import tool
from deepagents import create_deep_agent

@tool
def get_weather(city: str) -&amp;gt; str:
    """Get the weather in a city."""
    return f"The weather in {city} is sunny."

agent = create_deep_agent(
    model="anthropic:claude-sonnet-4-20250514",
    tools=[get_weather],
    interrupt_on={
        "get_weather": {
            "allowed_decisions": ["approve", "edit", "reject"]
        },
    }
)

&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Deep Agents Middleware&lt;/h2&gt; 
&lt;p&gt;Deep Agents are built with a modular middleware architecture. As a reminder, Deep Agents have access to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A planning tool&lt;/li&gt; 
 &lt;li&gt;A filesystem for storing context and long-term memories&lt;/li&gt; 
 &lt;li&gt;The ability to spawn subagents&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Each of these features is implemented as separate middleware. When you create a deep agent with &lt;code&gt;create_deep_agent&lt;/code&gt;, we automatically attach &lt;strong&gt;PlanningMiddleware&lt;/strong&gt;, &lt;strong&gt;FilesystemMiddleware&lt;/strong&gt; and &lt;strong&gt;SubAgentMiddleware&lt;/strong&gt; to your agent.&lt;/p&gt; 
&lt;p&gt;Middleware is a composable concept, and you can choose to add as many or as few middleware to an agent depending on your use case. That means that you can also use any of the aforementioned middleware independently!&lt;/p&gt; 
&lt;h3&gt;TodoListMiddleware&lt;/h3&gt; 
&lt;p&gt;Planning is integral to solving complex problems. If youâ€™ve used claude code recently, youâ€™ll notice how it writes out a To-Do list before tackling complex, multi-part tasks. Youâ€™ll also notice how it can adapt and update this To-Do list on the fly as more information comes in.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;TodoListMiddleware&lt;/strong&gt; provides your agent with a tool specifically for updating this To-Do list. Before, and while it executes a multi-part task, the agent is prompted to use the write_todos tool to keep track of what its doing, and what still needs to be done.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from langchain.agents import create_agent
from langchain.agents.middleware import TodoListMiddleware

# TodoListMiddleware is included by default in create_deep_agent
# You can customize it if building a custom agent
agent = create_agent(
    model="anthropic:claude-sonnet-4-20250514",
    # Custom planning instructions can be added via middleware
    middleware=[
        TodoListMiddleware(
            system_prompt="Use the write_todos tool to..."  # Optional: Custom addition to the system prompt
        ),
    ],
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;FilesystemMiddleware&lt;/h3&gt; 
&lt;p&gt;Context engineering is one of the main challenges in building effective agents. This can be particularly hard when using tools that can return variable length results (ex. web_search, rag), as long ToolResults can quickly fill up your context window. &lt;strong&gt;FilesystemMiddleware&lt;/strong&gt; provides four tools to your agent to interact with both short-term and long-term memory.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ls&lt;/strong&gt;: List the files in your filesystem&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;read_file&lt;/strong&gt;: Read an entire file, or a certain number of lines from a file&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;write_file&lt;/strong&gt;: Write a new file to your filesystem&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;edit_file&lt;/strong&gt;: Edit an existing file in your filesystem&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from langchain.agents import create_agent
from deepagents.middleware.filesystem import FilesystemMiddleware


# FilesystemMiddleware is included by default in create_deep_agent
# You can customize it if building a custom agent
agent = create_agent(
    model="anthropic:claude-sonnet-4-20250514",
    middleware=[
        FilesystemMiddleware(
            backend=..., # Optional: customize storage backend
            system_prompt="Write to the filesystem when...",  # Optional custom system prompt override
            custom_tool_descriptions={
                "ls": "Use the ls tool when...",
                "read_file": "Use the read_file tool to..."
            }  # Optional: Custom descriptions for filesystem tools
        ),
    ],
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;SubAgentMiddleware&lt;/h3&gt; 
&lt;p&gt;Handing off tasks to subagents is a great way to isolate context, keeping the context window of the main (supervisor) agent clean while still going deep on a task. The subagents middleware allows you supply subagents through a task tool.&lt;/p&gt; 
&lt;p&gt;A subagent is defined with a name, description, system prompt, and tools. You can also provide a subagent with a custom model, or with additional middleware. This can be particularly useful when you want to give the subagent an additional state key to share with the main agent.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from langchain_core.tools import tool
from langchain.agents import create_agent
from deepagents.middleware.subagents import SubAgentMiddleware


@tool
def get_weather(city: str) -&amp;gt; str:
    """Get the weather in a city."""
    return f"The weather in {city} is sunny."

agent = create_agent(
    model="claude-sonnet-4-20250514",
    middleware=[
        SubAgentMiddleware(
            default_model="claude-sonnet-4-20250514",
            default_tools=[],
            subagents=[
                {
                    "name": "weather",
                    "description": "This subagent can get weather in cities.",
                    "system_prompt": "Use the get_weather tool to get the weather in a city.",
                    "tools": [get_weather],
                    "model": "gpt-4.1",
                    "middleware": [],
                }
            ],
        )
    ],
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more complex use cases, you can also provide your own pre-built LangGraph graph as a subagent.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Create a custom LangGraph graph
def create_weather_graph():
    workflow = StateGraph(...)
    # Build your custom graph
    return workflow.compile()

weather_graph = create_weather_graph()

# Wrap it in a CompiledSubAgent
weather_subagent = CompiledSubAgent(
    name="weather",
    description="This subagent can get weather in cities.",
    runnable=weather_graph
)

agent = create_agent(
    model="anthropic:claude-sonnet-4-20250514",
    middleware=[
        SubAgentMiddleware(
            default_model="claude-sonnet-4-20250514",
            default_tools=[],
            subagents=[weather_subagent],
        )
    ],
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Sync vs Async&lt;/h2&gt; 
&lt;p&gt;Prior versions of deepagents separated sync and async agent factories.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;async_create_deep_agent&lt;/code&gt; has been folded in to &lt;code&gt;create_deep_agent&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;You should use &lt;code&gt;create_deep_agent&lt;/code&gt; as the factory for both sync and async agents&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;MCP&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;deepagents&lt;/code&gt; library can be ran with MCP tools. This can be achieved by using the &lt;a href="https://github.com/langchain-ai/langchain-mcp-adapters"&gt;Langchain MCP Adapter library&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; You will want to use &lt;code&gt;from deepagents import async_create_deep_agent&lt;/code&gt; to use the async version of &lt;code&gt;deepagents&lt;/code&gt;, since MCP tools are async&lt;/p&gt; 
&lt;p&gt;(To run the example below, will need to &lt;code&gt;pip install langchain-mcp-adapters&lt;/code&gt;)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from langchain_mcp_adapters.client import MultiServerMCPClient
from deepagents import create_deep_agent

async def main():
    # Collect MCP tools
    mcp_client = MultiServerMCPClient(...)
    mcp_tools = await mcp_client.get_tools()

    # Create agent
    agent = create_deep_agent(tools=mcp_tools, ....)

    # Stream the agent
    async for chunk in agent.astream(
        {"messages": [{"role": "user", "content": "what is langgraph?"}]},
        stream_mode="values"
    ):
        if "messages" in chunk:
            chunk["messages"][-1].pretty_print()

asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>666ghj/BettaFish</title>
      <link>https://github.com/666ghj/BettaFish</link>
      <description>&lt;p&gt;å¾®èˆ†ï¼šäººäººå¯ç”¨çš„å¤šAgentèˆ†æƒ…åˆ†æåŠ©æ‰‹ï¼Œæ‰“ç ´ä¿¡æ¯èŒ§æˆ¿ï¼Œè¿˜åŸèˆ†æƒ…åŸè²Œï¼Œé¢„æµ‹æœªæ¥èµ°å‘ï¼Œè¾…åŠ©å†³ç­–ï¼ä»0å®ç°ï¼Œä¸ä¾èµ–ä»»ä½•æ¡†æ¶ã€‚&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/logo_compressed.png" alt="Weibo Public Opinion Analysis System Logo" width="100%" /&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/15286" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/15286" alt="666ghj%2FBettaFish | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://leaflow.net/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/Leaflow_logo.png" alt="666ghj%2FWeibo_PublicOpinion_AnalysisSystem | Leaflow" style="width: 150px;" width="150" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/666ghj/Weibo_PublicOpinion_AnalysisSystem?style=flat-square" alt="GitHub Stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/watchers"&gt;&lt;img src="https://img.shields.io/github/watchers/666ghj/Weibo_PublicOpinion_AnalysisSystem?style=flat-square" alt="GitHub Watchers" /&gt;&lt;/a&gt; &lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/network"&gt;&lt;img src="https://img.shields.io/github/forks/666ghj/Weibo_PublicOpinion_AnalysisSystem?style=flat-square" alt="GitHub Forks" /&gt;&lt;/a&gt; &lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/issues"&gt;&lt;img src="https://img.shields.io/github/issues/666ghj/Weibo_PublicOpinion_AnalysisSystem?style=flat-square" alt="GitHub Issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/pulls"&gt;&lt;img src="https://img.shields.io/github/issues-pr/666ghj/Weibo_PublicOpinion_AnalysisSystem?style=flat-square" alt="GitHub Pull Requests" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/666ghj/Weibo_PublicOpinion_AnalysisSystem?style=flat-square" alt="GitHub License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem"&gt;&lt;img src="https://img.shields.io/badge/version-v1.0.0-green.svg?style=flat-square" alt="Version" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/"&gt;&lt;img src="https://img.shields.io/badge/Docker-Build-2496ED?style=flat-square&amp;amp;logo=docker&amp;amp;logoColor=white" alt="Docker" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/666ghj/BettaFish/main/README-EN.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/666ghj/BettaFish/main/README.md"&gt;ä¸­æ–‡æ–‡æ¡£&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;âš¡ é¡¹ç›®æ¦‚è¿°&lt;/h2&gt; 
&lt;p&gt;â€œ&lt;strong&gt;å¾®èˆ†&lt;/strong&gt;â€ æ˜¯ä¸€ä¸ªä»0å®ç°çš„åˆ›æ–°å‹ å¤šæ™ºèƒ½ä½“ èˆ†æƒ…åˆ†æç³»ç»Ÿï¼Œå¸®åŠ©å¤§å®¶ç ´é™¤ä¿¡æ¯èŒ§æˆ¿ï¼Œè¿˜åŸèˆ†æƒ…åŸè²Œï¼Œé¢„æµ‹æœªæ¥èµ°å‘ï¼Œè¾…åŠ©å†³ç­–ã€‚ç”¨æˆ·åªéœ€åƒèŠå¤©ä¸€æ ·æå‡ºåˆ†æéœ€æ±‚ï¼Œæ™ºèƒ½ä½“å¼€å§‹å…¨è‡ªåŠ¨åˆ†æ å›½å†…å¤–30+ä¸»æµç¤¾åª’ ä¸ æ•°ç™¾ä¸‡æ¡å¤§ä¼—è¯„è®ºã€‚&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;â€œå¾®èˆ†â€è°éŸ³â€œå¾®é±¼â€ï¼ŒBettaFishæ˜¯ä¸€ç§ä½“å‹å¾ˆå°ä½†éå¸¸å¥½æ–—ã€æ¼‚äº®çš„é±¼ï¼Œå®ƒè±¡å¾ç€â€œå°è€Œå¼ºå¤§ï¼Œä¸ç•æŒ‘æˆ˜â€&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;æŸ¥çœ‹ç³»ç»Ÿä»¥â€œæ­¦æ±‰å¤§å­¦èˆ†æƒ…â€ä¸ºä¾‹ï¼Œç”Ÿæˆçš„ç ”ç©¶æŠ¥å‘Šï¼š&lt;a href="https://raw.githubusercontent.com/666ghj/BettaFish/main/final_reports/final_report__20250827_131630.html"&gt;æ­¦æ±‰å¤§å­¦å“ç‰Œå£°èª‰æ·±åº¦åˆ†ææŠ¥å‘Š&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;ä¸ä»…ä»…ä½“ç°åœ¨æŠ¥å‘Šè´¨é‡ä¸Šï¼Œç›¸æ¯”åŒç±»äº§å“ï¼Œæˆ‘ä»¬æ‹¥æœ‰ğŸš€å…­å¤§ä¼˜åŠ¿ï¼š&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;AIé©±åŠ¨çš„å…¨åŸŸç›‘æ§&lt;/strong&gt;ï¼šAIçˆ¬è™«é›†ç¾¤7x24å°æ—¶ä¸é—´æ–­ä½œä¸šï¼Œå…¨é¢è¦†ç›–å¾®åšã€å°çº¢ä¹¦ã€æŠ–éŸ³ã€å¿«æ‰‹ç­‰10+å›½å†…å¤–å…³é”®ç¤¾åª’ã€‚ä¸ä»…å®æ—¶æ•è·çƒ­ç‚¹å†…å®¹ï¼Œæ›´èƒ½ä¸‹é’»è‡³æµ·é‡ç”¨æˆ·è¯„è®ºï¼Œè®©æ‚¨å¬åˆ°æœ€çœŸå®ã€æœ€å¹¿æ³›çš„å¤§ä¼—å£°éŸ³ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;è¶…è¶ŠLLMçš„å¤åˆåˆ†æå¼•æ“&lt;/strong&gt;ï¼šæˆ‘ä»¬ä¸ä»…ä¾èµ–è®¾è®¡çš„5ç±»ä¸“ä¸šAgentï¼Œæ›´èåˆäº†å¾®è°ƒæ¨¡å‹ã€ç»Ÿè®¡æ¨¡å‹ç­‰ä¸­é—´ä»¶ã€‚é€šè¿‡å¤šæ¨¡å‹ååŒå·¥ä½œï¼Œç¡®ä¿äº†åˆ†æç»“æœçš„æ·±åº¦ã€å‡†åº¦ä¸å¤šç»´è§†è§’ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;å¼ºå¤§çš„å¤šæ¨¡æ€èƒ½åŠ›&lt;/strong&gt;ï¼šçªç ´å›¾æ–‡é™åˆ¶ï¼Œèƒ½æ·±åº¦è§£ææŠ–éŸ³ã€å¿«æ‰‹ç­‰çŸ­è§†é¢‘å†…å®¹ï¼Œå¹¶ç²¾å‡†æå–ç°ä»£æœç´¢å¼•æ“ä¸­çš„å¤©æ°”ã€æ—¥å†ã€è‚¡ç¥¨ç­‰ç»“æ„åŒ–å¤šæ¨¡æ€ä¿¡æ¯å¡ç‰‡ï¼Œè®©æ‚¨å…¨é¢æŒæ¡èˆ†æƒ…åŠ¨æ€ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Agentâ€œè®ºå›â€åä½œæœºåˆ¶&lt;/strong&gt;ï¼šä¸ºä¸åŒAgentèµ‹äºˆç‹¬ç‰¹çš„å·¥å…·é›†ä¸æ€ç»´æ¨¡å¼ï¼Œå¼•å…¥è¾©è®ºä¸»æŒäººæ¨¡å‹ï¼Œé€šè¿‡â€œè®ºå›â€æœºåˆ¶è¿›è¡Œé“¾å¼æ€ç»´ç¢°æ’ä¸è¾©è®ºã€‚è¿™ä¸ä»…é¿å…äº†å•ä¸€æ¨¡å‹çš„æ€ç»´å±€é™ä¸äº¤æµå¯¼è‡´çš„åŒè´¨åŒ–ï¼Œæ›´å‚¬ç”Ÿå‡ºæ›´é«˜è´¨é‡çš„é›†ä½“æ™ºèƒ½ä¸å†³ç­–æ”¯æŒã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;å…¬ç§åŸŸæ•°æ®æ— ç¼èåˆ&lt;/strong&gt;ï¼šå¹³å°ä¸ä»…åˆ†æå…¬å¼€èˆ†æƒ…ï¼Œè¿˜æä¾›é«˜å®‰å…¨æ€§çš„æ¥å£ï¼Œæ”¯æŒæ‚¨å°†å†…éƒ¨ä¸šåŠ¡æ•°æ®åº“ä¸èˆ†æƒ…æ•°æ®æ— ç¼é›†æˆã€‚æ‰“é€šæ•°æ®å£å’ï¼Œä¸ºå‚ç›´ä¸šåŠ¡æä¾›â€œå¤–éƒ¨è¶‹åŠ¿+å†…éƒ¨æ´å¯Ÿâ€çš„å¼ºå¤§åˆ†æèƒ½åŠ›ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;è½»é‡åŒ–ä¸é«˜æ‰©å±•æ€§æ¡†æ¶&lt;/strong&gt;ï¼šåŸºäºçº¯Pythonæ¨¡å—åŒ–è®¾è®¡ï¼Œå®ç°è½»é‡åŒ–ã€ä¸€é”®å¼éƒ¨ç½²ã€‚ä»£ç ç»“æ„æ¸…æ™°ï¼Œå¼€å‘è€…å¯è½»æ¾é›†æˆè‡ªå®šä¹‰æ¨¡å‹ä¸ä¸šåŠ¡é€»è¾‘ï¼Œå®ç°å¹³å°çš„å¿«é€Ÿæ‰©å±•ä¸æ·±åº¦å®šåˆ¶ã€‚&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;å§‹äºèˆ†æƒ…ï¼Œè€Œä¸æ­¢äºèˆ†æƒ…&lt;/strong&gt;ã€‚â€œå¾®èˆ†â€çš„ç›®æ ‡ï¼Œæ˜¯æˆä¸ºé©±åŠ¨ä¸€åˆ‡ä¸šåŠ¡åœºæ™¯çš„ç®€æ´é€šç”¨çš„æ•°æ®åˆ†æå¼•æ“ã€‚&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ä¸¾ä¸ªä¾‹å­. ä½ åªéœ€ç®€å•ä¿®æ”¹Agentå·¥å…·é›†çš„apiå‚æ•°ä¸promptï¼Œå°±å¯ä»¥æŠŠä»–å˜æˆä¸€ä¸ªé‡‘èé¢†åŸŸçš„å¸‚åœºåˆ†æç³»ç»Ÿ&lt;/p&gt; 
 &lt;p&gt;é™„ä¸€ä¸ªæ¯”è¾ƒæ´»è·ƒçš„Lç«™é¡¹ç›®è®¨è®ºå¸–ï¼š&lt;a href="https://linux.do/t/topic/1009280"&gt;https://linux.do/t/topic/1009280&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/system_schematic.png" alt="banner" width="800" /&gt; 
 &lt;p&gt;å‘Šåˆ«ä¼ ç»Ÿçš„æ•°æ®çœ‹æ¿ï¼Œåœ¨â€œå¾®èˆ†â€ï¼Œä¸€åˆ‡ç”±ä¸€ä¸ªç®€å•çš„é—®é¢˜å¼€å§‹ï¼Œæ‚¨åªéœ€åƒå¯¹è¯ä¸€æ ·ï¼Œæå‡ºæ‚¨çš„åˆ†æéœ€æ±‚&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ—ï¸ ç³»ç»Ÿæ¶æ„&lt;/h2&gt; 
&lt;h3&gt;æ•´ä½“æ¶æ„å›¾&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Insight Agent&lt;/strong&gt; ç§æœ‰æ•°æ®åº“æŒ–æ˜ï¼šç§æœ‰èˆ†æƒ…æ•°æ®åº“æ·±åº¦åˆ†æAIä»£ç†&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Media Agent&lt;/strong&gt; å¤šæ¨¡æ€å†…å®¹åˆ†æï¼šå…·å¤‡å¼ºå¤§å¤šæ¨¡æ€èƒ½åŠ›çš„AIä»£ç†&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Query Agent&lt;/strong&gt; ç²¾å‡†ä¿¡æ¯æœç´¢ï¼šå…·å¤‡å›½å†…å¤–ç½‘é¡µæœç´¢èƒ½åŠ›çš„AIä»£ç†&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Report Agent&lt;/strong&gt; æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆï¼šå†…ç½®æ¨¡æ¿çš„å¤šè½®æŠ¥å‘Šç”ŸæˆAIä»£ç†&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/framework.png" alt="banner" width="800" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;ä¸€æ¬¡å®Œæ•´åˆ†ææµç¨‹&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;æ­¥éª¤&lt;/th&gt; 
   &lt;th&gt;é˜¶æ®µåç§°&lt;/th&gt; 
   &lt;th&gt;ä¸»è¦æ“ä½œ&lt;/th&gt; 
   &lt;th&gt;å‚ä¸ç»„ä»¶&lt;/th&gt; 
   &lt;th&gt;å¾ªç¯ç‰¹æ€§&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;ç”¨æˆ·æé—®&lt;/td&gt; 
   &lt;td&gt;Flaskä¸»åº”ç”¨æ¥æ”¶æŸ¥è¯¢&lt;/td&gt; 
   &lt;td&gt;Flaskä¸»åº”ç”¨&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;å¹¶è¡Œå¯åŠ¨&lt;/td&gt; 
   &lt;td&gt;ä¸‰ä¸ªAgentåŒæ—¶å¼€å§‹å·¥ä½œ&lt;/td&gt; 
   &lt;td&gt;Query Agentã€Media Agentã€Insight Agent&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;åˆæ­¥åˆ†æ&lt;/td&gt; 
   &lt;td&gt;å„Agentä½¿ç”¨ä¸“å±å·¥å…·è¿›è¡Œæ¦‚è§ˆæœç´¢&lt;/td&gt; 
   &lt;td&gt;å„Agent + ä¸“å±å·¥å…·é›†&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;ç­–ç•¥åˆ¶å®š&lt;/td&gt; 
   &lt;td&gt;åŸºäºåˆæ­¥ç»“æœåˆ¶å®šåˆ†å—ç ”ç©¶ç­–ç•¥&lt;/td&gt; 
   &lt;td&gt;å„Agentå†…éƒ¨å†³ç­–æ¨¡å—&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5-N&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;å¾ªç¯é˜¶æ®µ&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;è®ºå›åä½œ + æ·±åº¦ç ”ç©¶&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;ForumEngine + æ‰€æœ‰Agent&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;å¤šè½®å¾ªç¯&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5.1&lt;/td&gt; 
   &lt;td&gt;æ·±åº¦ç ”ç©¶&lt;/td&gt; 
   &lt;td&gt;å„AgentåŸºäºè®ºå›ä¸»æŒäººå¼•å¯¼è¿›è¡Œä¸“é¡¹æœç´¢&lt;/td&gt; 
   &lt;td&gt;å„Agent + åæ€æœºåˆ¶ + è®ºå›å¼•å¯¼&lt;/td&gt; 
   &lt;td&gt;æ¯è½®å¾ªç¯&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5.2&lt;/td&gt; 
   &lt;td&gt;è®ºå›åä½œ&lt;/td&gt; 
   &lt;td&gt;ForumEngineç›‘æ§Agentå‘è¨€å¹¶ç”Ÿæˆä¸»æŒäººæ€»ç»“&lt;/td&gt; 
   &lt;td&gt;ForumEngine + LLMä¸»æŒäºº&lt;/td&gt; 
   &lt;td&gt;æ¯è½®å¾ªç¯&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5.3&lt;/td&gt; 
   &lt;td&gt;äº¤æµèåˆ&lt;/td&gt; 
   &lt;td&gt;å„Agentæ ¹æ®è®¨è®ºè°ƒæ•´ç ”ç©¶æ–¹å‘&lt;/td&gt; 
   &lt;td&gt;å„Agent + forum_readerå·¥å…·&lt;/td&gt; 
   &lt;td&gt;æ¯è½®å¾ªç¯&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;N+1&lt;/td&gt; 
   &lt;td&gt;ç»“æœæ•´åˆ&lt;/td&gt; 
   &lt;td&gt;Report Agentæ”¶é›†æ‰€æœ‰åˆ†æç»“æœå’Œè®ºå›å†…å®¹&lt;/td&gt; 
   &lt;td&gt;Report Agent&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;N+2&lt;/td&gt; 
   &lt;td&gt;æŠ¥å‘Šç”Ÿæˆ&lt;/td&gt; 
   &lt;td&gt;åŠ¨æ€é€‰æ‹©æ¨¡æ¿å’Œæ ·å¼ï¼Œå¤šè½®ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š&lt;/td&gt; 
   &lt;td&gt;Report Agent + æ¨¡æ¿å¼•æ“&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;é¡¹ç›®ä»£ç ç»“æ„æ ‘&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;Weibo_PublicOpinion_AnalysisSystem/
â”œâ”€â”€ QueryEngine/                   # å›½å†…å¤–æ–°é—»å¹¿åº¦æœç´¢Agent
â”‚   â”œâ”€â”€ agent.py                   # Agentä¸»é€»è¾‘
â”‚   â”œâ”€â”€ llms/                      # LLMæ¥å£å°è£…
â”‚   â”œâ”€â”€ nodes/                     # å¤„ç†èŠ‚ç‚¹
â”‚   â”œâ”€â”€ tools/                     # æœç´¢å·¥å…·
â”‚   â”œâ”€â”€ utils/                     # å·¥å…·å‡½æ•°
â”‚   â””â”€â”€ ...                        # å…¶ä»–æ¨¡å—
â”œâ”€â”€ MediaEngine/                   # å¼ºå¤§çš„å¤šæ¨¡æ€ç†è§£Agent
â”‚   â”œâ”€â”€ agent.py                   # Agentä¸»é€»è¾‘
â”‚   â”œâ”€â”€ nodes/                     # å¤„ç†èŠ‚ç‚¹
â”‚   â”œâ”€â”€ llms/                      # LLMæ¥å£
â”‚   â”œâ”€â”€ tools/                     # æœç´¢å·¥å…·
â”‚   â”œâ”€â”€ utils/                     # å·¥å…·å‡½æ•°
â”‚   â””â”€â”€ ...                        # å…¶ä»–æ¨¡å—
â”œâ”€â”€ InsightEngine/                 # ç§æœ‰æ•°æ®åº“æŒ–æ˜Agent
â”‚   â”œâ”€â”€ agent.py                   # Agentä¸»é€»è¾‘
â”‚   â”œâ”€â”€ llms/                      # LLMæ¥å£å°è£…
â”‚   â”‚   â””â”€â”€ base.py                # ç»Ÿä¸€çš„ OpenAI å…¼å®¹å®¢æˆ·ç«¯
â”‚   â”œâ”€â”€ nodes/                     # å¤„ç†èŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ base_node.py           # åŸºç¡€èŠ‚ç‚¹ç±»
â”‚   â”‚   â”œâ”€â”€ formatting_node.py     # æ ¼å¼åŒ–èŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ report_structure_node.py # æŠ¥å‘Šç»“æ„èŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ search_node.py         # æœç´¢èŠ‚ç‚¹
â”‚   â”‚   â””â”€â”€ summary_node.py        # æ€»ç»“èŠ‚ç‚¹
â”‚   â”œâ”€â”€ tools/                     # æ•°æ®åº“æŸ¥è¯¢å’Œåˆ†æå·¥å…·
â”‚   â”‚   â”œâ”€â”€ keyword_optimizer.py   # Qwenå…³é”®è¯ä¼˜åŒ–ä¸­é—´ä»¶
â”‚   â”‚   â”œâ”€â”€ search.py              # æ•°æ®åº“æ“ä½œå·¥å…·é›†
â”‚   â”‚   â””â”€â”€ sentiment_analyzer.py  # æƒ…æ„Ÿåˆ†æé›†æˆå·¥å…·
â”‚   â”œâ”€â”€ state/                     # çŠ¶æ€ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ state.py               # AgentçŠ¶æ€å®šä¹‰
â”‚   â”œâ”€â”€ prompts/                   # æç¤ºè¯æ¨¡æ¿
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ prompts.py             # å„ç±»æç¤ºè¯
â”‚   â””â”€â”€ utils/                     # å·¥å…·å‡½æ•°
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py              # é…ç½®ç®¡ç†
â”‚       â””â”€â”€ text_processing.py     # æ–‡æœ¬å¤„ç†å·¥å…·
â”œâ”€â”€ ReportEngine/                  # å¤šè½®æŠ¥å‘Šç”ŸæˆAgent
â”‚   â”œâ”€â”€ agent.py                   # Agentä¸»é€»è¾‘
â”‚   â”œâ”€â”€ llms/                      # LLMæ¥å£
â”‚   â”œâ”€â”€ nodes/                     # æŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ template_selection.py  # æ¨¡æ¿é€‰æ‹©èŠ‚ç‚¹
â”‚   â”‚   â””â”€â”€ html_generation.py     # HTMLç”ŸæˆèŠ‚ç‚¹
â”‚   â”œâ”€â”€ report_template/           # æŠ¥å‘Šæ¨¡æ¿åº“
â”‚   â”‚   â”œâ”€â”€ ç¤¾ä¼šå…¬å…±çƒ­ç‚¹äº‹ä»¶åˆ†æ.md
â”‚   â”‚   â”œâ”€â”€ å•†ä¸šå“ç‰Œèˆ†æƒ…ç›‘æµ‹.md
â”‚   â”‚   â””â”€â”€ ...                    # æ›´å¤šæ¨¡æ¿
â”‚   â””â”€â”€ flask_interface.py         # Flask APIæ¥å£
â”œâ”€â”€ ForumEngine/                   # è®ºå›å¼•æ“ç®€æ˜“å®ç°
â”‚   â”œâ”€â”€ monitor.py                 # æ—¥å¿—ç›‘æ§å’Œè®ºå›ç®¡ç†
â”‚   â””â”€â”€ llm_host.py                # è®ºå›ä¸»æŒäººLLMæ¨¡å—
â”œâ”€â”€ MindSpider/                    # å¾®åšçˆ¬è™«ç³»ç»Ÿ
â”‚   â”œâ”€â”€ main.py                    # çˆ¬è™«ä¸»ç¨‹åº
â”‚   â”œâ”€â”€ config.py                  # çˆ¬è™«é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ BroadTopicExtraction/      # è¯é¢˜æå–æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ database_manager.py    # æ•°æ®åº“ç®¡ç†å™¨
â”‚   â”‚   â”œâ”€â”€ get_today_news.py      # ä»Šæ—¥æ–°é—»è·å–
â”‚   â”‚   â”œâ”€â”€ main.py                # è¯é¢˜æå–ä¸»ç¨‹åº
â”‚   â”‚   â””â”€â”€ topic_extractor.py     # è¯é¢˜æå–å™¨
â”‚   â”œâ”€â”€ DeepSentimentCrawling/     # æ·±åº¦èˆ†æƒ…çˆ¬å–
â”‚   â”‚   â”œâ”€â”€ keyword_manager.py     # å…³é”®è¯ç®¡ç†å™¨
â”‚   â”‚   â”œâ”€â”€ main.py                # æ·±åº¦çˆ¬å–ä¸»ç¨‹åº
â”‚   â”‚   â”œâ”€â”€ MediaCrawler/          # åª’ä½“çˆ¬è™«æ ¸å¿ƒ
â”‚   â”‚   â””â”€â”€ platform_crawler.py    # å¹³å°çˆ¬è™«ç®¡ç†
â”‚   â””â”€â”€ schema/                    # æ•°æ®åº“ç»“æ„
â”‚       â”œâ”€â”€ db_manager.py          # æ•°æ®åº“ç®¡ç†å™¨
â”‚       â”œâ”€â”€ init_database.py       # æ•°æ®åº“åˆå§‹åŒ–
â”‚       â””â”€â”€ mindspider_tables.sql  # æ•°æ®åº“è¡¨ç»“æ„
â”œâ”€â”€ SentimentAnalysisModel/        # æƒ…æ„Ÿåˆ†ææ¨¡å‹é›†åˆ
â”‚   â”œâ”€â”€ WeiboSentiment_Finetuned/  # å¾®è°ƒBERT/GPT-2æ¨¡å‹
â”‚   â”œâ”€â”€ WeiboMultilingualSentiment/# å¤šè¯­è¨€æƒ…æ„Ÿåˆ†æï¼ˆæ¨èï¼‰
â”‚   â”œâ”€â”€ WeiboSentiment_SmallQwen/  # å°å‚æ•°Qwen3å¾®è°ƒ
â”‚   â””â”€â”€ WeiboSentiment_MachineLearning/ # ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•
â”œâ”€â”€ SingleEngineApp/               # å•ç‹¬Agentçš„Streamlitåº”ç”¨
â”‚   â”œâ”€â”€ query_engine_streamlit_app.py
â”‚   â”œâ”€â”€ media_engine_streamlit_app.py
â”‚   â””â”€â”€ insight_engine_streamlit_app.py
â”œâ”€â”€ templates/                     # Flaskæ¨¡æ¿
â”‚   â””â”€â”€ index.html                 # ä¸»ç•Œé¢å‰ç«¯
â”œâ”€â”€ static/                        # é™æ€èµ„æº
â”œâ”€â”€ logs/                          # è¿è¡Œæ—¥å¿—ç›®å½•
â”œâ”€â”€ final_reports/                 # æœ€ç»ˆç”Ÿæˆçš„HTMLæŠ¥å‘Šæ–‡ä»¶
â”œâ”€â”€ utils/                         # é€šç”¨å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ forum_reader.py            # Agenté—´è®ºå›é€šä¿¡
â”‚   â””â”€â”€ retry_helper.py            # ç½‘ç»œè¯·æ±‚é‡è¯•æœºåˆ¶å·¥å…·
â”œâ”€â”€ app.py                         # Flaskä¸»åº”ç”¨å…¥å£
â”œâ”€â”€ config.py                      # å…¨å±€é…ç½®æ–‡ä»¶
â””â”€â”€ requirements.txt               # Pythonä¾èµ–åŒ…æ¸…å•
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸš€ å¿«é€Ÿå¼€å§‹&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;å¦‚æœä½ æ˜¯åˆæ¬¡å­¦ä¹ ä¸€ä¸ªAgentç³»ç»Ÿçš„æ­å»ºï¼Œå¯ä»¥ä»ä¸€ä¸ªéå¸¸ç®€å•çš„demoå¼€å§‹ï¼š&lt;a href="https://github.com/666ghj/DeepSearchAgent-Demo"&gt;Deep Search Agent Demo&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;ç¯å¢ƒè¦æ±‚&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;æ“ä½œç³»ç»Ÿ&lt;/strong&gt;: Windowsã€Linuxã€MacOS&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Pythonç‰ˆæœ¬&lt;/strong&gt;: 3.9+&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Conda&lt;/strong&gt;: Anacondaæˆ–Miniconda&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ•°æ®åº“&lt;/strong&gt;: MySQLï¼ˆå¯é€‰æ‹©æˆ‘ä»¬çš„äº‘æ•°æ®åº“æœåŠ¡ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;å†…å­˜&lt;/strong&gt;: å»ºè®®2GBä»¥ä¸Š&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;1. åˆ›å»ºCondaç¯å¢ƒ&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# åˆ›å»ºcondaç¯å¢ƒ
conda create -n your_conda_name python=3.11
conda activate your_conda_name
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. å®‰è£…ä¾èµ–åŒ…&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# åŸºç¡€ä¾èµ–å®‰è£…
pip install -r requirements.txt
# å¦‚æœä¸æƒ³ä½¿ç”¨æœ¬åœ°æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼ˆç®—åŠ›éœ€æ±‚å¾ˆå°ï¼Œé»˜è®¤å®‰è£…cpuç‰ˆæœ¬ï¼‰ï¼Œå¯ä»¥å°†è¯¥æ–‡ä»¶ä¸­çš„â€œæœºå™¨å­¦ä¹ â€éƒ¨åˆ†æ³¨é‡Šæ‰å†æ‰§è¡ŒæŒ‡ä»¤
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. å®‰è£…Playwrightæµè§ˆå™¨é©±åŠ¨&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# å®‰è£…æµè§ˆå™¨é©±åŠ¨ï¼ˆç”¨äºçˆ¬è™«åŠŸèƒ½ï¼‰
playwright install chromium
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4. é…ç½®ç³»ç»Ÿ&lt;/h3&gt; 
&lt;h4&gt;4.1 é…ç½®APIå¯†é’¥&lt;/h4&gt; 
&lt;p&gt;ç¼–è¾‘ &lt;code&gt;config.py&lt;/code&gt; æ–‡ä»¶ï¼Œå¡«å…¥æ‚¨çš„APIå¯†é’¥ï¼ˆæ‚¨ä¹Ÿå¯ä»¥é€‰æ‹©è‡ªå·±çš„æ¨¡å‹ã€æœç´¢ä»£ç†ï¼Œè¯¦æƒ…è§configæ–‡ä»¶å†…ï¼‰ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# MySQLæ•°æ®åº“é…ç½®
DB_HOST = "localhost"
DB_PORT = 3306
DB_USER = "your_username"
DB_PASSWORD = "your_password"
DB_NAME = "your_db_name"
DB_CHARSET = "utf8mb4"

# LLMé…ç½®
# æ‚¨å¯ä»¥æ›´æ”¹æ¯ä¸ªéƒ¨åˆ†LLMä½¿ç”¨çš„APIï¼Œåªè¦å…¼å®¹OpenAIè¯·æ±‚æ ¼å¼éƒ½å¯ä»¥

# Insight Agent
INSIGHT_ENGINE_API_KEY = "your_api_key"
INSIGHT_ENGINE_BASE_URL = "https://api.moonshot.cn/v1"
INSIGHT_ENGINE_MODEL_NAME = "kimi-k2-0711-preview"
# Media Agent
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;4.2 æ•°æ®åº“åˆå§‹åŒ–&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;é€‰æ‹©1ï¼šä½¿ç”¨æœ¬åœ°æ•°æ®åº“&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;MindSpiderçˆ¬è™«ç³»ç»Ÿè·Ÿèˆ†æƒ…ç³»ç»Ÿæ˜¯å„è‡ªç‹¬ç«‹çš„ï¼Œæ‰€ä»¥éœ€è¦å†å»&lt;code&gt;MindSpider\config.py&lt;/code&gt;é…ç½®ä¸€ä¸‹&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# æœ¬åœ°MySQLæ•°æ®åº“åˆå§‹åŒ–
cd MindSpider
python schema/init_database.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;é€‰æ‹©2ï¼šä½¿ç”¨äº‘æ•°æ®åº“æœåŠ¡ï¼ˆæ¨èï¼‰&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;æˆ‘ä»¬æä¾›ä¾¿æ·çš„äº‘æ•°æ®åº“æœåŠ¡ï¼ŒåŒ…å«æ—¥å‡10ä¸‡+çœŸå®èˆ†æƒ…æ•°æ®ï¼Œç›®å‰&lt;strong&gt;å…è´¹ç”³è¯·&lt;/strong&gt;ï¼&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;çœŸå®èˆ†æƒ…æ•°æ®ï¼Œå®æ—¶æ›´æ–°&lt;/li&gt; 
 &lt;li&gt;å¤šç»´åº¦æ ‡ç­¾åˆ†ç±»&lt;/li&gt; 
 &lt;li&gt;é«˜å¯ç”¨äº‘ç«¯æœåŠ¡&lt;/li&gt; 
 &lt;li&gt;ä¸“ä¸šæŠ€æœ¯æ”¯æŒ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;è”ç³»æˆ‘ä»¬ç”³è¯·å…è´¹äº‘æ•°æ®åº“è®¿é—®ï¼šğŸ“§ &lt;a href="mailto:670939375@qq.com"&gt;670939375@qq.com&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ä¸ºè¿›è¡Œæ•°æ®åˆè§„æ€§å®¡æŸ¥ä¸æœåŠ¡å‡çº§ï¼Œäº‘æ•°æ®åº“è‡ª2025å¹´10æœˆ1æ—¥èµ·æš‚åœæ¥æ”¶æ–°çš„ä½¿ç”¨ç”³è¯·&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;5. å¯åŠ¨ç³»ç»Ÿ&lt;/h3&gt; 
&lt;h4&gt;5.1 å®Œæ•´ç³»ç»Ÿå¯åŠ¨ï¼ˆæ¨èï¼‰&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ï¼Œæ¿€æ´»condaç¯å¢ƒ
conda activate your_conda_name

# å¯åŠ¨ä¸»åº”ç”¨å³å¯
python app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ³¨1ï¼šä¸€æ¬¡è¿è¡Œç»ˆæ­¢åï¼Œstreamlit appå¯èƒ½ç»“æŸå¼‚å¸¸ä»ç„¶å ç”¨ç«¯å£ï¼Œæ­¤æ—¶æœç´¢å ç”¨ç«¯å£çš„è¿›ç¨‹killæ‰å³å¯&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ³¨2ï¼šæ•°æ®çˆ¬å–éœ€è¦å•ç‹¬æ“ä½œï¼Œè§5.3æŒ‡å¼•&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ³¨3ï¼šå¦‚æœæœåŠ¡å™¨è¿œç¨‹éƒ¨ç½²å‡ºç°é¡µé¢æ˜¾ç¤ºé—®é¢˜ï¼Œè§&lt;a href="https://github.com/666ghj/BettaFish/pull/45"&gt;PR#45&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;è®¿é—® &lt;a href="http://localhost:5000"&gt;http://localhost:5000&lt;/a&gt; å³å¯ä½¿ç”¨å®Œæ•´ç³»ç»Ÿ&lt;/p&gt; 
&lt;h4&gt;5.2 å•ç‹¬å¯åŠ¨æŸä¸ªAgent&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# å¯åŠ¨QueryEngine
streamlit run SingleEngineApp/query_engine_streamlit_app.py --server.port 8503

# å¯åŠ¨MediaEngine  
streamlit run SingleEngineApp/media_engine_streamlit_app.py --server.port 8502

# å¯åŠ¨InsightEngine
streamlit run SingleEngineApp/insight_engine_streamlit_app.py --server.port 8501
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;5.3 çˆ¬è™«ç³»ç»Ÿå•ç‹¬ä½¿ç”¨&lt;/h4&gt; 
&lt;p&gt;è¿™éƒ¨åˆ†æœ‰è¯¦ç»†çš„é…ç½®æ–‡æ¡£ï¼š&lt;a href="https://raw.githubusercontent.com/666ghj/BettaFish/main/MindSpider/README.md"&gt;MindeSpiderä½¿ç”¨è¯´æ˜&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="MindSpider\img\example.png" alt="banner" width="600" /&gt; 
 &lt;p&gt;MindSpider è¿è¡Œç¤ºä¾‹&lt;/p&gt; 
&lt;/div&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# è¿›å…¥çˆ¬è™«ç›®å½•
cd MindSpider

# é¡¹ç›®åˆå§‹åŒ–
python main.py --setup

# è¿è¡Œå®Œæ•´çˆ¬è™«æµç¨‹
python main.py --complete --date 2024-01-20

# ä»…è¿è¡Œè¯é¢˜æå–
python main.py --broad-topic --date 2024-01-20

# ä»…è¿è¡Œæ·±åº¦çˆ¬å–
python main.py --deep-sentiment --platforms xhs dy wb
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;âš™ï¸ é«˜çº§é…ç½®&lt;/h2&gt; 
&lt;h3&gt;ä¿®æ”¹å…³é”®å‚æ•°&lt;/h3&gt; 
&lt;h4&gt;Agenté…ç½®å‚æ•°&lt;/h4&gt; 
&lt;p&gt;æ¯ä¸ªAgentéƒ½æœ‰ä¸“é—¨çš„é…ç½®æ–‡ä»¶ï¼Œå¯æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼Œä¸‹é¢æ˜¯éƒ¨åˆ†ç¤ºä¾‹ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# QueryEngine/utils/config.py
class Config:
    max_reflections = 2           # åæ€è½®æ¬¡
    max_search_results = 15       # æœ€å¤§æœç´¢ç»“æœæ•°
    max_content_length = 8000     # æœ€å¤§å†…å®¹é•¿åº¦
    
# MediaEngine/utils/config.py  
class Config:
    comprehensive_search_limit = 10  # ç»¼åˆæœç´¢é™åˆ¶
    web_search_limit = 15           # ç½‘é¡µæœç´¢é™åˆ¶
    
# InsightEngine/utils/config.py
class Config:
    default_search_topic_globally_limit = 200    # å…¨å±€æœç´¢é™åˆ¶
    default_get_comments_limit = 500             # è¯„è®ºè·å–é™åˆ¶
    max_search_results_for_llm = 50              # ä¼ ç»™LLMçš„æœ€å¤§ç»“æœæ•°
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;æƒ…æ„Ÿåˆ†ææ¨¡å‹é…ç½®&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# InsightEngine/tools/sentiment_analyzer.py
SENTIMENT_CONFIG = {
    'model_type': 'multilingual',     # å¯é€‰: 'bert', 'multilingual', 'qwen'ç­‰
    'confidence_threshold': 0.8,      # ç½®ä¿¡åº¦é˜ˆå€¼
    'batch_size': 32,                 # æ‰¹å¤„ç†å¤§å°
    'max_sequence_length': 512,       # æœ€å¤§åºåˆ—é•¿åº¦
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;æ¥å…¥ä¸åŒçš„LLMæ¨¡å‹&lt;/h3&gt; 
&lt;p&gt;æ”¯æŒä»»æ„openAIè°ƒç”¨æ ¼å¼çš„LLMæä¾›å•†ï¼Œåªéœ€è¦åœ¨/config.pyä¸­å¡«å†™å¯¹åº”çš„KEYã€BASE_URLã€MODEL_NAMEå³å¯ã€‚&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ä»€ä¹ˆæ˜¯openAIè°ƒç”¨æ ¼å¼ï¼Ÿä¸‹é¢æä¾›ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼š&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI

client = OpenAI(api_key="your_api_key", 
               base_url="https://api.siliconflow.cn/v1")

response = client.chat.completions.create(
   model="Qwen/Qwen2.5-72B-Instruct",
   messages=[
       {'role': 'user', 
        'content': "æ¨ç†æ¨¡å‹ä¼šç»™å¸‚åœºå¸¦æ¥å“ªäº›æ–°çš„æœºä¼š"}
   ],
)

complete_response = response.choices[0].message.content
print(complete_response)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;æ›´æ”¹æƒ…æ„Ÿåˆ†ææ¨¡å‹&lt;/h3&gt; 
&lt;p&gt;ç³»ç»Ÿé›†æˆäº†å¤šç§æƒ…æ„Ÿåˆ†ææ–¹æ³•ï¼Œå¯æ ¹æ®éœ€æ±‚é€‰æ‹©ï¼š&lt;/p&gt; 
&lt;h4&gt;1. å¤šè¯­è¨€æƒ…æ„Ÿåˆ†æ&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd SentimentAnalysisModel/WeiboMultilingualSentiment
python predict.py --text "This product is amazing!" --lang "en"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;2. å°å‚æ•°Qwen3å¾®è°ƒ&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd SentimentAnalysisModel/WeiboSentiment_SmallQwen
python predict_universal.py --text "è¿™æ¬¡æ´»åŠ¨åŠå¾—å¾ˆæˆåŠŸ"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3. åŸºäºBERTçš„å¾®è°ƒæ¨¡å‹&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ä½¿ç”¨BERTä¸­æ–‡æ¨¡å‹
cd SentimentAnalysisModel/WeiboSentiment_Finetuned/BertChinese-Lora
python predict.py --text "è¿™ä¸ªäº§å“çœŸçš„å¾ˆä¸é”™"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;4. GPT-2 LoRAå¾®è°ƒæ¨¡å‹&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd SentimentAnalysisModel/WeiboSentiment_Finetuned/GPT2-Lora
python predict.py --text "ä»Šå¤©å¿ƒæƒ…ä¸å¤ªå¥½"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;5. ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd SentimentAnalysisModel/WeiboSentiment_MachineLearning
python predict.py --model_type "svm" --text "æœåŠ¡æ€åº¦éœ€è¦æ”¹è¿›"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;æ¥å…¥è‡ªå®šä¹‰ä¸šåŠ¡æ•°æ®åº“&lt;/h3&gt; 
&lt;h4&gt;1. ä¿®æ”¹æ•°æ®åº“è¿æ¥é…ç½®&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# config.py ä¸­æ·»åŠ æ‚¨çš„ä¸šåŠ¡æ•°æ®åº“é…ç½®
BUSINESS_DB_HOST = "your_business_db_host"
BUSINESS_DB_PORT = 3306
BUSINESS_DB_USER = "your_business_user"
BUSINESS_DB_PASSWORD = "your_business_password"
BUSINESS_DB_NAME = "your_business_database"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;2. åˆ›å»ºè‡ªå®šä¹‰æ•°æ®è®¿é—®å·¥å…·&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# InsightEngine/tools/custom_db_tool.py
class CustomBusinessDBTool:
    """è‡ªå®šä¹‰ä¸šåŠ¡æ•°æ®åº“æŸ¥è¯¢å·¥å…·"""
    
    def __init__(self):
        self.connection_config = {
            'host': config.BUSINESS_DB_HOST,
            'port': config.BUSINESS_DB_PORT,
            'user': config.BUSINESS_DB_USER,
            'password': config.BUSINESS_DB_PASSWORD,
            'database': config.BUSINESS_DB_NAME,
        }
    
    def search_business_data(self, query: str, table: str):
        """æŸ¥è¯¢ä¸šåŠ¡æ•°æ®"""
        # å®ç°æ‚¨çš„ä¸šåŠ¡é€»è¾‘
        pass
    
    def get_customer_feedback(self, product_id: str):
        """è·å–å®¢æˆ·åé¦ˆæ•°æ®"""
        # å®ç°å®¢æˆ·åé¦ˆæŸ¥è¯¢é€»è¾‘
        pass
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3. é›†æˆåˆ°InsightEngine&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# InsightEngine/agent.py ä¸­é›†æˆè‡ªå®šä¹‰å·¥å…·
from .tools.custom_db_tool import CustomBusinessDBTool

class DeepSearchAgent:
    def __init__(self, config=None):
        # ... å…¶ä»–åˆå§‹åŒ–ä»£ç 
        self.custom_db_tool = CustomBusinessDBTool()
    
    def execute_custom_search(self, query: str):
        """æ‰§è¡Œè‡ªå®šä¹‰ä¸šåŠ¡æ•°æ®æœç´¢"""
        return self.custom_db_tool.search_business_data(query, "your_table")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;è‡ªå®šä¹‰æŠ¥å‘Šæ¨¡æ¿&lt;/h3&gt; 
&lt;h4&gt;1. åœ¨Webç•Œé¢ä¸­ä¸Šä¼ &lt;/h4&gt; 
&lt;p&gt;ç³»ç»Ÿæ”¯æŒä¸Šä¼ è‡ªå®šä¹‰æ¨¡æ¿æ–‡ä»¶ï¼ˆ.mdæˆ–.txtæ ¼å¼ï¼‰ï¼Œå¯åœ¨ç”ŸæˆæŠ¥å‘Šæ—¶é€‰æ‹©ä½¿ç”¨ã€‚&lt;/p&gt; 
&lt;h4&gt;2. åˆ›å»ºæ¨¡æ¿æ–‡ä»¶&lt;/h4&gt; 
&lt;p&gt;åœ¨ &lt;code&gt;ReportEngine/report_template/&lt;/code&gt; ç›®å½•ä¸‹åˆ›å»ºæ–°çš„æ¨¡æ¿ï¼Œæˆ‘ä»¬çš„Agentä¼šè‡ªè¡Œé€‰ç”¨æœ€åˆé€‚çš„æ¨¡æ¿ã€‚&lt;/p&gt; 
&lt;h2&gt;ğŸ¤ è´¡çŒ®æŒ‡å—&lt;/h2&gt; 
&lt;p&gt;æˆ‘ä»¬æ¬¢è¿æ‰€æœ‰å½¢å¼çš„è´¡çŒ®ï¼&lt;/p&gt; 
&lt;h3&gt;å¦‚ä½•è´¡çŒ®&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Forké¡¹ç›®&lt;/strong&gt;åˆ°æ‚¨çš„GitHubè´¦å·&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;åˆ›å»ºFeatureåˆ†æ”¯&lt;/strong&gt;ï¼š&lt;code&gt;git checkout -b feature/AmazingFeature&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æäº¤æ›´æ”¹&lt;/strong&gt;ï¼š&lt;code&gt;git commit -m 'Add some AmazingFeature'&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ¨é€åˆ°åˆ†æ”¯&lt;/strong&gt;ï¼š&lt;code&gt;git push origin feature/AmazingFeature&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;å¼€å¯Pull Request&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;å¼€å‘è§„èŒƒ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ä»£ç éµå¾ªPEP8è§„èŒƒ&lt;/li&gt; 
 &lt;li&gt;æäº¤ä¿¡æ¯ä½¿ç”¨æ¸…æ™°çš„ä¸­è‹±æ–‡æè¿°&lt;/li&gt; 
 &lt;li&gt;æ–°åŠŸèƒ½éœ€è¦åŒ…å«ç›¸åº”çš„æµ‹è¯•ç”¨ä¾‹&lt;/li&gt; 
 &lt;li&gt;æ›´æ–°ç›¸å…³æ–‡æ¡£&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ¦– ä¸‹ä¸€æ­¥å¼€å‘è®¡åˆ’&lt;/h2&gt; 
&lt;p&gt;ç°åœ¨ç³»ç»Ÿåªå®Œæˆäº†"ä¸‰æ¿æ–§"ä¸­çš„å‰ä¸¤æ­¥ï¼Œå³ï¼šè¾“å…¥è¦æ±‚-&amp;gt;è¯¦ç»†åˆ†æï¼Œè¿˜ç¼ºå°‘ä¸€æ­¥é¢„æµ‹ï¼Œç›´æ¥å°†ä»–ç»§ç»­äº¤ç»™LLMæ˜¯ä¸å…·æœ‰è¯´æœåŠ›çš„ã€‚&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/banner_compressed.png" alt="banner" width="800" /&gt; 
&lt;/div&gt; 
&lt;p&gt;ç›®å‰æˆ‘ä»¬ç»è¿‡å¾ˆé•¿ä¸€æ®µæ—¶é—´çš„çˆ¬å–æ”¶é›†ï¼Œæ‹¥æœ‰äº†å¤§é‡å…¨ç½‘è¯é¢˜çƒ­åº¦éšæ—¶é—´ã€çˆ†ç‚¹ç­‰çš„å˜åŒ–è¶‹åŠ¿çƒ­åº¦æ•°æ®ï¼Œå·²ç»å…·å¤‡äº†å¯ä»¥å¼€å‘é¢„æµ‹æ¨¡å‹çš„æ¡ä»¶ã€‚æˆ‘ä»¬å›¢é˜Ÿå°†è¿ç”¨æ—¶åºæ¨¡å‹ã€å›¾ç¥ç»ç½‘ç»œã€å¤šæ¨¡æ€èåˆç­‰é¢„æµ‹æ¨¡å‹æŠ€æœ¯å‚¨å¤‡äºæ­¤ï¼Œå®ç°çœŸæ­£åŸºäºæ•°æ®é©±åŠ¨çš„èˆ†æƒ…é¢„æµ‹åŠŸèƒ½ã€‚&lt;/p&gt; 
&lt;h2&gt;âš ï¸ å…è´£å£°æ˜&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;é‡è¦æé†’ï¼šæœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ ã€å­¦æœ¯ç ”ç©¶å’Œæ•™è‚²ç›®çš„ä½¿ç”¨&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;åˆè§„æ€§å£°æ˜&lt;/strong&gt;ï¼š&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;æœ¬é¡¹ç›®ä¸­çš„æ‰€æœ‰ä»£ç ã€å·¥å…·å’ŒåŠŸèƒ½å‡ä»…ä¾›å­¦ä¹ ã€å­¦æœ¯ç ”ç©¶å’Œæ•™è‚²ç›®çš„ä½¿ç”¨&lt;/li&gt; 
   &lt;li&gt;ä¸¥ç¦å°†æœ¬é¡¹ç›®ç”¨äºä»»ä½•å•†ä¸šç”¨é€”æˆ–ç›ˆåˆ©æ€§æ´»åŠ¨&lt;/li&gt; 
   &lt;li&gt;ä¸¥ç¦å°†æœ¬é¡¹ç›®ç”¨äºä»»ä½•è¿æ³•ã€è¿è§„æˆ–ä¾µçŠ¯ä»–äººæƒç›Šçš„è¡Œä¸º&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;çˆ¬è™«åŠŸèƒ½å…è´£&lt;/strong&gt;ï¼š&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;é¡¹ç›®ä¸­çš„çˆ¬è™«åŠŸèƒ½ä»…ç”¨äºæŠ€æœ¯å­¦ä¹ å’Œç ”ç©¶ç›®çš„&lt;/li&gt; 
   &lt;li&gt;ä½¿ç”¨è€…å¿…é¡»éµå®ˆç›®æ ‡ç½‘ç«™çš„robots.txtåè®®å’Œä½¿ç”¨æ¡æ¬¾&lt;/li&gt; 
   &lt;li&gt;ä½¿ç”¨è€…å¿…é¡»éµå®ˆç›¸å…³æ³•å¾‹æ³•è§„ï¼Œä¸å¾—è¿›è¡Œæ¶æ„çˆ¬å–æˆ–æ•°æ®æ»¥ç”¨&lt;/li&gt; 
   &lt;li&gt;å› ä½¿ç”¨çˆ¬è™«åŠŸèƒ½äº§ç”Ÿçš„ä»»ä½•æ³•å¾‹åæœç”±ä½¿ç”¨è€…è‡ªè¡Œæ‰¿æ‹…&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;æ•°æ®ä½¿ç”¨å…è´£&lt;/strong&gt;ï¼š&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;é¡¹ç›®æ¶‰åŠçš„æ•°æ®åˆ†æåŠŸèƒ½ä»…ä¾›å­¦æœ¯ç ”ç©¶ä½¿ç”¨&lt;/li&gt; 
   &lt;li&gt;ä¸¥ç¦å°†åˆ†æç»“æœç”¨äºå•†ä¸šå†³ç­–æˆ–ç›ˆåˆ©ç›®çš„&lt;/li&gt; 
   &lt;li&gt;ä½¿ç”¨è€…åº”ç¡®ä¿æ‰€åˆ†ææ•°æ®çš„åˆæ³•æ€§å’Œåˆè§„æ€§&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;æŠ€æœ¯å…è´£&lt;/strong&gt;ï¼š&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;æœ¬é¡¹ç›®æŒ‰"ç°çŠ¶"æä¾›ï¼Œä¸æä¾›ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„ä¿è¯&lt;/li&gt; 
   &lt;li&gt;ä½œè€…ä¸å¯¹ä½¿ç”¨æœ¬é¡¹ç›®é€ æˆçš„ä»»ä½•ç›´æ¥æˆ–é—´æ¥æŸå¤±æ‰¿æ‹…è´£ä»»&lt;/li&gt; 
   &lt;li&gt;ä½¿ç”¨è€…åº”è‡ªè¡Œè¯„ä¼°é¡¹ç›®çš„é€‚ç”¨æ€§å’Œé£é™©&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;è´£ä»»é™åˆ¶&lt;/strong&gt;ï¼š&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ä½¿ç”¨è€…åœ¨ä½¿ç”¨æœ¬é¡¹ç›®å‰åº”å……åˆ†äº†è§£ç›¸å…³æ³•å¾‹æ³•è§„&lt;/li&gt; 
   &lt;li&gt;ä½¿ç”¨è€…åº”ç¡®ä¿å…¶ä½¿ç”¨è¡Œä¸ºç¬¦åˆå½“åœ°æ³•å¾‹æ³•è§„è¦æ±‚&lt;/li&gt; 
   &lt;li&gt;å› è¿åæ³•å¾‹æ³•è§„ä½¿ç”¨æœ¬é¡¹ç›®è€Œäº§ç”Ÿçš„ä»»ä½•åæœç”±ä½¿ç”¨è€…è‡ªè¡Œæ‰¿æ‹…&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;è¯·åœ¨ä½¿ç”¨æœ¬é¡¹ç›®å‰ä»”ç»†é˜…è¯»å¹¶ç†è§£ä¸Šè¿°å…è´£å£°æ˜ã€‚ä½¿ç”¨æœ¬é¡¹ç›®å³è¡¨ç¤ºæ‚¨å·²åŒæ„å¹¶æ¥å—ä¸Šè¿°æ‰€æœ‰æ¡æ¬¾ã€‚&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ“„ è®¸å¯è¯&lt;/h2&gt; 
&lt;p&gt;æœ¬é¡¹ç›®é‡‡ç”¨ &lt;a href="https://raw.githubusercontent.com/666ghj/BettaFish/main/LICENSE"&gt;GPL-2.0è®¸å¯è¯&lt;/a&gt;ã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚é˜…LICENSEæ–‡ä»¶ã€‚&lt;/p&gt; 
&lt;h2&gt;ğŸ‰ æ”¯æŒä¸è”ç³»&lt;/h2&gt; 
&lt;h3&gt;è·å–å¸®åŠ©&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;é¡¹ç›®ä¸»é¡µ&lt;/strong&gt;ï¼š&lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem"&gt;GitHubä»“åº“&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;é—®é¢˜åé¦ˆ&lt;/strong&gt;ï¼š&lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/issues"&gt;Issuesé¡µé¢&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;åŠŸèƒ½å»ºè®®&lt;/strong&gt;ï¼š&lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/discussions"&gt;Discussionsé¡µé¢&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;è”ç³»æ–¹å¼&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“§ &lt;strong&gt;é‚®ç®±&lt;/strong&gt;ï¼š&lt;a href="mailto:670939375@qq.com"&gt;670939375@qq.com&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;å•†åŠ¡åˆä½œ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ä¼ä¸šå®šåˆ¶å¼€å‘&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;å¤§æ•°æ®æœåŠ¡&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;å­¦æœ¯åˆä½œ&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æŠ€æœ¯åŸ¹è®­&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ‘¥ è´¡çŒ®è€…&lt;/h2&gt; 
&lt;p&gt;æ„Ÿè°¢ä»¥ä¸‹ä¼˜ç§€çš„è´¡çŒ®è€…ä»¬ï¼š&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/graphs/contributors"&gt;&lt;img src="https://contrib.rocks/image?repo=666ghj/Weibo_PublicOpinion_AnalysisSystem" alt="Contributors" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ“ˆ é¡¹ç›®ç»Ÿè®¡&lt;/h2&gt; 
&lt;a href="https://www.star-history.com/#666ghj/BettaFish&amp;amp;type=date&amp;amp;legend=top-left"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=666ghj/BettaFish&amp;amp;type=date&amp;amp;theme=dark&amp;amp;legend=top-left" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=666ghj/BettaFish&amp;amp;type=date&amp;amp;legend=top-left" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=666ghj/BettaFish&amp;amp;type=date&amp;amp;legend=top-left" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;p&gt;&lt;img src="https://repobeats.axiom.co/api/embed/e04e3eea4674edc39c148a7845c8d09c1b7b1922.svg?sanitize=true" alt="Alt" title="Repobeats analytics image" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>HKUDS/AI-Researcher</title>
      <link>https://github.com/HKUDS/AI-Researcher</link>
      <description>&lt;p&gt;[NeurIPS2025] "AI-Researcher: Autonomous Scientific Innovation" -- A production-ready version: https://novix.science/chat&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/assets/ai-researcher.png" alt="Logo" width="400" /&gt; 
 &lt;h1 align="center"&gt;"AI-Researcher: Autonomous Scientific Innovation" &lt;/h1&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/14638" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14638" alt="HKUDS%2FAI-Researcher | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://autoresearcher.github.io"&gt;&lt;img src="https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&amp;amp;color=FFE165&amp;amp;logo=homepage&amp;amp;logoColor=white" alt="Project Page" /&gt;&lt;/a&gt; 
 &lt;a href="https://join.slack.com/t/ai-researchergroup/shared_invite/zt-30y5a070k-C0ajQt1zmVczFnfGkIicvA"&gt;&lt;img src="https://img.shields.io/badge/Slack-Join%20Us-red?logo=slack&amp;amp;logoColor=white&amp;amp;style=for-the-badge" alt="Join our Slack community" /&gt;&lt;/a&gt; 
 &lt;a href="https://discord.gg/zBNYTk5q2g"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20Us-purple?logo=discord&amp;amp;logoColor=white&amp;amp;style=for-the-badge" alt="Join our Discord community" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://autoresearcher.github.io/docs"&gt;&lt;img src="https://img.shields.io/badge/Documentation-000?logo=googledocs&amp;amp;logoColor=FFE165&amp;amp;style=for-the-badge" alt="Check out the documentation" /&gt;&lt;/a&gt; 
 &lt;a href="https://arxiv.org/abs/2505.18705"&gt;&lt;img src="https://img.shields.io/badge/Paper%20on%20Arxiv-000?logoColor=FFE165&amp;amp;logo=arxiv&amp;amp;style=for-the-badge" alt="Paper" /&gt;&lt;/a&gt; 
 &lt;a href="https://autoresearcher.github.io/leaderboard"&gt;&lt;img src="https://img.shields.io/badge/DATASETS-000?logoColor=FFE165&amp;amp;logo=huggingface&amp;amp;style=for-the-badge" alt="Benchmark" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/Communication.md"&gt;&lt;img src="https://img.shields.io/badge/ğŸ’¬Feishu-Group-07c160?style=for-the-badge&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; 
 &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/Communication.md"&gt;&lt;img src="https://img.shields.io/badge/WeChat-Group-07c160?style=for-the-badge&amp;amp;logo=wechat&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; 
 &lt;hr /&gt; 
&lt;/div&gt; 
&lt;p&gt;Welcome to &lt;strong&gt;AI-Researcher&lt;/strong&gt;ğŸ¤— AI-Researcher introduces a revolutionary breakthrough in &lt;strong&gt;Automated Scientific Discovery&lt;/strong&gt;ğŸ”¬, presenting a new system that fundamentally &lt;strong&gt;Reshapes the Traditional Research Paradigm&lt;/strong&gt;. This state-of-the-art platform empowers researchers with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ¯ &lt;strong&gt;Full Autonomy&lt;/strong&gt;: Complete end-to-end research automation&lt;/li&gt; 
 &lt;li&gt;ğŸ”„ &lt;strong&gt;Seamless Orchestration&lt;/strong&gt;: From concept to publication&lt;/li&gt; 
 &lt;li&gt;ğŸ§  &lt;strong&gt;Advanced AI Integration&lt;/strong&gt;: Powered by cutting-edge AI agents&lt;/li&gt; 
 &lt;li&gt;ğŸš€ &lt;strong&gt;Research Acceleration&lt;/strong&gt;: Streamlined scientific innovation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;âœ¨ The AI-Researcher system accepts user input queries at two distinct levels âœ¨&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Level 1: Detailed Idea Description&lt;/strong&gt; &lt;br /&gt; At this level, users provide comprehensive descriptions of their specific research ideas. The system processes these detailed inputs to develop implementation strategies based on the user's explicit requirements.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Level 2: Reference-Based Ideation&lt;/strong&gt; &lt;br /&gt; This simpler level involves users submitting reference papers without a specific idea in mind. The user query typically follows the format: "I have some reference papers, please come up with an innovative idea and implement it with these papers." The system then analyzes the provided references to generate and develop novel research concepts.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;ğŸŒŸ&lt;strong&gt;Core Capabilities &amp;amp; Integration&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;AI-Researcher&lt;/strong&gt; delivers a &lt;strong&gt;Comprehensive Research Ecosystem&lt;/strong&gt; through seamless integration of critical components:&lt;/p&gt; 
&lt;p&gt;ğŸš€&lt;strong&gt;Primary Research Functions&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“š &lt;strong&gt;Literature Review&lt;/strong&gt;: Conducts comprehensive analysis and synthesis of existing research.&lt;/li&gt; 
 &lt;li&gt;ğŸ“Š &lt;strong&gt;Idea Generation&lt;/strong&gt;: Systematically gathers, organizes, and formulates novel research directions.&lt;/li&gt; 
 &lt;li&gt;ğŸ§ª &lt;strong&gt;Algorithm Design and Implementation&lt;/strong&gt;: Develops methodologies and transforms ideas into functional implementations.&lt;/li&gt; 
 &lt;li&gt;ğŸ’» &lt;strong&gt;Algorithm Validation and Refinement&lt;/strong&gt;: Automates testing, performance evaluation, and iterative optimization.&lt;/li&gt; 
 &lt;li&gt;ğŸ“ˆ &lt;strong&gt;Result Analysis&lt;/strong&gt;: Delivers advanced interpretation of experimental data and insights.&lt;/li&gt; 
 &lt;li&gt;âœï¸ &lt;strong&gt;Manuscript Creation&lt;/strong&gt;: Automatically generates polished, full-length academic papers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;!-- &lt;img src="./assets/AI-Researchernew-intro.pdf" alt="Logo" width="100%"&gt; --&gt; 
 &lt;figure&gt; 
  &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/assets/AI-Researcher-Framework.png" alt="Logo" style="max-width: 100%; height: auto;" /&gt; 
  &lt;br /&gt; 
  &lt;figcaption&gt;
   &lt;em&gt;Quick Overview of AI-Researcher.&lt;/em&gt;
  &lt;/figcaption&gt; 
 &lt;/figure&gt; 
&lt;/div&gt; 
&lt;span id="news"&gt;&lt;/span&gt; 
&lt;h2&gt;ğŸ”¥ News&lt;/h2&gt; 
&lt;div class="scrollable"&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;[2025. 09]&lt;/strong&gt;: &amp;nbsp; ğŸ¯ğŸ¯ğŸ“¢ğŸ“¢ Exciting News! We are thrilled to announce that our ğŸŒŸAI-ResearcherğŸŒŸ has been accepted as a Spotlight paper at NeurIPS 2025! ğŸ‰ğŸ‰ Thanks to all the team members ğŸ¤—  &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2025. 05]&lt;/strong&gt;: &amp;nbsp;ğŸ‰ğŸ‰ &lt;b&gt;Major Release! AI-Researcher Comprehensive Upgrade!&lt;/b&gt; ğŸš€ &lt;br /&gt;We are excited to announce a significant milestone for AI-Researcher: 
   &lt;ul&gt; 
    &lt;li&gt;ğŸ“„ &lt;b&gt;&lt;a href="https://arxiv.org/abs/2505.18705"&gt;Academic Paper Release&lt;/a&gt;&lt;/b&gt;: Detailed exposition of our innovative methods and experimental results&lt;/li&gt; 
    &lt;li&gt;ğŸ“Š &lt;b&gt;&lt;a href="https://autoresearcher.github.io/leaderboard"&gt;Benchmark Suite&lt;/a&gt;&lt;/b&gt;: Comprehensive evaluation framework and datasets&lt;/li&gt; 
    &lt;li&gt;ğŸ–¥ï¸ &lt;b&gt;Web GUI Interface&lt;/b&gt;: User-friendly graphical interface making research more convenient&lt;/li&gt; 
   &lt;/ul&gt; &lt;b&gt;ğŸ¤ Join Us!&lt;/b&gt; We welcome researchers, developers, and AI enthusiasts to contribute together and advance AI research development. Whether it's code contributions, bug reports, feature suggestions, or documentation improvements, every contribution is valuable! &lt;br /&gt;ğŸ’¡ &lt;i&gt;Let's build a smarter AI research assistant together!&lt;/i&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2025, Mar 04]&lt;/strong&gt;: &amp;nbsp;ğŸ‰ğŸ‰We've launched &lt;b&gt;AI-Researcher!&lt;/b&gt;, The release includes the complete framework, datasets, benchmark construction pipeline, and much more. Stay tunedâ€”there's plenty more to come! ğŸš€&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;span id="table-of-contents"&gt;&lt;/span&gt; 
&lt;h2&gt;ğŸ“‘ Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#news"&gt;ğŸ”¥ News&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#quick-start"&gt;âš¡ Quick Start&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#api-keys-setup"&gt;API Keys Setup&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#examples"&gt;â¬‡ï¸ Examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#how-it-works"&gt;âœ¨ How AI-Researcher works&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#how-to-use"&gt;ğŸ” How to use AI-Researcher&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#documentation"&gt;ğŸ“– Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#community"&gt;ğŸ¤ Join the Community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#acknowledgements"&gt;ğŸ™ Acknowledgements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#cite"&gt;ğŸŒŸ Cite&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="quick-start"&gt;&lt;/span&gt; 
&lt;h2&gt;âš¡ Quick Start&lt;/h2&gt; 
&lt;span id="installation"&gt;&lt;/span&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;h4&gt;AI Installation&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Using &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;We recommend to use &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt; to manage packages in our project (Much more faster than conda)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# install uv
curl -LsSf https://astral.sh/uv/install.sh | sh
source ~/.bashrc

# clone the project
git clone https://github.com/HKUDS/AI-Researcher.git
cd AI-Researcher

# install and activate enviroment
uv venv --python 3.11
source ./.venv/bin/activate
uv pip install -e .
playwright install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Docker Installation&lt;/h4&gt; 
&lt;p&gt;To set up the agent-interactive environment, we use Docker for containerization. Please ensure you have &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt; installed on your system before proceeding. For running the research agent, we utilize the Docker image 'tjbtech1/airesearcher:v1t'. You can pull this image by executing the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker pull tjbtech1/airesearcher:v1
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or you can build the docker image from our provided &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/docker/Dockerfile"&gt;Dockerfile&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd ./docker &amp;amp;&amp;amp; docker build -t tjbtech1/airesearcher:v1 .
&lt;/code&gt;&lt;/pre&gt; 
&lt;span id="api-keys-setup"&gt;&lt;/span&gt; 
&lt;h3&gt;API Keys Setup&lt;/h3&gt; 
&lt;p&gt;Create an environment variable file based on the provided '.env.template' file. In this file, you should set the configuration including api key, instance id of the test case.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;
# ================ container configuration ================
# workplace of the research agent
DOCKER_WORKPLACE_NAME=workplace_paper
# base image of the research agent
BASE_IMAGES=tjbtech1/airesearcher:v1
# completion model name, configuration details see: https://docs.litellm.ai/docs/
COMPLETION_MODEL=openrouter/google/gemini-2.5-pro-preview-05-20
# cheep model name, configuration details see: https://docs.litellm.ai/docs/
CHEEP_MODEL=openrouter/google/gemini-2.5-pro-preview-05-20
# specific gpu of the research agent, can be: 
# '"device=0"' using the first gpu
# '"device=0,1"' using the first and second gpu
# '"all"' using all gpus
# None for no gpu
GPUS='"device=0"'
# name of the container
CONTAINER_NAME=paper_eval
# name of the workplace
WORKPLACE_NAME=workplace
# path of the cache
CACHE_PATH=cache
# port of the research agent
PORT=7020
# platform of the research agent
PLATFORM=linux/amd64

# ================ llm configuration ================
# github ai token of the research agent
GITHUB_AI_TOKEN=your_github_ai_token
# openrouter api key of the research agent
OPENROUTER_API_KEY=your_openrouter_api_key
# openrouter api base url of the research agent
OPENROUTER_API_BASE=https://openrouter.ai/api/v1

# ================ task configuration ================
# category of the research agent, based on: ./benchmark/final. Can be: 
# diffu_flow
# gnn
# reasoning
# recommendation
# vq
# example: ./benchmark/final/vq
CATEGORY=vq
# instance id of the research agent, example: ./benchmark/final/vq/one_layer_vq.json
INSTANCE_ID=one_layer_vq
# task level of the research agent, can be: 
# task1
# task2
TASK_LEVEL=task1
# maximum iteration times of the research agent
MAX_ITER_TIMES=0
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ğŸ”¥ Web GUI&lt;/h3&gt; 
&lt;p&gt;We add a webgui based on gradio. Just run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python web_ai_researcher.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/assets/webgui/image-20250606135137558.png" alt="image-20250606135137558" /&gt;&lt;/p&gt; 
&lt;p&gt;You can configure the environment variables in the following tab:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/assets/webgui/image-20250606135325373.png" alt="image-20250606135325373" /&gt;&lt;/p&gt; 
&lt;p&gt;Select the following example to run our AI-Researcher:&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/assets/webgui/image-20250606135507970.png" alt="image-20250606135507970" style="zoom:67%;" /&gt; 
&lt;span id="examples"&gt;&lt;/span&gt; 
&lt;h2&gt;â¬‡ï¸ Examples&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;âš ï¸ &lt;strong&gt;ALERT&lt;/strong&gt;: The GIFs below are large files and may &lt;strong&gt;take some time to load&lt;/strong&gt;. &lt;strong&gt;Please be patient while they render completely&lt;/strong&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Example 1 (Vector Quantized)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed model designed in this paper is designed to improve the performance of Vector Quantized Variational AutoEncoders (VQ-VAEs) by addressing issues with gradient propagation through the non-differentiable vector quantization layer.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;The core methodologies utilized include: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Rotation and Rescaling Transformation&lt;/strong&gt;: A linear transformation that alters the encoder output to align it with the nearest codebook vector without changing the forward pass output.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Gradient Propagation Method&lt;/strong&gt;: The proposed model ensures that gradients flow from the decoder to the encoder while preserving the angle between the gradient and codebook vector.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Codebook Management&lt;/strong&gt;: Utilizes the connection between the encoder output and the corresponding codebook vectors to mitigate codebook collapse and improve utilization.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;The primary functions of these components are: 
    &lt;ul&gt; 
     &lt;li&gt;The rotation and rescaling transformation modifies how the encoder output is quantized and how information is retained during backpropagation, enabling gradients to reflect the true positioning of the encoder output relative to the codebook vectors.&lt;/li&gt; 
     &lt;li&gt;The gradient propagation method redefines how gradients are transported back to the encoder, allowing for an enhanced and nuanced movement through the quantization layer, which leads to a better performance during training.&lt;/li&gt; 
     &lt;li&gt;Codebook management practices help in maintaining a diverse set of codebook vectors throughout training, avoiding scenarios where multiple vectors become redundant or unused.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Implementation details for each component: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Key Parameters&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Codebook size should be configured based on the complexity of the dataset (e.g., 1024 or 8192).&lt;/li&gt; 
       &lt;li&gt;Commitment loss coefficient (Î²) is typically set within [0.25, 2].&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Input/Output Specifications&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Input to the encoder is a continuous high-dimensional vector, while the output is a corresponding quantized vector from the codebook.&lt;/li&gt; 
       &lt;li&gt;The output for reconstruction is generated using the decoder applied to the transformed codebook vectors.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Important Constraints&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Ensure that the codebook is updated correctly with an exponential moving average procedure, and treat both rotation and rescaling during the forward pass as constants with respect to the gradient.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Step-by-Step Integration of Components: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: Input the data vector into the encoder to obtain the continuous representation.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: Identify the nearest codebook vector to the encoder output.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 3&lt;/strong&gt;: Compute the rotation matrix that aligns the encoder output to the codebook vector.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 4&lt;/strong&gt;: Apply the rotation and rescaling transformation to obtain the modified output for the decoder (i.e., `Ëœ q`).&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 5&lt;/strong&gt;: Feed `Ëœ q` into the decoder to produce the reconstructed output.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 6&lt;/strong&gt;: Compute the loss using the reconstruction and apply backpropagation.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 7&lt;/strong&gt;: During backpropagation, modify the gradient transfer process to maintain the angle using the proposed model, replacing traditional shortcuts in gradient computation.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Critical implementation details affecting performance: 
    &lt;ul&gt; 
     &lt;li&gt;The choice of rotation matrix calculation should ensure computational efficiencyâ€”using Householder transformations to minimize resource demands.&lt;/li&gt; 
     &lt;li&gt;The deployment of the stop-gradient technique effectively turns off the back-propagation through the quantization layer, which is essential to reflect the intended change without inducing undesired noise in the gradient updates.&lt;/li&gt; 
     &lt;li&gt;Monitor the codebook usage regularly during training to detect any potential collapse early and adjust the training dynamics (e.g., learning rate) accordingly to maintain effective utilization throughout the training period.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Neural discrete representation learning&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;Neural discrete representation learning&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Straightening out the straight-through estimator: Overcoming optimization challenges in vector quantized networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Estimating or propagating gradients through stochastic neurons for conditional computation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;High-resolution image synthesis with latent diffusion models&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Finite scalar quantization: Vq-vae made simple&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Elements of information theory&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Vector-quantized image modeling with improved vqgan&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Uvim: A unified modeling approach for vision with learned guiding codes&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Auto-encoding variational bayes&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Categorical reparameterization with gumbel-softmax&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/rotation_vq/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/rotation_vq/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/rotation_vq/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/rotation_vq/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Example 2 (Category: Vector Quantized)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed model focuses on discrete representation learning for tasks such as image generation, depth estimation, colorization, and segmentation using the proposed approach integrated into architectures like autoregressive transformers.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;Core Techniques: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Simplified Quantization&lt;/strong&gt;: Use a simplified quantization approach utilizing scalar quantization instead of VQ.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Dimensionality Projection&lt;/strong&gt;: Define a function to project the encoder output to a manageable dimensionality (typically between 3 to 10).&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Gradient Propagation&lt;/strong&gt;: Implement the Straight-Through Estimator (STE) for gradient propagation through the quantization operation.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Technical Components: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Bounding Function&lt;/strong&gt;: This compresses data dimensionality and confines values to a desired range. Use a function like \(f(z) = \left\lfloor \frac{L}{2} \right\rfloor \tanh(z)\) to project the data, where \(L\) is the number of quantization levels.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Quantization process&lt;/strong&gt;: Round each bounded dimension to its nearest integer to yield the quantized output.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Loss function&lt;/strong&gt;: Operate under a reconstruction loss paradigm typical in VAEs to optimize the proposed model parameters.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Key Parameters&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Number of dimensions \(d\) and levels \(L\) per dimension should be defined based on the codebook size you aim to replicate (e.g., set \(L_i \geq 5\) for all \(i\)).&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Input/Output Specifications&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;The input to the bounding function will be the output from the final encoder layer; the output after quantization will be in the format \(\hat{z}\), with shape matching the original \(z\).&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Constraints&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Ensure all inputs are preprocessed adequately to be within the functioning range of the bounding function.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Step-by-Step Integration: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: Train a standard VAE model and obtain its encoder output \(z\).&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: Apply the bounding function \(f\) on \(z\) to limit the output dimensions to usable values.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 3&lt;/strong&gt;: Quantize the resultant bounded \(z\) using the rounding procedure to generate \( \hat{z} \).&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 4&lt;/strong&gt;: Use the original \(z\) and \(\hat{z}\) in conjunction with the reconstruction loss to backpropagate through the network using the STE for gradient calculation.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Critical Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;Ensure the rounding process is correctly differentiable; utilize the STE to maintain gradient flow during backpropagation.&lt;/li&gt; 
     &lt;li&gt;Maintain high codebook utilization by selecting optimal dimensions and levels based on empirical trials, and monitor performance to refine the parameters if needed.&lt;/li&gt; 
     &lt;li&gt;Adjust the proposed model configurations (number of epochs, batch size) based on the structures laid out in this paper, ensuring hyperparameters match those recommended for the proposed approach integration.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Neural discrete representation learning&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;Neural discrete representation learning&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Conditional probability models for deep image compression&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;High-fidelity generative image compression&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;End-to-end optimized image compression&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Taming transformers for high-resolution image generation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;An algorithm for vector quantizer design&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Joint autoregressive and hierarchical priors for learned image compression&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Assessing generative models via precision and recall&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Variational bayes on discrete representation with self-annealed stochastic quantization&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;High quality monocular depth estimation via transfer learning&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/fsq/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/fsq/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/fsq/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/fsq/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Example 3 (Category: Recommendation)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed model aims to improve user-item interaction predictions in recommendation systems by leveraging heterogeneous relational information.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;Core Techniques/Algorithms: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Heterogeneous Graph Neural Networks (GNNs)&lt;/strong&gt;: Used for embedding initialization and message propagation across different types of user-item and user-user/item-item graphs.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Contrastive Learning&lt;/strong&gt;: Specifically, a cross-view contrastive learning framework is utilized to enhance representation learning by aligning embeddings from auxiliary views with user-item interaction embeddings.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Meta Networks&lt;/strong&gt;: Employed to extract personalized knowledge and facilitate customized knowledge transfer between auxiliary views and the user-item interaction view.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Purpose and Function of Each Major Component: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Heterogeneous GNN&lt;/strong&gt;: Encodes user and item relationships into embeddings that capture the semantics of various interactions.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Contrastive Learning&lt;/strong&gt;: Provides self-supervision signals to enhance the robustness of learned representations, allowing the proposed model to distinguish between relevant and irrelevant interactions.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Meta Network&lt;/strong&gt;: Models personalized characteristics to facilitate adaptive knowledge transfer, ensuring that the influence of auxiliary information is tailored to individual users and items.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Heterogeneous GNN&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;&lt;strong&gt;Key Parameters&lt;/strong&gt;: Use Xavier initializer for embedding initialization; set the hidden dimensionality &lt;code&gt;d&lt;/code&gt;.&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Input/Output&lt;/strong&gt;: Take adjacency matrices for user-item, user-user, and item-item graphs as input; output relation-aware embeddings.&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Constraints&lt;/strong&gt;: Ensure that the GNN can handle varying types of nodes and relations.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Contrastive Learning&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;&lt;strong&gt;Key Parameters&lt;/strong&gt;: Use cosine similarity as the similarity function; define a temperature coefficient for handling negative samples.&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Input/Output&lt;/strong&gt;: Input embeddings from the meta network and user/item views; output contrastive loss values.&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Constraints&lt;/strong&gt;: Maintain diverse representations to avoid overfitting.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Meta Network&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;&lt;strong&gt;Key Parameters&lt;/strong&gt;: Set up fully connected layers with PReLU activation to generate personalized transformation matrices.&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Input/Output&lt;/strong&gt;: Input user and item embeddings; output transformed embeddings for personalized knowledge transfer.&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Constraints&lt;/strong&gt;: Ensure low-rank decomposition of transformation matrices to reduce parameter count.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Step-by-Step Interaction: 
    &lt;ul&gt; 
     &lt;li&gt;Initialize user and item embeddings using a heterogeneous GNN.&lt;/li&gt; 
     &lt;li&gt;Perform heterogeneous message propagation to refine embeddings iteratively across user-item, user-user, and item-item graphs.&lt;/li&gt; 
     &lt;li&gt;Aggregate the refined embeddings from various views using a mean pooling function to retain heterogeneous semantics.&lt;/li&gt; 
     &lt;li&gt;Extract meta knowledge from the learned embeddings to create personalized mapping functions using the meta network.&lt;/li&gt; 
     &lt;li&gt;Apply contrastive learning to align embeddings from auxiliary views with the user-item interaction embeddings, generating a contrastive loss.&lt;/li&gt; 
     &lt;li&gt;Combine the contrastive loss with a pairwise loss function (like Bayesian Personalized Ranking) to optimize the proposed model.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Critical Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;Choose appropriate hyperparameters such as embedding size, learning rate, and the number of GNN layers through systematic experimentation.&lt;/li&gt; 
     &lt;li&gt;Monitor the proposed model for signs of overfitting, especially when increasing the number of GNN layers or embedding dimensions.&lt;/li&gt; 
     &lt;li&gt;Ensure diverse user-item interaction patterns are captured through sufficient training data and effective augmentation techniques.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Revisiting Graph Based Collaborative Filtering: A Linear Residual Graph Convolutional Network Approach&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;Revisiting Graph Based Collaborative Filtering: A Linear Residual Graph Convolutional Network Approach&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Graph Neural Networks for Social Recommendation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Improving Graph Collaborative Filtering with Neighborhood-enriched Contrastive Learning&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Knowledge-aware Coupled Graph Neural Network for Social Recommendation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Heterogeneous Graph Transformer&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Sequential Recommendation with Graph Neural Networks&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/hgcl/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/hgcl/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/hgcl/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/hgcl/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Example 4 (Category: Recommendation)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed model focuses on collaborative filtering for recommendation systems by leveraging graph neural networks (GNNs) and contrastive learning to address the issue of sparse user-item interactions.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;Core Techniques: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Graph Neural Networks&lt;/strong&gt;: Utilize GNNs for message passing to learn user and item embeddings from the interaction graph.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Disentangled Representations&lt;/strong&gt;: Implement a mechanism to model multiple latent intent factors driving user-item interactions.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Contrastive Learning&lt;/strong&gt;: Use contrastive learning techniques to generate adaptive self-supervised signals from augmented views of user-item interactions.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Purpose of Components: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;GNN Layers&lt;/strong&gt;: Capture high-order interactions among users and items through iterative message passing.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Intent Encoding&lt;/strong&gt;: Differentiate latent intents to improve the representation of user preferences.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Adaptive Augmentation&lt;/strong&gt;: Generate contrastive views that account for both local and global dependencies to enhance robustness against noise.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Graph Construction&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;&lt;strong&gt;Input&lt;/strong&gt;: User-item interaction matrix \( A \) of size \( I \times J \) (where \( I \) is the number of users and \( J \) is the number of items).&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: Normalized adjacency matrix \( \bar{A} \).&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;GNN Configuration&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Number of layers \( L \): Choose based on your dataset, typically 2 or 3 layers.&lt;/li&gt; 
       &lt;li&gt;Dimensionality \( d \) of embeddings: Start with \( d = 32 \).&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Intent Prototypes&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Number of intents \( K \): Experiment with values from {32, 64, 128, 256}, starting with \( K = 128 \).&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Learning Rate&lt;/strong&gt;: Use Adam optimizer with a learning rate around \( 1e-3 \).&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Loss Functions&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Use Bayesian Personalized Ranking (BPR) loss for the recommendation task.&lt;/li&gt; 
       &lt;li&gt;Implement InfoNCE loss for contrastive learning, incorporating both local and global augmented views.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Step-by-Step Interaction: 
    &lt;ul&gt; 
     &lt;li&gt;Construct the interaction graph from the user-item matrix.&lt;/li&gt; 
     &lt;li&gt;For each GNN layer: 
      &lt;ul&gt; 
       &lt;li&gt;Compute the aggregated embeddings \( Z(u) \) and \( Z(v) \) using the normalized adjacency matrix.&lt;/li&gt; 
       &lt;li&gt;Update user and item embeddings using residual connections to prevent over-smoothing.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;Generate intent-aware representations by aggregating embeddings over the latent intents.&lt;/li&gt; 
     &lt;li&gt;Apply the learned parameterized masks for adaptive augmentation during message passing to create multiple contrastive views.&lt;/li&gt; 
     &lt;li&gt;Calculate contrastive learning signals using the generated augmented representations and optimize using the combined loss function.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Critical Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;Ensure that the augmentation matrices are learned adaptively based on the current user-item embeddings to differentiate the importance of interactions.&lt;/li&gt; 
     &lt;li&gt;Monitor the performance with different numbers of latent intents \( K \) to find an optimal balance between expressiveness and noise.&lt;/li&gt; 
     &lt;li&gt;Regularly assess the proposed model for over-smoothing by checking the Mean Average Distance (MAD) metric on the embeddings.&lt;/li&gt; 
     &lt;li&gt;Tune hyperparameters \( \lambda_1, \lambda_2, \lambda_3 \) for the multi-task loss to balance the contribution of the self-supervised learning signals.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Lightgcn: Simplifying and powering graph convolution network for recommendation&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;Lightgcn: Simplifying and powering graph convolution network for recommendation&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Neural collaborative filtering&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Disentangled contrastive learning on graphs&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Improving Graph Collaborative Filtering with Neighborhood-enriched Contrastive Learning&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Curriculum Disentangled Recommendation with Noisy Multi-feedback&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Disentangled heterogeneous graph attention network for recommendation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Learning intents behind interactions with knowledge graph for recommendation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;LightGCL: Simple Yet Effective Graph Contrastive Learning for Recommendation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Self-supervised graph learning for recommendation&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/dccf/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/dccf/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/dccf/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/dccf/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Example 5 (Category: Diffusion and Flow Matching)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed model presented in this paper focuses on the task of generative modeling through the framework of Continuous Normalizing Flows (CNFs) to define straight flows between noise and data samples.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;Architecture: 
    &lt;ul&gt; 
     &lt;li&gt;Implement a neural network to parameterize the velocity field \( v_{\theta}(t, x) \) that maps from noise to data distributions.&lt;/li&gt; 
     &lt;li&gt;Use architectures suitable for continuous functions, such as feedforward or convolutional networks.&lt;/li&gt; 
     &lt;li&gt;Each layer should have non-linear activation functions (e.g., ReLU, Tanh).&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Loss Functions: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Velocity Consistency Loss&lt;/strong&gt;: This should be structured as: \[ L_{\theta} = E_{t \sim U} E_{x_t, x_{t+\Delta t}} \| f_{\theta}(t, x_t) - f_{\theta}(t+\Delta t, x_{t+\Delta t}) \|^2_2 + \alpha \| v_{\theta}(t, x_t) - v_{\theta}(t+\Delta t, x_{t+\Delta t}) \|^2_2 \] where \( f_{\theta}(t, x_t) = x_t + (1 - t) v_{\theta}(t, x_t) \). Choose \( \alpha \) based on cross-validation performance. &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Training Procedure: 
    &lt;ul&gt; 
     &lt;li&gt;Sample \( x_0 \) from the noise distribution \( p_0 \).&lt;/li&gt; 
     &lt;li&gt;For multiple time segments, define intervals and compute velocity fields iteratively.&lt;/li&gt; 
     &lt;li&gt;Use the weights of the proposed approach in an exponential moving average to stabilize training.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Sampling Process: 
    &lt;ul&gt; 
     &lt;li&gt;For single-step or multi-step generation, heuristically sample from the noise distribution and use the learned velocity field as follows: \[ x_{i/k} = x_{(i-1)/k} + \frac{1}{k} v_{i\theta}((i-1)/k, x_{(i-1)/k}) \] &lt;/li&gt; 
     &lt;li&gt;Apply the Euler method for iterative updates: \[ x_{t + \Delta t} = x_t + \Delta t v_i(t, x_t) \] where \( t \in [i/k, (i + 1)/k - \Delta t] \). &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Key Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;Ensure the network is equipped with a suitable optimizer such as Adam with a learning rate around \( 2 \times 10^{-4} \).&lt;/li&gt; 
     &lt;li&gt;The batch size should be appropriately set (e.g., 512 for CIFAR-10).&lt;/li&gt; 
     &lt;li&gt;Employ an ODE solver, suggested as Euler's method, during the training and sampling processes.&lt;/li&gt; 
     &lt;li&gt;Maintain a uniform distribution for sampling time intervals \( U \).&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Performance Considerations: 
    &lt;ul&gt; 
     &lt;li&gt;Monitor convergence rates and empirically validate parameter configurations through experiments. Start with fewer segments and gradually increase to capture complex distributions better.&lt;/li&gt; 
     &lt;li&gt;Adjust the decay rate for the EMA based on the stability of convergence (commonly around 0.999).&lt;/li&gt; 
     &lt;li&gt;Analyze the trade-offs between sampling efficiency and sample quality, ensuring a balance during proposed model development.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Flow matching for generative modeling&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;Flow matching for generative modeling&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Consistency models&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Rectified Flow&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Denoising diffusion probabilistic models&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Optimal flow matching: Learning straight trajectories in just one step&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Maximum likelihood training of score-based diffusion models&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/con_flowmatching/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/con_flowmatching/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/con_flowmatching/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/con_flowmatching/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Example 6 (Category: Graph Neural Networks)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed model focuses on the task of node classification in large graphs, addressing challenges like scalability, heterophily, long-range dependencies, and the absence of edges.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;The core techniques used in this study include a kernelized Gumbel-Softmax operator for all-pair message passing, which reduces computational complexity to linear (O(N)), and a Transformer-style network architecture designed for layer-wise learning of latent graph structures.&lt;/li&gt; 
   &lt;li&gt;The purpose of the kernelized Gumbel-Softmax operator is to enable differentiable learning of discrete graph structures by approximating categorical distributions. The Transformer-style architecture facilitates information propagation between arbitrary pairs of nodes through learned latent graphs.&lt;/li&gt; 
   &lt;li&gt;Implementation details for each component: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Kernelized Gumbel-Softmax Operator&lt;/strong&gt;: Set the temperature parameter (Ï„) to a range typically between 0.25 and 0.4 for training. It operates on node feature representations (D-dimensional feature vectors). The output of this operator is a distribution over node connections, facilitating the selection of neighbors for message passing.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Node Feature Input&lt;/strong&gt;: Each node input should be represented as a feature vector (e.g., {x_u} âˆˆ R^D), and the output is an updated representation of the node embedding after message passing.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Relational Bias (if applicable)&lt;/strong&gt;: Introduces activation (e.g., sigmoid) to adjust the message passing weights based on an observed adjacency matrix, which enhances weight assignment for connected nodes.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Edge Regularization Loss&lt;/strong&gt;: Combines categorical edge probabilities with a supervised classification loss, encouraging the network to maintain predicted edges consistent with observed edges.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;The step-by-step interaction of these components includes: 
    &lt;ul&gt; 
     &lt;li&gt;Begin with an input matrix of node embeddings (X) and, if available, an adjacency matrix (A).&lt;/li&gt; 
     &lt;li&gt;Apply the kernelized Gumbel-Softmax operator to the embedding matrix to generate a probability distribution over neighbor selection for each node.&lt;/li&gt; 
     &lt;li&gt;Use these probabilities to sample neighbors, allowing for message passing where each node aggregates information from its selected neighbors.&lt;/li&gt; 
     &lt;li&gt;Update the node embeddings using an attention mechanism, which can be enhanced by relational bias if edges are available.&lt;/li&gt; 
     &lt;li&gt;After K iterations of neighbor sampling, apply loss functions comprising a supervised classification loss and, if applicable, edge-level regularization loss to optimize the embedding representations.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Critical implementation details affecting performance involve: 
    &lt;ul&gt; 
     &lt;li&gt;Careful tuning of the temperature parameter (Ï„) in the Gumbel-Softmax operator, as it significantly influences the proposed approach's capacity to capture the discrete nature of graph structures.&lt;/li&gt; 
     &lt;li&gt;Utilizing appropriate batch sizes for large-scale graphs, ensuring enough memory is available while also maintaining computational efficiency.&lt;/li&gt; 
     &lt;li&gt;Choosing the correct dimensionality for random features in the kernel approximation, balancing model expressiveness and training stability.&lt;/li&gt; 
     &lt;li&gt;The use of dropout or other regularization techniques such as edge-level regularization can influence the proposed model's generalization capabilities on unseen data.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;On the bottleneck of graph neural networks and its practical implications&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;On the bottleneck of graph neural networks and its practical implications&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Semi-supervised classification with graph convolutional networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Categorical reparameterization with gumbel-softmax&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Learning discrete structures for graph neural networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Graph attention networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Geometric deep learning: going beyond euclidean data&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Graph structure learning for robust graph neural networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Geom-gcn: Geometric graph convolutional networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;New benchmarks for learning on non-homophilous graphs&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Latent patient network learning for automatic diagnosis&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Few-shot learning with graph neural networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;The graph neural network model&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Characteristic functions on graphs: Birds of a feather, from statistical descriptors to parametric models&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Beyond homophily in graph neural networks: Current limitations and effective designs&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_nodeformer/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_nodeformer/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_nodeformer/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_nodeformer/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Example 7 (Category: Graph Neural Networks)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed approach works on the task of uncovering data dependencies and learning instance representations from datasets that may not have complete or reliable relationships, particularly in semi-supervised contexts like node classification, image/text classification, and spatial-temporal dynamics prediction.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;The core techniques/algorithms used in this paper include an energy-constrained diffusion model represented as a partial differential equation (PDE), an explicit Euler scheme for numerical solutions, and a form of adaptive diffusivity function based on the energy function. The proposed architecture utilizes a diffusion-based Transformer framework that allows for all-pair feature propagation among instances.&lt;/li&gt; 
   &lt;li&gt;The major technical components serve the following purposes: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Diffusion Process:&lt;/strong&gt; Encodes instances into evolving states by modeling information flow, where instance representations evolve according to a PDE illuminating the relationships among the instances.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Energy Function:&lt;/strong&gt; Provides constraints to regularize the diffusion process and guide the proposed model towards desired low-energy embeddings, enhancing the quality of representations.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Diffusivity Function:&lt;/strong&gt; Specifies the strength of information flow between instances, adapting based on the instance states, and allows for flexible and efficient propagation strategies.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Implementation details for each component: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Diffusion Process Input:&lt;/strong&gt; Requires a batch of instances represented as a matrix of size \(N \times D\), where \(N\) is the number of instances and \(D\) is the input feature dimension.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Diffusion Process Output:&lt;/strong&gt; Produces the updated instance representations after \(K\) propagation steps. The step size \(\tau\) should be set within the range (0, 1).&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Energy Function:&lt;/strong&gt; Implemented as \(E(Z, k; \delta) = ||Z - Z^{(k)}||^2_F + \lambda \sum_{i,j} \delta(||z_i - z_j||^2_2)\), with \(\delta\) being a non-decreasing, concave function.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Key Parameters:&lt;/strong&gt; 
      &lt;ul&gt; 
       &lt;li&gt;Step size \(\tau\)&lt;/li&gt; 
       &lt;li&gt;Layer number \(K\) (number of diffusion propagation steps)&lt;/li&gt; 
       &lt;li&gt;Regularization weight \(\lambda\).&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Step-by-step description of interactions: 
    &lt;ul&gt; 
     &lt;li&gt;Start by initializing the instance representations.&lt;/li&gt; 
     &lt;li&gt;For each layer of diffusion, compute the diffusivity \(S(k)\) based on current embeddings through a function \(f\) which can be defined differently depending on the proposed model implementation.&lt;/li&gt; 
     &lt;li&gt;Update the instance representations using the defined diffusion equations, ensuring to conserve states and introduce propagation according to the computed diffusivity.&lt;/li&gt; 
     &lt;li&gt;After \(K\) layers of diffusion, apply a final output layer to produce logits for predictions.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Critical implementation details that affect performance: 
    &lt;ul&gt; 
     &lt;li&gt;The choice of diffusivity function \(f\) greatly impacts the proposed model's capacity to learn complex dependencies, where specific formulations (like linear or logistic) yield different abilities in capturing inter-instance relationships.&lt;/li&gt; 
     &lt;li&gt;Ensure that the values of \(\tau\) and \(\lambda\) are set appropriately to balance convergence speed and representation quality; using a smaller \(\tau\) may require deeper layers to learn effectively.&lt;/li&gt; 
     &lt;li&gt;Optimization parameters like learning rate and early stopping criteria are essential, particularly for large-scale datasets where convergence behavior can vary widely depending on architecture size and complexity.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Diffusion-convolutional neural networks&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;Diffusion-convolutional neural networks&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Semi-supervised classification with graph convolutional networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Manifold regularization: A geometric framework for learning from labeled and unlabeled examples&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Geometric deep learning: going beyond euclidean data&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Artificial neural networks for solving ordinary and partial differential equations&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Scaling graph neural networks with approximate pagerank&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Learning discrete structures for graph neural networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Semi-supervised learning using gaussian fields and harmonic functions&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Graph convolutional networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Deep learning via semi-supervised embedding&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;A generalization of transformer networks to graphs&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Graph Convolution and Quadratic Time Complexity&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Bayesian graph convolutional neural networks for semi-supervised classification&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Do transformers really perform bad for graph representation?&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Big bird: Transformers for longer sequences&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Adaptive graph diffusion networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Transformers are RNNs&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Collective classification in network data&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;NodeFormer: A scalable graph structure learning transformer for node classification&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_difformer/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_difformer/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_difformer/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_difformer/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;span id="how-it-works"&gt;&lt;/span&gt; 
&lt;h2&gt;âœ¨How AI-Researcher works&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ”„ &lt;strong&gt;End-to-End Scientific Research Automation System&lt;/strong&gt; &lt;br /&gt;Our &lt;strong&gt;AI-Researcher&lt;/strong&gt; provides comprehensive automation for the complete scientific research lifecycle through an integrated pipeline. The system orchestrates research activities across three strategic phases: 
  &lt;ol&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Literature Review &amp;amp; Idea Generation&lt;/strong&gt; ğŸ“šğŸ’¡&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt; &lt;p&gt;ğŸ” &lt;strong&gt;Resource Collector&lt;/strong&gt;: Systematically gathers comprehensive research materials across multiple scientific domains through automated collection from major academic databases (e.g., arXiv, IEEE Xplore, ACM Digital Library, and Google Scholar), code platforms (e.g., GitHub, Hugging Face), and open datasets across scientific domains.&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;ğŸ§  &lt;strong&gt;Resource Filter&lt;/strong&gt;: Evaluates and selects high-impact papers, well-maintained code implementations, and benchmark datasets through quality metrics (e.g., citation count, code maintenance, data completeness) and relevance assessment.&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;ğŸ’­ &lt;strong&gt;Idea Generator&lt;/strong&gt;: Leveraging the identified research resources, including high-impact papers and code repositories, the Idea Generator systematically formulates novel research directions through comprehensive analysis. It automatedly evaluates current methodological limitations, map emerging technological trends, and explore uncharted research territories.&lt;/p&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;New Algorithm Design, Implementation &amp;amp; Validation&lt;/strong&gt; ğŸ§ªğŸ’» &lt;br /&gt;&lt;strong&gt;Design â†’ Implementation â†’ Validation â†’ Refinement&lt;/strong&gt;&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt; &lt;p&gt;ğŸ“&lt;strong&gt;Design Phase&lt;/strong&gt;: The initial phase focuses on conceptual development, where novel algorithmic ideas are formulated and theoretical foundations are established. During this stage, we carefully plan the implementation strategy, ensuring the proposed solution advances beyond existing approaches while maintaining practical feasibility.&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;âš™ï¸&lt;strong&gt;Implementation Phase&lt;/strong&gt;: proceed to transform abstract concepts into concrete code implementations. This phase involves developing functional modules, establishing a robust testing environment, and creating necessary infrastructure for experimental validation.&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;ğŸ”¬&lt;strong&gt;Validation Phase&lt;/strong&gt;: Systematic experimentation forms the core of our validation process. We execute comprehensive tests to evaluate algorithm performance, collect metrics, and document all findings. This phase ensures rigorous implementation verification with practical requirements.&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;ğŸ”§&lt;strong&gt;Refinement Phase&lt;/strong&gt; ğŸ”¬: Based on validation results, we enter an iterative refinement cycle. This phase involves identifying areas for improvement, optimizing code efficiency, and implementing necessary enhancements. We carefully analyze performance bottlenecks and plan strategic improvements for the next development iteration.&lt;/p&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Paper Writing&lt;/strong&gt; âœï¸ğŸ“&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Writer Agent&lt;/strong&gt; ğŸ“„: Automatically generates full-length academic papers by integrating research ideas, motivations, newly designed algorithm frameworks, and algorithm validation performance. Leveraging a hierarchical writing approach, it creates polished manuscripts with precision and clarity.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ğŸš€ This fully automated system removes the need for manual intervention across the entire research lifecycle, enabling effortless and seamless scientific discoveryâ€”from initial concept to final publication. ğŸš€ It serves as an excellent research assistant, aiding researchers in achieving their goals efficiently and effectively.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ”¬ &lt;strong&gt;Comprehensive Benchmark Suite&lt;/strong&gt; &lt;br /&gt;We have developed a comprehensive and standardized evaluation framework to objectively assess the academic capabilities of AI researchers and the quality of their scholarly work, integrating several key innovations to ensure thorough and reliable evaluation.&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt; &lt;p&gt;ğŸ‘¨â€ğŸ”¬ &lt;strong&gt;Expert-Level Ground Truth&lt;/strong&gt;: TThe benchmark leverages human expert-written papers as ground truth references, establishing a high-quality standard for comparison and validation.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;ğŸŒˆ &lt;strong&gt;Multi-Domain Coverage&lt;/strong&gt;: Our benchmark is designed to comprehensively span 4 major research domains, ensuring broad applicability: Computer Vision (CV), Nature Language Processing (NLP), Data Mining (DM), and Information Retrieval (IR).&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;ğŸŒ &lt;strong&gt;Fully Open-Source Benchmark Construction&lt;/strong&gt;: We have fully open-sourced the methodology and process for building the benchmark, including complete access to processed datasets, data collection pipelines, and processing code. This ensures &lt;strong&gt;Transparency in Evaluation&lt;/strong&gt; while empowering the community to customize and construct benchmarks tailored to their specific domains for testing AI researchers.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;ğŸ“Š &lt;strong&gt;Comprehensive Evaluation Metrics&lt;/strong&gt;: Our evaluation framework adopts a hierarchical and systematic approach, where tasks are organized into two levels based on the extent of idea provision. Leveraging specialized &lt;strong&gt;Evaluator Agents&lt;/strong&gt;, the framework conducts thorough assessments across multiple dimensions, ensuring a robust and comprehensive evaluation. Key evaluation metrics include: 1) &lt;strong&gt;Novelty&lt;/strong&gt;: Assessing the innovation and uniqueness of the research work. 2) &lt;strong&gt;Experimental Comprehensiveness&lt;/strong&gt;: Evaluating the design, execution, and rigor of the experiments. 3) &lt;strong&gt;Theoretical Foundation&lt;/strong&gt;: Measuring the strength of the theoretical background and foundations. 4) &lt;strong&gt;Result Analysis&lt;/strong&gt;: Analyzing the depth and accuracy of result interpretation. 5) &lt;strong&gt;Writing Quality&lt;/strong&gt;: Reviewing the clarity, coherence, and structure of the written report.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ğŸš€ &lt;strong&gt;Advancing Research Automation&lt;/strong&gt;. This benchmark suite provides an objective framework for assessing research automation capabilities. It is designed to evolve continuously, incorporating new advancements and expanding its scope to meet the growing demands of the research community.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;ğŸŒŸ &lt;strong&gt;Easy-to-Use AI Research Assistant&lt;/strong&gt; &lt;br /&gt;&lt;strong&gt;AI-Researcher&lt;/strong&gt;E delivers a truly seamless and accessible experience for research automation, empowering users to focus on innovation without technical barriers. Key features include:&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt; &lt;p&gt;ğŸŒ &lt;strong&gt;Multi-LLM Provider Support&lt;/strong&gt;: Effortlessly integrates with leading language model providers such as Claude, OpenAI, Deepseek, and more. Researchers can select the most suitable AI capabilities for their specific needs.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;ğŸ“š &lt;strong&gt;Effortless Research Kickoff&lt;/strong&gt;: Kickstart your research journey with unparalleled ease! Simply provide a list of relevant papers, and &lt;strong&gt;AI-Researcher&lt;/strong&gt; takes care of the restâ€”no need to upload files, contribute initial ideas, or navigate complex configurations. It's the ultimate tool to help you jumpstart your research process efficiently and effectively.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;ğŸ§  &lt;strong&gt;Minimal Domain Expertise Needed&lt;/strong&gt;: AI-Researcher simplifies the research process by autonomously identifying critical research gaps, proposing innovative approaches, and executing the entire research pipeline. While some domain understanding can enhance results, the tool is designed to empower users of all expertise levels to achieve impactful outcomes with ease.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;ğŸ“¦ &lt;strong&gt;Out-of-the-Box Functionality&lt;/strong&gt;: Experience seamless research automation right from the start. AI-Researcher is ready to use with minimal setup, giving you instant access to advanced capabilities. Skip the hassle of complex configurations and dive straight into accelerating your research process with ease and efficiency.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="how-to-use"&gt;&lt;/span&gt; 
&lt;h2&gt;ğŸ” How to use AI-Researcher&lt;/h2&gt; 
&lt;h3&gt;1. Research Agent&lt;/h3&gt; 
&lt;p&gt;If you want to use research agent with the given idea (Level 1 tasks), conducting extensive survey and experiments, you can use the following command in the &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/research_agent/run_infer_level_1.sh"&gt;&lt;code&gt;research_agent/run_infer_level_1.sh&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;current_dir=$(dirname "$(readlink -f "$0")")
cd $current_dir
export DOCKER_WORKPLACE_NAME=workplace_paper

export BASE_IMAGES=tjbtech1/paperagent:latest

export COMPLETION_MODEL=claude-3-5-sonnet-20241022
export CHEEP_MODEL=claude-3-5-haiku-20241022

category=vq
instance_id=one_layer_vq
export GPUS='"device=0,1"'

python run_infer_plan.py --instance_path ../benchmark/final/${category}/${instance_id}.json --container_name paper_eval --task_level task1 --model $COMPLETION_MODEL --workplace_name workplace --cache_path cache --port 12372 --max_iter_times 0 --category ${category}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to just give the reference papers, and let the research agent to generate the idea then conduct the experiments (Level 2 tasks), you can use the following command in the &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/research_agent/run_infer_level_2.sh"&gt;&lt;code&gt;research_agent/run_infer_level_2.sh&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;current_dir=$(dirname "$(readlink -f "$0")")
cd $current_dir
export DOCKER_WORKPLACE_NAME=workplace_paper

export BASE_IMAGES=tjbtech1/paperagent:latest

export COMPLETION_MODEL=claude-3-5-sonnet-20241022
export CHEEP_MODEL=claude-3-5-haiku-20241022

category=vq
instance_id=one_layer_vq
export GPUS='"device=0,1"'

python run_infer_idea.py --instance_path ../benchmark/final/${category}/${instance_id}.json --container_name paper_eval --model $COMPLETION_MODEL --workplace_name workplace --cache_path cache --port 12372 --max_iter_times 0 --category ${category}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Paper Writing Agent&lt;/h3&gt; 
&lt;p&gt;If you want to generate the paper after the research agent has conducted the research, you can use the following command in the &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/paper_agent/run_paper.sh"&gt;&lt;code&gt;paper_agent/run_infer.sh&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;#!/bin/bash

cd path/to/AI-Researcher/paper_agent

export OPENAI_API_KEY=sk-SKlupNntta4WPmvDCRo7uuPbYGwOnUQcb25Twn8c718tPpXN


research_field=vq
instance_id=rotated_vq

python path/to/AI-Researcher/paper_agent/writing.py --research_field ${research_field} --instance_id ${instance_id}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Benchmark Data and Collection&lt;/h3&gt; 
&lt;p&gt;Our benchmark is also fully-open-sourced:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Detailed benchmark data is available in the &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/benchmark"&gt;&lt;code&gt;benchmark&lt;/code&gt;&lt;/a&gt; folder.&lt;/li&gt; 
 &lt;li&gt;Detailed benchmark collection process is available in the &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/benchmark_collection"&gt;&lt;code&gt;benchmark_collection&lt;/code&gt;&lt;/a&gt; folder.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="documentation"&gt;&lt;/span&gt; 
&lt;h2&gt;ğŸ“– Documentation&lt;/h2&gt; 
&lt;p&gt;Comprehensive documentation is on its way ğŸš€! Stay tuned for updates on our &lt;a href="https://auto-researcher.github.io/docs"&gt;Documentation&lt;/a&gt; page.&lt;/p&gt; 
&lt;span id="community"&gt;&lt;/span&gt; 
&lt;h2&gt;ğŸ¤ Join the Community&lt;/h2&gt; 
&lt;p&gt;We aim to build a vibrant community around AI-Researcher and warmly invite everyone to join us. Here's how you can become part of our community:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://join.slack.com/t/ai-researchergroup/shared_invite/zt-30y5a070k-C0ajQt1zmVczFnfGkIicvA"&gt;Join our Slack workspace&lt;/a&gt; - Here we talk about research, architecture, and future development.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/ghSnKGkq"&gt;Join our Discord server&lt;/a&gt; - This is a community-run server for general discussion, questions, and feedback.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/HKUDS/AI-Researcher/issues"&gt;Read or post Github Issues&lt;/a&gt; - Check out the issues we're working on, or add your own ideas.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Misc&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/HKUDS/AI-Researcher/stargazers"&gt;&lt;img src="https://reporoster.com/stars/HKUDS/AI-Researcher" alt="Stargazers repo roster for @HKUDS/AI-Researcher" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/HKUDS/AI-Researcher/network/members"&gt;&lt;img src="https://reporoster.com/forks/HKUDS/AI-Researcher" alt="Forkers repo roster for @HKUDS/AI-Researcher" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://star-history.com/#HKUDS/AI-Researcher&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=HKUDS/AI-Researcher&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;span id="cite"&gt;&lt;/span&gt; 
&lt;h2&gt;ğŸŒŸ Cite&lt;/h2&gt; 
&lt;p&gt;A more detailed technical report will be released soon. ğŸš€:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-tex"&gt;@misc{airesearcher,
      title={{AI-Researcher: Autonomous Scientific Innovation}},
      author={Jiabin Tang, Lianghao Xia, Zhonghang Li, Chao Huang},
      year={2025},
      eprint={2505.18705},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2505.18705},
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>yichuan-w/LEANN</title>
      <link>https://github.com/yichuan-w/LEANN</link>
      <description>&lt;p&gt;RAG on Everything with LEANN. Enjoy 97% storage savings while running a fast, accurate, and 100% private RAG application on your personal device.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/assets/logo-text.png" alt="LEANN Logo" width="400" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://img.shields.io/badge/Python-3.9%20%7C%203.10%20%7C%203.11%20%7C%203.12%20%7C%203.13-blue.svg?sanitize=true" alt="Python Versions" /&gt; &lt;img src="https://github.com/yichuan-w/LEANN/actions/workflows/build-and-publish.yml/badge.svg?sanitize=true" alt="CI Status" /&gt; &lt;img src="https://img.shields.io/badge/Platform-Ubuntu%20%26%20Arch%20%26%20WSL%20%7C%20macOS%20(ARM64%2FIntel)-lightgrey" alt="Platform" /&gt; &lt;img src="https://img.shields.io/badge/License-MIT-green.svg?sanitize=true" alt="MIT License" /&gt; &lt;img src="https://img.shields.io/badge/MCP-Native%20Integration-blue" alt="MCP Integration" /&gt; &lt;a href="https://join.slack.com/t/leann-e2u9779/shared_invite/zt-3ckd2f6w1-OX08~NN4gkWhh10PRVBj1Q"&gt; &lt;img src="https://img.shields.io/badge/Slack-Join-4A154B?logo=slack&amp;amp;logoColor=white" alt="Join Slack" /&gt; &lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/assets/wechat_user_group.JPG" title="Join WeChat group"&gt; &lt;img src="https://img.shields.io/badge/WeChat-Join-2DC100?logo=wechat&amp;amp;logoColor=white" alt="Join WeChat group" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2 align="center" tabindex="-1" class="heading-element" dir="auto"&gt; The smallest vector index in the world. RAG Everything with LEANN! &lt;/h2&gt; 
&lt;p&gt;LEANN is an innovative vector database that democratizes personal AI. Transform your laptop into a powerful RAG system that can index and search through millions of documents while using &lt;strong&gt;97% less storage&lt;/strong&gt; than traditional solutions &lt;strong&gt;without accuracy loss&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;LEANN achieves this through &lt;em&gt;graph-based selective recomputation&lt;/em&gt; with &lt;em&gt;high-degree preserving pruning&lt;/em&gt;, computing embeddings on-demand instead of storing them all. &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#%EF%B8%8F-architecture--how-it-works"&gt;Illustration Fig â†’&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2506.08276"&gt;Paper â†’&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Ready to RAG Everything?&lt;/strong&gt; Transform your laptop into a personal AI assistant that can semantic search your &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-personal-data-manager-process-any-documents-pdf-txt-md"&gt;file system&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-your-personal-email-secretary-rag-on-apple-mail"&gt;emails&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-time-machine-for-the-web-rag-your-entire-browser-history"&gt;browser history&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-wechat-detective-unlock-your-golden-memories"&gt;chat history&lt;/a&gt;&lt;/strong&gt; (&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-wechat-detective-unlock-your-golden-memories"&gt;WeChat&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-imessage-history-your-personal-conversation-archive"&gt;iMessage&lt;/a&gt;), &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-chatgpt-chat-history-your-personal-ai-conversation-archive"&gt;agent memory&lt;/a&gt;&lt;/strong&gt; (&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-chatgpt-chat-history-your-personal-ai-conversation-archive"&gt;ChatGPT&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-claude-chat-history-your-personal-ai-conversation-archive"&gt;Claude&lt;/a&gt;), &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#mcp-integration-rag-on-live-data-from-any-platform"&gt;live data&lt;/a&gt;&lt;/strong&gt; (&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#mcp-integration-rag-on-live-data-from-any-platform"&gt;Slack&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#mcp-integration-rag-on-live-data-from-any-platform"&gt;Twitter&lt;/a&gt;), &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-claude-code-integration-transform-your-development-workflow"&gt;codebase&lt;/a&gt;&lt;/strong&gt;* , or external knowledge bases (i.e., 60M documents) - all on your laptop, with zero cloud costs and complete privacy.&lt;/p&gt; 
&lt;p&gt;* Claude Code only supports basic &lt;code&gt;grep&lt;/code&gt;-style keyword search. &lt;strong&gt;LEANN&lt;/strong&gt; is a drop-in &lt;strong&gt;semantic search MCP service fully compatible with Claude Code&lt;/strong&gt;, unlocking intelligent retrieval without changing your workflow. ğŸ”¥ Check out &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/packages/leann-mcp/README.md"&gt;the easy setup â†’&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Why LEANN?&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/assets/effects.png" alt="LEANN vs Traditional Vector DB Storage Comparison" width="70%" /&gt; &lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;The numbers speak for themselves:&lt;/strong&gt; Index 60 million text chunks in just 6GB instead of 201GB. From emails to browser history, everything fits on your laptop. &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-storage-comparison"&gt;See detailed benchmarks for different applications below â†“&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;ğŸ”’ &lt;strong&gt;Privacy:&lt;/strong&gt; Your data never leaves your laptop. No OpenAI, no cloud, no "terms of service".&lt;/p&gt; 
&lt;p&gt;ğŸª¶ &lt;strong&gt;Lightweight:&lt;/strong&gt; Graph-based recomputation eliminates heavy embedding storage, while smart graph pruning and CSR format minimize graph storage overhead. Always less storage, less memory usage!&lt;/p&gt; 
&lt;p&gt;ğŸ“¦ &lt;strong&gt;Portable:&lt;/strong&gt; Transfer your entire knowledge base between devices (even with others) with minimal cost - your personal AI memory travels with you.&lt;/p&gt; 
&lt;p&gt;ğŸ“ˆ &lt;strong&gt;Scalability:&lt;/strong&gt; Handle messy personal data that would crash traditional vector DBs, easily managing your growing personalized data and agent generated memory!&lt;/p&gt; 
&lt;p&gt;âœ¨ &lt;strong&gt;No Accuracy Loss:&lt;/strong&gt; Maintain the same search quality as heavyweight solutions while using 97% less storage.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;ğŸ“¦ Prerequisites: Install uv&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://docs.astral.sh/uv/getting-started/installation/#installation-methods"&gt;Install uv&lt;/a&gt; first if you don't have it. Typically, you can install it with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ğŸš€ Quick Install&lt;/h3&gt; 
&lt;p&gt;Clone the repository to access all examples and try amazing applications,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/yichuan-w/LEANN.git leann
cd leann
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and install LEANN from &lt;a href="https://pypi.org/project/leann/"&gt;PyPI&lt;/a&gt; to run them immediately:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv
source .venv/bin/activate
uv pip install leann
&lt;/code&gt;&lt;/pre&gt; 
&lt;!--
&gt; Low-resource? See "Low-resource setups" in the [Configuration Guide](docs/configuration-guide.md#low-resource-setups). --&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;strong&gt;ğŸ”§ Build from Source (Recommended for development)&lt;/strong&gt; &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/yichuan-w/LEANN.git leann
cd leann
git submodule update --init --recursive
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;macOS:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Note: DiskANN requires MacOS 13.3 or later.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;brew install libomp boost protobuf zeromq pkgconf
uv sync --extra diskann
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Linux (Ubuntu/Debian):&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Note: On Ubuntu 20.04, you may need to build a newer Abseil and pin Protobuf (e.g., v3.20.x) for building DiskANN. See &lt;a href="https://github.com/yichuan-w/LEANN/issues/30"&gt;Issue #30&lt;/a&gt; for a step-by-step note.&lt;/p&gt; 
 &lt;p&gt;You can manually install &lt;a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html"&gt;Intel oneAPI MKL&lt;/a&gt; instead of &lt;code&gt;libmkl-full-dev&lt;/code&gt; for DiskANN. You can also use &lt;code&gt;libopenblas-dev&lt;/code&gt; for building HNSW only, by removing &lt;code&gt;--extra diskann&lt;/code&gt; in the command below.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install -y \
  libomp-dev libboost-all-dev protobuf-compiler libzmq3-dev \
  pkg-config libabsl-dev libaio-dev libprotobuf-dev \
  libmkl-full-dev

uv sync --extra diskann
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Linux (Arch Linux):&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;sudo pacman -Syu &amp;amp;&amp;amp; sudo pacman -S --needed base-devel cmake pkgconf git gcc \
  boost boost-libs protobuf abseil-cpp libaio zeromq

# For MKL in DiskANN
sudo pacman -S --needed base-devel git
git clone https://aur.archlinux.org/paru-bin.git
cd paru-bin &amp;amp;&amp;amp; makepkg -si
paru -S intel-oneapi-mkl intel-oneapi-compiler
source /opt/intel/oneapi/setvars.sh

uv sync --extra diskann
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Linux (RHEL / CentOS Stream / Oracle / Rocky / AlmaLinux):&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;See &lt;a href="https://github.com/yichuan-w/LEANN/issues/50"&gt;Issue #50&lt;/a&gt; for more details.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;sudo dnf groupinstall -y "Development Tools"
sudo dnf install -y libomp-devel boost-devel protobuf-compiler protobuf-devel \
  abseil-cpp-devel libaio-devel zeromq-devel pkgconf-pkg-config

# For MKL in DiskANN
sudo dnf install -y intel-oneapi-mkl intel-oneapi-mkl-devel \
  intel-oneapi-openmp || sudo dnf install -y intel-oneapi-compiler
source /opt/intel/oneapi/setvars.sh

uv sync --extra diskann
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;Our declarative API makes RAG as easy as writing a config file.&lt;/p&gt; 
&lt;p&gt;Check out &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/demo.ipynb"&gt;demo.ipynb&lt;/a&gt; or &lt;a href="https://colab.research.google.com/github/yichuan-w/LEANN/blob/main/demo.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from leann import LeannBuilder, LeannSearcher, LeannChat
from pathlib import Path
INDEX_PATH = str(Path("./").resolve() / "demo.leann")

# Build an index
builder = LeannBuilder(backend_name="hnsw")
builder.add_text("LEANN saves 97% storage compared to traditional vector databases.")
builder.add_text("Tung Tung Tung Sahur calledâ€”they need their bananaâ€‘crocodile hybrid back")
builder.build_index(INDEX_PATH)

# Search
searcher = LeannSearcher(INDEX_PATH)
results = searcher.search("fantastical AI-generated creatures", top_k=1)

# Chat with your data
chat = LeannChat(INDEX_PATH, llm_config={"type": "hf", "model": "Qwen/Qwen3-0.6B"})
response = chat.ask("How much storage does LEANN save?", top_k=1)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;RAG on Everything!&lt;/h2&gt; 
&lt;p&gt;LEANN supports RAG on various data sources including documents (&lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.md&lt;/code&gt;), Apple Mail, Google Search History, WeChat, ChatGPT conversations, Claude conversations, iMessage conversations, and &lt;strong&gt;live data from any platform through MCP (Model Context Protocol) servers&lt;/strong&gt; - including Slack, Twitter, and more.&lt;/p&gt; 
&lt;h3&gt;Generation Model Setup&lt;/h3&gt; 
&lt;h4&gt;LLM Backend&lt;/h4&gt; 
&lt;p&gt;LEANN supports many LLM providers for text generation (HuggingFace, Ollama, and Any OpenAI compatible API).&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ”‘ OpenAI API Setup (Default)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Set your OpenAI API key as an environment variable:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY="your-api-key-here"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Make sure to use &lt;code&gt;--llm openai&lt;/code&gt; flag when using the CLI. You can also specify the model name with &lt;code&gt;--llm-model &amp;lt;model-name&amp;gt;&lt;/code&gt; flag.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ› ï¸ Supported LLM &amp;amp; Embedding Providers (via OpenAI Compatibility)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Thanks to the widespread adoption of the OpenAI API format, LEANN is compatible out-of-the-box with a vast array of LLM and embedding providers. Simply set the &lt;code&gt;OPENAI_BASE_URL&lt;/code&gt; and &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variables to connect to your preferred service.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;export OPENAI_API_KEY="xxx"
export OPENAI_BASE_URL="http://localhost:1234/v1" # base url of the provider
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To use OpenAI compatible endpoint with the CLI interface:&lt;/p&gt; 
 &lt;p&gt;If you are using it for text generation, make sure to use &lt;code&gt;--llm openai&lt;/code&gt; flag and specify the model name with &lt;code&gt;--llm-model &amp;lt;model-name&amp;gt;&lt;/code&gt; flag.&lt;/p&gt; 
 &lt;p&gt;If you are using it for embedding, set the &lt;code&gt;--embedding-mode openai&lt;/code&gt; flag and specify the model name with &lt;code&gt;--embedding-model &amp;lt;MODEL&amp;gt;&lt;/code&gt;.&lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;p&gt;Below is a list of base URLs for common providers to get you started.&lt;/p&gt; 
 &lt;h3&gt;ğŸ–¥ï¸ Local Inference Engines (Recommended for full privacy)&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Provider&lt;/th&gt; 
    &lt;th&gt;Sample Base URL&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Ollama&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:11434/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;LM Studio&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:1234/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;vLLM&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:8000/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;llama.cpp&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:8080/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;SGLang&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:30000/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;LiteLLM&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:4000&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;â˜ï¸ Cloud Providers&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;ğŸš¨ A Note on Privacy:&lt;/strong&gt; Before choosing a cloud provider, carefully review their privacy and data retention policies. Depending on their terms, your data may be used for their own purposes, including but not limited to human reviews and model training, which can lead to serious consequences if not handled properly.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Provider&lt;/th&gt; 
    &lt;th&gt;Base URL&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;OpenAI&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.openai.com/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;OpenRouter&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://openrouter.ai/api/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Gemini&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://generativelanguage.googleapis.com/v1beta/openai/&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;x.AI (Grok)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.x.ai/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Groq AI&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.groq.com/openai/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;DeepSeek&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.deepseek.com/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;SiliconFlow&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.siliconflow.cn/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Zhipu (BigModel)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://open.bigmodel.cn/api/paas/v4/&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Mistral AI&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.mistral.ai/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;If your provider isn't on this list, don't worry! Check their documentation for an OpenAI-compatible endpointâ€”chances are, it's OpenAI Compatible too!&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ”§ Ollama Setup (Recommended for full privacy)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;macOS:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;First, &lt;a href="https://ollama.com/download/mac"&gt;download Ollama for macOS&lt;/a&gt;.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Pull a lightweight model (recommended for consumer hardware)
ollama pull llama3.2:1b
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Linux:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Start Ollama service manually
ollama serve &amp;amp;

# Pull a lightweight model (recommended for consumer hardware)
ollama pull llama3.2:1b
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;â­ Flexible Configuration&lt;/h2&gt; 
&lt;p&gt;LEANN provides flexible parameters for embedding models, search strategies, and data processing to fit your specific needs.&lt;/p&gt; 
&lt;p&gt;ğŸ“š &lt;strong&gt;Need configuration best practices?&lt;/strong&gt; Check our &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/configuration-guide.md"&gt;Configuration Guide&lt;/a&gt; for detailed optimization tips, model selection advice, and solutions to common issues like slow embeddings or poor search quality.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ“‹ Click to expand: Common Parameters (Available in All Examples)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;All RAG examples share these common parameters. &lt;strong&gt;Interactive mode&lt;/strong&gt; is available in all examples - simply run without &lt;code&gt;--query&lt;/code&gt; to start a continuous Q&amp;amp;A session where you can ask multiple questions. Type 'quit' to exit.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Core Parameters (General preprocessing for all examples)
--index-dir DIR              # Directory to store the index (default: current directory)
--query "YOUR QUESTION"      # Single query mode. Omit for interactive chat (type 'quit' to exit), and now you can play with your index interactively
--max-items N                # Limit data preprocessing (default: -1, process all data)
--force-rebuild              # Force rebuild index even if it exists

# Embedding Parameters
--embedding-model MODEL      # e.g., facebook/contriever, text-embedding-3-small, mlx-community/Qwen3-Embedding-0.6B-8bit or nomic-embed-text
--embedding-mode MODE        # sentence-transformers, openai, mlx, or ollama

# LLM Parameters (Text generation models)
--llm TYPE                   # LLM backend: openai, ollama, or hf (default: openai)
--llm-model MODEL            # Model name (default: gpt-4o) e.g., gpt-4o-mini, llama3.2:1b, Qwen/Qwen2.5-1.5B-Instruct
--thinking-budget LEVEL      # Thinking budget for reasoning models: low/medium/high (supported by o3, o3-mini, GPT-Oss:20b, and other reasoning models)

# Search Parameters
--top-k N                    # Number of results to retrieve (default: 20)
--search-complexity N        # Search complexity for graph traversal (default: 32)

# Chunking Parameters
--chunk-size N               # Size of text chunks (default varies by source: 256 for most, 192 for WeChat)
--chunk-overlap N            # Overlap between chunks (default varies: 25-128 depending on source)

# Index Building Parameters
--backend-name NAME          # Backend to use: hnsw or diskann (default: hnsw)
--graph-degree N             # Graph degree for index construction (default: 32)
--build-complexity N         # Build complexity for index construction (default: 64)
--compact / --no-compact     # Use compact storage (default: true). Must be `no-compact` for `no-recompute` build.
--recompute / --no-recompute # Enable/disable embedding recomputation (default: enabled). Should not do a `no-recompute` search in a `recompute` build.
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;ğŸ“„ Personal Data Manager: Process Any Documents (&lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.md&lt;/code&gt;)!&lt;/h3&gt; 
&lt;p&gt;Ask questions directly about your personal PDFs, documents, and any directory containing your files!&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/videos/paper_clear.gif" alt="LEANN Document Search Demo" width="600" /&gt; &lt;/p&gt; 
&lt;p&gt;The example below asks a question about summarizing our paper (uses default data in &lt;code&gt;data/&lt;/code&gt;, which is a directory with diverse data sources: two papers, Pride and Prejudice, and a Technical report about LLM in Huawei in Chinese), and this is the &lt;strong&gt;easiest example&lt;/strong&gt; to run here:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;source .venv/bin/activate # Don't forget to activate the virtual environment
python -m apps.document_rag --query "What are the main techniques LEANN explores?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ“‹ Click to expand: Document-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--data-dir DIR           # Directory containing documents to process (default: data)
--file-types .ext .ext   # Filter by specific file types (optional - all LlamaIndex supported types if omitted)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Process all documents with larger chunks for academic papers
python -m apps.document_rag --data-dir "~/Documents/Papers" --chunk-size 1024

# Filter only markdown and Python files with smaller chunks
python -m apps.document_rag --data-dir "./docs" --chunk-size 256 --file-types .md .py

# Enable AST-aware chunking for code files
python -m apps.document_rag --enable-code-chunking --data-dir "./my_project"

# Or use the specialized code RAG for better code understanding
python -m apps.code_rag --repo-dir "./my_codebase" --query "How does authentication work?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;ğŸ“§ Your Personal Email Secretary: RAG on Apple Mail!&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The examples below currently support macOS only. Windows support coming soon.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/videos/mail_clear.gif" alt="LEANN Email Search Demo" width="600" /&gt; &lt;/p&gt; 
&lt;p&gt;Before running the example below, you need to grant full disk access to your terminal/VS Code in System Preferences â†’ Privacy &amp;amp; Security â†’ Full Disk Access.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.email_rag --query "What's the food I ordered by DoorDash or Uber Eats mostly?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;780K email chunks â†’ 78MB storage.&lt;/strong&gt; Finally, search your email like you search Google.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ“‹ Click to expand: Email-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--mail-path PATH         # Path to specific mail directory (auto-detects if omitted)
--include-html          # Include HTML content in processing (useful for newsletters)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Search work emails from a specific account
python -m apps.email_rag --mail-path "~/Library/Mail/V10/WORK_ACCOUNT"

# Find all receipts and order confirmations (includes HTML)
python -m apps.email_rag --query "receipt order confirmation invoice" --include-html
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ“‹ Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Once the index is built, you can ask questions like:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"Find emails from my boss about deadlines"&lt;/li&gt; 
  &lt;li&gt;"What did John say about the project timeline?"&lt;/li&gt; 
  &lt;li&gt;"Show me emails about travel expenses"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;ğŸ” Time Machine for the Web: RAG Your Entire Chrome Browser History!&lt;/h3&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/videos/google_clear.gif" alt="LEANN Browser History Search Demo" width="600" /&gt; &lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.browser_rag --query "Tell me my browser history about machine learning?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;38K browser entries â†’ 6MB storage.&lt;/strong&gt; Your browser history becomes your personal search engine.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ“‹ Click to expand: Browser-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--chrome-profile PATH    # Path to Chrome profile directory (auto-detects if omitted)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Search academic research from your browsing history
python -m apps.browser_rag --query "arxiv papers machine learning transformer architecture"

# Track competitor analysis across work profile
python -m apps.browser_rag --chrome-profile "~/Library/Application Support/Google/Chrome/Work Profile" --max-items 5000
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ“‹ Click to expand: How to find your Chrome profile&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;The default Chrome profile path is configured for a typical macOS setup. If you need to find your specific Chrome profile:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Open Terminal&lt;/li&gt; 
  &lt;li&gt;Run: &lt;code&gt;ls ~/Library/Application\ Support/Google/Chrome/&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;Look for folders like "Default", "Profile 1", "Profile 2", etc.&lt;/li&gt; 
  &lt;li&gt;Use the full path as your &lt;code&gt;--chrome-profile&lt;/code&gt; argument&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;Common Chrome profile locations:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;macOS: &lt;code&gt;~/Library/Application Support/Google/Chrome/Default&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;Linux: &lt;code&gt;~/.config/google-chrome/Default&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ’¬ Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Once the index is built, you can ask questions like:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"What websites did I visit about machine learning?"&lt;/li&gt; 
  &lt;li&gt;"Find my search history about programming"&lt;/li&gt; 
  &lt;li&gt;"What YouTube videos did I watch recently?"&lt;/li&gt; 
  &lt;li&gt;"Show me websites I visited about travel planning"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;ğŸ’¬ WeChat Detective: Unlock Your Golden Memories!&lt;/h3&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/videos/wechat_clear.gif" alt="LEANN WeChat Search Demo" width="600" /&gt; &lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.wechat_rag --query "Show me all group chats about weekend plans"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;400K messages â†’ 64MB storage&lt;/strong&gt; Search years of chat history in any language.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ”§ Click to expand: Installation Requirements&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;First, you need to install the &lt;a href="https://github.com/sunnyyoung/WeChatTweak-CLI"&gt;WeChat exporter&lt;/a&gt;,&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;brew install sunnyyoung/repo/wechattweak-cli
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;or install it manually (if you have issues with Homebrew):&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;sudo packages/wechat-exporter/wechattweak-cli install
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Troubleshooting:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Installation issues&lt;/strong&gt;: Check the &lt;a href="https://github.com/sunnyyoung/WeChatTweak-CLI/issues/41"&gt;WeChatTweak-CLI issues page&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Export errors&lt;/strong&gt;: If you encounter the error below, try restarting WeChat &lt;pre&gt;&lt;code class="language-bash"&gt;Failed to export WeChat data. Please ensure WeChat is running and WeChatTweak is installed.
Failed to find or export WeChat data. Exiting.
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ“‹ Click to expand: WeChat-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--export-dir DIR         # Directory to store exported WeChat data (default: wechat_export_direct)
--force-export          # Force re-export even if data exists
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Search for travel plans discussed in group chats
python -m apps.wechat_rag --query "travel plans" --max-items 10000

# Re-export and search recent chats (useful after new messages)
python -m apps.wechat_rag --force-export --query "work schedule"
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ’¬ Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Once the index is built, you can ask questions like:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"æˆ‘æƒ³ä¹°é­”æœ¯å¸ˆçº¦ç¿°é€Šçš„çƒè¡£ï¼Œç»™æˆ‘ä¸€äº›å¯¹åº”èŠå¤©è®°å½•?" (Chinese: Show me chat records about buying Magic Johnson's jersey)&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;ğŸ¤– ChatGPT Chat History: Your Personal AI Conversation Archive!&lt;/h3&gt; 
&lt;p&gt;Transform your ChatGPT conversations into a searchable knowledge base! Search through all your ChatGPT discussions about coding, research, brainstorming, and more.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.chatgpt_rag --export-path chatgpt_export.html --query "How do I create a list in Python?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Unlock your AI conversation history.&lt;/strong&gt; Never lose track of valuable insights from your ChatGPT discussions again.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ“‹ Click to expand: How to Export ChatGPT Data&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Step-by-step export process:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Sign in to ChatGPT&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Click your profile icon&lt;/strong&gt; in the top right corner&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Navigate to Settings&lt;/strong&gt; â†’ &lt;strong&gt;Data Controls&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Click "Export"&lt;/strong&gt; under Export Data&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Confirm the export&lt;/strong&gt; request&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Download the ZIP file&lt;/strong&gt; from the email link (expires in 24 hours)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Extract or use directly&lt;/strong&gt; with LEANN&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;Supported formats:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;.html&lt;/code&gt; files from ChatGPT exports&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;.zip&lt;/code&gt; archives from ChatGPT&lt;/li&gt; 
  &lt;li&gt;Directories with multiple export files&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ“‹ Click to expand: ChatGPT-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--export-path PATH           # Path to ChatGPT export file (.html/.zip) or directory (default: ./chatgpt_export)
--separate-messages         # Process each message separately instead of concatenated conversations
--chunk-size N              # Text chunk size (default: 512)
--chunk-overlap N           # Overlap between chunks (default: 128)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Basic usage with HTML export
python -m apps.chatgpt_rag --export-path conversations.html

# Process ZIP archive from ChatGPT
python -m apps.chatgpt_rag --export-path chatgpt_export.zip

# Search with specific query
python -m apps.chatgpt_rag --export-path chatgpt_data.html --query "Python programming help"

# Process individual messages for fine-grained search
python -m apps.chatgpt_rag --separate-messages --export-path chatgpt_export.html

# Process directory containing multiple exports
python -m apps.chatgpt_rag --export-path ./chatgpt_exports/ --max-items 1000
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ’¡ Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Once your ChatGPT conversations are indexed, you can search with queries like:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"What did I ask ChatGPT about Python programming?"&lt;/li&gt; 
  &lt;li&gt;"Show me conversations about machine learning algorithms"&lt;/li&gt; 
  &lt;li&gt;"Find discussions about web development frameworks"&lt;/li&gt; 
  &lt;li&gt;"What coding advice did ChatGPT give me?"&lt;/li&gt; 
  &lt;li&gt;"Search for conversations about debugging techniques"&lt;/li&gt; 
  &lt;li&gt;"Find ChatGPT's recommendations for learning resources"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;ğŸ¤– Claude Chat History: Your Personal AI Conversation Archive!&lt;/h3&gt; 
&lt;p&gt;Transform your Claude conversations into a searchable knowledge base! Search through all your Claude discussions about coding, research, brainstorming, and more.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.claude_rag --export-path claude_export.json --query "What did I ask about Python dictionaries?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Unlock your AI conversation history.&lt;/strong&gt; Never lose track of valuable insights from your Claude discussions again.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ“‹ Click to expand: How to Export Claude Data&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Step-by-step export process:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Open Claude&lt;/strong&gt; in your browser&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Navigate to Settings&lt;/strong&gt; (look for gear icon or settings menu)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Find Export/Download&lt;/strong&gt; options in your account settings&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Download conversation data&lt;/strong&gt; (usually in JSON format)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Place the file&lt;/strong&gt; in your project directory&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;em&gt;Note: Claude export methods may vary depending on the interface you're using. Check Claude's help documentation for the most current export instructions.&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Supported formats:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;.json&lt;/code&gt; files (recommended)&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;.zip&lt;/code&gt; archives containing JSON data&lt;/li&gt; 
  &lt;li&gt;Directories with multiple export files&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ“‹ Click to expand: Claude-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--export-path PATH           # Path to Claude export file (.json/.zip) or directory (default: ./claude_export)
--separate-messages         # Process each message separately instead of concatenated conversations
--chunk-size N              # Text chunk size (default: 512)
--chunk-overlap N           # Overlap between chunks (default: 128)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Basic usage with JSON export
python -m apps.claude_rag --export-path my_claude_conversations.json

# Process ZIP archive from Claude
python -m apps.claude_rag --export-path claude_export.zip

# Search with specific query
python -m apps.claude_rag --export-path claude_data.json --query "machine learning advice"

# Process individual messages for fine-grained search
python -m apps.claude_rag --separate-messages --export-path claude_export.json

# Process directory containing multiple exports
python -m apps.claude_rag --export-path ./claude_exports/ --max-items 1000
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ’¡ Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Once your Claude conversations are indexed, you can search with queries like:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"What did I ask Claude about Python programming?"&lt;/li&gt; 
  &lt;li&gt;"Show me conversations about machine learning algorithms"&lt;/li&gt; 
  &lt;li&gt;"Find discussions about software architecture patterns"&lt;/li&gt; 
  &lt;li&gt;"What debugging advice did Claude give me?"&lt;/li&gt; 
  &lt;li&gt;"Search for conversations about data structures"&lt;/li&gt; 
  &lt;li&gt;"Find Claude's recommendations for learning resources"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;ğŸ’¬ iMessage History: Your Personal Conversation Archive!&lt;/h3&gt; 
&lt;p&gt;Transform your iMessage conversations into a searchable knowledge base! Search through all your text messages, group chats, and conversations with friends, family, and colleagues.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.imessage_rag --query "What did we discuss about the weekend plans?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Unlock your message history.&lt;/strong&gt; Never lose track of important conversations, shared links, or memorable moments from your iMessage history.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ“‹ Click to expand: How to Access iMessage Data&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;iMessage data location:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;iMessage conversations are stored in a SQLite database on your Mac at:&lt;/p&gt; 
 &lt;pre&gt;&lt;code&gt;~/Library/Messages/chat.db
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Important setup requirements:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Grant Full Disk Access&lt;/strong&gt; to your terminal or IDE:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Open &lt;strong&gt;System Preferences&lt;/strong&gt; â†’ &lt;strong&gt;Security &amp;amp; Privacy&lt;/strong&gt; â†’ &lt;strong&gt;Privacy&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Select &lt;strong&gt;Full Disk Access&lt;/strong&gt; from the left sidebar&lt;/li&gt; 
    &lt;li&gt;Click the &lt;strong&gt;+&lt;/strong&gt; button and add your terminal app (Terminal, iTerm2) or IDE (VS Code, etc.)&lt;/li&gt; 
    &lt;li&gt;Restart your terminal/IDE after granting access&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alternative: Use a backup database&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;If you have Time Machine backups or manual copies of the database&lt;/li&gt; 
    &lt;li&gt;Use &lt;code&gt;--db-path&lt;/code&gt; to specify a custom location&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;Supported formats:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Direct access to &lt;code&gt;~/Library/Messages/chat.db&lt;/code&gt; (default)&lt;/li&gt; 
  &lt;li&gt;Custom database path with &lt;code&gt;--db-path&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;Works with backup copies of the database&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ“‹ Click to expand: iMessage-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--db-path PATH                    # Path to chat.db file (default: ~/Library/Messages/chat.db)
--concatenate-conversations       # Group messages by conversation (default: True)
--no-concatenate-conversations    # Process each message individually
--chunk-size N                    # Text chunk size (default: 1000)
--chunk-overlap N                 # Overlap between chunks (default: 200)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Basic usage (requires Full Disk Access)
python -m apps.imessage_rag

# Search with specific query
python -m apps.imessage_rag --query "family dinner plans"

# Use custom database path
python -m apps.imessage_rag --db-path /path/to/backup/chat.db

# Process individual messages instead of conversations
python -m apps.imessage_rag --no-concatenate-conversations

# Limit processing for testing
python -m apps.imessage_rag --max-items 100 --query "weekend"
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ’¡ Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Once your iMessage conversations are indexed, you can search with queries like:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"What did we discuss about vacation plans?"&lt;/li&gt; 
  &lt;li&gt;"Find messages about restaurant recommendations"&lt;/li&gt; 
  &lt;li&gt;"Show me conversations with John about the project"&lt;/li&gt; 
  &lt;li&gt;"Search for shared links about technology"&lt;/li&gt; 
  &lt;li&gt;"Find group chat discussions about weekend events"&lt;/li&gt; 
  &lt;li&gt;"What did mom say about the family gathering?"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;MCP Integration: RAG on Live Data from Any Platform&lt;/h3&gt; 
&lt;p&gt;Connect to live data sources through the Model Context Protocol (MCP). LEANN now supports real-time RAG on platforms like Slack, Twitter, and more through standardized MCP servers.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key Benefits:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Live Data Access&lt;/strong&gt;: Fetch real-time data without manual exports&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Standardized Protocol&lt;/strong&gt;: Use any MCP-compatible server&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy Extension&lt;/strong&gt;: Add new platforms with minimal code&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Secure Access&lt;/strong&gt;: MCP servers handle authentication&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;ğŸ’¬ Slack Messages: Search Your Team Conversations&lt;/h4&gt; 
&lt;p&gt;Transform your Slack workspace into a searchable knowledge base! Find discussions, decisions, and shared knowledge across all your channels.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Test MCP server connection
python -m apps.slack_rag --mcp-server "slack-mcp-server" --test-connection

# Index and search Slack messages
python -m apps.slack_rag \
  --mcp-server "slack-mcp-server" \
  --workspace-name "my-team" \
  --channels general dev-team random \
  --query "What did we decide about the product launch?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;ğŸ“– Comprehensive Setup Guide&lt;/strong&gt;: For detailed setup instructions, troubleshooting common issues (like "users cache is not ready yet"), and advanced configuration options, see our &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/slack-setup-guide.md"&gt;&lt;strong&gt;Slack Setup Guide&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Quick Setup:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install a Slack MCP server (e.g., &lt;code&gt;npm install -g slack-mcp-server&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Create a Slack App and get API credentials (see detailed guide above)&lt;/li&gt; 
 &lt;li&gt;Set environment variables: &lt;pre&gt;&lt;code class="language-bash"&gt;export SLACK_BOT_TOKEN="xoxb-your-bot-token"
export SLACK_APP_TOKEN="xapp-your-app-token"  # Optional
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Test connection with &lt;code&gt;--test-connection&lt;/code&gt; flag&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Arguments:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--mcp-server&lt;/code&gt;: Command to start the Slack MCP server&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--workspace-name&lt;/code&gt;: Slack workspace name for organization&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--channels&lt;/code&gt;: Specific channels to index (optional)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--concatenate-conversations&lt;/code&gt;: Group messages by channel (default: true)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--max-messages-per-channel&lt;/code&gt;: Limit messages per channel (default: 100)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--max-retries&lt;/code&gt;: Maximum retries for cache sync issues (default: 5)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--retry-delay&lt;/code&gt;: Initial delay between retries in seconds (default: 2.0)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;ğŸ¦ Twitter Bookmarks: Your Personal Tweet Library&lt;/h4&gt; 
&lt;p&gt;Search through your Twitter bookmarks! Find that perfect article, thread, or insight you saved for later.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Test MCP server connection
python -m apps.twitter_rag --mcp-server "twitter-mcp-server" --test-connection

# Index and search Twitter bookmarks
python -m apps.twitter_rag \
  --mcp-server "twitter-mcp-server" \
  --max-bookmarks 1000 \
  --query "What AI articles did I bookmark about machine learning?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Setup Requirements:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install a Twitter MCP server (e.g., &lt;code&gt;npm install -g twitter-mcp-server&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Get Twitter API credentials: 
  &lt;ul&gt; 
   &lt;li&gt;Apply for a Twitter Developer Account at &lt;a href="https://developer.twitter.com"&gt;developer.twitter.com&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Create a new app in the Twitter Developer Portal&lt;/li&gt; 
   &lt;li&gt;Generate API keys and access tokens with "Read" permissions&lt;/li&gt; 
   &lt;li&gt;For bookmarks access, you may need Twitter API v2 with appropriate scopes&lt;/li&gt; 
  &lt;/ul&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;export TWITTER_API_KEY="your-api-key"
export TWITTER_API_SECRET="your-api-secret"
export TWITTER_ACCESS_TOKEN="your-access-token"
export TWITTER_ACCESS_TOKEN_SECRET="your-access-token-secret"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Test connection with &lt;code&gt;--test-connection&lt;/code&gt; flag&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Arguments:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--mcp-server&lt;/code&gt;: Command to start the Twitter MCP server&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--username&lt;/code&gt;: Filter bookmarks by username (optional)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--max-bookmarks&lt;/code&gt;: Maximum bookmarks to fetch (default: 1000)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--no-tweet-content&lt;/code&gt;: Exclude tweet content, only metadata&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--no-metadata&lt;/code&gt;: Exclude engagement metadata&lt;/li&gt; 
&lt;/ul&gt;  
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ’¡ Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Slack Queries:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"What did the team discuss about the project deadline?"&lt;/li&gt; 
  &lt;li&gt;"Find messages about the new feature launch"&lt;/li&gt; 
  &lt;li&gt;"Show me conversations about budget planning"&lt;/li&gt; 
  &lt;li&gt;"What decisions were made in the dev-team channel?"&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Twitter Queries:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"What AI articles did I bookmark last month?"&lt;/li&gt; 
  &lt;li&gt;"Find tweets about machine learning techniques"&lt;/li&gt; 
  &lt;li&gt;"Show me bookmarked threads about startup advice"&lt;/li&gt; 
  &lt;li&gt;"What Python tutorials did I save?"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;summary&gt;&lt;strong&gt;ğŸ”§ Using MCP with CLI Commands&lt;/strong&gt;&lt;/summary&gt; 
&lt;p&gt;&lt;strong&gt;Want to use MCP data with regular LEANN CLI?&lt;/strong&gt; You can combine MCP apps with CLI commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Step 1: Use MCP app to fetch and index data
python -m apps.slack_rag --mcp-server "slack-mcp-server" --workspace-name "my-team"

# Step 2: The data is now indexed and available via CLI
leann search slack_messages "project deadline"
leann ask slack_messages "What decisions were made about the product launch?"

# Same for Twitter bookmarks
python -m apps.twitter_rag --mcp-server "twitter-mcp-server"
leann search twitter_bookmarks "machine learning articles"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;MCP vs Manual Export:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;MCP&lt;/strong&gt;: Live data, automatic updates, requires server setup&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Manual Export&lt;/strong&gt;: One-time setup, works offline, requires manual data export&lt;/li&gt; 
&lt;/ul&gt;  
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ”§ Adding New MCP Platforms&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Want to add support for other platforms? LEANN's MCP integration is designed for easy extension:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Find or create an MCP server&lt;/strong&gt; for your platform&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Create a reader class&lt;/strong&gt; following the pattern in &lt;code&gt;apps/slack_data/slack_mcp_reader.py&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Create a RAG application&lt;/strong&gt; following the pattern in &lt;code&gt;apps/slack_rag.py&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Test and contribute&lt;/strong&gt; back to the community!&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;Popular MCP servers to explore:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;GitHub repositories and issues&lt;/li&gt; 
  &lt;li&gt;Discord messages&lt;/li&gt; 
  &lt;li&gt;Notion pages&lt;/li&gt; 
  &lt;li&gt;Google Drive documents&lt;/li&gt; 
  &lt;li&gt;And many more in the MCP ecosystem!&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;ğŸš€ Claude Code Integration: Transform Your Development Workflow!&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ASTâ€‘Aware Code Chunking&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;LEANN features intelligent code chunking that preserves semantic boundaries (functions, classes, methods) for Python, Java, C#, and TypeScript, improving code understanding compared to text-based chunking.&lt;/p&gt; 
 &lt;p&gt;ğŸ“– Read the &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/ast_chunking_guide.md"&gt;AST Chunking Guide â†’&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;strong&gt;The future of code assistance is here.&lt;/strong&gt; Transform your development workflow with LEANN's native MCP integration for Claude Code. Index your entire codebase and get intelligent code assistance directly in your IDE.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ” &lt;strong&gt;Semantic code search&lt;/strong&gt; across your entire project, fully local index and lightweight&lt;/li&gt; 
 &lt;li&gt;ğŸ§  &lt;strong&gt;AST-aware chunking&lt;/strong&gt; preserves code structure (functions, classes)&lt;/li&gt; 
 &lt;li&gt;ğŸ“š &lt;strong&gt;Context-aware assistance&lt;/strong&gt; for debugging and development&lt;/li&gt; 
 &lt;li&gt;ğŸš€ &lt;strong&gt;Zero-config setup&lt;/strong&gt; with automatic language detection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install LEANN globally for MCP integration
uv tool install leann-core --with leann
claude mcp add --scope user leann-server -- leann_mcp
# Setup is automatic - just start using Claude Code!
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Try our fully agentic pipeline with auto query rewriting, semantic search planning, and more:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/assets/mcp_leann.png" alt="LEANN MCP Integration" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ğŸ”¥ Ready to supercharge your coding?&lt;/strong&gt; &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/packages/leann-mcp/README.md"&gt;Complete Setup Guide â†’&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Command Line Interface&lt;/h2&gt; 
&lt;p&gt;LEANN includes a powerful CLI for document processing and search. Perfect for quick document indexing and interactive chat.&lt;/p&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;If you followed the Quick Start, &lt;code&gt;leann&lt;/code&gt; is already installed in your virtual environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;source .venv/bin/activate
leann --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;To make it globally available:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install the LEANN CLI globally using uv tool
uv tool install leann-core --with leann


# Now you can use leann from anywhere without activating venv
leann --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Global installation is required for Claude Code integration. The &lt;code&gt;leann_mcp&lt;/code&gt; server depends on the globally available &lt;code&gt;leann&lt;/code&gt; command.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Usage Examples&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# build from a specific directory, and my_docs is the index name(Here you can also build from multiple dict or multiple files)
leann build my-docs --docs ./your_documents

# Search your documents
leann search my-docs "machine learning concepts"

# Interactive chat with your documents
leann ask my-docs --interactive

# Ask a single question (non-interactive)
leann ask my-docs "Where are prompts configured?"

# List all your indexes
leann list

# Remove an index
leann remove my-docs
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Key CLI features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Auto-detects document formats (PDF, TXT, MD, DOCX, PPTX + code files)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ§  AST-aware chunking&lt;/strong&gt; for Python, Java, C#, TypeScript files&lt;/li&gt; 
 &lt;li&gt;Smart text chunking with overlap for all other content&lt;/li&gt; 
 &lt;li&gt;Multiple LLM providers (Ollama, OpenAI, HuggingFace)&lt;/li&gt; 
 &lt;li&gt;Organized index storage in &lt;code&gt;.leann/indexes/&lt;/code&gt; (project-local)&lt;/li&gt; 
 &lt;li&gt;Support for advanced search parameters&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ“‹ Click to expand: Complete CLI Reference&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;You can use &lt;code&gt;leann --help&lt;/code&gt;, or &lt;code&gt;leann build --help&lt;/code&gt;, &lt;code&gt;leann search --help&lt;/code&gt;, &lt;code&gt;leann ask --help&lt;/code&gt;, &lt;code&gt;leann list --help&lt;/code&gt;, &lt;code&gt;leann remove --help&lt;/code&gt; to get the complete CLI reference.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Build Command:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;leann build INDEX_NAME --docs DIRECTORY|FILE [DIRECTORY|FILE ...] [OPTIONS]

Options:
  --backend {hnsw,diskann}     Backend to use (default: hnsw)
  --embedding-model MODEL      Embedding model (default: facebook/contriever)
  --graph-degree N             Graph degree (default: 32)
  --complexity N               Build complexity (default: 64)
  --force                      Force rebuild existing index
  --compact / --no-compact     Use compact storage (default: true). Must be `no-compact` for `no-recompute` build.
  --recompute / --no-recompute Enable recomputation (default: true)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Search Command:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;leann search INDEX_NAME QUERY [OPTIONS]

Options:
  --top-k N                     Number of results (default: 5)
  --complexity N                Search complexity (default: 64)
  --recompute / --no-recompute  Enable/disable embedding recomputation (default: enabled). Should not do a `no-recompute` search in a `recompute` build.
  --pruning-strategy {global,local,proportional}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Ask Command:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;leann ask INDEX_NAME [OPTIONS]

Options:
  --llm {ollama,openai,hf}    LLM provider (default: ollama)
  --model MODEL               Model name (default: qwen3:8b)
  --interactive              Interactive chat mode
  --top-k N                  Retrieval count (default: 20)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;List Command:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;leann list

# Lists all indexes across all projects with status indicators:
# âœ… - Index is complete and ready to use
# âŒ - Index is incomplete or corrupted
# ğŸ“ - CLI-created index (in .leann/indexes/)
# ğŸ“„ - App-created index (*.leann.meta.json files)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Remove Command:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;leann remove INDEX_NAME [OPTIONS]

Options:
  --force, -f    Force removal without confirmation

# Smart removal: automatically finds and safely removes indexes
# - Shows all matching indexes across projects
# - Requires confirmation for cross-project removal
# - Interactive selection when multiple matches found
# - Supports both CLI and app-created indexes
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸš€ Advanced Features&lt;/h2&gt; 
&lt;h3&gt;ğŸ¯ Metadata Filtering&lt;/h3&gt; 
&lt;p&gt;LEANN supports a simple metadata filtering system to enable sophisticated use cases like document filtering by date/type, code search by file extension, and content management based on custom criteria.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Add metadata during indexing
builder.add_text(
    "def authenticate_user(token): ...",
    metadata={"file_extension": ".py", "lines_of_code": 25}
)

# Search with filters
results = searcher.search(
    query="authentication function",
    metadata_filters={
        "file_extension": {"==": ".py"},
        "lines_of_code": {"&amp;lt;": 100}
    }
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Supported operators&lt;/strong&gt;: &lt;code&gt;==&lt;/code&gt;, &lt;code&gt;!=&lt;/code&gt;, &lt;code&gt;&amp;lt;&lt;/code&gt;, &lt;code&gt;&amp;lt;=&lt;/code&gt;, &lt;code&gt;&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;gt;=&lt;/code&gt;, &lt;code&gt;in&lt;/code&gt;, &lt;code&gt;not_in&lt;/code&gt;, &lt;code&gt;contains&lt;/code&gt;, &lt;code&gt;starts_with&lt;/code&gt;, &lt;code&gt;ends_with&lt;/code&gt;, &lt;code&gt;is_true&lt;/code&gt;, &lt;code&gt;is_false&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;ğŸ“– &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/metadata_filtering.md"&gt;Complete Metadata filtering guide â†’&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;ğŸ” Grep Search&lt;/h3&gt; 
&lt;p&gt;For exact text matching instead of semantic search, use the &lt;code&gt;use_grep&lt;/code&gt; parameter:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Exact text search
results = searcher.search("bananaâ€‘crocodile", use_grep=True, top_k=1)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Use cases&lt;/strong&gt;: Finding specific code patterns, error messages, function names, or exact phrases where semantic similarity isn't needed.&lt;/p&gt; 
&lt;p&gt;ğŸ“– &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/grep_search.md"&gt;Complete grep search guide â†’&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ—ï¸ Architecture &amp;amp; How It Works&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/assets/arch.png" alt="LEANN Architecture" width="800" /&gt; &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;The magic:&lt;/strong&gt; Most vector DBs store every single embedding (expensive). LEANN stores a pruned graph structure (cheap) and recomputes embeddings only when needed (fast).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Core techniques:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Graph-based selective recomputation:&lt;/strong&gt; Only compute embeddings for nodes in the search path&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High-degree preserving pruning:&lt;/strong&gt; Keep important "hub" nodes while removing redundant connections&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dynamic batching:&lt;/strong&gt; Efficiently batch embedding computations for GPU utilization&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Two-level search:&lt;/strong&gt; Smart graph traversal that prioritizes promising nodes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Backends:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;HNSW&lt;/strong&gt; (default): Ideal for most datasets with maximum storage savings through full recomputation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DiskANN&lt;/strong&gt;: Advanced option with superior search performance, using PQ-based graph traversal with real-time reranking for the best speed-accuracy trade-off&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Benchmarks&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/benchmarks/diskann_vs_hnsw_speed_comparison.py"&gt;DiskANN vs HNSW Performance Comparison â†’&lt;/a&gt;&lt;/strong&gt; - Compare search performance between both backends&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/benchmarks/compare_faiss_vs_leann.py"&gt;Simple Example: Compare LEANN vs FAISS â†’&lt;/a&gt;&lt;/strong&gt; - See storage savings in action&lt;/p&gt; 
&lt;h3&gt;ğŸ“Š Storage Comparison&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;System&lt;/th&gt; 
   &lt;th&gt;DPR (2.1M)&lt;/th&gt; 
   &lt;th&gt;Wiki (60M)&lt;/th&gt; 
   &lt;th&gt;Chat (400K)&lt;/th&gt; 
   &lt;th&gt;Email (780K)&lt;/th&gt; 
   &lt;th&gt;Browser (38K)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Traditional vector database (e.g., FAISS)&lt;/td&gt; 
   &lt;td&gt;3.8 GB&lt;/td&gt; 
   &lt;td&gt;201 GB&lt;/td&gt; 
   &lt;td&gt;1.8 GB&lt;/td&gt; 
   &lt;td&gt;2.4 GB&lt;/td&gt; 
   &lt;td&gt;130 MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LEANN&lt;/td&gt; 
   &lt;td&gt;324 MB&lt;/td&gt; 
   &lt;td&gt;6 GB&lt;/td&gt; 
   &lt;td&gt;64 MB&lt;/td&gt; 
   &lt;td&gt;79 MB&lt;/td&gt; 
   &lt;td&gt;6.4 MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Savings&lt;/td&gt; 
   &lt;td&gt;91%&lt;/td&gt; 
   &lt;td&gt;97%&lt;/td&gt; 
   &lt;td&gt;97%&lt;/td&gt; 
   &lt;td&gt;97%&lt;/td&gt; 
   &lt;td&gt;95%&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Reproduce Our Results&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run benchmarks/run_evaluation.py    # Will auto-download evaluation data and run benchmarks
uv run benchmarks/run_evaluation.py benchmarks/data/indices/rpj_wiki/rpj_wiki --num-queries 2000    # After downloading data, you can run the benchmark with our biggest index
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The evaluation script downloads data automatically on first run. The last three results were tested with partial personal data, and you can reproduce them with your own data!&lt;/p&gt; 
&lt;h2&gt;ğŸ”¬ Paper&lt;/h2&gt; 
&lt;p&gt;If you find Leann useful, please cite:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://arxiv.org/abs/2506.08276"&gt;LEANN: A Low-Storage Vector Index&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{wang2025leannlowstoragevectorindex,
      title={LEANN: A Low-Storage Vector Index},
      author={Yichuan Wang and Shu Liu and Zhifei Li and Yongji Wu and Ziming Mao and Yilong Zhao and Xiao Yan and Zhiying Xu and Yang Zhou and Ion Stoica and Sewon Min and Matei Zaharia and Joseph E. Gonzalez},
      year={2025},
      eprint={2506.08276},
      archivePrefix={arXiv},
      primaryClass={cs.DB},
      url={https://arxiv.org/abs/2506.08276},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;âœ¨ &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/features.md"&gt;Detailed Features â†’&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;ğŸ¤ &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/CONTRIBUTING.md"&gt;CONTRIBUTING â†’&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;â“ &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/faq.md"&gt;FAQ â†’&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;ğŸ“ˆ &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/roadmap.md"&gt;Roadmap â†’&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; 
&lt;p&gt;MIT License - see &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/LICENSE"&gt;LICENSE&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h2&gt;ğŸ™ Acknowledgments&lt;/h2&gt; 
&lt;p&gt;Core Contributors: &lt;a href="https://yichuan-w.github.io/"&gt;Yichuan Wang&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/andylizf"&gt;Zhifei Li&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Active Contributors: &lt;a href="https://github.com/gabriel-dehan"&gt;Gabriel Dehan&lt;/a&gt;, &lt;a href="https://github.com/ASuresh0524"&gt;Aakash Suresh&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We welcome more contributors! Feel free to open issues or submit PRs.&lt;/p&gt; 
&lt;p&gt;This work is done at &lt;a href="https://sky.cs.berkeley.edu/"&gt;&lt;strong&gt;Berkeley Sky Computing Lab&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#yichuan-w/LEANN&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=yichuan-w/LEANN&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;strong&gt;â­ Star us on GitHub if Leann is useful for your research or applications!&lt;/strong&gt; &lt;/p&gt; 
&lt;p align="center"&gt; Made with â¤ï¸ by the Leann team &lt;/p&gt; 
&lt;h2&gt;ğŸ¤– Explore LEANN with AI&lt;/h2&gt; 
&lt;p&gt;LEANN is indexed on &lt;a href="https://deepwiki.com/yichuan-w/LEANN"&gt;DeepWiki&lt;/a&gt;, so you can ask questions to LLMs using Deep Research to explore the codebase and get help to add new features.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>hiyouga/LLaMA-Factory</title>
      <link>https://github.com/hiyouga/LLaMA-Factory</link>
      <description>&lt;p&gt;Unified Efficient Fine-Tuning of 100+ LLMs &amp; VLMs (ACL 2024)&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/logo.png" alt="# LLaMA Factory" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/hiyouga/LLaMA-Factory/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social" alt="GitHub Repo stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/hiyouga/LLaMA-Factory/commits/main"&gt;&lt;img src="https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory" alt="GitHub last commit" /&gt;&lt;/a&gt; &lt;a href="https://github.com/hiyouga/LLaMA-Factory/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/hiyouga/LLaMA-Factory?color=orange" alt="GitHub contributors" /&gt;&lt;/a&gt; &lt;a href="https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml"&gt;&lt;img src="https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml/badge.svg?sanitize=true" alt="GitHub workflow" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/llamafactory/"&gt;&lt;img src="https://img.shields.io/pypi/v/llamafactory" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://scholar.google.com/scholar?cites=12620864006390196564"&gt;&lt;img src="https://img.shields.io/badge/citation-1000+-green" alt="Citation" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/hiyouga/llamafactory/tags"&gt;&lt;img src="https://img.shields.io/docker/pulls/hiyouga/llamafactory" alt="Docker Pulls" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://twitter.com/llamafactory_ai"&gt;&lt;img src="https://img.shields.io/twitter/follow/llamafactory_ai" alt="Twitter" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/rKfvV9r9FK"&gt;&lt;img src="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/thirdparty/discord.svg?sanitize=true" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing"&gt;&lt;img src="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/thirdparty/colab.svg?sanitize=true" alt="Open in Colab" /&gt;&lt;/a&gt; &lt;a href="https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory"&gt;&lt;img src="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/thirdparty/dsw.svg?sanitize=true" alt="Open in DSW" /&gt;&lt;/a&gt; &lt;a href="https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&amp;amp;utm_source=LLaMA-Factory"&gt;&lt;img src="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/thirdparty/lab4ai.svg?sanitize=true" alt="Open in Lab4ai" /&gt;&lt;/a&gt; &lt;a href="https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory"&gt;&lt;img src="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/thirdparty/online.svg?sanitize=true" alt="Open in Online" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/spaces/hiyouga/LLaMA-Board"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue" alt="Open in Spaces" /&gt;&lt;/a&gt; &lt;a href="https://modelscope.cn/studios/hiyouga/LLaMA-Board"&gt;&lt;img src="https://img.shields.io/badge/ModelScope-Open%20in%20Studios-blue" alt="Open in Studios" /&gt;&lt;/a&gt; &lt;a href="https://novita.ai/templates-library/105981?sharer=88115474-394e-4bda-968e-b88e123d0c47"&gt;&lt;img src="https://img.shields.io/badge/Novita-Deploy%20Template-blue" alt="Open in Novita" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Used by &lt;a href="https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/"&gt;Amazon&lt;/a&gt;, &lt;a href="https://developer.nvidia.com/rtx/ai-toolkit"&gt;NVIDIA&lt;/a&gt;, &lt;a href="https://help.aliyun.com/zh/pai/use-cases/fine-tune-a-llama-3-model-with-llama-factory"&gt;Aliyun&lt;/a&gt;, etc.&lt;/h3&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;h3&gt;Supporters â¤ï¸&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;
     &lt;div style="text-align: center;"&gt;
      &lt;a href="https://warp.dev/llama-factory"&gt;&lt;img alt="Warp sponsorship" width="400" src="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/sponsors/warp.jpg" /&gt;&lt;/a&gt;
      &lt;br /&gt;
      &lt;a href="https://warp.dev/llama-factory" style="font-size:larger;"&gt;Warp, the agentic terminal for developers&lt;/a&gt;
      &lt;br /&gt;
      &lt;a href="https://warp.dev/llama-factory"&gt;Available for MacOS, Linux, &amp;amp; Windows&lt;/a&gt;
     &lt;/div&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;a href="https://serpapi.com"&gt;&lt;img alt="SerpAPI sponsorship" width="250" src="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/sponsors/serpapi.svg?sanitize=true" /&gt; &lt;/a&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
 &lt;/table&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;Easily fine-tune 100+ large language models with zero-code &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#quickstart"&gt;CLI&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#fine-tuning-with-llama-board-gui-powered-by-gradio"&gt;Web UI&lt;/a&gt;&lt;/h3&gt; 
 &lt;p&gt;&lt;img src="https://trendshift.io/api/badge/repositories/4535" alt="GitHub Trend" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;ğŸ‘‹ Join our &lt;a href="https://github.com/hiyouga/llamafactory-community/raw/main/wechat/main.jpg"&gt;WeChat&lt;/a&gt;, &lt;a href="https://github.com/hiyouga/llamafactory-community/raw/main/wechat/npu.jpg"&gt;NPU&lt;/a&gt;, &lt;a href="https://github.com/hiyouga/llamafactory-community/raw/main/wechat/lab4ai.jpg"&gt;Lab4AI&lt;/a&gt;, &lt;a href="https://github.com/hiyouga/llamafactory-community/raw/main/wechat/online.jpg"&gt;LLaMA Factory Online&lt;/a&gt; user group.&lt;/p&gt; 
&lt;p&gt;[ English | &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/README_zh.md"&gt;ä¸­æ–‡&lt;/a&gt; ]&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Fine-tuning a large language model can be easy as...&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/3991a3a8-4276-4d30-9cab-4cb0c4b9b99e"&gt;https://github.com/user-attachments/assets/3991a3a8-4276-4d30-9cab-4cb0c4b9b99e&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Choose your path:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation (WIP)&lt;/strong&gt;: &lt;a href="https://llamafactory.readthedocs.io/en/latest/"&gt;https://llamafactory.readthedocs.io/en/latest/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation (AMD GPU)&lt;/strong&gt;: &lt;a href="https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/fine_tune/llama_factory_llama3.html"&gt;https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/fine_tune/llama_factory_llama3.html&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Colab (free)&lt;/strong&gt;: &lt;a href="https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing"&gt;https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Local machine&lt;/strong&gt;: Please refer to &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#getting-started"&gt;usage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;PAI-DSW (free trial)&lt;/strong&gt;: &lt;a href="https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory"&gt;https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Alaya NeW (cloud GPU deal)&lt;/strong&gt;: &lt;a href="https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory"&gt;https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Official Course&lt;/strong&gt;: &lt;a href="https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&amp;amp;utm_source=LLaMA-Factory"&gt;https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&amp;amp;utm_source=LLaMA-Factory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLaMA Factory Online&lt;/strong&gt;: &lt;a href="https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory"&gt;https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Except for the above links, all other websites are unauthorized third-party websites. Please carefully use them.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#blogs"&gt;Blogs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#changelog"&gt;Changelog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#supported-models"&gt;Supported Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#supported-training-approaches"&gt;Supported Training Approaches&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#provided-datasets"&gt;Provided Datasets&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#requirement"&gt;Requirement&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#getting-started"&gt;Getting Started&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#data-preparation"&gt;Data Preparation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#quickstart"&gt;Quickstart&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#fine-tuning-with-llama-board-gui-powered-by-gradio"&gt;Fine-Tuning with LLaMA Board GUI&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#llama-factory-online"&gt;LLaMA Factory Online&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#build-docker"&gt;Build Docker&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#deploy-with-openai-style-api-and-vllm"&gt;Deploy with OpenAI-style API and vLLM&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#download-from-modelscope-hub"&gt;Download from ModelScope Hub&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#download-from-modelers-hub"&gt;Download from Modelers Hub&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#use-wb-logger"&gt;Use W&amp;amp;B Logger&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#use-swanlab-logger"&gt;Use SwanLab Logger&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#projects-using-llama-factory"&gt;Projects using LLaMA Factory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#acknowledgement"&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Various models&lt;/strong&gt;: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Qwen2-VL, DeepSeek, Yi, Gemma, ChatGLM, Phi, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Integrated methods&lt;/strong&gt;: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, ORPO, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scalable resources&lt;/strong&gt;: 16-bit full-tuning, freeze-tuning, LoRA and 2/3/4/5/6/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced algorithms&lt;/strong&gt;: &lt;a href="https://github.com/jiaweizzhao/GaLore"&gt;GaLore&lt;/a&gt;, &lt;a href="https://github.com/Ledzy/BAdam"&gt;BAdam&lt;/a&gt;, &lt;a href="https://github.com/zhuhanqing/APOLLO"&gt;APOLLO&lt;/a&gt;, &lt;a href="https://github.com/zyushun/Adam-mini"&gt;Adam-mini&lt;/a&gt;, &lt;a href="https://github.com/KellerJordan/Muon"&gt;Muon&lt;/a&gt;, &lt;a href="https://github.com/huggingface/peft/tree/main/src/peft/tuners/oft"&gt;OFT&lt;/a&gt;, DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ and PiSSA.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Practical tricks&lt;/strong&gt;: &lt;a href="https://github.com/Dao-AILab/flash-attention"&gt;FlashAttention-2&lt;/a&gt;, &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt;, &lt;a href="https://github.com/linkedin/Liger-Kernel"&gt;Liger Kernel&lt;/a&gt;, RoPE scaling, NEFTune and rsLoRA.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Wide tasks&lt;/strong&gt;: Multi-turn dialogue, tool using, image understanding, visual grounding, video recognition, audio understanding, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Experiment monitors&lt;/strong&gt;: LlamaBoard, TensorBoard, Wandb, MLflow, &lt;a href="https://github.com/SwanHubX/SwanLab"&gt;SwanLab&lt;/a&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Faster inference&lt;/strong&gt;: OpenAI-style API, Gradio UI and CLI with &lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM worker&lt;/a&gt; or &lt;a href="https://github.com/sgl-project/sglang"&gt;SGLang worker&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Day-N Support for Fine-Tuning Cutting-Edge Models&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Support Date&lt;/th&gt; 
   &lt;th&gt;Model Name&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Day 0&lt;/td&gt; 
   &lt;td&gt;Qwen3 / Qwen2.5-VL / Gemma 3 / GLM-4.1V / InternLM 3 / MiniCPM-o-2.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Day 1&lt;/td&gt; 
   &lt;td&gt;Llama 3 / GLM-4 / Mistral Small / PaliGemma2 / Llama 4&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Blogs&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ’¡ &lt;a href="https://buaa-act.feishu.cn/wiki/GVzlwYcRFiR8OLkHbL6cQpYin7g"&gt;Easy Dataset Ã— LLaMA Factory: Enabling LLMs to Efficiently Learn Domain Knowledge&lt;/a&gt; (English)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.lab4ai.cn/project/detail?id=25cce32ec131497b9e06a93336a0817f&amp;amp;type=project&amp;amp;utm_source=LLaMA-Factory"&gt;Fine-tune a mental health LLM using LLaMA-Factory&lt;/a&gt; (Chinese)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.llamafactory.com.cn/docs/documents/best-practice/gptroleplay/?utm_source=LLaMA-Factory"&gt;Fine-tune GPT-OSS for Role-Playing using LLaMA-Factory&lt;/a&gt; (Chinese)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/cn/blogs/china/building-llm-model-hub-based-on-llamafactory-and-easyr1/"&gt;A One-Stop Code-Free Model Reinforcement Learning and Deployment Platform based on LLaMA-Factory and EasyR1&lt;/a&gt; (Chinese)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/"&gt;How Apoidea Group enhances visual information extraction from banking documents with multimodal models using LLaMA-Factory on Amazon SageMaker HyperPod&lt;/a&gt; (English)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt;
 &lt;summary&gt;All Blogs&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://docs.alayanew.com/docs/documents/bestPractice/bigModel/llama70B/?utm_source=LLaMA-Factory"&gt;Fine-tune Llama3.1-70B for Medical Diagnosis using LLaMA-Factory&lt;/a&gt; (Chinese)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory"&gt;Fine-tune Qwen2.5-VL for Autonomous Driving using LLaMA-Factory&lt;/a&gt; (Chinese)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_deepseek_r1_distill_7b"&gt;LLaMA Factory: Fine-tuning the DeepSeek-R1-Distill-Qwen-7B Model for News Classifier&lt;/a&gt; (Chinese)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/"&gt;A One-Stop Code-Free Model Fine-Tuning &amp;amp; Deployment Platform based on SageMaker and LLaMA-Factory&lt;/a&gt; (Chinese)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_qwen2vl"&gt;LLaMA Factory Multi-Modal Fine-Tuning Practice: Fine-Tuning Qwen2-VL for Personal Tourist Guide&lt;/a&gt; (Chinese)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory"&gt;LLaMA Factory: Fine-tuning Llama3 for Role-Playing&lt;/a&gt; (Chinese)&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Changelog&lt;/h2&gt; 
&lt;p&gt;[25/10/26] We support Megatron-core training backend with &lt;a href="https://github.com/alibaba/ROLL/tree/main/mcore_adapter"&gt;&lt;strong&gt;mcore_adapter&lt;/strong&gt;&lt;/a&gt;. See &lt;a href="https://github.com/hiyouga/LLaMA-Factory/pull/9237"&gt;PR #9237&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;p&gt;[25/08/22] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2306.07280"&gt;OFT&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2506.19847"&gt;OFTv2&lt;/a&gt;&lt;/strong&gt;. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
&lt;p&gt;[25/08/20] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://huggingface.co/internlm/Intern-S1-mini"&gt;Intern-S1-mini&lt;/a&gt;&lt;/strong&gt; models. See &lt;a href="https://github.com/hiyouga/LLaMA-Factory/pull/8976"&gt;PR #8976&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;p&gt;[25/08/06] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://github.com/openai/gpt-oss"&gt;GPT-OSS&lt;/a&gt;&lt;/strong&gt; models. See &lt;a href="https://github.com/hiyouga/LLaMA-Factory/pull/8826"&gt;PR #8826&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;details&gt;
 &lt;summary&gt;Full Changelog&lt;/summary&gt; 
 &lt;p&gt;[25/07/02] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://github.com/THUDM/GLM-4.1V-Thinking"&gt;GLM-4.1V-9B-Thinking&lt;/a&gt;&lt;/strong&gt; model.&lt;/p&gt; 
 &lt;p&gt;[25/04/28] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://qwenlm.github.io/blog/qwen3/"&gt;Qwen3&lt;/a&gt;&lt;/strong&gt; model family.&lt;/p&gt; 
 &lt;p&gt;[25/04/21] We supported the &lt;strong&gt;&lt;a href="https://github.com/KellerJordan/Muon"&gt;Muon&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage. Thank &lt;a href="https://github.com/tianshijing"&gt;@tianshijing&lt;/a&gt;'s PR.&lt;/p&gt; 
 &lt;p&gt;[25/04/16] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://huggingface.co/OpenGVLab/InternVL3-8B"&gt;InternVL3&lt;/a&gt;&lt;/strong&gt; model. See &lt;a href="https://github.com/hiyouga/LLaMA-Factory/pull/7258"&gt;PR #7258&lt;/a&gt; to get started.&lt;/p&gt; 
 &lt;p&gt;[25/04/14] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://huggingface.co/THUDM/GLM-Z1-9B-0414"&gt;GLM-Z1&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-VL-A3B-Instruct"&gt;Kimi-VL&lt;/a&gt;&lt;/strong&gt; models.&lt;/p&gt; 
 &lt;p&gt;[25/04/06] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/"&gt;Llama 4&lt;/a&gt;&lt;/strong&gt; model. See &lt;a href="https://github.com/hiyouga/LLaMA-Factory/pull/7611"&gt;PR #7611&lt;/a&gt; to get started.&lt;/p&gt; 
 &lt;p&gt;[25/03/31] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://qwenlm.github.io/blog/qwen2.5-omni/"&gt;Qwen2.5 Omni&lt;/a&gt;&lt;/strong&gt; model. See &lt;a href="https://github.com/hiyouga/LLaMA-Factory/pull/7537"&gt;PR #7537&lt;/a&gt; to get started.&lt;/p&gt; 
 &lt;p&gt;[25/03/15] We supported &lt;strong&gt;&lt;a href="https://github.com/sgl-project/sglang"&gt;SGLang&lt;/a&gt;&lt;/strong&gt; as inference backend. Try &lt;code&gt;infer_backend: sglang&lt;/code&gt; to accelerate inference.&lt;/p&gt; 
 &lt;p&gt;[25/03/12] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://huggingface.co/blog/gemma3"&gt;Gemma 3&lt;/a&gt;&lt;/strong&gt; model.&lt;/p&gt; 
 &lt;p&gt;[25/02/24] Announcing &lt;strong&gt;&lt;a href="https://github.com/hiyouga/EasyR1"&gt;EasyR1&lt;/a&gt;&lt;/strong&gt;, an efficient, scalable and multi-modality RL training framework for efficient GRPO training.&lt;/p&gt; 
 &lt;p&gt;[25/02/11] We supported saving the &lt;strong&gt;&lt;a href="https://github.com/ollama/ollama"&gt;Ollama&lt;/a&gt;&lt;/strong&gt; modelfile when exporting the model checkpoints. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[25/02/05] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/Qwen/Qwen2-Audio-7B-Instruct"&gt;Qwen2-Audio&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6"&gt;MiniCPM-o-2.6&lt;/a&gt;&lt;/strong&gt; on audio understanding tasks.&lt;/p&gt; 
 &lt;p&gt;[25/01/31] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1"&gt;DeepSeek-R1&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct"&gt;Qwen2.5-VL&lt;/a&gt;&lt;/strong&gt; models.&lt;/p&gt; 
 &lt;p&gt;[25/01/15] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2412.05270"&gt;APOLLO&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[25/01/14] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6"&gt;MiniCPM-o-2.6&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-V-2_6"&gt;MiniCPM-V-2.6&lt;/a&gt;&lt;/strong&gt; models. Thank &lt;a href="https://github.com/BUAADreamer"&gt;@BUAADreamer&lt;/a&gt;'s PR.&lt;/p&gt; 
 &lt;p&gt;[25/01/14] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://huggingface.co/collections/internlm/"&gt;InternLM 3&lt;/a&gt;&lt;/strong&gt; models. Thank &lt;a href="https://github.com/hhaAndroid"&gt;@hhaAndroid&lt;/a&gt;'s PR.&lt;/p&gt; 
 &lt;p&gt;[25/01/10] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://huggingface.co/microsoft/phi-4"&gt;Phi-4&lt;/a&gt;&lt;/strong&gt; model.&lt;/p&gt; 
 &lt;p&gt;[24/12/21] We supported using &lt;strong&gt;&lt;a href="https://github.com/SwanHubX/SwanLab"&gt;SwanLab&lt;/a&gt;&lt;/strong&gt; for experiment tracking and visualization. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#use-swanlab-logger"&gt;this section&lt;/a&gt; for details.&lt;/p&gt; 
 &lt;p&gt;[24/11/27] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://huggingface.co/Skywork/Skywork-o1-Open-Llama-3.1-8B"&gt;Skywork-o1&lt;/a&gt;&lt;/strong&gt; model and the &lt;strong&gt;&lt;a href="https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT"&gt;OpenO1&lt;/a&gt;&lt;/strong&gt; dataset.&lt;/p&gt; 
 &lt;p&gt;[24/10/09] We supported downloading pre-trained models and datasets from the &lt;strong&gt;&lt;a href="https://modelers.cn/models"&gt;Modelers Hub&lt;/a&gt;&lt;/strong&gt;. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#download-from-modelers-hub"&gt;this tutorial&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/09/19] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://qwenlm.github.io/blog/qwen2.5/"&gt;Qwen2.5&lt;/a&gt;&lt;/strong&gt; models.&lt;/p&gt; 
 &lt;p&gt;[24/08/30] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://qwenlm.github.io/blog/qwen2-vl/"&gt;Qwen2-VL&lt;/a&gt;&lt;/strong&gt; models. Thank &lt;a href="https://github.com/simonJJJ"&gt;@simonJJJ&lt;/a&gt;'s PR.&lt;/p&gt; 
 &lt;p&gt;[24/08/27] We supported &lt;strong&gt;&lt;a href="https://github.com/linkedin/Liger-Kernel"&gt;Liger Kernel&lt;/a&gt;&lt;/strong&gt;. Try &lt;code&gt;enable_liger_kernel: true&lt;/code&gt; for efficient training.&lt;/p&gt; 
 &lt;p&gt;[24/08/09] We supported &lt;strong&gt;&lt;a href="https://github.com/zyushun/Adam-mini"&gt;Adam-mini&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage. Thank &lt;a href="https://github.com/relic-yuexi"&gt;@relic-yuexi&lt;/a&gt;'s PR.&lt;/p&gt; 
 &lt;p&gt;[24/07/04] We supported &lt;a href="https://github.com/MeetKai/functionary/tree/main/functionary/train/packing"&gt;contamination-free packed training&lt;/a&gt;. Use &lt;code&gt;neat_packing: true&lt;/code&gt; to activate it. Thank &lt;a href="https://github.com/chuan298"&gt;@chuan298&lt;/a&gt;'s PR.&lt;/p&gt; 
 &lt;p&gt;[24/06/16] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2404.02948"&gt;PiSSA&lt;/a&gt;&lt;/strong&gt; algorithm. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/06/07] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://qwenlm.github.io/blog/qwen2/"&gt;Qwen2&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href="https://github.com/THUDM/GLM-4"&gt;GLM-4&lt;/a&gt;&lt;/strong&gt; models.&lt;/p&gt; 
 &lt;p&gt;[24/05/26] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2405.14734"&gt;SimPO&lt;/a&gt;&lt;/strong&gt; algorithm for preference learning. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/05/20] We supported fine-tuning the &lt;strong&gt;PaliGemma&lt;/strong&gt; series models. Note that the PaliGemma models are pre-trained models, you need to fine-tune them with &lt;code&gt;paligemma&lt;/code&gt; template for chat completion.&lt;/p&gt; 
 &lt;p&gt;[24/05/18] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2402.01306"&gt;KTO&lt;/a&gt;&lt;/strong&gt; algorithm for preference learning. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/05/14] We supported training and inference on the Ascend NPU devices. Check &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#installation"&gt;installation&lt;/a&gt; section for details.&lt;/p&gt; 
 &lt;p&gt;[24/04/26] We supported fine-tuning the &lt;strong&gt;LLaVA-1.5&lt;/strong&gt; multimodal LLMs. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/04/22] We provided a &lt;strong&gt;&lt;a href="https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing"&gt;Colab notebook&lt;/a&gt;&lt;/strong&gt; for fine-tuning the Llama-3 model on a free T4 GPU. Two Llama-3-derived models fine-tuned using LLaMA Factory are available at Hugging Face, check &lt;a href="https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat"&gt;Llama3-8B-Chinese-Chat&lt;/a&gt; and &lt;a href="https://huggingface.co/zhichen/Llama3-Chinese"&gt;Llama3-Chinese&lt;/a&gt; for details.&lt;/p&gt; 
 &lt;p&gt;[24/04/21] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2404.02258"&gt;Mixture-of-Depths&lt;/a&gt;&lt;/strong&gt; according to &lt;a href="https://github.com/astramind-ai/Mixture-of-depths"&gt;AstraMindAI's implementation&lt;/a&gt;. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/04/16] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2404.02827"&gt;BAdam&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/04/16] We supported &lt;strong&gt;&lt;a href="https://github.com/unslothai/unsloth"&gt;unsloth&lt;/a&gt;&lt;/strong&gt;'s long-sequence training (Llama-2-7B-56k within 24GB). It achieves &lt;strong&gt;117%&lt;/strong&gt; speed and &lt;strong&gt;50%&lt;/strong&gt; memory compared with FlashAttention-2, more benchmarks can be found in &lt;a href="https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison"&gt;this page&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;[24/03/31] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2403.07691"&gt;ORPO&lt;/a&gt;&lt;/strong&gt;. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/03/21] Our paper "&lt;a href="https://arxiv.org/abs/2403.13372"&gt;LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models&lt;/a&gt;" is available at arXiv!&lt;/p&gt; 
 &lt;p&gt;[24/03/20] We supported &lt;strong&gt;FSDP+QLoRA&lt;/strong&gt; that fine-tunes a 70B model on 2x24GB GPUs. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/03/13] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2402.12354"&gt;LoRA+&lt;/a&gt;&lt;/strong&gt;. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/03/07] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2403.03507"&gt;GaLore&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/03/07] We integrated &lt;strong&gt;&lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;&lt;/strong&gt; for faster and concurrent inference. Try &lt;code&gt;infer_backend: vllm&lt;/code&gt; to enjoy &lt;strong&gt;270%&lt;/strong&gt; inference speed.&lt;/p&gt; 
 &lt;p&gt;[24/02/28] We supported weight-decomposed LoRA (&lt;strong&gt;&lt;a href="https://arxiv.org/abs/2402.09353"&gt;DoRA&lt;/a&gt;&lt;/strong&gt;). Try &lt;code&gt;use_dora: true&lt;/code&gt; to activate DoRA training.&lt;/p&gt; 
 &lt;p&gt;[24/02/15] We supported &lt;strong&gt;block expansion&lt;/strong&gt; proposed by &lt;a href="https://github.com/TencentARC/LLaMA-Pro"&gt;LLaMA Pro&lt;/a&gt;. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/02/05] Qwen1.5 (Qwen2 beta version) series models are supported in LLaMA-Factory. Check this &lt;a href="https://qwenlm.github.io/blog/qwen1.5/"&gt;blog post&lt;/a&gt; for details.&lt;/p&gt; 
 &lt;p&gt;[24/01/18] We supported &lt;strong&gt;agent tuning&lt;/strong&gt; for most models, equipping model with tool using abilities by fine-tuning with &lt;code&gt;dataset: glaive_toolcall_en&lt;/code&gt;.&lt;/p&gt; 
 &lt;p&gt;[23/12/23] We supported &lt;strong&gt;&lt;a href="https://github.com/unslothai/unsloth"&gt;unsloth&lt;/a&gt;&lt;/strong&gt;'s implementation to boost LoRA tuning for the LLaMA, Mistral and Yi models. Try &lt;code&gt;use_unsloth: true&lt;/code&gt; argument to activate unsloth patch. It achieves &lt;strong&gt;170%&lt;/strong&gt; speed in our benchmark, check &lt;a href="https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison"&gt;this page&lt;/a&gt; for details.&lt;/p&gt; 
 &lt;p&gt;[23/12/12] We supported fine-tuning the latest MoE model &lt;strong&gt;&lt;a href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1"&gt;Mixtral 8x7B&lt;/a&gt;&lt;/strong&gt; in our framework. See hardware requirement &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#hardware-requirement"&gt;here&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;[23/12/01] We supported downloading pre-trained models and datasets from the &lt;strong&gt;&lt;a href="https://modelscope.cn/models"&gt;ModelScope Hub&lt;/a&gt;&lt;/strong&gt;. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#download-from-modelscope-hub"&gt;this tutorial&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[23/10/21] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2310.05914"&gt;NEFTune&lt;/a&gt;&lt;/strong&gt; trick for fine-tuning. Try &lt;code&gt;neftune_noise_alpha: 5&lt;/code&gt; argument to activate NEFTune.&lt;/p&gt; 
 &lt;p&gt;[23/09/27] We supported &lt;strong&gt;$S^2$-Attn&lt;/strong&gt; proposed by &lt;a href="https://github.com/dvlab-research/LongLoRA"&gt;LongLoRA&lt;/a&gt; for the LLaMA models. Try &lt;code&gt;shift_attn: true&lt;/code&gt; argument to enable shift short attention.&lt;/p&gt; 
 &lt;p&gt;[23/09/23] We integrated MMLU, C-Eval and CMMLU benchmarks in this repo. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[23/09/10] We supported &lt;strong&gt;&lt;a href="https://github.com/Dao-AILab/flash-attention"&gt;FlashAttention-2&lt;/a&gt;&lt;/strong&gt;. Try &lt;code&gt;flash_attn: fa2&lt;/code&gt; argument to enable FlashAttention-2 if you are using RTX4090, A100 or H100 GPUs.&lt;/p&gt; 
 &lt;p&gt;[23/08/12] We supported &lt;strong&gt;RoPE scaling&lt;/strong&gt; to extend the context length of the LLaMA models. Try &lt;code&gt;rope_scaling: linear&lt;/code&gt; argument in training and &lt;code&gt;rope_scaling: dynamic&lt;/code&gt; argument at inference to extrapolate the position embeddings.&lt;/p&gt; 
 &lt;p&gt;[23/08/11] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2305.18290"&gt;DPO training&lt;/a&gt;&lt;/strong&gt; for instruction-tuned models. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[23/07/31] We supported &lt;strong&gt;dataset streaming&lt;/strong&gt;. Try &lt;code&gt;streaming: true&lt;/code&gt; and &lt;code&gt;max_steps: 10000&lt;/code&gt; arguments to load your dataset in streaming mode.&lt;/p&gt; 
 &lt;p&gt;[23/07/29] We released two instruction-tuned 13B models at Hugging Face. See these Hugging Face Repos (&lt;a href="https://huggingface.co/hiyouga/Llama-2-Chinese-13b-chat"&gt;LLaMA-2&lt;/a&gt; / &lt;a href="https://huggingface.co/hiyouga/Baichuan-13B-sft"&gt;Baichuan&lt;/a&gt;) for details.&lt;/p&gt; 
 &lt;p&gt;[23/07/18] We developed an &lt;strong&gt;all-in-one Web UI&lt;/strong&gt; for training, evaluation and inference. Try &lt;code&gt;train_web.py&lt;/code&gt; to fine-tune models in your Web browser. Thank &lt;a href="https://github.com/KanadeSiina"&gt;@KanadeSiina&lt;/a&gt; and &lt;a href="https://github.com/codemayq"&gt;@codemayq&lt;/a&gt; for their efforts in the development.&lt;/p&gt; 
 &lt;p&gt;[23/07/09] We released &lt;strong&gt;&lt;a href="https://github.com/hiyouga/FastEdit"&gt;FastEdit&lt;/a&gt;&lt;/strong&gt; âš¡ğŸ©¹, an easy-to-use package for editing the factual knowledge of large language models efficiently. Please follow &lt;a href="https://github.com/hiyouga/FastEdit"&gt;FastEdit&lt;/a&gt; if you are interested.&lt;/p&gt; 
 &lt;p&gt;[23/06/29] We provided a &lt;strong&gt;reproducible example&lt;/strong&gt; of training a chat model using instruction-following datasets, see &lt;a href="https://huggingface.co/hiyouga/Baichuan-7B-sft"&gt;Baichuan-7B-sft&lt;/a&gt; for details.&lt;/p&gt; 
 &lt;p&gt;[23/06/22] We aligned the &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/src/api_demo.py"&gt;demo API&lt;/a&gt; with the &lt;a href="https://platform.openai.com/docs/api-reference/chat"&gt;OpenAI's&lt;/a&gt; format where you can insert the fine-tuned model in &lt;strong&gt;arbitrary ChatGPT-based applications&lt;/strong&gt;.&lt;/p&gt; 
 &lt;p&gt;[23/06/03] We supported quantized training and inference (aka &lt;strong&gt;&lt;a href="https://github.com/artidoro/qlora"&gt;QLoRA&lt;/a&gt;&lt;/strong&gt;). See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
&lt;/details&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] If you cannot use the latest feature, please pull the latest code and install LLaMA-Factory again.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Supported Models&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Model size&lt;/th&gt; 
   &lt;th&gt;Template&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/baichuan-inc"&gt;Baichuan 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/13B&lt;/td&gt; 
   &lt;td&gt;baichuan2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/bigscience"&gt;BLOOM/BLOOMZ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;560M/1.1B/1.7B/3B/7.1B/176B&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/THUDM"&gt;ChatGLM3&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;6B&lt;/td&gt; 
   &lt;td&gt;chatglm3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/CohereForAI"&gt;Command R&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;35B/104B&lt;/td&gt; 
   &lt;td&gt;cohere&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/deepseek-ai"&gt;DeepSeek (Code/MoE)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/16B/67B/236B&lt;/td&gt; 
   &lt;td&gt;deepseek&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/deepseek-ai"&gt;DeepSeek 2.5/3&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;236B/671B&lt;/td&gt; 
   &lt;td&gt;deepseek3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/deepseek-ai"&gt;DeepSeek R1 (Distill)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.5B/7B/8B/14B/32B/70B/671B&lt;/td&gt; 
   &lt;td&gt;deepseekr1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/baidu"&gt;ERNIE-4.5&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;0.3B/21B/300B&lt;/td&gt; 
   &lt;td&gt;ernie/ernie_nothink&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/tiiuae"&gt;Falcon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/11B/40B/180B&lt;/td&gt; 
   &lt;td&gt;falcon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/tiiuae"&gt;Falcon-H1&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;0.5B/1.5B/3B/7B/34B&lt;/td&gt; 
   &lt;td&gt;falcon_h1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/google"&gt;Gemma/Gemma 2/CodeGemma&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2B/7B/9B/27B&lt;/td&gt; 
   &lt;td&gt;gemma/gemma2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/google"&gt;Gemma 3/Gemma 3n&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;270M/1B/4B/6B/8B/12B/27B&lt;/td&gt; 
   &lt;td&gt;gemma3/gemma3n&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/zai-org"&gt;GLM-4/GLM-4-0414/GLM-Z1&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;9B/32B&lt;/td&gt; 
   &lt;td&gt;glm4/glmz1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/zai-org"&gt;GLM-4.1V&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;9B&lt;/td&gt; 
   &lt;td&gt;glm4v&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/zai-org"&gt;GLM-4.5/GLM-4.5V&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;106B/355B&lt;/td&gt; 
   &lt;td&gt;glm4_moe/glm4v_moe&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/openai-community"&gt;GPT-2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;0.1B/0.4B/0.8B/1.5B&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/openai"&gt;GPT-OSS&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;20B/120B&lt;/td&gt; 
   &lt;td&gt;gpt&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/ibm-granite"&gt;Granite 3.0-3.3&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1B/2B/3B/8B&lt;/td&gt; 
   &lt;td&gt;granite3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/ibm-granite"&gt;Granite 4&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;granite4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/tencent/"&gt;Hunyuan (MT)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;hunyuan&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/IndexTeam"&gt;Index&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.9B&lt;/td&gt; 
   &lt;td&gt;index&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/internlm"&gt;InternLM 2-3&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/8B/20B&lt;/td&gt; 
   &lt;td&gt;intern2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/OpenGVLab"&gt;InternVL 2.5-3.5&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1B/2B/4B/8B/14B/30B/38B/78B/241B&lt;/td&gt; 
   &lt;td&gt;intern_vl&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/internlm/"&gt;InternLM/Intern-S1-mini&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;8B&lt;/td&gt; 
   &lt;td&gt;intern_s1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/moonshotai"&gt;Kimi-VL&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;16B&lt;/td&gt; 
   &lt;td&gt;kimi_vl&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/inclusionAI"&gt;Ling 2.0 (mini/flash)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;16B/100B&lt;/td&gt; 
   &lt;td&gt;bailing_v2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/facebookresearch/llama"&gt;Llama&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/13B/33B/65B&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/meta-llama"&gt;Llama 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/13B/70B&lt;/td&gt; 
   &lt;td&gt;llama2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/meta-llama"&gt;Llama 3-3.3&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1B/3B/8B/70B&lt;/td&gt; 
   &lt;td&gt;llama3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/meta-llama"&gt;Llama 4&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;109B/402B&lt;/td&gt; 
   &lt;td&gt;llama4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/meta-llama"&gt;Llama 3.2 Vision&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11B/90B&lt;/td&gt; 
   &lt;td&gt;mllama&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/llava-hf"&gt;LLaVA-1.5&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/13B&lt;/td&gt; 
   &lt;td&gt;llava&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/llava-hf"&gt;LLaVA-NeXT&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/8B/13B/34B/72B/110B&lt;/td&gt; 
   &lt;td&gt;llava_next&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/llava-hf"&gt;LLaVA-NeXT-Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/34B&lt;/td&gt; 
   &lt;td&gt;llava_next_video&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/XiaomiMiMo"&gt;MiMo&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;mimo&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/openbmb"&gt;MiniCPM 1-4.1&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;0.5B/1B/2B/4B/8B&lt;/td&gt; 
   &lt;td&gt;cpm/cpm3/cpm4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/openbmb"&gt;MiniCPM-o-2.6/MiniCPM-V-2.6&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;8B&lt;/td&gt; 
   &lt;td&gt;minicpm_o/minicpm_v&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mistralai"&gt;Ministral/Mistral-Nemo&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;8B/12B&lt;/td&gt; 
   &lt;td&gt;ministral&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mistralai"&gt;Mistral/Mixtral&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/8x7B/8x22B&lt;/td&gt; 
   &lt;td&gt;mistral&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mistralai"&gt;Mistral Small&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;24B&lt;/td&gt; 
   &lt;td&gt;mistral_small&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/allenai"&gt;OLMo&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1B/7B&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/google"&gt;PaliGemma/PaliGemma2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3B/10B/28B&lt;/td&gt; 
   &lt;td&gt;paligemma&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/microsoft"&gt;Phi-1.5/Phi-2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.3B/2.7B&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/microsoft"&gt;Phi-3/Phi-3.5&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;4B/14B&lt;/td&gt; 
   &lt;td&gt;phi&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/microsoft"&gt;Phi-3-small&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;phi_small&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/microsoft"&gt;Phi-4&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;14B&lt;/td&gt; 
   &lt;td&gt;phi4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mistralai"&gt;Pixtral&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12B&lt;/td&gt; 
   &lt;td&gt;pixtral&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/Qwen"&gt;Qwen (1-2.5) (Code/Math/MoE/QwQ)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;0.5B/1.5B/3B/7B/14B/32B/72B/110B&lt;/td&gt; 
   &lt;td&gt;qwen&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/Qwen"&gt;Qwen3 (MoE/Instruct/Thinking/Next)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;0.6B/1.7B/4B/8B/14B/32B/80B/235B&lt;/td&gt; 
   &lt;td&gt;qwen3/qwen3_nothink&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/Qwen"&gt;Qwen2-Audio&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;qwen2_audio&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/Qwen"&gt;Qwen2.5-Omni&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3B/7B&lt;/td&gt; 
   &lt;td&gt;qwen2_omni&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/Qwen"&gt;Qwen3-Omni&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;30B&lt;/td&gt; 
   &lt;td&gt;qwen3_omni&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/Qwen"&gt;Qwen2-VL/Qwen2.5-VL/QVQ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2B/3B/7B/32B/72B&lt;/td&gt; 
   &lt;td&gt;qwen2_vl&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/Qwen"&gt;Qwen3-VL&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2B/4B/8B/30B/32B/235B&lt;/td&gt; 
   &lt;td&gt;qwen3_vl&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/ByteDance-Seed"&gt;Seed (OSS/Coder)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;8B/36B&lt;/td&gt; 
   &lt;td&gt;seed_oss/seed_coder&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/Skywork"&gt;Skywork o1&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;8B&lt;/td&gt; 
   &lt;td&gt;skywork_o1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/bigcode"&gt;StarCoder 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3B/7B/15B&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/Tele-AI"&gt;TeleChat2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3B/7B/35B/115B&lt;/td&gt; 
   &lt;td&gt;telechat2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/xverse"&gt;XVERSE&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/13B/65B&lt;/td&gt; 
   &lt;td&gt;xverse&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/01-ai"&gt;Yi/Yi-1.5 (Code)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.5B/6B/9B/34B&lt;/td&gt; 
   &lt;td&gt;yi&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/01-ai"&gt;Yi-VL&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;6B/34B&lt;/td&gt; 
   &lt;td&gt;yi_vl&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/IEITYuan"&gt;Yuan 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2B/51B/102B&lt;/td&gt; 
   &lt;td&gt;yuan&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] For the "base" models, the &lt;code&gt;template&lt;/code&gt; argument can be chosen from &lt;code&gt;default&lt;/code&gt;, &lt;code&gt;alpaca&lt;/code&gt;, &lt;code&gt;vicuna&lt;/code&gt; etc. But make sure to use the &lt;strong&gt;corresponding template&lt;/strong&gt; for the "instruct/chat" models.&lt;/p&gt; 
 &lt;p&gt;Remember to use the &lt;strong&gt;SAME&lt;/strong&gt; template in training and inference.&lt;/p&gt; 
 &lt;p&gt;*: You should install the &lt;code&gt;transformers&lt;/code&gt; from main branch and use &lt;code&gt;DISABLE_VERSION_CHECK=1&lt;/code&gt; to skip version check.&lt;/p&gt; 
 &lt;p&gt;**: You need to install a specific version of &lt;code&gt;transformers&lt;/code&gt; to use the corresponding model.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Please refer to &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/src/llamafactory/extras/constants.py"&gt;constants.py&lt;/a&gt; for a full list of models we supported.&lt;/p&gt; 
&lt;p&gt;You also can add a custom chat template to &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/src/llamafactory/data/template.py"&gt;template.py&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Supported Training Approaches&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Approach&lt;/th&gt; 
   &lt;th&gt;Full-tuning&lt;/th&gt; 
   &lt;th&gt;Freeze-tuning&lt;/th&gt; 
   &lt;th&gt;LoRA&lt;/th&gt; 
   &lt;th&gt;QLoRA&lt;/th&gt; 
   &lt;th&gt;OFT&lt;/th&gt; 
   &lt;th&gt;QOFT&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Pre-Training&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Supervised Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Reward Modeling&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PPO Training&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DPO Training&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;KTO Training&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ORPO Training&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SimPO Training&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] The implementation details of PPO can be found in &lt;a href="https://newfacade.github.io/notes-on-reinforcement-learning/17-ppo-trl.html"&gt;this blog&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Provided Datasets&lt;/h2&gt; 
&lt;details&gt;
 &lt;summary&gt;Pre-training datasets&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/data/wiki_demo.txt"&gt;Wiki Demo (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/tiiuae/falcon-refinedweb"&gt;RefinedWeb (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2"&gt;RedPajama V2 (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/olm/olm-wikipedia-20221220"&gt;Wikipedia (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered"&gt;Wikipedia (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/EleutherAI/pile"&gt;Pile (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/Skywork/SkyPile-150B"&gt;SkyPile (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb"&gt;FineWeb (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu"&gt;FineWeb-Edu (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BAAI/CCI3-HQ"&gt;CCI3-HQ (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BAAI/CCI3-Data"&gt;CCI3-Data (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BAAI/CCI4.0-M2-Base-v1"&gt;CCI4.0-M2-Base-v1 (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BAAI/CCI4.0-M2-CoT-v1"&gt;CCI4.0-M2-CoT-v1 (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BAAI/CCI4.0-M2-Extra-v1"&gt;CCI4.0-M2-Extra-v1 (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/bigcode/the-stack"&gt;The Stack (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/bigcode/starcoderdata"&gt;StarCoder (en)&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Supervised fine-tuning datasets&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/data/identity.json"&gt;Identity (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/tatsu-lab/stanford_alpaca"&gt;Stanford Alpaca (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-3"&gt;Stanford Alpaca (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM"&gt;Alpaca GPT4 (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2"&gt;Glaive Function Calling V2 (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/GAIR/lima"&gt;LIMA (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/JosephusCheung/GuanacoDataset"&gt;Guanaco Dataset (multilingual)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BelleGroup/train_2M_CN"&gt;BELLE 2M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BelleGroup/train_1M_CN"&gt;BELLE 1M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BelleGroup/train_0.5M_CN"&gt;BELLE 0.5M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M"&gt;BELLE Dialogue 0.4M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BelleGroup/school_math_0.25M"&gt;BELLE School Math 0.25M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M"&gt;BELLE Multiturn Chat 0.8M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/thunlp/UltraChat"&gt;UltraChat (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/garage-bAInd/Open-Platypus"&gt;OpenPlatypus (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k"&gt;CodeAlpaca 20k (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/QingyiSi/Alpaca-CoT"&gt;Alpaca CoT (multilingual)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/Open-Orca/OpenOrca"&gt;OpenOrca (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/Open-Orca/SlimOrca"&gt;SlimOrca (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/TIGER-Lab/MathInstruct"&gt;MathInstruct (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M"&gt;Firefly 1.1M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/wiki_qa"&gt;Wiki QA (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/suolyer/webqa"&gt;Web QA (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/zxbsmk/webnovel_cn"&gt;WebNovel (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/berkeley-nest/Nectar"&gt;Nectar (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data"&gt;deepctrl (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/HasturOfficial/adgen"&gt;Advertise Generating (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/totally-not-an-llm/sharegpt-hyperfiltered-3k"&gt;ShareGPT Hyperfiltered (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/shibing624/sharegpt_gpt4"&gt;ShareGPT4 (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k"&gt;UltraChat 200k (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BAAI/Infinity-Instruct"&gt;Infinity Instruct (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/THUDM/AgentInstruct"&gt;AgentInstruct (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/lmsys/lmsys-chat-1m"&gt;LMSYS Chat 1M (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k"&gt;Evol Instruct V2 (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/HuggingFaceTB/cosmopedia"&gt;Cosmopedia (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/hfl/stem_zh_instruction"&gt;STEM (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/hfl/ruozhiba_gpt4_turbo"&gt;Ruozhiba (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/m-a-p/neo_sft_phase2"&gt;Neo-sft (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/Magpie-Align/Magpie-Pro-300K-Filtered"&gt;Magpie-Pro-300K-Filtered (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/argilla/magpie-ultra-v0.1"&gt;Magpie-ultra-v0.1 (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/TIGER-Lab/WebInstructSub"&gt;WebInstructSub (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT"&gt;OpenO1-SFT (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k"&gt;Open-Thoughts (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/open-r1/OpenR1-Math-220k"&gt;Open-R1-Math (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/Congliu/Chinese-DeepSeek-R1-Distill-data-110k-SFT"&gt;Chinese-DeepSeek-R1-Distill (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BUAADreamer/llava-en-zh-300k"&gt;LLaVA mixed (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/jugg1024/pokemon-gpt4o-captions"&gt;Pokemon-gpt4o-captions (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/mayflowergmbh/oasst_de"&gt;Open Assistant (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/mayflowergmbh/dolly-15k_de"&gt;Dolly 15k (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/mayflowergmbh/alpaca-gpt4_de"&gt;Alpaca GPT4 (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/mayflowergmbh/openschnabeltier_de"&gt;OpenSchnabeltier (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/mayflowergmbh/evol-instruct_de"&gt;Evol Instruct (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/mayflowergmbh/dolphin_de"&gt;Dolphin (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/mayflowergmbh/booksum_de"&gt;Booksum (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/mayflowergmbh/airoboros-3.0_de"&gt;Airoboros (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/mayflowergmbh/ultra-chat_de"&gt;Ultrachat (de)&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Preference datasets&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/hiyouga/DPO-En-Zh-20k"&gt;DPO mixed (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized"&gt;UltraFeedback (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/m-a-p/COIG-P"&gt;COIG-P (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/openbmb/RLHF-V-Dataset"&gt;RLHF-V (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/Zhihui/VLFeedback"&gt;VLFeedback (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset"&gt;RLAIF-V (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/Intel/orca_dpo_pairs"&gt;Orca DPO Pairs (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/Anthropic/hh-rlhf"&gt;HH-RLHF (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/berkeley-nest/Nectar"&gt;Nectar (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/mayflowergmbh/intel_orca_dpo_pairs_de"&gt;Orca DPO (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/argilla/kto-mix-15k"&gt;KTO mixed (en)&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;p&gt;Some datasets require confirmation before using them, so we recommend logging in with your Hugging Face account using these commands.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade huggingface_hub
huggingface-cli login
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Requirement&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Mandatory&lt;/th&gt; 
   &lt;th&gt;Minimum&lt;/th&gt; 
   &lt;th&gt;Recommend&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;python&lt;/td&gt; 
   &lt;td&gt;3.9&lt;/td&gt; 
   &lt;td&gt;3.10&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;torch&lt;/td&gt; 
   &lt;td&gt;2.0.0&lt;/td&gt; 
   &lt;td&gt;2.6.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;torchvision&lt;/td&gt; 
   &lt;td&gt;0.15.0&lt;/td&gt; 
   &lt;td&gt;0.21.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;transformers&lt;/td&gt; 
   &lt;td&gt;4.49.0&lt;/td&gt; 
   &lt;td&gt;4.50.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;datasets&lt;/td&gt; 
   &lt;td&gt;2.16.0&lt;/td&gt; 
   &lt;td&gt;3.2.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;accelerate&lt;/td&gt; 
   &lt;td&gt;0.34.0&lt;/td&gt; 
   &lt;td&gt;1.2.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;peft&lt;/td&gt; 
   &lt;td&gt;0.14.0&lt;/td&gt; 
   &lt;td&gt;0.15.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;trl&lt;/td&gt; 
   &lt;td&gt;0.8.6&lt;/td&gt; 
   &lt;td&gt;0.9.6&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Optional&lt;/th&gt; 
   &lt;th&gt;Minimum&lt;/th&gt; 
   &lt;th&gt;Recommend&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CUDA&lt;/td&gt; 
   &lt;td&gt;11.6&lt;/td&gt; 
   &lt;td&gt;12.2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;deepspeed&lt;/td&gt; 
   &lt;td&gt;0.10.0&lt;/td&gt; 
   &lt;td&gt;0.16.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;bitsandbytes&lt;/td&gt; 
   &lt;td&gt;0.39.0&lt;/td&gt; 
   &lt;td&gt;0.43.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;vllm&lt;/td&gt; 
   &lt;td&gt;0.4.3&lt;/td&gt; 
   &lt;td&gt;0.8.2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;flash-attn&lt;/td&gt; 
   &lt;td&gt;2.5.6&lt;/td&gt; 
   &lt;td&gt;2.7.2&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Hardware Requirement&lt;/h3&gt; 
&lt;p&gt;* &lt;em&gt;estimated&lt;/em&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Bits&lt;/th&gt; 
   &lt;th&gt;7B&lt;/th&gt; 
   &lt;th&gt;14B&lt;/th&gt; 
   &lt;th&gt;30B&lt;/th&gt; 
   &lt;th&gt;70B&lt;/th&gt; 
   &lt;th&gt;&lt;code&gt;x&lt;/code&gt;B&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Full (&lt;code&gt;bf16&lt;/code&gt; or &lt;code&gt;fp16&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;32&lt;/td&gt; 
   &lt;td&gt;120GB&lt;/td&gt; 
   &lt;td&gt;240GB&lt;/td&gt; 
   &lt;td&gt;600GB&lt;/td&gt; 
   &lt;td&gt;1200GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;18x&lt;/code&gt;GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Full (&lt;code&gt;pure_bf16&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;60GB&lt;/td&gt; 
   &lt;td&gt;120GB&lt;/td&gt; 
   &lt;td&gt;300GB&lt;/td&gt; 
   &lt;td&gt;600GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;8x&lt;/code&gt;GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Freeze/LoRA/GaLore/APOLLO/BAdam/OFT&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;16GB&lt;/td&gt; 
   &lt;td&gt;32GB&lt;/td&gt; 
   &lt;td&gt;64GB&lt;/td&gt; 
   &lt;td&gt;160GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;2x&lt;/code&gt;GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QLoRA / QOFT&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;10GB&lt;/td&gt; 
   &lt;td&gt;20GB&lt;/td&gt; 
   &lt;td&gt;40GB&lt;/td&gt; 
   &lt;td&gt;80GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;x&lt;/code&gt;GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QLoRA / QOFT&lt;/td&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;6GB&lt;/td&gt; 
   &lt;td&gt;12GB&lt;/td&gt; 
   &lt;td&gt;24GB&lt;/td&gt; 
   &lt;td&gt;48GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;x/2&lt;/code&gt;GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QLoRA / QOFT&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;4GB&lt;/td&gt; 
   &lt;td&gt;8GB&lt;/td&gt; 
   &lt;td&gt;16GB&lt;/td&gt; 
   &lt;td&gt;24GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;x/4&lt;/code&gt;GB&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Installation is mandatory.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Install from Source&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e ".[torch,metrics]" --no-build-isolation
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Extra dependencies available: torch, torch-npu, metrics, deepspeed, liger-kernel, bitsandbytes, hqq, eetq, gptq, aqlm, vllm, sglang, galore, apollo, badam, adam-mini, qwen, minicpm_v, openmind, swanlab, dev&lt;/p&gt; 
&lt;h4&gt;Install from Docker Image&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -it --rm --gpus=all --ipc=host hiyouga/llamafactory:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This image is built on Ubuntu 22.04 (x86_64), CUDA 12.4, Python 3.11, PyTorch 2.6.0, and Flash-attn 2.7.4.&lt;/p&gt; 
&lt;p&gt;Find the pre-built images: &lt;a href="https://hub.docker.com/r/hiyouga/llamafactory/tags"&gt;https://hub.docker.com/r/hiyouga/llamafactory/tags&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Please refer to &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#build-docker"&gt;build docker&lt;/a&gt; to build the image yourself.&lt;/p&gt; 
&lt;details&gt;
 &lt;summary&gt;Setting up a virtual environment with &lt;b&gt;uv&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Create an isolated Python environment with &lt;a href="https://github.com/astral-sh/uv"&gt;uv&lt;/a&gt;:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;uv sync --extra torch --extra metrics --prerelease=allow
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Run LLaMA-Factory in the isolated environment:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;uv run --prerelease=allow llamafactory-cli train examples/train_lora/llama3_lora_pretrain.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;For Windows users&lt;/summary&gt; 
 &lt;h4&gt;Install PyTorch&lt;/h4&gt; 
 &lt;p&gt;You need to manually install the GPU version of PyTorch on the Windows platform. Please refer to the &lt;a href="https://pytorch.org/get-started/locally/"&gt;official website&lt;/a&gt; and the following command to install PyTorch with CUDA support:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
python -c "import torch; print(torch.cuda.is_available())"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;If you see &lt;code&gt;True&lt;/code&gt; then you have successfully installed PyTorch with CUDA support.&lt;/p&gt; 
 &lt;p&gt;Try &lt;code&gt;dataloader_num_workers: 0&lt;/code&gt; if you encounter &lt;code&gt;Can't pickle local object&lt;/code&gt; error.&lt;/p&gt; 
 &lt;h4&gt;Install BitsAndBytes&lt;/h4&gt; 
 &lt;p&gt;If you want to enable the quantized LoRA (QLoRA) on the Windows platform, you need to install a pre-built version of &lt;code&gt;bitsandbytes&lt;/code&gt; library, which supports CUDA 11.1 to 12.2, please select the appropriate &lt;a href="https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels"&gt;release version&lt;/a&gt; based on your CUDA version.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Install Flash Attention-2&lt;/h4&gt; 
 &lt;p&gt;To enable FlashAttention-2 on the Windows platform, please use the script from &lt;a href="https://huggingface.co/lldacing/flash-attention-windows-wheel"&gt;flash-attention-windows-wheel&lt;/a&gt; to compile and install it by yourself.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;For Ascend NPU users&lt;/summary&gt; 
 &lt;p&gt;To install LLaMA Factory on Ascend NPU devices, please upgrade Python to version 3.10 or higher and specify extra dependencies: &lt;code&gt;pip install -e ".[torch-npu,metrics]"&lt;/code&gt;. Additionally, you need to install the &lt;strong&gt;&lt;a href="https://www.hiascend.com/developer/download/community/result?module=cann"&gt;Ascend CANN Toolkit and Kernels&lt;/a&gt;&lt;/strong&gt;. Please follow the &lt;a href="https://www.hiascend.com/document/detail/en/CANNCommunityEdition/600alphaX/softwareinstall/instg/atlasdeploy_03_0031.html"&gt;installation tutorial&lt;/a&gt; or use the following commands:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# replace the url according to your CANN version and devices
# install CANN Toolkit
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-toolkit_8.0.0.alpha002_linux-"$(uname -i)".run
bash Ascend-cann-toolkit_8.0.0.alpha002_linux-"$(uname -i)".run --install

# install CANN Kernels
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-kernels-910b_8.0.0.alpha002_linux-"$(uname -i)".run
bash Ascend-cann-kernels-910b_8.0.0.alpha002_linux-"$(uname -i)".run --install

# set env variables
source /usr/local/Ascend/ascend-toolkit/set_env.sh
&lt;/code&gt;&lt;/pre&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Requirement&lt;/th&gt; 
    &lt;th&gt;Minimum&lt;/th&gt; 
    &lt;th&gt;Recommend&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CANN&lt;/td&gt; 
    &lt;td&gt;8.0.RC1&lt;/td&gt; 
    &lt;td&gt;8.0.0.alpha002&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torch&lt;/td&gt; 
    &lt;td&gt;2.1.0&lt;/td&gt; 
    &lt;td&gt;2.4.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torch-npu&lt;/td&gt; 
    &lt;td&gt;2.1.0&lt;/td&gt; 
    &lt;td&gt;2.4.0.post2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;deepspeed&lt;/td&gt; 
    &lt;td&gt;0.13.2&lt;/td&gt; 
    &lt;td&gt;0.13.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;vllm-ascend&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;0.7.3&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;Remember to use &lt;code&gt;ASCEND_RT_VISIBLE_DEVICES&lt;/code&gt; instead of &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; to specify the device to use.&lt;/p&gt; 
 &lt;p&gt;If you cannot infer model on NPU devices, try setting &lt;code&gt;do_sample: false&lt;/code&gt; in the configurations.&lt;/p&gt; 
 &lt;p&gt;Download the pre-built Docker images: &lt;a href="http://mirrors.cn-central-221.ovaijisuan.com/detail/130.html"&gt;32GB&lt;/a&gt; | &lt;a href="http://mirrors.cn-central-221.ovaijisuan.com/detail/131.html"&gt;64GB&lt;/a&gt;&lt;/p&gt; 
 &lt;h4&gt;Install BitsAndBytes&lt;/h4&gt; 
 &lt;p&gt;To use QLoRA based on bitsandbytes on Ascend NPU, please follow these 3 steps:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Manually compile bitsandbytes: Refer to &lt;a href="https://huggingface.co/docs/bitsandbytes/installation?backend=Ascend+NPU&amp;amp;platform=Ascend+NPU"&gt;the installation documentation&lt;/a&gt; for the NPU version of bitsandbytes to complete the compilation and installation. The compilation requires a cmake version of at least 3.22.1 and a g++ version of at least 12.x.&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Install bitsandbytes from source
# Clone bitsandbytes repo, Ascend NPU backend is currently enabled on multi-backend-refactor branch
git clone -b multi-backend-refactor https://github.com/bitsandbytes-foundation/bitsandbytes.git
cd bitsandbytes/

# Install dependencies
pip install -r requirements-dev.txt

# Install the dependencies for the compilation tools. Note that the commands for this step may vary depending on the operating system. The following are provided for reference
apt-get install -y build-essential cmake

# Compile &amp;amp; install  
cmake -DCOMPUTE_BACKEND=npu -S .
make
pip install .
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Install transformers from the main branch.&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;git clone -b main https://github.com/huggingface/transformers.git
cd transformers
pip install .
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="3"&gt; 
  &lt;li&gt;Set &lt;code&gt;double_quantization: false&lt;/code&gt; in the configuration. You can refer to the &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/train_qlora/llama3_lora_sft_bnb_npu.yaml"&gt;example&lt;/a&gt;.&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h3&gt;Data Preparation&lt;/h3&gt; 
&lt;p&gt;Please refer to &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/data/README.md"&gt;data/README.md&lt;/a&gt; for checking the details about the format of dataset files. You can use datasets on HuggingFace / ModelScope / Modelers hub, load the dataset in local disk, or specify a path to s3/gcs cloud storage.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Please update &lt;code&gt;data/dataset_info.json&lt;/code&gt; to use your custom dataset.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;You can also use &lt;strong&gt;&lt;a href="https://github.com/ConardLi/easy-dataset"&gt;Easy Dataset&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://github.com/OpenDCAI/DataFlow"&gt;DataFlow&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href="https://github.com/open-sciencelab/GraphGen"&gt;GraphGen&lt;/a&gt;&lt;/strong&gt; to create synthetic data for fine-tuning.&lt;/p&gt; 
&lt;h3&gt;Quickstart&lt;/h3&gt; 
&lt;p&gt;Use the following 3 commands to run LoRA &lt;strong&gt;fine-tuning&lt;/strong&gt;, &lt;strong&gt;inference&lt;/strong&gt; and &lt;strong&gt;merging&lt;/strong&gt; of the Llama3-8B-Instruct model, respectively.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
llamafactory-cli chat examples/inference/llama3_lora_sft.yaml
llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples/README.md&lt;/a&gt; for advanced usage (including distributed training).&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Use &lt;code&gt;llamafactory-cli help&lt;/code&gt; to show help information.&lt;/p&gt; 
 &lt;p&gt;Read &lt;a href="https://github.com/hiyouga/LLaMA-Factory/issues/4614"&gt;FAQs&lt;/a&gt; first if you encounter any problems.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Fine-Tuning with LLaMA Board GUI (powered by &lt;a href="https://github.com/gradio-app/gradio"&gt;Gradio&lt;/a&gt;)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;llamafactory-cli webui
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;LLaMA Factory Online&lt;/h3&gt; 
&lt;p&gt;Read our &lt;a href="https://docs.llamafactory.com.cn/docs/documents/quickstart/getstarted/?utm_source=LLaMA-Factory"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Build Docker&lt;/h3&gt; 
&lt;p&gt;For CUDA users:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd docker/docker-cuda/
docker compose up -d
docker compose exec llamafactory bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Ascend NPU users:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd docker/docker-npu/
docker compose up -d
docker compose exec llamafactory bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For AMD ROCm users:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd docker/docker-rocm/
docker compose up -d
docker compose exec llamafactory bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt;
 &lt;summary&gt;Build without Docker Compose&lt;/summary&gt; 
 &lt;p&gt;For CUDA users:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;docker build -f ./docker/docker-cuda/Dockerfile \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    --build-arg EXTRAS=metrics \
    -t llamafactory:latest .

docker run -dit --ipc=host --gpus=all \
    -p 7860:7860 \
    -p 8000:8000 \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;For Ascend NPU users:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;docker build -f ./docker/docker-npu/Dockerfile \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    --build-arg EXTRAS=torch-npu,metrics \
    -t llamafactory:latest .

docker run -dit --ipc=host \
    -v /usr/local/dcmi:/usr/local/dcmi \
    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \
    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \
    -v /etc/ascend_install.info:/etc/ascend_install.info \
    -p 7860:7860 \
    -p 8000:8000 \
    --device /dev/davinci0 \
    --device /dev/davinci_manager \
    --device /dev/devmm_svm \
    --device /dev/hisi_hdc \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;For AMD ROCm users:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;docker build -f ./docker/docker-rocm/Dockerfile \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    --build-arg EXTRAS=metrics \
    -t llamafactory:latest .

docker run -dit --ipc=host \
    -p 7860:7860 \
    -p 8000:8000 \
    --device /dev/kfd \
    --device /dev/dri \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Use Docker volumes&lt;/summary&gt; 
 &lt;p&gt;You can uncomment &lt;code&gt;VOLUME [ "/root/.cache/huggingface", "/app/shared_data", "/app/output" ]&lt;/code&gt; in the Dockerfile to use data volumes.&lt;/p&gt; 
 &lt;p&gt;When building the Docker image, use &lt;code&gt;-v ./hf_cache:/root/.cache/huggingface&lt;/code&gt; argument to mount the local directory to the container. The following data volumes are available.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;hf_cache&lt;/code&gt;: Utilize Hugging Face cache on the host machine.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;shared_data&lt;/code&gt;: The directionary to store datasets on the host machine.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;output&lt;/code&gt;: Set export dir to this location so that the merged result can be accessed directly on the host machine.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;Deploy with OpenAI-style API and vLLM&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;API_PORT=8000 llamafactory-cli api examples/inference/llama3.yaml infer_backend=vllm vllm_enforce_eager=true
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Visit &lt;a href="https://platform.openai.com/docs/api-reference/chat/create"&gt;this page&lt;/a&gt; for API document.&lt;/p&gt; 
 &lt;p&gt;Examples: &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/scripts/api_example/test_image.py"&gt;Image understanding&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/scripts/api_example/test_toolcall.py"&gt;Function calling&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Download from ModelScope Hub&lt;/h3&gt; 
&lt;p&gt;If you have trouble with downloading models and datasets from Hugging Face, you can use ModelScope.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export USE_MODELSCOPE_HUB=1 # `set USE_MODELSCOPE_HUB=1` for Windows
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Train the model by specifying a model ID of the ModelScope Hub as the &lt;code&gt;model_name_or_path&lt;/code&gt;. You can find a full list of model IDs at &lt;a href="https://modelscope.cn/models"&gt;ModelScope Hub&lt;/a&gt;, e.g., &lt;code&gt;LLM-Research/Meta-Llama-3-8B-Instruct&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Download from Modelers Hub&lt;/h3&gt; 
&lt;p&gt;You can also use Modelers Hub to download models and datasets.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export USE_OPENMIND_HUB=1 # `set USE_OPENMIND_HUB=1` for Windows
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Train the model by specifying a model ID of the Modelers Hub as the &lt;code&gt;model_name_or_path&lt;/code&gt;. You can find a full list of model IDs at &lt;a href="https://modelers.cn/models"&gt;Modelers Hub&lt;/a&gt;, e.g., &lt;code&gt;TeleAI/TeleChat-7B-pt&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Use W&amp;amp;B Logger&lt;/h3&gt; 
&lt;p&gt;To use &lt;a href="https://wandb.ai"&gt;Weights &amp;amp; Biases&lt;/a&gt; for logging experimental results, you need to add the following arguments to yaml files.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;report_to: wandb
run_name: test_run # optional
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Set &lt;code&gt;WANDB_API_KEY&lt;/code&gt; to &lt;a href="https://wandb.ai/authorize"&gt;your key&lt;/a&gt; when launching training tasks to log in with your W&amp;amp;B account.&lt;/p&gt; 
&lt;h3&gt;Use SwanLab Logger&lt;/h3&gt; 
&lt;p&gt;To use &lt;a href="https://github.com/SwanHubX/SwanLab"&gt;SwanLab&lt;/a&gt; for logging experimental results, you need to add the following arguments to yaml files.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;use_swanlab: true
swanlab_run_name: test_run # optional
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When launching training tasks, you can log in to SwanLab in three ways:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Add &lt;code&gt;swanlab_api_key=&amp;lt;your_api_key&amp;gt;&lt;/code&gt; to the yaml file, and set it to your &lt;a href="https://swanlab.cn/settings"&gt;API key&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Set the environment variable &lt;code&gt;SWANLAB_API_KEY&lt;/code&gt; to your &lt;a href="https://swanlab.cn/settings"&gt;API key&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Use the &lt;code&gt;swanlab login&lt;/code&gt; command to complete the login.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Projects using LLaMA Factory&lt;/h2&gt; 
&lt;p&gt;If you have a project that should be incorporated, please contact via email or create a pull request.&lt;/p&gt; 
&lt;details&gt;
 &lt;summary&gt;Click to show&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Wang et al. ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation. 2023. &lt;a href="https://arxiv.org/abs/2308.02223"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yu et al. Open, Closed, or Small Language Models for Text Classification? 2023. &lt;a href="https://arxiv.org/abs/2308.10092"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Wang et al. UbiPhysio: Support Daily Functioning, Fitness, and Rehabilitation with Action Understanding and Feedback in Natural Language. 2023. &lt;a href="https://arxiv.org/abs/2308.10526"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Luceri et al. Leveraging Large Language Models to Detect Influence Campaigns in Social Media. 2023. &lt;a href="https://arxiv.org/abs/2311.07816"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. Alleviating Hallucinations of Large Language Models through Induced Hallucinations. 2023. &lt;a href="https://arxiv.org/abs/2312.15710"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Wang et al. Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. KDD 2024. &lt;a href="https://arxiv.org/abs/2401.04319"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Wang et al. CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning. ACL 2024. &lt;a href="https://arxiv.org/abs/2401.07286"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Choi et al. FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs. 2024. &lt;a href="https://arxiv.org/abs/2402.05904"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts. 2024. &lt;a href="https://arxiv.org/abs/2402.07625"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Lyu et al. KnowTuning: Knowledge-aware Fine-tuning for Large Language Models. 2024. &lt;a href="https://arxiv.org/abs/2402.11176"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yang et al. LaCo: Large Language Model Pruning via Layer Collaps. 2024. &lt;a href="https://arxiv.org/abs/2402.11187"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Bhardwaj et al. Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic. 2024. &lt;a href="https://arxiv.org/abs/2402.11746"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yang et al. Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models. 2024. &lt;a href="https://arxiv.org/abs/2402.11801"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yi et al. Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding. ACL 2024 Findings. &lt;a href="https://arxiv.org/abs/2402.11809"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Cao et al. Head-wise Shareable Attention for Large Language Models. 2024. &lt;a href="https://arxiv.org/abs/2402.11819"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages. 2024. &lt;a href="https://arxiv.org/abs/2402.12204"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Kim et al. Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models. 2024. &lt;a href="https://arxiv.org/abs/2402.14714"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yu et al. KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models. ACL 2024. &lt;a href="https://arxiv.org/abs/2402.15043"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Huang et al. Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning. 2024. &lt;a href="https://arxiv.org/abs/2403.02333"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Duan et al. Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization. 2024. &lt;a href="https://arxiv.org/abs/2403.03419"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Xie and Schwertfeger. Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs. 2024. &lt;a href="https://arxiv.org/abs/2403.08228"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Wu et al. Large Language Models are Parallel Multilingual Learners. 2024. &lt;a href="https://arxiv.org/abs/2403.09073"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling. 2024. &lt;a href="https://arxiv.org/abs/2403.14541"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Weller et al. FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions. 2024. &lt;a href="https://arxiv.org/abs/2403.15246"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Hongbin Na. CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering. COLING 2024. &lt;a href="https://arxiv.org/abs/2403.16008"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zan et al. CodeS: Natural Language to Code Repository via Multi-Layer Sketch. 2024. &lt;a href="https://arxiv.org/abs/2403.16443"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Liu et al. Extensive Self-Contrast Enables Feedback-Free Language Model Alignment. 2024. &lt;a href="https://arxiv.org/abs/2404.00604"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Luo et al. BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models. 2024. &lt;a href="https://arxiv.org/abs/2404.02827"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Du et al. Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model. 2024. &lt;a href="https://arxiv.org/abs/2404.04167"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Ma et al. Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation. ICML 2024. &lt;a href="https://arxiv.org/abs/2404.04316"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Liu et al. Dynamic Generation of Personalities with Large Language Models. 2024. &lt;a href="https://arxiv.org/abs/2404.07084"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Shang et al. How Far Have We Gone in Stripped Binary Code Understanding Using Large Language Models. 2024. &lt;a href="https://arxiv.org/abs/2404.09836"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Huang et al. LLMTune: Accelerate Database Knob Tuning with Large Language Models. 2024. &lt;a href="https://arxiv.org/abs/2404.11581"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Deng et al. Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction. 2024. &lt;a href="https://arxiv.org/abs/2404.14215"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Acikgoz et al. Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare. 2024. &lt;a href="https://arxiv.org/abs/2404.16621"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. Small Language Models Need Strong Verifiers to Self-Correct Reasoning. ACL 2024 Findings. &lt;a href="https://arxiv.org/abs/2404.17140"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhou et al. FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering. NAACL 2024. &lt;a href="https://arxiv.org/abs/2404.18585"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Xu et al. Large Language Models for Cyber Security: A Systematic Literature Review. 2024. &lt;a href="https://arxiv.org/abs/2405.04760"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Dammu et al. "They are uncultured": Unveiling Covert Harms and Social Threats in LLM Generated Conversations. 2024. &lt;a href="https://arxiv.org/abs/2405.05378"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yi et al. A safety realignment framework via subspace-oriented model fusion for large language models. 2024. &lt;a href="https://arxiv.org/abs/2405.09055"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Lou et al. SPO: Multi-Dimensional Preference Sequential Alignment With Implicit Reward Modeling. 2024. &lt;a href="https://arxiv.org/abs/2405.12739"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners. 2024. &lt;a href="https://arxiv.org/abs/2405.13816"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models. 2024. &lt;a href="https://arxiv.org/abs/2405.20215"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zihong Chen. Sentence Segmentation and Sentence Punctuation Based on XunziALLM. 2024. &lt;a href="https://aclanthology.org/2024.lt4hala-1.30"&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Gao et al. The Best of Both Worlds: Toward an Honest and Helpful Large Language Model. 2024. &lt;a href="https://arxiv.org/abs/2406.00380"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Wang and Song. MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset. 2024. &lt;a href="https://arxiv.org/abs/2406.02106"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Hu et al. Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models. 2024. &lt;a href="https://arxiv.org/abs/2406.03136"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Ge et al. Time Sensitive Knowledge Editing through Efficient Finetuning. ACL 2024. &lt;a href="https://arxiv.org/abs/2406.04496"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Tan et al. Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions. 2024. &lt;a href="https://arxiv.org/abs/2406.05688"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Song et al. Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters. 2024. &lt;a href="https://arxiv.org/abs/2406.05955"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Gu et al. RWKV-CLIP: A Robust Vision-Language Representation Learner. 2024. &lt;a href="https://arxiv.org/abs/2406.06973"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Chen et al. Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees. 2024. &lt;a href="https://arxiv.org/abs/2406.07115"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhu et al. Are Large Language Models Good Statisticians?. 2024. &lt;a href="https://arxiv.org/abs/2406.07815"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Li et al. Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning. 2024. &lt;a href="https://arxiv.org/abs/2406.10099"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Ding et al. IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce. 2024. &lt;a href="https://arxiv.org/abs/2406.10173"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;He et al. COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities. 2024. &lt;a href="https://arxiv.org/abs/2406.12074"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Lin et al. FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving. 2024. &lt;a href="https://arxiv.org/abs/2406.14408"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Treutlein et al. Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data. 2024. &lt;a href="https://arxiv.org/abs/2406.14546"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Feng et al. SS-Bench: A Benchmark for Social Story Generation and Evaluation. 2024. &lt;a href="https://arxiv.org/abs/2406.15695"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Feng et al. Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement. 2024. &lt;a href="https://arxiv.org/abs/2406.17233"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Liu et al. Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals. 2024. &lt;a href="https://arxiv.org/abs/2406.18069"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Iyer et al. Exploring Very Low-Resource Translation with LLMs: The University of Edinburgh's Submission to AmericasNLP 2024 Translation Task. AmericasNLP 2024. &lt;a href="https://aclanthology.org/2024.americasnlp-1.25"&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Li et al. Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring. 2024. &lt;a href="https://arxiv.org/abs/2406.19949"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yang et al. Financial Knowledge Large Language Model. 2024. &lt;a href="https://arxiv.org/abs/2407.00365"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Lin et al. DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging. 2024. &lt;a href="https://arxiv.org/abs/2407.01470"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Bako et al. Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization. 2024. &lt;a href="https://arxiv.org/abs/2407.06129"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Huang et al. RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization. 2024. &lt;a href="https://arxiv.org/abs/2407.08044"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Jiang et al. LLM-Collaboration on Automatic Science Journalism for the General Audience. 2024. &lt;a href="https://arxiv.org/abs/2407.09756"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Inouye et al. Applied Auto-tuning on LoRA Hyperparameters. 2024. &lt;a href="https://scholarcommons.scu.edu/cseng_senior/272/"&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Qi et al. Research on Tibetan Tourism Viewpoints information generation system based on LLM. 2024. &lt;a href="https://arxiv.org/abs/2407.13561"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Xu et al. Course-Correction: Safety Alignment Using Synthetic Preferences. 2024. &lt;a href="https://arxiv.org/abs/2407.16637"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Sun et al. LAMBDA: A Large Model Based Data Agent. 2024. &lt;a href="https://arxiv.org/abs/2407.17535"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhu et al. CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare. 2024. &lt;a href="https://arxiv.org/abs/2407.19705"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yu et al. Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment. 2024. &lt;a href="https://arxiv.org/abs/2408.00137"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Xie et al. The Power of Personalized Datasets: Advancing Chinese Composition Writing for Elementary School through Targeted Model Fine-Tuning. IALP 2024. &lt;a href="https://www.asianlp.sg/conferences/ialp2024/proceedings/papers/IALP2024_P055.pdf"&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Liu et al. Instruct-Code-Llama: Improving Capabilities of Language Model in Competition Level Code Generation by Online Judge Feedback. ICIC 2024. &lt;a href="https://link.springer.com/chapter/10.1007/978-981-97-5669-8_11"&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Wang et al. Cybernetic Sentinels: Unveiling the Impact of Safety Data Selection on Model Security in Supervised Fine-Tuning. ICIC 2024. &lt;a href="https://link.springer.com/chapter/10.1007/978-981-97-5669-8_23"&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Xia et al. Understanding the Performance and Estimating the Cost of LLM Fine-Tuning. 2024. &lt;a href="https://arxiv.org/abs/2408.04693"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zeng et al. Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions. 2024. &lt;a href="https://arxiv.org/abs/2408.04168"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Xia et al. Using Pre-trained Language Model for Accurate ESG Prediction. FinNLP 2024. &lt;a href="https://aclanthology.org/2024.finnlp-2.1/"&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Liang et al. I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm. 2024. &lt;a href="https://arxiv.org/abs/2408.08072"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Bai et al. Aligning Large Language Model with Direct Multi-Preference Optimization for Recommendation. CIKM 2024. &lt;a href="https://dl.acm.org/doi/10.1145/3627673.3679611"&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. CPsyCoun: A Report-based Multi-turn Dialogue Reconstruction and Evaluation Framework for Chinese Psychological Counseling. ACL 2024. &lt;a href="https://aclanthology.org/2024.findings-acl.830.pdf"&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/Yu-Yang-Li/StarWhisper"&gt;StarWhisper&lt;/a&gt;&lt;/strong&gt;: A large language model for Astronomy, based on ChatGLM2-6B and Qwen-14B.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/FudanDISC/DISC-LawLLM"&gt;DISC-LawLLM&lt;/a&gt;&lt;/strong&gt;: A large language model specialized in Chinese legal domain, based on Baichuan-13B, is capable of retrieving and reasoning on legal knowledge.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/X-D-Lab/Sunsimiao"&gt;Sunsimiao&lt;/a&gt;&lt;/strong&gt;: A large language model specialized in Chinese medical domain, based on Baichuan-7B and ChatGLM-6B.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/WangRongsheng/CareGPT"&gt;CareGPT&lt;/a&gt;&lt;/strong&gt;: A series of large language models for Chinese medical domain, based on LLaMA2-7B and Baichuan-13B.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/PKU-YuanGroup/Machine-Mindset/"&gt;MachineMindset&lt;/a&gt;&lt;/strong&gt;: A series of MBTI Personality large language models, capable of giving any LLM 16 different personality types based on different datasets and training methods.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://huggingface.co/Nekochu/Luminia-13B-v3"&gt;Luminia-13B-v3&lt;/a&gt;&lt;/strong&gt;: A large language model specialized in generate metadata for stable diffusion. &lt;a href="https://huggingface.co/spaces/Nekochu/Luminia-13B_SD_Prompt"&gt;[demo]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/BUAADreamer/Chinese-LLaVA-Med"&gt;Chinese-LLaVA-Med&lt;/a&gt;&lt;/strong&gt;: A multimodal large language model specialized in Chinese medical domain, based on LLaVA-1.5-7B.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/THUDM/AutoRE"&gt;AutoRE&lt;/a&gt;&lt;/strong&gt;: A document-level relation extraction system based on large language models.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/NVIDIA/RTX-AI-Toolkit"&gt;NVIDIA RTX AI Toolkit&lt;/a&gt;&lt;/strong&gt;: SDKs for fine-tuning LLMs on Windows PC for NVIDIA RTX.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/LazyAGI/LazyLLM"&gt;LazyLLM&lt;/a&gt;&lt;/strong&gt;: An easy and lazy way for building multi-agent LLMs applications and supports model fine-tuning via LLaMA Factory.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/NLPJCL/RAG-Retrieval"&gt;RAG-Retrieval&lt;/a&gt;&lt;/strong&gt;: A full pipeline for RAG retrieval model fine-tuning, inference, and distillation. &lt;a href="https://zhuanlan.zhihu.com/p/987727357"&gt;[blog]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/Qihoo360/360-LLaMA-Factory"&gt;360-LLaMA-Factory&lt;/a&gt;&lt;/strong&gt;: A modified library that supports long sequence SFT &amp;amp; DPO using ring attention.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://novasky-ai.github.io/posts/sky-t1/"&gt;Sky-T1&lt;/a&gt;&lt;/strong&gt;: An o1-like model fine-tuned by NovaSky AI with very small cost.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/xming521/WeClone"&gt;WeClone&lt;/a&gt;&lt;/strong&gt;: One-stop solution for creating your digital avatar from chat logs.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/SmartFlowAI/EmoLLM"&gt;EmoLLM&lt;/a&gt;&lt;/strong&gt;: A project about large language models (LLMs) and mental health.&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This repository is licensed under the &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE"&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Please follow the model licenses to use the corresponding model weights: &lt;a href="https://huggingface.co/baichuan-inc/Baichuan2-7B-Base/blob/main/Community%20License%20for%20Baichuan%202%20Model.pdf"&gt;Baichuan 2&lt;/a&gt; / &lt;a href="https://huggingface.co/spaces/bigscience/license"&gt;BLOOM&lt;/a&gt; / &lt;a href="https://github.com/THUDM/ChatGLM3/raw/main/MODEL_LICENSE"&gt;ChatGLM3&lt;/a&gt; / &lt;a href="https://cohere.com/c4ai-cc-by-nc-license"&gt;Command R&lt;/a&gt; / &lt;a href="https://github.com/deepseek-ai/DeepSeek-LLM/raw/main/LICENSE-MODEL"&gt;DeepSeek&lt;/a&gt; / &lt;a href="https://huggingface.co/tiiuae/falcon-180B/blob/main/LICENSE.txt"&gt;Falcon&lt;/a&gt; / &lt;a href="https://ai.google.dev/gemma/terms"&gt;Gemma&lt;/a&gt; / &lt;a href="https://huggingface.co/THUDM/glm-4-9b/blob/main/LICENSE"&gt;GLM-4&lt;/a&gt; / &lt;a href="https://github.com/openai/gpt-2/raw/master/LICENSE"&gt;GPT-2&lt;/a&gt; / &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE"&gt;Granite&lt;/a&gt; / &lt;a href="https://huggingface.co/IndexTeam/Index-1.9B/blob/main/LICENSE"&gt;Index&lt;/a&gt; / &lt;a href="https://github.com/InternLM/InternLM#license"&gt;InternLM&lt;/a&gt; / &lt;a href="https://github.com/facebookresearch/llama/raw/main/MODEL_CARD.md"&gt;Llama&lt;/a&gt; / &lt;a href="https://ai.meta.com/llama/license/"&gt;Llama 2&lt;/a&gt; / &lt;a href="https://llama.meta.com/llama3/license/"&gt;Llama 3&lt;/a&gt; / &lt;a href="https://github.com/meta-llama/llama-models/raw/main/models/llama4/LICENSE"&gt;Llama 4&lt;/a&gt; / &lt;a href="https://github.com/OpenBMB/MiniCPM/raw/main/MiniCPM%20Model%20License.md"&gt;MiniCPM&lt;/a&gt; / &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE"&gt;Mistral/Mixtral/Pixtral&lt;/a&gt; / &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE"&gt;OLMo&lt;/a&gt; / &lt;a href="https://huggingface.co/microsoft/phi-1_5/resolve/main/Research%20License.docx"&gt;Phi-1.5/Phi-2&lt;/a&gt; / &lt;a href="https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE"&gt;Phi-3/Phi-4&lt;/a&gt; / &lt;a href="https://github.com/QwenLM/Qwen/raw/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT"&gt;Qwen&lt;/a&gt; / &lt;a href="https://huggingface.co/Skywork/Skywork-13B-base/blob/main/Skywork%20Community%20License.pdf"&gt;Skywork&lt;/a&gt; / &lt;a href="https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement"&gt;StarCoder 2&lt;/a&gt; / &lt;a href="https://huggingface.co/Tele-AI/telechat-7B/blob/main/TeleChat%E6%A8%A1%E5%9E%8B%E7%A4%BE%E5%8C%BA%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf"&gt;TeleChat2&lt;/a&gt; / &lt;a href="https://github.com/xverse-ai/XVERSE-13B/raw/main/MODEL_LICENSE.pdf"&gt;XVERSE&lt;/a&gt; / &lt;a href="https://huggingface.co/01-ai/Yi-6B/blob/main/LICENSE"&gt;Yi&lt;/a&gt; / &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE"&gt;Yi-1.5&lt;/a&gt; / &lt;a href="https://github.com/IEIT-Yuan/Yuan-2.0/raw/main/LICENSE-Yuan"&gt;Yuan 2&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If this work is helpful, please kindly cite as:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@inproceedings{zheng2024llamafactory,
  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},
  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},
  address={Bangkok, Thailand},
  publisher={Association for Computational Linguistics},
  year={2024},
  url={http://arxiv.org/abs/2403.13372}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgement&lt;/h2&gt; 
&lt;p&gt;This repo benefits from &lt;a href="https://github.com/huggingface/peft"&gt;PEFT&lt;/a&gt;, &lt;a href="https://github.com/huggingface/trl"&gt;TRL&lt;/a&gt;, &lt;a href="https://github.com/artidoro/qlora"&gt;QLoRA&lt;/a&gt; and &lt;a href="https://github.com/lm-sys/FastChat"&gt;FastChat&lt;/a&gt;. Thanks for their wonderful works.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://api.star-history.com/svg?repos=hiyouga/LLaMA-Factory&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>NVIDIA/physicsnemo</title>
      <link>https://github.com/NVIDIA/physicsnemo</link>
      <description>&lt;p&gt;Open-source deep-learning framework for building, training, and fine-tuning deep learning models using state-of-the-art Physics-ML methods&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;NVIDIA PhysicsNeMo&lt;/h1&gt; 
&lt;!-- markdownlint-disable --&gt; 
&lt;p&gt;ğŸ“ NVIDIA Modulus has been renamed to NVIDIA PhysicsNeMo&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.repostatus.org/#active"&gt;&lt;img src="https://www.repostatus.org/badges/latest/active.svg?sanitize=true" alt="Project Status: Active - The project has reached a stable, usable state and is being actively developed." /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/physicsnemo/raw/master/LICENSE.txt"&gt;&lt;img src="https://img.shields.io/github/license/NVIDIA/physicsnemo" alt="GitHub" /&gt;&lt;/a&gt; &lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true" alt="Code style: black" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- markdownlint-enable --&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#what-is-physicsnemo"&gt;&lt;strong&gt;NVIDIA PhysicsNeMo&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/index.html"&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#installation"&gt;&lt;strong&gt;Install Guide&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#getting-started"&gt;&lt;strong&gt;Getting Started&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#contributing-to-physicsnemo"&gt;&lt;strong&gt;Contributing Guidelines&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#license"&gt;&lt;strong&gt;License&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What is PhysicsNeMo?&lt;/h2&gt; 
&lt;p&gt;NVIDIA PhysicsNeMo is an open-source deep-learning framework for building, training, fine-tuning, and inferring Physics AI models using state-of-the-art SciML methods for AI4Science and engineering.&lt;/p&gt; 
&lt;p&gt;PhysicsNeMo provides Python modules to compose scalable and optimized training and inference pipelines to explore, develop, validate, and deploy AI models that combine physics knowledge with data, enabling real-time predictions.&lt;/p&gt; 
&lt;p&gt;Whether you are exploring the use of neural operators, GNNs, or transformers, or are interested in Physics-Informed Neural Networks or a hybrid approach in between, PhysicsNeMo provides you with an optimized stack that will enable you to train your models at scale.&lt;/p&gt; 
&lt;!-- markdownlint-disable --&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/docs/img/value_prop/Knowledge_guided_models.gif" alt="PhysicsNeMo" /&gt; &lt;/p&gt; 
&lt;!-- markdownlint-enable --&gt; 
&lt;!-- toc --&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#more-about-physicsnemo"&gt;More About PhysicsNeMo&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#scalable-gpu-optimized-training-library"&gt;Scalable GPU-Optimized Training Library&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#a-suite-of-physics-informed-ml-models"&gt;A Suite of Physics-Informed ML Models&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#seamless-pytorch-integration"&gt;Seamless PyTorch Integration&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#easy-customization-and-extension"&gt;Easy Customization and Extension&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#ai4science-library"&gt;AI4Science Library&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#domain-specific-packages"&gt;Domain-Specific Packages&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#who-is-using-and-contributing-to-physicsnemo"&gt;Who is Using and Contributing to PhysicsNeMo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#why-are-they-using-physicsnemo"&gt;Why Use PhysicsNeMo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#getting-started"&gt;Getting Started&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#resources"&gt;Resources&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#contributing-to-physicsnemo"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#communication"&gt;Communication&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- tocstop --&gt; 
&lt;h2&gt;More About PhysicsNeMo&lt;/h2&gt; 
&lt;p&gt;At a granular level, PhysicsNeMo is developed as modular functionality and therefore provides built-in composable modules that are packaged into a few key components:&lt;/p&gt; 
&lt;!-- markdownlint-disable --&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Component&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.models.html"&gt;&lt;strong&gt;physicsnemo.models&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A collection of optimized, customizable, and easy-to-use families of model architectures such as Neural Operators, Graph Neural Networks, Diffusion models, Transformer models, and many more&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.datapipes.html"&gt;&lt;strong&gt;physicsnemo.datapipes&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Optimized and scalable built-in data pipelines fine-tuned to handle engineering and scientific data structures like point clouds, meshes, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.distributed.html"&gt;&lt;strong&gt;physicsnemo.distributed&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A distributed computing sub-module built on top of &lt;code&gt;torch.distributed&lt;/code&gt; to enable parallel training with just a few steps&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/physicsnemo-curator"&gt;&lt;strong&gt;physicsnemo.curator&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A sub-module to streamline and accelerate the process of data curation for engineering datasets&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-sym/user_guide/features/csg_and_tessellated_module.html"&gt;&lt;strong&gt;physicsnemo.sym.geometry&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A sub-module to handle geometry for DL training using Constructive Solid Geometry modeling and CAD files in STL format&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-sym/user_guide/features/nodes.html"&gt;&lt;strong&gt;physicsnemo.sym.eq&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A sub-module to use PDEs in your DL training with several implementations of commonly observed equations and easy ways for customization&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!-- markdownlint-enable --&gt; 
&lt;p&gt;For a complete list, refer to the PhysicsNeMo API documentation for &lt;a href="https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/index.html"&gt;PhysicsNeMo&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;AI4Science Library&lt;/h2&gt; 
&lt;p&gt;Usually, PhysicsNeMo is used either as:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A complementary tool to PyTorch when exploring AI for SciML and AI4Science applications.&lt;/li&gt; 
 &lt;li&gt;A deep learning research platform that provides scale and optimal performance on NVIDIA GPUs.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Domain-Specific Packages&lt;/h3&gt; 
&lt;p&gt;The following are packages dedicated to domain experts of specific communities, catering to their unique exploration needs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/NVIDIA/physicsnemo-cfd"&gt;PhysicsNeMo CFD&lt;/a&gt;: Inference sub-module of PhysicsNeMo to enable CFD domain experts to explore, experiment, and validate using pretrained AI models for CFD use cases.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/NVIDIA/physicsnemo-curator"&gt;PhysicsNeMo Curator&lt;/a&gt;: Inference sub-module of PhysicsNeMo to streamline and accelerate the process of data curation for engineering datasets.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/NVIDIA/earth2studio"&gt;Earth-2 Studio&lt;/a&gt;: Inference sub-module of PhysicsNeMo to enable climate researchers and scientists to explore and experiment with pretrained AI models for weather and climate.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Scalable GPU-Optimized Training Library&lt;/h3&gt; 
&lt;p&gt;PhysicsNeMo provides a highly optimized and scalable training library for maximizing the power of NVIDIA GPUs. &lt;a href="https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.distributed.html"&gt;Distributed computing&lt;/a&gt; utilities allow for efficient scaling from a single GPU to multi-node GPU clusters with a few lines of code, ensuring that large-scale physics-informed machine learning (ML) models can be trained quickly and effectively. The framework includes support for advanced &lt;a href="https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.utils.html#module-physicsnemo.utils.capture"&gt;optimization utilities&lt;/a&gt;, &lt;a href="https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.datapipes.html"&gt;tailor-made datapipes&lt;/a&gt;, and &lt;a href="https://github.com/NVIDIA/physicsnemo-sym/tree/main/physicsnemo/sym/eq"&gt;validation utilities&lt;/a&gt; to enhance end-to-end training speed.&lt;/p&gt; 
&lt;h3&gt;A Suite of Physics-Informed ML Models&lt;/h3&gt; 
&lt;p&gt;PhysicsNeMo offers a library of state-of-the-art models specifically designed for Physics-ML applications. Users can build any model architecture by using the underlying PyTorch layers and combining them with curated PhysicsNeMo layers.&lt;/p&gt; 
&lt;p&gt;The &lt;a href="https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.models.html#model-zoo"&gt;Model Zoo&lt;/a&gt; includes optimized implementations of families of model architectures such as Neural Operators:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/physicsnemo/models/fno"&gt;Fourier Neural Operators (FNOs)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-sym/user_guide/neural_operators/deeponet.html"&gt;DeepONet&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/examples/cfd/external_aerodynamics/domino/readme.html"&gt;DoMINO&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/physicsnemo/models/gnn_layers"&gt;Graph Neural Networks (GNNs)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/examples/cfd/vortex_shedding_mgn/readme.html"&gt;MeshGraphNet&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/examples/cfd/lagrangian_mgn/readme.html"&gt;MeshGraphNet for Lagrangian&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/examples/cfd/external_aerodynamics/xaeronet/readme.html"&gt;XAeroNet&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/physicsnemo/models/diffusion"&gt;Diffusion Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/examples/generative/corrdiff/readme.html"&gt;Correction Diffusion Model&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/examples/generative/diffusion/readme.html"&gt;DDPM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/examples/weather/graphcast/readme.html"&gt;PhysicsNeMo GraphCast&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/NVIDIA/physicsnemo/tree/main/examples/cfd/darcy_transolver"&gt;Transsolver&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/NVIDIA/physicsnemo/tree/main/physicsnemo/models"&gt;RNNs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/NVIDIA/physicsnemo/tree/main/physicsnemo/models/swinvrnn"&gt;SwinVRNN&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-sym/user_guide/foundational/1d_wave_equation.html"&gt;Physics-Informed Neural Networks (PINNs)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;And many others.&lt;/p&gt; 
&lt;p&gt;These models are optimized for various physics domains, such as computational fluid dynamics, structural mechanics, and electromagnetics. Users can download, customize, and build upon these models to suit their specific needs, significantly reducing the time required to develop high-fidelity simulations.&lt;/p&gt; 
&lt;h3&gt;Seamless PyTorch Integration&lt;/h3&gt; 
&lt;p&gt;PhysicsNeMo is built on top of PyTorch, providing a familiar and user-friendly experience for those already proficient with PyTorch. This includes a simple Python interface and modular design, making it easy to use PhysicsNeMo with existing PyTorch workflows. Users can leverage the extensive PyTorch ecosystem, including its libraries and tools, while benefiting from PhysicsNeMo's specialized capabilities for physics-ML. This seamless integration ensures users can quickly adopt PhysicsNeMo without a steep learning curve.&lt;/p&gt; 
&lt;p&gt;For more information, refer to &lt;a href="https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.models.html#converting-pytorch-models-to-physicsnemo-models"&gt;Converting PyTorch Models to PhysicsNeMo Models&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Easy Customization and Extension&lt;/h3&gt; 
&lt;p&gt;PhysicsNeMo is designed to be highly extensible, allowing users to add new functionality with minimal effort. The framework provides Pythonic APIs for defining new physics models, geometries, and constraints, making it easy to extend its capabilities to new use cases. The adaptability of PhysicsNeMo is further enhanced by key features such as &lt;a href="https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.deploy.html"&gt;ONNX support&lt;/a&gt; for flexible model deployment, robust &lt;a href="https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.launch.logging.html"&gt;logging utilities&lt;/a&gt; for streamlined error handling, and efficient &lt;a href="https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.launch.utils.html#module-physicsnemo.launch.utils.checkpoint"&gt;checkpointing&lt;/a&gt; to simplify model loading and saving.&lt;/p&gt; 
&lt;p&gt;This extensibility ensures that PhysicsNeMo can adapt to the evolving needs of researchers and engineers, facilitating the development of innovative solutions in the field of physics-ML.&lt;/p&gt; 
&lt;p&gt;Detailed information on features and capabilities can be found in the &lt;a href="https://docs.nvidia.com/physicsnemo/index.html#core"&gt;PhysicsNeMo documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/examples/README.md"&gt;Reference samples&lt;/a&gt; cover a broad spectrum of physics-constrained and data-driven workflows to suit the diversity of use cases in the science and engineering disciplines.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Have questions about how PhysicsNeMo can assist you? Try our [Experimental] chatbot, &lt;a href="https://chatgpt.com/g/g-PXrBv20SC-modulus-guide"&gt;PhysicsNeMo Guide&lt;/a&gt;, for answers.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Hello World&lt;/h3&gt; 
&lt;p&gt;You can start using PhysicsNeMo in your PyTorch code as simply as shown here:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;&amp;gt;&amp;gt;&amp;gt; import torch
&amp;gt;&amp;gt;&amp;gt; from physicsnemo.models.mlp.fully_connected import FullyConnected
&amp;gt;&amp;gt;&amp;gt; model = FullyConnected(in_features=32, out_features=64)
&amp;gt;&amp;gt;&amp;gt; input = torch.randn(128, 32)
&amp;gt;&amp;gt;&amp;gt; output = model(input)
&amp;gt;&amp;gt;&amp;gt; output.shape
torch.Size([128, 64])
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use the distributed module, you can do the following (example for distributed data parallel training; for a more in-depth tutorial, refer to &lt;a href="https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.distributed.html#"&gt;PhysicsNeMo Distributed&lt;/a&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import torch
from torch.nn.parallel import DistributedDataParallel
from physicsnemo.distributed import DistributedManager
from physicsnemo.models.mlp.fully_connected import FullyConnected

def main():
    DistributedManager.initialize()
    dist = DistributedManager()

    arch = FullyConnected(in_features=32, out_features=64).to(dist.device)

    if dist.distributed:
        ddps = torch.cuda.Stream()
        with torch.cuda.stream(ddps):
            arch = DistributedDataParallel(
                arch,
                device_ids=[dist.local_rank],
                output_device=dist.device,
                broadcast_buffers=dist.broadcast_buffers,
                find_unused_parameters=dist.find_unused_parameters,
            )
        torch.cuda.current_stream().wait_stream(ddps)

    # Set up the optimizer
    optimizer = torch.optim.Adam(
        arch.parameters(),
        lr=0.001,
    )

    def training_step(invar, target):
        pred = arch(invar)
        loss = torch.sum(torch.pow(pred - target, 2))
        loss.backward()
        optimizer.step()
        return loss

    # Sample training loop
    for i in range(20):
        # Random inputs and targets for simplicity
        input = torch.randn(128, 32, device=dist.device)
        target = torch.randn(128, 64, device=dist.device)

        # Training step
        loss = training_step(input, target)

if __name__ == "__main__":
    main()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use the PDE module, you can do the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;&amp;gt;&amp;gt;&amp;gt; from physicsnemo.sym.eq.pdes.navier_stokes import NavierStokes
&amp;gt;&amp;gt;&amp;gt; ns = NavierStokes(nu=0.01, rho=1, dim=2)
&amp;gt;&amp;gt;&amp;gt; ns.pprint()
continuity: u__x + v__y
momentum_x: u*u__x + v*u__y + p__x + u__t - 0.01*u__x__x - 0.01*u__y__y
momentum_y: u*v__x + v*v__y + p__y + v__t - 0.01*v__x__x - 0.01*v__y__y
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Who is Using and Contributing to PhysicsNeMo&lt;/h2&gt; 
&lt;p&gt;PhysicsNeMo is an open-source project and gets contributions from researchers in the SciML and AI4Science fields. While the PhysicsNeMo team works on optimizing the underlying software stack, the community collaborates and contributes model architectures, datasets, and reference applications so we can innovate in the pursuit of developing generalizable model architectures and algorithms.&lt;/p&gt; 
&lt;p&gt;Some recent examples of community contributors are the &lt;a href="https://developer.nvidia.com/blog/spotlight-hp-3d-printing-and-nvidia-physicsnemo-collaborate-on-open-source-manufacturing-digital-twin/"&gt;HP Labs 3D Printing team&lt;/a&gt;, &lt;a href="https://developer.nvidia.com/blog/enabling-greater-patient-specific-cardiovascular-care-with-ai-surrogates/"&gt;Stanford Cardiovascular research team&lt;/a&gt;, &lt;a href="https://github.com/NVIDIA/physicsnemo/tree/main/examples/cfd/mhd_pino"&gt;UIUC team&lt;/a&gt;, &lt;a href="https://github.com/NVIDIA/physicsnemo/tree/main/examples/generative/diffusion"&gt;CMU team&lt;/a&gt;, etc.&lt;/p&gt; 
&lt;p&gt;Recent examples of research teams using PhysicsNeMo are the &lt;a href="https://arxiv.org/abs/2404.05768"&gt;ORNL team&lt;/a&gt;, &lt;a href="https://www.nvidia.com/en-us/on-demand/session/gtc24-s62237/"&gt;TU Munich CFD team&lt;/a&gt;, etc.&lt;/p&gt; 
&lt;p&gt;Please navigate to this page for a complete list of research work leveraging PhysicsNeMo. For a list of enterprises using PhysicsNeMo, refer to the &lt;a href="https://developer.nvidia.com/physicsnemo"&gt;PhysicsNeMo Webpage&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Using PhysicsNeMo and interested in showcasing your work on &lt;a href="https://developer.nvidia.com/blog/category/simulation-modeling-design/"&gt;NVIDIA Blogs&lt;/a&gt;? Fill out this &lt;a href="https://forms.gle/XsBdWp3ji67yZAUF7"&gt;proposal form&lt;/a&gt; and we will get back to you!&lt;/p&gt; 
&lt;h2&gt;Why Are They Using PhysicsNeMo&lt;/h2&gt; 
&lt;p&gt;Here are some of the key benefits of PhysicsNeMo for SciML model development:&lt;/p&gt; 
&lt;!-- markdownlint-disable --&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;img src="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/docs/img/value_prop/benchmarking.svg?sanitize=true" width="100" /&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;img src="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/docs/img/value_prop/recipe.svg?sanitize=true" width="100" /&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;img src="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/docs/img/value_prop/performance.svg?sanitize=true" width="100" /&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SciML Benchmarking and Validation&lt;/td&gt; 
   &lt;td&gt;Ease of Using Generalized SciML Recipes with Heterogeneous Datasets&lt;/td&gt; 
   &lt;td&gt;Out-of-the-Box Performance and Scalability&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PhysicsNeMo enables researchers to benchmark their AI models against proven architectures for standard benchmark problems with detailed domain-specific validation criteria.&lt;/td&gt; 
   &lt;td&gt;PhysicsNeMo enables researchers to pick from state-of-the-art SciML architectures and use built-in data pipelines for their use case.&lt;/td&gt; 
   &lt;td&gt;PhysicsNeMo provides out-of-the-box performant training pipelines, including optimized ETL pipelines for heterogeneous engineering and scientific datasets and out-of-the-box scaling across multi-GPU and multi-node GPUs.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!-- markdownlint-enable --&gt; 
&lt;p&gt;See what your peer SciML researchers are saying about PhysicsNeMo (coming soon).&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;The following resources will help you learn how to use PhysicsNeMo. The best way is to start with a reference sample and then update it for your own use case.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/tutorials/simple_training_example.html#using-custom-models-in-physicsnemo"&gt;Using PhysicsNeMo with your PyTorch model&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/tutorials/simple_training_example.html#using-built-in-models"&gt;Using PhysicsNeMo built-in models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.nvidia.com/deeplearning/physicsnemo/getting-started/index.html"&gt;Getting Started Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/NVIDIA/physicsnemo/raw/main/examples/README.md"&gt;Reference Samples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/index.html"&gt;User Guide Documentation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.nvidia.com/en-us/on-demand/session/gtc24-dlit61460/?playlistId=playList-bd07f4dc-1397-4783-a959-65cec79aa985"&gt;Getting Started Webinar&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/openhackathons-org/End-to-End-AI-for-Science"&gt;AI4Science PhysicsNeMo Bootcamp&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://catalog.ngc.nvidia.com/models?filters=&amp;amp;orderBy=scoreDESC&amp;amp;query=PhysicsNeMo&amp;amp;page=&amp;amp;pageSize="&gt;PhysicsNeMo Pretrained Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://catalog.ngc.nvidia.com/resources?filters=&amp;amp;orderBy=scoreDESC&amp;amp;query=PhysicsNeMo&amp;amp;page=&amp;amp;pageSize="&gt;PhysicsNeMo Datasets and Supplementary Materials&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+S-OV-04+V1"&gt;Self-Paced PhysicsNeMo DLI Training&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.nvidia.com/en-us/on-demand/deep-learning-for-science-and-engineering/"&gt;Deep Learning for Science and Engineering Lecture Series with PhysicsNeMo&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://www.nvidia.com/en-us/on-demand/session/dliteachingkit-setk5002/"&gt;PhysicsNeMo: Purpose and Usage&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.nvidia.com/en-us/on-demand/search/?facet.mimetype%5B%5D=event%20session&amp;amp;layout=list&amp;amp;page=1&amp;amp;q=physicsnemo&amp;amp;sort=relevance&amp;amp;sortDir=desc"&gt;Video Tutorials&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;The following instructions help you install the base PhysicsNeMo modules to get started. There are additional optional dependencies for specific models that are listed under &lt;a href="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#optional-dependencies"&gt;optional dependencies&lt;/a&gt;. The training recipes are not packaged into the pip wheels or the container to keep the footprint low. We recommend users clone the appropriate training recipes and use them as a starting point. These training recipes may require additional example-specific dependencies, as indicated through their associated &lt;code&gt;requirements.txt&lt;/code&gt; file.&lt;/p&gt; 
&lt;h3&gt;PyPI&lt;/h3&gt; 
&lt;p&gt;The recommended method for installing the latest version of PhysicsNeMo is using PyPI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-Bash"&gt;pip install nvidia-physicsnemo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The installation can be verified by running the &lt;a href="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#hello-world"&gt;Hello World&lt;/a&gt; example.&lt;/p&gt; 
&lt;h4&gt;Optional Dependencies&lt;/h4&gt; 
&lt;p&gt;PhysicsNeMo has many optional dependencies that are used in specific components. When using pip, all dependencies used in PhysicsNeMo can be installed with &lt;code&gt;pip install nvidia-physicsnemo[all]&lt;/code&gt;. If you are developing PhysicsNeMo, developer dependencies can be installed using &lt;code&gt;pip install nvidia-physicsnemo[dev]&lt;/code&gt;. Otherwise, additional dependencies can be installed on a case-by-case basis. Detailed information on installing the optional dependencies can be found in the &lt;a href="https://docs.nvidia.com/deeplearning/physicsnemo/getting-started/index.html"&gt;Getting Started Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;NVCR Container&lt;/h3&gt; 
&lt;p&gt;The recommended PhysicsNeMo Docker image can be pulled from the &lt;a href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/physicsnemo/containers/physicsnemo"&gt;NVIDIA Container Registry&lt;/a&gt; (refer to the NGC registry for the latest tag):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-Bash"&gt;docker pull nvcr.io/nvidia/physicsnemo/physicsnemo:25.06
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Inside the container, you can clone the PhysicsNeMo git repositories and get started with the examples. The command below shows the instructions to launch the PhysicsNeMo container and run examples from this repo:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 --runtime nvidia \
--rm -it nvcr.io/nvidia/physicsnemo/physicsnemo:25.06 bash
git clone https://github.com/NVIDIA/physicsnemo.git
cd physicsnemo/examples/cfd/darcy_fno/
pip install warp-lang # install NVIDIA Warp to run the Darcy example
python train_fno_darcy.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;From Source&lt;/h2&gt; 
&lt;h3&gt;Package&lt;/h3&gt; 
&lt;p&gt;For a local build of the PhysicsNeMo Python package from source, use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-Bash"&gt;git clone git@github.com:NVIDIA/physicsnemo.git &amp;amp;&amp;amp; cd physicsnemo

pip install --upgrade pip
pip install .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Source Container&lt;/h3&gt; 
&lt;p&gt;To build the PhysicsNeMo Docker image:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -t physicsnemo:deploy \
    --build-arg TARGETPLATFORM=linux/amd64 --target deploy -f Dockerfile .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can run &lt;code&gt;make container-deploy&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;To build the CI image:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -t physicsnemo:ci \
    --build-arg TARGETPLATFORM=linux/amd64 --target ci -f Dockerfile .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can run &lt;code&gt;make container-ci&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Currently, only &lt;code&gt;linux/amd64&lt;/code&gt; and &lt;code&gt;linux/arm64&lt;/code&gt; platforms are supported. If using &lt;code&gt;linux/arm64&lt;/code&gt;, some dependencies like &lt;code&gt;warp-lang&lt;/code&gt; might not install correctly.&lt;/p&gt; 
&lt;h2&gt;PhysicsNeMo Migration Guide&lt;/h2&gt; 
&lt;p&gt;NVIDIA Modulus has been renamed to NVIDIA PhysicsNeMo. For migration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use &lt;code&gt;pip install nvidia-physicsnemo&lt;/code&gt; rather than &lt;code&gt;pip install nvidia-modulus&lt;/code&gt; for PyPI wheels.&lt;/li&gt; 
 &lt;li&gt;Use &lt;code&gt;nvcr.io/nvidia/physicsnemo/physicsnemo:&amp;lt;tag&amp;gt;&lt;/code&gt; rather than &lt;code&gt;nvcr.io/nvidia/modulus/modulus:&amp;lt;tag&amp;gt;&lt;/code&gt; for Docker containers.&lt;/li&gt; 
 &lt;li&gt;Replace &lt;code&gt;nvidia-modulus&lt;/code&gt; with &lt;code&gt;nvidia-physicsnemo&lt;/code&gt; in your pip requirements files (&lt;code&gt;requirements.txt&lt;/code&gt;, &lt;code&gt;setup.py&lt;/code&gt;, &lt;code&gt;setup.cfg&lt;/code&gt;, &lt;code&gt;pyproject.toml&lt;/code&gt;, etc.).&lt;/li&gt; 
 &lt;li&gt;In your code, change the import statements from &lt;code&gt;import modulus&lt;/code&gt; to &lt;code&gt;import physicsnemo&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The old PyPI registry and the NGC container registry will be deprecated soon and will not receive any bug fixes/updates. The old checkpoints will remain compatible with these updates.&lt;/p&gt; 
&lt;p&gt;More details to follow soon.&lt;/p&gt; 
&lt;h2&gt;DGL to PyTorch Geometric Migration Guide&lt;/h2&gt; 
&lt;p&gt;PhysicsNeMo supports a wide range of Graph Neural Networks (GNNs), including MeshGraphNet and others. Currently, PhysicsNeMo uses the DGL library as its GNN backend, with plans to completely transition to PyTorch Geometric (PyG) in a future release. For more details, please refer to the &lt;a href="https://github.com/NVIDIA/physicsnemo/raw/main/examples/dgl_to_pyg_migration.md"&gt;DGL-to-PyG migration guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing to PhysicsNeMo&lt;/h2&gt; 
&lt;p&gt;PhysicsNeMo is an open-source collaboration, and its success is rooted in community contributions to further the field of Physics-ML. Thank you for contributing to the project so others can build on top of your contributions.&lt;/p&gt; 
&lt;p&gt;For guidance on contributing to PhysicsNeMo, please refer to the &lt;a href="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Cite PhysicsNeMo&lt;/h2&gt; 
&lt;p&gt;If PhysicsNeMo helped your research and you would like to cite it, please refer to the &lt;a href="https://github.com/NVIDIA/physicsnemo/raw/main/CITATION.cff"&gt;guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Communication&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;GitHub Discussions: Discuss new architectures, implementations, Physics-ML research, etc.&lt;/li&gt; 
 &lt;li&gt;GitHub Issues: Bug reports, feature requests, install issues, etc.&lt;/li&gt; 
 &lt;li&gt;PhysicsNeMo Forum: The &lt;a href="https://forums.developer.nvidia.com/t/welcome-to-the-physicsnemo-ml-model-framework-forum/178556"&gt;PhysicsNeMo Forum&lt;/a&gt; hosts an audience of new to moderate-level users and developers for general chat, online discussions, collaboration, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Feedback&lt;/h2&gt; 
&lt;p&gt;Want to suggest some improvements to PhysicsNeMo? Use our &lt;a href="https://docs.google.com/forms/d/e/1FAIpQLSfX4zZ0Lp7MMxzi3xqvzX4IQDdWbkNh5H_a_clzIhclE2oSBQ/viewform?usp=sf_link"&gt;feedback form&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;PhysicsNeMo is provided under the Apache License 2.0. Please see &lt;a href="https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/LICENSE.txt"&gt;LICENSE.txt&lt;/a&gt; for the full license text. Enterprise SLA, support, and preview access are available under NVAIE.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>wandb/wandb</title>
      <link>https://github.com/wandb/wandb</link>
      <description>&lt;p&gt;The AI developer platform. Use Weights &amp; Biases to train and fine-tune models, and manage models from experimentation to production.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/wandb/wandb/main/assets/logo-dark.svg#gh-dark-mode-only" width="600" alt="Weights &amp;amp; Biases" /&gt; &lt;img src="https://raw.githubusercontent.com/wandb/wandb/main/assets/logo-light.svg#gh-light-mode-only" width="600" alt="Weights &amp;amp; Biases" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://pypi.python.org/pypi/wandb"&gt;&lt;img src="https://img.shields.io/pypi/v/wandb" /&gt;&lt;/a&gt; &lt;a href="https://anaconda.org/conda-forge/wandb"&gt;&lt;img src="https://img.shields.io/conda/vn/conda-forge/wandb" /&gt;&lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/wandb"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/wandb" /&gt;&lt;/a&gt; &lt;a href="https://circleci.com/gh/wandb/wandb"&gt;&lt;img src="https://img.shields.io/circleci/build/github/wandb/wandb/main" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/wandb/wandb"&gt;&lt;img src="https://img.shields.io/codecov/c/gh/wandb/wandb" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/intro/Intro_to_Weights_%26_Biases.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;Use W&amp;amp;B to build better models faster. Track and visualize all the pieces of your machine learning pipeline, from datasets to production machine learning models. Get started with W&amp;amp;B today, &lt;a href="https://wandb.com?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=readme"&gt;sign up for a W&amp;amp;B account!&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;Building an LLM app? Track, debug, evaluate, and monitor LLM apps with &lt;a href="https://wandb.github.io/weave?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=readme"&gt;Weave&lt;/a&gt;, our new suite of tools for GenAI.&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h1&gt;Documentation&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;a target="_blank" href="https://docs.wandb.ai/guides/track?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=readme"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="./assets/Product_Icons_dark_background/experiments-dark.svg" width="12.5%" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="./assets/Product_Icons_light/experiments-light.svg" width="12.5%" /&gt; 
   &lt;img alt="Weights and Biases Experiments" src="" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;a target="_blank" href="https://docs.wandb.ai/guides/reports?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=readme"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="./assets/Product_Icons_dark_background/reports-dark.svg" width="12.5%" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="./assets/Product_Icons_light/reports-light.svg" width="12.5%" /&gt; 
   &lt;img alt="Weights and Biases Reports" src="" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;a target="_blank" href="https://docs.wandb.ai/guides/artifacts?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=readme"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="./assets/Product_Icons_dark_background/artifacts-dark.svg" width="12.5%" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="./assets/Product_Icons_light/artifacts-light.svg" width="12.5%" /&gt; 
   &lt;img alt="Weights and Biases Artifacts" src="" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;a target="_blank" href="https://docs.wandb.ai/guides/tables?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=readme"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="./assets/Product_Icons_dark_background/tables-dark.svg" width="12.5%" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="./assets/Product_Icons_light/tables-light.svg" width="12.5%" /&gt; 
   &lt;img alt="Weights and Biases Tables" src="" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;a target="_blank" href="https://docs.wandb.ai/guides/sweeps?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=readme"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="./assets/Product_Icons_dark_background/sweeps-dark.svg" width="12.5%" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="./assets/Product_Icons_light/sweeps-light.svg" width="12.5%" /&gt; 
   &lt;img alt="Weights and Biases Sweeps" src="" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;a target="_blank" href="https://docs.wandb.ai/guides/model_registry?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=readme"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="./assets/Product_Icons_dark_background/model-registry-dark.svg" width="12.5%" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="./assets/Product_Icons_light/model-registry-light.svg" width="12.5%" /&gt; 
   &lt;img alt="Weights and Biases Model Management" src="" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;a target="_blank" href="https://docs.wandb.ai/guides/artifacts/project-scoped-automations?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=readme"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="./assets/Product_Icons_dark_background/automations-dark.svg" width="12.5%" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="./assets/Product_Icons_light/automations-light.svg" width="12.5%" /&gt; 
   &lt;img alt="Weights and Biases Prompts" src="" /&gt; 
  &lt;/picture&gt; &lt;/a&gt;&lt;/p&gt;
&lt;a target="_blank" href="https://docs.wandb.ai/guides/artifacts/project-scoped-automations?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=readme"&gt; &lt;/a&gt;
&lt;p&gt;&lt;a target="_blank" href="https://docs.wandb.ai/guides/artifacts/project-scoped-automations?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=readme"&gt;See the &lt;/a&gt;&lt;a href="https://docs.wandb.ai/?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=documentation"&gt;W&amp;amp;B Developer Guide&lt;/a&gt; and &lt;a href="https://docs.wandb.ai/ref?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=documentation"&gt;API Reference Guide&lt;/a&gt; for a full technical description of the W&amp;amp;B platform.&lt;/p&gt; 
&lt;h1&gt;Quickstart&lt;/h1&gt; 
&lt;p&gt;Get started with W&amp;amp;B in four steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;First, sign up for a &lt;a href="https://wandb.ai/login?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=quickstart"&gt;W&amp;amp;B account&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Second, install&amp;nbsp;the W&amp;amp;B SDK with &lt;a href="https://pip.pypa.io/en/stable/"&gt;pip&lt;/a&gt;. Navigate to your terminal and type the following command:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install wandb
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Third, log into W&amp;amp;B:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;wandb.login()
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Use the example code snippet below as a template to integrate W&amp;amp;B to your Python script:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import wandb

# Start a W&amp;amp;B Run with wandb.init
run = wandb.init(project="my_first_project")

# Save model inputs and hyperparameters in a wandb.config object
config = run.config
config.learning_rate = 0.01

# Model training code here ...

# Log metrics over time to visualize performance with wandb.log
for i in range(10):
    run.log({"loss": ...})

# Mark the run as finished, and finish uploading all data
run.finish()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That's it! Navigate to the W&amp;amp;B App to view a dashboard of your first W&amp;amp;B Experiment. Use the W&amp;amp;B App to compare multiple experiments in a unified place, dive into the results of a single run, and much more!&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h1&gt;Integrations&lt;/h1&gt; 
&lt;p&gt;Use your favorite framework with W&amp;amp;B. W&amp;amp;B integrations make it fast and easy to set up experiment tracking and data versioning inside existing projects. For more information on how to integrate W&amp;amp;B with the framework of your choice, see the &lt;a href="https://docs.wandb.ai/guides/integrations"&gt;Integrations chapter&lt;/a&gt; in the W&amp;amp;B Developer Guide.&lt;/p&gt; 
&lt;!-- &lt;p align='center'&gt;
&lt;img src="./assets/integrations.png" width="100%" /&gt;
&lt;/p&gt; --&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ”¥ PyTorch&lt;/summary&gt; 
 &lt;p&gt;Call &lt;code&gt;.watch&lt;/code&gt; and pass in your PyTorch model to automatically log gradients and store the network topology. Next, use &lt;code&gt;.log&lt;/code&gt; to track other metrics. The following example demonstrates an example of how to do this:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import wandb

# 1. Start a new run
run = wandb.init(project="gpt4")

# 2. Save model inputs and hyperparameters
config = run.config
config.dropout = 0.01

# 3. Log gradients and model parameters
run.watch(model)
for batch_idx, (data, target) in enumerate(train_loader):
    ...
    if batch_idx % args.log_interval == 0:
        # 4. Log metrics to visualize performance
        run.log({"loss": loss})
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Run an example &lt;a href="http://wandb.me/pytorch-colab"&gt;Google Colab Notebook&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;Read the &lt;a href="https://docs.wandb.com/guides/integrations/pytorch?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=integrations"&gt;Developer Guide&lt;/a&gt; for technical details on how to integrate PyTorch with W&amp;amp;B.&lt;/li&gt; 
  &lt;li&gt;Explore &lt;a href="https://app.wandb.ai/wandb/getting-started/reports/Pytorch--VmlldzoyMTEwNzM?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=integrations"&gt;W&amp;amp;B Reports&lt;/a&gt;.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸŒŠ TensorFlow/Keras&lt;/summary&gt; Use W&amp;amp;B Callbacks to automatically save metrics to W&amp;amp;B when you call `model.fit` during training. 
 &lt;p&gt;The following code example demonstrates how your script might look like when you integrate W&amp;amp;B with Keras:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# This script needs these libraries to be installed:
#   tensorflow, numpy

import wandb
from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint

import random
import numpy as np
import tensorflow as tf


# Start a run, tracking hyperparameters
run = wandb.init(
    # set the wandb project where this run will be logged
    project="my-awesome-project",
    # track hyperparameters and run metadata with wandb.config
    config={
        "layer_1": 512,
        "activation_1": "relu",
        "dropout": random.uniform(0.01, 0.80),
        "layer_2": 10,
        "activation_2": "softmax",
        "optimizer": "sgd",
        "loss": "sparse_categorical_crossentropy",
        "metric": "accuracy",
        "epoch": 8,
        "batch_size": 256,
    },
)

# [optional] use wandb.config as your config
config = run.config

# get the data
mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
x_train, y_train = x_train[::5], y_train[::5]
x_test, y_test = x_test[::20], y_test[::20]
labels = [str(digit) for digit in range(np.max(y_train) + 1)]

# build a model
model = tf.keras.models.Sequential(
    [
        tf.keras.layers.Flatten(input_shape=(28, 28)),
        tf.keras.layers.Dense(config.layer_1, activation=config.activation_1),
        tf.keras.layers.Dropout(config.dropout),
        tf.keras.layers.Dense(config.layer_2, activation=config.activation_2),
    ]
)

# compile the model
model.compile(optimizer=config.optimizer, loss=config.loss, metrics=[config.metric])

# WandbMetricsLogger will log train and validation metrics to wandb
# WandbModelCheckpoint will upload model checkpoints to wandb
history = model.fit(
    x=x_train,
    y=y_train,
    epochs=config.epoch,
    batch_size=config.batch_size,
    validation_data=(x_test, y_test),
    callbacks=[
        WandbMetricsLogger(log_freq=5),
        WandbModelCheckpoint("models"),
    ],
)

# [optional] finish the wandb run, necessary in notebooks
run.finish()
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Get started integrating your Keras model with W&amp;amp;B today:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Run an example &lt;a href="https://wandb.me/intro-keras?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=integrations"&gt;Google Colab Notebook&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Read the &lt;a href="https://docs.wandb.com/guides/integrations/keras?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=integrations"&gt;Developer Guide&lt;/a&gt; for technical details on how to integrate Keras with W&amp;amp;B.&lt;/li&gt; 
  &lt;li&gt;Explore &lt;a href="https://app.wandb.ai/wandb/getting-started/reports/Keras--VmlldzoyMTEwNjQ?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=integrations"&gt;W&amp;amp;B Reports&lt;/a&gt;.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ¤— Hugging Face Transformers&lt;/summary&gt; 
 &lt;p&gt;Pass &lt;code&gt;wandb&lt;/code&gt; to the &lt;code&gt;report_to&lt;/code&gt; argument when you run a script using a Hugging Face Trainer. W&amp;amp;B will automatically log losses, evaluation metrics, model topology, and gradients.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The environment you run your script in must have &lt;code&gt;wandb&lt;/code&gt; installed.&lt;/p&gt; 
 &lt;p&gt;The following example demonstrates how to integrate W&amp;amp;B with Hugging Face:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# This script needs these libraries to be installed:
#   numpy, transformers, datasets

import wandb

import os
import numpy as np
from datasets import load_dataset
from transformers import TrainingArguments, Trainer
from transformers import AutoTokenizer, AutoModelForSequenceClassification


def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return {"accuracy": np.mean(predictions == labels)}


# download prepare the data
dataset = load_dataset("yelp_review_full")
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

small_train_dataset = dataset["train"].shuffle(seed=42).select(range(1000))
small_eval_dataset = dataset["test"].shuffle(seed=42).select(range(300))

small_train_dataset = small_train_dataset.map(tokenize_function, batched=True)
small_eval_dataset = small_train_dataset.map(tokenize_function, batched=True)

# download the model
model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased", num_labels=5
)

# set the wandb project where this run will be logged
os.environ["WANDB_PROJECT"] = "my-awesome-project"

# save your trained model checkpoint to wandb
os.environ["WANDB_LOG_MODEL"] = "true"

# turn off watch to log faster
os.environ["WANDB_WATCH"] = "false"

# pass "wandb" to the `report_to` parameter to turn on wandb logging
training_args = TrainingArguments(
    output_dir="models",
    report_to="wandb",
    logging_steps=5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    evaluation_strategy="steps",
    eval_steps=20,
    max_steps=100,
    save_steps=100,
)

# define the trainer and start training
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)
trainer.train()

# [optional] finish the wandb run, necessary in notebooks
wandb.finish()
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Run an example &lt;a href="http://wandb.me/hf?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=integrations"&gt;Google Colab Notebook&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;Read the &lt;a href="https://docs.wandb.com/guides/integrations/huggingface?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=integrations"&gt;Developer Guide&lt;/a&gt; for technical details on how to integrate Hugging Face with W&amp;amp;B.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;âš¡ï¸ PyTorch Lightning&lt;/summary&gt; 
 &lt;p&gt;Build scalable, structured, high-performance PyTorch models with Lightning and log them with W&amp;amp;B.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# This script needs these libraries to be installed:
#   torch, torchvision, pytorch_lightning

import wandb

import os
from torch import optim, nn, utils
from torchvision.datasets import MNIST
from torchvision.transforms import ToTensor

import pytorch_lightning as pl
from pytorch_lightning.loggers import WandbLogger


class LitAutoEncoder(pl.LightningModule):
    def __init__(self, lr=1e-3, inp_size=28, optimizer="Adam"):
        super().__init__()

        self.encoder = nn.Sequential(
            nn.Linear(inp_size * inp_size, 64), nn.ReLU(), nn.Linear(64, 3)
        )
        self.decoder = nn.Sequential(
            nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, inp_size * inp_size)
        )
        self.lr = lr

        # save hyperparameters to self.hparamsm auto-logged by wandb
        self.save_hyperparameters()

    def training_step(self, batch, batch_idx):
        x, y = batch
        x = x.view(x.size(0), -1)
        z = self.encoder(x)
        x_hat = self.decoder(z)
        loss = nn.functional.mse_loss(x_hat, x)

        # log metrics to wandb
        self.log("train_loss", loss)
        return loss

    def configure_optimizers(self):
        optimizer = optim.Adam(self.parameters(), lr=self.lr)
        return optimizer


# init the autoencoder
autoencoder = LitAutoEncoder(lr=1e-3, inp_size=28)

# setup data
batch_size = 32
dataset = MNIST(os.getcwd(), download=True, transform=ToTensor())
train_loader = utils.data.DataLoader(dataset, shuffle=True)

# initialise the wandb logger and name your wandb project
wandb_logger = WandbLogger(project="my-awesome-project")

# add your batch size to the wandb config
wandb_logger.experiment.config["batch_size"] = batch_size

# pass wandb_logger to the Trainer
trainer = pl.Trainer(limit_train_batches=750, max_epochs=5, logger=wandb_logger)

# train the model
trainer.fit(model=autoencoder, train_dataloaders=train_loader)

# [optional] finish the wandb run, necessary in notebooks
wandb.finish()
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Run an example &lt;a href="http://wandb.me/lightning?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=integrations"&gt;Google Colab Notebook&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;Read the &lt;a href="https://docs.wandb.ai/guides/integrations/lightning?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=integrations"&gt;Developer Guide&lt;/a&gt; for technical details on how to integrate PyTorch Lightning with W&amp;amp;B.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ’¨ XGBoost&lt;/summary&gt; Use W&amp;amp;B Callbacks to automatically save metrics to W&amp;amp;B when you call `model.fit` during training. 
 &lt;p&gt;The following code example demonstrates how your script might look like when you integrate W&amp;amp;B with XGBoost:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# This script needs these libraries to be installed:
#   numpy, xgboost

import wandb
from wandb.xgboost import WandbCallback

import numpy as np
import xgboost as xgb


# setup parameters for xgboost
param = {
    "objective": "multi:softmax",
    "eta": 0.1,
    "max_depth": 6,
    "nthread": 4,
    "num_class": 6,
}

# start a new wandb run to track this script
run = wandb.init(
    # set the wandb project where this run will be logged
    project="my-awesome-project",
    # track hyperparameters and run metadata
    config=param,
)

# download data from wandb Artifacts and prep data
run.use_artifact("wandb/intro/dermatology_data:v0", type="dataset").download(".")
data = np.loadtxt(
    "./dermatology.data",
    delimiter=",",
    converters={33: lambda x: int(x == "?"), 34: lambda x: int(x) - 1},
)
sz = data.shape

train = data[: int(sz[0] * 0.7), :]
test = data[int(sz[0] * 0.7) :, :]

train_X = train[:, :33]
train_Y = train[:, 34]

test_X = test[:, :33]
test_Y = test[:, 34]

xg_train = xgb.DMatrix(train_X, label=train_Y)
xg_test = xgb.DMatrix(test_X, label=test_Y)
watchlist = [(xg_train, "train"), (xg_test, "test")]

# add another config to the wandb run
num_round = 5
run.config["num_round"] = 5
run.config["data_shape"] = sz

# pass WandbCallback to the booster to log its configs and metrics
bst = xgb.train(
    param, xg_train, num_round, evals=watchlist, callbacks=[WandbCallback()]
)

# get prediction
pred = bst.predict(xg_test)
error_rate = np.sum(pred != test_Y) / test_Y.shape[0]

# log your test metric to wandb
run.summary["Error Rate"] = error_rate

# [optional] finish the wandb run, necessary in notebooks
run.finish()
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Run an example &lt;a href="https://wandb.me/xgboost?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=integrations"&gt;Google Colab Notebook&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;Read the &lt;a href="https://docs.wandb.ai/guides/integrations/xgboost?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=integrations"&gt;Developer Guide&lt;/a&gt; for technical details on how to integrate XGBoost with W&amp;amp;B.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ§® Sci-Kit Learn&lt;/summary&gt; Use wandb to visualize and compare your scikit-learn models' performance: 
 &lt;pre&gt;&lt;code class="language-python"&gt;# This script needs these libraries to be installed:
#   numpy, sklearn

import wandb
from wandb.sklearn import plot_precision_recall, plot_feature_importances
from wandb.sklearn import plot_class_proportions, plot_learning_curve, plot_roc

import numpy as np
from sklearn import datasets
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split


# load and process data
wbcd = datasets.load_breast_cancer()
feature_names = wbcd.feature_names
labels = wbcd.target_names

test_size = 0.2
X_train, X_test, y_train, y_test = train_test_split(
    wbcd.data, wbcd.target, test_size=test_size
)

# train model
model = RandomForestClassifier()
model.fit(X_train, y_train)
model_params = model.get_params()

# get predictions
y_pred = model.predict(X_test)
y_probas = model.predict_proba(X_test)
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]

# start a new wandb run and add your model hyperparameters
run = wandb.init(project="my-awesome-project", config=model_params)

# Add additional configs to wandb
run.config.update(
    {
        "test_size": test_size,
        "train_len": len(X_train),
        "test_len": len(X_test),
    }
)

# log additional visualisations to wandb
plot_class_proportions(y_train, y_test, labels)
plot_learning_curve(model, X_train, y_train)
plot_roc(y_test, y_probas, labels)
plot_precision_recall(y_test, y_probas, labels)
plot_feature_importances(model)

# [optional] finish the wandb run, necessary in notebooks
run.finish()
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Run an example &lt;a href="https://wandb.me/scikit-colab?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=integrations"&gt;Google Colab Notebook&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;Read the &lt;a href="https://docs.wandb.ai/guides/integrations/scikit?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=integrations"&gt;Developer Guide&lt;/a&gt; for technical details on how to integrate Scikit-Learn with W&amp;amp;B.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h1&gt;W&amp;amp;B Hosting Options&lt;/h1&gt; 
&lt;p&gt;Weights &amp;amp; Biases is available in the cloud or installed on your private infrastructure. Set up a W&amp;amp;B Server in a production environment in one of three ways:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://docs.wandb.ai/platform/hosting/hosting-options/multi_tenant_cloud?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=hosting"&gt;Multi-tenant Cloud&lt;/a&gt;: Fully managed platform deployed in W&amp;amp;Bâ€™s Google Cloud Platform (GCP) account in GCPâ€™s North America regions.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.wandb.ai/platform/hosting/hosting-options/dedicated_cloud?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=hosting"&gt;Dedicated Cloud&lt;/a&gt;: Single-tenant, fully managed platform deployed in W&amp;amp;Bâ€™s AWS, GCP, or Azure cloud accounts. Each Dedicated Cloud instance has its own isolated network, compute and storage from other W&amp;amp;B Dedicated Cloud instances.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.wandb.ai/platform/hosting/hosting-options/self-managed?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=hosting"&gt;Self-Managed&lt;/a&gt;: Deploy W&amp;amp;B Server on your AWS, GCP, or Azure cloud account or within your on-premises infrastructure.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;See the &lt;a href="https://docs.wandb.ai/guides/hosting?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=hosting"&gt;Hosting documentation&lt;/a&gt; in the W&amp;amp;B Developer Guide for more information.&lt;/p&gt; 
&lt;!-- &amp;nbsp;
    &lt;br /&gt;
# Tutorials

Explore example Colab Notebooks at [wandb/examples GitHub repository](https://github.com/wandb/examples/tree/master/colabs). Here are some of our favorites:

[INSERT] --&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h1&gt;Python Version Support&lt;/h1&gt; 
&lt;p&gt;We are committed to supporting our minimum required Python version for &lt;em&gt;at least&lt;/em&gt; six months after its official end-of-life (EOL) date, as defined by the Python Software Foundation. You can find a list of Python EOL dates &lt;a href="https://devguide.python.org/versions/"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When we discontinue support for a Python version, we will increment the libraryâ€™s minor version number to reflect this change.&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h1&gt;Contribution guidelines&lt;/h1&gt; 
&lt;p&gt;Weights &amp;amp; Biases â¤ï¸ open source, and we welcome contributions from the community! See the &lt;a href="https://github.com/wandb/wandb/raw/main/CONTRIBUTING.md"&gt;Contribution guide&lt;/a&gt; for more information on the development workflow and the internals of the wandb library. For wandb bugs and feature requests, visit &lt;a href="https://github.com/wandb/wandb/issues"&gt;GitHub Issues&lt;/a&gt; or contact &lt;a href="mailto:support@wandb.com"&gt;support@wandb.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h1&gt;W&amp;amp;B Community&lt;/h1&gt; 
&lt;p&gt;Be a part of the growing W&amp;amp;B Community and interact with the W&amp;amp;B team in our &lt;a href="https://wandb.me/discord"&gt;Discord&lt;/a&gt;. Stay connected with the latest ML updates and tutorials with &lt;a href="https://wandb.ai/fully-connected"&gt;W&amp;amp;B Fully Connected&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/wandb/wandb/raw/main/LICENSE"&gt;MIT License&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google-agentic-commerce/AP2</title>
      <link>https://github.com/google-agentic-commerce/AP2</link>
      <description>&lt;p&gt;Building a Secure and Interoperable Future for AI-Driven Payments.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Agent Payments Protocol (AP2)&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/google-agentic-commerce/AP2/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true" alt="Apache License" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/google-agentic-commerce/AP2"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- markdownlint-disable MD041 --&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/google-agentic-commerce/AP2/main/docs/assets/ap2_graphic.png" alt="Agent Payments Protocol Graphic" /&gt; &lt;/p&gt; 
&lt;p&gt;This repository contains code samples and demos of the Agent Payments Protocol.&lt;/p&gt; 
&lt;h2&gt;Intro to AP2 Video&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://goo.gle/ap2-video"&gt;&lt;img src="https://img.youtube.com/vi/yLTp3ic2j5c/hqdefault.jpg" alt="A2A Intro Video" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;AP2 on The Agent Factory&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://youtu.be/T1MtWnEYXM0?si=QkJWnAiav0JAP9F6"&gt;&lt;img src="https://img.youtube.com/vi/T1MtWnEYXM0/hqdefault.jpg" alt="The Agent Factory - Episode 8: Agent payments, can you do my shopping?" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;About the Samples&lt;/h2&gt; 
&lt;p&gt;These samples use &lt;a href="https://google.github.io/adk-docs/"&gt;Agent Development Kit (ADK)&lt;/a&gt; and Gemini 2.5 Flash.&lt;/p&gt; 
&lt;p&gt;The Agent Payments Protocol doesn't require the use of either. While these were used in the samples, you're free to use any tools you prefer to build your agents.&lt;/p&gt; 
&lt;h2&gt;Navigating the Repository&lt;/h2&gt; 
&lt;p&gt;The &lt;strong&gt;&lt;code&gt;samples&lt;/code&gt;&lt;/strong&gt; directory contains a collection of curated scenarios meant to demonstrate the key components of the Agent Payments Protocol.&lt;/p&gt; 
&lt;p&gt;The scenarios can be found in the &lt;a href="https://raw.githubusercontent.com/google-agentic-commerce/AP2/main/samples/android/scenarios"&gt;&lt;strong&gt;&lt;code&gt;samples/android/scenarios&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/google-agentic-commerce/AP2/main/samples/python/scenarios"&gt;&lt;strong&gt;&lt;code&gt;samples/python/scenarios&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; directories.&lt;/p&gt; 
&lt;p&gt;Each scenario contains:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;a &lt;code&gt;README.md&lt;/code&gt; file describing the scenario and instructions for running it.&lt;/li&gt; 
 &lt;li&gt;a &lt;code&gt;run.sh&lt;/code&gt; script to simplify the process of running the scenario locally.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This demonstration features various agents and servers, with most source code located in &lt;a href="https://raw.githubusercontent.com/google-agentic-commerce/AP2/main/samples/python/src/"&gt;&lt;strong&gt;&lt;code&gt;samples/python/src&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;. Scenarios that use an Android app as the shopping assistant have their source code in &lt;a href="https://raw.githubusercontent.com/google-agentic-commerce/AP2/main/samples/android/"&gt;&lt;strong&gt;&lt;code&gt;samples/android&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10 or higher&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;&lt;code&gt;uv&lt;/code&gt;&lt;/a&gt; package manager&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Setup&lt;/h3&gt; 
&lt;p&gt;You can authenticate using either a Google API Key or Vertex AI.&lt;/p&gt; 
&lt;p&gt;For either method, you can set the required credentials as environment variables in your shell or place them in a &lt;code&gt;.env&lt;/code&gt; file at the root of your project.&lt;/p&gt; 
&lt;h4&gt;Option 1: Google API Key (Recommended for development)&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Obtain a Google API key from &lt;a href="http://aistudio.google.com/apikey"&gt;Google AI Studio&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Set the &lt;code&gt;GOOGLE_API_KEY&lt;/code&gt; environment variable.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;As an environment variable:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;export GOOGLE_API_KEY='your_key'
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;In a &lt;code&gt;.env&lt;/code&gt; file:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;GOOGLE_API_KEY='your_key'
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Option 2: &lt;a href="https://cloud.google.com/vertex-ai"&gt;Vertex AI&lt;/a&gt; (Recommended for production)&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configure your environment to use Vertex AI.&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;As environment variables:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;export GOOGLE_GENAI_USE_VERTEXAI=true
export GOOGLE_CLOUD_PROJECT='your-project-id'
export GOOGLE_CLOUD_LOCATION='global' # or your preferred region
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;In a &lt;code&gt;.env&lt;/code&gt; file:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;GOOGLE_GENAI_USE_VERTEXAI=true
GOOGLE_CLOUD_PROJECT='your-project-id'
GOOGLE_CLOUD_LOCATION='global'
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Authenticate your application.&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Using the &lt;a href="https://cloud.google.com/sdk/docs/install"&gt;&lt;code&gt;gcloud&lt;/code&gt; CLI&lt;/a&gt;:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;gcloud auth application-default login
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Using a Service Account:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;export GOOGLE_APPLICATION_CREDENTIALS='/path/to/your/service-account-key.json'
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;How to Run a Scenario&lt;/h3&gt; 
&lt;p&gt;To run a specific scenario, follow the instructions in its &lt;code&gt;README.md&lt;/code&gt;. It will generally follow this pattern:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Navigate to the root of the repository.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;cd AP2
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the run script to install dependencies &amp;amp; start the agents.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;bash samples/python/scenarios/your-scenario-name/run.sh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Navigate to the Shopping Agent URL and begin engaging.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Installing the AP2 Types Package&lt;/h3&gt; 
&lt;p&gt;The protocol's core objects are defined in the &lt;a href="https://raw.githubusercontent.com/google-agentic-commerce/AP2/main/src/ap2/types"&gt;&lt;code&gt;src/ap2/types&lt;/code&gt;&lt;/a&gt; directory. A PyPI package will be published at a later time. Until then, you can install the types package directly using this command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;uv pip install git+https://github.com/google-agentic-commerce/AP2.git@main
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>squidfunk/mkdocs-material</title>
      <link>https://github.com/squidfunk/mkdocs-material</link>
      <description>&lt;p&gt;Documentation that simply works&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://squidfunk.github.io/mkdocs-material/"&gt; &lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/logo.svg?sanitize=true" width="320" alt="Material for MkDocs" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;strong&gt; A powerful documentation framework on top of &lt;a href="https://www.mkdocs.org/"&gt;MkDocs&lt;/a&gt; &lt;/strong&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/squidfunk/mkdocs-material/actions"&gt;&lt;img src="https://github.com/squidfunk/mkdocs-material/workflows/build/badge.svg?sanitize=true" alt="Build" /&gt;&lt;/a&gt; &lt;a href="https://pypistats.org/packages/mkdocs-material"&gt;&lt;img src="https://img.shields.io/pypi/dm/mkdocs-material.svg?sanitize=true" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/mkdocs-material"&gt;&lt;img src="https://img.shields.io/pypi/v/mkdocs-material.svg?sanitize=true" alt="Python Package Index" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/squidfunk/mkdocs-material/"&gt;&lt;img src="https://img.shields.io/docker/pulls/squidfunk/mkdocs-material" alt="Docker Pulls" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sponsors/squidfunk"&gt;&lt;img src="https://img.shields.io/github/sponsors/squidfunk" alt="Sponsors" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; Write your documentation in Markdown and create a professional static site for your Open Source or commercial project in minutes â€“ searchable, customizable, more than 60 languages, for all devices. &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://squidfunk.github.io/mkdocs-material/getting-started/"&gt; &lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/screenshot.png" width="700" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;em&gt; Check out the demo â€“ &lt;a href="https://squidfunk.github.io/mkdocs-material/"&gt;squidfunk.github.io/mkdocs-material&lt;/a&gt;. &lt;/em&gt; &lt;/p&gt; 
&lt;h2&gt;&lt;/h2&gt; 
&lt;p id="premium-sponsors"&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p align="center"&gt;&lt;strong&gt;Silver sponsors&lt;/strong&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://fastapi.tiangolo.com/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-fastapi.png" height="120" /&gt;&lt;/a&gt; &lt;a href="https://www.trendpop.com/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-trendpop.png" height="120" /&gt;&lt;/a&gt; &lt;a href="https://documentation.sailpoint.com/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-sailpoint.png" height="120" /&gt;&lt;/a&gt; &lt;a href="https://futureplc.com/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-future.svg?sanitize=true" width="332" height="120" /&gt;&lt;/a&gt; &lt;a href="https://opensource.siemens.com/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-siemens.png" height="120" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p align="center"&gt;&lt;strong&gt;Bronze sponsors&lt;/strong&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://cirrus-ci.org/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-cirrus-ci.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://docs.baslerweb.com/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-basler.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://kx.com/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-kx.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://orion-docs.prefect.io/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-prefect.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://www.zenoss.com/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-zenoss.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://docs.posit.co" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-posit.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://n8n.io" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-n8n.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://www.dogado.de" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-dogado.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://wwt.com" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-wwt.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://elastic.co" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-elastic.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://ipfabric.io/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-ip-fabric.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://www.apex.ai/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-apex-ai.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://jitterbit.com/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-jitterbit.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://sparkfun.com/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-sparkfun.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://eccenca.com/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-eccenca.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://neptune.ai/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-neptune-ai.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://rackn.com/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-rackn.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://civicactions.com/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-civic-actions.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://getscreen.me/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-getscreenme.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://botcity.dev/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-botcity.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://kolena.io/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-kolena.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://www.evergiving.com/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-evergiving.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://astral.sh/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-astral.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://oikolab.com/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-oikolab.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://www.buhlergroup.com/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-buhler.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://3dr.com/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-3dr.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://spotware.com/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-spotware.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://milfordasset.com/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-milford.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://www.lechler.com/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-lechler.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://invers.com/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-invers.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://maxar.com/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-maxar.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://www.equipmentshare.com/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-equipmentshare.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://hummingbot.org/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-hummingbot.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://octoperf.com/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-octoperf.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://intercomestibles.ch/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-intercomestibles.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://www.centara.com/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-centara.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://pydantic.dev/logfire/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-logfire.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://www.vector.com/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-vector.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://second.tech/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-second.png" height="58" /&gt;&lt;/a&gt; &lt;a href="https://mvtec.com/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-mvtec.png" height="58" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;Everything you would expect&lt;/h2&gt; 
&lt;h3&gt;It's just Markdown&lt;/h3&gt; 
&lt;p&gt;Focus on the content of your documentation and create a professional static site in minutes. No need to know HTML, CSS or JavaScript â€“ let Material for MkDocs do the heavy lifting for you.&lt;/p&gt; 
&lt;h3&gt;Works on all devices&lt;/h3&gt; 
&lt;p&gt;Serve your documentation with confidence â€“ Material for MkDocs automatically adapts to perfectly fit the available screen estate, no matter the type or size of the viewing device. Desktop. Tablet. Mobile. All great.&lt;/p&gt; 
&lt;h3&gt;Made to measure&lt;/h3&gt; 
&lt;p&gt;Make it yours â€“ change the colors, fonts, language, icons, logo, and more with a few lines of configuration. Material for MkDocs can be easily extended and provides many options to alter appearance and behavior.&lt;/p&gt; 
&lt;h3&gt;Fast and lightweight&lt;/h3&gt; 
&lt;p&gt;Don't let your users wait â€“ get incredible value with a small footprint by using one of the fastest themes available with excellent performance, yielding optimal search engine rankings and happy users that return.&lt;/p&gt; 
&lt;h3&gt;Maintain ownership&lt;/h3&gt; 
&lt;p&gt;Own your documentation's complete sources and outputs, guaranteeing both integrity and security â€“ no need to entrust the backbone of your product knowledge to third-party platforms. Retain full control.&lt;/p&gt; 
&lt;h3&gt;Open Source&lt;/h3&gt; 
&lt;p&gt;You're in good company â€“ choose a mature and actively maintained solution built with state-of-the-art Open Source technologies, trusted by more than 50,000 individuals and organizations. Licensed under MIT.&lt;/p&gt; 
&lt;h2&gt;Quick start&lt;/h2&gt; 
&lt;p&gt;Material for MkDocs can be installed with &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install mkdocs-material
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Add the following lines to &lt;code&gt;mkdocs.yml&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;theme:
  name: material
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For detailed installation instructions, configuration options, and a demo, visit &lt;a href="https://squidfunk.github.io/mkdocs-material/"&gt;squidfunk.github.io/mkdocs-material&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Trusted by ...&lt;/h2&gt; 
&lt;h3&gt;... industry leaders&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://info.arxiv.org"&gt;ArXiv&lt;/a&gt;, &lt;a href="https://atlassian.github.io/data-center-helm-charts/"&gt;Atlassian&lt;/a&gt;, &lt;a href="https://aws.github.io/copilot-cli/"&gt;AWS&lt;/a&gt;, &lt;a href="https://bloomberg.github.io/selekt/"&gt;Bloomberg&lt;/a&gt;, &lt;a href="http://abpcomputing.web.cern.ch/"&gt;CERN&lt;/a&gt;, &lt;a href="https://datadoghq.dev/integrations-core/"&gt;Datadog&lt;/a&gt;, &lt;a href="https://google.github.io/accompanist/"&gt;Google&lt;/a&gt;, &lt;a href="https://informatics.fas.harvard.edu/"&gt;Harvard&lt;/a&gt;, &lt;a href="https://hewlettpackard.github.io/squest/"&gt;Hewlett Packard&lt;/a&gt;, &lt;a href="https://hsbc.github.io/pyratings/"&gt;HSBC&lt;/a&gt;, &lt;a href="https://ing-bank.github.io/baker/"&gt;ING&lt;/a&gt;, &lt;a href="https://open-amt-cloud-toolkit.github.io/docs/"&gt;Intel&lt;/a&gt;, &lt;a href="https://jetbrains.github.io/projector-client/mkdocs/"&gt;JetBrains&lt;/a&gt;, &lt;a href="https://linkedin.github.io/school-of-sre/"&gt;LinkedIn&lt;/a&gt;, &lt;a href="https://microsoft.github.io/code-with-engineering-playbook/"&gt;Microsoft&lt;/a&gt;, &lt;a href="https://mozillafoundation.github.io/engineering-handbook/"&gt;Mozilla&lt;/a&gt;, &lt;a href="https://netflix.github.io/titus/"&gt;Netflix&lt;/a&gt;, &lt;a href="https://openai.github.io/openai-agents-python/"&gt;OpenAI&lt;/a&gt;, &lt;a href="https://ansible.readthedocs.io/projects/lint/"&gt;Red Hat&lt;/a&gt;, &lt;a href="https://inference.roboflow.com/"&gt;Roboflow&lt;/a&gt;, &lt;a href="https://policy-sentry.readthedocs.io/"&gt;Salesforce&lt;/a&gt;, &lt;a href="https://opensource.siemens.com/"&gt;SIEMENS&lt;/a&gt;, &lt;a href="https://slackhq.github.io/circuit/"&gt;Slack&lt;/a&gt;, &lt;a href="https://square.github.io/okhttp/"&gt;Square&lt;/a&gt;, &lt;a href="https://uber-go.github.io/fx/"&gt;Uber&lt;/a&gt;, &lt;a href="https://opensource.zalando.com/skipper/"&gt;Zalando&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;... and successful Open Source projects&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://amp.rs/docs/"&gt;Amp&lt;/a&gt;, &lt;a href="https://iceberg.apache.org/"&gt;Apache Iceberg&lt;/a&gt;, &lt;a href="https://arduino.github.io/arduino-cli/"&gt;Arduino&lt;/a&gt;, &lt;a href="https://asahilinux.org/docs/"&gt;Asahi Linux&lt;/a&gt;, &lt;a href="https://docs.agpt.co/"&gt;Auto-GPT&lt;/a&gt;, &lt;a href="https://autokeras.com/"&gt;AutoKeras&lt;/a&gt;, &lt;a href="https://www.bfe-networks.net/"&gt;BFE&lt;/a&gt;, &lt;a href="https://docs.infra.centos.org/"&gt;CentOS&lt;/a&gt;, &lt;a href="https://crystal-lang.org/reference/"&gt;Crystal&lt;/a&gt;, &lt;a href="https://ebpf-go.dev/"&gt;eBPF&lt;/a&gt;, &lt;a href="https://docs.ejabberd.im/"&gt;ejabberd&lt;/a&gt;, &lt;a href="https://www.electron.build/"&gt;Electron&lt;/a&gt;, &lt;a href="https://fastapi.tiangolo.com/"&gt;FastAPI&lt;/a&gt;, &lt;a href="https://flatbuffers.dev/"&gt;FlatBuffers&lt;/a&gt;, &lt;a href="https://fmt.dev/"&gt;{fmt}&lt;/a&gt;, &lt;a href="https://www.freqtrade.io/en/stable/"&gt;Freqtrade&lt;/a&gt;, &lt;a href="https://goreleaser.com/"&gt;GoReleaser&lt;/a&gt;, &lt;a href="https://microsoft.github.io/graphrag/"&gt;GraphRAG&lt;/a&gt;, &lt;a href="https://headscale.net/"&gt;Headscale&lt;/a&gt;, &lt;a href="https://docs.hedgedoc.org/"&gt;HedgeDoc&lt;/a&gt;, &lt;a href="https://hummingbot.org/"&gt;Hummingbot&lt;/a&gt;, &lt;a href="https://knative.dev/docs/"&gt;Knative&lt;/a&gt;, &lt;a href="https://kops.sigs.k8s.io/"&gt;Kubernetes&lt;/a&gt;, &lt;a href="https://docs.ksqldb.io/"&gt;kSQL&lt;/a&gt;, &lt;a href="https://square.github.io/leakcanary/"&gt;LeakCanary&lt;/a&gt;, &lt;a href="https://docs.llamaindex.ai/"&gt;LlamaIndex&lt;/a&gt;, &lt;a href="https://netboxlabs.com/docs/netbox/en/stable/"&gt;NetBox&lt;/a&gt;, &lt;a href="https://nokogiri.org/"&gt;Nokogiri&lt;/a&gt;, &lt;a href="https://openai.github.io/openai-agents-python/"&gt;OpenAI&lt;/a&gt;, &lt;a href="https://docs.openfaas.com/"&gt;OpenFaaS&lt;/a&gt;, &lt;a href="https://docs.openssl.org/"&gt;OpenSSL&lt;/a&gt;, &lt;a href="https://docs.orchardcore.net/en/latest/"&gt;Orchard Core&lt;/a&gt;, &lt;a href="https://docs.percona.com/percona-monitoring-and-management/"&gt;Percona&lt;/a&gt;, &lt;a href="https://docs.pi-hole.net/"&gt;Pi-Hole&lt;/a&gt;, &lt;a href="https://docs.pola.rs/"&gt;Polars&lt;/a&gt;, &lt;a href="https://pydantic-docs.helpmanual.io/"&gt;Pydantic&lt;/a&gt;, &lt;a href="https://docs.pypi.org/"&gt;PyPI&lt;/a&gt;, &lt;a href="https://core.quivr.com/"&gt;Quivr&lt;/a&gt;, &lt;a href="https://docs.renovatebot.com/"&gt;Renovate&lt;/a&gt;, &lt;a href="https://retropie.org.uk/docs/"&gt;RetroPie&lt;/a&gt;, &lt;a href="https://docs.astral.sh/ruff/"&gt;Ruff&lt;/a&gt;, &lt;a href="https://supervision.roboflow.com/latest/"&gt;Supervision&lt;/a&gt;, &lt;a href="https://textual.textualize.io/"&gt;Textual&lt;/a&gt;, &lt;a href="https://docs.traefik.io/"&gt;Traefik&lt;/a&gt;, &lt;a href="https://aquasecurity.github.io/trivy/"&gt;Trivy&lt;/a&gt;, &lt;a href="https://typer.tiangolo.com/"&gt;Typer&lt;/a&gt;, &lt;a href="https://docs.tinygrad.org/"&gt;tinygrad&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/"&gt;Ultralytics&lt;/a&gt;, &lt;a href="https://docs.astral.sh/uv/"&gt;UV&lt;/a&gt;, &lt;a href="https://docs.vapor.codes/"&gt;Vapor&lt;/a&gt;, &lt;a href="https://docs.webkit.org/"&gt;WebKit&lt;/a&gt;, &lt;a href="https://wtfutil.com/"&gt;WTF&lt;/a&gt;, &lt;a href="https://zeronet.io/docs/"&gt;ZeroNet&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;MIT License&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Copyright (c) 2016-2025 Martin Donath&lt;/p&gt; 
&lt;p&gt;Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:&lt;/p&gt; 
&lt;p&gt;The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.&lt;/p&gt; 
&lt;p&gt;THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>