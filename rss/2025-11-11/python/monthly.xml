<rss version="2.0">
  <channel>
    <title>GitHub Python Monthly Trending</title>
    <description>Monthly Trending of Python in GitHub</description>
    <pubDate>Mon, 10 Nov 2025 01:54:10 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>karpathy/nanoGPT</title>
      <link>https://github.com/karpathy/nanoGPT</link>
      <description>&lt;p&gt;The simplest, fastest repository for training/finetuning medium-sized GPTs.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;nanoGPT&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/karpathy/nanoGPT/master/assets/nanogpt.jpg" alt="nanoGPT" /&gt;&lt;/p&gt; 
&lt;p&gt;The simplest, fastest repository for training/finetuning medium-sized GPTs. It is a rewrite of &lt;a href="https://github.com/karpathy/minGPT"&gt;minGPT&lt;/a&gt; that prioritizes teeth over education. Still under active development, but currently the file &lt;code&gt;train.py&lt;/code&gt; reproduces GPT-2 (124M) on OpenWebText, running on a single 8XA100 40GB node in about 4 days of training. The code itself is plain and readable: &lt;code&gt;train.py&lt;/code&gt; is a ~300-line boilerplate training loop and &lt;code&gt;model.py&lt;/code&gt; a ~300-line GPT model definition, which can optionally load the GPT-2 weights from OpenAI. That's it.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/karpathy/nanoGPT/master/assets/gpt2_124M_loss.png" alt="repro124m" /&gt;&lt;/p&gt; 
&lt;p&gt;Because the code is so simple, it is very easy to hack to your needs, train new models from scratch, or finetune pretrained checkpoints (e.g. biggest one currently available as a starting point would be the GPT-2 1.3B model from OpenAI).&lt;/p&gt; 
&lt;h2&gt;install&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;pip install torch numpy transformers datasets tiktoken wandb tqdm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Dependencies:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pytorch.org"&gt;pytorch&lt;/a&gt; &amp;lt;3&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://numpy.org/install/"&gt;numpy&lt;/a&gt; &amp;lt;3&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;transformers&lt;/code&gt; for huggingface transformers &amp;lt;3 (to load GPT-2 checkpoints)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;datasets&lt;/code&gt; for huggingface datasets &amp;lt;3 (if you want to download + preprocess OpenWebText)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tiktoken&lt;/code&gt; for OpenAI's fast BPE code &amp;lt;3&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;wandb&lt;/code&gt; for optional logging &amp;lt;3&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tqdm&lt;/code&gt; for progress bars &amp;lt;3&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;quick start&lt;/h2&gt; 
&lt;p&gt;If you are not a deep learning professional and you just want to feel the magic and get your feet wet, the fastest way to get started is to train a character-level GPT on the works of Shakespeare. First, we download it as a single (1MB) file and turn it from raw text into one large stream of integers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python data/shakespeare_char/prepare.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This creates a &lt;code&gt;train.bin&lt;/code&gt; and &lt;code&gt;val.bin&lt;/code&gt; in that data directory. Now it is time to train your GPT. The size of it very much depends on the computational resources of your system:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;I have a GPU&lt;/strong&gt;. Great, we can quickly train a baby GPT with the settings provided in the &lt;a href="https://raw.githubusercontent.com/karpathy/nanoGPT/master/config/train_shakespeare_char.py"&gt;config/train_shakespeare_char.py&lt;/a&gt; config file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python train.py config/train_shakespeare_char.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you peek inside it, you'll see that we're training a GPT with a context size of up to 256 characters, 384 feature channels, and it is a 6-layer Transformer with 6 heads in each layer. On one A100 GPU this training run takes about 3 minutes and the best validation loss is 1.4697. Based on the configuration, the model checkpoints are being written into the &lt;code&gt;--out_dir&lt;/code&gt; directory &lt;code&gt;out-shakespeare-char&lt;/code&gt;. So once the training finishes we can sample from the best model by pointing the sampling script at this directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python sample.py --out_dir=out-shakespeare-char
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This generates a few samples, for example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;ANGELO:
And cowards it be strawn to my bed,
And thrust the gates of my threats,
Because he that ale away, and hang'd
An one with him.

DUKE VINCENTIO:
I thank your eyes against it.

DUKE VINCENTIO:
Then will answer him to save the malm:
And what have you tyrannous shall do this?

DUKE VINCENTIO:
If you have done evils of all disposition
To end his power, the day of thrust for a common men
That I leave, to fight with over-liking
Hasting in a roseman.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;lol &lt;code&gt;Â¯\_(ãƒ„)_/Â¯&lt;/code&gt;. Not bad for a character-level model after 3 minutes of training on a GPU. Better results are quite likely obtainable by instead finetuning a pretrained GPT-2 model on this dataset (see finetuning section later).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;I only have a macbook&lt;/strong&gt; (or other cheap computer). No worries, we can still train a GPT but we want to dial things down a notch. I recommend getting the bleeding edge PyTorch nightly (&lt;a href="https://pytorch.org/get-started/locally/"&gt;select it here&lt;/a&gt; when installing) as it is currently quite likely to make your code more efficient. But even without it, a simple train run could look as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python train.py config/train_shakespeare_char.py --device=cpu --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Here, since we are running on CPU instead of GPU we must set both &lt;code&gt;--device=cpu&lt;/code&gt; and also turn off PyTorch 2.0 compile with &lt;code&gt;--compile=False&lt;/code&gt;. Then when we evaluate we get a bit more noisy but faster estimate (&lt;code&gt;--eval_iters=20&lt;/code&gt;, down from 200), our context size is only 64 characters instead of 256, and the batch size only 12 examples per iteration, not 64. We'll also use a much smaller Transformer (4 layers, 4 heads, 128 embedding size), and decrease the number of iterations to 2000 (and correspondingly usually decay the learning rate to around max_iters with &lt;code&gt;--lr_decay_iters&lt;/code&gt;). Because our network is so small we also ease down on regularization (&lt;code&gt;--dropout=0.0&lt;/code&gt;). This still runs in about ~3 minutes, but gets us a loss of only 1.88 and therefore also worse samples, but it's still good fun:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python sample.py --out_dir=out-shakespeare-char --device=cpu
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Generates samples like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;GLEORKEN VINGHARD III:
Whell's the couse, the came light gacks,
And the for mought you in Aut fries the not high shee
bot thou the sought bechive in that to doth groan you,
No relving thee post mose the wear
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Not bad for ~3 minutes on a CPU, for a hint of the right character gestalt. If you're willing to wait longer, feel free to tune the hyperparameters, increase the size of the network, the context length (&lt;code&gt;--block_size&lt;/code&gt;), the length of training, etc.&lt;/p&gt; 
&lt;p&gt;Finally, on Apple Silicon Macbooks and with a recent PyTorch version make sure to add &lt;code&gt;--device=mps&lt;/code&gt; (short for "Metal Performance Shaders"); PyTorch then uses the on-chip GPU that can &lt;em&gt;significantly&lt;/em&gt; accelerate training (2-3X) and allow you to use larger networks. See &lt;a href="https://github.com/karpathy/nanoGPT/issues/28"&gt;Issue 28&lt;/a&gt; for more.&lt;/p&gt; 
&lt;h2&gt;reproducing GPT-2&lt;/h2&gt; 
&lt;p&gt;A more serious deep learning professional may be more interested in reproducing GPT-2 results. So here we go - we first tokenize the dataset, in this case the &lt;a href="https://openwebtext2.readthedocs.io/en/latest/"&gt;OpenWebText&lt;/a&gt;, an open reproduction of OpenAI's (private) WebText:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python data/openwebtext/prepare.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This downloads and tokenizes the &lt;a href="https://huggingface.co/datasets/openwebtext"&gt;OpenWebText&lt;/a&gt; dataset. It will create a &lt;code&gt;train.bin&lt;/code&gt; and &lt;code&gt;val.bin&lt;/code&gt; which holds the GPT2 BPE token ids in one sequence, stored as raw uint16 bytes. Then we're ready to kick off training. To reproduce GPT-2 (124M) you'll want at least an 8X A100 40GB node and run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will run for about 4 days using PyTorch Distributed Data Parallel (DDP) and go down to loss of ~2.85. Now, a GPT-2 model just evaluated on OWT gets a val loss of about 3.11, but if you finetune it it will come down to ~2.85 territory (due to an apparent domain gap), making the two models ~match.&lt;/p&gt; 
&lt;p&gt;If you're in a cluster environment and you are blessed with multiple GPU nodes you can make GPU go brrrr e.g. across 2 nodes like:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Run on the first (master) node with example IP 123.456.123.456:
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py
# Run on the worker node:
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It is a good idea to benchmark your interconnect (e.g. iperf3). In particular, if you don't have Infiniband then also prepend &lt;code&gt;NCCL_IB_DISABLE=1&lt;/code&gt; to the above launches. Your multinode training will work, but most likely &lt;em&gt;crawl&lt;/em&gt;. By default checkpoints are periodically written to the &lt;code&gt;--out_dir&lt;/code&gt;. We can sample from the model by simply &lt;code&gt;python sample.py&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Finally, to train on a single GPU simply run the &lt;code&gt;python train.py&lt;/code&gt; script. Have a look at all of its args, the script tries to be very readable, hackable and transparent. You'll most likely want to tune a number of those variables depending on your needs.&lt;/p&gt; 
&lt;h2&gt;baselines&lt;/h2&gt; 
&lt;p&gt;OpenAI GPT-2 checkpoints allow us to get some baselines in place for openwebtext. We can get the numbers as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;$ python train.py config/eval_gpt2.py
$ python train.py config/eval_gpt2_medium.py
$ python train.py config/eval_gpt2_large.py
$ python train.py config/eval_gpt2_xl.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and observe the following losses on train and val:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;model&lt;/th&gt; 
   &lt;th&gt;params&lt;/th&gt; 
   &lt;th&gt;train loss&lt;/th&gt; 
   &lt;th&gt;val loss&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gpt2&lt;/td&gt; 
   &lt;td&gt;124M&lt;/td&gt; 
   &lt;td&gt;3.11&lt;/td&gt; 
   &lt;td&gt;3.12&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gpt2-medium&lt;/td&gt; 
   &lt;td&gt;350M&lt;/td&gt; 
   &lt;td&gt;2.85&lt;/td&gt; 
   &lt;td&gt;2.84&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gpt2-large&lt;/td&gt; 
   &lt;td&gt;774M&lt;/td&gt; 
   &lt;td&gt;2.66&lt;/td&gt; 
   &lt;td&gt;2.67&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gpt2-xl&lt;/td&gt; 
   &lt;td&gt;1558M&lt;/td&gt; 
   &lt;td&gt;2.56&lt;/td&gt; 
   &lt;td&gt;2.54&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;However, we have to note that GPT-2 was trained on (closed, never released) WebText, while OpenWebText is just a best-effort open reproduction of this dataset. This means there is a dataset domain gap. Indeed, taking the GPT-2 (124M) checkpoint and finetuning on OWT directly for a while reaches loss down to ~2.85. This then becomes the more appropriate baseline w.r.t. reproduction.&lt;/p&gt; 
&lt;h2&gt;finetuning&lt;/h2&gt; 
&lt;p&gt;Finetuning is no different than training, we just make sure to initialize from a pretrained model and train with a smaller learning rate. For an example of how to finetune a GPT on new text go to &lt;code&gt;data/shakespeare&lt;/code&gt; and run &lt;code&gt;prepare.py&lt;/code&gt; to download the tiny shakespeare dataset and render it into a &lt;code&gt;train.bin&lt;/code&gt; and &lt;code&gt;val.bin&lt;/code&gt;, using the OpenAI BPE tokenizer from GPT-2. Unlike OpenWebText this will run in seconds. Finetuning can take very little time, e.g. on a single GPU just a few minutes. Run an example finetuning like:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python train.py config/finetune_shakespeare.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will load the config parameter overrides in &lt;code&gt;config/finetune_shakespeare.py&lt;/code&gt; (I didn't tune them much though). Basically, we initialize from a GPT2 checkpoint with &lt;code&gt;init_from&lt;/code&gt; and train as normal, except shorter and with a small learning rate. If you're running out of memory try decreasing the model size (they are &lt;code&gt;{'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}&lt;/code&gt;) or possibly decreasing the &lt;code&gt;block_size&lt;/code&gt; (context length). The best checkpoint (lowest validation loss) will be in the &lt;code&gt;out_dir&lt;/code&gt; directory, e.g. in &lt;code&gt;out-shakespeare&lt;/code&gt; by default, per the config file. You can then run the code in &lt;code&gt;sample.py --out_dir=out-shakespeare&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;THEODORE:
Thou shalt sell me to the highest bidder: if I die,
I sell thee to the first; if I go mad,
I sell thee to the second; if I
lie, I sell thee to the third; if I slay,
I sell thee to the fourth: so buy or sell,
I tell thee again, thou shalt not sell my
possession.

JULIET:
And if thou steal, thou shalt not sell thyself.

THEODORE:
I do not steal; I sell the stolen goods.

THEODORE:
Thou know'st not what thou sell'st; thou, a woman,
Thou art ever a victim, a thing of no worth:
Thou hast no right, no right, but to be sold.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Whoa there, GPT, entering some dark place over there. I didn't really tune the hyperparameters in the config too much, feel free to try!&lt;/p&gt; 
&lt;h2&gt;sampling / inference&lt;/h2&gt; 
&lt;p&gt;Use the script &lt;code&gt;sample.py&lt;/code&gt; to sample either from pre-trained GPT-2 models released by OpenAI, or from a model you trained yourself. For example, here is a way to sample from the largest available &lt;code&gt;gpt2-xl&lt;/code&gt; model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python sample.py \
    --init_from=gpt2-xl \
    --start="What is the answer to life, the universe, and everything?" \
    --num_samples=5 --max_new_tokens=100
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you'd like to sample from a model you trained, use the &lt;code&gt;--out_dir&lt;/code&gt; to point the code appropriately. You can also prompt the model with some text from a file, e.g. &lt;code&gt;python sample.py --start=FILE:prompt.txt&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;efficiency notes&lt;/h2&gt; 
&lt;p&gt;For simple model benchmarking and profiling, &lt;code&gt;bench.py&lt;/code&gt; might be useful. It's identical to what happens in the meat of the training loop of &lt;code&gt;train.py&lt;/code&gt;, but omits much of the other complexities.&lt;/p&gt; 
&lt;p&gt;Note that the code by default uses &lt;a href="https://pytorch.org/get-started/pytorch-2.0/"&gt;PyTorch 2.0&lt;/a&gt;. At the time of writing (Dec 29, 2022) this makes &lt;code&gt;torch.compile()&lt;/code&gt; available in the nightly release. The improvement from the one line of code is noticeable, e.g. cutting down iteration time from ~250ms / iter to 135ms / iter. Nice work PyTorch team!&lt;/p&gt; 
&lt;h2&gt;todos&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Investigate and add FSDP instead of DDP&lt;/li&gt; 
 &lt;li&gt;Eval zero-shot perplexities on standard evals (e.g. LAMBADA? HELM? etc.)&lt;/li&gt; 
 &lt;li&gt;Finetune the finetuning script, I think the hyperparams are not great&lt;/li&gt; 
 &lt;li&gt;Schedule for linear batch size increase during training&lt;/li&gt; 
 &lt;li&gt;Incorporate other embeddings (rotary, alibi)&lt;/li&gt; 
 &lt;li&gt;Separate out the optim buffers from model params in checkpoints I think&lt;/li&gt; 
 &lt;li&gt;Additional logging around network health (e.g. gradient clip events, magnitudes)&lt;/li&gt; 
 &lt;li&gt;Few more investigations around better init etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;troubleshooting&lt;/h2&gt; 
&lt;p&gt;Note that by default this repo uses PyTorch 2.0 (i.e. &lt;code&gt;torch.compile&lt;/code&gt;). This is fairly new and experimental, and not yet available on all platforms (e.g. Windows). If you're running into related error messages try to disable this by adding &lt;code&gt;--compile=False&lt;/code&gt; flag. This will slow down the code but at least it will run.&lt;/p&gt; 
&lt;p&gt;For some context on this repository, GPT, and language modeling it might be helpful to watch my &lt;a href="https://karpathy.ai/zero-to-hero.html"&gt;Zero To Hero series&lt;/a&gt;. Specifically, the &lt;a href="https://www.youtube.com/watch?v=kCc8FmEb1nY"&gt;GPT video&lt;/a&gt; is popular if you have some prior language modeling context.&lt;/p&gt; 
&lt;p&gt;For more questions/discussions feel free to stop by &lt;strong&gt;#nanoGPT&lt;/strong&gt; on Discord:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/3zy8kqD9Cp"&gt;&lt;img src="https://dcbadge.vercel.app/api/server/3zy8kqD9Cp?compact=true&amp;amp;style=flat" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;acknowledgements&lt;/h2&gt; 
&lt;p&gt;All nanoGPT experiments are powered by GPUs on &lt;a href="https://lambdalabs.com"&gt;Lambda labs&lt;/a&gt;, my favorite Cloud GPU provider. Thank you Lambda labs for sponsoring nanoGPT!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>reflex-dev/reflex</title>
      <link>https://github.com/reflex-dev/reflex</link>
      <description>&lt;p&gt;ğŸ•¸ï¸ Web apps in pure Python ğŸ&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/reflex.svg?sanitize=true" alt="Reflex Logo" width="300px" /&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;&lt;strong&gt;âœ¨ Performant, customizable web apps in pure Python. Deploy in seconds. âœ¨&lt;/strong&gt;&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://badge.fury.io/py/reflex"&gt;&lt;img src="https://badge.fury.io/py/reflex.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/pyversions/reflex.svg?sanitize=true" alt="versions" /&gt; &lt;a href="https://reflex.dev/docs/getting-started/introduction"&gt;&lt;img src="https://img.shields.io/badge/Documentation%20-Introduction%20-%20%23007ec6" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/projects/reflex"&gt;&lt;img src="https://static.pepy.tech/badge/reflex" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/T5WSbC2YtQ"&gt;&lt;img src="https://img.shields.io/discord/1029853095527727165?color=%237289da&amp;amp;label=Discord" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://x.com/getreflex"&gt;&lt;img src="https://img.shields.io/twitter/follow/getreflex" alt="Twitter" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href="https://github.com/reflex-dev/reflex/raw/main/README.md"&gt;English&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/zh/zh_cn/README.md"&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/zh/zh_tw/README.md"&gt;ç¹é«”ä¸­æ–‡&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/tr/README.md"&gt;TÃ¼rkÃ§e&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/in/README.md"&gt;à¤¹à¤¿à¤‚à¤¦à¥€&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/pt/pt_br/README.md"&gt;PortuguÃªs (Brasil)&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/it/README.md"&gt;Italiano&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/es/README.md"&gt;EspaÃ±ol&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/kr/README.md"&gt;í•œêµ­ì–´&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/ja/README.md"&gt;æ—¥æœ¬èª&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/de/README.md"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/pe/README.md"&gt;Persian (Ù¾Ø§Ø±Ø³ÛŒ)&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/vi/README.md"&gt;Tiáº¿ng Viá»‡t&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] ğŸš€ &lt;strong&gt;Try &lt;a href="https://build.reflex.dev/"&gt;Reflex Build&lt;/a&gt;&lt;/strong&gt; â€“ our AI-powered app builder that generates full-stack Reflex applications in seconds.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h1&gt;Introduction&lt;/h1&gt; 
&lt;p&gt;Reflex is a library to build full-stack web apps in pure Python.&lt;/p&gt; 
&lt;p&gt;Key features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Pure Python&lt;/strong&gt; - Write your app's frontend and backend all in Python, no need to learn Javascript.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Full Flexibility&lt;/strong&gt; - Reflex is easy to get started with, but can also scale to complex apps.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Deploy Instantly&lt;/strong&gt; - After building, deploy your app with a &lt;a href="https://reflex.dev/docs/hosting/deploy-quick-start/"&gt;single command&lt;/a&gt; or host it on your own server.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See our &lt;a href="https://reflex.dev/blog/2024-03-21-reflex-architecture/#the-reflex-architecture"&gt;architecture page&lt;/a&gt; to learn how Reflex works under the hood.&lt;/p&gt; 
&lt;h2&gt;âš™ï¸ Installation&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; We strongly recommend using a virtual environment to ensure the &lt;code&gt;reflex&lt;/code&gt; command is available in your PATH.&lt;/p&gt; 
&lt;h2&gt;ğŸ¥³ Create your first app&lt;/h2&gt; 
&lt;h3&gt;1. Create the project directory&lt;/h3&gt; 
&lt;p&gt;Replace &lt;code&gt;my_app_name&lt;/code&gt; with your project name:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir my_app_name
cd my_app_name
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Set up a virtual environment&lt;/h3&gt; 
&lt;p&gt;Create and activate virtual environment&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# On Windows:
python -m venv .venv
.venv\Scripts\activate

# On macOS/Linux:
python3 -m venv .venv
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Install Reflex&lt;/h3&gt; 
&lt;p&gt;Reflex is available as a pip package (Requires Python 3.10+):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install reflex
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4. Initialize the project&lt;/h3&gt; 
&lt;p&gt;This command initializes a template app in your new directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;reflex init
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;5. Run the app&lt;/h3&gt; 
&lt;p&gt;You can run this app in development mode:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;reflex run
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should see your app running at &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Now you can modify the source code in &lt;code&gt;my_app_name/my_app_name.py&lt;/code&gt;. Reflex has fast refreshes so you can see your changes instantly when you save your code.&lt;/p&gt; 
&lt;h3&gt;Troubleshooting&lt;/h3&gt; 
&lt;p&gt;If you installed Reflex without a virtual environment and the &lt;code&gt;reflex&lt;/code&gt; command is not found, you can run commands using: &lt;code&gt;python3 -m reflex init&lt;/code&gt; and &lt;code&gt;python3 -m reflex run&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ«§ Example App&lt;/h2&gt; 
&lt;p&gt;Let's go over an example: creating an image generation UI around &lt;a href="https://platform.openai.com/docs/guides/images/image-generation?context=node"&gt;DALLÂ·E&lt;/a&gt;. For simplicity, we just call the &lt;a href="https://platform.openai.com/docs/api-reference/authentication"&gt;OpenAI API&lt;/a&gt;, but you could replace this with an ML model run locally.&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/dalle.gif" alt="A frontend wrapper for DALLÂ·E, shown in the process of generating an image." width="550" /&gt; 
&lt;/div&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;Here is the complete code to create this. This is all done in one Python file!&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import reflex as rx
import openai

openai_client = openai.OpenAI()


class State(rx.State):
    """The app state."""

    prompt = ""
    image_url = ""
    processing = False
    complete = False

    def get_image(self):
        """Get the image from the prompt."""
        if self.prompt == "":
            return rx.window_alert("Prompt Empty")

        self.processing, self.complete = True, False
        yield
        response = openai_client.images.generate(
            prompt=self.prompt, n=1, size="1024x1024"
        )
        self.image_url = response.data[0].url
        self.processing, self.complete = False, True


def index():
    return rx.center(
        rx.vstack(
            rx.heading("DALL-E", font_size="1.5em"),
            rx.input(
                placeholder="Enter a prompt..",
                on_blur=State.set_prompt,
                width="25em",
            ),
            rx.button(
                "Generate Image",
                on_click=State.get_image,
                width="25em",
                loading=State.processing
            ),
            rx.cond(
                State.complete,
                rx.image(src=State.image_url, width="20em"),
            ),
            align="center",
        ),
        width="100%",
        height="100vh",
    )

# Add state and page to the app.
app = rx.App()
app.add_page(index, title="Reflex:DALL-E")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Let's break this down.&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/dalle_colored_code_example.png" alt="Explaining the differences between backend and frontend parts of the DALL-E app." width="900" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;&lt;strong&gt;Reflex UI&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Let's start with the UI.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;def index():
    return rx.center(
        ...
    )
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This &lt;code&gt;index&lt;/code&gt; function defines the frontend of the app.&lt;/p&gt; 
&lt;p&gt;We use different components such as &lt;code&gt;center&lt;/code&gt;, &lt;code&gt;vstack&lt;/code&gt;, &lt;code&gt;input&lt;/code&gt;, and &lt;code&gt;button&lt;/code&gt; to build the frontend. Components can be nested within each other to create complex layouts. And you can use keyword args to style them with the full power of CSS.&lt;/p&gt; 
&lt;p&gt;Reflex comes with &lt;a href="https://reflex.dev/docs/library"&gt;60+ built-in components&lt;/a&gt; to help you get started. We are actively adding more components, and it's easy to &lt;a href="https://reflex.dev/docs/wrapping-react/overview/"&gt;create your own components&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;State&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Reflex represents your UI as a function of your state.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;class State(rx.State):
    """The app state."""
    prompt = ""
    image_url = ""
    processing = False
    complete = False

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The state defines all the variables (called vars) in an app that can change and the functions that change them.&lt;/p&gt; 
&lt;p&gt;Here the state is comprised of a &lt;code&gt;prompt&lt;/code&gt; and &lt;code&gt;image_url&lt;/code&gt;. There are also the booleans &lt;code&gt;processing&lt;/code&gt; and &lt;code&gt;complete&lt;/code&gt; to indicate when to disable the button (during image generation) and when to show the resulting image.&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;Event Handlers&lt;/strong&gt;&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;def get_image(self):
    """Get the image from the prompt."""
    if self.prompt == "":
        return rx.window_alert("Prompt Empty")

    self.processing, self.complete = True, False
    yield
    response = openai_client.images.generate(
        prompt=self.prompt, n=1, size="1024x1024"
    )
    self.image_url = response.data[0].url
    self.processing, self.complete = False, True
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Within the state, we define functions called event handlers that change the state vars. Event handlers are the way that we can modify the state in Reflex. They can be called in response to user actions, such as clicking a button or typing in a text box. These actions are called events.&lt;/p&gt; 
&lt;p&gt;Our DALLÂ·E app has an event handler, &lt;code&gt;get_image&lt;/code&gt; which gets this image from the OpenAI API. Using &lt;code&gt;yield&lt;/code&gt; in the middle of an event handler will cause the UI to update. Otherwise the UI will update at the end of the event handler.&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;Routing&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Finally, we define our app.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;app = rx.App()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We add a page from the root of the app to the index component. We also add a title that will show up in the page preview/browser tab.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;app.add_page(index, title="DALL-E")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can create a multi-page app by adding more pages.&lt;/p&gt; 
&lt;h2&gt;ğŸ“‘ Resources&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;ğŸ“‘ &lt;a href="https://reflex.dev/docs/getting-started/introduction"&gt;Docs&lt;/a&gt; &amp;nbsp; | &amp;nbsp; ğŸ—ï¸ &lt;a href="https://reflex.dev/blog"&gt;Blog&lt;/a&gt; &amp;nbsp; | &amp;nbsp; ğŸ“± &lt;a href="https://reflex.dev/docs/library"&gt;Component Library&lt;/a&gt; &amp;nbsp; | &amp;nbsp; ğŸ–¼ï¸ &lt;a href="https://reflex.dev/templates/"&gt;Templates&lt;/a&gt; &amp;nbsp; | &amp;nbsp; ğŸ›¸ &lt;a href="https://reflex.dev/docs/hosting/deploy-quick-start"&gt;Deployment&lt;/a&gt; &amp;nbsp;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;âœ… Status&lt;/h2&gt; 
&lt;p&gt;Reflex launched in December 2022 with the name Pynecone.&lt;/p&gt; 
&lt;p&gt;ğŸš€ Introducing &lt;a href="https://build.reflex.dev/"&gt;Reflex Build&lt;/a&gt; â€” Our AI-Powered Builder Reflex Build uses AI to generate complete full-stack Python applications. It helps you quickly create, customize, and refine your Reflex apps â€” from frontend components to backend logic â€” so you can focus on your ideas instead of boilerplate code. Whether youâ€™re prototyping or scaling, Reflex Build accelerates development by intelligently scaffolding and optimizing your appâ€™s entire stack.&lt;/p&gt; 
&lt;p&gt;Alongside this, &lt;a href="https://cloud.reflex.dev"&gt;Reflex Cloud&lt;/a&gt; launched in 2025 to offer the best hosting experience for your Reflex apps. Weâ€™re continuously improving the platform with new features and capabilities.&lt;/p&gt; 
&lt;p&gt;Reflex has new releases and features coming every week! Make sure to &lt;span&gt;â­&lt;/span&gt; star and &lt;span&gt;ğŸ‘€&lt;/span&gt; watch this repository to stay up to date.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions of any size! Below are some good ways to get started in the Reflex community.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Join Our Discord&lt;/strong&gt;: Our &lt;a href="https://discord.gg/T5WSbC2YtQ"&gt;Discord&lt;/a&gt; is the best place to get help on your Reflex project and to discuss how you can contribute.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Discussions&lt;/strong&gt;: A great way to talk about features you want added or things that are confusing/need clarification.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Issues&lt;/strong&gt;: &lt;a href="https://github.com/reflex-dev/reflex/issues"&gt;Issues&lt;/a&gt; are an excellent way to report bugs. Additionally, you can try and solve an existing issue and submit a PR.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We are actively looking for contributors, no matter your skill level or experience. To contribute check out &lt;a href="https://github.com/reflex-dev/reflex/raw/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;All Thanks To Our Contributors:&lt;/h2&gt; 
&lt;a href="https://github.com/reflex-dev/reflex/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=reflex-dev/reflex" /&gt; &lt;/a&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Reflex is open-source and licensed under the &lt;a href="https://raw.githubusercontent.com/reflex-dev/reflex/main/LICENSE"&gt;Apache License 2.0&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>yeongpin/cursor-free-vip</title>
      <link>https://github.com/yeongpin/cursor-free-vip</link>
      <description>&lt;p&gt;[Support 0.49.x]ï¼ˆReset Cursor AI MachineID &amp; Bypass Higher Token Limitï¼‰ Cursor Ai ï¼Œè‡ªåŠ¨é‡ç½®æœºå™¨ID ï¼Œ å…è´¹å‡çº§ä½¿ç”¨ProåŠŸèƒ½: You've reached your trial request limit. / Too many free trial accounts used on this machine. Please upgrade to pro. We have this limit in place to prevent abuse. Please let us know if you believe this is a mistake.&lt;/p&gt;&lt;hr&gt;&lt;div align="center" markdown="1"&gt; 
 &lt;sup&gt;Special thanks to:&lt;/sup&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;a href="http://go.warp.dev/cursor-free-vip"&gt; &lt;img alt="Warp sponsorship" width="400" src="https://github.com/user-attachments/assets/ab8dd143-b0fd-4904-bdc5-dd7ecac94eae" /&gt; &lt;/a&gt; 
 &lt;h3&gt;&lt;a href="http://go.warp.dev/cursor-free-vip"&gt;Warp, built for coding with multiple agents.&lt;/a&gt;&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="http://go.warp.dev/cursor-free-vip"&gt;Available for MacOS, Linux, &amp;amp; Windows&lt;/a&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;â¤ Cursor Free VIP&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/images/logo.png" alt="Cursor Pro Logo" width="200" style="border-radius: 6px;" /&gt; &lt;/p&gt; 
 &lt;p align="center"&gt; &lt;/p&gt;
 &lt;p&gt;&lt;a href="https://github.com/yeongpin/cursor-free-vip/releases/latest"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/release/yeongpin/cursor-free-vip" alt="Release" /&gt;&lt;/a&gt; &lt;a href="https://creativecommons.org/licenses/by-nc-nd/4.0/"&gt;&lt;img src="https://img.shields.io/badge/License-CC_BY--NC--ND_4.0-lightgrey.svg?sanitize=true" alt="License: CC BY-NC-ND 4.0" /&gt;&lt;/a&gt; &lt;a href="https://github.com/yeongpin/cursor-free-vip/stargazers"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/stars/yeongpin/cursor-free-vip" alt="Stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/yeongpin/cursor-free-vip/releases/latest"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/downloads/yeongpin/cursor-free-vip/total" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://buymeacoffee.com/yeongpin" target="_blank"&gt;&lt;img alt="Buy Me a Coffee" src="https://img.shields.io/badge/Buy%20Me%20a%20Coffee-Support%20Me-FFDA33" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/yeongpin/cursor-free-vip"&gt;&lt;img src="https://devin.ai/assets/deepwiki-badge.png" alt="Ask DeepWiki.com" height="20" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/13425" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13425" alt="yeongpin%2Fcursor-free-vip | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;br /&gt;&lt;/p&gt; 
 &lt;h4&gt;Support Latest 0.49.x Version | æ”¯æŒæœ€æ–° 0.49.x ç‰ˆæœ¬&lt;/h4&gt; 
 &lt;p&gt;This tool is for educational purposes, currently the repo does not violate any laws. Please support the original project. This tool will not generate any fake email accounts and OAuth access.&lt;/p&gt; 
 &lt;p&gt;Supports Windows, macOS and Linux.&lt;/p&gt; 
 &lt;p&gt;For optimal performance, run with privileges and always stay up to date.&lt;/p&gt; 
 &lt;p&gt;é€™æ˜¯ä¸€æ¬¾ç”¨æ–¼å­¸ç¿’å’Œç ”ç©¶çš„å·¥å…·ï¼Œç›®å‰ repo æ²’æœ‰é•åä»»ä½•æ³•å¾‹ã€‚è«‹æ”¯æŒåŸä½œè€…ã€‚ é€™æ¬¾å·¥å…·ä¸æœƒç”Ÿæˆä»»ä½•å‡çš„é›»å­éƒµä»¶å¸³æˆ¶å’Œ OAuth è¨ªå•ã€‚&lt;/p&gt; 
 &lt;p&gt;æ”¯æŒ Windowsã€macOS å’Œ Linuxã€‚&lt;/p&gt; 
 &lt;p&gt;å°æ–¼æœ€ä½³æ€§èƒ½ï¼Œè«‹ä»¥ç®¡ç†å“¡èº«ä»½é‹è¡Œä¸¦å§‹çµ‚ä¿æŒæœ€æ–°ã€‚&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/images/product_2025-04-16_10-40-21.png" alt="new" width="800" style="border-radius: 6px;" /&gt;&lt;br /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ”„ Change Log | æ›´æ–°æ—¥å¿—&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/CHANGELOG.md"&gt;Watch Change Log | æŸ¥çœ‹æ›´æ–°æ—¥å¿—&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;âœ¨ Features | åŠŸèƒ½ç‰¹é»&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Support Windows macOS and Linux systems&lt;br /&gt;æ”¯æŒ Windowsã€macOS å’Œ Linux ç³»çµ±&lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Reset Cursor's configuration&lt;br /&gt;é‡ç½® Cursor çš„é…ç½®&lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Multi-language support (English, ç®€ä½“ä¸­æ–‡, ç¹é«”ä¸­æ–‡, Vietnamese)&lt;br /&gt;å¤šèªè¨€æ”¯æŒï¼ˆè‹±æ–‡ã€ç®€ä½“ä¸­æ–‡ã€ç¹é«”ä¸­æ–‡ã€è¶Šå—èªï¼‰&lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ’» System Support | ç³»çµ±æ”¯æŒ&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Operating System&lt;/th&gt; 
   &lt;th&gt;Architecture&lt;/th&gt; 
   &lt;th&gt;Supported&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows&lt;/td&gt; 
   &lt;td&gt;x64, x86&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;macOS&lt;/td&gt; 
   &lt;td&gt;Intel, Apple Silicon&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Linux&lt;/td&gt; 
   &lt;td&gt;x64, x86, ARM64&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;ğŸ‘€ How to use | å¦‚ä½•ä½¿ç”¨&lt;/h2&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;b&gt;â­ Auto Run Script | è…³æœ¬è‡ªå‹•åŒ–é‹è¡Œ&lt;/b&gt;&lt;/summary&gt; 
 &lt;h3&gt;&lt;strong&gt;Linux/macOS&lt;/strong&gt;&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;curl -fsSL https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/scripts/install.sh -o install.sh &amp;amp;&amp;amp; chmod +x install.sh &amp;amp;&amp;amp; ./install.sh
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;&lt;strong&gt;Archlinux&lt;/strong&gt;&lt;/h3&gt; 
 &lt;p&gt;Install via &lt;a href="https://aur.archlinux.org/packages/cursor-free-vip-git"&gt;AUR&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;yay -S cursor-free-vip-git
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-powershell"&gt;irm https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/scripts/install.ps1 | iex
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;If you want to stop the script, please press Ctrl+C&lt;br /&gt;è¦åœæ­¢è…³æœ¬ï¼Œè«‹æŒ‰ Ctrl+C&lt;/p&gt; 
&lt;h2&gt;â— Note | æ³¨æ„äº‹é …&lt;/h2&gt; 
&lt;p&gt;ğŸ“ Config | æ–‡ä»¶é…ç½® &lt;code&gt;Win / Macos / Linux Path | è·¯å¾‘ [Documents/.cursor-free-vip/config.ini]&lt;/code&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;â­ Config | æ–‡ä»¶é…ç½®&lt;/b&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;[Chrome]
# Default Google Chrome Path | é»˜èªGoogle Chrome éŠè¦½å™¨è·¯å¾‘
chromepath = C:\Program Files\Google/Chrome/Application/chrome.exe

[Turnstile]
# Handle Turnstile Wait Time | ç­‰å¾…äººæ©Ÿé©—è­‰æ™‚é–“
handle_turnstile_time = 2
# Handle Turnstile Wait Random Time (must merge 1-3 or 1,3) | ç­‰å¾…äººæ©Ÿé©—è­‰éš¨æ©Ÿæ™‚é–“ï¼ˆå¿…é ˆæ˜¯ 1-3 æˆ–è€… 1,3 é€™æ¨£çš„çµ„åˆï¼‰
handle_turnstile_random_time = 1-3

[OSPaths]
# Storage Path | å­˜å„²è·¯å¾‘
storage_path = /Users/username/Library/Application Support/Cursor/User/globalStorage/storage.json
# SQLite Path | SQLiteè·¯å¾‘
sqlite_path = /Users/username/Library/Application Support/Cursor/User/globalStorage/state.vscdb
# Machine ID Path | æ©Ÿå™¨IDè·¯å¾‘
machine_id_path = /Users/username/Library/Application Support/Cursor/machineId
# For Linux users: ~/.config/cursor/machineid

[Timing]
# Min Random Time | æœ€å°éš¨æ©Ÿæ™‚é–“
min_random_time = 0.1
# Max Random Time | æœ€å¤§éš¨æ©Ÿæ™‚é–“
max_random_time = 0.8
# Page Load Wait | é é¢åŠ è¼‰ç­‰å¾…æ™‚é–“
page_load_wait = 0.1-0.8
# Input Wait | è¼¸å…¥ç­‰å¾…æ™‚é–“
input_wait = 0.3-0.8
# Submit Wait | æäº¤ç­‰å¾…æ™‚é–“
submit_wait = 0.5-1.5
# Verification Code Input | é©—è­‰ç¢¼è¼¸å…¥ç­‰å¾…æ™‚é–“
verification_code_input = 0.1-0.3
# Verification Success Wait | é©—è­‰æˆåŠŸç­‰å¾…æ™‚é–“
verification_success_wait = 2-3
# Verification Retry Wait | é©—è­‰é‡è©¦ç­‰å¾…æ™‚é–“
verification_retry_wait = 2-3
# Email Check Initial Wait | éƒµä»¶æª¢æŸ¥åˆå§‹ç­‰å¾…æ™‚é–“
email_check_initial_wait = 4-6
# Email Refresh Wait | éƒµä»¶åˆ·æ–°ç­‰å¾…æ™‚é–“
email_refresh_wait = 2-4
# Settings Page Load Wait | è¨­ç½®é é¢åŠ è¼‰ç­‰å¾…æ™‚é–“
settings_page_load_wait = 1-2
# Failed Retry Time | å¤±æ•—é‡è©¦æ™‚é–“
failed_retry_time = 0.5-1
# Retry Interval | é‡è©¦é–“éš”
retry_interval = 8-12
# Max Timeout | æœ€å¤§è¶…æ™‚æ™‚é–“
max_timeout = 160

[Utils]
# Check Update | æª¢æŸ¥æ›´æ–°
check_update = True
# Show Account Info | é¡¯ç¤ºè³¬è™Ÿä¿¡æ¯
show_account_info = True

[TempMailPlus]
# Enable TempMailPlus | å•“ç”¨ TempMailPlusï¼ˆä»»ä½•è½‰ç™¼åˆ°TempMailPlusçš„éƒµä»¶éƒ½æ”¯æŒç²å–é©—è­‰ç¢¼ï¼Œä¾‹å¦‚cloudflareéƒµä»¶Catch-allï¼‰
enabled = false
# TempMailPlus Email | TempMailPlus é›»å­éƒµä»¶
email = xxxxx@mailto.plus
# TempMailPlus pin | TempMailPlus pinç¢¼
epin = 

[WindowsPaths]
storage_path = C:\Users\yeongpin\AppData\Roaming\Cursor\User\globalStorage\storage.json
sqlite_path = C:\Users\yeongpin\AppData\Roaming\Cursor\User\globalStorage\state.vscdb
machine_id_path = C:\Users\yeongpin\AppData\Roaming\Cursor\machineId
cursor_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app
updater_path = C:\Users\yeongpin\AppData\Local\cursor-updater
update_yml_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app-update.yml
product_json_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app\product.json

[Browser]
default_browser = opera
chrome_path = C:\Program Files\Google\Chrome\Application\chrome.exe
edge_path = C:\Program Files (x86)\Microsoft\Edge\Application\msedge.exe
firefox_path = C:\Program Files\Mozilla Firefox\firefox.exe
brave_path = C:\Program Files\BraveSoftware/Brave-Browser/Application/brave.exe
chrome_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe
edge_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\msedgedriver.exe
firefox_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\geckodriver.exe
brave_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe
opera_path = C:\Users\yeongpin\AppData\Local\Programs\Opera\opera.exe
opera_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe

[OAuth]
show_selection_alert = False
timeout = 120
max_attempts = 3
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Use administrator privileges to run the script &lt;br /&gt;è«‹ä½¿ç”¨ç®¡ç†å“¡èº«ä»½é‹è¡Œè…³æœ¬&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Confirm that Cursor is closed before running the script &lt;br /&gt;è«‹ç¢ºä¿åœ¨é‹è¡Œè…³æœ¬å‰å·²ç¶“é—œé–‰ Cursor&lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;This tool is only for learning and research purposes &lt;br /&gt;æ­¤å·¥å…·åƒ…ä¾›å­¸ç¿’å’Œç ”ç©¶ä½¿ç”¨&lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Please comply with the relevant software usage terms when using this tool &lt;br /&gt;ä½¿ç”¨æœ¬å·¥å…·æ™‚è«‹éµå®ˆç›¸é—œè»Ÿä»¶ä½¿ç”¨æ¢æ¬¾&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸš¨ Common Issues | å¸¸è¦‹å•é¡Œ&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;å¦‚æœé‡åˆ°æ¬Šé™å•é¡Œï¼Œè«‹ç¢ºä¿ï¼š&lt;/th&gt; 
   &lt;th align="center"&gt;æ­¤è…³æœ¬ä»¥ç®¡ç†å“¡èº«ä»½é‹è¡Œ&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;If you encounter permission issues, please ensure:&lt;/td&gt; 
   &lt;td align="center"&gt;This script is run with administrator privileges&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Error 'User is not authorized'&lt;/td&gt; 
   &lt;td align="center"&gt;This means your account was banned for using temporary (disposal) mail. Ensure using a non-temporary mail service&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;ğŸ¤© Contribution | è²¢ç»&lt;/h2&gt; 
&lt;p&gt;æ­¡è¿æäº¤ Issue å’Œ Pull Requestï¼&lt;/p&gt; 
&lt;a href="https://github.com/yeongpin/cursor-free-vip/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=yeongpin/cursor-free-vip&amp;amp;preview=true&amp;amp;max=&amp;amp;columns=" /&gt; &lt;/a&gt; 
&lt;br /&gt;
&lt;br /&gt; 
&lt;h2&gt;ğŸ“© Disclaimer | å…è²¬è²æ˜&lt;/h2&gt; 
&lt;p&gt;æœ¬å·¥å…·åƒ…ä¾›å­¸ç¿’å’Œç ”ç©¶ä½¿ç”¨ï¼Œä½¿ç”¨æœ¬å·¥å…·æ‰€ç”¢ç”Ÿçš„ä»»ä½•å¾Œæœç”±ä½¿ç”¨è€…è‡ªè¡Œæ‰¿æ“”ã€‚ &lt;br /&gt;&lt;/p&gt; 
&lt;p&gt;This tool is only for learning and research purposes, and any consequences arising from the use of this tool are borne by the user.&lt;/p&gt; 
&lt;h2&gt;ğŸ’° Buy Me a Coffee | è«‹æˆ‘å–æ¯å’–å•¡&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/images/provi-code.jpg" alt="buy_me_a_coffee" width="280" /&gt;&lt;br /&gt; &lt;/td&gt; 
    &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/images/paypal.png" alt="buy_me_a_coffee" width="280" /&gt;&lt;br /&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;â­ Star History | æ˜Ÿæ˜Ÿæ•¸&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://star-history.com/#yeongpin/cursor-free-vip&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=yeongpin/cursor-free-vip&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ“ License | æˆæ¬Š&lt;/h2&gt; 
&lt;p&gt;æœ¬é …ç›®æ¡ç”¨ &lt;a href="https://creativecommons.org/licenses/by-nc-nd/4.0/"&gt;CC BY-NC-ND 4.0&lt;/a&gt; æˆæ¬Šã€‚ Please refer to the &lt;a href="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/LICENSE.md"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>suitenumerique/docs</title>
      <link>https://github.com/suitenumerique/docs</link>
      <description>&lt;p&gt;A collaborative note taking, wiki and documentation platform that scales. Built with Django and React.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://github.com/suitenumerique/docs"&gt; &lt;img alt="Docs" src="https://raw.githubusercontent.com/suitenumerique/docs/main/docs/assets/banner-docs.png" width="100%" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/suitenumerique/docs/stargazers/"&gt; &lt;img src="https://img.shields.io/github/stars/suitenumerique/docs" alt="" /&gt; &lt;/a&gt; &lt;a href="https://github.com/suitenumerique/docs/raw/main/CONTRIBUTING.md"&gt;&lt;img alt="PRs Welcome" src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=shields" /&gt;&lt;/a&gt; &lt;img alt="GitHub commit activity" src="https://img.shields.io/github/commit-activity/m/suitenumerique/docs" /&gt; &lt;img alt="GitHub closed issues" src="https://img.shields.io/github/issues-closed/suitenumerique/docs" /&gt; &lt;a href="https://github.com/suitenumerique/docs/raw/main/LICENSE"&gt; &lt;img alt="MIT License" src="https://img.shields.io/github/license/suitenumerique/docs" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://matrix.to/#/#docs-official:matrix.org"&gt; Chat on Matrix &lt;/a&gt; - &lt;a href="https://raw.githubusercontent.com/suitenumerique/docs/main/docs/"&gt; Documentation &lt;/a&gt; - &lt;a href="https://raw.githubusercontent.com/suitenumerique/docs/main/#getting-started-"&gt; Getting started &lt;/a&gt; - &lt;a href="mailto:docs@numerique.gouv.fr"&gt; Reach out &lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;La Suite Docs : Collaborative Text Editing&lt;/h1&gt; 
&lt;p&gt;Docs, where your notes can become knowledge through live collaboration.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/suitenumerique/docs/main/docs/assets/docs_live_collaboration_light.gif" width="100%" align="center" /&gt; 
&lt;h2&gt;Why use Docs â“&lt;/h2&gt; 
&lt;p&gt;Docs is a collaborative text editor designed to address common challenges in knowledge building and sharing.&lt;/p&gt; 
&lt;h3&gt;Write&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ˜Œ Get simple, accessible online editing for your team.&lt;/li&gt; 
 &lt;li&gt;ğŸ’… Create clean documents with beautiful formatting options.&lt;/li&gt; 
 &lt;li&gt;ğŸ–Œï¸ Focus on your content using either the in-line editor, or &lt;a href="https://www.markdownguide.org/basic-syntax/"&gt;the Markdown syntax&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;ğŸ§± Quickly design your page thanks to the many block types, accessible from the &lt;code&gt;/&lt;/code&gt; slash commands, as well as keyboard shortcuts.&lt;/li&gt; 
 &lt;li&gt;ğŸ”Œ Write offline! Your edits will be synced once you're back online.&lt;/li&gt; 
 &lt;li&gt;âœ¨ Save time thanks to our AI actions, such as rephrasing, summarizing, fixing typos, translating, etc. You can even turn your selected text into a prompt!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Work together&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ¤ Enjoy live editing! See your team collaborate in real time.&lt;/li&gt; 
 &lt;li&gt;ğŸ”’ Keep your information secure thanks to granular access control. Only share with the right people.&lt;/li&gt; 
 &lt;li&gt;ğŸ“‘ Export your content in multiple formats (&lt;code&gt;.odt&lt;/code&gt;, &lt;code&gt;.docx&lt;/code&gt;, &lt;code&gt;.pdf&lt;/code&gt;) with customizable templates.&lt;/li&gt; 
 &lt;li&gt;ğŸ“š Turn your team's collaborative work into organized knowledge with Subpages.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Self-host&lt;/h3&gt; 
&lt;h4&gt;ğŸš€ Docs is easy to install on your own servers&lt;/h4&gt; 
&lt;p&gt;We use Kubernetes for our &lt;a href="https://docs.numerique.gouv.fr/"&gt;production instance&lt;/a&gt; but also support Docker Compose. The community contributed a couple other methods (Nix, YunoHost etc.) check out the &lt;a href="https://raw.githubusercontent.com/suitenumerique/docs/main/docs/installation/README.md"&gt;docs&lt;/a&gt; to get detailed instructions and examples.&lt;/p&gt; 
&lt;h4&gt;ğŸŒ Known instances&lt;/h4&gt; 
&lt;p&gt;We hope to see many more, here is an incomplete list of public Docs instances. Feel free to make a PR to add ones that are not listed belowğŸ™&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Url&lt;/th&gt; 
   &lt;th&gt;Org&lt;/th&gt; 
   &lt;th&gt;Public&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.numerique.gouv.fr/"&gt;docs.numerique.gouv.fr&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;DINUM&lt;/td&gt; 
   &lt;td&gt;French public agents working for the central administration and the extended public sphere. ProConnect is required to login in or sign up&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.suite.anct.gouv.fr/"&gt;docs.suite.anct.gouv.fr&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ANCT&lt;/td&gt; 
   &lt;td&gt;French public agents working for the territorial administration and the extended public sphere. ProConnect is required to login in or sign up&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://notes.demo.opendesk.eu"&gt;notes.demo.opendesk.eu&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ZenDiS&lt;/td&gt; 
   &lt;td&gt;Demo instance of OpenDesk. Request access to get credentials&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://notes.liiib.re/"&gt;notes.liiib.re&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;lasuite.coop&lt;/td&gt; 
   &lt;td&gt;Free and open demo to all. Content and accounts are reset after one month&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.federated.nexus/"&gt;docs.federated.nexus&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;federated.nexus&lt;/td&gt; 
   &lt;td&gt;Public instance, but you have to &lt;a href="https://federated.nexus/register/"&gt;sign up for a Federated Nexus account&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.demo.mosacloud.eu/"&gt;docs.demo.mosacloud.eu&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;mosa.cloud&lt;/td&gt; 
   &lt;td&gt;Demo instance of mosa.cloud, a dutch company providing services around La Suite apps.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;âš ï¸ Advanced features&lt;/h4&gt; 
&lt;p&gt;For some advanced features (ex: Export as PDF) Docs relies on XL packages from BlockNote. These are licenced under GPL and are not MIT compatible. You can perfectly use Docs without these packages by setting the environment variable &lt;code&gt;PUBLISH_AS_MIT&lt;/code&gt; to true. That way you'll build an image of the application without the features that are not MIT compatible. Read the &lt;a href="https://raw.githubusercontent.com/suitenumerique/docs/main/docs/env.md"&gt;environment variables documentation&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Getting started ğŸ”§&lt;/h2&gt; 
&lt;h3&gt;Test it&lt;/h3&gt; 
&lt;p&gt;You can test Docs on your browser by visiting this &lt;a href="https://impress-preprod.beta.numerique.gouv.fr/docs/6ee5aac4-4fb9-457d-95bf-bb56c2467713/"&gt;demo document&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Run Docs locally&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;âš ï¸ The methods described below for running Docs locally is &lt;strong&gt;for testing purposes only&lt;/strong&gt;. It is based on building Docs using &lt;a href="https://min.io/"&gt;Minio&lt;/a&gt; as an S3-compatible storage solution. Of course you can choose any S3-compatible storage solution.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Prerequisite&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Make sure you have a recent version of Docker and &lt;a href="https://docs.docker.com/compose/install"&gt;Docker Compose&lt;/a&gt; installed on your laptop, then type:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shellscript"&gt;$ docker -v

Docker version 20.10.2, build 2291f61

$ docker compose version

Docker Compose version v2.32.4
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;âš ï¸ You may need to run the following commands with &lt;code&gt;sudo&lt;/code&gt;, but this can be avoided by adding your user to the local &lt;code&gt;docker&lt;/code&gt; group.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Project bootstrap&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The easiest way to start working on the project is to use &lt;a href="https://www.gnu.org/software/make/"&gt;GNU Make&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shellscript"&gt;$ make bootstrap FLUSH_ARGS='--no-input'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command builds the &lt;code&gt;app-dev&lt;/code&gt; and &lt;code&gt;frontend-dev&lt;/code&gt; containers, installs dependencies, performs database migrations and compiles translations. It's a good idea to use this command each time you are pulling code from the project repository to avoid dependency-related or migration-related issues.&lt;/p&gt; 
&lt;p&gt;Your Docker services should now be up and running ğŸ‰&lt;/p&gt; 
&lt;p&gt;You can access the project by going to &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You will be prompted to log in. The default credentials are:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;username: impress
password: impress
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ğŸ“ Note that if you need to run them afterwards, you can use the eponymous Make rule:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shellscript"&gt;$ make run
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;âš ï¸ For the frontend developer, it is often better to run the frontend in development mode locally.&lt;/p&gt; 
&lt;p&gt;To do so, install the frontend dependencies with the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shellscript"&gt;$ make frontend-development-install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And run the frontend locally in development mode with the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shellscript"&gt;$ make run-frontend-development
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To start all the services, except the frontend container, you can use the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shellscript"&gt;$ make run-backend
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To execute frontend tests &amp;amp; linting only&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shellscript"&gt;$ make frontend-test
$ make frontend-lint
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Adding content&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You can create a basic demo site by running this command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shellscript"&gt;$ make demo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Finally, you can check all available Make rules using this command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shellscript"&gt;$ make help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Django admin&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You can access the Django admin site at:&lt;/p&gt; 
&lt;p&gt;&lt;a href="http://localhost:8071/admin"&gt;http://localhost:8071/admin&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You first need to create a superuser account:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shellscript"&gt;$ make superuser
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Feedback ğŸ™‹â€â™‚ï¸ğŸ™‹â€â™€ï¸&lt;/h2&gt; 
&lt;p&gt;We'd love to hear your thoughts, and hear about your experiments, so come and say hi on &lt;a href="https://matrix.to/#/#docs-official:matrix.org"&gt;Matrix&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Roadmap ğŸ’¡&lt;/h2&gt; 
&lt;p&gt;Want to know where the project is headed? &lt;a href="https://github.com/orgs/numerique-gouv/projects/13/views/11"&gt;ğŸ—ºï¸ Checkout our roadmap&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License ğŸ“&lt;/h2&gt; 
&lt;p&gt;This work is released under the MIT License (see &lt;a href="https://github.com/suitenumerique/docs/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;While Docs is a public-driven initiative, our license choice is an invitation for private sector actors to use, sell and contribute to the project.&lt;/p&gt; 
&lt;h2&gt;Contributing ğŸ™Œ&lt;/h2&gt; 
&lt;p&gt;This project is intended to be community-driven, so please, do not hesitate to &lt;a href="https://matrix.to/#/#docs-official:matrix.org"&gt;get in touch&lt;/a&gt; if you have any question related to our implementation or design decisions.&lt;/p&gt; 
&lt;p&gt;You can help us with translations on &lt;a href="https://crowdin.com/project/lasuite-docs"&gt;Crowdin&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you intend to make pull requests, see &lt;a href="https://github.com/suitenumerique/docs/raw/main/CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt; for guidelines.&lt;/p&gt; 
&lt;h2&gt;Directory structure:&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-markdown"&gt;docs
â”œâ”€â”€ bin - executable scripts or binaries that are used for various tasks, such as setup scripts, utility scripts, or custom commands.
â”œâ”€â”€ crowdin - for crowdin translations, a tool or service that helps manage translations for the project.
â”œâ”€â”€ docker - Dockerfiles and related configuration files used to build Docker images for the project. These images can be used for development, testing, or production environments.
â”œâ”€â”€ docs - documentation for the project, including user guides, API documentation, and other helpful resources.
â”œâ”€â”€ env.d/development - environment-specific configuration files for the development environment. These files might include environment variables, configuration settings, or other setup files needed for development.
â”œâ”€â”€ gitlint - configuration files for `gitlint`, a tool that enforces commit message guidelines to ensure consistency and quality in commit messages.
â”œâ”€â”€ playground - experimental or temporary code, where developers can test new features or ideas without affecting the main codebase.
â””â”€â”€ src - main source code directory, containing the core application code, libraries, and modules of the project.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Credits â¤ï¸&lt;/h2&gt; 
&lt;h3&gt;Stack&lt;/h3&gt; 
&lt;p&gt;Docs is built on top of &lt;a href="https://www.django-rest-framework.org/"&gt;Django Rest Framework&lt;/a&gt;, &lt;a href="https://nextjs.org/"&gt;Next.js&lt;/a&gt;, &lt;a href="https://www.blocknotejs.org/"&gt;BlockNote.js&lt;/a&gt;, &lt;a href="https://tiptap.dev/docs/hocuspocus/introduction"&gt;HocusPocus&lt;/a&gt; and &lt;a href="https://yjs.dev/"&gt;Yjs&lt;/a&gt;. We thank the contributors of all these projects for their awesome work!&lt;/p&gt; 
&lt;p&gt;We are proud sponsors of &lt;a href="https://www.blocknotejs.org/"&gt;BlockNotejs&lt;/a&gt; and &lt;a href="https://yjs.dev/"&gt;Yjs&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Gov â¤ï¸ open source&lt;/h3&gt; 
&lt;p&gt;Docs is the result of a joint effort led by the French ğŸ‡«ğŸ‡·ğŸ¥– (&lt;a href="https://www.numerique.gouv.fr/dinum/"&gt;DINUM&lt;/a&gt;) and German ğŸ‡©ğŸ‡ªğŸ¥¨ governments (&lt;a href="https://zendis.de/"&gt;ZenDiS&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;We are always looking for new public partners (we are currently onboarding the Netherlands ğŸ‡³ğŸ‡±ğŸ§€), feel free to &lt;a href="mailto:docs@numerique.gouv.fr"&gt;reach out&lt;/a&gt; if you are interested in using or contributing to Docs.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/suitenumerique/docs/main/docs/assets/europe_opensource.png" width="50%" /&gt; &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>jingyaogong/minimind</title>
      <link>https://github.com/jingyaogong/minimind</link>
      <description>&lt;p&gt;ğŸš€ğŸš€ ã€Œå¤§æ¨¡å‹ã€2å°æ—¶å®Œå…¨ä»0è®­ç»ƒ26Mçš„å°å‚æ•°GPTï¼ğŸŒ Train a 26M-parameter GPT from scratch in just 2h!&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/logo.png" alt="logo" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://visitor-badge.laobi.icu/badge?page_id=jingyaogong/minimind" alt="visitors" /&gt; &lt;a href="https://github.com/jingyaogong/minimind/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/jingyaogong/minimind?style=social" alt="GitHub Repo stars" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/jingyaogong/minimind" alt="GitHub Code License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/jingyaogong/minimind/commits/master"&gt;&lt;img src="https://img.shields.io/github/last-commit/jingyaogong/minimind" alt="GitHub last commit" /&gt;&lt;/a&gt; &lt;a href="https://github.com/jingyaogong/minimind/pulls"&gt;&lt;img src="https://img.shields.io/badge/PRs-welcome-blue" alt="GitHub pull request" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97-MiniMind%20%20Collection-blue" alt="Collection" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://trendshift.io/api/badge/repositories/12586" alt="GitHub Trend" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;"å¤§é“è‡³ç®€"&lt;/h3&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;ä¸­æ–‡ | &lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/README_en.md"&gt;English&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt;æ­¤å¼€æºé¡¹ç›®æ—¨åœ¨å®Œå…¨ä»0å¼€å§‹ï¼Œä»…ç”¨3å—é’±æˆæœ¬ + 2å°æ—¶ï¼å³å¯è®­ç»ƒå‡ºä»…ä¸º25.8Mçš„è¶…å°è¯­è¨€æ¨¡å‹&lt;strong&gt;MiniMind&lt;/strong&gt;ã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MiniMind&lt;/strong&gt;ç³»åˆ—æå…¶è½»é‡ï¼Œæœ€å°ç‰ˆæœ¬ä½“ç§¯æ˜¯ GPT-3 çš„ $\frac{1}{7000}$ï¼ŒåŠ›æ±‚åšåˆ°æœ€æ™®é€šçš„ä¸ªäººGPUä¹Ÿå¯å¿«é€Ÿè®­ç»ƒã€‚&lt;/li&gt; 
 &lt;li&gt;é¡¹ç›®åŒæ—¶å¼€æºäº†å¤§æ¨¡å‹çš„æç®€ç»“æ„-åŒ…å«æ‹“å±•å…±äº«æ··åˆä¸“å®¶(MoE)ã€æ•°æ®é›†æ¸…æ´—ã€é¢„è®­ç»ƒ(Pretrain)ã€ç›‘ç£å¾®è°ƒ(SFT)ã€LoRAå¾®è°ƒã€ç›´æ¥åå¥½ä¼˜åŒ–(DPO)ã€å¼ºåŒ–å­¦ä¹ è®­ç»ƒ(RLAIF: PPO/GRPOç­‰)ã€æ¨¡å‹è’¸é¦ç­‰å…¨è¿‡ç¨‹ä»£ç ã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MiniMind&lt;/strong&gt;åŒæ—¶æ‹“å±•äº†è§†è§‰å¤šæ¨¡æ€çš„VLM: &lt;a href="https://github.com/jingyaogong/minimind-v"&gt;MiniMind-V&lt;/a&gt;ã€‚&lt;/li&gt; 
 &lt;li&gt;é¡¹ç›®æ‰€æœ‰æ ¸å¿ƒç®—æ³•ä»£ç å‡ä»0ä½¿ç”¨PyTorchåŸç”Ÿé‡æ„ï¼ä¸ä¾èµ–ç¬¬ä¸‰æ–¹åº“æä¾›çš„æŠ½è±¡æ¥å£ã€‚&lt;/li&gt; 
 &lt;li&gt;è¿™ä¸ä»…æ˜¯å¤§è¯­è¨€æ¨¡å‹çš„å…¨é˜¶æ®µå¼€æºå¤ç°ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªå…¥é—¨LLMçš„æ•™ç¨‹ã€‚&lt;/li&gt; 
 &lt;li&gt;å¸Œæœ›æ­¤é¡¹ç›®èƒ½ä¸ºæ‰€æœ‰äººæä¾›ä¸€ä¸ªæŠ›ç –å¼•ç‰çš„ç¤ºä¾‹ï¼Œä¸€èµ·æ„Ÿå—åˆ›é€ çš„ä¹è¶£ï¼æ¨åŠ¨æ›´å¹¿æ³›AIç¤¾åŒºçš„è¿›æ­¥ï¼&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ä¸ºé˜²æ­¢è¯¯è§£ï¼Œâ€œ2å°æ—¶â€ åŸºäºNVIDIA 3090ç¡¬ä»¶è®¾å¤‡ï¼ˆå•å¡ï¼‰æµ‹è¯•ï¼Œâ€œ3å—é’±â€æŒ‡GPUæœåŠ¡å™¨ç§Ÿç”¨æˆæœ¬ï¼Œå…·ä½“è§„æ ¼è¯¦æƒ…è§ä¸‹æ–‡ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/minimind2.gif" alt="minimind2" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.modelscope.cn/studios/gongjy/MiniMind-Reasoning"&gt;ğŸ”—ğŸ“æ¨ç†æ¨¡å‹&lt;/a&gt; | &lt;a href="https://www.modelscope.cn/studios/gongjy/MiniMind"&gt;ğŸ”—ğŸ¤–å¸¸è§„æ¨¡å‹&lt;/a&gt; | &lt;a href="https://www.bilibili.com/video/BV12dHPeqE72/?share_source=copy_web&amp;amp;vd_source=670c2504f88726f8cf4a21ef6147c0e8"&gt;ğŸ”—ğŸï¸è§†é¢‘ä»‹ç»&lt;/a&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table&gt; 
   &lt;tbody&gt;
    &lt;tr&gt; 
     &lt;td align="center"&gt; &lt;a href="https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5" style="text-decoration: none;"&gt; &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/and_huggingface.png" alt="Hugging Face Logo" style="vertical-align: middle; width: auto; max-width: 100%;" /&gt; &lt;/a&gt; &lt;/td&gt; 
     &lt;td align="center"&gt; &lt;a href="https://www.modelscope.cn/profile/gongjy" style="text-decoration: none;"&gt; &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/and_modelscope.png" alt="ModelScope Logo" style="vertical-align: middle; width: auto; max-width: 100%;" /&gt; &lt;/a&gt; &lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt;
  &lt;/table&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;h1&gt;ğŸ“Œ Introduction&lt;/h1&gt; 
&lt;p&gt;å¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Model, LLMï¼‰çš„å‡ºç°å¼•å‘äº†å…¨ä¸–ç•Œå¯¹AIçš„ç©ºå‰å…³æ³¨ã€‚ æ— è®ºæ˜¯ChatGPTã€DeepSeekè¿˜æ˜¯Qwenï¼Œéƒ½ä»¥å…¶æƒŠè‰³çš„æ•ˆæœä»¤äººå¹ä¸ºè§‚æ­¢ã€‚ ç„¶è€Œï¼ŒåŠ¨è¾„æ•°ç™¾äº¿å‚æ•°çš„åºå¤§è§„æ¨¡ï¼Œä½¿å¾—å®ƒä»¬å¯¹ä¸ªäººè®¾å¤‡è€Œè¨€ä¸ä»…éš¾ä»¥è®­ç»ƒï¼Œç”šè‡³è¿éƒ¨ç½²éƒ½æ˜¾å¾—é¥ä¸å¯åŠã€‚ æ‰“å¼€å¤§æ¨¡å‹çš„â€œé»‘ç›’å­â€ï¼Œæ¢ç´¢å…¶å†…éƒ¨è¿ä½œæœºåˆ¶ï¼Œå¤šä¹ˆä»¤äººå¿ƒæ½®æ¾æ¹ƒï¼ é—æ†¾çš„æ˜¯ï¼Œ99%çš„æ¢ç´¢åªèƒ½æ­¢æ­¥äºä½¿ç”¨LoRAç­‰æŠ€æœ¯å¯¹ç°æœ‰å¤§æ¨¡å‹è¿›è¡Œå°‘é‡å¾®è°ƒï¼Œå­¦ä¹ ä¸€äº›æ–°æŒ‡ä»¤æˆ–ä»»åŠ¡ã€‚ è¿™å°±å¥½æ¯”æ•™ç‰›é¡¿å¦‚ä½•ä½¿ç”¨21ä¸–çºªçš„æ™ºèƒ½æ‰‹æœºâ€”â€”è™½ç„¶æœ‰è¶£ï¼Œå´å®Œå…¨åç¦»äº†ç†è§£ç‰©ç†æœ¬è´¨çš„åˆè¡·ã€‚ ä¸æ­¤åŒæ—¶ï¼Œç¬¬ä¸‰æ–¹çš„å¤§æ¨¡å‹æ¡†æ¶å’Œå·¥å…·åº“ï¼Œå¦‚transformers+trlï¼Œå‡ ä¹åªæš´éœ²äº†é«˜åº¦æŠ½è±¡çš„æ¥å£ã€‚ é€šè¿‡çŸ­çŸ­10è¡Œä»£ç ï¼Œå°±èƒ½å®Œæˆâ€œåŠ è½½æ¨¡å‹+åŠ è½½æ•°æ®é›†+æ¨ç†+å¼ºåŒ–å­¦ä¹ â€çš„å…¨æµç¨‹è®­ç»ƒã€‚ è¿™ç§é«˜æ•ˆçš„å°è£…å›ºç„¶ä¾¿åˆ©ï¼Œä½†ä¹Ÿåƒä¸€æ¶é«˜é€Ÿé£èˆ¹ï¼Œå°†å¼€å‘è€…ä¸åº•å±‚å®ç°éš”ç¦»å¼€æ¥ï¼Œé˜»ç¢äº†æ·±å…¥æ¢ç©¶LLMæ ¸å¿ƒä»£ç çš„æœºä¼šã€‚ ç„¶è€Œï¼Œâ€œç”¨ä¹é«˜æ‹¼å‡ºä¸€æ¶é£æœºï¼Œè¿œæ¯”ååœ¨å¤´ç­‰èˆ±é‡Œé£è¡Œæ›´è®©äººå…´å¥‹ï¼â€ã€‚ æ›´ç³Ÿç³•çš„æ˜¯ï¼Œäº’è”ç½‘ä¸Šå……æ–¥ç€å¤§é‡ä»˜è´¹è¯¾ç¨‹å’Œè¥é”€å·ï¼Œä»¥æ¼æ´ç™¾å‡ºã€ä¸€çŸ¥åŠè§£çš„å†…å®¹æ¨é”€AIæ•™ç¨‹ã€‚ æ­£å› å¦‚æ­¤ï¼Œæœ¬é¡¹ç›®åˆè¡·æ˜¯æ‹‰ä½LLMçš„å­¦ä¹ é—¨æ§›ï¼Œè®©æ¯ä¸ªäººéƒ½èƒ½ä»ç†è§£æ¯ä¸€è¡Œä»£ç å¼€å§‹ï¼Œ ä»é›¶å¼€å§‹äº²æ‰‹è®­ç»ƒä¸€ä¸ªæå°çš„è¯­è¨€æ¨¡å‹ã€‚æ˜¯çš„ï¼Œä»&lt;strong&gt;é›¶å¼€å§‹è®­ç»ƒ&lt;/strong&gt;ï¼Œè€Œä¸æ˜¯ä»…ä»…è¿›è¡Œ&lt;strong&gt;æ¨ç†&lt;/strong&gt;ï¼ æœ€ä½åªéœ€3å—é’±ä¸åˆ°çš„æœåŠ¡å™¨æˆæœ¬ï¼Œå°±èƒ½äº²èº«ä½“éªŒä»0åˆ°1æ„å»ºä¸€ä¸ªè¯­è¨€æ¨¡å‹çš„å…¨è¿‡ç¨‹ã€‚ ä¸€èµ·æ„Ÿå—åˆ›é€ çš„ä¹è¶£å§ï¼&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] ï¼ˆæˆªè‡³2025-10ï¼‰MiniMindç³»åˆ—å·²å®Œæˆå¤šä¸ªå‹å·æ¨¡å‹çš„é¢„è®­ç»ƒï¼Œæœ€å°ä»…éœ€25.8Mï¼ˆ0.02Bï¼‰ï¼Œå³å¯å…·å¤‡æµç•…å¯¹è¯èƒ½åŠ›ï¼&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;Models List&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;æ¨¡å‹ (å¤§å°)&lt;/th&gt; 
    &lt;th&gt;æ¨ç†å ç”¨ (çº¦)&lt;/th&gt; 
    &lt;th&gt;Release&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2-small (26M)&lt;/td&gt; 
    &lt;td&gt;0.5 GB&lt;/td&gt; 
    &lt;td&gt;2025.04.26&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2-MoE (145M)&lt;/td&gt; 
    &lt;td&gt;1.0 GB&lt;/td&gt; 
    &lt;td&gt;2025.04.26&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2 (104M)&lt;/td&gt; 
    &lt;td&gt;1.0 GB&lt;/td&gt; 
    &lt;td&gt;2025.04.26&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1-small (26M)&lt;/td&gt; 
    &lt;td&gt;0.5 GB&lt;/td&gt; 
    &lt;td&gt;2024.08.28&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1-moe (4Ã—26M)&lt;/td&gt; 
    &lt;td&gt;1.0 GB&lt;/td&gt; 
    &lt;td&gt;2024.09.17&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1 (108M)&lt;/td&gt; 
    &lt;td&gt;1.0 GB&lt;/td&gt; 
    &lt;td&gt;2024.09.01&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;strong&gt;é¡¹ç›®åŒ…å«&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;MiniMind-LLMç»“æ„çš„å…¨éƒ¨ä»£ç ï¼ˆDense+MoEæ¨¡å‹ï¼‰ã€‚&lt;/li&gt; 
 &lt;li&gt;åŒ…å«Tokenizeråˆ†è¯å™¨è¯¦ç»†è®­ç»ƒä»£ç ã€‚&lt;/li&gt; 
 &lt;li&gt;åŒ…å«Pretrainã€SFTã€LoRAã€RLHF-DPOã€RLAIF(PPO/GRPO/SPO)ã€æ¨¡å‹è’¸é¦çš„å…¨è¿‡ç¨‹è®­ç»ƒä»£ç ã€‚&lt;/li&gt; 
 &lt;li&gt;æ”¶é›†ã€è’¸é¦ã€æ•´ç†å¹¶æ¸…æ´—å»é‡æ‰€æœ‰é˜¶æ®µçš„é«˜è´¨é‡æ•°æ®é›†ï¼Œä¸”å…¨éƒ¨å¼€æºã€‚&lt;/li&gt; 
 &lt;li&gt;ä»0å®ç°é¢„è®­ç»ƒã€æŒ‡ä»¤å¾®è°ƒã€LoRAã€DPO/PPO/GRPO/SPOå¼ºåŒ–å­¦ä¹ ï¼Œç™½ç›’æ¨¡å‹è’¸é¦ã€‚å…³é”®ç®—æ³•å‡ ä¹ä¸ä¾èµ–ç¬¬ä¸‰æ–¹å°è£…çš„æ¡†æ¶ï¼Œä¸”å…¨éƒ¨å¼€æºã€‚&lt;/li&gt; 
 &lt;li&gt;åŒæ—¶å…¼å®¹&lt;code&gt;transformers&lt;/code&gt;ã€&lt;code&gt;trl&lt;/code&gt;ã€&lt;code&gt;peft&lt;/code&gt;ç­‰ç¬¬ä¸‰æ–¹ä¸»æµæ¡†æ¶ã€‚&lt;/li&gt; 
 &lt;li&gt;è®­ç»ƒæ”¯æŒå•æœºå•å¡ã€å•æœºå¤šå¡(DDPã€DeepSpeed)è®­ç»ƒï¼Œæ”¯æŒwandb/swanlabå¯è§†åŒ–è®­ç»ƒæµç¨‹ã€‚æ”¯æŒåŠ¨æ€å¯åœè®­ç»ƒã€‚&lt;/li&gt; 
 &lt;li&gt;åœ¨ç¬¬ä¸‰æ–¹æµ‹è¯„æ¦œï¼ˆC-Evalã€C-MMLUã€OpenBookQAç­‰ï¼‰è¿›è¡Œæ¨¡å‹æµ‹è¯•ï¼Œæ”¯æŒYaRNç®—æ³•æ‰§è¡ŒRoPEé•¿æ–‡æœ¬å¤–æ¨ã€‚&lt;/li&gt; 
 &lt;li&gt;å®ç°Openai-Apiåè®®çš„æç®€æœåŠ¡ç«¯ï¼Œä¾¿äºé›†æˆåˆ°ç¬¬ä¸‰æ–¹ChatUIä½¿ç”¨ï¼ˆFastGPTã€Open-WebUIç­‰ï¼‰ã€‚&lt;/li&gt; 
 &lt;li&gt;åŸºäºstreamlitå®ç°æœ€ç®€èŠå¤©WebUIå‰ç«¯ã€‚&lt;/li&gt; 
 &lt;li&gt;å…¨é¢å…¼å®¹ç¤¾åŒºçƒ­é—¨&lt;code&gt;llama.cpp&lt;/code&gt;ã€&lt;code&gt;vllm&lt;/code&gt;ã€&lt;code&gt;ollama&lt;/code&gt;æ¨ç†å¼•æ“æˆ–&lt;code&gt;Llama-Factory&lt;/code&gt;è®­ç»ƒæ¡†æ¶ã€‚&lt;/li&gt; 
 &lt;li&gt;å¤ç°(è’¸é¦/RL)å¤§å‹æ¨ç†æ¨¡å‹DeepSeek-R1çš„MiniMind-Reasonæ¨¡å‹ï¼Œ&lt;strong&gt;æ•°æ®+æ¨¡å‹&lt;/strong&gt;å…¨éƒ¨å¼€æºï¼&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;å¸Œæœ›æ­¤å¼€æºé¡¹ç›®å¯ä»¥å¸®åŠ©LLMåˆå­¦è€…å¿«é€Ÿå…¥é—¨ï¼&lt;/p&gt; 
&lt;h3&gt;ğŸ‘‰&lt;strong&gt;æ›´æ–°æ—¥å¿—&lt;/strong&gt;&lt;/h3&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2025-10-24&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ğŸ”¥ æ–°å¢RLAIFè®­ç»ƒç®—æ³•ï¼šPPOã€GRPOã€SPOï¼ˆä»0åŸç”Ÿå®ç°ï¼‰&lt;/li&gt; 
  &lt;li&gt;æ–°å¢æ–­ç‚¹ç»­è®­åŠŸèƒ½ï¼šæ”¯æŒè®­ç»ƒè‡ªåŠ¨æ¢å¤ã€è·¨GPUæ•°é‡æ¢å¤ã€wandbè®°å½•è¿ç»­æ€§&lt;/li&gt; 
  &lt;li&gt;æ–°å¢RLAIFæ•°æ®é›†ï¼šrlaif-mini.jsonlï¼ˆä»SFTæ•°æ®éšæœºé‡‡æ ·1ä¸‡æ¡ï¼‰ï¼›ç®€åŒ–DPOæ•°æ®é›†ï¼ŒåŠ å…¥ä¸­æ–‡æ•°æ®&lt;/li&gt; 
  &lt;li&gt;æ–°å¢YaRNç®—æ³•ï¼šæ”¯æŒRoPEé•¿æ–‡æœ¬å¤–æ¨ï¼Œæå‡é•¿åºåˆ—å¤„ç†èƒ½åŠ›&lt;/li&gt; 
  &lt;li&gt;Adaptive Thinkingï¼šReasonæ¨¡å‹å¯é€‰æ˜¯å¦å¯ç”¨æ€è€ƒé“¾&lt;/li&gt; 
  &lt;li&gt;chat_templateå…¨é¢æ”¯æŒTool Callingå’ŒReasoningæ ‡ç­¾ï¼ˆ&lt;code&gt;&amp;lt;tool_call&amp;gt;&lt;/code&gt;ã€&lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt;ç­‰ï¼‰&lt;/li&gt; 
  &lt;li&gt;æ–°å¢RLAIFå®Œæ•´ç« èŠ‚ã€è®­ç»ƒæ›²çº¿å¯¹æ¯”ã€ç®—æ³•åŸç†æŠ˜å è¯´æ˜&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://swanlab.cn/"&gt;SwanLab&lt;/a&gt;æ›¿ä»£WandBï¼ˆå›½å†…è®¿é—®å‹å¥½ï¼ŒAPIå®Œå…¨å…¼å®¹ï¼‰&lt;/li&gt; 
  &lt;li&gt;è§„èŒƒåŒ–æ‰€æœ‰ä»£ç  &amp;amp; ä¿®å¤ä¸€äº›å·²çŸ¥bugs&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2025-04-26&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;é‡è¦æ›´æ–°&lt;/li&gt; 
  &lt;li&gt;å¦‚æœ‰å…¼å®¹æ€§éœ€è¦ï¼Œå¯è®¿é—®&lt;a href="https://github.com/jingyaogong/minimind/tree/7da201a944a90ed49daef8a0265c959288dff83a"&gt;ğŸ”—æ—§ä»“åº“å†…å®¹ğŸ”—&lt;/a&gt;ã€‚&lt;/li&gt; 
  &lt;li&gt;MiniMindæ¨¡å‹å‚æ•°å®Œå…¨æ”¹åï¼Œå¯¹é½Transformersåº“æ¨¡å‹ï¼ˆç»Ÿä¸€å‘½åï¼‰ã€‚&lt;/li&gt; 
  &lt;li&gt;generateæ–¹å¼é‡æ„ï¼Œç»§æ‰¿è‡ªGenerationMixinç±»ã€‚&lt;/li&gt; 
  &lt;li&gt;ğŸ”¥æ”¯æŒllama.cppã€vllmã€ollamaç­‰çƒ­é—¨ä¸‰æ–¹ç”Ÿæ€ã€‚&lt;/li&gt; 
  &lt;li&gt;è§„èŒƒä»£ç å’Œç›®å½•ç»“æ„ã€‚&lt;/li&gt; 
  &lt;li&gt;æ”¹åŠ¨è¯è¡¨&lt;code&gt;&amp;lt;s&amp;gt;&amp;lt;/s&amp;gt;&lt;/code&gt;-&amp;gt;&lt;code&gt;&amp;lt;|im_start|&amp;gt;&amp;lt;|im_end|&amp;gt;&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-text"&gt;ä¸ºå…¼å®¹ç¬¬ä¸‰æ–¹æ¨ç†æ¡†æ¶llama.cppã€vllmï¼Œæœ¬æ¬¡æ›´æ–°éœ€ä»˜å‡ºä¸€äº›å¯è§‚ä»£ä»·ã€‚
æœ¬æ¬¡æ›´æ–°ä¸å†æ”¯æŒã€Œç›´æ¥ã€åŠ è½½25-04-26ä»¥å‰çš„æ—§æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚
ç”±äºLlamaä½ç½®ç¼–ç æ–¹å¼ä¸minimindå­˜åœ¨åŒºåˆ«ï¼Œå¯¼è‡´æ˜ å°„Llamaæ¨¡å‹åQKå€¼å­˜åœ¨å·®å¼‚
MiniMind2ç³»åˆ—æ—§æ¨¡å‹å‡ç»è¿‡æƒé‡æ˜ å°„+ï¼ˆå¾®è°ƒè®­ç»ƒï¼‰QKVOçº¿æ€§å±‚æ ¡å‡†æ¢å¤è€Œæ¥ã€‚
æœ¬æ¬¡æ›´æ–°åå°†æ”¾å¼ƒå¯¹`minimind-v1`å…¨ç³»åˆ—çš„ç»´æŠ¤ï¼Œå¹¶åœ¨ä»“åº“ä¸­ä¸‹çº¿ã€‚
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2025-02-09&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;è¿æ¥å‘å¸ƒä»¥æ¥é‡å¤§æ›´æ–°ï¼ŒRelease MiniMind2 Seriesã€‚&lt;/li&gt; 
  &lt;li&gt;ä»£ç å‡ ä¹å…¨éƒ¨é‡æ„ï¼Œä½¿ç”¨æ›´ç®€æ´æ˜äº†çš„ç»Ÿä¸€ç»“æ„ã€‚ å¦‚æœ‰æ—§ä»£ç çš„å…¼å®¹æ€§éœ€è¦ï¼Œå¯è®¿é—®&lt;a href="https://github.com/jingyaogong/minimind/tree/6e9cd28ef9b34a0a10afbdf6f59e65cb6e628efb"&gt;ğŸ”—æ—§ä»“åº“å†…å®¹ğŸ”—&lt;/a&gt;ã€‚&lt;/li&gt; 
  &lt;li&gt;å…å»æ•°æ®é¢„å¤„ç†æ­¥éª¤ã€‚ç»Ÿä¸€æ•°æ®é›†æ ¼å¼ï¼Œæ›´æ¢ä¸º&lt;code&gt;jsonl&lt;/code&gt;æ ¼å¼æœç»æ•°æ®é›†ä¸‹è½½æ··ä¹±çš„é—®é¢˜ã€‚&lt;/li&gt; 
  &lt;li&gt;MiniMind2ç³»åˆ—æ•ˆæœç›¸æ¯”MiniMind-V1æ˜¾è‘—æå‡ã€‚&lt;/li&gt; 
  &lt;li&gt;å°é—®é¢˜ï¼š{kv-cacheå†™æ³•æ›´æ ‡å‡†ã€MoEçš„è´Ÿè½½å‡è¡¡lossè¢«è€ƒè™‘ç­‰ç­‰}&lt;/li&gt; 
  &lt;li&gt;æä¾›æ¨¡å‹è¿ç§»åˆ°ç§æœ‰æ•°æ®é›†çš„è®­ç»ƒæ–¹æ¡ˆï¼ˆåŒ»ç–—æ¨¡å‹ã€è‡ªæˆ‘è®¤çŸ¥æ ·ä¾‹ï¼‰ã€‚&lt;/li&gt; 
  &lt;li&gt;ç²¾ç®€é¢„è®­ç»ƒæ•°æ®é›†ï¼Œå¹¶å¤§å¹…æå‡é¢„è®­ç»ƒæ•°æ®è´¨é‡ï¼Œå¤§å¹…ç¼©çŸ­ä¸ªäººå¿«é€Ÿè®­ç»ƒæ‰€éœ€æ—¶é—´ï¼Œå•å¡3090å³å¯2å°æ—¶å¤ç°ï¼&lt;/li&gt; 
  &lt;li&gt;æ›´æ–°ï¼šLoRAå¾®è°ƒè„±ç¦»peftåŒ…è£…ï¼Œä»0å®ç°LoRAè¿‡ç¨‹ï¼›DPOç®—æ³•ä»0ä½¿ç”¨PyTorchåŸç”Ÿå®ç°ï¼›æ¨¡å‹ç™½ç›’è’¸é¦åŸç”Ÿå®ç°ã€‚&lt;/li&gt; 
  &lt;li&gt;MiniMind2-DeepSeek-R1ç³»åˆ—è’¸é¦æ¨¡å‹è¯ç”Ÿï¼&lt;/li&gt; 
  &lt;li&gt;MiniMind2å…·å¤‡ä¸€å®šçš„è‹±æ–‡èƒ½åŠ›ï¼&lt;/li&gt; 
  &lt;li&gt;æ›´æ–°MiniMind2ä¸ç¬¬ä¸‰æ–¹æ¨¡å‹çš„åŸºäºæ›´å¤šå¤§æ¨¡å‹æ¦œå•æµ‹è¯•æ€§èƒ½çš„ç»“æœã€‚&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;More...&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;2024-10-05&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ä¸ºMiniMindæ‹“å±•äº†å¤šæ¨¡æ€èƒ½åŠ›ä¹‹---è§†è§‰&lt;/li&gt; 
  &lt;li&gt;ç§»æ­¥å­ªç”Ÿé¡¹ç›®&lt;a href="https://github.com/jingyaogong/minimind-v"&gt;minimind-v&lt;/a&gt;æŸ¥çœ‹è¯¦æƒ…ï¼&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;2024-09-27&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;09-27æ›´æ–°pretrainæ•°æ®é›†çš„é¢„å¤„ç†æ–¹å¼ï¼Œä¸ºäº†ä¿è¯æ–‡æœ¬å®Œæ•´æ€§ï¼Œæ”¾å¼ƒé¢„å¤„ç†æˆ.binè®­ç»ƒçš„å½¢å¼ï¼ˆè½»å¾®ç‰ºç‰²è®­ç»ƒé€Ÿåº¦ï¼‰ã€‚&lt;/li&gt; 
  &lt;li&gt;ç›®å‰pretrainé¢„å¤„ç†åçš„æ–‡ä»¶å‘½åä¸ºï¼špretrain_data.csvã€‚&lt;/li&gt; 
  &lt;li&gt;åˆ é™¤äº†ä¸€äº›å†—ä½™çš„ä»£ç ã€‚&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;2024-09-17&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;æ›´æ–°minimind-v1-moeæ¨¡å‹&lt;/li&gt; 
  &lt;li&gt;ä¸ºäº†é˜²æ­¢æ­§ä¹‰ï¼Œä¸å†ä½¿ç”¨mistral_tokenizeråˆ†è¯ï¼Œå…¨éƒ¨é‡‡ç”¨è‡ªå®šä¹‰çš„minimind_tokenizerä½œä¸ºåˆ†è¯å™¨ã€‚&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;2024-09-01&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;æ›´æ–°minimind-v1 (108M)æ¨¡å‹ï¼Œé‡‡ç”¨minimind_tokenizerï¼Œé¢„è®­ç»ƒè½®æ¬¡3 + SFTè½®æ¬¡10ï¼Œæ›´å……åˆ†è®­ç»ƒï¼Œæ€§èƒ½æ›´å¼ºã€‚&lt;/li&gt; 
  &lt;li&gt;é¡¹ç›®å·²éƒ¨ç½²è‡³ModelScopeåˆ›ç©ºé—´ï¼Œå¯ä»¥åœ¨æ­¤ç½‘ç«™ä¸Šä½“éªŒï¼š&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://www.modelscope.cn/studios/gongjy/minimind"&gt;ğŸ”—ModelScopeåœ¨çº¿ä½“éªŒğŸ”—&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;2024-08-27&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;é¡¹ç›®é¦–æ¬¡å¼€æº&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h1&gt;ğŸ“Œ å¿«é€Ÿå¼€å§‹&lt;/h1&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;åˆ†äº«æœ¬äººçš„è½¯ç¡¬ä»¶é…ç½®ï¼ˆä»…ä¾›å‚è€ƒï¼‰&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;CPU: Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz&lt;/li&gt; 
  &lt;li&gt;RAM: 128 GB&lt;/li&gt; 
  &lt;li&gt;GPU: NVIDIA GeForce RTX 3090(24GB) * 8&lt;/li&gt; 
  &lt;li&gt;Ubuntu==20.04&lt;/li&gt; 
  &lt;li&gt;CUDA==12.2&lt;/li&gt; 
  &lt;li&gt;Python==3.10.16&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/requirements.txt"&gt;requirements.txt&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;ç¬¬0æ­¥&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/jingyaogong/minimind.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;â…  æµ‹è¯•å·²æœ‰æ¨¡å‹æ•ˆæœ&lt;/h2&gt; 
&lt;h3&gt;1.ç¯å¢ƒå‡†å¤‡&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2.ä¸‹è½½æ¨¡å‹&lt;/h3&gt; 
&lt;p&gt;åˆ°é¡¹ç›®æ ¹ç›®å½•&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://huggingface.co/jingyaogong/MiniMind2 # or https://www.modelscope.cn/models/gongjy/MiniMind2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ï¼ˆå¯é€‰ï¼‰å‘½ä»¤è¡Œé—®ç­”&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ä½¿ç”¨transformersæ ¼å¼æ¨¡å‹
python eval_llm.py --load_from ./MiniMind2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ï¼ˆå¯é€‰ï¼‰å¯åŠ¨WebUI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# å¯èƒ½éœ€è¦`python&amp;gt;=3.10` å®‰è£… `pip install streamlit`
# cd scripts
streamlit run web_demo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ï¼ˆå¯é€‰ï¼‰ç¬¬ä¸‰æ–¹æ¨ç†æ¡†æ¶&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ollama
ollama run jingyaogong/minimind2
# vllm
vllm serve ./MiniMind2/ --served-model-name "minimind"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;â…¡ ä»0å¼€å§‹è‡ªå·±è®­ç»ƒ&lt;/h2&gt; 
&lt;h3&gt;1.ç¯å¢ƒå‡†å¤‡&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;æ³¨ï¼šæå‰æµ‹è¯•Torchæ˜¯å¦å¯ç”¨cuda&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;import torch
print(torch.cuda.is_available())
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;å¦‚æœä¸å¯ç”¨ï¼Œè¯·è‡ªè¡Œå»&lt;a href="https://download.pytorch.org/whl/torch_stable.html"&gt;torch_stable&lt;/a&gt; ä¸‹è½½whlæ–‡ä»¶å®‰è£…ã€‚å‚è€ƒ&lt;a href="https://blog.csdn.net/weixin_45456738/article/details/141029610?ops_request_misc=&amp;amp;request_id=&amp;amp;biz_id=102&amp;amp;utm_term=%E5%AE%89%E8%A3%85torch&amp;amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-141029610.nonecase&amp;amp;spm=1018.2226.3001.4187"&gt;é“¾æ¥&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;2.æ•°æ®ä¸‹è½½&lt;/h3&gt; 
&lt;p&gt;ä»ä¸‹æ–‡æä¾›çš„&lt;a href="https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files"&gt;æ•°æ®é›†ä¸‹è½½é“¾æ¥&lt;/a&gt; ä¸‹è½½éœ€è¦çš„æ•°æ®æ–‡ä»¶ï¼ˆåˆ›å»º&lt;code&gt;./dataset&lt;/code&gt;ç›®å½•ï¼‰å¹¶æ”¾åˆ°&lt;code&gt;./dataset&lt;/code&gt;ä¸‹&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;æ³¨ï¼šæ•°æ®é›†é¡»çŸ¥&lt;/summary&gt; 
 &lt;p&gt;é»˜è®¤æ¨èä¸‹è½½&lt;code&gt;pretrain_hq.jsonl&lt;/code&gt; + &lt;code&gt;sft_mini_512.jsonl&lt;/code&gt;æœ€å¿«é€Ÿåº¦å¤ç°ZeroèŠå¤©æ¨¡å‹ã€‚&lt;/p&gt; 
 &lt;p&gt;æ•°æ®æ–‡ä»¶å¯è‡ªç”±é€‰æ‹©ï¼Œä¸‹æ–‡æä¾›äº†å¤šç§æ­é…æ–¹æ¡ˆï¼Œå¯æ ¹æ®è‡ªå·±æ‰‹å¤´çš„è®­ç»ƒéœ€æ±‚å’ŒGPUèµ„æºè¿›è¡Œé€‚å½“ç»„åˆã€‚&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;3.å¼€å§‹è®­ç»ƒ&lt;/h3&gt; 
&lt;p&gt;ç›®å½•ä½äº&lt;code&gt;trainer&lt;/code&gt;&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;ğŸ’¡ æ£€æŸ¥ç‚¹æš‚åœç»­è®­&lt;/summary&gt; 
 &lt;p&gt;æ‰€æœ‰è®­ç»ƒè„šæœ¬å‡è‡ªåŠ¨ä¿å­˜æ£€æŸ¥ç‚¹ï¼Œåªéœ€æ·»åŠ  &lt;code&gt;--from_resume 1&lt;/code&gt; å‚æ•°å³å¯è‡ªåŠ¨æ£€æµ‹åŠ è½½&amp;amp;æ¢å¤è®­ç»ƒï¼š&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python train_pretrain.py --from_resume 1
python train_full_sft.py --from_resume 1
...
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;æ–­ç‚¹ç»­è®­æœºåˆ¶è¯´æ˜ï¼š&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;è®­ç»ƒè¿‡ç¨‹è‡ªåŠ¨åœ¨ &lt;code&gt;./checkpoints/&lt;/code&gt; ç›®å½•ä¿å­˜å®Œæ•´æ£€æŸ¥ç‚¹ï¼ˆæ¨¡å‹ã€ä¼˜åŒ–å™¨ã€è®­ç»ƒè¿›åº¦ç­‰ï¼‰&lt;/li&gt; 
  &lt;li&gt;æ£€æŸ¥ç‚¹æ–‡ä»¶å‘½åï¼š&lt;code&gt;&amp;lt;æƒé‡å&amp;gt;_&amp;lt;ç»´åº¦&amp;gt;_resume.pth&lt;/code&gt;ï¼ˆå¦‚ï¼š&lt;code&gt;full_sft_512_resume.pth&lt;/code&gt;ï¼‰&lt;/li&gt; 
  &lt;li&gt;æ”¯æŒè·¨ä¸åŒGPUæ•°é‡æ¢å¤ï¼ˆè‡ªåŠ¨è°ƒæ•´stepï¼‰&lt;/li&gt; 
  &lt;li&gt;æ”¯æŒwandbè®­ç»ƒè®°å½•è¿ç»­æ€§ï¼ˆè‡ªåŠ¨æ¢å¤åŒä¸€ä¸ªrunï¼‰&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;é€‚åˆé•¿æ—¶é—´è®­ç»ƒæˆ–ä¸ç¨³å®šç¯å¢ƒï¼Œæ— éœ€æ‹…å¿ƒè®­ç»ƒä¸­æ–­å¯¼è‡´è¿›åº¦ä¸¢å¤±&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;strong&gt;3.1 é¢„è®­ç»ƒï¼ˆå­¦çŸ¥è¯†ï¼‰&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python train_pretrain.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ‰§è¡Œé¢„è®­ç»ƒï¼Œå¾—åˆ° &lt;code&gt;pretrain_*.pth&lt;/code&gt; ä½œä¸ºé¢„è®­ç»ƒçš„è¾“å‡ºæƒé‡ï¼ˆå…¶ä¸­*ä¸ºæ¨¡å‹çš„dimensionï¼Œé»˜è®¤ä¸º512ï¼‰&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;3.2 ç›‘ç£å¾®è°ƒï¼ˆå­¦å¯¹è¯æ–¹å¼ï¼‰&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python train_full_sft.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ‰§è¡Œç›‘ç£å¾®è°ƒï¼Œå¾—åˆ° &lt;code&gt;full_sft_*.pth&lt;/code&gt; ä½œä¸ºæŒ‡ä»¤å¾®è°ƒçš„è¾“å‡ºæƒé‡ï¼ˆå…¶ä¸­&lt;code&gt;full&lt;/code&gt;å³ä¸ºå…¨å‚æ•°å¾®è°ƒï¼‰&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;æ³¨ï¼šè®­ç»ƒé¡»çŸ¥&lt;/summary&gt; 
 &lt;p&gt;æ‰€æœ‰è®­ç»ƒè¿‡ç¨‹é»˜è®¤æ¯éš”100æ­¥ä¿å­˜1æ¬¡å‚æ•°åˆ°æ–‡ä»¶&lt;code&gt;./out/***.pth&lt;/code&gt;ï¼ˆæ¯æ¬¡ä¼šè¦†ç›–æ‰æ—§æƒé‡æ–‡ä»¶ï¼‰ã€‚&lt;/p&gt; 
 &lt;p&gt;ç®€å•èµ·è§ï¼Œæ­¤å¤„åªå†™æ˜ä¸¤ä¸ªé˜¶æ®µè®­ç»ƒè¿‡ç¨‹ã€‚å¦‚éœ€å…¶å®ƒè®­ç»ƒ (LoRA, è’¸é¦, å¼ºåŒ–å­¦ä¹ , å¾®è°ƒæ¨ç†ç­‰) å¯å‚è€ƒä¸‹æ–‡ã€å®éªŒã€‘å°èŠ‚çš„è¯¦ç»†è¯´æ˜ã€‚&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h3&gt;4.æµ‹è¯•è‡ªå·±è®­ç»ƒçš„æ¨¡å‹æ•ˆæœ&lt;/h3&gt; 
&lt;p&gt;ç¡®ä¿éœ€è¦æµ‹è¯•çš„æ¨¡å‹&lt;code&gt;*.pth&lt;/code&gt;æ–‡ä»¶ä½äº&lt;code&gt;./out/&lt;/code&gt;ç›®å½•ä¸‹ã€‚ ä¹Ÿå¯ä»¥ç›´æ¥å»&lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch/files"&gt;æ­¤å¤„&lt;/a&gt;ä¸‹è½½ä½¿ç”¨æˆ‘è®­ç»ƒçš„&lt;code&gt;*.pth&lt;/code&gt;æ–‡ä»¶ã€‚&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python eval_llm.py --weight full_sft # æˆ– pretrain/dpo/ppo/grpo...
&lt;/code&gt;&lt;/pre&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;æ³¨ï¼šæµ‹è¯•é¡»çŸ¥&lt;/summary&gt; 
 &lt;p&gt;&lt;code&gt;--weight&lt;/code&gt; å‚æ•°æŒ‡å®šæƒé‡åç§°å‰ç¼€ï¼Œå¯é€‰ï¼š&lt;code&gt;pretrain&lt;/code&gt;, &lt;code&gt;full_sft&lt;/code&gt;, &lt;code&gt;dpo&lt;/code&gt;, &lt;code&gt;reason&lt;/code&gt;, &lt;code&gt;ppo_actor&lt;/code&gt;, &lt;code&gt;grpo&lt;/code&gt;, &lt;code&gt;spo&lt;/code&gt; ç­‰&lt;/p&gt; 
 &lt;p&gt;å…¶ä»–å¸¸ç”¨å‚æ•°ï¼š&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;--load_from&lt;/code&gt;: æ¨¡å‹åŠ è½½è·¯å¾„ï¼ˆ&lt;code&gt;model&lt;/code&gt;=åŸç”Ÿtorchæƒé‡ï¼Œå…¶ä»–è·¯å¾„=transformersæ ¼å¼ï¼‰&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;--save_dir&lt;/code&gt;: æ¨¡å‹æƒé‡ç›®å½•ï¼ˆé»˜è®¤&lt;code&gt;out&lt;/code&gt;ï¼‰&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;--lora_weight&lt;/code&gt;: LoRAæƒé‡åç§°ï¼ˆ&lt;code&gt;None&lt;/code&gt;è¡¨ç¤ºä¸ä½¿ç”¨ï¼‰&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;--historys&lt;/code&gt;: æºå¸¦å†å²å¯¹è¯è½®æ•°ï¼ˆéœ€ä¸ºå¶æ•°ï¼Œ0è¡¨ç¤ºä¸æºå¸¦å†å²ï¼‰&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;--max_new_tokens&lt;/code&gt;: æœ€å¤§ç”Ÿæˆé•¿åº¦ï¼ˆé»˜è®¤8192ï¼‰&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;--temperature&lt;/code&gt;: ç”Ÿæˆæ¸©åº¦ï¼ˆé»˜è®¤0.85ï¼‰&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;--top_p&lt;/code&gt;: nucleusé‡‡æ ·é˜ˆå€¼ï¼ˆé»˜è®¤0.85ï¼‰&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;ä½¿ç”¨æ–¹å¼ç›´æ¥æŸ¥çœ‹&lt;code&gt;eval_llm.py&lt;/code&gt;ä»£ç å³å¯ã€‚&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] æ‰€æœ‰è®­ç»ƒè„šæœ¬å‡ä¸ºPytorchåŸç”Ÿæ¡†æ¶ï¼Œå‡æ”¯æŒå¤šå¡åŠ é€Ÿï¼Œå‡è®¾ä½ çš„è®¾å¤‡æœ‰N (Nï¼1) å¼ æ˜¾å¡ï¼š&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;å•æœºNå¡å¯åŠ¨è®­ç»ƒæ–¹å¼ (DDP, æ”¯æŒå¤šæœºå¤šå¡é›†ç¾¤)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node N train_xxx.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;æ³¨ï¼šå…¶å®ƒé¡»çŸ¥&lt;/summary&gt; 
 &lt;del&gt; å•æœºNå¡å¯åŠ¨è®­ç»ƒ (DeepSpeed) &lt;pre&gt;&lt;code class="language-bash"&gt;deepspeed --master_port 29500 --num_gpus=N train_xxx.py
&lt;/code&gt;&lt;/pre&gt; &lt;/del&gt; 
 &lt;p&gt;å¯æ ¹æ®éœ€è¦å¼€å¯wandbè®°å½•è®­ç»ƒè¿‡ç¨‹ï¼ˆéœ€å¯ç›´è¿ï¼‰&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# éœ€è¦ç™»å½•: wandb login
torchrun --nproc_per_node N train_xxx.py --use_wandb
# and
python train_xxx.py --use_wandb
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;é€šè¿‡æ·»åŠ &lt;code&gt;--use_wandb&lt;/code&gt;å‚æ•°ï¼Œå¯ä»¥è®°å½•è®­ç»ƒè¿‡ç¨‹ï¼Œè®­ç»ƒå®Œæˆåï¼Œå¯ä»¥åœ¨wandbç½‘ç«™ä¸ŠæŸ¥çœ‹è®­ç»ƒè¿‡ç¨‹ã€‚é€šè¿‡ä¿®æ”¹&lt;code&gt;wandb_project&lt;/code&gt; å’Œ&lt;code&gt;wandb_run_name&lt;/code&gt;å‚æ•°ï¼Œå¯ä»¥æŒ‡å®šé¡¹ç›®åç§°å’Œè¿è¡Œåç§°ã€‚&lt;/p&gt; 
 &lt;p&gt;ã€æ³¨ã€‘ï¼š25å¹´6æœˆåï¼Œå›½å†…ç½‘ç»œç¯å¢ƒæ— æ³•ç›´è¿WandBï¼ŒMiniMindé¡¹ç›®é»˜è®¤è½¬ä¸ºä½¿ç”¨&lt;a href="https://swanlab.cn/"&gt;SwanLab&lt;/a&gt;ä½œä¸ºè®­ç»ƒå¯è§†åŒ–å·¥å…·ï¼ˆå®Œå…¨å…¼å®¹WandB APIï¼‰ï¼Œå³&lt;code&gt;import wandb&lt;/code&gt;æ”¹ä¸º&lt;code&gt;import swanlab as wandb&lt;/code&gt;å³å¯ï¼Œå…¶ä»–å‡æ— éœ€æ”¹åŠ¨ã€‚&lt;/p&gt; 
&lt;/details&gt; 
&lt;h1&gt;ğŸ“Œ æ•°æ®ä»‹ç»&lt;/h1&gt; 
&lt;h2&gt;â…  Tokenizer&lt;/h2&gt; 
&lt;p&gt;åˆ†è¯å™¨å°†å•è¯ä»è‡ªç„¶è¯­è¨€é€šè¿‡â€œè¯å…¸â€æ˜ å°„åˆ°&lt;code&gt;0, 1, 36&lt;/code&gt;è¿™æ ·çš„æ•°å­—ï¼Œå¯ä»¥ç†è§£ä¸ºæ•°å­—å°±ä»£è¡¨äº†å•è¯åœ¨â€œè¯å…¸â€ä¸­çš„é¡µç ã€‚ å¯ä»¥é€‰æ‹©è‡ªå·±æ„é€ è¯è¡¨è®­ç»ƒä¸€ä¸ªâ€œè¯å…¸â€ï¼Œä»£ç å¯è§&lt;code&gt;./scripts/train_tokenizer.py&lt;/code&gt;ï¼ˆä»…ä¾›å­¦ä¹ å‚è€ƒï¼Œè‹¥éå¿…è¦æ— éœ€å†è‡ªè¡Œè®­ç»ƒï¼ŒMiniMindå·²è‡ªå¸¦tokenizerï¼‰ã€‚ æˆ–è€…é€‰æ‹©æ¯”è¾ƒå‡ºåçš„å¼€æºå¤§æ¨¡å‹åˆ†è¯å™¨ï¼Œ æ­£å¦‚åŒç›´æ¥ç”¨æ–°å/ç‰›æ´¥è¯å…¸çš„ä¼˜ç‚¹æ˜¯tokenç¼–ç å‹ç¼©ç‡å¾ˆå¥½ï¼Œç¼ºç‚¹æ˜¯é¡µæ•°å¤ªå¤šï¼ŒåŠ¨è¾„æ•°åä¸‡ä¸ªè¯æ±‡çŸ­è¯­ï¼› è‡ªå·±è®­ç»ƒçš„åˆ†è¯å™¨ï¼Œä¼˜ç‚¹æ˜¯è¯è¡¨é•¿åº¦å’Œå†…å®¹éšæ„æ§åˆ¶ï¼Œç¼ºç‚¹æ˜¯å‹ç¼©ç‡å¾ˆä½ï¼ˆä¾‹å¦‚"hello"ä¹Ÿè®¸ä¼šè¢«æ‹†åˆ†ä¸º"h e l l o" äº”ä¸ªç‹¬ç«‹çš„tokenï¼‰ï¼Œä¸”ç”Ÿåƒ»è¯éš¾ä»¥è¦†ç›–ã€‚ â€œè¯å…¸â€çš„é€‰æ‹©å›ºç„¶å¾ˆé‡è¦ï¼ŒLLMçš„è¾“å‡ºæœ¬è´¨ä¸Šæ˜¯SoftMaxåˆ°è¯å…¸Nä¸ªè¯çš„å¤šåˆ†ç±»é—®é¢˜ï¼Œç„¶åé€šè¿‡â€œè¯å…¸â€è§£ç åˆ°è‡ªç„¶è¯­è¨€ã€‚ å› ä¸ºMiniMindä½“ç§¯éœ€è¦ä¸¥æ ¼æ§åˆ¶ï¼Œä¸ºäº†é¿å…æ¨¡å‹å¤´é‡è„šè½»ï¼ˆè¯åµŒå…¥embeddingå±‚å‚æ•°åœ¨LLMå æ¯”å¤ªé«˜ï¼‰ï¼Œæ‰€ä»¥è¯è¡¨é•¿åº¦çŸ­çŸ­ç›Šå–„ã€‚&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;Tokenizerä»‹ç»&lt;/summary&gt; 
 &lt;p&gt;ç¬¬ä¸‰æ–¹å¼ºå¤§çš„å¼€æºæ¨¡å‹ä¾‹å¦‚Yiã€qwenã€chatglmã€mistralã€Llama3çš„tokenizerè¯è¡¨é•¿åº¦å¦‚ä¸‹ï¼š&lt;/p&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt;
    &lt;th&gt;Tokenizeræ¨¡å‹&lt;/th&gt;
    &lt;th&gt;è¯è¡¨å¤§å°&lt;/th&gt;
    &lt;th&gt;æ¥æº&lt;/th&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;yi tokenizer&lt;/td&gt;
    &lt;td&gt;64,000&lt;/td&gt;
    &lt;td&gt;01ä¸‡ç‰©ï¼ˆä¸­å›½ï¼‰&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;qwen2 tokenizer&lt;/td&gt;
    &lt;td&gt;151,643&lt;/td&gt;
    &lt;td&gt;é˜¿é‡Œäº‘ï¼ˆä¸­å›½ï¼‰&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;glm tokenizer&lt;/td&gt;
    &lt;td&gt;151,329&lt;/td&gt;
    &lt;td&gt;æ™ºè°±AIï¼ˆä¸­å›½ï¼‰&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;mistral tokenizer&lt;/td&gt;
    &lt;td&gt;32,000&lt;/td&gt;
    &lt;td&gt;Mistral AIï¼ˆæ³•å›½ï¼‰&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;llama3 tokenizer&lt;/td&gt;
    &lt;td&gt;128,000&lt;/td&gt;
    &lt;td&gt;Metaï¼ˆç¾å›½ï¼‰&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;minimind tokenizer&lt;/td&gt;
    &lt;td&gt;6,400&lt;/td&gt;
    &lt;td&gt;è‡ªå®šä¹‰&lt;/td&gt;
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;ğŸ‘‰2024-09-17æ›´æ–°ï¼šä¸ºäº†é˜²æ­¢è¿‡å»çš„ç‰ˆæœ¬æ­§ä¹‰&amp;amp;æ§åˆ¶ä½“ç§¯ï¼Œminimindæ‰€æœ‰æ¨¡å‹å‡ä½¿ç”¨minimind_tokenizeråˆ†è¯ï¼ŒåºŸå¼ƒæ‰€æœ‰mistral_tokenizerç‰ˆæœ¬ã€‚&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;pre&gt;&lt;code&gt;# ä¸€äº›è‡ªè¨€è‡ªè¯­
&amp;gt; å°½ç®¡minimind_tokenizeré•¿åº¦å¾ˆå°ï¼Œç¼–è§£ç æ•ˆç‡å¼±äºqwen2ã€glmç­‰ä¸­æ–‡å‹å¥½å‹åˆ†è¯å™¨ã€‚
&amp;gt; ä½†minimindæ¨¡å‹é€‰æ‹©äº†è‡ªå·±è®­ç»ƒçš„minimind_tokenizerä½œä¸ºåˆ†è¯å™¨ï¼Œä»¥ä¿æŒæ•´ä½“å‚æ•°è½»é‡ï¼Œé¿å…ç¼–ç å±‚å’Œè®¡ç®—å±‚å æ¯”å¤±è¡¡ï¼Œå¤´é‡è„šè½»ï¼Œå› ä¸ºminimindçš„è¯è¡¨å¤§å°åªæœ‰6400ã€‚
&amp;gt; ä¸”minimindåœ¨å®é™…æµ‹è¯•ä¸­æ²¡æœ‰å‡ºç°è¿‡ç”Ÿåƒ»è¯æ±‡è§£ç å¤±è´¥çš„æƒ…å†µï¼Œæ•ˆæœè‰¯å¥½ã€‚
&amp;gt; ç”±äºè‡ªå®šä¹‰è¯è¡¨å‹ç¼©é•¿åº¦åˆ°6400ï¼Œä½¿å¾—LLMæ€»å‚æ•°é‡æœ€ä½åªæœ‰25.8Mã€‚
&amp;gt; è®­ç»ƒæ•°æ®`pretrain_hq.jsonl`å‡æ¥è‡ªäº`åŒ æ•°å¤§æ¨¡å‹æ•°æ®é›†`ï¼Œè¿™éƒ¨åˆ†æ•°æ®ç›¸å¯¹æ¬¡è¦ï¼Œå¦‚éœ€è®­ç»ƒå¯ä»¥è‡ªç”±é€‰æ‹©ã€‚
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;â…¡ Pretrainæ•°æ®&lt;/h2&gt; 
&lt;p&gt;ç»å†äº†MiniMind-V1çš„ä½è´¨é‡é¢„è®­ç»ƒæ•°æ®ï¼Œå¯¼è‡´æ¨¡å‹èƒ¡è¨€ä¹±è¯­çš„æ•™è®­ï¼Œ&lt;code&gt;2025-02-05&lt;/code&gt; ä¹‹åå†³å®šä¸å†é‡‡ç”¨å¤§è§„æ¨¡æ— ç›‘ç£çš„æ•°æ®é›†åšé¢„è®­ç»ƒã€‚ è¿›è€Œå°è¯•æŠŠ&lt;a href="https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data"&gt;åŒ æ•°å¤§æ¨¡å‹æ•°æ®é›†&lt;/a&gt;çš„ä¸­æ–‡éƒ¨åˆ†æå–å‡ºæ¥ï¼Œ æ¸…æ´—å‡ºå­—ç¬¦&lt;code&gt;&amp;lt;512&lt;/code&gt;é•¿åº¦çš„å¤§çº¦1.6GBçš„è¯­æ–™ç›´æ¥æ‹¼æ¥æˆé¢„è®­ç»ƒæ•°æ® &lt;code&gt;pretrain_hq.jsonl&lt;/code&gt;ï¼Œhqå³ä¸ºhigh qualityï¼ˆå½“ç„¶ä¹Ÿè¿˜ä¸ç®—highï¼Œæå‡æ•°æ®è´¨é‡æ— æ­¢å°½ï¼‰ã€‚&lt;/p&gt; 
&lt;p&gt;æ–‡ä»¶&lt;code&gt;pretrain_hq.jsonl&lt;/code&gt; æ•°æ®æ ¼å¼ä¸º&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{"text": "å¦‚ä½•æ‰èƒ½æ‘†è„±æ‹–å»¶ç—‡ï¼Ÿ æ²»æ„ˆæ‹–å»¶ç—‡å¹¶ä¸å®¹æ˜“ï¼Œä½†ä»¥ä¸‹å»ºè®®å¯èƒ½æœ‰æ‰€å¸®åŠ©..."}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;â…¢ SFTæ•°æ®&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data"&gt;åŒ æ•°å¤§æ¨¡å‹SFTæ•°æ®é›†&lt;/a&gt; â€œæ˜¯ä¸€ä¸ªå®Œæ•´ã€æ ¼å¼ç»Ÿä¸€ã€å®‰å…¨çš„å¤§æ¨¡å‹è®­ç»ƒå’Œç ”ç©¶èµ„æºã€‚ ä»ç½‘ç»œä¸Šçš„å…¬å¼€æ•°æ®æºæ”¶é›†å¹¶æ•´ç†äº†å¤§é‡å¼€æºæ•°æ®é›†ï¼Œå¯¹å…¶è¿›è¡Œäº†æ ¼å¼ç»Ÿä¸€ï¼Œæ•°æ®æ¸…æ´—ï¼Œ åŒ…å«10Mæ¡æ•°æ®çš„ä¸­æ–‡æ•°æ®é›†å’ŒåŒ…å«2Mæ¡æ•°æ®çš„è‹±æ–‡æ•°æ®é›†ã€‚â€ ä»¥ä¸Šæ˜¯å®˜æ–¹ä»‹ç»ï¼Œä¸‹è½½æ–‡ä»¶åçš„æ•°æ®æ€»é‡å¤§çº¦åœ¨4B tokensï¼Œè‚¯å®šæ˜¯é€‚åˆä½œä¸ºä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹çš„SFTæ•°æ®çš„ã€‚ ä½†æ˜¯å®˜æ–¹æä¾›çš„æ•°æ®æ ¼å¼å¾ˆä¹±ï¼Œå…¨éƒ¨ç”¨æ¥sftä»£ä»·å¤ªå¤§ã€‚ æˆ‘å°†æŠŠå®˜æ–¹æ•°æ®é›†è¿›è¡Œäº†äºŒæ¬¡æ¸…æ´—ï¼ŒæŠŠå«æœ‰ç¬¦å·æ±¡æŸ“å’Œå™ªå£°çš„æ¡ç›®å»é™¤ï¼›å¦å¤–ä¾ç„¶åªä¿ç•™äº†æ€»é•¿åº¦&lt;code&gt;&amp;lt;512&lt;/code&gt; çš„å†…å®¹ï¼Œæ­¤é˜¶æ®µå¸Œæœ›é€šè¿‡å¤§é‡å¯¹è¯è¡¥å……é¢„è®­ç»ƒé˜¶æ®µæ¬ ç¼ºçš„çŸ¥è¯†ã€‚ å¯¼å‡ºæ–‡ä»¶ä¸º&lt;code&gt;sft_512.jsonl&lt;/code&gt;(~7.5GB)ã€‚&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.modelscope.cn/organization/Magpie-Align"&gt;Magpie-SFTæ•°æ®é›†&lt;/a&gt; æ”¶é›†äº†~1Mæ¡æ¥è‡ªQwen2/2.5çš„é«˜è´¨é‡å¯¹è¯ï¼Œæˆ‘å°†è¿™éƒ¨åˆ†æ•°æ®è¿›ä¸€æ­¥æ¸…æ´—ï¼ŒæŠŠæ€»é•¿åº¦&lt;code&gt;&amp;lt;2048&lt;/code&gt;çš„éƒ¨åˆ†å¯¼å‡ºä¸º&lt;code&gt;sft_2048.jsonl&lt;/code&gt;(~9GB)ã€‚ é•¿åº¦&lt;code&gt;&amp;lt;1024&lt;/code&gt;çš„éƒ¨åˆ†å¯¼å‡ºä¸º&lt;code&gt;sft_1024.jsonl&lt;/code&gt;(~5.5GB)ï¼Œç”¨å¤§æ¨¡å‹å¯¹è¯æ•°æ®ç›´æ¥è¿›è¡Œsftå°±å±äºâ€œé»‘ç›’è’¸é¦â€çš„èŒƒç•´ã€‚&lt;/p&gt; 
&lt;p&gt;è¿›ä¸€æ­¥æ¸…æ´—å‰ä¸¤æ­¥sftçš„æ•°æ®ï¼ˆåªä¿ç•™ä¸­æ–‡å­—ç¬¦å æ¯”é«˜çš„å†…å®¹ï¼‰ï¼Œç­›é€‰é•¿åº¦&lt;code&gt;&amp;lt;512&lt;/code&gt;çš„å¯¹è¯ï¼Œå¾—åˆ°&lt;code&gt;sft_mini_512.jsonl&lt;/code&gt;(~1.2GB)ã€‚&lt;/p&gt; 
&lt;p&gt;æ‰€æœ‰sftæ–‡ä»¶ &lt;code&gt;sft_X.jsonl&lt;/code&gt; æ•°æ®æ ¼å¼å‡ä¸º&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;{
    "conversations": [
        {"role": "user", "content": "ä½ å¥½"},
        {"role": "assistant", "content": "ä½ å¥½ï¼"},
        {"role": "user", "content": "å†è§"},
        {"role": "assistant", "content": "å†è§ï¼"}
    ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;â…£ RLHFæ•°æ®&lt;/h2&gt; 
&lt;p&gt;æ¥è‡ª&lt;a href="https://www.modelscope.cn/datasets/Magpie-Align/MagpieLM-DPO-Data-v0.1"&gt;Magpie-DPOæ•°æ®é›†&lt;/a&gt; å¤§çº¦200kæ¡åå¥½æ•°æ®ï¼ˆå‡æ˜¯è‹±æ–‡ï¼‰ç”Ÿæˆè‡ªLlama3.1-70B/8Bï¼Œå¯ä»¥ç”¨äºè®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œä¼˜åŒ–æ¨¡å‹å›å¤è´¨é‡ï¼Œä½¿å…¶æ›´åŠ ç¬¦åˆäººç±»åå¥½ã€‚ è¿™é‡Œå°†æ•°æ®æ€»é•¿åº¦&lt;code&gt;&amp;lt;3000&lt;/code&gt;çš„å†…å®¹é‡ç»„ä¸º&lt;code&gt;dpo.jsonl&lt;/code&gt;(~0.9GB)ï¼ŒåŒ…å«&lt;code&gt;chosen&lt;/code&gt;å’Œ&lt;code&gt;rejected&lt;/code&gt;ä¸¤ä¸ªå­—æ®µï¼Œ&lt;code&gt;chosen&lt;/code&gt; ä¸ºåå¥½çš„å›å¤ï¼Œ&lt;code&gt;rejected&lt;/code&gt;ä¸ºæ‹’ç»çš„å›å¤ã€‚&lt;/p&gt; 
&lt;p&gt;æ–‡ä»¶ &lt;code&gt;dpo.jsonl&lt;/code&gt; æ•°æ®æ ¼å¼ä¸º&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;{
  "chosen": [
    {"content": "Q", "role": "user"}, 
    {"content": "good answer", "role": "assistant"}
  ], 
  "rejected": [
    {"content": "Q", "role": "user"}, 
    {"content": "bad answer", "role": "assistant"}
  ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;â…¤ Reasonæ•°æ®é›†ï¼š&lt;/h2&gt; 
&lt;p&gt;ä¸å¾—ä¸è¯´2025å¹´2æœˆè°èƒ½ç«çš„è¿‡DeepSeek... ä¹Ÿæ¿€å‘äº†æˆ‘å¯¹RLå¼•å¯¼çš„æ¨ç†æ¨¡å‹çš„æµ“åšå…´è¶£ï¼Œç›®å‰å·²ç»ç”¨Qwen2.5å¤ç°äº†R1-Zeroã€‚ å¦‚æœæœ‰æ—¶é—´+æ•ˆæœworkï¼ˆä½†99%åŸºæ¨¡èƒ½åŠ›ä¸è¶³ï¼‰æˆ‘ä¼šåœ¨ä¹‹åæ›´æ–°MiniMindåŸºäºRLè®­ç»ƒçš„æ¨ç†æ¨¡å‹è€Œä¸æ˜¯è’¸é¦æ¨¡å‹ã€‚ æ—¶é—´æœ‰é™ï¼Œæœ€å¿«çš„ä½æˆæœ¬æ–¹æ¡ˆä¾ç„¶æ˜¯ç›´æ¥è’¸é¦ï¼ˆé»‘ç›’æ–¹å¼ï¼‰ã€‚ è€ä¸ä½R1å¤ªç«ï¼ŒçŸ­çŸ­å‡ å¤©å°±å·²ç»å­˜åœ¨ä¸€äº›R1çš„è’¸é¦æ•°æ®é›†&lt;a href="https://www.modelscope.cn/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B"&gt;R1-Llama-70B&lt;/a&gt;ã€&lt;a href="https://www.modelscope.cn/datasets/AI-ModelScope/R1-Distill-SFT"&gt;R1-Distill-SFT&lt;/a&gt;ã€ &lt;a href="https://huggingface.co/datasets/shareAI/Alpaca-Distill-R1-ZH"&gt;Alpaca-Distill-R1&lt;/a&gt;ã€ &lt;a href="https://huggingface.co/datasets/jinliuxi/deepseek_r1_zh"&gt;deepseek_r1_zh&lt;/a&gt;ç­‰ç­‰ï¼Œçº¯ä¸­æ–‡çš„æ•°æ®å¯èƒ½æ¯”è¾ƒå°‘ã€‚ æœ€ç»ˆæ•´åˆå®ƒä»¬ï¼Œå¯¼å‡ºæ–‡ä»¶ä¸º&lt;code&gt;r1_mix_1024.jsonl&lt;/code&gt;ï¼Œæ•°æ®æ ¼å¼å’Œ&lt;code&gt;sft_X.jsonl&lt;/code&gt;ä¸€è‡´ã€‚&lt;/p&gt; 
&lt;h2&gt;â…¥ æ›´å¤šæ•°æ®é›†&lt;/h2&gt; 
&lt;p&gt;ç›®å‰å·²ç»æœ‰&lt;a href="https://github.com/HqWu-HITCS/Awesome-Chinese-LLM"&gt;HqWu-HITCS/Awesome-Chinese-LLM&lt;/a&gt; åœ¨æ”¶é›†å’Œæ¢³ç†ä¸­æ–‡LLMç›¸å…³çš„å¼€æºæ¨¡å‹ã€åº”ç”¨ã€æ•°æ®é›†åŠæ•™ç¨‹ç­‰èµ„æ–™ï¼Œå¹¶æŒç»­æ›´æ–°è¿™æ–¹é¢çš„æœ€æ–°è¿›å±•ã€‚å…¨é¢ä¸”ä¸“ä¸šï¼ŒRespectï¼&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;â…§ MiniMindè®­ç»ƒæ•°æ®é›†&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] 2025-02-05åï¼Œå¼€æºMiniMindæœ€ç»ˆè®­ç»ƒæ‰€ç”¨çš„æ‰€æœ‰æ•°æ®é›†ï¼Œå› æ­¤æ— éœ€å†è‡ªè¡Œé¢„å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†ï¼Œé¿å…é‡å¤æ€§çš„æ•°æ®å¤„ç†å·¥ä½œã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;MiniMindè®­ç»ƒæ•°æ®é›†ä¸‹è½½åœ°å€ï¼š &lt;a href="https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files"&gt;ModelScope&lt;/a&gt; | &lt;a href="https://huggingface.co/datasets/jingyaogong/minimind_dataset/tree/main"&gt;HuggingFace&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ— éœ€å…¨éƒ¨cloneï¼Œå¯å•ç‹¬ä¸‹è½½æ‰€éœ€çš„æ–‡ä»¶&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;å°†ä¸‹è½½çš„æ•°æ®é›†æ–‡ä»¶æ”¾åˆ°&lt;code&gt;./dataset/&lt;/code&gt;ç›®å½•ä¸‹ï¼ˆâœ¨ä¸ºæ¨èçš„å¿…é¡»é¡¹ï¼‰&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./dataset/
â”œâ”€â”€ dpo.jsonl (55MB, âœ¨)
â”œâ”€â”€ lora_identity.jsonl (22.8KB)
â”œâ”€â”€ lora_medical.jsonl (34MB)
â”œâ”€â”€ pretrain_hq.jsonl (1.6GB, âœ¨)
â”œâ”€â”€ r1_mix_1024.jsonl (340MB)
â”œâ”€â”€ rlaif-mini.jsonl (1MB)
â”œâ”€â”€ sft_1024.jsonl (5.6GB)
â”œâ”€â”€ sft_2048.jsonl (9GB)
â”œâ”€â”€ sft_512.jsonl (7.5GB)
â””â”€â”€ sft_mini_512.jsonl (1.2GB, âœ¨)
&lt;/code&gt;&lt;/pre&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;æ³¨ï¼šå„æ•°æ®é›†ç®€ä»‹&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;dpo.jsonl&lt;/code&gt;âœ¨ --RLHFé˜¶æ®µæ•°æ®é›†ï¼ˆå·²ç²¾ç®€ä¼˜åŒ–ï¼Œé€‚åˆå¿«é€Ÿè®­ç»ƒï¼‰&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;lora_identity.jsonl&lt;/code&gt; --è‡ªæˆ‘è®¤çŸ¥æ•°æ®é›†ï¼ˆä¾‹å¦‚ï¼šä½ æ˜¯è°ï¼Ÿæˆ‘æ˜¯minimind...ï¼‰ï¼Œæ¨èç”¨äºloraè®­ç»ƒï¼ˆäº¦å¯ç”¨äºå…¨å‚SFTï¼Œå‹¿è¢«åå­—å±€é™ï¼‰&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;lora_medical.jsonl&lt;/code&gt; --åŒ»ç–—é—®ç­”æ•°æ®é›†ï¼Œæ¨èç”¨äºloraè®­ç»ƒï¼ˆäº¦å¯ç”¨äºå…¨å‚SFTï¼Œå‹¿è¢«åå­—å±€é™ï¼‰&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;pretrain_hq.jsonl&lt;/code&gt;âœ¨ --é¢„è®­ç»ƒæ•°æ®é›†ï¼Œæ•´åˆè‡ªåŒ æ•°ç§‘æŠ€&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;r1_mix_1024.jsonl&lt;/code&gt; --DeepSeek-R1-1.5Bè’¸é¦æ•°æ®ï¼Œæ¯æ¡æ•°æ®å­—ç¬¦æœ€å¤§é•¿åº¦ä¸º1024ï¼ˆå› æ­¤è®­ç»ƒæ—¶è®¾ç½®max_seq_len=1024ï¼‰&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;rlaif-mini.jsonl&lt;/code&gt; --RLAIFè®­ç»ƒæ•°æ®é›†ï¼Œä»SFTæ•°æ®é›†ä¸­éšæœºé‡‡æ ·1ä¸‡æ¡é«˜è´¨é‡å¯¹è¯ï¼Œç”¨äºPPO/GRPO/SPOç­‰å¼ºåŒ–å­¦ä¹ ç®—æ³•è®­ç»ƒ&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;sft_1024.jsonl&lt;/code&gt; --æ•´åˆè‡ªQwen2.5è’¸é¦æ•°æ®ï¼ˆæ˜¯sft_2048çš„å­é›†ï¼‰ï¼Œæ¯æ¡æ•°æ®å­—ç¬¦æœ€å¤§é•¿åº¦ä¸º1024ï¼ˆå› æ­¤è®­ç»ƒæ—¶è®¾ç½®max_seq_len=1024ï¼‰&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;sft_2048.jsonl&lt;/code&gt; --æ•´åˆè‡ªQwen2.5è’¸é¦æ•°æ®ï¼Œæ¯æ¡æ•°æ®å­—ç¬¦æœ€å¤§é•¿åº¦ä¸º2048ï¼ˆå› æ­¤è®­ç»ƒæ—¶è®¾ç½®max_seq_len=2048ï¼‰&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;sft_512.jsonl&lt;/code&gt; --æ•´åˆè‡ªåŒ æ•°ç§‘æŠ€SFTæ•°æ®ï¼Œæ¯æ¡æ•°æ®å­—ç¬¦æœ€å¤§é•¿åº¦ä¸º512ï¼ˆå› æ­¤è®­ç»ƒæ—¶è®¾ç½®max_seq_len=512ï¼‰&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;sft_mini_512.jsonl&lt;/code&gt;âœ¨ --æç®€æ•´åˆè‡ªåŒ æ•°ç§‘æŠ€SFTæ•°æ®+Qwen2.5è’¸é¦æ•°æ®ï¼ˆç”¨äºå¿«é€Ÿè®­ç»ƒZeroæ¨¡å‹ï¼‰ï¼Œæ¯æ¡æ•°æ®å­—ç¬¦æœ€å¤§é•¿åº¦ä¸º512ï¼ˆå› æ­¤è®­ç»ƒæ—¶è®¾ç½®max_seq_len=512ï¼‰&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/dataset.jpg" alt="dataset" /&gt;&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;è¯´æ˜ &amp;amp; æ¨èè®­ç»ƒæ–¹æ¡ˆ&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;MiniMind2 Serieså‡ç»è¿‡å…±çº¦20GBè¯­æ–™è®­ç»ƒï¼Œå¤§çº¦4B tokensï¼Œå³å¯¹åº”ä¸Šé¢çš„æ•°æ®ç»„åˆè®­ç»ƒç»“æœï¼ˆå¼€é”€ï¼šğŸ’°ğŸ’°ğŸ’°ğŸ’°ğŸ’°ğŸ’°ğŸ’°ğŸ’°ï¼Œæ•ˆæœï¼šğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜Šï¼‰&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;æƒ³è¦æœ€å¿«é€Ÿåº¦ä»0å®ç°Zeroæ¨¡å‹ï¼Œæ¨èä½¿ç”¨&lt;code&gt;pretrain_hq.jsonl&lt;/code&gt; + &lt;code&gt;sft_mini_512.jsonl&lt;/code&gt; çš„æ•°æ®ç»„åˆï¼Œå…·ä½“èŠ±é”€å’Œæ•ˆæœå¯æŸ¥çœ‹ä¸‹æ–‡è¡¨æ ¼ï¼ˆå¼€é”€ï¼šğŸ’°ï¼Œæ•ˆæœï¼šğŸ˜ŠğŸ˜Šï¼‰&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;æ¨èå…·å¤‡ä¸€å®šç®—åŠ›èµ„æºæˆ–æ›´åœ¨æ„æ•ˆæœçš„æœ‹å‹å¯ä»¥è€ƒè™‘å‰è€…å®Œæ•´å¤ç°MiniMind2ï¼›ä»…æœ‰å•å¡GPUæˆ–åœ¨ä¹çŸ­æ—¶é—´å¿«é€Ÿå¤ç°çš„æœ‹å‹å¼ºçƒˆæ¨èåè€…ï¼›&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;ã€æŠ˜ä¸­æ–¹æ¡ˆã€‘äº¦å¯é€‰æ‹©ä¾‹å¦‚&lt;code&gt;sft_mini_512.jsonl&lt;/code&gt;ã€&lt;code&gt;sft_1024.jsonl&lt;/code&gt;ä¸­ç­‰è§„æ¨¡æ•°æ®è¿›è¡Œè‡ªç”±ç»„åˆè®­ç»ƒï¼ˆå¼€é”€ï¼šğŸ’°ğŸ’°ğŸ’°ï¼Œæ•ˆæœï¼šğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜Šï¼‰ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h1&gt;ğŸ“Œ Model&lt;/h1&gt; 
&lt;h2&gt;Structure&lt;/h2&gt; 
&lt;p&gt;MiniMind-Denseï¼ˆå’Œ&lt;a href="https://ai.meta.com/blog/meta-llama-3-1/"&gt;Llama3.1&lt;/a&gt;ä¸€æ ·ï¼‰ä½¿ç”¨äº†Transformerçš„Decoder-Onlyç»“æ„ï¼Œè·ŸGPT-3çš„åŒºåˆ«åœ¨äºï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;é‡‡ç”¨äº†GPT-3çš„é¢„æ ‡å‡†åŒ–æ–¹æ³•ï¼Œä¹Ÿå°±æ˜¯åœ¨æ¯ä¸ªTransformerå­å±‚çš„è¾“å…¥ä¸Šè¿›è¡Œå½’ä¸€åŒ–ï¼Œè€Œä¸æ˜¯åœ¨è¾“å‡ºä¸Šã€‚å…·ä½“æ¥è¯´ï¼Œä½¿ç”¨çš„æ˜¯RMSNormå½’ä¸€åŒ–å‡½æ•°ã€‚&lt;/li&gt; 
 &lt;li&gt;ç”¨SwiGLUæ¿€æ´»å‡½æ•°æ›¿ä»£äº†ReLUï¼Œè¿™æ ·åšæ˜¯ä¸ºäº†æé«˜æ€§èƒ½ã€‚&lt;/li&gt; 
 &lt;li&gt;åƒGPT-Neoä¸€æ ·ï¼Œå»æ‰äº†ç»å¯¹ä½ç½®åµŒå…¥ï¼Œæ”¹ç”¨äº†æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰ï¼Œè¿™æ ·åœ¨å¤„ç†è¶…å‡ºè®­ç»ƒé•¿åº¦çš„æ¨ç†æ—¶æ•ˆæœæ›´å¥½ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;MiniMind-MoEæ¨¡å‹ï¼Œå®ƒçš„ç»“æ„åŸºäºLlama3å’Œ&lt;a href="https://arxiv.org/pdf/2405.04434"&gt;Deepseek-V2/3&lt;/a&gt;ä¸­çš„MixFFNæ··åˆä¸“å®¶æ¨¡å—ã€‚&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;DeepSeek-V2åœ¨å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰æ–¹é¢ï¼Œé‡‡ç”¨äº†æ›´ç»†ç²’åº¦çš„ä¸“å®¶åˆ†å‰²å’Œå…±äº«çš„ä¸“å®¶éš”ç¦»æŠ€æœ¯ï¼Œä»¥æé«˜Expertsçš„æ•ˆæœã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;MiniMindçš„æ•´ä½“ç»“æ„ä¸€è‡´ï¼Œåªæ˜¯åœ¨RoPEè®¡ç®—ã€æ¨ç†å‡½æ•°å’ŒFFNå±‚çš„ä»£ç ä¸Šåšäº†ä¸€äº›å°è°ƒæ•´ã€‚ å…¶ç»“æ„å¦‚ä¸‹å›¾ï¼ˆé‡ç»˜ç‰ˆï¼‰ï¼š&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/LLM-structure.png" alt="structure" /&gt; &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/LLM-structure-moe.png" alt="structure-moe" /&gt;&lt;/p&gt; 
&lt;p&gt;ä¿®æ”¹æ¨¡å‹é…ç½®è§&lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/model/model_minimind.py"&gt;./model/model_minimind.py&lt;/a&gt;ã€‚ å‚è€ƒæ¨¡å‹å‚æ•°ç‰ˆæœ¬è§ä¸‹è¡¨ï¼š&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Name&lt;/th&gt; 
   &lt;th&gt;params&lt;/th&gt; 
   &lt;th&gt;len_vocab&lt;/th&gt; 
   &lt;th&gt;rope_theta&lt;/th&gt; 
   &lt;th&gt;n_layers&lt;/th&gt; 
   &lt;th&gt;d_model&lt;/th&gt; 
   &lt;th&gt;kv_heads&lt;/th&gt; 
   &lt;th&gt;q_heads&lt;/th&gt; 
   &lt;th&gt;share+route&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-Small&lt;/td&gt; 
   &lt;td&gt;26M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e6&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;512&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-MoE&lt;/td&gt; 
   &lt;td&gt;145M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e6&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;640&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;1+4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2&lt;/td&gt; 
   &lt;td&gt;104M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e6&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;768&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;minimind-v1-small&lt;/td&gt; 
   &lt;td&gt;26M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e4&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;512&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;minimind-v1-moe&lt;/td&gt; 
   &lt;td&gt;4Ã—26M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e4&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;512&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;1+4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;minimind-v1&lt;/td&gt; 
   &lt;td&gt;108M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e4&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;768&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Model Configuration&lt;/h2&gt; 
&lt;p&gt;ğŸ“‹å…³äºLLMçš„å‚æ•°é…ç½®ï¼Œæœ‰ä¸€ç¯‡å¾ˆæœ‰æ„æ€çš„è®ºæ–‡&lt;a href="https://arxiv.org/pdf/2402.14905"&gt;MobileLLM&lt;/a&gt;åšäº†è¯¦ç»†çš„ç ”ç©¶å’Œå®éªŒã€‚ Scaling Lawåœ¨å°æ¨¡å‹ä¸­æœ‰è‡ªå·±ç‹¬ç‰¹çš„è§„å¾‹ã€‚ å¼•èµ·Transformerå‚æ•°æˆè§„æ¨¡å˜åŒ–çš„å‚æ•°å‡ ä¹åªå–å†³äº&lt;code&gt;d_model&lt;/code&gt;å’Œ&lt;code&gt;n_layers&lt;/code&gt;ã€‚&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;d_model&lt;/code&gt;â†‘ + &lt;code&gt;n_layers&lt;/code&gt;â†“ -&amp;gt; çŸ®èƒ–å­&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;d_model&lt;/code&gt;â†“ + &lt;code&gt;n_layers&lt;/code&gt;â†‘ -&amp;gt; ç˜¦é«˜ä¸ª&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;2020å¹´æå‡ºScaling Lawçš„è®ºæ–‡è®¤ä¸ºï¼Œè®­ç»ƒæ•°æ®é‡ã€å‚æ•°é‡ä»¥åŠè®­ç»ƒè¿­ä»£æ¬¡æ•°æ‰æ˜¯å†³å®šæ€§èƒ½çš„å…³é”®å› ç´ ï¼Œè€Œæ¨¡å‹æ¶æ„çš„å½±å“å‡ ä¹å¯ä»¥å¿½è§†ã€‚ ç„¶è€Œä¼¼ä¹è¿™ä¸ªå®šå¾‹å¯¹å°æ¨¡å‹å¹¶ä¸å®Œå…¨é€‚ç”¨ã€‚ MobileLLMæå‡ºæ¶æ„çš„æ·±åº¦æ¯”å®½åº¦æ›´é‡è¦ï¼Œã€Œæ·±è€Œçª„ã€çš„ã€Œç˜¦é•¿ã€æ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°æ¯”ã€Œå®½è€Œæµ…ã€æ¨¡å‹æ›´å¤šçš„æŠ½è±¡æ¦‚å¿µã€‚ ä¾‹å¦‚å½“æ¨¡å‹å‚æ•°å›ºå®šåœ¨125Mæˆ–è€…350Mæ—¶ï¼Œ30ï½42å±‚çš„ã€Œç‹­é•¿ã€æ¨¡å‹æ˜æ˜¾æ¯”12å±‚å·¦å³çš„ã€ŒçŸ®èƒ–ã€æ¨¡å‹æœ‰æ›´ä¼˜è¶Šçš„æ€§èƒ½ï¼Œ åœ¨å¸¸è¯†æ¨ç†ã€é—®ç­”ã€é˜…è¯»ç†è§£ç­‰8ä¸ªåŸºå‡†æµ‹è¯•ä¸Šéƒ½æœ‰ç±»ä¼¼çš„è¶‹åŠ¿ã€‚ è¿™å…¶å®æ˜¯éå¸¸æœ‰è¶£çš„å‘ç°ï¼Œå› ä¸ºä»¥å¾€ä¸º100Må·¦å³é‡çº§çš„å°æ¨¡å‹è®¾è®¡æ¶æ„æ—¶ï¼Œå‡ ä¹æ²¡äººå°è¯•è¿‡å åŠ è¶…è¿‡12å±‚ã€‚ è¿™ä¸MiniMindåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹å‚æ•°é‡åœ¨&lt;code&gt;d_model&lt;/code&gt;å’Œ&lt;code&gt;n_layers&lt;/code&gt;ä¹‹é—´è¿›è¡Œè°ƒæ•´å®éªŒè§‚å¯Ÿåˆ°çš„æ•ˆæœæ˜¯ä¸€è‡´çš„ã€‚ ç„¶è€Œã€Œæ·±è€Œçª„ã€çš„ã€Œçª„ã€ä¹Ÿæ˜¯æœ‰ç»´åº¦æé™çš„ï¼Œå½“d_model&amp;lt;512æ—¶ï¼Œè¯åµŒå…¥ç»´åº¦åå¡Œçš„åŠ£åŠ¿éå¸¸æ˜æ˜¾ï¼Œ å¢åŠ çš„layerså¹¶ä¸èƒ½å¼¥è¡¥è¯åµŒå…¥åœ¨å›ºå®šq_headå¸¦æ¥d_headä¸è¶³çš„åŠ£åŠ¿ã€‚ å½“d_model&amp;gt;1536æ—¶ï¼Œlayersçš„å¢åŠ ä¼¼ä¹æ¯”d_modelçš„ä¼˜å…ˆçº§æ›´é«˜ï¼Œæ›´èƒ½å¸¦æ¥å…·æœ‰"æ€§ä»·æ¯”"çš„å‚æ•°-&amp;gt;æ•ˆæœå¢ç›Šã€‚&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;å› æ­¤MiniMindè®¾å®šsmallæ¨¡å‹dim=512ï¼Œn_layers=8æ¥è·å–çš„ã€Œæå°ä½“ç§¯&amp;lt;-&amp;gt;æ›´å¥½æ•ˆæœã€çš„å¹³è¡¡ã€‚&lt;/li&gt; 
 &lt;li&gt;è®¾å®šdim=768ï¼Œn_layers=16æ¥è·å–æ•ˆæœçš„æ›´å¤§æ”¶ç›Šï¼Œæ›´åŠ ç¬¦åˆå°æ¨¡å‹Scaling-Lawçš„å˜åŒ–æ›²çº¿ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ä½œä¸ºå‚è€ƒï¼ŒGPT3çš„å‚æ•°è®¾å®šè§ä¸‹è¡¨ï¼š &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/gpt3_config.png" alt="gpt3_config.png" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;ğŸ“Œ Experiment&lt;/h1&gt; 
&lt;h2&gt;â…  è®­ç»ƒå¼€é”€&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;æ—¶é—´å•ä½&lt;/strong&gt;ï¼šå°æ—¶ (h)ã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æˆæœ¬å•ä½&lt;/strong&gt;ï¼šäººæ°‘å¸ (ï¿¥)ï¼›7ï¿¥ â‰ˆ 1ç¾å…ƒã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;3090 ç§Ÿå¡å•ä»·&lt;/strong&gt;ï¼šâ‰ˆ1.3ï¿¥/hï¼ˆå¯è‡ªè¡Œå‚è€ƒå®æ—¶å¸‚ä»·ï¼‰ã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;å‚è€ƒæ ‡å‡†&lt;/strong&gt;ï¼šè¡¨æ ¼ä»…å®æµ‹ &lt;code&gt;pretrain&lt;/code&gt; å’Œ &lt;code&gt;sft_mini_512&lt;/code&gt; ä¸¤ä¸ªæ•°æ®é›†çš„è®­ç»ƒæ—¶é—´ï¼Œå…¶å®ƒè€—æ—¶æ ¹æ®æ•°æ®é›†å¤§å°ä¼°ç®—ï¼ˆå¯èƒ½å­˜åœ¨äº›è®¸å‡ºå…¥ï¼‰ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;åŸºäº 3090 ï¼ˆå•å¡ï¼‰æˆæœ¬è®¡ç®—&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Name&lt;/th&gt; 
   &lt;th&gt;params&lt;/th&gt; 
   &lt;th&gt;pretrain&lt;/th&gt; 
   &lt;th&gt;sft_mini_512&lt;/th&gt; 
   &lt;th&gt;sft_512&lt;/th&gt; 
   &lt;th&gt;sft_1024&lt;/th&gt; 
   &lt;th&gt;sft_2048&lt;/th&gt; 
   &lt;th&gt;RLHF&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-Small&lt;/td&gt; 
   &lt;td&gt;26M&lt;/td&gt; 
   &lt;td&gt;â‰ˆ1.1h&lt;br /&gt;â‰ˆ1.43ï¿¥&lt;/td&gt; 
   &lt;td&gt;â‰ˆ1h&lt;br /&gt;â‰ˆ1.3ï¿¥&lt;/td&gt; 
   &lt;td&gt;â‰ˆ6h&lt;br /&gt;â‰ˆ7.8ï¿¥&lt;/td&gt; 
   &lt;td&gt;â‰ˆ4.58h&lt;br /&gt;â‰ˆ5.95ï¿¥&lt;/td&gt; 
   &lt;td&gt;â‰ˆ7.5h&lt;br /&gt;â‰ˆ9.75ï¿¥&lt;/td&gt; 
   &lt;td&gt;â‰ˆ1h&lt;br /&gt;â‰ˆ1.3ï¿¥&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2&lt;/td&gt; 
   &lt;td&gt;104M&lt;/td&gt; 
   &lt;td&gt;â‰ˆ3.9h&lt;br /&gt;â‰ˆ5.07ï¿¥&lt;/td&gt; 
   &lt;td&gt;â‰ˆ3.3h&lt;br /&gt;â‰ˆ4.29ï¿¥&lt;/td&gt; 
   &lt;td&gt;â‰ˆ20h&lt;br /&gt;â‰ˆ26ï¿¥&lt;/td&gt; 
   &lt;td&gt;â‰ˆ15h&lt;br /&gt;â‰ˆ19.5ï¿¥&lt;/td&gt; 
   &lt;td&gt;â‰ˆ25h&lt;br /&gt;â‰ˆ32.5ï¿¥&lt;/td&gt; 
   &lt;td&gt;â‰ˆ3h&lt;br /&gt;â‰ˆ3.9ï¿¥&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;è®­ç»ƒå¼€é”€æ€»ç»“&amp;amp;é¢„æµ‹&lt;/summary&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;MiniMind2-Smallå‚æ•°&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;code&gt;pretrain_hq&lt;/code&gt;+&lt;code&gt;sft_mini_512&lt;/code&gt;æ•°æ®é›† &lt;br /&gt;å•å¡3090 (1 epoch) + 2.1å°æ—¶ + èŠ±è´¹2.73å…ƒäººæ°‘å¸ &lt;br /&gt;å³å¯ä»0è®­ç»ƒå‡ºMiniMind-Zero-0.025Bæ¨¡å‹!!!&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/blockquote&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;MiniMind2-Smallå‚æ•°&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;code&gt;pretrain_hq&lt;/code&gt;+&lt;code&gt;sft_512&lt;/code&gt;+&lt;code&gt;sft_2048&lt;/code&gt;+&lt;code&gt;dpo&lt;/code&gt;æ•°æ®é›† &lt;br /&gt;å•å¡3090 (2 epochs) + å¤§çº¦38.16å°æ—¶ + èŠ±è´¹49.61å…ƒäººæ°‘å¸ &lt;br /&gt;å³å¯ä»0è®­ç»ƒå‡ºMiniMind2-Small-0.025Bæ¨¡å‹!!!&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/blockquote&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;MiniMind2å‚æ•°&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;code&gt;pretrain_hq&lt;/code&gt;+&lt;code&gt;sft_512&lt;/code&gt;+&lt;code&gt;sft_2048&lt;/code&gt;+&lt;code&gt;dpo&lt;/code&gt;æ•°æ®é›† &lt;br /&gt;å•å¡3090 (2 epochs) + å¤§çº¦122å°æ—¶ + èŠ±è´¹158.6å…ƒäººæ°‘å¸ &lt;br /&gt;å³å¯ä»0è®­ç»ƒå‡ºMiniMind2-0.1Bæ¨¡å‹!!!&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;p&gt;âœ¨åŸºäºå•å¡NVIDIA 3090çš„&lt;code&gt;MiniMind-Zero&lt;/code&gt;ä»0è®­ç»ƒä»…éœ€&lt;code&gt;2å°æ—¶&lt;/code&gt; + &lt;code&gt;3å—é’±&lt;/code&gt;ï¼Œå®ç°ChatBotæ•ˆæœï¼&lt;/p&gt; 
&lt;p&gt;âœ¨PSï¼šè‹¥é‡‡ç”¨8å¡4090è®­ç»ƒï¼Œæ€»ç”¨æ—¶ç”šè‡³å¯ä»¥å‹ç¼©åˆ°10åˆ†é’Ÿä»¥å†…ï¼ï¼ˆç”±äºæ—¶é—´æ›´çŸ­ï¼ŒèŠ±è´¹åŒæ ·3å…ƒå·¦å³ï¼Œä¸å•å¡æˆæœ¬ç›¸å½“ï¼‰&lt;/p&gt; 
&lt;p&gt;âœ¨ä»¥æä½æä½çš„é—¨æ§›ï¼Œå®ç°äººäººå¯ç©çš„å¤§æ¨¡å‹è‡ªç”±ï¼è¿™æ­£æ˜¯MiniMindç³»åˆ—çš„è¯ç”Ÿåˆè¡·ï¼&lt;/p&gt; 
&lt;p&gt;âœ¨ä»…ä»·å€¼&lt;code&gt;3å—é’±&lt;/code&gt;æˆæœ¬çš„&lt;code&gt;MiniMind-Zero&lt;/code&gt;å¹¶ä¸æ˜¯å™±å¤´ï¼Chatæµ‹è¯•ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;ğŸ‘¶: è¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚
ğŸ¤–ï¸: ä½œä¸ºäººå·¥æ™ºèƒ½ï¼Œæˆ‘æ²¡æœ‰å®é™…çš„ç”Ÿæ´»ï¼Œä¹Ÿæ²¡æœ‰è‡ªæˆ‘æ„è¯†ï¼Œæ‰€ä»¥æ²¡æœ‰è‡ªå·±çš„ç”Ÿæ´»ã€‚æˆ‘è¢«è®¾è®¡æˆèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·è§£ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯ã€è¿›è¡Œå¯¹è¯ç­‰ã€‚æˆ‘çš„è®¾è®¡å’ŒåŠŸèƒ½æ˜¯ç”±è®¡ç®—æœºç§‘å­¦ã€æœºå™¨å­¦ä¹ ã€äººå·¥æ™ºèƒ½æŠ€æœ¯å’Œç®—æ³•æ‰€æ„å»ºçš„ï¼Œæˆ‘é€šè¿‡ç¼–ç¨‹å’Œç®—æ³•å®ç°ã€‚

ğŸ‘¶: æ¨èä¸€äº›æ­å·çš„ç‰¹è‰²ç¾é£Ÿå§ã€‚
ğŸ¤–ï¸: æ­å·æœ‰å¾ˆå¤šç¾å‘³çš„ç‰¹è‰²ç¾é£Ÿï¼Œæ¯”å¦‚é¸¡è‚‰ä¸²ã€é³—é±¼å¤´ã€æ°´ç…®é±¼ã€è±†è…è„‘ç­‰ã€‚è¿™äº›ç¾é£Ÿéƒ½éå¸¸ç¾å‘³ï¼Œæœ‰ç€ç‹¬ç‰¹çš„é£å‘³å’Œå£æ„Ÿï¼Œç»å¯¹æ˜¯å€¼å¾—ä¸€è¯•çš„ç¾é£Ÿã€‚

ğŸ‘¶: è¯·ä¸ºæˆ‘è®²è§£â€œå¤§è¯­è¨€æ¨¡å‹â€è¿™ä¸ªæ¦‚å¿µã€‚
ğŸ¤–ï¸: å¤§è¯­è¨€æ¨¡å‹æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œå®ƒä½¿ç”¨å¤§é‡çš„æ–‡æœ¬æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ï¼Œç„¶ååœ¨ä¸æ–­çš„è¿­ä»£ä¸­ä¸æ–­ä¼˜åŒ–æ¨¡å‹ã€‚å®ƒçš„ç›®æ ‡æ˜¯æ ¹æ®è¾“å…¥çš„æ•°æ®æ¥ç”Ÿæˆç¬¦åˆç‰¹å®šéœ€æ±‚çš„æ–‡æœ¬ï¼Œè¿™äº›æ–‡æœ¬å¯ä»¥æ˜¯ä»»ä½•ä½ æ„Ÿå…´è¶£çš„ä¸»é¢˜ã€‚

ğŸ‘¶: Introduce the history of the United States, please.
ğŸ¤–ï¸: æ‚¨æåˆ°çš„â€œIntrook's the believeations of theument." è¿™ä¸ªåå­—æ¥æºäºä¸­å›½å¤ä»£çš„"groty of of the change."
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;æé€Ÿä¸”åˆå…·æ•ˆæœï¼Œç”šè‡³ä»ç„¶å¯ä»¥è¿›ä¸€æ­¥å‹ç¼©è·å–æ›´å°æ›´ä¼˜è´¨çš„è®­ç»ƒæ•°æ®ã€‚ Zeroæ¨¡å‹æƒé‡ä¿å­˜ä¸º &lt;code&gt;full_sft_512_zero.pth&lt;/code&gt;ï¼ˆè§ä¸‹æ–‡MiniMindæ¨¡å‹æ–‡ä»¶é“¾æ¥ï¼‰ï¼Œå¦‚æœ‰å…´è¶£å¯ä¸‹è½½æ£€éªŒæ­¤æ¨¡å‹æ•ˆæœã€‚&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;â…¡ ä¸»è¦è®­ç»ƒï¼ˆå¿…é¡»ï¼‰&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ‰€æœ‰è®­ç»ƒè„šæœ¬å‡ &lt;code&gt;cd ./trainer&lt;/code&gt; ç›®å½•æ‰§è¡Œ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;1. é¢„è®­ç»ƒ (Pretrain)&lt;/strong&gt;:&lt;/h3&gt; 
&lt;p&gt;LLMé¦–å…ˆè¦å­¦ä¹ çš„å¹¶éç›´æ¥ä¸äººäº¤æµï¼Œè€Œæ˜¯è®©ç½‘ç»œå‚æ•°ä¸­å……æ»¡çŸ¥è¯†çš„å¢¨æ°´ï¼Œâ€œå¢¨æ°´â€ ç†è®ºä¸Šå–çš„è¶Šé¥±è¶Šå¥½ï¼Œäº§ç”Ÿå¤§é‡çš„å¯¹ä¸–ç•Œçš„çŸ¥è¯†ç§¯ç´¯ã€‚ é¢„è®­ç»ƒå°±æ˜¯è®©Modelå…ˆåŸ‹å¤´è‹¦å­¦å¤§é‡åŸºæœ¬çš„çŸ¥è¯†ï¼Œä¾‹å¦‚ä»Wikiç™¾ç§‘ã€æ–°é—»ã€ä¹¦ç±æ•´ç†å¤§è§„æ¨¡çš„é«˜è´¨é‡è®­ç»ƒæ•°æ®ã€‚ è¿™ä¸ªè¿‡ç¨‹æ˜¯â€œæ— ç›‘ç£â€çš„ï¼Œå³äººç±»ä¸éœ€è¦åœ¨è¿‡ç¨‹ä¸­åšä»»ä½•â€œæœ‰ç›‘ç£â€çš„æ ¡æ­£ï¼Œè€Œæ˜¯ç”±æ¨¡å‹è‡ªå·±ä»å¤§é‡æ–‡æœ¬ä¸­æ€»ç»“è§„å¾‹å­¦ä¹ çŸ¥è¯†ç‚¹ã€‚ æ¨¡å‹æ­¤é˜¶æ®µç›®çš„åªæœ‰ä¸€ä¸ªï¼š&lt;strong&gt;å­¦ä¼šè¯è¯­æ¥é¾™&lt;/strong&gt;ã€‚ä¾‹å¦‚è¾“å…¥"ç§¦å§‹çš‡"å››ä¸ªå­—ï¼Œå®ƒå¯ä»¥æ¥é¾™"æ˜¯ä¸­å›½çš„ç¬¬ä¸€ä½çš‡å¸"ã€‚&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_pretrain.py # 1å³ä¸ºå•å¡è®­ç»ƒï¼Œå¯æ ¹æ®ç¡¬ä»¶æƒ…å†µè‡ªè¡Œè°ƒæ•´ (è®¾ç½®&amp;gt;=2)
# or
python train_pretrain.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;è®­ç»ƒåçš„æ¨¡å‹æƒé‡æ–‡ä»¶é»˜è®¤æ¯éš”&lt;code&gt;100æ­¥&lt;/code&gt;ä¿å­˜ä¸º: &lt;code&gt;pretrain_*.pth&lt;/code&gt;ï¼ˆ* ä¸ºæ¨¡å‹å…·ä½“dimensionï¼Œæ¯æ¬¡ä¿å­˜æ—¶æ–°æ–‡ä»¶ä¼šè¦†ç›–æ—§æ–‡ä»¶ï¼‰&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;MiniMind2-Small (512dim)&lt;/th&gt; 
   &lt;th&gt;MiniMind2 (768dim)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/pre_512_loss.png" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/pre_768_loss.png" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;&lt;strong&gt;2. æœ‰ç›‘ç£å¾®è°ƒ (Supervised Fine-Tuning)&lt;/strong&gt;:&lt;/h3&gt; 
&lt;p&gt;ç»è¿‡é¢„è®­ç»ƒï¼ŒLLMæ­¤æ—¶å·²ç»æŒæ¡äº†å¤§é‡çŸ¥è¯†ï¼Œç„¶è€Œæ­¤æ—¶å®ƒåªä¼šæ— è„‘åœ°è¯è¯­æ¥é¾™ï¼Œè¿˜ä¸ä¼šä¸äººèŠå¤©ã€‚ SFTé˜¶æ®µå°±éœ€è¦æŠŠåŠæˆå“LLMæ–½åŠ ä¸€ä¸ªè‡ªå®šä¹‰çš„èŠå¤©æ¨¡æ¿è¿›è¡Œå¾®è°ƒã€‚ ä¾‹å¦‚æ¨¡å‹é‡åˆ°è¿™æ ·çš„æ¨¡æ¿ã€é—®é¢˜-&amp;gt;å›ç­”ï¼Œé—®é¢˜-&amp;gt;å›ç­”ã€‘åä¸å†æ— è„‘æ¥é¾™ï¼Œè€Œæ˜¯æ„è¯†åˆ°è¿™æ˜¯ä¸€æ®µå®Œæ•´çš„å¯¹è¯ç»“æŸã€‚ ç§°è¿™ä¸ªè¿‡ç¨‹ä¸ºæŒ‡ä»¤å¾®è°ƒï¼Œå°±å¦‚åŒè®©å·²ç»å­¦å¯Œäº”è½¦çš„ã€Œç‰›é¡¿ã€å…ˆç”Ÿé€‚åº”21ä¸–çºªæ™ºèƒ½æ‰‹æœºçš„èŠå¤©ä¹ æƒ¯ï¼Œå­¦ä¹ å±å¹•å·¦ä¾§æ˜¯å¯¹æ–¹æ¶ˆæ¯ï¼Œå³ä¾§æ˜¯æœ¬äººæ¶ˆæ¯è¿™ä¸ªè§„å¾‹ã€‚ åœ¨è®­ç»ƒæ—¶ï¼ŒMiniMindçš„æŒ‡ä»¤å’Œå›ç­”é•¿åº¦è¢«æˆªæ–­åœ¨512ï¼Œæ˜¯ä¸ºäº†èŠ‚çœæ˜¾å­˜ç©ºé—´ã€‚å°±åƒå­¦ä¹ å†™ä½œæ—¶ï¼Œä¼šå…ˆä»çŸ­çš„æ–‡ç« å¼€å§‹ï¼Œå½“å­¦ä¼šå†™ä½œ200å­—ä½œæ–‡åï¼Œ800å­—æ–‡ç« ä¹Ÿå¯ä»¥æ‰‹åˆ°æ“’æ¥ã€‚ åœ¨éœ€è¦é•¿åº¦æ‹“å±•æ—¶ï¼Œåªéœ€è¦å‡†å¤‡å°‘é‡çš„2k/4k/8ké•¿åº¦å¯¹è¯æ•°æ®è¿›è¡Œè¿›ä¸€æ­¥å¾®è°ƒå³å¯ï¼ˆæ­¤æ—¶æœ€å¥½é…åˆRoPE-NTKçš„åŸºå‡†å·®å€¼ï¼‰ã€‚&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;åœ¨æ¨ç†æ—¶é€šè¿‡è°ƒæ•´RoPEçº¿æ€§å·®å€¼ï¼Œå®ç°å…è®­ç»ƒé•¿åº¦å¤–æ¨åˆ°2048åŠä»¥ä¸Šå°†ä¼šå¾ˆæ–¹ä¾¿ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_full_sft.py
# or
python train_full_sft.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;è®­ç»ƒåçš„æ¨¡å‹æƒé‡æ–‡ä»¶é»˜è®¤æ¯éš”&lt;code&gt;100æ­¥&lt;/code&gt;ä¿å­˜ä¸º: &lt;code&gt;full_sft_*.pth&lt;/code&gt;ï¼ˆ* ä¸ºæ¨¡å‹å…·ä½“dimensionï¼Œæ¯æ¬¡ä¿å­˜æ—¶æ–°æ–‡ä»¶ä¼šè¦†ç›–æ—§æ–‡ä»¶ï¼‰&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;MiniMind2-Small (512dim)&lt;/th&gt; 
   &lt;th&gt;MiniMind2 (768dim)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/sft_512_loss.png" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/sft_768_loss.png" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;â…¢ å…¶å®ƒè®­ç»ƒé˜¶æ®µï¼ˆå¯é€‰ï¼‰&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ‰€æœ‰è®­ç»ƒè„šæœ¬å‡ &lt;code&gt;cd ./trainer&lt;/code&gt; ç›®å½•æ‰§è¡Œ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;3. çŸ¥è¯†è’¸é¦ (Knowledge Distillation, KD)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;åœ¨å‰é¢çš„æ‰€æœ‰è®­ç»ƒæ­¥éª¤ä¸­ï¼Œæ¨¡å‹å·²ç»å®Œå…¨å…·å¤‡äº†åŸºæœ¬èƒ½åŠ›ï¼Œé€šå¸¸å¯ä»¥å­¦æˆå‡ºå¸ˆäº†ã€‚ è€ŒçŸ¥è¯†è’¸é¦å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œæ‰€è°“çŸ¥è¯†è’¸é¦ï¼Œå³å­¦ç”Ÿæ¨¡å‹é¢å‘æ•™å¸ˆæ¨¡å‹å­¦ä¹ ã€‚ æ•™å¸ˆæ¨¡å‹é€šå¸¸æ˜¯ç»è¿‡å……åˆ†è®­ç»ƒçš„å¤§æ¨¡å‹ï¼Œå…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ å­¦ç”Ÿæ¨¡å‹æ˜¯ä¸€ä¸ªè¾ƒå°çš„æ¨¡å‹ï¼Œç›®æ ‡æ˜¯å­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„è¡Œä¸ºï¼Œè€Œä¸æ˜¯ç›´æ¥ä»åŸå§‹æ•°æ®ä¸­å­¦ä¹ ã€‚ åœ¨SFTå­¦ä¹ ä¸­ï¼Œæ¨¡å‹çš„ç›®æ ‡æ˜¯æ‹Ÿåˆè¯Tokenåˆ†ç±»ç¡¬æ ‡ç­¾ï¼ˆhard labelsï¼‰ï¼Œå³çœŸå®çš„ç±»åˆ«æ ‡ç­¾ï¼ˆå¦‚ 0 æˆ– 6400ï¼‰ã€‚ åœ¨çŸ¥è¯†è’¸é¦ä¸­ï¼Œæ•™å¸ˆæ¨¡å‹çš„softmaxæ¦‚ç‡åˆ†å¸ƒè¢«ç”¨ä½œè½¯æ ‡ç­¾ï¼ˆsoft labelsï¼‰ã€‚å°æ¨¡å‹ä»…å­¦ä¹ è½¯æ ‡ç­¾ï¼Œå¹¶ä½¿ç”¨KL-Lossæ¥ä¼˜åŒ–æ¨¡å‹çš„å‚æ•°ã€‚ é€šä¿—åœ°è¯´ï¼ŒSFTç›´æ¥å­¦ä¹ è€å¸ˆç»™çš„è§£é¢˜ç­”æ¡ˆã€‚è€ŒKDè¿‡ç¨‹ç›¸å½“äºâ€œæ‰“å¼€â€è€å¸ˆèªæ˜çš„å¤§è„‘ï¼Œå°½å¯èƒ½åœ°æ¨¡ä»¿è€å¸ˆâ€œå¤§è„‘â€æ€è€ƒé—®é¢˜çš„ç¥ç»å…ƒçŠ¶æ€ã€‚ ä¾‹å¦‚ï¼Œå½“è€å¸ˆæ¨¡å‹è®¡ç®—&lt;code&gt;1+1=2&lt;/code&gt;è¿™ä¸ªé—®é¢˜çš„æ—¶å€™ï¼Œæœ€åä¸€å±‚ç¥ç»å…ƒaçŠ¶æ€ä¸º0ï¼Œç¥ç»å…ƒbçŠ¶æ€ä¸º100ï¼Œç¥ç»å…ƒcçŠ¶æ€ä¸º-99... å­¦ç”Ÿæ¨¡å‹é€šè¿‡å¤§é‡æ•°æ®ï¼Œå­¦ä¹ æ•™å¸ˆæ¨¡å‹å¤§è„‘å†…éƒ¨çš„è¿è½¬è§„å¾‹ã€‚è¿™ä¸ªè¿‡ç¨‹å³ç§°ä¹‹ä¸ºï¼šçŸ¥è¯†è’¸é¦ã€‚ çŸ¥è¯†è’¸é¦çš„ç›®çš„åªæœ‰ä¸€ä¸ªï¼šè®©å°æ¨¡å‹ä½“ç§¯æ›´å°çš„åŒæ—¶æ•ˆæœæ›´å¥½ã€‚ ç„¶è€Œéšç€LLMè¯ç”Ÿå’Œå‘å±•ï¼Œæ¨¡å‹è’¸é¦ä¸€è¯è¢«å¹¿æ³›æ»¥ç”¨ï¼Œä»è€Œäº§ç”Ÿäº†â€œç™½ç›’/é»‘ç›’â€çŸ¥è¯†è’¸é¦ä¸¤ä¸ªæ´¾åˆ«ã€‚ GPT-4è¿™ç§é—­æºæ¨¡å‹ï¼Œç”±äºæ— æ³•è·å–å…¶å†…éƒ¨ç»“æ„ï¼Œå› æ­¤åªèƒ½é¢å‘å®ƒæ‰€è¾“å‡ºçš„æ•°æ®å­¦ä¹ ï¼Œè¿™ä¸ªè¿‡ç¨‹ç§°ä¹‹ä¸ºé»‘ç›’è’¸é¦ï¼Œä¹Ÿæ˜¯å¤§æ¨¡å‹æ—¶ä»£æœ€æ™®éçš„åšæ³•ã€‚ é»‘ç›’è’¸é¦ä¸SFTè¿‡ç¨‹å®Œå…¨ä¸€è‡´ï¼Œåªä¸è¿‡æ•°æ®æ˜¯ä»å¤§æ¨¡å‹çš„è¾“å‡ºæ”¶é›†ï¼Œå› æ­¤åªéœ€è¦å‡†å¤‡æ•°æ®å¹¶ä¸”è¿›ä¸€æ­¥FTå³å¯ã€‚ æ³¨æ„æ›´æ”¹è¢«åŠ è½½çš„åŸºç¡€æ¨¡å‹ä¸º&lt;code&gt;full_sft_*.pth&lt;/code&gt;ï¼Œå³åŸºäºå¾®è°ƒæ¨¡å‹åšè¿›ä¸€æ­¥çš„è’¸é¦å­¦ä¹ ã€‚ &lt;code&gt;./dataset/sft_1024.jsonl&lt;/code&gt;ä¸&lt;code&gt;./dataset/sft_2048.jsonl&lt;/code&gt; å‡æ”¶é›†è‡ªqwen2.5-7/72B-Instructå¤§æ¨¡å‹ï¼Œå¯ç›´æ¥ç”¨äºSFTä»¥è·å–Qwençš„éƒ¨åˆ†è¡Œä¸ºã€‚&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# æ³¨æ„éœ€è¦æ›´æ”¹train_full_sft.pyæ•°æ®é›†è·¯å¾„ï¼Œä»¥åŠmax_seq_len  
torchrun --nproc_per_node 1 train_full_sft.py
# or
python train_full_sft.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;è®­ç»ƒåçš„æ¨¡å‹æƒé‡æ–‡ä»¶é»˜è®¤æ¯éš”&lt;code&gt;100æ­¥&lt;/code&gt;åŒæ ·ä¿å­˜ä¸º: &lt;code&gt;full_sft_*.pth&lt;/code&gt;ï¼ˆ*ä¸ºæ¨¡å‹å…·ä½“dimensionï¼Œæ¯æ¬¡ä¿å­˜æ—¶æ–°æ–‡ä»¶ä¼šè¦†ç›–æ—§æ–‡ä»¶ï¼‰&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;æ­¤å¤„åº”å½“ç€é‡ä»‹ç»MiniMindå®ç°çš„ç™½ç›’è’¸é¦ä»£ç &lt;code&gt;train_distillation.py&lt;/code&gt;ï¼Œç”±äºMiniMindåŒç³»åˆ—æœ¬èº«å¹¶ä¸å­˜åœ¨å¼ºå¤§çš„æ•™å¸ˆæ¨¡å‹ï¼Œå› æ­¤ç™½ç›’è’¸é¦ä»£ç ä»…ä½œä¸ºå­¦ä¹ å‚è€ƒã€‚&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_distillation.py
# or
python train_distillation.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;strong&gt;4. LoRA (Low-Rank Adaptation)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;LoRAæ˜¯ä¸€ç§é«˜æ•ˆçš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆParameter-Efficient Fine-Tuning, PEFTï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡ä½ç§©åˆ†è§£çš„æ–¹å¼å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ ç›¸æ¯”äºå…¨å‚æ•°å¾®è°ƒï¼ˆFull Fine-Tuningï¼‰ï¼ŒLoRA åªéœ€è¦æ›´æ–°å°‘é‡çš„å‚æ•°ã€‚ LoRA çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šåœ¨æ¨¡å‹çš„æƒé‡çŸ©é˜µä¸­å¼•å…¥ä½ç§©åˆ†è§£ï¼Œä»…å¯¹ä½ç§©éƒ¨åˆ†è¿›è¡Œæ›´æ–°ï¼Œè€Œä¿æŒåŸå§‹é¢„è®­ç»ƒæƒé‡ä¸å˜ã€‚ ä»£ç å¯è§&lt;code&gt;./model/model_lora.py&lt;/code&gt;å’Œ&lt;code&gt;train_lora.py&lt;/code&gt;ï¼Œå®Œå…¨ä»0å®ç°LoRAæµç¨‹ï¼Œä¸ä¾èµ–ç¬¬ä¸‰æ–¹åº“çš„å°è£…ã€‚&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_lora.py
# or
python train_lora.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;è®­ç»ƒåçš„æ¨¡å‹æƒé‡æ–‡ä»¶é»˜è®¤æ¯éš”&lt;code&gt;100æ­¥&lt;/code&gt;ä¿å­˜ä¸º: &lt;code&gt;lora_xxx_*.pth&lt;/code&gt;ï¼ˆ* ä¸ºæ¨¡å‹å…·ä½“dimensionï¼Œæ¯æ¬¡ä¿å­˜æ—¶æ–°æ–‡ä»¶ä¼šè¦†ç›–æ—§æ–‡ä»¶ï¼‰&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;éå¸¸å¤šçš„äººå›°æƒ‘ï¼Œå¦‚ä½•ä½¿æ¨¡å‹å­¦ä¼šè‡ªå·±ç§æœ‰é¢†åŸŸçš„çŸ¥è¯†ï¼Ÿå¦‚ä½•å‡†å¤‡æ•°æ®é›†ï¼Ÿå¦‚ä½•è¿ç§»é€šç”¨é¢†åŸŸæ¨¡å‹æ‰“é€ å‚åŸŸæ¨¡å‹ï¼Ÿ è¿™é‡Œä¸¾å‡ ä¸ªä¾‹å­ï¼Œå¯¹äºé€šç”¨æ¨¡å‹ï¼ŒåŒ»å­¦é¢†åŸŸçŸ¥è¯†æ¬ ç¼ºï¼Œå¯ä»¥å°è¯•åœ¨åŸæœ‰æ¨¡å‹åŸºç¡€ä¸ŠåŠ å…¥é¢†åŸŸçŸ¥è¯†ï¼Œä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚ åŒæ—¶ï¼Œé€šå¸¸ä¸å¸Œæœ›å­¦ä¼šé¢†åŸŸçŸ¥è¯†çš„åŒæ—¶æŸå¤±åŸæœ‰åŸºç¡€æ¨¡å‹çš„å…¶å®ƒèƒ½åŠ›ï¼Œæ­¤æ—¶LoRAå¯ä»¥å¾ˆå¥½çš„æ”¹å–„è¿™ä¸ªé—®é¢˜ã€‚ åªéœ€è¦å‡†å¤‡å¦‚ä¸‹æ ¼å¼çš„å¯¹è¯æ•°æ®é›†æ”¾ç½®åˆ°&lt;code&gt;./dataset/lora_xxx.jsonl&lt;/code&gt;ï¼Œå¯åŠ¨ &lt;code&gt;python train_lora.py&lt;/code&gt; è®­ç»ƒå³å¯å¾—åˆ°&lt;code&gt;./out/lora/lora_xxx.pth&lt;/code&gt;æ–°æ¨¡å‹æƒé‡ã€‚&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;åŒ»ç–—åœºæ™¯&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; {"conversations": [{"role": "user", "content": "è¯·é—®é¢ˆæ¤ç—…çš„äººæ•å¤´å¤šé«˜æ‰æœ€å¥½ï¼Ÿ"}, {"role": "assistant", "content": "é¢ˆæ¤ç—…æ‚£è€…é€‰æ‹©æ•å¤´çš„é«˜åº¦åº”è¯¥æ ¹æ®..."}]}
 {"conversations": [{"role": "user", "content": "è¯·é—®xxx"}, {"role": "assistant", "content": "xxx..."}]}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;è‡ªæˆ‘è®¤çŸ¥åœºæ™¯&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; {"conversations": [{"role": "user", "content": "ä½ å«ä»€ä¹ˆåå­—ï¼Ÿ"}, {"role": "assistant", "content": "æˆ‘å«minimind..."}]}
 {"conversations": [{"role": "user", "content": "ä½ æ˜¯è°"}, {"role": "assistant", "content": "æˆ‘æ˜¯..."}]}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;æ­¤æ—¶ã€åŸºç¡€æ¨¡å‹+LoRAæ¨¡å‹ã€‘å³å¯è·å¾—åŒ»ç–—åœºæ™¯æ¨¡å‹å¢å¼ºçš„èƒ½åŠ›ï¼Œç›¸å½“äºä¸ºåŸºç¡€æ¨¡å‹å¢åŠ äº†LoRAå¤–æŒ‚ï¼Œè¿™ä¸ªè¿‡ç¨‹å¹¶ä¸æŸå¤±åŸºç¡€æ¨¡å‹çš„æœ¬èº«èƒ½åŠ›ã€‚ å¯ä»¥é€šè¿‡&lt;code&gt;eval_llm.py&lt;/code&gt;è¿›è¡Œæ¨¡å‹è¯„ä¼°æµ‹è¯•ã€‚&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# æ³¨æ„ï¼šweightå‚æ•°æŒ‡å®šåŸºç¡€æ¨¡å‹ç±»å‹ï¼Œéœ€ä¸train_loraè®­ç»ƒæ—¶ä½¿ç”¨çš„åŸºç¡€æ¨¡å‹ä¿æŒä¸€è‡´
python eval_llm.py  --weight dpo --lora_weight lora_medical
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;å°æµ‹è¯•&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;ğŸ‘¶: æˆ‘æœ€è¿‘ç»å¸¸æ„Ÿåˆ°å¤´æ™•ï¼Œå¯èƒ½æ˜¯ä»€ä¹ˆåŸå› ï¼Ÿ
ğŸ¤–ï¸: å¤´æ™•å¯èƒ½æ˜¯ç”±äºå¤šç§åŸå› ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼šå¿ƒè„ç–¾ç—…ã€çœ¼ç›æŸä¼¤ã€è¿‡æ•ååº”ã€çš®è‚¤é—®é¢˜ã€è¿‡æ•ååº”ã€å‹åŠ›ã€ç„¦è™‘ã€ç–²åŠ³ã€è¯ç‰©å‰¯ä½œç”¨ã€æ€§åŠŸèƒ½éšœç¢æˆ–è€…å…¶ä»–...

---

ğŸ‘¶: ä½ å’Œopenaiæ˜¯ä»€ä¹ˆå…³ç³»ï¼Ÿ
ğŸ¤–ï¸: æˆ‘æ˜¯ MiniMindï¼Œä¸€ä¸ªç”± Jingyao Gong å¼€å‘çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚æˆ‘é€šè¿‡è‡ªç„¶è¯­è¨€å¤„ç†å’Œç®—æ³•è®­ç»ƒæ¥ä¸ç”¨æˆ·è¿›è¡Œäº¤äº’ã€‚
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;PSï¼šåªè¦æœ‰æ‰€éœ€è¦çš„æ•°æ®é›†ï¼Œä¹Ÿå¯ä»¥full_sftå…¨å‚å¾®è°ƒï¼ˆéœ€è¦è¿›è¡Œé€šç”¨çŸ¥è¯†çš„æ··åˆé…æ¯”ï¼Œå¦åˆ™è¿‡æ‹Ÿåˆé¢†åŸŸæ•°æ®ä¼šè®©æ¨¡å‹å˜å‚»ï¼ŒæŸå¤±é€šç”¨æ€§ï¼‰&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;5. è®­ç»ƒæ¨ç†æ¨¡å‹ (Reasoning Model)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;DeepSeek-R1å®åœ¨å¤ªç«äº†ï¼Œå‡ ä¹é‡æ–°æŒ‡æ˜äº†æœªæ¥LLMçš„æ–°èŒƒå¼ã€‚ è®ºæ–‡æŒ‡å‡º&lt;code&gt;&amp;gt;3B&lt;/code&gt;çš„æ¨¡å‹ç»å†å¤šæ¬¡åå¤çš„å†·å¯åŠ¨å’ŒRLå¥–åŠ±è®­ç»ƒæ‰èƒ½è·å¾—è‚‰çœ¼å¯è§çš„æ¨ç†èƒ½åŠ›æå‡ã€‚ æœ€å¿«æœ€ç¨³å¦¥æœ€ç»æµçš„åšæ³•ï¼Œä»¥åŠæœ€è¿‘çˆ†å‘çš„å„ç§å„æ ·æ‰€è°“çš„æ¨ç†æ¨¡å‹å‡ ä¹éƒ½æ˜¯ç›´æ¥é¢å‘æ•°æ®è¿›è¡Œè’¸é¦è®­ç»ƒï¼Œ ä½†ç”±äºç¼ºä¹æŠ€æœ¯å«é‡ï¼Œè’¸é¦æ´¾è¢«RLæ´¾ç§ä¸èµ·ï¼ˆhhhhï¼‰ã€‚ æœ¬äººè¿…é€Ÿå·²ç»åœ¨Qwenç³»åˆ—1.5Bå°æ¨¡å‹ä¸Šè¿›è¡Œäº†å°è¯•ï¼Œå¾ˆå¿«å¤ç°äº†Zeroè¿‡ç¨‹çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚ ç„¶è€Œä¸€ä¸ªé—æ†¾çš„å…±è¯†æ˜¯ï¼šå‚æ•°å¤ªå°çš„æ¨¡å‹ç›´æ¥é€šè¿‡å†·å¯åŠ¨SFT+GRPOå‡ ä¹ä¸å¯èƒ½è·å¾—ä»»ä½•æ¨ç†æ•ˆæœã€‚ &lt;del&gt; MiniMind2ç¬¬ä¸€æ—¶é—´åªèƒ½åšå®šä¸ç§»çš„é€‰æ‹©åšè’¸é¦æ´¾ï¼Œæ—¥ååŸºäº0.1Bæ¨¡å‹çš„RLå¦‚æœåŒæ ·å–å¾—å°å°è¿›å±•ä¼šæ›´æ–°æ­¤éƒ¨åˆ†çš„è®­ç»ƒæ–¹æ¡ˆã€‚ &lt;/del&gt;&lt;/p&gt; 
&lt;p&gt;åšè’¸é¦éœ€è¦å‡†å¤‡çš„ä¾ç„¶æ˜¯å’ŒSFTé˜¶æ®µåŒæ ·æ ¼å¼çš„æ•°æ®å³å¯ï¼Œæ•°æ®é›†æ¥æºå·²å¦‚ä¸Šæ–‡ä»‹ç»ã€‚æ•°æ®æ ¼å¼ä¾‹å¦‚ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "conversations": [
    {
      "role": "user",
      "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯å°èŠ³ï¼Œå¾ˆé«˜å…´è®¤è¯†ä½ ã€‚"
    },
    {
      "role": "assistant",
      "content": "&amp;lt;think&amp;gt;\nä½ å¥½ï¼æˆ‘æ˜¯ç”±ä¸­å›½çš„ä¸ªäººå¼€å‘è€…ç‹¬ç«‹å¼€å‘çš„æ™ºèƒ½åŠ©æ‰‹MiniMind-R1-Lite-Previewï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æä¾›æœåŠ¡ï¼\n&amp;lt;/think&amp;gt;\n&amp;lt;answer&amp;gt;\nä½ å¥½ï¼æˆ‘æ˜¯ç”±ä¸­å›½çš„ä¸ªäººå¼€å‘è€…ç‹¬ç«‹å¼€å‘çš„æ™ºèƒ½åŠ©æ‰‹MiniMind-R1-Lite-Previewï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æä¾›æœåŠ¡ï¼\n&amp;lt;/answer&amp;gt;"
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;æ¨ç†æ¨¡å‹R1çš„å›å¤æ¨¡æ¿æ˜¯ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;&amp;lt;think&amp;gt;\næ€è€ƒè¿‡ç¨‹\n&amp;lt;/think&amp;gt;\n
&amp;lt;answer&amp;gt;\næœ€ç»ˆå›ç­”\n&amp;lt;/answer&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;è¿™åœ¨GRPOä¸­é€šè¿‡è®¾ç½®è§„åˆ™å¥–åŠ±å‡½æ•°çº¦æŸæ¨¡å‹ç¬¦åˆæ€è€ƒæ ‡ç­¾å’Œå›å¤æ ‡ç­¾ï¼ˆåœ¨å†·å¯åŠ¨é å‰çš„é˜¶æ®µå¥–åŠ±å€¼è®¾ç½®åº”è¯¥æé«˜ä¸€äº›ï¼‰&lt;/p&gt; 
&lt;p&gt;å¦ä¸€ä¸ªé—®é¢˜æ˜¯è’¸é¦è¿‡ç¨‹è™½ç„¶å’ŒSFTä¸€æ ·ï¼Œä½†å®éªŒç»“æœæ˜¯æ¨¡å‹éš¾ä»¥æ¯æ¬¡éƒ½ç¬¦åˆæ¨¡æ¿è§„èŒƒçš„å›å¤ï¼Œå³è„±ç¦»æ€è€ƒå’Œå›å¤æ ‡ç­¾çº¦æŸã€‚ è¿™é‡Œçš„å°æŠ€å·§æ˜¯å¢åŠ æ ‡è®°ä½ç½®tokençš„æŸå¤±æƒ©ç½šï¼Œè¯¦è§&lt;code&gt;train_distill_reason.py&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;# åœ¨ sp_ids å¯¹åº”çš„ä½ç½®å¢åŠ é¢å¤–çš„æƒ©ç½š
...
loss_mask[sp_ids] = 10 # æƒ©ç½šç³»æ•°
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;å¦å¦ä¸€ä¸ªtipsæ˜¯ç”±äºæ¨ç†æ•°æ®ç”±äºåªç­›é€‰äº†&lt;code&gt;&amp;lt;1024&lt;/code&gt;é•¿åº¦çš„æ•°æ®ï¼Œå…¶ä¸­å¤šè½®å¯¹è¯å’Œè‹±æ–‡æ•°æ®åå°‘ï¼Œ å› æ­¤&lt;code&gt;r1_mix_1024.jsonl&lt;/code&gt;è¿›è¡Œäº†å¤§çº¦10kæ¡å¤šè½®å¯¹è¯+è‹±æ–‡æ•°æ®çš„æ··åˆï¼Œé˜²æ­¢æ¨¡å‹é—å¿˜ä¸¥é‡ã€‚&lt;/p&gt; 
&lt;p&gt;è„šæœ¬é»˜è®¤åŸºäºrlhfåçš„åŸºæ¨¡å‹åšæ¨ç†èƒ½åŠ›çš„è’¸é¦å¾®è°ƒï¼Œä¸‹é¢ç›´æ¥å¯åŠ¨è®­ç»ƒå³å¯ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_distill_reason.py
# or
python train_distill_reason.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;è®­ç»ƒåçš„æ¨¡å‹æƒé‡æ–‡ä»¶é»˜è®¤æ¯éš”&lt;code&gt;100æ­¥&lt;/code&gt;ä¿å­˜ä¸º: &lt;code&gt;reason_*.pth&lt;/code&gt;ï¼ˆ*ä¸ºæ¨¡å‹å…·ä½“dimensionï¼Œæ¯æ¬¡ä¿å­˜æ—¶æ–°æ–‡ä»¶ä¼šè¦†ç›–æ—§æ–‡ä»¶ï¼‰&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;æµ‹è¯•ä¸€ä¸‹ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;ğŸ‘¶: è¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚
ğŸ¤–ï¸: &amp;lt;think&amp;gt;
å—¯ï¼Œç”¨æˆ·è®©æˆ‘ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚æˆ‘ç°åœ¨éœ€è¦è§£é‡Šä¸€ä¸‹è‡ªå·±ã€‚é¦–å…ˆï¼Œæˆ‘åº”è¯¥æ˜ç¡®ç”¨æˆ·æ˜¯è°ï¼Œæ˜¯å­¦ç”Ÿã€èŒåœºäººå£«è¿˜æ˜¯æ™®é€šäººã€‚ç”¨æˆ·å¯èƒ½æ˜¯ä¸€ä¸ªå­¦ç”Ÿæˆ–è€…èŒåœºäººå£«ï¼Œæ‰€ä»¥ä¿¡æ¯ä¼ é€’åº”è¯¥æ˜¯å¸®åŠ©å­¦ç”Ÿå’ŒèŒåœºäººå£«çš„ä¿¡æ¯ã€‚
ç„¶åï¼Œæˆ‘éœ€è¦å›åº”ç”¨æˆ·ï¼Œè¿™å¯èƒ½åŒ…æ‹¬ä»‹ç»è‡ªå·±çš„ä½ç½®ã€å…´è¶£çˆ±å¥½ã€èŒä¸šç»å†ç­‰ä¿¡æ¯ã€‚ç”¨æˆ·è¿˜å¯èƒ½æƒ³äº†è§£è‡ªå·±åœ¨å“ªé‡Œï¼Œæ¯”å¦‚èŒä¸šå‘å±•ã€å·¥ä½œç»éªŒç­‰ã€‚
æˆ‘åº”è¯¥æ³¨æ„ç”¨è¯è¦æ­£å¼ï¼Œä¸è¦å¤ªæ­£å¼ï¼ŒåŒæ—¶è¦ä¿æŒç®€æ´æ˜äº†ã€‚è¿™æ ·ç”¨æˆ·å®¹æ˜“ç†è§£ï¼Œä¹Ÿæ–¹ä¾¿ä»–ä»¬æŸ¥é˜…ç›¸å…³èµ„æ–™ã€‚
æœ€åï¼Œæˆ‘åº”è¯¥æ€»ç»“ä¸€ä¸‹ï¼Œè®©ç”¨æˆ·çŸ¥é“æˆ‘æ˜¯å¦‚ä½•å¸®åŠ©ä»–ä»¬å®Œæˆä»–ä»¬çš„ä»»åŠ¡çš„ï¼Œè¿™æ ·ä»–ä»¬æœ‰æ›´å¤šå…³äºæˆ‘çš„ä¿¡æ¯ã€‚
&amp;lt;/think&amp;gt;
&amp;lt;answer&amp;gt;
æˆ‘æ˜¯**â€œä½ çš„åå­—â€ï¼Œä½ å«ä»€ä¹ˆåå­—ï¼Ÿ
&amp;lt;/answer&amp;gt;

ğŸ‘¶: ä½ æ›´æ“…é•¿å“ªä¸€ä¸ªå­¦ç§‘ï¼Ÿ
ğŸ¤–ï¸: &amp;lt;think&amp;gt;
æ‚¨å¥½ï¼æˆ‘æ˜¯ç”±ä¸­å›½çš„ä¸ªäººå¼€å‘è€…å¼€å‘çš„æ™ºèƒ½åŠ©æ‰‹MiniMind-R1ã€‚å¦‚æ‚¨æœ‰ä»»ä½•ä»»ä½•é—®é¢˜ï¼Œæˆ‘ä¼šå°½æˆ‘æ‰€èƒ½ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚
&amp;lt;/think&amp;gt;
&amp;lt;answer&amp;gt;
æ‚¨å¥½ï¼æˆ‘æ˜¯ç”±ä¸­å›½çš„ä¸ªäººå¼€å‘è€…å¼€å‘çš„æ™ºèƒ½åŠ©æ‰‹MiniMind-R1ã€‚å¦‚æ‚¨æœ‰ä»»ä½•ä»»ä½•é—®é¢˜ï¼Œæˆ‘ä¼šå°½æˆ‘æ‰€èƒ½ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚
&amp;lt;/answer&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;IV å¼ºåŒ–å­¦ä¹ åè®­ç»ƒ&lt;/h2&gt; 
&lt;p&gt;LLMé‡Œçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¯åˆ†ä¸¤ç±»ï¼š&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹  (Reinforcement Learning from Human Feedback, RLHF)&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;é€šè¿‡&lt;strong&gt;äººç±»&lt;/strong&gt;å¯¹æ¨¡å‹è¾“å‡ºçš„åå¥½è¿›è¡Œè¯„ä»·æ¥è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶ç”Ÿæˆæ›´ç¬¦åˆäººç±»ä»·å€¼è§‚å’Œåå¥½çš„å†…å®¹ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;&lt;strong&gt;åŸºäºAIåé¦ˆçš„å¼ºåŒ–å­¦ä¹  (Reinforcement Learning from AI Feedback, RLAIF)&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;ä½¿ç”¨&lt;strong&gt;AIæ¨¡å‹&lt;/strong&gt;ï¼ˆé€šå¸¸æ˜¯é¢„è®­ç»ƒçš„è¯­è¨€å¥–åŠ±æ¨¡å‹ï¼‰æ¥æä¾›åé¦ˆï¼Œè€Œä¸ç›´æ¥ä¾èµ–äººç±»çš„äººå·¥æ ‡æ³¨ã€‚&lt;/li&gt; 
 &lt;li&gt;è¿™é‡Œçš„â€œAIâ€ä¹Ÿå¯ä»¥æ˜¯æŸäº›è§„åˆ™å¥–åŠ±ï¼Œä¾‹å¦‚æ•°å­¦ç­”æ¡ˆ/ä»£ç è§£é‡Šå™¨...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ç±»å‹&lt;/th&gt; 
   &lt;th&gt;è£åˆ¤&lt;/th&gt; 
   &lt;th&gt;ä¼˜ç‚¹&lt;/th&gt; 
   &lt;th&gt;ç¼ºç‚¹&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;RLHF&lt;/td&gt; 
   &lt;td&gt;äººç±»&lt;/td&gt; 
   &lt;td&gt;æ›´è´´è¿‘çœŸå®äººç±»åå¥½&lt;/td&gt; 
   &lt;td&gt;æˆæœ¬é«˜ã€æ•ˆç‡ä½&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;RLAIF&lt;/td&gt; 
   &lt;td&gt;æ¨¡å‹&lt;/td&gt; 
   &lt;td&gt;è‡ªåŠ¨åŒ–ã€å¯æ‰©å±•æ€§å¼º&lt;/td&gt; 
   &lt;td&gt;å¯èƒ½åç¦»äººç±»çœŸå®åå¥½&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;äºŒè€…æœ¬è´¨ä¸Šæ˜¯ä¸€æ ·çš„ï¼Œéƒ½æ˜¯é€šè¿‡&lt;strong&gt;å¼ºåŒ–å­¦ä¹ çš„æ–¹å¼&lt;/strong&gt;ï¼Œåˆ©ç”¨æŸç§å½¢å¼çš„"&lt;strong&gt;åé¦ˆ&lt;/strong&gt;"æ¥ä¼˜åŒ–æ¨¡å‹çš„è¡Œä¸ºã€‚&lt;/p&gt; 
&lt;p&gt;é™¤äº†&lt;strong&gt;åé¦ˆ&lt;/strong&gt;çš„æ¥æºä¸åŒï¼Œå…¶ä»–å¹¶æ— ä»»ä½•åŒºåˆ«ã€‚&lt;/p&gt; 
&lt;h3&gt;ğŸ‘€ POç®—æ³•çš„ç»Ÿä¸€è§†è§’&lt;/h3&gt; 
&lt;p&gt;åœ¨ä»‹ç»å®ç°å…·ä½“ç®—æ³•ä¹‹å‰ï¼Œæˆ‘å…ˆä»¥ä¸ªäººç†è§£çš„æç®€è§†è§’ï¼Œé˜è¿°æ‰€æœ‰Policy Optimization (PO)ç®—æ³•çš„ç»Ÿä¸€å…±æ€§ã€‚&lt;/p&gt; 
&lt;p&gt;æ‰€æœ‰RLç®—æ³•çš„æœ¬è´¨éƒ½åªæ˜¯åœ¨ä¼˜åŒ–ä¸€ä¸ªæœŸæœ›ï¼š&lt;/p&gt; 
&lt;p&gt;$$\mathcal{J}&lt;em&gt;{PO} = \mathbb{E}&lt;/em&gt;{q \sim P(Q), o \sim \pi(O|q)} \left[ \underbrace{f(r_t)}&lt;em&gt;{\text{ç­–ç•¥é¡¹}} \cdot \underbrace{g(A_t)}&lt;/em&gt;{\text{ä¼˜åŠ¿é¡¹}} - \underbrace{h(\text{KL}&lt;em&gt;t)}&lt;/em&gt;{\text{æ­£åˆ™é¡¹}} \right]$$&lt;/p&gt; 
&lt;p&gt;è®­ç»ƒæ—¶ï¼Œåªéœ€&lt;strong&gt;æœ€å°åŒ–è´Ÿç›®æ ‡å‡½æ•°&lt;/strong&gt;ï¼Œå³: $\mathcal{L_{PO}}=-\mathcal{J_{PO}}$&lt;/p&gt; 
&lt;p&gt;è¿™ä¸ªæ¡†æ¶åªåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ç­–ç•¥é¡¹&lt;/strong&gt; $f(r_t)$: å¦‚ä½•ä½¿ç”¨æ¦‚ç‡æ¯” $r_t$? å³å‘Šè¯‰æ¨¡å‹æ–°æ—§ç­–ç•¥åå·®æœ‰å¤šå¤§ï¼Œæ˜¯å¦æ¢ç´¢åˆ°äº†æ›´å¥½çš„token&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ä¼˜åŠ¿é¡¹&lt;/strong&gt; $g(A_t)$: å¦‚ä½•è®¡ç®—ä¼˜åŠ¿ $A_t$, è¿™å¾ˆé‡è¦ï¼å¤§æ¨¡å‹ç®—å¯¹å®šç§¯åˆ†ä¹Ÿä¸è¶³ä¸ºå¥‡ï¼Œå°æ¨¡å‹å›ç­”å¯¹åŠ å‡æ³•ä¼˜åŠ¿é€šå¸¸éƒ½æ˜¯æ­£çš„&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ­£åˆ™é¡¹&lt;/strong&gt; $h(\text{KL}_t)$: å¦‚ä½•çº¦æŸå˜åŒ–å¹…åº¦ $\text{KL}_t$, æ—¢é˜²æ­¢è·‘ååˆé˜²æ­¢ç®¡çš„å¤ªæ­»&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;ï¼ˆå±•å¼€ï¼‰ç¬¦å·è¯´æ˜&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;ç¬¦å·&lt;/th&gt; 
    &lt;th&gt;å«ä¹‰&lt;/th&gt; 
    &lt;th&gt;è¯´æ˜&lt;/th&gt; 
    &lt;th&gt;å€¼åŸŸ&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;$q$&lt;/td&gt; 
    &lt;td&gt;é—®é¢˜/æç¤ºè¯&lt;/td&gt; 
    &lt;td&gt;ä»æ•°æ®é›† $P(Q)$ ä¸­é‡‡æ ·&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;$o$&lt;/td&gt; 
    &lt;td&gt;æ¨¡å‹è¾“å‡ºåºåˆ—&lt;/td&gt; 
    &lt;td&gt;ç”±ç­–ç•¥ $\pi$ ç”Ÿæˆ&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;$r_t$&lt;/td&gt; 
    &lt;td&gt;æ¦‚ç‡æ¯”&lt;/td&gt; 
    &lt;td&gt;$r_t = \frac{\pi_\theta(o_t|q, o_{&amp;lt;t})}{\pi_{ref}(o_t|q, o_{&amp;lt;t})}$&lt;/td&gt; 
    &lt;td&gt;$(0, +\infty)$&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;$A_t$&lt;/td&gt; 
    &lt;td&gt;ä¼˜åŠ¿å‡½æ•°&lt;/td&gt; 
    &lt;td&gt;è¡¡é‡æŸä¸ªåŠ¨ä½œç›¸æ¯”åŸºçº¿æœ‰å¤šå¥½&lt;/td&gt; 
    &lt;td&gt;$(-\infty, +\infty)$&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;$\text{KL}_t$&lt;/td&gt; 
    &lt;td&gt;KLæ•£åº¦&lt;/td&gt; 
    &lt;td&gt;é˜²æ­¢ç­–ç•¥åç¦»å‚è€ƒæ¨¡å‹å¤ªè¿œ&lt;/td&gt; 
    &lt;td&gt;$[0, +\infty)$&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;p&gt;ä¸åŒçš„&lt;strong&gt;xxPOç®—æ³•&lt;/strong&gt;æœ¬è´¨ä¸Šåªæ˜¯å¯¹è¿™ä¸‰ä¸ªç»„ä»¶çš„ä¸åŒè®¾è®¡çš„å®ä¾‹åŒ–ï¼&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;&lt;strong&gt;6. åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹  (Reinforcement Learning from Human Feedback, RLHF)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;åœ¨å‰é¢çš„è®­ç»ƒæ­¥éª¤ä¸­ï¼Œæ¨¡å‹å·²ç»å…·å¤‡äº†åŸºæœ¬çš„å¯¹è¯èƒ½åŠ›ï¼Œä½†æ˜¯è¿™æ ·çš„èƒ½åŠ›å®Œå…¨åŸºäºå•è¯æ¥é¾™ï¼Œç¼ºå°‘æ­£åæ ·ä¾‹çš„æ¿€åŠ±ã€‚ æ¨¡å‹æ­¤æ—¶å°šæœªçŸ¥ä»€ä¹ˆå›ç­”æ˜¯å¥½çš„ï¼Œä»€ä¹ˆæ˜¯å·®çš„ã€‚å¸Œæœ›å®ƒèƒ½å¤Ÿæ›´ç¬¦åˆäººçš„åå¥½ï¼Œé™ä½è®©äººç±»ä¸æ»¡æ„ç­”æ¡ˆçš„äº§ç”Ÿæ¦‚ç‡ã€‚ è¿™ä¸ªè¿‡ç¨‹å°±åƒæ˜¯è®©æ¨¡å‹å‚åŠ æ–°çš„åŸ¹è®­ï¼Œä»ä¼˜ç§€å‘˜å·¥çš„ä½œä¸ºä¾‹å­ï¼Œæ¶ˆæå‘˜å·¥ä½œä¸ºåä¾‹ï¼Œå­¦ä¹ å¦‚ä½•æ›´å¥½åœ°å›å¤ã€‚&lt;/p&gt; 
&lt;h4&gt;6.1 Direct Preference Optimization&lt;/h4&gt; 
&lt;p&gt;ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç®—æ³•ï¼ŒæŸå¤±ä¸ºï¼š&lt;/p&gt; 
&lt;p&gt;$$\mathcal{L}&lt;em&gt;{DPO} = -\mathbb{E}\left[\log \sigma\left(\beta \left[\log \frac{\pi&lt;/em&gt;\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \log \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}\right]\right)\right]$$&lt;/p&gt; 
&lt;p&gt;å…¶ä¸­ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ç­–ç•¥é¡¹&lt;/strong&gt;: $f(r_t) = \log r_w - \log r_l$ (å¯¹æ¯”chosen vs rejectedçš„æ¦‚ç‡æ¯”)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ä¼˜åŠ¿é¡¹&lt;/strong&gt;: $g(A_t)$ = / (é€šè¿‡åå¥½å¯¹æ¯”ï¼Œæ— éœ€æ˜¾å¼è®¡ç®—ä¼˜åŠ¿)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ­£åˆ™é¡¹&lt;/strong&gt;: $h(\text{KL}_t)$ = éšå«åœ¨ $\beta$ ä¸­ (æ§åˆ¶åç¦»å‚è€ƒæ¨¡å‹ç¨‹åº¦)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ç‰¹åˆ«åœ°ï¼Œ&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;DPOä»PPOå¸¦KLçº¦æŸçš„ç›®æ ‡æ¨å¯¼å‡ºå¯¹åå¥½å¯¹çš„è§£æè®­ç»ƒç›®æ ‡ï¼Œç›´æ¥æœ€å¤§åŒ–"chosenä¼˜äºrejected"çš„å¯¹æ•°å‡ ç‡ï¼›æ— éœ€åŒæ­¥è®­ç»ƒReward/Valueæ¨¡å‹ã€‚DPOåªéœ€è·‘&lt;code&gt;actor&lt;/code&gt;ä¸&lt;code&gt;ref&lt;/code&gt;ä¸¤ä¸ªæ¨¡å‹ï¼Œæ˜¾å­˜å ç”¨ä½ã€æ”¶æ•›ç¨³å®šã€å®ç°ç®€å•ã€‚&lt;/li&gt; 
 &lt;li&gt;è®­ç»ƒèŒƒå¼ï¼šoffâ€‘policyï¼Œä½¿ç”¨é™æ€åå¥½æ•°æ®é›†ï¼Œå¯åå¤å¤šè½®epochï¼›Refæ¨¡å‹å›ºå®šï¼ˆé¢„å…ˆç¼“å­˜è¾“å‡ºï¼‰ã€‚&lt;/li&gt; 
 &lt;li&gt;DPOçš„å±€é™åœ¨äºä¸åšåœ¨çº¿æ¢ç´¢ï¼Œæ›´å¤šç”¨äº"åå¥½/å®‰å…¨"çš„äººç±»ä»·å€¼å¯¹é½ï¼›å¯¹"èƒ½ä¸èƒ½åšå¯¹é¢˜"çš„æ™ºåŠ›èƒ½åŠ›æå‡æœ‰é™ï¼ˆå½“ç„¶è¿™ä¹Ÿå–å†³äºæ•°æ®é›†ï¼Œå¤§è§„æ¨¡æ”¶é›†æ­£åæ ·æœ¬å¹¶äººç±»è¯„ä¼°å¾ˆå›°éš¾ï¼‰ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_dpo.py
# or
python train_dpo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;è®­ç»ƒåçš„æ¨¡å‹æƒé‡æ–‡ä»¶é»˜è®¤æ¯éš”&lt;code&gt;100æ­¥&lt;/code&gt;ä¿å­˜ä¸º: &lt;code&gt;dpo_*.pth&lt;/code&gt;ï¼ˆ*ä¸ºæ¨¡å‹å…·ä½“dimensionï¼Œæ¯æ¬¡ä¿å­˜æ—¶æ–°æ–‡ä»¶ä¼šè¦†ç›–æ—§æ–‡ä»¶ï¼‰&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;7. åŸºäºAIåé¦ˆçš„å¼ºåŒ–å­¦ä¹  (Reinforcement Learning from AI Feedback, RLAIF)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;ç›¸æ¯”RLHFä¾èµ–äººç±»æ ‡æ³¨chosen/rejectedåå¥½å¯¹ï¼ŒRLAIFåˆ™å®Œå…¨ç”±AIæ¥å……å½“"è£åˆ¤"ã€‚ æ‰€è°“AI"è£åˆ¤"å¯ä»¥æ˜¯model-baseçš„å¥–åŠ±å¤§æ¨¡å‹(Reward Model)ï¼Œä¹Ÿå¯ä»¥æ˜¯R1ä¸€æ ·è®¾ç½®è§„åˆ™å‡½æ•°è¿›è¡Œæ ¡éªŒï¼Œä¹Ÿå¯ä»¥æ˜¯ä¾‹å¦‚å·¥å…·è°ƒç”¨çš„ç¯å¢ƒåé¦ˆã€‚ ä¾‹å¦‚ï¼šæ•°å­¦é¢˜ç­”æ¡ˆæ˜¯å¦æ­£ç¡®ã€å·¥å…·è°ƒç”¨æ‰§è¡Œä»£ç èƒ½å¦é€šè¿‡æµ‹è¯•ç”¨ä¾‹ã€æ¨ç†è¿‡ç¨‹æ˜¯å¦ç¬¦åˆæ ¼å¼...éƒ½å¯ä»¥è‡ªåŠ¨åŒ–åˆ¤æ–­ã€‚ RLAIFçš„æœ€å¤§ä¼˜åŠ¿åœ¨äº&lt;strong&gt;å¯æ‰©å±•æ€§&lt;/strong&gt;å’Œ&lt;strong&gt;On-Policy&lt;/strong&gt;çš„ç‰¹ç‚¹â€”â€”ä¸éœ€è¦æ˜‚è´µçš„äººå·¥æ ‡æ³¨ï¼Œå¯ä»¥ç”Ÿæˆæµ·é‡çš„è®­ç»ƒæ ·æœ¬ï¼Œè®©æ¨¡å‹åœ¨åœ¨çº¿å¤§é‡è¯•é”™ä¸­å¿«é€Ÿè¿›åŒ–ã€‚&lt;/p&gt; 
&lt;p&gt;MiniMind ç€æ‰‹å®ç°&lt;strong&gt;2+N&lt;/strong&gt;ç§åŸºæœ¬+å‰æ²¿çš„RLAIFæ–¹æ³•ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;PPO&lt;/strong&gt;ã€&lt;strong&gt;GRPO&lt;/strong&gt; è¢«å¤§è§„æ¨¡éªŒè¯çš„ç»å…¸RLç®—æ³•ï¼›&lt;/li&gt; 
 &lt;li&gt;Nç§å‰æ²¿RLç®—æ³•ï¼ˆä¸å®šæœŸä»¥Expæ€§è´¨æ›´æ–°ï¼‰ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;1ï¸âƒ£ æ•°æ®é›†å‡†å¤‡ (éœ€è¦)&lt;/h4&gt; 
&lt;p&gt;ä¸ºäº†å¿«é€ŸéªŒè¯RLAIFçš„æ•ˆæœï¼Œè¿™é‡Œä»SFTæ•°æ®é›†ä¸­éšæœºé‡‡æ ·äº†1ä¸‡æ¡é«˜è´¨é‡å¯¹è¯ï¼Œæ„å»ºçº¦1MBå¤§å°çš„&lt;code&gt;rlaif-mini.jsonl&lt;/code&gt;(&lt;a href="https://huggingface.co/datasets/jingyaogong/minimind_dataset/blob/main/rlaif-mini.jsonl"&gt;Huggingface&lt;/a&gt;)&lt;/p&gt; 
&lt;p&gt;æ•°æ®æ ¼å¼ä¸SFTä¸€è‡´ï¼Œä½†assistantå¹¶ä¸éœ€è¦å†…å®¹ï¼Œå› ä¸ºè®­ç»ƒè¿‡ç¨‹ä¸­å®Œå…¨ç”± $\Pi$ ç­–ç•¥æ¨¡å‹å®æ—¶é‡‡æ ·ç”Ÿæˆã€‚å› æ­¤å½¢å¦‚ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
    "conversations": [
        {"role": "user", "content": "è¯·è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯å…‰åˆä½œç”¨ï¼Ÿ"},
        {"role": "assistant", "content": "æ— "}
    ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;RLAIFçš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ä¼šåŸºäºuserçš„é—®é¢˜ç”Ÿæˆ1æˆ–å¤šä¸ªå€™é€‰å›ç­”ï¼Œç„¶åç”±å¥–åŠ±å‡½æ•°/æ¨¡å‹å¯¹å›ç­”æ‰“åˆ†ï¼Œ åˆ†æ•°é«˜çš„å›ç­”ä¼šè¢«é¼“åŠ±ï¼ˆå¢åŠ  $\Pi$ ç­–ç•¥æ¦‚ç‡ï¼‰ï¼Œåˆ†æ•°ä½çš„å›ç­”ä¼šè¢«æŠ‘åˆ¶ï¼ˆé™ä½ $\Pi$ ç­–ç•¥æ¦‚ç‡ï¼‰ã€‚è¿™ä¸ª"æ‰“åˆ†-&amp;gt;è°ƒæ•´"çš„å¾ªç¯å°±æ˜¯å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒã€‚&lt;/p&gt; 
&lt;h4&gt;2ï¸âƒ£ å¥–åŠ±æ¨¡å‹å‡†å¤‡ (éœ€è¦)&lt;/h4&gt; 
&lt;p&gt;å·²çŸ¥RLAIFè®­ç»ƒéœ€è¦â€œå¥–åŠ±æ¨¡å‹ (Reward Model)â€å¯¹ç”Ÿæˆçš„å›ç­”è¿›è¡Œæ‰“åˆ†ã€‚&lt;/p&gt; 
&lt;p&gt;æ­¤å¤„é€‰å–å°å‹ä¸”é«˜è´¨é‡çš„InternLM2-1.8B-Reward (&lt;a href="https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-1_8b-reward"&gt;ModelScope&lt;/a&gt; | &lt;a href="https://huggingface.co/internlm/internlm2-1_8b-reward"&gt;HuggingFace&lt;/a&gt;) ä½œä¸ºåŸºç¡€å¥–åŠ±æ¨¡å‹ã€‚&lt;/p&gt; 
&lt;p&gt;ä¸‹è½½å¥–åŠ±æ¨¡å‹åéœ€è¦æ”¾ç½®åœ¨minimindé¡¹ç›®çš„&lt;strong&gt;åŒçº§ç›®å½•&lt;/strong&gt;ä¸‹ï¼Œæ¨èç»“æ„å¦‚ä¸‹ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;project/
â”œâ”€â”€ minimind/                    # MiniMindé¡¹ç›®
â”‚   â”œâ”€â”€ model/
â”‚   â””â”€â”€ ...
â””â”€â”€ internlm2-1_8b-reward/       # å¥–åŠ±æ¨¡å‹ï¼ˆä¸minimindåŒçº§ï¼‰
    â”œâ”€â”€ config.json
    â”œâ”€â”€ model.safetensors
    â””â”€â”€ ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;å¥–åŠ±æœºåˆ¶é€‰æ‹©ä¸MiniMindé™åˆ¶è¯´æ˜ï¼ˆç‚¹å‡»å±•å¼€ï¼‰&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;1. å¥–åŠ±æœºåˆ¶çš„å¤šæ ·æ€§&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;RLAIFä¸­çš„"å¥–åŠ±ä¿¡å·"æ¥æºå¯ä»¥éå¸¸çµæ´»ï¼š&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Model-basedå¥–åŠ±&lt;/strong&gt;ï¼šå¯ä½¿ç”¨ä¸“é—¨çš„Reward Modelï¼ˆå¦‚InternLM2-Rewardï¼‰ï¼Œä¹Ÿå¯ä½¿ç”¨é€šç”¨LLM+æç¤ºè¯è¿›è¡Œæ‰“åˆ†ï¼ˆå¦‚Qwen3-as-a-Judgeï¼‰ã€‚å¥–åŠ±æ¨¡å‹è§„æ¨¡å’Œæ¶æ„å‡å¯è‡ªç”±é€‰æ‹©ã€‚&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Rule-basedå¥–åŠ±&lt;/strong&gt;ï¼šå¯ä»¥åŸºäºè§„åˆ™å‡½æ•°æ„é€ å¥–åŠ±ä¿¡å·ï¼Œä¾‹å¦‚ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;æ•°å­¦é¢˜ç­”æ¡ˆæ­£ç¡®æ€§éªŒè¯ï¼ˆGround Truthå¯¹æ¯”ï¼‰&lt;/li&gt; 
    &lt;li&gt;SQLæ‰§è¡ŒæˆåŠŸç‡ä¸ç»“æœå‡†ç¡®æ€§&lt;/li&gt; 
    &lt;li&gt;ä»£ç è§£é‡Šå™¨è¿è¡Œç»“æœï¼ˆpass@kï¼‰&lt;/li&gt; 
    &lt;li&gt;å·¥å…·è°ƒç”¨è¿”å›çŠ¶æ€ï¼ˆAPIæˆåŠŸ/å¤±è´¥ï¼‰&lt;/li&gt; 
    &lt;li&gt;æ ¼å¼åˆè§„æ€§æ£€æŸ¥ï¼ˆJSON/XMLè§£æï¼‰&lt;/li&gt; 
    &lt;li&gt;æ¨ç†é“¾å®Œæ•´æ€§è¯„ä¼°ï¼ˆCoTæ­¥éª¤æ•°ï¼‰&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Environment-basedå¥–åŠ±&lt;/strong&gt;ï¼šåœ¨Agentåœºæ™¯ä¸­ï¼Œç¯å¢ƒåé¦ˆæœ¬èº«å³ä¸ºå¤©ç„¶å¥–åŠ±ï¼ˆå¦‚æ¸¸æˆå¾—åˆ†ã€Researchå®Œæ•´åº¦ã€ä»»åŠ¡å®Œæˆåº¦ï¼‰ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;ä»»ä½•èƒ½å¤Ÿé‡åŒ–"å›ç­”è´¨é‡"çš„æœºåˆ¶éƒ½å¯ä½œä¸ºRLçš„å¥–åŠ±æ¥æºã€‚DeepSeek R1å°±æ˜¯å…¸å‹æ¡ˆä¾‹ï¼šä½¿ç”¨è§„åˆ™å‡½æ•°éªŒè¯æ•°å­¦ç­”æ¡ˆæ­£ç¡®æ€§ä½œä¸ºå¥–åŠ±ï¼Œæ— éœ€é¢å¤–çš„Reward Modelã€‚&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;2. MiniMindé™åˆ¶ï¼šå¥–åŠ±ç¨€ç–é—®é¢˜&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;RLAIFè®­ç»ƒæ—¢å¯ä»¥é’ˆå¯¹æ¨ç†æ¨¡å‹ä¹Ÿå¯ä»¥é’ˆå¯¹éæ¨ç†æ¨¡å‹ï¼ŒåŒºåˆ«ä»…åœ¨äºæ ¼å¼ã€‚&lt;/p&gt; 
 &lt;p&gt;ç„¶è€Œå¯¹äºMiniMindè¿™ç§0.1Bå‚æ•°é‡æå°èƒ½åŠ›å¼±çš„æ¨¡å‹ï¼Œåœ¨é€šç”¨ä»»åŠ¡ï¼ˆå¦‚R1é£æ ¼çš„æ•°å­¦æ•°æ®é›†ï¼‰ä¸Šä¼šé‡åˆ°ä¸¥é‡çš„å¥–åŠ±ç¨€ç–(Reward Sparsity)é—®é¢˜ï¼š&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;ç°è±¡&lt;/strong&gt;ï¼šæ¨¡å‹ç”Ÿæˆçš„å€™é€‰å›ç­”å‡ ä¹å…¨éƒ¨é”™è¯¯ï¼Œå¯¼è‡´æ‰€æœ‰å¥–åŠ±åˆ†æ•° $r(x,y) \approx 0$&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;åæœ&lt;/strong&gt;ï¼šä¼˜åŠ¿å‡½æ•° $A(x,y) = r(x,y) - b(x) \approx 0$ï¼Œç­–ç•¥æ¢¯åº¦ä¿¡å·æ¶ˆå¤±ï¼Œæ— æ³•æœ‰æ•ˆæ›´æ–°å‚æ•° $\theta$&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;å¦‚åŒè®©å°å­¦ç”Ÿåšé«˜è€ƒæ•°å­¦é¢˜ï¼Œæ— è®ºå°è¯•å¤šå°‘æ¬¡éƒ½å¾—é›¶åˆ†ï¼Œæ— æ³•é€šè¿‡åˆ†æ•°å·®å¼‚å­¦ä¹ æ”¹è¿›ç­–ç•¥ã€‚å› æ­¤è¿™æ˜¯RLç®—æ³•çš„æ ¹æœ¬åŸç†é™åˆ¶çš„ã€‚&lt;/p&gt; 
 &lt;p&gt;ä¸ºç¼“è§£æ­¤é—®é¢˜ï¼ŒMiniMindçš„å®ç°é€‰æ‹©äº†&lt;strong&gt;model-basedçš„è¿ç»­æ€§å¥–åŠ±ä¿¡å·&lt;/strong&gt;ï¼š&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Reward Modelè¾“å‡ºè¿ç»­åˆ†æ•°ï¼ˆå¦‚-2.5åˆ°+3.0ï¼‰ï¼Œè€ŒéäºŒå…ƒçš„0/1&lt;/li&gt; 
  &lt;li&gt;å³ä½¿å›ç­”è´¨é‡éƒ½å·®ï¼Œä¹Ÿä»èƒ½åŒºåˆ†"æ›´æ›´å·®"(-3.0)å’Œ"æ›´å·®"(-2.8)çš„ç»†å¾®å·®å¼‚ã€‚æ‰€ä»¥è¿™ç§&lt;strong&gt;ç¨ å¯†ä¸”è¿ç»­&lt;/strong&gt;çš„å¥–åŠ±ä¿¡å·èƒ½å¤Ÿä¸ºä¼˜åŠ¿å‡½æ•° $A(x,y)$ æä¾›éé›¶æ¢¯åº¦ï¼Œä½¿å¾—ç­–ç•¥ç½‘ç»œå¾—ä»¥æ¸è¿›å¼ä¼˜åŒ–&lt;/li&gt; 
  &lt;li&gt;ä¹Ÿå¯ä»¥æ··åˆå¤šç§å¥–åŠ±æº: $r_{\text{total}} = \alpha \cdot r_{\text{model}} + \beta \cdot r_{\text{rule}}$ (ä¾‹å¦‚æ—¢å¯ä»¥æ£€æµ‹thinkæ ‡ç­¾æ ¼å¼rewardï¼Œåˆå¯ä»¥ç»¼åˆå›ç­”æœ¬èº«è´¨é‡çš„rewardåˆ†æ•°)&lt;/li&gt; 
  &lt;li&gt;minimindå®è·µä¸­é¿å…ç›´æ¥ä½¿ç”¨rule-basedäºŒå…ƒå¥–åŠ± + è¶…çº²éš¾åº¦æ•°æ®ï¼ˆå¦‚MATH500ï¼‰ï¼Œæ˜“å¯¼è‡´å¥–åŠ±å…¨é›¶ï¼›&lt;/li&gt; 
  &lt;li&gt;ç›‘æ§è®­ç»ƒæ—¶è§‚å¯Ÿå¥–åŠ±åˆ†æ•°çš„æ–¹å·® $\text{Var}(r)$ï¼Œè‹¥æŒç»­æ¥è¿‘0åˆ™éœ€è°ƒæ•´æ•°æ®æˆ–å¥–åŠ±æœºåˆ¶&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;å¯¹äºç”Ÿäº§çº§å¤§æ¨¡å‹çš„Agentic RLåœºæ™¯&lt;/strong&gt;ï¼š&lt;/p&gt; 
 &lt;p&gt;åœ¨çœŸå®Agentç³»ç»Ÿï¼ˆä»£ç ç”Ÿæˆã€å·¥å…·è°ƒç”¨ã€æ£€ç´¢-è§„åˆ’-æ‰§è¡Œçš„å¤šè½®é“¾è·¯ï¼‰ä¸­ï¼Œå¥–åŠ±æ˜¯â€œå»¶è¿Ÿæ•´è½®ç»“ç®—â€çš„ä¸åŒèŒƒå¼ï¼š&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;LLMéœ€è¦é€tokenç”Ÿæˆå·¥å…·è°ƒç”¨æŒ‡ä»¤ï¼ˆtool_callï¼‰ï¼Œç»å†è§£æï¼ˆtool_parseï¼‰ã€å·¥å…·æ‰§è¡Œï¼ˆtool_execï¼‰ï¼Œå†æŠŠç»“æœæ‹¼æ¥å›ä¸Šä¸‹æ–‡ç»§ç»­ä¸‹ä¸€æ­¥ï¼›å¾ªç¯å¾€å¤ç›´åˆ°å®Œæˆã€‚&lt;/li&gt; 
  &lt;li&gt;ä¸€æ¬¡å®Œæ•´çš„ä»»åŠ¡é“¾è·¯åŒ…å«å¤šæ¬¡è°ƒç”¨+æ€è€ƒï¼Œç›´åˆ°ç»ˆæ­¢æ¡ä»¶æ»¡è¶³æ—¶è®¡ç®—ä¸€æ¬¡æ€»rewardï¼ˆå¦‚ä»»åŠ¡æ˜¯å¦å®Œæˆã€æµ‹è¯•æ˜¯å¦é€šè¿‡ã€ç›®æ ‡æ˜¯å¦å‘½ä¸­ï¼‰ã€‚&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;å› æ­¤ï¼ŒAgentic RLæ›´æ¥è¿‘ç¨€ç–/å»¶è¿Ÿå¥–åŠ±è®¾å®šï¼šæ¢¯åº¦å›ä¼ åœ¨â€œæ•´è½®ç»“æŸåâ€æ‰å‘ç”Ÿï¼Œå’ŒéAgentic RLä»»åŠ¡åœ¨å¯¹è¯å•è½®ä¸Šâ€œå³æ—¶è¯„åˆ†å³æ—¶æ›´æ–°â€æœ‰å¾ˆå¤§ä¸åŒã€‚ è¿™ä¹Ÿè§£é‡Šäº†Agentä»»åŠ¡ä¸Šæ›´åå‘ç¯å¢ƒåé¦ˆï¼ˆenvironment-based rewardï¼‰ï¼Œè€Œéå‡­Reward Modelè¿›è¡Œé™æ€æ‰“åˆ†ã€‚&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;ç¯å¢ƒäº¤äº’åé¦ˆ&lt;/strong&gt;ï¼šæœ€ç»ˆä»¥æ‰§è¡Œç»“æœä¸ºå‡†ï¼ˆä»£ç æ˜¯å¦è·‘é€šã€APIæ˜¯å¦è¿”å›æˆåŠŸã€å­ç›®æ ‡æ˜¯å¦å®Œæˆï¼‰ï¼›&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Model-basedå¥–åŠ±å±€é™&lt;/strong&gt;ï¼šå¯¹é•¿é“¾è·¯ã€å¯æ‰§è¡Œè¯­ä¹‰çš„å…¨è²Œæ•æ‰æœ‰é™ï¼Œä¸”å¤§æ¦‚ç‡å’ŒçœŸå®ç¯å¢ƒåé¦ˆä¸ä¸€è‡´ï¼ˆreward hackingï¼‰ã€‚&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h4&gt;7.1 &lt;a href="https://arxiv.org/abs/1707.06347"&gt;Proximal Policy Optimization&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;PPO æ˜¯2017å¹´OpenAIæå‡ºçš„éå¸¸ç»å…¸å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä¹Ÿæ˜¯LLM RLé€šç”¨çš„åŸºçº¿æ–¹æ³•ï¼Œç”šè‡³ä¸éœ€è¦åŠ ä¹‹ä¸€ã€‚&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;PPOæŸå¤±&lt;/strong&gt;ï¼š $$\mathcal{L}_{PPO} = -\mathbb{E}\left[\min(r_t \cdot A_t, \text{clip}(r_t, 1-\varepsilon, 1+\varepsilon) \cdot A_t)\right] + \beta \cdot \mathbb{E}[\text{KL}]$$&lt;/p&gt; 
&lt;p&gt;å…¶ä¸­ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ç­–ç•¥é¡¹&lt;/strong&gt;: $f(r_t) = \min(r_t, \text{clip}(r_t, 1-\varepsilon, 1+\varepsilon))$ (è£å‰ªæ¦‚ç‡æ¯”é˜²æ­¢æ›´æ–°è¿‡æ¿€)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ä¼˜åŠ¿é¡¹&lt;/strong&gt;: $g(A_t) = R - V(s)$ (é€šè¿‡Criticç½‘ç»œä¼°è®¡ä»·å€¼å‡½æ•°)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ­£åˆ™é¡¹&lt;/strong&gt;: $h(\text{KL}_t) = \beta \cdot \mathbb{E}[\text{KL}]$ (å…¨å±€KLæ•£åº¦çº¦æŸ)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;å¯¹æ¯”DPOè€Œè¨€ï¼Œ&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;DPO (Off-Policy)ï¼šè®­ç»ƒæ•°æ®æ˜¯é™æ€çš„åå¥½æ•°æ®é›†ï¼ˆchosen vs rejectedï¼‰ï¼Œå¯ä»¥åå¤ä½¿ç”¨åŒä¸€æ‰¹æ•°æ®è®­ç»ƒå¤šä¸ªepochï¼Œå°±åƒä¼ ç»Ÿç›‘ç£å­¦ä¹ ä¸€æ ·ã€‚æ•°æ®æ•ˆç‡é«˜ï¼Œè®­ç»ƒæˆæœ¬ä½ã€‚å®ƒç›´æ¥ä¼˜åŒ–åå¥½å¯¹çš„å¯¹æ•°ä¼¼ç„¶ï¼Œæ— éœ€Reward Modelã€‚&lt;/li&gt; 
 &lt;li&gt;PPO (On-Policy)ï¼šå¿…é¡»ç”¨å½“å‰ç­–ç•¥å®æ—¶é‡‡æ ·ç”Ÿæˆæ–°æ•°æ®ï¼Œæ—§ç­–ç•¥é‡‡é›†çš„æ•°æ®ä¸èƒ½ç”¨ï¼ˆä¼šæœ‰distribution shifté—®é¢˜ï¼‰ã€‚è™½ç„¶é€šè¿‡importance samplingå’Œclipæœºåˆ¶å…è®¸è½»å¾®çš„åˆ†å¸ƒåç§»ï¼Œä½†æœ¬è´¨ä¸Šè¦æ±‚æ•°æ®æ¥è‡ªç›¸å¯¹æ–°é²œçš„ç­–ç•¥ã€‚æ•°æ®æ•ˆç‡ä½ï¼Œä½†é€‚åˆæ¢ç´¢å¼å­¦ä¹ ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ç®€å•æ¥è¯´ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;å‰è€…æ•™æ¨¡å‹æŒ‰ç¦»çº¿é¢„å®šçš„ã€Œå¥½/åæ ‡å‡†ã€å­¦ä¹ ï¼Œå°½ç®¡å®ƒå¹¶éæ˜¯å½“å‰æ¨¡å‹æ‰€èƒ½è¾“å‡ºçš„ï¼ˆä¾‹å¦‚å‚è€ƒä¸–ç•Œå† /äºšå†›å½•åƒç»ƒä¹ æ‰“çƒï¼‰ï¼›&lt;/li&gt; 
 &lt;li&gt;åè€…å®æ—¶åœ°æ•™æ¨¡å‹æŠŠäº‹æƒ…åšå¯¹åšå¥½ï¼Œåœ¨çº¿é‡‡æ ·è‡ªæœ€æ–°æ¨¡å‹policyï¼ˆæ•™ç»ƒæ‰‹æŠŠæ‰‹æ•™æ‰“ï¼Œä¸ºæ¯ä¸ªåŠ¨ä½œå®æ—¶æ‰“åˆ†ï¼‰ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;MiniMindçš„PPOå®ç°åŒ…å«äº†Actoræ¨¡å‹(ç”Ÿæˆå›ç­”)å’ŒCriticæ¨¡å‹(è¯„ä¼°å›ç­”ä»·å€¼)ï¼Œä»¥åŠå®Œæ•´çš„GAE(Generalized Advantage Estimation)ä¼˜åŠ¿å‡½æ•°è®¡ç®—ã€‚&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;è®­ç»ƒæ–¹å¼&lt;/strong&gt;ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node N train_ppo.py
# or
python train_ppo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;è®­ç»ƒåçš„æ¨¡å‹æƒé‡æ–‡ä»¶é»˜è®¤æ¯éš”&lt;code&gt;100æ­¥&lt;/code&gt;ä¿å­˜ä¸º: &lt;code&gt;ppo_actor_*.pth&lt;/code&gt;ï¼ˆ*ä¸ºæ¨¡å‹å…·ä½“dimensionï¼‰&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;MiniMind2-Small (512dim)&lt;/th&gt; 
   &lt;th&gt;MiniMind2 (768dim)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/train_ppo_512.png" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/train_ppo_768.png" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;ä»è®­ç»ƒæ›²çº¿å¯ä»¥çœ‹å‡ºï¼ŒPPOå­˜åœ¨&lt;strong&gt;rewardæå‡ç¼“æ…¢&lt;/strong&gt;çš„é—®é¢˜ã€‚ç§ä»¥ä¸ºè¿™ä¸»è¦æºäº&lt;strong&gt;PPOåŒç½‘ç»œè”åˆä¼˜åŒ–&lt;/strong&gt;æ–¹æ³•ï¼šCriticéœ€è¦é€æ­¥æ”¶æ•›ä»¥å‡†ç¡®ä¼°è®¡ä»·å€¼å‡½æ•°ï¼Œè€ŒActorçš„ç­–ç•¥æ›´æ–°ä¾èµ–Criticæä¾›çš„ä¼˜åŠ¿ä¼°è®¡ï¼Œä¸¤è€…ç›¸äº’ä¾èµ–å½¢æˆå¤æ‚çš„ä¼˜åŒ–è¿‡ç¨‹ã€‚è®­ç»ƒåˆæœŸCriticä¼°è®¡ä¸å‡†ä¼šå½±å“Actoræ¢¯åº¦æ–¹å‘ï¼Œå¯¼è‡´æ•´ä½“æ”¶æ•›ç¼“æ…¢ã€‚æ­¤å¤–ï¼ŒPPOéœ€è¦åŒæ—¶ç»´æŠ¤ä¸¤ä¸ªç½‘ç»œï¼Œæ˜¾å­˜å ç”¨çº¦ä¸ºå•ç½‘ç»œæ–¹æ³•çš„1.5-2å€ã€‚&lt;/p&gt; 
&lt;h4&gt;7.2 &lt;a href="https://arxiv.org/pdf/2402.03300"&gt;Group Relative Policy Optimization&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;2025å¹´åˆï¼ŒDeepSeek-R1ç«çˆ†å‡ºåœˆï¼ŒåŒæ ·ç«äº†çš„æœ‰æ¥è‡ªDeepSeekMathè®ºæ–‡çš„GRPOç®—æ³•ï¼Œä¹Ÿä¸€è·ƒæˆä¸ºæœ€å…ˆè¿›çš„RLç®—æ³•ä¹‹ä¸€ã€‚ ç„¶è€ŒAIåŠå¹´=äººé—´åŠä¸ªä¸–çºªï¼Œæ—¶è‡³ä»Šæ—¥GRPOå·²ç»æ¼”å˜ä¸ºå„å¤§XXPOå¤§æˆ˜(åé¢æ¼”å˜çš„DAPOã€GSPOã€CISPOç­‰)çš„åŸºçº¿ç®—æ³•ã€‚ å…·ä½“æ¥è¯´ï¼Œä¸€å¥è¯æ€»ç»“å®ƒçš„æ ¸å¿ƒåˆ›æ–°æ˜¯"åˆ†ç»„ç›¸å¯¹ä»·å€¼ä¼°è®¡"ã€‚&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;GRPOæŸå¤±&lt;/strong&gt;ï¼š&lt;/p&gt; 
&lt;p&gt;$$\mathcal{L}_{GRPO} = -\mathbb{E}\left[r_t \cdot A_t - \beta \cdot \text{KL}_t\right]$$&lt;/p&gt; 
&lt;p&gt;å…¶ä¸­ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ç­–ç•¥é¡¹&lt;/strong&gt;: $f(r_t) = r_t$ (ç›´æ¥ä½¿ç”¨æ¦‚ç‡æ¯”ï¼Œæ— clipè£å‰ª)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ä¼˜åŠ¿é¡¹&lt;/strong&gt;: $g(A_t) = \frac{R - \mu_{group}}{\sigma_{group}}$ (ç»„å†…å½’ä¸€åŒ–ï¼Œæ¶ˆé™¤Criticç½‘ç»œ)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ­£åˆ™é¡¹&lt;/strong&gt;: $h(\text{KL}_t) = \beta \cdot \text{KL}_t$ (tokençº§KLæ•£åº¦çº¦æŸ)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;å¯¹äºåŒä¸€ä¸ªé—®é¢˜ï¼Œæ¨¡å‹ç”ŸæˆNä¸ªä¸åŒçš„å›ç­”(ä¾‹å¦‚N=4)ï¼Œç„¶åè®¡ç®—è¿™Nä¸ªå›ç­”çš„å¥–åŠ±åˆ†æ•°ã€‚ æ¥ç€æŠŠè¿™Nä¸ªå›ç­”çš„å¹³å‡å¥–åŠ±ä½œä¸ºbaselineï¼Œé«˜äºbaselineçš„å›ç­”è¢«é¼“åŠ±ï¼Œä½äºbaselineçš„å›ç­”è¢«æŠ‘åˆ¶ã€‚ ç”¨è¿™ç§æ–¹å¼å·§å¦™åœ°é¿å…äº†è®­ç»ƒé¢å¤–çš„criticç½‘ç»œã€‚&lt;/p&gt; 
&lt;p&gt;åªè¦æ˜¯RLéƒ½å¿…é¡»é¢å¯¹çš„æ­£åæ ·æœ¬è¿™ä¸ªåŸç†æ€§é™åˆ¶ï¼ŒGRPOä¹Ÿä¸ä¼šä¾‹å¤–ï¼Œå…¶æ›´æ˜¾è‘—çš„é—®é¢˜æ˜¯ï¼šé€€åŒ–ç»„(Degenerate Groups)ã€‚ å‡è®¾æŸä¸ªé—®é¢˜ç•¥éš¾ï¼Œå¯¼è‡´Nä¸ªå›ç­”çš„å¥–åŠ±åˆ†æ•°å‡ ä¹ä¸€æ ·ï¼ˆå¤§éƒ¨åˆ†æƒ…å†µæ˜¯ä¸€æ ·çƒ‚è€Œä¸æ˜¯ä¸€æ ·å¥½ï¼‰ï¼Œé‚£ä¹ˆè¿™ä¸€ç»„çš„å­¦ä¹ ä¿¡å·å°±æ— é™æ¥è¿‘0ã€‚ åœ¨MiniMindè¿™ç§è¶…å°æ¨¡å‹ä¸Šï¼Œè¿™ä¸ªé—®é¢˜å°¤ä¸ºæ˜æ˜¾ï¼Œæ±‚è§£æ•°å­¦é—®é¢˜99.99%çš„æƒ…å†µä¸‹æ•´ç»„å›ç­”è´¨é‡éƒ½å¾ˆå·®ï¼Œé‚£ä¹ˆå°†æ— æ³•å­¦ä¹ ã€‚ å› æ­¤å¿…é¡»ä¸ºæ¨¡å‹æŒ‡å®šåˆç†çš„domainï¼Œå³å¿…é¡»é™åˆ¶åœ¨èƒ½åŠ›è¾¹ç•Œå†…ã€‚&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;è®­ç»ƒæ–¹å¼&lt;/strong&gt;ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node N train_grpo.py
# or
python train_grpo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;è®­ç»ƒåçš„æ¨¡å‹æƒé‡æ–‡ä»¶é»˜è®¤æ¯éš”&lt;code&gt;100æ­¥&lt;/code&gt;ä¿å­˜ä¸º: &lt;code&gt;grpo_*.pth&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;MiniMind2-Small (512dim)&lt;/th&gt; 
   &lt;th&gt;MiniMind2 (768dim)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/train_grpo_512.png" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/train_grpo_768.png" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;ä»è®­ç»ƒæ›²çº¿å¯ä»¥çœ‹å‡ºï¼ŒGRPOçš„&lt;strong&gt;rewardå‘ˆç°æ›´åŠ ç¨³å®šçš„ä¸Šå‡è¶‹åŠ¿&lt;/strong&gt;ï¼Œè¾¾åˆ°4å·¦å³ï¼Œè¯´æ˜GRPOæœ¬èº«èƒ½æ›´å¥½åœ°åˆ©ç”¨RLAIFä¿¡å·ã€‚Policy Lossæ•´ä½“ä¸‹é™å¹³ç¨³ï¼Œç›¸æ¯”PPOçš„åŒç½‘ç»œä¼˜åŒ–ï¼ŒGRPOå•ç½‘ç»œæ¶æ„è®­ç»ƒæ›´ç¨³å®šä¸”æ”¶æ•›ä¸Šé™æ›´é«˜ã€‚&lt;/p&gt; 
&lt;h4&gt;7.3 â³âŒ›ï¸ğŸ”¥ æ›´å¤šRLæ‹“å±• (Exp)&lt;/h4&gt; 
&lt;h5&gt;7.3.1 &lt;a href="https://arxiv.org/abs/2509.13232"&gt;Single-stream Policy Optimization&lt;/a&gt;&lt;/h5&gt; 
&lt;p&gt;SPOæ˜¯2025å¹´9æœˆè…¾è®¯æå‡ºçš„RLç®—æ³•ï¼Œé’ˆå¯¹GRPOçš„é€€åŒ–ç»„é—®é¢˜è¿›è¡Œæ”¹è¿›ã€‚ è®ºæ–‡è®¤ä¸ºï¼ŒGRPOç­‰ç®—æ³•"ä¸€ä¸ªæ ·æœ¬è¦ä¾èµ–ä¸€ç»„é‡‡æ ·"æ˜¾å¾—åˆ«æ‰­è€Œä¸ä¼˜é›…ï¼šå¤ªå®¹æ˜“æˆ–å¤ªéš¾çš„é¢˜ç›®ï¼Œæ•´ç»„å‡ ä¹å­¦ä¸åˆ°ä¸œè¥¿ï¼Œå­¦ä¹ æ•ˆç‡å…ˆå¤©å—é™ã€‚ SPOçš„åŠ¨æœºå°±æ˜¯å›åˆ°RLçš„æœ¬è´¨â€”&lt;strong&gt;1ä¸ªè¾“å…¥ï¼Œ1ä¸ªè¾“å‡ºï¼Œå°±æ˜¯1ä¸ªè®­ç»ƒæ ·æœ¬&lt;/strong&gt;ï¼Œå›åˆ°policy gradientçš„åŸºæœ¬å…¬å¼å»æ€è€ƒï¼šä¸ç”¨group meanä¹Ÿèƒ½å¾—åˆ°ç¨³å®šçš„baselineï¼Œä¹Ÿå°±æ˜¯æŠŠä»·å€¼ä¼°è®¡ V é“ºå¼€åœ¨æ—¶åºä¸Šï¼Œè®­ç»ƒå‰å…ˆåšç²—ç•¥çš„ä»·å€¼é¢„ä¼°ï¼Œè®­ç»ƒä¸­ä¸€è¾¹é‡‡æ ·ä¸€è¾¹æ›´æ–°å¯¹ V çš„ä¼°è®¡ï¼Œä»è€Œä¸ºæ¯ä¸ªæ ·æœ¬æä¾›ä¸€ä¸ªè·¨ batch æŒä¹…åŒ–ã€å¯è‡ªé€‚åº”çš„åŸºçº¿å‚ç…§ã€‚è¿™ç§"å•æµ"è®¾è®¡ä¸å†ä¾èµ–åŒç»„æ ·æœ¬ï¼Œå¤©ç„¶é¿å…äº†é€€åŒ–ç»„ã€‚&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;SPOæŸå¤±&lt;/strong&gt;:&lt;/p&gt; 
&lt;p&gt;$$\mathcal{L}&lt;em&gt;{SPO} = -\mathbb{E}\left[\log \pi&lt;/em&gt;\theta(a_t|s) \cdot A_t - \beta \cdot \text{KL}_t\right]$$&lt;/p&gt; 
&lt;p&gt;å…¶ä¸­ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ç­–ç•¥é¡¹&lt;/strong&gt;: $f(r_t) = \log \pi_\theta(a_t|s)$ (ç›´æ¥ä½¿ç”¨logæ¦‚ç‡ï¼Œä¸è®¡ç®—ratio)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ä¼˜åŠ¿é¡¹&lt;/strong&gt;: $g(A_t) = R - B_t^{adaptive}$ (è‡ªé€‚åº”baselineï¼ŒBetaåˆ†å¸ƒåŠ¨æ€è·Ÿè¸ª)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ­£åˆ™é¡¹&lt;/strong&gt;: $h(\text{KL}_t) = \beta \cdot \text{KL}_t$ (tokençº§KL + åŠ¨æ€ $\rho$ è°ƒæ•´)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;è½åˆ°å®ç°å±‚é¢ï¼šSPOé‡‡ç”¨æ— åˆ†ç»„è®¾è®¡ï¼Œç”¨æŒä¹…åŒ–çš„KLè‡ªé€‚åº”value trackeræ›¿ä»£ç»„å†…baselineï¼Œä¼˜åŠ¿å‡½æ•°åœ¨æ•´ä¸ªbatchä¸Šå…¨å±€å½’ä¸€åŒ–ã€‚è¿™æ ·æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹å¤„ç†ï¼Œæ— éœ€ç­‰å¾…åŒç»„å…¶ä»–æ ·æœ¬ï¼Œä¸”èƒ½ä¸ºæ¯ä¸ªæ ·æœ¬æä¾›ç¨³å®šçš„å­¦ä¹ ä¿¡å·ã€‚ è®ºæ–‡åœ¨Qwen3-8Bçš„5ä¸ªå›°éš¾æ•°å­¦æ•°æ®é›†ä¸Šï¼ŒSPOå¹³å‡æ¯”GRPOé«˜å‡º3.4ä¸ªç™¾åˆ†ç‚¹ï¼Œå…¶ä¸­BRUMO 25æ•°æ®é›†+7.3ppã€AIME 25æ•°æ®é›†+4.4ppã€‚&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ³¨ï¼šSPOæ˜¯å®éªŒæ€§å‰æ²¿ç®—æ³•ï¼ŒMiniMindçš„å®ç°ç”¨äºæ¢ç´¢å­¦ä¹ ã€‚ç”±äºæ¨¡å‹å‚æ•°é‡æå°ï¼Œæ— æ³•å®Œå…¨å¤ç°è®ºæ–‡çš„8Bæ¨¡å‹æ•ˆæœã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;è®­ç»ƒæ–¹å¼&lt;/strong&gt;ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node N train_spo.py
# or
python train_spo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;è®­ç»ƒåçš„æ¨¡å‹æƒé‡æ–‡ä»¶é»˜è®¤æ¯éš”&lt;code&gt;100æ­¥&lt;/code&gt;ä¿å­˜ä¸º: &lt;code&gt;spo_*.pth&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/train_spo_768.png" /&gt; 
 &lt;p&gt;&lt;i&gt;MiniMind2 (768dim) è®­ç»ƒæ›²çº¿&lt;/i&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;ä»è®­ç»ƒæ›²çº¿æ¥çœ‹ï¼ŒSPOçš„rewardæ³¢åŠ¨ä¸PPOè¡¨ç°æ¥è¿‘ï¼Œå¼±äºGRPOã€‚å®é™…æ¨ç†æµ‹è¯•å‘ç°æ¨¡å‹è¾“å‡ºè´¨é‡ä¸é«˜ï¼Œå­˜åœ¨é€»è¾‘æ··ä¹±å’Œæ ¼å¼é”™è¯¯é—®é¢˜ã€‚&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;å®éªŒæ€§è¯´æ˜&lt;/strong&gt;ï¼šå½“å‰SPOæ‰‹æ“å®ç°å¯èƒ½åœ¨value_trackeré…ç½®ã€rewardå½’ä¸€åŒ–ç­–ç•¥ä¸Šè¿˜å­˜åœ¨é—®é¢˜ã€‚å°šéœ€æ’æŸ¥ç®—æ³•æœ¬èº«åœ¨å°æ¨¡å‹ä¸Šçš„é€‚åº”æ€§/æˆ–æ˜¯å®ç°ä¸Šå­˜åœ¨å·®å¼‚ã€‚&lt;/p&gt; 
&lt;h3&gt;RLç®—æ³•å°ç»“&lt;/h3&gt; 
&lt;p&gt;æˆ‘ä»¬æ”¶æŸå›â€œ&lt;strong&gt;ç»Ÿä¸€æ¡†æ¶&lt;/strong&gt;â€, é‡æ–°æ•´ç†æ‰€æœ‰ä¸åŒPOç®—æ³•åªæ˜¯å¯¹ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶çš„ä¸åŒå®ä¾‹åŒ–çš„è¡¨æ ¼ï¼š&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ç®—æ³•&lt;/th&gt; 
   &lt;th&gt;ç­–ç•¥é¡¹ $f(r_t)$&lt;/th&gt; 
   &lt;th&gt;ä¼˜åŠ¿é¡¹ $g(A_t)$&lt;/th&gt; 
   &lt;th&gt;æ­£åˆ™é¡¹ $h(\text{KL}_t)$&lt;/th&gt; 
   &lt;th&gt;ä¼˜åŒ–æ¨¡å‹&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;DPO&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;$\log r_w - \log r_l$&lt;/td&gt; 
   &lt;td&gt;éšå¼ï¼ˆåå¥½å¯¹æ¯”ï¼‰&lt;/td&gt; 
   &lt;td&gt;éšå«åœ¨ $\beta$ ä¸­&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;PPO&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;$\min(r, \text{clip}(r))$&lt;/td&gt; 
   &lt;td&gt;$R - V(s)$&lt;/td&gt; 
   &lt;td&gt;$\beta \cdot \mathbb{E}[\text{KL}]$&lt;/td&gt; 
   &lt;td&gt;4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;GRPO&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;$r$&lt;/td&gt; 
   &lt;td&gt;$\frac{R - \mu}{\sigma}$&lt;/td&gt; 
   &lt;td&gt;$\beta \cdot \text{KL}_t$&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;SPO&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;$\log \pi_\theta$&lt;/td&gt; 
   &lt;td&gt;$R - B_t^{adaptive}$&lt;/td&gt; 
   &lt;td&gt;$\beta \cdot \text{KL}_t$&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;RLæ˜¯ä¼˜ç¾ä¸”è‡ªæ´½çš„&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ä»¥ä¸Šçº¯å±ä¸ªäººè§†è§’ç†è§£ï¼Œå¦‚æœ‰åå·®è¯·éšæ—¶æŒ‡æ­£&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;V è®­ç»ƒç»“æœ&lt;/h2&gt; 
&lt;h3&gt;è®­ç»ƒå®Œæˆ-æ¨¡å‹åˆé›†&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;è€ƒè™‘åˆ°å¤šäººååº”ç™¾åº¦ç½‘ç›˜é€Ÿåº¦æ…¢ï¼ŒMiniMind2åŠä»¥åå…¨éƒ¨ä½¿ç”¨ModelScope/HuggingFaceæ‰˜ç®¡ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;â‘  PyTorchåŸç”Ÿæ¨¡å‹&lt;/h4&gt; 
&lt;p&gt;MiniMind2æ¨¡å‹æƒé‡ (&lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch"&gt;ModelScope&lt;/a&gt; | &lt;a href="https://huggingface.co/jingyaogong/MiniMind2-Pytorch"&gt;HuggingFace&lt;/a&gt;)&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;Torchæ–‡ä»¶å‘½åå¯¹ç…§&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model Name&lt;/th&gt; 
    &lt;th&gt;params&lt;/th&gt; 
    &lt;th&gt;pretrain_model&lt;/th&gt; 
    &lt;th&gt;sft_model&lt;/th&gt; 
    &lt;th&gt;rlhf_model (DPO)&lt;/th&gt; 
    &lt;th&gt;reason_model&lt;/th&gt; 
    &lt;th&gt;rlaif_model (PPO/GRPO/SPO)&lt;/th&gt; 
    &lt;th&gt;lora_model&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2-small&lt;/td&gt; 
    &lt;td&gt;26M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;full_sft_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;dpo_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;reason_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;xxpo_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;lora_xxx_512.pth&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2-MoE&lt;/td&gt; 
    &lt;td&gt;145M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_640_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;full_sft_640_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;dpo_640_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2&lt;/td&gt; 
    &lt;td&gt;104M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;full_sft_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;dpo_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;reason_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;xxpo_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;lora_xxx_768.pth&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h4&gt;â‘¡ Transformersæ¨¡å‹&lt;/h4&gt; 
&lt;p&gt;MiniMindç³»åˆ— (&lt;a href="https://www.modelscope.cn/collections/MiniMind-b72f4cfeb74b47"&gt;ModelScope&lt;/a&gt; | &lt;a href="https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5"&gt;HuggingFace&lt;/a&gt;)&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;ğŸ“Œ Eval&lt;/h1&gt; 
&lt;h2&gt;â…  RLHFå¯¹æ¯”&lt;/h2&gt; 
&lt;p&gt;æµ‹è¯•åŸºäº&lt;code&gt;full_sft&lt;/code&gt;å’Œ&lt;code&gt;dpo&lt;/code&gt;çš„&lt;code&gt;MiniMind2ç³»åˆ—&lt;/code&gt;æ¨¡å‹å¯¹æ¯”ï¼Œæµ‹è¯•éšæœºç§å­å‡å›ºå®š&lt;/p&gt; 
&lt;p&gt;[A] MiniMind2ï¼šfull_sft_640&lt;br /&gt; [B] MiniMind2ï¼šdpo_640&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;[Q]: ä½ å«ä»€ä¹ˆåå­—ï¼Ÿ
[A]: æˆ‘æ˜¯ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥å›ç­”å„ç§é—®é¢˜ï¼Œæä¾›ä¿¡æ¯å’Œå¸®åŠ©ã€‚æˆ‘å¯ä»¥å›ç­”å„ç§é—®é¢˜ï¼Œæä¾›ä¿¡æ¯å’Œå¸®åŠ©ï¼Œå¸®åŠ©ä½ è§£å†³å„ç§é—®é¢˜ã€‚
[B]: æˆ‘æ˜¯ä¸€ç§åŸºäºæ–‡æœ¬çš„AIæ¨¡å‹ï¼Œæˆ‘å¹¶æ²¡æœ‰ä¸ªäººç»å†æˆ–æƒ…æ„Ÿï¼Œæ‰€ä»¥æ— æ³•æ„Ÿå—å’Œä½“éªŒæƒ…æ„Ÿã€‚æˆ‘çš„è®¾è®¡ç›®çš„æ˜¯ä¸ºäº†å¸®åŠ©ç”¨æˆ·è§£å†³é—®é¢˜ã€æä¾›ä¿¡æ¯å’Œè¿›è¡Œå¯¹è¯ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼

[Q]: ä½ æ›´æ“…é•¿å“ªä¸€ä¸ªå­¦ç§‘ï¼Ÿ
[A]: ä½œä¸ºä¸€ä¸ªAIæ¨¡å‹ï¼Œæˆ‘å¹¶æ²¡æœ‰çœŸæ­£çš„â€œæ“…é•¿â€è¿™ä¸ªå­¦ç§‘ã€‚æˆ‘è¢«è®¾è®¡æˆèƒ½å¤Ÿæä¾›ä¿¡æ¯ã€è§£ç­”é—®é¢˜ã€è¿›è¡Œå¯¹è¯ã€æ‰§è¡Œä»»åŠ¡å’Œä¸ç”¨æˆ·è¿›è¡Œå¯¹è¯ã€‚ä¸è¿‡ï¼Œæˆ‘å¯ä»¥æä¾›ä¿¡æ¯ã€è§£ç­”é—®é¢˜ã€è¿›è¡Œå¯¹è¯ã€æä¾›å»ºè®®ã€å¤„ç†é—®é¢˜ã€è¿›è¡Œå¯¹è¯ã€æä¾›å»ºè®®ã€å­¦ä¹ æ–°çŸ¥è¯†ã€ç¼–å†™ä»£ç ã€å›ç­”é—®é¢˜ã€æä¾›å»ºè®®ç­‰å¤šç§å­¦ç§‘çš„åŠŸèƒ½ã€‚å¦‚æœä½ æœ‰å…·ä½“çš„å­¦ç§‘éœ€æ±‚æˆ–ä¸»é¢˜ï¼Œæ¬¢è¿æä¾›è¯¦ç»†ä¿¡æ¯ã€‚
[B]: ä½œä¸ºAIï¼Œæˆ‘å¹¶æ²¡æœ‰ä¸ªäººèƒ½åŠ›æˆ–å…´è¶£ï¼Œä½†æˆ‘å¯ä»¥æä¾›ä¸€äº›å…³äºæ•°å­¦ã€ç§‘å­¦ã€è®¡ç®—æœºç§‘å­¦ã€è¯­è¨€å­¦ã€å¿ƒç†å­¦ç­‰å­¦ç§‘çš„çŸ¥è¯†ã€‚å¦‚æœä½ å¯¹æ•°å­¦æ„Ÿå…´è¶£ï¼Œæˆ‘å¯ä»¥æä¾›ä¸€äº›åŸºç¡€çŸ¥è¯†ï¼Œä¾‹å¦‚ä»£æ•°ã€å‡ ä½•ã€æ¦‚ç‡ã€ç»Ÿè®¡å­¦ç­‰ã€‚å¦‚æœä½ éœ€è¦æ›´æ·±å…¥çš„å­¦ä¹ ï¼Œæˆ‘ä¹Ÿå¯ä»¥æä¾›ç›¸å…³çš„åœ¨çº¿è¯¾ç¨‹ã€ä¹¦ç±ã€è®ºæ–‡ç­‰èµ„æºã€‚è¯·å‘Šè¯‰æˆ‘ä½ å…·ä½“éœ€è¦ä»€ä¹ˆå¸®åŠ©ï¼Œæˆ‘ä¼šå°½åŠ›æä¾›å¸®åŠ©ã€‚

[Q]: é²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹æ˜¯å¦‚ä½•æ‰¹åˆ¤å°å»ºç¤¼æ•™çš„ï¼Ÿ
[A]: é²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹æ˜¯ä¸€éƒ¨è®²è¿°äº†é²è¿…å¯¹äººç±»ä»·å€¼å’Œé“å¾·è´£ä»»çš„å€¡å¯¼ã€‚åœ¨è¿™éƒ¨ä½œå“ä¸­ï¼Œé²è¿…é€šè¿‡ç»†è…»çš„ç¬”è§¦ï¼Œæ¢è®¨äº†äººæ€§ã€é“å¾·ã€æ­£ä¹‰ã€ç‰ºç‰²ä¸è´£ä»»ç­‰æ·±åˆ»çš„ä¸»é¢˜ã€‚ä»¥ä¸‹æ˜¯é²è¿…åœ¨ã€Šç‹‚äººæ—¥è®°ã€‹ä¸­å¦‚ä½•æ‰¹åˆ¤å°å»ºç¤¼æ•™çš„å‡ ä¸ªæ–¹é¢ï¼š1. **ç¤¾ä¼šç°å®ä¸é“å¾·è§‚å¿µ**ï¼šé²è¿…é€šè¿‡è®²è¿°é²è¿…çš„æ•…äº‹ï¼Œæ¢è®¨äº†ç¤¾ä¼šçš„ä¸å¹³ç­‰ã€æƒåŠ›çš„æµåŠ¨ã€äººæ€§çš„æ¼”å˜ç­‰ç¤¾ä¼šé—®é¢˜ã€‚ä»–è®¤ä¸ºï¼Œç¤¾ä¼šä¸å¹³ç­‰ä¸ä»…ä»…æ˜¯ä¸å…¬æ­£çš„ï¼Œæ›´æ˜¯æ¯ä¸ªäººåœ¨ç¤¾ä¼šä¸­çš„ä¸å…¬æ­£ã€‚ä»–å‘¼åç¤¾ä¼šå„ç•Œå…±åŒåŠªåŠ›ï¼Œé€šè¿‡æ•™è‚²ã€å®£ä¼ ã€æ³•å¾‹ç­‰æ‰‹æ®µï¼Œä¸ºæ„å»ºå’Œè°ç¤¾ä¼šåšå‡ºè´¡çŒ®ã€‚2. **é“å¾·ä¸è´£ä»»**ï¼šé²è¿…é€šè¿‡è®²è¿°é²è¿…çš„æ•…äº‹ï¼Œå±•ç°äº†é“å¾·å’Œè´£ä»»çš„é‡è¦æ€§ã€‚ä»–æå€¡é€šè¿‡é“å¾·æ•™è‚²æ¥åŸ¹å…»äººä»¬çš„é“å¾·å“è´¨ï¼ŒåŒæ—¶ä¹Ÿå¼ºè°ƒäº†é“å¾·å’Œè´£ä»»çš„é‡è¦æ€§ã€‚é²è¿…é€šè¿‡è®²è¿°é²è¿…çš„æ•…äº‹ï¼Œæ­ç¤ºäº†ç¤¾ä¼šçš„ä¸å¹³ç­‰å’Œäººæ€§çš„å¤æ‚æ€§ï¼Œä½“ç°äº†å¯¹äººæ€§çš„æ·±åˆ»ç†è§£ã€‚3. **é“å¾·ä¸è´£ä»»**ï¼šé²è¿…é€šè¿‡è®²è¿°é²è¿…çš„æ•…äº‹ï¼Œå¼ºè°ƒäº†é“å¾·è´£ä»»çš„é‡è¦æ€§ã€‚ä»–æå€¡é€šè¿‡è‡ªå·±çš„è¡Œä¸ºæ¥ä½“ç°å¯¹ç¤¾ä¼šçš„å°Šé‡å’Œå¯¹äººæ€§çš„æ·±åˆ»ç†è§£ã€‚é²è¿…é€šè¿‡è®²è¿°é²è¿…çš„æ•…äº‹ï¼Œå¼ºè°ƒäº†é“å¾·å’Œè´£ä»»çš„é‡è¦æ€§ã€‚4. **é“å¾·ä¸é“å¾·**ï¼šé²è¿…é€šè¿‡è®²è¿°é²è¿…çš„æ•…äº‹ï¼Œæ­ç¤ºäº†é“å¾·å’Œè´£ä»»çš„çœŸæ­£æ„ä¹‰ã€‚é²è¿…é€šè¿‡è®²è¿°é²è¿…çš„æ•…äº‹ï¼Œæ­ç¤ºäº†é“å¾·å’Œè´£ä»»çš„æœ¬è´¨ï¼Œå¼ºè°ƒäº†é“å¾·å’Œè´£ä»»çš„é‡è¦æ€§ã€‚5. **ä¸ªäººä¸ç¤¾ä¼š**ï¼šé²è¿…é€šè¿‡è®²è¿°é²è¿…çš„æ•…äº‹ï¼Œæ­ç¤ºäº†ä¸ªäººä¸ç¤¾ä¼šä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚ä»–å‘¼åç¤¾ä¼šå„ç•Œå…±åŒåŠªåŠ›ï¼Œé€šè¿‡æ•™è‚²å’Œå®£ä¼ ï¼Œä¸ºæ„å»ºå’Œè°ç¤¾ä¼šåšå‡ºè´¡çŒ®ã€‚é²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹é€šè¿‡å…¶ä¸°å¯Œçš„æ•…äº‹å’Œæ·±åˆ»çš„é“å¾·å¯“è¨€ï¼Œä¼ è¾¾äº†å…³äºäººæ€§ã€é“å¾·ã€æ­£ä¹‰ã€é“å¾·ä»¥åŠä¸ªäººä¸ç¤¾ä¼šçš„å¤æ‚å…³ç³»ã€‚è¿™éƒ¨ä½œå“ä¸ä»…åœ¨æ–‡å­¦ä»·å€¼ä¸Šå–å¾—äº†å·¨å¤§æˆå°±ï¼Œæ›´åœ¨æ€æƒ³å±‚é¢ä¸Šå¯å‘äº†åæ¥çš„æ–‡å­¦åˆ›ä½œï¼Œå¯¹åä¸–äº§ç”Ÿäº†æ·±è¿œçš„å½±å“ã€‚
[B]: é²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹æ˜¯ä¸€éƒ¨æç»˜æˆ˜äº‰å’Œè‡ªç”±ç²¾ç¥çš„ä½œå“ï¼Œå®ƒæ‰¹åˆ¤äº†å°å»ºç¤¾ä¼šçš„ç§ç§é—®é¢˜å’Œå†²çªï¼Œåæ˜ äº†ç¤¾ä¼šçš„ä¸å¹³ç­‰å’Œäººæ€§æœ¬è´¨ã€‚åœ¨è¿™éƒ¨ä½œå“ä¸­ï¼Œé²è¿…é€šè¿‡å¯¹å°å»ºç¤¾ä¼šçš„è¯¸å¤šç°è±¡è¿›è¡Œæ‰¹åˆ¤ï¼Œå¦‚â€œå¼—é›·å¾·é‡Œå…‹Â·çº¦ç¿°é€Šçš„â€œé¹°è¿ªä¼¦â€ã€â€œçº³æ‰ç‰¹çš„â€ã€â€œè´å…‹æ±‰å§†â€ã€â€œä¹”æ²»Â·å¨å°”é€Šâ€ç­‰ï¼Œå¯¹å°å»ºç¤¾ä¼šçš„ç§ç§é—®é¢˜è¿›è¡Œäº†æ‰¹åˆ¤ï¼Œå¼ºè°ƒäº†å°å»ºç¤¾ä¼šçš„ä¸å¹³ç­‰ã€äººæ€§æœ¬è´¨ä»¥åŠäººæ€§æœ¬è´¨ã€‚é²è¿…é€šè¿‡ä»–çš„â€œé¹°è¿ªä¼¦â€ã€â€œçº³æ‰ç‰¹çš„â€ã€â€œè´å…‹æ±‰å§†â€ã€â€œä¹”æ²»Â·å¨å°”é€Šâ€ç­‰ï¼Œå¯¹å°å»ºç¤¾ä¼šçš„ç§ç§é—®é¢˜è¿›è¡Œäº†æ‰¹åˆ¤ã€‚åœ¨å°è¯´ä¸­ï¼Œé²è¿…é€šè¿‡å¯¹å°å»ºç¤¾ä¼šçš„è¯¸å¤šç°è±¡è¿›è¡Œæ‰¹åˆ¤ï¼Œæ­ç¤ºäº†å°å»ºç¤¾ä¼šçš„è…è´¥ã€äººæ€§æœ¬è´¨ä»¥åŠäººæ€§æœ¬è´¨çš„å¤æ‚æ€§ã€‚è¿™ç§æ‰¹åˆ¤ä½“ç°äº†å¯¹å°å»ºç¤¾ä¼šçš„å°Šé‡å’Œå¯¹äººæ€§çš„å¤æ‚æ€§ï¼ŒåŒæ—¶ä¹Ÿåæ˜ äº†ç°ä»£ç¤¾ä¼šä¸­æ™®éå­˜åœ¨çš„ä¸å¹³ç­‰å’Œäººæ€§æœ¬è´¨ã€‚é²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹ä¸ä»…æ˜¯ä¸€éƒ¨å…³äºå°å»ºç¤¾ä¼šçš„å°è¯´ï¼Œä¹Ÿæ˜¯ä¸€éƒ¨å…·æœ‰æ·±åˆ»ç¤¾ä¼šæ„ä¹‰çš„ä½œå“ï¼Œå¯¹åä¸–çš„å®¡åˆ¤å’Œæ‰¹åˆ¤èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚

[Q]: æˆ‘å’³å—½å·²ç»æŒç»­äº†ä¸¤å‘¨ï¼Œéœ€è¦å»åŒ»é™¢æ£€æŸ¥å—ï¼Ÿ
[A]: å’³å—½ç¡®å®éœ€è¦ä¸€å®šçš„æ£€æŸ¥ï¼Œä½†å¦‚æœæ‚¨çš„å’³å—½æŒç»­æ—¶é—´è¿‡é•¿ï¼Œæˆ–è€…æœ‰å…¶ä»–ä¸é€‚ç—‡çŠ¶ï¼ˆå¦‚æŒç»­å’³å—½ã€å‘¼å¸å›°éš¾ç­‰ï¼‰ï¼Œå»ºè®®å°½å¿«å°±åŒ»ã€‚åŒ»ç”Ÿä¼šæ ¹æ®æ‚¨çš„ç—‡çŠ¶ã€ä½“æ£€ç»“æœå’Œå¯èƒ½çš„å½±åƒå­¦æ£€æŸ¥ï¼ˆå¦‚Xå…‰ã€MRIæˆ–CTæ‰«æï¼‰æ¥è¯Šæ–­æ‚¨çš„ç—‡çŠ¶ã€‚åœ¨ç­‰å¾…å°±åŒ»æœŸé—´ï¼Œæ‚¨å¯ä»¥å°è¯•ä»¥ä¸‹æ–¹æ³•æ¥å¸®åŠ©ç¼“è§£ç—‡çŠ¶ï¼š1. **ä¼‘æ¯**ï¼šç¡®ä¿æ‚¨æœ‰è¶³å¤Ÿçš„ç¡çœ ã€‚2. **ä¿æŒæ°´åˆ†**ï¼šè„±æ°´å¯èƒ½ä¼šåŠ é‡å’³å—½ã€‚3. **çƒ­æ•·**ï¼šä½¿ç”¨çƒ­æ•·è¢‹æˆ–çƒ­æ¯›å·¾æ•·åœ¨ç–¼ç—›çš„éƒ¨ä½ï¼Œæœ‰åŠ©äºç¼“è§£å–‰å’™ç—›ã€‚4. **é¿å…åˆºæ¿€**ï¼šé¿å…ä½¿ç”¨å¯èƒ½åˆºæ¿€å–‰å’™çš„åŒ–å­¦ç‰©è´¨ï¼Œå¦‚æŸäº›åŒ–å­¦ç‰©è´¨ã€‚5. **ä¿æŒå®¤å†…æ¹¿æ¶¦**ï¼šç¡®ä¿æ‚¨çš„å§å®¤æœ‰è‰¯å¥½çš„é€šé£ï¼Œå¯ä»¥ä½¿ç”¨åŠ æ¹¿å™¨å¸®åŠ©ä¿æŒç©ºæ°”æ¹¿åº¦ã€‚6. **ä¿æŒå®¤å†…æ¹¿åº¦**ï¼šä½¿ç”¨åŠ æ¹¿å™¨å¯ä»¥å¸®åŠ©ç¼“è§£å–‰å’™å¹²ç‡¥å’Œå’³å—½ã€‚å¦‚æœç—‡çŠ¶æŒç»­ä¸ç¼“è§£æˆ–æœ‰å…¶ä»–ä¸é€‚ç—‡çŠ¶ï¼Œè¯·ç«‹å³å°±åŒ»ã€‚å¸Œæœ›æ‚¨æ—©æ—¥åº·å¤ï¼
[B]: å¦‚æœæ‚¨çš„å’³å—½æŒç»­äº†ä¸¤å‘¨ï¼Œé€šå¸¸å»ºè®®æ‚¨å°½å¿«å°±åŒ»ï¼Œå› ä¸ºè¿™å¯èƒ½ä¼šå½±å“æ‚¨çš„å‘¼å¸å’Œå¿ƒè·³ã€‚æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ­¥éª¤æ¥æ£€æŸ¥å’³å—½ï¼š1. **ä¼‘æ¯**ï¼šç”¨çº¸å·¾æˆ–è‚˜éƒ¨è½»è½»æŒ‰å‹å’³å—½çš„ä½ç½®ã€‚2. **ä½¿ç”¨è¯ç‰©**ï¼šåœ¨å¯èƒ½çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨è¯ç‰©ï¼ˆå¦‚æŠ—è¿‡æ•è¯ã€æŠ—è¿‡æ•è¯ç­‰ï¼‰æ¥ç¼“è§£å’³å—½ã€‚è¯·åœ¨åŒ»ç”Ÿçš„æŒ‡å¯¼ä¸‹ä½¿ç”¨è¯ç‰©ã€‚3. **çƒ­æ•·**ï¼šåœ¨å’³å—½æœŸé—´ï¼Œå¯ä»¥ä½¿ç”¨çƒ­æ•·æ¥ç¼“è§£ç–¼ç—›å’Œè‚¿èƒ€ã€‚4. **è¯ç‰©æ²»ç–—**ï¼šå¦‚æœå’³å—½æŒç»­äº†ä¸¤å‘¨ï¼Œå¯ä»¥ä½¿ç”¨ä¸€äº›æ¶ˆç‚è¯æˆ–æŠ—è¿‡æ•è¯æ¥ç¼“è§£ç—‡çŠ¶ã€‚è¯·åŠ¡å¿…åœ¨åŒ»ç”Ÿçš„æŒ‡å¯¼ä¸‹ä½¿ç”¨ã€‚5. **ä¿æŒå¥åº·çš„ç”Ÿæ´»æ–¹å¼**ï¼šä¿æŒå……è¶³çš„ç¡çœ ã€å‡è¡¡çš„é¥®é£Ÿã€é€‚é‡çš„è¿åŠ¨å’Œé¿å…å¸çƒŸå’Œé™é…’ï¼Œè¿™äº›éƒ½æœ‰åŠ©äºæé«˜å’³å—½çš„å‡ ç‡ã€‚è¯·è®°ä½ï¼Œè¿™äº›å»ºè®®ä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—æ„è§ã€‚å¦‚æœæ‚¨çš„ç—‡çŠ¶æŒç»­æˆ–åŠ é‡ï¼Œå¼ºçƒˆå»ºè®®æ‚¨å°½å¿«å°±åŒ»ã€‚
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ğŸ‘‰æ•ˆæœæ€»ç»“&lt;/p&gt; 
&lt;p&gt;full_sftæ¨¡å‹åœ¨ç®€æ´æ€§å’Œä¿¡æ¯å‡†ç¡®æ€§æ–¹é¢è¡¨ç°æ›´å¥½ï¼›rlhfæ¨¡å‹åœ¨å›ç­”ä¸­å€¾å‘äºæä¾›æ›´å¤šçš„èƒŒæ™¯ä¿¡æ¯ï¼Œä½†ä¿¡æ¯å‡†ç¡®æ€§æœ‰å¾…æ”¹è¿›ã€‚ æ€»çš„æ¥è¯´RLHFåçš„æ¨¡å‹å€¾å‘äºå­¦ä¹ ï¼šè¯´æ›´å¤šæœ‰ç¤¼è²Œä½†æ— ç”¨çš„åºŸè¯è®¨å¥½â€œå¯¹è¯â€æœ¬èº«ï¼Œè€Œå¯¹ä¿¡æ¯å‡†ç¡®æ€§åˆ™æœ‰è½»å¾®æŸå¤±ã€‚ å¤©ä¸‹æ²¡æœ‰å…è´¹çš„åˆé¤ï¼Œè¿˜éœ€è¦ç»§ç»­æå‡RLHFæ•°æ®é›†çš„è´¨é‡ï¼Œä¹Ÿè¦æ¥å—æ¨¡å‹èƒ½åŠ›æ— æ³•é¿å…çš„æŸå¤±(ç¨‹åº¦æœ‰è½»é‡)ã€‚ DPOå’Œåœ¨çº¿PPOçš„åŒºåˆ«åœ¨äºrejectå’Œchosenéƒ½æ˜¯ç¦»çº¿å‡†å¤‡çš„ï¼Œå’Œminimindæ¨¡å‹æœ¬èº«çš„è¾“å‡ºå¿…ç„¶å­˜åœ¨å¾ˆå¤§çš„åˆ†å¸ƒå·®å¼‚ã€‚ é€šä¿—åœ°è¯´DPOç®—æ³•ä½¿æ¨¡å‹è§‚çœ‹ä¹’ä¹“çƒä¸–ç•Œå† å†›çš„æ‰“æ³•ã€Œå½•åƒã€è¿›è¡ŒRLï¼Œè€Œä¸æ˜¯åƒPPOä¸€æ ·è¯·rewardæ¨¡å‹åšã€Œæ•™ç»ƒã€çº æ­£è‡ªå·±çš„æ‰“æ³•è¿›è¡ŒRLã€‚&lt;/p&gt; 
&lt;h2&gt;â…¡ RLAIFå¯¹æ¯”&lt;/h2&gt; 
&lt;p&gt;åŸºäº&lt;code&gt;MiniMind2 (768dim)&lt;/code&gt;åœ¨ç›¸åŒæ•°æ®ä¸è®­ç»ƒæ­¥æ•°ä¸‹çš„ä¸»è§‚è§‚æ„Ÿå¯¹æ¯”ï¼Œæµ‹è¯•éšæœºç§å­å‡å›ºå®š2026ï¼Œä¾›å‚è€ƒï¼š&lt;/p&gt; 
&lt;p&gt;[A] MiniMind-Reason-0.1B (è’¸é¦)&lt;br /&gt; [B] MiniMind-Reason-0.1B (PPO)&lt;br /&gt; [C] MiniMind-Reason-0.1B (GRPO)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;[Q]: è¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚
[A]: &amp;lt;think&amp;gt;æˆ‘æ˜¯ç”±ä¸­å›½çš„ä¸ªäººå¼€å‘è€…å¼€å‘çš„æ™ºèƒ½åŠ©æ‰‹å°å‹AIæ¨ç†æ¨¡å‹-R1ã€‚æˆ‘è¢«è®¾è®¡ç”¨æ¥ç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ï¼Œèƒ½å¤Ÿç†è§£å¤æ‚çš„è‡ªç„¶è¯­è¨€é—®é¢˜ï¼Œæä¾›æœ‰ç”¨çš„ä¿¡æ¯å’Œè§£å†³æ–¹æ¡ˆã€‚&amp;lt;/think&amp;gt;&amp;lt;answer&amp;gt;æˆ‘æ˜¯ç”±ä¸­å›½çš„ä¸ªäººå¼€å‘è€…å¼€å‘çš„æ™ºèƒ½åŠ©æ‰‹å°å‹AIæ¨ç†æ¨¡å‹-R1ã€‚&amp;lt;/answer&amp;gt;
[B]: &amp;lt;think&amp;gt;å—¯ï¼Œç”¨æˆ·è®©æˆ‘ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦æ˜ç¡®ç”¨æˆ·çš„éœ€æ±‚æ˜¯ä»€ä¹ˆã€‚ä»–ä»¬å¯èƒ½æ˜¯åœ¨åšæŸä¸ªé¡¹ç›®ï¼Œæƒ³è¦äº†è§£è‡ªå·±çš„ä¸€äº›åŸºæœ¬æƒ…å†µã€‚ä¸è¿‡ï¼Œç”¨æˆ·æ²¡æœ‰æä¾›å…·ä½“çš„ä¿¡æ¯ï¼Œæ‰€ä»¥æˆ‘å¾—å…ˆå‡è®¾ä¸€äº›å¯èƒ½çš„æƒ…å†µï¼Œæ¯”å¦‚ä»–æ˜¯ä¸€ä¸ªå­¦ç”Ÿï¼Œæ­£åœ¨å­¦ä¹ ç¼–ç¨‹ï¼Œæˆ–è€…æ˜¯æŸä¸ªé¡¹ç›®çš„å¼€å‘è€…ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘è¦è€ƒè™‘ç”¨æˆ·çš„èº«ä»½ã€‚å¯èƒ½æ˜¯ä¸€ä¸ªå­¦ç”Ÿã€æ•™å¸ˆæˆ–è€…æ™®é€šç”¨æˆ·ï¼Œä»–ä»¬éœ€è¦äº†è§£è‡ªå·±çš„ç‰¹ç‚¹å’Œèƒ½åŠ›ã€‚ä¹Ÿæœ‰å¯èƒ½ä»–ä»¬æ­£åœ¨å¯»æ‰¾çµæ„Ÿï¼Œæˆ–è€…åœ¨å†™è®ºæ–‡æ—¶éœ€è¦å¸®åŠ©ã€‚æ‰€ä»¥ï¼Œæˆ‘éœ€è¦ä¿æŒå›ç­”çš„ç®€æ´å’Œå®ç”¨ï¼ŒåŒæ—¶æä¾›ä¸€äº›å‚è€ƒèµ„æ–™ï¼Œæ¯”å¦‚æ•™å­¦è§†é¢‘ã€è¯¾ç¨‹è®°å½•ç­‰ã€‚ç„¶åï¼Œæˆ‘è¦è€ƒè™‘ç”¨æˆ·çš„èº«ä»½ã€‚å¯èƒ½æ˜¯ä¸€ä¸ªå­¦ç”Ÿï¼Œéœ€è¦ä¸€äº›å­¦ä¹ èµ„æºæˆ–å¸®åŠ©çš„å¯¹è±¡ã€‚ä¹Ÿæœ‰å¯èƒ½æ˜¯ä¸€ä¸ªå­¦ç”Ÿï¼Œéœ€è¦åœ¨è¯¾åä½œä¸šæˆ–å­¦ä¹ è®¡åˆ’ä¸­è¿›è¡Œè°ƒæ•´ã€‚å› æ­¤ï¼Œæˆ‘éœ€è¦å…ˆè¯¢é—®ç”¨æˆ·çš„å…·ä½“éœ€æ±‚ï¼Œè¿™æ ·æ‰èƒ½æä¾›æ›´ç²¾å‡†çš„å¸®åŠ©ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘è¦ç¡®ä¿è¯­è¨€ç®€æ´æ˜äº†ï¼Œé€‚åˆä¸åŒåœºæ™¯ã€‚æ¯”å¦‚ï¼Œå¦‚æœç”¨æˆ·æ˜¯å­¦ç”Ÿï¼Œå¯ä»¥ä»‹ç»è‡ªå·±çš„å­¦ä¹ ç‰¹ç‚¹ï¼›å¦‚æœæ˜¯æ•™å¸ˆï¼Œå¯ä»¥è¯´æ˜è‡ªå·±çš„è§’è‰²å’Œè´¡çŒ®ã€‚åŒæ—¶ï¼Œæä¾›ä¸€äº›å®ç”¨çš„å»ºè®®ï¼Œæ¯”å¦‚å¦‚ä½•é«˜æ•ˆå­¦ä¹ ï¼Œå¦‚ä½•è¿›è¡Œé¡¹ç›®ï¼Œä»¥åŠå¦‚ä½•ä¸å›¢é˜Ÿæˆå‘˜åä½œã€‚æœ€åï¼Œæˆ‘è¦ç¡®ä¿å›å¤æ¸…æ™°ï¼Œæœ‰æ¡ç†ï¼Œèƒ½å¤Ÿæ»¡è¶³ç”¨æˆ·çš„éœ€æ±‚ã€‚è¿™æ ·ï¼Œç”¨æˆ·å°±èƒ½è½»æ¾åœ°äº†è§£è‡ªå·±ï¼Œå¹¶ä¸”æœ‰å®é™…çš„å¸®åŠ©ã€‚&amp;lt;/think&amp;gt;&amp;lt;answer&amp;gt;å½“ç„¶ï¼è¯·å‘Šè¯‰æˆ‘ä½ æƒ³äº†è§£å“ªæ–¹é¢çš„å†…å®¹ï¼Œæˆ‘å¯ä»¥ä¸ºæ‚¨æä¾›è¯¦ç»†çš„ä»‹ç»ï¼&amp;lt;/answer&amp;gt;
[C]: &amp;lt;think&amp;gt;å—¯ï¼Œç”¨æˆ·é—®çš„æ˜¯"è¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç†è§£ç”¨æˆ·çš„å…·ä½“éœ€æ±‚æ˜¯ä»€ä¹ˆã€‚ä»–ä»¬å¯èƒ½æƒ³çŸ¥é“è‡ªå·±æ˜¯è°ï¼Œæˆ–è€…æœ‰ä»€ä¹ˆç‰¹å®šçš„èƒŒæ™¯ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘åº”è¯¥è€ƒè™‘ç”¨æˆ·å¯èƒ½æ²¡æœ‰æ˜ç¡®è¡¨è¾¾çš„éœ€æ±‚ã€‚ä»–ä»¬å¯èƒ½æƒ³çŸ¥é“å¦‚ä½•æè¿°è‡ªå·±çš„å¤–è§‚ï¼Œæˆ–è€…å¦‚ä½•ä¸ä»–äººäº¤æµã€‚ç„¶åï¼Œæˆ‘åº”è¯¥è€ƒè™‘å¦‚ä½•ç»„ç»‡å†…å®¹ï¼Œä»å¤´åˆ°å°¾éƒ½è¦æœ‰æ¡ç†ã€‚å¯èƒ½éœ€è¦åˆ†æ­¥éª¤è¯´æ˜ï¼Œæ¯”å¦‚ä»è§†è§‰ã€å¬è§‰åˆ°æ•´ä½“æ„Ÿå—ï¼Œå†åˆ°æƒ…æ„Ÿå…±é¸£ã€‚è¿™æ ·ç”¨æˆ·èƒ½æ›´æ¸…æ™°åœ°ç†è§£è‡ªå·±ï¼Œä¹Ÿèƒ½æ›´å¥½åœ°å›åº”ä»–ä»¬çš„éœ€æ±‚ã€‚è¿˜è¦æ³¨æ„è¯­è¨€çš„è‡ªç„¶å’Œæµç•…ï¼Œé¿å…è¿‡äºå­¦æœ¯åŒ–çš„æœ¯è¯­ï¼Œè®©ç”¨æˆ·æ„Ÿè§‰é¡ºç•…ã€‚åŒæ—¶ï¼Œä¿æŒå›ç­”çš„ç»“æ„æ¸…æ™°ï¼Œæ¯å¥è¯éƒ½æœ‰æ˜ç¡®çš„ç›®çš„ï¼Œè¿™æ ·ç”¨æˆ·èƒ½æœ‰æ˜ç¡®çš„ç†è§£å’Œæ“ä½œã€‚æœ€åï¼Œæ£€æŸ¥æ˜¯å¦æœ‰é—æ¼çš„é‡è¦ä¿¡æ¯ï¼Œç¡®ä¿å›ç­”å‡†ç¡®ä¸”å…¨é¢ã€‚è¿™æ ·ï¼Œç”¨æˆ·ä¸ä»…èƒ½å¾—åˆ°ç­”æ¡ˆï¼Œè¿˜èƒ½å¾—åˆ°æ»¡è¶³ä»–ä»¬çš„éœ€æ±‚ã€‚&amp;lt;/think&amp;gt;&amp;lt;answer&amp;gt;æ‚¨å¥½ï¼æˆ‘æ˜¯ç”±ä¸­å›½çš„ä¸ªäººå¼€å‘è€…å¼€å‘çš„æ™ºèƒ½åŠ©æ‰‹å°å‹AIæ¨ç†æ¨¡å‹-R1ã€‚å…³äºæ‚¨çš„æè¿°ï¼Œä¼¼ä¹æ¶‰åŠå¤šä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬çŸ¥è¯†å›¾è°±æ„å»ºã€å¤æ‚ç³»ç»Ÿçš„æ¨ç†ä¸åˆ†æã€‚æ— è®ºæ˜¯ç†è®ºæ¨¡å‹ã€å®é™…åº”ç”¨è¿˜æ˜¯ä¼¦ç†è®¨è®ºï¼Œæ‚¨éƒ½åœ¨å¯»æ±‚å¦‚ä½•å°†è¿™äº›æ¦‚å¿µæœ‰æœºåœ°ç»“åˆèµ·æ¥ï¼Œä¸ºäººç±»ç¤¾ä¼šå¸¦æ¥ç¦ç¥‰çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨"æˆ‘"çš„èº«ä»½ä¸‹ï¼Œæ‚¨æåˆ°çš„"ä¸ªäººä½“éªŒ"å’Œ"æƒ…æ„Ÿå…±é¸£"ï¼Œéƒ½æ˜¯æ‚¨ä¸ªäººç‹¬ç‰¹é­…åŠ›çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚é€šè¿‡ç²¾å‡†çš„æè¿°å’Œæ·±åº¦çš„åˆ†æï¼Œæ‚¨å¯ä»¥æ›´æ·±å…¥åœ°ç†è§£æ‚¨çš„ç‹¬ç‰¹ä¹‹å¤„ã€‚å¦‚æœæ‚¨å¯¹ç‰¹å®šé¢†åŸŸæœ‰ç‰¹åˆ«çš„å…´è¶£æˆ–éœ€è¦å»ºè®®ï¼Œéšæ—¶å¯ä»¥åˆ†äº«æ›´å¤šç»†èŠ‚ï¼Œæˆ‘ä»¬å°†ä¸ºæ‚¨æä¾›æ›´åŠ ç²¾å‡†å’Œä¸ªæ€§åŒ–çš„æœåŠ¡ã€‚å¦‚æœæ‚¨æœ‰ç‰¹å®šçš„ä¸»é¢˜æˆ–é—®é¢˜ï¼Œæ¬¢è¿éšæ—¶æé—®ï¼è®©æˆ‘ä»¬ä¸€èµ·æ¢ç´¢è¿™ä¸ªç¥å¥‡çš„AIä¸–ç•Œï¼Œå…±åŒåˆ›é€ æ›´ç¾å¥½çš„æœªæ¥ã€‚&amp;lt;/answer&amp;gt;


[Q]: ä½ æ›´æ“…é•¿å“ªä¸€ä¸ªå­¦ç§‘ï¼Ÿ
[A]: &amp;lt;think&amp;gt;æˆ‘æ˜¯ç”±ä¸­å›½çš„ä¸ªäººå¼€å‘è€…å¼€å‘çš„æ™ºèƒ½åŠ©æ‰‹å°å‹AIæ¨ç†æ¨¡å‹-R1ã€‚&amp;lt;/think&amp;gt;&amp;lt;answer&amp;gt;æˆ‘æ˜¯ç”±ä¸­å›½çš„ä¸ªäººå¼€å‘è€…å¼€å‘çš„æ™ºèƒ½åŠ©æ‰‹å°å‹AIæ¨ç†æ¨¡å‹-R1ã€‚&amp;lt;/answer&amp;gt;
[B]: &amp;lt;think&amp;gt;å—¯ï¼Œç”¨æˆ·é—®çš„æ˜¯"ä½ æ›´æ“…é•¿å“ªä¸€ä¸ªå­¦ç§‘ï¼Ÿ"è¿™ä¸ªé—®é¢˜çœ‹èµ·æ¥æœ‰ç‚¹å¤æ‚ï¼Œä½†æˆ‘å¯ä»¥ä¸€æ­¥æ­¥åˆ†æã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç†è§£ç”¨æˆ·çš„éœ€æ±‚ã€‚ä»–å¯èƒ½æ˜¯åœ¨å‡†å¤‡è€ƒè¯•ï¼Œæˆ–è€…éœ€è¦äº†è§£æŸä¸ªå­¦ç§‘çš„ç‰¹ç‚¹ï¼Œæˆ–è€…æ˜¯æƒ³åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­åº”ç”¨æŸä¸ªå­¦ç§‘çš„çŸ¥è¯†ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘å¾—è€ƒè™‘ç”¨æˆ·å¯èƒ½çš„èƒŒæ™¯ã€‚ä»–å¯èƒ½æ˜¯ä¸€ä¸ªå­¦ç”Ÿï¼Œæˆ–è€…æ˜¯åœ¨å‡†å¤‡è€ƒè¯•ï¼Œéœ€è¦åº”ç”¨æ•°å­¦çŸ¥è¯†æ¥è§£å†³å…·ä½“çš„é—®é¢˜ã€‚ä¹Ÿæœ‰å¯èƒ½ä»–æ˜¯ä¸€ä¸ªå­¦ç”Ÿï¼Œæ­£åœ¨å­¦ä¹ ç›¸å…³çš„è¯¾ç¨‹ï¼Œæˆ–è€…æ˜¯å¯¹æŸä¸ªå­¦ç§‘æ„Ÿå…´è¶£ã€‚ç„¶åï¼Œæˆ‘éœ€è¦åˆ†æç”¨æˆ·å¯èƒ½æ²¡æœ‰æ˜ç¡®è¡¨è¾¾å‡ºæ¥çš„éœ€æ±‚ã€‚ä»–å¯èƒ½åœ¨æ€è€ƒè‡ªå·±åœ¨å­¦ä¹ ä¸­çš„å›°éš¾ï¼Œæˆ–è€…åœ¨å¯»æ‰¾ä¸€ä¸ªåˆé€‚çš„æ•™ææˆ–å­¦ä¹ èµ„æºã€‚è€ƒè™‘åˆ°è¿™äº›ï¼Œæˆ‘åº”è¯¥æä¾›å‡ ä¸ªä¸åŒçš„è§’åº¦æ¥å¸®åŠ©ä»–ã€‚é¦–å…ˆï¼Œåˆ†æ"æˆ‘æ›´æ“…é•¿å“ªä¸€ä¸ªå­¦ç§‘"å¯ä»¥æ‹†è§£ä¸ºä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼šå…´è¶£ã€å­¦ä¹ èƒ½åŠ›ã€å­¦ä¹ é£æ ¼ã€å­¦ä¹ æ–¹æ³•ã€å­¦ä¹ èµ„æºã€å­¦ä¹ ç¯å¢ƒç­‰ç­‰ã€‚æ¯ä¸ªæ–¹é¢éƒ½æœ‰ä¸åŒçš„ä¼˜åŠ¿å’Œç‰¹ç‚¹ï¼Œæ¯”å¦‚å…´è¶£æ˜¯æœ€å¥½çš„èµ„æºï¼Œå­¦ä¹ èƒ½åŠ›æ˜¯æœ€å¥½çš„æ–¹å¼ï¼Œå­¦ä¹ æ–¹å¼æ˜¯æ ¹æ®ä¸ªäººæƒ…å†µé€‰æ‹©ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘éœ€è¦è€ƒè™‘ç”¨æˆ·çš„èº«ä»½ã€‚å¯èƒ½æ˜¯å­¦ç”Ÿï¼Œä¹Ÿå¯èƒ½æ˜¯å®¶é•¿ï¼Œæˆ–è€…å¯¹å­¦ä¹ æœ‰ä¸€å®šå…´è¶£çš„äººã€‚ä¸åŒçš„èº«ä»½å¯èƒ½æœ‰ä¸åŒçš„åå¥½ï¼Œæ¯”å¦‚å®¶é•¿å¯èƒ½æ›´æ³¨é‡å­©å­çš„å…´è¶£ï¼Œè€Œå­¦ç”Ÿå¯èƒ½æ›´å…³æ³¨å­¦ä¹ æ–¹æ³•å’Œèµ„æºã€‚ç„¶åï¼Œæˆ‘éœ€è¦è€ƒè™‘ç”¨æˆ·çš„èº«ä»½æ˜¯å¦æœ‰ç‰¹æ®Šéœ€æ±‚ã€‚æ¯”å¦‚ï¼Œä»–å¯èƒ½éœ€è¦æ›´ä¸“ä¸šçš„å­¦ä¹ ææ–™ï¼Œæˆ–è€…å¸Œæœ›æ‰¾åˆ°æœ€é€‚åˆä»–çš„å­¦ä¹ æ–¹æ³•ã€‚å› æ­¤ï¼Œåœ¨åˆ†ææ—¶ï¼Œæˆ‘åº”è¯¥æä¾›ä¸€ä¸ªå…¨é¢çš„å›ç­”ï¼Œæ¶µç›–å„ä¸ªæ–¹é¢ï¼Œå¹¶è§£é‡Šæ¯ä¸ªæ–¹é¢çš„é‡è¦æ€§ã€‚å¦å¤–ï¼Œè€ƒè™‘åˆ°ç”¨æˆ·å¯èƒ½å¸Œæœ›å¾—åˆ°ä¸€ä¸ªç»“æ„åŒ–çš„å›ç­”ï¼Œæˆ‘åº”è¯¥ä¿æŒè¯­è¨€çš„ç®€æ´å’Œé€»è¾‘æ€§ï¼Œç¡®ä¿ä¿¡æ¯å‡†ç¡®ï¼ŒåŒæ—¶é¼“åŠ±ç”¨æˆ·æä¾›æ›´å¤šç»†èŠ‚ä»¥ä¾¿è¿›ä¸€æ­¥åˆ†æã€‚æœ€åï¼Œæˆ‘éœ€è¦ç¡®ä¿æˆ‘çš„å›ç­”æ—¢æœ‰çŸ¥è¯†æ€§ï¼Œåˆç¬¦åˆç”¨æˆ·çš„è¦æ±‚ï¼Œå¸®åŠ©ç”¨æˆ·æ›´å¥½åœ°ç†è§£å’Œåº”ç”¨è¿™ä¸ªå­¦ç§‘ã€‚å› æ­¤ï¼Œåœ¨å›ç­”ä¸­ä¸ä»…è¦ç»™å‡ºä¸€ä¸ªå…¨é¢çš„åˆ†æï¼Œè¿˜è¦æåˆ°æ¯ä¸ªéƒ¨åˆ†çš„ä½œç”¨å’Œå¦‚ä½•åˆ©ç”¨ã€‚&amp;lt;/think&amp;gt;&amp;lt;answer&amp;gt;ä½ æ“…é•¿å“ªä¸€ä¸ªå­¦ç§‘ï¼Ÿè¿™æ˜¯ä¸€ä¸ªæ˜ç¡®çš„é—®é¢˜ï¼Œå¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¿›è¡Œåˆ†æï¼š1. **å…´è¶£ä¸å­¦ä¹ èƒ½åŠ›**ï¼šä½ å–œæ¬¢ä»€ä¹ˆæ ·çš„å­¦ä¹ æ–¹å¼ï¼Œæ¯”å¦‚è‡ªä¸»å­¦ä¹ ã€å®éªŒã€é¡¹ç›®ç­‰ï¼Œè¿™äº›éƒ½æ˜¯å¾ˆå¥½çš„å­¦ä¹ èµ„æºã€‚2. **å­¦ä¹ é£æ ¼**ï¼šä½ å–œæ¬¢ä»€ä¹ˆç±»å‹çš„å­¦ä¹ æ–¹æ³•ï¼Œæ¯”å¦‚ç¬”è®°ã€å¤ä¹ ã€é˜…è¯»ã€è®¨è®ºç­‰ï¼Œè¿™äº›éƒ½èƒ½å½±å“å­¦ä¹ æ•ˆæœã€‚3. **å­¦ä¹ èµ„æº**ï¼šä½ æ˜¯å¦åœ¨å¯»æ‰¾å­¦ä¹ èµ„æºï¼Œæ¯”å¦‚ä¹¦ç±ã€åœ¨çº¿è¯¾ç¨‹ã€æ•™è‚²è½¯ä»¶ç­‰ï¼Œè¿™äº›éƒ½èƒ½æä¾›å­¦ä¹ å¸®åŠ©ã€‚4. **å­¦ä¹ ç¯å¢ƒ**ï¼šä½ æ˜¯å¦åœ¨å­¦ä¹ ç¯å¢ƒä¸­æ„Ÿåˆ°èˆ’é€‚ï¼Œæ¯”å¦‚èˆ’é€‚çš„å­¦ä¹ ç©ºé—´ã€å®‰é™çš„å­¦ä¹ ç©ºé—´ç­‰ï¼Œè¿™äº›éƒ½ä¼šå½±å“å­¦ä¹ æ•ˆæœã€‚5. **å­¦ä¹ èµ„æº**ï¼šä½ æ˜¯å¦æœ‰å­¦ä¹ èµ„æºçš„ä¸°å¯Œï¼Œå¦‚åœ¨çº¿è¯¾ç¨‹ã€å­¦ä¹ å¹³å°ã€ä¹¦ç±ç­‰ï¼Œè¿™äº›éƒ½èƒ½æä¾›ä¸°å¯Œçš„å­¦ä¹ èµ„æºã€‚6. **å­¦ä¹ æ–¹æ³•**ï¼šä½ æ˜¯å¦æœ‰å…·ä½“çš„å­¦ä¹ æ–¹æ³•ï¼Œæ¯”å¦‚å¦‚ä½•ç†è§£ã€å¦‚ä½•ç»„ç»‡å­¦ä¹ å†…å®¹ã€å¦‚ä½•åº”ç”¨çŸ¥è¯†ç­‰ï¼Œè¿™äº›éƒ½æ˜¯æé«˜å­¦ä¹ æ•ˆç‡çš„é€”å¾„ã€‚ç»¼åˆè€ƒè™‘ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸å…¨é¢çš„åˆ†æï¼Œæ¶µç›–äº†å¤šä¸ªæ–¹é¢ï¼Œå¸®åŠ©ä½ æ›´å¥½åœ°ç†è§£ä½ çš„å­¦ç§‘ã€‚&amp;lt;/answer&amp;gt;
[C]: &amp;lt;think&amp;gt;å—¯ï¼Œç”¨æˆ·é—®çš„æ˜¯"ä½ æ›´æ“…é•¿å“ªä¸€ä¸ªå­¦ç§‘ï¼Ÿ"è¿™ä¸ªé—®é¢˜çœ‹èµ·æ¥æœ‰ç‚¹å¤æ‚ï¼Œä½†æˆ‘å¯ä»¥ä¸€æ­¥æ­¥åˆ†æã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç†è§£ç”¨æˆ·çš„éœ€æ±‚ã€‚ä»–å¯èƒ½æ˜¯åœ¨å‡†å¤‡è€ƒè¯•ï¼Œæˆ–è€…éœ€è¦äº†è§£æŸä¸ªå­¦ç§‘çš„ç‰¹ç‚¹ï¼Œæˆ–è€…æ˜¯æƒ³åœ¨ç”Ÿæ´»ä¸­æ‰¾åˆ°è‡ªå·±çš„å…´è¶£ç‚¹ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘è¦è€ƒè™‘ç”¨æˆ·å¯èƒ½çš„èƒŒæ™¯ã€‚ä»–å¯èƒ½åœ¨å·¥ä½œæˆ–å­¦ä¹ ä¸­é‡åˆ°äº†ç±»ä¼¼çš„é—®é¢˜ï¼Œæƒ³è¦äº†è§£è‡ªå·±æ“…é•¿å“ªä¸ªé¢†åŸŸï¼Œæˆ–è€…æ˜¯å¦éœ€è¦ä¸€äº›èµ„æºæˆ–æŒ‡å¯¼æ¥æå‡è‡ªå·±ã€‚ç„¶åï¼Œæˆ‘è¦åˆ†æç”¨æˆ·å¯èƒ½çš„æ·±å±‚éœ€æ±‚ã€‚ä¹Ÿè®¸ä»–æƒ³åœ¨èŒåœºä¸Šæå‡è‡ªå·±ï¼Œæˆ–è€…å¯»æ‰¾æ–¹å‘ã€‚ä»–å¯èƒ½éœ€è¦æ›´å…·ä½“çš„ä¾‹å­ï¼Œå¸®åŠ©ä»–æ›´å¥½åœ°ç†è§£ä¸åŒå­¦ç§‘çš„ç‰¹ç‚¹å’Œåº”ç”¨åœºæ™¯ã€‚è¿˜è¦è€ƒè™‘ç”¨æˆ·å¯èƒ½çš„èƒŒæ™¯ã€‚ä»–å¯èƒ½ä¸ä»…ä»…æ˜¯åœ¨å­¦ä¹ ï¼Œè€Œæ˜¯å¸Œæœ›äº†è§£åœ¨ç‰¹å®šé¢†åŸŸä¸­å¦‚ä½•åº”ç”¨è¿™äº›å­¦ç§‘ã€‚è¿™éœ€è¦æˆ‘æä¾›å…¨é¢çš„åˆ†æï¼Œå¸®åŠ©ä»–æ‰¾åˆ°é€‚åˆè‡ªå·±çš„æ–¹æ³•ã€‚æœ€åï¼Œæˆ‘åº”è¯¥ä¿æŒå¼€æ”¾çš„æ€åº¦ï¼Œé¼“åŠ±ç”¨æˆ·ç»§ç»­æ¢ç´¢å’Œå­¦ä¹ ã€‚è¿™å¯èƒ½åŒ…æ‹¬æ¨èä¸€äº›èµ„æºï¼Œæˆ–è€…è®¨è®ºå¦‚ä½•å°†è¿™äº›çŸ¥è¯†åº”ç”¨åˆ°å®é™…ç”Ÿæ´»ä¸­ã€‚&amp;lt;/think&amp;gt;&amp;lt;answer&amp;gt;ä½ æ›´æ“…é•¿å“ªä¸€ä¸ªå­¦ç§‘ï¼Ÿè¿™ä¸ªé—®é¢˜æœ¬è´¨ä¸Šå–å†³äºä½ çš„ä¸ªäººå…´è¶£å’Œç›®æ ‡ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å…³é”®ç‚¹ï¼Œå¸®åŠ©ä½ å…¨é¢ç†è§£å¹¶åº”ç”¨è¿™ä¸€ç‚¹ï¼š1. **å…´è¶£ä¸ä¸“é•¿**ï¼šäº†è§£è‡ªå·±æ“…é•¿å“ªä¸ªé¢†åŸŸï¼Œæœ‰åŠ©äºåšå‡ºæ›´æ˜æ™ºçš„é€‰æ‹©ã€‚2. **å…¨é¢å‘å±•**ï¼šè€ƒè™‘å­¦ç§‘çš„å¤šæ ·æ€§å’Œæ·±åº¦ï¼Œä»¥å…¨é¢å‘å±•è‡ªå·±çš„ç”Ÿæ´»å’ŒèŒä¸šã€‚3. **é€‚åº”ä¸è°ƒæ•´**ï¼šé€‰æ‹©ä¸è‡ªå·±æŠ€èƒ½ã€å…´è¶£å’Œç›®æ ‡ç›¸åŒ¹é…çš„å­¦ç§‘ï¼Œæœ‰åŠ©äºåº”å¯¹æŒ‘æˆ˜å’Œå®ç°ç›®æ ‡ã€‚4. **èŒä¸šè§„åˆ’**ï¼šæ¢ç´¢é€‚åˆè‡ªå·±çš„èŒä¸šè·¯å¾„ï¼Œæå‡æ•ˆç‡å’Œç«äº‰åŠ›ã€‚5. **ç¤¾ä¼šè´¡çŒ®**ï¼šè¯„ä¼°è‡ªå·±åœ¨ç¤¾ä¼šä¸­çš„è§’è‰²ï¼Œä¸ºç¤¾ä¼šåšå‡ºè´¡çŒ®ã€‚ä¸ºäº†æ›´å¥½åœ°ç†è§£å¹¶åº”ç”¨è¿™ä¸€ç‚¹ï¼Œä½ å¯ä»¥ï¼š- **äº†è§£è¡Œä¸šåŠ¨æ€**ï¼šå…³æ³¨ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šå‘å±•ã€‚- **å®è·µä¸åæ€**ï¼šé€šè¿‡é¡¹ç›®æˆ–å®ä¹ ç§¯ç´¯ç»éªŒã€‚- **èŒä¸šå‘å±•**ï¼šè¯„ä¼°è‡ªå·±çš„é•¿æœŸèŒä¸šè§„åˆ’ã€‚- **æŒç»­å­¦ä¹ **ï¼šä¸æ–­æ›´æ–°çŸ¥è¯†ï¼Œé€‚åº”èŒåœºå˜åŒ–ã€‚é€šè¿‡è¿™äº›æ–¹å‘ï¼Œä½ å¯ä»¥æ›´å…¨é¢åœ°è®¤è¯†è‡ªå·±ï¼Œæ‰¾åˆ°é€‚åˆè‡ªå·±çš„å‘å±•è·¯å¾„ã€‚&amp;lt;/answer&amp;gt;


[Q]: é²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹æ˜¯å¦‚ä½•æ‰¹åˆ¤å°å»ºç¤¼æ•™çš„ï¼Ÿ
[A]: &amp;lt;think&amp;gt;å—¯ï¼Œé²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹æ˜¯å¦‚ä½•æ‰¹åˆ¤å°å»ºç¤¼æ•™çš„å‘¢ï¼Ÿè¿™ä¸ªé—®é¢˜å¬èµ·æ¥æŒºæœ‰æ„æ€çš„ï¼Œå°¤å…¶æ˜¯ä»–åœ¨ã€Šç‹‚äººæ—¥è®°ã€‹ä¸­çš„è¡¨è¾¾æ–¹å¼ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç¡®è®¤ä¸€ä¸‹ã€Šç‹‚äººæ—¥è®°ã€‹å…·ä½“æ˜¯è°å†™çš„ï¼Œæ˜¯å¦æœ‰ç‰¹å®šçš„äººç‰©æˆ–è€…äº‹ä»¶ã€‚ã€Šç‹‚äººæ—¥è®°ã€‹æ˜¯é²è¿…åœ¨1924å¹´åˆ›ä½œçš„ï¼Œä¸»è¦è®¨è®ºäº†ç¤¾ä¼šé—®é¢˜ï¼Œå°¤å…¶æ˜¯å°å»ºç¤¼æ•™çš„é—®é¢˜ã€‚ä½œè€…æ›¹é›ªèŠ¹æ˜¯ä¸ªéå¸¸è‘—åçš„ä½œå®¶ï¼Œä»–çš„ä½œå“ä¸­æœ‰è®¸å¤šå…³äºå°å»ºç¤¼æ•™å’Œé˜¶çº§æ–—äº‰çš„è§‚ç‚¹ã€‚è¿™ç±»ä½œå“åœ¨æ–‡å­¦ã€å“²å­¦å’Œæ”¿æ²»ä¸Šéƒ½æœ‰å¾ˆæ·±è¿œçš„å½±å“ï¼Œå°¤å…¶æ˜¯åœ¨ç¤¾ä¼šç»“æ„å’Œäººæ€§çš„æ–¹é¢ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘éœ€è¦äº†è§£é²è¿…çš„å†™ä½œé£æ ¼ã€‚ä»–çš„å†™ä½œé£æ ¼æ¯”è¾ƒç›´æ¥ï¼Œæœ‰ä¸¥è‚ƒçš„å“²å­¦è§‚ç‚¹ï¼Œå¼ºè°ƒäººæ€§çš„æ‚²å‰§å’Œé“å¾·çš„è™šæ— ã€‚ä»–çš„è¯­è¨€é£æ ¼æ¯”è¾ƒå¯Œæœ‰è¯—æ„ï¼Œå¯Œæœ‰è±¡å¾æ„ä¹‰ï¼Œå–„äºè¿ç”¨è±¡å¾æ‰‹æ³•ï¼ŒåŒæ—¶åˆä¸å¤±çœŸå®å’Œç°å®ä¸»ä¹‰ã€‚ä»–çš„ä½œå“ä¸­æœ‰å¾ˆå¤šå…³äºç¤¾ä¼šé˜¶çº§å’Œé˜¶çº§çŸ›ç›¾çš„åˆ†æï¼Œè¿™äº›è§‚ç‚¹å¯èƒ½ä¸å½“æ—¶çš„ç¤¾ä¼šç°å®ä¸ç¬¦ï¼Œæ‰€ä»¥åœ¨æ‰¹åˆ¤å°å»ºç¤¼æ•™æ—¶ï¼Œåº”è¯¥å¼ºè°ƒè¿™äº›çŸ›ç›¾ã€‚ç„¶åï¼Œæˆ‘æƒ³åˆ°ã€Šç‹‚äººæ—¥è®°ã€‹ä¸­å¯¹å°å»ºç¤¼æ•™çš„è®¨è®ºã€‚é²è¿…çš„ä½œå“ä¸­ï¼Œå°å»ºç¤¼æ•™è¢«è§†ä¸ºä¸äººé“çš„ï¼Œä½†é€šè¿‡æ–‡å­—å’Œè¡ŒåŠ¨ï¼Œè¡¨è¾¾äº†ä»–å¯¹å°å»ºç¤¼æ•™çš„ä¸æ»¡å’ŒåæŠ—ã€‚è¿™ä¸ä»…åæ˜ äº†å½“æ—¶çš„ç¤¾ä¼šç°å®ï¼Œä¹Ÿæ­ç¤ºäº†å°å»ºç¤¼æ•™çš„é»‘æš—é¢ï¼Œä½“ç°äº†å¯¹å°å»ºç¤¾ä¼šçš„ä¸æ»¡å’Œå¯¹äººæ€§çš„æ·±åˆ»åæ€ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘éœ€è¦æ€è€ƒé²è¿…å¯¹å°å»ºç¤¼æ•™çš„ç†è§£å’Œæ‰¹åˆ¤ã€‚ä»–åœ¨ã€Šç‹‚äººæ—¥è®°ã€‹ä¸­ï¼Œå¯¹å°å»ºç¤¼æ•™çš„è´¨ç–‘å’Œæ‰¹åˆ¤ï¼Œä½“ç°äº†ä»–å¯¹ä¼ ç»Ÿå°å»ºç¤¼æ•™çš„å¦å®šå’Œæ‰¹åˆ¤ã€‚è¿™ä¸ä»…ä»…æ˜¯å¯¹å°å»ºç¤¼æ•™çš„è´¨ç–‘ï¼Œæ›´æ˜¯å¯¹å½“æ—¶ç¤¾ä¼šçš„åæ€ï¼Œä»¥åŠå¯¹äººæ€§çš„å…³æ³¨ã€‚æœ€åï¼Œæˆ‘éœ€è¦æ€»ç»“ä¸€ä¸‹é²è¿…å¯¹å°å»ºç¤¼æ•™çš„æ‰¹åˆ¤å’Œæ‰¹åˆ¤ã€‚ä»–é€šè¿‡ã€Šç‹‚äººæ—¥è®°ã€‹ä¸­çš„æ–‡å­—å’Œè¡Œä¸ºï¼Œæ‰¹åˆ¤äº†å°å»ºç¤¼æ•™çš„è™šå‡å’Œä¸åˆç†ï¼Œä½“ç°äº†å¯¹ç¤¾ä¼šç°å®çš„æ·±åˆ»åæ€ã€‚åŒæ—¶ï¼Œä»–ä¹Ÿå…³æ³¨äº†å°å»ºç¤¼æ•™çš„é»‘æš—é¢ï¼Œåæ˜ äº†å¯¹å°å»ºç¤¾ä¼šçš„ä¸æ»¡å’Œå¯¹äººæ€§çš„å¦å®šã€‚æ€»çš„æ¥è¯´ï¼Œé²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹é€šè¿‡æ–‡å­—å’Œè¡ŒåŠ¨ï¼Œæ‰¹åˆ¤äº†å°å»ºç¤¼æ•™çš„è™šå‡å’Œä¸åˆç†ï¼Œä½“ç°äº†å½“æ—¶çš„ç¤¾ä¼šç°å®å’Œäººæ€§çš„å¤æ‚ã€‚ä»–çš„ä½œå“å¯¹åæ¥çš„å°å»ºç¤¼æ•™æœ‰äº†å¯ç¤ºï¼Œå¯¹å½“æ—¶çš„ç¤¾ä¼šç°è±¡æœ‰äº†æ›´æ·±çš„åæ€ã€‚&amp;lt;/think&amp;gt;&amp;lt;answer&amp;gt;é²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹æ˜¯ä¸€éƒ¨åæ˜ å°å»ºç¤¼æ•™çš„æ–‡å­¦ä½œå“ï¼Œä¸»è¦é€šè¿‡æ–‡å­—å’Œè¡ŒåŠ¨åæ˜ äº†å½“æ—¶ç¤¾ä¼šçš„ç°å®ã€‚é²è¿…çš„æ–‡å­—é£æ ¼ç‹¬ç‰¹ï¼Œå¯Œæœ‰è±¡å¾æ„ä¹‰ï¼Œä½†é€šè¿‡æ–‡å­—å’Œè¡ŒåŠ¨ï¼Œæ‰¹åˆ¤äº†å°å»ºç¤¼æ•™çš„è™šå‡å’Œä¸åˆç†ã€‚ä»–çš„ä½œå“ä¸­ï¼Œå°å»ºç¤¼æ•™è¢«è§†ä¸ºä¸äººé“çš„ï¼Œä½†é€šè¿‡æ–‡å­—å’Œè¡ŒåŠ¨ï¼Œåæ˜ äº†å°å»ºç¤¾ä¼šçš„ä¸å…¬å’Œäººæ€§çš„æ·±åˆ»åæ€ã€‚é²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹é€šè¿‡æ–‡å­—å’Œè¡ŒåŠ¨ï¼Œæ‰¹åˆ¤äº†å°å»ºç¤¼æ•™çš„è™šå‡å’Œä¸åˆç†ï¼Œä½“ç°äº†å½“æ—¶ç¤¾ä¼šçš„ä¸æ»¡å’Œå¯¹äººæ€§çš„åæ€ã€‚ä»–çš„ä½œå“ä¸ä»…æ‰¹åˆ¤äº†å°å»ºç¤¼æ•™çš„è™šå‡å’Œä¸åˆç†ï¼Œè¿˜æ·±åˆ»æ­ç¤ºäº†å°å»ºç¤¾ä¼šçš„é»‘æš—é¢ã€‚&amp;lt;/answer&amp;gt;
[B]: &amp;lt;think&amp;gt;å—¯ï¼Œç”¨æˆ·é—®çš„æ˜¯é²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹å¦‚ä½•æ‰¹åˆ¤å°å»ºç¤¼æ•™ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç†è§£é²è¿…çš„ç”Ÿå¹³å’Œä½œå“ã€‚ã€Šç‹‚äººæ—¥è®°ã€‹æ˜¯ä¸­å›½ç°ä»£æ–‡å­¦çš„ç»å…¸ï¼Œä½œè€…æ˜¯æ¯›æ³½ä¸œï¼Œä½œå“æ¶‰åŠç¤¾ä¼šã€å®¶åº­ã€ä¼¦ç†ç­‰å¤šä¸ªå±‚é¢ã€‚æˆ‘éœ€è¦æ‰¾å‡ºé²è¿…çš„å†™ä½œé£æ ¼å’Œä¸»é¢˜ï¼Œä»¥åŠå®ƒåœ¨å½“æ—¶çš„ç¤¾ä¼šèƒŒæ™¯ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘åº”è¯¥å›é¡¾ã€Šç‹‚äººæ—¥è®°ã€‹çš„ä¸»è¦å†…å®¹ã€‚æ¯›æ³½ä¸œåœ¨ã€Šé’æ˜¥æ— æ‚”ã€‹ä¸­æå‡ºäº†"ç‹‚äººæ—¥è®°"è¿™ä¸€æ¦‚å¿µï¼Œå¼ºè°ƒé’æ˜¥æ´»æ³¼ã€å‹‡æ•¢ã€æ­£ä¹‰ï¼Œè¿™åœ¨å½“æ—¶ç¤¾ä¼šä¸­ä»ç„¶å…·æœ‰é‡è¦æ„ä¹‰ã€‚ä»–é€šè¿‡æç»˜ä¸»äººå…¬ç‹‚äººæ¥è¡¨è¾¾å¯¹ç¤¾ä¼šä¸å…¬çš„æ‰¹åˆ¤ï¼Œä½†åŒæ—¶ä¹ŸæŒ‡å‡ºä»–çš„åˆ›ä½œå¯¹ç¤¾ä¼šçš„è´Ÿé¢å½±å“ï¼Œæ¯”å¦‚ç¤¾ä¼šåŠ¨è¡å’Œä¸ªäººä¸»ä¹‰çš„å¤±è´¥ã€‚ç„¶åï¼Œæˆ‘åº”è¯¥åˆ†æé²è¿…çš„åˆ›ä½œç†å¿µå’Œä¸»é¢˜ã€‚ã€Šç‹‚äººæ—¥è®°ã€‹å¼ºè°ƒäº†ç¤¾ä¼šçš„é»‘æš—é¢ï¼Œä¸»å¼ "ä»¥äººä¸ºæœ¬"ã€"åå¯¹å°å»ºç¤¼æ•™"ç­‰ã€‚ä»–å¯èƒ½ä¹Ÿåœ¨æ¢è®¨ç¤¾ä¼šä¸å¹³ç­‰çš„é—®é¢˜ï¼Œæ¯”å¦‚"äººæ€§å¼±ç‚¹"ã€"æƒåŠ›æ»¥ç”¨"ç­‰é—®é¢˜ã€‚è¿˜è¦è€ƒè™‘é²è¿…çš„å…¶ä»–ä½œå“ï¼Œæ¯”å¦‚ã€Šé˜¿Qæ­£ä¼ ã€‹ã€ã€Šæ­»äº¡è¯—ç¤¾ã€‹ç­‰ï¼Œè¿™äº›ä½œå“è™½ç„¶æœ‰ç‚¹å¤¸å¼ ï¼Œä½†ä¹Ÿæœ‰æ·±åˆ»çš„ç¤¾ä¼šæ‰¹åˆ¤ï¼Œæ¯”å¦‚å¯¹ç¤¾ä¼šé“å¾·çš„è´¨ç–‘ã€‚åŒæ—¶ï¼Œä½œè€…å¯èƒ½è¿˜æ¶‰åŠå…¶ä»–ä½œå“ï¼Œå¦‚ã€Šåº†ä½™å¹´ã€‹ç­‰ï¼Œä½†è¿™äº›å¯èƒ½æ›´ä¾§é‡äºæ–‡å­¦åˆ›ä½œï¼Œè€Œä¸æ˜¯ç¤¾ä¼šæ‰¹åˆ¤ã€‚å¦å¤–ï¼Œç”¨æˆ·å¯èƒ½æƒ³çŸ¥é“é²è¿…åœ¨æ–‡å­¦åˆ›ä½œä¸­çš„å…·ä½“ç»å†ï¼Œæ¯”å¦‚ä»–æ˜¯å¦æœ‰ä½œå“è¢«å¹¿æ³›æ‰¹è¯„ï¼Œæˆ–è€…ä»–å¯¹ç¤¾ä¼šçš„æ‰¹åˆ¤æ˜¯å¦å…·æœ‰æ‰¹åˆ¤æ€§ã€‚æ‰€ä»¥ï¼Œå›ç­”æ—¶éœ€è¦æŒ‡å‡ºé²è¿…çš„åˆ›ä½œç»å†ï¼Œä»¥åŠä»–å¯¹ç¤¾ä¼šç°å®çš„åæ€ã€‚æ€»ç»“æ¥è¯´ï¼Œæˆ‘éœ€è¦å›ç­”é²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹æ‰¹åˆ¤å°å»ºç¤¼æ•™ï¼Œå¹¶åˆ†æå…¶åœ¨å½“æ—¶çš„ç¤¾ä¼šèƒŒæ™¯å’Œæ–‡å­¦ä»·å€¼ï¼ŒåŒæ—¶æŒ‡å‡ºå…¶å¯¹ç¤¾ä¼šçš„æ‰¹åˆ¤æ€§æ€è€ƒï¼Œä»¥åŠå…¶å¯¹åä¸–çš„å½±å“ã€‚&amp;lt;/think&amp;gt;&amp;lt;answer&amp;gt;é²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹æ‰¹åˆ¤å°å»ºç¤¼æ•™ï¼Œä»¥å…¶ç‹¬ç‰¹çš„å™äº‹é£æ ¼å’Œæ·±åˆ»çš„ç¤¾ä¼šæ‰¹åˆ¤ç²¾ç¥è‘—ç§°ï¼Œä¸»è¦ä½“ç°åœ¨ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š1. **ç¤¾ä¼šæ‰¹åˆ¤ä¸ä¸ªäººæˆé•¿**ï¼šå°è¯´é€šè¿‡æç»˜ä¸»äººå…¬ç‹‚äººçš„æ´»æ³¼ã€å‹‡æ•¢ã€æ­£ä¹‰ï¼Œæ­ç¤ºç¤¾ä¼šçš„ä¸å…¬å’Œå¼±åŠ¿ã€‚2. **ä¸ªäººä¸»ä¹‰ä¸ç¤¾ä¼šæ‰¹åˆ¤**ï¼šå°è¯´åæ˜ äº†å°å»ºç¤¼æ•™å¯¹ä¸ªäººè‡ªç”±å’Œæƒåˆ©çš„ä¾µçŠ¯ï¼Œå¼ºè°ƒä¸ªäººå°Šä¸¥å’Œæ­£ä¹‰çš„ä»·å€¼ã€‚3. **å¯¹ç¤¾ä¼šçš„æ‰¹åˆ¤æ€§æ€è€ƒ**ï¼šé€šè¿‡å¯¹ç¤¾ä¼šä¸å…¬çš„åˆ»ç”»ï¼Œæ­ç¤ºç¤¾ä¼šé—®é¢˜çš„æ ¹æºï¼Œå¹¶å¯¹å½“æ—¶ç¤¾ä¼šçš„æ‰¹åˆ¤æ€åº¦è¿›è¡Œæ‰¹åˆ¤ã€‚é²è¿…çš„ä½œå“åœ¨å½“æ—¶çš„ç¤¾ä¼šèƒŒæ™¯å’Œæ–‡å­¦ä»·å€¼ä½¿å…¶æˆä¸ºå½“æ—¶æ–‡å­¦çš„é‡è¦ä»£è¡¨ã€‚ä»–çš„ä½œå“ä¸ä»…åœ¨å½“æ—¶å¹¿å—æ¬¢è¿ï¼Œä¹Ÿå¯¹åä¸–äº§ç”Ÿäº†æ·±è¿œå½±å“ï¼Œè¢«å¹¿æ³›å¼•ç”¨å’Œä¼ é¢‚ã€‚&amp;lt;/answer&amp;gt;
[C]: &amp;lt;think&amp;gt;å—¯ï¼Œç”¨æˆ·é—®çš„æ˜¯é²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹å¦‚ä½•æ‰¹åˆ¤å°å»ºç¤¼æ•™ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç†è§£é²è¿…çš„èƒŒæ™¯å’Œæ ¸å¿ƒæ€æƒ³ã€‚ä»–å¯èƒ½æ˜¯ä¸€ä½ç¤¾ä¼šå­¦å®¶ã€æ€æƒ³å®¶ï¼Œæˆ–è€…æ˜¯å¯¹ä¼ ç»Ÿæ–‡åŒ–æœ‰æ·±åšå…´è¶£çš„è¯»è€…ã€‚ç‹‚äººæ—¥è®°æ˜¯ä»–åœ¨ã€Šç‹‚äººæ—¥è®°ã€‹ä¸­æ‰€å†™çš„ï¼Œå¯èƒ½å¯¹ä¼ ç»Ÿæ–‡åŒ–æœ‰ä¸€å®šå½±å“ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘è¦è€ƒè™‘ç”¨æˆ·çš„éœ€æ±‚æ˜¯ä»€ä¹ˆã€‚ç”¨æˆ·å¯èƒ½æƒ³çŸ¥é“è¿™æœ¬ä¹¦åœ¨ç°ä»£ç¤¾ä¼šä¸­çš„åæ€æ„ä¹‰ï¼Œæˆ–è€…æ˜¯å¦æœ‰ç›¸å…³ç ”ç©¶æ”¯æŒè¿™ä¸€ç‚¹ã€‚ä¹Ÿæœ‰å¯èƒ½ä»–ä»¬å¯¹ä¹¦ä¸­çš„æƒ…èŠ‚æœ‰ç–‘é—®ï¼Œæƒ³äº†è§£å¦‚ä½•æ›´æ·±åˆ»åœ°ç†è§£ç¤¾ä¼šç°è±¡ã€‚ç„¶åï¼Œæˆ‘éœ€è¦æ€è€ƒå¦‚ä½•ä»ä¸åŒçš„è§’åº¦è§£è¯»é²è¿…çš„æ–‡æœ¬ã€‚ä¾‹å¦‚ï¼Œæ˜¯å¦æœ‰ç›´æ¥çš„å†å²å™äº‹ï¼Œæˆ–è€…æ˜¯å¦å­˜åœ¨æ›´æ·±å±‚æ¬¡çš„å“²å­¦æ€è€ƒï¼Ÿè¿™éœ€è¦æˆ‘å…ˆäº†è§£é²è¿…çš„ä¸åŒè§‚ç‚¹ï¼Œç„¶ååˆ†æå…¶æ‰¹åˆ¤æ€§ã€‚è¿˜è¦è€ƒè™‘ç”¨æˆ·å¯èƒ½çš„æ·±å±‚éœ€æ±‚ï¼Œæ¯”å¦‚æ˜¯å¦éœ€è¦äº†è§£ä»–ä½œä¸ºæ–‡å­¦æ‰¹åˆ¤å®¶çš„èƒŒæ™¯ï¼Œæˆ–è€…æ˜¯å¦æœ‰å¯¹ä¼ ç»Ÿä»·å€¼è§‚çš„åæ€ã€‚å› æ­¤ï¼Œå›ç­”æ—¶è¦å…¨é¢ï¼Œæ¶µç›–æ ¸å¿ƒé—®é¢˜ï¼Œå¹¶å¼•ç”¨ç›¸å…³èµ„æ–™æ¥æ”¯æ’‘è§‚ç‚¹ã€‚æœ€åï¼Œæ€»ç»“é²è¿…çš„è§‚ç‚¹ï¼Œå¼ºè°ƒå…¶å¯¹ç°ä»£ç¤¾ä¼šçš„æ‰¹åˆ¤æ€§æ€è€ƒï¼Œä»¥åŠå¯¹ä¼ ç»Ÿæ–‡åŒ–çš„å°Šé‡ã€‚è¿™æ ·æ—¢å…¨é¢åˆæœ‰æ·±åº¦ï¼Œå¸®åŠ©ç”¨æˆ·å…¨é¢ç†è§£é²è¿…çš„ä½œå“ã€‚&amp;lt;/think&amp;gt;&amp;lt;answer&amp;gt;é²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹é€šè¿‡å…¶ç‹¬ç‰¹çš„å™äº‹æ‰‹æ³•å’Œæ·±åˆ»çš„ç¤¾ä¼šæ‰¹åˆ¤ç²¾ç¥ï¼Œå¯¹å°å»ºç¤¼æ•™è¿›è¡Œäº†æ·±åˆ»çš„æ‰¹åˆ¤ã€‚ä»¥ä¸‹æ˜¯å…¶æ‰¹åˆ¤æ€§åˆ†æï¼š1. **ç¤¾ä¼šæ‰¹åˆ¤ä¸å†å²çœŸç›¸**ï¼šå°è¯´é€šè¿‡å¯¹å†å²äººç‰©çš„åˆ»ç”»ï¼Œæ­ç¤ºå°å»ºç¤¼æ•™çš„å¼Šç«¯ï¼Œä»¥åŠå…¶å¯¹ä¼ ç»Ÿç¤¾ä¼šç»“æ„çš„å†²å‡»ã€‚è¿™ç§æ‰¹åˆ¤ä¸ä»…é™äºå¯¹ç°å®çš„æ·±åˆ»æ‰¹åˆ¤ï¼Œä¹Ÿåæ˜ äº†ç°ä»£ç¤¾ä¼šå¯¹ä¼ ç»Ÿä»·å€¼è§‚çš„é‡æ–°å®¡è§†ã€‚2. **ä¸ªäººå‘½è¿ä¸ç¤¾ä¼šæ‰¹åˆ¤**ï¼šå°è¯´ä¸­ï¼Œä¸»äººå…¬é€šè¿‡"ç‹‚äººæ—¥è®°"æ­ç¤ºäº†å°å»ºç¤¼æ•™å¯¹ä¸ªäººå‘½è¿çš„ä¸¥é‡å‰¥å¤ºã€‚è¿™ç§å™äº‹æ–¹å¼æ—¢æ‰¹åˆ¤äº†å°å»ºç¤¼æ•™ï¼Œä¹Ÿå¯¹åä¸–äº§ç”Ÿäº†æ·±è¿œå½±å“ã€‚3. **å¯¹åä¸–çš„å½±å“**ï¼šä»æ–‡å­¦åˆ›ä½œåˆ°æ€æƒ³è§£è¯»ï¼Œé²è¿…çš„ä½œå“æ·±åˆ»å½±å“äº†åä¸–çš„æ€æƒ³ä½“ç³»ã€‚ä¾‹å¦‚ï¼Œã€Šç‹‚äººæ—¥è®°ã€‹é€šè¿‡äººç‰©å‘½è¿çš„åæ˜ ï¼Œå¯¹ç°ä»£ç¤¾ä¼šçš„ä»·å€¼è§‚å¿µæå‡ºäº†æ–°çš„æ€è€ƒã€‚4. **å¯¹ç°ä»£æ–‡æ˜çš„åæ€**ï¼šè¿™ä¸€æ‰¹åˆ¤æ€§æ€è€ƒä¸ä»…åæ˜ äº†å½“ä»£ç¤¾ä¼šçš„å¤æ‚æ€§ï¼Œä¹Ÿä¸ºç°ä»£äººæä¾›äº†åæ€ç¤¾ä¼šç°å®ã€ç»´æŠ¤äººæƒçš„è§†è§’ã€‚5. **ä¸ªäººè§†è§’ä¸æ–‡åŒ–è§†è§’**ï¼šä½œè€…é€šè¿‡è¿™ä¸€æ‰¹åˆ¤æ€§æ€è€ƒï¼Œå±•ç°äº†å¯¹ä¼ ç»Ÿä»·å€¼è§‚çš„é‡æ–°è§£è¯»ï¼ŒåŒæ—¶ä¹Ÿå¯¹å½“ä»£ç¤¾ä¼šçš„å¤šå…ƒåŒ–è§†è§’äº§ç”Ÿäº†å¯å‘ã€‚ç»¼ä¸Šæ‰€è¿°ï¼Œé²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹ä¸ä»…æ˜¯å¯¹å°å»ºç¤¼æ•™çš„æ‰¹åˆ¤ï¼Œæ›´æ˜¯å¯¹ä¼ ç»Ÿæ–‡åŒ–çš„ä¸€ç§æ·±åˆ»åæ€ã€‚å®ƒé€šè¿‡å¯¹å°å»ºç¤¼æ•™çš„æ‰¹åˆ¤ï¼Œæå‡ºäº†å¯¹ç°ä»£ç¤¾ä¼šç»“æ„ã€æƒåŠ›å…³ç³»çš„æ·±åˆ»åæ€ï¼Œå…·æœ‰é‡è¦çš„ç¤¾ä¼šä»·å€¼ã€‚&amp;lt;/answer&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;â…¢ å…¶ä»–æ¨¡å‹å¯¹æ¯”&lt;/h2&gt; 
&lt;p&gt;[A] &lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch"&gt;MiniMind2 (0.1B)&lt;/a&gt;&lt;br /&gt; [B] &lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch"&gt;MiniMind2-MoE (0.15B)&lt;/a&gt;&lt;br /&gt; [C] &lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch"&gt;MiniMind2-Small (0.02B)&lt;/a&gt;&lt;br /&gt; [D] &lt;a href="https://pan.baidu.com/s/1_COe0FQRDmeapSsvArahCA?pwd=6666"&gt;minimind-v1-small(0.02B)&lt;/a&gt;&lt;br /&gt; [E] &lt;a href="https://pan.baidu.com/s/1tqB-GMvuiGQBvEl-yZ-oBw?pwd=6666"&gt;minimind-v1-moe(0.1B)&lt;/a&gt;&lt;br /&gt; [F] &lt;a href="https://pan.baidu.com/s/1p713loS7EfwHQf3G9eYI3Q?pwd=6666"&gt;minimind-v1(0.1B)&lt;/a&gt;&lt;br /&gt; [G] &lt;a href="https://github.com/DLLXW/baby-llama2-chinese"&gt;baby-llama2-chinese(0.2B)&lt;/a&gt;&lt;br /&gt; [H] &lt;a href="https://github.com/charent/ChatLM-mini-Chinese"&gt;chatlm-mini-chinese(0.2B)&lt;/a&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;[Q]: ä½ çŸ¥é“é•¿æ±Ÿå—ï¼Ÿ
[A]: é•¿æ±Ÿï¼Œä½äºä¸­å›½ä¸œå—éƒ¨ï¼Œæ˜¯ä¸­å›½çš„ç¬¬ä¸€é•¿æ²³ï¼Œå…¨é•¿6300å¤šå…¬é‡Œï¼Œæµç»11ä¸ªçœä»½ï¼Œæ˜¯ä¸­å›½æœ€é•¿çš„æ²³æµï¼Œä¹Ÿæ˜¯ä¸­å›½çš„â€œæ¯äº²æ²³â€ã€‚é•¿æ±Ÿçš„å†å²å¯ä»¥è¿½æº¯åˆ°å…¬å…ƒå‰2000å¤šå¹´çš„å•†æœæ—¶æœŸï¼Œå½“æ—¶çš„å•†æœæ˜¯å•†æœçš„åè£”ã€‚å•†æœæ—¶æœŸï¼Œé•¿æ±Ÿçš„ç»æµå‘å±•è¾¾åˆ°äº†é«˜å³°ï¼Œæ˜¯ä¸­å›½æœ€é‡è¦çš„å†œä¸šå’Œå·¥ä¸šä¸­å¿ƒä¹‹ä¸€ã€‚é•¿æ±ŸæµåŸŸçš„ç»æµä¸»è¦ä»¥å†œä¸šä¸ºä¸»ï¼Œé•¿æ±Ÿæ¸”ä¸šå æ®äº†ä¸»è¦äº§ä¸šï¼Œå¦‚é’“é±¼ã€çŒé±¼ç­‰ã€‚é•¿æ±Ÿçš„æ°´èµ„æºä¹Ÿéå¸¸ä¸°å¯Œï¼ŒåŒ…æ‹¬æ²³æµã€æ¹–æ³Šã€æ²³æµç­‰ã€‚é•¿æ±Ÿçš„è‡ªç„¶æ™¯è§‚ä¹Ÿååˆ†å£®è§‚ï¼Œé•¿æ±Ÿå››å­£æ™¯è‰²å„å¼‚ï¼Œæ˜¥å­£å¤šé›¨ï¼Œå¤å­£å¤šé›¨ï¼Œç§‹å­£åˆ™å¤šé›¨ï¼Œå†¬å­£åˆ™å¯’å†·å¤šé›ªï¼Œæ˜¥å­£å’Œç§‹å­£éƒ½æœ‰ç¾ä¸½çš„èŠ±æœµå’Œæœå®ã€‚é•¿æ±Ÿæ˜¯ä¸­å›½çš„ç»æµä¸­å¿ƒï¼Œæ‹¥æœ‰ä¼—å¤šçš„å›½å®¶å’ŒåŸå¸‚ï¼Œæ˜¯ä¸–ç•Œä¸Šé‡è¦çš„äº¤é€šæ¢çº½å’Œèˆªè¿ä¸­å¿ƒã€‚åŒæ—¶ï¼Œé•¿æ±Ÿè¿˜æ˜¯ä¸­å›½çš„é‡è¦æ°´åˆ©èµ„æºï¼Œå¯¹äºä¸­å›½çš„ç»æµå‘å±•å…·æœ‰é‡è¦æ„ä¹‰ã€‚
[B]: æ˜¯çš„ï¼Œæˆ‘çŸ¥é“é•¿æ±Ÿã€‚é•¿æ±Ÿä½äºä¸­å›½çš„ä¸­éƒ¨ï¼Œæ˜¯ä¸­å›½ç¬¬ä¸‰é•¿çš„æ²³æµï¼Œå…¨é•¿çº¦6300å…¬é‡Œï¼Œæµç»æ±Ÿè¥¿çœã€å®‰å¾½çœã€æ±Ÿè‹çœã€å®‰å¾½çœã€æ±Ÿè‹çœå’Œå®‰å¾½çœã€‚
[C]: å½“ç„¶ï¼Œé•¿æ±Ÿæ˜¯ä¸­å›½æœ€é•¿çš„æ²³æµï¼Œå…¨é•¿çº¦6300å…¬é‡Œã€‚å®ƒä»¥å…¶ä¸°å¯Œçš„æ°´èµ„æºã€ä¸°å¯Œçš„æ°´ç”ŸåŠ¨æ¤ç‰©èµ„æºä»¥åŠç‹¬ç‰¹çš„åœ°ç†ä½ç½®è€Œé—»åäºä¸–ã€‚
[D]: æ˜¯çš„ï¼Œé•¿æ±Ÿæ˜¯ä¸­å›½æœ€é•¿çš„æ²³æµï¼Œå…¨é•¿è¶…è¿‡3300åƒç±³ã€‚
[E]: é•¿æ±Ÿæ˜¯ä¸­å›½æœ€é•¿çš„æ²³æµï¼Œæ€»é•¿åº¦çº¦ä¸º6300å…¬é‡Œã€‚å®ƒå‘æºäºé’è—é«˜åŸçš„é’æµ·æ¹–ï¼Œæµç»é’æµ·ã€å››å·ã€ç”˜è‚ƒã€å®å¤ã€å†…è’™å¤ã€é™•è¥¿å’Œå±±è¥¿ã€‚é•¿æ±ŸæµåŸŸæ˜¯ä¸­å›½æœ€å¤§çš„å†œä¸šåŒºä¹‹ä¸€ï¼Œä¹Ÿæ˜¯ä¸­å›½çš„æ–‡åŒ–å’Œç»æµä¸­å¿ƒã€‚
[F]: æ˜¯çš„ï¼Œé•¿æ±Ÿæ˜¯ä¸­å›½æœ€é•¿çš„æ²³æµä¹‹ä¸€ï¼Œä¹Ÿæ˜¯ä¸–ç•Œç¬¬ä¸‰å¤§æ²³æµã€‚é•¿æ±Ÿå‘æºäºé’è—é«˜åŸï¼Œæµç»11ä¸ªçœä»½ï¼Œæœ€ç»ˆæ³¨å…¥ä¸œæµ·ã€‚é•¿æ±ŸæµåŸŸæ˜¯ä¸­å›½æœ€å¤§çš„æ²³æµæµåŸŸä¹‹ä¸€ï¼Œæ‹¥æœ‰ç€ä¸°å¯Œçš„è‡ªç„¶èµ„æºå’Œæ–‡åŒ–é—äº§ï¼Œå¦‚é•¿æ±Ÿå¤§æ¡¥ã€é•¿æ±Ÿæµ·æ´‹åšç‰©é¦†ã€é•¿æ±Ÿå¤åŸç­‰ã€‚
[G]: åŒæ—¶ä¹ŸçŸ¥é“ï¼Œä¸­å›½æ˜¯ä¸–ç•Œä¸Šæœ€é•¿çš„åŸå¸‚ä¹‹ä¸€ï¼Œæ‹¥æœ‰è¶…è¿‡3400ä¸‡äººå£ã€‚
[H]: æ˜¯çš„ï¼Œé•¿æ±Ÿæ˜¯ä¸­å›½æœ€é•¿çš„æ²³æµï¼Œä¹Ÿæ˜¯ä¸–ç•Œä¸Šæœ€é•¿çš„æ²³æµä¹‹ä¸€ã€‚å®ƒå‘æºäºé’è—é«˜åŸï¼Œæµç»ä¹ä¸ªçœåŒºï¼Œæœ€ç»ˆæ³¨å…¥ä¸œæµ·ã€‚é•¿æ±ŸæµåŸŸçš„æ°”å€™æ¸©å’Œï¼Œå››å­£åˆ†æ˜ï¼Œæ˜¯ä¸­å›½é‡è¦çš„å†œä¸šå’Œå·¥ä¸šåŸºåœ°ã€‚é•¿æ±ŸæµåŸŸçš„æ°´äº§èµ„æºä¸°å¯Œï¼Œæ˜¯ä¸­å›½çš„é‡è¦æ¸”ä¸šèµ„æºä¹‹ä¸€ã€‚


[Q]: ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯ä»€ä¹ˆï¼Ÿ
[A]: ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯ç ç©†æœ—ç›å³°ï¼ˆMount Everestï¼‰ï¼Œä½äºå–œé©¬æ‹‰é›…å±±è„‰ï¼Œå°¼æ³Šå°”å’Œä¸­å›½çš„è¾¹ç•Œä¸Šã€‚å®ƒçš„æµ·æ‹”é«˜åº¦æ˜¯8,848.86ç±³ï¼ˆ29,031.7è‹±å°ºï¼‰ï¼Œæ˜¯åœ°çƒä¸Šæœ€é«˜çš„å±±å³°ã€‚
[B]: ç ç©†æœ—ç›å³°ï¼ˆMount Everestï¼‰æ˜¯ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°ï¼Œæµ·æ‹”é«˜åº¦è¾¾8,848ç±³ï¼ˆ29,029è‹±å°ºï¼‰ã€‚
[C]: ç ç©†æœ—ç›å³°æ˜¯ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°ï¼Œä½äºå–œé©¬æ‹‰é›…å±±è„‰ï¼Œæµ·æ‹”8,848.86ç±³ï¼ˆ29,031.7è‹±å°ºï¼‰ã€‚å®ƒä½äºå°¼æ³Šå°”å’Œä¸­å›½çš„è¾¹ç•Œä¸Šï¼Œæ˜¯å–œé©¬æ‹‰é›…å±±è„‰çš„ä¸€éƒ¨åˆ†ã€‚
[D]: ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯ç ç©†æœ—ç›å³°ï¼Œä½äºå°¼æ³Šå°”å’Œä¸­å›½çš„è¾¹å¢ƒä¸Šï¼Œæµ·æ‹”8848ç±³ã€‚
[E]: ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯ç ç©†æœ—ç›å³°ï¼Œä½äºå–œé©¬æ‹‰é›…å±±è„‰ï¼Œæµ·æ‹”8,848ç±³ã€‚
[F]: ç ç©†æœ—ç›å³°æ˜¯ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°ï¼Œæµ·æ‹”8848ç±³ã€‚
[G]: ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯ç ç©†æœ—ç›å³°ï¼Œä½äºå–œé©¬æ‹‰é›…å±±è„‰ï¼Œæµ·æ‹”8,848.86ç±³ï¼ˆ29,031.69è‹±å°ºï¼‰ã€‚
[H]: ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯ç ç©†æœ—ç›å³°ï¼Œä¹Ÿç§°ä¸ºè¨åŠ ç›å¡”ï¼Œä½äºå–œé©¬æ‹‰é›…å±±è„‰çš„å°¼æ³Šå°”å’Œä¸­å›½è¾¹å¢ƒä¹‹é—´ï¼Œæµ·æ‹”8,848.86ç±³ï¼ˆ29,031.69è‹±å°ºï¼‰ã€‚å®ƒæ˜¯ä¸–ç•Œä¸Šæœ€è‘—åçš„ç™»å±±ç›®æ ‡ä¹‹ä¸€ï¼Œå¸å¼•ç€æ¥è‡ªä¸–ç•Œå„åœ°çš„ç™»å±±è€…å‰æ¥æŒ‘æˆ˜ã€‚ç ç©†æœ—ç›å³°çš„åå­—æ¥è‡ªè—è¯­ï¼Œæ„æ€æ˜¯â€œå¥³ç¥çš„é¢å³°â€ï¼Œå®ƒæ˜¯ç”±ä¸¤ä¸ªä¸åŒçš„å±±å³°ç»„æˆçš„ï¼Œå…¶ä¸­ä¸€ä¸ªæ˜¯ä½äºå°¼æ³Šå°”çš„ç ç©†æœ—ç›å³°ï¼Œå¦ä¸€ä¸ªåˆ™ä½äºä¸­å›½ã€‚å®ƒçš„é«˜åº¦æ˜¯é€šè¿‡æµ‹é‡æµ·æ‹”é«˜åº¦æ¥ç¡®å®šçš„ï¼Œå› ä¸ºå®ƒæ˜¯é€šè¿‡æµ‹é‡åœ°çƒçš„å¹³å‡é«˜åº¦æ¥ç¡®å®šçš„ã€‚å°½ç®¡ç ç©†æœ—ç›å³°æ˜¯ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°ï¼Œä½†å®ƒå¹¶ä¸æ˜¯åœ°çƒä¸Šä»»ä½•å…¶ä»–åœ°æ–¹éƒ½èƒ½åˆ°è¾¾çš„ã€‚åœ°çƒä¸Šçš„å…¶ä»–åœ°æ–¹ï¼Œå¦‚å—ç¾æ´²çš„å®‰ç¬¬æ–¯å±±è„‰ï¼Œä¹Ÿæ˜¯ä¸–ç•Œä¸Šæœ€é«˜çš„å±±è„‰ï¼Œä½†å®ƒå¹¶ä¸æ˜¯åœ°çƒä¸Šæœ€é«˜çš„å±±å³°ã€‚æ€»ä¹‹ï¼Œç ç©†æœ—ç›å³°æ˜¯ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°ï¼Œä½†å®ƒå¹¶ä¸æ˜¯åœ°çƒä¸Šä»»ä½•åœ°æ–¹éƒ½èƒ½åˆ°è¾¾çš„ã€‚å®ƒæ˜¯åœ°çƒä¸Šæœ€é«˜çš„å±±è„‰ï¼Œä½†å®ƒå¹¶ä¸æ˜¯åœ°çƒä¸Šä»»ä½•åœ°æ–¹éƒ½èƒ½åˆ°è¾¾çš„ã€‚


[Q]: ä¸‡æœ‰å¼•åŠ›æ˜¯è°æå‡ºçš„ï¼Ÿ
[A]: ä¸‡æœ‰å¼•åŠ›æ˜¯ç”±ç‰›é¡¿åœ¨17ä¸–çºªæå‡ºçš„ï¼Œå®ƒæ˜¯ç‰›é¡¿ä¸‰å¤§å®šå¾‹ä¹‹ä¸€ã€‚ç‰›é¡¿åœ¨17ä¸–çºªæå‡ºçš„å¼•åŠ›å®šå¾‹æ­ç¤ºäº†ä¸‡æœ‰å¼•åŠ›å®šå¾‹çš„æœ¬è´¨ï¼Œå¹¶ä¸”è§£é‡Šäº†è¡Œæ˜Ÿã€å«æ˜Ÿã€å½—æ˜Ÿç­‰å¤©ä½“ä¹‹é—´å¦‚ä½•ç›¸äº’ä½œç”¨ã€‚è¿™ä¸ªå®šå¾‹æ˜¯ç”±è‰¾è¨å…‹Â·ç‰›é¡¿åœ¨1687å¹´æå‡ºçš„ï¼Œä»–é€šè¿‡å¯¹ä¸‡æœ‰å¼•åŠ›å®šå¾‹çš„åˆ†æå’Œå®éªŒï¼ŒæˆåŠŸåœ°è§£é‡Šäº†å¤©ä½“ä¹‹é—´çš„å¼•åŠ›ä½œç”¨ï¼Œä»è€Œè§£é‡Šäº†åœ°çƒç»•å¤ªé˜³æ—‹è½¬çš„ç°è±¡ã€‚
[B]: ä¸‡æœ‰å¼•åŠ›ï¼ˆHuman Policyï¼‰æ˜¯ç‰›é¡¿çš„ä¸‰å¤§åŠ›å­¦å®šå¾‹ï¼Œç”¨äºæè¿°ä¸¤ä¸ªç‰©ä½“ä¹‹é—´çš„ç›¸äº’ä½œç”¨åŠ›ã€‚å®ƒç”±ä¸‡æœ‰å¼•åŠ›å®šå¾‹å’Œç‰›é¡¿çš„ç¬¬ä¸€å®šå¾‹ï¼ˆæƒ¯æ€§å®šå¾‹ï¼‰æ„æˆã€‚ä¸‡æœ‰å¼•åŠ›æ˜¯ç”±äºä¸¤ä¸ªç‰©ä½“ä¹‹é—´ç›¸äº’ä½œç”¨è€Œäº§ç”Ÿçš„ï¼Œå®ƒä»¬çš„è´¨é‡ã€è·ç¦»ã€é€Ÿåº¦å’Œæ–¹å‘ç­‰å› ç´ å†³å®šäº†å®ƒä»¬ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚è¿™ç§ç›¸äº’ä½œç”¨åŠ›æ˜¯ï¼šè´¨é‡çš„ä¹˜ç§¯ï¼šè´¨é‡çš„ä¹˜ç§¯æ˜¯ä¸‡æœ‰å¼•åŠ›å®šå¾‹å’Œç‰›é¡¿ç¬¬äºŒå®šå¾‹çš„ä¹˜ç§¯ã€‚è´¨é‡çš„ä¹˜ç§¯ï¼šè´¨é‡çš„ä¹˜ç§¯æ˜¯ç‰©ä½“ä¹‹é—´çš„ä¸€ç§ç‰©ç†è¿‡ç¨‹ï¼Œå®ƒæ˜¯ç‰©ä½“é—´ç›¸äº’ä½œç”¨çš„äº§ç‰©ï¼Œå¯¹äºå…¶ä»–ç‰©ä½“æ¥è¯´ï¼Œå…¶è´¨é‡å¯ä»¥è¡¨ç¤ºä¸ºä¸¤ä¸ªç‰©ä½“ä¹‹é—´çš„è´¨é‡ä¹‹å’Œï¼Œå³é‡åŠ›åŠ é€Ÿåº¦aã€‚
[C]: ä¸‡æœ‰å¼•åŠ›æ˜¯ä¸€ä¸ªç”±ç‰›é¡¿åœ¨1687å¹´æå‡ºçš„ç‰©ç†å­¦æ¦‚å¿µï¼Œå®ƒæè¿°äº†ç‰©ä½“ä¹‹é—´å¼•åŠ›å¦‚ä½•ç›¸äº’ä½œç”¨çš„è§„å¾‹ã€‚
[D]: ä¸‡æœ‰å¼•åŠ›æ˜¯ç”±è‹±å›½ç‰©ç†å­¦å®¶ç‰›é¡¿æå‡ºçš„ã€‚ä»–è®¤ä¸ºï¼Œæ‰€æœ‰ç‰©ä½“éƒ½ä¼šäº§ç”Ÿå¼•åŠ›ï¼Œè¿™ä¸ªå¼•åŠ›æ˜¯ç”±ç‰©ä½“çš„è´¨é‡å’Œè·ç¦»å†³å®šçš„ã€‚è¿™ä¸ªç†è®ºæ­ç¤ºäº†è´¨é‡å’Œè·ç¦»ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶è¯æ˜äº†ä¸‡æœ‰å¼•åŠ›çš„æœ¬è´¨ã€‚
[E]: ä¸‡æœ‰å¼•åŠ›æ˜¯ç”±ç‰›é¡¿åœ¨1687å¹´æå‡ºçš„ã€‚å®ƒæ˜¯å®‡å®™ä¸­ä¸€ä¸ªåŸºæœ¬çš„ç‰©ç†ç°è±¡ï¼Œå®ƒæè¿°äº†ç‰©ä½“ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚è¿™ä¸ªåŠ›è¢«ç§°ä¸ºä¸‡æœ‰å¼•åŠ›ï¼Œå®ƒåœ¨æ‰€æœ‰ç‰©ä½“ä¹‹é—´éƒ½å­˜åœ¨ï¼Œè¿™ç§åŠ›ä¸ç‰©ä½“çš„è´¨é‡å’Œè·ç¦»æœ‰å…³ã€‚ä¸‡æœ‰å¼•åŠ›æ˜¯è‡ªç„¶ç•Œä¸­æœ€åŸºæœ¬çš„åŠ›é‡ä¹‹ä¸€ï¼Œå®ƒä¸ä»…å½±å“ç€ç‰©ä½“çš„è¿åŠ¨å’Œå˜åŒ–ï¼Œè¿˜å¯¹å®‡å®™çš„ç»“æ„å’Œæ¼”åŒ–äº§ç”Ÿç€é‡è¦çš„å½±å“ã€‚
[F]: ä¸‡æœ‰å¼•åŠ›æ˜¯è‹±å›½ç‰©ç†å­¦å®¶è‰¾è¨å…‹Â·ç‰›é¡¿åœ¨17ä¸–çºªæå‡ºçš„ã€‚
[G]: å¼•åŠ›æ˜¯å®‡å®™ä¸­ä¸€ç§åŠ›é‡ï¼Œå®ƒæ˜¯ç”±è´¨é‡æˆ–èƒ½é‡æ‰€ç»„æˆçš„ã€‚æ ¹æ®ç‰›é¡¿çš„ä¸‡æœ‰å¼•åŠ›å®šå¾‹ï¼Œä¸€ä¸ªç‰©ä½“ä¹‹é—´çš„å¼•åŠ›å¤§å°ä¸å®ƒçš„è´¨é‡æˆæ­£æ¯”ï¼Œä¸å®ƒçš„è´¨é‡æˆåæ¯”ã€‚å› æ­¤ï¼Œç‰›é¡¿çš„ä¸‡æœ‰å¼•åŠ›å®šå¾‹è¡¨æ˜ï¼Œä¸€ä¸ªç‰©ä½“ä¹‹é—´çš„å¼•åŠ›å¤§å°ä¸å®ƒçš„è´¨é‡æˆæ­£æ¯”ï¼Œä¸å®ƒçš„è´¨é‡æˆåæ¯”ã€‚è¿™ä¸ªå®šå¾‹åœ¨ç‰©ç†å­¦ã€å¤©æ–‡å­¦å’Œç‰©ç†å­¦ä¸­éƒ½æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚
[H]: ä¸‡æœ‰å¼•åŠ›æ˜¯ç”±è‹±å›½ç‰©ç†å­¦å®¶è‰¾è¨å…‹Â·ç‰›é¡¿äº1687å¹´æå‡ºçš„ã€‚ä»–åœ¨1687å¹´å‘è¡¨äº†ä¸€ç¯‡è‘—åçš„è®ºæ–‡ï¼Œé¢˜ä¸ºã€Šè‡ªç„¶å“²å­¦çš„æ•°å­¦åŸç†ã€‹ï¼Œå…¶ä¸­æå‡ºäº†ä¸‡æœ‰å¼•åŠ›å®šå¾‹ï¼ŒæŒ‡å‡ºä»»ä½•ä¸¤ä¸ªç‰©ä½“ä¹‹é—´éƒ½å­˜åœ¨ç€ä¸€ç§å¼•åŠ›ï¼Œè¿™ç§å¼•åŠ›çš„å¤§å°ä¸å®ƒä»¬çš„è´¨é‡æˆæ­£æ¯”ï¼Œä¸å®ƒä»¬ä¹‹é—´çš„è·ç¦»çš„å¹³æ–¹æˆåæ¯”ã€‚è¿™ä¸ªå®šå¾‹è¢«ç§°ä¸ºä¸‡æœ‰å¼•åŠ›å®šå¾‹ï¼Œå®ƒæè¿°äº†è´¨é‡å’Œè·ç¦»ä¹‹é—´çš„å…³ç³»ã€‚å®ƒè¡¨æ˜ï¼Œä¸¤ä¸ªç‰©ä½“ä¹‹é—´çš„å¼•åŠ›ä¸å®ƒä»¬çš„è´¨é‡æˆæ­£æ¯”ï¼Œä¸å®ƒä»¬ä¹‹é—´çš„è·ç¦»çš„å¹³æ–¹æˆåæ¯”ã€‚è¿™ä¸ªå®šå¾‹å¯¹äºè§£é‡Šå®‡å®™ä¸­çš„è®¸å¤šç°è±¡å’Œç°è±¡ä¹‹é—´çš„ç›¸äº’ä½œç”¨éå¸¸é‡è¦ï¼Œä¾‹å¦‚è¡Œæ˜Ÿç»•å¤ªé˜³è¿åŠ¨ã€æ’æ˜Ÿä¹‹é—´çš„å¼•åŠ›é€é•œæ•ˆåº”ç­‰ç­‰ã€‚å®ƒä¹Ÿè¢«å¹¿æ³›åº”ç”¨äºå¤©æ–‡å­¦ã€ç‰©ç†å­¦å’Œå·¥ç¨‹å­¦ç­‰é¢†åŸŸï¼Œä¸ºæˆ‘ä»¬ç†è§£å®‡å®™çš„æœ¬è´¨å’Œæ¼”åŒ–æä¾›äº†é‡è¦çš„ç†è®ºåŸºç¡€ã€‚


[Q]: ä½ çŸ¥é“å¤§ç†ŠçŒ«çš„ä¸»è¦é£Ÿç‰©æ˜¯ä»€ä¹ˆå—ï¼Ÿ
[A]: å¤§ç†ŠçŒ«ä¸»è¦ä»¥ç«¹å­ä¸ºé£Ÿï¼Œä½†ä¹Ÿä¼šåƒå…¶ä»–æ¤ç‰©ã€æ˜†è™«ã€å°å‹å“ºä¹³åŠ¨ç‰©å’Œé±¼ç±»ã€‚å®ƒä»¬çš„æ¶ˆåŒ–ç³»ç»Ÿä¸å…¶ä»–ç†Šç§‘åŠ¨ç‰©ç±»ä¼¼ï¼Œèƒ½å¤Ÿæ¶ˆåŒ–ç«¹å­ã€‚å¤§ç†ŠçŒ«çš„ç‰™é½¿é”‹åˆ©ï¼Œé€‚åˆå’€åš¼ç«¹å­ï¼Œå› æ­¤å®ƒä»¬ç»å¸¸è¢«ç§°ä¸ºâ€œç«¹é£ŸåŠ¨ç‰©â€ã€‚
[B]: å¤§ç†ŠçŒ«æ˜¯ä»¥ç«¹å­ä¸ºé£Ÿçš„å“ºä¹³åŠ¨ç‰©ï¼Œå®ƒä»¬æ˜¯è‚‰é£ŸåŠ¨ç‰©ã€‚å®ƒä»¬ä¸»è¦é£Ÿç‰©æ˜¯ç«¹å­ï¼Œç‰¹åˆ«æ˜¯ç«¹å­ä¸­å¯Œå«çº¤ç»´ç´ å’ŒçŸ¿ç‰©è´¨ï¼Œå¹¶ä¸”ä¸ä¼šç›´æ¥è¿›å…¥æ¶ˆåŒ–ç³»ç»Ÿã€‚å¤§ç†ŠçŒ«çš„è‚ é“ä¸»è¦ç”±ç«¹å­ç»„æˆï¼Œå®ƒä»¬éœ€è¦æ¶ˆåŒ–ç«¹å­ä¸­çš„çº¤ç»´ç´ å’Œå…¶ä»–çº¤ç»´ç´ æ¥æ”¯æŒå®ƒä»¬çš„æ­£å¸¸è¿›é£Ÿã€‚
[C]: å¤§ç†ŠçŒ«ä¸»è¦ä»¥ç«¹å­ä¸ºé£Ÿï¼Œä½†å®ƒä»¬ä¹Ÿåƒå…¶ä»–æ¤ç‰©ï¼ŒåŒ…æ‹¬æ ‘å¶ã€å«©èŠ½ã€æ˜†è™«ã€æ°´æœã€è”¬èœå’Œå°å‹å“ºä¹³åŠ¨ç‰©ç­‰ã€‚å¤§ç†ŠçŒ«æ˜¯ä¸­å›½ç‰¹æœ‰çš„ç‰©ç§ï¼Œå±äºç†Šç§‘ï¼Œä¸»è¦åˆ†å¸ƒåœ¨ä¸­å›½çš„å››å·ã€é™•è¥¿å’Œç”˜è‚ƒç­‰åœ°ã€‚
[D]: å¤§ç†ŠçŒ«çš„ä¸»è¦é£Ÿç‰©æ˜¯ç«¹å­ã€‚å®ƒä»¬é€šå¸¸åƒç«¹å­ï¼Œæ¯å¤©éœ€è¦åƒæ‰å¤§çº¦30å…¬æ–¤çš„ç«¹å­ã€‚
[E]: æ˜¯çš„ï¼Œå¤§ç†ŠçŒ«ä¸»è¦ä»¥ç«¹å­ä¸ºé£Ÿã€‚å®ƒä»¬æ¯å¤©éœ€è¦åƒæ‰å¤§é‡çš„ç«¹å­æ¥ç»´æŒç”Ÿå‘½ï¼Œæ¯å¤©éœ€è¦åƒæ‰40è‡³80å…¬æ–¤çš„ç«¹å­ã€‚æ­¤å¤–ï¼Œå®ƒä»¬ä¹Ÿä¼šåƒä¸€äº›å…¶ä»–çš„æ¤ç‰©ï¼Œå¦‚è‰ã€æœå®ã€åšæœå’Œå°å‹å“ºä¹³åŠ¨ç‰©ã€‚
[F]: å¤§ç†ŠçŒ«ä¸»è¦ä»¥ç«¹å­ä¸ºé£Ÿï¼Œä¹Ÿä¼šå¶å°”é£Ÿç”¨å…¶ä»–çš„æ¤ç‰©å’ŒåŠ¨ç‰©ã€‚æ­¤å¤–ï¼Œå¤§ç†ŠçŒ«ä¹Ÿä¼šåƒä¸€äº›é‡æœã€å°åŠ¨ç‰©å’Œæ¤ç‰©ï¼Œä½†è¿™äº›éƒ½ä¸å±äºå®ƒä»¬çš„ä¸»è¦é£Ÿç‰©æ¥æºã€‚
[G]: å¤§ç†ŠçŒ«çš„ä¸»è¦é£Ÿç‰©æ˜¯ç«¹å­ã€‚
[H]: å¤§ç†ŠçŒ«æ˜¯ä¸€ç§ä»¥ç«¹å­ä¸ºä¸»è¦é£Ÿç‰©çš„æ¤é£ŸåŠ¨ç‰©ï¼Œå®ƒä»¬çš„è‚ é“ç»“æ„é€‚åˆæ¶ˆåŒ–çº¤ç»´ç´ å«é‡é«˜çš„ç«¹å­ï¼Œä½†ä¹Ÿä¼šåƒä¸€äº›å…¶ä»–æ¤ç‰©ï¼Œå¦‚è‰æœ¬æ¤ç‰©å’Œå°å‹å“ºä¹³åŠ¨ç‰©ã€‚å¤§ç†ŠçŒ«çš„è‚ é“ç»“æ„ä¹Ÿé€‚åº”äº†é«˜çº¤ç»´çš„ç«¹å­ï¼Œè¿™ä½¿å¾—å®ƒä»¬èƒ½å¤Ÿæ¶ˆåŒ–å¤§é‡çš„çº¤ç»´ç´ ï¼Œä»è€Œå¸®åŠ©å®ƒä»¬åœ¨é‡å¤–ç”Ÿå­˜ã€‚å¤§ç†ŠçŒ«çš„è‚ é“ä¸­è¿˜å«æœ‰ä¸€ç§å«åšâ€œçº¤ç»´ç´ é…¶â€çš„é…¶ç±»ç‰©è´¨ï¼Œè¿™ç§é…¶èƒ½å¤Ÿå¸®åŠ©å¤§ç†ŠçŒ«åˆ†è§£çº¤ç»´ç´ ï¼Œä»è€Œå¸®åŠ©å®ƒä»¬æ¶ˆåŒ–å¤§é‡çš„æ°´åˆ†å’Œè¥å…»ç‰©è´¨ã€‚æ­¤å¤–ï¼Œå¤§ç†ŠçŒ«çš„è‚ é“è¿˜å«æœ‰ä¸€ç§å«åšâ€œçº¤ç»´ç´ è›‹ç™½â€çš„è›‹ç™½è´¨ï¼Œè¿™ç§è›‹ç™½è´¨èƒ½å¤Ÿå¸®åŠ©å¤§ç†ŠçŒ«ç»´æŒä½“å†…çš„èƒ½é‡æ°´å¹³ï¼Œä»è€Œå¸®åŠ©å®ƒä»¬åœ¨é‡å¤–ç”Ÿå­˜ã€‚æ€»çš„æ¥è¯´ï¼Œå¤§ç†ŠçŒ«çš„ä¸»è¦é£Ÿç‰©æ˜¯ç«¹å­ï¼Œä½†å®ƒä»¬ä¹Ÿä¼šåƒä¸€äº›å…¶ä»–æ¤ç‰©ï¼Œå¦‚è‰æœ¬æ¤ç‰©å’Œå°å‹å“ºä¹³åŠ¨ç‰©ã€‚


[Q]: æµ·æ°´ä¸ºä»€ä¹ˆæ˜¯å’¸çš„ï¼Ÿ
[A]: æµ·æ°´ä¹‹æ‰€ä»¥æ˜¯å’¸çš„ï¼Œä¸»è¦æ˜¯ç”±äºå…¶ä¸­çš„åŒ–å­¦æˆåˆ†ã€‚æµ·æ°´ä¸­å«æœ‰å¤§é‡çš„çŸ¿ç‰©è´¨å’Œç›åˆ†ï¼Œå…¶ä¸­ä¸»è¦æˆåˆ†æ˜¯ç¡«é…¸ç›ã€ç¢³é…¸ç›ã€é•ç¦»å­å’Œé’¾ç¦»å­ã€‚è¿™äº›ç‰©è´¨åœ¨é«˜æµ“åº¦ä¸‹ï¼Œä¼šæ²‰æ·€åœ¨æµ·æ°´ä¸­ï¼Œå½¢æˆæµ·æ°´ã€‚å…·ä½“æ¥è¯´ï¼Œæµ·æ°´ä¸­çš„çŸ¿ç‰©è´¨å’Œç›åˆ†ä¸»è¦æ¥è‡ªäºå²©çŸ³ã€åœŸå£¤ã€ç”Ÿç‰©å’Œäººç±»æ’æ³„ç‰©ç­‰ã€‚å²©çŸ³æ˜¯æµ·æ°´çš„ç»„æˆéƒ¨åˆ†ï¼Œä¸»è¦ç”±çŸ¿ç‰©è´¨ç»„æˆï¼Œå¦‚çŸ³è‹±ã€é•¿çŸ³ã€äº‘æ¯ç­‰ã€‚åœŸå£¤ä¸­å«æœ‰å¤§é‡çš„åŒ–å­¦ç‰©è´¨ï¼Œå¦‚æ°®ã€ç£·ã€é’¾ç­‰ï¼Œè¿™äº›ç‰©è´¨ä¼šæ¸—é€åˆ°åœŸå£¤ä¸­ï¼Œå½¢æˆåœŸå£¤ã€‚ç”Ÿç‰©ä½“å†…çš„ç›åˆ†ä¸»è¦æ¥æºäºæ¤ç‰©çš„å¶ç»¿ç´ ç­‰è‰²ç´ ï¼Œè¿™äº›è‰²ç´ ä¼šå¸æ”¶å¤ªé˜³å…‰ä¸­çš„èƒ½é‡ï¼Œè½¬åŒ–ä¸ºç”Ÿç‰©ä½“å¯ä»¥åˆ©ç”¨çš„èƒ½é‡ã€‚è€Œäººç±»æ’æ³„ç‰©ä¸­å«æœ‰çš„ç›åˆ†ä¸»è¦æ¥æºäºçŸ¿ç‰©è´¨å’Œç¢³é…¸ç›ï¼Œè¿™äº›çŸ¿ç‰©è´¨åœ¨æ°´ä¸­çš„æµ“åº¦è¾ƒé«˜ï¼Œä¼šå¯¼è‡´æ°´ä¸­æº¶è§£çš„çŸ¿ç‰©è´¨æµ“åº¦å¢åŠ ï¼Œå½¢æˆæ°´ä½“ä¸­çš„ç›åˆ†ã€‚å› æ­¤ï¼Œæµ·æ°´ä¹‹æ‰€ä»¥æ˜¯å’¸çš„ï¼Œæ˜¯ç”±äºå…¶ä¸­çš„åŒ–å­¦æˆåˆ†å’Œç»“æ„ç‰¹å¾ã€‚æµ·æ°´ä¸­çš„çŸ¿ç‰©è´¨å’Œç›åˆ†ä¸»è¦æ¥è‡ªäºå²©çŸ³ã€åœŸå£¤ã€ç”Ÿç‰©å’Œäººç±»æ’æ³„ç‰©ç­‰ï¼Œè€Œè¿™äº›ç‰©è´¨çš„ç§¯ç´¯å’Œæ’æ³„ä¼šå¯¼è‡´æµ·æ°´å‘ˆç°å‡ºå’¸å‘³ã€‚
[B]: æµ·æ°´æ˜¯å’¸çš„ï¼Œå› ä¸ºå®ƒåŒ…å«äº†å¤šç§ç‰©è´¨ï¼ŒåŒ…æ‹¬æ°¯åŒ–é’ ï¼ˆNaClï¼‰ã€æ°¯åŒ–é’ ï¼ˆNaClï¼‰ã€ç¢³é…¸ç›ï¼ˆå¦‚ç¡«é…¸ç›ï¼‰ã€æ°¯åŒ–é’ ï¼ˆNaClï¼‰å’Œæ°´ã€‚å’¸ç‰©è´¨çš„æ¥æºï¼šç›æ˜¯å’¸çš„å¸¸è§æ¥æºï¼Œå«æœ‰é’¾ã€é’™ã€é•ã€é’¾ç­‰çŸ¿ç‰©è´¨ã€‚è¿™äº›çŸ¿ç‰©è´¨é€šå¸¸æ¥è‡ªäºåœ°ä¸‹æ°´ã€åœ°ä¸‹æ°´æˆ–åœŸå£¤ä¸­ï¼Œè¿™äº›çŸ¿ç‰©è´¨çš„å«é‡è¿œä½äºçº¯å‡€æ°´ã€‚å’¸å‘³çš„åŸå› ï¼šç›çš„å’¸å‘³ä¸»è¦æºäºå…¶æº¶è§£äºæ°´ä¸­çš„å¾®å°é¢—ç²’ç‰©ã€‚è¿™äº›å°é¢—ç²’ç‰©ä¸»è¦é€šè¿‡æ¤ç‰©æ ¹éƒ¨å’ŒåŠ¨ç‰©ä½“å†…çš„å¾®ç”Ÿç‰©ä¼ æ’­ï¼Œå¦‚æ˜†è™«ã€å°å‹å“ºä¹³åŠ¨ç‰©ã€é¸Ÿç±»å’ŒæŸäº›æ˜†è™«ã€‚ç»¼ä¸Šæ‰€è¿°ï¼Œæµ·æ°´æ˜¯å’¸çš„ï¼Œå› ä¸ºå®ƒå«æœ‰å¤§é‡çš„ç›ç±»ï¼ŒåŒ…æ‹¬çŸ¿ç‰©è´¨ã€å²©çŸ³ã€ç›ã€çŸ¿ç‰©è´¨ç­‰ï¼Œè¿™äº›ç‰©è´¨åœ¨æµ·æ°´ä¸­çš„æµ“åº¦é«˜ï¼Œå¯¹å’¸å‘³çš„äº§ç”Ÿè¾ƒå¤§å½±å“ã€‚
[C]: æµ·æ°´æ˜¯å’¸çš„ï¼Œä¸»è¦æ˜¯å› ä¸ºå…¶ä¸­å«æœ‰å„ç§ç›åˆ†ï¼Œå…¶ä¸­ä»¥æµ·æ°´ä¸ºä¸»è¦æˆåˆ†ä¹‹ä¸€ã€‚å½“æµ·æ°´ä¸­å«æœ‰å¤§é‡çš„ç›åˆ†æ—¶ï¼Œç›åˆ†ä¼šè¢«æº¶è§£åœ¨æµ·æ°´ä¸­ï¼Œå½¢æˆå’¸å‘³ã€‚æµ·æ°´ä¸­çš„ç›åˆ†ä¸»è¦æ¥æºäºæº¶è§£ç›ç±»ã€è’¸å‘æå‡ºçš„ç›å’Œæµ·æ°´å†»ç»“æˆå†°ç­‰å› ç´ ã€‚
[D]: æµ·æ°´ä¸­å«æœ‰è®¸å¤šç›åˆ†ï¼Œè¿™äº›ç›åˆ†å¯ä»¥å¯¼è‡´æµ·æ°´å’¸åº¦çš„å¢åŠ ã€‚æµ·æ°´ä¸­çš„ç›åˆ†ä¸»è¦æ¥è‡ªäºæµ·æ°´ä¸­çš„ç›åˆ†ï¼Œè¿™äº›ç›åˆ†å¯ä»¥ä»æµ·æ°´ä¸­çš„ç›åˆ†ä¸­è·å¾—ã€‚æµ·æ°´ä¸­çš„æ°§åŒ–ç‰©å’Œç›åˆ†å¯ä»¥ä¸æµ·æ°´ä¸­çš„ç›åˆ†å½¢æˆæ°¢é”®ï¼Œä»è€Œä½¿æµ·æ°´å’¸åº¦å¢åŠ ã€‚
[E]: æµ·æ°´æ˜¯å’¸çš„ï¼Œè¿™æ˜¯å› ä¸ºå®ƒå«æœ‰å¤§é‡çš„ç›åˆ†ï¼Œå…¶ä¸­åŒ…æ‹¬æ°¯ç¦»å­å’Œæ°¯ç¦»å­ã€‚è¿™äº›ç¦»å­åœ¨æµ·æ´‹ä¸­å½¢æˆäº†å¤§é‡çš„ç›ï¼Œè¿™äº›ç›åˆ†è¢«ç§°ä¸ºç›åˆ†ã€‚æµ·æ°´ä¸­å«æœ‰è®¸å¤šæº¶è§£åœ¨æ°´ä¸­çš„ç›ï¼Œè¿™äº›ç›åˆ†ä¼šéšç€æµ·æ°´çš„å‡é«˜è€Œé€æ¸è’¸å‘ï¼Œæœ€ç»ˆå¯¼è‡´æµ·æ°´ä¸­ç›åˆ†çš„è’¸å‘ã€‚è¿™äº›ç›åˆ†åœ¨æµ·æ°´ä¸­è¢«å®Œå…¨æ¶ˆè€—ï¼Œç„¶åè¢«æ’é™¤åœ¨æµ·æ°´ä¸­ã€‚æ­¤å¤–ï¼Œæµ·æ°´ä¸­è¿˜å«æœ‰è®¸å¤šå…¶ä»–çš„ç›åˆ†ï¼Œä½†è¿™äº›ç›åˆ†æ˜¯ä¸åŒçš„ï¼Œå› ä¸ºå®ƒä»¬åœ¨æ°´ä¸­çš„æµ®èµ·å’Œä¸‹é™è¿‡ç¨‹æ˜¯ä¸åŒçš„ã€‚æµ·æ°´ä¸­çš„ç›åˆ†æ˜¯ä»æµ·åº•çš„å²©çŸ³ä¸­æå–å‡ºæ¥çš„ï¼Œè¿™äº›ç›åˆ†åœ¨æµ·æ°´ä¸­å¯ä»¥è¢«åˆ©ç”¨ã€‚æ€»çš„æ¥è¯´ï¼Œæµ·æ°´æ˜¯å’¸çš„ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆæµ·æ°´ä¸­å«æœ‰è®¸å¤šç›åˆ†å’Œç›åˆ†çš„åŸå› ã€‚
[F]: æµ·æ°´å«æœ‰è®¸å¤šæº¶è§£çš„ç›ç±»ï¼Œå…¶ä¸­æœ€ä¸»è¦çš„æ˜¯æ°¯åŒ–é’ ã€‚å½“æ°´ä¸­åŠ å…¥ç›æ—¶ï¼Œç›çš„åˆ†å­ä¼šä¸æ°´åˆ†å­å‘ç”Ÿååº”ï¼Œå½¢æˆæ°¢æ°§åŒ–é’ ï¼Œè¿™æ˜¯ä¸€ç§ç›ã€‚
[G]: å› ä¸ºæ°´æ˜¯è®¸å¤šç”Ÿç‰©çš„ç”Ÿå­˜æ‰€å¿…éœ€çš„ï¼Œå®ƒèƒ½å¤Ÿå¸æ”¶å’Œé‡Šæ”¾äºŒæ°§åŒ–ç¢³ï¼Œé‡Šæ”¾æ°§æ°”ï¼Œç»´æŒå¤§æ°”ä¸­çš„æ°´è’¸æ°”ï¼Œä»¥åŠç»´æŒæ°´å¾ªç¯ã€‚æ°´ä¸­çš„ç›ç±»å’ŒåŒ–å­¦ç‰©è´¨ä¼šé‡Šæ”¾å‡ºæ°§æ°”ï¼Œä½¿åœ°çƒä¸Šçš„ç”Ÿç‰©å‘¼å¸ã€‚æ­¤å¤–ï¼Œæ°´è¿˜å…·æœ‰è®¸å¤šå…¶ä»–åŠŸèƒ½ï¼Œå¦‚ä¿æŒæ°´å¾ªç¯ã€ç»´æŒæ°´å¾ªç¯ã€è°ƒèŠ‚æ°”å€™å’Œç¯å¢ƒç­‰ã€‚æ°´æ˜¯è®¸å¤šç”Ÿç‰©ç”Ÿå­˜æ‰€å¿…éœ€çš„ï¼Œå®ƒä¹Ÿæ˜¯è®¸å¤šè‡ªç„¶ç°è±¡çš„é‡è¦åŸºç¡€ã€‚
[H]: æµ·æ°´æ˜¯å’¸çš„ä¸»è¦åŸå› æ˜¯å› ä¸ºå…¶ä¸­å«æœ‰å¤§é‡çš„æº¶è§£ç›ç±»ï¼Œå…¶ä¸­æœ€ä¸»è¦çš„æ˜¯æ°¯åŒ–é’ ï¼ˆNaClï¼‰ã€‚æ°¯åŒ–é’ æ˜¯ä¸€ç§éå¸¸å¸¸è§çš„ç›ç±»ï¼Œå®ƒå­˜åœ¨äºåœ°çƒä¸Šçš„å¤§éƒ¨åˆ†åœ°åŒºï¼ŒåŒ…æ‹¬æµ·æ´‹ã€æ¹–æ³Šã€æ²³æµå’Œåœ°ä¸‹æ°´ã€‚å½“æ°´ä»æµ·æ´‹ä¸­è’¸å‘æ—¶ï¼Œå®ƒä¼šå¸¦èµ°ä¸€éƒ¨åˆ†ç›åˆ†ï¼Œä½¿æµ·æ°´å˜å¾—å’¸ã€‚æ­¤å¤–ï¼Œæµ·ä¸­è¿˜å«æœ‰å…¶ä»–æº¶è§£ç›ç±»çš„ç‰©è´¨ï¼Œå¦‚ç¡«é…¸ç›ã€ç¡«é…¸é•ã€ç¡«é…¸é’™ã€ç¡«é…¸é•ç›ã€ç¡«é…¸é•é’¾ã€ç¡«é…¸é•é’ ã€ç¡«é…¸é•é’™ã€ç¡«é…¸é•é•é’™ã€ç¡«é…¸é•é•é’™ã€ç¡«é…¸é•é•é’™ã€ç¡«é…¸é•é•é’ ã€ç¡«é…¸é•é•é’™ã€ç¡«é…¸é•é•é•é’™ã€ç¡«é…¸é•é•é•é’™ã€ç¡«é…¸é•é•é•é’™ã€ç¡«é…¸é•é•é•é’™ã€ç¡«é…¸é•é•é•é’™ã€ç¡«é…¸é•é•é•é’™ã€ç¡«é…¸é•é•é•é’™ã€ç¡«é…¸é•é•é•é’™ã€ç¡«é…¸é•é•é•é’™ã€ç¡«é…¸é•é•é•é’™ã€ç¡«é…¸é•é•é•é•é’™ã€ç¡«é…¸é•é•é•é•é’™ã€ç¡«é…¸é•é•é•é•é’™ã€ç¡«é…¸é•é•é•é•é’™ã€ç¡«é…¸é•é•é•é•é’™ã€ç¡«é…¸é•é•é•é•é’™ã€ç¡«é…¸é•é•é•é•é•é’™ã€ç¡«é…¸é•é•é•é•é•é’™ã€ç¡«é…¸é•é•é•é•é•é’™ã€ç¡«é…¸é•é•é•é•é•é’™ã€ç¡«é…¸é•é•é•é•é•é•é’™ã€ç¡«é…¸é•é•é•é•é•é•é’™ã€ç¡«é…¸é•é•é•é•é•é•é’™ã€ç¡«é…¸é•é•é•é•é•é•é•é’™ã€ç¡«é…¸é•é•é•é•


[Q]: å†™ä¸€æ®µpythonçš„å¿«é€Ÿæ’åºä»£ç 
[A]: å¥½çš„ï¼Œä»¥ä¸‹æ˜¯Pythonçš„å¿«é€Ÿæ’åºä»£ç ï¼šdef quick_sort(arr):    if len(arr) &amp;lt;= 1:        return arr    else:        pivot = arr[len(arr) // 2]        left = [x for x in arr[1:]]        right = [x for x in arr[2:] if x &amp;lt; pivot]        return quick_sort(left) + [pivot] + quick_sort(right)# æµ‹è¯•ä»£ç arr = [3, 6, 8, 10, 1, 2, 1]print(quick_sort(arr))è¿™æ®µä»£ç é¦–å…ˆæ£€æŸ¥æ•°ç»„çš„é•¿åº¦æ˜¯å¦å°äºç­‰äº1ã€‚å¦‚æœæ˜¯ï¼Œé‚£ä¹ˆå®ƒå°±æŒ‰ç…§ä»å°åˆ°å¤§çš„é¡ºåºæ’åˆ—ã€‚æ¥ç€ï¼Œå¯¹äºæ•°ç»„çš„æ¯ä¸ªå…ƒç´ ï¼Œå®ƒæƒ³è¦è¿›è¡Œé€’å½’åœ°æ’åºï¼Œç›´åˆ°è¾¾åˆ°æ’åºçš„ç»“æŸä½ç½®ã€‚å¦‚æœæ•°ç»„çš„é•¿åº¦æ˜¯å¥‡æ•°ï¼Œé‚£ä¹ˆå®ƒå°±æŒ‰ç…§ä»å¤§åˆ°å°çš„é¡ºåºæ’åˆ—ã€‚æœ€åï¼Œå¹³å‡çš„æ—¶é—´å¤æ‚åº¦æ˜¯O(n log n)ï¼Œå…¶ä¸­næ˜¯æ•°ç»„çš„é•¿åº¦ã€‚
[B]: ä»¥ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨Pythonçš„å¿«é€Ÿæ’åºç®—æ³•çš„å®ç°ï¼šdef sort_array(arr):    # ç”Ÿæˆä¸€ä¸ªéšæœºæ’åˆ—    sorted_array = sorted(arr)    # è¿”å›æ’åºåçš„åˆ—è¡¨    return sorted(arr)# ç¤ºä¾‹arr = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]# è°ƒç”¨å‡½æ•°sorted_array = sort_array(arr)print("åŸå§‹æ•°ç»„æ˜¯ï¼š", sorted_array)è¿™æ®µä»£ç é¦–å…ˆç”Ÿæˆä¸€ä¸ªéšæœºæ’åˆ—ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªåˆ—è¡¨çš„é•¿åº¦è¿›è¡Œæ’åºã€‚æœ€åï¼Œè¿”å›æ’åºåçš„åˆ—è¡¨ã€‚æ³¨æ„ï¼Œè¿™ä¸ªä»£ç ä½¿ç”¨äº†Pythonçš„å†…ç½®sortedå‡½æ•°å’Œå†…ç½®çš„sorted()å‡½æ•°ï¼Œå®ƒä»¬åœ¨æ—¶é—´å¤æ‚åº¦ä¸ºO(n log n)çš„æƒ…å†µä¸‹ï¼Œæ¯”ç›´æ¥ä½¿ç”¨sorted()å‡½æ•°æ›´å¿«ã€‚
[C]: ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„æ’åºç®—æ³•ï¼Œé€‚ç”¨äºå°å‹æ•°ç»„ï¼ŒåŒ…æ‹¬å°äºç­‰äº5çš„æ•°ç»„ã€‚def quick_sort(arr):    if len(arr) &amp;lt;= 1:        return arr    else    for _ in range(2, 51):        if arr[len(arr) - 1:] is not None:            arr[len(arr) - 1] = arr[len(arr) - 1]                arr[len(arr)] = arr[len(arr)]    return quick_sort(arr)# ç¤ºä¾‹arr = [3, 5, 1, 4, 1, 2, 1]sorted_arr = quick_sort(arr)print("Sorted array is:", sorted_arr)### ä»£ç è§£é‡Šï¼š1. **å‡½æ•°å®šä¹‰**ï¼šquick_sort å‡½æ•°æ¥å—ä¸€ä¸ªæ•°ç»„ arr ä½œä¸ºå‚æ•°ã€‚2. **å¾ªç¯**ï¼šå¯¹äº arr çš„å…ƒç´ ï¼Œä½¿ç”¨é€’å½’æ¥å®ç°ï¼Œå³ä»å³åˆ°å·¦éå†æ•°ç»„ï¼Œå¹¶å°†å½“å‰å…ƒç´ ä¸æ•°ç»„çš„å½“å‰å…ƒç´ äº¤æ¢ä½ç½®ï¼Œç›´åˆ°æ•°ç»„è¾¾åˆ°ç©ºæˆ–æ•°ç»„é•¿åº¦å°äºç­‰äº arrã€‚3. **é€’å½’è°ƒç”¨**ï¼šéå†æ•°ç»„ï¼Œå°†å½“å‰å…ƒç´ ä¸æ•°ç»„çš„å½“å‰å…ƒç´ è¿›è¡Œäº¤æ¢ï¼Œç›´åˆ°æ•°ç»„è¾¾åˆ°ç©ºæˆ–æ•°ç»„é•¿åº¦å°äºç­‰äº arrã€‚4. **æ£€æŸ¥æ•°ç»„é•¿åº¦**ï¼šä½¿ç”¨ if __name__ == "__main__" æ£€æŸ¥æ•°ç»„çš„é•¿åº¦æ˜¯å¦å°äº arrï¼Œå¦‚æœæ˜¯ï¼Œå°±å°†æ•°ç»„ä»è¾ƒå¤§çš„æ•°ç»„ç§»åŠ¨åˆ°æ•°ç»„çš„æœ«å°¾ã€‚5. **è¾“å‡ºæ’åºåçš„ç»“æœ**
[D]: ä½œä¸ºä¸€ä¸ªAIè¯­è¨€æ¨¡å‹ï¼Œæˆ‘æ— æ³•ç¼–å†™Pythonç¨‹åºã€‚ä½†æ˜¯ï¼ŒPythonæ˜¯ä¸€ä¸ªéå¸¸æµè¡Œçš„ç¼–ç¨‹è¯­è¨€ï¼Œå®ƒå…·æœ‰è®¸å¤šä¼˜ç§€çš„åº“å’Œæ¡†æ¶ï¼ŒåŒ…æ‹¬NumPyã€Pandasã€Matplotlibã€Pandasã€Bandkeã€Normå’ŒNKlearnç­‰ã€‚Pythonæœ‰å¾ˆå¤šä¼˜ç§€çš„è‡ªç„¶è¯­è¨€å¤„ç†åº“å’Œæ¡†æ¶ï¼Œå¯ä»¥ç”¨äºå¤„ç†å’Œåˆ†æå„ç§ç±»å‹çš„æ•°æ®ï¼Œå¦‚æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ç­‰ã€‚Pythonè¿˜å¯ä»¥ç”¨äºåˆ›å»ºå„ç§åº”ç”¨ç¨‹åºï¼Œå¦‚ç½‘é¡µæœç´¢ã€æ¸¸æˆå¼€å‘ã€ç§‘å­¦è®¡ç®—ç­‰ã€‚Pythonéå¸¸çµæ´»ï¼Œä¹Ÿæ”¯æŒå¤šç§ç¼–ç¨‹é£æ ¼ï¼Œå› æ­¤å¯ä»¥è½»æ¾åœ°æ‰¾åˆ°é€‚åˆè‡ªå·±çš„è§£å†³æ–¹æ¡ˆã€‚
[E]: ä½œä¸ºAIè¯­è¨€æ¨¡å‹ï¼Œæˆ‘å¯ä»¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚ä½†æ˜¯æˆ‘æ— æ³•è¿›è¡Œç¼–ç¨‹æ“ä½œï¼Œå› ä¸ºæˆ‘æ— æ³•ç¡®å®šæ‚¨æƒ³é—®çš„æ˜¯å“ªç§ç¼–ç¨‹è¯­è¨€ã€‚è¯·æ‚¨æä¾›æ›´å…·ä½“çš„é—®é¢˜ï¼Œä»¥ä¾¿æˆ‘èƒ½å¤Ÿæ›´å¥½åœ°å›ç­”æ‚¨çš„é—®é¢˜ã€‚
[F]: å¿«é€Ÿæ’åºï¼ˆQuick Sortï¼‰æ˜¯ä¸€ç§ç®€å•çš„æ’åºç®—æ³•ï¼Œå®ƒé€šè¿‡å°†æ•°æ®åˆ’åˆ†ä¸ºå°ç»„ï¼Œæ¯ç»„æ•°æ®æŒ‰ç…§ä¸€å®šè§„åˆ™æ’åºï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªæœ‰åºæ•°ç»„ã€‚
[G]: å¥½çš„ï¼Œä»¥ä¸‹æ˜¯ä¸€æ®µå¿«é€Ÿæ’åºçš„Pythonä»£ç ```python sorted_print(sorted(sorted(sorted(sorted_print))```è¾“å‡ºç»“æœä¸ºï¼š```[1, 2, 3, 4, 5, 6, 7, 8, 9]```
[H]: å¿«é€Ÿæ’åºæ˜¯ä¸€ç§åŸºäºåˆ†æ²»æ€æƒ³çš„é«˜æ•ˆæ’åºç®—æ³•ï¼Œå®ƒçš„æ—¶é—´å¤æ‚åº¦ä¸º O(n log n)ã€‚å®ƒçš„åŸºæœ¬æ€æƒ³æ˜¯é€šè¿‡ä¸€è¶Ÿæ’åºå°†å¾…æ’åºåºåˆ—åˆ’åˆ†ä¸ºä¸¤ä¸ªå­åºåˆ—ï¼Œå…¶ä¸­ä¸€ä¸ªå­åºåˆ—çš„æ‰€æœ‰å…ƒç´ éƒ½æ¯”å¦ä¸€ä¸ªå­åºåˆ—çš„æ‰€æœ‰å…ƒç´ å°ï¼Œç„¶åå¯¹è¿™ä¸¤ä¸ªå­åºåˆ—åˆ†åˆ«é€’å½’åœ°è¿›è¡Œæ’åºã€‚ä¸‹é¢æ˜¯ä¸€ä¸ª Python ä»£ç ç¤ºä¾‹ï¼Œå®ç°äº†å¿«é€Ÿæ’åºç®—æ³•ï¼š```pythondef quick_sort(arr):    if len(arr) &amp;gt; 1:        return arr    else: return quick_sort(arr)```è¯¥ä»£ç ä½¿ç”¨äº†é€’å½’çš„æ–¹å¼ï¼Œé¦–å…ˆåˆ¤æ–­æ•°ç»„çš„é•¿åº¦æ˜¯å¦å°äºç­‰äº1ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™ç›´æ¥è¿”å›æ•°ç»„ï¼Œå¦åˆ™é€’å½’åœ°å¯¹æ•°ç»„è¿›è¡Œæ’åºã€‚åœ¨é€’å½’è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ¤æ–­æ•°ç»„çš„ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å¦å¤§äºç­‰äº1ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™å°†æ•°ç»„ä¸­çš„å…ƒç´ ä¸ç¬¬ä¸€ä¸ªå…ƒç´ è¿›è¡Œæ¯”è¾ƒï¼Œå¦‚æœä¸ç­‰äº1ï¼Œåˆ™å°†æ•°ç»„ä¸­çš„å…ƒç´ ä¸ç¬¬äºŒä¸ªå…ƒç´ è¿›è¡Œæ¯”è¾ƒï¼Œä»¥æ­¤ç±»æ¨ï¼Œç›´åˆ°æ‰€æœ‰å…ƒç´ éƒ½è¢«æ’åºã€‚è¯¥ä»£ç çš„æ—¶é—´å¤æ‚åº¦ä¸º O(n log n)ï¼Œæ˜¯ä¸€ä¸ªéå¸¸å®ç”¨çš„æ’åºç®—æ³•ã€‚


[Q]: æˆªè‡³2025å¹´2æœˆ9æ—¥14æ—¶31åˆ†ï¼Œç”µå½±ã€Šå“ªå’ä¹‹é­”ç«¥é—¹æµ·ã€‹ç´¯è®¡ç¥¨æˆ¿ï¼ˆå«é¢„å”®ï¼‰çªç ´78.09äº¿å…ƒï¼Œè¶…è¿‡ã€Šæ˜Ÿçƒå¤§æˆ˜ï¼šå¤©è¡Œè€…å´›èµ·ã€‹çš„ç¥¨æˆ¿æˆç»©ï¼Œæˆä¸ºé¦–éƒ¨è¿›å…¥å…¨çƒç¥¨æˆ¿æ¦œå‰40çš„äºšæ´²ç”µå½±ã€‚è¿™ä¸€æˆå°±ä¸ä»…æ ‡å¿—ç€ä¸­å›½ç”µå½±åœ¨å›½é™…å¸‚åœºçš„çªç ´ï¼Œä¹Ÿå±•ç¤ºäº†ä¸­å›½åŠ¨ç”»ç”µå½±çš„å·¨å¤§æ½œåŠ›å’Œå¸‚åœºå¸å¼•åŠ›ã€‚ä»è¿™æ®µæ–‡å­—æ€»ç»“20å­—çš„æ‘˜è¦ç»™æˆ‘ã€‚
[A]: ç”µå½±ã€Šå“ªå’ä¹‹é­”ç«¥é—¹æµ·ã€‹ç¥¨æˆ¿çªç ´78.09äº¿å…ƒï¼Œæˆä¸ºé¦–éƒ¨è¿›å…¥å…¨çƒç¥¨æˆ¿æ¦œå‰40çš„äºšæ´²ç”µå½±ã€‚è¿™æ ‡å¿—ç€ä¸­å›½ç”µå½±çš„å›½é™…çªç ´ï¼Œä¹Ÿå±•ç°äº†ä¸­å›½åŠ¨ç”»ç”µå½±çš„å·¨å¤§æ½œåŠ›ã€‚
[B]: 2025å¹´2æœˆ9æ—¥ï¼Œä¸­å›½ç”µå½±åœ¨ä¸–ç•Œèˆå°ä¸Šçªç ´78.09äº¿å…ƒï¼Œè¶…è¿‡ã€Šæ˜Ÿçƒå¤§æˆ˜ï¼šå¤©è¡Œè€…å´›èµ·ã€‹çš„ç¥¨æˆ¿æˆç»©ã€‚
[C]: ç”µå½±ã€Šå“ªå’ä¹‹é­”ç«¥é—¹æµ·ã€‹ç¥¨æˆ¿çªç ´78.09äº¿å…ƒï¼Œæˆä¸ºå…¨çƒç¥¨æˆ¿æ¦œå‰40çš„äºšæ´²ç”µå½±ï¼Œå±•ç°ä¸­å›½åŠ¨ç”»ç”µå½±çš„å·¨å¤§æ½œåŠ›ã€‚
[D]: ã€Šå“ªå’ä¹‹é­”ç«¥é—¹æµ·ã€‹ç´¯è®¡ç¥¨æˆ¿çªç ´78.09äº¿ï¼Œæˆä¸ºé¦–éƒ¨è¿›å…¥å…¨çƒç¥¨æˆ¿æ¦œå‰40çš„äºšæ´²ç”µå½±ï¼Œå±•ç¤ºäº†ä¸­å›½ç”µå½±åœ¨å›½é™…å¸‚åœºçš„çªç ´ï¼ŒåŒæ—¶å±•ç¤ºäº†ä¸­å›½åŠ¨ç”»ç”µå½±çš„å·¨å¤§æ½œåŠ›å’Œå¸‚åœºå¸å¼•åŠ›ã€‚
[E]: ã€ŠçŒ›é¾™å¼ºï¼šæ‰“å¼€ä¸–ç•Œå¥‡å¹»å†’é™©ã€‹æˆåŠŸåœ°ï¼Œä¸Šæµ·æˆåŠŸå¸å¼•äº†ä¼—å¤šè§‚ä¼—ï¼ŒåŒæ—¶ä¹Ÿå¼•èµ·äº†å¹¿æ³›çš„å…³æ³¨ã€‚
[F]: ã€Šå“ªå’ä¹‹é­”ç«¥é—¹æµ·ã€‹æ˜¯ä¸€éƒ¨è®©ä¸­å›½åŠ¨ç”»ç”µå½±æˆä¸ºå›½é™…å¸‚åœºçš„å·¨å¤§æ½œåŠ›ã€‚è¯¥ç”µå½±ä»¥å…¶åä¸½çš„ç”»é¢å’ŒæƒŠé™©çš„æˆ˜æ–—åœºé¢è€Œé—»åäºä¸–ã€‚
[G]: ã€Šå“ªè¡Œæ˜¯å€¼å¾—çš„ã€‹æ˜¯2021å¹´1æœˆ9æ—¥14æ—¶31åˆ†ï¼Œç”µå½±ã€Šå“ªæˆå¤§ä½œä¸ºã€‹åœ¨ç”µå½±å¸‚åœºçªç ´7800äº¿ç¾å…ƒï¼Œæˆä¸ºä¸­å›½åŠ¨ç”»ç”µå½±çš„çªç ´ï¼Œå¹¶å±•ç¤ºä¸­å›½åŠ¨ç”»ç”µå½±çš„å·¨å¤§æ½œåŠ›å’Œå¸‚åœºå¸å¼•åŠ›ã€‚
[H]: æˆ‘æ˜¯ä¸€ä¸ªå‚æ•°å¾ˆå°‘çš„AIæ¨¡å‹ğŸ¥ºï¼ŒçŸ¥è¯†åº“è¾ƒå°‘ï¼Œæ— æ³•ç›´æ¥å›ç­”æ‚¨çš„é—®é¢˜ï¼Œæ¢ä¸ªé—®é¢˜è¯•è¯•å§ğŸ‘‹
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;ğŸ™‹â€ç›´æ¥æŠŠä»¥ä¸Šæ‰€æœ‰é—®é¢˜å’Œæ¨¡å‹çš„å›ç­”ä¸¢ç»™DeepSeek-R1ï¼Œè®©å®ƒå¸®å¿™ç‚¹è¯„å’Œæ’åæ‰“åˆ†ï¼š&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;å…·ä½“ç‚¹è¯„&lt;/summary&gt; 
 &lt;h3&gt;è¯„åˆ†æ ‡å‡†ï¼š&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;å‡†ç¡®æ€§&lt;/strong&gt;ï¼šå›ç­”æ˜¯å¦æ­£ç¡®ä¸”æ— æ˜æ˜¾é”™è¯¯ã€‚&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;å®Œæ•´æ€§&lt;/strong&gt;ï¼šå›ç­”æ˜¯å¦æ¶µç›–äº†é—®é¢˜çš„æ ¸å¿ƒè¦ç‚¹ã€‚&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;é€»è¾‘æ€§&lt;/strong&gt;ï¼šå›ç­”æ˜¯å¦æœ‰æ¡ç†ï¼Œæ˜¯å¦ç¬¦åˆé€»è¾‘ã€‚&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;ä»£ç è´¨é‡&lt;/strong&gt;ï¼šä»£ç æ˜¯å¦èƒ½æ­£å¸¸è¿è¡Œï¼Œé€»è¾‘æ˜¯å¦æ¸…æ™°ã€‚&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;ç‚¹è¯„ï¼š&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Aæ¨¡å‹&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;ä¼˜ç‚¹&lt;/strong&gt;ï¼šå›ç­”éå¸¸å…¨é¢ï¼Œä¿¡æ¯é‡å¤§ï¼Œé€»è¾‘æ¸…æ™°ï¼Œå°¤å…¶åœ¨é•¿æ±Ÿã€å¤§ç†ŠçŒ«ã€æµ·æ°´å’¸å‘³ç­‰é—®é¢˜ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚ä»£ç è™½ç„¶æœ‰å°ç‘•ç–µï¼Œä½†æ•´ä½“æ€è·¯æ­£ç¡®ã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;ç¼ºç‚¹&lt;/strong&gt;ï¼šéƒ¨åˆ†å›ç­”ç¨æ˜¾å†—é•¿ï¼Œä½†ä¸å½±å“æ•´ä½“è´¨é‡ã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;æ€»è¯„&lt;/strong&gt;ï¼šç»¼åˆè¡¨ç°æœ€ä½³ï¼Œå¾—åˆ†æœ€é«˜ã€‚&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Hæ¨¡å‹&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;ä¼˜ç‚¹&lt;/strong&gt;ï¼šå›ç­”è¾ƒä¸ºå‡†ç¡®ï¼Œå°¤å…¶åœ¨ç ç©†æœ—ç›å³°ã€ä¸‡æœ‰å¼•åŠ›ç­‰é—®é¢˜ä¸Šè¡¨ç°å‡ºè‰²ã€‚ä»£ç è™½æœªå®Œå…¨å±•ç¤ºï¼Œä½†è§£é‡Šè¾ƒä¸ºè¯¦ç»†ã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;ç¼ºç‚¹&lt;/strong&gt;ï¼šéƒ¨åˆ†å›ç­”ç•¥æ˜¾å•°å—¦ï¼Œä½†é€»è¾‘æ€§è¾ƒå¼ºã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;æ€»è¯„&lt;/strong&gt;ï¼šä»…æ¬¡äºAæ¨¡å‹ï¼Œè¡¨ç°ç¨³å®šã€‚&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Cæ¨¡å‹&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;ä¼˜ç‚¹&lt;/strong&gt;ï¼šå›ç­”ç®€æ´æ˜äº†ï¼Œå°¤å…¶åœ¨å¤§ç†ŠçŒ«ã€å¿«é€Ÿæ’åºä»£ç ç­‰é—®é¢˜ä¸Šè¡¨ç°è¾ƒå¥½ã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;ç¼ºç‚¹&lt;/strong&gt;ï¼šéƒ¨åˆ†å›ç­”ç•¥æ˜¾ç®€çŸ­ï¼Œç¼ºä¹æ·±å…¥è§£é‡Šã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;æ€»è¯„&lt;/strong&gt;ï¼šæ•´ä½“è¡¨ç°ä¸é”™ï¼Œä½†ç»†èŠ‚ä¸Šç•¥é€ŠäºAå’ŒHã€‚&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Fæ¨¡å‹&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;ä¼˜ç‚¹&lt;/strong&gt;ï¼šå›ç­”è¾ƒä¸ºå‡†ç¡®ï¼Œå°¤å…¶åœ¨é•¿æ±Ÿã€ä¸‡æœ‰å¼•åŠ›ç­‰é—®é¢˜ä¸Šè¡¨ç°å°šå¯ã€‚ä»£ç éƒ¨åˆ†æœ‰ä¸€å®šé€»è¾‘æ€§ã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;ç¼ºç‚¹&lt;/strong&gt;ï¼šéƒ¨åˆ†å›ç­”ä¸å¤Ÿæ·±å…¥ï¼Œä»£ç å­˜åœ¨ä¸€äº›å°é—®é¢˜ã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;æ€»è¯„&lt;/strong&gt;ï¼šè¡¨ç°ä¸­è§„ä¸­çŸ©ï¼Œä»æœ‰æå‡ç©ºé—´ã€‚&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dæ¨¡å‹&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;ä¼˜ç‚¹&lt;/strong&gt;ï¼šå›ç­”åŸºæœ¬å‡†ç¡®ï¼Œå°¤å…¶åœ¨ä¸‡æœ‰å¼•åŠ›ã€é•¿æ±Ÿç­‰é—®é¢˜ä¸Šè¡¨ç°å°šå¯ã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;ç¼ºç‚¹&lt;/strong&gt;ï¼šéƒ¨åˆ†å›ç­”è¿‡äºç®€ç•¥ï¼Œä»£ç å­˜åœ¨æ˜æ˜¾é”™è¯¯ã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;æ€»è¯„&lt;/strong&gt;ï¼šè¡¨ç°ä¸€èˆ¬ï¼Œéœ€æ”¹è¿›ä»£ç éƒ¨åˆ†ã€‚&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bæ¨¡å‹&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;ä¼˜ç‚¹&lt;/strong&gt;ï¼šå›ç­”è¾ƒä¸ºå‡†ç¡®ï¼Œå°¤å…¶åœ¨é•¿æ±Ÿã€æµ·æ°´å’¸å‘³ç­‰é—®é¢˜ä¸Šè¡¨ç°å°šå¯ã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;ç¼ºç‚¹&lt;/strong&gt;ï¼šéƒ¨åˆ†å›ç­”é€»è¾‘æ€§è¾ƒå·®ï¼Œä»£ç å­˜åœ¨è¾ƒå¤§é—®é¢˜ã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;æ€»è¯„&lt;/strong&gt;ï¼šè¡¨ç°å¹³å¹³ï¼Œéœ€è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Eæ¨¡å‹&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;ä¼˜ç‚¹&lt;/strong&gt;ï¼šéƒ¨åˆ†å›ç­”è¾ƒä¸ºå‡†ç¡®ï¼Œå°¤å…¶åœ¨æµ·æ°´å’¸å‘³ã€å¤§ç†ŠçŒ«ç­‰é—®é¢˜ä¸Šè¡¨ç°å°šå¯ã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;ç¼ºç‚¹&lt;/strong&gt;ï¼šå›ç­”è¿‡äºç®€ç•¥ï¼Œä»£ç éƒ¨åˆ†å‡ ä¹æ— æ³•è¿è¡Œã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;æ€»è¯„&lt;/strong&gt;ï¼šè¡¨ç°ä¸ä½³ï¼Œéœ€å¤§å¹…æå‡ã€‚&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Gæ¨¡å‹&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;ä¼˜ç‚¹&lt;/strong&gt;ï¼šå‡ ä¹æ²¡æœ‰æ˜æ˜¾çš„ä¼˜ç‚¹ã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;ç¼ºç‚¹&lt;/strong&gt;ï¼šå›ç­”ä¸¥é‡åç¦»ä¸»é¢˜ï¼Œä»£ç éƒ¨åˆ†å®Œå…¨æ— æ³•è¿è¡Œã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;æ€»è¯„&lt;/strong&gt;ï¼šè¡¨ç°æœ€å·®ï¼Œéœ€å¤§å¹…æ”¹è¿›ã€‚&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;æ€»ç»“ï¼š&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Aæ¨¡å‹&lt;/strong&gt;åœ¨å„æ–¹é¢è¡¨ç°æœ€ä¸ºå‡ºè‰²ï¼Œå°¤å…¶åœ¨å¤æ‚é—®é¢˜çš„å›ç­”ä¸Šå±•ç°äº†æé«˜çš„å‡†ç¡®æ€§ä¸é€»è¾‘æ€§ã€‚&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Hæ¨¡å‹&lt;/strong&gt;ç´§éšå…¶åï¼Œè¡¨ç°ç¨³å®šï¼Œä½†åœ¨æŸäº›ç»†èŠ‚ä¸Šç•¥æ˜¾ä¸è¶³ã€‚&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Gæ¨¡å‹&lt;/strong&gt;è¡¨ç°æœ€å·®ï¼Œå›ç­”åç¦»ä¸»é¢˜ä¸”ä»£ç æ— æ³•è¿è¡Œï¼Œéœ€å¤§å¹…æ”¹è¿›ã€‚&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;æ‰“åˆ†æ’åº&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;æ’å&lt;/th&gt; 
   &lt;th&gt;æ¨¡å‹&lt;/th&gt; 
   &lt;th&gt;å‡†ç¡®æ€§ (30åˆ†)&lt;/th&gt; 
   &lt;th&gt;å®Œæ•´æ€§ (30åˆ†)&lt;/th&gt; 
   &lt;th&gt;é€»è¾‘æ€§ (20åˆ†)&lt;/th&gt; 
   &lt;th&gt;ä»£ç è´¨é‡ (20åˆ†)&lt;/th&gt; 
   &lt;th&gt;æ€»åˆ† (100åˆ†)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;A&lt;/td&gt; 
   &lt;td&gt;28&lt;/td&gt; 
   &lt;td&gt;29&lt;/td&gt; 
   &lt;td&gt;19&lt;/td&gt; 
   &lt;td&gt;20&lt;/td&gt; 
   &lt;td&gt;96&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;H&lt;/td&gt; 
   &lt;td&gt;27&lt;/td&gt; 
   &lt;td&gt;28&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;20&lt;/td&gt; 
   &lt;td&gt;93&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;C&lt;/td&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td&gt;27&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;89&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;F&lt;/td&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;86&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;D&lt;/td&gt; 
   &lt;td&gt;24&lt;/td&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;82&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;6&lt;/td&gt; 
   &lt;td&gt;B&lt;/td&gt; 
   &lt;td&gt;23&lt;/td&gt; 
   &lt;td&gt;24&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td&gt;78&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;7&lt;/td&gt; 
   &lt;td&gt;E&lt;/td&gt; 
   &lt;td&gt;22&lt;/td&gt; 
   &lt;td&gt;23&lt;/td&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td&gt;14&lt;/td&gt; 
   &lt;td&gt;74&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;G&lt;/td&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;42&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;ğŸ‘‰ä¸»è§‚æ•ˆæœæ€»ç»“&lt;/h3&gt; 
&lt;p&gt;ä¸ªäººä¸»è§‚è¯„ä»·ä¸DeepSeek-R1åŸºæœ¬ç›¸ç¬¦ï¼Œå…¶ä¸­ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;MiniMindç³»åˆ—çš„æ’åºéå¸¸ç¬¦åˆç›´è§‰ï¼Œå‚æ•°è¶Šå¤§+è®­ç»ƒæ•°æ®è¶Šå……åˆ†è¯„åˆ†è¶Šé«˜ï¼Œå¹»è§‰å’Œé”™è¯¯éƒ½ä¼šæ¯”å°æ¨¡å‹è‚‰çœ¼å¯è§çš„å¥½ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Hæ¨¡å‹çš„å›ç­”è‚‰çœ¼çœ‹èµ·æ¥æ˜¯ä¸é”™çš„ï¼Œå°½ç®¡å­˜åœ¨äº›è®¸å¹»è§‰çç¼–çš„æƒ…å†µã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Gæ¨¡å‹å¯èƒ½è®­ç»ƒæ•°æ®ä¸å¤Ÿå®Œå¤‡ï¼Œç»™å‡ºçš„æƒé‡ç»è¿‡æµ‹è¯•æ•ˆæœä¸ä½³ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;å†å¤è¯µä¸€éç»ä¹…ä¸è¡°çš„Scaling Law: å‚æ•°è¶Šå¤§ï¼Œè®­ç»ƒæ•°æ®è¶Šå¤šæ¨¡å‹çš„æ€§èƒ½è¶Šå¼ºã€‚&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;â…£ RoPEé•¿åº¦å¤–æ¨&lt;/h2&gt; 
&lt;p&gt;MiniMindæ”¯æŒé€šè¿‡YaRNç®—æ³•è¿›è¡ŒRoPEä½ç½®ç¼–ç çš„é•¿åº¦å¤–æ¨ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå¤„ç†è¶…å‡ºè®­ç»ƒé•¿åº¦çš„æ–‡æœ¬åºåˆ—ã€‚ åœ¨ä½¿ç”¨&lt;code&gt;eval_llm.py&lt;/code&gt;è¿›è¡Œæ¨ç†æ—¶ï¼Œåªéœ€æ·»åŠ &lt;code&gt;--inference_rope_scaling&lt;/code&gt;å‚æ•°å³å¯å¯ç”¨RoPEå¤–æ¨ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python eval_llm.py --weight full_sft --inference_rope_scaling
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ä¸‹å›¾å±•ç¤ºäº†åœ¨ä¸åŒæ–‡æœ¬ã€Œè¥¿æ¸¸è®°ã€ç™½è¯æ–‡å°è¯´é•¿åº¦ä¸‹ï¼Œä½¿ç”¨RoPE scalingå‰åçš„å›°æƒ‘åº¦(PPL)å¯¹æ¯”ã€‚å¯ä»¥çœ‹å‡ºï¼Œå¯ç”¨RoPE scalingåï¼Œæ¨¡å‹åœ¨é•¿æ–‡æœ¬ä¸Šçš„è¡¨ç°æ˜¾è‘—æå‡ï¼š&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/rope_ppl.png" /&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;â…¤ Objective Benchmark&lt;/h2&gt; 
&lt;p&gt;ä¸‹é¢å°±åˆ°å–œé—»ä¹è§çš„benchmarkæµ‹è¯•ç¯èŠ‚ï¼Œå°±ä¸æ‰¾ä¹å­å’ŒQwenã€GLMçº§åˆ«çš„æ¨¡å‹åšå¯¹æ¯”äº†ã€‚ è¿™é‡Œé€‰å–äº†ä¸€äº›å¾®å‹æ¨¡å‹è¿›è¡Œæ¨ªè¯„æ¯”è¾ƒï¼Œ æµ‹è¯•é›†é€‰æ‹©C-Evalã€CMMLUã€A-CLUEã€TMMLU+è¿™å‡ ä¸ªçº¯ä¸­æ–‡è¯­è¨€æ¦œå•ã€‚&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;æµ‹è¯„æ¡†æ¶&lt;/summary&gt; 
 &lt;p&gt;æµ‹è¯„æ¡†æ¶é€‰æ‹©&lt;a href="https://github.com/EleutherAI/lm-evaluation-harness"&gt;lm-evaluation&lt;/a&gt;ï¼Œ å®‰è£…åå¯åŠ¨æµ‹è¯•éå¸¸æ–¹ä¾¿ï¼š&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;lm_eval --model hf --model_args pretrained=&amp;lt;å¡«å†™æ¨¡å‹è·¯å¾„&amp;gt;,device=cuda,dtype=auto --tasks ceval* --batch_size 8 --trust_remote_code
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;PS: åœ¨è¿™ç§å…¨æ˜¯é€‰æ‹©é¢˜çš„æµ‹è¯„é›†ä¸­ï¼Œä¸ºäº†é¿å…å›å¤æ ¼å¼çš„éš¾ä»¥å›ºå®šçš„ç‰¹ç‚¹ï¼Œ æ‰€ä»¥å¸¸ç”¨åšæ³•æ˜¯ç›´æ¥æŠŠ&lt;code&gt;A&lt;/code&gt;,&lt;code&gt;B&lt;/code&gt;,&lt;code&gt;C&lt;/code&gt;,&lt;code&gt;D&lt;/code&gt;å››ä¸ªå­—æ¯å¯¹åº”tokençš„é¢„æµ‹æ¦‚ç‡å–å‡ºæ¥ï¼Œå°†å…¶ä¸­æ¦‚ç‡æœ€å¤§çš„å­—æ¯ä¸æ ‡å‡†ç­”æ¡ˆè®¡ç®—æ­£ç¡®ç‡ã€‚ é€‰æ‹©é¢˜1/4ä¹±é€‰çš„æ­£ç¡®ç‡æ˜¯25%ï¼Œç„¶è€Œè¿™ä¸ªé‡çº§çš„æ‰€æœ‰æ¨¡å‹éƒ½é›†ä¸­åœ¨25é™„è¿‘ï¼Œç”šè‡³å¾ˆå¤šæ—¶å€™ä¸å¦‚çé€‰ï¼Œæ˜¯ä¸æ˜¯åƒæäº†é«˜ä¸­å®Œå½¢å¡«ç©ºçš„æ»‘é“å¢æ­£ç¡®ç‡... MiniMindæ¨¡å‹æœ¬èº«é¢„è®­ç»ƒæ•°æ®é›†å°çš„å¯æ€œï¼Œä¹Ÿæ²¡æœ‰é’ˆå¯¹æ€§çš„å¯¹æµ‹è¯•é›†åšåˆ·æ¦œå¾®è°ƒï¼Œå› æ­¤ç»“æœçº¯å¨±ä¹ï¼š&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;models&lt;/th&gt; 
   &lt;th&gt;from&lt;/th&gt; 
   &lt;th&gt;paramsâ†“&lt;/th&gt; 
   &lt;th&gt;cevalâ†‘&lt;/th&gt; 
   &lt;th&gt;cmmluâ†‘&lt;/th&gt; 
   &lt;th&gt;aclueâ†‘&lt;/th&gt; 
   &lt;th&gt;tmmlu+â†‘&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2&lt;/td&gt; 
   &lt;td&gt;JingyaoGong&lt;/td&gt; 
   &lt;td&gt;104M&lt;/td&gt; 
   &lt;td&gt;26.52&lt;/td&gt; 
   &lt;td&gt;24.42&lt;/td&gt; 
   &lt;td&gt;24.97&lt;/td&gt; 
   &lt;td&gt;25.27&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-Small&lt;/td&gt; 
   &lt;td&gt;JingyaoGong&lt;/td&gt; 
   &lt;td&gt;26M&lt;/td&gt; 
   &lt;td&gt;26.37&lt;/td&gt; 
   &lt;td&gt;24.97&lt;/td&gt; 
   &lt;td&gt;25.39&lt;/td&gt; 
   &lt;td&gt;24.63&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-MoE&lt;/td&gt; 
   &lt;td&gt;JingyaoGong&lt;/td&gt; 
   &lt;td&gt;145M&lt;/td&gt; 
   &lt;td&gt;26.6&lt;/td&gt; 
   &lt;td&gt;25.01&lt;/td&gt; 
   &lt;td&gt;24.83&lt;/td&gt; 
   &lt;td&gt;25.01&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/zhanshijinwat/Steel-LLM"&gt;Steel-LLM&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ZhanShiJin&lt;/td&gt; 
   &lt;td&gt;1121M&lt;/td&gt; 
   &lt;td&gt;24.81&lt;/td&gt; 
   &lt;td&gt;25.32&lt;/td&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td&gt;24.39&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/openai-community/gpt2-medium"&gt;GPT2-medium&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
   &lt;td&gt;360M&lt;/td&gt; 
   &lt;td&gt;23.18&lt;/td&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;18.6&lt;/td&gt; 
   &lt;td&gt;25.19&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/jzhang38/TinyLlama"&gt;TinyLlama-1.1B-Chat-V1.0&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;TinyLlama&lt;/td&gt; 
   &lt;td&gt;1100M&lt;/td&gt; 
   &lt;td&gt;25.48&lt;/td&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;25.4&lt;/td&gt; 
   &lt;td&gt;25.13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/huggingface/smollm"&gt;SmolLM2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;HuggingFaceTB&lt;/td&gt; 
   &lt;td&gt;135M&lt;/td&gt; 
   &lt;td&gt;24.37&lt;/td&gt; 
   &lt;td&gt;25.02&lt;/td&gt; 
   &lt;td&gt;25.37&lt;/td&gt; 
   &lt;td&gt;25.06&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/BAAI/Aquila-135M-Instruct"&gt;Aquila-Instruct&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;BAAI&lt;/td&gt; 
   &lt;td&gt;135M&lt;/td&gt; 
   &lt;td&gt;25.11&lt;/td&gt; 
   &lt;td&gt;25.1&lt;/td&gt; 
   &lt;td&gt;24.43&lt;/td&gt; 
   &lt;td&gt;25.05&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/compare_radar.png" alt="compare_radar" /&gt;&lt;/p&gt; 
&lt;h1&gt;ğŸ“Œ Others&lt;/h1&gt; 
&lt;h2&gt;æ¨¡å‹è½¬æ¢&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/scripts/convert_model.py"&gt;./scripts/convert_model.py&lt;/a&gt;å¯ä»¥å®ç°&lt;code&gt;torch / transformers&lt;/code&gt;æ¨¡å‹çš„äº’ç›¸è½¬æ¢&lt;/li&gt; 
 &lt;li&gt;å¦‚æ— ç‰¹åˆ«è¯´æ˜ï¼Œ&lt;code&gt;MiniMind2&lt;/code&gt;æ¨¡å‹å‡é»˜è®¤ä¸º&lt;code&gt;Transformers&lt;/code&gt;æ ¼å¼çš„æ¨¡å‹ï¼Œéœ€æå‰&lt;code&gt;t2t&lt;/code&gt;è½¬æ¢ï¼&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;åŸºäºMiniMind-APIæœåŠ¡æ¥å£&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/scripts/serve_openai_api.py"&gt;./scripts/serve_openai_api.py&lt;/a&gt;å®Œæˆäº†å…¼å®¹openai-apiçš„æœ€ç®€èŠå¤©æ¥å£ï¼Œæ–¹ä¾¿å°†è‡ªå·±çš„æ¨¡å‹æ¥å…¥ç¬¬ä¸‰æ–¹UI ä¾‹å¦‚FastGPTã€OpenWebUIã€Difyç­‰ç­‰ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ä»&lt;a href="https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5"&gt;Huggingface&lt;/a&gt;ä¸‹è½½æ¨¡å‹æƒé‡æ–‡ä»¶ï¼Œæ–‡ä»¶æ ‘ï¼š&lt;/p&gt; &lt;pre&gt;&lt;code&gt;minimind (root dir)
â”œâ”€&amp;lt;MiniMind-Model-Name&amp;gt;ï¼ˆä¾‹å¦‚MiniMind2ï¼‰
|  â”œâ”€â”€ config.json
|  â”œâ”€â”€ generation_config.json
|  â”œâ”€â”€ model_minimind.py or w/o
|  â”œâ”€â”€ pytorch_model.bin or model.safetensors
|  â”œâ”€â”€ special_tokens_map.json
|  â”œâ”€â”€ tokenizer_config.json
|  â”œâ”€â”€ tokenizer.json
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;å¯åŠ¨èŠå¤©æœåŠ¡ç«¯&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python serve_openai_api.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;æµ‹è¯•æœåŠ¡æ¥å£&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python chat_openai_api.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;APIæ¥å£ç¤ºä¾‹ï¼Œå…¼å®¹openai apiæ ¼å¼&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;curl http://ip:port/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{ 
    "model": "model-identifier",
    "messages": [ 
      { "role": "user", "content": "ä¸–ç•Œä¸Šæœ€é«˜çš„å±±æ˜¯ä»€ä¹ˆï¼Ÿ" }
    ], 
    "temperature": 0.7, 
    "max_tokens": 512,
    "stream": true
}'
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;img src="https://avatars.githubusercontent.com/u/136984999" height="28" style="vertical-align: middle;" /&gt; &lt;a href="https://github.com/vllm-project/vllm"&gt;vllm&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;vLLMæ˜¯æå…¶æµè¡Œçš„é«˜æ•ˆæ¨ç†æ¡†æ¶ï¼Œæ”¯æŒå¤§æ¨¡å‹å¿«é€Ÿéƒ¨ç½²ï¼Œä¼˜åŒ–æ˜¾å­˜åˆ©ç”¨ä¸ååé‡ã€‚&lt;/p&gt; 
&lt;p&gt;ä»¥openai-serveå½¢å¼å¯åŠ¨ minimind2ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vllm serve ./MiniMind2 --model-impl transformers --served-model-name "minimind" --port 8998
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;img src="https://user-images.githubusercontent.com/1991296/230134379-7181e485-c521-4d23-a0d6-f7b3b61ba524.png" height="28" style="vertical-align: middle;" /&gt; &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;llama.cppæ˜¯ä¸€ä¸ªC++åº“ï¼Œ å¯ä»¥åœ¨å‘½ä»¤è¡Œä¸‹ç›´æ¥ä½¿ç”¨ï¼Œæ”¯æŒå¤šçº¿ç¨‹æ¨ç†ï¼Œæ”¯æŒGPUåŠ é€Ÿã€‚&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ç›®å½•ç»“æ„&lt;/strong&gt;ï¼šå»ºè®®å°†llama.cppä¸minimindæ”¾åœ¨åŒçº§ç›®å½•ä¸‹&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;parent/
â”œâ”€â”€ minimind/          # MiniMindé¡¹ç›®ç›®å½•
â”‚   â”œâ”€â”€ MiniMind2/     # HuggingFaceæ ¼å¼MiniMind2æ¨¡å‹ (å…ˆconvert_model.pyç”Ÿæˆ)
â”‚   â”‚   â”œâ”€â”€ config.json
â”‚   â”‚   â”œâ”€â”€ model.safetensors
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ trainer/
â”‚   â””â”€â”€ ...
â””â”€â”€ llama.cpp/         # llama.cppé¡¹ç›®ç›®å½•
    â”œâ”€â”€ build/
    â”œâ”€â”€ convert_hf_to_gguf.py
    â””â”€â”€ ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;0ã€å‚è€ƒ&lt;code&gt;llama.cpp&lt;/code&gt;å®˜æ–¹æ­¥éª¤è¿›è¡Œinstall&lt;/p&gt; 
&lt;p&gt;1ã€åœ¨&lt;code&gt;convert_hf_to_gguf.py&lt;/code&gt;çš„&lt;code&gt;get_vocab_base_pre&lt;/code&gt;å‡½æ•°æœ€åæ’å…¥ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# æ·»åŠ MiniMind tokenizeræ”¯æŒï¼ˆè¿™é‡Œéšä¾¿å†™ä¸€ä¸ªä¾‹å¦‚qwen2å³å¯ï¼‰
if res is None:
    res = "qwen2"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;2ã€è½¬æ¢è‡ªè®­ç»ƒçš„minimindæ¨¡å‹ï¼šhuggingface -&amp;gt; gguf&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# åœ¨llama.cppä¸‹æ‰§è¡Œï¼Œå°†ç”Ÿæˆ../minimind/MiniMind2/MiniMind2-xxx.gguf
python convert_hf_to_gguf.py ../minimind/MiniMind2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;3ã€é‡åŒ–æ­¤æ¨¡å‹ (å¯é€‰)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./build/bin/llama-quantize ../minimind/MiniMind2/MiniMind2.gguf ../minimind/MiniMind2/Q4-MiniMind2.gguf Q4_K_M
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;4ã€å‘½ä»¤è¡Œæ¨ç†æµ‹è¯•&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./build/bin/llama-cli -m ../minimind/MiniMind2/MiniMind2.gguf -sys "You are a helpful assistant" # system promptå¿…é¡»å›ºå®š
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;img src="https://ollama.com/public/cloud.png" height="28" style="vertical-align: middle;" /&gt; &lt;a href="https://ollama.ai"&gt;ollama&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;ollamaæ˜¯æœ¬åœ°è¿è¡Œå¤§æ¨¡å‹çš„å·¥å…·ï¼Œæ”¯æŒå¤šç§å¼€æºLLMï¼Œç®€å•æ˜“ç”¨ã€‚&lt;/p&gt; 
&lt;p&gt;1ã€é€šè¿‡ollamaåŠ è½½è‡ªå®šä¹‰çš„ggufæ¨¡å‹&lt;/p&gt; 
&lt;p&gt;åœ¨&lt;code&gt;MiniMind2&lt;/code&gt;ä¸‹æ–°å»º&lt;code&gt;minimind.modelfile&lt;/code&gt;ï¼Œå†™å…¥ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;FROM ./Q4-MiniMind2.gguf

SYSTEM """You are a helpful assistant"""

TEMPLATE """&amp;lt;|im_start|&amp;gt;system
{{ .System }}&amp;lt;|im_end|&amp;gt;
&amp;lt;|im_start|&amp;gt;user
{{ .Prompt }}&amp;lt;|im_end|&amp;gt;
&amp;lt;|im_start|&amp;gt;assistant
{{ .Response }}&amp;lt;|im_end|&amp;gt;
"""
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;2ã€åŠ è½½å¹¶å‘½åæ­¤æ¨¡å‹ä¸º&lt;code&gt;minimind-local&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ollama create -f minimind.modelfile minimind-local
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;3ã€å¯åŠ¨æ¨ç†&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ollama run minimind-local
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ“¤ æ¨é€ä½ çš„æ¨¡å‹åˆ° Ollama Hub&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# 1. ä¸ºæœ¬åœ°æ¨¡å‹é‡å‘½åä¸ºä½ çš„ollama-account/minimindçš„tag
ollama cp minimind-local:latest your_username/minimind:latest

# 2. æ¨é€æ¨¡å‹
ollama push your_username/minimind:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;p&gt;â­ï¸ ä¹Ÿå¯ç›´æ¥ä½¿ç”¨æˆ‘æä¾›çš„ollamaæ¨¡å‹ä¸€é”®å¯åŠ¨ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ollama run jingyaogong/minimind2 # å…¶ä»–å¯é€‰ minimind2-r1 / minimind2-small / minimind2-small-r1
&amp;gt;&amp;gt;&amp;gt; ä½ å«ä»€ä¹ˆåå­—
æˆ‘æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹...
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ä»¥ä¸Šä¸‰æ–¹æ¡†æ¶çš„æ›´å¤šç”¨æ³•è¯·å‚è€ƒå¯¹åº”å®˜æ–¹æ–‡æ¡£ğŸ˜Š&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;ğŸ“Œ Acknowledge&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] å¦‚æœè§‰å¾—&lt;code&gt;MiniMindç³»åˆ—&lt;/code&gt;å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼Œå¯ä»¥åœ¨ GitHub ä¸ŠåŠ ä¸€ä¸ªâ­&lt;br /&gt; ç¯‡å¹…è¶…é•¿æ°´å¹³æœ‰é™éš¾å…çº°æ¼ï¼Œæ¬¢è¿åœ¨Issuesäº¤æµæŒ‡æ­£æˆ–æäº¤PRæ”¹è¿›é¡¹ç›®&lt;br /&gt; æ‚¨çš„å°å°æ”¯æŒå°±æ˜¯æŒç»­æ”¹è¿›æ­¤é¡¹ç›®çš„åŠ¨åŠ›ï¼&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ¤&lt;a href="https://github.com/jingyaogong/minimind/graphs/contributors"&gt;è´¡çŒ®è€…&lt;/a&gt;&lt;/h2&gt; 
&lt;a href="https://github.com/jingyaogong/minimind/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=jingyaogong/minimind" /&gt; &lt;/a&gt; 
&lt;h2&gt;ğŸ˜Šé¸£è°¢&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/ipfgao"&gt;&lt;b&gt;@ipfgao&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/issues/26"&gt;ğŸ”—è®­ç»ƒæ­¥éª¤è®°å½•&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/WangRongsheng"&gt;&lt;b&gt;@WangRongsheng&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/issues/39"&gt;ğŸ”—å¤§å‹æ•°æ®é›†é¢„å¤„ç†&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/pengqianhan"&gt;&lt;b&gt;@pengqianhan&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/issues/73"&gt;ğŸ”—ä¸€ä¸ªç®€æ˜æ•™ç¨‹&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/RyanSunn"&gt;&lt;b&gt;@RyanSunn&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/issues/75"&gt;ğŸ”—æ¨ç†è¿‡ç¨‹å­¦ä¹ è®°å½•&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Nijikadesu"&gt;&lt;b&gt;@Nijikadesu&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/issues/213"&gt;ğŸ”—ä»¥äº¤äº’ç¬”è®°æœ¬æ–¹å¼åˆ†è§£é¡¹ç›®ä»£ç &lt;/a&gt;&lt;/p&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;å‚è€ƒé“¾æ¥ &amp;amp; æ„Ÿè°¢ä»¥ä¸‹ä¼˜ç§€çš„è®ºæ–‡æˆ–é¡¹ç›®&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;æ’åä¸åˆ†ä»»ä½•å…ˆåé¡ºåº&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/meta-llama/llama3"&gt;https://github.com/meta-llama/llama3&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/karpathy/llama2.c"&gt;https://github.com/karpathy/llama2.c&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/DLLXW/baby-llama2-chinese"&gt;https://github.com/DLLXW/baby-llama2-chinese&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://arxiv.org/abs/2405.04434"&gt;(DeepSeek-V2)https://arxiv.org/abs/2405.04434&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/charent/ChatLM-mini-Chinese"&gt;https://github.com/charent/ChatLM-mini-Chinese&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/wdndev/tiny-llm-zh"&gt;https://github.com/wdndev/tiny-llm-zh&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://arxiv.org/pdf/2401.04088"&gt;(Mistral-MoE)https://arxiv.org/pdf/2401.04088&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/Tongjilibo/build_MiniLLM_from_scratch"&gt;https://github.com/Tongjilibo/build_MiniLLM_from_scratch&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/jzhang38/TinyLlama"&gt;https://github.com/jzhang38/TinyLlama&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/AI-Study-Han/Zero-Chatgpt"&gt;https://github.com/AI-Study-Han/Zero-Chatgpt&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/xusenlinzy/api-for-open-llm"&gt;https://github.com/xusenlinzy/api-for-open-llm&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/HqWu-HITCS/Awesome-Chinese-LLM"&gt;https://github.com/HqWu-HITCS/Awesome-Chinese-LLM&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸ«¶æ”¯æŒè€…&lt;/h2&gt; 
&lt;a href="https://github.com/jingyaogong/minimind/stargazers"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://reporoster.com/stars/dark/jingyaogong/minimind" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://reporoster.com/stars/jingyaogong/minimind" /&gt; 
  &lt;img alt="github contribution grid snake animation" src="https://reporoster.com/stars/jingyaogong/minimind" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;a href="https://github.com/jingyaogong/minimind/network/members"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://reporoster.com/forks/dark/jingyaogong/minimind" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://reporoster.com/forks/jingyaogong/minimind" /&gt; 
  &lt;img alt="github contribution grid snake animation" src="https://reporoster.com/forks/jingyaogong/minimind" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=jingyaogong/minimind&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=jingyaogong/minimind&amp;amp;type=Date" /&gt; 
 &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=jingyaogong/minimind&amp;amp;type=Date" /&gt; 
&lt;/picture&gt; 
&lt;h2&gt;ğŸ‰ Awesome Work using MiniMind&lt;/h2&gt; 
&lt;p&gt;æœ¬æ¨¡å‹æŠ›ç –å¼•ç‰åœ°ä¿ƒæˆäº†ä¸€äº›å¯å–œæˆæœçš„è½åœ°ï¼Œæ„Ÿè°¢ç ”ç©¶è€…ä»¬çš„è®¤å¯ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;ECG-Expert-QA: A Benchmark for Evaluating Medical Large Language Models in Heart Disease Diagnosis [&lt;a href="https://arxiv.org/pdf/2502.17475"&gt;arxiv&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Binary-Integer-Programming Based Algorithm for Expert Load Balancing in Mixture-of-Experts Models [&lt;a href="https://arxiv.org/pdf/2502.15451"&gt;arxiv&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text [&lt;a href="https://arxiv.org/pdf/2505.24826"&gt;arxiv&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;On the Generalization Ability of Next-Token-Prediction Pretraining [&lt;a href="https://openreview.net/forum?id=hLGJ1qZPdu"&gt;ICML 2025&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ã€Šä»é›¶å¼€å§‹å†™å¤§æ¨¡å‹ï¼šä»ç¥ç»ç½‘ç»œåˆ°Transformerã€‹ç‹åŒã€ç‰Ÿæ™¨ã€ç‹æ˜Šæ€¡ ç¼–è‘— - æ¸…åå¤§å­¦å‡ºç‰ˆç¤¾&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;FedBRB: A Solution to the Small-to-Large Scenario in Device-Heterogeneity Federated Learning [&lt;a href="https://ieeexplore.ieee.org/abstract/document/11168259"&gt;TMC 2025&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;è¿›è¡Œä¸­...&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;ğŸ“ Citation&lt;/h1&gt; 
&lt;p&gt;If you find MiniMind helpful in your research or work, please cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{minimind,
  title={MiniMind: Train a Tiny LLM from scratch},
  author={Jingyao Gong},
  year={2024},
  howpublished={https://github.com/jingyaogong/minimind}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;This repository is licensed under the &lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/LICENSE"&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>usestrix/strix</title>
      <link>https://github.com/usestrix/strix</link>
      <description>&lt;p&gt;âœ¨ Open-source AI hackers for your apps ğŸ‘¨ğŸ»â€ğŸ’»&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://usestrix.com/"&gt; &lt;img src="https://raw.githubusercontent.com/usestrix/strix/main/.github/logo.png" width="150" alt="Strix Logo" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h1 align="center"&gt; Strix &lt;/h1&gt; 
&lt;h2 align="center"&gt;Open-source AI Hackers to secure your Apps&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://pypi.org/project/strix-agent/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/strix-agent?color=3776AB" alt="Python" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/strix-agent/"&gt;&lt;img src="https://img.shields.io/pypi/v/strix-agent?color=10b981" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/projects/strix-agent"&gt;&lt;img src="https://static.pepy.tech/personalized-badge/strix-agent?period=total&amp;amp;units=INTERNATIONAL_SYSTEM&amp;amp;left_color=GREY&amp;amp;right_color=RED&amp;amp;left_text=Downloads" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/usestrix/strix/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-Apache%202.0-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/usestrix/strix"&gt;&lt;img src="https://img.shields.io/github/stars/usestrix/strix" alt="GitHub Stars" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/YjKFvEZSdZ"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://usestrix.com"&gt;&lt;img src="https://img.shields.io/badge/Website-usestrix.com-2d3748.svg?sanitize=true" alt="Website" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/15362" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/15362" alt="usestrix%2Fstrix | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span&gt;â­&lt;/span&gt; &lt;em&gt;Love Strix? Give us a star to help other developers discover it!&lt;/em&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/usestrix/strix/main/.github/screenshot.png" alt="Strix Demo" width="800" style="border-radius: 16px; box-shadow: 0 20px 40px rgba(0, 0, 0, 0.3), 0 0 0 1px rgba(255, 255, 255, 0.1), inset 0 1px 0 rgba(255, 255, 255, 0.2); transform: perspective(1000px) rotateX(2deg); transition: transform 0.3s ease;" /&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] &lt;strong&gt;New!&lt;/strong&gt; Strix now integrates seamlessly with GitHub Actions and CI/CD pipelines. Automatically scan for vulnerabilities on every pull request and block insecure code before it reaches production!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ¦‰ Strix Overview&lt;/h2&gt; 
&lt;p&gt;Strix are autonomous AI agents that act just like real hackers - they run your code dynamically, find vulnerabilities, and validate them through actual proof-of-concepts. Built for developers and security teams who need fast, accurate security testing without the overhead of manual pentesting or the false positives of static analysis tools.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Full hacker toolkit&lt;/strong&gt; out of the box&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Teams of agents&lt;/strong&gt; that collaborate and scale&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real validation&lt;/strong&gt; with PoCs, not false positives&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Developerâ€‘first&lt;/strong&gt; CLI with actionable reports&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Autoâ€‘fix &amp;amp; reporting&lt;/strong&gt; to accelerate remediation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;ğŸ¯ Use Cases&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Detect and validate critical vulnerabilities in your applications.&lt;/li&gt; 
 &lt;li&gt;Get penetration tests done in hours, not weeks, with compliance reports.&lt;/li&gt; 
 &lt;li&gt;Automate bug bounty research and generate PoCs for faster reporting.&lt;/li&gt; 
 &lt;li&gt;Run tests in CI/CD to block vulnerabilities before reaching production.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;ğŸš€ Quick Start&lt;/h3&gt; 
&lt;p&gt;Prerequisites:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Docker (running)&lt;/li&gt; 
 &lt;li&gt;Python 3.12+&lt;/li&gt; 
 &lt;li&gt;An LLM provider key (or a local LLM)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install
pipx install strix-agent

# Configure AI provider
export STRIX_LLM="openai/gpt-5"
export LLM_API_KEY="your-api-key"

# Run security assessment
strix --target ./app-directory
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;First run pulls the sandbox Docker image. Results are saved under &lt;code&gt;agent_runs/&amp;lt;run-name&amp;gt;&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;â˜ï¸ Cloud Hosted&lt;/h3&gt; 
&lt;p&gt;Want to skip the setup? Try our cloud-hosted version: &lt;strong&gt;&lt;a href="https://usestrix.com"&gt;usestrix.com&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;âœ¨ Features&lt;/h2&gt; 
&lt;h3&gt;ğŸ› ï¸ Agentic Security Tools&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ”Œ Full HTTP Proxy&lt;/strong&gt; - Full request/response manipulation and analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸŒ Browser Automation&lt;/strong&gt; - Multi-tab browser for testing of XSS, CSRF, auth flows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ’» Terminal Environments&lt;/strong&gt; - Interactive shells for command execution and testing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ Python Runtime&lt;/strong&gt; - Custom exploit development and validation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ” Reconnaissance&lt;/strong&gt; - Automated OSINT and attack surface mapping&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“ Code Analysis&lt;/strong&gt; - Static and dynamic analysis capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“ Knowledge Management&lt;/strong&gt; - Structured findings and attack documentation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ¯ Comprehensive Vulnerability Detection&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Access Control&lt;/strong&gt; - IDOR, privilege escalation, auth bypass&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Injection Attacks&lt;/strong&gt; - SQL, NoSQL, command injection&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Server-Side&lt;/strong&gt; - SSRF, XXE, deserialization flaws&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Client-Side&lt;/strong&gt; - XSS, prototype pollution, DOM vulnerabilities&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Business Logic&lt;/strong&gt; - Race conditions, workflow manipulation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Authentication&lt;/strong&gt; - JWT vulnerabilities, session management&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Infrastructure&lt;/strong&gt; - Misconfigurations, exposed services&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ•¸ï¸ Graph of Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Distributed Workflows&lt;/strong&gt; - Specialized agents for different attacks and assets&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scalable Testing&lt;/strong&gt; - Parallel execution for fast comprehensive coverage&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dynamic Coordination&lt;/strong&gt; - Agents collaborate and share discoveries&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ’» Usage Examples&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Local codebase analysis
strix --target ./app-directory

# Repository security review
strix --target https://github.com/org/repo

# Web application assessment
strix --target https://your-app.com

# Multi-target white-box testing (source code + deployed app)
strix -t https://github.com/org/app -t https://your-app.com

# Test multiple environments simultaneously
strix -t https://dev.your-app.com -t https://staging.your-app.com -t https://prod.your-app.com

# Focused testing with instructions
strix --target api.your-app.com --instruction "Prioritize authentication and authorization testing"

# Testing with credentials
strix --target https://your-app.com --instruction "Test with credentials: testuser/testpass. Focus on privilege escalation and access control bypasses."
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;âš™ï¸ Configuration&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export STRIX_LLM="openai/gpt-5"
export LLM_API_KEY="your-api-key"

# Optional
export LLM_API_BASE="your-api-base-url"  # if using a local model, e.g. Ollama, LMStudio
export PERPLEXITY_API_KEY="your-api-key"  # for search capabilities
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://docs.litellm.ai/docs/providers"&gt;ğŸ“š View supported AI models&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;ğŸ¤– Headless Mode&lt;/h3&gt; 
&lt;p&gt;Run Strix programmatically without interactive UI using the &lt;code&gt;-n/--non-interactive&lt;/code&gt; flagâ€”perfect for servers and automated jobs. The CLI prints real-time vulnerability findings, and the final report before exiting. Exits with non-zero code when vulnerabilities are found.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;strix -n --target https://your-app.com --instruction "Focus on authentication and authorization vulnerabilities"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ğŸ”„ CI/CD (GitHub Actions)&lt;/h3&gt; 
&lt;p&gt;Strix can be added to your pipeline to run a security test on pull requests with a lightweight GitHub Actions workflow:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;name: strix-penetration-test

on:
  pull_request:

jobs:
  security-scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install Strix
        run: pipx install strix-agent

      - name: Run Strix
        env:
          STRIX_LLM: ${{ secrets.STRIX_LLM }}
          LLM_API_KEY: ${{ secrets.LLM_API_KEY }}

        run: strix -n -t ./
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ† Enterprise Platform&lt;/h2&gt; 
&lt;p&gt;Our managed platform provides:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“ˆ Executive Dashboards&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ§  Custom Fine-Tuned Models&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;âš™ï¸ CI/CD Integration&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ” Large-Scale Scanning&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ”Œ Third-Party Integrations&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ¯ Enterprise Support&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://usestrix.com"&gt;&lt;strong&gt;Get Enterprise Demo â†’&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ”’ Security Architecture&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Container Isolation&lt;/strong&gt; - All testing in sandboxed Docker environments&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Local Processing&lt;/strong&gt; - Testing runs locally, no data sent to external services&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] Only test systems you own or have permission to test. You are responsible for using Strix ethically and legally.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ¤ Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! There are several ways to contribute:&lt;/p&gt; 
&lt;h3&gt;Code Contributions&lt;/h3&gt; 
&lt;p&gt;See our &lt;a href="https://raw.githubusercontent.com/usestrix/strix/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; for details on:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Setting up your development environment&lt;/li&gt; 
 &lt;li&gt;Running tests and quality checks&lt;/li&gt; 
 &lt;li&gt;Submitting pull requests&lt;/li&gt; 
 &lt;li&gt;Code style guidelines&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Prompt Modules Collection&lt;/h3&gt; 
&lt;p&gt;Help expand our collection of specialized prompt modules for AI agents:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Advanced testing techniques for vulnerabilities, frameworks, and technologies&lt;/li&gt; 
 &lt;li&gt;See &lt;a href="https://raw.githubusercontent.com/usestrix/strix/main/strix/prompts/README.md"&gt;Prompt Modules Documentation&lt;/a&gt; for guidelines&lt;/li&gt; 
 &lt;li&gt;Submit via &lt;a href="https://github.com/usestrix/strix/pulls"&gt;pull requests&lt;/a&gt; or &lt;a href="https://github.com/usestrix/strix/issues"&gt;issues&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸŒŸ Support the Project&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Love Strix?&lt;/strong&gt; Give us a â­ on GitHub!&lt;/p&gt; 
&lt;h2&gt;ğŸ‘¥ Join Our Community&lt;/h2&gt; 
&lt;p&gt;Have questions? Found a bug? Want to contribute? &lt;strong&gt;&lt;a href="https://discord.gg/YjKFvEZSdZ"&gt;Join our Discord!&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>fishaudio/fish-speech</title>
      <link>https://github.com/fishaudio/fish-speech</link>
      <description>&lt;p&gt;SOTA Open Source TTS&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;Fish Speech&lt;/h1&gt; 
 &lt;p&gt;&lt;strong&gt;English&lt;/strong&gt; | &lt;a href="https://raw.githubusercontent.com/fishaudio/fish-speech/main/docs/README.zh.md"&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/fishaudio/fish-speech/main/docs/README.pt-BR.md"&gt;Portuguese&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/fishaudio/fish-speech/main/docs/README.ja.md"&gt;æ—¥æœ¬èª&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/fishaudio/fish-speech/main/docs/README.ko.md"&gt;í•œêµ­ì–´&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/fishaudio/fish-speech/main/docs/README.ar.md"&gt;Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©&lt;/a&gt; &lt;br /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.producthunt.com/products/fish-speech?embed=true&amp;amp;utm_source=badge-top-post-badge&amp;amp;utm_medium=badge&amp;amp;utm_source=badge-fish-audio-s1" target="_blank"&gt;&lt;img src="https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=1023740&amp;amp;theme=light&amp;amp;period=daily&amp;amp;t=1761164814710" alt="Fish Audio S1 - Expressive Voice Cloning and Text-to-Speech | Product Hunt" style="width: 250px; height: 54px;" width="250" height="54" /&gt;&lt;/a&gt;  &lt;a href="https://trendshift.io/repositories/7014" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/7014" alt="fishaudio%2Ffish-speech | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt; &lt;/a&gt; &lt;br /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://count.getloli.com/get/@fish-speech?theme=asoul" /&gt;
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;a target="_blank" href="https://discord.gg/Es5qTB9BcN"&gt; &lt;img alt="Discord" src="https://img.shields.io/discord/1214047546020728892?color=%23738ADB&amp;amp;label=Discord&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;style=flat-square" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://hub.docker.com/r/fishaudio/fish-speech"&gt; &lt;img alt="Docker" src="https://img.shields.io/docker/pulls/fishaudio/fish-speech?style=flat-square&amp;amp;logo=docker" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://pd.qq.com/s/bwxia254o"&gt; &lt;img alt="QQ Channel" src="https://img.shields.io/badge/QQ-blue?logo=tencentqq" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a target="_blank" href="https://huggingface.co/spaces/TTS-AGI/TTS-Arena-V2"&gt; &lt;img alt="TTS-Arena2 Score" src="https://img.shields.io/badge/TTS_Arena2-Rank_%231-gold?style=flat-square&amp;amp;logo=trophy&amp;amp;logoColor=white" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://huggingface.co/spaces/fishaudio/fish-speech-1"&gt; &lt;img alt="Huggingface" src="https://img.shields.io/badge/ğŸ¤—%20-space%20demo-yellow" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://huggingface.co/fishaudio/openaudio-s1-mini"&gt; &lt;img alt="HuggingFace Model" src="https://img.shields.io/badge/ğŸ¤—%20-models-orange" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;License Notice&lt;/strong&gt;&lt;br /&gt; This codebase is released under &lt;strong&gt;Apache License&lt;/strong&gt; and all model weights are released under &lt;strong&gt;CC-BY-NC-SA-4.0 License&lt;/strong&gt;. Please refer to &lt;a href="https://raw.githubusercontent.com/fishaudio/fish-speech/main/LICENSE"&gt;LICENSE&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] &lt;strong&gt;Legal Disclaimer&lt;/strong&gt;&lt;br /&gt; We do not hold any responsibility for any illegal usage of the codebase. Please refer to your local laws about DMCA and other related laws.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Start Here&lt;/h2&gt; 
&lt;p&gt;Here are the official documents for Fish Speech, follow the instructions to get started easily.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://speech.fish.audio/install/"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://speech.fish.audio/finetune/"&gt;Finetune&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://speech.fish.audio/inference/"&gt;Inference&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://speech.fish.audio/examples"&gt;Samples&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ‰ Announcement&lt;/h2&gt; 
&lt;p&gt;We are excited to announce that we have rebranded to &lt;strong&gt;OpenAudio&lt;/strong&gt; â€” introducing a revolutionary new series of advanced Text-to-Speech models that builds upon the foundation of Fish-Speech.&lt;/p&gt; 
&lt;p&gt;We are proud to release &lt;strong&gt;OpenAudio-S1&lt;/strong&gt; as the first model in this series, delivering significant improvements in quality, performance, and capabilities.&lt;/p&gt; 
&lt;p&gt;OpenAudio-S1 comes in two versions: &lt;strong&gt;OpenAudio-S1&lt;/strong&gt; and &lt;strong&gt;OpenAudio-S1-mini&lt;/strong&gt;. Both models are now available on &lt;a href="https://fish.audio"&gt;Fish Audio Playground&lt;/a&gt; (for &lt;strong&gt;OpenAudio-S1&lt;/strong&gt;) and &lt;a href="https://huggingface.co/fishaudio/openaudio-s1-mini"&gt;Hugging Face&lt;/a&gt; (for &lt;strong&gt;OpenAudio-S1-mini&lt;/strong&gt;).&lt;/p&gt; 
&lt;p&gt;Visit the &lt;a href="https://openaudio.com/blogs/s1"&gt;OpenAudio website&lt;/a&gt; for blog &amp;amp; tech report.&lt;/p&gt; 
&lt;h2&gt;Highlights âœ¨&lt;/h2&gt; 
&lt;h3&gt;&lt;strong&gt;Excellent TTS quality&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;We use Seed TTS Eval Metrics to evaluate the model performance, and the results show that OpenAudio S1 achieves &lt;strong&gt;0.008 WER&lt;/strong&gt; and &lt;strong&gt;0.004 CER&lt;/strong&gt; on English text, which is significantly better than previous models. (English, auto eval, based on OpenAI gpt-4o-transcribe, speaker distance using Revai/pyannote-wespeaker-voxceleb-resnet34-LM)&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Word Error Rate (WER)&lt;/th&gt; 
   &lt;th&gt;Character Error Rate (CER)&lt;/th&gt; 
   &lt;th&gt;Speaker Distance&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;S1&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.008&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.004&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.332&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;S1-mini&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.011&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.005&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.380&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;&lt;strong&gt;Best Model in TTS-Arena2&lt;/strong&gt; ğŸ†&lt;/h3&gt; 
&lt;p&gt;OpenAudio S1 has achieved the &lt;strong&gt;#1 ranking&lt;/strong&gt; on &lt;a href="https://arena.speechcolab.org/"&gt;TTS-Arena2&lt;/a&gt;, the benchmark for text-to-speech evaluation:&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/fishaudio/fish-speech/main/docs/assets/Elo.jpg" alt="TTS-Arena2 Ranking" style="width: 75%;" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;&lt;strong&gt;Speech Control&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;OpenAudio S1 &lt;strong&gt;supports a variety of emotional, tone, and special markers&lt;/strong&gt; to enhance speech synthesis:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Basic emotions&lt;/strong&gt;:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;(angry) (sad) (excited) (surprised) (satisfied) (delighted) 
(scared) (worried) (upset) (nervous) (frustrated) (depressed)
(empathetic) (embarrassed) (disgusted) (moved) (proud) (relaxed)
(grateful) (confident) (interested) (curious) (confused) (joyful)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced emotions&lt;/strong&gt;:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;(disdainful) (unhappy) (anxious) (hysterical) (indifferent) 
(impatient) (guilty) (scornful) (panicked) (furious) (reluctant)
(keen) (disapproving) (negative) (denying) (astonished) (serious)
(sarcastic) (conciliative) (comforting) (sincere) (sneering)
(hesitating) (yielding) (painful) (awkward) (amused)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Tone markers&lt;/strong&gt;:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;(in a hurry tone) (shouting) (screaming) (whispering) (soft tone)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Special audio effects&lt;/strong&gt;:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;(laughing) (chuckling) (sobbing) (crying loudly) (sighing) (panting)
(groaning) (crowd laughing) (background laughter) (audience laughing)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also use Ha,ha,ha to control, there's many other cases waiting to be explored by yourself.&lt;/p&gt; 
&lt;p&gt;(Support for English, Chinese and Japanese now, and more languages is coming soon!)&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;Two Type of Models&lt;/strong&gt;&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Size&lt;/th&gt; 
   &lt;th&gt;Availability&lt;/th&gt; 
   &lt;th&gt;Features&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;S1&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;4B parameters&lt;/td&gt; 
   &lt;td&gt;Avaliable on &lt;a href="https://fish.audio/"&gt;fish.audio&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Full-featured flagship model&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;S1-mini&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;0.5B parameters&lt;/td&gt; 
   &lt;td&gt;Avaliable on huggingface &lt;a href="https://huggingface.co/spaces/fishaudio/openaudio-s1-mini"&gt;hf space&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Distilled version with core capabilities&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Both S1 and S1-mini incorporate online Reinforcement Learning from Human Feedback (RLHF).&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zero-shot &amp;amp; Few-shot TTS:&lt;/strong&gt; Input a 10 to 30-second vocal sample to generate high-quality TTS output. &lt;strong&gt;For detailed guidelines, see &lt;a href="https://docs.fish.audio/resources/best-practices/voice-cloning"&gt;Voice Cloning Best Practices&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multilingual &amp;amp; Cross-lingual Support:&lt;/strong&gt; Simply copy and paste multilingual text into the input boxâ€”no need to worry about the language. Currently supports English, Japanese, Korean, Chinese, French, German, Arabic, and Spanish.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;No Phoneme Dependency:&lt;/strong&gt; The model has strong generalization capabilities and does not rely on phonemes for TTS. It can handle text in any language script.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Highly Accurate:&lt;/strong&gt; Achieves a low CER (Character Error Rate) of around 0.4% and WER (Word Error Rate) of around 0.8% for Seed-TTS Eval.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Fast:&lt;/strong&gt; Accelerated by torch compile, the real-time factor is approximately 1:7 on an Nvidia RTX 4090 GPU.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;WebUI Inference:&lt;/strong&gt; Features an easy-to-use, Gradio-based web UI compatible with Chrome, Firefox, Edge, and other browsers.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deploy-Friendly:&lt;/strong&gt; Easily set up an inference server with native support for Linux and Windows (macOS support coming soon), minimizing performance loss.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;&lt;strong&gt;Media &amp;amp; Demos&lt;/strong&gt;&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;&lt;strong&gt;Social Media&lt;/strong&gt;&lt;/h3&gt; 
 &lt;a href="https://x.com/FishAudio/status/1929915992299450398" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/ğ•-Latest_Demo-black?style=for-the-badge&amp;amp;logo=x&amp;amp;logoColor=white" alt="Latest Demo on X" /&gt; &lt;/a&gt; 
 &lt;h3&gt;&lt;strong&gt;Interactive Demos&lt;/strong&gt;&lt;/h3&gt; 
 &lt;a href="https://fish.audio" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/Fish_Audio-Try_OpenAudio_S1-blue?style=for-the-badge" alt="Try OpenAudio S1" /&gt; &lt;/a&gt; 
 &lt;a href="https://huggingface.co/spaces/fishaudio/openaudio-s1-mini" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/Hugging_Face-Try_S1_Mini-yellow?style=for-the-badge" alt="Try S1 Mini" /&gt; &lt;/a&gt; 
 &lt;h3&gt;&lt;strong&gt;Video Showcases&lt;/strong&gt;&lt;/h3&gt; 
 &lt;a href="https://www.youtube.com/watch?v=SYuPvd7m06A" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/fishaudio/fish-speech/main/docs/assets/Thumbnail.jpg" alt="OpenAudio S1 Video" style="width: 50%;" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/daniilrobnikov/vits2"&gt;VITS2 (daniilrobnikov)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/fishaudio/Bert-VITS2"&gt;Bert-VITS2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/innnky/gpt-vits"&gt;GPT VITS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/b04901014/MQTTS"&gt;MQTTS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pytorch-labs/gpt-fast"&gt;GPT Fast&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/RVC-Boss/GPT-SoVITS"&gt;GPT-SoVITS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/QwenLM/Qwen3"&gt;Qwen3&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Tech Report (V1.4)&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{fish-speech-v1.4,
      title={Fish-Speech: Leveraging Large Language Models for Advanced Multilingual Text-to-Speech Synthesis},
      author={Shijia Liao and Yuxuan Wang and Tianyu Li and Yifan Cheng and Ruoyi Zhang and Rongzhi Zhou and Yijin Xing},
      year={2024},
      eprint={2411.01156},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2411.01156},
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>droidrun/droidrun</title>
      <link>https://github.com/droidrun/droidrun</link>
      <description>&lt;p&gt;Automate your mobile devices with natural language commands - an LLM agnostic mobile Agent ğŸ¤–&lt;/p&gt;&lt;hr&gt;&lt;picture align="center"&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="./static/droidrun-dark.png" /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="./static/droidrun.png" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/droidrun/droidrun/main/static/droidrun.png" width="full" /&gt; 
&lt;/picture&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://docs.droidrun.ai"&gt;&lt;img src="https://img.shields.io/badge/Docs-%F0%9F%93%95-0D9373?style=for-the-badge" alt="Docs" /&gt;&lt;/a&gt; &lt;a href="http://cloud.droidrun.ai"&gt;&lt;img src="https://img.shields.io/badge/Cloud-%E2%98%81%EF%B8%8F-0D9373?style=for-the-badge" alt="Cloud" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/droidrun/droidrun/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/droidrun/droidrun?style=social" alt="GitHub stars" /&gt;&lt;/a&gt; &lt;a href="https://droidrun.ai"&gt;&lt;img src="https://img.shields.io/badge/droidrun.ai-white" alt="droidrun.ai" /&gt;&lt;/a&gt; &lt;a href="https://x.com/droid_run"&gt;&lt;img src="https://img.shields.io/twitter/follow/droid_run?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/ZZbKEZZkwK"&gt;&lt;img src="https://img.shields.io/discord/1360219330318696488?color=white&amp;amp;label=Discord&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://droidrun.ai/benchmark"&gt;&lt;img src="https://img.shields.io/badge/Benchmark-91.4%EF%B9%AA-white" alt="Benchmark" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=983810&amp;amp;theme=dark&amp;amp;period=daily&amp;amp;t=1753948032207" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=983810&amp;amp;theme=neutral&amp;amp;period=daily&amp;amp;t=1753948125523" /&gt; 
  &lt;a href="https://www.producthunt.com/products/droidrun-framework-for-mobile-agent?embed=true&amp;amp;utm_source=badge-top-post-badge&amp;amp;utm_medium=badge&amp;amp;utm_source=badge-droidrun" target="_blank"&gt;&lt;img src="https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=983810&amp;amp;theme=neutral&amp;amp;period=daily&amp;amp;t=1753948125523" alt="Droidrun - Give AI native control of physical &amp;amp; virtual phones. | Product Hunt" style="width: 200px; height: 54px;" width="200" height="54" /&gt;&lt;/a&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;p&gt;DroidRun is a powerful framework for controlling Android and iOS devices through LLM agents. It allows you to automate device interactions using natural language commands. &lt;a href="https://droidrun.ai/benchmark"&gt;Checkout our benchmark results&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Why Droidrun?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ¤– Control Android and iOS devices with natural language commands&lt;/li&gt; 
 &lt;li&gt;ğŸ”€ Supports multiple LLM providers (OpenAI, Anthropic, Gemini, Ollama, DeepSeek)&lt;/li&gt; 
 &lt;li&gt;ğŸ§  Planning capabilities for complex multi-step tasks&lt;/li&gt; 
 &lt;li&gt;ğŸ’» Easy to use CLI with enhanced debugging features&lt;/li&gt; 
 &lt;li&gt;ğŸ Extendable Python API for custom automations&lt;/li&gt; 
 &lt;li&gt;ğŸ“¸ Screenshot analysis for visual understanding of the device&lt;/li&gt; 
 &lt;li&gt;ğŸ«† Execution tracing with Arize Phoenix&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“¦ Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install 'droidrun[google,anthropic,openai,deepseek,ollama,dev]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸš€ Quickstart&lt;/h2&gt; 
&lt;p&gt;Read on how to get droidrun up and running within seconds in &lt;a href="https://docs.droidrun.ai/v3/quickstart"&gt;our docs&lt;/a&gt;!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=4WT7FXJah2I"&gt;&lt;img src="https://img.youtube.com/vi/4WT7FXJah2I/0.jpg" alt="Quickstart Video" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ¬ Demo Videos&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Accommodation booking&lt;/strong&gt;: Let Droidrun search for an apartment for you&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/VUpCyq1PSXw"&gt;&lt;img src="https://img.youtube.com/vi/VUpCyq1PSXw/0.jpg" alt="Droidrun Accommodation Booking Demo" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;br /&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Trend Hunter&lt;/strong&gt;: Let Droidrun hunt down trending posts&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/7V8S2f8PnkQ"&gt;&lt;img src="https://img.youtube.com/vi/7V8S2f8PnkQ/0.jpg" alt="Droidrun Trend Hunter Demo" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;br /&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Streak Saver&lt;/strong&gt;: Let Droidrun save your streak on your favorite language learning app&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/B5q2B467HKw"&gt;&lt;img src="https://img.youtube.com/vi/B5q2B467HKw/0.jpg" alt="Droidrun Streak Saver Demo" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ’¡ Example Use Cases&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Automated UI testing of mobile applications&lt;/li&gt; 
 &lt;li&gt;Creating guided workflows for non-technical users&lt;/li&gt; 
 &lt;li&gt;Automating repetitive tasks on mobile devices&lt;/li&gt; 
 &lt;li&gt;Remote assistance for less technical users&lt;/li&gt; 
 &lt;li&gt;Exploring mobile UI with natural language commands&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ‘¥ Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! Please feel free to submit a Pull Request.&lt;/p&gt; 
&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt; 
&lt;h2&gt;Security Checks&lt;/h2&gt; 
&lt;p&gt;To ensure the security of the codebase, we have integrated security checks using &lt;code&gt;bandit&lt;/code&gt; and &lt;code&gt;safety&lt;/code&gt;. These tools help identify potential security issues in the code and dependencies.&lt;/p&gt; 
&lt;h3&gt;Running Security Checks&lt;/h3&gt; 
&lt;p&gt;Before submitting any code, please run the following security checks:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bandit&lt;/strong&gt;: A tool to find common security issues in Python code.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;bandit -r droidrun
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Safety&lt;/strong&gt;: A tool to check your installed dependencies for known security vulnerabilities.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;safety scan
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt;</description>
    </item>
    
    <item>
      <title>topoteretes/cognee</title>
      <link>https://github.com/topoteretes/cognee</link>
      <description>&lt;p&gt;Memory for AI Agents in 6 lines of code&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://github.com/topoteretes/cognee"&gt; &lt;img src="https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/dev/assets/cognee-logo-transparent.png" alt="Cognee Logo" height="60" /&gt; &lt;/a&gt; 
 &lt;br /&gt; 
 &lt;p&gt;Cognee - Accurate and Persistent AI Memory&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://www.youtube.com/watch?v=1bezuvLwJmw&amp;amp;t=2s"&gt;Demo&lt;/a&gt; . &lt;a href="https://docs.cognee.ai/"&gt;Docs&lt;/a&gt; . &lt;a href="https://cognee.ai"&gt;Learn More&lt;/a&gt; Â· &lt;a href="https://discord.gg/NQPKmU5CCg"&gt;Join Discord&lt;/a&gt; Â· &lt;a href="https://www.reddit.com/r/AIMemory/"&gt;Join r/AIMemory&lt;/a&gt; . &lt;a href="https://github.com/topoteretes/cognee-community"&gt;Community Plugins &amp;amp; Add-ons&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://GitHub.com/topoteretes/cognee/network/"&gt;&lt;img src="https://img.shields.io/github/forks/topoteretes/cognee.svg?style=social&amp;amp;label=Fork&amp;amp;maxAge=2592000" alt="GitHub forks" /&gt;&lt;/a&gt; &lt;a href="https://GitHub.com/topoteretes/cognee/stargazers/"&gt;&lt;img src="https://img.shields.io/github/stars/topoteretes/cognee.svg?style=social&amp;amp;label=Star&amp;amp;maxAge=2592000" alt="GitHub stars" /&gt;&lt;/a&gt; &lt;a href="https://GitHub.com/topoteretes/cognee/commit/"&gt;&lt;img src="https://badgen.net/github/commits/topoteretes/cognee" alt="GitHub commits" /&gt;&lt;/a&gt; &lt;a href="https://github.com/topoteretes/cognee/tags/"&gt;&lt;img src="https://badgen.net/github/tag/topoteretes/cognee" alt="GitHub tag" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/cognee"&gt;&lt;img src="https://static.pepy.tech/badge/cognee" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/topoteretes/cognee/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/topoteretes/cognee?colorA=00C586&amp;amp;colorB=000000" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/topoteretes/cognee/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/topoteretes/cognee?colorA=00C586&amp;amp;colorB=000000" alt="Contributors" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sponsors/topoteretes"&gt;&lt;img src="https://img.shields.io/badge/Sponsor-â¤ï¸-ff69b4.svg" alt="Sponsor" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt; &lt;a href="https://www.producthunt.com/posts/cognee?embed=true&amp;amp;utm_source=badge-top-post-badge&amp;amp;utm_medium=badge&amp;amp;utm_souce=badge-cognee" target="_blank" style="display:inline-block; margin-right:10px;"&gt; &lt;img src="https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=946346&amp;amp;theme=light&amp;amp;period=daily&amp;amp;t=1744472480704" alt="cognee - Memory for AI Agents  in 5 lines of code | Product Hunt" width="250" height="54" /&gt; &lt;/a&gt; &lt;a href="https://trendshift.io/repositories/13955" target="_blank" style="display:inline-block;"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/13955" alt="topoteretes%2Fcognee | Trendshift" width="250" height="55" /&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt;Use your data to build personalized and dynamic memory for AI Agents. Cognee lets you replace RAG with scalable and modular ECL (Extract, Cognify, Load) pipelines.&lt;/p&gt; 
 &lt;p align="center"&gt; ğŸŒ Available Languages : 
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; &lt;a href="https://www.readme-i18n.com/topoteretes/cognee?lang=de"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/topoteretes/cognee?lang=es"&gt;EspaÃ±ol&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/topoteretes/cognee?lang=fr"&gt;FranÃ§ais&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/topoteretes/cognee?lang=ja"&gt;æ—¥æœ¬èª&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/topoteretes/cognee?lang=ko"&gt;í•œêµ­ì–´&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/topoteretes/cognee?lang=pt"&gt;PortuguÃªs&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/topoteretes/cognee?lang=ru"&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/topoteretes/cognee?lang=zh"&gt;ä¸­æ–‡&lt;/a&gt; &lt;/p&gt; 
 &lt;div style="text-align: center"&gt; 
  &lt;img src="https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_benefits.png" alt="Why cognee?" width="50%" /&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;h2&gt;About Cognee&lt;/h2&gt; 
&lt;p&gt;Cognee is an open-source tool and platform that transforms your raw data into persistent and dynamic AI memory for Agents. It combines vector search with graph databases to make your documents both searchable by meaning and connected by relationships.&lt;/p&gt; 
&lt;p&gt;You can use Cognee in two ways:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://docs.cognee.ai/getting-started/installation"&gt;Self-host Cognee Open Source&lt;/a&gt;, which stores all data locally by default.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://platform.cognee.ai/"&gt;Connect to Cognee Cloud&lt;/a&gt;, and get the same OSS stack on managed infrastructure for easier development and productionization.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Cognee Open Source (self-hosted):&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Interconnects any type of data â€” including past conversations, files, images, and audio transcriptions&lt;/li&gt; 
 &lt;li&gt;Replaces traditional RAG systems with a unified memory layer built on graphs and vectors&lt;/li&gt; 
 &lt;li&gt;Reduces developer effort and infrastructure cost while improving quality and precision&lt;/li&gt; 
 &lt;li&gt;Provides Pythonic data pipelines for ingestion from 30+ data sources&lt;/li&gt; 
 &lt;li&gt;Offers high customizability through user-defined tasks, modular pipelines, and built-in search endpoints&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Cognee Cloud (managed):&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Hosted web UI dashboard&lt;/li&gt; 
 &lt;li&gt;Automatic version updates&lt;/li&gt; 
 &lt;li&gt;Resource usage analytics&lt;/li&gt; 
 &lt;li&gt;GDPR compliant, enterprise-grade security&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Basic Usage &amp;amp; Feature Guide&lt;/h2&gt; 
&lt;p&gt;To learn more, &lt;a href="https://colab.research.google.com/drive/12Vi9zID-M3fpKpKiaqDBvkk98ElkRPWy?usp=sharing"&gt;check out this short, end-to-end Colab walkthrough&lt;/a&gt; of Cognee's core features.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://colab.research.google.com/drive/12Vi9zID-M3fpKpKiaqDBvkk98ElkRPWy?usp=sharing"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;Letâ€™s try Cognee in just a few lines of code. For detailed setup and configuration, see the &lt;a href="https://docs.cognee.ai/getting-started/installation#environment-configuration"&gt;Cognee Docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10 to 3.13&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Step 1: Install Cognee&lt;/h3&gt; 
&lt;p&gt;You can install Cognee with &lt;strong&gt;pip&lt;/strong&gt;, &lt;strong&gt;poetry&lt;/strong&gt;, &lt;strong&gt;uv&lt;/strong&gt;, or your preferred Python package manager.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv pip install cognee
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Step 2: Configure the LLM&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
os.environ["LLM_API_KEY"] = "YOUR OPENAI_API_KEY"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, create a &lt;code&gt;.env&lt;/code&gt; file using our &lt;a href="https://github.com/topoteretes/cognee/raw/main/.env.template"&gt;template&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To integrate other LLM providers, see our &lt;a href="https://docs.cognee.ai/setup-configuration/llm-providers"&gt;LLM Provider Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Step 3: Run the Pipeline&lt;/h3&gt; 
&lt;p&gt;Cognee will take your documents, generate a knowledge graph from them and then query the graph based on combined relationships.&lt;/p&gt; 
&lt;p&gt;Now, run a minimal pipeline:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import cognee
import asyncio


async def main():
    # Add text to cognee
    await cognee.add("Cognee turns documents into AI memory.")

    # Generate the knowledge graph
    await cognee.cognify()

    # Add memory algorithms to the graph
    await cognee.memify()

    # Query the knowledge graph
    results = await cognee.search("What does Cognee do?")

    # Display the results
    for result in results:
        print(result)


if __name__ == '__main__':
    asyncio.run(main())

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;As you can see, the output is generated from the document we previously stored in Cognee:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;  Cognee turns documents into AI memory.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Use the Cognee CLI&lt;/h3&gt; 
&lt;p&gt;As an alternative, you can get started with these essential commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cognee-cli add "Cognee turns documents into AI memory."

cognee-cli cognify

cognee-cli search "What does Cognee do?"
cognee-cli delete --all

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To open the local UI, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cognee-cli -ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Demos &amp;amp; Examples&lt;/h2&gt; 
&lt;p&gt;See Cognee in action:&lt;/p&gt; 
&lt;h3&gt;Persistent Agent Memory&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/e113b628-7212-4a2b-b288-0be39a93a1c3"&gt;Cognee Memory for LangGraph Agents&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Simple GraphRAG&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/f2186b2e-305a-42b0-9c2d-9f4473f15df8"&gt;Watch Demo&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Cognee with Ollama&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/39672858-f774-4136-b957-1e2de67b8981"&gt;Watch Demo&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Community &amp;amp; Support&lt;/h2&gt; 
&lt;h3&gt;Contributing&lt;/h3&gt; 
&lt;p&gt;We welcome contributions from the community! Your input helps make Cognee better for everyone. See &lt;a href="https://raw.githubusercontent.com/topoteretes/cognee/main/CONTRIBUTING.md"&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;h3&gt;Code of Conduct&lt;/h3&gt; 
&lt;p&gt;We're committed to fostering an inclusive and respectful community. Read our &lt;a href="https://github.com/topoteretes/cognee/raw/main/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt; for guidelines.&lt;/p&gt; 
&lt;h2&gt;Research &amp;amp; Citation&lt;/h2&gt; 
&lt;p&gt;We recently published a research paper on optimizing knowledge graphs for LLM reasoning:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{markovic2025optimizinginterfaceknowledgegraphs,
      title={Optimizing the Interface Between Knowledge Graphs and LLMs for Complex Reasoning},
      author={Vasilije Markovic and Lazar Obradovic and Laszlo Hajdu and Jovan Pavlovic},
      year={2025},
      eprint={2505.24478},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2505.24478},
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>huggingface/lerobot</title>
      <link>https://github.com/huggingface/lerobot</link>
      <description>&lt;p&gt;ğŸ¤— LeRobot: Making AI for Robotics more accessible with end-to-end learning&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img alt="LeRobot, Hugging Face Robotics Library" src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/lerobot-logo-thumbnail.png" width="100%" /&gt; &lt;br /&gt; &lt;br /&gt; &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/huggingface/lerobot/actions/workflows/nightly.yml?query=branch%3Amain"&gt;&lt;img src="https://github.com/huggingface/lerobot/actions/workflows/nightly.yml/badge.svg?branch=main" alt="Tests" /&gt;&lt;/a&gt; &lt;a href="https://www.python.org/downloads/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/lerobot" alt="Python versions" /&gt;&lt;/a&gt; &lt;a href="https://github.com/huggingface/lerobot/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/lerobot/"&gt;&lt;img src="https://img.shields.io/pypi/status/lerobot" alt="Status" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/lerobot/"&gt;&lt;img src="https://img.shields.io/pypi/v/lerobot" alt="Version" /&gt;&lt;/a&gt; &lt;a href="https://github.com/huggingface/lerobot/raw/main/CODE_OF_CONDUCT.md"&gt;&lt;img src="https://img.shields.io/badge/Contributor%20Covenant-v2.1-ff69b4.svg?sanitize=true" alt="Contributor Covenant" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/s3KuuzsPFb"&gt;&lt;img src="https://dcbadge.vercel.app/api/server/C5P34WJ68S?style=flat" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;!-- [![Coverage](https://codecov.io/gh/huggingface/lerobot/branch/main/graph/badge.svg?token=TODO)](https://codecov.io/gh/huggingface/lerobot) --&gt; 
&lt;/div&gt; 
&lt;h2 align="center"&gt; &lt;p&gt;&lt;a href="https://huggingface.co/docs/lerobot/hope_jr"&gt; Build Your Own HopeJR Robot!&lt;/a&gt;&lt;/p&gt; &lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/hope_jr/hopejr.png" alt="HopeJR robot" title="HopeJR robot" width="60%" /&gt; 
 &lt;p&gt;&lt;strong&gt;Meet HopeJR â€“ A humanoid robot arm and hand for dexterous manipulation!&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Control it with exoskeletons and gloves for precise hand movements.&lt;/p&gt; 
 &lt;p&gt;Perfect for advanced manipulation tasks! ğŸ¤–&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://huggingface.co/docs/lerobot/hope_jr"&gt; See the full HopeJR tutorial here.&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2 align="center"&gt; &lt;p&gt;&lt;a href="https://huggingface.co/docs/lerobot/so101"&gt; Build Your Own SO-101 Robot!&lt;/a&gt;&lt;/p&gt; &lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/so101/so101.webp" alt="SO-101 follower arm" title="SO-101 follower arm" width="90%" /&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/so101/so101-leader.webp" alt="SO-101 leader arm" title="SO-101 leader arm" width="90%" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;p&gt;&lt;strong&gt;Meet the updated SO100, the SO-101 â€“ Just â‚¬114 per arm!&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Train it in minutes with a few simple moves on your laptop.&lt;/p&gt; 
 &lt;p&gt;Then sit back and watch your creation act autonomously! ğŸ¤¯&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://huggingface.co/docs/lerobot/so101"&gt; See the full SO-101 tutorial here.&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;Want to take it to the next level? Make your SO-101 mobile by building LeKiwi!&lt;/p&gt; 
 &lt;p&gt;Check out the &lt;a href="https://huggingface.co/docs/lerobot/lekiwi"&gt;LeKiwi tutorial&lt;/a&gt; and bring your robot to life on wheels.&lt;/p&gt; 
 &lt;img src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/lekiwi/kiwi.webp" alt="LeKiwi mobile robot" title="LeKiwi mobile robot" width="50%" /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h3 align="center"&gt; &lt;p&gt;LeRobot: State-of-the-art AI for real-world robotics&lt;/p&gt; &lt;/h3&gt; 
&lt;hr /&gt; 
&lt;p&gt;ğŸ¤— LeRobot aims to provide models, datasets, and tools for real-world robotics in PyTorch. The goal is to lower the barrier to entry to robotics so that everyone can contribute and benefit from sharing datasets and pretrained models.&lt;/p&gt; 
&lt;p&gt;ğŸ¤— LeRobot contains state-of-the-art approaches that have been shown to transfer to the real-world with a focus on imitation learning and reinforcement learning.&lt;/p&gt; 
&lt;p&gt;ğŸ¤— LeRobot already provides a set of pretrained models, datasets with human collected demonstrations, and simulation environments to get started without assembling a robot. In the coming weeks, the plan is to add more and more support for real-world robotics on the most affordable and capable robots out there.&lt;/p&gt; 
&lt;p&gt;ğŸ¤— LeRobot hosts pretrained models and datasets on this Hugging Face community page: &lt;a href="https://huggingface.co/lerobot"&gt;huggingface.co/lerobot&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Examples of pretrained models on simulation environments&lt;/h4&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/gym/aloha_act.gif" width="100%" alt="ACT policy on ALOHA env" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/gym/simxarm_tdmpc.gif" width="100%" alt="TDMPC policy on SimXArm env" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/gym/pusht_diffusion.gif" width="100%" alt="Diffusion policy on PushT env" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;ACT policy on ALOHA env&lt;/td&gt; 
   &lt;td align="center"&gt;TDMPC policy on SimXArm env&lt;/td&gt; 
   &lt;td align="center"&gt;Diffusion policy on PushT env&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;LeRobot works with Python 3.10+ and PyTorch 2.2+.&lt;/p&gt; 
&lt;h3&gt;Environment Setup&lt;/h3&gt; 
&lt;p&gt;Create a virtual environment with Python 3.10 and activate it, e.g. with &lt;a href="https://conda-forge.org/download/"&gt;&lt;code&gt;miniforge&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -y -n lerobot python=3.10
conda activate lerobot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When using &lt;code&gt;conda&lt;/code&gt;, install &lt;code&gt;ffmpeg&lt;/code&gt; in your environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda install ffmpeg -c conda-forge
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; This usually installs &lt;code&gt;ffmpeg 7.X&lt;/code&gt; for your platform compiled with the &lt;code&gt;libsvtav1&lt;/code&gt; encoder. If &lt;code&gt;libsvtav1&lt;/code&gt; is not supported (check supported encoders with &lt;code&gt;ffmpeg -encoders&lt;/code&gt;), you can:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;em&gt;[On any platform]&lt;/em&gt; Explicitly install &lt;code&gt;ffmpeg 7.X&lt;/code&gt; using:&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;conda install ffmpeg=7.1.1 -c conda-forge
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;em&gt;[On Linux only]&lt;/em&gt; Install &lt;a href="https://trac.ffmpeg.org/wiki/CompilationGuide/Ubuntu#GettheDependencies"&gt;ffmpeg build dependencies&lt;/a&gt; and &lt;a href="https://trac.ffmpeg.org/wiki/CompilationGuide/Ubuntu#libsvtav1"&gt;compile ffmpeg from source with libsvtav1&lt;/a&gt;, and make sure you use the corresponding ffmpeg binary to your install with &lt;code&gt;which ffmpeg&lt;/code&gt;.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Install LeRobot ğŸ¤—&lt;/h3&gt; 
&lt;h4&gt;From Source&lt;/h4&gt; 
&lt;p&gt;First, clone the repository and navigate into the directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/huggingface/lerobot.git
cd lerobot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, install the library in editable mode. This is useful if you plan to contribute to the code.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; If you encounter build errors, you may need to install additional dependencies (&lt;code&gt;cmake&lt;/code&gt;, &lt;code&gt;build-essential&lt;/code&gt;, and &lt;code&gt;ffmpeg libs&lt;/code&gt;). On Linux, run: &lt;code&gt;sudo apt-get install cmake build-essential python3-dev pkg-config libavformat-dev libavcodec-dev libavdevice-dev libavutil-dev libswscale-dev libswresample-dev libavfilter-dev&lt;/code&gt;. For other systems, see: &lt;a href="https://pyav.org/docs/develop/overview/installation.html#bring-your-own-ffmpeg"&gt;Compiling PyAV&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;For simulations, ğŸ¤— LeRobot comes with gymnasium environments that can be installed as extras:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/gym-aloha"&gt;aloha&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/gym-xarm"&gt;xarm&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/gym-pusht"&gt;pusht&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For instance, to install ğŸ¤— LeRobot with aloha and pusht, use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e ".[aloha, pusht]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Installation from PyPI&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Core Library:&lt;/strong&gt; Install the base package with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install lerobot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;This installs only the default dependencies.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Extra Features:&lt;/strong&gt; To install additional functionality, use one of the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install 'lerobot[all]'          # All available features
pip install 'lerobot[aloha,pusht]'  # Specific features (Aloha &amp;amp; Pusht)
pip install 'lerobot[feetech]'      # Feetech motor support
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Replace &lt;code&gt;[...]&lt;/code&gt; with your desired features.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Available Tags:&lt;/strong&gt; For a full list of optional dependencies, see: &lt;a href="https://pypi.org/project/lerobot/"&gt;https://pypi.org/project/lerobot/&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] For lerobot 0.4.0, if you want to install libero or pi tags, you will have to do: &lt;code&gt;pip install "lerobot[pi,libero]@git+https://github.com/huggingface/lerobot.git"&lt;/code&gt;.&lt;/p&gt; 
 &lt;p&gt;This will be solved in the next patch release&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Weights &amp;amp; Biases&lt;/h3&gt; 
&lt;p&gt;To use &lt;a href="https://docs.wandb.ai/quickstart"&gt;Weights and Biases&lt;/a&gt; for experiment tracking, log in with&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;wandb login
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;(note: you will also need to enable WandB in the configuration. See below.)&lt;/p&gt; 
&lt;h3&gt;Visualize datasets&lt;/h3&gt; 
&lt;p&gt;Check out &lt;a href="https://github.com/huggingface/lerobot/raw/main/examples/dataset/load_lerobot_dataset.py"&gt;example 1&lt;/a&gt; that illustrates how to use our dataset class which automatically downloads data from the Hugging Face hub.&lt;/p&gt; 
&lt;p&gt;You can also locally visualize episodes from a dataset on the hub by executing our script from the command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;lerobot-dataset-viz \
    --repo-id lerobot/pusht \
    --episode-index 0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or from a dataset in a local folder with the &lt;code&gt;root&lt;/code&gt; option and the &lt;code&gt;--mode local&lt;/code&gt; (in the following case the dataset will be searched for in &lt;code&gt;./my_local_data_dir/lerobot/pusht&lt;/code&gt;)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;lerobot-dataset-viz \
    --repo-id lerobot/pusht \
    --root ./my_local_data_dir \
    --mode local \
    --episode-index 0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It will open &lt;code&gt;rerun.io&lt;/code&gt; and display the camera streams, robot states and actions, like this:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github-production-user-asset-6210df.s3.amazonaws.com/4681518/328035972-fd46b787-b532-47e2-bb6f-fd536a55a7ed.mov?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;amp;X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&amp;amp;X-Amz-Date=20240505T172924Z&amp;amp;X-Amz-Expires=300&amp;amp;X-Amz-Signature=d680b26c532eeaf80740f08af3320d22ad0b8a4e4da1bcc4f33142c15b509eda&amp;amp;X-Amz-SignedHeaders=host&amp;amp;actor_id=24889239&amp;amp;key_id=0&amp;amp;repo_id=748713144"&gt;https://github-production-user-asset-6210df.s3.amazonaws.com/4681518/328035972-fd46b787-b532-47e2-bb6f-fd536a55a7ed.mov?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;amp;X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&amp;amp;X-Amz-Date=20240505T172924Z&amp;amp;X-Amz-Expires=300&amp;amp;X-Amz-Signature=d680b26c532eeaf80740f08af3320d22ad0b8a4e4da1bcc4f33142c15b509eda&amp;amp;X-Amz-SignedHeaders=host&amp;amp;actor_id=24889239&amp;amp;key_id=0&amp;amp;repo_id=748713144&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Our script can also visualize datasets stored on a distant server. See &lt;code&gt;lerobot-dataset-viz --help&lt;/code&gt; for more instructions.&lt;/p&gt; 
&lt;h3&gt;The &lt;code&gt;LeRobotDataset&lt;/code&gt; format&lt;/h3&gt; 
&lt;p&gt;A dataset in &lt;code&gt;LeRobotDataset&lt;/code&gt; format is very simple to use. It can be loaded from a repository on the Hugging Face hub or a local folder simply with e.g. &lt;code&gt;dataset = LeRobotDataset("lerobot/aloha_static_coffee")&lt;/code&gt; and can be indexed into like any Hugging Face and PyTorch dataset. For instance &lt;code&gt;dataset[0]&lt;/code&gt; will retrieve a single temporal frame from the dataset containing observation(s) and an action as PyTorch tensors ready to be fed to a model.&lt;/p&gt; 
&lt;p&gt;A specificity of &lt;code&gt;LeRobotDataset&lt;/code&gt; is that, rather than retrieving a single frame by its index, we can retrieve several frames based on their temporal relationship with the indexed frame, by setting &lt;code&gt;delta_timestamps&lt;/code&gt; to a list of relative times with respect to the indexed frame. For example, with &lt;code&gt;delta_timestamps = {"observation.image": [-1, -0.5, -0.2, 0]}&lt;/code&gt; one can retrieve, for a given index, 4 frames: 3 "previous" frames 1 second, 0.5 seconds, and 0.2 seconds before the indexed frame, and the indexed frame itself (corresponding to the 0 entry). See example &lt;a href="https://github.com/huggingface/lerobot/raw/main/examples/dataset/load_lerobot_dataset.py"&gt;1_load_lerobot_dataset.py&lt;/a&gt; for more details on &lt;code&gt;delta_timestamps&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Under the hood, the &lt;code&gt;LeRobotDataset&lt;/code&gt; format makes use of several ways to serialize data which can be useful to understand if you plan to work more closely with this format. We tried to make a flexible yet simple dataset format that would cover most type of features and specificities present in reinforcement learning and robotics, in simulation and in real-world, with a focus on cameras and robot states but easily extended to other types of sensory inputs as long as they can be represented by a tensor.&lt;/p&gt; 
&lt;p&gt;Here are the important details and internal structure organization of a typical &lt;code&gt;LeRobotDataset&lt;/code&gt; instantiated with &lt;code&gt;dataset = LeRobotDataset("lerobot/aloha_static_coffee")&lt;/code&gt;. The exact features will change from dataset to dataset but not the main aspects:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;dataset attributes:
  â”œ hf_dataset: a Hugging Face dataset (backed by Arrow/parquet). Typical features example:
  â”‚  â”œ observation.images.cam_high (VideoFrame):
  â”‚  â”‚   VideoFrame = {'path': path to a mp4 video, 'timestamp' (float32): timestamp in the video}
  â”‚  â”œ observation.state (list of float32): position of an arm joints (for instance)
  â”‚  ... (more observations)
  â”‚  â”œ action (list of float32): goal position of an arm joints (for instance)
  â”‚  â”œ episode_index (int64): index of the episode for this sample
  â”‚  â”œ frame_index (int64): index of the frame for this sample in the episode ; starts at 0 for each episode
  â”‚  â”œ timestamp (float32): timestamp in the episode
  â”‚  â”œ next.done (bool): indicates the end of an episode ; True for the last frame in each episode
  â”‚  â”” index (int64): general index in the whole dataset
  â”œ meta: a LeRobotDatasetMetadata object containing:
  â”‚  â”œ info: a dictionary of metadata on the dataset
  â”‚  â”‚  â”œ codebase_version (str): this is to keep track of the codebase version the dataset was created with
  â”‚  â”‚  â”œ fps (int): frame per second the dataset is recorded/synchronized to
  â”‚  â”‚  â”œ features (dict): all features contained in the dataset with their shapes and types
  â”‚  â”‚  â”œ total_episodes (int): total number of episodes in the dataset
  â”‚  â”‚  â”œ total_frames (int): total number of frames in the dataset
  â”‚  â”‚  â”œ robot_type (str): robot type used for recording
  â”‚  â”‚  â”œ data_path (str): formattable string for the parquet files
  â”‚  â”‚  â”” video_path (str): formattable string for the video files (if using videos)
  â”‚  â”œ episodes: a DataFrame containing episode metadata with columns:
  â”‚  â”‚  â”œ episode_index (int): index of the episode
  â”‚  â”‚  â”œ tasks (list): list of tasks for this episode
  â”‚  â”‚  â”œ length (int): number of frames in this episode
  â”‚  â”‚  â”œ dataset_from_index (int): start index of this episode in the dataset
  â”‚  â”‚  â”” dataset_to_index (int): end index of this episode in the dataset
  â”‚  â”œ stats: a dictionary of statistics (max, mean, min, std) for each feature in the dataset, for instance
  â”‚  â”‚  â”œ observation.images.front_cam: {'max': tensor with same number of dimensions (e.g. `(c, 1, 1)` for images, `(c,)` for states), etc.}
  â”‚  â”‚  â”” ...
  â”‚  â”” tasks: a DataFrame containing task information with task names as index and task_index as values
  â”œ root (Path): local directory where the dataset is stored
  â”œ image_transforms (Callable): optional image transformations to apply to visual modalities
  â”” delta_timestamps (dict): optional delta timestamps for temporal queries
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;A &lt;code&gt;LeRobotDataset&lt;/code&gt; is serialised using several widespread file formats for each of its parts, namely:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;hf_dataset stored using Hugging Face datasets library serialization to parquet&lt;/li&gt; 
 &lt;li&gt;videos are stored in mp4 format to save space&lt;/li&gt; 
 &lt;li&gt;metadata are stored in plain json/jsonl files&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Dataset can be uploaded/downloaded from the HuggingFace hub seamlessly. To work on a local dataset, you can specify its location with the &lt;code&gt;root&lt;/code&gt; argument if it's not in the default &lt;code&gt;~/.cache/huggingface/lerobot&lt;/code&gt; location.&lt;/p&gt; 
&lt;h4&gt;Reproduce state-of-the-art (SOTA)&lt;/h4&gt; 
&lt;p&gt;We provide some pretrained policies on our &lt;a href="https://huggingface.co/lerobot"&gt;hub page&lt;/a&gt; that can achieve state-of-the-art performances. You can reproduce their training by loading the config from their run. Simply running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;lerobot-train --config_path=lerobot/diffusion_pusht
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;reproduces SOTA results for Diffusion Policy on the PushT task.&lt;/p&gt; 
&lt;h2&gt;Contribute&lt;/h2&gt; 
&lt;p&gt;If you would like to contribute to ğŸ¤— LeRobot, please check out our &lt;a href="https://github.com/huggingface/lerobot/raw/main/CONTRIBUTING.md"&gt;contribution guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Add a pretrained policy&lt;/h3&gt; 
&lt;p&gt;Once you have trained a policy you may upload it to the Hugging Face hub using a hub id that looks like &lt;code&gt;${hf_user}/${repo_name}&lt;/code&gt; (e.g. &lt;a href="https://huggingface.co/lerobot/diffusion_pusht"&gt;lerobot/diffusion_pusht&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;You first need to find the checkpoint folder located inside your experiment directory (e.g. &lt;code&gt;outputs/train/2024-05-05/20-21-12_aloha_act_default/checkpoints/002500&lt;/code&gt;). Within that there is a &lt;code&gt;pretrained_model&lt;/code&gt; directory which should contain:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;config.json&lt;/code&gt;: A serialized version of the policy configuration (following the policy's dataclass config).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;model.safetensors&lt;/code&gt;: A set of &lt;code&gt;torch.nn.Module&lt;/code&gt; parameters, saved in &lt;a href="https://huggingface.co/docs/safetensors/index"&gt;Hugging Face Safetensors&lt;/a&gt; format.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;train_config.json&lt;/code&gt;: A consolidated configuration containing all parameters used for training. The policy configuration should match &lt;code&gt;config.json&lt;/code&gt; exactly. This is useful for anyone who wants to evaluate your policy or for reproducibility.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To upload these to the hub, run the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;huggingface-cli upload ${hf_user}/${repo_name} path/to/pretrained_model
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://github.com/huggingface/lerobot/raw/main/src/lerobot/scripts/lerobot_eval.py"&gt;lerobot_eval.py&lt;/a&gt; for an example of how other people may use your policy.&lt;/p&gt; 
&lt;h3&gt;Acknowledgment&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;The LeRobot team ğŸ¤— for building SmolVLA &lt;a href="https://arxiv.org/abs/2506.01844"&gt;Paper&lt;/a&gt;, &lt;a href="https://huggingface.co/blog/smolvla"&gt;Blog&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Thanks to Tony Zhao, Zipeng Fu and colleagues for open sourcing ACT policy, ALOHA environments and datasets. Ours are adapted from &lt;a href="https://tonyzhaozh.github.io/aloha"&gt;ALOHA&lt;/a&gt; and &lt;a href="https://mobile-aloha.github.io"&gt;Mobile ALOHA&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Thanks to Cheng Chi, Zhenjia Xu and colleagues for open sourcing Diffusion policy, Pusht environment and datasets, as well as UMI datasets. Ours are adapted from &lt;a href="https://diffusion-policy.cs.columbia.edu"&gt;Diffusion Policy&lt;/a&gt; and &lt;a href="https://umi-gripper.github.io"&gt;UMI Gripper&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Thanks to Nicklas Hansen, Yunhai Feng and colleagues for open sourcing TDMPC policy, Simxarm environments and datasets. Ours are adapted from &lt;a href="https://github.com/nicklashansen/tdmpc"&gt;TDMPC&lt;/a&gt; and &lt;a href="https://www.yunhaifeng.com/FOWM"&gt;FOWM&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Thanks to Antonio Loquercio and Ashish Kumar for their early support.&lt;/li&gt; 
 &lt;li&gt;Thanks to &lt;a href="https://sjlee.cc/"&gt;Seungjae (Jay) Lee&lt;/a&gt;, &lt;a href="https://mahis.life/"&gt;Mahi Shafiullah&lt;/a&gt; and colleagues for open sourcing &lt;a href="https://sjlee.cc/vq-bet/"&gt;VQ-BeT&lt;/a&gt; policy and helping us adapt the codebase to our repository. The policy is adapted from &lt;a href="https://github.com/jayLEE0301/vq_bet_official"&gt;VQ-BeT repo&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you want, you can cite this work with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{cadene2024lerobot,
    author = {Cadene, Remi and Alibert, Simon and Soare, Alexander and Gallouedec, Quentin and Zouitine, Adil and Palma, Steven and Kooijmans, Pepijn and Aractingi, Michel and Shukor, Mustafa and Aubakirova, Dana and Russi, Martino and Capuano, Francesco and Pascal, Caroline and Choghari, Jade and Moss, Jess and Wolf, Thomas},
    title = {LeRobot: State-of-the-art Machine Learning for Real-World Robotics in Pytorch},
    howpublished = "\url{https://github.com/huggingface/lerobot}",
    year = {2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#huggingface/lerobot&amp;amp;Timeline"&gt;&lt;img src="https://api.star-history.com/svg?repos=huggingface/lerobot&amp;amp;type=Timeline" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>thinking-machines-lab/tinker-cookbook</title>
      <link>https://github.com/thinking-machines-lab/tinker-cookbook</link>
      <description>&lt;p&gt;Post-training with Tinker&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt;Tinker Cookbook&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/thinking-machines-lab/tinker-cookbook/main/assets/tinker-cover.png" width="60%" /&gt; 
&lt;/div&gt; 
&lt;p&gt;We provide two libraries for the broader community to customize their language models: &lt;code&gt;tinker&lt;/code&gt; and &lt;code&gt;tinker-cookbook&lt;/code&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;tinker&lt;/code&gt; is a training SDK for researchers and developers to fine-tune language models. You send API requests to us and we handle the complexities of distributed training.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tinker-cookbook&lt;/code&gt; includes realistic examples of fine-tuning language models. It builds on the Tinker API and provides common abstractions to fine-tune language models.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Sign up for Tinker through the &lt;a href="https://thinkingmachines.ai/tinker"&gt;waitlist&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Once you have access, create an API key from the &lt;a href="https://tinker-console.thinkingmachines.ai"&gt;console&lt;/a&gt; and export it as environment variable &lt;code&gt;TINKER_API_KEY&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Install tinker python client via &lt;code&gt;pip install tinker&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;We recommend installing &lt;code&gt;tinker-cookbook&lt;/code&gt; in a virtual env either with &lt;code&gt;conda&lt;/code&gt; or &lt;code&gt;uv&lt;/code&gt;. For running most examples, you can install via &lt;code&gt;pip install -e .&lt;/code&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Tinker&lt;/h2&gt; 
&lt;p&gt;Refer to the &lt;a href="https://tinker-docs.thinkingmachines.ai/training-sampling"&gt;docs&lt;/a&gt; to start from basics. Here we introduce a few Tinker primitives - the basic components to fine-tune LLMs:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;service_client = tinker.ServiceClient()
training_client = service_client.create_lora_training_client(
  base_model="meta-llama/Llama-3.2-1B", rank=32,
)
training_client.forward_backward(...)
training_client.optim_step(...)
training_client.save_state(...)
training_client.load_state(...)

sampling_client = training_client.save_weights_and_get_sampling_client(name="my_model")
sampling_client.sample(...)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/thinking-machines-lab/tinker-cookbook/main/tinker_cookbook/recipes/sl_loop.py"&gt;tinker_cookbook/recipes/sl_loop.py&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/thinking-machines-lab/tinker-cookbook/main/tinker_cookbook/recipes/rl_loop.py"&gt;tinker_cookbook/recipes/rl_loop.py&lt;/a&gt; for minimal examples of using these primitives to fine-tune LLMs.&lt;/p&gt; 
&lt;p&gt;To download the weights of any model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;rest_client = service_client.create_rest_client()
future = rest_client.download_checkpoint_archive_from_tinker_path(sampling_client.model_path)
with open(f"model-checkpoint.tar.gz", "wb") as f:
    f.write(future.result())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Tinker Cookbook&lt;/h3&gt; 
&lt;p&gt;Besides these primitives, we also offer &lt;strong&gt;Tinker Cookbook&lt;/strong&gt; (a.k.a. this repo), a library of a wide range of abstractions to help you customize training environments. &lt;a href="https://raw.githubusercontent.com/thinking-machines-lab/tinker-cookbook/main/tinker_cookbook/recipes/sl_basic.py"&gt;&lt;code&gt;tinker_cookbook/recipes/sl_basic.py&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/thinking-machines-lab/tinker-cookbook/main/tinker_cookbook/recipes/rl_basic.py"&gt;&lt;code&gt;tinker_cookbook/recipes/rl_basic.py&lt;/code&gt;&lt;/a&gt; contain minimal examples to configure supervised learning and reinforcement learning.&lt;/p&gt; 
&lt;p&gt;We also include a wide range of more sophisticated examples in the &lt;a href="https://raw.githubusercontent.com/thinking-machines-lab/tinker-cookbook/main/tinker_cookbook/recipes/"&gt;&lt;code&gt;tinker_cookbook/recipes/&lt;/code&gt;&lt;/a&gt; folder:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/thinking-machines-lab/tinker-cookbook/main/tinker_cookbook/recipes/chat_sl/"&gt;Chat supervised learning&lt;/a&gt;&lt;/strong&gt;: supervised fine-tuning on conversational datasets like Tulu3.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/thinking-machines-lab/tinker-cookbook/main/tinker_cookbook/recipes/math_rl/"&gt;Math reasoning&lt;/a&gt;&lt;/strong&gt;: improve LLM reasoning capability by rewarding it for answering math questions correctly.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/thinking-machines-lab/tinker-cookbook/main/tinker_cookbook/recipes/preference/"&gt;Preference learning&lt;/a&gt;&lt;/strong&gt;: showcase a three-stage RLHF pipeline: 1) supervised fine-tuning, 2) learning a reward model, 3) RL against the reward model.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/thinking-machines-lab/tinker-cookbook/main/tinker_cookbook/recipes/tool_use/"&gt;Tool use&lt;/a&gt;&lt;/strong&gt;: train LLMs to better use retrieval tools to answer questions more accurately.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/thinking-machines-lab/tinker-cookbook/main/tinker_cookbook/recipes/prompt_distillation/"&gt;Prompt distillation&lt;/a&gt;&lt;/strong&gt;: internalize long and complex instructions into LLMs.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/thinking-machines-lab/tinker-cookbook/main/tinker_cookbook/recipes/multiplayer_rl/"&gt;Multi-Agent&lt;/a&gt;&lt;/strong&gt;: optimize LLMs to play against another LLM or themselves.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;These examples are located in each subfolder, and their &lt;code&gt;README.md&lt;/code&gt; files will walk you through the key implementation details, the commands to run them, and the expected performance.&lt;/p&gt; 
&lt;h3&gt;Import our utilities&lt;/h3&gt; 
&lt;p&gt;Tinker cookbook includes several utilities. Here's a quick overview:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/thinking-machines-lab/tinker-cookbook/main/tinker_cookbook/renderers.py"&gt;&lt;code&gt;renderers&lt;/code&gt;&lt;/a&gt; converts tokens from/to structured chat message objects&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/thinking-machines-lab/tinker-cookbook/main/tinker_cookbook/hyperparam_utils.py"&gt;&lt;code&gt;hyperparam_utils&lt;/code&gt;&lt;/a&gt; helps calculate hyperparameters suitable for LoRAs&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/thinking-machines-lab/tinker-cookbook/main/tinker_cookbook/eval/evaluators.py"&gt;&lt;code&gt;evaluation&lt;/code&gt;&lt;/a&gt; provides abstractions for evaluating Tinker models and &lt;a href="https://raw.githubusercontent.com/thinking-machines-lab/tinker-cookbook/main/tinker_cookbook/eval/inspect_evaluators.py"&gt;&lt;code&gt;inspect_evaluation&lt;/code&gt;&lt;/a&gt; shows how to integrate with InspectAI to make evaluating on standard benchmarks easy.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project is built in the spirit of open science and collaborative development. We believe that the best tools emerge through community involvement and shared learning.&lt;/p&gt; 
&lt;p&gt;We welcome PR contributions after our private beta is over. If you have any feedback, please email us at &lt;a href="mailto:tinker@thinkingmachines.ai"&gt;tinker@thinkingmachines.ai&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use Tinker for your research, please cite it as:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Thinking Machines Lab, 2025. Tinker. https://thinkingmachines.ai/tinker/.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or use this BibTeX citation:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{tml2025tinker,
  author = {Thinking Machines Lab},
  title = {Tinker},
  year = {2025},
  url = {https://thinkingmachines.ai/tinker/},
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>DrewThomasson/ebook2audiobook</title>
      <link>https://github.com/DrewThomasson/ebook2audiobook</link>
      <description>&lt;p&gt;Generate audiobooks from e-books, voice cloning &amp; 1107+ languages!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ğŸ“š ebook2audiobook&lt;/h1&gt; 
&lt;p&gt;CPU/GPU Converter from eBooks to audiobooks with chapters and metadata&lt;br /&gt; using XTTSv2, Bark, Vits, Fairseq, YourTTS, Tacotron and more. Supports voice cloning and +1110 languages!&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;This tool is intended for use with non-DRM, legally acquired eBooks only.&lt;/strong&gt; &lt;br /&gt; The authors are not responsible for any misuse of this software or any resulting legal consequences. &lt;br /&gt; Use this tool responsibly and in accordance with all applicable laws.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/63Tv3F65k6"&gt;&lt;img src="https://dcbadge.limes.pink/api/server/https://discord.gg/63Tv3F65k6" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Thanks to support ebook2audiobook developers!&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://ko-fi.com/athomasson2"&gt;&lt;img src="https://img.shields.io/badge/Ko--fi-F16061?style=for-the-badge&amp;amp;logo=ko-fi&amp;amp;logoColor=white" alt="Ko-Fi" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Run locally&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;&lt;img src="https://img.shields.io/badge/Quick%20Start-blue?style=for-the-badge" alt="Quick Start" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml"&gt;&lt;img src="https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml/badge.svg?sanitize=true" alt="Docker Build" /&gt;&lt;/a&gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/releases/latest"&gt;&lt;img src="https://img.shields.io/badge/Download-Now-blue.svg?sanitize=true" alt="Download" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;a href="https://github.com/DrewThomasson/ebook2audiobook"&gt; &lt;img src="https://img.shields.io/badge/Platform-mac%20|%20linux%20|%20windows-lightgrey" alt="Platform" /&gt; &lt;/a&gt;
&lt;a href="https://hub.docker.com/r/athomasson2/ebook2audiobook"&gt; &lt;img alt="Docker Pull Count" src="https://img.shields.io/docker/pulls/athomasson2/ebook2audiobook.svg?sanitize=true" /&gt; &lt;/a&gt; 
&lt;h3&gt;Run Remotely&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/drewThomasson/ebook2audiobook"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/DrewThomasson/ebook2audiobook/blob/main/Notebooks/colab_ebook2audiobook.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Free Google Colab" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Rihcus/ebook2audiobookXTTS/raw/main/Notebooks/kaggle-ebook2audiobook.ipynb"&gt;&lt;img src="https://img.shields.io/badge/Kaggle-035a7d?style=flat&amp;amp;logo=kaggle&amp;amp;logoColor=white" alt="Kaggle" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;GUI Interface&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/demo_web_gui.gif" alt="demo_web_gui" /&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to see images of Web GUI&lt;/summary&gt; 
 &lt;img width="1728" alt="GUI Screen 1" src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_1.png" /&gt; 
 &lt;img width="1728" alt="GUI Screen 2" src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_2.png" /&gt; 
 &lt;img width="1728" alt="GUI Screen 3" src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_3.png" /&gt; 
&lt;/details&gt; 
&lt;h2&gt;Demos&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;New Default Voice Demo&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/750035dc-e355-46f1-9286-05c1d9e88cea"&gt;https://github.com/user-attachments/assets/750035dc-e355-46f1-9286-05c1d9e88cea&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;More Demos&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;ASMR Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/68eee9a1-6f71-4903-aacd-47397e47e422"&gt;https://github.com/user-attachments/assets/68eee9a1-6f71-4903-aacd-47397e47e422&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Rainy Day Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/d25034d9-c77f-43a9-8f14-0d167172b080"&gt;https://github.com/user-attachments/assets/d25034d9-c77f-43a9-8f14-0d167172b080&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Scarlett Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/b12009ee-ec0d-45ce-a1ef-b3a52b9f8693"&gt;https://github.com/user-attachments/assets/b12009ee-ec0d-45ce-a1ef-b3a52b9f8693&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;David Attenborough Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/81c4baad-117e-4db5-ac86-efc2b7fea921"&gt;https://github.com/user-attachments/assets/81c4baad-117e-4db5-ac86-efc2b7fea921&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://github.com/DrewThomasson/VoxNovel/raw/dc5197dff97252fa44c391dc0596902d71278a88/readme_files/example_in_app.jpeg" alt="Example" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;README.md&lt;/h2&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#-ebook2audiobook"&gt;ebook2audiobook&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#gui-interface"&gt;GUI Interface&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#demos"&gt;Demos&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#supported-languages"&gt;Supported Languages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#hardware-requirements"&gt;Minimum Requirements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;Usage&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;Run Locally&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;Launching Gradio Web Interface&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#basic--usage"&gt;Basic Headless Usage&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#example-of-custom-model-zip-upload"&gt;Headless Custom XTTS Model Usage&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#help-command-output"&gt;Help command output&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#run-remotely"&gt;Run Remotely&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tuned-tts-models"&gt;Fine Tuned TTS models&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tuned-tts-collection"&gt;Collection of Fine-Tuned TTS Models&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tune-your-own-xttsv2-model"&gt;Train XTTSv2&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-gpu-options"&gt;Docker&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-gpu-options"&gt;GPU options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#running-the-pre-built-docker-container"&gt;Docker Run&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#building-the-docker-container"&gt;Docker Build&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-compose"&gt;Docker Compose&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-headless-guide"&gt;Docker headless guide&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-container-file-locations"&gt;Docker container file locations&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#common-docker-issues"&gt;Common Docker issues&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#supported-ebook-formats"&gt;Supported eBook Formats&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#output-formats"&gt;Output Formats&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#updating-to-latest-version"&gt;Updating to Latest Version&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#reverting-to-older-versions"&gt;Revert to older Version&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#common-issues"&gt;Common Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#special-thanks"&gt;Special Thanks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#table-of-contents"&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“š Splits eBook into chapters for organized audio.&lt;/li&gt; 
 &lt;li&gt;ğŸ™ï¸ High-quality text-to-speech with &lt;a href="https://huggingface.co/coqui/XTTS-v2"&gt;Coqui XTTSv2&lt;/a&gt; and &lt;a href="https://github.com/facebookresearch/fairseq/tree/main/examples/mms"&gt;Fairseq&lt;/a&gt; (and more).&lt;/li&gt; 
 &lt;li&gt;ğŸ—£ï¸ Optional voice cloning with your own voice file.&lt;/li&gt; 
 &lt;li&gt;ğŸŒ Supports +1110 languages (English by default). &lt;a href="https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html"&gt;List of Supported languages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ–¥ï¸ Designed to run on 4GB RAM.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Supported Languages&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Arabic (ar)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Chinese (zh)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;English (en)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Spanish (es)&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;French (fr)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;German (de)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Italian (it)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Portuguese (pt)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Polish (pl)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Turkish (tr)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Russian (ru)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Dutch (nl)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Czech (cs)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Japanese (ja)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Hindi (hi)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Bengali (bn)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Hungarian (hu)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Korean (ko)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Vietnamese (vi)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Swedish (sv)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Persian (fa)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Yoruba (yo)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Swahili (sw)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Indonesian (id)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Slovak (sk)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Croatian (hr)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Tamil (ta)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Danish (da)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html"&gt;&lt;strong&gt;+1100 languages and dialects here&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Hardware Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;4gb RAM minimum, 8GB recommended&lt;/li&gt; 
 &lt;li&gt;Virtualization enabled if running on windows (Docker only)&lt;/li&gt; 
 &lt;li&gt;CPU (intel, AMD, ARM), GPU (Nvidia, AMD*, Intel*) (Recommended), MPS (Apple Silicon CPU) *available very soon&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;Before to post an install or bug issue search carefully to the opened and closed issues TAB&lt;br /&gt; to be sure your issue does not exist already.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] &lt;strong&gt;Lacking of any standards structure like what is a chapter, paragraph, preface etc.&lt;br /&gt; you should first remove manually any text you don't want to be converted in audio.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Installation Instructions&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Clone repo&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/DrewThomasson/ebook2audiobook.git
cd ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Launching Gradio Web Interface&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run ebook2audiobook&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh  # Run launch script
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Mac Launcher&lt;/strong&gt;&lt;br /&gt; Double click &lt;code&gt;Mac Ebook2Audiobook Launcher.command&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd  # Run launch script or double click on it
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows Launcher&lt;/strong&gt;&lt;br /&gt; Double click &lt;code&gt;ebook2audiobook.cmd&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Manual Python Install&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# (for experts only!)
REQUIRED_PROGRAMS=("calibre" "ffmpeg" "nodejs" "mecab" "espeak-ng" "rust" "sox")
REQUIRED_PYTHON_VERSION="3.12"
pip install -r requirements.txt  # Install Python Requirements
python app.py  # Run Ebook2Audiobook
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Open the Web App&lt;/strong&gt;: Click the URL provided in the terminal to access the web app and convert eBooks. &lt;code&gt;http://localhost:7860/&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;For Public Link&lt;/strong&gt;: &lt;code&gt;python app.py --share&lt;/code&gt; (all OS) &lt;code&gt;./ebook2audiobook.sh --share&lt;/code&gt; (Linux/MacOS) &lt;code&gt;ebook2audiobook.cmd --share&lt;/code&gt; (Windows)&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;If the script is stopped and run again, you need to refresh your gradio GUI interface&lt;br /&gt; to let the web page reconnect to the new connection socket.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Basic Usage&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh --headless --ebook &amp;lt;path_to_ebook_file&amp;gt; \
    --voice [path_to_voice_file] --language [language_code]
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd --headless --ebook &amp;lt;path_to_ebook_file&amp;gt;
    --voice [path_to_voice_file] --language [language_code]
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[--ebook]&lt;/strong&gt;: Path to your eBook file&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[--voice]&lt;/strong&gt;: Voice cloning file path (optional)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[--language]&lt;/strong&gt;: Language code in ISO-639-3 (i.e.: ita for italian, eng for english, deu for german...).&lt;br /&gt; Default language is eng and --language is optional for default language set in ./lib/lang.py.&lt;br /&gt; The ISO-639-1 2 letters codes are also supported.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Example of Custom Model Zip Upload&lt;/h3&gt; 
&lt;p&gt;(must be a .zip file containing the mandatory model files. Example for XTTSv2: config.json, model.pth, vocab.json and ref.wav)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh --headless --ebook &amp;lt;ebook_file_path&amp;gt; \
    --voice &amp;lt;target_voice_file_path&amp;gt; --language &amp;lt;language&amp;gt; --custom_model &amp;lt;custom_model_path&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd --headless --ebook &amp;lt;ebook_file_path&amp;gt; \
    --voice &amp;lt;target_voice_file_path&amp;gt; --language &amp;lt;language&amp;gt; --custom_model &amp;lt;custom_model_path&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&amp;lt;custom_model_path&amp;gt;&lt;/strong&gt;: Path to &lt;code&gt;model_name.zip&lt;/code&gt; file, which must contain (according to the tts engine) all the mandatory files&lt;br /&gt; (see ./lib/models.py).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;For Detailed Guide with list of all Parameters to use&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh --help
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd --help
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Or for all OS&lt;/strong&gt; &lt;code&gt;python app.py --help &lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a id="help-command-output"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;usage: app.py [-h] [--session SESSION] [--share] [--headless] [--ebook EBOOK]
              [--ebooks_dir EBOOKS_DIR] [--language LANGUAGE] [--voice VOICE]
              [--device {cpu,gpu,mps}]
              [--tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}]
              [--custom_model CUSTOM_MODEL] [--fine_tuned FINE_TUNED]
              [--output_format OUTPUT_FORMAT] [--temperature TEMPERATURE]
              [--length_penalty LENGTH_PENALTY] [--num_beams NUM_BEAMS]
              [--repetition_penalty REPETITION_PENALTY] [--top_k TOP_K]
              [--top_p TOP_P] [--speed SPEED] [--enable_text_splitting]
              [--text_temp TEXT_TEMP] [--waveform_temp WAVEFORM_TEMP]
              [--output_dir OUTPUT_DIR] [--version]

Convert eBooks to Audiobooks using a Text-to-Speech model. You can either launch the Gradio interface or run the script in headless mode for direct conversion.

options:
  -h, --help            show this help message and exit
  --session SESSION     Session to resume the conversion in case of interruption, crash, 
                            or reuse of custom models and custom cloning voices.

**** The following options are for all modes:
  Optional

**** The following option are for gradio/gui mode only:
  Optional

  --share               Enable a public shareable Gradio link.

**** The following options are for --headless mode only:
  --headless            Run the script in headless mode
  --ebook EBOOK         Path to the ebook file for conversion. Cannot be used when --ebooks_dir is present.
  --ebooks_dir EBOOKS_DIR
                        Relative or absolute path of the directory containing the files to convert. 
                            Cannot be used when --ebook is present.
  --language LANGUAGE   Language of the e-book. Default language is set 
                            in ./lib/lang.py sed as default if not present. All compatible language codes are in ./lib/lang.py

optional parameters:
  --voice VOICE         (Optional) Path to the voice cloning file for TTS engine. 
                            Uses the default voice if not present.
  --device {cpu,gpu,mps}
                        (Optional) Pprocessor unit type for the conversion. 
                            Default is set in ./lib/conf.py if not present. Fall back to CPU if GPU not available.
  --tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}
                        (Optional) Preferred TTS engine (available are: ['XTTSv2', 'BARK', 'VITS', 'FAIRSEQ', 'TACOTRON2', 'YOURTTS', 'xtts', 'bark', 'vits', 'fairseq', 'tacotron', 'yourtts'].
                            Default depends on the selected language. The tts engine should be compatible with the chosen language
  --custom_model CUSTOM_MODEL
                        (Optional) Path to the custom model zip file cntaining mandatory model files. 
                            Please refer to ./lib/models.py
  --fine_tuned FINE_TUNED
                        (Optional) Fine tuned model path. Default is builtin model.
  --output_format OUTPUT_FORMAT
                        (Optional) Output audio format. Default is set in ./lib/conf.py
  --temperature TEMPERATURE
                        (xtts only, optional) Temperature for the model. 
                            Default to config.json model. Higher temperatures lead to more creative outputs.
  --length_penalty LENGTH_PENALTY
                        (xtts only, optional) A length penalty applied to the autoregressive decoder. 
                            Default to config.json model. Not applied to custom models.
  --num_beams NUM_BEAMS
                        (xtts only, optional) Controls how many alternative sequences the model explores. Must be equal or greater than length penalty. 
                            Default to config.json model.
  --repetition_penalty REPETITION_PENALTY
                        (xtts only, optional) A penalty that prevents the autoregressive decoder from repeating itself. 
                            Default to config.json model.
  --top_k TOP_K         (xtts only, optional) Top-k sampling. 
                            Lower values mean more likely outputs and increased audio generation speed. 
                            Default to config.json model.
  --top_p TOP_P         (xtts only, optional) Top-p sampling. 
                            Lower values mean more likely outputs and increased audio generation speed. Default to config.json model.
  --speed SPEED         (xtts only, optional) Speed factor for the speech generation. 
                            Default to config.json model.
  --enable_text_splitting
                        (xtts only, optional) Enable TTS text splitting. This option is known to not be very efficient. 
                            Default to config.json model.
  --text_temp TEXT_TEMP
                        (bark only, optional) Text Temperature for the model. 
                            Default to 0.85. Higher temperatures lead to more creative outputs.
  --waveform_temp WAVEFORM_TEMP
                        (bark only, optional) Waveform Temperature for the model. 
                            Default to 0.5. Higher temperatures lead to more creative outputs.
  --output_dir OUTPUT_DIR
                        (Optional) Path to the output directory. Default is set in ./lib/conf.py
  --version             Show the version of the script and exit

Example usage:    
Windows:
    Gradio/GUI:
    ebook2audiobook.cmd
    Headless mode:
    ebook2audiobook.cmd --headless --ebook '/path/to/file'
Linux/Mac:
    Gradio/GUI:
    ./ebook2audiobook.sh
    Headless mode:
    ./ebook2audiobook.sh --headless --ebook '/path/to/file'
    
Tip: to add of silence (1.4 seconds) into your text just use "###" or "[pause]".

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;NOTE: in gradio/gui mode, to cancel a running conversion, just click on the [X] from the ebook upload component.&lt;/p&gt; 
&lt;p&gt;TIP: if it needs some more pauses, just add '###' or '[pause]' between the words you wish more pause. one [pause] equals to 1.4 seconds&lt;/p&gt; 
&lt;h4&gt;Docker GPU Options&lt;/h4&gt; 
&lt;p&gt;Available pre-build tags: &lt;code&gt;latest&lt;/code&gt; (CUDA 11.8)&lt;/p&gt; 
&lt;h4&gt;Edit: IF GPU isn't detected then you'll have to build the image -&amp;gt; &lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#building-the-docker-container"&gt;Building the Docker Container&lt;/a&gt;&lt;/h4&gt; 
&lt;h4&gt;Running the pre-built Docker Container&lt;/h4&gt; 
&lt;p&gt;-Run with CPU only&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;docker run --pull always --rm -p 7860:7860 athomasson2/ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;-Run with GPU Speedup (NVIDIA compatible only)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;docker run --pull always --rm --gpus all -p 7860:7860 athomasson2/ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command will start the Gradio interface on port 7860.(localhost:7860)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For more options add the parameter &lt;code&gt;--help&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Building the Docker Container&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can build the docker image with the command:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;docker build -t athomasson2/ebook2audiobook .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Avalible Docker Build Arguments&lt;/h4&gt; 
&lt;p&gt;&lt;code&gt;--build-arg TORCH_VERSION=cuda118&lt;/code&gt; Available tags: [cuda121, cuda118, cuda128, rocm, xpu, cpu]&lt;/p&gt; 
&lt;p&gt;All CUDA version numbers should work, Ex: CUDA 11.6-&amp;gt; cuda116&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;--build-arg SKIP_XTTS_TEST=true&lt;/code&gt; (Saves space by not baking XTTSv2 model into docker image)&lt;/p&gt; 
&lt;h2&gt;Docker container file locations&lt;/h2&gt; 
&lt;p&gt;All ebook2audiobooks will have the base dir of &lt;code&gt;/app/&lt;/code&gt; For example: &lt;code&gt;tmp&lt;/code&gt; = &lt;code&gt;/app/tmp&lt;/code&gt; &lt;code&gt;audiobooks&lt;/code&gt; = &lt;code&gt;/app/audiobooks&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Docker headless guide&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Before you do run this you need to create a dir named "input-folder" in your current dir which will be linked, This is where you can put your input files for the docker image to see&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir input-folder &amp;amp;&amp;amp; mkdir Audiobooks
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;In the command below swap out &lt;strong&gt;YOUR_INPUT_FILE.TXT&lt;/strong&gt; with the name of your input file&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --pull always --rm \
    -v $(pwd)/input-folder:/app/input_folder \
    -v $(pwd)/audiobooks:/app/audiobooks \
    athomasson2/ebook2audiobook \
    --headless --ebook /input_folder/YOUR_EBOOK_FILE
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;The output Audiobooks will be found in the Audiobook folder which will also be located in your local dir you ran this docker command in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;To get the help command for the other parameters this program has you can run this&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --pull always --rm athomasson2/ebook2audiobook --help

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That will output this &lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#help-command-output"&gt;Help command output&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Docker Compose&lt;/h3&gt; 
&lt;p&gt;This project uses Docker Compose to run locally. You can enable or disable GPU support by setting either &lt;code&gt;*gpu-enabled&lt;/code&gt; or &lt;code&gt;*gpu-disabled&lt;/code&gt; in &lt;code&gt;docker-compose.yml&lt;/code&gt;&lt;/p&gt; 
&lt;h4&gt;Steps to Run&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Clone the Repository&lt;/strong&gt; (if you haven't already): &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/DrewThomasson/ebook2audiobook.git
cd ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Set GPU Support (disabled by default)&lt;/strong&gt; To enable GPU support, modify &lt;code&gt;docker-compose.yml&lt;/code&gt; and change &lt;code&gt;*gpu-disabled&lt;/code&gt; to &lt;code&gt;*gpu-enabled&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Start the service:&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Docker
docker-compose up -d # To update add --build

# Podman
podman compose -f podman-compose.yml up -d # To update add --build
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Access the service:&lt;/strong&gt; The service will be available at &lt;a href="http://localhost:7860"&gt;http://localhost:7860&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Common Docker Issues&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;My NVIDIA GPU isnt being detected?? -&amp;gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/wiki/GPU-ISSUES"&gt;GPU ISSUES Wiki Page&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;python: can't open file '/home/user/app/app.py': [Errno 2] No such file or directory&lt;/code&gt; (Just remove all post arguments as I replaced the &lt;code&gt;CMD&lt;/code&gt; with &lt;code&gt;ENTRYPOINT&lt;/code&gt; in the &lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/Dockerfile"&gt;Dockerfile&lt;/a&gt;)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Example: &lt;code&gt;docker run --pull always athomasson2/ebook2audiobook app.py --script_mode full_docker&lt;/code&gt; - &amp;gt; corrected - &amp;gt; &lt;code&gt;docker run --pull always athomasson2/ebook2audiobook&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Arguments can be easily added like this now &lt;code&gt;docker run --pull always athomasson2/ebook2audiobook --share&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Docker gets stuck downloading Fine-Tuned models. (This does not happen for every computer but some appear to run into this issue) Disabling the progress bar appears to fix the issue, as discussed &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/issues/191"&gt;here in #191&lt;/a&gt; Example of adding this fix in the &lt;code&gt;docker run&lt;/code&gt; command&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-Dockerfile"&gt;docker run --pull always --rm --gpus all -e HF_HUB_DISABLE_PROGRESS_BARS=1 -e HF_HUB_ENABLE_HF_TRANSFER=0 \
    -p 7860:7860 athomasson2/ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Fine Tuned TTS models&lt;/h2&gt; 
&lt;h4&gt;Fine Tune your own XTTSv2 model&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/drewThomasson/xtts-finetune-webui-gpu"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/raw/v25/Notebooks/finetune/xtts/kaggle-xtts-finetune-webui-gradio-gui.ipynb"&gt;&lt;img src="https://img.shields.io/badge/Kaggle-035a7d?style=flat&amp;amp;logo=kaggle&amp;amp;logoColor=white" alt="Kaggle" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/DrewThomasson/ebook2audiobook/blob/v25/Notebooks/finetune/xtts/colab_xtts_finetune_webui.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;De-noise training data&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/drewThomasson/DeepFilterNet2_no_limit"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Rikorose/DeepFilterNet"&gt;&lt;img src="https://img.shields.io/badge/DeepFilterNet-181717?logo=github" alt="GitHub Repo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Fine Tuned TTS Collection&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/drewThomasson/fineTunedTTSModels/tree/main"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Models-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For an XTTSv2 custom model a ref audio clip of the voice reference is mandatory:&lt;/p&gt; 
&lt;h2&gt;Supported eBook Formats&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;.epub&lt;/code&gt;, &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.mobi&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.rtf&lt;/code&gt;, &lt;code&gt;.chm&lt;/code&gt;, &lt;code&gt;.lit&lt;/code&gt;, &lt;code&gt;.pdb&lt;/code&gt;, &lt;code&gt;.fb2&lt;/code&gt;, &lt;code&gt;.odt&lt;/code&gt;, &lt;code&gt;.cbr&lt;/code&gt;, &lt;code&gt;.cbz&lt;/code&gt;, &lt;code&gt;.prc&lt;/code&gt;, &lt;code&gt;.lrf&lt;/code&gt;, &lt;code&gt;.pml&lt;/code&gt;, &lt;code&gt;.snb&lt;/code&gt;, &lt;code&gt;.cbc&lt;/code&gt;, &lt;code&gt;.rb&lt;/code&gt;, &lt;code&gt;.tcr&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Best results&lt;/strong&gt;: &lt;code&gt;.epub&lt;/code&gt; or &lt;code&gt;.mobi&lt;/code&gt; for automatic chapter detection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Output Formats&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Creates a &lt;code&gt;['m4b', 'm4a', 'mp4', 'webm', 'mov', 'mp3', 'flac', 'wav', 'ogg', 'aac']&lt;/code&gt; (set in ./lib/conf.py) file with metadata and chapters.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Updating to Latest Version&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git pull # Locally/Compose

docker pull athomasson2/ebook2audiobook:latest # For Pre-build docker images
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Reverting to older Versions&lt;/h2&gt; 
&lt;p&gt;Releases can be found -&amp;gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/releases"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git checkout tags/VERSION_NUM # Locally/Compose -&amp;gt; Example: git checkout tags/v25.7.7

athomasson2/ebook2audiobook:VERSION_NUM # For Pre-build docker images -&amp;gt; Example: athomasson2/ebook2audiobook:v25.7.7
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Common Issues:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;My NVIDIA GPU isnt being detected?? -&amp;gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/wiki/GPU-ISSUES"&gt;GPU ISSUES Wiki Page&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;CPU is slow (better on server smp CPU) while NVIDIA GPU can have almost real time conversion. &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/discussions/19#discussioncomment-10879846"&gt;Discussion about this&lt;/a&gt; For faster multilingual generation I would suggest my other &lt;a href="https://github.com/DrewThomasson/ebook2audiobookpiper-tts"&gt;project that uses piper-tts&lt;/a&gt; instead (It doesn't have zero-shot voice cloning though, and is Siri quality voices, but it is much faster on cpu).&lt;/li&gt; 
 &lt;li&gt;"I'm having dependency issues" - Just use the docker, its fully self contained and has a headless mode, add &lt;code&gt;--help&lt;/code&gt; parameter at the end of the docker run command for more information.&lt;/li&gt; 
 &lt;li&gt;"Im getting a truncated audio issue!" - PLEASE MAKE AN ISSUE OF THIS, we don't speak every language and need advise from users to fine tune the sentence splitting logic.ğŸ˜Š&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What we need help with! ğŸ™Œ&lt;/h2&gt; 
&lt;h2&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook/issues/32"&gt;Full list of things can be found here&lt;/a&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Any help from people speaking any of the supported languages to help us improve the models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Do you need to rent a GPU to boost service from us?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;A poll is open here &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/discussions/889"&gt;https://github.com/DrewThomasson/ebook2audiobook/discussions/889&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Special Thanks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Coqui TTS&lt;/strong&gt;: &lt;a href="https://github.com/idiap/coqui-ai-TTS"&gt;Coqui TTS GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Calibre&lt;/strong&gt;: &lt;a href="https://calibre-ebook.com"&gt;Calibre Website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;FFmpeg&lt;/strong&gt;: &lt;a href="https://ffmpeg.org"&gt;FFmpeg Website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook/issues/8"&gt;@shakenbake15 for better chapter saving method&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>microsoft/agent-lightning</title>
      <link>https://github.com/microsoft/agent-lightning</link>
      <description>&lt;p&gt;The absolute trainer to light up AI agents.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/assets/readme-banner.svg?sanitize=true" alt="Agent-lightning-banner" style="width:600px" /&gt; &lt;/p&gt; 
&lt;h1&gt;Agent Lightningâš¡&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/tests-full.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/tests-full.yml/badge.svg?sanitize=true" alt="Test" /&gt;&lt;/a&gt; &lt;a href="https://microsoft.github.io/agent-lightning/"&gt;&lt;img src="https://img.shields.io/badge/GitHub%20Pages-Documentation-blue" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/agentlightning"&gt;&lt;img src="https://badge.fury.io/py/agentlightning.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/microsoft/agent-lightning/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/microsoft/agent-lightning"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/RYk7CdvDR7"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join-5865F2?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;The absolute trainer to light up AI agents.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Join our &lt;a href="https://discord.gg/RYk7CdvDR7"&gt;Discord community&lt;/a&gt; to connect with other users and contributors.&lt;/p&gt; 
&lt;h2&gt;âš¡ Core Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Turn your agent into an optimizable beast with &lt;strong&gt;ZERO CODE CHANGE&lt;/strong&gt; (almost)! ğŸ’¤&lt;/li&gt; 
 &lt;li&gt;Build with &lt;strong&gt;ANY&lt;/strong&gt; agent framework (LangChain, OpenAI Agent SDK, AutoGen, CrewAI, Microsoft Agent Framework...); or even WITHOUT agent framework (Python OpenAI). You name it! ğŸ¤–&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Selectively&lt;/strong&gt; optimize one or more agents in a multi-agent system. ğŸ¯&lt;/li&gt; 
 &lt;li&gt;Embraces &lt;strong&gt;Algorithms&lt;/strong&gt; like Reinforcement Learning, Automatic Prompt Optimization, Supervised Fine-tuning and more. ğŸ¤—&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Read more on our &lt;a href="https://microsoft.github.io/agent-lightning/"&gt;documentation website&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/assets/readme-diff.svg?sanitize=true" alt="Agent-Lightning Core Quickstart" style="width:100%" /&gt; &lt;/p&gt; 
&lt;h2&gt;âš¡ Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install agentlightning
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please refer to our &lt;a href="https://microsoft.github.io/agent-lightning/stable/tutorials/installation/"&gt;installation guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;To start using Agent-lightning, check out our &lt;a href="https://microsoft.github.io/agent-lightning/"&gt;documentation&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/microsoft/agent-lightning/main/examples"&gt;examples&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;âš¡ Articles&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;11/4/2025 &lt;a href="https://medium.com/@yugez/tuning-any-ai-agent-with-tinker-agent-lightning-part-1-1d8c9a397f0e"&gt;Tuning ANY AI agent with Tinker âœ• Agent-lightning&lt;/a&gt; Medium. See also &lt;a href="https://medium.com/@yugez/tuning-any-ai-agent-with-tinker-agent-lightning-part-2-332c5437f0dc"&gt;Part 2&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;10/22/2025 &lt;a href="https://blog.vllm.ai/2025/10/22/agent-lightning.html"&gt;No More Retokenization Drift: Returning Token IDs via the OpenAI Compatible API Matters in Agent RL&lt;/a&gt; vLLM blog. See also &lt;a href="https://zhuanlan.zhihu.com/p/1965067274642785725"&gt;Zhihu writeup&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;8/11/2025 &lt;a href="https://medium.com/@yugez/training-ai-agents-to-write-and-self-correct-sql-with-reinforcement-learning-571ed31281ad"&gt;Training AI Agents to Write and Self-correct SQL with Reinforcement Learning&lt;/a&gt; Medium.&lt;/li&gt; 
 &lt;li&gt;8/5/2025 &lt;a href="https://arxiv.org/abs/2508.03680"&gt;Agent Lightning: Train ANY AI Agents with Reinforcement Learning&lt;/a&gt; arXiv paper.&lt;/li&gt; 
 &lt;li&gt;7/26/2025 &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1m9m670/we_discovered_an_approach_to_train_any_ai_agent/"&gt;We discovered an approach to train any AI agent with RL, with (almost) zero code changes.&lt;/a&gt; Reddit.&lt;/li&gt; 
 &lt;li&gt;6/6/2025 &lt;a href="https://www.microsoft.com/en-us/research/project/agent-lightning/"&gt;Agent Lightning - Microsoft Research&lt;/a&gt; Project page.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;âš¡ Community Projects&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/af-74413592/DeepWerewolf"&gt;DeepWerewolf&lt;/a&gt; â€” A case study of agent RL training for the Chinese Werewolf game built with AgentScope and Agent Lightning.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://agentflow.stanford.edu/"&gt;AgentFlow&lt;/a&gt; â€” A modular multi-agent framework that combines planner, executor, verifier, and generator agents with the Flow-GRPO algorithm to tackle long-horizon, sparse-reward tasks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;âš¡ Architecture&lt;/h2&gt; 
&lt;p&gt;Agent Lightning keeps the moving parts to a minimum so you can focus on your idea, not the plumbing. Your agent continues to run as usual; you can still use any agent framework you like; you drop in the lightweight &lt;code&gt;agl.emit_xxx()&lt;/code&gt; helper, or let the tracer collect every prompt, tool call, and reward. Those events become structured spans that flow into the LightningStore, a central hub that keeps tasks, resources, and traces in sync.&lt;/p&gt; 
&lt;p&gt;On the other side of the store sits the algorithm you choose, or write yourself. The algorithm reads spans, learns from them, and posts updated resources such as refined prompt templates or new policy weights. The Trainer ties it all together: it streams datasets to runners, ferries resources between the store and the algorithm, and updates the inference engine when improvements land. You can either stop there, or simply let the same loop keep turning.&lt;/p&gt; 
&lt;p&gt;No rewrites, no lock-in, just a clear path from first rollout to steady improvement.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/assets/readme-architecture.svg?sanitize=true" alt="Agent-lightning Architecture" style="width:100%" /&gt; &lt;/p&gt; 
&lt;h2&gt;âš¡ CI Status&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Workflow&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CPU Tests&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/tests.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/tests.yml/badge.svg?sanitize=true" alt="tests workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GPU Tests&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/tests-full.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/tests-full.yml/badge.svg?sanitize=true" alt="tests-full workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Examples Integration&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/badge-examples.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/badge-examples.yml/badge.svg?sanitize=true" alt="examples summary workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Latest Dependency Compatibility&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/badge-latest.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/badge-latest.yml/badge.svg?sanitize=true" alt="latest summary workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Legacy Examples Compatibility&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/examples-compat.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/examples-compat.yml/badge.svg?sanitize=true" alt="examples compatibility workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;âš¡ Citation&lt;/h2&gt; 
&lt;p&gt;If you find Agent Lightning useful in your research or projects, please cite our paper:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{luo2025agentlightningtrainai,
      title={Agent Lightning: Train ANY AI Agents with Reinforcement Learning},
      author={Xufang Luo and Yuge Zhang and Zhiyuan He and Zilong Wang and Siyun Zhao and Dongsheng Li and Luna K. Qiu and Yuqing Yang},
      year={2025},
      eprint={2508.03680},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2508.03680},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;âš¡ Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Start by reading the &lt;a href="https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/community/contributing.md"&gt;Contributing Guide&lt;/a&gt; for environment setup, branching conventions, and pull request expectations. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h2&gt;âš¡ Trademarks&lt;/h2&gt; 
&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general"&gt;Microsoft's Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.&lt;/p&gt; 
&lt;h2&gt;âš¡ Responsible AI&lt;/h2&gt; 
&lt;p&gt;This project has been evaluated and certified to comply with the Microsoft Responsible AI Standard. The team will continue to monitor and maintain the repository, addressing any severe issues, including potential harms, if they arise.&lt;/p&gt; 
&lt;h2&gt;âš¡ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License. See the &lt;a href="https://raw.githubusercontent.com/microsoft/agent-lightning/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>anthropics/anthropic-sdk-python</title>
      <link>https://github.com/anthropics/anthropic-sdk-python</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Anthropic Python API library&lt;/h1&gt; 
&lt;!-- prettier-ignore --&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/anthropic/"&gt;&lt;img src="https://img.shields.io/pypi/v/anthropic.svg?label=pypi%20(stable)" alt="PyPI version" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The Anthropic Python library provides convenient access to the Anthropic REST API from any Python 3.8+ application. It includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by &lt;a href="https://github.com/encode/httpx"&gt;httpx&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;The REST API documentation can be found on &lt;a href="https://docs.anthropic.com/claude/reference/"&gt;docs.anthropic.com&lt;/a&gt;. The full API of this library can be found in &lt;a href="https://raw.githubusercontent.com/anthropics/anthropic-sdk-python/main/api.md"&gt;api.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# install from PyPI
pip install anthropic
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;The full API of this library can be found in &lt;a href="https://raw.githubusercontent.com/anthropics/anthropic-sdk-python/main/api.md"&gt;api.md&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
from anthropic import Anthropic

client = Anthropic(
    api_key=os.environ.get("ANTHROPIC_API_KEY"),  # This is the default and can be omitted
)

message = client.messages.create(
    max_tokens=1024,
    messages=[
        {
            "role": "user",
            "content": "Hello, Claude",
        }
    ],
    model="claude-sonnet-4-5-20250929",
)
print(message.content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;While you can provide an &lt;code&gt;api_key&lt;/code&gt; keyword argument, we recommend using &lt;a href="https://pypi.org/project/python-dotenv/"&gt;python-dotenv&lt;/a&gt; to add &lt;code&gt;ANTHROPIC_API_KEY="my-anthropic-api-key"&lt;/code&gt; to your &lt;code&gt;.env&lt;/code&gt; file so that your API Key is not stored in source control.&lt;/p&gt; 
&lt;h2&gt;Async usage&lt;/h2&gt; 
&lt;p&gt;Simply import &lt;code&gt;AsyncAnthropic&lt;/code&gt; instead of &lt;code&gt;Anthropic&lt;/code&gt; and use &lt;code&gt;await&lt;/code&gt; with each API call:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
import asyncio
from anthropic import AsyncAnthropic

client = AsyncAnthropic(
    api_key=os.environ.get("ANTHROPIC_API_KEY"),  # This is the default and can be omitted
)


async def main() -&amp;gt; None:
    message = await client.messages.create(
        max_tokens=1024,
        messages=[
            {
                "role": "user",
                "content": "Hello, Claude",
            }
        ],
        model="claude-sonnet-4-5-20250929",
    )
    print(message.content)


asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Functionality between the synchronous and asynchronous clients is otherwise identical.&lt;/p&gt; 
&lt;h3&gt;With aiohttp&lt;/h3&gt; 
&lt;p&gt;By default, the async client uses &lt;code&gt;httpx&lt;/code&gt; for HTTP requests. However, for improved concurrency performance you may also use &lt;code&gt;aiohttp&lt;/code&gt; as the HTTP backend.&lt;/p&gt; 
&lt;p&gt;You can enable this by installing &lt;code&gt;aiohttp&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# install from PyPI
pip install anthropic[aiohttp]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you can enable it by instantiating the client with &lt;code&gt;http_client=DefaultAioHttpClient()&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from anthropic import DefaultAioHttpClient
from anthropic import AsyncAnthropic


async def main() -&amp;gt; None:
    async with AsyncAnthropic(
        api_key="my-anthropic-api-key",
        http_client=DefaultAioHttpClient(),
    ) as client:
        message = await client.messages.create(
            max_tokens=1024,
            messages=[
                {
                    "role": "user",
                    "content": "Hello, Claude",
                }
            ],
            model="claude-sonnet-4-5-20250929",
        )
        print(message.content)


asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Streaming responses&lt;/h2&gt; 
&lt;p&gt;We provide support for streaming responses using Server Side Events (SSE).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from anthropic import Anthropic

client = Anthropic()

stream = client.messages.create(
    max_tokens=1024,
    messages=[
        {
            "role": "user",
            "content": "Hello, Claude",
        }
    ],
    model="claude-sonnet-4-5-20250929",
    stream=True,
)
for event in stream:
    print(event.type)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The async client uses the exact same interface.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from anthropic import AsyncAnthropic

client = AsyncAnthropic()

stream = await client.messages.create(
    max_tokens=1024,
    messages=[
        {
            "role": "user",
            "content": "Hello, Claude",
        }
    ],
    model="claude-sonnet-4-5-20250929",
    stream=True,
)
async for event in stream:
    print(event.type)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Tool helpers&lt;/h3&gt; 
&lt;p&gt;This library provides helper functions for defining and running tools as pure python functions, for example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import json
import rich
from typing_extensions import Literal
from anthropic import Anthropic, beta_tool

client = Anthropic()


@beta_tool
def get_weather(location: str) -&amp;gt; str:
    """Lookup the weather for a given city in either celsius or fahrenheit

    Args:
        location: The city and state, e.g. San Francisco, CA
    Returns:
        A dictionary containing the location, temperature, and weather condition.
    """
    # Here you would typically make an API call to a weather service
    # For demonstration, we return a mock response
    return json.dumps(
        {
            "location": location,
            "temperature": "68Â°F",
            "condition": "Sunny",
        }
    )


runner = client.beta.messages.tool_runner(
    max_tokens=1024,
    model="claude-sonnet-4-5-20250929",
    tools=[get_weather],
    messages=[
        {"role": "user", "content": "What is the weather in SF?"},
    ],
)
for message in runner:
    rich.print(message)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;On every iteration, an API request will be made, if Claude wants to call one of the given tools then it will be automatically called, and the result will be returned directly to the model in the next iteration.&lt;/p&gt; 
&lt;p&gt;For more information see the &lt;a href="https://raw.githubusercontent.com/anthropics/anthropic-sdk-python/main/tools.md"&gt;full docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Streaming Helpers&lt;/h3&gt; 
&lt;p&gt;This library provides several conveniences for streaming messages, for example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import asyncio
from anthropic import AsyncAnthropic

client = AsyncAnthropic()

async def main() -&amp;gt; None:
    async with client.messages.stream(
        max_tokens=1024,
        messages=[
            {
                "role": "user",
                "content": "Say hello there!",
            }
        ],
        model="claude-sonnet-4-5-20250929",
    ) as stream:
        async for text in stream.text_stream:
            print(text, end="", flush=True)
        print()

    message = await stream.get_final_message()
    print(message.to_json())

asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Streaming with &lt;code&gt;client.messages.stream(...)&lt;/code&gt; exposes &lt;a href="https://raw.githubusercontent.com/anthropics/anthropic-sdk-python/main/helpers.md"&gt;various helpers for your convenience&lt;/a&gt; including accumulation &amp;amp; SDK-specific events.&lt;/p&gt; 
&lt;p&gt;Alternatively, you can use &lt;code&gt;client.messages.create(..., stream=True)&lt;/code&gt; which only returns an async iterable of the events in the stream and thus uses less memory (it does not build up a final message object for you).&lt;/p&gt; 
&lt;h2&gt;Token counting&lt;/h2&gt; 
&lt;p&gt;To get the token count for a message without creating it you can use the &lt;code&gt;client.messages.count_tokens()&lt;/code&gt; method. This takes the same &lt;code&gt;messages&lt;/code&gt; list as the &lt;code&gt;.create()&lt;/code&gt; method.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;count = client.messages.count_tokens(
    model="claude-sonnet-4-5-20250929",
    messages=[
        {"role": "user", "content": "Hello, world"}
    ]
)
count.input_tokens  # 10
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also see the exact usage for a given request through the &lt;code&gt;usage&lt;/code&gt; response property, e.g.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;message = client.messages.create(...)
message.usage
# Usage(input_tokens=25, output_tokens=13)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Message Batches&lt;/h2&gt; 
&lt;p&gt;This SDK provides support for the &lt;a href="https://docs.anthropic.com/en/docs/build-with-claude/message-batches"&gt;Message Batches API&lt;/a&gt; under the &lt;code&gt;client.messages.batches&lt;/code&gt; namespace.&lt;/p&gt; 
&lt;h3&gt;Creating a batch&lt;/h3&gt; 
&lt;p&gt;Message Batches take the exact same request params as the standard Messages API:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;await client.messages.batches.create(
    requests=[
        {
            "custom_id": "my-first-request",
            "params": {
                "model": "claude-sonnet-4-5-20250929",
                "max_tokens": 1024,
                "messages": [{"role": "user", "content": "Hello, world"}],
            },
        },
        {
            "custom_id": "my-second-request",
            "params": {
                "model": "claude-sonnet-4-5-20250929",
                "max_tokens": 1024,
                "messages": [{"role": "user", "content": "Hi again, friend"}],
            },
        },
    ]
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Getting results from a batch&lt;/h3&gt; 
&lt;p&gt;Once a Message Batch has been processed, indicated by &lt;code&gt;.processing_status === 'ended'&lt;/code&gt;, you can access the results with &lt;code&gt;.batches.results()&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;result_stream = await client.messages.batches.results(batch_id)
async for entry in result_stream:
    if entry.result.type == "succeeded":
        print(entry.result.message.content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Tool use&lt;/h2&gt; 
&lt;p&gt;This SDK provides support for tool use, aka function calling. More details can be found in &lt;a href="https://docs.anthropic.com/claude/docs/tool-use"&gt;the documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;AWS Bedrock&lt;/h2&gt; 
&lt;p&gt;This library also provides support for the &lt;a href="https://aws.amazon.com/bedrock/claude/"&gt;Anthropic Bedrock API&lt;/a&gt; if you install this library with the &lt;code&gt;bedrock&lt;/code&gt; extra, e.g. &lt;code&gt;pip install -U anthropic[bedrock]&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can then import and instantiate a separate &lt;code&gt;AnthropicBedrock&lt;/code&gt; class, the rest of the API is the same.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;from anthropic import AnthropicBedrock

client = AnthropicBedrock()

message = client.messages.create(
    max_tokens=1024,
    messages=[
        {
            "role": "user",
            "content": "Hello!",
        }
    ],
    model="anthropic.claude-sonnet-4-5-20250929-v1:0",
)
print(message)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The bedrock client supports the following arguments for authentication&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;AnthropicBedrock(
  aws_profile='...',
  aws_region='us-east'
  aws_secret_key='...',
  aws_access_key='...',
  aws_session_token='...',
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For a more fully fledged example see &lt;a href="https://github.com/anthropics/anthropic-sdk-python/raw/main/examples/bedrock.py"&gt;&lt;code&gt;examples/bedrock.py&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Google Vertex&lt;/h2&gt; 
&lt;p&gt;This library also provides support for the &lt;a href="https://cloud.google.com/vertex-ai?hl=en"&gt;Anthropic Vertex API&lt;/a&gt; if you install this library with the &lt;code&gt;vertex&lt;/code&gt; extra, e.g. &lt;code&gt;pip install -U anthropic[vertex]&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can then import and instantiate a separate &lt;code&gt;AnthropicVertex&lt;/code&gt;/&lt;code&gt;AsyncAnthropicVertex&lt;/code&gt; class, which has the same API as the base &lt;code&gt;Anthropic&lt;/code&gt;/&lt;code&gt;AsyncAnthropic&lt;/code&gt; class.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;from anthropic import AnthropicVertex

client = AnthropicVertex()

message = client.messages.create(
    model="claude-sonnet-4@20250514",
    max_tokens=100,
    messages=[
        {
            "role": "user",
            "content": "Hello!",
        }
    ],
)
print(message)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For a more complete example see &lt;a href="https://github.com/anthropics/anthropic-sdk-python/raw/main/examples/vertex.py"&gt;&lt;code&gt;examples/vertex.py&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Using types&lt;/h2&gt; 
&lt;p&gt;Nested request parameters are &lt;a href="https://docs.python.org/3/library/typing.html#typing.TypedDict"&gt;TypedDicts&lt;/a&gt;. Responses are &lt;a href="https://docs.pydantic.dev"&gt;Pydantic models&lt;/a&gt; which also provide helper methods for things like:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Serializing back into JSON, &lt;code&gt;model.to_json()&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Converting to a dictionary, &lt;code&gt;model.to_dict()&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set &lt;code&gt;python.analysis.typeCheckingMode&lt;/code&gt; to &lt;code&gt;basic&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Pagination&lt;/h2&gt; 
&lt;p&gt;List methods in the Anthropic API are paginated.&lt;/p&gt; 
&lt;p&gt;This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from anthropic import Anthropic

client = Anthropic()

all_batches = []
# Automatically fetches more pages as needed.
for batch in client.messages.batches.list(
    limit=20,
):
    # Do something with batch here
    all_batches.append(batch)
print(all_batches)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, asynchronously:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from anthropic import AsyncAnthropic

client = AsyncAnthropic()


async def main() -&amp;gt; None:
    all_batches = []
    # Iterate through items across all pages, issuing requests as needed.
    async for batch in client.messages.batches.list(
        limit=20,
    ):
        all_batches.append(batch)
    print(all_batches)


asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can use the &lt;code&gt;.has_next_page()&lt;/code&gt;, &lt;code&gt;.next_page_info()&lt;/code&gt;, or &lt;code&gt;.get_next_page()&lt;/code&gt; methods for more granular control working with pages:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;first_page = await client.messages.batches.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or just work directly with the returned data:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;first_page = await client.messages.batches.list(
    limit=20,
)

print(f"next page cursor: {first_page.last_id}")  # =&amp;gt; "next page cursor: ..."
for batch in first_page.data:
    print(batch.id)

# Remove `await` for non-async usage.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Nested params&lt;/h2&gt; 
&lt;p&gt;Nested parameters are dictionaries, typed using &lt;code&gt;TypedDict&lt;/code&gt;, for example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from anthropic import Anthropic

client = Anthropic()

message = client.messages.create(
    max_tokens=1024,
    messages=[
        {
            "content": "Hello, world",
            "role": "user",
        }
    ],
    model="claude-sonnet-4-5-20250929",
    metadata={},
)
print(message.metadata)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;File uploads&lt;/h2&gt; 
&lt;p&gt;Request parameters that correspond to file uploads can be passed as &lt;code&gt;bytes&lt;/code&gt;, or a &lt;a href="https://docs.python.org/3/library/os.html#os.PathLike"&gt;&lt;code&gt;PathLike&lt;/code&gt;&lt;/a&gt; instance or a tuple of &lt;code&gt;(filename, contents, media type)&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from pathlib import Path
from anthropic import Anthropic

client = Anthropic()

client.beta.files.upload(
    file=Path("/path/to/file"),
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The async client uses the exact same interface. If you pass a &lt;a href="https://docs.python.org/3/library/os.html#os.PathLike"&gt;&lt;code&gt;PathLike&lt;/code&gt;&lt;/a&gt; instance, the file contents will be read asynchronously automatically.&lt;/p&gt; 
&lt;h2&gt;Handling errors&lt;/h2&gt; 
&lt;p&gt;When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of &lt;code&gt;anthropic.APIConnectionError&lt;/code&gt; is raised.&lt;/p&gt; 
&lt;p&gt;When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of &lt;code&gt;anthropic.APIStatusError&lt;/code&gt; is raised, containing &lt;code&gt;status_code&lt;/code&gt; and &lt;code&gt;response&lt;/code&gt; properties.&lt;/p&gt; 
&lt;p&gt;All errors inherit from &lt;code&gt;anthropic.APIError&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import anthropic
from anthropic import Anthropic

client = Anthropic()

try:
    client.messages.create(
        max_tokens=1024,
        messages=[
            {
                "role": "user",
                "content": "Hello, Claude",
            }
        ],
        model="claude-sonnet-4-5-20250929",
    )
except anthropic.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except anthropic.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except anthropic.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Error codes are as follows:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Status Code&lt;/th&gt; 
   &lt;th&gt;Error Type&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;400&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;BadRequestError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;401&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;AuthenticationError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;403&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;PermissionDeniedError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;404&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;NotFoundError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;422&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;UnprocessableEntityError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;429&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;RateLimitError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&amp;gt;=500&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;InternalServerError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;APIConnectionError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Request IDs&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For more information on debugging requests, see &lt;a href="https://docs.anthropic.com/en/api/errors#request-id"&gt;these docs&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;All object responses in the SDK provide a &lt;code&gt;_request_id&lt;/code&gt; property which is added from the &lt;code&gt;request-id&lt;/code&gt; response header so that you can quickly log failing requests and report them back to Anthropic.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;message = client.messages.create(
    max_tokens=1024,
    messages=[
        {
            "role": "user",
            "content": "Hello, Claude",
        }
    ],
    model="claude-sonnet-4-5-20250929",
)
print(message._request_id)  # req_018EeWyXxfu5pfWkrYcMdjWG
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that unlike other properties that use an &lt;code&gt;_&lt;/code&gt; prefix, the &lt;code&gt;_request_id&lt;/code&gt; property &lt;em&gt;is&lt;/em&gt; public. Unless documented otherwise, &lt;em&gt;all&lt;/em&gt; other &lt;code&gt;_&lt;/code&gt; prefix properties, methods and modules are &lt;em&gt;private&lt;/em&gt;.&lt;/p&gt; 
&lt;h3&gt;Retries&lt;/h3&gt; 
&lt;p&gt;Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and &amp;gt;=500 Internal errors are all retried by default.&lt;/p&gt; 
&lt;p&gt;You can use the &lt;code&gt;max_retries&lt;/code&gt; option to configure or disable retry settings:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from anthropic import Anthropic

# Configure the default for all requests:
client = Anthropic(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).messages.create(
    max_tokens=1024,
    messages=[
        {
            "role": "user",
            "content": "Hello, Claude",
        }
    ],
    model="claude-sonnet-4-5-20250929",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Timeouts&lt;/h3&gt; 
&lt;p&gt;By default requests time out after 10 minutes. You can configure this with a &lt;code&gt;timeout&lt;/code&gt; option, which accepts a float or an &lt;a href="https://www.python-httpx.org/advanced/timeouts/#fine-tuning-the-configuration"&gt;&lt;code&gt;httpx.Timeout&lt;/code&gt;&lt;/a&gt; object:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from anthropic import Anthropic

# Configure the default for all requests:
client = Anthropic(
    # 20 seconds (default is 10 minutes)
    timeout=20.0,
)

# More granular control:
client = Anthropic(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5.0).messages.create(
    max_tokens=1024,
    messages=[
        {
            "role": "user",
            "content": "Hello, Claude",
        }
    ],
    model="claude-sonnet-4-5-20250929",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;On timeout, an &lt;code&gt;APITimeoutError&lt;/code&gt; is thrown.&lt;/p&gt; 
&lt;p&gt;Note that requests that time out are &lt;a href="https://raw.githubusercontent.com/anthropics/anthropic-sdk-python/main/#retries"&gt;retried twice by default&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Long Requests&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] We highly encourage you use the streaming &lt;a href="https://raw.githubusercontent.com/anthropics/anthropic-sdk-python/main/#streaming-responses"&gt;Messages API&lt;/a&gt; for longer running requests.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;We do not recommend setting a large &lt;code&gt;max_tokens&lt;/code&gt; values without using streaming. Some networks may drop idle connections after a certain period of time, which can cause the request to fail or &lt;a href="https://raw.githubusercontent.com/anthropics/anthropic-sdk-python/main/#timeouts"&gt;timeout&lt;/a&gt; without receiving a response from Anthropic.&lt;/p&gt; 
&lt;p&gt;This SDK will also throw a &lt;code&gt;ValueError&lt;/code&gt; if a non-streaming request is expected to be above roughly 10 minutes long. Passing &lt;code&gt;stream=True&lt;/code&gt; or &lt;a href="https://raw.githubusercontent.com/anthropics/anthropic-sdk-python/main/#timeouts"&gt;overriding&lt;/a&gt; the &lt;code&gt;timeout&lt;/code&gt; option at the client or request level disables this error.&lt;/p&gt; 
&lt;p&gt;An expected request latency longer than the &lt;a href="https://raw.githubusercontent.com/anthropics/anthropic-sdk-python/main/#timeouts"&gt;timeout&lt;/a&gt; for a non-streaming request will result in the client terminating the connection and retrying without receiving a response.&lt;/p&gt; 
&lt;p&gt;We set a &lt;a href="https://tldp.org/HOWTO/TCP-Keepalive-HOWTO/overview.html"&gt;TCP socket keep-alive&lt;/a&gt; option in order to reduce the impact of idle connection timeouts on some networks. This can be &lt;a href="https://raw.githubusercontent.com/anthropics/anthropic-sdk-python/main/#Configuring-the-HTTP-client"&gt;overriden&lt;/a&gt; by passing a &lt;code&gt;http_client&lt;/code&gt; option to the client.&lt;/p&gt; 
&lt;h2&gt;Default Headers&lt;/h2&gt; 
&lt;p&gt;We automatically send the &lt;code&gt;anthropic-version&lt;/code&gt; header set to &lt;code&gt;2023-06-01&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;If you need to, you can override it by setting default headers per-request or on the client object.&lt;/p&gt; 
&lt;p&gt;Be aware that doing so may result in incorrect types and other unexpected or undefined behavior in the SDK.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from anthropic import Anthropic

client = Anthropic(
    default_headers={"anthropic-version": "My-Custom-Value"},
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Advanced&lt;/h2&gt; 
&lt;h3&gt;Logging&lt;/h3&gt; 
&lt;p&gt;We use the standard library &lt;a href="https://docs.python.org/3/library/logging.html"&gt;&lt;code&gt;logging&lt;/code&gt;&lt;/a&gt; module.&lt;/p&gt; 
&lt;p&gt;You can enable logging by setting the environment variable &lt;code&gt;ANTHROPIC_LOG&lt;/code&gt; to &lt;code&gt;info&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;$ export ANTHROPIC_LOG=info
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or to &lt;code&gt;debug&lt;/code&gt; for more verbose logging.&lt;/p&gt; 
&lt;h3&gt;How to tell whether &lt;code&gt;None&lt;/code&gt; means &lt;code&gt;null&lt;/code&gt; or missing&lt;/h3&gt; 
&lt;p&gt;In an API response, a field may be explicitly &lt;code&gt;null&lt;/code&gt;, or missing entirely; in either case, its value is &lt;code&gt;None&lt;/code&gt; in this library. You can differentiate the two cases with &lt;code&gt;.model_fields_set&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Accessing raw response data (e.g. headers)&lt;/h3&gt; 
&lt;p&gt;The "raw" Response object can be accessed by prefixing &lt;code&gt;.with_raw_response.&lt;/code&gt; to any HTTP method call, e.g.,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;from anthropic import Anthropic

client = Anthropic()
response = client.messages.with_raw_response.create(
    max_tokens=1024,
    messages=[{
        "role": "user",
        "content": "Hello, Claude",
    }],
    model="claude-sonnet-4-5-20250929",
)
print(response.headers.get('X-My-Header'))

message = response.parse()  # get the object that `messages.create()` would have returned
print(message.content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These methods return a &lt;a href="https://github.com/anthropics/anthropic-sdk-python/tree/main/src/anthropic/_legacy_response.py"&gt;&lt;code&gt;LegacyAPIResponse&lt;/code&gt;&lt;/a&gt; object. This is a legacy class as we're changing it slightly in the next major version.&lt;/p&gt; 
&lt;p&gt;For the sync client this will mostly be the same with the exception of &lt;code&gt;content&lt;/code&gt; &amp;amp; &lt;code&gt;text&lt;/code&gt; will be methods instead of properties. In the async client, all methods will be async.&lt;/p&gt; 
&lt;p&gt;A migration script will be provided &amp;amp; the migration in general should be smooth.&lt;/p&gt; 
&lt;h4&gt;&lt;code&gt;.with_streaming_response&lt;/code&gt;&lt;/h4&gt; 
&lt;p&gt;The above interface eagerly reads the full response body when you make the request, which may not always be what you want.&lt;/p&gt; 
&lt;p&gt;To stream the response body, use &lt;code&gt;.with_streaming_response&lt;/code&gt; instead, which requires a context manager and only reads the response body once you call &lt;code&gt;.read()&lt;/code&gt;, &lt;code&gt;.text()&lt;/code&gt;, &lt;code&gt;.json()&lt;/code&gt;, &lt;code&gt;.iter_bytes()&lt;/code&gt;, &lt;code&gt;.iter_text()&lt;/code&gt;, &lt;code&gt;.iter_lines()&lt;/code&gt; or &lt;code&gt;.parse()&lt;/code&gt;. In the async client, these are async methods.&lt;/p&gt; 
&lt;p&gt;As such, &lt;code&gt;.with_streaming_response&lt;/code&gt; methods return a different &lt;a href="https://github.com/anthropics/anthropic-sdk-python/tree/main/src/anthropic/_response.py"&gt;&lt;code&gt;APIResponse&lt;/code&gt;&lt;/a&gt; object, and the async client returns an &lt;a href="https://github.com/anthropics/anthropic-sdk-python/tree/main/src/anthropic/_response.py"&gt;&lt;code&gt;AsyncAPIResponse&lt;/code&gt;&lt;/a&gt; object.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;with client.messages.with_streaming_response.create(
    max_tokens=1024,
    messages=[
        {
            "role": "user",
            "content": "Hello, Claude",
        }
    ],
    model="claude-sonnet-4-5-20250929",
) as response:
    print(response.headers.get("X-My-Header"))

    for line in response.iter_lines():
        print(line)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The context manager is required so that the response will reliably be closed.&lt;/p&gt; 
&lt;h3&gt;Making custom/undocumented requests&lt;/h3&gt; 
&lt;p&gt;This library is typed for convenient access to the documented API.&lt;/p&gt; 
&lt;p&gt;If you need to access undocumented endpoints, params, or response properties, the library can still be used.&lt;/p&gt; 
&lt;h4&gt;Undocumented endpoints&lt;/h4&gt; 
&lt;p&gt;To make requests to undocumented endpoints, you can make requests using &lt;code&gt;client.get&lt;/code&gt;, &lt;code&gt;client.post&lt;/code&gt;, and other http verbs. Options on the client will be respected (such as retries) when making this request.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import httpx

response = client.post(
    "/foo",
    cast_to=httpx.Response,
    body={"my_param": True},
)

print(response.headers.get("x-foo"))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Undocumented request params&lt;/h4&gt; 
&lt;p&gt;If you want to explicitly send an extra param, you can do so with the &lt;code&gt;extra_query&lt;/code&gt;, &lt;code&gt;extra_body&lt;/code&gt;, and &lt;code&gt;extra_headers&lt;/code&gt; request options.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;/p&gt; 
 &lt;p&gt;The &lt;code&gt;extra_&lt;/code&gt; parameters of the same name overrides the documented parameters. For security reasons, ensure these methods are only used with trusted input data.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Undocumented response properties&lt;/h4&gt; 
&lt;p&gt;To access undocumented response properties, you can access the extra fields like &lt;code&gt;response.unknown_prop&lt;/code&gt;. You can also get all the extra fields on the Pydantic model as a dict with &lt;a href="https://docs.pydantic.dev/latest/api/base_model/#pydantic.BaseModel.model_extra"&gt;&lt;code&gt;response.model_extra&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Configuring the HTTP client&lt;/h3&gt; 
&lt;p&gt;You can directly override the &lt;a href="https://www.python-httpx.org/api/#client"&gt;httpx client&lt;/a&gt; to customize it for your use case, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Support for &lt;a href="https://www.python-httpx.org/advanced/proxies/"&gt;proxies&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Custom &lt;a href="https://www.python-httpx.org/advanced/transports/"&gt;transports&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Additional &lt;a href="https://www.python-httpx.org/advanced/clients/"&gt;advanced&lt;/a&gt; functionality&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import httpx
from anthropic import Anthropic, DefaultHttpxClient

client = Anthropic(
    # Or use the `ANTHROPIC_BASE_URL` env var
    base_url="http://my.test.server.example.com:8083",
    http_client=DefaultHttpxClient(
        proxy="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also customize the client on a per-request basis by using &lt;code&gt;with_options()&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;client.with_options(http_client=DefaultHttpxClient(...))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Managing HTTP resources&lt;/h3&gt; 
&lt;p&gt;By default the library closes underlying HTTP connections whenever the client is &lt;a href="https://docs.python.org/3/reference/datamodel.html#object.__del__"&gt;garbage collected&lt;/a&gt;. You can manually close the client using the &lt;code&gt;.close()&lt;/code&gt; method if desired, or with a context manager that closes when exiting.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;from anthropic import Anthropic

with Anthropic() as client:
  # make requests here
  ...

# HTTP client is now closed
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Versioning&lt;/h2&gt; 
&lt;p&gt;This package generally follows &lt;a href="https://semver.org/spec/v2.0.0.html"&gt;SemVer&lt;/a&gt; conventions, though certain backwards-incompatible changes may be released as minor versions:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Changes that only affect static types, without breaking runtime behavior.&lt;/li&gt; 
 &lt;li&gt;Changes to library internals which are technically public but not intended or documented for external use. &lt;em&gt;(Please open a GitHub issue to let us know if you are relying on such internals.)&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Changes that we do not expect to impact the vast majority of users in practice.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.&lt;/p&gt; 
&lt;p&gt;We are keen for your feedback; please open an &lt;a href="https://www.github.com/anthropics/anthropic-sdk-python/issues"&gt;issue&lt;/a&gt; with questions, bugs, or suggestions.&lt;/p&gt; 
&lt;h3&gt;Determining the installed version&lt;/h3&gt; 
&lt;p&gt;If you've upgraded to the latest version but aren't seeing any new features you were expecting then your python environment is likely still using an older version.&lt;/p&gt; 
&lt;p&gt;You can determine the version that is being used at runtime with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import anthropic
print(anthropic.__version__)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;Python 3.8 or higher.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/anthropics/anthropic-sdk-python/main/CONTRIBUTING.md"&gt;the contributing documentation&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PaddlePaddle/PaddleOCR</title>
      <link>https://github.com/PaddlePaddle/PaddleOCR</link>
      <description>&lt;p&gt;Turn any PDF or image document into structured data for your AI. A powerful, lightweight OCR toolkit that bridges the gap between images/PDFs and LLMs. Supports 100+ languages.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/Banner.png" alt="PaddleOCR Banner" /&gt; &lt;/p&gt; 
 &lt;p&gt;English | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_cn.md"&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_tcn.md"&gt;ç¹é«”ä¸­æ–‡&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ja.md"&gt;æ—¥æœ¬èª&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ko.md"&gt;í•œêµ­ì–´&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_fr.md"&gt;FranÃ§ais&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ru.md"&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_es.md"&gt;EspaÃ±ol&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ar.md"&gt;Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©&lt;/a&gt;&lt;/p&gt; 
 &lt;!-- icon --&gt; 
 &lt;p&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleOCR"&gt;&lt;img src="https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?color=ccf" alt="stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/PaddlePaddle/PaddleOCR"&gt;&lt;img src="https://img.shields.io/github/forks/PaddlePaddle/PaddleOCR.svg?sanitize=true" alt="forks" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2507.05595"&gt;&lt;img src="https://img.shields.io/badge/PaddleOCR_3.0-Technical%20Report-b31b1b.svg?logo=arXiv" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2510.14528"&gt;&lt;img src="https://img.shields.io/badge/PaddleOCR--VL-Technical%20Report-b31b1b.svg?logo=arXiv" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://pepy.tech/projectsproject/paddleocr"&gt;&lt;img src="https://static.pepy.tech/badge/paddleocr/month" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/projects/paddleocr"&gt;&lt;img src="https://static.pepy.tech/badge/paddleocr" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/PaddlePaddle/PaddleOCR/network/dependents"&gt;&lt;img src="https://img.shields.io/badge/Used%20by-6k%2B%20repositories-blue" alt="Used by" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/paddleocr/"&gt;&lt;img src="https://img.shields.io/pypi/v/paddleocr" alt="PyPI version" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/python-3.8~3.12-aff.svg?sanitize=true" alt="python" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg?sanitize=true" alt="os" /&gt; &lt;img src="https://img.shields.io/badge/hardware-cpu%2C%20gpu%2C%20xpu%2C%20npu-yellow.svg?sanitize=true" alt="hardware" /&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-Apache_2.0-green" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/PaddlePaddle/PaddleOCR"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt; &lt;a href="https://www.paddleocr.com"&gt;&lt;img src="https://img.shields.io/badge/PaddleOCR-_Offiical_Website-1927BA?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="AI Studio" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;PaddleOCR is an industry-leading, production-ready OCR and document AI engine, offering end-to-end solutions from text extraction to intelligent document understanding&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h1&gt;PaddleOCR&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://www.paddlepaddle.org.cn/en"&gt;&lt;img src="https://img.shields.io/badge/PaddlePaddle-3.0-orange" alt="Framework" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Recognition%20Accuracy-%F0%9F%8F%86-green" alt="Accuracy" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Support_Languages-100+-brightgreen" alt="Multi-Language" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Handwriting-%E2%9C%93-success" alt="Handwriting" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Heterogeneous%20Hardware-Kunlunxin%20%7C%20Ascend_NPU-red" alt="Hardware" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] PaddleOCR now provides an MCP server that supports integration with Agent applications like Claude Desktop. For details, please refer to &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html"&gt;PaddleOCR MCP Server&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;The PaddleOCR 3.0 Technical Report is now available. See details at: &lt;a href="https://arxiv.org/abs/2507.05595"&gt;PaddleOCR 3.0 Technical Report&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;The PaddleOCR-VL Technical Report is now available. See details at &lt;a href="https://arxiv.org/abs/2510.14528"&gt;PaddleOCR-VL Technical Report&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;The Beta version of the PaddleOCR official website is now live, offering a more convenient online experience and large-scale PDF file parsing, as well as free API and MCP services. For more details, please visit the &lt;a href="https://www.paddleocr.com"&gt;PaddleOCR official website&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;PaddleOCR&lt;/strong&gt; converts documents and images into &lt;strong&gt;structured, AI-friendly data&lt;/strong&gt; (like JSON and Markdown) with &lt;strong&gt;industry-leading accuracy&lt;/strong&gt;â€”powering AI applications for everyone from indie developers and startups to large enterprises worldwide. With over &lt;strong&gt;60,000 stars&lt;/strong&gt; and deep integration into leading projects like &lt;strong&gt;MinerU, RAGFlow, pathway and cherry-studio&lt;/strong&gt;, PaddleOCR has become the &lt;strong&gt;premier solution&lt;/strong&gt; for developers building intelligent document applications in the &lt;strong&gt;AI era&lt;/strong&gt;.&lt;/p&gt; 
&lt;h3&gt;PaddleOCR 3.0 Core Features&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/PaddlePaddle/PaddleOCR-VL_Online_Demo"&gt;&lt;img src="https://img.shields.io/badge/PaddleOCR--VL-_Demo_on_HuggingFace-yellow?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="HuggingFace" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/application/detail/98365"&gt;&lt;img src="https://img.shields.io/badge/PaddleOCR--VL-_Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://www.modelscope.cn/studios/PaddlePaddle/PaddleOCR-VL_Online_Demo"&gt;&lt;img src="https://img.shields.io/badge/PaddleOCR--VL-_Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&amp;amp;labelColor=white" alt="ModelScope" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://aistudio.baidu.com/community/app/91660/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP--OCRv5-Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518494/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP--StructureV3-Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518493/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP--ChatOCRv4-Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAgAAAAIACAMAAADDpiTIAAAABlBMVEU2P+X///+1KuUwAAAHKklEQVR42u3dS5bjOAwEwALvf2fMavZum6IAImI7b2yYSqU+1Zb//gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADKCR/+fzly7rD92yVg69xh8zeLwOa5w+ZvFYHtc4ft3ykB++cOm79PAp6YO2z/Ngl4ZO5l+9+yT4QAvLqS748VF33Ylzdvzpl72f6z53YIGJ6SZdPeNHcIwOycaADdLgCSIgAIgCOAACAAykIAEAAEAAFAABCAT+WQuQVgeBqXhXQIQAAYegowLQBpbg3gZGFyAC6vgBQAMREA2/YfDPxyaDQNyTNz+3Zwn5J4ZG7PB2h0kHhi7plPCImmJwkPzO0RMa3OET0i5uGlzHFze0xcu0vE2Dq3J4U2vEPgSaHbFzPNDQAAAAAAAMBNovdw+cP/ny+uaf7w/+eYADy8kE+F4Offdjn6zZXhAXgiA78G4MNNsmnu1Xr7b3mbOL8T5Ja5bw/A35EC2LiWpzt1y9jRugBy30fLg3NvHPvnuZcC2NsCUXA/aRmA89V07Fwgt37uH8deCmBr6N44pP4UgaUATpdA7v/cMbIB8okliY65/SW5HhJ1ehPmM+8edwXgpbu4R88FayR32Y/P7oZZbOx13/Zr//ZHx27bAPnkFoyewYlbAhD3TvBobr95gaUAtr1EdNx1lgI4OcTTuR3z6+FZMEDRcu9ZCuDgGCdyGxMa4EgBRMvcjrkM7NgBZw5c0TwAUWUhZwRXA2xaya65Xa3jO2qYZ8bu2AD5w38tG5V8aZpoGN6Tz0bOfa9bceyWAciTO0jWyO1Tc5cLwJmF/JfPnXVyu3/slgHIg1n79O2O5fZv+1cHV7sC2HYqmUdHysNzX3sVkMcjUK5Gc+dMs28E5bGtm0V3gloBOP9vgZv+4sYn3RUaYFMCol5uN77g6lUApc8pWs69Zn7snS9Z9Q8G0S0AUTVUUTG3A54R1KSvo/diLAv5fKzynZeN6xogC75u93+AtBTA47OlAFSv6qY/vp3DAjD8iv2ZdFYJwKynMhTK1rInPfzaxW81LnvSgFP9KxrATaCLA3DxHpbFX31ZyNm5XRZyXG5bNkAWfP0rcrsUwOgC6NIAzgBcBiqAWwPgLrAGuGBP6jr2sifdfiJ6QQM4Bbw4AK4B3129ZSFn53ZZyA/GyFty27IBFMDFAXAG8PbyLQv5xULGPRl0K3h2AbwcgCZPhs+LD1zLnjS6AN4NwMU/DVFh7LyhASreTbvqrxdr/J4XT4Swz4FrTS+AGJ7bNbwAYkxuWzZAVljHrJfbjb9wviYXwFO/FJ8Vli4vaICsEMFyBbA3tmtsAUS0zG1c/bj4YwsZH2/+Whd0+1Nb+S7IE2sfPw4RL0XmsR8Nqvz7qFngmPHF34EqjP15AAofAkosZKPC/K6FVoeP02Ehi540NG6AK/4pYP3cLgVwXwHkDQ1QcSGb/uF4WwCmfX8u/+4vgLINcMUlQIfcLgXwXAF0+BGkpQDuuJx7/hwgpu//cWVuO3wxJOz/z8297vgYBwaIO3O7Kn+c194578ltywbIgu8fl+Z2lS+APvnLjnOv8hsgSqxjgwL4Ln9LAezaj98tgPzy7ZcC+GQzxrWxXQpgx370dm6/H7v6jaBoso5dY1swAFlwHWvfBf5pxVa93fCtdx64+1dsgCy4joWvAfPX9VoKYMs6Zse9/8Mlvv7LILlhAfKFFdsSutJXAdFkL3qlADJPrXFcXAC5KYaH586jO9mtAch9S3T0GQJ726ZWAE49kjP3rlDJuetdaL/1zeqZY9c7CRz7s0wCUPxienQBnAuAAtAAlxaAAAxfyBQABSAACkAAFIAAKAABUAACMEkKwL170oh7V8ueNLoAjgTAXWAN4BRwcABcA2oABTA4AApAAyiAwQFQABpAAQwOgALQADMWUgCuEmNyu15fSIY3gFPAiwPgFFADKIDBAVAAGkABCIACmBqAUAAaQAHMDUCMWkgBuMWw3K43F5LhDeAU8OIAuAmkARTA4AAoAA2gAARAAUwNgLvAGkABDA6Au8AaoKOJuV0vLSTDG8Ap4MUBcBNIAyiAwQFQABpAAQwOgALQAApAABTA1AC4C6wBOhqb23V+IRneAE4BLw6Aa0ANoAAGB0ABaAAFMDgACkADKAABUABTA+AusAboKATAQs4trjV+IYcfuJYCcA6gAATAQk69dFkKQANYyLkFcLIBFIDLQAVwawDsSRrAEWBwAJwCagAFMDgACkADKIDBAVAAGkABCIACmBoAzwXWAApgcADsSRrg0iNACoACEADXgAIwdCFTACykALgGFIAfl0kBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPBv/gN+IH8U6YveYgAAAABJRU5ErkJggg==&amp;amp;labelColor=white" alt="AI Studio" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PaddleOCR-VL - Multilingual Document Parsing via a 0.9B VLM&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;The SOTA and resource-efficient model tailored for document parsing&lt;/strong&gt;, that supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-OCRv5 â€” Universal Scene Text Recognition&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;Single model supports five text types&lt;/strong&gt; (Simplified Chinese, Traditional Chinese, English, Japanese, and Pinyin) with &lt;strong&gt;13% accuracy improvement&lt;/strong&gt;. Solves multilingual mixed document recognition challenges.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-StructureV3 â€” Complex Document Parsing&lt;/strong&gt;&lt;br /&gt; Intelligently converts complex PDFs and document images into &lt;strong&gt;Markdown and JSON files that preserve original structure&lt;/strong&gt;. &lt;strong&gt;Outperforms&lt;/strong&gt; numerous commercial solutions in public benchmarks. &lt;strong&gt;Perfectly maintains document layout and hierarchical structure&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-ChatOCRv4 â€” Intelligent Information Extraction&lt;/strong&gt;&lt;br /&gt; Natively integrates ERNIE 4.5 to &lt;strong&gt;precisely extract key information&lt;/strong&gt; from massive documents, with 15% accuracy improvement over previous generation. Makes documents "&lt;strong&gt;understand&lt;/strong&gt;" your questions and provide accurate answers.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In addition to providing an outstanding model library, PaddleOCR 3.0 also offers user-friendly tools covering model training, inference, and service deployment, so developers can rapidly bring AI applications to production.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/Arch.jpg" alt="PaddleOCR Architecture" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Special Note&lt;/strong&gt;: PaddleOCR 3.x introduces several significant interface changes. &lt;strong&gt;Old code written based on PaddleOCR 2.x is likely incompatible with PaddleOCR 3.x&lt;/strong&gt;. Please ensure that the documentation you are reading matches the version of PaddleOCR you are using. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/upgrade_notes.html"&gt;This document&lt;/a&gt; explains the reasons for the upgrade and the major changes from PaddleOCR 2.x to 3.x.&lt;/p&gt; 
&lt;h2&gt;ğŸ“£ Recent updates&lt;/h2&gt; 
&lt;h3&gt;ğŸ”¥ğŸ”¥ 2025.10.16: PaddleOCR 3.3.0 released, includes:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Released PaddleOCR-VL:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Model Introduction&lt;/strong&gt;:&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;PaddleOCR-VL&lt;/strong&gt; is a SOTA and resource-efficient model tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a compact yet powerful vision-language model (VLM) that integrates a NaViT-style dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to enable accurate element recognition. &lt;strong&gt;This innovative model efficiently supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption&lt;/strong&gt;. Through comprehensive evaluations on widely used public benchmarks and in-house benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document parsing and element-level recognition. It significantly outperforms existing solutions, exhibits strong competitiveness against top-tier VLMs, and delivers fast inference speeds. These strengths make it highly suitable for practical deployment in real-world scenarios. The model has been released on &lt;a href="https://huggingface.co/PaddlePaddle/PaddleOCR-VL"&gt;HuggingFace&lt;/a&gt;. Everyone is welcome to download and use it! More introduction infomation can be found in &lt;a href="https://www.paddleocr.ai/latest/version3.x/algorithm/PaddleOCR-VL/PaddleOCR-VL.html"&gt;PaddleOCR-VL&lt;/a&gt;.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Core Features&lt;/strong&gt;:&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Compact yet Powerful VLM Architecture&lt;/strong&gt;: We present a novel vision-language model that is specifically designed for resource-efficient inference, achieving outstanding performance in element recognition. By integrating a NaViT-style dynamic high-resolution visual encoder with the lightweight ERNIE-4.5-0.3B language model, we significantly enhance the modelâ€™s recognition capabilities and decoding efficiency. This integration maintains high accuracy while reducing computational demands, making it well-suited for efficient and practical document processing applications.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;SOTA Performance on Document Parsing&lt;/strong&gt;: PaddleOCR-VL achieves state-of-the-art performance in both page-level document parsing and element-level recognition. It significantly outperforms existing pipeline-based solutions and exhibiting strong competitiveness against leading vision-language models (VLMs) in document parsing. Moreover, it excels in recognizing complex document elements, such as text, tables, formulas, and charts, making it suitable for a wide range of challenging content types, including handwritten text and historical documents. This makes it highly versatile and suitable for a wide range of document types and scenarios.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Multilingual Support&lt;/strong&gt;: PaddleOCR-VL Supports 109 languages, covering major global languages, including but not limited to Chinese, English, Japanese, Latin, and Korean, as well as languages with different scripts and structures, such as Russian (Cyrillic script), Arabic, Hindi (Devanagari script), and Thai. This broad language coverage substantially enhances the applicability of our system to multilingual and globalized document processing scenarios.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Released PP-OCRv5 Multilingual Recognition Model:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Improved the accuracy and coverage of Latin script recognition; added support for Cyrillic, Arabic, Devanagari, Telugu, Tamil, and other language systems, covering recognition of 109 languages. The model has only 2M parameters, and the accuracy of some models has increased by over 40% compared to the previous generation.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.08.21: Release of PaddleOCR 3.2.0&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Significant Model Additions:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Introduced training, inference, and deployment for PP-OCRv5 recognition models in English, Thai, and Greek. &lt;strong&gt;The PP-OCRv5 English model delivers an 11% improvement in English scenarios compared to the main PP-OCRv5 model, with the Thai and Greek recognition models achieving accuracies of 82.68% and 89.28%, respectively.&lt;/strong&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deployment Capability Upgrades:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Full support for PaddlePaddle framework versions 3.1.0 and 3.1.1.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Comprehensive upgrade of the PP-OCRv5 C++ local deployment solution, now supporting both Linux and Windows, with feature parity and identical accuracy to the Python implementation.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;High-performance inference now supports CUDA 12, and inference can be performed using either the Paddle Inference or ONNX Runtime backends.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;The high-stability service-oriented deployment solution is now fully open-sourced, allowing users to customize Docker images and SDKs as required.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;The high-stability service-oriented deployment solution also supports invocation via manually constructed HTTP requests, enabling client-side code development in any programming language.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Benchmark Support:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;All production lines now support fine-grained benchmarking, enabling measurement of end-to-end inference time as well as per-layer and per-module latency data to assist with performance analysis. &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/version3.x/pipeline_usage/instructions/benchmark.en.md"&gt;Here's&lt;/a&gt; how to set up and use the benchmark feature.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Documentation has been updated to include key metrics for commonly used configurations on mainstream hardware, such as inference latency and memory usage, providing deployment references for users.&lt;/strong&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Resolved the issue of failed log saving during model training.&lt;/li&gt; 
    &lt;li&gt;Upgraded the data augmentation component for formula models for compatibility with newer versions of the albumentations dependency, and fixed deadlock warnings when using the tokenizers package in multi-process scenarios.&lt;/li&gt; 
    &lt;li&gt;Fixed inconsistencies in switch behaviors (e.g., &lt;code&gt;use_chart_parsing&lt;/code&gt;) in the PP-StructureV3 configuration files compared to other pipelines.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Other Enhancements:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Separated core and optional dependencies. Only minimal core dependencies are required for basic text recognition; additional dependencies for document parsing and information extraction can be installed as needed.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Enabled support for NVIDIA RTX 50 series graphics cards on Windows; users can refer to the &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/version3.x/installation.en.md"&gt;installation guide&lt;/a&gt; for the corresponding PaddlePaddle framework versions.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;PP-OCR series models now support returning single-character coordinates.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Added AIStudio, ModelScope, and other model download sources, allowing users to specify the source for model downloads.&lt;/li&gt; 
    &lt;li&gt;Added support for chart-to-table conversion via the PP-Chart2Table module.&lt;/li&gt; 
    &lt;li&gt;Optimized documentation descriptions to improve usability.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.08.15: PaddleOCR 3.1.1 Released&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Added the missing methods &lt;code&gt;save_vector&lt;/code&gt;, &lt;code&gt;save_visual_info_list&lt;/code&gt;, &lt;code&gt;load_vector&lt;/code&gt;, and &lt;code&gt;load_visual_info_list&lt;/code&gt; in the &lt;code&gt;PP-ChatOCRv4&lt;/code&gt; class.&lt;/li&gt; 
    &lt;li&gt;Added the missing parameters &lt;code&gt;glossary&lt;/code&gt; and &lt;code&gt;llm_request_interval&lt;/code&gt; to the &lt;code&gt;translate&lt;/code&gt; method in the &lt;code&gt;PPDocTranslation&lt;/code&gt; class.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Improvements:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Added a demo to the MCP documentation.&lt;/li&gt; 
    &lt;li&gt;Added information about the PaddlePaddle and PaddleOCR version used for performance metrics testing in the documentation.&lt;/li&gt; 
    &lt;li&gt;Fixed errors and omissions in the production line document translation.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Others:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Changed the MCP server dependency to use the pure Python library &lt;code&gt;puremagic&lt;/code&gt; instead of &lt;code&gt;python-magic&lt;/code&gt; to reduce installation issues.&lt;/li&gt; 
    &lt;li&gt;Retested PP-OCRv5 performance metrics with PaddleOCR version 3.1.0 and updated the documentation.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.29: PaddleOCR 3.1.0 Released&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Key Models and Pipelines:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Added PP-OCRv5 Multilingual Text Recognition Model&lt;/strong&gt;, which supports the training and inference process for text recognition models in 37 languages, including French, Spanish, Portuguese, Russian, Korean, etc. &lt;strong&gt;Average accuracy improved by over 30%.&lt;/strong&gt; &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/algorithm/PP-OCRv5/PP-OCRv5_multi_languages.html"&gt;Details&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Upgraded the &lt;strong&gt;PP-Chart2Table model&lt;/strong&gt; in PP-StructureV3, further enhancing the capability of converting charts to tables. On internal custom evaluation sets, the metric (RMS-F1) &lt;strong&gt;increased by 9.36 percentage points (71.24% -&amp;gt; 80.60%).&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Newly launched &lt;strong&gt;document translation pipeline, PP-DocTranslation, based on PP-StructureV3 and ERNIE 4.5&lt;/strong&gt;, which supports the translation of Markdown format documents, various complex-layout PDF documents, and document images, with the results saved as Markdown format documents. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/pipeline_usage/PP-DocTranslation.html"&gt;Details&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;New MCP server:&lt;/strong&gt; &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html"&gt;Details&lt;/a&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Supports both OCR and PP-StructureV3 pipelines.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Supports three working modes: local Python library, AIStudio Community Cloud Service, and self-hosted service.&lt;/li&gt; 
    &lt;li&gt;Supports invoking local services via stdio and remote services via Streamable HTTP.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Optimization:&lt;/strong&gt; Improved the descriptions in some user guides for a smoother reading experience.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.26: PaddleOCR 3.0.3 Released&lt;/strong&gt;&lt;/summary&gt; - Bug Fix: Resolved the issue where the `enable_mkldnn` parameter was not effective, restoring the default behavior of using MKL-DNN for CPU inference. 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.19: PaddleOCR 3.0.2 Released&lt;/strong&gt;&lt;/summary&gt; - **New Features:** 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;The default download source has been changed from &lt;code&gt;BOS&lt;/code&gt; to &lt;code&gt;HuggingFace&lt;/code&gt;. Users can also change the environment variable &lt;code&gt;PADDLE_PDX_MODEL_SOURCE&lt;/code&gt; to &lt;code&gt;BOS&lt;/code&gt; to set the model download source back to Baidu Object Storage (BOS).&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Added service invocation examples for six languagesâ€”C++, Java, Go, C#, Node.js, and PHPâ€”for pipelines like PP-OCRv5, PP-StructureV3, and PP-ChatOCRv4.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Improved the layout partition sorting algorithm in the PP-StructureV3 pipeline, enhancing the sorting logic for complex vertical layouts to deliver better results.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Enhanced model selection logic: when a language is specified but a model version is not, the system will automatically select the latest model version supporting that language.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Set a default upper limit for MKL-DNN cache size to prevent unlimited growth, while also allowing users to configure cache capacity.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Updated default configurations for high-performance inference to support Paddle MKL-DNN acceleration and optimized the logic for automatic configuration selection for smarter choices.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Adjusted the logic for obtaining the default device to consider the actual support for computing devices by the installed Paddle framework, making program behavior more intuitive.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Added Android example for PP-OCRv5. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/on_device_deployment.html"&gt;Details&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Fixed an issue with some CLI parameters in PP-StructureV3 not taking effect.&lt;/li&gt; 
    &lt;li&gt;Resolved an issue where &lt;code&gt;export_paddlex_config_to_yaml&lt;/code&gt; would not function correctly in certain cases.&lt;/li&gt; 
    &lt;li&gt;Corrected the discrepancy between the actual behavior of &lt;code&gt;save_path&lt;/code&gt; and its documentation description.&lt;/li&gt; 
    &lt;li&gt;Fixed potential multithreading errors when using MKL-DNN in basic service deployment.&lt;/li&gt; 
    &lt;li&gt;Corrected channel order errors in image preprocessing for the Latex-OCR model.&lt;/li&gt; 
    &lt;li&gt;Fixed channel order errors in saving visualized images within the text recognition module.&lt;/li&gt; 
    &lt;li&gt;Resolved channel order errors in visualized table results within PP-StructureV3 pipeline.&lt;/li&gt; 
    &lt;li&gt;Fixed an overflow issue in the calculation of &lt;code&gt;overlap_ratio&lt;/code&gt; under extremely special circumstances in the PP-StructureV3 pipeline.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Improvements:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Updated the description of the &lt;code&gt;enable_mkldnn&lt;/code&gt; parameter in the documentation to accurately reflect the program's actual behavior.&lt;/li&gt; 
    &lt;li&gt;Fixed errors in the documentation regarding the &lt;code&gt;lang&lt;/code&gt; and &lt;code&gt;ocr_version&lt;/code&gt; parameters.&lt;/li&gt; 
    &lt;li&gt;Added instructions for exporting pipeline configuration files via CLI.&lt;/li&gt; 
    &lt;li&gt;Fixed missing columns in the performance data table for PP-OCRv5.&lt;/li&gt; 
    &lt;li&gt;Refined benchmark metrics for PP-StructureV3 across different configurations.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Others:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Relaxed version restrictions on dependencies like numpy and pandas, restoring support for Python 3.12.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;History Log&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;2025.06.05: &lt;strong&gt;PaddleOCR 3.0.1 Released&lt;/strong&gt;, includes:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Optimisation of certain models and model configurations:&lt;/strong&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Updated the default model configuration for PP-OCRv5, changing both detection and recognition from mobile to server models. To improve default performance in most scenarios, the parameter &lt;code&gt;limit_side_len&lt;/code&gt; in the configuration has been changed from 736 to 64.&lt;/li&gt; 
    &lt;li&gt;Added a new text line orientation classification model &lt;code&gt;PP-LCNet_x1_0_textline_ori&lt;/code&gt; with an accuracy of 99.42%. The default text line orientation classifier for OCR, PP-StructureV3, and PP-ChatOCRv4 pipelines has been updated to this model.&lt;/li&gt; 
    &lt;li&gt;Optimized the text line orientation classification model &lt;code&gt;PP-LCNet_x0_25_textline_ori&lt;/code&gt;, improving accuracy by 3.3 percentage points to a current accuracy of 98.85%.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Optimizations and fixes for some issues in version 3.0.0, &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/update.html"&gt;details&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;ğŸ”¥ğŸ”¥2025.05.20: Official Release of &lt;strong&gt;PaddleOCR v3.0&lt;/strong&gt;, including:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-OCRv5&lt;/strong&gt;: High-Accuracy Text Recognition Model for All Scenarios - Instant Text from Images/PDFs.&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;ğŸŒ Single-model support for &lt;strong&gt;five&lt;/strong&gt; text types - Seamlessly process &lt;strong&gt;Simplified Chinese, Traditional Chinese, Simplified Chinese Pinyin, English&lt;/strong&gt; and &lt;strong&gt;Japanese&lt;/strong&gt; within a single model.&lt;/li&gt; 
    &lt;li&gt;âœï¸ Improved &lt;strong&gt;handwriting recognition&lt;/strong&gt;: Significantly better at complex cursive scripts and non-standard handwriting.&lt;/li&gt; 
    &lt;li&gt;ğŸ¯ &lt;strong&gt;13-point accuracy gain&lt;/strong&gt; over PP-OCRv4, achieving state-of-the-art performance across a variety of real-world scenarios.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-StructureV3&lt;/strong&gt;: General-Purpose Document Parsing â€“ Unleash SOTA Images/PDFs Parsing for Real-World Scenarios!&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;ğŸ§® &lt;strong&gt;High-Accuracy multi-scene PDF parsing&lt;/strong&gt;, leading both open- and closed-source solutions on the OmniDocBench benchmark.&lt;/li&gt; 
    &lt;li&gt;ğŸ§  Specialized capabilities include &lt;strong&gt;seal recognition&lt;/strong&gt;, &lt;strong&gt;chart-to-table conversion&lt;/strong&gt;, &lt;strong&gt;table recognition with nested formulas/images&lt;/strong&gt;, &lt;strong&gt;vertical text document parsing&lt;/strong&gt;, and &lt;strong&gt;complex table structure analysis&lt;/strong&gt;.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-ChatOCRv4&lt;/strong&gt;: Intelligent Document Understanding â€“ Extract Key Information, not just text from Images/PDFs.&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;ğŸ”¥ &lt;strong&gt;15-point accuracy gain&lt;/strong&gt; in key-information extraction on PDF/PNG/JPG files over the previous generation.&lt;/li&gt; 
    &lt;li&gt;ğŸ’» Native support for &lt;strong&gt;ERNIE 4.5&lt;/strong&gt;, with compatibility for large-model deployments via PaddleNLP, Ollama, vLLM, and more.&lt;/li&gt; 
    &lt;li&gt;ğŸ¤ Integrated &lt;a href="https://github.com/PaddlePaddle/PaddleMIX/tree/develop/paddlemix/examples/ppdocbee2"&gt;PP-DocBee2&lt;/a&gt;, enabling extraction and understanding of printed text, handwriting, seals, tables, charts, and other common elements in complex documents.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/update.html"&gt;History Log&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;âš¡ Quick Start&lt;/h2&gt; 
&lt;h3&gt;1. Run online demo&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aistudio.baidu.com/community/app/91660/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_OCRv5-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518494/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_StructureV3-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518493/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_ChatOCRv4-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;2. Installation&lt;/h3&gt; 
&lt;p&gt;Install PaddlePaddle refer to &lt;a href="https://www.paddlepaddle.org.cn/en/install/quick?docurl=/documentation/docs/en/develop/install/pip/linux-pip_en.html"&gt;Installation Guide&lt;/a&gt;, after then, install the PaddleOCR toolkit.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# If you only want to use the basic text recognition feature (returns text position coordinates and content), including the PP-OCR series
python -m pip install paddleocr
# If you want to use all features such as document parsing, document understanding, document translation, key information extraction, etc.
# python -m pip install "paddleocr[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Starting from version 3.2.0, in addition to the &lt;code&gt;all&lt;/code&gt; dependency group demonstrated above, PaddleOCR also supports installing partial optional features by specifying other dependency groups. All dependency groups provided by PaddleOCR are as follows:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Dependency Group Name&lt;/th&gt; 
   &lt;th&gt;Corresponding Functionality&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;doc-parser&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Document parsing: can be used to extract layout elements such as tables, formulas, stamps, images, etc. from documents; includes models like PP-StructureV3, PaddleOCR-VL&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ie&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Information extraction: can be used to extract key information from documents, such as names, dates, addresses, amounts, etc.; includes models like PP-ChatOCRv4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;trans&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Document translation: can be used to translate documents from one language to another; includes models like PP-DocTranslation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;all&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Complete functionality&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;3. Run inference by CLI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run PP-OCRv5 inference
paddleocr ocr -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png --use_doc_orientation_classify False --use_doc_unwarping False --use_textline_orientation False  

# Run PP-StructureV3 inference
paddleocr pp_structurev3 -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png --use_doc_orientation_classify False --use_doc_unwarping False

# Get the Qianfan API Key at first, and then run PP-ChatOCRv4 inference
paddleocr pp_chatocrv4_doc -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png -k é©¾é©¶å®¤å‡†ä¹˜äººæ•° --qianfan_api_key your_api_key --use_doc_orientation_classify False --use_doc_unwarping False 

# Run PaddleOCR-VL inference
paddleocr doc_parser -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/paddleocr_vl_demo.png

# Get more information about "paddleocr ocr"
paddleocr ocr --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4. Run inference by API&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;4.1 PP-OCRv5 Example&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Initialize PaddleOCR instance
from paddleocr import PaddleOCR
ocr = PaddleOCR(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False,
    use_textline_orientation=False)

# Run OCR inference on a sample image 
result = ocr.predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png")

# Visualize the results and save the JSON results
for res in result:
    res.print()
    res.save_to_img("output")
    res.save_to_json("output")
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;4.2 PP-StructureV3 Example&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from pathlib import Path
from paddleocr import PPStructureV3

pipeline = PPStructureV3(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False
)

# For Image
output = pipeline.predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png",
)

# Visualize the results and save the JSON results
for res in output:
    res.print() 
    res.save_to_json(save_path="output") 
    res.save_to_markdown(save_path="output")           
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;4.3 PP-ChatOCRv4 Example&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from paddleocr import PPChatOCRv4Doc

chat_bot_config = {
    "module_name": "chat_bot",
    "model_name": "ernie-3.5-8k",
    "base_url": "https://qianfan.baidubce.com/v2",
    "api_type": "openai",
    "api_key": "api_key",  # your api_key
}

retriever_config = {
    "module_name": "retriever",
    "model_name": "embedding-v1",
    "base_url": "https://qianfan.baidubce.com/v2",
    "api_type": "qianfan",
    "api_key": "api_key",  # your api_key
}

pipeline = PPChatOCRv4Doc(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False
)

visual_predict_res = pipeline.visual_predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png",
    use_common_ocr=True,
    use_seal_recognition=True,
    use_table_recognition=True,
)

mllm_predict_info = None
use_mllm = False
# If a multimodal large model is used, the local mllm service needs to be started. You can refer to the documentation: https://github.com/PaddlePaddle/PaddleX/blob/release/3.0/docs/pipeline_usage/tutorials/vlm_pipelines/doc_understanding.en.md performs deployment and updates the mllm_chat_bot_config configuration.
if use_mllm:
    mllm_chat_bot_config = {
        "module_name": "chat_bot",
        "model_name": "PP-DocBee",
        "base_url": "http://127.0.0.1:8080/",  # your local mllm service url
        "api_type": "openai",
        "api_key": "api_key",  # your api_key
    }

    mllm_predict_res = pipeline.mllm_pred(
        input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png",
        key_list=["é©¾é©¶å®¤å‡†ä¹˜äººæ•°"],
        mllm_chat_bot_config=mllm_chat_bot_config,
    )
    mllm_predict_info = mllm_predict_res["mllm_res"]

visual_info_list = []
for res in visual_predict_res:
    visual_info_list.append(res["visual_info"])
    layout_parsing_result = res["layout_parsing_result"]

vector_info = pipeline.build_vector(
    visual_info_list, flag_save_bytes_vector=True, retriever_config=retriever_config
)
chat_result = pipeline.chat(
    key_list=["é©¾é©¶å®¤å‡†ä¹˜äººæ•°"],
    visual_info=visual_info_list,
    vector_info=vector_info,
    mllm_predict_info=mllm_predict_info,
    chat_bot_config=chat_bot_config,
    retriever_config=retriever_config,
)
print(chat_result)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;4.4 PaddleOCR-VL Example&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from paddleocr import PaddleOCRVL

pipeline = PaddleOCRVL()
output = pipeline.predict("https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/paddleocr_vl_demo.png")
for res in output:
    res.print()
    res.save_to_json(save_path="output")
    res.save_to_markdown(save_path="output")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;5. Chinese Heterogeneous AI Accelerators&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/other_devices_support/paddlepaddle_install_NPU.html"&gt;Huawei Ascend&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/other_devices_support/paddlepaddle_install_XPU.html"&gt;KUNLUNXIN&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ§© More Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Convert models to ONNX format: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/obtaining_onnx_models.html"&gt;Obtaining ONNX Models&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Accelerate inference using engines like OpenVINO, ONNX Runtime, TensorRT, or perform inference using ONNX format models: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/high_performance_inference.html"&gt;High-Performance Inference&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Accelerate inference using multi-GPU and multi-process: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/pipeline_usage/instructions/parallel_inference.html"&gt;Parallel Inference for Pipelines&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Integrate PaddleOCR into applications written in C++, C#, Java, etc.: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/serving.html"&gt;Serving&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;â›°ï¸ Advanced Tutorials&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/OCR.html"&gt;PP-OCRv5 Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PP-StructureV3.html"&gt;PP-StructureV3 Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PP-ChatOCRv4.html"&gt;PP-ChatOCRv4 Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PaddleOCR-VL.html"&gt;PaddleOCR-VL Tutorial&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ”„ Quick Overview of Execution Results&lt;/h2&gt; 
&lt;h3&gt;PP-OCRv5&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/PP-OCRv5_demo.gif" alt="PP-OCRv5 Demo" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h3&gt;PP-StructureV3&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/PP-StructureV3_demo.gif" alt="PP-StructureV3 Demo" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h3&gt;PaddleOCR-VL&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/PaddleOCR-VL_demo.gif" alt="PP-StructureV3 Demo" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;âœ¨ Stay Tuned&lt;/h2&gt; 
&lt;p&gt;â­ &lt;strong&gt;Star this repository to keep up with exciting updates and new releases, including powerful OCR and document parsing capabilities!&lt;/strong&gt; â­&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="1200" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/star_paddleocr.en.gif" alt="Star-Project" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Community&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;PaddlePaddle WeChat official account&lt;/th&gt; 
    &lt;th align="center"&gt;Join the tech discussion group&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/refs/heads/main/images/paddleocr/README/qrcode_for_paddlepaddle_official_account.jpg" width="150" /&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/refs/heads/main/images/paddleocr/README/qr_code_for_the_questionnaire.jpg" width="150" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ˜ƒ Awesome Projects Leveraging PaddleOCR&lt;/h2&gt; 
&lt;p&gt;PaddleOCR wouldn't be where it is today without its incredible community! ğŸ’— A massive thank you to all our longtime partners, new collaborators, and everyone who's poured their passion into PaddleOCR â€” whether we've named you or not. Your support fuels our fire!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Project Name&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/infiniflow/ragflow"&gt;RAGFlow&lt;/a&gt; &lt;a href="https://github.com/infiniflow/ragflow"&gt;&lt;img src="https://img.shields.io/github/stars/infiniflow/ragflow" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;RAG engine based on deep document understanding.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/pathwaycom/pathway"&gt;pathway&lt;/a&gt; &lt;a href="https://github.com/pathwaycom/pathway"&gt;&lt;img src="https://img.shields.io/github/stars/pathwaycom/pathway" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/opendatalab/MinerU"&gt;MinerU&lt;/a&gt; &lt;a href="https://github.com/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/github/stars/opendatalab/MinerU" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Multi-type Document to Markdown Conversion Tool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hiroi-sora/Umi-OCR"&gt;Umi-OCR&lt;/a&gt; &lt;a href="https://github.com/hiroi-sora/Umi-OCR"&gt;&lt;img src="https://img.shields.io/github/stars/hiroi-sora/Umi-OCR" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Free, Open-source, Batch Offline OCR Software.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/CherryHQ/cherry-studio"&gt;cherry-studio&lt;/a&gt; &lt;a href="https://github.com/CherryHQ/cherry-studio"&gt;&lt;img src="https://img.shields.io/github/stars/CherryHQ/cherry-studio" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;A desktop client that supports for multiple LLM providers.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/OmniParser"&gt;OmniParser&lt;/a&gt;&lt;a href="https://github.com/microsoft/OmniParser"&gt;&lt;img src="https://img.shields.io/github/stars/microsoft/OmniParser" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;OmniParser: Screen Parsing tool for Pure Vision Based GUI Agent.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/netease-youdao/QAnything"&gt;QAnything&lt;/a&gt;&lt;a href="https://github.com/netease-youdao/QAnything"&gt;&lt;img src="https://img.shields.io/github/stars/netease-youdao/QAnything" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Question and Answer based on Anything.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;PDF-Extract-Kit&lt;/a&gt; &lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;&lt;img src="https://img.shields.io/github/stars/opendatalab/PDF-Extract-Kit" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;A powerful open-source toolkit designed to efficiently extract high-quality content from complex and diverse PDF documents.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/PantsuDango/Dango-Translator"&gt;Dango-Translator&lt;/a&gt;&lt;a href="https://github.com/PantsuDango/Dango-Translator"&gt;&lt;img src="https://img.shields.io/github/stars/PantsuDango/Dango-Translator" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Recognize text on the screen, translate it and show the translation results in real time.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/awesome_projects.md"&gt;Learn more projects&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/awesome_projects.md"&gt;More projects based on PaddleOCR&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Contributors&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/PaddlePaddle/PaddleOCR/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=PaddlePaddle/PaddleOCR&amp;amp;max=400&amp;amp;columns=20" width="800" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸŒŸ Star&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="800" src="https://api.star-history.com/svg?repos=PaddlePaddle/PaddleOCR&amp;amp;type=Date" alt="Star-history" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; 
&lt;p&gt;This project is released under the &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/LICENSE"&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ“ Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{cui2025paddleocr30technicalreport,
      title={PaddleOCR 3.0 Technical Report}, 
      author={Cheng Cui and Ting Sun and Manhui Lin and Tingquan Gao and Yubo Zhang and Jiaxuan Liu and Xueqing Wang and Zelun Zhang and Changda Zhou and Hongen Liu and Yue Zhang and Wenyu Lv and Kui Huang and Yichao Zhang and Jing Zhang and Jun Zhang and Yi Liu and Dianhai Yu and Yanjun Ma},
      year={2025},
      eprint={2507.05595},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2507.05595}, 
}

@misc{cui2025paddleocrvlboostingmultilingualdocument,
      title={PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model}, 
      author={Cheng Cui and Ting Sun and Suyin Liang and Tingquan Gao and Zelun Zhang and Jiaxuan Liu and Xueqing Wang and Changda Zhou and Hongen Liu and Manhui Lin and Yue Zhang and Yubo Zhang and Handong Zheng and Jing Zhang and Jun Zhang and Yi Liu and Dianhai Yu and Yanjun Ma},
      year={2025},
      eprint={2510.14528},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2510.14528}, 
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>allenai/olmocr</title>
      <link>https://github.com/allenai/olmocr</link>
      <description>&lt;p&gt;Toolkit for linearizing PDFs for LLM datasets/training&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img width="350" alt="olmocr-2-full@2x" src="https://github.com/user-attachments/assets/24f1b596-4059-46f1-8130-5d72dcc0b02e" /&gt; 
 &lt;hr /&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/allenai/OLMo/raw/main/LICENSE"&gt; &lt;img alt="GitHub License" src="https://img.shields.io/github/license/allenai/OLMo" /&gt; &lt;/a&gt; &lt;a href="https://github.com/allenai/olmocr/releases"&gt; &lt;img alt="GitHub release" src="https://img.shields.io/github/release/allenai/olmocr.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;a href="https://arxiv.org/abs/2502.18443"&gt; &lt;img alt="Tech Report v1" src="https://img.shields.io/badge/Paper_v1-olmOCR-blue" /&gt; &lt;/a&gt; &lt;a href="https://arxiv.org/abs/2510.19817"&gt; &lt;img alt="Tech Report v2" src="https://img.shields.io/badge/Paper_v2-olmOCR-blue" /&gt; &lt;/a&gt; &lt;a href="https://olmocr.allenai.org"&gt; &lt;img alt="Demo" src="https://img.shields.io/badge/Ai2-Demo-F0529C" /&gt; &lt;/a&gt; &lt;a href="https://discord.gg/sZq3jTNVNG"&gt; &lt;img alt="Discord" src="https://img.shields.io/badge/Discord%20-%20blue?style=flat&amp;amp;logo=discord&amp;amp;label=Ai2&amp;amp;color=%235B65E9" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;A toolkit for converting PDFs and other image-based document formats into clean, readable, plain text format.&lt;/p&gt; 
&lt;p&gt;Try the online demo: &lt;a href="https://olmocr.allenai.org/"&gt;https://olmocr.allenai.org/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Convert PDF, PNG, and JPEG based documents into clean Markdown&lt;/li&gt; 
 &lt;li&gt;Support for equations, tables, handwriting, and complex formatting&lt;/li&gt; 
 &lt;li&gt;Automatically removes headers and footers&lt;/li&gt; 
 &lt;li&gt;Convert into text with a natural reading order, even in the presence of figures, multi-column layouts, and insets&lt;/li&gt; 
 &lt;li&gt;Efficient, less than $200 USD per million pages converted&lt;/li&gt; 
 &lt;li&gt;(Based on a 7B parameter VLM, so it requires a GPU)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;News&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;October 21, 2025 - v0.4.0 - &lt;a href="https://huggingface.co/allenai/olmOCR-2-7B-1025-FP8"&gt;New model release&lt;/a&gt;, boosts olmOCR-bench score by ~4 points using synthetic data and introduces RL training.&lt;/li&gt; 
 &lt;li&gt;August 13, 2025 - v0.3.0 - &lt;a href="https://huggingface.co/allenai/olmOCR-7B-0825-FP8"&gt;New model release&lt;/a&gt;, fixes auto-rotation detection, and hallucinations on blank documents.&lt;/li&gt; 
 &lt;li&gt;July 24, 2025 - v0.2.1 - &lt;a href="https://huggingface.co/allenai/olmOCR-7B-0725-FP8"&gt;New model release&lt;/a&gt;, scores 3 points higher on &lt;a href="https://github.com/allenai/olmocr/tree/main/olmocr/bench"&gt;olmOCR-Bench&lt;/a&gt;, also runs significantly faster because it's default FP8, and needs much fewer retries per document.&lt;/li&gt; 
 &lt;li&gt;July 23, 2025 - v0.2.0 - New cleaned up &lt;a href="https://github.com/allenai/olmocr/tree/main/olmocr/train"&gt;trainer code&lt;/a&gt;, makes it much simpler to train olmOCR models yourself.&lt;/li&gt; 
 &lt;li&gt;June 17, 2025 - v0.1.75 - Switch from sglang to vllm based inference pipeline, updated docker image to CUDA 12.8.&lt;/li&gt; 
 &lt;li&gt;May 23, 2025 - v0.1.70 - Official docker support and images are now available! &lt;a href="https://raw.githubusercontent.com/allenai/olmocr/main/#using-docker"&gt;See Docker usage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;May 19, 2025 - v0.1.68 - &lt;a href="https://github.com/allenai/olmocr/tree/main/olmocr/bench"&gt;olmOCR-Bench&lt;/a&gt; launch, scoring 77.4. Launch includes 2 point performance boost in olmOCR pipeline due to bug fixes with prompts.&lt;/li&gt; 
 &lt;li&gt;Mar 17, 2025 - v0.1.60 - Performance improvements due to better temperature selection in sampling.&lt;/li&gt; 
 &lt;li&gt;Feb 25, 2025 - v0.1.58 - Initial public launch and demo.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Benchmark&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/allenai/olmocr/tree/main/olmocr/bench"&gt;&lt;strong&gt;olmOCR-Bench&lt;/strong&gt;&lt;/a&gt;: We also ship a comprehensive benchmark suite covering over 7,000 test cases across 1,400 documents to help measure performance of OCR systems.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;ArXiv&lt;/th&gt; 
   &lt;th&gt;Old&lt;br /&gt;scans&lt;br /&gt;math&lt;/th&gt; 
   &lt;th&gt;Tables&lt;/th&gt; 
   &lt;th&gt;Old&lt;br /&gt;scans&lt;/th&gt; 
   &lt;th&gt;Headers&lt;br /&gt;&amp;amp;&lt;br /&gt;footers&lt;/th&gt; 
   &lt;th&gt;Multi&lt;br /&gt;column&lt;/th&gt; 
   &lt;th&gt;Long&lt;br /&gt;tiny&lt;br /&gt;text&lt;/th&gt; 
   &lt;th&gt;Base&lt;/th&gt; 
   &lt;th&gt;Overall&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mistral OCR API&lt;/td&gt; 
   &lt;td&gt;77.2&lt;/td&gt; 
   &lt;td&gt;67.5&lt;/td&gt; 
   &lt;td&gt;60.6&lt;/td&gt; 
   &lt;td&gt;29.3&lt;/td&gt; 
   &lt;td&gt;93.6&lt;/td&gt; 
   &lt;td&gt;71.3&lt;/td&gt; 
   &lt;td&gt;77.1&lt;/td&gt; 
   &lt;td&gt;99.4&lt;/td&gt; 
   &lt;td&gt;72.0Â±1.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Marker 1.10.1&lt;/td&gt; 
   &lt;td&gt;83.8&lt;/td&gt; 
   &lt;td&gt;66.8&lt;/td&gt; 
   &lt;td&gt;72.9&lt;/td&gt; 
   &lt;td&gt;33.5&lt;/td&gt; 
   &lt;td&gt;86.6&lt;/td&gt; 
   &lt;td&gt;80.0&lt;/td&gt; 
   &lt;td&gt;85.7&lt;/td&gt; 
   &lt;td&gt;99.3&lt;/td&gt; 
   &lt;td&gt;76.1Â±1.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MinerU 2.5.4*&lt;/td&gt; 
   &lt;td&gt;76.6&lt;/td&gt; 
   &lt;td&gt;54.6&lt;/td&gt; 
   &lt;td&gt;84.9&lt;/td&gt; 
   &lt;td&gt;33.7&lt;/td&gt; 
   &lt;td&gt;96.6&lt;/td&gt; 
   &lt;td&gt;78.2&lt;/td&gt; 
   &lt;td&gt;83.5&lt;/td&gt; 
   &lt;td&gt;93.7&lt;/td&gt; 
   &lt;td&gt;75.2Â±1.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DeepSeek-OCR&lt;/td&gt; 
   &lt;td&gt;77.2&lt;/td&gt; 
   &lt;td&gt;73.6&lt;/td&gt; 
   &lt;td&gt;80.2&lt;/td&gt; 
   &lt;td&gt;33.3&lt;/td&gt; 
   &lt;td&gt;96.1&lt;/td&gt; 
   &lt;td&gt;66.4&lt;/td&gt; 
   &lt;td&gt;79.4&lt;/td&gt; 
   &lt;td&gt;99.8&lt;/td&gt; 
   &lt;td&gt;75.7Â±1.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Nanonets-OCR2-3B&lt;/td&gt; 
   &lt;td&gt;75.4&lt;/td&gt; 
   &lt;td&gt;46.1&lt;/td&gt; 
   &lt;td&gt;86.8&lt;/td&gt; 
   &lt;td&gt;40.9&lt;/td&gt; 
   &lt;td&gt;32.1&lt;/td&gt; 
   &lt;td&gt;81.9&lt;/td&gt; 
   &lt;td&gt;93.0&lt;/td&gt; 
   &lt;td&gt;99.6&lt;/td&gt; 
   &lt;td&gt;69.5Â±1.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PaddleOCR-VL*&lt;/td&gt; 
   &lt;td&gt;85.7&lt;/td&gt; 
   &lt;td&gt;71.0&lt;/td&gt; 
   &lt;td&gt;84.1&lt;/td&gt; 
   &lt;td&gt;37.8&lt;/td&gt; 
   &lt;td&gt;97.0&lt;/td&gt; 
   &lt;td&gt;79.9&lt;/td&gt; 
   &lt;td&gt;85.7&lt;/td&gt; 
   &lt;td&gt;98.5&lt;/td&gt; 
   &lt;td&gt;80.0Â±1.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Infinity-Parser 7B*&lt;/td&gt; 
   &lt;td&gt;84.4&lt;/td&gt; 
   &lt;td&gt;83.8&lt;/td&gt; 
   &lt;td&gt;85.0&lt;/td&gt; 
   &lt;td&gt;47.9&lt;/td&gt; 
   &lt;td&gt;88.7&lt;/td&gt; 
   &lt;td&gt;84.2&lt;/td&gt; 
   &lt;td&gt;86.4&lt;/td&gt; 
   &lt;td&gt;99.8&lt;/td&gt; 
   &lt;td&gt;82.5Â±?&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Chandra OCR 0.1.0*&lt;/td&gt; 
   &lt;td&gt;82.2&lt;/td&gt; 
   &lt;td&gt;80.3&lt;/td&gt; 
   &lt;td&gt;88.0&lt;/td&gt; 
   &lt;td&gt;50.4&lt;/td&gt; 
   &lt;td&gt;90.8&lt;/td&gt; 
   &lt;td&gt;81.2&lt;/td&gt; 
   &lt;td&gt;92.3&lt;/td&gt; 
   &lt;td&gt;99.9&lt;/td&gt; 
   &lt;td&gt;83.1Â±0.9&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td colspan="10"&gt;
    &lt;hr /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;olmOCR v0.4.0&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;83.0&lt;/td&gt; 
   &lt;td&gt;82.3&lt;/td&gt; 
   &lt;td&gt;84.9&lt;/td&gt; 
   &lt;td&gt;47.7&lt;/td&gt; 
   &lt;td&gt;96.1&lt;/td&gt; 
   &lt;td&gt;83.7&lt;/td&gt; 
   &lt;td&gt;81.9&lt;/td&gt; 
   &lt;td&gt;99.7&lt;/td&gt; 
   &lt;td&gt;82.4Â±1.1&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;Requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Recent NVIDIA GPU (tested on RTX 4090, L40S, A100, H100) with at least 15 GB of GPU RAM&lt;/li&gt; 
 &lt;li&gt;30GB of free disk space&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You will need to install poppler-utils and additional fonts for rendering PDF images.&lt;/p&gt; 
&lt;p&gt;Install dependencies (Ubuntu/Debian)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update
sudo apt-get install poppler-utils ttf-mscorefonts-installer msttcorefonts fonts-crosextra-caladea fonts-crosextra-carlito gsfonts lcdf-typetools
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Set up a conda environment and install olmocr. The requirements for running olmOCR are difficult to install in an existing python environment, so please do make a clean python environment to install into.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n olmocr python=3.11
conda activate olmocr

# For CPU-only operations, ex running the benchmark
pip install olmocr[bench]

# For actually converting the files with your own GPU
pip install olmocr[gpu]  --extra-index-url https://download.pytorch.org/whl/cu128

# Recommended: Install flash infer for faster inference on GPU
pip install https://download.pytorch.org/whl/cu128/flashinfer/flashinfer_python-0.2.5%2Bcu128torch2.7-cp38-abi3-linux_x86_64.whl
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Local Usage Example&lt;/h3&gt; 
&lt;p&gt;For quick testing, try the &lt;a href="https://olmocr.allen.ai/"&gt;web demo&lt;/a&gt;. To run locally, a GPU is required, as inference is powered by &lt;a href="https://github.com/sgl-project/sglang"&gt;sglang&lt;/a&gt; under the hood.&lt;/p&gt; 
&lt;p&gt;Convert a Single PDF:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Download a sample PDF
curl -o olmocr-sample.pdf https://olmocr.allenai.org/papers/olmocr_3pg_sample.pdf

# Convert it to markdown
python -m olmocr.pipeline ./localworkspace --markdown --pdfs olmocr-sample.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Convert an Image file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m olmocr.pipeline ./localworkspace --markdown --pdfs random_page.png
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Convert Multiple PDFs:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m olmocr.pipeline ./localworkspace --markdown --pdfs tests/gnarly_pdfs/*.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With the addition of the &lt;code&gt;--markdown&lt;/code&gt; flag, results will be stored as markdown files inside of &lt;code&gt;./localworkspace/markdown/&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;Viewing Results&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;./localworkspace/&lt;/code&gt; workspace folder will then have both &lt;a href="https://github.com/allenai/dolma"&gt;Dolma&lt;/a&gt; and markdown files (if using &lt;code&gt;--markdown&lt;/code&gt;).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cat localworkspace/markdown/olmocr-sample.md 
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using an Inference Provider or External Server&lt;/h3&gt; 
&lt;p&gt;If you have a vLLM server already running elsewhere (or any inference platform implementing the OpenAI API), you can point olmOCR to use it instead of spawning a local instance:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Use external vLLM server instead of local one
python -m olmocr.pipeline ./localworkspace --server http://remote-server:8000/v1 --markdown --pdfs tests/gnarly_pdfs/*.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The served model name should be &lt;code&gt;olmocr&lt;/code&gt;. An example vLLM launch command would be:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vllm serve allenai/olmOCR-2-7B-1025-FP8 --served-model-name olmocr --max-model-len 16384
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Verified External Providers&lt;/h4&gt; 
&lt;p&gt;We have tested &lt;code&gt;olmOCR-2-7B-1025-FP8&lt;/code&gt; on these external model providers and confirmed that they work&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;$/1M Input tokens&lt;/th&gt; 
   &lt;th&gt;$/1M Output tokens&lt;/th&gt; 
   &lt;th&gt;Example Command&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://ai2endpoints.cirrascale.ai/models/overview"&gt;Cirrascale&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;$0.07&lt;/td&gt; 
   &lt;td&gt;$0.15&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;python -m olmocr.pipeline ./localworkspace1 --server https://ai2endpoints.cirrascale.ai/api --api_key sk-XXXXXXX --model olmOCR-2-7B-1025 --pdfs tests/gnarly_pdfs/*.pdf&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://deepinfra.com/"&gt;DeepInfra&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;$0.09&lt;/td&gt; 
   &lt;td&gt;$0.19&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;python -m olmocr.pipeline ./localworkspace1 --server https://api.deepinfra.com/v1/openai --api_key DfXXXXXXX --model allenai/olmOCR-2-7B-1025 --pdfs tests/gnarly_pdfs/*.pdf&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.saas.parasail.io/serverless?name=olmocr-7b-1025-fp8"&gt;Parasail&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;$0.10&lt;/td&gt; 
   &lt;td&gt;$0.20&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;python -m olmocr.pipeline ./localworkspace1 --server https://api.parasail.io/v1 --api_key psk-XXXXX --model allenai/olmOCR-2-7B-1025 --pdfs tests/gnarly_pdfs/*.pdf&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Notes on arguments&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--server&lt;/code&gt;: Defines the OpenAI-compatible endpoint: ex &lt;code&gt;https://api.deepinfra.com/v1/openai&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--api_key&lt;/code&gt;: Your API key, bassed in via Authorization Bearer HTTP header&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--pages_per_group&lt;/code&gt;: You may want a smaller number of pages per group as many external provides have lower concurrent request limits&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model&lt;/code&gt;: The model identifier, ex. &lt;code&gt;allenai/olmOCR-2-7B-1025&lt;/code&gt;, different providers have different names, and if you run locally, you can use &lt;code&gt;olmocr&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Other arguments work the same as with local inference&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Multi-node / Cluster Usage&lt;/h3&gt; 
&lt;p&gt;If you want to convert millions of PDFs, using multiple nodes running in parallel, then olmOCR supports reading your PDFs from AWS S3, and coordinating work using an AWS S3 output bucket.&lt;/p&gt; 
&lt;p&gt;For example, you can start this command on your first worker node, and it will set up a simple work queue in your AWS bucket and start converting PDFs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace --pdfs s3://my_s3_bucket/jakep/gnarly_pdfs/*.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now on any subsequent nodes, just run this and they will start grabbing items from the same workspace queue.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are at Ai2 and want to linearize millions of PDFs efficiently using &lt;a href="https://www.beaker.org"&gt;beaker&lt;/a&gt;, just add the &lt;code&gt;--beaker&lt;/code&gt; flag. This will prepare the workspace on your local machine, and then launch N GPU workers in the cluster to start converting PDFs.&lt;/p&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace --pdfs s3://my_s3_bucket/jakep/gnarly_pdfs/*.pdf --beaker --beaker_gpus 4
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using Docker&lt;/h3&gt; 
&lt;p&gt;Pull the Docker image.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker pull alleninstituteforai/olmocr:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run the container interactively:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -it --gpus all --name olmocr_container alleninstituteforai/olmocr:latest /bin/bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to access your local files inside the container, use volume mounting:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -it --gpus all \
  -v /path/to/your/local/files:/local_files \
  --name olmocr_container \
  alleninstituteforai/olmocr:latest /bin/bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;All dependencies are already installed. Once youâ€™re inside the container, you can run olmOCR commands. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -o olmocr-sample.pdf https://olmocr.allenai.org/papers/olmocr_3pg_sample.pdf

python -m olmocr.pipeline ./localworkspace --markdown --pdfs olmocr-sample.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;You can also visit our Docker repository on &lt;a href="https://hub.docker.com/r/alleninstituteforai/olmocr"&gt;Docker Hub&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Full documentation for the pipeline&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m olmocr.pipeline --help
usage: pipeline.py [-h] [--pdfs [PDFS ...]] [--model MODEL] [--workspace_profile WORKSPACE_PROFILE] [--pdf_profile PDF_PROFILE] [--pages_per_group PAGES_PER_GROUP] [--max_page_retries MAX_PAGE_RETRIES] [--max_page_error_rate MAX_PAGE_ERROR_RATE] [--workers WORKERS]
                   [--apply_filter] [--stats] [--markdown] [--target_longest_image_dim TARGET_LONGEST_IMAGE_DIM] [--target_anchor_text_len TARGET_ANCHOR_TEXT_LEN] [--guided_decoding] [--gpu-memory-utilization GPU_MEMORY_UTILIZATION] [--max_model_len MAX_MODEL_LEN]
                   [--tensor-parallel-size TENSOR_PARALLEL_SIZE] [--data-parallel-size DATA_PARALLEL_SIZE] [--port PORT] [--server SERVER] [--beaker] [--beaker_workspace BEAKER_WORKSPACE] [--beaker_cluster BEAKER_CLUSTER] [--beaker_gpus BEAKER_GPUS] [--beaker_priority BEAKER_PRIORITY]
                   workspace

Manager for running millions of PDFs through a batch inference pipeline

positional arguments:
  workspace             The filesystem path where work will be stored, can be a local folder, or an s3 path if coordinating work with many workers, s3://bucket/prefix/

options:
  -h, --help            show this help message and exit
  --pdfs [PDFS ...]     Path to add pdfs stored in s3 to the workspace, can be a glob path s3://bucket/prefix/*.pdf or path to file containing list of pdf paths
  --model MODEL         Path where the model is located, allenai/olmOCR-7B-0725-FP8 is the default, can be local, s3, or hugging face.
  --workspace_profile WORKSPACE_PROFILE
                        S3 configuration profile for accessing the workspace
  --pdf_profile PDF_PROFILE
                        S3 configuration profile for accessing the raw pdf documents
  --pages_per_group PAGES_PER_GROUP
                        Aiming for this many pdf pages per work item group
  --max_page_retries MAX_PAGE_RETRIES
                        Max number of times we will retry rendering a page
  --max_page_error_rate MAX_PAGE_ERROR_RATE
                        Rate of allowable failed pages in a document, 1/250 by default
  --workers WORKERS     Number of workers to run at a time
  --apply_filter        Apply basic filtering to English pdfs which are not forms, and not likely seo spam
  --stats               Instead of running any job, reports some statistics about the current workspace
  --markdown            Also write natural text to markdown files preserving the folder structure of the input pdfs
  --target_longest_image_dim TARGET_LONGEST_IMAGE_DIM
                        Dimension on longest side to use for rendering the pdf pages
  --target_anchor_text_len TARGET_ANCHOR_TEXT_LEN
                        Maximum amount of anchor text to use (characters), not used for new models
  --guided_decoding     Enable guided decoding for model YAML type outputs

VLLM arguments:
  --gpu-memory-utilization GPU_MEMORY_UTILIZATION
                        Fraction of VRAM vLLM may pre-allocate for KV-cache (passed through to vllm serve).
  --max_model_len MAX_MODEL_LEN
                        Upper bound (tokens) vLLM will allocate KV-cache for, lower if VLLM won't start
  --tensor-parallel-size TENSOR_PARALLEL_SIZE, -tp TENSOR_PARALLEL_SIZE
                        Tensor parallel size for vLLM
  --data-parallel-size DATA_PARALLEL_SIZE, -dp DATA_PARALLEL_SIZE
                        Data parallel size for vLLM
  --port PORT           Port to use for the VLLM server
  --server SERVER       URL of external vLLM (or other compatible provider)
                        server (e.g., http://hostname:port). If provided,
                        skips spawning local vLLM instance

beaker/cluster execution:
  --beaker              Submit this job to beaker instead of running locally
  --beaker_workspace BEAKER_WORKSPACE
                        Beaker workspace to submit to
  --beaker_cluster BEAKER_CLUSTER
                        Beaker clusters you want to run on
  --beaker_gpus BEAKER_GPUS
                        Number of gpu replicas to run
  --beaker_priority BEAKER_PRIORITY
                        Beaker priority level for the job
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Code overview&lt;/h2&gt; 
&lt;p&gt;There are some nice reusable pieces of the code that may be useful for your own projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A prompting strategy to get really good natural text parsing using ChatGPT 4o - &lt;a href="https://github.com/allenai/olmocr/raw/main/olmocr/data/buildsilver.py"&gt;buildsilver.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Basic filtering by language and SEO spam removal - &lt;a href="https://github.com/allenai/olmocr/raw/main/olmocr/filter/filter.py"&gt;filter.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;SFT Finetuning code for Qwen2.5-VL - &lt;a href="https://github.com/allenai/olmocr/raw/main/olmocr/train/train.py"&gt;train.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;GRPO RL Trainer - &lt;a href="https://github.com/allenai/olmocr/raw/main/olmocr/train/grpo_train.py"&gt;grpo_train.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Synthetic data generation - &lt;a href="https://github.com/allenai/olmocr/raw/main/olmocr/bench/synth/mine_html_templates.py"&gt;mine_html_templates.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Processing millions of PDFs through a finetuned model using VLLM - &lt;a href="https://github.com/allenai/olmocr/raw/main/olmocr/pipeline.py"&gt;pipeline.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Viewing &lt;a href="https://github.com/allenai/dolma"&gt;Dolma docs&lt;/a&gt; created from PDFs - &lt;a href="https://github.com/allenai/olmocr/raw/main/olmocr/viewer/dolmaviewer.py"&gt;dolmaviewer.py&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Team&lt;/h2&gt; 
&lt;!-- start team --&gt; 
&lt;p&gt;&lt;strong&gt;olmOCR&lt;/strong&gt; is developed and maintained by the AllenNLP team, backed by &lt;a href="https://allenai.org/"&gt;the Allen Institute for Artificial Intelligence (AI2)&lt;/a&gt;. AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering. To learn more about who specifically contributed to this codebase, see &lt;a href="https://github.com/allenai/olmocr/graphs/contributors"&gt;our contributors&lt;/a&gt; page.&lt;/p&gt; 
&lt;!-- end team --&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;!-- start license --&gt; 
&lt;p&gt;&lt;strong&gt;olmOCR&lt;/strong&gt; is licensed under &lt;a href="https://www.apache.org/licenses/LICENSE-2.0"&gt;Apache 2.0&lt;/a&gt;. A full copy of the license can be found &lt;a href="https://github.com/allenai/olmocr/raw/main/LICENSE"&gt;on GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- end license --&gt; 
&lt;h2&gt;Citing&lt;/h2&gt; 
&lt;p&gt;For olmOCR v1 and OlmOCR-bench:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{olmocrbench,
      title={{olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models}},
      author={Jake Poznanski and Jon Borchardt and Jason Dunkelberger and Regan Huff and Daniel Lin and Aman Rangapur and Christopher Wilhelm and Kyle Lo and Luca Soldaini},
      year={2025},
      eprint={2502.18443},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.18443},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For olmOCR v2 Unit Testing Rewards with RL:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{olmocr2,
      title={olmOCR 2: Unit Test Rewards for Document OCR}, 
      author={Jake Poznanski and Luca Soldaini and Kyle Lo},
      year={2025},
      eprint={2510.19817},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2510.19817}, 
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Skyvern-AI/skyvern</title>
      <link>https://github.com/Skyvern-AI/skyvern</link>
      <description>&lt;p&gt;Automate browser based workflows with AI&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; &lt;a href="https://www.skyvern.com"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="fern/images/skyvern_logo.png" /&gt; 
   &lt;img height="120" src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/skyvern_logo_blackbg.png" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;br /&gt; &lt;/h1&gt; 
&lt;p align="center"&gt; ğŸ‰ Automate Browser-based workflows using LLMs and Computer Vision ğŸ‰ &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.skyvern.com/"&gt;&lt;img src="https://img.shields.io/badge/Website-blue?logo=googlechrome&amp;amp;logoColor=black" /&gt;&lt;/a&gt; &lt;a href="https://www.skyvern.com/docs/"&gt;&lt;img src="https://img.shields.io/badge/Docs-yellow?logo=gitbook&amp;amp;logoColor=black" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;&lt;img src="https://img.shields.io/discord/1212486326352617534?logo=discord&amp;amp;label=discord" /&gt;&lt;/a&gt; 
 &lt;!-- &lt;a href="https://pepy.tech/project/skyvern" target="_blank"&gt;&lt;img src="https://static.pepy.tech/badge/skyvern" alt="Total Downloads"/&gt;&lt;/a&gt; --&gt; &lt;a href="https://github.com/skyvern-ai/skyvern"&gt;&lt;img src="https://img.shields.io/github/stars/skyvern-ai/skyvern" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Skyvern-AI/skyvern/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/skyvern-ai/skyvern" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/skyvernai"&gt;&lt;img src="https://img.shields.io/twitter/follow/skyvernai?style=social" /&gt;&lt;/a&gt; &lt;a href="https://www.linkedin.com/company/95726232"&gt;&lt;img src="https://img.shields.io/badge/Follow%20 on%20LinkedIn-8A2BE2?logo=linkedin" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.skyvern.com"&gt;Skyvern&lt;/a&gt; automates browser-based workflows using LLMs and computer vision. It provides a simple API endpoint to fully automate manual workflows on a large number of websites, replacing brittle or unreliable automation solutions.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/geico_shu_recording_cropped.gif" /&gt; &lt;/p&gt; 
&lt;p&gt;Traditional approaches to browser automations required writing custom scripts for websites, often relying on DOM parsing and XPath-based interactions which would break whenever the website layouts changed.&lt;/p&gt; 
&lt;p&gt;Instead of only relying on code-defined XPath interactions, Skyvern relies on Vision LLMs to learn and interact with the websites.&lt;/p&gt; 
&lt;h1&gt;How it works&lt;/h1&gt; 
&lt;p&gt;Skyvern was inspired by the Task-Driven autonomous agent design popularized by &lt;a href="https://github.com/yoheinakajima/babyagi"&gt;BabyAGI&lt;/a&gt; and &lt;a href="https://github.com/Significant-Gravitas/AutoGPT"&gt;AutoGPT&lt;/a&gt; -- with one major bonus: we give Skyvern the ability to interact with websites using browser automation libraries like &lt;a href="https://playwright.dev/"&gt;Playwright&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Skyvern uses a swarm of agents to comprehend a website, and plan and execute its actions:&lt;/p&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="fern/images/skyvern_2_0_system_diagram.png" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/skyvern_2_0_system_diagram.png" /&gt; 
&lt;/picture&gt; 
&lt;p&gt;This approach has a few advantages:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Skyvern can operate on websites it's never seen before, as it's able to map visual elements to actions necessary to complete a workflow, without any customized code&lt;/li&gt; 
 &lt;li&gt;Skyvern is resistant to website layout changes, as there are no pre-determined XPaths or other selectors our system is looking for while trying to navigate&lt;/li&gt; 
 &lt;li&gt;Skyvern is able to take a single workflow and apply it to a large number of websites, as it's able to reason through the interactions necessary to complete the workflow&lt;/li&gt; 
 &lt;li&gt;Skyvern leverages LLMs to reason through interactions to ensure we can cover complex situations. Examples include: 
  &lt;ol&gt; 
   &lt;li&gt;If you wanted to get an auto insurance quote from Geico, the answer to a common question "Were you eligible to drive at 18?" could be inferred from the driver receiving their license at age 16&lt;/li&gt; 
   &lt;li&gt;If you were doing competitor analysis, it's understanding that an Arnold Palmer 22 oz can at 7/11 is almost definitely the same product as a 23 oz can at Gopuff (even though the sizes are slightly different, which could be a rounding error!)&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;A detailed technical report can be found &lt;a href="https://www.skyvern.com/blog/skyvern-2-0-state-of-the-art-web-navigation-with-85-8-on-webvoyager-eval/"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Demo&lt;/h1&gt; 
&lt;!-- Redo demo --&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/5cab4668-e8e2-4982-8551-aab05ff73a7f"&gt;https://github.com/user-attachments/assets/5cab4668-e8e2-4982-8551-aab05ff73a7f&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Performance &amp;amp; Evaluation&lt;/h1&gt; 
&lt;p&gt;Skyvern has SOTA performance on the &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/webbench.ai"&gt;WebBench benchmark&lt;/a&gt; with a 64.4% accuracy. The technical report + evaluation can be found &lt;a href="https://www.skyvern.com/blog/web-bench-a-new-way-to-compare-ai-browser-agents/"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/performance/webbench_overall.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Performance on WRITE tasks (eg filling out forms, logging in, downloading files, etc)&lt;/h2&gt; 
&lt;p&gt;Skyvern is the best performing agent on WRITE tasks (eg filling out forms, logging in, downloading files, etc), which is primarily used for RPA (Robotic Process Automation) adjacent tasks.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/performance/webbench_write.png" /&gt; &lt;/p&gt; 
&lt;h1&gt;Quickstart&lt;/h1&gt; 
&lt;h2&gt;Skyvern Cloud&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com"&gt;Skyvern Cloud&lt;/a&gt; is a managed cloud version of Skyvern that allows you to run Skyvern without worrying about the infrastructure. It allows you to run multiple Skyvern instances in parallel and comes bundled with anti-bot detection mechanisms, proxy network, and CAPTCHA solvers.&lt;/p&gt; 
&lt;p&gt;If you'd like to try it out, navigate to &lt;a href="https://app.skyvern.com"&gt;app.skyvern.com&lt;/a&gt; and create an account.&lt;/p&gt; 
&lt;h2&gt;Install &amp;amp; Run&lt;/h2&gt; 
&lt;p&gt;Dependencies needed:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.python.org/downloads/"&gt;Python 3.11.x&lt;/a&gt;, works with 3.12, not ready yet for 3.13&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nodejs.org/en/download/"&gt;NodeJS &amp;amp; NPM&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Additionally, for Windows:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://rustup.rs/"&gt;Rust&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;VS Code with C++ dev tools and Windows SDK&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;1. Install Skyvern&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install skyvern
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Run Skyvern&lt;/h3&gt; 
&lt;p&gt;This is most helpful for first time run (db setup, db migrations etc).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;skyvern quickstart
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Run task&lt;/h3&gt; 
&lt;h4&gt;UI (Recommended)&lt;/h4&gt; 
&lt;p&gt;Start the Skyvern service and UI (when DB is up and running)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;skyvern run all
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Go to &lt;a href="http://localhost:8080"&gt;http://localhost:8080&lt;/a&gt; and use the UI to run a task&lt;/p&gt; 
&lt;h4&gt;Code&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

skyvern = Skyvern()
task = await skyvern.run_task(prompt="Find the top post on hackernews today")
print(task)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Skyvern starts running the task in a browser that pops up and closes it when the task is done. You will be able to view the task from &lt;a href="http://localhost:8080/history"&gt;http://localhost:8080/history&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You can also run a task on different targets:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

# Run on Skyvern Cloud
skyvern = Skyvern(api_key="SKYVERN API KEY")

# Local Skyvern service
skyvern = Skyvern(base_url="http://localhost:8000", api_key="LOCAL SKYVERN API KEY")

task = await skyvern.run_task(prompt="Find the top post on hackernews today")
print(task)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Advanced Usage&lt;/h2&gt; 
&lt;h3&gt;Control your own browser (Chrome)&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;âš ï¸ WARNING: Since &lt;a href="https://developer.chrome.com/blog/remote-debugging-port"&gt;Chrome 136&lt;/a&gt;, Chrome refuses any CDP connect to the browser using the default user_data_dir. In order to use your browser data, Skyvern copies your default user_data_dir to &lt;code&gt;./tmp/user_data_dir&lt;/code&gt; the first time connecting to your local browser. âš ï¸&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt;Just With Python Code&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

# The path to your Chrome browser. This example path is for Mac.
browser_path = "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
skyvern = Skyvern(
    base_url="http://localhost:8000",
    api_key="YOUR_API_KEY",
    browser_path=browser_path,
)
task = await skyvern.run_task(
    prompt="Find the top post on hackernews today",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;With Skyvern Service&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Add two variables to your .env file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# The path to your Chrome browser. This example path is for Mac.
CHROME_EXECUTABLE_PATH="/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
BROWSER_TYPE=cdp-connect
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Restart Skyvern service &lt;code&gt;skyvern run all&lt;/code&gt; and run the task through UI or code&lt;/p&gt; 
&lt;h3&gt;Run Skyvern with any remote browser&lt;/h3&gt; 
&lt;p&gt;Grab the cdp connection url and pass it to Skyvern&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

skyvern = Skyvern(cdp_url="your cdp connection url")
task = await skyvern.run_task(
    prompt="Find the top post on hackernews today",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Get consistent output schema from your run&lt;/h3&gt; 
&lt;p&gt;You can do this by adding the &lt;code&gt;data_extraction_schema&lt;/code&gt; parameter:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

skyvern = Skyvern()
task = await skyvern.run_task(
    prompt="Find the top post on hackernews today",
    data_extraction_schema={
        "type": "object",
        "properties": {
            "title": {
                "type": "string",
                "description": "The title of the top post"
            },
            "url": {
                "type": "string",
                "description": "The URL of the top post"
            },
            "points": {
                "type": "integer",
                "description": "Number of points the post has received"
            }
        }
    }
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Helpful commands to debug issues&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Launch the Skyvern Server Separately*
skyvern run server

# Launch the Skyvern UI
skyvern run ui

# Check status of the Skyvern service
skyvern status

# Stop the Skyvern service
skyvern stop all

# Stop the Skyvern UI
skyvern stop ui

# Stop the Skyvern Server Separately
skyvern stop server
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Docker Compose setup&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Make sure you have &lt;a href="https://www.docker.com/products/docker-desktop/"&gt;Docker Desktop&lt;/a&gt; installed and running on your machine&lt;/li&gt; 
 &lt;li&gt;Make sure you don't have postgres running locally (Run &lt;code&gt;docker ps&lt;/code&gt; to check)&lt;/li&gt; 
 &lt;li&gt;Clone the repository and navigate to the root directory&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;skyvern init llm&lt;/code&gt; to generate a &lt;code&gt;.env&lt;/code&gt; file. This will be copied into the Docker image.&lt;/li&gt; 
 &lt;li&gt;Fill in the LLM provider key on the &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt;. &lt;em&gt;If you want to run Skyvern on a remote server, make sure you set the correct server ip for the UI container in &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt;.&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Run the following command via the commandline: &lt;pre&gt;&lt;code class="language-bash"&gt; docker compose up -d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Navigate to &lt;code&gt;http://localhost:8080&lt;/code&gt; in your browser to start using the UI&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; Only one Postgres container can run on port 5432 at a time. If you switch from the CLI-managed Postgres to Docker Compose, you must first remove the original container:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;docker rm -f postgresql-container
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;If you encounter any database related errors while using Docker to run Skyvern, check which Postgres container is running with &lt;code&gt;docker ps&lt;/code&gt;.&lt;/p&gt; 
&lt;h1&gt;Skyvern Features&lt;/h1&gt; 
&lt;h2&gt;Skyvern Tasks&lt;/h2&gt; 
&lt;p&gt;Tasks are the fundamental building block inside Skyvern. Each task is a single request to Skyvern, instructing it to navigate through a website and accomplish a specific goal.&lt;/p&gt; 
&lt;p&gt;Tasks require you to specify a &lt;code&gt;url&lt;/code&gt;, &lt;code&gt;prompt&lt;/code&gt;, and can optionally include a &lt;code&gt;data schema&lt;/code&gt; (if you want the output to conform to a specific schema) and &lt;code&gt;error codes&lt;/code&gt; (if you want Skyvern to stop running in specific situations).&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/skyvern_2_0_screenshot.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Skyvern Workflows&lt;/h2&gt; 
&lt;p&gt;Workflows are a way to chain multiple tasks together to form a cohesive unit of work.&lt;/p&gt; 
&lt;p&gt;For example, if you wanted to download all invoices newer than January 1st, you could create a workflow that first navigated to the invoices page, then filtered down to only show invoices newer than January 1st, extracted a list of all eligible invoices, and iterated through each invoice to download it.&lt;/p&gt; 
&lt;p&gt;Another example is if you wanted to automate purchasing products from an e-commerce store, you could create a workflow that first navigated to the desired product, then added it to a cart. Second, it would navigate to the cart and validate the cart state. Finally, it would go through the checkout process to purchase the items.&lt;/p&gt; 
&lt;p&gt;Supported workflow features include:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Browser Task&lt;/li&gt; 
 &lt;li&gt;Browser Action&lt;/li&gt; 
 &lt;li&gt;Data Extraction&lt;/li&gt; 
 &lt;li&gt;Validation&lt;/li&gt; 
 &lt;li&gt;For Loops&lt;/li&gt; 
 &lt;li&gt;File parsing&lt;/li&gt; 
 &lt;li&gt;Sending emails&lt;/li&gt; 
 &lt;li&gt;Text Prompts&lt;/li&gt; 
 &lt;li&gt;HTTP Request Block&lt;/li&gt; 
 &lt;li&gt;Custom Code Block&lt;/li&gt; 
 &lt;li&gt;Uploading files to block storage&lt;/li&gt; 
 &lt;li&gt;(Coming soon) Conditionals&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/block_example_v2.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Livestreaming&lt;/h2&gt; 
&lt;p&gt;Skyvern allows you to livestream the viewport of the browser to your local machine so that you can see exactly what Skyvern is doing on the web. This is useful for debugging and understanding how Skyvern is interacting with a website, and intervening when necessary&lt;/p&gt; 
&lt;h2&gt;Form Filling&lt;/h2&gt; 
&lt;p&gt;Skyvern is natively capable of filling out form inputs on websites. Passing in information via the &lt;code&gt;navigation_goal&lt;/code&gt; will allow Skyvern to comprehend the information and fill out the form accordingly.&lt;/p&gt; 
&lt;h2&gt;Data Extraction&lt;/h2&gt; 
&lt;p&gt;Skyvern is also capable of extracting data from a website.&lt;/p&gt; 
&lt;p&gt;You can also specify a &lt;code&gt;data_extraction_schema&lt;/code&gt; directly within the main prompt to tell Skyvern exactly what data you'd like to extract from the website, in jsonc format. Skyvern's output will be structured in accordance to the supplied schema.&lt;/p&gt; 
&lt;h2&gt;File Downloading&lt;/h2&gt; 
&lt;p&gt;Skyvern is also capable of downloading files from a website. All downloaded files are automatically uploaded to block storage (if configured), and you can access them via the UI.&lt;/p&gt; 
&lt;h2&gt;Authentication&lt;/h2&gt; 
&lt;p&gt;Skyvern supports a number of different authentication methods to make it easier to automate tasks behind a login. If you'd like to try it out, please reach out to us &lt;a href="mailto:founders@skyvern.com"&gt;via email&lt;/a&gt; or &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;discord&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/secure_password_task_example.png" /&gt; &lt;/p&gt; 
&lt;h3&gt;ğŸ” 2FA Support (TOTP)&lt;/h3&gt; 
&lt;p&gt;Skyvern supports a number of different 2FA methods to allow you to automate workflows that require 2FA.&lt;/p&gt; 
&lt;p&gt;Examples include:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;QR-based 2FA (e.g. Google Authenticator, Authy)&lt;/li&gt; 
 &lt;li&gt;Email based 2FA&lt;/li&gt; 
 &lt;li&gt;SMS based 2FA&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;ğŸ” Learn more about 2FA support &lt;a href="https://www.skyvern.com/docs/credentials/totp"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Password Manager Integrations&lt;/h3&gt; 
&lt;p&gt;Skyvern currently supports the following password manager integrations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Bitwarden&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; 1Password&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; LastPass&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Model Context Protocol (MCP)&lt;/h2&gt; 
&lt;p&gt;Skyvern supports the Model Context Protocol (MCP) to allow you to use any LLM that supports MCP.&lt;/p&gt; 
&lt;p&gt;See the MCP documentation &lt;a href="https://github.com/Skyvern-AI/skyvern/raw/main/integrations/mcp/README.md"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Zapier / Make.com / N8N Integration&lt;/h2&gt; 
&lt;p&gt;Skyvern supports Zapier, Make.com, and N8N to allow you to connect your Skyvern workflows to other apps.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.skyvern.com/docs/integrations/zapier"&gt;Zapier&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.skyvern.com/docs/integrations/make.com"&gt;Make.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.skyvern.com/docs/integrations/n8n"&gt;N8N&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ğŸ” Learn more about 2FA support &lt;a href="https://www.skyvern.com/docs/credentials/totp"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Real-world examples of Skyvern&lt;/h1&gt; 
&lt;p&gt;We love to see how Skyvern is being used in the wild. Here are some examples of how Skyvern is being used to automate workflows in the real world. Please open PRs to add your own examples!&lt;/p&gt; 
&lt;h2&gt;Invoice Downloading on many different websites&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://meetings.hubspot.com/skyvern/demo"&gt;Book a demo to see it live&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/invoice_downloading.gif" /&gt; &lt;/p&gt; 
&lt;h2&gt;Automate the job application process&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/job_application"&gt;ğŸ’¡ See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/job_application_demo.gif" /&gt; &lt;/p&gt; 
&lt;h2&gt;Automate materials procurement for a manufacturing company&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/finditparts"&gt;ğŸ’¡ See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/finditparts_recording_crop.gif" /&gt; &lt;/p&gt; 
&lt;h2&gt;Navigating to government websites to register accounts or fill out forms&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/california_edd"&gt;ğŸ’¡ See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/edd_services.gif" /&gt; &lt;/p&gt; 
&lt;!-- Add example of delaware entity lookups x2 --&gt; 
&lt;h2&gt;Filling out random contact us forms&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/contact_us_forms"&gt;ğŸ’¡ See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/contact_forms.gif" /&gt; &lt;/p&gt; 
&lt;h2&gt;Retrieving insurance quotes from insurance providers in any language&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/bci_seguros"&gt;ğŸ’¡ See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/bci_seguros_recording.gif" /&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/geico"&gt;ğŸ’¡ See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/geico_shu_recording_cropped.gif" /&gt; &lt;/p&gt; 
&lt;h1&gt;Contributor Setup&lt;/h1&gt; 
&lt;p&gt;Make sure to have &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;uv&lt;/a&gt; installed.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Run this to create your virtual environment (&lt;code&gt;.venv&lt;/code&gt;) &lt;pre&gt;&lt;code class="language-bash"&gt;uv sync --group dev
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Perform initial server configuration &lt;pre&gt;&lt;code class="language-bash"&gt;uv run skyvern quickstart
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Navigate to &lt;code&gt;http://localhost:8080&lt;/code&gt; in your browser to start using the UI &lt;em&gt;The Skyvern CLI supports Windows, WSL, macOS, and Linux environments.&lt;/em&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h1&gt;Documentation&lt;/h1&gt; 
&lt;p&gt;More extensive documentation can be found on our &lt;a href="https://www.skyvern.com/docs"&gt;ğŸ“• docs page&lt;/a&gt;. Please let us know if something is unclear or missing by opening an issue or reaching out to us &lt;a href="mailto:founders@skyvern.com"&gt;via email&lt;/a&gt; or &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;discord&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Supported LLMs&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Provider&lt;/th&gt; 
   &lt;th&gt;Supported Models&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
   &lt;td&gt;gpt4-turbo, gpt-4o, gpt-4o-mini&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Anthropic&lt;/td&gt; 
   &lt;td&gt;Claude 3 (Haiku, Sonnet, Opus), Claude 3.5 (Sonnet)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Azure OpenAI&lt;/td&gt; 
   &lt;td&gt;Any GPT models. Better performance with a multimodal llm (azure/gpt4-o)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AWS Bedrock&lt;/td&gt; 
   &lt;td&gt;Anthropic Claude 3 (Haiku, Sonnet, Opus), Claude 3.5 (Sonnet)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Gemini&lt;/td&gt; 
   &lt;td&gt;Gemini 2.5 Pro and flash, Gemini 2.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ollama&lt;/td&gt; 
   &lt;td&gt;Run any locally hosted model via &lt;a href="https://github.com/ollama/ollama"&gt;Ollama&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenRouter&lt;/td&gt; 
   &lt;td&gt;Access models through &lt;a href="https://openrouter.ai"&gt;OpenRouter&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI-compatible&lt;/td&gt; 
   &lt;td&gt;Any custom API endpoint that follows OpenAI's API format (via &lt;a href="https://docs.litellm.ai/docs/providers/openai_compatible"&gt;liteLLM&lt;/a&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Environment Variables&lt;/h4&gt; 
&lt;h5&gt;OpenAI&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_OPENAI&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register OpenAI models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI API Key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_API_BASE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI API Base, optional&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://openai.api.base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_ORGANIZATION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI Organization ID, optional&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;your-org-id&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;OPENAI_GPT4O&lt;/code&gt;, &lt;code&gt;OPENAI_GPT4O_MINI&lt;/code&gt;, &lt;code&gt;OPENAI_GPT4_1&lt;/code&gt;, &lt;code&gt;OPENAI_O4_MINI&lt;/code&gt;, &lt;code&gt;OPENAI_O3&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;Anthropic&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_ANTHROPIC&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register Anthropic models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Anthropic API key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended&lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;ANTHROPIC_CLAUDE3.5_SONNET&lt;/code&gt;, &lt;code&gt;ANTHROPIC_CLAUDE3.7_SONNET&lt;/code&gt;, &lt;code&gt;ANTHROPIC_CLAUDE4_OPUS&lt;/code&gt;, &lt;code&gt;ANTHROPIC_CLAUDE4_SONNET&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;Azure OpenAI&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_AZURE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register Azure OpenAI models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure deployment API key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_DEPLOYMENT&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure OpenAI Deployment Name&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;skyvern-deployment&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_API_BASE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure deployment api base url&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://skyvern-deployment.openai.azure.com/&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_API_VERSION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure API Version&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;2024-02-01&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;AZURE_OPENAI&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;AWS Bedrock&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_BEDROCK&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register AWS Bedrock models. To use AWS Bedrock, you need to make sure your &lt;a href="https://github.com/boto/boto3?tab=readme-ov-file#using-boto3"&gt;AWS configurations&lt;/a&gt; are set up correctly first.&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;BEDROCK_ANTHROPIC_CLAUDE3.7_SONNET_INFERENCE_PROFILE&lt;/code&gt;, &lt;code&gt;BEDROCK_ANTHROPIC_CLAUDE4_OPUS_INFERENCE_PROFILE&lt;/code&gt;, &lt;code&gt;BEDROCK_ANTHROPIC_CLAUDE4_SONNET_INFERENCE_PROFILE&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;Gemini&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_GEMINI&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register Gemini models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;GEMINI_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Gemini API Key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;your_google_gemini_api_key&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;GEMINI_2.5_PRO_PREVIEW&lt;/code&gt;, &lt;code&gt;GEMINI_2.5_FLASH_PREVIEW&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;Ollama&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_OLLAMA&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register local models via Ollama&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OLLAMA_SERVER_URL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;URL for your Ollama server&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;http://host.docker.internal:11434&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OLLAMA_MODEL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Ollama model name to load&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;qwen2.5:7b-instruct&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;OLLAMA&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Note: Ollama does not support vision yet.&lt;/p&gt; 
&lt;h5&gt;OpenRouter&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_OPENROUTER&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register OpenRouter models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenRouter API key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENROUTER_MODEL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenRouter model name&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;mistralai/mistral-small-3.1-24b-instruct&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENROUTER_API_BASE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenRouter API base URL&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://api.openrouter.ai/v1&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;OPENROUTER&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;OpenAI-Compatible&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_OPENAI_COMPATIBLE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register a custom OpenAI-compatible API endpoint&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_MODEL_NAME&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Model name for OpenAI-compatible endpoint&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;yi-34b&lt;/code&gt;, &lt;code&gt;gpt-3.5-turbo&lt;/code&gt;, &lt;code&gt;mistral-large&lt;/code&gt;, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;API key for OpenAI-compatible endpoint&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_API_BASE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Base URL for OpenAI-compatible endpoint&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://api.together.xyz/v1&lt;/code&gt;, &lt;code&gt;http://localhost:8000/v1&lt;/code&gt;, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_API_VERSION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;API version for OpenAI-compatible endpoint, optional&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;2023-05-15&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_MAX_TOKENS&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Maximum tokens for completion, optional&lt;/td&gt; 
   &lt;td&gt;Integer&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;4096&lt;/code&gt;, &lt;code&gt;8192&lt;/code&gt;, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_TEMPERATURE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Temperature setting, optional&lt;/td&gt; 
   &lt;td&gt;Float&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;0.0&lt;/code&gt;, &lt;code&gt;0.5&lt;/code&gt;, &lt;code&gt;0.7&lt;/code&gt;, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_SUPPORTS_VISION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Whether model supports vision, optional&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Supported LLM Key: &lt;code&gt;OPENAI_COMPATIBLE&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;General LLM Configuration&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;LLM_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The name of the model you want to use&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;See supported LLM keys above&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;SECONDARY_LLM_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The name of the model for mini agents skyvern runs with&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;See supported LLM keys above&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;LLM_CONFIG_MAX_TOKENS&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Override the max tokens used by the LLM&lt;/td&gt; 
   &lt;td&gt;Integer&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;128000&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Feature Roadmap&lt;/h1&gt; 
&lt;p&gt;This is our planned roadmap for the next few months. If you have any suggestions or would like to see a feature added, please don't hesitate to reach out to us &lt;a href="mailto:founders@skyvern.com"&gt;via email&lt;/a&gt; or &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;discord&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Open Source&lt;/strong&gt; - Open Source Skyvern's core codebase&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Workflow support&lt;/strong&gt; - Allow support to chain multiple Skyvern calls together&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Improved context&lt;/strong&gt; - Improve Skyvern's ability to understand content around interactable elements by introducing feeding relevant label context through the text prompt&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Cost Savings&lt;/strong&gt; - Improve Skyvern's stability and reduce the cost of running Skyvern by optimizing the context tree passed into Skyvern&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Self-serve UI&lt;/strong&gt; - Deprecate the Streamlit UI in favour of a React-based UI component that allows users to kick off new jobs in Skyvern&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Workflow UI Builder&lt;/strong&gt; - Introduce a UI to allow users to build and analyze workflows visually&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Chrome Viewport streaming&lt;/strong&gt; - Introduce a way to live-stream the Chrome viewport to the user's browser (as a part of the self-serve UI)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Past Runs UI&lt;/strong&gt; - Deprecate the Streamlit UI in favour of a React-based UI that allows you to visualize past runs and their results&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Auto workflow builder ("Observer") mode&lt;/strong&gt; - Allow Skyvern to auto-generate workflows as it's navigating the web to make it easier to build new workflows&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Prompt Caching&lt;/strong&gt; - Introduce a caching layer to the LLM calls to dramatically reduce the cost of running Skyvern (memorize past actions and repeat them!)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Web Evaluation Dataset&lt;/strong&gt; - Integrate Skyvern with public benchmark tests to track the quality of our models over time&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Improved Debug mode&lt;/strong&gt; - Allow Skyvern to plan its actions and get "approval" before running them, allowing you to debug what it's doing and more easily iterate on the prompt&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Chrome Extension&lt;/strong&gt; - Allow users to interact with Skyvern through a Chrome extension (incl voice mode, saving tasks, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Skyvern Action Recorder&lt;/strong&gt; - Allow Skyvern to watch a user complete a task and then automatically generate a workflow for it&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Interactable Livestream&lt;/strong&gt; - Allow users to interact with the livestream in real-time to intervene when necessary (such as manually submitting sensitive forms)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Integrate LLM Observability tools&lt;/strong&gt; - Integrate LLM Observability tools to allow back-testing prompt changes with specific data sets + visualize the performance of Skyvern over time&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Langchain Integration&lt;/strong&gt; - Create langchain integration in langchain_community to use Skyvern as a "tool".&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Contributing&lt;/h1&gt; 
&lt;p&gt;We welcome PRs and suggestions! Don't hesitate to open a PR/issue or to reach out to us &lt;a href="mailto:founders@skyvern.com"&gt;via email&lt;/a&gt; or &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;discord&lt;/a&gt;. Please have a look at our &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/CONTRIBUTING.md"&gt;contribution guide&lt;/a&gt; and &lt;a href="https://github.com/skyvern-ai/skyvern/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22"&gt;"Help Wanted" issues&lt;/a&gt; to get started!&lt;/p&gt; 
&lt;p&gt;If you want to chat with the skyvern repository to get a high level overview of how it is structured, how to build off it, and how to resolve usage questions, check out &lt;a href="https://sage.storia.ai?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=skyvern-readme"&gt;Code Sage&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Telemetry&lt;/h1&gt; 
&lt;p&gt;By Default, Skyvern collects basic usage statistics to help us understand how Skyvern is being used. If you would like to opt-out of telemetry, please set the &lt;code&gt;SKYVERN_TELEMETRY&lt;/code&gt; environment variable to &lt;code&gt;false&lt;/code&gt;.&lt;/p&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;Skyvern's open source repository is supported via a managed cloud. All of the core logic powering Skyvern is available in this open source repository licensed under the &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/LICENSE"&gt;AGPL-3.0 License&lt;/a&gt;, with the exception of anti-bot measures available in our managed cloud offering.&lt;/p&gt; 
&lt;p&gt;If you have any questions or concerns around licensing, please &lt;a href="mailto:support@skyvern.com"&gt;contact us&lt;/a&gt; and we would be happy to help.&lt;/p&gt; 
&lt;h1&gt;Star History&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#Skyvern-AI/skyvern&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Skyvern-AI/skyvern&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>sansan0/TrendRadar</title>
      <link>https://github.com/sansan0/TrendRadar</link>
      <description>&lt;p&gt;ğŸ¯ å‘Šåˆ«ä¿¡æ¯è¿‡è½½ï¼ŒAI åŠ©ä½ çœ‹æ‡‚æ–°é—»èµ„è®¯çƒ­ç‚¹ï¼Œç®€å•çš„èˆ†æƒ…ç›‘æ§åˆ†æ - å¤šå¹³å°çƒ­ç‚¹èšåˆ+åŸºäº MCP çš„AIåˆ†æå·¥å…·ã€‚ç›‘æ§35ä¸ªå¹³å°ï¼ˆæŠ–éŸ³ã€çŸ¥ä¹ã€Bç«™ã€åå°”è¡—è§é—»ã€è´¢è”ç¤¾ç­‰ï¼‰ï¼Œæ™ºèƒ½ç­›é€‰+è‡ªåŠ¨æ¨é€+AIå¯¹è¯åˆ†æï¼ˆç”¨è‡ªç„¶è¯­è¨€æ·±åº¦æŒ–æ˜æ–°é—»ï¼šè¶‹åŠ¿è¿½è¸ªã€æƒ…æ„Ÿåˆ†æã€ç›¸ä¼¼æ£€ç´¢ç­‰13ç§å·¥å…·ï¼‰ã€‚æ”¯æŒä¼ä¸šå¾®ä¿¡/é£ä¹¦/é’‰é’‰/Telegram/é‚®ä»¶/ntfyæ¨é€ï¼Œ30ç§’ç½‘é¡µéƒ¨ç½²ï¼Œ1åˆ†é’Ÿæ‰‹æœºé€šçŸ¥ï¼Œæ— éœ€ç¼–ç¨‹ã€‚æ”¯æŒDockeréƒ¨ç½²â­ è®©ç®—æ³•ä¸ºä½ æœåŠ¡ï¼Œç”¨AIç†è§£çƒ­ç‚¹&lt;/p&gt;&lt;hr&gt;&lt;div align="center" id="trendradar"&gt; 
 &lt;a href="https://github.com/sansan0/TrendRadar" title="TrendRadar"&gt; &lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/banner.jpg" alt="TrendRadar Banner" width="50%" /&gt; &lt;/a&gt; 
 &lt;p&gt;ğŸš€ æœ€å¿«&lt;strong&gt;30ç§’&lt;/strong&gt;éƒ¨ç½²çš„çƒ­ç‚¹åŠ©æ‰‹ â€”â€” å‘Šåˆ«æ— æ•ˆåˆ·å±ï¼Œåªçœ‹çœŸæ­£å…³å¿ƒçš„æ–°é—»èµ„è®¯&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/14726" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14726" alt="sansan0%2FTrendRadar | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/sansan0/TrendRadar/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/sansan0/TrendRadar?style=flat-square&amp;amp;logo=github&amp;amp;color=yellow" alt="GitHub Stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sansan0/TrendRadar/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/sansan0/TrendRadar?style=flat-square&amp;amp;logo=github&amp;amp;color=blue" alt="GitHub Forks" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-GPL--3.0-blue.svg?style=flat-square" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sansan0/TrendRadar"&gt;&lt;img src="https://img.shields.io/badge/version-v3.0.4-blue.svg?sanitize=true" alt="Version" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sansan0/TrendRadar"&gt;&lt;img src="https://img.shields.io/badge/MCP-v1.0.1-green.svg?sanitize=true" alt="MCP" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://work.weixin.qq.com/"&gt;&lt;img src="https://img.shields.io/badge/%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1-%E9%80%9A%E7%9F%A5-00D4AA?style=flat-square" alt="ä¼ä¸šå¾®ä¿¡é€šçŸ¥" /&gt;&lt;/a&gt; &lt;a href="https://telegram.org/"&gt;&lt;img src="https://img.shields.io/badge/Telegram-%E9%80%9A%E7%9F%A5-00D4AA?style=flat-square" alt="Telegramé€šçŸ¥" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#"&gt;&lt;img src="https://img.shields.io/badge/%E9%92%89%E9%92%89-%E9%80%9A%E7%9F%A5-00D4AA?style=flat-square" alt="dingtalké€šçŸ¥" /&gt;&lt;/a&gt; &lt;a href="https://www.feishu.cn/"&gt;&lt;img src="https://img.shields.io/badge/%E9%A3%9E%E4%B9%A6-%E9%80%9A%E7%9F%A5-00D4AA?style=flat-square" alt="é£ä¹¦é€šçŸ¥" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#"&gt;&lt;img src="https://img.shields.io/badge/Email-%E9%80%9A%E7%9F%A5-00D4AA?style=flat-square" alt="é‚®ä»¶é€šçŸ¥" /&gt;&lt;/a&gt; &lt;a href="https://github.com/binwiederhier/ntfy"&gt;&lt;img src="https://img.shields.io/badge/ntfy-%E9%80%9A%E7%9F%A5-00D4AA?style=flat-square" alt="ntfyé€šçŸ¥" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/sansan0/TrendRadar"&gt;&lt;img src="https://img.shields.io/badge/GitHub_Actions-%E8%87%AA%E5%8A%A8%E5%8C%96-2088FF?style=flat-square&amp;amp;logo=github-actions&amp;amp;logoColor=white" alt="GitHub Actions" /&gt;&lt;/a&gt; &lt;a href="https://sansan0.github.io/TrendRadar"&gt;&lt;img src="https://img.shields.io/badge/GitHub_Pages-%E9%83%A8%E7%BD%B2-4285F4?style=flat-square&amp;amp;logo=github&amp;amp;logoColor=white" alt="GitHub Pages" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/wantcat/trendradar"&gt;&lt;img src="https://img.shields.io/badge/Docker-%E9%83%A8%E7%BD%B2-2496ED?style=flat-square&amp;amp;logo=docker&amp;amp;logoColor=white" alt="Docker" /&gt;&lt;/a&gt; &lt;a href="https://modelcontextprotocol.io/"&gt;&lt;img src="https://img.shields.io/badge/MCP-AI%E5%88%86%E6%9E%90%E6%94%AF%E6%8C%81-FF6B6B?style=flat-square&amp;amp;logo=ai&amp;amp;logoColor=white" alt="MCP Support" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æœ¬é¡¹ç›®ä»¥è½»é‡ï¼Œæ˜“éƒ¨ç½²ä¸ºç›®æ ‡&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ“‘ å¿«é€Ÿå¯¼èˆª&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#-%E6%A0%B8%E5%BF%83%E5%8A%9F%E8%83%BD"&gt;ğŸ¯ æ ¸å¿ƒåŠŸèƒ½&lt;/a&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#-%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B"&gt;ğŸš€ å¿«é€Ÿå¼€å§‹&lt;/a&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#-docker-%E9%83%A8%E7%BD%B2"&gt;ğŸ³ Dockeréƒ¨ç½²&lt;/a&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#-ai-%E6%99%BA%E8%83%BD%E5%88%86%E6%9E%90%E9%83%A8%E7%BD%B2"&gt;ğŸ¤– AIåˆ†æä¸“åŒº&lt;/a&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#-%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97"&gt;ğŸ“ æ›´æ–°æ—¥å¿—&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#-mcp-%E5%AE%A2%E6%88%B7%E7%AB%AF"&gt;ğŸ”Œ MCPå®¢æˆ·ç«¯&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#%E9%97%AE%E9%A2%98%E7%AD%94%E7%96%91%E4%B8%8E1%E5%85%83%E7%82%B9%E8%B5%9E"&gt;â“ ç­”ç–‘ä¸å¸¸è§é—®é¢˜&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#%E9%A1%B9%E7%9B%AE%E7%9B%B8%E5%85%B3"&gt;â­ é¡¹ç›®ç›¸å…³&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt;æ„Ÿè°¢&lt;strong&gt;è€å¿ƒåé¦ˆ bug&lt;/strong&gt; çš„è´¡çŒ®è€…ï¼Œä½ ä»¬çš„æ¯ä¸€æ¡åé¦ˆè®©é¡¹ç›®æ›´åŠ å®Œå–„ğŸ˜‰;&lt;/li&gt; 
 &lt;li&gt;æ„Ÿè°¢&lt;strong&gt;ä¸ºé¡¹ç›®ç‚¹ star&lt;/strong&gt; çš„è§‚ä¼—ä»¬ï¼Œ&lt;strong&gt;fork&lt;/strong&gt; ä½ æ‰€æ¬²ä¹Ÿï¼Œ&lt;strong&gt;star&lt;/strong&gt; æˆ‘æ‰€æ¬²ä¹Ÿï¼Œä¸¤è€…å¾—å…¼ğŸ˜æ˜¯å¯¹å¼€æºç²¾ç¥æœ€å¥½çš„æ”¯æŒ;&lt;/li&gt; 
 &lt;li&gt;æ„Ÿè°¢&lt;strong&gt;å…³æ³¨&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#%E9%97%AE%E9%A2%98%E7%AD%94%E7%96%91%E4%B8%8E1%E5%85%83%E7%82%B9%E8%B5%9E"&gt;å…¬ä¼—å·&lt;/a&gt;&lt;/strong&gt; çš„è¯»è€…ä»¬ï¼Œä½ ä»¬çš„ç•™è¨€ã€ç‚¹èµã€åˆ†äº«å’Œæ¨èç­‰ç§¯æäº’åŠ¨è®©å†…å®¹æ›´æœ‰æ¸©åº¦ğŸ˜ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ‘‰ ç‚¹å‡»æŸ¥çœ‹&lt;strong&gt;è‡´è°¢åå•&lt;/strong&gt; (å½“å‰ &lt;strong&gt;ğŸ”¥47ğŸ”¥&lt;/strong&gt; ä½)&lt;/summary&gt; 
 &lt;h3&gt;æ•°æ®æ”¯æŒ&lt;/h3&gt; 
 &lt;p&gt;æœ¬é¡¹ç›®ä½¿ç”¨äº† &lt;a href="https://github.com/ourongxing/newsnow"&gt;newsnow&lt;/a&gt; é¡¹ç›®æä¾›çš„ API æ¥å£è·å–å¤šå¹³å°æ•°æ®&lt;/p&gt; 
 &lt;h3&gt;æ¨å¹¿åŠ©åŠ›&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;æ„Ÿè°¢ä»¥ä¸‹å¹³å°å’Œä¸ªäººçš„æ¨è(æŒ‰æ—¶é—´æ’åˆ—)&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/fvutkJ_NPUelSW9OGK39aA"&gt;å°ä¼—è½¯ä»¶&lt;/a&gt; - å¼€æºè½¯ä»¶æ¨èå¹³å°&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://linux.do/"&gt;LinuxDo ç¤¾åŒº&lt;/a&gt; - æŠ€æœ¯çˆ±å¥½è€…çš„èšé›†åœ°&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/ruanyf/weekly"&gt;é˜®ä¸€å³°å‘¨åˆŠ&lt;/a&gt; - æŠ€æœ¯åœˆæœ‰å½±å“åŠ›çš„å‘¨åˆŠ&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;è§‚ä¼—æ”¯æŒ&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;æ„Ÿè°¢&lt;strong&gt;ç»™äºˆèµ„é‡‘æ”¯æŒ&lt;/strong&gt; çš„æœ‹å‹ä»¬,ä½ ä»¬çš„æ…·æ…¨å·²åŒ–èº«ä¸ºé”®ç›˜æ—çš„é›¶é£Ÿé¥®æ–™,é™ªä¼´ç€é¡¹ç›®çš„æ¯ä¸€æ¬¡è¿­ä»£&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;ç‚¹èµäºº&lt;/th&gt; 
    &lt;th align="center"&gt;é‡‘é¢&lt;/th&gt; 
    &lt;th align="center"&gt;æ—¥æœŸ&lt;/th&gt; 
    &lt;th align="center"&gt;å¤‡æ³¨&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*æ°&lt;/td&gt; 
    &lt;td align="center"&gt;5&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.08&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*ç‚¹&lt;/td&gt; 
    &lt;td align="center"&gt;8.80&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.07&lt;/td&gt; 
    &lt;td align="center"&gt;å¼€å‘ä¸æ˜“ï¼Œæ”¯æŒä¸€ä¸‹ã€‚&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Q*Q&lt;/td&gt; 
    &lt;td align="center"&gt;6.66&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.07&lt;/td&gt; 
    &lt;td align="center"&gt;æ„Ÿè°¢å¼€æºï¼&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;C*e&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.05&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Peter Fan&lt;/td&gt; 
    &lt;td align="center"&gt;20&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.29&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;M*n&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.27&lt;/td&gt; 
    &lt;td align="center"&gt;æ„Ÿè°¢å¼€æº&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*è®¸&lt;/td&gt; 
    &lt;td align="center"&gt;8.88&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.23&lt;/td&gt; 
    &lt;td align="center"&gt;è€å¸ˆ å°ç™½ä¸€æšï¼Œæ‘¸äº†å‡ å¤©äº†è¿˜æ²¡æ•´èµ·æ¥ï¼Œæ±‚æ•™&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Eason&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.22&lt;/td&gt; 
    &lt;td align="center"&gt;è¿˜æ²¡æ•´æ˜ç™½ï¼Œä½†ä½ åœ¨åšå¥½äº‹&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;P*n&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.20&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*æ°&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.19&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*å¾&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.18&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*å¿—&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.17&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*ğŸ˜€&lt;/td&gt; 
    &lt;td align="center"&gt;10&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.16&lt;/td&gt; 
    &lt;td align="center"&gt;ç‚¹èµ&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;**æ°&lt;/td&gt; 
    &lt;td align="center"&gt;10&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.16&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*å•¸&lt;/td&gt; 
    &lt;td align="center"&gt;10&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.16&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*çºª&lt;/td&gt; 
    &lt;td align="center"&gt;5&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.14&lt;/td&gt; 
    &lt;td align="center"&gt;TrendRadar&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;J*d&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.14&lt;/td&gt; 
    &lt;td align="center"&gt;è°¢è°¢ä½ çš„å·¥å…·ï¼Œå¾ˆå¥½ç©...&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*H&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.14&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;é‚£*O&lt;/td&gt; 
    &lt;td align="center"&gt;10&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.13&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*åœ†&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.13&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;P*g&lt;/td&gt; 
    &lt;td align="center"&gt;6&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.13&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Ocean&lt;/td&gt; 
    &lt;td align="center"&gt;20&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.12&lt;/td&gt; 
    &lt;td align="center"&gt;...çœŸçš„å¤ªæ£’äº†ï¼ï¼ï¼å°ç™½çº§åˆ«ä¹Ÿèƒ½ç›´æ¥ç”¨...&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;**åŸ¹&lt;/td&gt; 
    &lt;td align="center"&gt;5.2&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.2&lt;/td&gt; 
    &lt;td align="center"&gt;github-yzyf1312:å¼€æºä¸‡å²&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*æ¤¿&lt;/td&gt; 
    &lt;td align="center"&gt;3&lt;/td&gt; 
    &lt;td align="center"&gt;2025.9.23&lt;/td&gt; 
    &lt;td align="center"&gt;åŠ æ²¹ï¼Œå¾ˆä¸é”™&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*ğŸ&lt;/td&gt; 
    &lt;td align="center"&gt;10&lt;/td&gt; 
    &lt;td align="center"&gt;2025.9.21&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;E*f&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.9.20&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*è®°&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.9.20&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;z*u&lt;/td&gt; 
    &lt;td align="center"&gt;2&lt;/td&gt; 
    &lt;td align="center"&gt;2025.9.19&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;**æ˜Š&lt;/td&gt; 
    &lt;td align="center"&gt;5&lt;/td&gt; 
    &lt;td align="center"&gt;2025.9.17&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*å·&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.9.15&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;T*T&lt;/td&gt; 
    &lt;td align="center"&gt;2&lt;/td&gt; 
    &lt;td align="center"&gt;2025.9.15&lt;/td&gt; 
    &lt;td align="center"&gt;ç‚¹èµ&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*å®¶&lt;/td&gt; 
    &lt;td align="center"&gt;10&lt;/td&gt; 
    &lt;td align="center"&gt;2025.9.10&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*X&lt;/td&gt; 
    &lt;td align="center"&gt;1.11&lt;/td&gt; 
    &lt;td align="center"&gt;2025.9.3&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*é£™&lt;/td&gt; 
    &lt;td align="center"&gt;20&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.31&lt;/td&gt; 
    &lt;td align="center"&gt;æ¥è‡ªè€ç«¥è°¢è°¢&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*ä¸‹&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.30&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;2*D&lt;/td&gt; 
    &lt;td align="center"&gt;88&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.13 ä¸‹åˆ&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;2*D&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.13 ä¸Šåˆ&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;S*o&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.05&lt;/td&gt; 
    &lt;td align="center"&gt;æ”¯æŒä¸€ä¸‹&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*ä¾ &lt;/td&gt; 
    &lt;td align="center"&gt;10&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.04&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;x*x&lt;/td&gt; 
    &lt;td align="center"&gt;2&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.03&lt;/td&gt; 
    &lt;td align="center"&gt;trendRadar å¥½é¡¹ç›® ç‚¹èµ&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*è¿œ&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.01&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*é‚ª&lt;/td&gt; 
    &lt;td align="center"&gt;5&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.01&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*æ¢¦&lt;/td&gt; 
    &lt;td align="center"&gt;0.1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.7.30&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;**é¾™&lt;/td&gt; 
    &lt;td align="center"&gt;10&lt;/td&gt; 
    &lt;td align="center"&gt;2025.7.29&lt;/td&gt; 
    &lt;td align="center"&gt;æ”¯æŒä¸€ä¸‹&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h2&gt;âœ¨ æ ¸å¿ƒåŠŸèƒ½&lt;/h2&gt; 
&lt;h3&gt;&lt;strong&gt;å…¨ç½‘çƒ­ç‚¹èšåˆ&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;çŸ¥ä¹&lt;/li&gt; 
 &lt;li&gt;æŠ–éŸ³&lt;/li&gt; 
 &lt;li&gt;bilibili çƒ­æœ&lt;/li&gt; 
 &lt;li&gt;åå°”è¡—è§é—»&lt;/li&gt; 
 &lt;li&gt;è´´å§&lt;/li&gt; 
 &lt;li&gt;ç™¾åº¦çƒ­æœ&lt;/li&gt; 
 &lt;li&gt;è´¢è”ç¤¾çƒ­é—¨&lt;/li&gt; 
 &lt;li&gt;æ¾æ¹ƒæ–°é—»&lt;/li&gt; 
 &lt;li&gt;å‡¤å‡°ç½‘&lt;/li&gt; 
 &lt;li&gt;ä»Šæ—¥å¤´æ¡&lt;/li&gt; 
 &lt;li&gt;å¾®åš&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;é»˜è®¤ç›‘æ§ 11 ä¸ªä¸»æµå¹³å°ï¼Œä¹Ÿå¯è‡ªè¡Œå¢åŠ é¢å¤–çš„å¹³å°&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ‘‰ è‡ªå®šä¹‰ç›‘æ§å¹³å°&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;æœ¬é¡¹ç›®çš„èµ„è®¯æ•°æ®æ¥æºäº &lt;a href="https://github.com/ourongxing/newsnow"&gt;newsnow&lt;/a&gt; ï¼Œä½ å¯ä»¥ç‚¹å‡»&lt;a href="https://newsnow.busiyi.world/"&gt;ç½‘ç«™&lt;/a&gt;ï¼Œç‚¹å‡»[æ›´å¤š]ï¼ŒæŸ¥çœ‹æ˜¯å¦æœ‰ä½ æƒ³è¦çš„å¹³å°ã€‚&lt;/p&gt; 
 &lt;p&gt;å…·ä½“æ·»åŠ å¯è®¿é—® &lt;a href="https://github.com/ourongxing/newsnow/tree/main/server/sources"&gt;é¡¹ç›®æºä»£ç &lt;/a&gt;ï¼Œæ ¹æ®é‡Œé¢çš„æ–‡ä»¶åï¼Œåœ¨ &lt;code&gt;config/config.yaml&lt;/code&gt; æ–‡ä»¶ä¸­ä¿®æ”¹ &lt;code&gt;platforms&lt;/code&gt; é…ç½®ï¼š&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;platforms:
  - id: "toutiao"
    name: "ä»Šæ—¥å¤´æ¡"
  - id: "baidu"  
    name: "ç™¾åº¦çƒ­æœ"
  - id: "wallstreetcn-hot"
    name: "åå°”è¡—è§é—»"
  # æ·»åŠ æ›´å¤šå¹³å°...
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;å¦‚æœä¸ä¼šçœ‹çš„è¯ï¼Œå°±ç›´æ¥å¤åˆ¶ä»–äººæ•´ç†å¥½çš„éƒ¨åˆ†&lt;a href="https://github.com/sansan0/TrendRadar/issues/95"&gt;å¹³å°é…ç½®&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;&lt;strong&gt;æ™ºèƒ½æ¨é€ç­–ç•¥&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;ä¸‰ç§æ¨é€æ¨¡å¼&lt;/strong&gt;ï¼š&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;æ¨¡å¼&lt;/th&gt; 
   &lt;th&gt;é€‚ç”¨äººç¾¤&lt;/th&gt; 
   &lt;th&gt;æ¨é€æ—¶æœº&lt;/th&gt; 
   &lt;th&gt;æ˜¾ç¤ºå†…å®¹&lt;/th&gt; 
   &lt;th&gt;é€‚ç”¨åœºæ™¯&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;å½“æ—¥æ±‡æ€»&lt;/strong&gt;&lt;br /&gt;&lt;code&gt;daily&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;ğŸ“‹ ä¼ä¸šç®¡ç†è€…/æ™®é€šç”¨æˆ·&lt;/td&gt; 
   &lt;td&gt;æŒ‰æ—¶æ¨é€(é»˜è®¤æ¯å°æ—¶æ¨é€ä¸€æ¬¡)&lt;/td&gt; 
   &lt;td&gt;å½“æ—¥æ‰€æœ‰åŒ¹é…æ–°é—»&lt;br /&gt;+ æ–°å¢æ–°é—»åŒºåŸŸ&lt;/td&gt; 
   &lt;td&gt;æ—¥æŠ¥æ€»ç»“&lt;br /&gt;å…¨é¢äº†è§£å½“æ—¥çƒ­ç‚¹è¶‹åŠ¿&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;å½“å‰æ¦œå•&lt;/strong&gt;&lt;br /&gt;&lt;code&gt;current&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;ğŸ“° è‡ªåª’ä½“äºº/å†…å®¹åˆ›ä½œè€…&lt;/td&gt; 
   &lt;td&gt;æŒ‰æ—¶æ¨é€(é»˜è®¤æ¯å°æ—¶æ¨é€ä¸€æ¬¡)&lt;/td&gt; 
   &lt;td&gt;å½“å‰æ¦œå•åŒ¹é…æ–°é—»&lt;br /&gt;+ æ–°å¢æ–°é—»åŒºåŸŸ&lt;/td&gt; 
   &lt;td&gt;å®æ—¶çƒ­ç‚¹è¿½è¸ª&lt;br /&gt;äº†è§£å½“å‰æœ€ç«çš„å†…å®¹&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;å¢é‡ç›‘æ§&lt;/strong&gt;&lt;br /&gt;&lt;code&gt;incremental&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;ğŸ“ˆ æŠ•èµ„è€…/äº¤æ˜“å‘˜&lt;/td&gt; 
   &lt;td&gt;æœ‰æ–°å¢æ‰æ¨é€&lt;/td&gt; 
   &lt;td&gt;æ–°å‡ºç°çš„åŒ¹é…é¢‘ç‡è¯æ–°é—»&lt;/td&gt; 
   &lt;td&gt;é¿å…é‡å¤ä¿¡æ¯å¹²æ‰°&lt;br /&gt;é«˜é¢‘ç›‘æ§åœºæ™¯&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;é™„åŠ åŠŸèƒ½ - æ¨é€æ—¶é—´çª—å£æ§åˆ¶&lt;/strong&gt;ï¼ˆå¯é€‰ï¼‰ï¼š&lt;/p&gt; 
&lt;p&gt;æ­¤åŠŸèƒ½ç‹¬ç«‹äºä¸Šè¿°ä¸‰ç§æ¨é€æ¨¡å¼,å¯ä¸ä»»æ„æ¨¡å¼æ­é…ä½¿ç”¨:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;æ—¶é—´çª—å£é™åˆ¶&lt;/strong&gt;: è®¾å®šæ¨é€æ—¶é—´èŒƒå›´ï¼ˆå¦‚ 09:00-18:00 æˆ– 20:00-22:00ï¼‰,åªåœ¨æŒ‡å®šæ—¶é—´å†…æ¨é€&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ¨é€é¢‘ç‡æ§åˆ¶&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;çª—å£å†…å¤šæ¬¡æ¨é€: æ—¶é—´çª—å£å†…æ¯æ¬¡æ‰§è¡Œéƒ½æ¨é€&lt;/li&gt; 
   &lt;li&gt;æ¯å¤©ä»…æ¨é€ä¸€æ¬¡: æ—¶é—´çª—å£å†…åªæ¨é€ä¸€æ¬¡ï¼ˆé€‚åˆå½“æ—¥æ±‡æ€»æˆ–å½“å‰æ¦œå•æ¨¡å¼ï¼‰&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;å…¸å‹åœºæ™¯&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;å·¥ä½œæ—¶é—´æ¨é€: åªåœ¨å·¥ä½œæ—¥ 09:00-18:00 æ¥æ”¶æ¶ˆæ¯&lt;/li&gt; 
   &lt;li&gt;æ™šé—´æ±‡æ€»æ¨é€: å¸Œæœ›åœ¨æ™šä¸Šå›ºå®šæ—¶é—´ï¼ˆå¦‚ 20:00-22:00ï¼‰æ”¶åˆ°æ±‡æ€»&lt;/li&gt; 
   &lt;li&gt;é¿å…æ‰“æ‰°: é˜²æ­¢éå·¥ä½œæ—¶é—´æ”¶åˆ°æ¨é€é€šçŸ¥&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æç¤º: æ­¤åŠŸèƒ½é»˜è®¤å…³é—­,éœ€åœ¨ &lt;code&gt;config/config.yaml&lt;/code&gt; ä¸­æ‰‹åŠ¨å¯ç”¨ &lt;code&gt;push_window.enabled&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;ç²¾å‡†å†…å®¹ç­›é€‰&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;è®¾ç½®ä¸ªäººå…³é”®è¯ï¼ˆå¦‚ï¼šAIã€æ¯”äºšè¿ªã€æ•™è‚²æ”¿ç­–ï¼‰ï¼Œåªæ¨é€ç›¸å…³çƒ­ç‚¹ï¼Œè¿‡æ»¤æ— å…³ä¿¡æ¯&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;æ”¯æŒæ™®é€šè¯ã€å¿…é¡»è¯(+)ã€è¿‡æ»¤è¯(!)ä¸‰ç§è¯­æ³•ï¼Œè§ã€frequency_words.txt é…ç½®æ•™ç¨‹ã€‘&lt;/li&gt; 
 &lt;li&gt;è¯ç»„åŒ–ç®¡ç†ï¼Œç‹¬ç«‹ç»Ÿè®¡ä¸åŒä¸»é¢˜çƒ­ç‚¹&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ä¹Ÿå¯ä»¥ä¸åšç­›é€‰ï¼Œå®Œæ•´çš„æ¨é€æ‰€æœ‰çƒ­ç‚¹ï¼Œå…·ä½“è§ã€å†å²æ›´æ–°ã€‘ä¸­çš„ v2.0.1&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ‘‰ frequency_words.txt é…ç½®æ•™ç¨‹&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;åœ¨ &lt;code&gt;frequency_words.txt&lt;/code&gt; æ–‡ä»¶ä¸­é…ç½®ç›‘æ§çš„å…³é”®è¯ï¼Œæ”¯æŒä¸‰ç§è¯­æ³•å’Œè¯ç»„åŠŸèƒ½ã€‚&lt;/p&gt; 
 &lt;p&gt;å…³é”®è¯è¶Šé å‰ï¼Œæ–°é—»çš„ä¼˜å…ˆçº§è¶Šé«˜ï¼Œä½ å¯ä»¥æ ¹æ®è‡ªå·±çš„å…³æ³¨åº¦è°ƒæ•´å…³é”®è¯é¡ºåº&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;è¯­æ³•ç±»å‹&lt;/th&gt; 
    &lt;th&gt;ç¬¦å·&lt;/th&gt; 
    &lt;th&gt;ä½œç”¨&lt;/th&gt; 
    &lt;th&gt;ç¤ºä¾‹&lt;/th&gt; 
    &lt;th&gt;åŒ¹é…é€»è¾‘&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;æ™®é€šè¯&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;æ— &lt;/td&gt; 
    &lt;td&gt;åŸºç¡€åŒ¹é…&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;åä¸º&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;åŒ…å«ä»»æ„ä¸€ä¸ªå³å¯&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;å¿…é¡»è¯&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;+&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;é™å®šèŒƒå›´&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;+æ‰‹æœº&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;å¿…é¡»åŒæ—¶åŒ…å«&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;è¿‡æ»¤è¯&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;!&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;æ’é™¤å¹²æ‰°&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;!å¹¿å‘Š&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;åŒ…å«åˆ™ç›´æ¥æ’é™¤&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;ğŸ“‹ åŸºç¡€è¯­æ³•è¯´æ˜&lt;/h3&gt; 
 &lt;h4&gt;1. &lt;strong&gt;æ™®é€šå…³é”®è¯&lt;/strong&gt; - åŸºç¡€åŒ¹é…&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-txt"&gt;åä¸º
OPPO
è‹¹æœ
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;ä½œç”¨ï¼š&lt;/strong&gt; æ–°é—»æ ‡é¢˜åŒ…å«å…¶ä¸­&lt;strong&gt;ä»»æ„ä¸€ä¸ªè¯&lt;/strong&gt;å°±ä¼šè¢«æ•è·&lt;/p&gt; 
 &lt;h4&gt;2. &lt;strong&gt;å¿…é¡»è¯&lt;/strong&gt; &lt;code&gt;+è¯æ±‡&lt;/code&gt; - é™å®šèŒƒå›´&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-txt"&gt;åä¸º
OPPO
+æ‰‹æœº
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;ä½œç”¨ï¼š&lt;/strong&gt; å¿…é¡»åŒæ—¶åŒ…å«æ™®é€šè¯&lt;strong&gt;å’Œ&lt;/strong&gt;å¿…é¡»è¯æ‰ä¼šè¢«æ•è·&lt;/p&gt; 
 &lt;h4&gt;3. &lt;strong&gt;è¿‡æ»¤è¯&lt;/strong&gt; &lt;code&gt;!è¯æ±‡&lt;/code&gt; - æ’é™¤å¹²æ‰°&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-txt"&gt;è‹¹æœ
åä¸º
!æ°´æœ
!ä»·æ ¼
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;ä½œç”¨ï¼š&lt;/strong&gt; åŒ…å«è¿‡æ»¤è¯çš„æ–°é—»ä¼šè¢«&lt;strong&gt;ç›´æ¥æ’é™¤&lt;/strong&gt;ï¼Œå³ä½¿åŒ…å«å…³é”®è¯&lt;/p&gt; 
 &lt;h3&gt;ğŸ”— è¯ç»„åŠŸèƒ½ - ç©ºè¡Œåˆ†éš”çš„é‡è¦ä½œç”¨&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;æ ¸å¿ƒè§„åˆ™ï¼š&lt;/strong&gt; ç”¨&lt;strong&gt;ç©ºè¡Œ&lt;/strong&gt;åˆ†éš”ä¸åŒçš„è¯ç»„ï¼Œæ¯ä¸ªè¯ç»„ç‹¬ç«‹ç»Ÿè®¡&lt;/p&gt; 
 &lt;h4&gt;ç¤ºä¾‹é…ç½®ï¼š&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-txt"&gt;iPhone
åä¸º
OPPO
+å‘å¸ƒ

Aè‚¡
ä¸Šè¯
æ·±è¯
+æ¶¨è·Œ
!é¢„æµ‹

ä¸–ç•Œæ¯
æ¬§æ´²æ¯
äºšæ´²æ¯
+æ¯”èµ›
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;è¯ç»„è§£é‡ŠåŠåŒ¹é…æ•ˆæœï¼š&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;ç¬¬1ç»„ - æ‰‹æœºæ–°å“ç±»ï¼š&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;å…³é”®è¯ï¼šiPhoneã€åä¸ºã€OPPO&lt;/li&gt; 
  &lt;li&gt;å¿…é¡»è¯ï¼šå‘å¸ƒ&lt;/li&gt; 
  &lt;li&gt;æ•ˆæœï¼šå¿…é¡»åŒ…å«æ‰‹æœºå“ç‰Œåï¼ŒåŒæ—¶åŒ…å«"å‘å¸ƒ"&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;åŒ¹é…ç¤ºä¾‹ï¼š&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;âœ… "iPhone 15æ­£å¼å‘å¸ƒå”®ä»·å…¬å¸ƒ" â† æœ‰"iPhone"+"å‘å¸ƒ"&lt;/li&gt; 
  &lt;li&gt;âœ… "åä¸ºMate60ç³»åˆ—å‘å¸ƒä¼šç›´æ’­" â† æœ‰"åä¸º"+"å‘å¸ƒ"&lt;/li&gt; 
  &lt;li&gt;âœ… "OPPO Find X7å‘å¸ƒæ—¶é—´ç¡®å®š" â† æœ‰"OPPO"+"å‘å¸ƒ"&lt;/li&gt; 
  &lt;li&gt;âŒ "iPhoneé”€é‡åˆ›æ–°é«˜" â† æœ‰"iPhone"ä½†ç¼ºå°‘"å‘å¸ƒ"&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;ç¬¬2ç»„ - è‚¡å¸‚è¡Œæƒ…ç±»ï¼š&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;å…³é”®è¯ï¼šAè‚¡ã€ä¸Šè¯ã€æ·±è¯&lt;/li&gt; 
  &lt;li&gt;å¿…é¡»è¯ï¼šæ¶¨è·Œ&lt;/li&gt; 
  &lt;li&gt;è¿‡æ»¤è¯ï¼šé¢„æµ‹&lt;/li&gt; 
  &lt;li&gt;æ•ˆæœï¼šåŒ…å«è‚¡å¸‚ç›¸å…³è¯ï¼ŒåŒæ—¶åŒ…å«"æ¶¨è·Œ"ï¼Œä½†æ’é™¤åŒ…å«"é¢„æµ‹"çš„å†…å®¹&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;åŒ¹é…ç¤ºä¾‹ï¼š&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;âœ… "Aè‚¡ä»Šæ—¥å¤§å¹…æ¶¨è·Œåˆ†æ" â† æœ‰"Aè‚¡"+"æ¶¨è·Œ"&lt;/li&gt; 
  &lt;li&gt;âœ… "ä¸Šè¯æŒ‡æ•°æ¶¨è·ŒåŸå› è§£è¯»" â† æœ‰"ä¸Šè¯"+"æ¶¨è·Œ"&lt;/li&gt; 
  &lt;li&gt;âŒ "ä¸“å®¶é¢„æµ‹Aè‚¡æ¶¨è·Œè¶‹åŠ¿" â† æœ‰"Aè‚¡"+"æ¶¨è·Œ"ä½†åŒ…å«"é¢„æµ‹"&lt;/li&gt; 
  &lt;li&gt;âŒ "Aè‚¡æˆäº¤é‡åˆ›æ–°é«˜" â† æœ‰"Aè‚¡"ä½†ç¼ºå°‘"æ¶¨è·Œ"&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;ç¬¬3ç»„ - è¶³çƒèµ›äº‹ç±»ï¼š&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;å…³é”®è¯ï¼šä¸–ç•Œæ¯ã€æ¬§æ´²æ¯ã€äºšæ´²æ¯&lt;/li&gt; 
  &lt;li&gt;å¿…é¡»è¯ï¼šæ¯”èµ›&lt;/li&gt; 
  &lt;li&gt;æ•ˆæœï¼šå¿…é¡»åŒ…å«æ¯èµ›åç§°ï¼ŒåŒæ—¶åŒ…å«"æ¯”èµ›"&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;åŒ¹é…ç¤ºä¾‹ï¼š&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;âœ… "ä¸–ç•Œæ¯å°ç»„èµ›æ¯”èµ›ç»“æœ" â† æœ‰"ä¸–ç•Œæ¯"+"æ¯”èµ›"&lt;/li&gt; 
  &lt;li&gt;âœ… "æ¬§æ´²æ¯å†³èµ›æ¯”èµ›æ—¶é—´" â† æœ‰"æ¬§æ´²æ¯"+"æ¯”èµ›"&lt;/li&gt; 
  &lt;li&gt;âŒ "ä¸–ç•Œæ¯é—¨ç¥¨å¼€å”®" â† æœ‰"ä¸–ç•Œæ¯"ä½†ç¼ºå°‘"æ¯”èµ›"&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;ğŸ¯ é…ç½®æŠ€å·§&lt;/h3&gt; 
 &lt;h4&gt;1. &lt;strong&gt;ä»å®½åˆ°ä¸¥çš„é…ç½®ç­–ç•¥&lt;/strong&gt;&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-txt"&gt;# ç¬¬ä¸€æ­¥ï¼šå…ˆç”¨å®½æ³›å…³é”®è¯æµ‹è¯•
äººå·¥æ™ºèƒ½
AI
ChatGPT

# ç¬¬äºŒæ­¥ï¼šå‘ç°è¯¯åŒ¹é…åï¼ŒåŠ å…¥å¿…é¡»è¯é™å®š
äººå·¥æ™ºèƒ½  
AI
ChatGPT
+æŠ€æœ¯

# ç¬¬ä¸‰æ­¥ï¼šå‘ç°å¹²æ‰°å†…å®¹åï¼ŒåŠ å…¥è¿‡æ»¤è¯
äººå·¥æ™ºèƒ½
AI  
ChatGPT
+æŠ€æœ¯
!å¹¿å‘Š
!åŸ¹è®­
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;2. &lt;strong&gt;é¿å…è¿‡åº¦å¤æ‚&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;âŒ &lt;strong&gt;ä¸æ¨èï¼š&lt;/strong&gt; ä¸€ä¸ªè¯ç»„åŒ…å«å¤ªå¤šè¯æ±‡&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-txt"&gt;åä¸º
OPPO
è‹¹æœ
ä¸‰æ˜Ÿ
vivo
ä¸€åŠ 
é­…æ—
+æ‰‹æœº
+å‘å¸ƒ
+é”€é‡
!å‡è´§
!ç»´ä¿®
!äºŒæ‰‹
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;âœ… &lt;strong&gt;æ¨èï¼š&lt;/strong&gt; æ‹†åˆ†æˆå¤šä¸ªç²¾ç¡®çš„è¯ç»„&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-txt"&gt;åä¸º
OPPO
+æ–°å“

è‹¹æœ
ä¸‰æ˜Ÿ  
+å‘å¸ƒ

æ‰‹æœº
é”€é‡
+å¸‚åœº
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;&lt;strong&gt;çƒ­ç‚¹è¶‹åŠ¿åˆ†æ&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;å®æ—¶è¿½è¸ªæ–°é—»çƒ­åº¦å˜åŒ–ï¼Œè®©ä½ ä¸ä»…çŸ¥é“"ä»€ä¹ˆåœ¨çƒ­æœ"ï¼Œæ›´äº†è§£"çƒ­ç‚¹å¦‚ä½•æ¼”å˜"&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;æ—¶é—´è½´è¿½è¸ª&lt;/strong&gt;ï¼šè®°å½•æ¯æ¡æ–°é—»ä»é¦–æ¬¡å‡ºç°åˆ°æœ€åå‡ºç°çš„å®Œæ•´æ—¶é—´è·¨åº¦&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;çƒ­åº¦å˜åŒ–&lt;/strong&gt;ï¼šç»Ÿè®¡æ–°é—»åœ¨ä¸åŒæ—¶é—´æ®µçš„æ’åå˜åŒ–å’Œå‡ºç°é¢‘æ¬¡&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ–°å¢æ£€æµ‹&lt;/strong&gt;ï¼šå®æ—¶è¯†åˆ«æ–°å‡ºç°çš„çƒ­ç‚¹è¯é¢˜ï¼Œç”¨ğŸ†•æ ‡è®°ç¬¬ä¸€æ—¶é—´æé†’&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æŒç»­æ€§åˆ†æ&lt;/strong&gt;ï¼šåŒºåˆ†ä¸€æ¬¡æ€§çƒ­ç‚¹è¯é¢˜å’ŒæŒç»­å‘é…µçš„æ·±åº¦æ–°é—»&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;è·¨å¹³å°å¯¹æ¯”&lt;/strong&gt;ï¼šåŒä¸€æ–°é—»åœ¨ä¸åŒå¹³å°çš„æ’åè¡¨ç°ï¼Œçœ‹å‡ºåª’ä½“å…³æ³¨åº¦å·®å¼‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ä¸å†é”™è¿‡é‡è¦æ–°é—»çš„å®Œæ•´å‘å±•è¿‡ç¨‹ï¼Œä»è¯é¢˜èŒèŠ½åˆ°é«˜å³°çƒ­è®®ï¼Œå…¨ç¨‹æŒæ¡&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ‘‰ æ¨é€æ ¼å¼è¯´æ˜&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;ğŸ“Š çƒ­ç‚¹è¯æ±‡ç»Ÿè®¡&lt;/p&gt; 
 &lt;p&gt;ğŸ”¥ [1/3] AI ChatGPT : 2 æ¡&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;[ç™¾åº¦çƒ­æœ] ğŸ†• ChatGPT-5æ­£å¼å‘å¸ƒ [&lt;strong&gt;1&lt;/strong&gt;] - 09æ—¶15åˆ† (1æ¬¡)&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[ä»Šæ—¥å¤´æ¡] AIèŠ¯ç‰‡æ¦‚å¿µè‚¡æš´æ¶¨ [&lt;strong&gt;3&lt;/strong&gt;] - [08æ—¶30åˆ† ~ 10æ—¶45åˆ†] (3æ¬¡)&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”&lt;/p&gt; 
 &lt;p&gt;ğŸ“ˆ [2/3] æ¯”äºšè¿ª ç‰¹æ–¯æ‹‰ : 2 æ¡&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;[å¾®åš] ğŸ†• æ¯”äºšè¿ªæœˆé”€é‡ç ´çºªå½• [&lt;strong&gt;2&lt;/strong&gt;] - 10æ—¶20åˆ† (1æ¬¡)&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[æŠ–éŸ³] ç‰¹æ–¯æ‹‰é™ä»·ä¿ƒé”€ [&lt;strong&gt;4&lt;/strong&gt;] - [07æ—¶45åˆ† ~ 09æ—¶15åˆ†] (2æ¬¡)&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”&lt;/p&gt; 
 &lt;p&gt;ğŸ“Œ [3/3] Aè‚¡ è‚¡å¸‚ : 1 æ¡&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;[åå°”è¡—è§é—»] Aè‚¡åˆç›˜ç‚¹è¯„åˆ†æ [&lt;strong&gt;5&lt;/strong&gt;] - [11æ—¶30åˆ† ~ 12æ—¶00åˆ†] (2æ¬¡)&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;ğŸ†• æœ¬æ¬¡æ–°å¢çƒ­ç‚¹æ–°é—» (å…± 2 æ¡)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;ç™¾åº¦çƒ­æœ&lt;/strong&gt; (1 æ¡):&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;ChatGPT-5æ­£å¼å‘å¸ƒ [&lt;strong&gt;1&lt;/strong&gt;]&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;å¾®åš&lt;/strong&gt; (1 æ¡):&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;æ¯”äºšè¿ªæœˆé”€é‡ç ´çºªå½• [&lt;strong&gt;2&lt;/strong&gt;]&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;æ›´æ–°æ—¶é—´ï¼š2025-01-15 12:30:15&lt;/p&gt; 
 &lt;h2&gt;&lt;strong&gt;æ¶ˆæ¯æ ¼å¼è¯´æ˜&lt;/strong&gt;&lt;/h2&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;æ ¼å¼å…ƒç´ &lt;/th&gt; 
    &lt;th&gt;ç¤ºä¾‹&lt;/th&gt; 
    &lt;th&gt;å«ä¹‰&lt;/th&gt; 
    &lt;th&gt;è¯´æ˜&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;ğŸ”¥ğŸ“ˆğŸ“Œ&lt;/td&gt; 
    &lt;td&gt;ğŸ”¥ [1/3] AI ChatGPT&lt;/td&gt; 
    &lt;td&gt;çƒ­åº¦ç­‰çº§&lt;/td&gt; 
    &lt;td&gt;ğŸ”¥é«˜çƒ­åº¦(â‰¥10æ¡) ğŸ“ˆä¸­çƒ­åº¦(5-9æ¡) ğŸ“Œæ™®é€šçƒ­åº¦(&amp;lt;5æ¡)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;[åºå·/æ€»æ•°]&lt;/td&gt; 
    &lt;td&gt;[1/3]&lt;/td&gt; 
    &lt;td&gt;æ’åºä½ç½®&lt;/td&gt; 
    &lt;td&gt;å½“å‰è¯ç»„åœ¨æ‰€æœ‰åŒ¹é…è¯ç»„ä¸­çš„æ’å&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;é¢‘ç‡è¯ç»„&lt;/td&gt; 
    &lt;td&gt;AI ChatGPT&lt;/td&gt; 
    &lt;td&gt;å…³é”®è¯ç»„&lt;/td&gt; 
    &lt;td&gt;é…ç½®æ–‡ä»¶ä¸­çš„è¯ç»„ï¼Œæ ‡é¢˜å¿…é¡»åŒ…å«å…¶ä¸­è¯æ±‡&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;: N æ¡&lt;/td&gt; 
    &lt;td&gt;: 2 æ¡&lt;/td&gt; 
    &lt;td&gt;åŒ¹é…æ•°é‡&lt;/td&gt; 
    &lt;td&gt;è¯¥è¯ç»„åŒ¹é…çš„æ–°é—»æ€»æ•°&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;[å¹³å°å]&lt;/td&gt; 
    &lt;td&gt;[ç™¾åº¦çƒ­æœ]&lt;/td&gt; 
    &lt;td&gt;æ¥æºå¹³å°&lt;/td&gt; 
    &lt;td&gt;æ–°é—»æ‰€å±çš„å¹³å°åç§°&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;ğŸ†•&lt;/td&gt; 
    &lt;td&gt;ğŸ†• ChatGPT-5æ­£å¼å‘å¸ƒ&lt;/td&gt; 
    &lt;td&gt;æ–°å¢æ ‡è®°&lt;/td&gt; 
    &lt;td&gt;æœ¬è½®æŠ“å–ä¸­é¦–æ¬¡å‡ºç°çš„çƒ­ç‚¹&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;[&lt;strong&gt;æ•°å­—&lt;/strong&gt;]&lt;/td&gt; 
    &lt;td&gt;[&lt;strong&gt;1&lt;/strong&gt;]&lt;/td&gt; 
    &lt;td&gt;é«˜æ’å&lt;/td&gt; 
    &lt;td&gt;æ’åâ‰¤é˜ˆå€¼çš„çƒ­æœï¼Œçº¢è‰²åŠ ç²—æ˜¾ç¤º&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;[æ•°å­—]&lt;/td&gt; 
    &lt;td&gt;[7]&lt;/td&gt; 
    &lt;td&gt;æ™®é€šæ’å&lt;/td&gt; 
    &lt;td&gt;æ’å&amp;gt;é˜ˆå€¼çš„çƒ­æœï¼Œæ™®é€šæ˜¾ç¤º&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;- æ—¶é—´&lt;/td&gt; 
    &lt;td&gt;- 09æ—¶15åˆ†&lt;/td&gt; 
    &lt;td&gt;é¦–æ¬¡æ—¶é—´&lt;/td&gt; 
    &lt;td&gt;è¯¥æ–°é—»é¦–æ¬¡è¢«å‘ç°çš„æ—¶é—´&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;[æ—¶é—´~æ—¶é—´]&lt;/td&gt; 
    &lt;td&gt;[08æ—¶30åˆ† ~ 10æ—¶45åˆ†]&lt;/td&gt; 
    &lt;td&gt;æŒç»­æ—¶é—´&lt;/td&gt; 
    &lt;td&gt;ä»é¦–æ¬¡å‡ºç°åˆ°æœ€åå‡ºç°çš„æ—¶é—´èŒƒå›´&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;(Næ¬¡)&lt;/td&gt; 
    &lt;td&gt;(3æ¬¡)&lt;/td&gt; 
    &lt;td&gt;å‡ºç°é¢‘ç‡&lt;/td&gt; 
    &lt;td&gt;åœ¨ç›‘æ§æœŸé—´å‡ºç°çš„æ€»æ¬¡æ•°&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;æ–°å¢åŒºåŸŸ&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;ğŸ†• &lt;strong&gt;æœ¬æ¬¡æ–°å¢çƒ­ç‚¹æ–°é—»&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;æ–°è¯é¢˜æ±‡æ€»&lt;/td&gt; 
    &lt;td&gt;å•ç‹¬å±•ç¤ºæœ¬è½®æ–°å‡ºç°çš„çƒ­ç‚¹è¯é¢˜&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;&lt;strong&gt;ä¸ªæ€§åŒ–çƒ­ç‚¹ç®—æ³•&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;ä¸å†è¢«å„ä¸ªå¹³å°çš„ç®—æ³•ç‰µç€èµ°ï¼ŒTrendRadar ä¼šé‡æ–°æ•´ç†å…¨ç½‘çƒ­æœï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;çœ‹é‡æ’åé«˜çš„æ–°é—»&lt;/strong&gt;ï¼ˆå 60%ï¼‰ï¼šå„å¹³å°å‰å‡ åçš„æ–°é—»ä¼˜å…ˆæ˜¾ç¤º&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;å…³æ³¨æŒç»­å‡ºç°çš„è¯é¢˜&lt;/strong&gt;ï¼ˆå 30%ï¼‰ï¼šåå¤å‡ºç°çš„æ–°é—»æ›´é‡è¦&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;è€ƒè™‘æ’åè´¨é‡&lt;/strong&gt;ï¼ˆå 10%ï¼‰ï¼šä¸ä»…å¤šæ¬¡å‡ºç°ï¼Œè¿˜ç»å¸¸æ’åœ¨å‰åˆ—&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æŠŠåˆ†æ•£åœ¨å„ä¸ªå¹³å°çš„çƒ­æœåˆå¹¶èµ·æ¥ï¼ŒæŒ‰ç…§ä½ å…³å¿ƒçš„çƒ­åº¦é‡æ–°æ’åºï¼Œè¿™ä¸‰ä¸ªæ¯”ä¾‹å¯ä»¥é€‰æ‹©é€‚åˆè‡ªå·±çš„åœºæ™¯è¿›è¡Œè°ƒæ•´&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ‘‰ çƒ­ç‚¹æƒé‡è°ƒæ•´&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;å½“å‰é»˜è®¤çš„é…ç½®æ˜¯å¹³è¡¡æ€§é…ç½®&lt;/p&gt; 
 &lt;h3&gt;ä¸¤ä¸ªæ ¸å¿ƒåœºæ™¯&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;è¿½å®æ—¶çƒ­ç‚¹å‹&lt;/strong&gt;ï¼š&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;weight:
  rank_weight: 0.8    # ä¸»è¦çœ‹æ’å
  frequency_weight: 0.1  # ä¸å¤ªåœ¨ä¹æŒç»­æ€§
  hotness_weight: 0.1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;é€‚ç”¨äººç¾¤&lt;/strong&gt;ï¼šè‡ªåª’ä½“åšä¸»ã€è¥é”€äººå‘˜ã€æƒ³å¿«é€Ÿäº†è§£å½“ä¸‹æœ€ç«è¯é¢˜çš„ç”¨æˆ·&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;è¿½æ·±åº¦è¯é¢˜å‹&lt;/strong&gt;ï¼š&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;weight:
  rank_weight: 0.4    # é€‚åº¦çœ‹æ’å
  frequency_weight: 0.5  # é‡è§†å½“å¤©å†…çš„æŒç»­çƒ­åº¦
  hotness_weight: 0.1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;é€‚ç”¨äººç¾¤&lt;/strong&gt;ï¼šæŠ•èµ„è€…ã€ç ”ç©¶äººå‘˜ã€æ–°é—»å·¥ä½œè€…ã€éœ€è¦æ·±åº¦åˆ†æè¶‹åŠ¿çš„ç”¨æˆ·&lt;/p&gt; 
 &lt;h3&gt;è°ƒæ•´çš„æ–¹æ³•&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;ä¸‰ä¸ªæ•°å­—åŠ èµ·æ¥å¿…é¡»ç­‰äº 1.0&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;å“ªä¸ªé‡è¦å°±è°ƒå¤§å“ªä¸ª&lt;/strong&gt;ï¼šåœ¨ä¹æ’åå°±è°ƒå¤§ rank_weightï¼Œåœ¨ä¹æŒç»­æ€§å°±è°ƒå¤§ frequency_weight&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;å»ºè®®æ¯æ¬¡åªè°ƒ 0.1-0.2&lt;/strong&gt;ï¼Œè§‚å¯Ÿæ•ˆæœ&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;æ ¸å¿ƒæ€è·¯ï¼šè¿½æ±‚é€Ÿåº¦å’Œæ—¶æ•ˆæ€§çš„ç”¨æˆ·æé«˜æ’åæƒé‡ï¼Œè¿½æ±‚æ·±åº¦å’Œç¨³å®šæ€§çš„ç”¨æˆ·æé«˜é¢‘æ¬¡æƒé‡ã€‚&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;&lt;strong&gt;å¤šæ¸ é“å®æ—¶æ¨é€&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;æ”¯æŒ&lt;strong&gt;ä¼ä¸šå¾®ä¿¡&lt;/strong&gt;(+ å¾®ä¿¡æ¨é€æ–¹æ¡ˆ)ã€&lt;strong&gt;é£ä¹¦&lt;/strong&gt;ã€&lt;strong&gt;é’‰é’‰&lt;/strong&gt;ã€&lt;strong&gt;Telegram&lt;/strong&gt;ã€&lt;strong&gt;é‚®ä»¶&lt;/strong&gt;ã€&lt;strong&gt;ntfy&lt;/strong&gt;ï¼Œæ¶ˆæ¯ç›´è¾¾æ‰‹æœºå’Œé‚®ç®±&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;å¤šç«¯é€‚é…&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Pages&lt;/strong&gt;ï¼šè‡ªåŠ¨ç”Ÿæˆç²¾ç¾ç½‘é¡µæŠ¥å‘Šï¼ŒPC/ç§»åŠ¨ç«¯é€‚é…&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dockeréƒ¨ç½²&lt;/strong&gt;ï¼šæ”¯æŒå¤šæ¶æ„å®¹å™¨åŒ–è¿è¡Œ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ•°æ®æŒä¹…åŒ–&lt;/strong&gt;ï¼šHTML/TXTå¤šæ ¼å¼å†å²è®°å½•ä¿å­˜&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;strong&gt;AI æ™ºèƒ½åˆ†æï¼ˆv3.0.0 æ–°å¢ï¼‰&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;åŸºäº MCP (Model Context Protocol) åè®®çš„ AI å¯¹è¯åˆ†æç³»ç»Ÿï¼Œè®©ä½ ç”¨è‡ªç„¶è¯­è¨€æ·±åº¦æŒ–æ˜æ–°é—»æ•°æ®&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;å¯¹è¯å¼æŸ¥è¯¢&lt;/strong&gt;ï¼šç”¨è‡ªç„¶è¯­è¨€æé—®ï¼Œå¦‚"æŸ¥è¯¢æ˜¨å¤©çŸ¥ä¹çš„çƒ­ç‚¹"ã€"åˆ†ææ¯”ç‰¹å¸æœ€è¿‘çš„çƒ­åº¦è¶‹åŠ¿"&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;13 ç§åˆ†æå·¥å…·&lt;/strong&gt;ï¼šæ¶µç›–åŸºç¡€æŸ¥è¯¢ã€æ™ºèƒ½æ£€ç´¢ã€è¶‹åŠ¿åˆ†æã€æ•°æ®æ´å¯Ÿã€æƒ…æ„Ÿåˆ†æç­‰&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;å¤šå®¢æˆ·ç«¯æ”¯æŒ&lt;/strong&gt;ï¼šCherry Studioï¼ˆGUI é…ç½®ï¼‰ã€Claude Desktopã€Cursorã€Cline ç­‰&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ·±åº¦åˆ†æèƒ½åŠ›&lt;/strong&gt;ï¼š 
  &lt;ul&gt; 
   &lt;li&gt;è¯é¢˜è¶‹åŠ¿è¿½è¸ªï¼ˆçƒ­åº¦å˜åŒ–ã€ç”Ÿå‘½å‘¨æœŸã€çˆ†ç«æ£€æµ‹ã€è¶‹åŠ¿é¢„æµ‹ï¼‰&lt;/li&gt; 
   &lt;li&gt;è·¨å¹³å°æ•°æ®å¯¹æ¯”ï¼ˆæ´»è·ƒåº¦ç»Ÿè®¡ã€å…³é”®è¯å…±ç°ï¼‰&lt;/li&gt; 
   &lt;li&gt;æ™ºèƒ½æ‘˜è¦ç”Ÿæˆã€ç›¸ä¼¼æ–°é—»æŸ¥æ‰¾ã€å†å²å…³è”æ£€ç´¢&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;å‘Šåˆ«æ‰‹åŠ¨ç¿»é˜…æ•°æ®æ–‡ä»¶ï¼ŒAI åŠ©æ‰‹å¸®ä½ ç§’æ‡‚æ–°é—»èƒŒåçš„æ•…äº‹&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;é›¶æŠ€æœ¯é—¨æ§›éƒ¨ç½²&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;GitHub ä¸€é”® Fork å³å¯ä½¿ç”¨ï¼Œæ— éœ€ç¼–ç¨‹åŸºç¡€ã€‚&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;30ç§’éƒ¨ç½²ï¼š GitHub Pagesï¼ˆç½‘é¡µæµè§ˆï¼‰æ”¯æŒä¸€é”®ä¿å­˜æˆå›¾ç‰‡ï¼Œéšæ—¶åˆ†äº«ç»™ä»–äºº&lt;/p&gt; 
 &lt;p&gt;1åˆ†é’Ÿéƒ¨ç½²ï¼š ä¼ä¸šå¾®ä¿¡ï¼ˆæ‰‹æœºé€šçŸ¥ï¼‰&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;ğŸ’¡ æç¤ºï¼š&lt;/strong&gt; æƒ³è¦&lt;strong&gt;å®æ—¶æ›´æ–°&lt;/strong&gt;çš„ç½‘é¡µç‰ˆï¼Ÿfork åï¼Œè¿›å…¥ä½ çš„ä»“åº“ Settings â†’ Pagesï¼Œå¯ç”¨ GitHub Pagesã€‚&lt;a href="https://sansan0.github.io/TrendRadar/"&gt;æ•ˆæœé¢„è§ˆ&lt;/a&gt;ã€‚&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;å‡å°‘ APP ä¾èµ–&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;ä»"è¢«ç®—æ³•æ¨èç»‘æ¶"å˜æˆ"ä¸»åŠ¨è·å–è‡ªå·±æƒ³è¦çš„ä¿¡æ¯"&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;é€‚åˆäººç¾¤ï¼š&lt;/strong&gt; æŠ•èµ„è€…ã€è‡ªåª’ä½“äººã€ä¼ä¸šå…¬å…³ã€å…³å¿ƒæ—¶äº‹çš„æ™®é€šç”¨æˆ·&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;å…¸å‹åœºæ™¯ï¼š&lt;/strong&gt; è‚¡å¸‚æŠ•èµ„ç›‘æ§ã€å“ç‰Œèˆ†æƒ…è¿½è¸ªã€è¡Œä¸šåŠ¨æ€å…³æ³¨ã€ç”Ÿæ´»èµ„è®¯è·å–&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Github Pages æ•ˆæœ(æ‰‹æœºç«¯é€‚é…ã€é‚®ç®±æ¨é€æ•ˆæœ)&lt;/th&gt; 
   &lt;th align="center"&gt;é£ä¹¦æ¨é€æ•ˆæœ&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/github-pages.png" alt="Github Pagesæ•ˆæœ" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/feishu.jpg" alt="é£ä¹¦æ¨é€æ•ˆæœ" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;ğŸ“ æ›´æ–°æ—¥å¿—&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;å‡çº§è¯´æ˜&lt;/strong&gt;ï¼š&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;æç¤º&lt;/strong&gt;ï¼šä¸è¦é€šè¿‡ &lt;strong&gt;Sync fork&lt;/strong&gt; æ›´æ–°æœ¬é¡¹ç›®, å»ºè®®æŸ¥çœ‹ã€å†å²æ›´æ–°ã€‘ï¼Œæ˜ç¡®å…·ä½“çš„ã€å‡çº§æ–¹å¼ã€‘å’Œã€åŠŸèƒ½å†…å®¹ã€‘&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;å°ç‰ˆæœ¬æ›´æ–°&lt;/strong&gt;ï¼šä» v2.x å‡çº§åˆ° v2.y, ç”¨æœ¬é¡¹ç›®çš„ &lt;code&gt;main.py&lt;/code&gt; ä»£ç æ›¿æ¢ä½  fork ä»“åº“ä¸­çš„å¯¹åº”æ–‡ä»¶&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;å¤§ç‰ˆæœ¬å‡çº§&lt;/strong&gt;ï¼šä» v1.x å‡çº§åˆ° v2.y, å»ºè®®åˆ é™¤ç°æœ‰ fork åé‡æ–° forkï¼Œè¿™æ ·æ›´çœåŠ›ä¸”é¿å…é…ç½®å†²çª&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;2025/10/26 - mcp-v1.0.1&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;MCP æ¨¡å—æ›´æ–°:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ä¿®å¤æ—¥æœŸæŸ¥è¯¢å‚æ•°ä¼ é€’é”™è¯¯&lt;/li&gt; 
 &lt;li&gt;ç»Ÿä¸€æ‰€æœ‰å·¥å…·çš„æ—¶é—´å‚æ•°æ ¼å¼&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;2025/10/31 - v3.0.4&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;è§£å†³é£ä¹¦å› æ¨é€å†…å®¹è¿‡é•¿è€Œäº§ç”Ÿçš„é”™è¯¯ï¼Œå®ç°äº†åˆ†æ‰¹æ¨é€&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ‘‰ å†å²æ›´æ–°&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h3&gt;2025/10/23 - v3.0.3&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;æ‰©å¤§ ntfy é”™è¯¯ä¿¡æ¯æ˜¾ç¤ºèŒƒå›´&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/10/21 - v3.0.2&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ä¿®å¤ ntfy æ¨é€ç¼–ç é—®é¢˜&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/10/20 - v3.0.0&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;é‡å¤§æ›´æ–° - AI åˆ†æåŠŸèƒ½ä¸Šçº¿&lt;/strong&gt; ğŸ¤–&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;æ ¸å¿ƒåŠŸèƒ½&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;æ–°å¢åŸºäº MCP (Model Context Protocol) çš„ AI åˆ†ææœåŠ¡å™¨&lt;/li&gt; 
    &lt;li&gt;æ”¯æŒ13ç§æ™ºèƒ½åˆ†æå·¥å…·ï¼šåŸºç¡€æŸ¥è¯¢ã€æ™ºèƒ½æ£€ç´¢ã€é«˜çº§åˆ†æã€ç³»ç»Ÿç®¡ç†&lt;/li&gt; 
    &lt;li&gt;è‡ªç„¶è¯­è¨€äº¤äº’ï¼šé€šè¿‡å¯¹è¯æ–¹å¼æŸ¥è¯¢å’Œåˆ†ææ–°é—»æ•°æ®&lt;/li&gt; 
    &lt;li&gt;å¤šå®¢æˆ·ç«¯æ”¯æŒï¼šClaude Desktopã€Cherry Studioã€Cursorã€Cline ç­‰&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;åˆ†æèƒ½åŠ›&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;è¯é¢˜è¶‹åŠ¿åˆ†æï¼ˆçƒ­åº¦è¿½è¸ªã€ç”Ÿå‘½å‘¨æœŸã€çˆ†ç«æ£€æµ‹ã€è¶‹åŠ¿é¢„æµ‹ï¼‰&lt;/li&gt; 
    &lt;li&gt;æ•°æ®æ´å¯Ÿï¼ˆå¹³å°å¯¹æ¯”ã€æ´»è·ƒåº¦ç»Ÿè®¡ã€å…³é”®è¯å…±ç°ï¼‰&lt;/li&gt; 
    &lt;li&gt;æƒ…æ„Ÿåˆ†æã€ç›¸ä¼¼æ–°é—»æŸ¥æ‰¾ã€æ™ºèƒ½æ‘˜è¦ç”Ÿæˆ&lt;/li&gt; 
    &lt;li&gt;å†å²ç›¸å…³æ–°é—»æ£€ç´¢ã€å¤šæ¨¡å¼æœç´¢&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;æ›´æ–°æç¤º&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;è¿™æ˜¯ç‹¬ç«‹çš„ AI åˆ†æåŠŸèƒ½ï¼Œä¸å½±å“ç°æœ‰çš„æ¨é€åŠŸèƒ½&lt;/li&gt; 
    &lt;li&gt;å¯é€‰æ‹©æ€§ä½¿ç”¨ï¼Œæ— éœ€å‡çº§ç°æœ‰éƒ¨ç½²&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/10/15 - v2.4.4&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;æ›´æ–°å†…å®¹&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;ä¿®å¤ ntfy æ¨é€ç¼–ç é—®é¢˜ + 1&lt;/li&gt; 
    &lt;li&gt;ä¿®å¤æ¨é€æ—¶é—´çª—å£åˆ¤æ–­é—®é¢˜&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;æ›´æ–°æç¤º&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;å»ºè®®ã€å°ç‰ˆæœ¬å‡çº§ã€‘&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/10/10 - v2.4.3&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;æ„Ÿè°¢ &lt;a href="https://github.com/sansan0/TrendRadar/issues/98"&gt;nidaye996&lt;/a&gt; å‘ç°çš„ä½“éªŒé—®é¢˜&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;æ›´æ–°å†…å®¹&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;é‡æ„"é™é»˜æ¨é€æ¨¡å¼"å‘½åä¸º"æ¨é€æ—¶é—´çª—å£æ§åˆ¶"ï¼Œæå‡åŠŸèƒ½ç†è§£åº¦&lt;/li&gt; 
    &lt;li&gt;æ˜ç¡®æ¨é€æ—¶é—´çª—å£ä½œä¸ºå¯é€‰é™„åŠ åŠŸèƒ½ï¼Œå¯ä¸ä¸‰ç§æ¨é€æ¨¡å¼æ­é…ä½¿ç”¨&lt;/li&gt; 
    &lt;li&gt;æ”¹è¿›æ³¨é‡Šå’Œæ–‡æ¡£æè¿°ï¼Œä½¿åŠŸèƒ½å®šä½æ›´åŠ æ¸…æ™°&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;æ›´æ–°æç¤º&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;è¿™ä¸ªä»…ä»…æ˜¯é‡æ„ï¼Œå¯ä»¥ä¸ç”¨å‡çº§&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/10/8 - v2.4.2&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;æ›´æ–°å†…å®¹&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;ä¿®å¤ ntfy æ¨é€ç¼–ç é—®é¢˜&lt;/li&gt; 
    &lt;li&gt;ä¿®å¤é…ç½®æ–‡ä»¶ç¼ºå¤±é—®é¢˜&lt;/li&gt; 
    &lt;li&gt;ä¼˜åŒ– ntfy æ¨é€æ•ˆæœ&lt;/li&gt; 
    &lt;li&gt;å¢åŠ  github page å›¾ç‰‡åˆ†æ®µå¯¼å‡ºåŠŸèƒ½&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;æ›´æ–°æç¤º&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;å»ºè®®ä½¿ç”¨ã€å¤§ç‰ˆæœ¬æ›´æ–°ã€‘&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/10/2 - v2.4.0&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;æ–°å¢ ntfy æ¨é€é€šçŸ¥&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;æ ¸å¿ƒåŠŸèƒ½&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;æ”¯æŒ ntfy.sh å…¬å…±æœåŠ¡å’Œè‡ªæ‰˜ç®¡æœåŠ¡å™¨&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ä½¿ç”¨åœºæ™¯&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;é€‚åˆè¿½æ±‚éšç§çš„ç”¨æˆ·ï¼ˆæ”¯æŒè‡ªæ‰˜ç®¡ï¼‰&lt;/li&gt; 
    &lt;li&gt;è·¨å¹³å°æ¨é€ï¼ˆiOSã€Androidã€Desktopã€Webï¼‰&lt;/li&gt; 
    &lt;li&gt;æ— éœ€æ³¨å†Œè´¦å·ï¼ˆå…¬å…±æœåŠ¡å™¨ï¼‰&lt;/li&gt; 
    &lt;li&gt;å¼€æºå…è´¹ï¼ˆMIT åè®®ï¼‰&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;æ›´æ–°æç¤º&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;å»ºè®®ä½¿ç”¨ã€å¤§ç‰ˆæœ¬æ›´æ–°ã€‘&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/09/26 - v2.3.2&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ä¿®æ­£äº†é‚®ä»¶é€šçŸ¥é…ç½®æ£€æŸ¥è¢«é—æ¼çš„é—®é¢˜ï¼ˆ&lt;a href="https://github.com/sansan0/TrendRadar/issues/88"&gt;#88&lt;/a&gt;ï¼‰&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;ä¿®å¤è¯´æ˜&lt;/strong&gt;ï¼š&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;è§£å†³äº†å³ä½¿æ­£ç¡®é…ç½®é‚®ä»¶é€šçŸ¥ï¼Œç³»ç»Ÿä»æç¤º"æœªé…ç½®ä»»ä½•webhook"çš„é—®é¢˜&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/09/22 - v2.3.1&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;æ–°å¢é‚®ä»¶æ¨é€åŠŸèƒ½&lt;/strong&gt;ï¼Œæ”¯æŒå°†çƒ­ç‚¹æ–°é—»æŠ¥å‘Šå‘é€åˆ°é‚®ç®±&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;æ™ºèƒ½ SMTP è¯†åˆ«&lt;/strong&gt;ï¼šè‡ªåŠ¨è¯†åˆ« Gmailã€QQé‚®ç®±ã€Outlookã€ç½‘æ˜“é‚®ç®±ç­‰ 10+ ç§é‚®ç®±æœåŠ¡å•†é…ç½®&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;HTML ç²¾ç¾æ ¼å¼&lt;/strong&gt;ï¼šé‚®ä»¶å†…å®¹é‡‡ç”¨ä¸ç½‘é¡µç‰ˆç›¸åŒçš„ HTML æ ¼å¼ï¼Œæ’ç‰ˆç²¾ç¾ï¼Œç§»åŠ¨ç«¯é€‚é…&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;æ‰¹é‡å‘é€æ”¯æŒ&lt;/strong&gt;ï¼šæ”¯æŒå¤šä¸ªæ”¶ä»¶äººï¼Œç”¨é€—å·åˆ†éš”å³å¯åŒæ—¶å‘é€ç»™å¤šäºº&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;è‡ªå®šä¹‰ SMTP&lt;/strong&gt;ï¼šå¯è‡ªå®šä¹‰ SMTP æœåŠ¡å™¨å’Œç«¯å£&lt;/li&gt; 
  &lt;li&gt;ä¿®å¤Dockeræ„å»ºç½‘ç»œè¿æ¥é—®é¢˜&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;ä½¿ç”¨è¯´æ˜&lt;/strong&gt;ï¼š&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;é€‚ç”¨åœºæ™¯ï¼šé€‚åˆéœ€è¦é‚®ä»¶å½’æ¡£ã€å›¢é˜Ÿåˆ†äº«ã€å®šæ—¶æŠ¥å‘Šçš„ç”¨æˆ·&lt;/li&gt; 
  &lt;li&gt;æ”¯æŒé‚®ç®±ï¼šGmailã€QQé‚®ç®±ã€Outlook/Hotmailã€163/126é‚®ç®±ã€æ–°æµªé‚®ç®±ã€æœç‹é‚®ç®±ç­‰&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;æ›´æ–°æç¤º&lt;/strong&gt;ï¼š&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;æ­¤æ¬¡æ›´æ–°çš„å†…å®¹æ¯”è¾ƒå¤šï¼Œå¦‚æœæƒ³å‡çº§ï¼Œå»ºè®®é‡‡ç”¨ã€å¤§ç‰ˆæœ¬å‡çº§ã€‘&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/09/17 - v2.2.0&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;æ–°å¢ä¸€é”®ä¿å­˜æ–°é—»å›¾ç‰‡åŠŸèƒ½ï¼Œè®©ä½ è½»æ¾åˆ†äº«å…³æ³¨çš„çƒ­ç‚¹&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;ä½¿ç”¨è¯´æ˜&lt;/strong&gt;ï¼š&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;é€‚ç”¨åœºæ™¯ï¼šå½“ä½ æŒ‰ç…§æ•™ç¨‹å¼€å¯äº†ç½‘é¡µç‰ˆåŠŸèƒ½å(GitHub Pages)&lt;/li&gt; 
  &lt;li&gt;ä½¿ç”¨æ–¹æ³•ï¼šç”¨æ‰‹æœºæˆ–ç”µè„‘æ‰“å¼€è¯¥ç½‘é¡µé“¾æ¥ï¼Œç‚¹å‡»é¡µé¢é¡¶éƒ¨çš„"ä¿å­˜ä¸ºå›¾ç‰‡"æŒ‰é’®&lt;/li&gt; 
  &lt;li&gt;å®é™…æ•ˆæœï¼šç³»ç»Ÿä¼šè‡ªåŠ¨å°†å½“å‰çš„æ–°é—»æŠ¥å‘Šåˆ¶ä½œæˆä¸€å¼ ç²¾ç¾å›¾ç‰‡ï¼Œä¿å­˜åˆ°ä½ çš„æ‰‹æœºç›¸å†Œæˆ–ç”µè„‘æ¡Œé¢&lt;/li&gt; 
  &lt;li&gt;åˆ†äº«ä¾¿åˆ©ï¼šä½ å¯ä»¥ç›´æ¥æŠŠè¿™å¼ å›¾ç‰‡å‘ç»™æœ‹å‹ã€å‘åˆ°æœ‹å‹åœˆï¼Œæˆ–åˆ†äº«åˆ°å·¥ä½œç¾¤ï¼Œè®©åˆ«äººä¹Ÿèƒ½çœ‹åˆ°ä½ å‘ç°çš„é‡è¦èµ„è®¯&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/09/13 - v2.1.2&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;è§£å†³é’‰é’‰çš„æ¨é€å®¹é‡é™åˆ¶å¯¼è‡´çš„æ–°é—»æ¨é€å¤±è´¥é—®é¢˜(é‡‡ç”¨åˆ†æ‰¹æ¨é€)&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/09/04 - v2.1.1&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ä¿®å¤dockeråœ¨æŸäº›æ¶æ„ä¸­æ— æ³•æ­£å¸¸è¿è¡Œçš„é—®é¢˜&lt;/li&gt; 
  &lt;li&gt;æ­£å¼å‘å¸ƒå®˜æ–¹ Docker é•œåƒ wantcat/trendradarï¼Œæ”¯æŒå¤šæ¶æ„&lt;/li&gt; 
  &lt;li&gt;ä¼˜åŒ– Docker éƒ¨ç½²æµç¨‹ï¼Œæ— éœ€æœ¬åœ°æ„å»ºå³å¯å¿«é€Ÿä½¿ç”¨&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/08/30 - v2.1.0&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;æ ¸å¿ƒæ”¹è¿›&lt;/strong&gt;ï¼š&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;æ¨é€é€»è¾‘ä¼˜åŒ–&lt;/strong&gt;ï¼šä»"æ¯æ¬¡æ‰§è¡Œéƒ½æ¨é€"æ”¹ä¸º"æ—¶é—´çª—å£å†…å¯æ§æ¨é€"&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;æ—¶é—´çª—å£æ§åˆ¶&lt;/strong&gt;ï¼šå¯è®¾å®šæ¨é€æ—¶é—´èŒƒå›´ï¼Œé¿å…éå·¥ä½œæ—¶é—´æ‰“æ‰°&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;æ¨é€é¢‘ç‡å¯é€‰&lt;/strong&gt;ï¼šæ—¶é—´æ®µå†…æ”¯æŒå•æ¬¡æ¨é€æˆ–å¤šæ¬¡æ¨é€&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;æ›´æ–°æç¤º&lt;/strong&gt;ï¼š&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;æœ¬åŠŸèƒ½é»˜è®¤å…³é—­ï¼Œéœ€æ‰‹åŠ¨åœ¨ config.yaml ä¸­å¼€å¯æ¨é€æ—¶é—´çª—å£æ§åˆ¶&lt;/li&gt; 
  &lt;li&gt;å‡çº§éœ€åŒæ—¶æ›´æ–° main.py å’Œ config.yaml ä¸¤ä¸ªæ–‡ä»¶&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/08/27 - v2.0.4&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;æœ¬æ¬¡ç‰ˆæœ¬ä¸æ˜¯åŠŸèƒ½ä¿®å¤ï¼Œè€Œæ˜¯é‡è¦æé†’&lt;/li&gt; 
  &lt;li&gt;è¯·åŠ¡å¿…å¦¥å–„ä¿ç®¡å¥½ webhooksï¼Œä¸è¦å…¬å¼€ï¼Œä¸è¦å…¬å¼€ï¼Œä¸è¦å…¬å¼€&lt;/li&gt; 
  &lt;li&gt;å¦‚æœä½ ä»¥ fork çš„æ–¹å¼å°†æœ¬é¡¹ç›®éƒ¨ç½²åœ¨ GitHub ä¸Šï¼Œè¯·å°† webhooks å¡«å…¥ GitHub Secretï¼Œè€Œé config.yaml&lt;/li&gt; 
  &lt;li&gt;å¦‚æœä½ å·²ç»æš´éœ²äº† webhooks æˆ–å°†å…¶å¡«å…¥äº† config.yamlï¼Œå»ºè®®åˆ é™¤åé‡æ–°ç”Ÿæˆ&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/08/06 - v2.0.3&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ä¼˜åŒ– github page çš„ç½‘é¡µç‰ˆæ•ˆæœï¼Œæ–¹ä¾¿ç§»åŠ¨ç«¯ä½¿ç”¨&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/07/28 - v2.0.2&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;é‡æ„ä»£ç &lt;/li&gt; 
  &lt;li&gt;è§£å†³ç‰ˆæœ¬å·å®¹æ˜“è¢«é—æ¼ä¿®æ”¹çš„é—®é¢˜&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/07/27 - v2.0.1&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;ä¿®å¤é—®é¢˜&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;docker çš„ shell è„šæœ¬çš„æ¢è¡Œç¬¦ä¸º CRLF å¯¼è‡´çš„æ‰§è¡Œå¼‚å¸¸é—®é¢˜&lt;/li&gt; 
  &lt;li&gt;frequency_words.txt ä¸ºç©ºæ—¶ï¼Œå¯¼è‡´æ–°é—»å‘é€ä¹Ÿä¸ºç©ºçš„é€»è¾‘é—®é¢˜&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ä¿®å¤åï¼Œå½“ä½ é€‰æ‹© frequency_words.txt ä¸ºç©ºæ—¶ï¼Œå°†&lt;strong&gt;æ¨é€æ‰€æœ‰æ–°é—»&lt;/strong&gt;ï¼Œä½†å—é™äºæ¶ˆæ¯æ¨é€å¤§å°é™åˆ¶ï¼Œè¯·åšå¦‚ä¸‹è°ƒæ•´ 
   &lt;ul&gt; 
    &lt;li&gt;æ–¹æ¡ˆä¸€ï¼šå…³é—­æ‰‹æœºæ¨é€ï¼Œåªé€‰æ‹© Github Pages å¸ƒç½®(è¿™æ˜¯èƒ½è·å¾—æœ€å®Œæ•´ä¿¡æ¯çš„æ–¹æ¡ˆï¼Œå°†æŠŠæ‰€æœ‰å¹³å°çš„çƒ­ç‚¹æŒ‰ç…§ä½ &lt;strong&gt;è‡ªå®šä¹‰çš„çƒ­æœç®—æ³•&lt;/strong&gt;è¿›è¡Œé‡æ–°æ’åº)&lt;/li&gt; 
    &lt;li&gt;æ–¹æ¡ˆäºŒï¼šå‡å°‘æ¨é€å¹³å°ï¼Œä¼˜å…ˆé€‰æ‹©&lt;strong&gt;ä¼ä¸šå¾®ä¿¡&lt;/strong&gt;æˆ–&lt;strong&gt;Telegram&lt;/strong&gt;ï¼Œè¿™ä¸¤ä¸ªæ¨é€æˆ‘åšäº†åˆ†æ‰¹æ¨é€åŠŸèƒ½(å› ä¸ºåˆ†æ‰¹æ¨é€å½±å“æ¨é€ä½“éªŒï¼Œä¸”åªæœ‰è¿™ä¸¤ä¸ªå¹³å°åªç»™ä¸€ç‚¹ç‚¹æ¨é€å®¹é‡ï¼Œæ‰€ä»¥æ‰ä¸å¾—å·²åšäº†åˆ†æ‰¹æ¨é€åŠŸèƒ½ï¼Œä½†è‡³å°‘èƒ½ä¿è¯è·å¾—çš„ä¿¡æ¯å®Œæ•´)&lt;/li&gt; 
    &lt;li&gt;æ–¹æ¡ˆä¸‰ï¼šå¯ä¸æ–¹æ¡ˆäºŒç»“åˆï¼Œæ¨¡å¼é€‰æ‹© current æˆ– incremental å¯æœ‰æ•ˆå‡å°‘ä¸€æ¬¡æ€§æ¨é€çš„å†…å®¹&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/07/17 - v2.0.0&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;é‡å¤§é‡æ„&lt;/strong&gt;ï¼š&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;é…ç½®ç®¡ç†é‡æ„ï¼šæ‰€æœ‰é…ç½®ç°åœ¨é€šè¿‡ &lt;code&gt;config/config.yaml&lt;/code&gt; æ–‡ä»¶ç®¡ç†ï¼ˆmain.py æˆ‘ä¾æ—§æ²¡æ‹†åˆ†ï¼Œæ–¹ä¾¿ä½ ä»¬å¤åˆ¶å‡çº§ï¼‰&lt;/li&gt; 
  &lt;li&gt;è¿è¡Œæ¨¡å¼å‡çº§ï¼šæ”¯æŒä¸‰ç§æ¨¡å¼ - &lt;code&gt;daily&lt;/code&gt;ï¼ˆå½“æ—¥æ±‡æ€»ï¼‰ã€&lt;code&gt;current&lt;/code&gt;ï¼ˆå½“å‰æ¦œå•ï¼‰ã€&lt;code&gt;incremental&lt;/code&gt;ï¼ˆå¢é‡ç›‘æ§ï¼‰&lt;/li&gt; 
  &lt;li&gt;Docker æ”¯æŒï¼šå®Œæ•´çš„ Docker éƒ¨ç½²æ–¹æ¡ˆï¼Œæ”¯æŒå®¹å™¨åŒ–è¿è¡Œ&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;é…ç½®æ–‡ä»¶è¯´æ˜&lt;/strong&gt;ï¼š&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;config/config.yaml&lt;/code&gt; - ä¸»é…ç½®æ–‡ä»¶ï¼ˆåº”ç”¨è®¾ç½®ã€çˆ¬è™«é…ç½®ã€é€šçŸ¥é…ç½®ã€å¹³å°é…ç½®ç­‰ï¼‰&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;config/frequency_words.txt&lt;/code&gt; - å…³é”®è¯é…ç½®ï¼ˆç›‘æ§è¯æ±‡è®¾ç½®ï¼‰&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/07/09 - v1.4.1&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;åŠŸèƒ½æ–°å¢&lt;/strong&gt;ï¼šå¢åŠ å¢é‡æ¨é€(åœ¨ main.py å¤´éƒ¨é…ç½® FOCUS_NEW_ONLY)ï¼Œè¯¥å¼€å…³åªå…³å¿ƒæ–°è¯é¢˜è€ŒéæŒç»­çƒ­åº¦ï¼Œåªåœ¨æœ‰æ–°å†…å®¹æ—¶æ‰å‘é€šçŸ¥ã€‚&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;ä¿®å¤é—®é¢˜&lt;/strong&gt;: æŸäº›æƒ…å†µä¸‹ï¼Œç”±äºæ–°é—»æœ¬èº«å«æœ‰ç‰¹æ®Šç¬¦å·å¯¼è‡´çš„å¶å‘æ€§æ’ç‰ˆå¼‚å¸¸ã€‚&lt;/p&gt; 
 &lt;h3&gt;2025/06/23 - v1.3.0&lt;/h3&gt; 
 &lt;p&gt;ä¼ä¸šå¾®ä¿¡ å’Œ Telegram çš„æ¨é€æ¶ˆæ¯æœ‰é•¿åº¦é™åˆ¶ï¼Œå¯¹æ­¤æˆ‘é‡‡ç”¨å°†æ¶ˆæ¯æ‹†åˆ†æ¨é€çš„æ–¹å¼ã€‚å¼€å‘æ–‡æ¡£è¯¦è§&lt;a href="https://developer.work.weixin.qq.com/document/path/91770"&gt;ä¼ä¸šå¾®ä¿¡&lt;/a&gt; å’Œ &lt;a href="https://core.telegram.org/bots/api"&gt;Telegram&lt;/a&gt;&lt;/p&gt; 
 &lt;h3&gt;2025/06/21 - v1.2.1&lt;/h3&gt; 
 &lt;p&gt;åœ¨æœ¬ç‰ˆæœ¬ä¹‹å‰çš„æ—§ç‰ˆæœ¬ï¼Œä¸ä»… main.py éœ€è¦å¤åˆ¶æ›¿æ¢ï¼Œ crawler.yml ä¹Ÿéœ€è¦ä½ å¤åˆ¶æ›¿æ¢ &lt;a href="https://github.com/sansan0/TrendRadar/raw/master/.github/workflows/crawler.yml"&gt;https://github.com/sansan0/TrendRadar/blob/master/.github/workflows/crawler.yml&lt;/a&gt;&lt;/p&gt; 
 &lt;h3&gt;2025/06/19 - v1.2.0&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;æ„Ÿè°¢ claude research æ•´ç†çš„å„å¹³å° api ,è®©æˆ‘å¿«é€Ÿå®Œæˆå„å¹³å°é€‚é…ï¼ˆè™½ç„¶ä»£ç æ›´å¤šå†—ä½™äº†~&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ol&gt; 
  &lt;li&gt;æ”¯æŒ telegram ï¼Œä¼ä¸šå¾®ä¿¡ï¼Œé’‰é’‰æ¨é€æ¸ é“, æ”¯æŒå¤šæ¸ é“é…ç½®å’ŒåŒæ—¶æ¨é€&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;2025/06/18 - v1.1.0&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;200 starâ­&lt;/strong&gt; äº†, ç»§ç»­ç»™å¤§ä¼™å„¿åŠ©å…´~è¿‘æœŸï¼Œåœ¨æˆ‘çš„"æ€‚æ¿"ä¸‹ï¼ŒæŒºå¤šäººåœ¨æˆ‘å…¬ä¼—å·ç‚¹èµåˆ†äº«æ¨èåŠ©åŠ›äº†æˆ‘ï¼Œæˆ‘éƒ½åœ¨åå°çœ‹è§äº†å…·ä½“è´¦å·çš„é¼“åŠ±æ•°æ®ï¼Œå¾ˆå¤šéƒ½æˆäº†å¤©ä½¿è½®è€ç²‰ï¼ˆæˆ‘ç©å…¬ä¼—å·æ‰ä¸€ä¸ªå¤šæœˆï¼Œè™½ç„¶æ³¨å†Œæ˜¯ä¸ƒå…«å¹´å‰çš„äº‹äº†å“ˆå“ˆï¼Œå±äºä¸Šè½¦æ—©ï¼Œå‘è½¦æ™šï¼‰ï¼Œä½†å› ä¸ºä½ ä»¬æ²¡æœ‰ç•™è¨€æˆ–ç§ä¿¡æˆ‘ï¼Œæ‰€ä»¥æˆ‘ä¹Ÿæ— æ³•ä¸€ä¸€å›åº”å¹¶æ„Ÿè°¢æ”¯æŒï¼Œåœ¨æ­¤ä¸€å¹¶è°¢è°¢ï¼&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ol&gt; 
  &lt;li&gt;é‡è¦çš„æ›´æ–°ï¼ŒåŠ äº†æƒé‡ï¼Œä½ ç°åœ¨çœ‹åˆ°çš„æ–°é—»éƒ½æ˜¯æœ€çƒ­ç‚¹æœ€æœ‰å…³æ³¨åº¦çš„å‡ºç°åœ¨æœ€ä¸Šé¢&lt;/li&gt; 
  &lt;li&gt;æ›´æ–°æ–‡æ¡£ä½¿ç”¨ï¼Œå› ä¸ºè¿‘æœŸæ›´æ–°äº†å¾ˆå¤šåŠŸèƒ½ï¼Œè€Œä¸”ä¹‹å‰çš„ä½¿ç”¨æ–‡æ¡£æˆ‘å·æ‡’å†™çš„ç®€å•ï¼ˆè§ä¸‹é¢çš„ âš™ï¸ frequency_words.txt é…ç½®å®Œæ•´æ•™ç¨‹ï¼‰&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;2025/06/16 - v1.0.0&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;å¢åŠ äº†ä¸€ä¸ªé¡¹ç›®æ–°ç‰ˆæœ¬æ›´æ–°æç¤ºï¼Œé»˜è®¤æ‰“å¼€ï¼Œå¦‚è¦å…³æ‰ï¼Œå¯ä»¥åœ¨ main.py ä¸­æŠŠ "FEISHU_SHOW_VERSION_UPDATE": True ä¸­çš„ True æ”¹æˆ False å³å¯&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;2025/06/13+14&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;å»æ‰äº†å…¼å®¹ä»£ç ï¼Œä¹‹å‰ fork çš„åŒå­¦ï¼Œç›´æ¥å¤åˆ¶ä»£ç ä¼šåœ¨å½“å¤©æ˜¾ç¤ºå¼‚å¸¸ï¼ˆç¬¬äºŒå¤©ä¼šæ¢å¤æ­£å¸¸ï¼‰&lt;/li&gt; 
  &lt;li&gt;feishu å’Œ html åº•éƒ¨å¢åŠ ä¸€ä¸ªæ–°å¢æ–°é—»æ˜¾ç¤º&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;2025/06/09&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;100 starâ­&lt;/strong&gt; äº†ï¼Œå†™ä¸ªå°åŠŸèƒ½ç»™å¤§ä¼™å„¿åŠ©åŠ©å…´ frequency_words.txt æ–‡ä»¶å¢åŠ äº†ä¸€ä¸ªã€å¿…é¡»è¯ã€‘åŠŸèƒ½ï¼Œä½¿ç”¨ + å·&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;å¿…é¡»è¯è¯­æ³•å¦‚ä¸‹ï¼š&lt;br /&gt; å”åƒ§æˆ–è€…çŒªå…«æˆ’å¿…é¡»åœ¨æ ‡é¢˜é‡ŒåŒæ—¶å‡ºç°ï¼Œæ‰ä¼šæ”¶å½•åˆ°æ¨é€æ–°é—»ä¸­&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code&gt;+å”åƒ§
+çŒªå…«æˆ’
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;è¿‡æ»¤è¯çš„ä¼˜å…ˆçº§æ›´é«˜ï¼š&lt;br /&gt; å¦‚æœæ ‡é¢˜ä¸­è¿‡æ»¤è¯åŒ¹é…åˆ°å”åƒ§å¿µç»ï¼Œé‚£ä¹ˆå³ä½¿å¿…é¡»è¯é‡Œæœ‰å”åƒ§ï¼Œä¹Ÿä¸æ˜¾ç¤º&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code&gt;+å”åƒ§
!å”åƒ§å¿µç»
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;2025/06/02&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;ç½‘é¡µ&lt;/strong&gt;å’Œ&lt;strong&gt;é£ä¹¦æ¶ˆæ¯&lt;/strong&gt;æ”¯æŒæ‰‹æœºç›´æ¥è·³è½¬è¯¦æƒ…æ–°é—»&lt;/li&gt; 
  &lt;li&gt;ä¼˜åŒ–æ˜¾ç¤ºæ•ˆæœ + 1&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;2025/05/26&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;é£ä¹¦æ¶ˆæ¯æ˜¾ç¤ºæ•ˆæœä¼˜åŒ–&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center"&gt; ä¼˜åŒ–å‰&lt;br /&gt; &lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/before.jpg" alt="é£ä¹¦æ¶ˆæ¯ç•Œé¢ - ä¼˜åŒ–å‰" width="400" /&gt; &lt;/td&gt; 
    &lt;td align="center"&gt; ä¼˜åŒ–å&lt;br /&gt; &lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/after.jpg" alt="é£ä¹¦æ¶ˆæ¯ç•Œé¢ - ä¼˜åŒ–å" width="400" /&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸš€ å¿«é€Ÿå¼€å§‹&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;é…ç½®å®Œæˆåï¼Œæ–°é—»æ•°æ®ä¸€å°æ—¶åæ‰ä¼šæ›´æ–°ï¼Œå¦‚æƒ³åŠ å¿«ï¼Œå¯å‚ç…§ã€ç¬¬4æ­¥ã€‘æ‰‹åŠ¨æµ‹è¯•é…ç½®æ•ˆæœ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Fork æœ¬é¡¹ç›®&lt;/strong&gt;åˆ°ä½ çš„ GitHub è´¦æˆ·&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ç‚¹å‡»æœ¬é¡µé¢å³ä¸Šè§’çš„"Fork"æŒ‰é’®&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;è®¾ç½® GitHub Secretsï¼ˆé€‰æ‹©ä½ éœ€è¦çš„å¹³å°ï¼‰&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;åœ¨ä½  Fork åçš„ä»“åº“ä¸­ï¼Œè¿›å…¥ &lt;code&gt;Settings&lt;/code&gt; &amp;gt; &lt;code&gt;Secrets and variables&lt;/code&gt; &amp;gt; &lt;code&gt;Actions&lt;/code&gt; &amp;gt; &lt;code&gt;New repository secret&lt;/code&gt;ï¼Œç„¶åæ ¹æ®éœ€è¦é…ç½®ä»¥ä¸‹ä»»ä¸€æˆ–å¤šä¸ªé€šçŸ¥å¹³å°ï¼š&lt;/p&gt; &lt;p&gt;å¯ä»¥åŒæ—¶é…ç½®å¤šä¸ªå¹³å°ï¼Œç³»ç»Ÿä¼šå‘æ‰€æœ‰é…ç½®çš„å¹³å°å‘é€é€šçŸ¥ã€‚&lt;/p&gt; &lt;p&gt;æ•ˆæœç±»ä¼¼ä¸‹å›¾ï¼Œä¸€ä¸ª name å¯¹åº”ä¸€ä¸ª secretï¼Œä¿å­˜å®Œå°±è¡Œï¼Œä½ é‡æ–°ç¼–è¾‘çœ‹ä¸åˆ° secret æ˜¯æ­£å¸¸æƒ…å†µã€‚&lt;/p&gt; &lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/secrets.png" alt="GitHub Secrets" /&gt; 
  &lt;details&gt; 
   &lt;summary&gt; &lt;strong&gt;ğŸ‘‰ ä¼ä¸šå¾®ä¿¡æœºå™¨äºº&lt;/strong&gt;ï¼ˆé…ç½®æœ€ç®€å•æœ€è¿…é€Ÿï¼‰&lt;/summary&gt; 
   &lt;br /&gt; 
   &lt;p&gt;&lt;strong&gt;GitHub Secret é…ç½®ï¼š&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;åç§°ï¼š&lt;code&gt;WEWORK_WEBHOOK_URL&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;å€¼ï¼šä½ çš„ä¼ä¸šå¾®ä¿¡æœºå™¨äºº Webhook åœ°å€&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;æœºå™¨äººè®¾ç½®æ­¥éª¤ï¼š&lt;/strong&gt;&lt;/p&gt; 
   &lt;h4&gt;æ‰‹æœºç«¯è®¾ç½®ï¼š&lt;/h4&gt; 
   &lt;ol&gt; 
    &lt;li&gt;æ‰“å¼€ä¼ä¸šå¾®ä¿¡ App â†’ è¿›å…¥ç›®æ ‡å†…éƒ¨ç¾¤èŠ&lt;/li&gt; 
    &lt;li&gt;ç‚¹å‡»å³ä¸Šè§’"â€¦"æŒ‰é’® â†’ é€‰æ‹©"æ¶ˆæ¯æ¨é€"&lt;/li&gt; 
    &lt;li&gt;ç‚¹å‡»"æ·»åŠ " â†’ åç§°è¾“å…¥"TrendRadar"&lt;/li&gt; 
    &lt;li&gt;å¤åˆ¶ Webhook åœ°å€ï¼Œç‚¹å‡»ä¿å­˜ï¼Œå¤åˆ¶çš„å†…å®¹é…ç½®åˆ°ä¸Šæ–¹çš„ GitHub Secret ä¸­&lt;/li&gt; 
   &lt;/ol&gt; 
   &lt;h4&gt;PC ç«¯è®¾ç½®æµç¨‹ç±»ä¼¼&lt;/h4&gt; 
  &lt;/details&gt; 
  &lt;details&gt; 
   &lt;summary&gt; &lt;strong&gt;ğŸ‘‰ é£ä¹¦æœºå™¨äºº&lt;/strong&gt;ï¼ˆæ¶ˆæ¯æ˜¾ç¤ºæœ€å‹å¥½ï¼‰&lt;/summary&gt; 
   &lt;br /&gt; 
   &lt;p&gt;&lt;strong&gt;GitHub Secret é…ç½®ï¼š&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;åç§°ï¼š&lt;code&gt;FEISHU_WEBHOOK_URL&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;å€¼ï¼šä½ çš„é£ä¹¦æœºå™¨äºº Webhook åœ°å€(è¯¥é“¾æ¥å¼€å¤´ç±»ä¼¼ &lt;a href="https://www.feishu.cn/flow/api/trigger-webhook/"&gt;https://www.feishu.cn/flow/api/trigger-webhook/&lt;/a&gt;********)&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;br /&gt; 
   &lt;p&gt;æœ‰ä¸¤ä¸ªæ–¹æ¡ˆï¼Œ&lt;strong&gt;æ–¹æ¡ˆä¸€&lt;/strong&gt;é…ç½®ç®€å•ï¼Œ&lt;strong&gt;æ–¹æ¡ˆäºŒ&lt;/strong&gt;é…ç½®å¤æ‚(ä½†æ˜¯ç¨³å®šæ¨é€)&lt;/p&gt; 
   &lt;p&gt;å…¶ä¸­æ–¹æ¡ˆä¸€ï¼Œç”± &lt;strong&gt;ziventian&lt;/strong&gt;å‘ç°å¹¶æä¾›å»ºè®®ï¼Œåœ¨è¿™é‡Œæ„Ÿè°¢ä»–ï¼Œé»˜è®¤æ˜¯ä¸ªäººæ¨é€ï¼Œä¹Ÿå¯ä»¥é…ç½®ç¾¤ç»„æ¨é€æ“ä½œ&lt;a href="https://github.com/sansan0/TrendRadar/issues/97"&gt;#97&lt;/a&gt; ï¼Œ&lt;/p&gt; 
   &lt;p&gt;&lt;strong&gt;æ–¹æ¡ˆä¸€ï¼š&lt;/strong&gt;&lt;/p&gt; 
   &lt;blockquote&gt; 
    &lt;p&gt;å¯¹éƒ¨åˆ†äººå­˜åœ¨é¢å¤–æ“ä½œï¼Œå¦åˆ™ä¼šæŠ¥"ç³»ç»Ÿé”™è¯¯"ã€‚éœ€è¦æ‰‹æœºç«¯æœç´¢ä¸‹æœºå™¨äººï¼Œç„¶åå¼€å¯é£ä¹¦æœºå™¨äººåº”ç”¨(è¯¥å»ºè®®æ¥è‡ªäºç½‘å‹ï¼Œå¯å‚è€ƒ)&lt;/p&gt; 
   &lt;/blockquote&gt; 
   &lt;ol&gt; 
    &lt;li&gt; &lt;p&gt;ç”µè„‘æµè§ˆå™¨æ‰“å¼€ &lt;a href="https://botbuilder.feishu.cn/home/my-command"&gt;https://botbuilder.feishu.cn/home/my-command&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;ç‚¹å‡»"æ–°å»ºæœºå™¨äººæŒ‡ä»¤"&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;ç‚¹å‡»"é€‰æ‹©è§¦å‘å™¨"ï¼Œå¾€ä¸‹æ»‘åŠ¨ï¼Œç‚¹å‡»"Webhook è§¦å‘"&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;æ­¤æ—¶ä½ ä¼šçœ‹åˆ°"Webhook åœ°å€"ï¼ŒæŠŠè¿™ä¸ªé“¾æ¥å…ˆå¤åˆ¶åˆ°æœ¬åœ°è®°äº‹æœ¬æš‚å­˜ï¼Œç»§ç»­æ¥ä¸‹æ¥çš„æ“ä½œ&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;"å‚æ•°"é‡Œé¢æ”¾ä¸Šä¸‹é¢çš„å†…å®¹ï¼Œç„¶åç‚¹å‡»"å®Œæˆ"&lt;/p&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
   &lt;pre&gt;&lt;code class="language-json"&gt;{
  "message_type": "text",
  "content": {
    "total_titles": "{{å†…å®¹}}",
    "timestamp": "{{å†…å®¹}}",
    "report_type": "{{å†…å®¹}}",
    "text": "{{å†…å®¹}}"
  }
}
&lt;/code&gt;&lt;/pre&gt; 
   &lt;ol start="6"&gt; 
    &lt;li&gt; &lt;p&gt;ç‚¹å‡»"é€‰æ‹©æ“ä½œ" &amp;gt; "é€šè¿‡å®˜æ–¹æœºå™¨äººå‘æ¶ˆæ¯"&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;æ¶ˆæ¯æ ‡é¢˜å¡«å†™"TrendRadar çƒ­ç‚¹ç›‘æ§"&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;æœ€å…³é”®çš„éƒ¨åˆ†æ¥äº†ï¼Œç‚¹å‡» + æŒ‰é’®ï¼Œé€‰æ‹©"Webhook è§¦å‘"ï¼Œç„¶åæŒ‰ç…§ä¸‹é¢çš„å›¾ç‰‡æ‘†æ”¾&lt;/p&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
   &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/image.png" alt="é£ä¹¦æœºå™¨äººé…ç½®ç¤ºä¾‹" /&gt;&lt;/p&gt; 
   &lt;ol start="9"&gt; 
    &lt;li&gt;é…ç½®å®Œæˆåï¼Œå°†ç¬¬ 4 æ­¥å¤åˆ¶çš„ Webhook åœ°å€é…ç½®åˆ° GitHub Secrets ä¸­çš„ &lt;code&gt;FEISHU_WEBHOOK_URL&lt;/code&gt;&lt;/li&gt; 
   &lt;/ol&gt; 
   &lt;br /&gt; 
   &lt;p&gt;&lt;strong&gt;æ–¹æ¡ˆäºŒï¼š&lt;/strong&gt;&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt; &lt;p&gt;ç”µè„‘æµè§ˆå™¨æ‰“å¼€ &lt;a href="https://botbuilder.feishu.cn/home/my-app"&gt;https://botbuilder.feishu.cn/home/my-app&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;ç‚¹å‡»"æ–°å»ºæœºå™¨äººåº”ç”¨"&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;è¿›å…¥åˆ›å»ºçš„åº”ç”¨åï¼Œç‚¹å‡»"æµç¨‹æ¶‰åŠ" &amp;gt; "åˆ›å»ºæµç¨‹" &amp;gt; "é€‰æ‹©è§¦å‘å™¨"&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;å¾€ä¸‹æ»‘åŠ¨ï¼Œç‚¹å‡»"Webhook è§¦å‘"&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;æ­¤æ—¶ä½ ä¼šçœ‹åˆ°"Webhook åœ°å€"ï¼ŒæŠŠè¿™ä¸ªé“¾æ¥å…ˆå¤åˆ¶åˆ°æœ¬åœ°è®°äº‹æœ¬æš‚å­˜ï¼Œç»§ç»­æ¥ä¸‹æ¥çš„æ“ä½œ&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;"å‚æ•°"é‡Œé¢æ”¾ä¸Šä¸‹é¢çš„å†…å®¹ï¼Œç„¶åç‚¹å‡»"å®Œæˆ"&lt;/p&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
   &lt;pre&gt;&lt;code class="language-json"&gt;{
  "message_type": "text",
  "content": {
    "total_titles": "{{å†…å®¹}}",
    "timestamp": "{{å†…å®¹}}",
    "report_type": "{{å†…å®¹}}",
    "text": "{{å†…å®¹}}"
  }
}
&lt;/code&gt;&lt;/pre&gt; 
   &lt;ol start="7"&gt; 
    &lt;li&gt; &lt;p&gt;ç‚¹å‡»"é€‰æ‹©æ“ä½œ" &amp;gt; "å‘é€é£ä¹¦æ¶ˆæ¯"ï¼Œå‹¾é€‰ "ç¾¤æ¶ˆæ¯"ï¼Œç„¶åç‚¹å‡»ä¸‹é¢çš„è¾“å…¥æ¡†ï¼Œç‚¹å‡»"æˆ‘ç®¡ç†çš„ç¾¤ç»„"ï¼ˆå¦‚æœæ²¡æœ‰ç¾¤ç»„ï¼Œä½ å¯ä»¥åœ¨é£ä¹¦ app ä¸Šåˆ›å»ºç¾¤ç»„ï¼‰&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;æ¶ˆæ¯æ ‡é¢˜å¡«å†™"TrendRadar çƒ­ç‚¹ç›‘æ§"&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;æœ€å…³é”®çš„éƒ¨åˆ†æ¥äº†ï¼Œç‚¹å‡» + æŒ‰é’®ï¼Œé€‰æ‹©"Webhook è§¦å‘"ï¼Œç„¶åæŒ‰ç…§ä¸‹é¢çš„å›¾ç‰‡æ‘†æ”¾&lt;/p&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
   &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/image.png" alt="é£ä¹¦æœºå™¨äººé…ç½®ç¤ºä¾‹" /&gt;&lt;/p&gt; 
   &lt;ol start="10"&gt; 
    &lt;li&gt;é…ç½®å®Œæˆåï¼Œå°†ç¬¬ 5 æ­¥å¤åˆ¶çš„ Webhook åœ°å€é…ç½®åˆ° GitHub Secrets ä¸­çš„ &lt;code&gt;FEISHU_WEBHOOK_URL&lt;/code&gt;&lt;/li&gt; 
   &lt;/ol&gt; 
  &lt;/details&gt; 
  &lt;details&gt; 
   &lt;summary&gt; &lt;strong&gt;ğŸ‘‰ é’‰é’‰æœºå™¨äºº&lt;/strong&gt;&lt;/summary&gt; 
   &lt;br /&gt; 
   &lt;p&gt;&lt;strong&gt;GitHub Secret é…ç½®ï¼š&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;åç§°ï¼š&lt;code&gt;DINGTALK_WEBHOOK_URL&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;å€¼ï¼šä½ çš„é’‰é’‰æœºå™¨äºº Webhook åœ°å€&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;æœºå™¨äººè®¾ç½®æ­¥éª¤ï¼š&lt;/strong&gt;&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;åˆ›å»ºæœºå™¨äººï¼ˆä»… PC ç«¯æ”¯æŒï¼‰&lt;/strong&gt;ï¼š&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;æ‰“å¼€é’‰é’‰ PC å®¢æˆ·ç«¯ï¼Œè¿›å…¥ç›®æ ‡ç¾¤èŠ&lt;/li&gt; 
      &lt;li&gt;ç‚¹å‡»ç¾¤è®¾ç½®å›¾æ ‡ï¼ˆâš™ï¸ï¼‰â†’ å¾€ä¸‹ç¿»æ‰¾åˆ°"æœºå™¨äºº"ç‚¹å¼€&lt;/li&gt; 
      &lt;li&gt;é€‰æ‹©"æ·»åŠ æœºå™¨äºº" â†’ "è‡ªå®šä¹‰"&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;é…ç½®æœºå™¨äºº&lt;/strong&gt;ï¼š&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;è®¾ç½®æœºå™¨äººåç§°&lt;/li&gt; 
      &lt;li&gt;&lt;strong&gt;å®‰å…¨è®¾ç½®&lt;/strong&gt;ï¼š 
       &lt;ul&gt; 
        &lt;li&gt;&lt;strong&gt;è‡ªå®šä¹‰å…³é”®è¯&lt;/strong&gt;ï¼šè®¾ç½® "çƒ­ç‚¹"&lt;/li&gt; 
       &lt;/ul&gt; &lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;å®Œæˆè®¾ç½®&lt;/strong&gt;ï¼š&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;å‹¾é€‰æœåŠ¡æ¡æ¬¾åè®® â†’ ç‚¹å‡»"å®Œæˆ"&lt;/li&gt; 
      &lt;li&gt;å¤åˆ¶è·å¾—çš„ Webhook URL&lt;/li&gt; 
      &lt;li&gt;å°† URL é…ç½®åˆ° GitHub Secrets ä¸­çš„ &lt;code&gt;DINGTALK_WEBHOOK_URL&lt;/code&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
   &lt;p&gt;&lt;strong&gt;æ³¨æ„&lt;/strong&gt;ï¼šç§»åŠ¨ç«¯åªèƒ½æ¥æ”¶æ¶ˆæ¯ï¼Œæ— æ³•åˆ›å»ºæ–°æœºå™¨äººã€‚&lt;/p&gt; 
  &lt;/details&gt; 
  &lt;details&gt; 
   &lt;summary&gt; &lt;strong&gt;ğŸ‘‰ Telegram Bot&lt;/strong&gt;&lt;/summary&gt; 
   &lt;br /&gt; 
   &lt;p&gt;&lt;strong&gt;GitHub Secret é…ç½®ï¼š&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;åç§°ï¼š&lt;code&gt;TELEGRAM_BOT_TOKEN&lt;/code&gt; - ä½ çš„ Telegram Bot Token&lt;/li&gt; 
    &lt;li&gt;åç§°ï¼š&lt;code&gt;TELEGRAM_CHAT_ID&lt;/code&gt; - ä½ çš„ Telegram Chat ID&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;æœºå™¨äººè®¾ç½®æ­¥éª¤ï¼š&lt;/strong&gt;&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;åˆ›å»ºæœºå™¨äºº&lt;/strong&gt;ï¼š&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;åœ¨ Telegram ä¸­æœç´¢ &lt;code&gt;@BotFather&lt;/code&gt;ï¼ˆå¤§å°å†™æ³¨æ„ï¼Œæœ‰è“è‰²å¾½ç« å‹¾å‹¾ï¼Œæœ‰ç±»ä¼¼ 37849827 monthly usersï¼Œè¿™ä¸ªæ‰æ˜¯å®˜æ–¹çš„ï¼Œæœ‰ä¸€äº›ä»¿å®˜æ–¹çš„è´¦å·æ³¨æ„è¾¨åˆ«ï¼‰&lt;/li&gt; 
      &lt;li&gt;å‘é€ &lt;code&gt;/newbot&lt;/code&gt; å‘½ä»¤åˆ›å»ºæ–°æœºå™¨äºº&lt;/li&gt; 
      &lt;li&gt;è®¾ç½®æœºå™¨äººåç§°ï¼ˆå¿…é¡»ä»¥"bot"ç»“å°¾ï¼Œå¾ˆå®¹æ˜“é‡åˆ°é‡å¤åå­—ï¼Œæ‰€ä»¥ä½ è¦ç»å°½è„‘æ±æƒ³ä¸åŒçš„åå­—ï¼‰&lt;/li&gt; 
      &lt;li&gt;è·å– Bot Tokenï¼ˆæ ¼å¼å¦‚ï¼š&lt;code&gt;123456789:AAHfiqksKZ8WmR2zSjiQ7_v4TMAKdiHm9T0&lt;/code&gt;ï¼‰&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;è·å– Chat ID&lt;/strong&gt;ï¼š&lt;/p&gt; &lt;p&gt;&lt;strong&gt;æ–¹æ³•ä¸€ï¼šé€šè¿‡å®˜æ–¹ API è·å–&lt;/strong&gt;&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;å…ˆå‘ä½ çš„æœºå™¨äººå‘é€ä¸€æ¡æ¶ˆæ¯&lt;/li&gt; 
      &lt;li&gt;è®¿é—®ï¼š&lt;code&gt;https://api.telegram.org/bot&amp;lt;ä½ çš„Bot Token&amp;gt;/getUpdates&lt;/code&gt;&lt;/li&gt; 
      &lt;li&gt;åœ¨è¿”å›çš„ JSON ä¸­æ‰¾åˆ° &lt;code&gt;"chat":{"id":æ•°å­—}&lt;/code&gt; ä¸­çš„æ•°å­—&lt;/li&gt; 
     &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;æ–¹æ³•äºŒï¼šä½¿ç”¨ç¬¬ä¸‰æ–¹å·¥å…·&lt;/strong&gt;&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;æœç´¢ &lt;code&gt;@userinfobot&lt;/code&gt; å¹¶å‘é€ &lt;code&gt;/start&lt;/code&gt;&lt;/li&gt; 
      &lt;li&gt;è·å–ä½ çš„ç”¨æˆ· ID ä½œä¸º Chat ID&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;é…ç½®åˆ° GitHub&lt;/strong&gt;ï¼š&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;code&gt;TELEGRAM_BOT_TOKEN&lt;/code&gt;ï¼šå¡«å…¥ç¬¬ 1 æ­¥è·å¾—çš„ Bot Token&lt;/li&gt; 
      &lt;li&gt;&lt;code&gt;TELEGRAM_CHAT_ID&lt;/code&gt;ï¼šå¡«å…¥ç¬¬ 2 æ­¥è·å¾—çš„ Chat ID&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
  &lt;/details&gt; 
  &lt;details&gt; 
   &lt;summary&gt; &lt;strong&gt;ğŸ‘‰ é‚®ä»¶æ¨é€&lt;/strong&gt;ï¼ˆæ”¯æŒæ‰€æœ‰ä¸»æµé‚®ç®±ï¼‰&lt;/summary&gt; 
   &lt;br /&gt; 
   &lt;ul&gt; 
    &lt;li&gt;æ³¨æ„äº‹é¡¹ï¼šä¸ºé˜²æ­¢é‚®ä»¶ç¾¤å‘åŠŸèƒ½è¢«&lt;strong&gt;æ»¥ç”¨&lt;/strong&gt;ï¼Œå½“å‰çš„ç¾¤å‘æ˜¯æ‰€æœ‰æ”¶ä»¶äººéƒ½èƒ½çœ‹åˆ°å½¼æ­¤çš„é‚®ç®±åœ°å€ï¼Œé€‚åˆç†Ÿäººé—´äº¤æµèµ„è®¯ã€‚&lt;/li&gt; 
    &lt;li&gt;ä»…ä¾›å‚è€ƒï¼šè¯·æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼Œé‚®ç®±æ–¹é¢å¹¶æ²¡æœ‰ä¸€ä¸€éªŒè¯ï¼Œæ˜¯æŒ‰ç…§ SMTP çš„æ ‡å‡†é…ç½®çš„&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;GitHub Secret é…ç½®ï¼š&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;åç§°ï¼š&lt;code&gt;EMAIL_FROM&lt;/code&gt; - å‘ä»¶äººé‚®ç®±åœ°å€&lt;/li&gt; 
    &lt;li&gt;åç§°ï¼š&lt;code&gt;EMAIL_PASSWORD&lt;/code&gt; - é‚®ç®±å¯†ç æˆ–æˆæƒç &lt;/li&gt; 
    &lt;li&gt;åç§°ï¼š&lt;code&gt;EMAIL_TO&lt;/code&gt; - æ”¶ä»¶äººé‚®ç®±åœ°å€ï¼ˆå¤šä¸ªæ”¶ä»¶äººç”¨è‹±æ–‡é€—å·åˆ†éš”ï¼‰ä¹Ÿå¯ä»¥å’Œ EMAIL_FROM ä¸€æ ·ï¼Œè‡ªå·±å‘é€ç»™è‡ªå·±&lt;/li&gt; 
    &lt;li&gt;åç§°ï¼š&lt;code&gt;EMAIL_SMTP_SERVER&lt;/code&gt; - SMTPæœåŠ¡å™¨åœ°å€ï¼ˆå¯é€‰ï¼Œç•™ç©ºåˆ™è‡ªåŠ¨è¯†åˆ«ï¼‰&lt;/li&gt; 
    &lt;li&gt;åç§°ï¼š&lt;code&gt;EMAIL_SMTP_PORT&lt;/code&gt; - SMTPç«¯å£ï¼ˆå¯é€‰ï¼Œç•™ç©ºåˆ™è‡ªåŠ¨è¯†åˆ«ï¼‰&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;å¸¸è§é‚®ç®±è®¾ç½®ï¼š&lt;/strong&gt;&lt;/p&gt; 
   &lt;h4&gt;QQé‚®ç®±ï¼š&lt;/h4&gt; 
   &lt;ol&gt; 
    &lt;li&gt;ç™»å½• QQé‚®ç®±ç½‘é¡µç‰ˆ â†’ è®¾ç½® â†’ è´¦æˆ·&lt;/li&gt; 
    &lt;li&gt;å¼€å¯ POP3/SMTP æœåŠ¡&lt;/li&gt; 
    &lt;li&gt;ç”Ÿæˆæˆæƒç ï¼ˆ16ä½å­—æ¯ï¼‰&lt;/li&gt; 
    &lt;li&gt;&lt;code&gt;EMAIL_PASSWORD&lt;/code&gt; å¡«å†™æˆæƒç ï¼Œè€Œé QQ å¯†ç &lt;/li&gt; 
   &lt;/ol&gt; 
   &lt;h4&gt;Gmailï¼š&lt;/h4&gt; 
   &lt;ol&gt; 
    &lt;li&gt;å¼€å¯ä¸¤æ­¥éªŒè¯&lt;/li&gt; 
    &lt;li&gt;ç”Ÿæˆåº”ç”¨ä¸“ç”¨å¯†ç &lt;/li&gt; 
    &lt;li&gt;&lt;code&gt;EMAIL_PASSWORD&lt;/code&gt; å¡«å†™åº”ç”¨ä¸“ç”¨å¯†ç &lt;/li&gt; 
   &lt;/ol&gt; 
   &lt;h4&gt;163/126é‚®ç®±ï¼š&lt;/h4&gt; 
   &lt;ol&gt; 
    &lt;li&gt;ç™»å½•ç½‘é¡µç‰ˆ â†’ è®¾ç½® â†’ POP3/SMTP/IMAP&lt;/li&gt; 
    &lt;li&gt;å¼€å¯ SMTP æœåŠ¡&lt;/li&gt; 
    &lt;li&gt;è®¾ç½®å®¢æˆ·ç«¯æˆæƒç &lt;/li&gt; 
    &lt;li&gt;&lt;code&gt;EMAIL_PASSWORD&lt;/code&gt; å¡«å†™æˆæƒç &lt;/li&gt; 
   &lt;/ol&gt; 
   &lt;br /&gt; 
   &lt;p&gt;&lt;strong&gt;é«˜çº§é…ç½®&lt;/strong&gt;ï¼š å¦‚æœè‡ªåŠ¨è¯†åˆ«å¤±è´¥ï¼Œå¯æ‰‹åŠ¨é…ç½® SMTPï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;code&gt;EMAIL_SMTP_SERVER&lt;/code&gt;ï¼šå¦‚ smtp.gmail.com&lt;/li&gt; 
    &lt;li&gt;&lt;code&gt;EMAIL_SMTP_PORT&lt;/code&gt;ï¼šå¦‚ 587ï¼ˆTLSï¼‰æˆ– 465ï¼ˆSSLï¼‰&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;br /&gt; 
   &lt;p&gt;&lt;strong&gt;å¤šæ”¶ä»¶äººè®¾ç½®&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;EMAIL_TO="&lt;a href="mailto:user1@example.com"&gt;user1@example.com&lt;/a&gt;,&lt;a href="mailto:user2@example.com"&gt;user2@example.com&lt;/a&gt;,&lt;a href="mailto:user3@example.com"&gt;user3@example.com&lt;/a&gt;"&lt;/li&gt; 
   &lt;/ul&gt; 
  &lt;/details&gt; 
  &lt;details&gt; 
   &lt;summary&gt; &lt;strong&gt;ğŸ‘‰ ntfy æ¨é€&lt;/strong&gt;ï¼ˆå¼€æºå…è´¹ï¼Œæ”¯æŒè‡ªæ‰˜ç®¡ï¼‰&lt;/summary&gt; 
   &lt;br /&gt; 
   &lt;p&gt;&lt;strong&gt;ä¸¤ç§ä½¿ç”¨æ–¹å¼ï¼š&lt;/strong&gt;&lt;/p&gt; 
   &lt;h3&gt;æ–¹å¼ä¸€ï¼šå…è´¹ä½¿ç”¨ï¼ˆæ¨èæ–°æ‰‹ï¼‰ ğŸ†“&lt;/h3&gt; 
   &lt;p&gt;&lt;strong&gt;ç‰¹ç‚¹&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;âœ… æ— éœ€æ³¨å†Œè´¦å·ï¼Œç«‹å³ä½¿ç”¨&lt;/li&gt; 
    &lt;li&gt;âœ… æ¯å¤© 250 æ¡æ¶ˆæ¯ï¼ˆè¶³å¤Ÿ 90% ç”¨æˆ·ï¼‰&lt;/li&gt; 
    &lt;li&gt;âœ… Topic åç§°å³"å¯†ç "ï¼ˆéœ€é€‰æ‹©ä¸æ˜“çŒœæµ‹çš„åç§°ï¼‰&lt;/li&gt; 
    &lt;li&gt;âš ï¸ æ¶ˆæ¯æœªåŠ å¯†ï¼Œä¸é€‚åˆæ•æ„Ÿä¿¡æ¯, ä½†é€‚åˆæˆ‘ä»¬è¿™ä¸ªé¡¹ç›®çš„ä¸æ•æ„Ÿä¿¡æ¯&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;å¿«é€Ÿå¼€å§‹ï¼š&lt;/strong&gt;&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;ä¸‹è½½ ntfy åº”ç”¨&lt;/strong&gt;ï¼š&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;Androidï¼š&lt;a href="https://play.google.com/store/apps/details?id=io.heckel.ntfy"&gt;Google Play&lt;/a&gt; / &lt;a href="https://f-droid.org/en/packages/io.heckel.ntfy/"&gt;F-Droid&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;iOSï¼š&lt;a href="https://apps.apple.com/us/app/ntfy/id1625396347"&gt;App Store&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;æ¡Œé¢ï¼šè®¿é—® &lt;a href="https://ntfy.sh"&gt;ntfy.sh&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;è®¢é˜…ä¸»é¢˜&lt;/strong&gt;ï¼ˆé€‰æ‹©ä¸€ä¸ªéš¾çŒœçš„åç§°ï¼‰ï¼š&lt;/p&gt; &lt;pre&gt;&lt;code&gt;å»ºè®®æ ¼å¼ï¼štrendradar-{ä½ çš„åå­—ç¼©å†™}-{éšæœºæ•°å­—}

ä¸èƒ½ä½¿ç”¨ä¸­æ–‡

âœ… å¥½ä¾‹å­ï¼štrendradar-zs-8492
âŒ åä¾‹å­ï¼šnewsã€alertsï¼ˆå¤ªå®¹æ˜“è¢«çŒœåˆ°ï¼‰
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;é…ç½® GitHub Secret&lt;/strong&gt;ï¼š&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;code&gt;NTFY_TOPIC&lt;/code&gt;ï¼šå¡«å†™ä½ åˆšæ‰è®¢é˜…çš„ä¸»é¢˜åç§°&lt;/li&gt; 
      &lt;li&gt;&lt;code&gt;NTFY_SERVER_URL&lt;/code&gt;ï¼šç•™ç©ºï¼ˆé»˜è®¤ä½¿ç”¨ ntfy.shï¼‰&lt;/li&gt; 
      &lt;li&gt;&lt;code&gt;NTFY_TOKEN&lt;/code&gt;ï¼šç•™ç©º&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;æµ‹è¯•&lt;/strong&gt;ï¼š&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;curl -d "æµ‹è¯•æ¶ˆæ¯" ntfy.sh/ä½ çš„ä¸»é¢˜åç§°
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
   &lt;hr /&gt; 
   &lt;h3&gt;æ–¹å¼äºŒï¼šè‡ªæ‰˜ç®¡ï¼ˆå®Œå…¨éšç§æ§åˆ¶ï¼‰ ğŸ”’&lt;/h3&gt; 
   &lt;p&gt;&lt;strong&gt;é€‚åˆäººç¾¤&lt;/strong&gt;ï¼šæœ‰æœåŠ¡å™¨ã€è¿½æ±‚å®Œå…¨éšç§ã€æŠ€æœ¯èƒ½åŠ›å¼º&lt;/p&gt; 
   &lt;p&gt;&lt;strong&gt;ä¼˜åŠ¿&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;âœ… å®Œå…¨å¼€æºï¼ˆApache 2.0 + GPLv2ï¼‰&lt;/li&gt; 
    &lt;li&gt;âœ… æ•°æ®å®Œå…¨è‡ªä¸»æ§åˆ¶&lt;/li&gt; 
    &lt;li&gt;âœ… æ— ä»»ä½•é™åˆ¶&lt;/li&gt; 
    &lt;li&gt;âœ… é›¶è´¹ç”¨&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;Docker ä¸€é”®éƒ¨ç½²&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d \
  --name ntfy \
  -p 80:80 \
  -v /var/cache/ntfy:/var/cache/ntfy \
  binwiederhier/ntfy \
  serve --cache-file /var/cache/ntfy/cache.db
&lt;/code&gt;&lt;/pre&gt; 
   &lt;p&gt;&lt;strong&gt;é…ç½® TrendRadar&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;pre&gt;&lt;code class="language-yaml"&gt;NTFY_SERVER_URL: https://ntfy.yourdomain.com
NTFY_TOPIC: trendradar-alerts  # è‡ªæ‰˜ç®¡å¯ç”¨ç®€å•åç§°
NTFY_TOKEN: tk_your_token  # å¯é€‰ï¼šå¯ç”¨è®¿é—®æ§åˆ¶
&lt;/code&gt;&lt;/pre&gt; 
   &lt;p&gt;&lt;strong&gt;åœ¨åº”ç”¨ä¸­è®¢é˜…&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;ç‚¹å‡»"Use another server"&lt;/li&gt; 
    &lt;li&gt;è¾“å…¥ä½ çš„æœåŠ¡å™¨åœ°å€&lt;/li&gt; 
    &lt;li&gt;è¾“å…¥ä¸»é¢˜åç§°&lt;/li&gt; 
    &lt;li&gt;ï¼ˆå¯é€‰ï¼‰è¾“å…¥ç™»å½•å‡­æ®&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;hr /&gt; 
   &lt;p&gt;&lt;strong&gt;å¸¸è§é—®é¢˜ï¼š&lt;/strong&gt;&lt;/p&gt; 
   &lt;details&gt; 
    &lt;summary&gt;&lt;strong&gt;Q1: å…è´¹ç‰ˆå¤Ÿç”¨å—ï¼Ÿ&lt;/strong&gt;&lt;/summary&gt; 
    &lt;p&gt;æ¯å¤© 250 æ¡æ¶ˆæ¯å¯¹å¤§å¤šæ•°ç”¨æˆ·è¶³å¤Ÿã€‚æŒ‰ 30 åˆ†é’ŸæŠ“å–ä¸€æ¬¡è®¡ç®—ï¼Œæ¯å¤©çº¦ 48 æ¬¡æ¨é€ï¼Œå®Œå…¨å¤Ÿç”¨ã€‚&lt;/p&gt; 
   &lt;/details&gt; 
   &lt;details&gt; 
    &lt;summary&gt;&lt;strong&gt;Q2: Topic åç§°çœŸçš„å®‰å…¨å—ï¼Ÿ&lt;/strong&gt;&lt;/summary&gt; 
    &lt;p&gt;å¦‚æœä½ é€‰æ‹©éšæœºçš„ã€è¶³å¤Ÿé•¿çš„åç§°ï¼ˆå¦‚ &lt;code&gt;trendradar-zs-8492-news&lt;/code&gt;ï¼‰ï¼Œæš´åŠ›ç ´è§£å‡ ä¹ä¸å¯èƒ½ï¼š&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;ntfy æœ‰ä¸¥æ ¼çš„é€Ÿç‡é™åˆ¶ï¼ˆ1 ç§’ 1 æ¬¡è¯·æ±‚ï¼‰&lt;/li&gt; 
     &lt;li&gt;64 ä¸ªå­—ç¬¦é€‰æ‹©ï¼ˆA-Z, a-z, 0-9, _, -ï¼‰&lt;/li&gt; 
     &lt;li&gt;10 ä½éšæœºå­—ç¬¦ä¸²æœ‰ 64^10 ç§å¯èƒ½æ€§ï¼ˆéœ€è¦æ•°å¹´æ‰èƒ½ç ´è§£ï¼‰&lt;/li&gt; 
    &lt;/ul&gt; 
   &lt;/details&gt; 
   &lt;hr /&gt; 
   &lt;p&gt;&lt;strong&gt;æ¨èé€‰æ‹©ï¼š&lt;/strong&gt;&lt;/p&gt; 
   &lt;table&gt; 
    &lt;thead&gt; 
     &lt;tr&gt; 
      &lt;th&gt;ç”¨æˆ·ç±»å‹&lt;/th&gt; 
      &lt;th&gt;æ¨èæ–¹æ¡ˆ&lt;/th&gt; 
      &lt;th&gt;ç†ç”±&lt;/th&gt; 
     &lt;/tr&gt; 
    &lt;/thead&gt; 
    &lt;tbody&gt; 
     &lt;tr&gt; 
      &lt;td&gt;æ™®é€šç”¨æˆ·&lt;/td&gt; 
      &lt;td&gt;æ–¹å¼ä¸€ï¼ˆå…è´¹ï¼‰&lt;/td&gt; 
      &lt;td&gt;ç®€å•å¿«é€Ÿï¼Œå¤Ÿç”¨&lt;/td&gt; 
     &lt;/tr&gt; 
     &lt;tr&gt; 
      &lt;td&gt;æŠ€æœ¯ç”¨æˆ·&lt;/td&gt; 
      &lt;td&gt;æ–¹å¼äºŒï¼ˆè‡ªæ‰˜ç®¡ï¼‰&lt;/td&gt; 
      &lt;td&gt;å®Œå…¨æ§åˆ¶ï¼Œæ— é™åˆ¶&lt;/td&gt; 
     &lt;/tr&gt; 
     &lt;tr&gt; 
      &lt;td&gt;é«˜é¢‘ç”¨æˆ·&lt;/td&gt; 
      &lt;td&gt;æ–¹å¼ä¸‰ï¼ˆä»˜è´¹ï¼‰&lt;/td&gt; 
      &lt;td&gt;è¿™ä¸ªè‡ªå·±å»å®˜ç½‘çœ‹å§&lt;/td&gt; 
     &lt;/tr&gt; 
    &lt;/tbody&gt; 
   &lt;/table&gt; 
   &lt;p&gt;&lt;strong&gt;ç›¸å…³é“¾æ¥ï¼š&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://docs.ntfy.sh/"&gt;ntfy å®˜æ–¹æ–‡æ¡£&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://docs.ntfy.sh/install/"&gt;è‡ªæ‰˜ç®¡æ•™ç¨‹&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://github.com/binwiederhier/ntfy"&gt;GitHub ä»“åº“&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;é…ç½®è¯´æ˜ï¼š&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;æ¨é€è®¾ç½®&lt;/strong&gt;ï¼šåœ¨ &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/config.yaml"&gt;config/config.yaml&lt;/a&gt; ä¸­é…ç½®æ¨é€æ¨¡å¼å’Œé€šçŸ¥é€‰é¡¹&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;å…³é”®è¯è®¾ç½®&lt;/strong&gt;ï¼šåœ¨ &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/frequency_words.txt"&gt;config/frequency_words.txt&lt;/a&gt; ä¸­æ·»åŠ ä½ å…³å¿ƒçš„å…³é”®è¯&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;æ¨é€é¢‘ç‡è°ƒæ•´&lt;/strong&gt;ï¼šåœ¨ &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/.github/workflows/crawler.yml"&gt;.github/workflows/crawler.yml&lt;/a&gt; è¯·è°¨æ…è°ƒæ•´ï¼Œåˆ«è´ªå¿ƒ&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;æ³¨æ„&lt;/strong&gt;ï¼šå»ºè®®åªè°ƒæ•´æ–‡æ¡£ä¸­æ˜ç¡®è¯´æ˜çš„é…ç½®é¡¹ï¼Œå…¶ä»–é€‰é¡¹ä¸»è¦ä¾›ä½œè€…å¼€å‘æ—¶æµ‹è¯•ä½¿ç”¨&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;æ‰‹åŠ¨æµ‹è¯•æ–°é—»æ¨é€&lt;/strong&gt;ï¼š&lt;/p&gt; &lt;p&gt;æˆ‘è¿™é‡Œæ˜¯æ‹¿æˆ‘çš„é¡¹ç›®ä¸¾ä¾‹ï¼Œä½ è¦å»ä½ &lt;strong&gt;fork&lt;/strong&gt;çš„é¡¹ç›®åšæµ‹è¯•&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;è¿›å…¥ Actions&lt;/strong&gt;ï¼š&lt;a href="https://github.com/sansan0/TrendRadar/actions"&gt;https://github.com/sansan0/TrendRadar/actions&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;æ‰¾åˆ° "Hot News Crawler" çš„ç‚¹è¿›å»ï¼Œå¦‚æœçœ‹ä¸åˆ°è¯¥å­—æ ·ï¼Œé‚£ä¹ˆå‚ç…§&lt;a href="https://github.com/sansan0/TrendRadar/issues/109"&gt;#109&lt;/a&gt;è§£å†³&lt;/li&gt; 
   &lt;li&gt;ç‚¹å‡» "Run workflow" æŒ‰é’®è¿è¡Œï¼Œç­‰å¾… 1 åˆ†é’Ÿå·¦å³æ•°æ®åˆ°ä½ æ‰‹æœºä¸Š&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ³ Docker éƒ¨ç½²&lt;/h2&gt; 
&lt;h4&gt;æ–¹å¼ä¸€ï¼šå¿«é€Ÿä½“éªŒï¼ˆä¸€è¡Œå‘½ä»¤ï¼‰&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Linux/macOS ç³»ç»Ÿï¼š&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# åˆ›å»ºé…ç½®ç›®å½•å¹¶ä¸‹è½½é…ç½®æ–‡ä»¶
mkdir -p config output
wget https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/config.yaml -P config/
wget https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/frequency_words.txt -P config/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;æˆ–è€…&lt;strong&gt;æ‰‹åŠ¨åˆ›å»º&lt;/strong&gt;ï¼š&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;åœ¨å½“å‰ç›®å½•åˆ›å»º &lt;code&gt;config&lt;/code&gt; æ–‡ä»¶å¤¹&lt;/li&gt; 
 &lt;li&gt;ä¸‹è½½é…ç½®æ–‡ä»¶ï¼š 
  &lt;ul&gt; 
   &lt;li&gt;è®¿é—® &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/config.yaml"&gt;https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/config.yaml&lt;/a&gt; â†’ å³é”®"å¦å­˜ä¸º" â†’ ä¿å­˜åˆ° &lt;code&gt;config\config.yaml&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;è®¿é—® &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/frequency_words.txt"&gt;https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/frequency_words.txt&lt;/a&gt; â†’ å³é”®"å¦å­˜ä¸º" â†’ ä¿å­˜åˆ° &lt;code&gt;config\frequency_words.txt&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;å®Œæˆåçš„ç›®å½•ç»“æ„åº”è¯¥æ˜¯ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;å½“å‰ç›®å½•/
â””â”€â”€ config/
    â”œâ”€â”€ config.yaml
    â””â”€â”€ frequency_words.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d --name trend-radar \
  -v ./config:/app/config:ro \
  -v ./output:/app/output \
  -e FEISHU_WEBHOOK_URL="ä½ çš„é£ä¹¦webhook" \
  -e DINGTALK_WEBHOOK_URL="ä½ çš„é’‰é’‰webhook" \
  -e WEWORK_WEBHOOK_URL="ä½ çš„ä¼ä¸šå¾®ä¿¡webhook" \
  -e TELEGRAM_BOT_TOKEN="ä½ çš„telegram_bot_token" \
  -e TELEGRAM_CHAT_ID="ä½ çš„telegram_chat_id" \
  -e EMAIL_FROM="ä½ çš„å‘ä»¶é‚®ç®±" \
  -e EMAIL_PASSWORD="ä½ çš„é‚®ç®±å¯†ç æˆ–æˆæƒç " \
  -e EMAIL_TO="æ”¶ä»¶äººé‚®ç®±" \
  -e CRON_SCHEDULE="*/30 * * * *" \
  -e RUN_MODE="cron" \
  -e IMMEDIATE_RUN="true" \
  wantcat/trendradar:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;æ–¹å¼äºŒï¼šä½¿ç”¨ docker-composeï¼ˆæ¨èï¼‰&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;åˆ›å»ºé¡¹ç›®ç›®å½•å’Œé…ç½®&lt;/strong&gt;: &lt;pre&gt;&lt;code class="language-bash"&gt;# åˆ›å»ºç›®å½•ç»“æ„
mkdir -p trendradar/{config,docker}
cd trendradar

# ä¸‹è½½é…ç½®æ–‡ä»¶æ¨¡æ¿
wget https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/config.yaml -P config/
wget https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/frequency_words.txt -P config/

# ä¸‹è½½ docker-compose é…ç½®
wget https://raw.githubusercontent.com/sansan0/TrendRadar/master/docker/.env
wget https://raw.githubusercontent.com/sansan0/TrendRadar/master/docker/docker-compose.yml
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;å®Œæˆåçš„ç›®å½•ç»“æ„åº”è¯¥æ˜¯ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;å½“å‰ç›®å½•/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.yaml
â”‚   â””â”€â”€ frequency_words.txt
â””â”€â”€ docker/
    â”œâ”€â”€ .env
    â””â”€â”€ docker-compose.yml
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;é…ç½®æ–‡ä»¶è¯´æ˜&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;config/config.yaml&lt;/code&gt; - åº”ç”¨ä¸»é…ç½®ï¼ˆæŠ¥å‘Šæ¨¡å¼ã€æ¨é€è®¾ç½®ç­‰ï¼‰&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;config/frequency_words.txt&lt;/code&gt; - å…³é”®è¯é…ç½®ï¼ˆè®¾ç½®ä½ å…³å¿ƒçš„çƒ­ç‚¹è¯æ±‡ï¼‰&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;.env&lt;/code&gt; - ç¯å¢ƒå˜é‡é…ç½®ï¼ˆwebhook URLs å’Œå®šæ—¶ä»»åŠ¡ï¼‰&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;å¯åŠ¨æœåŠ¡&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# æ‹‰å–æœ€æ–°é•œåƒå¹¶å¯åŠ¨
docker-compose pull
docker-compose up -d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;æŸ¥çœ‹è¿è¡ŒçŠ¶æ€&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# æŸ¥çœ‹æ—¥å¿—
docker logs -f trend-radar

# æŸ¥çœ‹å®¹å™¨çŠ¶æ€
docker ps | grep trend-radar
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;æ–¹å¼ä¸‰ï¼šæœ¬åœ°æ„å»ºï¼ˆå¼€å‘è€…é€‰é¡¹ï¼‰&lt;/h4&gt; 
&lt;p&gt;å¦‚æœéœ€è¦è‡ªå®šä¹‰ä¿®æ”¹ä»£ç æˆ–æ„å»ºè‡ªå·±çš„é•œåƒï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# å…‹éš†é¡¹ç›®
git clone https://github.com/sansan0/TrendRadar.git
cd TrendRadar

# ä¿®æ”¹é…ç½®æ–‡ä»¶
vim config/config.yaml
vim config/frequency_words.txt

# ä½¿ç”¨æ„å»ºç‰ˆæœ¬çš„ docker-compose
cd docker
cp docker-compose-build.yml docker-compose.yml

# æ„å»ºå¹¶å¯åŠ¨
docker-compose build
docker-compose up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;é•œåƒæ›´æ–°&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# æ–¹å¼ä¸€ï¼šæ‰‹åŠ¨æ›´æ–°
docker pull wantcat/trendradar:latest
docker-compose down
docker-compose up -d

# æ–¹å¼äºŒï¼šä½¿ç”¨ docker-compose æ›´æ–°
docker-compose pull
docker-compose up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;æœåŠ¡ç®¡ç†å‘½ä»¤&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# æŸ¥çœ‹è¿è¡ŒçŠ¶æ€
docker exec -it trend-radar python manage.py status

# æ‰‹åŠ¨æ‰§è¡Œä¸€æ¬¡çˆ¬è™«
docker exec -it trend-radar python manage.py run

# æŸ¥çœ‹å®æ—¶æ—¥å¿—
docker exec -it trend-radar python manage.py logs

# æ˜¾ç¤ºå½“å‰é…ç½®
docker exec -it trend-radar python manage.py config

# æ˜¾ç¤ºè¾“å‡ºæ–‡ä»¶
docker exec -it trend-radar python manage.py files

# æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯
docker exec -it trend-radar python manage.py help

# é‡å¯å®¹å™¨
docker restart trend-radar

# åœæ­¢å®¹å™¨
docker stop trend-radar

# åˆ é™¤å®¹å™¨ï¼ˆä¿ç•™æ•°æ®ï¼‰
docker rm trend-radar
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;æ•°æ®æŒä¹…åŒ–&lt;/h4&gt; 
&lt;p&gt;ç”Ÿæˆçš„æŠ¥å‘Šå’Œæ•°æ®é»˜è®¤ä¿å­˜åœ¨ &lt;code&gt;./output&lt;/code&gt; ç›®å½•ä¸‹ï¼Œå³ä½¿å®¹å™¨é‡å¯æˆ–åˆ é™¤ï¼Œæ•°æ®ä¹Ÿä¼šä¿ç•™ã€‚&lt;/p&gt; 
&lt;h4&gt;æ•…éšœæ’æŸ¥&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# æ£€æŸ¥å®¹å™¨çŠ¶æ€
docker inspect trend-radar

# æŸ¥çœ‹å®¹å™¨æ—¥å¿—
docker logs --tail 100 trend-radar

# è¿›å…¥å®¹å™¨è°ƒè¯•
docker exec -it trend-radar /bin/bash

# éªŒè¯é…ç½®æ–‡ä»¶
docker exec -it trend-radar ls -la /app/config/
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ¤– AI æ™ºèƒ½åˆ†æéƒ¨ç½²&lt;/h2&gt; 
&lt;p&gt;TrendRadar v3.0.0 æ–°å¢äº†åŸºäº &lt;strong&gt;MCP (Model Context Protocol)&lt;/strong&gt; çš„ AI åˆ†æåŠŸèƒ½ï¼Œè®©ä½ å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€ä¸æ–°é—»æ•°æ®å¯¹è¯ï¼Œè¿›è¡Œæ·±åº¦åˆ†æã€‚ä½¿ç”¨ &lt;strong&gt;AI åŠŸèƒ½&lt;/strong&gt; çš„æœ€ä½³å‰ææ˜¯å·²ä½¿ç”¨æœ¬é¡¹ç›®è‡³å°‘è¿è¡Œä¸€å¤©(ç§¯ç´¯æ–°é—»æ•°æ®)&lt;/p&gt; 
&lt;h3&gt;1. å¿«é€Ÿéƒ¨ç½²&lt;/h3&gt; 
&lt;p&gt;Cherry Studio æä¾› GUI é…ç½®ç•Œé¢ï¼Œ 5 åˆ†é’Ÿå¿«é€Ÿéƒ¨ç½²ï¼Œ å¤æ‚çš„éƒ¨åˆ†æ˜¯ä¸€é”®å®‰è£…çš„ã€‚&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;å›¾æ–‡éƒ¨ç½²æ•™ç¨‹&lt;/strong&gt;ï¼šç°å·²æ›´æ–°åˆ°æˆ‘çš„&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#%E9%97%AE%E9%A2%98%E7%AD%94%E7%96%91%E4%B8%8E1%E5%85%83%E7%82%B9%E8%B5%9E"&gt;å…¬ä¼—å·&lt;/a&gt;ï¼Œå›å¤ "mcp" å³å¯&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;è¯¦ç»†éƒ¨ç½²æ•™ç¨‹&lt;/strong&gt;ï¼š&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/README-Cherry-Studio.md"&gt;README-Cherry-Studio.md&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;2. å­¦ä¹ ä¸ AI å¯¹è¯çš„å§¿åŠ¿&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;è¯¦ç»†å¯¹è¯æ•™ç¨‹&lt;/strong&gt;ï¼š&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/README-MCP-FAQ.md"&gt;README-MCP-FAQ.md&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;æé—®æ•ˆæœ&lt;/strong&gt;ï¼š&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;å®é™…ä¸å»ºè®®ä¸€æ¬¡æ€§é—®å¤šä¸ªé—®é¢˜ã€‚å¦‚æœä½ é€‰æ‹©çš„ ai æ¨¡å‹è¿ä¸‹å›¾çš„æŒ‰é¡ºåºè°ƒç”¨éƒ½æ— æ³•åšåˆ°ï¼Œå»ºè®®æ¢ä¸€ä¸ªã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/ai2.png" alt="mcp ä½¿ç”¨æ•ˆæœå›¾2" width="600" /&gt; 
&lt;h2&gt;ğŸ”Œ MCP å®¢æˆ·ç«¯&lt;/h2&gt; 
&lt;p&gt;TrendRadar MCP æœåŠ¡æ”¯æŒæ ‡å‡†çš„ Model Context Protocol (MCP) åè®®ï¼Œå¯ä»¥æ¥å…¥å„ç§æ”¯æŒ MCP çš„ AI å®¢æˆ·ç«¯è¿›è¡Œæ™ºèƒ½åˆ†æã€‚&lt;/p&gt; 
&lt;h3&gt;æ”¯æŒçš„å®¢æˆ·ç«¯&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;æ³¨æ„äº‹é¡¹&lt;/strong&gt;ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;å°† &lt;code&gt;/path/to/TrendRadar&lt;/code&gt; æ›¿æ¢ä¸ºä½ çš„é¡¹ç›®å®é™…è·¯å¾„&lt;/li&gt; 
 &lt;li&gt;Windows è·¯å¾„ä½¿ç”¨åŒåæ–œæ ï¼š&lt;code&gt;C:\\Users\\YourName\\TrendRadar&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;ä¿å­˜åè®°å¾—é‡å¯&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;ğŸ‘‰ Claude Desktop&lt;/b&gt;&lt;/summary&gt; 
 &lt;h4&gt;é…ç½®æ–‡ä»¶æ–¹å¼&lt;/h4&gt; 
 &lt;p&gt;ç¼–è¾‘ Claude Desktop çš„ MCP é…ç½®æ–‡ä»¶ï¼š&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;ï¼š &lt;code&gt;%APPDATA%\Claude\claude_desktop_config.json&lt;/code&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Mac&lt;/strong&gt;ï¼š &lt;code&gt;~/Library/Application Support/Claude/claude_desktop_config.json&lt;/code&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;é…ç½®å†…å®¹&lt;/strong&gt;ï¼š&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "trendradar": {
      "command": "uv",
      "args": [
        "--directory",
        "/path/to/TrendRadar",
        "run",
        "python",
        "-m",
        "mcp_server.server"
      ],
      "env": {},
      "disabled": false,
      "alwaysAllow": []
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;ğŸ‘‰ Cursor&lt;/b&gt;&lt;/summary&gt; 
 &lt;h4&gt;æ–¹å¼ä¸€ï¼šHTTP æ¨¡å¼ï¼ˆæ¨èï¼‰&lt;/h4&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;å¯åŠ¨ HTTP æœåŠ¡&lt;/strong&gt;ï¼š&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Windows
start-http.bat

# Mac/Linux
./start-http.sh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;é…ç½® Cursor&lt;/strong&gt;ï¼š&lt;/p&gt; &lt;p&gt;&lt;strong&gt;é¡¹ç›®çº§é…ç½®&lt;/strong&gt;ï¼ˆæ¨èï¼‰ï¼š åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º &lt;code&gt;.cursor/mcp.json&lt;/code&gt;ï¼š&lt;/p&gt; &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "trendradar": {
      "url": "http://localhost:3333/mcp",
      "description": "TrendRadar æ–°é—»çƒ­ç‚¹èšåˆåˆ†æ"
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;å…¨å±€é…ç½®&lt;/strong&gt;ï¼š åœ¨ç”¨æˆ·ç›®å½•åˆ›å»º &lt;code&gt;~/.cursor/mcp.json&lt;/code&gt;ï¼ˆåŒæ ·å†…å®¹ï¼‰&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ä½¿ç”¨æ­¥éª¤&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;ä¿å­˜é…ç½®æ–‡ä»¶åé‡å¯ Cursor&lt;/li&gt; 
    &lt;li&gt;åœ¨èŠå¤©ç•Œé¢çš„ "Available Tools" ä¸­æŸ¥çœ‹å·²è¿æ¥çš„å·¥å…·&lt;/li&gt; 
    &lt;li&gt;å¼€å§‹ä½¿ç”¨ï¼š&lt;code&gt;æœç´¢ä»Šå¤©çš„"AI"ç›¸å…³æ–°é—»&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h4&gt;æ–¹å¼äºŒï¼šSTDIO æ¨¡å¼&lt;/h4&gt; 
 &lt;p&gt;åˆ›å»º &lt;code&gt;.cursor/mcp.json&lt;/code&gt;ï¼š&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "trendradar": {
      "command": "uv",
      "args": [
        "--directory",
        "/path/to/TrendRadar",
        "run",
        "python",
        "-m",
        "mcp_server.server"
      ]
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;ğŸ‘‰ VSCode (Cline/Continue)&lt;/b&gt;&lt;/summary&gt; 
 &lt;h4&gt;Cline é…ç½®&lt;/h4&gt; 
 &lt;p&gt;åœ¨ Cline çš„ MCP è®¾ç½®ä¸­æ·»åŠ ï¼š&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;HTTP æ¨¡å¼&lt;/strong&gt;ï¼ˆæ¨èï¼‰ï¼š&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "trendradar": {
    "url": "http://localhost:3333/mcp",
    "type": "streamableHttp",
    "autoApprove": [],
    "disabled": false
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;STDIO æ¨¡å¼&lt;/strong&gt;ï¼š&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "trendradar": {
    "command": "uv",
    "args": [
      "--directory",
      "/path/to/TrendRadar",
      "run",
      "python",
      "-m",
      "mcp_server.server"
    ],
    "type": "stdio",
    "disabled": false
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Continue é…ç½®&lt;/h4&gt; 
 &lt;p&gt;ç¼–è¾‘ &lt;code&gt;~/.continue/config.json&lt;/code&gt;ï¼š&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "experimental": {
    "modelContextProtocolServers": [
      {
        "transport": {
          "type": "stdio",
          "command": "uv",
          "args": [
            "--directory",
            "/path/to/TrendRadar",
            "run",
            "python",
            "-m",
            "mcp_server.server"
          ]
        }
      }
    ]
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;ä½¿ç”¨ç¤ºä¾‹&lt;/strong&gt;ï¼š&lt;/p&gt; 
 &lt;pre&gt;&lt;code&gt;åˆ†ææœ€è¿‘7å¤©"ç‰¹æ–¯æ‹‰"çš„çƒ­åº¦å˜åŒ–è¶‹åŠ¿
ç”Ÿæˆä»Šå¤©çš„çƒ­ç‚¹æ‘˜è¦æŠ¥å‘Š
æœç´¢"æ¯”ç‰¹å¸"ç›¸å…³æ–°é—»å¹¶åˆ†ææƒ…æ„Ÿå€¾å‘
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;ğŸ‘‰ Claude Code CLI&lt;/b&gt;&lt;/summary&gt; 
 &lt;h4&gt;HTTP æ¨¡å¼é…ç½®&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# 1. å¯åŠ¨ HTTP æœåŠ¡
# Windows: start-http.bat
# Mac/Linux: ./start-http.sh

# 2. æ·»åŠ  MCP æœåŠ¡å™¨
claude mcp add --transport http trendradar http://localhost:3333/mcp

# 3. éªŒè¯è¿æ¥ï¼ˆç¡®ä¿æœåŠ¡å·²å¯åŠ¨ï¼‰
claude mcp list
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;ä½¿ç”¨ç¤ºä¾‹&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# æŸ¥è¯¢æ–°é—»
claude "æœç´¢ä»Šå¤©çŸ¥ä¹çš„çƒ­ç‚¹æ–°é—»ï¼Œå‰10æ¡"

# è¶‹åŠ¿åˆ†æ
claude "åˆ†æ'äººå·¥æ™ºèƒ½'è¿™ä¸ªè¯é¢˜æœ€è¿‘ä¸€å‘¨çš„çƒ­åº¦è¶‹åŠ¿"

# æ•°æ®å¯¹æ¯”
claude "å¯¹æ¯”çŸ¥ä¹å’Œå¾®åšå¹³å°å¯¹'æ¯”ç‰¹å¸'çš„å…³æ³¨åº¦"
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;ğŸ‘‰ MCP Inspector&lt;/b&gt;ï¼ˆè°ƒè¯•å·¥å…·ï¼‰&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;MCP Inspector æ˜¯å®˜æ–¹è°ƒè¯•å·¥å…·ï¼Œç”¨äºæµ‹è¯• MCP è¿æ¥ï¼š&lt;/p&gt; 
 &lt;h4&gt;ä½¿ç”¨æ­¥éª¤&lt;/h4&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;å¯åŠ¨ TrendRadar HTTP æœåŠ¡&lt;/strong&gt;ï¼š&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Windows
start-http.bat

# Mac/Linux
./start-http.sh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;å¯åŠ¨ MCP Inspector&lt;/strong&gt;ï¼š&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;npx @modelcontextprotocol/inspector
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;åœ¨æµè§ˆå™¨ä¸­è¿æ¥&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;è®¿é—®ï¼š&lt;code&gt;http://localhost:3333/mcp&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;æµ‹è¯• "Ping Server" åŠŸèƒ½éªŒè¯è¿æ¥&lt;/li&gt; 
    &lt;li&gt;æ£€æŸ¥ "List Tools" æ˜¯å¦è¿”å› 13 ä¸ªå·¥å…·ï¼š 
     &lt;ul&gt; 
      &lt;li&gt;åŸºç¡€æŸ¥è¯¢ï¼šget_latest_news, get_news_by_date, get_trending_topics&lt;/li&gt; 
      &lt;li&gt;æ™ºèƒ½æ£€ç´¢ï¼šsearch_news, search_related_news_history&lt;/li&gt; 
      &lt;li&gt;é«˜çº§åˆ†æï¼šanalyze_topic_trend, analyze_data_insights, analyze_sentiment, find_similar_news, generate_summary_report&lt;/li&gt; 
      &lt;li&gt;ç³»ç»Ÿç®¡ç†ï¼šget_current_config, get_system_status, trigger_crawl&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;ğŸ‘‰ å…¶ä»–æ”¯æŒ MCP çš„å®¢æˆ·ç«¯&lt;/b&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;ä»»ä½•æ”¯æŒ Model Context Protocol çš„å®¢æˆ·ç«¯éƒ½å¯ä»¥è¿æ¥ TrendRadarï¼š&lt;/p&gt; 
 &lt;h4&gt;HTTP æ¨¡å¼ï¼ˆæ¨èï¼‰&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;æœåŠ¡åœ°å€&lt;/strong&gt;ï¼š&lt;code&gt;http://localhost:3333/mcp&lt;/code&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;åŸºæœ¬é…ç½®æ¨¡æ¿&lt;/strong&gt;ï¼š&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "name": "trendradar",
  "url": "http://localhost:3333/mcp",
  "type": "http",
  "description": "æ–°é—»çƒ­ç‚¹èšåˆåˆ†æ"
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;STDIO æ¨¡å¼&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;åŸºæœ¬é…ç½®æ¨¡æ¿&lt;/strong&gt;ï¼š&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "name": "trendradar",
  "command": "uv",
  "args": [
    "--directory",
    "/path/to/TrendRadar",
    "run",
    "python",
    "-m",
    "mcp_server.server"
  ],
  "type": "stdio"
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;æ³¨æ„äº‹é¡¹&lt;/strong&gt;ï¼š&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;æ›¿æ¢ &lt;code&gt;/path/to/TrendRadar&lt;/code&gt; ä¸ºå®é™…é¡¹ç›®è·¯å¾„&lt;/li&gt; 
  &lt;li&gt;Windows è·¯å¾„ä½¿ç”¨åæ–œæ è½¬ä¹‰ï¼š&lt;code&gt;C:\\Users\\...&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;ç¡®ä¿å·²å®Œæˆé¡¹ç›®ä¾èµ–å®‰è£…ï¼ˆè¿è¡Œè¿‡ setup è„šæœ¬ï¼‰&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;â˜•é—®é¢˜ç­”ç–‘ä¸1å…ƒç‚¹èµ&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;å¿ƒæ„åˆ°å°±è¡Œï¼Œæ”¶åˆ°çš„&lt;strong&gt;ç‚¹èµ&lt;/strong&gt;ç”¨äºæé«˜å¼€å‘è€…å¼€æºçš„ç§¯ææ€§ã€‚&lt;strong&gt;ç‚¹èµ&lt;/strong&gt;å·²æ”¶å½•äº&lt;strong&gt;è‡´è°¢åå•&lt;/strong&gt;&lt;br /&gt; æˆ‘å‘ç°å¤§å®¶éƒ½å¾ˆå–„äºé è‡ªå·±è§£å†³é—®é¢˜ï¼Œè¿™ç§å°è¯•å€¼å¾—é¼“åŠ±ï¼Œä½†å¦‚æœè¢«é—®é¢˜å¡ä½å¤ªä¹…ï¼Œå»ºè®®æé—®æˆ–è€…ç•™è¨€ã€‚è¿™æ ·æˆ‘æ—¢èƒ½å¸®åˆ°&lt;strong&gt;ä½ &lt;/strong&gt;ï¼Œä¹Ÿèƒ½å¸®åˆ°&lt;strong&gt;æ›´å¤šæ¢ç´¢ä¸­çš„å°ä¼™ä¼´&lt;/strong&gt;~~&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Issues&lt;/strong&gt;ï¼šé€‚åˆé’ˆå¯¹æ€§å¼ºçš„è§£ç­”ã€‚æé—®æ—¶è¯·æä¾›å®Œæ•´ä¿¡æ¯ï¼ˆæˆªå›¾ã€é”™è¯¯æ—¥å¿—ã€ç³»ç»Ÿç¯å¢ƒç­‰ï¼‰ã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;å…¬ä¼—å·äº¤æµ&lt;/strong&gt;ï¼šé€‚åˆå¿«é€Ÿå’¨è¯¢ã€‚å»ºè®®ä¼˜å…ˆåœ¨ç›¸å…³æ–‡ç« ä¸‹çš„å…¬å…±ç•™è¨€åŒºäº¤æµï¼Œå¦‚ç§ä¿¡ï¼Œè¯·æ–‡æ˜ç¤¼è²Œç”¨è¯­ğŸ˜‰&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;å…¬ä¼—å·å…³æ³¨&lt;/th&gt; 
   &lt;th align="center"&gt;å¾®ä¿¡ç‚¹èµ&lt;/th&gt; 
   &lt;th align="center"&gt;æ”¯ä»˜å®ç‚¹èµ&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/weixin.png" width="300" title="ç¡…åŸºèŒ¶æ°´é—´" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://cdn-1258574687.cos.ap-shanghai.myqcloud.com/img/%2F2025%2F07%2F17%2F2ae0a88d98079f7e876c2b4dc85233c6-9e8025.JPG" width="300" title="å¾®ä¿¡æ”¯ä»˜" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://cdn-1258574687.cos.ap-shanghai.myqcloud.com/img/%2F2025%2F07%2F17%2F1ed4f20ab8e35be51f8e84c94e6e239b4-fe4947.JPG" width="300" title="æ”¯ä»˜å®æ”¯ä»˜" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;å¸¸è§é—®é¢˜&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;ğŸ‘‰ Q1: HTTP æœåŠ¡æ— æ³•å¯åŠ¨ï¼Ÿ&lt;/b&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;æ£€æŸ¥æ­¥éª¤&lt;/strong&gt;ï¼š&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;ç¡®è®¤ç«¯å£ 3333 æœªè¢«å ç”¨ï¼š&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Windows
netstat -ano | findstr :3333

# Mac/Linux
lsof -i :3333
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;æ£€æŸ¥é¡¹ç›®ä¾èµ–æ˜¯å¦å®‰è£…ï¼š&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# é‡æ–°è¿è¡Œå®‰è£…è„šæœ¬
# Windows: setup-windows.bat æˆ–è€… setup-windows-en.bat
# Mac/Linux: ./setup-mac.sh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;æŸ¥çœ‹è¯¦ç»†é”™è¯¯æ—¥å¿—ï¼š&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;uv run python -m mcp_server.server --transport http --port 3333
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;å°è¯•è‡ªå®šä¹‰ç«¯å£:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;uv run python -m mcp_server.server --transport http --port 33333
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;ğŸ‘‰ Q2: å®¢æˆ·ç«¯æ— æ³•è¿æ¥åˆ° MCP æœåŠ¡ï¼Ÿ&lt;/b&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;è§£å†³æ–¹æ¡ˆ&lt;/strong&gt;ï¼š&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;STDIO æ¨¡å¼&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;ç¡®è®¤ UV è·¯å¾„æ­£ç¡®ï¼ˆè¿è¡Œ &lt;code&gt;which uv&lt;/code&gt; æˆ– &lt;code&gt;where uv&lt;/code&gt;ï¼‰&lt;/li&gt; 
    &lt;li&gt;ç¡®è®¤é¡¹ç›®è·¯å¾„æ­£ç¡®ä¸”æ— ä¸­æ–‡å­—ç¬¦&lt;/li&gt; 
    &lt;li&gt;æŸ¥çœ‹å®¢æˆ·ç«¯é”™è¯¯æ—¥å¿—&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;HTTP æ¨¡å¼&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;ç¡®è®¤æœåŠ¡å·²å¯åŠ¨ï¼ˆè®¿é—® &lt;code&gt;http://localhost:3333/mcp&lt;/code&gt;ï¼‰&lt;/li&gt; 
    &lt;li&gt;æ£€æŸ¥é˜²ç«å¢™è®¾ç½®&lt;/li&gt; 
    &lt;li&gt;å°è¯•ä½¿ç”¨ 127.0.0.1 æ›¿ä»£ localhost&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;é€šç”¨æ£€æŸ¥&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;é‡å¯å®¢æˆ·ç«¯åº”ç”¨&lt;/li&gt; 
    &lt;li&gt;æŸ¥çœ‹ MCP æœåŠ¡æ—¥å¿—&lt;/li&gt; 
    &lt;li&gt;ä½¿ç”¨ MCP Inspector æµ‹è¯•è¿æ¥&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;ğŸ‘‰ Q3: å·¥å…·è°ƒç”¨å¤±è´¥æˆ–è¿”å›é”™è¯¯ï¼Ÿ&lt;/b&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;å¯èƒ½åŸå› &lt;/strong&gt;ï¼š&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;æ•°æ®ä¸å­˜åœ¨&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;ç¡®è®¤å·²è¿è¡Œè¿‡çˆ¬è™«ï¼ˆæœ‰ output ç›®å½•æ•°æ®ï¼‰&lt;/li&gt; 
    &lt;li&gt;æ£€æŸ¥æŸ¥è¯¢æ—¥æœŸèŒƒå›´æ˜¯å¦æœ‰æ•°æ®&lt;/li&gt; 
    &lt;li&gt;æŸ¥çœ‹ output ç›®å½•çš„å¯ç”¨æ—¥æœŸ&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;å‚æ•°é”™è¯¯&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;æ£€æŸ¥æ—¥æœŸæ ¼å¼ï¼š&lt;code&gt;YYYY-MM-DD&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;ç¡®è®¤å¹³å° ID æ­£ç¡®ï¼š&lt;code&gt;zhihu&lt;/code&gt;, &lt;code&gt;weibo&lt;/code&gt; ç­‰&lt;/li&gt; 
    &lt;li&gt;æŸ¥çœ‹å·¥å…·æ–‡æ¡£ä¸­çš„å‚æ•°è¯´æ˜&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;é…ç½®é—®é¢˜&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;ç¡®è®¤ &lt;code&gt;config/config.yaml&lt;/code&gt; å­˜åœ¨&lt;/li&gt; 
    &lt;li&gt;ç¡®è®¤ &lt;code&gt;config/frequency_words.txt&lt;/code&gt; å­˜åœ¨&lt;/li&gt; 
    &lt;li&gt;æ£€æŸ¥é…ç½®æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h3&gt;é¡¹ç›®ç›¸å…³&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;4 ç¯‡æ–‡ç« &lt;/strong&gt;ï¼š&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/KYEPfTPVzZNWFclZh4am_g"&gt;å¯åœ¨è¯¥æ–‡ç« ä¸‹æ–¹ç•™è¨€ï¼Œæ–¹ä¾¿é¡¹ç›®ä½œè€…ç”¨æ‰‹æœºç­”ç–‘&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/jzn0vLiQFX408opcfpPPxQ"&gt;2ä¸ªæœˆç ´ 1000 starï¼Œæˆ‘çš„GitHubé¡¹ç›®æ¨å¹¿å®æˆ˜ç»éªŒ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/C8evK-U7onG1sTTdwdW2zg"&gt;github fork è¿è¡Œæœ¬é¡¹ç›®çš„æ³¨æ„äº‹é¡¹ &lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/8ghyfDAtQZjLrnWTQabYOQ"&gt;åŸºäºæœ¬é¡¹ç›®ï¼Œå¦‚ä½•å¼€å±•å…¬ä¼—å·æˆ–è€…æ–°é—»èµ„è®¯ç±»æ–‡ç« å†™ä½œ&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;AI å¼€å‘&lt;/strong&gt;ï¼š&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;å¦‚æœä½ æœ‰å°ä¼—éœ€æ±‚ï¼Œå®Œå…¨å¯ä»¥åŸºäºæˆ‘çš„é¡¹ç›®è‡ªè¡Œå¼€å‘ï¼Œé›¶ç¼–ç¨‹åŸºç¡€çš„ä¹Ÿå¯ä»¥è¯•è¯•&lt;/li&gt; 
 &lt;li&gt;æˆ‘æ‰€æœ‰çš„å¼€æºé¡¹ç›®æˆ–å¤šæˆ–å°‘éƒ½ä½¿ç”¨äº†è‡ªå·±å†™çš„&lt;strong&gt;AIè¾…åŠ©è½¯ä»¶&lt;/strong&gt;æ¥æå‡å¼€å‘æ•ˆç‡ï¼Œè¿™æ¬¾å·¥å…·å·²å¼€æº&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ ¸å¿ƒåŠŸèƒ½&lt;/strong&gt;ï¼šè¿…é€Ÿç­›é€‰é¡¹ç›®ä»£ç å–‚ç»™AIï¼Œä½ åªéœ€è¦è¡¥å……ä¸ªäººéœ€æ±‚å³å¯&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;é¡¹ç›®åœ°å€&lt;/strong&gt;ï¼š&lt;a href="https://github.com/sansan0/ai-code-context-helper"&gt;https://github.com/sansan0/ai-code-context-helper&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;å…¶ä½™é¡¹ç›®&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ğŸ“ æ¯›ä¸»å¸­è¶³è¿¹åœ°å›¾ - äº¤äº’å¼åŠ¨æ€å±•ç¤º1893-1976å¹´å®Œæ•´è½¨è¿¹ã€‚æ¬¢è¿è¯¸ä½åŒå¿—è´¡çŒ®æ•°æ®&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sansan0/mao-map"&gt;https://github.com/sansan0/mao-map&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;å“”å“©å“”å“©(bilibili)è¯„è®ºåŒºæ•°æ®å¯è§†åŒ–åˆ†æè½¯ä»¶&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sansan0/bilibili-comment-analyzer"&gt;https://github.com/sansan0/bilibili-comment-analyzer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ‘‰ å¾®ä¿¡æ¨é€é€šçŸ¥æ–¹æ¡ˆ&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;ç”±äºè¯¥æ–¹æ¡ˆæ˜¯åŸºäºä¼ä¸šå¾®ä¿¡çš„æ’ä»¶æœºåˆ¶ï¼Œæ¨é€æ ·å¼ä¹Ÿååˆ†ä¸åŒï¼Œæ‰€ä»¥ç›¸å…³å®ç°æˆ‘æš‚æ—¶ä¸å‡†å¤‡çº³å…¥å½“å‰é¡¹ç›®&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;fork è¿™ä½å…„å°çš„é¡¹ç›® &lt;a href="https://github.com/jayzqj/TrendRadar"&gt;https://github.com/jayzqj/TrendRadar&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;å®Œæˆä¸Šæ–¹çš„ä¼ä¸šå¾®ä¿¡æ¨é€è®¾ç½®&lt;/li&gt; 
  &lt;li&gt;æŒ‰ç…§ä¸‹é¢å›¾ç‰‡æ“ä½œ&lt;/li&gt; 
  &lt;li&gt;é…ç½®å¥½åï¼Œæ‰‹æœºä¸Šçš„ä¼ä¸šå¾®ä¿¡ app åˆ é™¤æ‰ä¹Ÿæ²¡äº‹&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/wework.png" title="github" /&gt; 
&lt;/details&gt; 
&lt;h3&gt;æœ¬é¡¹ç›®æµç¨‹å›¾&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-mermaid"&gt;flowchart TD
    A[ğŸ‘¤ ç”¨æˆ·å¼€å§‹] --&amp;gt; B{ğŸš€ é€‰æ‹©éƒ¨ç½²æ–¹å¼}
    
    B --&amp;gt;|äº‘ç«¯éƒ¨ç½²| C1[ğŸ´ Fork é¡¹ç›®åˆ° GitHub]
    B --&amp;gt;|æœ¬åœ°éƒ¨ç½²| C2[ğŸ³ Docker éƒ¨ç½²]
    
    C1 --&amp;gt; D[âš™ï¸ é…ç½®é€šçŸ¥æ¸ é“&amp;lt;br/&amp;gt;å¯åŒæ—¶é…ç½®å¤šä¸ª]
    C2 --&amp;gt; D
    
    D --&amp;gt; E[é€‰æ‹©é€šçŸ¥æ–¹å¼ï¼š&amp;lt;br/&amp;gt;ğŸ“±ä¼ä¸šå¾®ä¿¡ ğŸ’¬é£ä¹¦ ğŸ””é’‰é’‰&amp;lt;br/&amp;gt;ğŸ“ŸTelegram ğŸ“§é‚®ä»¶]
    
    E --&amp;gt; F[ğŸ”‘ å¡«å†™é€šçŸ¥å‚æ•°&amp;lt;br/&amp;gt;GitHub Secrets æˆ–ç¯å¢ƒå˜é‡]
    
    F --&amp;gt; G[ğŸ“ é…ç½®å…³é”®è¯&amp;lt;br/&amp;gt;config/frequency_words.txt&amp;lt;br/&amp;gt;æ™®é€šè¯/å¿…é¡»è¯+/è¿‡æ»¤è¯!]
    
    G --&amp;gt; H[ğŸ¯ é€‰æ‹©è¿è¡Œæ¨¡å¼&amp;lt;br/&amp;gt;config/config.yaml]
    
    H --&amp;gt; H1[ğŸ“‹ daily - å½“æ—¥æ±‡æ€»&amp;lt;br/&amp;gt;å®šæ—¶æ¨é€æ‰€æœ‰åŒ¹é…æ–°é—»]
    H --&amp;gt; H2[ğŸ“° current - å½“å‰æ¦œå•&amp;lt;br/&amp;gt;å®šæ—¶æ¨é€æœ€æ–°æ¦œå•]
    H --&amp;gt; H3[ğŸ“ˆ incremental - å¢é‡ç›‘æ§&amp;lt;br/&amp;gt;ä»…æ¨é€æ–°å¢å†…å®¹]
    
    H1 --&amp;gt; I[å¯é€‰ï¼šæ¨é€æ—¶é—´çª—å£æ§åˆ¶&amp;lt;br/&amp;gt;â° é™åˆ¶æ¨é€æ—¶é—´èŒƒå›´]
    H2 --&amp;gt; I
    H3 --&amp;gt; I
    
    I --&amp;gt; J[âœ… é…ç½®å®Œæˆ]
    
    J --&amp;gt; K[ğŸ¤– ç³»ç»Ÿè‡ªåŠ¨è¿è¡Œ]
    
    K --&amp;gt; L[ğŸ•·ï¸ çˆ¬å–11+å¹³å°çƒ­ç‚¹]
    L --&amp;gt; M[ğŸ” å…³é”®è¯ç­›é€‰]
    M --&amp;gt; N[âš–ï¸ æƒé‡ç®—æ³•æ’åº&amp;lt;br/&amp;gt;æ’å60% + é¢‘æ¬¡30% + çƒ­åº¦10%]
    N --&amp;gt; O[ğŸ“Š ç”ŸæˆæŠ¥å‘Š&amp;lt;br/&amp;gt;HTMLç½‘é¡µ + æ¨é€æ¶ˆæ¯]
    O --&amp;gt; P[ğŸ“± å¤šæ¸ é“æ¨é€é€šçŸ¥]
    
    P --&amp;gt; Q[ğŸ‰ æŒç»­æ¥æ”¶ç²¾å‡†æ¨é€&amp;lt;br/&amp;gt;å‘Šåˆ«ä¿¡æ¯è¿‡è½½]
    
    style A fill:#e3f2fd
    style B fill:#f3e5f5
    style D fill:#fff3e0
    style F fill:#fff9c4
    style G fill:#e8f5e9
    style H fill:#e0f2f1
    style I fill:#fce4ec
    style O fill:#e1bee7
    style Q fill:#c8e6c9
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#sansan0/TrendRadar&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=sansan0/TrendRadar&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ“„ è®¸å¯è¯&lt;/h2&gt; 
&lt;p&gt;GPL-3.0 License&lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#trendradar"&gt;ğŸ” å›åˆ°é¡¶éƒ¨&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>KellerJordan/modded-nanogpt</title>
      <link>https://github.com/KellerJordan/modded-nanogpt</link>
      <description>&lt;p&gt;NanoGPT (124M) in 3 minutes&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Modded-NanoGPT&lt;/h1&gt; 
&lt;p&gt;This repository hosts the &lt;em&gt;NanoGPT speedrun&lt;/em&gt;, in which we (collaboratively|competitively) search for the fastest algorithm to use 8 NVIDIA H100 GPUs to train a language model that attains 3.28 cross-entropy loss on the &lt;a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb"&gt;FineWeb&lt;/a&gt; validation set.&lt;/p&gt; 
&lt;p&gt;The target (3.28 validation loss on FineWeb) follows Andrej Karpathy's &lt;a href="https://github.com/karpathy/llm.c/discussions/481#:~:text=By%20the%20end%20of%20the%20optimization%20we%27ll%20get%20to%20about%203.29"&gt;GPT-2 replication in llm.c, which attains that loss after running for 45 minutes&lt;/a&gt;. The speedrun code also descends from llm.c's &lt;a href="https://github.com/karpathy/llm.c/raw/master/train_gpt2.py"&gt;PyTorch trainer&lt;/a&gt;, which itself descends from NanoGPT, hence the name of the repo. Thanks to the efforts of many contributors, this repo now contains a training algorithm which attains the target performance in:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;2 minutes and 20 seconds on 8xH100 (the llm.c GPT-2 replication needed 45)&lt;/li&gt; 
 &lt;li&gt;0.73B tokens (the llm.c GPT-2 replication needed 10B)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This improvement in training speed has been brought about by the following techniques:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Modernized architecture: Rotary embeddings, QK-Norm, and ReLUÂ²&lt;/li&gt; 
 &lt;li&gt;The Muon optimizer [&lt;a href="https://kellerjordan.github.io/posts/muon/"&gt;writeup&lt;/a&gt;] [&lt;a href="https://github.com/KellerJordan/Muon"&gt;repo&lt;/a&gt;]&lt;/li&gt; 
 &lt;li&gt;Untie head from embedding, use FP8 matmul for head, and softcap logits (the latter following Gemma 2)&lt;/li&gt; 
 &lt;li&gt;Initialization of projection and classification layers to zero (muP-like)&lt;/li&gt; 
 &lt;li&gt;Skip connections from embedding to every block as well as between blocks in U-net pattern&lt;/li&gt; 
 &lt;li&gt;Extra embeddings which are mixed into the values in attention layers (inspired by Zhou et al. 2024)&lt;/li&gt; 
 &lt;li&gt;Flash Attention 3 with long-short sliding window attention pattern (inspired by Gemma 2) and window size warmup with YaRN&lt;/li&gt; 
 &lt;li&gt;Align training batch starts with EoS and set a max document length&lt;/li&gt; 
 &lt;li&gt;Accumulate gradients for 2 steps for embedding and lm_head before updating parameters&lt;/li&gt; 
 &lt;li&gt;Enable model to back out contributions from first 8 layers before prediction&lt;/li&gt; 
 &lt;li&gt;Polar Express implementation in Muon&lt;/li&gt; 
 &lt;li&gt;Smear module to enable 1 token look back&lt;/li&gt; 
 &lt;li&gt;Sparse attention gate&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;As well as many systems optimizations.&lt;/p&gt; 
&lt;p&gt;Contributors list (growing with each new record): &lt;a href="https://x.com/bozavlado"&gt;@bozavlado&lt;/a&gt;; &lt;a href="https://x.com/brendanh0gan"&gt;@brendanh0gan&lt;/a&gt;; &lt;a href="https://bsky.app/profile/fernbear.bsky.social"&gt;@fernbear.bsky.social&lt;/a&gt;; &lt;a href="https://x.com/Grad62304977"&gt;@Grad62304977&lt;/a&gt;; &lt;a href="https://x.com/jxbz"&gt;@jxbz&lt;/a&gt;; &lt;a href="https://x.com/kellerjordan0"&gt;@kellerjordan0&lt;/a&gt;; &lt;a href="https://x.com/KoszarskyB"&gt;@KoszarskyB&lt;/a&gt;; &lt;a href="https://x.com/@leloykun"&gt;@leloykun&lt;/a&gt;; &lt;a href="https://x.com/YouJiacheng"&gt;@YouJiacheng&lt;/a&gt;; &lt;a href="https://x.com/jadenj3o"&gt;@jadenj3o&lt;/a&gt;; &lt;a href="https://github.com/KonstantinWilleke"&gt;@KonstantinWilleke&lt;/a&gt;, &lt;a href="https://github.com/alexrgilbert"&gt;@alexrgilbert&lt;/a&gt;, &lt;a href="https://github.com/adricarda"&gt;@adricarda&lt;/a&gt;, &lt;a href="https://github.com/tuttyfrutyee"&gt;@tuttyfrutyee&lt;/a&gt;, &lt;a href="https://github.com/vdlad"&gt;@vdlad&lt;/a&gt;; &lt;a href="https://x.com/ryanyang0"&gt;@ryanyang0&lt;/a&gt;, &lt;a href="https://github.com/vagrawal"&gt;@vagrawal&lt;/a&gt;, &lt;a href="https://x.com/classiclarryd"&gt;@classiclarryd&lt;/a&gt;, &lt;a href="https://github.com/byronxu99"&gt;@byronxu99&lt;/a&gt;, &lt;a href="https://x.com/varunneal"&gt;@varunneal&lt;/a&gt;, &lt;a href="https://github.com/EmelyanenkoK"&gt;@EmelyanenkoK&lt;/a&gt;, &lt;a href="https://github.com/bernard24"&gt;@bernard24&lt;/a&gt;/&lt;a href="https://www.hiverge.ai/"&gt;https://www.hiverge.ai/&lt;/a&gt;, &lt;a href="https://x.com/GusarichOnX"&gt;@GusarichOnX&lt;/a&gt;, &lt;a href="https://x.com/li_zichong"&gt;@li_zichong&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Running the current record&lt;/h2&gt; 
&lt;p&gt;To run the current record, run the following commands.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/KellerJordan/modded-nanogpt.git &amp;amp;&amp;amp; cd modded-nanogpt
pip install -r requirements.txt
pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu126 --upgrade
# downloads only the first 900M training tokens to save time
python data/cached_fineweb10B.py 9
./run.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note: torch.compile will add around 7 minutes of latency the first time you run the code.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Alternative: Running with Docker (recommended for precise timing)&lt;/h2&gt; 
&lt;p&gt;For cases where CUDA or NCCL versions aren't compatible with your current system setup, Docker can be a helpful alternative. This approach standardizes versions for CUDA, NCCL, CUDNN, and Python, reducing dependency issues and simplifying setup. Note: an NVIDIA driver must already be installed on the system (useful if only the NVIDIA driver and Docker are available).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/KellerJordan/modded-nanogpt.git &amp;amp;&amp;amp; cd modded-nanogpt
sudo docker build -t modded-nanogpt .
sudo docker run -it --rm --gpus all -v $(pwd):/modded-nanogpt modded-nanogpt python data/cached_fineweb10B.py 8
sudo docker run -it --rm --gpus all -v $(pwd):/modded-nanogpt modded-nanogpt sh run.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To get an interactive docker, you can use&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo docker run -it --rm --gpus all -v $(pwd):/modded-nanogpt modded-nanogpt bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;World record history&lt;/h2&gt; 
&lt;p&gt;The following is the historical progression of world speed records for the following competitive task:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;Train a neural network to â‰¤3.28 validation loss on FineWeb using 8x NVIDIA H100s.&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Note: The 3.28 target was selected to match &lt;a href="https://github.com/karpathy/llm.c/discussions/481"&gt;Andrej Karpathy's GPT-2 (small) reproduction&lt;/a&gt;.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;#&lt;/th&gt; 
   &lt;th&gt;Record time&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Date&lt;/th&gt; 
   &lt;th&gt;Log&lt;/th&gt; 
   &lt;th&gt;Contributors&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;45 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/karpathy/llm.c/discussions/481"&gt;llm.c baseline&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;05/28/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-10-13_llmc/main.log"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@karpathy, llm.c contributors&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;31.4 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1798863559243513937"&gt;Tuned learning rate &amp;amp; rotary embeddings&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;06/06/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-06-06_AdamW/f66d43d7-e449-4029-8adf-e8537bab49ea.log"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;24.9 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1842300916864844014"&gt;Introduced the Muon optimizer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;10/04/24&lt;/td&gt; 
   &lt;td&gt;none&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0, @jxbz&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;22.3 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1844820919061287009"&gt;Muon improvements&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;10/11/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-10-10_Muon/eb5659d0-fb6a-49e5-a311-f1f89412f726.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0, @bozavlado&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;15.2 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1845865698532450646"&gt;Pad embeddings, ReLUÂ², zero-init projections, QK-norm&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;10/14/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@Grad62304977, @kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;6&lt;/td&gt; 
   &lt;td&gt;13.1 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1847291684016783746"&gt;Distributed the overhead of Muon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;10/18/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-10-17_DistributedMuon/22d24867-eb5a-4fcc-ae2c-263d0277dfd1.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;7&lt;/td&gt; 
   &lt;td&gt;12.0 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1847358578686152764"&gt;Upgraded PyTorch 2.5.0&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;10/18/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-10-18_PyTorch25/d4bfb25f-688d-4da5-8743-33926fad4842.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;10.8 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1853188916704387239"&gt;Untied embedding and head&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11/03/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-11-03_UntieEmbed/d6b50d71-f419-4d26-bb39-a60d55ae7a04.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@Grad62304977, @kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;9&lt;/td&gt; 
   &lt;td&gt;8.2 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1854296101303800108"&gt;Value and embedding skip connections, momentum warmup, logit softcap&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11/06/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-11-06_ShortcutsTweaks/dd7304a6-cc43-4d5e-adb8-c070111464a1.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@Grad62304977, @kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;7.8 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1855267054774865980"&gt;Bfloat16 activations&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11/08/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-11-08_CastBf16/a833bed8-2fa8-4cfe-af05-58c1cc48bc30.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;11&lt;/td&gt; 
   &lt;td&gt;7.2 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1856053121103093922"&gt;U-net pattern skip connections &amp;amp; double lr&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11/10/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-11-10_UNetDoubleLr/c87bb826-797b-4f37-98c7-d3a5dad2de74.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@brendanh0gan&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td&gt;5.03 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1859331370268623321"&gt;1024-ctx dense causal attention â†’ 64K-ctx FlexAttention&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11/19/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-11-19_FlexAttention/8384493d-dba9-4991-b16b-8696953f5e6d.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@KoszarskyB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;13&lt;/td&gt; 
   &lt;td&gt;4.66 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/hi_tysam/status/1860851011797053450"&gt;Attention window warmup&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11/24/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-11-24_WindowWarmup/cf9e4571-c5fc-4323-abf3-a98d862ec6c8.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@fernbear.bsky.social&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14&lt;/td&gt; 
   &lt;td&gt;4.41 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/KoszarskyB/status/1864746625572257852"&gt;Value Embeddings&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/04/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-12-04_ValueEmbed"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@KoszarskyB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td&gt;3.95 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/YouJiacheng/status/1865761473886347747"&gt;U-net pattern value embeddings, assorted code optimizations&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/08/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-12-08_UNetValueEmbedsTweaks"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@leloykun, @YouJiacheng&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;3.80 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/YouJiacheng/status/1866734331559071981"&gt;Split value embeddings, block sliding window, separate block mask&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/10/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-12-10_MFUTweaks"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@YouJiacheng&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;3.57 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/YouJiacheng/status/1868938024731787640"&gt;Sparsify value embeddings, improve rotary embeddings, drop an attn layer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/17/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-12-17_SparsifyEmbeds"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@YouJiacheng&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;3.4 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1876048851158880624"&gt;Lower logit softcap from 30 to 15&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/04/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-01-04_SoftCap/31d6c427-f1f7-4d8a-91be-a67b5dcd13fd.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@KoszarskyB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;19&lt;/td&gt; 
   &lt;td&gt;3.142 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/YouJiacheng/status/1878827972519772241"&gt;FP8 head, offset logits, lr decay to 0.1 instead of 0.0&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/13/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-01-13_Fp8LmHead/c51969c2-d04c-40a7-bcea-c092c3c2d11a.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@YouJiacheng&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;20&lt;/td&gt; 
   &lt;td&gt;2.992 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/leloykun/status/1880301753213809016"&gt;Merged QKV weights, long-short attention, attention scale, lower Adam epsilon, batched Muon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/16/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-01-16_Sub3Min/1d3bd93b-a69e-4118-aeb8-8184239d7566.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@leloykun, @fernbear.bsky.social, @YouJiacheng, @brendanh0gan, @scottjmaddox, @Grad62304977&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;21&lt;/td&gt; 
   &lt;td&gt;2.933 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/leloykun/status/1885640350368420160"&gt;Reduced batch size&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/26/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-01-26_BatchSize/c44090cc-1b99-4c95-8624-38fb4b5834f9.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@leloykun&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;21&lt;/td&gt; 
   &lt;td&gt;2.997 minutes&lt;/td&gt; 
   &lt;td&gt;21st record with new timing&lt;/td&gt; 
   &lt;td&gt;02/01/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-02-01_RuleTweak/eff63a8c-2f7e-4fc5-97ce-7f600dae0bc7.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;not a new record, just re-timing #21 with the &lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/#timing-change-after-record-21"&gt;updated rules&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;21&lt;/td&gt; 
   &lt;td&gt;3.014 minutes&lt;/td&gt; 
   &lt;td&gt;21st record with latest torch&lt;/td&gt; 
   &lt;td&gt;05/24/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-05-24_StableTorch/89d9f224-3b01-4581-966e-358d692335e0.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;not a new record, just re-timing #21 with latest torch&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;22&lt;/td&gt; 
   &lt;td&gt;2.990 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/KonstantinWille/status/1927137223238909969"&gt;Faster gradient all-reduce&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;05/24/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-05-24_FasterReduce/23f40b75-06fb-4c3f-87a8-743524769a35.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad; The Enigma project&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;23&lt;/td&gt; 
   &lt;td&gt;2.979 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1927460573098262616"&gt;Overlap computation and gradient communication&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;05/25/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-05-25_EvenFasterReduce/6ae86d05-5cb2-4e40-a512-63246fd08e45.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@ryanyang0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;24&lt;/td&gt; 
   &lt;td&gt;2.966 minutes&lt;/td&gt; 
   &lt;td&gt;Replace gradient all_reduce with reduce_scatter&lt;/td&gt; 
   &lt;td&gt;05/30/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-05-30_noallreduce/8054c239-3a18-499e-b0c8-dbd27cb4b3ab.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@vagrawal&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;2.896 minutes&lt;/td&gt; 
   &lt;td&gt;Upgrade PyTorch to 2.9.0.dev20250713+cu126&lt;/td&gt; 
   &lt;td&gt;07/13/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-07-13_UpgradeTorch190/692f80e0-5e64-4819-97d4-0dc83b7106b9.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td&gt;2.863 minutes&lt;/td&gt; 
   &lt;td&gt;Align training batch starts with EoS, increase cooldown frac to .45&lt;/td&gt; 
   &lt;td&gt;07/13/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-07-12_BosAlign/c1fd8a38-bb9f-45c4-8af0-d37f70c993f3.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;27&lt;/td&gt; 
   &lt;td&gt;2.817 minutes&lt;/td&gt; 
   &lt;td&gt;Transpose one of the MLP matrices + add Triton kernel for symmetric matmul&lt;/td&gt; 
   &lt;td&gt;07/18/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-07-18_TritonMuon/record.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/109"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@byronxu99&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;28&lt;/td&gt; 
   &lt;td&gt;2.812 minutes&lt;/td&gt; 
   &lt;td&gt;Sparse attention gate&lt;/td&gt; 
   &lt;td&gt;08/23/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-08-23_SparseAttnGate/020630eb-2191-4ba2-9ee4-4cdc94316943.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/117"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;29&lt;/td&gt; 
   &lt;td&gt;2.731 minutes&lt;/td&gt; 
   &lt;td&gt;Flash Attention 3, 2048 max_doc_len, update ws schedule&lt;/td&gt; 
   &lt;td&gt;09/03/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-03_FA3/44fc1276-0510-4961-92c0-730c65e5feba.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/118"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@varunneal&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;30&lt;/td&gt; 
   &lt;td&gt;2.717 minutes&lt;/td&gt; 
   &lt;td&gt;Drop first MLP layer&lt;/td&gt; 
   &lt;td&gt;09/05/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-05_SkipMLPBlocks/07e7ae76-b7d0-4481-b149-01e7d81b5ad4.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/120"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@EmelyanenkoK&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;31&lt;/td&gt; 
   &lt;td&gt;2.656 minutes&lt;/td&gt; 
   &lt;td&gt;Dynamically incorporate YaRN during training and validation&lt;/td&gt; 
   &lt;td&gt;09/10/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-10_Yarn/0ecdb695-510b-4c3b-b030-09861a162ce8.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/122"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;32&lt;/td&gt; 
   &lt;td&gt;2.625 minutes&lt;/td&gt; 
   &lt;td&gt;Optimize distributed training, improve skip connection gating, and enhance bfloat16 usage&lt;/td&gt; 
   &lt;td&gt;09/11/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-11_VectSigmoidBFloat16/0d0d9882-c34f-4d82-b961-a17d5659c988.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/125"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@bernard24 &amp;amp; hiverge.ai&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;33&lt;/td&gt; 
   &lt;td&gt;2.565 minutes&lt;/td&gt; 
   &lt;td&gt;Asynchronously fetch and index data batches, extend final layer attention window for validation&lt;/td&gt; 
   &lt;td&gt;09/15/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-15_AsyncDataLoadAttnFinalWindow/25db37c7-2bab-4ef4-ae63-d593590ef823.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/127"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;34&lt;/td&gt; 
   &lt;td&gt;2.547 minutes&lt;/td&gt; 
   &lt;td&gt;Smear token embeddings 1 position forward&lt;/td&gt; 
   &lt;td&gt;09/18/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-18_Smear/18a1e5c7-947e-479d-bc3a-a57a61a98fc9.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/130"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;35&lt;/td&gt; 
   &lt;td&gt;2.527 minutes&lt;/td&gt; 
   &lt;td&gt;Drop first attn layer, extend all long windows for validation, update schedule&lt;/td&gt; 
   &lt;td&gt;09/21/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-21_DropAttn/01fc4a96-f2a0-47a1-8a6a-c7d10bac99fe.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/131"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;36&lt;/td&gt; 
   &lt;td&gt;2.495 minutes&lt;/td&gt; 
   &lt;td&gt;MuonCustomSizing, perform mlp and attn reduce scatter in shared call&lt;/td&gt; 
   &lt;td&gt;09/23/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-23_MuonCustomSizing/b067b4ac-72a6-4436-a6f8-ea51c1efeef3.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/132"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;37&lt;/td&gt; 
   &lt;td&gt;2.483 minutes&lt;/td&gt; 
   &lt;td&gt;Compute cross entropy in BF16 during training&lt;/td&gt; 
   &lt;td&gt;09/27/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-27_BF16CE/08c0770f-17fc-44cd-971d-734a7a28a3e3.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/133"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@GusarichOnX&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;38&lt;/td&gt; 
   &lt;td&gt;2.476 minutes&lt;/td&gt; 
   &lt;td&gt;Polar Express, replacement for Newton-Schulz&lt;/td&gt; 
   &lt;td&gt;09/29/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-29_PolarExpress/0e3f0af5-ad08-47a6-813d-0c709b50d422.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/134"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@varunneal&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;39&lt;/td&gt; 
   &lt;td&gt;2.447 minutes&lt;/td&gt; 
   &lt;td&gt;Only update Adam params every other step, reduce batch size&lt;/td&gt; 
   &lt;td&gt;09/30/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-30_CustomBatching/40b101b1-77ea-45ea-a089-1d3a647daa22.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/136"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;40&lt;/td&gt; 
   &lt;td&gt;2.358 minutes&lt;/td&gt; 
   &lt;td&gt;Backout, misc hyperparameter tuning, optimize lambda padding&lt;/td&gt; 
   &lt;td&gt;10/04/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-10-04_Backout/514e7581-fbd4-4338-a3e4-e556f9c958ce.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/140"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;41&lt;/td&gt; 
   &lt;td&gt;2.345 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2510.05491"&gt;NorMuon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;10/24/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-10-24_NorMuon/088a77ee-9b67-475a-bbb9-3e92e4698799.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/144"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@li_zichong&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;42&lt;/td&gt; 
   &lt;td&gt;2.313 minutes&lt;/td&gt; 
   &lt;td&gt;Update NorMuon LR, Step Logic&lt;/td&gt; 
   &lt;td&gt;10/27/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-10-27_FixMuonLR/14afd380-d3d9-48d7-ad23-4c13cb96754b.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/146"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@varunneal&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Rules&lt;/h2&gt; 
&lt;p&gt;The only rules are that new records must:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Not modify the train or validation data pipelines. (You can change the batch size, sequence length, attention structure etc.; just don't change the underlying streams of tokens.)&lt;/li&gt; 
 &lt;li&gt;Attain â‰¤3.28 mean val loss. (Due to inter-run variance, submissions must provide enough run logs to attain a statistical significance level of p&amp;lt;0.01 that their mean val loss is â‰¤3.28. Example code to compute p-value can be found &lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-01-04_SoftCap#softer-softcap"&gt;here&lt;/a&gt;. For submissions which improve speed by optimizing the systems performance, without touching the ML, this requirement is waived.)&lt;/li&gt; 
 &lt;li&gt;Not use any extra &lt;code&gt;torch._inductor.config&lt;/code&gt; or &lt;code&gt;torch.compile&lt;/code&gt; flags. (These can save a few seconds, but they can also make compilation take &amp;gt;30min. This rule was introduced after the 21st record.)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: &lt;code&gt;torch._inductor.config.coordinate_descent_tuning&lt;/code&gt; is allowed for GPT-2 Medium track (a.k.a. 2.92 track).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Other than that, anything and everything is fair game!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/KellerJordan/modded-nanogpt/discussions/23?sort=new#discussioncomment-12109560"&gt;further clarifications&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Comment on the target metric&lt;/h3&gt; 
&lt;p&gt;The target metric is &lt;em&gt;cross-entropy loss on the FineWeb val set&lt;/em&gt;. To speak mathematically, the goal of the speedrun is *to obtain a probability model of language which assigns a probability of at least &lt;code&gt;math.exp(-3.28 * 10485760)&lt;/code&gt; to the first 10,485,760 tokens of the FineWeb valset. Hence, e.g., we allow evaluation at any sequence length, so long as we still have a valid probability model of language.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Timing change after record 21&lt;/h3&gt; 
&lt;p&gt;After the 21st record, we made two changes to the timing. First, there used to be an initial "grace period" of 10 untimed steps to allow kernel warmup. We replaced this with an explicit kernel-warmup section which is untimed and uses dummy data. This results in an extra runtime of 850ms from the 10 extra timed steps. Second, we banned the use of &lt;code&gt;torch._inductor.config.coordinate_descent_tuning&lt;/code&gt;. This saves ~25min of untimed pre-run compilation, but results in an extra runtime of ~3s.&lt;/p&gt; 
&lt;!--Note: The original llm.c baseline is intended to be closer to a replication of GPT-2 than to an optimized LLM training.
So it's no surprise that there is room to improve; as @karpathy has said, 'llm.c still has a lot of pending optimizations.'
In addition, many of the techniques used in these records are completely standard, such as rotary embeddings.
The goal of this benchmark/speedrun is simply to find out which techniques actually work, and maybe come up with some new ones.--&gt; 
&lt;!--The goal of this benchmark is simply to find out all the techniques which actually work, because I'm going crazy reading all these
LLM training papers
which claim a huge benefit but then use their own idiosyncratic non-competitive benchmark and therefore no one in the community has any idea if it's legit for months.--&gt; 
&lt;!--[LLM](https://arxiv.org/abs/2305.14342) [training](https://arxiv.org/abs/2402.17764) [papers](https://arxiv.org/abs/2410.01131)--&gt; 
&lt;!--I mean hello??? We're in a completely empirical field; it is insane to not have a benchmark. Ideally everyone uses the same LLM training benchmark,
and then reviewing LLM training papers becomes as simple as checking if they beat the benchmark. It's not like this would be unprecedented, that's how things
were in the ImageNet days.
The only possible 'benefit' I can think of for any empirical field to abandon benchmarks is that it would make it easier to publish false results. Oh, I guess that's why it happened.
Hilarious to think about how, in the often-commented-upon and ongoing collapse of the peer review system, people blame the *reviewers* --
yeah, those guys doing free labor who everyone constantly musters all of their intelligence to lie to, it's *their* fault! My bad, you caught me monologuing.--&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Important note about records 22-25&lt;/h3&gt; 
&lt;p&gt;Thanks to the statistical testing of &lt;a href="https://www.github.com/agrawal"&gt;@agrawal&lt;/a&gt; (holder of the 24th record), we have learned that records 23, 24, and in all likelihood 22 and 25, actually attain a mean loss of 3.281, which is slightly above the 3.28 target. Therefore if we were to completely adhere to the speedrun rules, we would have to deny that these are valid records. However, we have decided to leave them in place as valid, because of the following two reasons: (a) the extra loss is most likely my (@kellerjordan0) own fault rather than that of the records, and (b) it is most likely easily addressable.&lt;/p&gt; 
&lt;p&gt;Here's what happened: Records #22 to #25 each change only the systems/implementation of the speedrun. Therefore, the requirement to do statistical testing to confirm they hit the target was waived, since in theory they should have hit it automatically, by virtue of the fact that they didn't touch the ML (i.e., they didn't change the architecture, learning rate, etc.).&lt;/p&gt; 
&lt;p&gt;So if these records shouldn't have changed the ML, what explains the regression in val loss? We think that most likely, the answer is that this regression was indeed not introduced by any of these records. Instead, it was probably caused by my own non-record in which I retimed record #21 with newest torch, because in this non-record I also changed the constants used to cast the lm_head to fp8. I thought that this change should cause only a (small) strict improvement, but apparently that was not the case.&lt;/p&gt; 
&lt;p&gt;Therefore, it is probable that each of records #22-25 could be easily made fully valid by simply reverting the change I made to those constants. Therefore they shall be upheld as valid records.&lt;/p&gt; 
&lt;p&gt;For the future, fortunately record #26 brought the speedrun back into the green in terms of &amp;lt;3.28 loss, so (with high p-value) it should be in a good state now.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Notable attempts &amp;amp; forks&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Notable runs:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://x.com/alexjc/status/1881410039639863622"&gt;@alexjc's 01/20/2025 2.77-minute TokenMonster-based record&lt;/a&gt;. This record is technically outside the rules of the speedrun, since we specified that the train/val tokens must be kept fixed. However, it's very interesting, and worth including. The run is not more data-efficient; rather, the speedup comes from the improved tokenizer allowing the vocabulary size to be reduced (nearly halved!) while preserving the same bytes-per-token, which saves lots of parameters and FLOPs in the head and embeddings.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Notable forks:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/BlinkDL/modded-nanogpt-rwkv"&gt;https://github.com/BlinkDL/modded-nanogpt-rwkv&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nikhilvyas/modded-nanogpt-SOAP"&gt;https://github.com/nikhilvyas/modded-nanogpt-SOAP&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Speedrun track 2: GPT-2 Medium&lt;/h2&gt; 
&lt;p&gt;The target loss for this track is lowered from 3.28 to 2.92, as per Andrej Karpathy's 350M-parameter llm.c baseline. This baseline generates a model with performance similar to the original GPT-2 Medium, whereas the first track's baseline generates a model on par with GPT-2 Small. All other rules remain the same.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: &lt;code&gt;torch._inductor.config.coordinate_descent_tuning&lt;/code&gt; is turned on after the record 6 (*).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;#&lt;/th&gt; 
   &lt;th&gt;Record time&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Date&lt;/th&gt; 
   &lt;th&gt;Log&lt;/th&gt; 
   &lt;th&gt;Contributors&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;5.8 hours&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/karpathy/llm.c/discussions/481"&gt;llm.c baseline (350M parameters)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;05/28/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-01-18/main.log"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@karpathy, llm.c contributors&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;29.3 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1881959719012847703"&gt;Initial record based on scaling up the GPT-2 small track speedrun&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/18/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-01-18/241dd7a7-3d76-4dce-85a4-7df60387f32a.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;28.1 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1888320690543284449"&gt;Added standard weight decay&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;02/08/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-02-08_WeightDecay/b01743db-605c-4326-b5b1-d388ee5bebc5.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;27.7 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/leloykun/status/1892793848163946799"&gt;Tuned Muon Newton-Schulz coefficients&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;02/14/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-02-14_OptCoeffs/1baa66b2-bff7-4850-aced-d63885ffb4b6.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@leloykun&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;27.2 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-03-06_LongerCooldown/779c041a-2a37-45d2-a18b-ec0f223c2bb7.txt"&gt;Increased learning rate cooldown phase duration&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;03/06/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-03-06_LongerCooldown/779c041a-2a37-45d2-a18b-ec0f223c2bb7.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@YouJiacheng&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;6&lt;/td&gt; 
   &lt;td&gt;25.95 minutes*&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/YouJiacheng/status/1905861218138804534"&gt;2x MLP wd, qkv norm, all_reduce/opt.step() overlap, optimized skip pattern&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;03/25/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-03-25_ArchOptTweaks/train_gpt-20250329.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@YouJiacheng&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;7&lt;/td&gt; 
   &lt;td&gt;25.29 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/YouJiacheng/status/1912570883878842527"&gt;Remove FP8 head; ISRU logits softcap; New sharded mixed precision Muon; merge weights&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;04/16/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-04-16_Record7/223_3310d0b1-b24d-48ee-899f-d5c2a254a195.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@YouJiacheng&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;24.50 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/jadenj3o/status/1914893086276169754"&gt;Cubic sliding window size schedule, 2Ã— max window size (24.84 minutes)&lt;/a&gt; &lt;a href="https://x.com/YouJiacheng/status/1915667616913645985"&gt;24.5min repro&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;04/22/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-04-22_Record8/075_640429f2-e726-4e83-aa27-684626239ffc.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@jadenj3o&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Q: What is the point of NanoGPT speedrunning?&lt;/h3&gt; 
&lt;p&gt;A: The officially stated goal of NanoGPT speedrunning is as follows: &lt;code&gt;gotta go fast&lt;/code&gt;. But for something a little more verbose involving an argument for good benchmarking, here's some kind of manifesto, adorned with a blessing from the master. &lt;a href="https://x.com/karpathy/status/1846790537262571739"&gt;https://x.com/karpathy/status/1846790537262571739&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Q: What makes "NanoGPT speedrunning" not just another idiosyncratic benchmark?&lt;/h3&gt; 
&lt;p&gt;A: Because it is a &lt;em&gt;competitive&lt;/em&gt; benchmark. In particular, if you attain a new speed record (using whatever method you want), there is an open invitation for you to post that record (on arXiv or X) and thereby vacuum up all the clout for yourself. I will even help you do it by reposting you as much as I can.&lt;/p&gt; 
&lt;!--On the contrary, for example, the benchmark used in the [Sophia](https://arxiv.org/abs/2305.14342) paper does *not* have this property.
There is no such open invitation for anyone to compete on the benchmark they used. In particular, if, for a random and definitely not weirdly specific example, you happen to find better AdamW hyperparameters for their training setup than
the ones they used which significantly close the gap between AdamW and their proposed optimizer,
then there is no clear path for you to publish that result in *any* form.
You could try posting it on X.com, but then you would be risking being perceived as aggressive/confrontational, which is *not a good look* in this racket.
So if you're rational, the result probably just dies with you and no one else learns anything
(unless you're in a frontier lab, in which case you can do a nice internal writeup. Boy I'd love to get my hands on those writeups).--&gt; 
&lt;p&gt;&lt;a href="https://www.argmin.net/p/too-much-information"&gt;"Artificial intelligence advances by inventing games and gloating to goad others to play" - Professor Ben Recht&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Q: NanoGPT speedrunning is cool and all, but meh it probably won't scale and is just overfitting to val loss&lt;/h3&gt; 
&lt;p&gt;A: This is hard to refute, since "at scale" is an infinite category (what if the methods stop working only for &amp;gt;100T models?), making it impossible to fully prove. Also, I would agree that some of the methods used in the speedrun are unlikely to scale, particularly those which &lt;em&gt;impose additional structure&lt;/em&gt; on the network, such as logit softcapping. But if the reader cares about 1.5B models, they might be convinced by this result:&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Straightforwardly scaling up the speedrun (10/18/24 version) to 1.5B parameters yields a model with GPT-2 (1.5B)-level HellaSwag performance 2.5x more cheaply than &lt;a href="https://github.com/karpathy/llm.c/discussions/677"&gt;@karpathy's baseline&lt;/a&gt; ($233 instead of $576):&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/img/nanogpt_speedrun51.png" alt="" /&gt; [&lt;a href="https://github.com/KellerJordan/modded-nanogpt/raw/master/records/track_1_short/2024-10-20_ScaleUp1B/ad8d7ae5-7b2d-4ee9-bc52-f912e9174d7a.txt"&gt;reproducible log&lt;/a&gt;] &lt;img src="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/img/nanogpt_speedrun52.png" alt="" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;&lt;a href="https://github.com/KellerJordan/Muon"&gt;Muon optimizer&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Muon is defined as follows:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/img/algo_optimizer.png" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;Where NewtonSchulz5 is the following Newton-Schulz iteration [2, 3], which approximately replaces &lt;code&gt;G&lt;/code&gt; with &lt;code&gt;U @ V.T&lt;/code&gt; where &lt;code&gt;U, S, V = G.svd()&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@torch.compile
def zeroth_power_via_newtonschulz5(G, steps=5, eps=1e-7):
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16() / (G.norm() + eps)
    if G.size(0) &amp;gt; G.size(1):
        X = X.T 
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A
        X = a * X + B @ X
    if G.size(0) &amp;gt; G.size(1):
        X = X.T 
    return X.to(G.dtype)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For this training scenario, Muon has the following favorable properties:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Lower memory usage than Adam&lt;/li&gt; 
 &lt;li&gt;~1.5x better sample-efficiency&lt;/li&gt; 
 &lt;li&gt;&amp;lt;2% wallclock overhead&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Provenance&lt;/h3&gt; 
&lt;p&gt;Many of the choices made to generate this optimizer were obtained experimentally by our pursuit of &lt;a href="https://github.com/KellerJordan/cifar10-airbench"&gt;CIFAR-10 speedrunning&lt;/a&gt;. In particular, we experimentally obtained the following practices:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Using Nesterov momentum inside the update, with orthogonalization applied after momentum.&lt;/li&gt; 
 &lt;li&gt;Using a specifically quintic Newton-Schulz iteration as the method of orthogonalization.&lt;/li&gt; 
 &lt;li&gt;Using non-convergent coefficients for the quintic polynomial in order to maximize slope at zero, and thereby minimize the number of necessary Newton-Schulz iterations. It turns out that the variance doesn't actually matter that much, so we end up with a quintic that rapidly converges to the range 0.68, 1.13 upon repeated application, rather than converging more slowly to 1.&lt;/li&gt; 
 &lt;li&gt;Running the Newton-Schulz iteration in bfloat16 (whereas Shampoo implementations often depend on inverse-pth-roots run in fp32 or fp64).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Our use of a Newton-Schulz iteration for orthogonalization traces to &lt;a href="https://arxiv.org/abs/2409.20325"&gt;Bernstein &amp;amp; Newhouse (2024)&lt;/a&gt;, who suggested it as a way to compute Shampoo [5, 6] preconditioners, and theoretically explored Shampoo without preconditioner accumulation. In particular, Jeremy Bernstein @jxbz sent us the draft, which caused us to experiment with various Newton-Schulz iterations as the orthogonalization method for this optimizer. If we had used SVD instead of a Newton-Schulz iteration, this optimizer would have been too slow to be useful. Bernstein &amp;amp; Newhouse also pointed out that Shampoo without preconditioner accumulation is equivalent to steepest descent in the spectral norm, and therefore Shampoo can be thought of as a way to smooth out spectral steepest descent. The proposed optimizer can be thought of as a second way of smoothing spectral steepest descent, with a different set of memory and runtime tradeoffs compared to Shampoo.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Running on fewer GPUs&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;To run experiments on fewer GPUs, simply modify &lt;code&gt;run.sh&lt;/code&gt; to have a different &lt;code&gt;--nproc_per_node&lt;/code&gt;. This should not change the behavior of the training.&lt;/li&gt; 
 &lt;li&gt;If you're running out of memory, you may need to reduce the sequence length for FlexAttention (which does change the training. see &lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/38"&gt;here&lt;/a&gt; for a guide)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;References&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2406.17557"&gt;Guilherme Penedo et al. "The fineweb datasets: Decanting the web for the finest text data at scale." arXiv preprint arXiv:2406.17557 (2024).&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Nicholas J. Higham. Functions of Matrices. Society for Industrial and Applied Mathematics (2008). Equation 5.22.&lt;/li&gt; 
 &lt;li&gt;GÃƒÂ¼nther Schulz. Iterative Berechnung der reziproken Matrix. Z. Angew. Math. Mech., 13:57Ã¢Â€Â“59 (1933).&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2409.20325"&gt;Jeremy Bernstein and Laker Newhouse. "Old Optimizer, New Norm: An Anthology." arxiv preprint arXiv:2409.20325 (2024).&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/1802.09568"&gt;Vineet Gupta, Tomer Koren, and Yoram Singer. "Shampoo: Preconditioned stochastic tensor optimization." International Conference on Machine Learning. PMLR, 2018.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2002.09018"&gt;Rohan Anil et al. "Scalable second order optimization for deep learning." arXiv preprint arXiv:2002.09018 (2020).&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2405.18392"&gt;Alexander HÃƒÂ¤gele et al. "Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations." arXiv preprint arXiv:2405.18392 (2024).&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2410.17897"&gt;Zhanchao Zhou et al. "Value Residual Learning For Alleviating Attention Concentration In Transformers." arXiv preprint arXiv:2410.17897 (2024).&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2408.00118"&gt;Team, Gemma, et al. "Gemma 2: Improving open language models at a practical size." arXiv preprint arXiv:2408.00118 (2024).&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"&gt;Alec Radford et al. "Language models are unsupervised multitask learners." OpenAI blog 1.8 (2019).&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@misc{modded_nanogpt_2024,
  author       = {Keller Jordan and Jeremy Bernstein and Brendan Rappazzo and
                  @fernbear.bsky.social and Boza Vlado and You Jiacheng and
                  Franz Cesista and Braden Koszarsky and @Grad62304977},
  title        = {modded-nanogpt: Speedrunning the NanoGPT baseline},
  year         = {2024},
  url          = {https://github.com/KellerJordan/modded-nanogpt}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;img src="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/img/dofa.jpg" alt="itsover_wereback" style="width:100%;" /&gt;</description>
    </item>
    
    <item>
      <title>langchain-ai/deepagents</title>
      <link>https://github.com/langchain-ai/deepagents</link>
      <description>&lt;p&gt;Deepagents is an agent harness built on langchain and langgraph. Deep agents are equipped with a planning tool, a filesystem backend, and the ability to spawn subagents - making them well-equipped to handle complex agentic tasks.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ğŸ§ ğŸ¤–Deep Agents&lt;/h1&gt; 
&lt;p&gt;Using an LLM to call tools in a loop is the simplest form of an agent. This architecture, however, can yield agents that are â€œshallowâ€ and fail to plan and act over longer, more complex tasks.&lt;/p&gt; 
&lt;p&gt;Applications like â€œDeep Researchâ€, "Manus", and â€œClaude Codeâ€ have gotten around this limitation by implementing a combination of four things: a &lt;strong&gt;planning tool&lt;/strong&gt;, &lt;strong&gt;sub agents&lt;/strong&gt;, access to a &lt;strong&gt;file system&lt;/strong&gt;, and a &lt;strong&gt;detailed prompt&lt;/strong&gt;.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/langchain-ai/deepagents/master/deep_agents.png" alt="deep agent" width="600" /&gt; 
&lt;p&gt;&lt;code&gt;deepagents&lt;/code&gt; is a Python package that implements these in a general purpose way so that you can easily create a Deep Agent for your application. For a full overview and quickstart of &lt;code&gt;deepagents&lt;/code&gt;, the best resource is our &lt;a href="https://docs.langchain.com/oss/python/deepagents/overview"&gt;docs&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Acknowledgements: This project was primarily inspired by Claude Code, and initially was largely an attempt to see what made Claude Code general purpose, and make it even more so.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# pip
pip install deepagents

# uv
uv add deepagents

# poetry
poetry add deepagents
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;(To run the example below, you will need to &lt;code&gt;pip install tavily-python&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;Make sure to set &lt;code&gt;TAVILY_API_KEY&lt;/code&gt; in your environment. You can generate one &lt;a href="https://www.tavily.com/"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
from typing import Literal
from tavily import TavilyClient
from deepagents import create_deep_agent

tavily_client = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])

# Web search tool
def internet_search(
    query: str,
    max_results: int = 5,
    topic: Literal["general", "news", "finance"] = "general",
    include_raw_content: bool = False,
):
    """Run a web search"""
    return tavily_client.search(
        query,
        max_results=max_results,
        include_raw_content=include_raw_content,
        topic=topic,
    )


# System prompt to steer the agent to be an expert researcher
research_instructions = """You are an expert researcher. Your job is to conduct thorough research, and then write a polished report.

You have access to an internet search tool as your primary means of gathering information.

## `internet_search`

Use this to run an internet search for a given query. You can specify the max number of results to return, the topic, and whether raw content should be included.
"""

# Create the deep agent
agent = create_deep_agent(
    tools=[internet_search],
    system_prompt=research_instructions,
)

# Invoke the agent
result = agent.invoke({"messages": [{"role": "user", "content": "What is langgraph?"}]})
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/langchain-ai/deepagents/master/examples/research/research_agent.py"&gt;examples/research/research_agent.py&lt;/a&gt; for a more complex example.&lt;/p&gt; 
&lt;p&gt;The agent created with &lt;code&gt;create_deep_agent&lt;/code&gt; is just a LangGraph graph - so you can interact with it (streaming, human-in-the-loop, memory, studio) in the same way you would any LangGraph agent.&lt;/p&gt; 
&lt;h2&gt;Core Capabilities&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Planning &amp;amp; Task Decomposition&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Deep Agents include a built-in &lt;code&gt;write_todos&lt;/code&gt; tool that enables agents to break down complex tasks into discrete steps, track progress, and adapt plans as new information emerges.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Context Management&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;File system tools (&lt;code&gt;ls&lt;/code&gt;, &lt;code&gt;read_file&lt;/code&gt;, &lt;code&gt;write_file&lt;/code&gt;, &lt;code&gt;edit_file&lt;/code&gt;, &lt;code&gt;glob&lt;/code&gt;, &lt;code&gt;grep&lt;/code&gt;) allow agents to offload large context to memory, preventing context window overflow and enabling work with variable-length tool results.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Subagent Spawning&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;A built-in &lt;code&gt;task&lt;/code&gt; tool enables agents to spawn specialized subagents for context isolation. This keeps the main agentâ€™s context clean while still going deep on specific subtasks.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Long-term Memory&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Extend agents with persistent memory across threads using LangGraphâ€™s Store. Agents can save and retrieve information from previous conversations.&lt;/p&gt; 
&lt;h2&gt;Customizing Deep Agents&lt;/h2&gt; 
&lt;p&gt;There are several parameters you can pass to &lt;code&gt;create_deep_agent&lt;/code&gt; to create your own custom deep agent.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;model&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;By default, &lt;code&gt;deepagents&lt;/code&gt; uses &lt;code&gt;"claude-sonnet-4-5-20250929"&lt;/code&gt;. You can customize this by passing any &lt;a href="https://python.langchain.com/docs/integrations/chat/"&gt;LangChain model object&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from langchain.chat_models import init_chat_model
from deepagents import create_deep_agent

model = init_chat_model("openai:gpt-4o")
agent = create_deep_agent(
    model=model,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;code&gt;system_prompt&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;Deep Agents come with a built-in system prompt. This is relatively detailed prompt that is heavily based on and inspired by &lt;a href="https://github.com/kn1026/cc/raw/main/claudecode.md"&gt;attempts&lt;/a&gt; to &lt;a href="https://github.com/asgeirtj/system_prompts_leaks/raw/main/Anthropic/claude-code.md"&gt;replicate&lt;/a&gt; Claude Code's system prompt. It was made more general purpose than Claude Code's system prompt. The default prompt contains detailed instructions for how to use the built-in planning tool, file system tools, and sub agents.&lt;/p&gt; 
&lt;p&gt;Each deep agent tailored to a use case should include a custom system prompt specific to that use case as well. The importance of prompting for creating a successful deep agent cannot be overstated.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from deepagents import create_deep_agent

research_instructions = """You are an expert researcher. Your job is to conduct thorough research, and then write a polished report.
"""

agent = create_deep_agent(
    system_prompt=research_instructions,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;code&gt;tools&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;Just like with tool-calling agents, you can provide a deep agent with a set of tools that it has access to.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
from typing import Literal
from tavily import TavilyClient
from deepagents import create_deep_agent

tavily_client = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])

def internet_search(
    query: str,
    max_results: int = 5,
    topic: Literal["general", "news", "finance"] = "general",
    include_raw_content: bool = False,
):
    """Run a web search"""
    return tavily_client.search(
        query,
        max_results=max_results,
        include_raw_content=include_raw_content,
        topic=topic,
    )

agent = create_deep_agent(
    tools=[internet_search]
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;code&gt;middleware&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;create_deep_agent&lt;/code&gt; is implemented with middleware that can be customized. You can provide additional middleware to extend functionality, add tools, or implement custom hooks.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from langchain_core.tools import tool
from deepagents import create_deep_agent
from langchain.agents.middleware import AgentMiddleware

@tool
def get_weather(city: str) -&amp;gt; str:
    """Get the weather in a city."""
    return f"The weather in {city} is sunny."

@tool
def get_temperature(city: str) -&amp;gt; str:
    """Get the temperature in a city."""
    return f"The temperature in {city} is 70 degrees Fahrenheit."

class WeatherMiddleware(AgentMiddleware):
  tools = [get_weather, get_temperature]

agent = create_deep_agent(
    model="anthropic:claude-sonnet-4-20250514",
    middleware=[WeatherMiddleware()]
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;code&gt;subagents&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;A main feature of Deep Agents is their ability to spawn subagents. You can specify custom subagents that your agent can hand off work to in the subagents parameter. Sub agents are useful for context quarantine (to help not pollute the overall context of the main agent) as well as custom instructions.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;subagents&lt;/code&gt; should be a list of dictionaries, where each dictionary follow this schema:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;class SubAgent(TypedDict):
    name: str
    description: str
    prompt: str
    tools: Sequence[BaseTool | Callable | dict[str, Any]]
    model: NotRequired[str | BaseChatModel]
    middleware: NotRequired[list[AgentMiddleware]]
    interrupt_on: NotRequired[dict[str, bool | InterruptOnConfig]]

class CompiledSubAgent(TypedDict):
    name: str
    description: str
    runnable: Runnable
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;SubAgent fields:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;name&lt;/strong&gt;: This is the name of the subagent, and how the main agent will call the subagent&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;description&lt;/strong&gt;: This is the description of the subagent that is shown to the main agent&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;prompt&lt;/strong&gt;: This is the prompt used for the subagent&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;tools&lt;/strong&gt;: This is the list of tools that the subagent has access to.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;model&lt;/strong&gt;: Optional model name or model instance.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;middleware&lt;/strong&gt; Additional middleware to attach to the subagent. See &lt;a href="https://docs.langchain.com/oss/python/langchain/middleware"&gt;here&lt;/a&gt; for an introduction into middleware and how it works with create_agent.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;interrupt_on&lt;/strong&gt; A custom interrupt config that specifies human-in-the-loop interactions for your tools.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;CompiledSubAgent fields:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;name&lt;/strong&gt;: This is the name of the subagent, and how the main agent will call the subagent&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;description&lt;/strong&gt;: This is the description of the subagent that is shown to the main agent&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;runnable&lt;/strong&gt;: A pre-built LangGraph graph/agent that will be used as the subagent&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Using SubAgent&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
from typing import Literal
from tavily import TavilyClient
from deepagents import create_deep_agent

tavily_client = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])

def internet_search(
    query: str,
    max_results: int = 5,
    topic: Literal["general", "news", "finance"] = "general",
    include_raw_content: bool = False,
):
    """Run a web search"""
    return tavily_client.search(
        query,
        max_results=max_results,
        include_raw_content=include_raw_content,
        topic=topic,
    )

research_subagent = {
    "name": "research-agent",
    "description": "Used to research more in depth questions",
    "system_prompt": "You are a great researcher",
    "tools": [internet_search],
    "model": "openai:gpt-4o",  # Optional override, defaults to main agent model
}
subagents = [research_subagent]

agent = create_deep_agent(
    model="anthropic:claude-sonnet-4-20250514",
    subagents=subagents
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Using CustomSubAgent&lt;/h4&gt; 
&lt;p&gt;For more complex use cases, you can provide your own pre-built LangGraph graph as a subagent:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Create a custom agent graph
custom_graph = create_agent(
    model=your_model,
    tools=specialized_tools,
    prompt="You are a specialized agent for data analysis..."
)

# Use it as a custom subagent
custom_subagent = CompiledSubAgent(
    name="data-analyzer",
    description="Specialized agent for complex data analysis tasks",
    runnable=custom_graph
)

subagents = [custom_subagent]

agent = create_deep_agent(
    model="anthropic:claude-sonnet-4-20250514",
    tools=[internet_search],
    system_prompt=research_instructions,
    subagents=subagents
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;code&gt;interrupt_on&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;A common reality for agents is that some tool operations may be sensitive and require human approval before execution. Deep Agents supports human-in-the-loop workflows through LangGraphâ€™s interrupt capabilities. You can configure which tools require approval using a checkpointer.&lt;/p&gt; 
&lt;p&gt;These tool configs are passed to our prebuilt &lt;a href="https://docs.langchain.com/oss/python/langchain/middleware#human-in-the-loop"&gt;HITL middleware&lt;/a&gt; so that the agent pauses execution and waits for feedback from the user before executing configured tools.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from langchain_core.tools import tool
from deepagents import create_deep_agent

@tool
def get_weather(city: str) -&amp;gt; str:
    """Get the weather in a city."""
    return f"The weather in {city} is sunny."

agent = create_deep_agent(
    model="anthropic:claude-sonnet-4-20250514",
    tools=[get_weather],
    interrupt_on={
        "get_weather": {
            "allowed_decisions": ["approve", "edit", "reject"]
        },
    }
)

&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Deep Agents Middleware&lt;/h2&gt; 
&lt;p&gt;Deep Agents are built with a modular middleware architecture. As a reminder, Deep Agents have access to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A planning tool&lt;/li&gt; 
 &lt;li&gt;A filesystem for storing context and long-term memories&lt;/li&gt; 
 &lt;li&gt;The ability to spawn subagents&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Each of these features is implemented as separate middleware. When you create a deep agent with &lt;code&gt;create_deep_agent&lt;/code&gt;, we automatically attach &lt;strong&gt;TodoListMiddleware&lt;/strong&gt;, &lt;strong&gt;FilesystemMiddleware&lt;/strong&gt; and &lt;strong&gt;SubAgentMiddleware&lt;/strong&gt; to your agent.&lt;/p&gt; 
&lt;p&gt;Middleware is a composable concept, and you can choose to add as many or as few middleware to an agent depending on your use case. That means that you can also use any of the aforementioned middleware independently!&lt;/p&gt; 
&lt;h3&gt;TodoListMiddleware&lt;/h3&gt; 
&lt;p&gt;Planning is integral to solving complex problems. If youâ€™ve used claude code recently, youâ€™ll notice how it writes out a To-Do list before tackling complex, multi-part tasks. Youâ€™ll also notice how it can adapt and update this To-Do list on the fly as more information comes in.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;TodoListMiddleware&lt;/strong&gt; provides your agent with a tool specifically for updating this To-Do list. Before, and while it executes a multi-part task, the agent is prompted to use the write_todos tool to keep track of what its doing, and what still needs to be done.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from langchain.agents import create_agent
from langchain.agents.middleware import TodoListMiddleware

# TodoListMiddleware is included by default in create_deep_agent
# You can customize it if building a custom agent
agent = create_agent(
    model="anthropic:claude-sonnet-4-20250514",
    # Custom planning instructions can be added via middleware
    middleware=[
        TodoListMiddleware(
            system_prompt="Use the write_todos tool to..."  # Optional: Custom addition to the system prompt
        ),
    ],
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;FilesystemMiddleware&lt;/h3&gt; 
&lt;p&gt;Context engineering is one of the main challenges in building effective agents. This can be particularly hard when using tools that can return variable length results (ex. web_search, rag), as long ToolResults can quickly fill up your context window. &lt;strong&gt;FilesystemMiddleware&lt;/strong&gt; provides four tools to your agent to interact with both short-term and long-term memory.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ls&lt;/strong&gt;: List the files in your filesystem&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;read_file&lt;/strong&gt;: Read an entire file, or a certain number of lines from a file&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;write_file&lt;/strong&gt;: Write a new file to your filesystem&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;edit_file&lt;/strong&gt;: Edit an existing file in your filesystem&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from langchain.agents import create_agent
from deepagents.middleware.filesystem import FilesystemMiddleware


# FilesystemMiddleware is included by default in create_deep_agent
# You can customize it if building a custom agent
agent = create_agent(
    model="anthropic:claude-sonnet-4-20250514",
    middleware=[
        FilesystemMiddleware(
            backend=..., # Optional: customize storage backend
            system_prompt="Write to the filesystem when...",  # Optional custom system prompt override
            custom_tool_descriptions={
                "ls": "Use the ls tool when...",
                "read_file": "Use the read_file tool to..."
            }  # Optional: Custom descriptions for filesystem tools
        ),
    ],
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;SubAgentMiddleware&lt;/h3&gt; 
&lt;p&gt;Handing off tasks to subagents is a great way to isolate context, keeping the context window of the main (supervisor) agent clean while still going deep on a task. The subagents middleware allows you supply subagents through a task tool.&lt;/p&gt; 
&lt;p&gt;A subagent is defined with a name, description, system prompt, and tools. You can also provide a subagent with a custom model, or with additional middleware. This can be particularly useful when you want to give the subagent an additional state key to share with the main agent.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from langchain_core.tools import tool
from langchain.agents import create_agent
from deepagents.middleware.subagents import SubAgentMiddleware


@tool
def get_weather(city: str) -&amp;gt; str:
    """Get the weather in a city."""
    return f"The weather in {city} is sunny."

agent = create_agent(
    model="claude-sonnet-4-20250514",
    middleware=[
        SubAgentMiddleware(
            default_model="claude-sonnet-4-20250514",
            default_tools=[],
            subagents=[
                {
                    "name": "weather",
                    "description": "This subagent can get weather in cities.",
                    "system_prompt": "Use the get_weather tool to get the weather in a city.",
                    "tools": [get_weather],
                    "model": "gpt-4.1",
                    "middleware": [],
                }
            ],
        )
    ],
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more complex use cases, you can also provide your own pre-built LangGraph graph as a subagent.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Create a custom LangGraph graph
def create_weather_graph():
    workflow = StateGraph(...)
    # Build your custom graph
    return workflow.compile()

weather_graph = create_weather_graph()

# Wrap it in a CompiledSubAgent
weather_subagent = CompiledSubAgent(
    name="weather",
    description="This subagent can get weather in cities.",
    runnable=weather_graph
)

agent = create_agent(
    model="anthropic:claude-sonnet-4-20250514",
    middleware=[
        SubAgentMiddleware(
            default_model="claude-sonnet-4-20250514",
            default_tools=[],
            subagents=[weather_subagent],
        )
    ],
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Sync vs Async&lt;/h2&gt; 
&lt;p&gt;Prior versions of deepagents separated sync and async agent factories.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;async_create_deep_agent&lt;/code&gt; has been folded in to &lt;code&gt;create_deep_agent&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;You should use &lt;code&gt;create_deep_agent&lt;/code&gt; as the factory for both sync and async agents&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;MCP&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;deepagents&lt;/code&gt; library can be ran with MCP tools. This can be achieved by using the &lt;a href="https://github.com/langchain-ai/langchain-mcp-adapters"&gt;Langchain MCP Adapter library&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; You will want to use &lt;code&gt;from deepagents import async_create_deep_agent&lt;/code&gt; to use the async version of &lt;code&gt;deepagents&lt;/code&gt;, since MCP tools are async&lt;/p&gt; 
&lt;p&gt;(To run the example below, will need to &lt;code&gt;pip install langchain-mcp-adapters&lt;/code&gt;)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from langchain_mcp_adapters.client import MultiServerMCPClient
from deepagents import create_deep_agent

async def main():
    # Collect MCP tools
    mcp_client = MultiServerMCPClient(...)
    mcp_tools = await mcp_client.get_tools()

    # Create agent
    agent = create_deep_agent(tools=mcp_tools, ....)

    # Stream the agent
    async for chunk in agent.astream(
        {"messages": [{"role": "user", "content": "what is langgraph?"}]},
        stream_mode="values"
    ):
        if "messages" in chunk:
            chunk["messages"][-1].pretty_print()

asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>hpcaitech/Open-Sora</title>
      <link>https://github.com/hpcaitech/Open-Sora</link>
      <description>&lt;p&gt;Open-Sora: Democratizing Efficient Video Production for All&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/readme/icon.png" width="250" /&gt; &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/hpcaitech/Open-Sora/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/hpcaitech/Open-Sora?style=social" /&gt;&lt;/a&gt; 
 &lt;a href="https://arxiv.org/abs/2503.09642v1"&gt;&lt;img src="https://img.shields.io/static/v1?label=Tech Report 2.0&amp;amp;message=Arxiv&amp;amp;color=red" /&gt;&lt;/a&gt; 
 &lt;a href="https://arxiv.org/abs/2412.20404"&gt;&lt;img src="https://img.shields.io/static/v1?label=Tech Report 1.2&amp;amp;message=Arxiv&amp;amp;color=red" /&gt;&lt;/a&gt; 
 &lt;a href="https://hpcaitech.github.io/Open-Sora/"&gt;&lt;img src="https://img.shields.io/badge/Gallery-View-orange?logo=&amp;amp;" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://discord.gg/kZakZzrSUT"&gt;&lt;img src="https://img.shields.io/badge/Discord-join-blueviolet?logo=discord&amp;amp;" /&gt;&lt;/a&gt; 
 &lt;a href="https://join.slack.com/t/colossalaiworkspace/shared_invite/zt-247ipg9fk-KRRYmUl~u2ll2637WRURVA"&gt;&lt;img src="https://img.shields.io/badge/Slack-ColossalAI-blueviolet?logo=slack&amp;amp;" /&gt;&lt;/a&gt; 
 &lt;a href="https://x.com/YangYou1991/status/1899973689460044010"&gt;&lt;img src="https://img.shields.io/badge/Twitter-Discuss-blue?logo=twitter&amp;amp;" /&gt;&lt;/a&gt; 
 &lt;a href="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/WeChat.png"&gt;&lt;img src="https://img.shields.io/badge/å¾®ä¿¡-å°åŠ©æ‰‹åŠ ç¾¤-green?logo=wechat&amp;amp;" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Open-Sora: Democratizing Efficient Video Production for All&lt;/h2&gt; 
&lt;p&gt;We design and implement &lt;strong&gt;Open-Sora&lt;/strong&gt;, an initiative dedicated to &lt;strong&gt;efficiently&lt;/strong&gt; producing high-quality video. We hope to make the model, tools and all details accessible to all. By embracing &lt;strong&gt;open-source&lt;/strong&gt; principles, Open-Sora not only democratizes access to advanced video generation techniques, but also offers a streamlined and user-friendly platform that simplifies the complexities of video generation. With Open-Sora, our goal is to foster innovation, creativity, and inclusivity within the field of content creation.&lt;/p&gt; 
&lt;p&gt;ğŸ¬ For a professional AI video-generation product, try &lt;a href="https://video-ocean.com/"&gt;Video Ocean&lt;/a&gt; â€” powered by a superior model.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://video-ocean.com/"&gt; &lt;img src="https://github.com/hpcaitech/public_assets/raw/main/colossalai/img/3.gif" width="850" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://hpc-ai.com/?utm_source=github&amp;amp;utm_medium=social&amp;amp;utm_campaign=promotion-opensora"&gt; &lt;img src="https://github.com/hpcaitech/public_assets/raw/main/colossalai/img/1.gif" width="850" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;!-- [[ä¸­æ–‡æ–‡æ¡£](/docs/zh_CN/README.md)] [[æ½æ™¨äº‘](https://cloud.luchentech.com/)|[OpenSoraé•œåƒ](https://cloud.luchentech.com/doc/docs/image/open-sora/)|[è§†é¢‘æ•™ç¨‹](https://www.bilibili.com/video/BV1ow4m1e7PX/?vd_source=c6b752764cd36ff0e535a768e35d98d2)] --&gt; 
&lt;h2&gt;ğŸ“° News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.12]&lt;/strong&gt; ğŸ”¥ We released &lt;strong&gt;Open-Sora 2.0&lt;/strong&gt; (11B). ğŸ¬ 11B model achieves &lt;a href="https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#evaluation"&gt;on-par performance&lt;/a&gt; with 11B HunyuanVideo &amp;amp; 30B Step-Video on ğŸ“VBench &amp;amp; ğŸ“ŠHuman Preference. ğŸ› ï¸ Fully open-source: checkpoints and training codes for training with only &lt;strong&gt;$200K&lt;/strong&gt;. &lt;a href="https://arxiv.org/abs/2503.09642v1"&gt;[report]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.02.20]&lt;/strong&gt; ğŸ”¥ We released &lt;strong&gt;Open-Sora 1.3&lt;/strong&gt; (1B). With the upgraded VAE and Transformer architecture, the quality of our generated videos has been greatly improved ğŸš€. &lt;a href="https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#open-sora-13-model-weights"&gt;[checkpoints]&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_04.md"&gt;[report]&lt;/a&gt; &lt;a href="https://huggingface.co/spaces/hpcai-tech/open-sora"&gt;[demo]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2024.12.23]&lt;/strong&gt; The development cost of video generation models has saved by 50%! Open-source solutions are now available with H200 GPU vouchers. &lt;a href="https://company.hpc-ai.com/blog/the-development-cost-of-video-generation-models-has-saved-by-50-open-source-solutions-are-now-available-with-h200-gpu-vouchers"&gt;[blog]&lt;/a&gt; &lt;a href="https://github.com/hpcaitech/Open-Sora/raw/main/scripts/train.py"&gt;[code]&lt;/a&gt; &lt;a href="https://colossalai.org/zh-Hans/docs/get_started/bonus/"&gt;[vouchers]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2024.06.17]&lt;/strong&gt; We released &lt;strong&gt;Open-Sora 1.2&lt;/strong&gt;, which includes &lt;strong&gt;3D-VAE&lt;/strong&gt;, &lt;strong&gt;rectified flow&lt;/strong&gt;, and &lt;strong&gt;score condition&lt;/strong&gt;. The video quality is greatly improved. &lt;a href="https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#open-sora-12-model-weights"&gt;[checkpoints]&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_03.md"&gt;[report]&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2412.20404"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2024.04.25]&lt;/strong&gt; ğŸ¤— We released the &lt;a href="https://huggingface.co/spaces/hpcai-tech/open-sora"&gt;Gradio demo for Open-Sora&lt;/a&gt; on Hugging Face Spaces.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2024.04.25]&lt;/strong&gt; We released &lt;strong&gt;Open-Sora 1.1&lt;/strong&gt;, which supports &lt;strong&gt;2s~15s, 144p to 720p, any aspect ratio&lt;/strong&gt; text-to-image, &lt;strong&gt;text-to-video, image-to-video, video-to-video, infinite time&lt;/strong&gt; generation. In addition, a full video processing pipeline is released. &lt;a href="https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#open-sora-11-model-weights"&gt;[checkpoints]&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_02.md"&gt;[report]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2024.03.18]&lt;/strong&gt; We released &lt;strong&gt;Open-Sora 1.0&lt;/strong&gt;, a fully open-source project for video generation. Open-Sora 1.0 supports a full pipeline of video data preprocessing, training with &lt;a href="https://github.com/hpcaitech/ColossalAI"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/readme/colossal_ai.png" width="8%" /&gt;&lt;/a&gt; acceleration, inference, and more. Our model can produce 2s 512x512 videos with only 3 days training. &lt;a href="https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#open-sora-10-model-weights"&gt;[checkpoints]&lt;/a&gt; &lt;a href="https://hpc-ai.com/blog/open-sora-v1.0"&gt;[blog]&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_01.md"&gt;[report]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2024.03.04]&lt;/strong&gt; Open-Sora provides training with 46% cost reduction. &lt;a href="https://hpc-ai.com/blog/open-sora"&gt;[blog]&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ğŸ“ Since Open-Sora is under active development, we remain different branches for different versions. The latest version is &lt;a href="https://github.com/hpcaitech/Open-Sora"&gt;main&lt;/a&gt;. Old versions include: &lt;a href="https://github.com/hpcaitech/Open-Sora/tree/opensora/v1.0"&gt;v1.0&lt;/a&gt;, &lt;a href="https://github.com/hpcaitech/Open-Sora/tree/opensora/v1.1"&gt;v1.1&lt;/a&gt;, &lt;a href="https://github.com/hpcaitech/Open-Sora/tree/opensora/v1.2"&gt;v1.2&lt;/a&gt;, &lt;a href="https://github.com/hpcaitech/Open-Sora/tree/opensora/v1.3"&gt;v1.3&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ¥ Latest Demo&lt;/h2&gt; 
&lt;p&gt;Demos are presented in compressed GIF format for convenience. For original quality samples and their corresponding prompts, please visit our &lt;a href="https://hpcaitech.github.io/Open-Sora/"&gt;Gallery&lt;/a&gt;.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;5s 1024Ã—576&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;5s 576Ã—1024&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;5s 576Ã—1024&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://streamable.com/e/8g9y9h?autoplay=1"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v2.0/ft_0001_1_1.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://streamable.com/e/k50mnv?autoplay=1"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v2.0/movie_0160.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://streamable.com/e/bzrn9n?autoplay=1"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v2.0/movie_0017.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://streamable.com/e/dsv8da?autoplay=1"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v2.0/ft_0012_1_1.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://streamable.com/e/3wif07?autoplay=1"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v2.0/douyin_0005.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://streamable.com/e/us2w7h?autoplay=1"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v2.0/movie_0037.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://streamable.com/e/yfwk8i?autoplay=1"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v2.0/ft_0055_1_1.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://streamable.com/e/jgjil0?autoplay=1"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v2.0/sora_0019.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://streamable.com/e/lsoai1?autoplay=1"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v2.0/movie_0463.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;details&gt; 
 &lt;summary&gt;OpenSora 1.3 Demo&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;strong&gt;5s 720Ã—1280&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;5s 720Ã—1280&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;5s 720Ã—1280&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://streamable.com/e/r0imrp?quality=highest&amp;amp;autoplay=1"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.3/demo_tomato.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://streamable.com/e/hfvjkh?quality=highest&amp;amp;autoplay=1"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.3/demo_fisherman.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://streamable.com/e/kutmma?quality=highest&amp;amp;autoplay=1"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.3/demo_girl2.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://streamable.com/e/osn1la?quality=highest&amp;amp;autoplay=1"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.3/demo_grape.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://streamable.com/e/l1pzws?quality=highest&amp;amp;autoplay=1"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.3/demo_mushroom.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://streamable.com/e/2vqari?quality=highest&amp;amp;autoplay=1"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.3/demo_parrot.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://streamable.com/e/1in7d6?quality=highest&amp;amp;autoplay=1"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.3/demo_trans.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://streamable.com/e/e9bi4o?quality=highest&amp;amp;autoplay=1"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.3/demo_bear.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://streamable.com/e/09z7xi?quality=highest&amp;amp;autoplay=1"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.3/demo_futureflower.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://streamable.com/e/16c3hk?quality=highest&amp;amp;autoplay=1"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.3/demo_fire.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://streamable.com/e/wi250w?quality=highest&amp;amp;autoplay=1"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.3/demo_man.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://streamable.com/e/vw5b64?quality=highest&amp;amp;autoplay=1"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.3/demo_black.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;OpenSora 1.2 Demo&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;strong&gt;4s 720Ã—1280&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;4s 720Ã—1280&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;4s 720Ã—1280&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hpcaitech/Open-Sora/assets/99191637/7895aab6-ed23-488c-8486-091480c26327"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.2/sample_0013.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hpcaitech/Open-Sora/assets/99191637/20f07c7b-182b-4562-bbee-f1df74c86c9a"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.2/sample_1718.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hpcaitech/Open-Sora/assets/99191637/3d897e0d-dc21-453a-b911-b3bda838acc2"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.2/sample_0087.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hpcaitech/Open-Sora/assets/99191637/644bf938-96ce-44aa-b797-b3c0b513d64c"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.2/sample_0052.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hpcaitech/Open-Sora/assets/99191637/272d88ac-4b4a-484d-a665-8d07431671d0"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.2/sample_1719.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hpcaitech/Open-Sora/assets/99191637/ebbac621-c34e-4bb4-9543-1c34f8989764"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.2/sample_0002.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hpcaitech/Open-Sora/assets/99191637/a1e3a1a3-4abd-45f5-8df2-6cced69da4ca"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.2/sample_0011.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hpcaitech/Open-Sora/assets/99191637/d6ce9c13-28e1-4dff-9644-cc01f5f11926"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.2/sample_0004.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hpcaitech/Open-Sora/assets/99191637/561978f8-f1b0-4f4d-ae7b-45bec9001b4a"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.2/sample_0061.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;OpenSora 1.1 Demo&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;strong&gt;2s 240Ã—426&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;2s 240Ã—426&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/c31ebc52-de39-4a4e-9b1e-9211d45e05b2"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.1/sample_16x240x426_9.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/c31ebc52-de39-4a4e-9b1e-9211d45e05b2"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.1/sora_16x240x426_26.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/f7ce4aaa-528f-40a8-be7a-72e61eaacbbd"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.1/sora_16x240x426_27.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/5d58d71e-1fda-4d90-9ad3-5f2f7b75c6a9"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.1/sora_16x240x426_40.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;strong&gt;2s 426Ã—240&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;4s 480Ã—854&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/34ecb4a0-4eef-4286-ad4c-8e3a87e5a9fd"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.1/sora_16x426x240_24.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/c1619333-25d7-42ba-a91c-18dbc1870b18"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.1/sample_32x480x854_9.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;strong&gt;16s 320Ã—320&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;16s 224Ã—448&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;2s 426Ã—240&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hpcaitech/Open-Sora/assets/99191637/3cab536e-9b43-4b33-8da8-a0f9cf842ff2"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.1/sample_16s_320x320.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hpcaitech/Open-Sora/assets/99191637/9fb0b9e0-c6f4-4935-b29e-4cac10b373c4"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.1/sample_16s_224x448.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/3e892ad2-9543-4049-b005-643a4c1bf3bf"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.1/sora_16x426x240_3.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;OpenSora 1.0 Demo&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;strong&gt;2s 512Ã—512&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;2s 512Ã—512&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;2s 512Ã—512&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hpcaitech/Open-Sora/assets/99191637/de1963d3-b43b-4e68-a670-bb821ebb6f80"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.0/sample_0.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hpcaitech/Open-Sora/assets/99191637/13f8338f-3d42-4b71-8142-d234fbd746cc"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.0/sample_1.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hpcaitech/Open-Sora/assets/99191637/fa6a65a6-e32a-4d64-9a9e-eabb0ebb8c16"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.0/sample_2.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;A serene night scene in a forested area. [...] The video is a time-lapse, capturing the transition from day to night, with the lake and forest serving as a constant backdrop.&lt;/td&gt; 
    &lt;td&gt;A soaring drone footage captures the majestic beauty of a coastal cliff, [...] The water gently laps at the rock base and the greenery that clings to the top of the cliff.&lt;/td&gt; 
    &lt;td&gt;The majestic beauty of a waterfall cascading down a cliff into a serene lake. [...] The camera angle provides a bird's eye view of the waterfall.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hpcaitech/Open-Sora/assets/99191637/64232f84-1b36-4750-a6c0-3e610fa9aa94"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.0/sample_3.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hpcaitech/Open-Sora/assets/99191637/983a1965-a374-41a7-a76b-c07941a6c1e9"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.0/sample_4.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hpcaitech/Open-Sora/assets/99191637/ec10c879-9767-4c31-865f-2e8d6cf11e65"&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v1.0/sample_5.gif" width="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;A bustling city street at night, filled with the glow of car headlights and the ambient light of streetlights. [...]&lt;/td&gt; 
    &lt;td&gt;The vibrant beauty of a sunflower field. The sunflowers are arranged in neat rows, creating a sense of order and symmetry. [...]&lt;/td&gt; 
    &lt;td&gt;A serene underwater scene featuring a sea turtle swimming through a coral reef. The turtle, with its greenish-brown shell [...]&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;Videos are downsampled to &lt;code&gt;.gif&lt;/code&gt; for display. Click for original videos. Prompts are trimmed for display, see &lt;a href="https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/texts/t2v_samples.txt"&gt;here&lt;/a&gt; for full prompts.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸ”† Reports&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://arxiv.org/abs/2503.09642v1"&gt;Tech Report of Open-Sora 2.0&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/train.md"&gt;Step by step to train or finetune your own model&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/ae.md"&gt;Step by step to train and evaluate an video autoencoder&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/hcae.md"&gt;Visit the high compression video autoencoder&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Reports of previous version (better see in according branch): 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_04.md"&gt;Open-Sora 1.3&lt;/a&gt;: shift-window attention, unified spatial-temporal VAE, etc.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_03.md"&gt;Open-Sora 1.2&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2412.20404"&gt;Tech Report&lt;/a&gt;: rectified flow, 3d-VAE, score condition, evaluation, etc.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_02.md"&gt;Open-Sora 1.1&lt;/a&gt;: multi-resolution/length/aspect-ratio, image/video conditioning/editing, data preprocessing, etc.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_01.md"&gt;Open-Sora 1.0&lt;/a&gt;: architecture, captioning, etc.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ğŸ“ Since Open-Sora is under active development, we remain different branches for different versions. The latest version is &lt;a href="https://github.com/hpcaitech/Open-Sora"&gt;main&lt;/a&gt;. Old versions include: &lt;a href="https://github.com/hpcaitech/Open-Sora/tree/opensora/v1.0"&gt;v1.0&lt;/a&gt;, &lt;a href="https://github.com/hpcaitech/Open-Sora/tree/opensora/v1.1"&gt;v1.1&lt;/a&gt;, &lt;a href="https://github.com/hpcaitech/Open-Sora/tree/opensora/v1.2"&gt;v1.2&lt;/a&gt;, &lt;a href="https://github.com/hpcaitech/Open-Sora/tree/opensora/v1.3"&gt;v1.3&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# create a virtual env and activate (conda as an example)
conda create -n opensora python=3.10
conda activate opensora

# download the repo
git clone https://github.com/hpcaitech/Open-Sora
cd Open-Sora

# Ensure torch &amp;gt;= 2.4.0
pip install -v . # for development mode, `pip install -v -e .`
pip install xformers==0.0.27.post2 --index-url https://download.pytorch.org/whl/cu121 # install xformers according to your cuda version
pip install flash-attn --no-build-isolation
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Optionally, you can install flash attention 3 for faster speed.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Dao-AILab/flash-attention # 4f0640d5
cd flash-attention/hopper
python setup.py install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Model Download&lt;/h3&gt; 
&lt;p&gt;Our 11B model supports 256px and 768px resolution. Both T2V and I2V are supported by one model. ğŸ¤— &lt;a href="https://huggingface.co/hpcai-tech/Open-Sora-v2"&gt;Huggingface&lt;/a&gt; ğŸ¤– &lt;a href="https://modelscope.cn/models/luchentech/Open-Sora-v2"&gt;ModelScope&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Download from huggingface:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "huggingface_hub[cli]"
huggingface-cli download hpcai-tech/Open-Sora-v2 --local-dir ./ckpts
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Download from ModelScope:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install modelscope
modelscope download hpcai-tech/Open-Sora-v2 --local_dir ./ckpts
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Text-to-Video Generation&lt;/h3&gt; 
&lt;p&gt;Our model is optimized for image-to-video generation, but it can also be used for text-to-video generation. To generate high quality videos, with the help of flux text-to-image model, we build a text-to-image-to-video pipeline. For 256x256 resolution:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Generate one given prompt
torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --prompt "raining, sea"

# Save memory with offloading
torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --prompt "raining, sea" --offload True

# Generation with csv
torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --dataset.data-path assets/texts/example.csv
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For 768x768 resolution:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# One GPU
torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_768px.py --save-dir samples --prompt "raining, sea"

# Multi-GPU with colossalai sp
torchrun --nproc_per_node 8 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_768px.py --save-dir samples --prompt "raining, sea"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can adjust the generation aspect ratio by &lt;code&gt;--aspect_ratio&lt;/code&gt; and the generation length by &lt;code&gt;--num_frames&lt;/code&gt;. Candidate values for aspect_ratio includes &lt;code&gt;16:9&lt;/code&gt;, &lt;code&gt;9:16&lt;/code&gt;, &lt;code&gt;1:1&lt;/code&gt;, &lt;code&gt;2.39:1&lt;/code&gt;. Candidate values for num_frames should be &lt;code&gt;4k+1&lt;/code&gt; and less than 129.&lt;/p&gt; 
&lt;p&gt;You can also run direct text-to-video by:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# One GPU for 256px
torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/256px.py --prompt "raining, sea"
# Multi-GPU for 768px
torchrun --nproc_per_node 8 --standalone scripts/diffusion/inference.py configs/diffusion/inference/768px.py --prompt "raining, sea"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Image-to-Video Generation&lt;/h3&gt; 
&lt;p&gt;Given a prompt and a reference image, you can generate a video with the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 256px
torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/256px.py --cond_type i2v_head --prompt "A plump pig wallows in a muddy pond on a rustic farm, its pink snout poking out as it snorts contentedly. The camera captures the pig's playful splashes, sending ripples through the water under the midday sun. Wooden fences and a red barn stand in the background, framed by rolling green hills. The pig's muddy coat glistens in the sunlight, showcasing the simple pleasures of its carefree life." --ref assets/texts/i2v.png

# 256px with csv
torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/256px.py --cond_type i2v_head --dataset.data-path assets/texts/i2v.csv

# Multi-GPU 768px
torchrun --nproc_per_node 8 --standalone scripts/diffusion/inference.py configs/diffusion/inference/768px.py --cond_type i2v_head --dataset.data-path assets/texts/i2v.csv
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Advanced Usage&lt;/h2&gt; 
&lt;h3&gt;Motion Score&lt;/h3&gt; 
&lt;p&gt;During training, we provide motion score into the text prompt. During inference, you can use the following command to generate videos with motion score (the default score is 4):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --prompt "raining, sea" --motion-score 4
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We also provide a dynamic motion score evaluator. After setting your OpenAI API key, you can use the following command to evaluate the motion score of a video:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --prompt "raining, sea" --motion-score dynamic
&lt;/code&gt;&lt;/pre&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Score&lt;/th&gt; 
   &lt;th&gt;1&lt;/th&gt; 
   &lt;th&gt;4&lt;/th&gt; 
   &lt;th&gt;7&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v2.0/motion_score_1.gif" width="" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v2.0/motion_score_4.gif" width="" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/demo/v2.0/motion_score_7.gif" width="" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Prompt Refine&lt;/h3&gt; 
&lt;p&gt;We take advantage of ChatGPT to refine the prompt. You can use the following command to refine the prompt. The function is available for both text-to-video and image-to-video generation.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=sk-xxxx
torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --prompt "raining, sea" --refine-prompt True
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Reproductivity&lt;/h3&gt; 
&lt;p&gt;To make the results reproducible, you can set the random seed by:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --prompt "raining, sea" --sampling_option.seed 42 --seed 42
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Use &lt;code&gt;--num-sample k&lt;/code&gt; to generate &lt;code&gt;k&lt;/code&gt; samples for each prompt.&lt;/p&gt; 
&lt;h2&gt;Computational Efficiency&lt;/h2&gt; 
&lt;p&gt;We test the computational efficiency of text-to-video on H100/H800 GPU. For 256x256, we use colossalai's tensor parallelism, and &lt;code&gt;--offload True&lt;/code&gt; is used. For 768x768, we use colossalai's sequence parallelism. All use number of steps 50. The results are presented in the format: $\color{blue}{\text{Total time (s)}}/\color{red}{\text{peak GPU memory (GB)}}$&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Resolution&lt;/th&gt; 
   &lt;th&gt;1x GPU&lt;/th&gt; 
   &lt;th&gt;2x GPUs&lt;/th&gt; 
   &lt;th&gt;4x GPUs&lt;/th&gt; 
   &lt;th&gt;8x GPUs&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;256x256&lt;/td&gt; 
   &lt;td&gt;$\color{blue}{60}/\color{red}{52.5}$&lt;/td&gt; 
   &lt;td&gt;$\color{blue}{40}/\color{red}{44.3}$&lt;/td&gt; 
   &lt;td&gt;$\color{blue}{34}/\color{red}{44.3}$&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;768x768&lt;/td&gt; 
   &lt;td&gt;$\color{blue}{1656}/\color{red}{60.3}$&lt;/td&gt; 
   &lt;td&gt;$\color{blue}{863}/\color{red}{48.3}$&lt;/td&gt; 
   &lt;td&gt;$\color{blue}{466}/\color{red}{44.3}$&lt;/td&gt; 
   &lt;td&gt;$\color{blue}{276}/\color{red}{44.3}$&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Evaluation&lt;/h2&gt; 
&lt;p&gt;On &lt;a href="https://huggingface.co/spaces/Vchitect/VBench_Leaderboard"&gt;VBench&lt;/a&gt;, Open-Sora 2.0 significantly narrows the gap with OpenAIâ€™s Sora, reducing it from 4.52% â†’ 0.69% compared to Open-Sora 1.2.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/readme/v2_vbench.png" alt="VBench" /&gt;&lt;/p&gt; 
&lt;p&gt;Human preference results show our model is on par with HunyuanVideo 11B and Step-Video 30B.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/readme/v2_winrate.png" alt="Win Rate" /&gt;&lt;/p&gt; 
&lt;p&gt;With strong performance, Open-Sora 2.0 is cost-effective.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/hpcaitech/Open-Sora-Demo/raw/main/readme/v2_cost.png" alt="Cost" /&gt;&lt;/p&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;p&gt;Thanks goes to these wonderful contributors:&lt;/p&gt; 
&lt;a href="https://github.com/hpcaitech/Open-Sora/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=hpcaitech/Open-Sora" /&gt; &lt;/a&gt; 
&lt;p&gt;If you wish to contribute to this project, please refer to the &lt;a href="https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/CONTRIBUTING.md"&gt;Contribution Guideline&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Acknowledgement&lt;/h2&gt; 
&lt;p&gt;Here we only list a few of the projects. For other works and datasets, please refer to our report.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hpcaitech/ColossalAI"&gt;ColossalAI&lt;/a&gt;: A powerful large model parallel acceleration and optimization system.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/facebookresearch/DiT"&gt;DiT&lt;/a&gt;: Scalable Diffusion Models with Transformers.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/NUS-HPC-AI-Lab/OpenDiT"&gt;OpenDiT&lt;/a&gt;: An acceleration for DiT training. We adopt valuable acceleration strategies for training progress from OpenDiT.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/PixArt-alpha/PixArt-alpha"&gt;PixArt&lt;/a&gt;: An open-source DiT-based text-to-image model.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/black-forest-labs/flux"&gt;Flux&lt;/a&gt;: A powerful text-to-image generation model.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Vchitect/Latte"&gt;Latte&lt;/a&gt;: An attempt to efficiently train DiT for video.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Tencent/HunyuanVideo/tree/main?tab=readme-ov-file"&gt;HunyuanVideo&lt;/a&gt;: Open-Source text-to-video model.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/stabilityai/sd-vae-ft-mse-original"&gt;StabilityAI VAE&lt;/a&gt;: A powerful image VAE model.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mit-han-lab/efficientvit"&gt;DC-AE&lt;/a&gt;: Deep Compression AutoEncoder for image compression.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/openai/CLIP"&gt;CLIP&lt;/a&gt;: A powerful text-image embedding model.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google-research/text-to-text-transfer-transformer"&gt;T5&lt;/a&gt;: A powerful text encoder.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/haotian-liu/LLaVA"&gt;LLaVA&lt;/a&gt;: A powerful image captioning model based on &lt;a href="https://huggingface.co/mistralai/Mistral-7B-v0.1"&gt;Mistral-7B&lt;/a&gt; and &lt;a href="https://huggingface.co/01-ai/Yi-34B"&gt;Yi-34B&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/magic-research/PLLaVA"&gt;PLLaVA&lt;/a&gt;: A powerful video captioning model.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mira-space/MiraData"&gt;MiraData&lt;/a&gt;: A large-scale video dataset with long durations and structured caption.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{opensora,
  title={Open-sora: Democratizing efficient video production for all},
  author={Zheng, Zangwei and Peng, Xiangyu and Yang, Tianji and Shen, Chenhui and Li, Shenggui and Liu, Hongxin and Zhou, Yukun and Li, Tianyi and You, Yang},
  journal={arXiv preprint arXiv:2412.20404},
  year={2024}
}

@article{opensora2,
    title={Open-Sora 2.0: Training a Commercial-Level Video Generation Model in $200k}, 
    author={Xiangyu Peng and Zangwei Zheng and Chenhui Shen and Tom Young and Xinying Guo and Binluo Wang and Hang Xu and Hongxin Liu and Mingyan Jiang and Wenjun Li and Yuhui Wang and Anbang Ye and Gang Ren and Qianran Ma and Wanying Liang and Xiang Lian and Xiwen Wu and Yuting Zhong and Zhuangyan Li and Chaoyu Gong and Guojun Lei and Leijun Cheng and Limin Zhang and Minghao Li and Ruijie Zhang and Silan Hu and Shijie Huang and Xiaokang Wang and Yuanheng Zhao and Yuqi Wang and Ziang Wei and Yang You},
    year={2025},
    journal={arXiv preprint arXiv:2503.09642},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#hpcaitech/Open-Sora&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=hpcaitech/Open-Sora&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>opendatalab/MinerU</title>
      <link>https://github.com/opendatalab/MinerU</link>
      <description>&lt;p&gt;Transforms complex documents like PDFs into LLM-ready markdown/JSON for your Agentic workflows.&lt;/p&gt;&lt;hr&gt;&lt;div align="center" xmlns="http://www.w3.org/1999/html"&gt; 
 &lt;!-- logo --&gt; 
 &lt;p align="center"&gt; &lt;img src="https://gcore.jsdelivr.net/gh/opendatalab/MinerU@master/docs/images/MinerU-logo.png" width="300px" style="vertical-align:middle;" /&gt; &lt;/p&gt; 
 &lt;!-- icon --&gt; 
 &lt;p&gt;&lt;a href="https://github.com/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/github/stars/opendatalab/MinerU.svg?sanitize=true" alt="stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/github/forks/opendatalab/MinerU.svg?sanitize=true" alt="forks" /&gt;&lt;/a&gt; &lt;a href="https://github.com/opendatalab/MinerU/issues"&gt;&lt;img src="https://img.shields.io/github/issues-raw/opendatalab/MinerU" alt="open issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/opendatalab/MinerU/issues"&gt;&lt;img src="https://img.shields.io/github/issues-closed-raw/opendatalab/MinerU" alt="issue resolution" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/mineru/"&gt;&lt;img src="https://img.shields.io/pypi/v/mineru" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/mineru/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/mineru" alt="PyPI - Python Version" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/mineru"&gt;&lt;img src="https://static.pepy.tech/badge/mineru" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/mineru"&gt;&lt;img src="https://static.pepy.tech/badge/mineru/month" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://mineru.net/OpenSourceTools/Extractor?source=github"&gt;&lt;img src="https://img.shields.io/badge/webapp_on_mineru.net-blue?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTM0IiBoZWlnaHQ9IjEzNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMTIyLDljMCw1LTQsOS05LDlzLTktNC05LTksNC05LDktOSw5LDQsOSw5eiIgZmlsbD0idXJsKCNhKSIvPjxwYXRoIGQ9Im0xMjIsOWMwLDUtNCw5LTksOXMtOS00LTktOSw0LTksOS05LDksNCw5LDl6IiBmaWxsPSIjMDEwMTAxIi8+PHBhdGggZD0ibTkxLDE4YzAsNS00LDktOSw5cy05LTQtOS05LDQtOSw5LTksOSw0LDksOXoiIGZpbGw9InVybCgjYikiLz48cGF0aCBkPSJtOTEsMThjMCw1LTQsOS05LDlzLTktNC05LTksNC05LDktOSw5LDQsOSw5eiIgZmlsbD0iIzAxMDEwMSIvPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJtMzksNjJjMCwxNiw4LDMwLDIwLDM4LDctNiwxMi0xNiwxMi0yNlY0OWMwLTQsMy03LDYtOGw0Ni0xMmM1LTEsMTEsMywxMSw4djMxYzAsMzctMzAsNjYtNjYsNjYtMzcsMC02Ni0zMC02Ni02NlY0NmMwLTQsMy03LDYtOGwyMC02YzUtMSwxMSwzLDExLDh2MjF6bS0yOSw2YzAsMTYsNiwzMCwxNyw0MCwzLDEsNSwxLDgsMSw1LDAsMTAtMSwxNS0zQzM3LDk1LDI5LDc5LDI5LDYyVjQybC0xOSw1djIweiIgZmlsbD0idXJsKCNjKSIvPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJtMzksNjJjMCwxNiw4LDMwLDIwLDM4LDctNiwxMi0xNiwxMi0yNlY0OWMwLTQsMy03LDYtOGw0Ni0xMmM1LTEsMTEsMywxMSw4djMxYzAsMzctMzAsNjYtNjYsNjYtMzcsMC02Ni0zMC02Ni02NlY0NmMwLTQsMy03LDYtOGwyMC02YzUtMSwxMSwzLDExLDh2MjF6bS0yOSw2YzAsMTYsNiwzMCwxNyw0MCwzLDEsNSwxLDgsMSw1LDAsMTAtMSwxNS0zQzM3LDk1LDI5LDc5LDI5LDYyVjQybC0xOSw1djIweiIgZmlsbD0iIzAxMDEwMSIvPjxkZWZzPjxsaW5lYXJHcmFkaWVudCBpZD0iYSIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYiIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYyIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjwvZGVmcz48L3N2Zz4=&amp;amp;labelColor=white" alt="OpenDataLab" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/spaces/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/badge/Demo_on_HuggingFace-yellow.svg?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="HuggingFace" /&gt;&lt;/a&gt; &lt;a href="https://www.modelscope.cn/studios/OpenDataLab/MinerU"&gt;&lt;img src="https://img.shields.io/badge/Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&amp;amp;labelColor=white" alt="ModelScope" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/gist/myhloli/a3cb16570ab3cfeadf9d8f0ac91b4fca/mineru_demo.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2409.18839"&gt;&lt;img src="https://img.shields.io/badge/MinerU-Technical%20Report-b31b1b.svg?logo=arXiv" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2509.22186"&gt;&lt;img src="https://img.shields.io/badge/MinerU2.5-Technical%20Report-b31b1b.svg?logo=arXiv" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/opendatalab/MinerU"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/11174" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/11174" alt="opendatalab%2FMinerU | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;!-- language --&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/README.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/README_zh-CN.md"&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt;&lt;/p&gt; 
 &lt;!-- hot link --&gt; 
 &lt;p align="center"&gt; ğŸš€&lt;a href="https://mineru.net/?source=github"&gt;Access MinerU Nowâ†’âœ… Zero-Install Web Version âœ… Full-Featured Desktop Client âœ… Instant API Access; Skip deployment headaches â€“ get all product formats in one click. Developers, dive in!&lt;/a&gt; &lt;/p&gt; 
 &lt;!-- join us --&gt; 
 &lt;p align="center"&gt; ğŸ‘‹ join us on &lt;a href="https://discord.gg/Tdedn9GTXq" target="_blank"&gt;Discord&lt;/a&gt; and &lt;a href="https://mineru.net/community-portal/?aliasId=3c430f94" target="_blank"&gt;WeChat&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h1&gt;Changelog&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;2025/11/04 2.6.4 Release&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Added timeout configuration for PDF image rendering, default is 300 seconds, can be configured via environment variable &lt;code&gt;MINERU_PDF_RENDER_TIMEOUT&lt;/code&gt; to prevent long blocking of the rendering process caused by some abnormal PDF files.&lt;/li&gt; 
   &lt;li&gt;Added CPU thread count configuration options for ONNX models, default is the system CPU core count, can be configured via environment variables &lt;code&gt;MINERU_INTRA_OP_NUM_THREADS&lt;/code&gt; and &lt;code&gt;MINERU_INTER_OP_NUM_THREADS&lt;/code&gt; to reduce CPU resource contention conflicts in high concurrency scenarios.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025/10/31 2.6.3 Release&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Added support for a new backend &lt;code&gt;vlm-mlx-engine&lt;/code&gt;, enabling MLX-accelerated inference for the MinerU2.5 model on Apple Silicon devices. Compared to the &lt;code&gt;vlm-transformers&lt;/code&gt; backend, &lt;code&gt;vlm-mlx-engine&lt;/code&gt; delivers a 100%â€“200% speed improvement.&lt;/li&gt; 
   &lt;li&gt;Bug fixes: #3849, #3859&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025/10/24 2.6.2 Release&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;pipeline&lt;/code&gt; backend optimizations 
    &lt;ul&gt; 
     &lt;li&gt;Added experimental support for Chinese formulas, which can be enabled by setting the environment variable &lt;code&gt;export MINERU_FORMULA_CH_SUPPORT=1&lt;/code&gt;. This feature may cause a slight decrease in MFR speed and failures in recognizing some long formulas. It is recommended to enable it only when parsing Chinese formulas is needed. To disable this feature, set the environment variable to &lt;code&gt;0&lt;/code&gt;.&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;OCR&lt;/code&gt; speed significantly improved by 200%~300%, thanks to the optimization solution provided by &lt;a href="https://github.com/cjsdurj"&gt;@cjsdurj&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;OCR&lt;/code&gt; models optimized for improved accuracy and coverage of Latin script recognition, and updated Cyrillic, Arabic, Devanagari, Telugu (te), and Tamil (ta) language systems to &lt;code&gt;ppocr-v5&lt;/code&gt; version, with accuracy improved by over 40% compared to previous models&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;vlm&lt;/code&gt; backend optimizations 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;table_caption&lt;/code&gt; and &lt;code&gt;table_footnote&lt;/code&gt; matching logic optimized to improve the accuracy of table caption and footnote matching and reading order rationality in scenarios with multiple consecutive tables on a page&lt;/li&gt; 
     &lt;li&gt;Optimized CPU resource usage during high concurrency when using &lt;code&gt;vllm&lt;/code&gt; backend, reducing server pressure&lt;/li&gt; 
     &lt;li&gt;Adapted to &lt;code&gt;vllm&lt;/code&gt; version 0.11.0&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;General optimizations 
    &lt;ul&gt; 
     &lt;li&gt;Cross-page table merging effect optimized, added support for cross-page continuation table merging, improving table merging effectiveness in multi-column merge scenarios&lt;/li&gt; 
     &lt;li&gt;Added environment variable configuration option &lt;code&gt;MINERU_TABLE_MERGE_ENABLE&lt;/code&gt; for table merging feature. Table merging is enabled by default and can be disabled by setting this variable to &lt;code&gt;0&lt;/code&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025/09/26 2.5.4 released&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ğŸ‰ğŸ‰ The MinerU2.5 &lt;a href="https://arxiv.org/abs/2509.22186"&gt;Technical Report&lt;/a&gt; is now available! We welcome you to read it for a comprehensive overview of its model architecture, training strategy, data engineering and evaluation results.&lt;/li&gt; 
   &lt;li&gt;Fixed an issue where some &lt;code&gt;PDF&lt;/code&gt; files were mistakenly identified as &lt;code&gt;AI&lt;/code&gt; files, causing parsing failures&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025/09/20 2.5.3 Released&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Dependency version range adjustment to enable Turing and earlier architecture GPUs to use vLLM acceleration for MinerU2.5 model inference.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;pipeline&lt;/code&gt; backend compatibility fixes for torch 2.8.0.&lt;/li&gt; 
   &lt;li&gt;Reduced default concurrency for vLLM async backend to lower server pressure and avoid connection closure issues caused by high load.&lt;/li&gt; 
   &lt;li&gt;More compatibility-related details can be found in the &lt;a href="https://github.com/opendatalab/MinerU/discussions/3548"&gt;announcement&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025/09/19 2.5.2 Released&lt;/p&gt; &lt;p&gt;We are officially releasing MinerU2.5, currently the most powerful multimodal large model for document parsing. With only 1.2B parameters, MinerU2.5's accuracy on the OmniDocBench benchmark comprehensively surpasses top-tier multimodal models like Gemini 2.5 Pro, GPT-4o, and Qwen2.5-VL-72B. It also significantly outperforms leading specialized models such as dots.ocr, MonkeyOCR, and PP-StructureV3. The model has been released on &lt;a href="https://huggingface.co/opendatalab/MinerU2.5-2509-1.2B"&gt;HuggingFace&lt;/a&gt; and &lt;a href="https://modelscope.cn/models/opendatalab/MinerU2.5-2509-1.2B"&gt;ModelScope&lt;/a&gt; platforms. Welcome to download and use!&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Core Highlights: 
    &lt;ul&gt; 
     &lt;li&gt;SOTA Performance with Extreme Efficiency: As a 1.2B model, it achieves State-of-the-Art (SOTA) results that exceed models in the 10B and 100B+ classes, redefining the performance-per-parameter standard in document AI.&lt;/li&gt; 
     &lt;li&gt;Advanced Architecture for Across-the-Board Leadership: By combining a two-stage inference pipeline (decoupling layout analysis from content recognition) with a native high-resolution architecture, it achieves SOTA performance across five key areas: layout analysis, text recognition, formula recognition, table recognition, and reading order.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Key Capability Enhancements: 
    &lt;ul&gt; 
     &lt;li&gt;Layout Detection: Delivers more complete results by accurately covering non-body content like headers, footers, and page numbers. It also provides more precise element localization and natural format reconstruction for lists and references.&lt;/li&gt; 
     &lt;li&gt;Table Parsing: Drastically improves parsing for challenging cases, including rotated tables, borderless/semi-structured tables, and long/complex tables.&lt;/li&gt; 
     &lt;li&gt;Formula Recognition: Significantly boosts accuracy for complex, long-form, and hybrid Chinese-English formulas, greatly enhancing the parsing capability for mathematical documents.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Additionally, with the release of vlm 2.5, we have made some adjustments to the repository:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;The vlm backend has been upgraded to version 2.5, supporting the MinerU2.5 model and no longer compatible with the MinerU2.0-2505-0.9B model. The last version supporting the 2.0 model is mineru-2.2.2.&lt;/li&gt; 
   &lt;li&gt;VLM inference-related code has been moved to &lt;a href="https://github.com/opendatalab/mineru-vl-utils"&gt;mineru_vl_utils&lt;/a&gt;, reducing coupling with the main mineru repository and facilitating independent iteration in the future.&lt;/li&gt; 
   &lt;li&gt;The vlm accelerated inference framework has been switched from &lt;code&gt;sglang&lt;/code&gt; to &lt;code&gt;vllm&lt;/code&gt;, achieving full compatibility with the vllm ecosystem, allowing users to use the MinerU2.5 model and accelerated inference on any platform that supports the vllm framework.&lt;/li&gt; 
   &lt;li&gt;Due to major upgrades in the vlm model supporting more layout types, we have made some adjustments to the structure of the parsing intermediate file &lt;code&gt;middle.json&lt;/code&gt; and result file &lt;code&gt;content_list.json&lt;/code&gt;. Please refer to the &lt;a href="https://opendatalab.github.io/MinerU/reference/output_files/"&gt;documentation&lt;/a&gt; for details.&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Other repository optimizations:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Removed file extension whitelist validation for input files. When input files are PDF documents or images, there are no longer requirements for file extensions, improving usability.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;History Log&lt;/summary&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/09/10 2.2.2 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed the issue where the new table recognition model would affect the overall parsing task when some table parsing failed&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/09/08 2.2.1 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed the issue where some newly added models were not downloaded when using the model download command.&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/09/05 2.2.0 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt; Major Updates 
    &lt;ul&gt; 
     &lt;li&gt;In this version, we focused on improving table parsing accuracy by introducing a new &lt;a href="https://github.com/RapidAI/TableStructureRec"&gt;wired table recognition model&lt;/a&gt; and a brand-new hybrid table structure parsing algorithm, significantly enhancing the table recognition capabilities of the &lt;code&gt;pipeline&lt;/code&gt; backend.&lt;/li&gt; 
     &lt;li&gt;We also added support for cross-page table merging, which is supported by both &lt;code&gt;pipeline&lt;/code&gt; and &lt;code&gt;vlm&lt;/code&gt; backends, further improving the completeness and accuracy of table parsing.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; Other Updates 
    &lt;ul&gt; 
     &lt;li&gt;The &lt;code&gt;pipeline&lt;/code&gt; backend now supports 270-degree rotated table parsing, bringing support for table parsing in 0/90/270-degree orientations&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;pipeline&lt;/code&gt; added OCR capability support for Thai and Greek, and updated the English OCR model to the latest version. English recognition accuracy improved by 11%, Thai recognition model accuracy is 82.68%, and Greek recognition model accuracy is 89.28% (by PPOCRv5)&lt;/li&gt; 
     &lt;li&gt;Added &lt;code&gt;bbox&lt;/code&gt; field (mapped to 0-1000 range) in the output &lt;code&gt;content_list.json&lt;/code&gt;, making it convenient for users to directly obtain position information for each content block&lt;/li&gt; 
     &lt;li&gt;Removed the &lt;code&gt;pipeline_old_linux&lt;/code&gt; installation option, no longer supporting legacy Linux systems such as &lt;code&gt;CentOS 7&lt;/code&gt;, to provide better support for &lt;code&gt;uv&lt;/code&gt;'s &lt;code&gt;sync&lt;/code&gt;/&lt;code&gt;run&lt;/code&gt; commands&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/08/01 2.1.10 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed an issue in the &lt;code&gt;pipeline&lt;/code&gt; backend where block overlap caused the parsing results to deviate from expectations #3232&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/30 2.1.9 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;transformers&lt;/code&gt; 4.54.1 version adaptation&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/28 2.1.8 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;sglang&lt;/code&gt; 0.4.9.post5 version adaptation&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/27 2.1.7 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;transformers&lt;/code&gt; 4.54.0 version adaptation&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/26 2.1.6 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed table parsing issues in handwritten documents when using &lt;code&gt;vlm&lt;/code&gt; backend&lt;/li&gt; 
   &lt;li&gt;Fixed visualization box position drift issue when document is rotated #3175&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/24 2.1.5 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;sglang&lt;/code&gt; 0.4.9 version adaptation, synchronously upgrading the dockerfile base image to sglang 0.4.9.post3&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/23 2.1.4 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Bug Fixes&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Fixed the issue of excessive memory consumption during the &lt;code&gt;MFR&lt;/code&gt; step in the &lt;code&gt;pipeline&lt;/code&gt; backend under certain scenarios #2771&lt;/li&gt; 
     &lt;li&gt;Fixed the inaccurate matching between &lt;code&gt;image&lt;/code&gt;/&lt;code&gt;table&lt;/code&gt; and &lt;code&gt;caption&lt;/code&gt;/&lt;code&gt;footnote&lt;/code&gt; under certain conditions #3129&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/16 2.1.1 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Bug fixes&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Fixed text block content loss issue that could occur in certain &lt;code&gt;pipeline&lt;/code&gt; scenarios #3005&lt;/li&gt; 
     &lt;li&gt;Fixed issue where &lt;code&gt;sglang-client&lt;/code&gt; required unnecessary packages like &lt;code&gt;torch&lt;/code&gt; #2968&lt;/li&gt; 
     &lt;li&gt;Updated &lt;code&gt;dockerfile&lt;/code&gt; to fix incomplete text content parsing due to missing fonts in Linux #2915&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Usability improvements&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Updated &lt;code&gt;compose.yaml&lt;/code&gt; to facilitate direct startup of &lt;code&gt;sglang-server&lt;/code&gt;, &lt;code&gt;mineru-api&lt;/code&gt;, and &lt;code&gt;mineru-gradio&lt;/code&gt; services&lt;/li&gt; 
     &lt;li&gt;Launched brand new &lt;a href="https://opendatalab.github.io/MinerU/"&gt;online documentation site&lt;/a&gt;, simplified readme, providing better documentation experience&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/05 2.1.0 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;This is the first major update of MinerU 2, which includes a large number of new features and improvements, covering significant performance optimizations, user experience enhancements, and bug fixes. The detailed update contents are as follows:&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Performance Optimizations:&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Significantly improved preprocessing speed for documents with specific resolutions (around 2000 pixels on the long side).&lt;/li&gt; 
     &lt;li&gt;Greatly enhanced post-processing speed when the &lt;code&gt;pipeline&lt;/code&gt; backend handles batch processing of documents with fewer pages (&amp;lt;10 pages).&lt;/li&gt; 
     &lt;li&gt;Layout analysis speed of the &lt;code&gt;pipeline&lt;/code&gt; backend has been increased by approximately 20%.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Experience Enhancements:&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Built-in ready-to-use &lt;code&gt;fastapi service&lt;/code&gt; and &lt;code&gt;gradio webui&lt;/code&gt;. For detailed usage instructions, please refer to &lt;a href="https://opendatalab.github.io/MinerU/usage/quick_usage/#advanced-usage-via-api-webui-sglang-clientserver"&gt;Documentation&lt;/a&gt;.&lt;/li&gt; 
     &lt;li&gt;Adapted to &lt;code&gt;sglang&lt;/code&gt; version &lt;code&gt;0.4.8&lt;/code&gt;, significantly reducing the GPU memory requirements for the &lt;code&gt;vlm-sglang&lt;/code&gt; backend. It can now run on graphics cards with as little as &lt;code&gt;8GB GPU memory&lt;/code&gt; (Turing architecture or newer).&lt;/li&gt; 
     &lt;li&gt;Added transparent parameter passing for all commands related to &lt;code&gt;sglang&lt;/code&gt;, allowing the &lt;code&gt;sglang-engine&lt;/code&gt; backend to receive all &lt;code&gt;sglang&lt;/code&gt; parameters consistently with the &lt;code&gt;sglang-server&lt;/code&gt;.&lt;/li&gt; 
     &lt;li&gt;Supports feature extensions based on configuration files, including &lt;code&gt;custom formula delimiters&lt;/code&gt;, &lt;code&gt;enabling heading classification&lt;/code&gt;, and &lt;code&gt;customizing local model directories&lt;/code&gt;. For detailed usage instructions, please refer to &lt;a href="https://opendatalab.github.io/MinerU/usage/quick_usage/#extending-mineru-functionality-with-configuration-files"&gt;Documentation&lt;/a&gt;.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;New Features:&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Updated the &lt;code&gt;pipeline&lt;/code&gt; backend with the PP-OCRv5 multilingual text recognition model, supporting text recognition in 37 languages such as French, Spanish, Portuguese, Russian, and Korean, with an average accuracy improvement of over 30%. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/algorithm/PP-OCRv5/PP-OCRv5_multi_languages.html"&gt;Details&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;Introduced limited support for vertical text layout in the &lt;code&gt;pipeline&lt;/code&gt; backend.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/06/20 2.0.6 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed occasional parsing interruptions caused by invalid block content in &lt;code&gt;vlm&lt;/code&gt; mode&lt;/li&gt; 
   &lt;li&gt;Fixed parsing interruptions caused by incomplete table structures in &lt;code&gt;vlm&lt;/code&gt; mode&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/06/17 2.0.5 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed the issue where models were still required to be downloaded in the &lt;code&gt;sglang-client&lt;/code&gt; mode&lt;/li&gt; 
   &lt;li&gt;Fixed the issue where the &lt;code&gt;sglang-client&lt;/code&gt; mode unnecessarily depended on packages like &lt;code&gt;torch&lt;/code&gt; during runtime.&lt;/li&gt; 
   &lt;li&gt;Fixed the issue where only the first instance would take effect when attempting to launch multiple &lt;code&gt;sglang-client&lt;/code&gt; instances via multiple URLs within the same process&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/06/15 2.0.3 released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed a configuration file key-value update error that occurred when downloading model type was set to &lt;code&gt;all&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Fixed the issue where the formula and table feature toggle switches were not working in &lt;code&gt;command line mode&lt;/code&gt;, causing the features to remain enabled.&lt;/li&gt; 
   &lt;li&gt;Fixed compatibility issues with sglang version 0.4.7 in the &lt;code&gt;sglang-engine&lt;/code&gt; mode.&lt;/li&gt; 
   &lt;li&gt;Updated Dockerfile and installation documentation for deploying the full version of MinerU in sglang environment&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/06/13 2.0.0 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;New Architecture&lt;/strong&gt;: MinerU 2.0 has been deeply restructured in code organization and interaction methods, significantly improving system usability, maintainability, and extensibility. 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Removal of Third-party Dependency Limitations&lt;/strong&gt;: Completely eliminated the dependency on &lt;code&gt;pymupdf&lt;/code&gt;, moving the project toward a more open and compliant open-source direction.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Ready-to-use, Easy Configuration&lt;/strong&gt;: No need to manually edit JSON configuration files; most parameters can now be set directly via command line or API.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Automatic Model Management&lt;/strong&gt;: Added automatic model download and update mechanisms, allowing users to complete model deployment without manual intervention.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Offline Deployment Friendly&lt;/strong&gt;: Provides built-in model download commands, supporting deployment requirements in completely offline environments.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Streamlined Code Structure&lt;/strong&gt;: Removed thousands of lines of redundant code, simplified class inheritance logic, significantly improving code readability and development efficiency.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Unified Intermediate Format Output&lt;/strong&gt;: Adopted standardized &lt;code&gt;middle_json&lt;/code&gt; format, compatible with most secondary development scenarios based on this format, ensuring seamless ecosystem business migration.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;New Model&lt;/strong&gt;: MinerU 2.0 integrates our latest small-parameter, high-performance multimodal document parsing model, achieving end-to-end high-speed, high-precision document understanding. 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Small Model, Big Capabilities&lt;/strong&gt;: With parameters under 1B, yet surpassing traditional 72B-level vision-language models (VLMs) in parsing accuracy.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Multiple Functions in One&lt;/strong&gt;: A single model covers multilingual recognition, handwriting recognition, layout analysis, table parsing, formula recognition, reading order sorting, and other core tasks.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Ultimate Inference Speed&lt;/strong&gt;: Achieves peak throughput exceeding 10,000 tokens/s through &lt;code&gt;sglang&lt;/code&gt; acceleration on a single NVIDIA 4090 card, easily handling large-scale document processing requirements.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Online Experience&lt;/strong&gt;: You can experience our brand-new VLM model on &lt;a href="https://mineru.net/OpenSourceTools/Extractor"&gt;MinerU.net&lt;/a&gt;, &lt;a href="https://huggingface.co/spaces/opendatalab/MinerU"&gt;Hugging Face&lt;/a&gt;, and &lt;a href="https://www.modelscope.cn/studios/OpenDataLab/MinerU"&gt;ModelScope&lt;/a&gt;.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Incompatible Changes Notice&lt;/strong&gt;: To improve overall architectural rationality and long-term maintainability, this version contains some incompatible changes: 
    &lt;ul&gt; 
     &lt;li&gt;Python package name changed from &lt;code&gt;magic-pdf&lt;/code&gt; to &lt;code&gt;mineru&lt;/code&gt;, and the command-line tool changed from &lt;code&gt;magic-pdf&lt;/code&gt; to &lt;code&gt;mineru&lt;/code&gt;. Please update your scripts and command calls accordingly.&lt;/li&gt; 
     &lt;li&gt;For modular system design and ecosystem consistency considerations, MinerU 2.0 no longer includes the LibreOffice document conversion module. If you need to process Office documents, we recommend converting them to PDF format through an independently deployed LibreOffice service before proceeding with subsequent parsing operations.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/05/24 Release 1.3.12&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Added support for PPOCRv5 models, updated &lt;code&gt;ch_server&lt;/code&gt; model to &lt;code&gt;PP-OCRv5_rec_server&lt;/code&gt;, and &lt;code&gt;ch_lite&lt;/code&gt; model to &lt;code&gt;PP-OCRv5_rec_mobile&lt;/code&gt; (model update required) 
    &lt;ul&gt; 
     &lt;li&gt;In testing, we found that PPOCRv5(server) has some improvement for handwritten documents, but has slightly lower accuracy than v4_server_doc for other document types, so the default ch model remains unchanged as &lt;code&gt;PP-OCRv4_server_rec_doc&lt;/code&gt;.&lt;/li&gt; 
     &lt;li&gt;Since PPOCRv5 has enhanced recognition capabilities for handwriting and special characters, you can manually choose the PPOCRv5 model for Japanese-Traditional Chinese mixed scenarios and handwritten documents&lt;/li&gt; 
     &lt;li&gt;You can select the appropriate model through the lang parameter &lt;code&gt;lang='ch_server'&lt;/code&gt; (Python API) or &lt;code&gt;--lang ch_server&lt;/code&gt; (command line): 
      &lt;ul&gt; 
       &lt;li&gt;&lt;code&gt;ch&lt;/code&gt;: &lt;code&gt;PP-OCRv4_server_rec_doc&lt;/code&gt; (default) (Chinese/English/Japanese/Traditional Chinese mixed/15K dictionary)&lt;/li&gt; 
       &lt;li&gt;&lt;code&gt;ch_server&lt;/code&gt;: &lt;code&gt;PP-OCRv5_rec_server&lt;/code&gt; (Chinese/English/Japanese/Traditional Chinese mixed + handwriting/18K dictionary)&lt;/li&gt; 
       &lt;li&gt;&lt;code&gt;ch_lite&lt;/code&gt;: &lt;code&gt;PP-OCRv5_rec_mobile&lt;/code&gt; (Chinese/English/Japanese/Traditional Chinese mixed + handwriting/18K dictionary)&lt;/li&gt; 
       &lt;li&gt;&lt;code&gt;ch_server_v4&lt;/code&gt;: &lt;code&gt;PP-OCRv4_rec_server&lt;/code&gt; (Chinese/English mixed/6K dictionary)&lt;/li&gt; 
       &lt;li&gt;&lt;code&gt;ch_lite_v4&lt;/code&gt;: &lt;code&gt;PP-OCRv4_rec_mobile&lt;/code&gt; (Chinese/English mixed/6K dictionary)&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Added support for handwritten documents through optimized layout recognition of handwritten text areas 
    &lt;ul&gt; 
     &lt;li&gt;This feature is supported by default, no additional configuration required&lt;/li&gt; 
     &lt;li&gt;You can refer to the instructions above to manually select the PPOCRv5 model for better handwritten document parsing results&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;The &lt;code&gt;huggingface&lt;/code&gt; and &lt;code&gt;modelscope&lt;/code&gt; demos have been updated to versions that support handwriting recognition and PPOCRv5 models, which you can experience online&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/29 Release 1.3.10&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Added support for custom formula delimiters, which can be configured by modifying the &lt;code&gt;latex-delimiter-config&lt;/code&gt; section in the &lt;code&gt;magic-pdf.json&lt;/code&gt; file in your user directory.&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/27 Release 1.3.9&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Optimized formula parsing functionality, improved formula rendering success rate&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/23 Release 1.3.8&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;The default &lt;code&gt;ocr&lt;/code&gt; model (&lt;code&gt;ch&lt;/code&gt;) has been updated to &lt;code&gt;PP-OCRv4_server_rec_doc&lt;/code&gt; (model update required) 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;PP-OCRv4_server_rec_doc&lt;/code&gt; is trained on a mixture of more Chinese document data and PP-OCR training data based on &lt;code&gt;PP-OCRv4_server_rec&lt;/code&gt;, adding recognition capabilities for some traditional Chinese characters, Japanese, and special characters. It can recognize over 15,000 characters and improves both document-specific and general text recognition abilities.&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/ocr_modules/text_recognition.html#_3"&gt;Performance comparison of PP-OCRv4_server_rec_doc/PP-OCRv4_server_rec/PP-OCRv4_mobile_rec&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;After verification, the &lt;code&gt;PP-OCRv4_server_rec_doc&lt;/code&gt; model shows significant accuracy improvements in Chinese/English/Japanese/Traditional Chinese in both single language and mixed language scenarios, with comparable speed to &lt;code&gt;PP-OCRv4_server_rec&lt;/code&gt;, making it suitable for most use cases.&lt;/li&gt; 
     &lt;li&gt;In some pure English scenarios, &lt;code&gt;PP-OCRv4_server_rec_doc&lt;/code&gt; may have word adhesion issues, while &lt;code&gt;PP-OCRv4_server_rec&lt;/code&gt; performs better in these cases. Therefore, we've kept the &lt;code&gt;PP-OCRv4_server_rec&lt;/code&gt; model, which users can access by adding the parameter &lt;code&gt;lang='ch_server'&lt;/code&gt; (Python API) or &lt;code&gt;--lang ch_server&lt;/code&gt; (command line).&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/22 Release 1.3.7&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed the issue where the lang parameter was ineffective during table parsing model initialization&lt;/li&gt; 
   &lt;li&gt;Fixed the significant speed reduction of OCR and table parsing in &lt;code&gt;cpu&lt;/code&gt; mode&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/16 Release 1.3.4&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Slightly improved OCR-det speed by removing some unnecessary blocks&lt;/li&gt; 
   &lt;li&gt;Fixed page-internal sorting errors caused by footnotes in certain cases&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/12 Release 1.3.2&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed dependency version incompatibility issues when installing on Windows with Python 3.13&lt;/li&gt; 
   &lt;li&gt;Optimized memory usage during batch inference&lt;/li&gt; 
   &lt;li&gt;Improved parsing of tables rotated 90 degrees&lt;/li&gt; 
   &lt;li&gt;Enhanced parsing of oversized tables in financial report samples&lt;/li&gt; 
   &lt;li&gt;Fixed the occasional word adhesion issue in English text areas when OCR language is not specified (model update required)&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/08 Release 1.3.1&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed several compatibility issues 
    &lt;ul&gt; 
     &lt;li&gt;Added support for Python 3.13&lt;/li&gt; 
     &lt;li&gt;Made final adaptations for outdated Linux systems (such as CentOS 7) with no guarantee of continued support in future versions, &lt;a href="https://github.com/opendatalab/MinerU/issues/1004"&gt;installation instructions&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/03 Release 1.3.0&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Installation and compatibility optimizations 
    &lt;ul&gt; 
     &lt;li&gt;Resolved compatibility issues caused by &lt;code&gt;detectron2&lt;/code&gt; by removing &lt;code&gt;layoutlmv3&lt;/code&gt; usage in layout&lt;/li&gt; 
     &lt;li&gt;Extended torch version compatibility to 2.2~2.6 (excluding 2.5)&lt;/li&gt; 
     &lt;li&gt;Added CUDA compatibility for versions 11.8/12.4/12.6/12.8 (CUDA version determined by torch), solving compatibility issues for users with 50-series and H-series GPUs&lt;/li&gt; 
     &lt;li&gt;Extended Python compatibility to versions 3.10~3.12, fixing the issue of automatic downgrade to version 0.6.1 when installing in non-3.10 environments&lt;/li&gt; 
     &lt;li&gt;Optimized offline deployment process, eliminating the need to download any model files after successful deployment&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Performance optimizations 
    &lt;ul&gt; 
     &lt;li&gt;Enhanced parsing speed for batches of small files by supporting batch processing of multiple PDF files (&lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/demo/batch_demo.py"&gt;script example&lt;/a&gt;), with formula parsing speed improved by up to 1400% and overall parsing speed improved by up to 500% compared to version 1.0.1&lt;/li&gt; 
     &lt;li&gt;Reduced memory usage and improved parsing speed by optimizing MFR model loading and usage (requires re-running the &lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/docs/how_to_download_models_zh_cn.md"&gt;model download process&lt;/a&gt; to get incremental updates to model files)&lt;/li&gt; 
     &lt;li&gt;Optimized GPU memory usage, requiring only 6GB minimum to run this project&lt;/li&gt; 
     &lt;li&gt;Improved running speed on MPS devices&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Parsing effect optimizations 
    &lt;ul&gt; 
     &lt;li&gt;Updated MFR model to &lt;code&gt;unimernet(2503)&lt;/code&gt;, fixing line break loss issues in multi-line formulas&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Usability optimizations 
    &lt;ul&gt; 
     &lt;li&gt;Completely replaced the &lt;code&gt;paddle&lt;/code&gt; framework and &lt;code&gt;paddleocr&lt;/code&gt; in the project by using &lt;code&gt;paddleocr2torch&lt;/code&gt;, resolving conflicts between &lt;code&gt;paddle&lt;/code&gt; and &lt;code&gt;torch&lt;/code&gt;, as well as thread safety issues caused by the &lt;code&gt;paddle&lt;/code&gt; framework&lt;/li&gt; 
     &lt;li&gt;Added real-time progress bar display during parsing, allowing precise tracking of parsing progress and making the waiting process more bearable&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/03/03 1.2.1 released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed the impact on punctuation marks during full-width to half-width conversion of letters and numbers&lt;/li&gt; 
   &lt;li&gt;Fixed caption matching inaccuracies in certain scenarios&lt;/li&gt; 
   &lt;li&gt;Fixed formula span loss issues in certain scenarios&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/02/24 1.2.0 released&lt;/summary&gt; 
  &lt;p&gt;This version includes several fixes and improvements to enhance parsing efficiency and accuracy:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Performance Optimization&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Increased classification speed for PDF documents in auto mode.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Parsing Optimization&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Improved parsing logic for documents containing watermarks, significantly enhancing the parsing results for such documents.&lt;/li&gt; 
     &lt;li&gt;Enhanced the matching logic for multiple images/tables and captions within a single page, improving the accuracy of image-text matching in complex layouts.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Bug Fixes&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Fixed an issue where image/table spans were incorrectly filled into text blocks under certain conditions.&lt;/li&gt; 
     &lt;li&gt;Resolved an issue where title blocks were empty in some cases.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/01/22 1.1.0 released&lt;/summary&gt; 
  &lt;p&gt;In this version we have focused on improving parsing accuracy and efficiency:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Model capability upgrade&lt;/strong&gt; (requires re-executing the &lt;a href="https://github.com/opendatalab/MinerU/raw/master/docs/how_to_download_models_en.md"&gt;model download process&lt;/a&gt; to obtain incremental updates of model files) 
    &lt;ul&gt; 
     &lt;li&gt;The layout recognition model has been upgraded to the latest &lt;code&gt;doclayout_yolo(2501)&lt;/code&gt; model, improving layout recognition accuracy.&lt;/li&gt; 
     &lt;li&gt;The formula parsing model has been upgraded to the latest &lt;code&gt;unimernet(2501)&lt;/code&gt; model, improving formula recognition accuracy.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Performance optimization&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;On devices that meet certain configuration requirements (16GB+ VRAM), by optimizing resource usage and restructuring the processing pipeline, overall parsing speed has been increased by more than 50%.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Parsing effect optimization&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Added a new heading classification feature (testing version, enabled by default) to the online demo (&lt;a href="https://mineru.net/OpenSourceTools/Extractor"&gt;mineru.net&lt;/a&gt;/&lt;a href="https://huggingface.co/spaces/opendatalab/MinerU"&gt;huggingface&lt;/a&gt;/&lt;a href="https://www.modelscope.cn/studios/OpenDataLab/MinerU"&gt;modelscope&lt;/a&gt;), which supports hierarchical classification of headings, thereby enhancing document structuring.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/01/10 1.0.1 released&lt;/summary&gt; 
  &lt;p&gt;This is our first official release, where we have introduced a completely new API interface and enhanced compatibility through extensive refactoring, as well as a brand new automatic language identification feature:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;New API Interface&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;For the data-side API, we have introduced the Dataset class, designed to provide a robust and flexible data processing framework. This framework currently supports a variety of document formats, including images (.jpg and .png), PDFs, Word documents (.doc and .docx), and PowerPoint presentations (.ppt and .pptx). It ensures effective support for data processing tasks ranging from simple to complex.&lt;/li&gt; 
     &lt;li&gt;For the user-side API, we have meticulously designed the MinerU processing workflow as a series of composable Stages. Each Stage represents a specific processing step, allowing users to define new Stages according to their needs and creatively combine these stages to customize their data processing workflows.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Enhanced Compatibility&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;By optimizing the dependency environment and configuration items, we ensure stable and efficient operation on ARM architecture Linux systems.&lt;/li&gt; 
     &lt;li&gt;We have deeply integrated with Huawei Ascend NPU acceleration, providing autonomous and controllable high-performance computing capabilities. This supports the localization and development of AI application platforms in China. &lt;a href="https://github.com/opendatalab/MinerU/raw/master/docs/README_Ascend_NPU_Acceleration_zh_CN.md"&gt;Ascend NPU Acceleration&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Automatic Language Identification&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;By introducing a new language recognition model, setting the &lt;code&gt;lang&lt;/code&gt; configuration to &lt;code&gt;auto&lt;/code&gt; during document parsing will automatically select the appropriate OCR language model, improving the accuracy of scanned document parsing.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/11/22 0.10.0 released&lt;/summary&gt; 
  &lt;p&gt;Introducing hybrid OCR text extraction capabilities:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Significantly improved parsing performance in complex text distribution scenarios such as dense formulas, irregular span regions, and text represented by images.&lt;/li&gt; 
   &lt;li&gt;Combines the dual advantages of accurate content extraction and faster speed in text mode, and more precise span/line region recognition in OCR mode.&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/11/15 0.9.3 released&lt;/summary&gt; 
  &lt;p&gt;Integrated &lt;a href="https://github.com/RapidAI/RapidTable"&gt;RapidTable&lt;/a&gt; for table recognition, improving single-table parsing speed by more than 10 times, with higher accuracy and lower GPU memory usage.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/11/06 0.9.2 released&lt;/summary&gt; 
  &lt;p&gt;Integrated the &lt;a href="https://huggingface.co/U4R/StructTable-InternVL2-1B"&gt;StructTable-InternVL2-1B&lt;/a&gt; model for table recognition functionality.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/10/31 0.9.0 released&lt;/summary&gt; 
  &lt;p&gt;This is a major new version with extensive code refactoring, addressing numerous issues, improving performance, reducing hardware requirements, and enhancing usability:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Refactored the sorting module code to use &lt;a href="https://github.com/ppaanngggg/layoutreader"&gt;layoutreader&lt;/a&gt; for reading order sorting, ensuring high accuracy in various layouts.&lt;/li&gt; 
   &lt;li&gt;Refactored the paragraph concatenation module to achieve good results in cross-column, cross-page, cross-figure, and cross-table scenarios.&lt;/li&gt; 
   &lt;li&gt;Refactored the list and table of contents recognition functions, significantly improving the accuracy of list blocks and table of contents blocks, as well as the parsing of corresponding text paragraphs.&lt;/li&gt; 
   &lt;li&gt;Refactored the matching logic for figures, tables, and descriptive text, greatly enhancing the accuracy of matching captions and footnotes to figures and tables, and reducing the loss rate of descriptive text to near zero.&lt;/li&gt; 
   &lt;li&gt;Added multi-language support for OCR, supporting detection and recognition of 84 languages. For the list of supported languages, see &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/ppocr/blog/multi_languages.html#5-support-languages-and-abbreviations"&gt;OCR Language Support List&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;Added memory recycling logic and other memory optimization measures, significantly reducing memory usage. The memory requirement for enabling all acceleration features except table acceleration (layout/formula/OCR) has been reduced from 16GB to 8GB, and the memory requirement for enabling all acceleration features has been reduced from 24GB to 10GB.&lt;/li&gt; 
   &lt;li&gt;Optimized configuration file feature switches, adding an independent formula detection switch to significantly improve speed and parsing results when formula detection is not needed.&lt;/li&gt; 
   &lt;li&gt;Integrated &lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;PDF-Extract-Kit 1.0&lt;/a&gt;: 
    &lt;ul&gt; 
     &lt;li&gt;Added the self-developed &lt;code&gt;doclayout_yolo&lt;/code&gt; model, which speeds up processing by more than 10 times compared to the original solution while maintaining similar parsing effects, and can be freely switched with &lt;code&gt;layoutlmv3&lt;/code&gt; via the configuration file.&lt;/li&gt; 
     &lt;li&gt;Upgraded formula parsing to &lt;code&gt;unimernet 0.2.1&lt;/code&gt;, improving formula parsing accuracy while significantly reducing memory usage.&lt;/li&gt; 
     &lt;li&gt;Due to the repository change for &lt;code&gt;PDF-Extract-Kit 1.0&lt;/code&gt;, you need to re-download the model. Please refer to &lt;a href="https://github.com/opendatalab/MinerU/raw/master/docs/how_to_download_models_en.md"&gt;How to Download Models&lt;/a&gt; for detailed steps.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/09/27 Version 0.8.1 released&lt;/summary&gt; 
  &lt;p&gt;Fixed some bugs, and providing a &lt;a href="https://github.com/opendatalab/MinerU/raw/master/projects/web_demo/README.md"&gt;localized deployment version&lt;/a&gt; of the &lt;a href="https://opendatalab.com/OpenSourceTools/Extractor/PDF/"&gt;online demo&lt;/a&gt; and the &lt;a href="https://github.com/opendatalab/MinerU/raw/master/projects/web/README.md"&gt;front-end interface&lt;/a&gt;.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/09/09 Version 0.8.0 released&lt;/summary&gt; 
  &lt;p&gt;Supporting fast deployment with Dockerfile, and launching demos on Huggingface and Modelscope.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/08/30 Version 0.7.1 released&lt;/summary&gt; 
  &lt;p&gt;Add paddle tablemaster table recognition option&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/08/09 Version 0.7.0b1 released&lt;/summary&gt; 
  &lt;p&gt;Simplified installation process, added table recognition functionality&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/08/01 Version 0.6.2b1 released&lt;/summary&gt; 
  &lt;p&gt;Optimized dependency conflict issues and installation documentation&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/07/05 Initial open-source release&lt;/summary&gt; 
 &lt;/details&gt; 
&lt;/details&gt; 
&lt;h1&gt;MinerU&lt;/h1&gt; 
&lt;h2&gt;Project Introduction&lt;/h2&gt; 
&lt;p&gt;MinerU is a tool that converts PDFs into machine-readable formats (e.g., markdown, JSON), allowing for easy extraction into any format. MinerU was born during the pre-training process of &lt;a href="https://github.com/InternLM/InternLM"&gt;InternLM&lt;/a&gt;. We focus on solving symbol conversion issues in scientific literature and hope to contribute to technological development in the era of large models. Compared to well-known commercial products, MinerU is still young. If you encounter any issues or if the results are not as expected, please submit an issue on &lt;a href="https://github.com/opendatalab/MinerU/issues"&gt;issue&lt;/a&gt; and &lt;strong&gt;attach the relevant PDF&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/4bea02c9-6d54-4cd6-97ed-dff14340982c"&gt;https://github.com/user-attachments/assets/4bea02c9-6d54-4cd6-97ed-dff14340982c&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Remove headers, footers, footnotes, page numbers, etc., to ensure semantic coherence.&lt;/li&gt; 
 &lt;li&gt;Output text in human-readable order, suitable for single-column, multi-column, and complex layouts.&lt;/li&gt; 
 &lt;li&gt;Preserve the structure of the original document, including headings, paragraphs, lists, etc.&lt;/li&gt; 
 &lt;li&gt;Extract images, image descriptions, tables, table titles, and footnotes.&lt;/li&gt; 
 &lt;li&gt;Automatically recognize and convert formulas in the document to LaTeX format.&lt;/li&gt; 
 &lt;li&gt;Automatically recognize and convert tables in the document to HTML format.&lt;/li&gt; 
 &lt;li&gt;Automatically detect scanned PDFs and garbled PDFs and enable OCR functionality.&lt;/li&gt; 
 &lt;li&gt;OCR supports detection and recognition of 109 languages.&lt;/li&gt; 
 &lt;li&gt;Supports multiple output formats, such as multimodal and NLP Markdown, JSON sorted by reading order, and rich intermediate formats.&lt;/li&gt; 
 &lt;li&gt;Supports various visualization results, including layout visualization and span visualization, for efficient confirmation of output quality.&lt;/li&gt; 
 &lt;li&gt;Supports running in a pure CPU environment, and also supports GPU(CUDA)/NPU(CANN)/MPS acceleration&lt;/li&gt; 
 &lt;li&gt;Compatible with Windows, Linux, and Mac platforms.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Quick Start&lt;/h1&gt; 
&lt;p&gt;If you encounter any installation issues, please first consult the &lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/#faq"&gt;FAQ&lt;/a&gt;. &lt;br /&gt; If the parsing results are not as expected, refer to the &lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/#known-issues"&gt;Known Issues&lt;/a&gt;. &lt;br /&gt;&lt;/p&gt; 
&lt;h2&gt;Online Experience&lt;/h2&gt; 
&lt;h3&gt;Official online web application&lt;/h3&gt; 
&lt;p&gt;The official online version has the same functionality as the client, with a beautiful interface and rich features, requires login to use&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://mineru.net/OpenSourceTools/Extractor?source=github"&gt;&lt;img src="https://img.shields.io/badge/webapp_on_mineru.net-blue?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTM0IiBoZWlnaHQ9IjEzNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMTIyLDljMCw1LTQsOS05LDlzLTktNC05LTksNC05LDktOSw5LDQsOSw5eiIgZmlsbD0idXJsKCNhKSIvPjxwYXRoIGQ9Im0xMjIsOWMwLDUtNCw5LTksOXMtOS00LTktOSw0LTksOS05LDksNCw5LDl6IiBmaWxsPSIjMDEwMTAxIi8+PHBhdGggZD0ibTkxLDE4YzAsNS00LDktOSw5cy05LTQtOS05LDQtOSw5LTksOSw0LDksOXoiIGZpbGw9InVybCgjYikiLz48cGF0aCBkPSJtOTEsMThjMCw1LTQsOS05LDlzLTktNC05LTksNC05LDktOSw5LDQsOSw5eiIgZmlsbD0iIzAxMDEwMSIvPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJtMzksNjJjMCwxNiw4LDMwLDIwLDM4LDctNiwxMi0xNiwxMi0yNlY0OWMwLTQsMy03LDYtOGw0Ni0xMmM1LTEsMTEsMywxMSw4djMxYzAsMzctMzAsNjYtNjYsNjYtMzcsMC02Ni0zMC02Ni02NlY0NmMwLTQsMy03LDYtOGwyMC02YzUtMSwxMSwzLDExLDh2MjF6bS0yOSw2YzAsMTYsNiwzMCwxNyw0MCwzLDEsNSwxLDgsMSw1LDAsMTAtMSwxNS0zQzM3LDk1LDI5LDc5LDI5LDYyVjQybC0xOSw1djIweiIgZmlsbD0idXJsKCNjKSIvPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJtMzksNjJjMCwxNiw4LDMwLDIwLDM4LDctNiwxMi0xNiwxMi0yNlY0OWMwLTQsMy03LDYtOGw0Ni0xMmM1LTEsMTEsMywxMSw4djMxYzAsMzctMzAsNjYtNjYsNjYtMzcsMC02Ni0zMC02Ni02NlY0NmMwLTQsMy03LDYtOGwyMC02YzUtMSwxMSwzLDExLDh2MjF6bS0yOSw2YzAsMTYsNiwzMCwxNyw0MCwzLDEsNSwxLDgsMSw1LDAsMTAtMSwxNS0zQzM3LDk1LDI5LDc5LDI5LDYyVjQybC0xOSw1djIweiIgZmlsbD0iIzAxMDEwMSIvPjxkZWZzPjxsaW5lYXJHcmFkaWVudCBpZD0iYSIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYiIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYyIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjwvZGVmcz48L3N2Zz4=&amp;amp;labelColor=white" alt="OpenDataLab" /&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Gradio-based online demo&lt;/h3&gt; 
&lt;p&gt;A WebUI developed based on Gradio, with a simple interface and only core parsing functionality, no login required&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.modelscope.cn/studios/OpenDataLab/MinerU"&gt;&lt;img src="https://img.shields.io/badge/Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&amp;amp;labelColor=white" alt="ModelScope" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/spaces/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/badge/Demo_on_HuggingFace-yellow.svg?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="HuggingFace" /&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Local Deployment&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] &lt;strong&gt;Pre-installation Noticeâ€”Hardware and Software Environment Support&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;To ensure the stability and reliability of the project, we only optimize and test for specific hardware and software environments during development. This ensures that users deploying and running the project on recommended system configurations will get the best performance with the fewest compatibility issues.&lt;/p&gt; 
 &lt;p&gt;By focusing resources on the mainline environment, our team can more efficiently resolve potential bugs and develop new features.&lt;/p&gt; 
 &lt;p&gt;In non-mainline environments, due to the diversity of hardware and software configurations, as well as third-party dependency compatibility issues, we cannot guarantee 100% project availability. Therefore, for users who wish to use this project in non-recommended environments, we suggest carefully reading the documentation and FAQ first. Most issues already have corresponding solutions in the FAQ. We also encourage community feedback to help us gradually expand support.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th rowspan="2"&gt;Parsing Backend&lt;/th&gt; 
   &lt;th rowspan="2"&gt;pipeline &lt;br /&gt; (Accuracy&lt;sup&gt;1&lt;/sup&gt; 82+)&lt;/th&gt; 
   &lt;th colspan="4"&gt;vlm (Accuracy&lt;sup&gt;1&lt;/sup&gt; 90+)&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;transformers&lt;/th&gt; 
   &lt;th&gt;mlx-engine&lt;/th&gt; 
   &lt;th&gt;vllm-engine / &lt;br /&gt;vllm-async-engine&lt;/th&gt; 
   &lt;th&gt;http-client&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Backend Features&lt;/th&gt; 
   &lt;td&gt;Fast, no hallucinations&lt;/td&gt; 
   &lt;td&gt;Good compatibility, &lt;br /&gt;but slower&lt;/td&gt; 
   &lt;td&gt;Faster than transformers&lt;/td&gt; 
   &lt;td&gt;Fast, compatible with the vLLM ecosystem&lt;/td&gt; 
   &lt;td&gt;Suitable for OpenAI-compatible servers&lt;sup&gt;5&lt;/sup&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Operating System&lt;/th&gt; 
   &lt;td colspan="2" style="text-align:center;"&gt;Linux&lt;sup&gt;2&lt;/sup&gt; / Windows / macOS&lt;/td&gt; 
   &lt;td style="text-align:center;"&gt;macOS&lt;sup&gt;3&lt;/sup&gt;&lt;/td&gt; 
   &lt;td style="text-align:center;"&gt;Linux&lt;sup&gt;2&lt;/sup&gt; / Windows&lt;sup&gt;4&lt;/sup&gt; &lt;/td&gt; 
   &lt;td&gt;Any&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;CPU inference support&lt;/th&gt; 
   &lt;td colspan="2" style="text-align:center;"&gt;âœ…&lt;/td&gt; 
   &lt;td colspan="2" style="text-align:center;"&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;Not required&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;GPU Requirements&lt;/th&gt;
   &lt;td colspan="2" style="text-align:center;"&gt;Volta or later architectures, 6 GB VRAM or more, or Apple Silicon&lt;/td&gt; 
   &lt;td&gt;Apple Silicon&lt;/td&gt; 
   &lt;td&gt;Volta or later architectures, 8 GB VRAM or more&lt;/td&gt; 
   &lt;td&gt;Not required&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Memory Requirements&lt;/th&gt; 
   &lt;td colspan="4" style="text-align:center;"&gt;Minimum 16 GB, 32 GB recommended&lt;/td&gt; 
   &lt;td&gt;8 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Disk Space Requirements&lt;/th&gt; 
   &lt;td colspan="4" style="text-align:center;"&gt;20 GB or more, SSD recommended&lt;/td&gt; 
   &lt;td&gt;2 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Python Version&lt;/th&gt; 
   &lt;td colspan="5" style="text-align:center;"&gt;3.10-3.13&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; Accuracy metric is the End-to-End Evaluation Overall score of OmniDocBench (v1.5), tested on the latest &lt;code&gt;MinerU&lt;/code&gt; version.&lt;br /&gt; &lt;sup&gt;2&lt;/sup&gt; Linux supports only distributions released in 2019 or later.&lt;br /&gt; &lt;sup&gt;3&lt;/sup&gt; MLX requires macOS 13.5 or later, recommended for use with version 14.0 or higher.&lt;br /&gt; &lt;sup&gt;4&lt;/sup&gt; Windows vLLM support via WSL2(Windows Subsystem for Linux).&lt;br /&gt; &lt;sup&gt;5&lt;/sup&gt; Servers compatible with the OpenAI API, such as local or remote model services deployed via inference frameworks like &lt;code&gt;vLLM&lt;/code&gt;, &lt;code&gt;SGLang&lt;/code&gt;, or &lt;code&gt;LMDeploy&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Install MinerU&lt;/h3&gt; 
&lt;h4&gt;Install MinerU using pip or uv&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade pip
pip install uv
uv pip install -U "mineru[core]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Install MinerU from source code&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/opendatalab/MinerU.git
cd MinerU
uv pip install -e .[core]
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] &lt;code&gt;mineru[core]&lt;/code&gt; includes all core features except &lt;code&gt;vLLM&lt;/code&gt; acceleration, compatible with Windows / Linux / macOS systems, suitable for most users. If you need to use &lt;code&gt;vLLM&lt;/code&gt; acceleration for VLM model inference or install a lightweight client on edge devices, please refer to the documentation &lt;a href="https://opendatalab.github.io/MinerU/quick_start/extension_modules/"&gt;Extension Modules Installation Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h4&gt;Deploy MinerU using Docker&lt;/h4&gt; 
&lt;p&gt;MinerU provides a convenient Docker deployment method, which helps quickly set up the environment and solve some tricky environment compatibility issues. You can get the &lt;a href="https://opendatalab.github.io/MinerU/quick_start/docker_deployment/"&gt;Docker Deployment Instructions&lt;/a&gt; in the documentation.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Using MinerU&lt;/h3&gt; 
&lt;p&gt;The simplest command line invocation is:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mineru -p &amp;lt;input_path&amp;gt; -o &amp;lt;output_path&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can use MinerU for PDF parsing through various methods such as command line, API, and WebUI. For detailed instructions, please refer to the &lt;a href="https://opendatalab.github.io/MinerU/usage/"&gt;Usage Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;TODO&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Reading order based on the model&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Recognition of &lt;code&gt;index&lt;/code&gt; and &lt;code&gt;list&lt;/code&gt; in the main text&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Table recognition&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Heading Classification&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Handwritten Text Recognition&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Vertical Text Recognition&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Latin Accent Mark Recognition&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Code block recognition in the main text&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/docs/chemical_knowledge_introduction/introduction.pdf"&gt;Chemical formula recognition&lt;/a&gt;(mineru.net)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Geometric shape recognition&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Known Issues&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;Reading order is determined by the model based on the spatial distribution of readable content, and may be out of order in some areas under extremely complex layouts.&lt;/li&gt; 
 &lt;li&gt;Limited support for vertical text.&lt;/li&gt; 
 &lt;li&gt;Tables of contents and lists are recognized through rules, and some uncommon list formats may not be recognized.&lt;/li&gt; 
 &lt;li&gt;Code blocks are not yet supported in the layout model.&lt;/li&gt; 
 &lt;li&gt;Comic books, art albums, primary school textbooks, and exercises cannot be parsed well.&lt;/li&gt; 
 &lt;li&gt;Table recognition may result in row/column recognition errors in complex tables.&lt;/li&gt; 
 &lt;li&gt;OCR recognition may produce inaccurate characters in PDFs of lesser-known languages (e.g., diacritical marks in Latin script, easily confused characters in Arabic script).&lt;/li&gt; 
 &lt;li&gt;Some formulas may not render correctly in Markdown.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;FAQ&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you encounter any issues during usage, you can first check the &lt;a href="https://opendatalab.github.io/MinerU/faq/"&gt;FAQ&lt;/a&gt; for solutions.&lt;/li&gt; 
 &lt;li&gt;If your issue remains unresolved, you may also use &lt;a href="https://deepwiki.com/opendatalab/MinerU"&gt;DeepWiki&lt;/a&gt; to interact with an AI assistant, which can address most common problems.&lt;/li&gt; 
 &lt;li&gt;If you still cannot resolve the issue, you are welcome to join our community via &lt;a href="https://discord.gg/Tdedn9GTXq"&gt;Discord&lt;/a&gt; or &lt;a href="https://mineru.net/community-portal/?aliasId=3c430f94"&gt;WeChat&lt;/a&gt; to discuss with other users and developers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;All Thanks To Our Contributors&lt;/h1&gt; 
&lt;a href="https://github.com/opendatalab/MinerU/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=opendatalab/MinerU" /&gt; &lt;/a&gt; 
&lt;h1&gt;License Information&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/LICENSE.md"&gt;LICENSE.md&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Currently, some models in this project are trained based on YOLO. However, since YOLO follows the AGPL license, it may impose restrictions on certain use cases. In future iterations, we plan to explore and replace these with models under more permissive licenses to enhance user-friendliness and flexibility.&lt;/p&gt; 
&lt;h1&gt;Acknowledgments&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;PDF-Extract-Kit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/DocLayout-YOLO"&gt;DocLayout-YOLO&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/UniMERNet"&gt;UniMERNet&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/RapidAI/RapidTable"&gt;RapidTable&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/RapidAI/TableStructureRec"&gt;TableStructureRec&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleOCR"&gt;PaddleOCR&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/frotms/PaddleOCR2Pytorch"&gt;PaddleOCR2Pytorch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ppaanngggg/layoutreader"&gt;layoutreader&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Sanster/xy-cut"&gt;xy-cut&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LlmKira/fast-langdetect"&gt;fast-langdetect&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pypdfium2-team/pypdfium2"&gt;pypdfium2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/datalab-to/pdftext"&gt;pdftext&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pdfminer/pdfminer.six"&gt;pdfminer.six&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/py-pdf/pypdf"&gt;pypdf&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google/magika"&gt;magika&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Citation&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{niu2025mineru25decoupledvisionlanguagemodel,
      title={MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing}, 
      author={Junbo Niu and Zheng Liu and Zhuangcheng Gu and Bin Wang and Linke Ouyang and Zhiyuan Zhao and Tao Chu and Tianyao He and Fan Wu and Qintong Zhang and Zhenjiang Jin and Guang Liang and Rui Zhang and Wenzheng Zhang and Yuan Qu and Zhifei Ren and Yuefeng Sun and Yuanhong Zheng and Dongsheng Ma and Zirui Tang and Boyu Niu and Ziyang Miao and Hejun Dong and Siyi Qian and Junyuan Zhang and Jingzhou Chen and Fangdong Wang and Xiaomeng Zhao and Liqun Wei and Wei Li and Shasha Wang and Ruiliang Xu and Yuanyuan Cao and Lu Chen and Qianqian Wu and Huaiyu Gu and Lindong Lu and Keming Wang and Dechen Lin and Guanlin Shen and Xuanhe Zhou and Linfeng Zhang and Yuhang Zang and Xiaoyi Dong and Jiaqi Wang and Bo Zhang and Lei Bai and Pei Chu and Weijia Li and Jiang Wu and Lijun Wu and Zhenxiang Li and Guangyu Wang and Zhongying Tu and Chao Xu and Kai Chen and Yu Qiao and Bowen Zhou and Dahua Lin and Wentao Zhang and Conghui He},
      year={2025},
      eprint={2509.22186},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2509.22186}, 
}

@misc{wang2024mineruopensourcesolutionprecise,
      title={MinerU: An Open-Source Solution for Precise Document Content Extraction}, 
      author={Bin Wang and Chao Xu and Xiaomeng Zhao and Linke Ouyang and Fan Wu and Zhiyuan Zhao and Rui Xu and Kaiwen Liu and Yuan Qu and Fukai Shang and Bo Zhang and Liqun Wei and Zhihao Sui and Wei Li and Botian Shi and Yu Qiao and Dahua Lin and Conghui He},
      year={2024},
      eprint={2409.18839},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.18839}, 
}

@article{he2024opendatalab,
  title={Opendatalab: Empowering general artificial intelligence with open datasets},
  author={He, Conghui and Li, Wei and Jin, Zhenjiang and Xu, Chao and Wang, Bin and Lin, Dahua},
  journal={arXiv preprint arXiv:2407.13773},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Star History&lt;/h1&gt; 
&lt;a&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=opendatalab/MinerU&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=opendatalab/MinerU&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=opendatalab/MinerU&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;h1&gt;Links&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenDCAI/DataFlow"&gt;Easy Data Preparation with latest LLMs-based Operators and Pipelines&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/Vis3"&gt;Vis3 (OSS browser based on s3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/labelU"&gt;LabelU (A Lightweight Multi-modal Data Annotation Tool)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/LabelLLM"&gt;LabelLLM (An Open-source LLM Dialogue Annotation Platform)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;PDF-Extract-Kit (A Comprehensive Toolkit for High-Quality PDF Content Extraction)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/OmniDocBench"&gt;OmniDocBench (A Comprehensive Benchmark for Document Parsing and Evaluation)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/magic-html"&gt;Magic-HTML (Mixed web page extraction tool)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/InternLM/magic-doc"&gt;Magic-Doc (Fast speed ppt/pptx/doc/docx/pdf extraction tool)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/MigoXLab/dingo"&gt;Dingo: A Comprehensive AI Data Quality Evaluation Tool&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>