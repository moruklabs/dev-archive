<rss version="2.0">
  <channel>
    <title>GitHub All Languages Daily Trending</title>
    <description>Daily Trending of All Languages in GitHub</description>
    <pubDate>Sat, 16 Aug 2025 01:32:03 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>tadata-org/fastapi_mcp</title>
      <link>https://github.com/tadata-org/fastapi_mcp</link>
      <description>&lt;p&gt;Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt;&lt;a href="https://github.com/tadata-org/fastapi_mcp"&gt;&lt;img src="https://github.com/user-attachments/assets/7e44e98b-a0ba-4aff-a68a-4ffee3a6189c" alt="fastapi-to-mcp" height="100/" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;span style="font-size: 0.85em; font-weight: normal;"&gt;Built by &lt;a href="https://tadata.com"&gt;Tadata&lt;/a&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;h1 align="center"&gt; FastAPI-MCP &lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/14064" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14064" alt="tadata-org%2Ffastapi_mcp | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt;Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://pypi.org/project/fastapi-mcp/"&gt;&lt;img src="https://img.shields.io/pypi/v/fastapi-mcp?color=%2334D058&amp;amp;label=pypi%20package" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/fastapi-mcp/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/fastapi-mcp.svg?sanitize=true" alt="Python Versions" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/#"&gt;&lt;img src="https://img.shields.io/badge/FastAPI-009485.svg?logo=fastapi&amp;amp;logoColor=white" alt="FastAPI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/tadata-org/fastapi_mcp"&gt;&lt;img src="https://codecov.io/gh/tadata-org/fastapi_mcp/branch/main/graph/badge.svg?sanitize=true" alt="Coverage" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt;&lt;a href="https://github.com/tadata-org/fastapi_mcp"&gt;&lt;img src="https://github.com/user-attachments/assets/b205adc6-28c0-4e3c-a68b-9c1a80eb7d0c" alt="fastapi-mcp-usage" height="400" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Authentication&lt;/strong&gt; built in, using your existing FastAPI dependencies!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;FastAPI-native:&lt;/strong&gt; Not just another OpenAPI -&amp;gt; MCP converter&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zero/Minimal configuration&lt;/strong&gt; required - just point it at your FastAPI app and it works&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Preserving schemas&lt;/strong&gt; of your request models and response models&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Preserve documentation&lt;/strong&gt; of all your endpoints, just as it is in Swagger&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Flexible deployment&lt;/strong&gt; - Mount your MCP server to the same app, or deploy separately&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ASGI transport&lt;/strong&gt; - Uses FastAPI's ASGI interface directly for efficient communication&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Hosted Solution&lt;/h2&gt; 
&lt;p&gt;If you prefer a managed hosted solution check out &lt;a href="https://tadata.com"&gt;tadata.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;We recommend using &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;, a fast Python package installer:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add fastapi-mcp
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can install with pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install fastapi-mcp
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Basic Usage&lt;/h2&gt; 
&lt;p&gt;The simplest way to use FastAPI-MCP is to add an MCP server directly to your FastAPI application:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from fastapi import FastAPI
from fastapi_mcp import FastApiMCP

app = FastAPI()

mcp = FastApiMCP(app)

# Mount the MCP server directly to your FastAPI app
mcp.mount()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That's it! Your auto-generated MCP server is now available at &lt;code&gt;https://app.base.url/mcp&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation, Examples and Advanced Usage&lt;/h2&gt; 
&lt;p&gt;FastAPI-MCP provides &lt;a href="https://fastapi-mcp.tadata.com/"&gt;comprehensive documentation&lt;/a&gt;. Additionaly, check out the &lt;a href="https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/examples"&gt;examples directory&lt;/a&gt; for code samples demonstrating these features in action.&lt;/p&gt; 
&lt;h2&gt;FastAPI-first Approach&lt;/h2&gt; 
&lt;p&gt;FastAPI-MCP is designed as a native extension of FastAPI, not just a converter that generates MCP tools from your API. This approach offers several key advantages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Native dependencies&lt;/strong&gt;: Secure your MCP endpoints using familiar FastAPI &lt;code&gt;Depends()&lt;/code&gt; for authentication and authorization&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ASGI transport&lt;/strong&gt;: Communicates directly with your FastAPI app using its ASGI interface, eliminating the need for HTTP calls from the MCP to your API&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unified infrastructure&lt;/strong&gt;: Your FastAPI app doesn't need to run separately from the MCP server (though &lt;a href="https://fastapi-mcp.tadata.com/advanced/deploy#deploying-separately-from-original-fastapi-app"&gt;separate deployment&lt;/a&gt; is also supported)&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This design philosophy ensures minimum friction when adding MCP capabilities to your existing FastAPI services.&lt;/p&gt; 
&lt;h2&gt;Development and Contributing&lt;/h2&gt; 
&lt;p&gt;Thank you for considering contributing to FastAPI-MCP! We encourage the community to post Issues and create Pull Requests.&lt;/p&gt; 
&lt;p&gt;Before you get started, please see our &lt;a href="https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/CONTRIBUTING.md"&gt;Contribution Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Join &lt;a href="https://join.slack.com/t/themcparty/shared_invite/zt-30yxr1zdi-2FG~XjBA0xIgYSYuKe7~Xg"&gt;MCParty Slack community&lt;/a&gt; to connect with other MCP enthusiasts, ask questions, and share your experiences with FastAPI-MCP.&lt;/p&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+ (Recommended 3.12)&lt;/li&gt; 
 &lt;li&gt;uv&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT License. Copyright (c) 2025 Tadata Inc.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>manycore-research/SpatialLM</title>
      <link>https://github.com/manycore-research/SpatialLM</link>
      <description>&lt;p&gt;SpatialLM: Training Large Language Models for Structured Indoor Modeling&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SpatialLM&lt;/h1&gt; 
&lt;!-- markdownlint-disable first-line-h1 --&gt; 
&lt;!-- markdownlint-disable html --&gt; 
&lt;!-- markdownlint-disable no-duplicate-header --&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/manycore-research/SpatialLM/main/figures/logo_light.png#gh-light-mode-only" width="60%" alt="SpatialLM" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/manycore-research/SpatialLM/main/figures/logo_dark.png#gh-dark-mode-only" width="60%" alt="SpatialLM" /&gt; 
&lt;/div&gt; 
&lt;hr style="margin-top: 0; margin-bottom: 8px;" /&gt; 
&lt;div align="center" style="margin-top: 0; padding-top: 0; line-height: 1;"&gt; 
 &lt;a href="https://manycore-research.github.io/SpatialLM" target="_blank" style="margin: 2px;"&gt;&lt;img alt="Project" src="https://img.shields.io/badge/üåê%20Website-SpatialLM-ffc107?color=42a5f5&amp;amp;logoColor=white" style="display: inline-block; vertical-align: middle;" /&gt;&lt;/a&gt; 
 &lt;a href="https://arxiv.org/abs/2506.07491" target="_blank" style="margin: 2px;"&gt;&lt;img alt="arXiv" src="https://img.shields.io/badge/arXiv-Techreport-b31b1b?logo=arxiv&amp;amp;logoColor=white" style="display: inline-block; vertical-align: middle;" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/manycore-research/SpatialLM" target="_blank" style="margin: 2px;"&gt;&lt;img alt="GitHub" src="https://img.shields.io/badge/GitHub-SpatialLM-24292e?logo=github&amp;amp;logoColor=white" style="display: inline-block; vertical-align: middle;" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center" style="line-height: 1;"&gt; 
 &lt;a href="https://huggingface.co/manycore-research/SpatialLM1.1-Qwen-0.5B" target="_blank" style="margin: 2px;"&gt;&lt;img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-SpatialLM-ffc107?color=ffc107&amp;amp;logoColor=white" style="display: inline-block; vertical-align: middle;" /&gt;&lt;/a&gt; 
 &lt;a href="https://huggingface.co/datasets/manycore-research/SpatialLM-Testset" target="_blank" style="margin: 2px;"&gt;&lt;img alt="Dataset" src="https://img.shields.io/badge/%F0%9F%A4%97%20Dataset-Testset-ffc107?color=ffc107&amp;amp;logoColor=white" style="display: inline-block; vertical-align: middle;" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;‚ú® News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Jun, 2025] Check out our new models: &lt;a href="https://huggingface.co/manycore-research/SpatialLM1.1-Llama-1B"&gt;SpatialLM1.1-Llama-1B&lt;/a&gt; and &lt;a href="https://huggingface.co/manycore-research/SpatialLM1.1-Qwen-0.5B"&gt;SpatialLM1.1-Qwen-0.5B&lt;/a&gt;, now available on Hugging Face. SpatialLM1.1 doubles the point cloud resolution, incorporates a more powerful point cloud encoder &lt;a href="https://xywu.me/sonata/"&gt;Sonata&lt;/a&gt; and supports detection with user-specified categories.&lt;/li&gt; 
 &lt;li&gt;[Jun, 2025] SpatialLM &lt;a href="https://arxiv.org/abs/2506.07491"&gt;Technical Report&lt;/a&gt; is now on arXiv.&lt;/li&gt; 
 &lt;li&gt;[Mar, 2025] We're excited to release the &lt;a href="https://huggingface.co/manycore-research/SpatialLM-Llama-1B"&gt;SpatialLM-Llama-1B&lt;/a&gt; and &lt;a href="https://huggingface.co/manycore-research/SpatialLM-Qwen-0.5B"&gt;SpatialLM-Qwen-0.5B&lt;/a&gt; on Hugging Face.&lt;/li&gt; 
 &lt;li&gt;[Mar, 2025] Initial release of SpatialLM!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;SpatialLM is a 3D large language model designed to process 3D point cloud data and generate structured 3D scene understanding outputs. These outputs include architectural elements like walls, doors, windows, and oriented object bounding boxes with their semantic categories. Unlike previous methods that require specialized equipment for data collection, SpatialLM can handle point clouds from diverse sources such as monocular video sequences, RGBD images, and LiDAR sensors. This multimodal architecture effectively bridges the gap between unstructured 3D geometric data and structured 3D representations, offering high-level semantic understanding. It enhances spatial reasoning capabilities for applications in embodied robotics, autonomous navigation, and other complex 3D scene analysis tasks.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;video src="https://github.com/user-attachments/assets/c0218d6a-f676-41f8-ae76-bba228866306" poster="figures/cover.png"&gt; 
 &lt;/video&gt; 
 &lt;p&gt;&lt;i&gt;SpatialLM reconstructs 3D layout from a monocular RGB video with MASt3R-SLAM. Results aligned to video with GT cameras for visualization.&lt;/i&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;SpatialLM Models&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Download&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;SpatialLM1.1-Llama-1B&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/manycore-research/SpatialLM1.1-Llama-1B"&gt;ü§ó HuggingFace&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;SpatialLM1.1-Qwen-0.5B&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/manycore-research/SpatialLM1.1-Qwen-0.5B"&gt;ü§ó HuggingFace&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;SpatialLM1.0-Llama-1B&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/manycore-research/SpatialLM-Llama-1B"&gt;ü§ó HuggingFace&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;SpatialLM1.0-Qwen-0.5B&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/manycore-research/SpatialLM-Qwen-0.5B"&gt;ü§ó HuggingFace&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;Tested with the following environment:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.11&lt;/li&gt; 
 &lt;li&gt;Pytorch 2.4.1&lt;/li&gt; 
 &lt;li&gt;CUDA Version 12.4&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# clone the repository
git clone https://github.com/manycore-research/SpatialLM.git
cd SpatialLM

# create a conda environment with cuda 12.4
conda create -n spatiallm python=3.11
conda activate spatiallm
conda install -y -c nvidia/label/cuda-12.4.0 cuda-toolkit conda-forge::sparsehash

# Install dependencies with poetry
pip install poetry &amp;amp;&amp;amp; poetry config virtualenvs.create false --local
poetry install
# SpatialLM1.0 dependency
poe install-torchsparse # Building wheel for torchsparse will take a while
# SpatialLM1.1 dependency
poe install-sonata # Building wheel for flash-attn will take a while
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Inference&lt;/h3&gt; 
&lt;p&gt;In the current version of SpatialLM, input point clouds are considered axis-aligned where the z-axis is the up axis. This orientation is crucial for maintaining consistency in spatial understanding and scene interpretation across different datasets and applications. Example preprocessed point clouds, reconstructed from RGB videos using &lt;a href="https://github.com/rmurai0610/MASt3R-SLAM"&gt;MASt3R-SLAM&lt;/a&gt;, are available in &lt;a href="https://raw.githubusercontent.com/manycore-research/SpatialLM/main/#spatiallm-testset"&gt;SpatialLM-Testset&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Download an example point cloud:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;huggingface-cli download manycore-research/SpatialLM-Testset pcd/scene0000_00.ply --repo-type dataset --local-dir .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run inference:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python inference.py --point_cloud pcd/scene0000_00.ply --output scene0000_00.txt --model_path manycore-research/SpatialLM1.1-Qwen-0.5B
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Detection with user-specified categories&lt;/h3&gt; 
&lt;p&gt;SpatialLM1.1 supports object detection conditioned on user-specified categories by leveraging the flexibility of LLMs.&lt;/p&gt; 
&lt;p&gt;SpatialLM1.1 offers three variants of structured indoor modeling tasks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Structured Reconstruction&lt;/strong&gt;: Detect walls, doors, windows, boxes.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Layout Estimation&lt;/strong&gt;: Detect walls, doors, windows.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;3D Object Detection&lt;/strong&gt;: Detect boxes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For tasks that include object box estimation, you can specify a subset of the 59 furniture categories, and the model will only predict objects within those specified categories. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python inference.py --point_cloud pcd/scene0000_00.ply --output scene0000_00.txt --model_path manycore-research/SpatialLM1.1-Qwen-0.5B --detect_type object --category bed nightstand
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Visualization&lt;/h3&gt; 
&lt;p&gt;Use &lt;code&gt;rerun&lt;/code&gt; to visualize the point cloud and the predicted structured 3D layout output:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Convert the predicted layout to Rerun format
python visualize.py --point_cloud pcd/scene0000_00.ply --layout scene0000_00.txt --save scene0000_00.rrd

# Visualize the point cloud and the predicted layout
rerun scene0000_00.rrd
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Evaluation&lt;/h3&gt; 
&lt;p&gt;To evaluate the performance of SpatialLM, we provide &lt;code&gt;eval.py&lt;/code&gt; script that reports the benchmark results on the SpatialLM-Testset in the table below in section &lt;a href="https://raw.githubusercontent.com/manycore-research/SpatialLM/main/#benchmark-results"&gt;Benchmark Results&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Download the testset:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;huggingface-cli download manycore-research/SpatialLM-Testset --repo-type dataset --local-dir SpatialLM-Testset
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run evaluation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run inference on the PLY point clouds in folder SpatialLM-Testset/pcd with SpatialLM1.1-Qwen-0.5B model
python inference.py --point_cloud SpatialLM-Testset/pcd --output SpatialLM-Testset/pred --model_path manycore-research/SpatialLM1.1-Qwen-0.5B

# Evaluate the predicted layouts
python eval.py --metadata SpatialLM-Testset/test.csv --gt_dir SpatialLM-Testset/layout --pred_dir SpatialLM-Testset/pred --label_mapping SpatialLM-Testset/benchmark_categories.tsv
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Example using a custom video&lt;/h3&gt; 
&lt;p&gt;We provide an example of how to use our model to estimate scene layout starting from a RGB video with the newly released &lt;a href="https://github.com/PKU-VCL-3DV/SLAM3R"&gt;SLAM3R&lt;/a&gt; in &lt;a href="https://raw.githubusercontent.com/manycore-research/SpatialLM/main/EXAMPLE.md"&gt;EXAMPLE.md&lt;/a&gt;. These steps work for MASt3R-SLAM, and other reconstruction methods as well.&lt;/p&gt; 
&lt;h2&gt;SpatialLM Testset&lt;/h2&gt; 
&lt;p&gt;We provide a test set of 107 preprocessed point clouds, reconstructed from RGB videos using &lt;a href="https://github.com/rmurai0610/MASt3R-SLAM"&gt;MASt3R-SLAM&lt;/a&gt;. SpatialLM-Testset is quite challenging compared to prior clean RGBD scans datasets due to the noises and occlusions in the point clouds reconstructed from monocular RGB videos.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Dataset&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Download&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;SpatialLM-Testset&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/datasets/manycore-research/SpatialLM-TestSet"&gt;ü§ó Datasets&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;Benchmark Results&lt;/h2&gt; 
&lt;h3&gt;Layout Estimation&lt;/h3&gt; 
&lt;p&gt;Layout estimation focuses on predicting architectural elements, i.e., walls, doors, and windows, within an indoor scene. We evaluated this task on the &lt;a href="https://structured3d-dataset.org"&gt;Structured3D&lt;/a&gt; dataset. For &lt;a href="https://github.com/ywyue/RoomFormer"&gt;RoomFormer&lt;/a&gt;, we directly downloaded the model checkpoint. SceneScript and SpatialLM were first trained on our dataset, and further fine-tuned on Structured3D.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Method&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;RoomFormer&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;SceneScript (finetuned)&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;SpatialLM1.1-Qwen-0.5B (finetuned)&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;F1 @.25 IoU&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;70.4&lt;/td&gt; 
    &lt;td align="center"&gt;83.1&lt;/td&gt; 
    &lt;td align="center"&gt;86.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;F1 @.5 IoU&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;67.2&lt;/td&gt; 
    &lt;td align="center"&gt;80.8&lt;/td&gt; 
    &lt;td align="center"&gt;84.6&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h3&gt;3D Object Detection&lt;/h3&gt; 
&lt;p&gt;We evaluate 3D object detection on &lt;a href="http://www.scan-net.org"&gt;ScanNet&lt;/a&gt; with annotations of 18 object categories. For &lt;a href="https://github.com/V-DETR/V-DETR"&gt;V-DETR&lt;/a&gt;, we directly download the model checkpoint. SceneScript and SpatialLM were first trained on our dataset, and further fine-tuned on ScanNet.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Method&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;V-DETR&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;SceneScript (finetuned)&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;SpatialLM1.1-Qwen-0.5B (finetuned)&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;F1 @.25 IoU&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;65.1&lt;/td&gt; 
    &lt;td align="center"&gt;49.1&lt;/td&gt; 
    &lt;td align="center"&gt;65.6&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;F1 @.5 IoU&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;56.8&lt;/td&gt; 
    &lt;td align="center"&gt;36.8&lt;/td&gt; 
    &lt;td align="center"&gt;52.6&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h3&gt;Zero-shot Detection on Videos&lt;/h3&gt; 
&lt;p&gt;Zero-shot detection results on the challenging SpatialLM-Testset are reported in the following table:&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Method&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;SpatialLM1.1-Llama-1B&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;SpatialLM1.1-Qwen-0.5B&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;Layout&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;F1 @.25 IoU (2D)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;F1 @.25 IoU (2D)&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;wall&lt;/td&gt; 
    &lt;td align="center"&gt;68.9&lt;/td&gt; 
    &lt;td align="center"&gt;68.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;door&lt;/td&gt; 
    &lt;td align="center"&gt;46.3&lt;/td&gt; 
    &lt;td align="center"&gt;43.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;window&lt;/td&gt; 
    &lt;td align="center"&gt;43.8&lt;/td&gt; 
    &lt;td align="center"&gt;47.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;Objects&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;F1 @.25 IoU (3D)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;F1 @.25 IoU (2D)&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;curtain&lt;/td&gt; 
    &lt;td align="center"&gt;34.9&lt;/td&gt; 
    &lt;td align="center"&gt;37.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;nightstand&lt;/td&gt; 
    &lt;td align="center"&gt;62.8&lt;/td&gt; 
    &lt;td align="center"&gt;67.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;chandelier&lt;/td&gt; 
    &lt;td align="center"&gt;53.5&lt;/td&gt; 
    &lt;td align="center"&gt;36.8&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;wardrobe&lt;/td&gt; 
    &lt;td align="center"&gt;29.4&lt;/td&gt; 
    &lt;td align="center"&gt;39.6&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;bed&lt;/td&gt; 
    &lt;td align="center"&gt;96.8&lt;/td&gt; 
    &lt;td align="center"&gt;95.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;sofa&lt;/td&gt; 
    &lt;td align="center"&gt;66.9&lt;/td&gt; 
    &lt;td align="center"&gt;69.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;chair&lt;/td&gt; 
    &lt;td align="center"&gt;20.8&lt;/td&gt; 
    &lt;td align="center"&gt;32.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;cabinet&lt;/td&gt; 
    &lt;td align="center"&gt;15.2&lt;/td&gt; 
    &lt;td align="center"&gt;11.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;dining table&lt;/td&gt; 
    &lt;td align="center"&gt;40.7&lt;/td&gt; 
    &lt;td align="center"&gt;24.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;plants&lt;/td&gt; 
    &lt;td align="center"&gt;29.5&lt;/td&gt; 
    &lt;td align="center"&gt;26.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;tv cabinet&lt;/td&gt; 
    &lt;td align="center"&gt;34.4&lt;/td&gt; 
    &lt;td align="center"&gt;27.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;coffee table&lt;/td&gt; 
    &lt;td align="center"&gt;56.4&lt;/td&gt; 
    &lt;td align="center"&gt;64.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;side table&lt;/td&gt; 
    &lt;td align="center"&gt;14.6&lt;/td&gt; 
    &lt;td align="center"&gt;9.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;air conditioner&lt;/td&gt; 
    &lt;td align="center"&gt;16.7&lt;/td&gt; 
    &lt;td align="center"&gt;24.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;dresser&lt;/td&gt; 
    &lt;td align="center"&gt;46.7&lt;/td&gt; 
    &lt;td align="center"&gt;46.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;stool&lt;/td&gt; 
    &lt;td align="center"&gt;17.6&lt;/td&gt; 
    &lt;td align="center"&gt;30.8&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;refrigerator&lt;/td&gt; 
    &lt;td align="center"&gt;0.0&lt;/td&gt; 
    &lt;td align="center"&gt;16.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;painting&lt;/td&gt; 
    &lt;td align="center"&gt;34.9&lt;/td&gt; 
    &lt;td align="center"&gt;38.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;carpet&lt;/td&gt; 
    &lt;td align="center"&gt;40.3&lt;/td&gt; 
    &lt;td align="center"&gt;24.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;tv&lt;/td&gt; 
    &lt;td align="center"&gt;16.0&lt;/td&gt; 
    &lt;td align="center"&gt;18.0&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h3&gt;Result Visualizations&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;Layout Estimation&lt;/th&gt; 
    &lt;th align="center"&gt;Object Detection&lt;/th&gt; 
    &lt;th align="center"&gt;Zero-shot Reconstruction&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/manycore-research/SpatialLM/main/figures/stru3d.jpg" alt="Structured3D" /&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/manycore-research/SpatialLM/main/figures/scannet.jpg" alt="ScanNet" /&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/manycore-research/SpatialLM/main/figures/zeroshot.jpg" alt="Zero-shot" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;a href="https://manycore-research-azure.kujiale.com/manycore-research/SpatialLM/supplementary/visualization_layout.html"&gt;Structured3D Results&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://manycore-research-azure.kujiale.com/manycore-research/SpatialLM/supplementary/visualization_object.html"&gt;ScanNet Results&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://manycore-research-azure.kujiale.com/manycore-research/SpatialLM/supplementary/visualization_zeroshot.html"&gt;Zeroshot Results&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;SpatialLM-Llama-1B is derived from Llama3.2-1B-Instruct, which is licensed under the Llama3.2 license. SpatialLM-Qwen-0.5B is derived from the Qwen-2.5 series, originally licensed under the Apache 2.0 License.&lt;/p&gt; 
&lt;p&gt;SpatialLM1.0 are built upon the SceneScript point cloud encoder, licensed under the CC-BY-NC-4.0 License. TorchSparse, utilized in this project, is licensed under the MIT License.&lt;/p&gt; 
&lt;p&gt;SpatialLM1.1 are built upon Sonata point cloud encoder, model weight is licensed under the CC-BY-NC-4.0 License. Code built on Pointcept is licensed under the Apache 2.0 License.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find this work useful, please consider citing:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{SpatialLM,
    title         = {SpatialLM: Training Large Language Models for Structured Indoor Modeling},
    author        = {Mao, Yongsen and Zhong, Junhao and Fang, Chuan and Zheng, Jia and Tang, Rui and Zhu, Hao and Tan, Ping and Zhou, Zihan},
    journal       = {arXiv preprint},
    year          = {2025},
    eprint        = {2506.07491},
    archivePrefix = {arXiv},
    primaryClass  = {cs.CV}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;We would like to thank the following projects that made this work possible:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/meta-llama"&gt;Llama3.2&lt;/a&gt; | &lt;a href="https://github.com/QwenLM/Qwen2.5"&gt;Qwen2.5&lt;/a&gt; | &lt;a href="https://github.com/huggingface/transformers"&gt;Transformers&lt;/a&gt; | &lt;a href="https://github.com/facebookresearch/scenescript"&gt;SceneScript&lt;/a&gt; | &lt;a href="https://github.com/mit-han-lab/torchsparse"&gt;TorchSparse&lt;/a&gt; | &lt;a href="https://xywu.me/sonata/"&gt;Sonata&lt;/a&gt; | &lt;a href="https://github.com/Pointcept/Pointcept"&gt;Pointcept&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Librum-Reader/Librum</title>
      <link>https://github.com/Librum-Reader/Librum</link>
      <description>&lt;p&gt;The Librum client application&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Librum&lt;/h1&gt; 
&lt;p&gt;Librum is an application designed to make reading &lt;b&gt;enjoyable&lt;/b&gt; and &lt;b&gt;straightforward&lt;/b&gt; for everyone.&lt;/p&gt; 
&lt;p&gt;It's not &lt;strong&gt;just&lt;/strong&gt; an e-book reader. With Librum, you can manage your own online library and access it from any device anytime, anywhere. It has features like note-taking, AI tooling, and highlighting, while offering customization to make it as personal as you want!&lt;/p&gt; 
&lt;p&gt;Librum also provides free access to over 70,000 books and personal reading statistics while being free and completely open source.&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;Download Librum from &lt;a href="https://librumreader.com"&gt;our website&lt;/a&gt;!&lt;/p&gt; 
&lt;br /&gt; 
&lt;h1&gt;Table of Contents&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Librum-Reader/Librum/main/#Preview"&gt;Preview&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Librum-Reader/Librum/main/#Contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Librum-Reader/Librum/main/#Contact"&gt;Contact&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Librum-Reader/Librum/main/#Donations"&gt;Donations&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Librum-Reader/Librum/main/#Translations"&gt;Translations&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Librum-Reader/Librum/main/#Documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Librum-Reader/Librum/main/#Self-hosting"&gt;Self-hosting&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Librum-Reader/Librum/main/#Details"&gt;Details&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Librum-Reader/Librum/main/#Build-Guide"&gt;Build Guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h1&gt;Preview&lt;/h1&gt; 
&lt;p&gt;Setup and manage your own online library&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/Librum-Reader/Librum/assets/69865187/ea94fc68-1bf0-4933-8d80-43a57c6590c5" alt="HomeScreenDark" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;A simple and modern interface&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/Librum-Reader/Librum/assets/69865187/bf1d0401-62bd-4f4e-b008-523fb2efd275" alt="image" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;Add your books to collections, tag them, and sort them in any way you want&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/Librum-Reader/Librum/assets/69865187/00fec031-a835-4cae-89f1-79dbce24b356" alt="folders_dark" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;Customize Librum to make it personal to you&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/Librum-Reader/Librum/assets/69865187/b8995cf1-a0e6-4993-8c8b-92f7f8e79ebd" alt="image" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h1&gt;Contributing&lt;/h1&gt; 
&lt;p&gt;If you'd to support Librum's development, check out: &lt;a href="https://librumreader.com/contribute"&gt;https://librumreader.com/contribute&lt;/a&gt; &lt;br /&gt; &lt;br /&gt; All of the current feature requests, bugs and tasks are listed in the &lt;a href="https://github.com/Librum-Reader/Librum/issues"&gt;issues&lt;/a&gt;. Easy tasks are labeled "good first issue", so that is a good starting point. &lt;br /&gt; &lt;br /&gt; PS: Feel free to tag me (@DavidLazarescu) in the comments of any issue if you have questions.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h1&gt;Contact&lt;/h1&gt; 
&lt;p&gt;For questions, you can reach us under: &lt;a href="mailto:help@librumreader.com"&gt;help@librumreader.com&lt;/a&gt; &lt;br /&gt; For business related contact, reach out to us here: &lt;a href="mailto:contact@librumreader.com"&gt;contact@librumreader.com&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h1&gt;Donations&lt;/h1&gt; 
&lt;p&gt;Donations make it possible for us to cover our server costs and allow us to make investments into new areas of development. &lt;br /&gt; If you would like to support us, check out: &lt;a href="https://librumreader.com/contribute/donate"&gt;https://librumreader.com/contribute/donate&lt;/a&gt; or become a Github sponsor! &lt;br /&gt; &lt;br /&gt; As a team of opensource developers we rely on donations to continue working on projects like Librum. Your help is greatly appreciated.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h1&gt;Translations&lt;/h1&gt; 
&lt;p&gt;Librum is currently available in the languages listed &lt;a href="https://github.com/Librum-Reader/Librum/tree/main/src/presentation/translations"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;If you want to translate Librum to another language, follow the steps below:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Download &lt;a href="https://github.com/Librum-Reader/Librum/raw/dev/develop/src/presentation/translations/librum_en.ts"&gt;this file&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Rename the file to contain your language's suffix, e.g. "librum_ru.ts" for Russian or "librum_de.ts" for German&lt;/li&gt; 
 &lt;li&gt;Download the translation software (Qt Linguist) either for Windows from &lt;a href="https://github.com/thurask/Qt-Linguist"&gt;here&lt;/a&gt; or using the &lt;a href="https://www.qt.io/download-open-source"&gt;Qt Installer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Now start Qt Linguist, open the downloaded file, set the target language to the language you want to translate to and start translating. (Check out &lt;a href="https://youtu.be/xNIz78IPBu0?t=347"&gt;this guide&lt;/a&gt; for a quick overview of Qt Linguist)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Once you are done, create a pull request or open up an issue with your new translation file!&lt;br /&gt; If you run into any problems, need guidance or have questions, feel free to reach out to us at: &lt;a href="mailto:contact@librumreader.com"&gt;contact@librumreader.com&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;Notes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Make sure that your translations are approximately the same length as the original text&lt;/li&gt; 
 &lt;li&gt;Please carefully check for spelling mistakes (including punctuation and capitalization)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h1&gt;Documentation&lt;/h1&gt; 
&lt;p&gt;For documentation go to &lt;a href="https://github.com/Librum-Reader/Librum/wiki"&gt;Librum's GitHub-wiki&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h1&gt;Self-hosting&lt;/h1&gt; 
&lt;p&gt;To self-host Librum you need to run &lt;a href="https://github.com/Librum-Reader/Librum-Server"&gt;Librum-Server&lt;/a&gt; locally (instructions can be found &lt;a href="https://github.com/Librum-Reader/Librum-Server#self-hosting"&gt;here&lt;/a&gt;) and tell the client to use your self-hosted server by:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;(Linux) Editing ~/.config/Librum-Reader/Librum.conf and setting &lt;code&gt;selfHosted=true&lt;/code&gt; and &lt;code&gt;serverHost&lt;/code&gt; to your server's url (e.g. &lt;code&gt;serverHost=https://127.0.0.1:5001&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;(Windows) Opening the registry editor (Press &lt;code&gt;Win + R&lt;/code&gt; and search for &lt;code&gt;regedit&lt;/code&gt;), navigating to &lt;code&gt;HKEY_CURRENT_USER\Software\Librum-Reader\Librum&lt;/code&gt; and setting &lt;code&gt;selfHosted=true&lt;/code&gt; and &lt;code&gt;serverHost&lt;/code&gt; to your server's url&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Make sure to run the application before following the steps above, to generate the required files.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h1&gt;Details&lt;/h1&gt; 
&lt;h3&gt;Supported platforms&lt;/h3&gt; 
&lt;p&gt;Part of Librum's aim is to work on &lt;strong&gt;any&lt;/strong&gt; platform. No matter where you are or which device you use, you can always continue your book with Librum, as it is &lt;b&gt;cross platform&lt;/b&gt;.&lt;br /&gt; We support:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Windows&lt;/li&gt; 
 &lt;li&gt;GNU/Linux&lt;/li&gt; 
 &lt;li&gt;MacOS&lt;/li&gt; 
 &lt;li&gt;IOS (Coming Soon)&lt;/li&gt; 
 &lt;li&gt;Android (Coming Soon)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h3&gt;Supported formats&lt;/h3&gt; 
&lt;p&gt;Librum is the best choice for all kinds of books, since Librum supports &lt;b&gt;all&lt;/b&gt; major book formats&lt;br /&gt; including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PDF&lt;/li&gt; 
 &lt;li&gt;EPUB&lt;/li&gt; 
 &lt;li&gt;CBZ (Comic books)&lt;/li&gt; 
 &lt;li&gt;FB2&lt;/li&gt; 
 &lt;li&gt;TIFF&lt;/li&gt; 
 &lt;li&gt;Mobi&lt;/li&gt; 
 &lt;li&gt;XPS&lt;/li&gt; 
 &lt;li&gt;Images&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;And many more!&lt;/p&gt; 
&lt;br /&gt; 
&lt;h3&gt;Features&lt;/h3&gt; 
&lt;p&gt;Librum's objective is to make your reading more &lt;b&gt;productive&lt;/b&gt;; to that end, we provide you with a variety of features that you can access via a &lt;b&gt;simple&lt;/b&gt; and &lt;b&gt;straightforward&lt;/b&gt; interface.&lt;br /&gt; These features include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A modern e-reader&lt;/li&gt; 
 &lt;li&gt;A personalized and customizable online library&lt;/li&gt; 
 &lt;li&gt;Book meta-data editing&lt;/li&gt; 
 &lt;li&gt;A free in-app bookstore with more than 70,000 books&lt;/li&gt; 
 &lt;li&gt;Book syncing across all of your devices&lt;/li&gt; 
 &lt;li&gt;Highlighting&lt;/li&gt; 
 &lt;li&gt;Bookmarking&lt;/li&gt; 
 &lt;li&gt;Text search&lt;/li&gt; 
 &lt;li&gt;Unlimited customization&lt;/li&gt; 
 &lt;li&gt;Fine-grained organization through Librum's folder system&lt;/li&gt; 
 &lt;li&gt;Note-taking (Coming Soon)&lt;/li&gt; 
 &lt;li&gt;TTS (Coming Soon)&lt;/li&gt; 
 &lt;li&gt;Personalized reading statistics (Coming Soon)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Want a new feature? Feel free to leave a feature request ticket!&lt;/p&gt; 
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;h1&gt;Build Guide&lt;/h1&gt; 
&lt;p&gt;Follow this guide to build Librum from source. &lt;br /&gt;&lt;/p&gt; 
&lt;h2&gt;For GNU/Linux&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;cmake (&lt;a href="https://cmake.org/download"&gt;https://cmake.org/download&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;make (&lt;a href="http://ftp.gnu.org/gnu/make"&gt;http://ftp.gnu.org/gnu/make&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;g++ (&lt;a href="https://gcc.gnu.org"&gt;https://gcc.gnu.org&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;python3-venv (on ubuntu use &lt;code&gt;sudo apt install python3-venv&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Qt 6.5 (&lt;a href="https://www.qt.io/download-open-source"&gt;https://www.qt.io/download-open-source&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;The installation is straight forward, just follow the steps below:&lt;/p&gt; 
&lt;br /&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone the repository. &lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/Librum-Reader/Librum.git --recursive
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Step into the cloned project folder. &lt;pre&gt;&lt;code class="language-sh"&gt;cd Librum
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Create the build folder and step into it. &lt;pre&gt;&lt;code class="language-sh"&gt;mkdir build-Release
cd build-Release
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Run cmake. &lt;pre&gt;&lt;code class="language-sh"&gt;cmake -DCMAKE_INSTALL_PREFIX=/usr -DCMAKE_BUILD_TYPE=Release -DBUILD_TESTS=Off -DCMAKE_PREFIX_PATH=&amp;lt;path/to/Qt&amp;gt; ..
&lt;/code&gt;&lt;/pre&gt; Set &lt;code&gt;CMAKE_PREFIX_PATH&lt;/code&gt; to your Qt installation path. Installing Qt via the online installer usually installs it to &lt;code&gt;/home/&amp;lt;name&amp;gt;/Qt/&amp;lt;version&amp;gt;/gcc_64&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Build the project &lt;pre&gt;&lt;code class="language-sh"&gt;cmake --build . -j $(nproc)
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Install Librum &lt;pre&gt;&lt;code class="language-sh"&gt;cmake --install .
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;br /&gt; 
&lt;h3&gt;Troubleshooting&lt;/h3&gt; 
&lt;p&gt;Here are solutions to some common errors. If your error is not listed here, please open an issue. &lt;br /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Error: &lt;code&gt;Failed to find required Qt component "Quick".&lt;/code&gt;&lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Solution: Install the libGL mesa dev package, on ubuntu its &lt;code&gt;sudo apt install libgl1-mesa-dev&lt;/code&gt; and on fedora its &lt;code&gt;sudo dnf install mesa-libGL-devel&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Error: &lt;code&gt;Could not load the qt platform plugin "xcb" even though it was found&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Solution: Install the libxcb-cursor-dev, on ubuntu its &lt;code&gt;sudo apt install libxcb-cursor-dev&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h2&gt;For Windows&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;cmake (&lt;a href="https://cmake.org/download"&gt;https://cmake.org/download&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Visual Studio &lt;b&gt;19&lt;/b&gt; (&lt;a href="https://visualstudio.microsoft.com/de/vs/older-downloads"&gt;https://visualstudio.microsoft.com/de/vs/older-downloads&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Python (&lt;a href="https://www.python.org/downloads"&gt;https://www.python.org/downloads&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Qt 6.5 (&lt;a href="https://www.qt.io/download-open-source"&gt;https://www.qt.io/download-open-source&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;To build Librum on windows, run the following commands in the Powershell:&lt;/p&gt; 
&lt;br /&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone the repository. &lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/Librum-Reader/Librum.git --recursive
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Step into the cloned project folder. &lt;pre&gt;&lt;code class="language-sh"&gt;cd Librum
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Create the build folder and step into it. &lt;pre&gt;&lt;code class="language-sh"&gt;mkdir build
cd build
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Run cmake. &lt;pre&gt;&lt;code class="language-sh"&gt;cmake -DBUILD_TESTS=Off -DCMAKE_PREFIX_PATH=&amp;lt;path/to/qt&amp;gt; ..
&lt;/code&gt;&lt;/pre&gt; Set &lt;code&gt;CMAKE_PREFIX_PATH&lt;/code&gt; to your Qt installation path. Installing Qt via the online installer usually installs it to &lt;code&gt;&amp;lt;Drive&amp;gt;\\Qt\\&amp;lt;version&amp;gt;\\msvc2019_64&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Build the project &lt;pre&gt;&lt;code class="language-sh"&gt;cmake --build . --config Release
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Run the app &lt;pre&gt;&lt;code class="language-sh"&gt;./librum
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Additional Info&lt;/h3&gt; 
&lt;p&gt;Here are some things to keep in mind during the build process. &lt;br /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Make sure to add cmake and the Qt binaries to the &lt;code&gt;PATH&lt;/code&gt; environment variable&lt;/li&gt; 
 &lt;li&gt;You need Visual Studio 2019, newer versions will &lt;strong&gt;not&lt;/strong&gt; work&lt;/li&gt; 
 &lt;li&gt;For the Qt installation, you &lt;strong&gt;only&lt;/strong&gt; need to choose "MSVC 2019 64-bit", you can untick everything else to reduce the download size&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h2&gt;For MacOS&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;cmake (&lt;a href="https://cmake.org/download"&gt;https://cmake.org/download&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;make (&lt;a href="http://ftp.gnu.org/gnu/make"&gt;http://ftp.gnu.org/gnu/make&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;g++ (&lt;a href="https://gcc.gnu.org"&gt;https://gcc.gnu.org&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;python3 (&lt;a href="https://www.python.org/downloads"&gt;https://www.python.org/downloads&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Qt 6.5 (&lt;a href="https://www.qt.io/download-open-source"&gt;https://www.qt.io/download-open-source&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;The installation is straight forward, just follow the steps below:&lt;/p&gt; 
&lt;br /&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone the repository. &lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/Librum-Reader/Librum.git --recursive
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Step into the cloned project folder. &lt;pre&gt;&lt;code class="language-sh"&gt;cd Librum
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Create the build folder and step into it. &lt;pre&gt;&lt;code class="language-sh"&gt;mkdir build-Release
cd build-Release
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Run cmake. &lt;pre&gt;&lt;code class="language-sh"&gt;cmake -DCMAKE_INSTALL_PREFIX=/usr/local -DCMAKE_BUILD_TYPE=Release -DBUILD_TESTS=Off -DCMAKE_PREFIX_PATH=&amp;lt;path/to/Qt&amp;gt; ..
&lt;/code&gt;&lt;/pre&gt; Set &lt;code&gt;CMAKE_PREFIX_PATH&lt;/code&gt; to your Qt installation path. Installing Qt via the online installer usually installs it to &lt;code&gt;/Users/&amp;lt;name&amp;gt;/Qt/&amp;lt;version&amp;gt;/macos&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Build the project &lt;pre&gt;&lt;code class="language-sh"&gt;cmake --build . -j $(nproc)
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Install Librum &lt;pre&gt;&lt;code class="language-sh"&gt;cmake --install
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Note: Make sure to add &lt;code&gt;/usr/local/lib&lt;/code&gt; to your DYLIB path, for MacOS to find the installed libraries by exporting &lt;code&gt;DYLD_LIBRARY_PATH=$DYLD_LIBRARY_PATH:/usr/local/lib&lt;/code&gt;.&lt;/p&gt; 
&lt;br /&gt;</description>
    </item>
    
    <item>
      <title>dotnet/maui</title>
      <link>https://github.com/dotnet/maui</link>
      <description>&lt;p&gt;.NET MAUI is the .NET Multi-platform App UI, a framework for building native device applications spanning mobile, tablet, and desktop.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;.NET Multi-platform App UI (.NET MAUI)&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://dev.azure.com/xamarin/public/_build/latest?definitionId=57&amp;amp;repoName=dotnet%2Fmaui&amp;amp;branchName=main"&gt;&lt;img src="https://dev.azure.com/xamarin/public/_apis/build/status/MAUI-public?repoName=dotnet%2Fmaui&amp;amp;branchName=main&amp;amp;label=Public" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://devdiv.visualstudio.com/DevDiv/_build/latest?definitionId=13330&amp;amp;repoName=dotnet%2Fmaui&amp;amp;branchName=main"&gt;&lt;img src="https://devdiv.visualstudio.com/DevDiv/_apis/build/status/MAUI?repoName=dotnet%2Fmaui&amp;amp;branchName=main&amp;amp;label=Private" alt="Build Status" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://dotnet.microsoft.com/en-us/apps/maui"&gt;.NET Multi-platform App UI (.NET MAUI)&lt;/a&gt; is a cross-platform framework for creating mobile and desktop apps with C# and XAML. Using .NET MAUI, you can develop apps that can run on Android, iOS, iPadOS, macOS, and Windows from a single shared codebase.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://dot.net/maui"&gt;Install .NET MAUI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.microsoft.com/dotnet/maui"&gt;.NET MAUI Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dotnet/maui-samples"&gt;.NET MAUI Samples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dotnet/maui/main/.github/DEVELOPMENT.md"&gt;Development Guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;.NET Multi-platform App UI (.NET MAUI) is the evolution of Xamarin.Forms that expand capabilities beyond mobile Android and iOS into desktop apps for Windows and macOS. With .NET MAUI, you can build apps that perform great on any device that runs Windows, macOS, Android, &amp;amp; iOS from a single codebase. Coupled with Visual Studio productivity tools and emulators, .NET and Visual Studio significantly speed up the development process for building apps that target the widest possible set of devices. Use a single development stack that supports the best-of-breed solutions for all modern workloads with a unified SDK, base class libraries, and a toolchain. &lt;a href="https://docs.microsoft.com/dotnet/maui/what-is-maui"&gt;Read More&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/dotnet/maui/main/Assets/maui-weather-hero-sm.png" alt=".NET MAUI Weather App on all platforms" /&gt;&lt;/p&gt; 
&lt;h2&gt;Current News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;2025 - &lt;a href="https://learn.microsoft.com/dotnet/maui/whats-new/dotnet-10"&gt;What's new in .NET MAUI for .NET 10&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;November 13, 2024 - &lt;a href="https://learn.microsoft.com/dotnet/maui/whats-new/dotnet-9"&gt;What's new in .NET MAUI for .NET 9&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;November 12, 2024 - &lt;a href="https://devblogs.microsoft.com/dotnet/announcing-dotnet-9/#.net-maui-%E2%80%93-enhancing-multi-platform-app-development"&gt;Announcing .NET 9&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;October 22, 2024 - &lt;a href="https://devblogs.microsoft.com/dotnet/dotnet-maui-welcomes-syncfusion-open-source-contributions/"&gt;.NET MAUI Welcomes Syncfusion Open-source Contributions&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Follow the &lt;a href="https://devblogs.microsoft.com/dotnet/category/net-maui/"&gt;.NET MAUI Blog&lt;/a&gt; and visit the &lt;a href="https://github.com/dotnet/maui/wiki/News"&gt;News&lt;/a&gt; wiki page for more news and updates.&lt;/p&gt; 
&lt;h2&gt;FAQs&lt;/h2&gt; 
&lt;p&gt;Do you have questions? Do not worry; we have prepared a complete &lt;a href="https://github.com/dotnet/maui/wiki/FAQs"&gt;FAQ&lt;/a&gt; answering the most common questions.&lt;/p&gt; 
&lt;h2&gt;How to Engage, Contribute, and Give Feedback&lt;/h2&gt; 
&lt;p&gt;Some of the best ways to &lt;a href="https://raw.githubusercontent.com/dotnet/maui/main/.github/CONTRIBUTING.md"&gt;contribute&lt;/a&gt; are to try things out, file issues, join in design conversations, and make pull-requests. Proposals for changes specific to MAUI can be found &lt;a href="https://github.com/dotnet/maui/issues"&gt;here for discussion&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/dotnet/maui/main/.github/CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/dotnet/maui/main/.github/CODE_OF_CONDUCT.md"&gt;CODE-OF-CONDUCT&lt;/a&gt; and the &lt;a href="https://raw.githubusercontent.com/dotnet/maui/main/.github/DEVELOPMENT.md"&gt;Development Guide&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/magentic-ui</title>
      <link>https://github.com/microsoft/magentic-ui</link>
      <description>&lt;p&gt;A research prototype of a human-centered web agent&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/microsoft/magentic-ui/main/docs/img/magui-readme-logo.svg?sanitize=true" alt="Magentic-UI Logo" /&gt; 
 &lt;p&gt;&lt;em&gt;Automate your web tasks while you stay in control&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://pypi.python.org/pypi/magentic_ui"&gt;&lt;img src="https://img.shields.io/pypi/v/magentic_ui.svg?sanitize=true" alt="image" /&gt;&lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/magentic_ui"&gt;&lt;img src="https://img.shields.io/pypi/l/magentic_ui.svg?sanitize=true" alt="image" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/python-3.10%20%7C%203.11%20%7C%203.12%20%7C%203.13-blue" alt="Python Versions" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;Magentic-UI is a &lt;strong&gt;research prototype&lt;/strong&gt; of a human-centered interface powered by a multi-agent system that can browse and perform actions on the web, generate and execute code, and generate and analyze files.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/7975fc26-1a18-4acb-8bf9-321171eeade7"&gt;https://github.com/user-attachments/assets/7975fc26-1a18-4acb-8bf9-321171eeade7&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;p&gt;Here's how you can get started with Magentic-UI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Setup environment
python3 -m venv .venv
source .venv/bin/activate
pip install magentic-ui --upgrade

# 2. Set your API key
export OPENAI_API_KEY="your-api-key-here"

# 3. Launch Magentic-UI
magentic-ui --port 8081
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then open &lt;a href="http://localhost:8081"&gt;http://localhost:8081&lt;/a&gt; in your browser to interact with Magentic-UI!&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;: Requires Docker and Python 3.10+. Windows users should use WSL2. See &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#%EF%B8%8F-installation"&gt;detailed installation&lt;/a&gt; for more info.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;‚ú® What's New&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;File Upload Support&lt;/strong&gt;: Upload any file through the UI for analysis or modification&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP Agents&lt;/strong&gt;: Extend capabilities with your favorite MCP servers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easier Installation&lt;/strong&gt;: We have uploaded our docker containers to GHCR so you no longer need to build any containers! Installation time now is much quicker.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Alternative Usage Options&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Without Docker&lt;/strong&gt; (limited functionality: no code execution):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-ui --run-without-docker --port 8081
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Command Line Interface&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-cli --work-dir PATH/TO/STORE/DATA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Custom LLM Clients&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Azure
pip install magentic-ui[azure]

# Ollama (local models)
pip install magentic-ui[ollama]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can then pass a config file to the &lt;code&gt;magentic-ui&lt;/code&gt; command (&lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#model-client-configuration"&gt; client config&lt;/a&gt;) or change the model client inside the UI settings.&lt;/p&gt; 
&lt;p&gt;For further details on installation please read the &lt;a href="#Ô∏è-installation"&gt;üõ†Ô∏è Installation&lt;/a&gt; section. For common installation issues and their solutions, please refer to the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/TROUBLESHOOTING.md"&gt;troubleshooting document&lt;/a&gt;. See advanced usage instructions with the command &lt;code&gt;magentic-ui --help&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Quick Navigation:&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#-how-it-works"&gt;üü™ How it Works&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &lt;a href="#Ô∏è-installation"&gt;üõ†Ô∏è Installation&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#troubleshooting"&gt;‚ö†Ô∏è Troubleshooting&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#contributing"&gt;ü§ù Contributing&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#license"&gt;üìÑ License&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üü™ How it Works&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/magentic-ui/main/docs/img/magenticui_running.png" alt="Magentic-UI" height="400" /&gt; &lt;/p&gt; 
&lt;p&gt;Magentic-UI is especially useful for web tasks that require actions on the web (e.g., filling a form, customizing a food order), deep navigation through websites not indexed by search engines (e.g., filtering flights, finding a link from a personal site) or tasks that need web navigation and code execution (e.g., generate a chart from online data).&lt;/p&gt; 
&lt;p&gt;The interface of Magentic-UI is displayed in the screenshot above and consists of two panels. The left side panel is the sessions navigator where users can create new sessions to solve new tasks, switch between sessions and check on session progress with the session status indicators (üî¥ needs input, ‚úÖ task done, ‚Ü∫ task in progress).&lt;/p&gt; 
&lt;p&gt;The right-side panel displays the session selected. This is where you can type your query to Magentic-UI alongside any file attachments and observe detailed task progress as well as interact with the agents. The session display itself is split in two panels: the left side is where Magentic-UI presents the plan, task progress and asks for action approvals, the right side is a browser view where you can see web agent actions in real time and interact with the browser. Finally, at the top of the session display is a progress bar that updates as Magentic-UI makes progress.&lt;/p&gt; 
&lt;p&gt;The example below shows a step by step user interaction with Magentic-UI:&lt;/p&gt; 
&lt;!-- Screenshots --&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/magentic-ui/main/docs/img/magui-landing.png" alt="Magentic-UI Landing" width="45%" style="margin:10px;" /&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/magentic-ui/main/docs/img/magui-coplanning.png" alt="Co-Planning UI" width="45%" style="margin:10px;" /&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/magentic-ui/main/docs/img/magui-cotasking.png" alt="Co-Tasking UI" width="45%" style="margin:10px;" /&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/magentic-ui/main/docs/img/magui-actionguard.png" alt="Action Guard UI" width="45%" style="margin:10px;" /&gt; &lt;/p&gt; 
&lt;p&gt;What differentiates Magentic-UI from other browser use offerings is its transparent and controllable interface that allows for efficient human-in-the-loop involvement. Magentic-UI is built using &lt;a href="https://github.com/microsoft/autogen"&gt;AutoGen&lt;/a&gt; and provides a platform to study human-agent interaction and experiment with web agents. Key features include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üßë‚Äçü§ù‚Äçüßë &lt;strong&gt;Co-Planning&lt;/strong&gt;: Collaboratively create and approve step-by-step plans using chat and the plan editor.&lt;/li&gt; 
 &lt;li&gt;ü§ù &lt;strong&gt;Co-Tasking&lt;/strong&gt;: Interrupt and guide the task execution using the web browser directly or through chat. Magentic-UI can also ask for clarifications and help when needed.&lt;/li&gt; 
 &lt;li&gt;üõ°Ô∏è &lt;strong&gt;Action Guards&lt;/strong&gt;: Sensitive actions are only executed with explicit user approvals.&lt;/li&gt; 
 &lt;li&gt;üß† &lt;strong&gt;Plan Learning and Retrieval&lt;/strong&gt;: Learn from previous runs to improve future task automation and save them in a plan gallery. Automatically or manually retrieve saved plans in future tasks.&lt;/li&gt; 
 &lt;li&gt;üîÄ &lt;strong&gt;Parallel Task Execution&lt;/strong&gt;: You can run multiple tasks in parallel and session status indicators will let you know when Magentic-UI needs your input or has completed the task.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://www.youtube.com/watch?v=wOs-5SR8xOc" target="_blank"&gt; &lt;img src="https://img.youtube.com/vi/wOs-5SR8xOc/maxresdefault.jpg" alt="Watch the demo video" width="600" /&gt; &lt;/a&gt; 
 &lt;br /&gt; ‚ñ∂Ô∏è 
 &lt;em&gt; Click to watch a video and learn more about Magentic-UI &lt;/em&gt; 
&lt;/div&gt; 
&lt;h3&gt;Autonomous Evaluation&lt;/h3&gt; 
&lt;p&gt;To evaluate its autonomous capabilities, Magentic-UI has been tested against several benchmarks when running with o4-mini: &lt;a href="https://huggingface.co/datasets/gaia-benchmark/GAIA"&gt;GAIA&lt;/a&gt; test set (42.52%), which assesses general AI assistants across reasoning, tool use, and web interaction tasks ; &lt;a href="https://huggingface.co/AssistantBench"&gt;AssistantBench&lt;/a&gt; test set (27.60%), focusing on realistic, time-consuming web tasks; &lt;a href="https://github.com/MinorJerry/WebVoyager"&gt;WebVoyager&lt;/a&gt; (82.2%), measuring end-to-end web navigation in real-world scenarios; and &lt;a href="https://webgames.convergence.ai/"&gt;WebGames&lt;/a&gt; (45.5%), evaluating general-purpose web-browsing agents through interactive challenges. To reproduce these experimental results, please see the following &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/experiments/eval/README.md"&gt;instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you're interested in reading more checkout our &lt;a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/magentic-ui-report.pdf"&gt;technical report&lt;/a&gt; and &lt;a href="https://www.microsoft.com/en-us/research/blog/magentic-ui-an-experimental-human-centered-web-agent/"&gt;blog post&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üõ†Ô∏è Installation&lt;/h2&gt; 
&lt;h3&gt;Pre-Requisites&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you're using Windows, we highly recommend using &lt;a href="https://docs.microsoft.com/en-us/windows/wsl/install"&gt;WSL2&lt;/a&gt; (Windows Subsystem for Linux).&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;If running on &lt;strong&gt;Windows&lt;/strong&gt; or &lt;strong&gt;Mac&lt;/strong&gt; you should use &lt;a href="https://www.docker.com/products/docker-desktop/"&gt;Docker Desktop&lt;/a&gt; or if inside WSL2 you can install Docker directly inside WSL &lt;a href="https://gist.github.com/dehsilvadeveloper/c3bdf0f4cdcc5c177e2fe9be671820c7"&gt;docker in WSL2 guide&lt;/a&gt;. If running on &lt;strong&gt;Linux&lt;/strong&gt;, you should use &lt;a href="https://docs.docker.com/engine/install/"&gt;Docker Engine&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;If using Docker Desktop, make sure it is set up to use WSL2: - Go to Settings &amp;gt; Resources &amp;gt; WSL Integration - Enable integration with your development distro You can find more detailed instructions about this step &lt;a href="https://docs.microsoft.com/en-us/windows/wsl/tutorials/wsl-containers"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt; &lt;p&gt;During the Installation step, you will need to set up your &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;. To use other models, review the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#model-client-configuration"&gt;Model Client Configuration&lt;/a&gt; section below.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;You need at least &lt;a href="https://www.python.org/downloads/"&gt;Python 3.10&lt;/a&gt; installed.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;If you are on Windows, we recommend to run Magentic-UI inside &lt;a href="https://docs.microsoft.com/en-us/windows/wsl/install"&gt;WSL2&lt;/a&gt; (Windows Subsystem for Linux) for correct Docker and file path compatibility.&lt;/p&gt; 
&lt;h3&gt;PyPI Installation&lt;/h3&gt; 
&lt;p&gt;Magentic-UI is available on PyPI. We recommend using a virtual environment to avoid conflicts with other packages.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3 -m venv .venv
source .venv/bin/activate
pip install magentic-ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, if you use &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;&lt;code&gt;uv&lt;/code&gt;&lt;/a&gt; for dependency management, you can install Magentic-UI with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv --python=3.12 .venv
. .venv/bin/activate
uv pip install magentic-ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running Magentic-UI&lt;/h3&gt; 
&lt;p&gt;To run Magentic-UI, make sure that Docker is running, then run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-ui --port 8081
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Running this command for the first time will pull two docker images required for the Magentic-UI agents. If you encounter problems, you can build them directly with the following command:&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd docker
sh build-all.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you face issues with Docker, please refer to the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/TROUBLESHOOTING.md"&gt;TROUBLESHOOTING.md&lt;/a&gt; document.&lt;/p&gt; 
&lt;p&gt;Once the server is running, you can access the UI at &lt;a href="http://localhost:8081"&gt;http://localhost:8081&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Configuration&lt;/h3&gt; 
&lt;h4&gt;Model Client Configuration&lt;/h4&gt; 
&lt;p&gt;If you want to use a different OpenAI key, or if you want to configure use with Azure OpenAI or Ollama, you can do so inside the UI by navigating to settings (top right icon) and changing model configuration. Another option is to pass a yaml config file when you start Magentic-UI which will override any settings in the UI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-ui --port 8081 --config config.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Where the &lt;code&gt;config.yaml&lt;/code&gt; should look as follows with an AutoGen model client configuration:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;gpt4o_client: &amp;amp;gpt4o_client
    provider: OpenAIChatCompletionClient
    config:
      model: gpt-4o-2024-08-06
      api_key: null
      base_url: null
      max_retries: 5

orchestrator_client: *gpt4o_client
coder_client: *gpt4o_client
web_surfer_client: *gpt4o_client
file_surfer_client: *gpt4o_client
action_guard_client: *gpt4o_client
plan_learning_client: *gpt4o_client
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can change the client for each of the agents using the config file and use AzureOpenAI (&lt;code&gt;AzureOpenAIChatCompletionClient&lt;/code&gt;), Ollama and other clients.&lt;/p&gt; 
&lt;h4&gt;MCP Server Configuration&lt;/h4&gt; 
&lt;p&gt;You can also extend Magentic-UI's capabilities by adding custom "McpAgents" to the multi-agent team. Each McpAgent can have access to one or more MCP Servers. You can specify these agents via the &lt;code&gt;mcp_agent_configs&lt;/code&gt; parameter in your &lt;code&gt;config.yaml&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For example, here's an agent called "airbnb_surfer" that has access to the OpenBnb MCP Server running locally via Stdio.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;mcp_agent_configs:
  - name: airbnb_surfer
    description: "The airbnb_surfer has direct access to AirBnB."
    model_client: 
      provider: OpenAIChatCompletionClient
      config:
        model: gpt-4.1-2025-04-14
      max_retries: 10
    system_message: |-
      You are AirBnb Surfer, a helpful digital assistant that can help users acces AirBnB.

      You have access to a suite of tools provided by the AirBnB API. Use those tools to satisfy the users requests.
    reflect_on_tool_use: false
    mcp_servers:
      - server_name: AirBnB
        server_params:
          type: StdioServerParams
          command: npx
          args:
            - -y
            - "@openbnb/mcp-server-airbnb"
            - --ignore-robots-txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Under the hood, each &lt;code&gt;McpAgent&lt;/code&gt; is just a &lt;code&gt;autogen_agentchat.agents.AssistantAgent&lt;/code&gt; with the set of MCP Servers exposed as an &lt;code&gt;AggregateMcpWorkbench&lt;/code&gt; which is simply a named collection of &lt;code&gt;autogen_ext.tools.mcp.McpWorkbench&lt;/code&gt; objects (one per MCP Server).&lt;/p&gt; 
&lt;p&gt;Currently the supported MCP Server types are &lt;code&gt;autogen_ext.tools.mcp.StdioServerParams&lt;/code&gt; and &lt;code&gt;autogen_ext.tools.mcp.SseServerParams&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Building Magentic-UI from source&lt;/h3&gt; 
&lt;p&gt;This step is primarily for users seeking to make modifications to the code, are having trouble with the pypi installation or want the latest code before a pypi version release.&lt;/p&gt; 
&lt;h4&gt;1. Make sure the above prerequisites are installed, and that Docker is running.&lt;/h4&gt; 
&lt;h4&gt;2. Clone the repository to your local machine:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/microsoft/magentic-ui.git
cd magentic-ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3. Install Magentic-UI's dependencies with uv or your favorite package manager:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# install uv through https://docs.astral.sh/uv/getting-started/installation/
uv venv --python=3.12 .venv
uv sync --all-extras
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;4. Build the frontend:&lt;/h4&gt; 
&lt;p&gt;First make sure to install node:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# install nvm to install node
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.1/install.sh | bash
nvm install node
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then install the frontend:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd frontend
npm install -g gatsby-cli
npm install --global yarn
yarn install
yarn build
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;5. Run Magentic-UI, as usual.&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-ui --port 8081
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Running the UI from source&lt;/h4&gt; 
&lt;p&gt;If you are making changes to the source code of the UI, you can run the frontend in development mode so that it will automatically update when you make changes for faster development.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open a separate terminal and change directory to the frontend&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd frontend
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Create a &lt;code&gt;.env.development&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cp .env.default .env.development
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Launch frontend server&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm run start
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Then run the UI:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-ui --port 8081
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The frontend from source will be available at &lt;a href="http://localhost:8000"&gt;http://localhost:8000&lt;/a&gt;, and the compiled frontend will be available at &lt;a href="http://localhost:8081"&gt;http://localhost:8081&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;p&gt;If you were unable to get Magentic-UI running, do not worry! The first step is to make sure you have followed the steps outlined above, particularly with the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#pre-requisites"&gt;pre-requisites&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For common issues and their solutions, please refer to the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/TROUBLESHOOTING.md"&gt;TROUBLESHOOTING.md&lt;/a&gt; file in this repository. If you do not see your problem there, please open a &lt;code&gt;GitHub Issue&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. For information about contributing to Magentic-UI, please see our &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; guide, which includes current issues to be resolved and other forms of contributing.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information, see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Microsoft, and any contributors, grant you a license to any code in the repository under the &lt;a href="https://opensource.org/licenses/MIT"&gt;MIT License&lt;/a&gt;. See the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/LICENSE"&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt; 
&lt;p&gt;Microsoft, Windows, Microsoft Azure, and/or other Microsoft products and services referenced in the documentation may be either trademarks or registered trademarks of Microsoft in the United States and/or other countries. The licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks. Microsoft's general trademark guidelines can be found at &lt;a href="http://go.microsoft.com/fwlink/?LinkID=254653"&gt;http://go.microsoft.com/fwlink/?LinkID=254653&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Any use of third-party trademarks or logos are subject to those third-party's policies.&lt;/p&gt; 
&lt;p&gt;Privacy information can be found at &lt;a href="https://go.microsoft.com/fwlink/?LinkId=521839"&gt;https://go.microsoft.com/fwlink/?LinkId=521839&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Microsoft and any contributors reserve all other rights, whether under their respective copyrights, patents, or trademarks, whether by implication, estoppel, or otherwise.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>redis/go-redis</title>
      <link>https://github.com/redis/go-redis</link>
      <description>&lt;p&gt;Redis Go client&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Redis client for Go&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/redis/go-redis/actions"&gt;&lt;img src="https://github.com/redis/go-redis/actions/workflows/build.yml/badge.svg?sanitize=true" alt="build workflow" /&gt;&lt;/a&gt; &lt;a href="https://pkg.go.dev/github.com/redis/go-redis/v9?tab=doc"&gt;&lt;img src="https://pkg.go.dev/badge/github.com/redis/go-redis/v9" alt="PkgGoDev" /&gt;&lt;/a&gt; &lt;a href="https://redis.uptrace.dev/"&gt;&lt;img src="https://img.shields.io/badge/redis-documentation-informational" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://goreportcard.com/report/github.com/redis/go-redis/v9"&gt;&lt;img src="https://goreportcard.com/badge/github.com/redis/go-redis/v9" alt="Go Report Card" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/github/redis/go-redis"&gt;&lt;img src="https://codecov.io/github/redis/go-redis/graph/badge.svg?token=tsrCZKuSSw" alt="codecov" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/W4txy5AeKM"&gt;&lt;img src="https://img.shields.io/discord/697882427875393627.svg?style=social&amp;amp;logo=discord" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://www.twitch.tv/redisinc"&gt;&lt;img src="https://img.shields.io/twitch/status/redisinc?style=social" alt="Twitch" /&gt;&lt;/a&gt; &lt;a href="https://www.youtube.com/redisinc"&gt;&lt;img src="https://img.shields.io/youtube/channel/views/UCD78lHSwYqMlyetR0_P4Vig?style=social" alt="YouTube" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/redisinc"&gt;&lt;img src="https://img.shields.io/twitter/follow/redisinc?style=social" alt="Twitter" /&gt;&lt;/a&gt; &lt;a href="https://stackoverflow.com/questions/tagged/go-redis"&gt;&lt;img src="https://img.shields.io/stackexchange/stackoverflow/t/go-redis?style=social&amp;amp;logo=stackoverflow&amp;amp;label=Stackoverflow" alt="Stack Exchange questions" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;go-redis is the official Redis client library for the Go programming language. It offers a straightforward interface for interacting with Redis servers.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Supported versions&lt;/h2&gt; 
&lt;p&gt;In &lt;code&gt;go-redis&lt;/code&gt; we are aiming to support the last three releases of Redis. Currently, this means we do support:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/redis/redis/7.2/00-RELEASENOTES"&gt;Redis 7.2&lt;/a&gt; - using Redis Stack 7.2 for modules support&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/redis/redis/7.4/00-RELEASENOTES"&gt;Redis 7.4&lt;/a&gt; - using Redis Stack 7.4 for modules support&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/redis/redis/8.0/00-RELEASENOTES"&gt;Redis 8.0&lt;/a&gt; - using Redis CE 8.0 where modules are included&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/redis/redis/8.2/00-RELEASENOTES"&gt;Redis 8.2&lt;/a&gt; - using Redis CE 8.2 where modules are included&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Although the &lt;code&gt;go.mod&lt;/code&gt; states it requires at minimum &lt;code&gt;go 1.18&lt;/code&gt;, our CI is configured to run the tests against all three versions of Redis and latest two versions of Go (&lt;a href="https://go.dev/doc/devel/release#go1.23.0"&gt;1.23&lt;/a&gt;, &lt;a href="https://go.dev/doc/devel/release#go1.24.0"&gt;1.24&lt;/a&gt;). We observe that some modules related test may not pass with Redis Stack 7.2 and some commands are changed with Redis CE 8.0. Please do refer to the documentation and the tests if you experience any issues. We do plan to update the go version in the &lt;code&gt;go.mod&lt;/code&gt; to &lt;code&gt;go 1.24&lt;/code&gt; in one of the next releases.&lt;/p&gt; 
&lt;h2&gt;How do I Redis?&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://university.redis.com/"&gt;Learn for free at Redis University&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://launchpad.redis.com/"&gt;Build faster with the Redis Launchpad&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://redis.com/try-free/"&gt;Try the Redis Cloud&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://developer.redis.com/"&gt;Dive in developer tutorials&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://redis.com/community/"&gt;Join the Redis community&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://redis.com/company/careers/jobs/"&gt;Work at Redis&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://redis.uptrace.dev"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://redis.uptrace.dev/zh/"&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/redis/go-redis/discussions"&gt;Discussions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/W4txy5AeKM"&gt;Chat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pkg.go.dev/github.com/redis/go-redis/v9"&gt;Reference&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pkg.go.dev/github.com/redis/go-redis/v9#pkg-examples"&gt;Examples&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Ecosystem&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/go-redis/redismock"&gt;Redis Mock&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/bsm/redislock"&gt;Distributed Locks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/go-redis/cache"&gt;Redis Cache&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/go-redis/redis_rate"&gt;Rate limiting&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This client also works with &lt;a href="https://github.com/apache/incubator-kvrocks"&gt;Kvrocks&lt;/a&gt;, a distributed key value NoSQL database that uses RocksDB as storage engine and is compatible with Redis protocol.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Redis commands except QUIT and SYNC.&lt;/li&gt; 
 &lt;li&gt;Automatic connection pooling.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/redis/go-redis/master/#1-streaming-credentials-provider-highest-priority"&gt;StreamingCredentialsProvider (e.g. entra id, oauth)&lt;/a&gt; (experimental)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://redis.uptrace.dev/guide/go-redis-pubsub.html"&gt;Pub/Sub&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://redis.uptrace.dev/guide/go-redis-pipelines.html"&gt;Pipelines and transactions&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://redis.uptrace.dev/guide/lua-scripting.html"&gt;Scripting&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://redis.uptrace.dev/guide/go-redis-sentinel.html"&gt;Redis Sentinel&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://redis.uptrace.dev/guide/go-redis-cluster.html"&gt;Redis Cluster&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://redis.uptrace.dev/guide/ring.html"&gt;Redis Ring&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://redis.uptrace.dev/guide/redis-performance-monitoring.html"&gt;Redis Performance Monitoring&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://redis.io/docs/data-types/probabilistic/"&gt;Redis Probabilistic [RedisStack]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/redis/go-redis/master/#custom-buffer-sizes"&gt;Customizable read and write buffers size.&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;go-redis supports 2 last Go versions and requires a Go version with &lt;a href="https://github.com/golang/go/wiki/Modules"&gt;modules&lt;/a&gt; support. So make sure to initialize a Go module:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;go mod init github.com/my/repo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then install go-redis/&lt;strong&gt;v9&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;go get github.com/redis/go-redis/v9
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;import (
    "context"
    "fmt"

    "github.com/redis/go-redis/v9"
)

var ctx = context.Background()

func ExampleClient() {
    rdb := redis.NewClient(&amp;amp;redis.Options{
        Addr:     "localhost:6379",
        Password: "", // no password set
        DB:       0,  // use default DB
    })

    err := rdb.Set(ctx, "key", "value", 0).Err()
    if err != nil {
        panic(err)
    }

    val, err := rdb.Get(ctx, "key").Result()
    if err != nil {
        panic(err)
    }
    fmt.Println("key", val)

    val2, err := rdb.Get(ctx, "key2").Result()
    if err == redis.Nil {
        fmt.Println("key2 does not exist")
    } else if err != nil {
        panic(err)
    } else {
        fmt.Println("key2", val2)
    }
    // Output: key value
    // key2 does not exist
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Authentication&lt;/h3&gt; 
&lt;p&gt;The Redis client supports multiple ways to provide authentication credentials, with a clear priority order. Here are the available options:&lt;/p&gt; 
&lt;h4&gt;1. Streaming Credentials Provider (Highest Priority) - Experimental feature&lt;/h4&gt; 
&lt;p&gt;The streaming credentials provider allows for dynamic credential updates during the connection lifetime. This is particularly useful for managed identity services and token-based authentication.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;type StreamingCredentialsProvider interface {
    Subscribe(listener CredentialsListener) (Credentials, UnsubscribeFunc, error)
}

type CredentialsListener interface {
    OnNext(credentials Credentials)  // Called when credentials are updated
    OnError(err error)              // Called when an error occurs
}

type Credentials interface {
    BasicAuth() (username string, password string)
    RawCredentials() string
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Example usage:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;rdb := redis.NewClient(&amp;amp;redis.Options{
    Addr: "localhost:6379",
    StreamingCredentialsProvider: &amp;amp;MyCredentialsProvider{},
})
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The streaming credentials provider can be used with &lt;a href="https://github.com/redis/go-redis-entraid"&gt;go-redis-entraid&lt;/a&gt; to enable Entra ID (formerly Azure AD) authentication. This allows for seamless integration with Azure's managed identity services and token-based authentication.&lt;/p&gt; 
&lt;p&gt;Example with Entra ID:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;import (
    "github.com/redis/go-redis/v9"
    "github.com/redis/go-redis-entraid"
)

// Create an Entra ID credentials provider
provider := entraid.NewDefaultAzureIdentityProvider()

// Configure Redis client with Entra ID authentication
rdb := redis.NewClient(&amp;amp;redis.Options{
    Addr: "your-redis-server.redis.cache.windows.net:6380",
    StreamingCredentialsProvider: provider,
    TLSConfig: &amp;amp;tls.Config{
        MinVersion: tls.VersionTLS12,
    },
})
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;2. Context-based Credentials Provider&lt;/h4&gt; 
&lt;p&gt;The context-based provider allows credentials to be determined at the time of each operation, using the context.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;rdb := redis.NewClient(&amp;amp;redis.Options{
    Addr: "localhost:6379",
    CredentialsProviderContext: func(ctx context.Context) (string, string, error) {
        // Return username, password, and any error
        return "user", "pass", nil
    },
})
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3. Regular Credentials Provider&lt;/h4&gt; 
&lt;p&gt;A simple function-based provider that returns static credentials.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;rdb := redis.NewClient(&amp;amp;redis.Options{
    Addr: "localhost:6379",
    CredentialsProvider: func() (string, string) {
        // Return username and password
        return "user", "pass"
    },
})
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;4. Username/Password Fields (Lowest Priority)&lt;/h4&gt; 
&lt;p&gt;The most basic way to provide credentials is through the &lt;code&gt;Username&lt;/code&gt; and &lt;code&gt;Password&lt;/code&gt; fields in the options.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;rdb := redis.NewClient(&amp;amp;redis.Options{
    Addr:     "localhost:6379",
    Username: "user",
    Password: "pass",
})
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Priority Order&lt;/h4&gt; 
&lt;p&gt;The client will use credentials in the following priority order:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Streaming Credentials Provider (if set)&lt;/li&gt; 
 &lt;li&gt;Context-based Credentials Provider (if set)&lt;/li&gt; 
 &lt;li&gt;Regular Credentials Provider (if set)&lt;/li&gt; 
 &lt;li&gt;Username/Password fields (if set)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;If none of these are set, the client will attempt to connect without authentication.&lt;/p&gt; 
&lt;h3&gt;Protocol Version&lt;/h3&gt; 
&lt;p&gt;The client supports both RESP2 and RESP3 protocols. You can specify the protocol version in the options:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;rdb := redis.NewClient(&amp;amp;redis.Options{
    Addr:     "localhost:6379",
    Password: "", // no password set
    DB:       0,  // use default DB
    Protocol: 3,  // specify 2 for RESP 2 or 3 for RESP 3
})
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Connecting via a redis url&lt;/h3&gt; 
&lt;p&gt;go-redis also supports connecting via the &lt;a href="https://github.com/redis/redis-specifications/tree/master/uri/redis.txt"&gt;redis uri specification&lt;/a&gt;. The example below demonstrates how the connection can easily be configured using a string, adhering to this specification.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;import (
    "github.com/redis/go-redis/v9"
)

func ExampleClient() *redis.Client {
    url := "redis://user:password@localhost:6379/0?protocol=3"
    opts, err := redis.ParseURL(url)
    if err != nil {
        panic(err)
    }

    return redis.NewClient(opts)
}

&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Instrument with OpenTelemetry&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;import (
    "github.com/redis/go-redis/v9"
    "github.com/redis/go-redis/extra/redisotel/v9"
    "errors"
)

func main() {
    ...
    rdb := redis.NewClient(&amp;amp;redis.Options{...})

    if err := errors.Join(redisotel.InstrumentTracing(rdb), redisotel.InstrumentMetrics(rdb)); err != nil {
        log.Fatal(err)
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Buffer Size Configuration&lt;/h3&gt; 
&lt;p&gt;go-redis uses 0.5MiB read and write buffers by default for optimal performance. For high-throughput applications or large pipelines, you can customize buffer sizes:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;rdb := redis.NewClient(&amp;amp;redis.Options{
    Addr:            "localhost:6379",
    ReadBufferSize:  1024 * 1024, // 1MiB read buffer
    WriteBufferSize: 1024 * 1024, // 1MiB write buffer
})
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Advanced Configuration&lt;/h3&gt; 
&lt;p&gt;go-redis supports extending the client identification phase to allow projects to send their own custom client identification.&lt;/p&gt; 
&lt;h4&gt;Default Client Identification&lt;/h4&gt; 
&lt;p&gt;By default, go-redis automatically sends the client library name and version during the connection process. This feature is available in redis-server as of version 7.2. As a result, the command is "fire and forget", meaning it should fail silently, in the case that the redis server does not support this feature.&lt;/p&gt; 
&lt;h4&gt;Disabling Identity Verification&lt;/h4&gt; 
&lt;p&gt;When connection identity verification is not required or needs to be explicitly disabled, a &lt;code&gt;DisableIdentity&lt;/code&gt; configuration option exists. Initially there was a typo and the option was named &lt;code&gt;DisableIndentity&lt;/code&gt; instead of &lt;code&gt;DisableIdentity&lt;/code&gt;. The misspelled option is marked as Deprecated and will be removed in V10 of this library. Although both options will work at the moment, the correct option is &lt;code&gt;DisableIdentity&lt;/code&gt;. The deprecated option will be removed in V10 of this library, so please use the correct option name to avoid any issues.&lt;/p&gt; 
&lt;p&gt;To disable verification, set the &lt;code&gt;DisableIdentity&lt;/code&gt; option to &lt;code&gt;true&lt;/code&gt; in the Redis client options:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;rdb := redis.NewClient(&amp;amp;redis.Options{
    Addr:            "localhost:6379",
    Password:        "",
    DB:              0,
    DisableIdentity: true, // Disable set-info on connect
})
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Unstable RESP3 Structures for RediSearch Commands&lt;/h4&gt; 
&lt;p&gt;When integrating Redis with application functionalities using RESP3, it's important to note that some response structures aren't final yet. This is especially true for more complex structures like search and query results. We recommend using RESP2 when using the search and query capabilities, but we plan to stabilize the RESP3-based API-s in the coming versions. You can find more guidance in the upcoming release notes.&lt;/p&gt; 
&lt;p&gt;To enable unstable RESP3, set the option in your client configuration:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;redis.NewClient(&amp;amp;redis.Options{
			UnstableResp3: true,
		})
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; When UnstableResp3 mode is enabled, it's necessary to use RawResult() and RawVal() to retrieve a raw data. Since, raw response is the only option for unstable search commands Val() and Result() calls wouldn't have any affect on them:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;res1, err := client.FTSearchWithArgs(ctx, "txt", "foo bar", &amp;amp;redis.FTSearchOptions{}).RawResult()
val1 := client.FTSearchWithArgs(ctx, "txt", "foo bar", &amp;amp;redis.FTSearchOptions{}).RawVal()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Redis-Search Default Dialect&lt;/h4&gt; 
&lt;p&gt;In the Redis-Search module, &lt;strong&gt;the default dialect is 2&lt;/strong&gt;. If needed, you can explicitly specify a different dialect using the appropriate configuration in your queries.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Be aware that the query dialect may impact the results returned. If needed, you can revert to a different dialect version by passing the desired dialect in the arguments of the command you want to execute. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;	res2, err := rdb.FTSearchWithArgs(ctx,
		"idx:bicycle",
		"@pickup_zone:[CONTAINS $bike]",
		&amp;amp;redis.FTSearchOptions{
			Params: map[string]interface{}{
				"bike": "POINT(-0.1278 51.5074)",
			},
			DialectVersion: 3,
		},
	).Result()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can find further details in the &lt;a href="https://redis.io/docs/latest/develop/interact/search-and-query/advanced-concepts/dialects/"&gt;query dialect documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Custom buffer sizes&lt;/h4&gt; 
&lt;p&gt;Prior to v9.12, the buffer size was the default go value of 4096 bytes. Starting from v9.12, go-redis uses 256KiB read and write buffers by default for optimal performance. For high-throughput applications or large pipelines, you can customize buffer sizes:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;rdb := redis.NewClient(&amp;amp;redis.Options{
    Addr:            "localhost:6379",
    ReadBufferSize:  1024 * 1024, // 1MiB read buffer
    WriteBufferSize: 1024 * 1024, // 1MiB write buffer
})
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: If you experience any issues with the default buffer sizes, please try setting them to the go default of 4096 bytes.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions to the go-redis library! If you have a bug fix, feature request, or improvement, please open an issue or pull request on GitHub. We appreciate your help in making go-redis better for everyone. If you are interested in contributing to the go-redis library, please check out our &lt;a href="https://raw.githubusercontent.com/redis/go-redis/master/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt; for more information on how to get started.&lt;/p&gt; 
&lt;h2&gt;Look and feel&lt;/h2&gt; 
&lt;p&gt;Some corner cases:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;// SET key value EX 10 NX
set, err := rdb.SetNX(ctx, "key", "value", 10*time.Second).Result()

// SET key value keepttl NX
set, err := rdb.SetNX(ctx, "key", "value", redis.KeepTTL).Result()

// SORT list LIMIT 0 2 ASC
vals, err := rdb.Sort(ctx, "list", &amp;amp;redis.Sort{Offset: 0, Count: 2, Order: "ASC"}).Result()

// ZRANGEBYSCORE zset -inf +inf WITHSCORES LIMIT 0 2
vals, err := rdb.ZRangeByScoreWithScores(ctx, "zset", &amp;amp;redis.ZRangeBy{
    Min: "-inf",
    Max: "+inf",
    Offset: 0,
    Count: 2,
}).Result()

// ZINTERSTORE out 2 zset1 zset2 WEIGHTS 2 3 AGGREGATE SUM
vals, err := rdb.ZInterStore(ctx, "out", &amp;amp;redis.ZStore{
    Keys: []string{"zset1", "zset2"},
    Weights: []int64{2, 3}
}).Result()

// EVAL "return {KEYS[1],ARGV[1]}" 1 "key" "hello"
vals, err := rdb.Eval(ctx, "return {KEYS[1],ARGV[1]}", []string{"key"}, "hello").Result()

// custom command
res, err := rdb.Do(ctx, "set", "key", "value").Result()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Run the test&lt;/h2&gt; 
&lt;p&gt;go-redis will start a redis-server and run the test cases.&lt;/p&gt; 
&lt;p&gt;The paths of redis-server bin file and redis config file are defined in &lt;code&gt;main_test.go&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;var (
	redisServerBin, _  = filepath.Abs(filepath.Join("testdata", "redis", "src", "redis-server"))
	redisServerConf, _ = filepath.Abs(filepath.Join("testdata", "redis", "redis.conf"))
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For local testing, you can change the variables to refer to your local files, or create a soft link to the corresponding folder for redis-server and copy the config file to &lt;code&gt;testdata/redis/&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ln -s /usr/bin/redis-server ./go-redis/testdata/redis/src
cp ./go-redis/testdata/redis.conf ./go-redis/testdata/redis/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Lastly, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;go test
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Another option is to run your specific tests with an already running redis. The example below, tests against a redis running on port 9999.:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;REDIS_PORT=9999 go test &amp;lt;your options&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;See also&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://bun.uptrace.dev"&gt;Golang ORM&lt;/a&gt; for PostgreSQL, MySQL, MSSQL, and SQLite&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://bun.uptrace.dev/postgres/"&gt;Golang PostgreSQL&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://bunrouter.uptrace.dev/"&gt;Golang HTTP router&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/uptrace/go-clickhouse"&gt;Golang ClickHouse ORM&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The go-redis project was originally initiated by &lt;span&gt;‚≠ê&lt;/span&gt; &lt;a href="https://github.com/uptrace/uptrace"&gt;&lt;strong&gt;uptrace/uptrace&lt;/strong&gt;&lt;/a&gt;. Uptrace is an open-source APM tool that supports distributed tracing, metrics, and logs. You can use it to monitor applications and set up automatic alerts to receive notifications via email, Slack, Telegram, and others.&lt;/p&gt; 
 &lt;p&gt;See &lt;a href="https://github.com/redis/go-redis/tree/master/example/otel"&gt;OpenTelemetry&lt;/a&gt; example which demonstrates how you can use Uptrace to monitor go-redis.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Thanks to all the people who already contributed!&lt;/p&gt; 
&lt;a href="https://github.com/redis/go-redis/graphs/contributors"&gt; &lt;img src="https://contributors-img.web.app/image?repo=redis/go-redis" /&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>qarmin/czkawka</title>
      <link>https://github.com/qarmin/czkawka</link>
      <description>&lt;p&gt;Multi functional app to find duplicates, empty folders, similar images etc.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/41945903/102616149-66490400-4137-11eb-9cd6-813b2b070834.png" alt="com github qarmin czkawka" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Czkawka&lt;/strong&gt; (&lt;em&gt;tch‚Ä¢kav‚Ä¢ka&lt;/em&gt; (IPA: [Àà ßÃëkafka]), "hiccup" in Polish) is a simple, fast and free app to remove unnecessary files from your computer.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Krokiet&lt;/strong&gt; ((IPA: [Ààkr…îc…õt]), "croquet" in Polish) same as above, but uses Slint frontend.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Written in memory-safe Rust - almost 100% unsafe code free&lt;/li&gt; 
 &lt;li&gt;Amazingly fast - due to using more or less advanced algorithms and multithreading&lt;/li&gt; 
 &lt;li&gt;Free, Open Source without ads&lt;/li&gt; 
 &lt;li&gt;Multiplatform - works on Linux, Windows, macOS, FreeBSD and many more&lt;/li&gt; 
 &lt;li&gt;Cache support - second and further scans should be much faster than the first one&lt;/li&gt; 
 &lt;li&gt;CLI frontend - for easy automation&lt;/li&gt; 
 &lt;li&gt;GUI frontend - uses GTK 4 or Slint frameworks&lt;/li&gt; 
 &lt;li&gt;No spying - Czkawka does not have access to the Internet, nor does it collect any user information or statistics&lt;/li&gt; 
 &lt;li&gt;Multilingual - support multiple languages like Polish, English or Italian&lt;/li&gt; 
 &lt;li&gt;Multiple tools to use: 
  &lt;ul&gt; 
   &lt;li&gt;Duplicates - Finds duplicates based on file name, size or hash&lt;/li&gt; 
   &lt;li&gt;Empty Folders - Finds empty folders with the help of an advanced algorithm&lt;/li&gt; 
   &lt;li&gt;Big Files - Finds the provided number of the biggest files in given location&lt;/li&gt; 
   &lt;li&gt;Empty Files - Looks for empty files across the drive&lt;/li&gt; 
   &lt;li&gt;Temporary Files - Finds temporary files&lt;/li&gt; 
   &lt;li&gt;Similar Images - Finds images which are not exactly the same (different resolution, watermarks)&lt;/li&gt; 
   &lt;li&gt;Similar Videos - Looks for visually similar videos&lt;/li&gt; 
   &lt;li&gt;Same Music - Searches for similar music by tags or by reading content and comparing it&lt;/li&gt; 
   &lt;li&gt;Invalid Symbolic Links - Shows symbolic links which point to non-existent files/directories&lt;/li&gt; 
   &lt;li&gt;Broken Files - Finds files that are invalid or corrupted&lt;/li&gt; 
   &lt;li&gt;Bad Extensions - Lists files whose content not match with their extension&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/b0409515-1bec-4e13-8fac-7bdfa15f5848" alt="Czkawka" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/906cbbc3-f011-4306-81da-9e4e53b49a9f" alt="Krokiet" /&gt;&lt;/p&gt; 
&lt;p&gt;Changelog about each version can be found in &lt;a href="https://raw.githubusercontent.com/qarmin/czkawka/master/Changelog.md"&gt;CHANGELOG.md&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;New releases can be found in &lt;a href="https://github.com/qarmin/czkawka/releases"&gt;Github releases&lt;/a&gt; and nightly builds also in &lt;a href="https://github.com/qarmin/czkawka/releases/tag/Nightly"&gt;Nightly releases&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Usage, installation, compilation, requirements, license&lt;/h2&gt; 
&lt;p&gt;Each tool uses different technologies, so you can find instructions for each of them in the appropriate file:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/qarmin/czkawka/master/czkawka_gui/README.md"&gt;Czkawka GUI (GTK frontend)&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/qarmin/czkawka/master/czkawka_cli/README.md"&gt;Czkawka CLI&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/qarmin/czkawka/master/czkawka_core/README.md"&gt;Czkawka Core&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/qarmin/czkawka/master/krokiet/README.md"&gt;Krokiet GUI (Slint frontend)&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Comparison to other tools&lt;/h2&gt; 
&lt;p&gt;Bleachbit is a master at finding and removing temporary files, while Czkawka only finds the most basic ones. So these two apps shouldn't be compared directly or be considered as an alternative to one another.&lt;/p&gt; 
&lt;p&gt;In this comparison remember, that even if app have same features they may work different(e.g. one app may have more options to choose than other).&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;Czkawka&lt;/th&gt; 
   &lt;th align="center"&gt;Krokiet&lt;/th&gt; 
   &lt;th align="center"&gt;FSlint&lt;/th&gt; 
   &lt;th align="center"&gt;DupeGuru&lt;/th&gt; 
   &lt;th align="center"&gt;Bleachbit&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Language&lt;/td&gt; 
   &lt;td align="center"&gt;Rust&lt;/td&gt; 
   &lt;td align="center"&gt;Rust&lt;/td&gt; 
   &lt;td align="center"&gt;Python&lt;/td&gt; 
   &lt;td align="center"&gt;Python/Obj-C&lt;/td&gt; 
   &lt;td align="center"&gt;Python&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Framework base language&lt;/td&gt; 
   &lt;td align="center"&gt;C&lt;/td&gt; 
   &lt;td align="center"&gt;Rust&lt;/td&gt; 
   &lt;td align="center"&gt;C&lt;/td&gt; 
   &lt;td align="center"&gt;C/C++/Obj-C/Swift&lt;/td&gt; 
   &lt;td align="center"&gt;C&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Framework&lt;/td&gt; 
   &lt;td align="center"&gt;GTK 4&lt;/td&gt; 
   &lt;td align="center"&gt;Slint&lt;/td&gt; 
   &lt;td align="center"&gt;PyGTK2&lt;/td&gt; 
   &lt;td align="center"&gt;Qt 5 (PyQt)/Cocoa&lt;/td&gt; 
   &lt;td align="center"&gt;PyGTK3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;OS&lt;/td&gt; 
   &lt;td align="center"&gt;Lin,Mac,Win&lt;/td&gt; 
   &lt;td align="center"&gt;Lin,Mac,Win&lt;/td&gt; 
   &lt;td align="center"&gt;Lin&lt;/td&gt; 
   &lt;td align="center"&gt;Lin,Mac,Win&lt;/td&gt; 
   &lt;td align="center"&gt;Lin,Mac,Win&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Duplicate finder&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Empty files&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Empty folders&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Temporary files&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Big files&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Similar images&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Similar videos&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Music duplicates(tags)&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Invalid symlinks&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Broken files&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Names conflict&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Invalid names/extensions&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Installed packages&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Bad ID&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Non stripped binaries&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Redundant whitespace&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Overwriting files&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Multiple languages&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Cache support&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;In active development&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
   &lt;td align="center"&gt;No&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Other apps&lt;/h2&gt; 
&lt;p&gt;There are many similar applications to Czkawka on the Internet, which do some things better and some things worse:&lt;/p&gt; 
&lt;h3&gt;GUI&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/arsenetar/dupeguru"&gt;DupeGuru&lt;/a&gt; - Many options to customize; great photo compare tool&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pixelb/fslint"&gt;FSlint&lt;/a&gt; - A little outdated, but still have some tools not available in Czkawka&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ermig1979/AntiDupl"&gt;AntiDupl.NET&lt;/a&gt; - Shows a lot of metadata of compared images&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/0x90d/videoduplicatefinder"&gt;Video Duplicate Finder&lt;/a&gt; - Finds similar videos(surprising, isn't it), supports video thumbnails&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;CLI&lt;/h3&gt; 
&lt;p&gt;Due to limited time, the biggest emphasis is on the GUI version so if you are looking for really good and feature-packed console apps, then take a look at these:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pkolaczk/fclones"&gt;Fclones&lt;/a&gt; - One of the fastest tools to find duplicates; it is written also in Rust&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sahib/rmlint"&gt;Rmlint&lt;/a&gt; - Nice console interface and also is feature packed&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pauldreik/rdfind"&gt;RdFind&lt;/a&gt; - Fast, but written in C++ ¬Ø\_(„ÉÑ)_/¬Ø&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Thanks&lt;/h2&gt; 
&lt;p&gt;Big thanks to P√°draig Brady, creator of fantastic FSlint, because without his work I wouldn't create this tool.&lt;/p&gt; 
&lt;p&gt;Thanks also to all the people who create patches for this program, make it available on other systems, create videos, articles about it etc.&lt;/p&gt; 
&lt;p&gt;Also, I really appreciate work of people that create crates on which Czkawka is based and for that I try to report bugs to make it even better.&lt;/p&gt; 
&lt;h2&gt;Officially Supported Projects&lt;/h2&gt; 
&lt;p&gt;Only this repository, &lt;a href="https://github.com/qarmin/czkawka/releases"&gt;prebuild-binaries&lt;/a&gt;, projects on &lt;a href="https://crates.io/crates/czkawka_gui"&gt;crates.io&lt;/a&gt; and &lt;a href="https://flathub.org/apps/com.github.qarmin.czkawka"&gt;flathub&lt;/a&gt; are directly maintained by me.&lt;/p&gt; 
&lt;p&gt;Czkawka does not have an official website, so do not trust any sites that claim to be the official one.&lt;/p&gt; 
&lt;p&gt;If you use packages from unofficial sources, make sure they are safe.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The entire code in this repository is licensed under the &lt;a href="https://mit-license.org/"&gt;MIT&lt;/a&gt; license.&lt;/p&gt; 
&lt;p&gt;All images are licensed under the &lt;a href="https://creativecommons.org/licenses/by/4.0/"&gt;CC BY 4.0&lt;/a&gt; license.&lt;/p&gt; 
&lt;p&gt;The Czkawka GTK GUI and CLI applications are licensed under the &lt;a href="https://mit-license.org/"&gt;MIT&lt;/a&gt; license, while the Krokiet is licensed under the &lt;a href="https://www.gnu.org/licenses/gpl-3.0.en.html"&gt;GPL-3.0-only&lt;/a&gt; license.&lt;/p&gt; 
&lt;h2&gt;Donations&lt;/h2&gt; 
&lt;p&gt;If you are using the app, I would appreciate a donation for its further development, which can be done &lt;a href="https://github.com/sponsors/qarmin"&gt;here&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>budtmo/docker-android</title>
      <link>https://github.com/budtmo/docker-android</link>
      <description>&lt;p&gt;Android in docker solution with noVNC supported and video recording&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img id="header" src="https://raw.githubusercontent.com/budtmo/docker-android/master/images/logo_docker-android.png" /&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="http://paypal.me/budtmo"&gt;&lt;img src="https://img.shields.io/badge/paypal-donate-blue.svg?sanitize=true" alt="Paypal Donate" /&gt;&lt;/a&gt; &lt;a href="http://makeapullrequest.com"&gt;&lt;img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square" alt="PRs Welcome" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/budtmo/docker-android"&gt;&lt;img src="https://codecov.io/gh/budtmo/docker-android/branch/master/graph/badge.svg?sanitize=true" alt="codecov" /&gt;&lt;/a&gt; &lt;a href="https://gitter.im/budtmo/docker-android?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge"&gt;&lt;img src="https://badges.gitter.im/budtmo/docker-android.svg?sanitize=true" alt="Join the chat at https://gitter.im/budtmo/docker-android" /&gt;&lt;/a&gt; &lt;a href="https://github.com/budtmo/docker-android/releases"&gt;&lt;img src="https://img.shields.io/github/release/budtmo/docker-android.svg?sanitize=true" alt="GitHub release" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Docker-Android is a docker image built to be used for everything related to Android. It can be used for Application development and testing (native, web and hybrid-app).&lt;/p&gt; 
&lt;h2&gt;Advantages of using this project&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Emulator with different device profile and skins, such as Samsung Galaxy S6, LG Nexus 4, HTC Nexus One and more.&lt;/li&gt; 
 &lt;li&gt;Support vnc to be able to see what happen inside docker container&lt;/li&gt; 
 &lt;li&gt;Support log sharing feature where all logs can be accessed from web-UI&lt;/li&gt; 
 &lt;li&gt;Ability to control emulator from outside container by using adb connect&lt;/li&gt; 
 &lt;li&gt;Integrated with other cloud solutions, e.g. &lt;a href="https://www.genymotion.com/cloud/"&gt;Genymotion Cloud&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;It can be used to build Android project&lt;/li&gt; 
 &lt;li&gt;It can be used to run unit and UI-Test with different test-frameworks, e.g. Appium, Espresso, etc.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;List of Docker-Images&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Android&lt;/th&gt; 
   &lt;th align="left"&gt;API&lt;/th&gt; 
   &lt;th align="left"&gt;Image with latest release version&lt;/th&gt; 
   &lt;th align="left"&gt;Image with specific release version&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;9.0&lt;/td&gt; 
   &lt;td align="left"&gt;28&lt;/td&gt; 
   &lt;td align="left"&gt;budtmo/docker-android:emulator_9.0&lt;/td&gt; 
   &lt;td align="left"&gt;budtmo/docker-android:emulator_9.0_&amp;lt;release_version&amp;gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;10.0&lt;/td&gt; 
   &lt;td align="left"&gt;29&lt;/td&gt; 
   &lt;td align="left"&gt;budtmo/docker-android:emulator_10.0&lt;/td&gt; 
   &lt;td align="left"&gt;budtmo/docker-android:emulator_10.0_&amp;lt;release_version&amp;gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;11.0&lt;/td&gt; 
   &lt;td align="left"&gt;30&lt;/td&gt; 
   &lt;td align="left"&gt;budtmo/docker-android:emulator_11.0&lt;/td&gt; 
   &lt;td align="left"&gt;budtmo/docker-android:emulator_11.0_&amp;lt;release_version&amp;gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;12.0&lt;/td&gt; 
   &lt;td align="left"&gt;32&lt;/td&gt; 
   &lt;td align="left"&gt;budtmo/docker-android:emulator_12.0&lt;/td&gt; 
   &lt;td align="left"&gt;budtmo/docker-android:emulator_12.0_&amp;lt;release_version&amp;gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;13.0&lt;/td&gt; 
   &lt;td align="left"&gt;33&lt;/td&gt; 
   &lt;td align="left"&gt;budtmo/docker-android:emulator_13.0&lt;/td&gt; 
   &lt;td align="left"&gt;budtmo/docker-android:emulator_13.0_&amp;lt;release_version&amp;gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;14.0&lt;/td&gt; 
   &lt;td align="left"&gt;34&lt;/td&gt; 
   &lt;td align="left"&gt;budtmo/docker-android:emulator_14.0&lt;/td&gt; 
   &lt;td align="left"&gt;budtmo/docker-android:emulator_14.0_&amp;lt;release_version&amp;gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;-&lt;/td&gt; 
   &lt;td align="left"&gt;-&lt;/td&gt; 
   &lt;td align="left"&gt;budtmo/docker-android:genymotion&lt;/td&gt; 
   &lt;td align="left"&gt;budtmo/docker-android:genymotion_&amp;lt;release_version&amp;gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;List of Devices&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Device Name&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Phone&lt;/td&gt; 
   &lt;td&gt;Samsung Galaxy S10&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Phone&lt;/td&gt; 
   &lt;td&gt;Samsung Galaxy S9&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Phone&lt;/td&gt; 
   &lt;td&gt;Samsung Galaxy S8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Phone&lt;/td&gt; 
   &lt;td&gt;Samsung Galaxy S7 Edge&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Phone&lt;/td&gt; 
   &lt;td&gt;Samsung Galaxy S7&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Phone&lt;/td&gt; 
   &lt;td&gt;Samsung Galaxy S6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Phone&lt;/td&gt; 
   &lt;td&gt;Nexus 4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Phone&lt;/td&gt; 
   &lt;td&gt;Nexus 5&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Phone&lt;/td&gt; 
   &lt;td&gt;Nexus One&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Phone&lt;/td&gt; 
   &lt;td&gt;Nexus S&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Tablet&lt;/td&gt; 
   &lt;td&gt;Nexus 7&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Tablet&lt;/td&gt; 
   &lt;td&gt;Pixel C&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Docker is installed on your system.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;If you use &lt;em&gt;&lt;strong&gt;Ubuntu OS&lt;/strong&gt;&lt;/em&gt; on your host machine, you can skip this step. For &lt;em&gt;&lt;strong&gt;OSX&lt;/strong&gt;&lt;/em&gt; and &lt;em&gt;&lt;strong&gt;Windows OS&lt;/strong&gt;&lt;/em&gt; user, you need to use Virtual Machine that support Virtualization with Ubuntu OS because the image can be run under &lt;em&gt;&lt;strong&gt;Ubuntu OS only&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Your machine should support virtualization. To check if the virtualization is enabled is:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo apt install cpu-checker
kvm-ok
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run Docker-Android container&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker run -d -p 6080:6080 -e EMULATOR_DEVICE="Samsung Galaxy S10" -e WEB_VNC=true --device /dev/kvm --name android-container budtmo/docker-android:emulator_11.0
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Open &lt;em&gt;&lt;strong&gt;&lt;a href="http://localhost:6080"&gt;http://localhost:6080&lt;/a&gt;&lt;/strong&gt;&lt;/em&gt; to see inside running container.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;To check the status of the emulator&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker exec -it android-container cat device_status
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Persisting data&lt;/h2&gt; 
&lt;p&gt;The default behaviour is to destroy the emulated device on container restart. To persist data, you need to mount a volume at &lt;code&gt;/home/androidusr&lt;/code&gt;: &lt;code&gt;docker run -v data:/home/androidusr budtmo/docker-android:emulator_11.0&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;WSL2 Hardware acceleration (Windows 11 only)&lt;/h2&gt; 
&lt;p&gt;Credit goes to &lt;a href="https://www.paralint.com/2022/11/find-new-modified-and-unversioned-subversion-files-on-windows"&gt;Guillaume - The Parallel Interface blog&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://learn.microsoft.com/en-us/windows/wsl/wsl-config"&gt;Microsoft - Advanced settings configuration in WSL&lt;/a&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Add yourself to the &lt;code&gt;kvm&lt;/code&gt; usergroup.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo usermod -a -G kvm ${USER}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Add necessary flags to &lt;code&gt;/etc/wsl2.conf&lt;/code&gt; to their respective sections.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[boot]
command = /bin/bash -c 'chown -v root:kvm /dev/kvm &amp;amp;&amp;amp; chmod 660 /dev/kvm'

[wsl2]
nestedVirtualization=true
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Restart WSL2 via CMD prompt or Powershell&lt;/p&gt; &lt;pre&gt;&lt;code&gt;wsl --shutdown
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;code&gt;command = /bin/bash -c 'chown -v root:kvm /dev/kvm &amp;amp;&amp;amp; chmod 660 /dev/kvm'&lt;/code&gt; sets &lt;code&gt;/dev/kvm&lt;/code&gt; to &lt;code&gt;kvm&lt;/code&gt; usergroup rather than the default &lt;code&gt;root&lt;/code&gt; usergroup on WSL2 startup.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;nestedVirtualization&lt;/code&gt; flag is only available to Windows 11.&lt;/p&gt; 
&lt;h2&gt;Use-Cases&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/budtmo/docker-android/master/documentations/USE_CASE_BUILD_ANDROID_PROJECT.md"&gt;Build Android project&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/budtmo/docker-android/master/documentations/USE_CASE_APPIUM.md"&gt;UI-Test with Appium&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/budtmo/docker-android/master/documentations/USE_CASE_CONTROL_EMULATOR.md"&gt;Control Android emulator on host machine&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/budtmo/docker-android/master/documentations/USE_CASE_SMS.md"&gt;SMS Simulation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/budtmo/docker-android/master/documentations/USE_CASE_JENKINS.md"&gt;Jenkins&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/budtmo/docker-android/master/documentations/USE_CASE_CLOUD.md"&gt;Deploying on cloud (Azure, AWS, GCP)&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Custom-Configurations&lt;/h2&gt; 
&lt;p&gt;This &lt;a href="https://raw.githubusercontent.com/budtmo/docker-android/master/documentations/CUSTOM_CONFIGURATIONS.md"&gt;document&lt;/a&gt; contains information about configurations that can be used to enable some features, e.g. log-sharing, etc.&lt;/p&gt; 
&lt;h2&gt;Genymotion&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img id="geny" src="https://raw.githubusercontent.com/budtmo/docker-android/master/images/logo_genymotion_and_dockerandroid.png" /&gt; &lt;/p&gt; 
&lt;p&gt;For you who do not have ressources to maintain the simulator or to buy machines or need different device profiles, you can give a try by using &lt;a href="https://cloud.geny.io/"&gt;Genymotion SAAS&lt;/a&gt;. Docker-Android is &lt;a href="https://www.genymotion.com/blog/partner_tag/docker/"&gt;integrated with Genymotion&lt;/a&gt; on different cloud services, e.g. Genymotion SAAS, AWS, GCP, Alibaba Cloud. Please follow &lt;a href="https://raw.githubusercontent.com/budtmo/docker-android/master/documentations/THIRD_PARTY_GENYMOTION.md"&gt;this document&lt;/a&gt; for more detail.&lt;/p&gt; 
&lt;h2&gt;Emulator Skins&lt;/h2&gt; 
&lt;p&gt;The Emulator skins are taken from &lt;a href="https://developer.android.com/studio"&gt;Android Studio IDE&lt;/a&gt; and &lt;a href="https://developer.samsung.com/"&gt;Samsung Developer Website&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;USERS&lt;/h2&gt; 
&lt;a href="https://lookerstudio.google.com/s/iGaemHJqQvg"&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/budtmo/docker-android/master/images/docker-android_users.png" alt="docker-android-users" width="800" height="600" /&gt; &lt;/p&gt; &lt;/a&gt; 
&lt;h2&gt;PRO VERSION&lt;/h2&gt; 
&lt;p&gt;Due to high requests for help and to be able to actively maintain the projects, the creator has decided to create docker-android-pro. Docker-Android-Pro is a sponsor based project which mean that the docker image of pro-version can be pulled only by &lt;a href="https://github.com/sponsors/budtmo"&gt;active sponsor&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The differences between normal version and pro version are:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Feature&lt;/th&gt; 
   &lt;th align="left"&gt;Normal&lt;/th&gt; 
   &lt;th align="left"&gt;Pro&lt;/th&gt; 
   &lt;th align="left"&gt;Comment&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;user-behavior-analytics&lt;/td&gt; 
   &lt;td align="left"&gt;Yes&lt;/td&gt; 
   &lt;td align="left"&gt;No&lt;/td&gt; 
   &lt;td align="left"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;proxy&lt;/td&gt; 
   &lt;td align="left"&gt;No&lt;/td&gt; 
   &lt;td align="left"&gt;Yes&lt;/td&gt; 
   &lt;td align="left"&gt;Set up company proxy on Android emulator on fly&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;language&lt;/td&gt; 
   &lt;td align="left"&gt;No&lt;/td&gt; 
   &lt;td align="left"&gt;Yes&lt;/td&gt; 
   &lt;td align="left"&gt;Set up language on Android emulator on fly&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Newer Android version&lt;/td&gt; 
   &lt;td align="left"&gt;No&lt;/td&gt; 
   &lt;td align="left"&gt;Yes&lt;/td&gt; 
   &lt;td align="left"&gt;Support other newer Android version e.g. Android 15, Android 16, etc&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;root-privileged&lt;/td&gt; 
   &lt;td align="left"&gt;No&lt;/td&gt; 
   &lt;td align="left"&gt;Yes&lt;/td&gt; 
   &lt;td align="left"&gt;Able to run command with security privileged&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;headless-mode&lt;/td&gt; 
   &lt;td align="left"&gt;No&lt;/td&gt; 
   &lt;td align="left"&gt;Yes&lt;/td&gt; 
   &lt;td align="left"&gt;Save resources by using headless mode&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Selenium 4.x integration&lt;/td&gt; 
   &lt;td align="left"&gt;No&lt;/td&gt; 
   &lt;td align="left"&gt;Yes&lt;/td&gt; 
   &lt;td align="left"&gt;Running Appium UI-Tests againt one (Selenium Hub) endpoint for Android- and iOS emulator(s) / device(s)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;multiple Android-Simulators&lt;/td&gt; 
   &lt;td align="left"&gt;No&lt;/td&gt; 
   &lt;td align="left"&gt;Yes (soon)&lt;/td&gt; 
   &lt;td align="left"&gt;Save resources by having multiple Android-Simulators on one docker-container&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Google Play Store&lt;/td&gt; 
   &lt;td align="left"&gt;No&lt;/td&gt; 
   &lt;td align="left"&gt;Yes (soon)&lt;/td&gt; 
   &lt;td align="left"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Video Recording&lt;/td&gt; 
   &lt;td align="left"&gt;No&lt;/td&gt; 
   &lt;td align="left"&gt;Yes (soon)&lt;/td&gt; 
   &lt;td align="left"&gt;Helpful for debugging&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;This &lt;a href="https://raw.githubusercontent.com/budtmo/docker-android/master/documentations/DOCKER-ANDROID-PRO.md"&gt;document&lt;/a&gt; contains detail information about how to use docker-android-pro.&lt;/p&gt; 
&lt;h2&gt;LICENSE&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/budtmo/docker-android/master/LICENSE.md"&gt;License&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>datalab-to/marker</title>
      <link>https://github.com/datalab-to/marker</link>
      <description>&lt;p&gt;Convert PDF to markdown + JSON quickly with high accuracy&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Marker&lt;/h1&gt; 
&lt;p&gt;Marker converts documents to markdown, JSON, chunks, and HTML quickly and accurately.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Converts PDF, image, PPTX, DOCX, XLSX, HTML, EPUB files in all languages&lt;/li&gt; 
 &lt;li&gt;Formats tables, forms, equations, inline math, links, references, and code blocks&lt;/li&gt; 
 &lt;li&gt;Extracts and saves images&lt;/li&gt; 
 &lt;li&gt;Removes headers/footers/other artifacts&lt;/li&gt; 
 &lt;li&gt;Extensible with your own formatting and logic&lt;/li&gt; 
 &lt;li&gt;Does structured extraction, given a JSON schema (beta)&lt;/li&gt; 
 &lt;li&gt;Optionally boost accuracy with LLMs (and your own prompt)&lt;/li&gt; 
 &lt;li&gt;Works on GPU, CPU, or MPS&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;img src="https://raw.githubusercontent.com/datalab-to/marker/master/data/images/overall.png" width="800px" /&gt; 
&lt;p&gt;Marker benchmarks favorably compared to cloud services like Llamaparse and Mathpix, as well as other open source tools.&lt;/p&gt; 
&lt;p&gt;The above results are running single PDF pages serially. Marker is significantly faster when running in batch mode, with a projected throughput of 25 pages/second on an H100.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/datalab-to/marker/master/#benchmarks"&gt;below&lt;/a&gt; for detailed speed and accuracy benchmarks, and instructions on how to run your own benchmarks.&lt;/p&gt; 
&lt;h2&gt;Hybrid Mode&lt;/h2&gt; 
&lt;p&gt;For the highest accuracy, pass the &lt;code&gt;--use_llm&lt;/code&gt; flag to use an LLM alongside marker. This will do things like merge tables across pages, handle inline math, format tables properly, and extract values from forms. It can use any gemini or ollama model. By default, it uses &lt;code&gt;gemini-2.0-flash&lt;/code&gt;. See &lt;a href="https://raw.githubusercontent.com/datalab-to/marker/master/#llm-services"&gt;below&lt;/a&gt; for details.&lt;/p&gt; 
&lt;p&gt;Here is a table benchmark comparing marker, gemini flash alone, and marker with use_llm:&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/datalab-to/marker/master/data/images/table.png" width="400px" /&gt; 
&lt;p&gt;As you can see, the use_llm mode offers higher accuracy than marker or gemini alone.&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;PDF&lt;/th&gt; 
   &lt;th&gt;File type&lt;/th&gt; 
   &lt;th&gt;Markdown&lt;/th&gt; 
   &lt;th&gt;JSON&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://greenteapress.com/thinkpython/thinkpython.pdf"&gt;Think Python&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Textbook&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/VikParuchuri/marker/raw/master/data/examples/markdown/thinkpython/thinkpython.md"&gt;View&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/VikParuchuri/marker/raw/master/data/examples/json/thinkpython.json"&gt;View&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2101.03961.pdf"&gt;Switch Transformers&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;arXiv paper&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/VikParuchuri/marker/raw/master/data/examples/markdown/switch_transformers/switch_trans.md"&gt;View&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/VikParuchuri/marker/raw/master/data/examples/json/switch_trans.json"&gt;View&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/1804.07821.pdf"&gt;Multi-column CNN&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;arXiv paper&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/VikParuchuri/marker/raw/master/data/examples/markdown/multicolcnn/multicolcnn.md"&gt;View&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/VikParuchuri/marker/raw/master/data/examples/json/multicolcnn.json"&gt;View&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Commercial usage&lt;/h1&gt; 
&lt;p&gt;I want marker to be as widely accessible as possible, while still funding my development/training costs. Research and personal usage is always okay, but there are some restrictions on commercial usage.&lt;/p&gt; 
&lt;p&gt;The weights for the models are licensed &lt;code&gt;cc-by-nc-sa-4.0&lt;/code&gt;, but I will waive that for any organization under $2M USD in gross revenue in the most recent 12-month period AND under $2M in lifetime VC/angel funding raised. You also must not be competitive with the &lt;a href="https://www.datalab.to/"&gt;Datalab API&lt;/a&gt;. If you want to remove the GPL license requirements (dual-license) and/or use the weights commercially over the revenue limit, check out the options &lt;a href="https://www.datalab.to"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Hosted API&lt;/h1&gt; 
&lt;p&gt;There's a hosted API for marker available &lt;a href="https://www.datalab.to/"&gt;here&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports PDF, image, PPT, PPTX, DOC, DOCX, XLS, XLSX, HTML, EPUB files&lt;/li&gt; 
 &lt;li&gt;1/4th the price of leading cloud-based competitors&lt;/li&gt; 
 &lt;li&gt;Fast - ~15s for a 250 page PDF&lt;/li&gt; 
 &lt;li&gt;Supports LLM mode&lt;/li&gt; 
 &lt;li&gt;High uptime (99.99%)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Community&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://discord.gg//KuZwXNGnfH"&gt;Discord&lt;/a&gt; is where we discuss future development.&lt;/p&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;p&gt;You'll need python 3.10+ and &lt;a href="https://pytorch.org/get-started/locally/"&gt;PyTorch&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Install with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install marker-pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to use marker on documents other than PDFs, you will need to install additional dependencies with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install marker-pdf[full]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Usage&lt;/h1&gt; 
&lt;p&gt;First, some configuration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Your torch device will be automatically detected, but you can override this. For example, &lt;code&gt;TORCH_DEVICE=cuda&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Some PDFs, even digital ones, have bad text in them. Set &lt;code&gt;--force_ocr&lt;/code&gt; to force OCR on all lines, or the &lt;code&gt;strip_existing_ocr&lt;/code&gt; to keep all digital text, and strip out any existing OCR text.&lt;/li&gt; 
 &lt;li&gt;If you care about inline math, set &lt;code&gt;force_ocr&lt;/code&gt; to convert inline math to LaTeX.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Interactive App&lt;/h2&gt; 
&lt;p&gt;I've included a streamlit app that lets you interactively try marker with some basic options. Run it with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install streamlit streamlit-ace
marker_gui
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Convert a single file&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;marker_single /path/to/file.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can pass in PDFs or images.&lt;/p&gt; 
&lt;p&gt;Options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--page_range TEXT&lt;/code&gt;: Specify which pages to process. Accepts comma-separated page numbers and ranges. Example: &lt;code&gt;--page_range "0,5-10,20"&lt;/code&gt; will process pages 0, 5 through 10, and page 20.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--output_format [markdown|json|html|chunks]&lt;/code&gt;: Specify the format for the output results.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--output_dir PATH&lt;/code&gt;: Directory where output files will be saved. Defaults to the value specified in settings.OUTPUT_DIR.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--paginate_output&lt;/code&gt;: Paginates the output, using &lt;code&gt;\n\n{PAGE_NUMBER}&lt;/code&gt; followed by &lt;code&gt;-&lt;/code&gt; * 48, then &lt;code&gt;\n\n&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--use_llm&lt;/code&gt;: Uses an LLM to improve accuracy. You will need to configure the LLM backend - see &lt;a href="https://raw.githubusercontent.com/datalab-to/marker/master/#llm-services"&gt;below&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--force_ocr&lt;/code&gt;: Force OCR processing on the entire document, even for pages that might contain extractable text. This will also format inline math properly.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--block_correction_prompt&lt;/code&gt;: if LLM mode is active, an optional prompt that will be used to correct the output of marker. This is useful for custom formatting or logic that you want to apply to the output.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--strip_existing_ocr&lt;/code&gt;: Remove all existing OCR text in the document and re-OCR with surya.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--redo_inline_math&lt;/code&gt;: If you want the absolute highest quality inline math conversion, use this along with &lt;code&gt;--use_llm&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--disable_image_extraction&lt;/code&gt;: Don't extract images from the PDF. If you also specify &lt;code&gt;--use_llm&lt;/code&gt;, then images will be replaced with a description.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--debug&lt;/code&gt;: Enable debug mode for additional logging and diagnostic information.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--processors TEXT&lt;/code&gt;: Override the default processors by providing their full module paths, separated by commas. Example: &lt;code&gt;--processors "module1.processor1,module2.processor2"&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--config_json PATH&lt;/code&gt;: Path to a JSON configuration file containing additional settings.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;config --help&lt;/code&gt;: List all available builders, processors, and converters, and their associated configuration. These values can be used to build a JSON configuration file for additional tweaking of marker defaults.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--converter_cls&lt;/code&gt;: One of &lt;code&gt;marker.converters.pdf.PdfConverter&lt;/code&gt; (default) or &lt;code&gt;marker.converters.table.TableConverter&lt;/code&gt;. The &lt;code&gt;PdfConverter&lt;/code&gt; will convert the whole PDF, the &lt;code&gt;TableConverter&lt;/code&gt; will only extract and convert tables.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--llm_service&lt;/code&gt;: Which llm service to use if &lt;code&gt;--use_llm&lt;/code&gt; is passed. This defaults to &lt;code&gt;marker.services.gemini.GoogleGeminiService&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--help&lt;/code&gt;: see all of the flags that can be passed into marker. (it supports many more options then are listed above)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The list of supported languages for surya OCR is &lt;a href="https://github.com/VikParuchuri/surya/raw/master/surya/recognition/languages.py"&gt;here&lt;/a&gt;. If you don't need OCR, marker can work with any language.&lt;/p&gt; 
&lt;h2&gt;Convert multiple files&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;marker /path/to/input/folder
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;marker&lt;/code&gt; supports all the same options from &lt;code&gt;marker_single&lt;/code&gt; above.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--workers&lt;/code&gt; is the number of conversion workers to run simultaneously. This is automatically set by default, but you can increase it to increase throughput, at the cost of more CPU/GPU usage. Marker will use 5GB of VRAM per worker at the peak, and 3.5GB average.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Convert multiple files on multiple GPUs&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;NUM_DEVICES=4 NUM_WORKERS=15 marker_chunk_convert ../pdf_in ../md_out
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;NUM_DEVICES&lt;/code&gt; is the number of GPUs to use. Should be &lt;code&gt;2&lt;/code&gt; or greater.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;NUM_WORKERS&lt;/code&gt; is the number of parallel processes to run on each GPU.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Use from python&lt;/h2&gt; 
&lt;p&gt;See the &lt;code&gt;PdfConverter&lt;/code&gt; class at &lt;code&gt;marker/converters/pdf.py&lt;/code&gt; function for additional arguments that can be passed.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from marker.converters.pdf import PdfConverter
from marker.models import create_model_dict
from marker.output import text_from_rendered

converter = PdfConverter(
    artifact_dict=create_model_dict(),
)
rendered = converter("FILEPATH")
text, _, images = text_from_rendered(rendered)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;rendered&lt;/code&gt; will be a pydantic basemodel with different properties depending on the output type requested. With markdown output (default), you'll have the properties &lt;code&gt;markdown&lt;/code&gt;, &lt;code&gt;metadata&lt;/code&gt;, and &lt;code&gt;images&lt;/code&gt;. For json output, you'll have &lt;code&gt;children&lt;/code&gt;, &lt;code&gt;block_type&lt;/code&gt;, and &lt;code&gt;metadata&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Custom configuration&lt;/h3&gt; 
&lt;p&gt;You can pass configuration using the &lt;code&gt;ConfigParser&lt;/code&gt;. To see all available options, do &lt;code&gt;marker_single --help&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from marker.converters.pdf import PdfConverter
from marker.models import create_model_dict
from marker.config.parser import ConfigParser

config = {
    "output_format": "json",
    "ADDITIONAL_KEY": "VALUE"
}
config_parser = ConfigParser(config)

converter = PdfConverter(
    config=config_parser.generate_config_dict(),
    artifact_dict=create_model_dict(),
    processor_list=config_parser.get_processors(),
    renderer=config_parser.get_renderer(),
    llm_service=config_parser.get_llm_service()
)
rendered = converter("FILEPATH")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Extract blocks&lt;/h3&gt; 
&lt;p&gt;Each document consists of one or more pages. Pages contain blocks, which can themselves contain other blocks. It's possible to programmatically manipulate these blocks.&lt;/p&gt; 
&lt;p&gt;Here's an example of extracting all forms from a document:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from marker.converters.pdf import PdfConverter
from marker.models import create_model_dict
from marker.schema import BlockTypes

converter = PdfConverter(
    artifact_dict=create_model_dict(),
)
document = converter.build_document("FILEPATH")
forms = document.contained_blocks((BlockTypes.Form,))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Look at the processors for more examples of extracting and manipulating blocks.&lt;/p&gt; 
&lt;h2&gt;Other converters&lt;/h2&gt; 
&lt;p&gt;You can also use other converters that define different conversion pipelines:&lt;/p&gt; 
&lt;h3&gt;Extract tables&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;TableConverter&lt;/code&gt; will only convert and extract tables:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from marker.converters.table import TableConverter
from marker.models import create_model_dict
from marker.output import text_from_rendered

converter = TableConverter(
    artifact_dict=create_model_dict(),
)
rendered = converter("FILEPATH")
text, _, images = text_from_rendered(rendered)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This takes all the same configuration as the PdfConverter. You can specify the configuration &lt;code&gt;force_layout_block=Table&lt;/code&gt; to avoid layout detection and instead assume every page is a table. Set &lt;code&gt;output_format=json&lt;/code&gt; to also get cell bounding boxes.&lt;/p&gt; 
&lt;p&gt;You can also run this via the CLI with&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;marker_single FILENAME --use_llm --force_layout_block Table --converter_cls marker.converters.table.TableConverter --output_format json
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;OCR Only&lt;/h3&gt; 
&lt;p&gt;If you only want to run OCR, you can also do that through the &lt;code&gt;OCRConverter&lt;/code&gt;. Set &lt;code&gt;--keep_chars&lt;/code&gt; to keep individual characters and bounding boxes.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from marker.converters.ocr import OCRConverter
from marker.models import create_model_dict

converter = OCRConverter(
    artifact_dict=create_model_dict(),
)
rendered = converter("FILEPATH")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This takes all the same configuration as the PdfConverter.&lt;/p&gt; 
&lt;p&gt;You can also run this via the CLI with&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;marker_single FILENAME --converter_cls marker.converters.ocr.OCRConverter
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Structured Extraction (beta)&lt;/h3&gt; 
&lt;p&gt;You can run structured extraction via the &lt;code&gt;ExtractionConverter&lt;/code&gt;. This requires an llm service to be setup first (see &lt;a href="https://raw.githubusercontent.com/datalab-to/marker/master/#llm-services"&gt;here&lt;/a&gt; for details). You'll get a JSON output with the extracted values.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from marker.converters.extraction import ExtractionConverter
from marker.models import create_model_dict
from marker.config.parser import ConfigParser
from pydantic import BaseModel

class Links(BaseModel):
    links: list[str]
    
schema = Links.model_json_schema()
config_parser = ConfigParser({
    "page_schema": schema
})

converter = ExtractionConverter(
    artifact_dict=create_model_dict(),
    config=config_parser.generate_config_dict(),
    llm_service=config_parser.get_llm_service(),
)
rendered = converter("FILEPATH")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Rendered will have an &lt;code&gt;original_markdown&lt;/code&gt; field. If you pass this back in next time you run the converter, as the &lt;code&gt;existing_markdown&lt;/code&gt; config key, you can skip re-parsing the document.&lt;/p&gt; 
&lt;h1&gt;Output Formats&lt;/h1&gt; 
&lt;h2&gt;Markdown&lt;/h2&gt; 
&lt;p&gt;Markdown output will include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;image links (images will be saved in the same folder)&lt;/li&gt; 
 &lt;li&gt;formatted tables&lt;/li&gt; 
 &lt;li&gt;embedded LaTeX equations (fenced with &lt;code&gt;$$&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Code is fenced with triple backticks&lt;/li&gt; 
 &lt;li&gt;Superscripts for footnotes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;HTML&lt;/h2&gt; 
&lt;p&gt;HTML output is similar to markdown output:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Images are included via &lt;code&gt;img&lt;/code&gt; tags&lt;/li&gt; 
 &lt;li&gt;equations are fenced with &lt;code&gt;&amp;lt;math&amp;gt;&lt;/code&gt; tags&lt;/li&gt; 
 &lt;li&gt;code is in &lt;code&gt;pre&lt;/code&gt; tags&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;JSON&lt;/h2&gt; 
&lt;p&gt;JSON output will be organized in a tree-like structure, with the leaf nodes being blocks. Examples of leaf nodes are a single list item, a paragraph of text, or an image.&lt;/p&gt; 
&lt;p&gt;The output will be a list, with each list item representing a page. Each page is considered a block in the internal marker schema. There are different types of blocks to represent different elements.&lt;/p&gt; 
&lt;p&gt;Pages have the keys:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;id&lt;/code&gt; - unique id for the block.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;block_type&lt;/code&gt; - the type of block. The possible block types can be seen in &lt;code&gt;marker/schema/__init__.py&lt;/code&gt;. As of this writing, they are ["Line", "Span", "FigureGroup", "TableGroup", "ListGroup", "PictureGroup", "Page", "Caption", "Code", "Figure", "Footnote", "Form", "Equation", "Handwriting", "TextInlineMath", "ListItem", "PageFooter", "PageHeader", "Picture", "SectionHeader", "Table", "Text", "TableOfContents", "Document"]&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;html&lt;/code&gt; - the HTML for the page. Note that this will have recursive references to children. The &lt;code&gt;content-ref&lt;/code&gt; tags must be replaced with the child content if you want the full html. You can see an example of this at &lt;code&gt;marker/output.py:json_to_html&lt;/code&gt;. That function will take in a single block from the json output, and turn it into HTML.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;polygon&lt;/code&gt; - the 4-corner polygon of the page, in (x1,y1), (x2,y2), (x3, y3), (x4, y4) format. (x1,y1) is the top left, and coordinates go clockwise.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;children&lt;/code&gt; - the child blocks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The child blocks have two additional keys:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;section_hierarchy&lt;/code&gt; - indicates the sections that the block is part of. &lt;code&gt;1&lt;/code&gt; indicates an h1 tag, &lt;code&gt;2&lt;/code&gt; an h2, and so on.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;images&lt;/code&gt; - base64 encoded images. The key will be the block id, and the data will be the encoded image.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Note that child blocks of pages can have their own children as well (a tree structure).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
      "id": "/page/10/Page/366",
      "block_type": "Page",
      "html": "&amp;lt;content-ref src='/page/10/SectionHeader/0'&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src='/page/10/SectionHeader/1'&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src='/page/10/Text/2'&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src='/page/10/Text/3'&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src='/page/10/Figure/4'&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src='/page/10/SectionHeader/5'&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src='/page/10/SectionHeader/6'&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src='/page/10/TextInlineMath/7'&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src='/page/10/TextInlineMath/8'&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src='/page/10/Table/9'&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src='/page/10/SectionHeader/10'&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src='/page/10/Text/11'&amp;gt;&amp;lt;/content-ref&amp;gt;",
      "polygon": [[0.0, 0.0], [612.0, 0.0], [612.0, 792.0], [0.0, 792.0]],
      "children": [
        {
          "id": "/page/10/SectionHeader/0",
          "block_type": "SectionHeader",
          "html": "&amp;lt;h1&amp;gt;Supplementary Material for &amp;lt;i&amp;gt;Subspace Adversarial Training&amp;lt;/i&amp;gt; &amp;lt;/h1&amp;gt;",
          "polygon": [
            [217.845703125, 80.630859375], [374.73046875, 80.630859375],
            [374.73046875, 107.0],
            [217.845703125, 107.0]
          ],
          "children": null,
          "section_hierarchy": {
            "1": "/page/10/SectionHeader/1"
          },
          "images": {}
        },
        ...
        ]
    }


&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Chunks&lt;/h2&gt; 
&lt;p&gt;Chunks format is similar to JSON, but flattens everything into a single list instead of a tree. Only the top level blocks from each page show up. It also has the full HTML of each block inside, so you don't need to crawl the tree to reconstruct it. This enable flexible and easy chunking for RAG.&lt;/p&gt; 
&lt;h2&gt;Metadata&lt;/h2&gt; 
&lt;p&gt;All output formats will return a metadata dictionary, with the following fields:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
    "table_of_contents": [
      {
        "title": "Introduction",
        "heading_level": 1,
        "page_id": 0,
        "polygon": [...]
      }
    ], // computed PDF table of contents
    "page_stats": [
      {
        "page_id":  0, 
        "text_extraction_method": "pdftext",
        "block_counts": [("Span", 200), ...]
      },
      ...
    ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;LLM Services&lt;/h1&gt; 
&lt;p&gt;When running with the &lt;code&gt;--use_llm&lt;/code&gt; flag, you have a choice of services you can use:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;Gemini&lt;/code&gt; - this will use the Gemini developer API by default. You'll need to pass &lt;code&gt;--gemini_api_key&lt;/code&gt; to configuration.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Google Vertex&lt;/code&gt; - this will use vertex, which can be more reliable. You'll need to pass &lt;code&gt;--vertex_project_id&lt;/code&gt;. To use it, set &lt;code&gt;--llm_service=marker.services.vertex.GoogleVertexService&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Ollama&lt;/code&gt; - this will use local models. You can configure &lt;code&gt;--ollama_base_url&lt;/code&gt; and &lt;code&gt;--ollama_model&lt;/code&gt;. To use it, set &lt;code&gt;--llm_service=marker.services.ollama.OllamaService&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Claude&lt;/code&gt; - this will use the anthropic API. You can configure &lt;code&gt;--claude_api_key&lt;/code&gt;, and &lt;code&gt;--claude_model_name&lt;/code&gt;. To use it, set &lt;code&gt;--llm_service=marker.services.claude.ClaudeService&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;OpenAI&lt;/code&gt; - this supports any openai-like endpoint. You can configure &lt;code&gt;--openai_api_key&lt;/code&gt;, &lt;code&gt;--openai_model&lt;/code&gt;, and &lt;code&gt;--openai_base_url&lt;/code&gt;. To use it, set &lt;code&gt;--llm_service=marker.services.openai.OpenAIService&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Azure OpenAI&lt;/code&gt; - this uses the Azure OpenAI service. You can configure &lt;code&gt;--azure_endpoint&lt;/code&gt;, &lt;code&gt;--azure_api_key&lt;/code&gt;, and &lt;code&gt;--deployment_name&lt;/code&gt;. To use it, set &lt;code&gt;--llm_service=marker.services.azure_openai.AzureOpenAIService&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;These services may have additional optional configuration as well - you can see it by viewing the classes.&lt;/p&gt; 
&lt;h1&gt;Internals&lt;/h1&gt; 
&lt;p&gt;Marker is easy to extend. The core units of marker are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;Providers&lt;/code&gt;, at &lt;code&gt;marker/providers&lt;/code&gt;. These provide information from a source file, like a PDF.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Builders&lt;/code&gt;, at &lt;code&gt;marker/builders&lt;/code&gt;. These generate the initial document blocks and fill in text, using info from the providers.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Processors&lt;/code&gt;, at &lt;code&gt;marker/processors&lt;/code&gt;. These process specific blocks, for example the table formatter is a processor.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Renderers&lt;/code&gt;, at &lt;code&gt;marker/renderers&lt;/code&gt;. These use the blocks to render output.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Schema&lt;/code&gt;, at &lt;code&gt;marker/schema&lt;/code&gt;. The classes for all the block types.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Converters&lt;/code&gt;, at &lt;code&gt;marker/converters&lt;/code&gt;. They run the whole end to end pipeline.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To customize processing behavior, override the &lt;code&gt;processors&lt;/code&gt;. To add new output formats, write a new &lt;code&gt;renderer&lt;/code&gt;. For additional input formats, write a new &lt;code&gt;provider.&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Processors and renderers can be directly passed into the base &lt;code&gt;PDFConverter&lt;/code&gt;, so you can specify your own custom processing easily.&lt;/p&gt; 
&lt;h2&gt;API server&lt;/h2&gt; 
&lt;p&gt;There is a very simple API server you can run like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install -U uvicorn fastapi python-multipart
marker_server --port 8001
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will start a fastapi server that you can access at &lt;code&gt;localhost:8001&lt;/code&gt;. You can go to &lt;code&gt;localhost:8001/docs&lt;/code&gt; to see the endpoint options.&lt;/p&gt; 
&lt;p&gt;You can send requests like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;import requests
import json

post_data = {
    'filepath': 'FILEPATH',
    # Add other params here
}

requests.post("http://localhost:8001/marker", data=json.dumps(post_data)).json()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that this is not a very robust API, and is only intended for small-scale use. If you want to use this server, but want a more robust conversion option, you can use the hosted &lt;a href="https://www.datalab.to/plans"&gt;Datalab API&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Troubleshooting&lt;/h1&gt; 
&lt;p&gt;There are some settings that you may find useful if things aren't working the way you expect:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you have issues with accuracy, try setting &lt;code&gt;--use_llm&lt;/code&gt; to use an LLM to improve quality. You must set &lt;code&gt;GOOGLE_API_KEY&lt;/code&gt; to a Gemini API key for this to work.&lt;/li&gt; 
 &lt;li&gt;Make sure to set &lt;code&gt;force_ocr&lt;/code&gt; if you see garbled text - this will re-OCR the document.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;TORCH_DEVICE&lt;/code&gt; - set this to force marker to use a given torch device for inference.&lt;/li&gt; 
 &lt;li&gt;If you're getting out of memory errors, decrease worker count. You can also try splitting up long PDFs into multiple files.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Debugging&lt;/h2&gt; 
&lt;p&gt;Pass the &lt;code&gt;debug&lt;/code&gt; option to activate debug mode. This will save images of each page with detected layout and text, as well as output a json file with additional bounding box information.&lt;/p&gt; 
&lt;h1&gt;Benchmarks&lt;/h1&gt; 
&lt;h2&gt;Overall PDF Conversion&lt;/h2&gt; 
&lt;p&gt;We created a &lt;a href="https://huggingface.co/datasets/datalab-to/marker_benchmark"&gt;benchmark set&lt;/a&gt; by extracting single PDF pages from common crawl. We scored based on a heuristic that aligns text with ground truth text segments, and an LLM as a judge scoring method.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Avg Time&lt;/th&gt; 
   &lt;th&gt;Heuristic Score&lt;/th&gt; 
   &lt;th&gt;LLM Score&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;marker&lt;/td&gt; 
   &lt;td&gt;2.83837&lt;/td&gt; 
   &lt;td&gt;95.6709&lt;/td&gt; 
   &lt;td&gt;4.23916&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;llamaparse&lt;/td&gt; 
   &lt;td&gt;23.348&lt;/td&gt; 
   &lt;td&gt;84.2442&lt;/td&gt; 
   &lt;td&gt;3.97619&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;mathpix&lt;/td&gt; 
   &lt;td&gt;6.36223&lt;/td&gt; 
   &lt;td&gt;86.4281&lt;/td&gt; 
   &lt;td&gt;4.15626&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;docling&lt;/td&gt; 
   &lt;td&gt;3.69949&lt;/td&gt; 
   &lt;td&gt;86.7073&lt;/td&gt; 
   &lt;td&gt;3.70429&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Benchmarks were run on an H100 for markjer and docling - llamaparse and mathpix used their cloud services. We can also look at it by document type:&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/datalab-to/marker/master/data/images/per_doc.png" width="1000px" /&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Document Type&lt;/th&gt; 
   &lt;th&gt;Marker heuristic&lt;/th&gt; 
   &lt;th&gt;Marker LLM&lt;/th&gt; 
   &lt;th&gt;Llamaparse Heuristic&lt;/th&gt; 
   &lt;th&gt;Llamaparse LLM&lt;/th&gt; 
   &lt;th&gt;Mathpix Heuristic&lt;/th&gt; 
   &lt;th&gt;Mathpix LLM&lt;/th&gt; 
   &lt;th&gt;Docling Heuristic&lt;/th&gt; 
   &lt;th&gt;Docling LLM&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Scientific paper&lt;/td&gt; 
   &lt;td&gt;96.6737&lt;/td&gt; 
   &lt;td&gt;4.34899&lt;/td&gt; 
   &lt;td&gt;87.1651&lt;/td&gt; 
   &lt;td&gt;3.96421&lt;/td&gt; 
   &lt;td&gt;91.2267&lt;/td&gt; 
   &lt;td&gt;4.46861&lt;/td&gt; 
   &lt;td&gt;92.135&lt;/td&gt; 
   &lt;td&gt;3.72422&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Book page&lt;/td&gt; 
   &lt;td&gt;97.1846&lt;/td&gt; 
   &lt;td&gt;4.16168&lt;/td&gt; 
   &lt;td&gt;90.9532&lt;/td&gt; 
   &lt;td&gt;4.07186&lt;/td&gt; 
   &lt;td&gt;93.8886&lt;/td&gt; 
   &lt;td&gt;4.35329&lt;/td&gt; 
   &lt;td&gt;90.0556&lt;/td&gt; 
   &lt;td&gt;3.64671&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Other&lt;/td&gt; 
   &lt;td&gt;95.1632&lt;/td&gt; 
   &lt;td&gt;4.25076&lt;/td&gt; 
   &lt;td&gt;81.1385&lt;/td&gt; 
   &lt;td&gt;4.01835&lt;/td&gt; 
   &lt;td&gt;79.6231&lt;/td&gt; 
   &lt;td&gt;4.00306&lt;/td&gt; 
   &lt;td&gt;83.8223&lt;/td&gt; 
   &lt;td&gt;3.76147&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Form&lt;/td&gt; 
   &lt;td&gt;88.0147&lt;/td&gt; 
   &lt;td&gt;3.84663&lt;/td&gt; 
   &lt;td&gt;66.3081&lt;/td&gt; 
   &lt;td&gt;3.68712&lt;/td&gt; 
   &lt;td&gt;64.7512&lt;/td&gt; 
   &lt;td&gt;3.33129&lt;/td&gt; 
   &lt;td&gt;68.3857&lt;/td&gt; 
   &lt;td&gt;3.40491&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Presentation&lt;/td&gt; 
   &lt;td&gt;95.1562&lt;/td&gt; 
   &lt;td&gt;4.13669&lt;/td&gt; 
   &lt;td&gt;81.2261&lt;/td&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;83.6737&lt;/td&gt; 
   &lt;td&gt;3.95683&lt;/td&gt; 
   &lt;td&gt;84.8405&lt;/td&gt; 
   &lt;td&gt;3.86331&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Financial document&lt;/td&gt; 
   &lt;td&gt;95.3697&lt;/td&gt; 
   &lt;td&gt;4.39106&lt;/td&gt; 
   &lt;td&gt;82.5812&lt;/td&gt; 
   &lt;td&gt;4.16111&lt;/td&gt; 
   &lt;td&gt;81.3115&lt;/td&gt; 
   &lt;td&gt;4.05556&lt;/td&gt; 
   &lt;td&gt;86.3882&lt;/td&gt; 
   &lt;td&gt;3.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Letter&lt;/td&gt; 
   &lt;td&gt;98.4021&lt;/td&gt; 
   &lt;td&gt;4.5&lt;/td&gt; 
   &lt;td&gt;93.4477&lt;/td&gt; 
   &lt;td&gt;4.28125&lt;/td&gt; 
   &lt;td&gt;96.0383&lt;/td&gt; 
   &lt;td&gt;4.45312&lt;/td&gt; 
   &lt;td&gt;92.0952&lt;/td&gt; 
   &lt;td&gt;4.09375&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Engineering document&lt;/td&gt; 
   &lt;td&gt;93.9244&lt;/td&gt; 
   &lt;td&gt;4.04412&lt;/td&gt; 
   &lt;td&gt;77.4854&lt;/td&gt; 
   &lt;td&gt;3.72059&lt;/td&gt; 
   &lt;td&gt;80.3319&lt;/td&gt; 
   &lt;td&gt;3.88235&lt;/td&gt; 
   &lt;td&gt;79.6807&lt;/td&gt; 
   &lt;td&gt;3.42647&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Legal document&lt;/td&gt; 
   &lt;td&gt;96.689&lt;/td&gt; 
   &lt;td&gt;4.27759&lt;/td&gt; 
   &lt;td&gt;86.9769&lt;/td&gt; 
   &lt;td&gt;3.87584&lt;/td&gt; 
   &lt;td&gt;91.601&lt;/td&gt; 
   &lt;td&gt;4.20805&lt;/td&gt; 
   &lt;td&gt;87.8383&lt;/td&gt; 
   &lt;td&gt;3.65552&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Newspaper page&lt;/td&gt; 
   &lt;td&gt;98.8733&lt;/td&gt; 
   &lt;td&gt;4.25806&lt;/td&gt; 
   &lt;td&gt;84.7492&lt;/td&gt; 
   &lt;td&gt;3.90323&lt;/td&gt; 
   &lt;td&gt;96.9963&lt;/td&gt; 
   &lt;td&gt;4.45161&lt;/td&gt; 
   &lt;td&gt;92.6496&lt;/td&gt; 
   &lt;td&gt;3.51613&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Magazine page&lt;/td&gt; 
   &lt;td&gt;98.2145&lt;/td&gt; 
   &lt;td&gt;4.38776&lt;/td&gt; 
   &lt;td&gt;87.2902&lt;/td&gt; 
   &lt;td&gt;3.97959&lt;/td&gt; 
   &lt;td&gt;93.5934&lt;/td&gt; 
   &lt;td&gt;4.16327&lt;/td&gt; 
   &lt;td&gt;93.0892&lt;/td&gt; 
   &lt;td&gt;4.02041&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Throughput&lt;/h2&gt; 
&lt;p&gt;We benchmarked throughput using a &lt;a href="https://www.greenteapress.com/thinkpython/thinkpython.pdf"&gt;single long PDF&lt;/a&gt;.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Time per page&lt;/th&gt; 
   &lt;th&gt;Time per document&lt;/th&gt; 
   &lt;th&gt;VRAM used&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;marker&lt;/td&gt; 
   &lt;td&gt;0.18&lt;/td&gt; 
   &lt;td&gt;43.42&lt;/td&gt; 
   &lt;td&gt;3.17GB&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The projected throughput is 122 pages per second on an H100 - we can run 22 individual processes given the VRAM used.&lt;/p&gt; 
&lt;h2&gt;Table Conversion&lt;/h2&gt; 
&lt;p&gt;Marker can extract tables from PDFs using &lt;code&gt;marker.converters.table.TableConverter&lt;/code&gt;. The table extraction performance is measured by comparing the extracted HTML representation of tables against the original HTML representations using the test split of &lt;a href="https://developer.ibm.com/exchanges/data/all/fintabnet/"&gt;FinTabNet&lt;/a&gt;. The HTML representations are compared using a tree edit distance based metric to judge both structure and content. Marker detects and identifies the structure of all tables in a PDF page and achieves these scores:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Avg score&lt;/th&gt; 
   &lt;th&gt;Total tables&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;marker&lt;/td&gt; 
   &lt;td&gt;0.816&lt;/td&gt; 
   &lt;td&gt;99&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;marker w/use_llm&lt;/td&gt; 
   &lt;td&gt;0.907&lt;/td&gt; 
   &lt;td&gt;99&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gemini&lt;/td&gt; 
   &lt;td&gt;0.829&lt;/td&gt; 
   &lt;td&gt;99&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The &lt;code&gt;--use_llm&lt;/code&gt; flag can significantly improve table recognition performance, as you can see.&lt;/p&gt; 
&lt;p&gt;We filter out tables that we cannot align with the ground truth, since fintabnet and our layout model have slightly different detection methods (this results in some tables being split/merged).&lt;/p&gt; 
&lt;h2&gt;Running your own benchmarks&lt;/h2&gt; 
&lt;p&gt;You can benchmark the performance of marker on your machine. Install marker manually with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/VikParuchuri/marker.git
poetry install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Overall PDF Conversion&lt;/h3&gt; 
&lt;p&gt;Download the benchmark data &lt;a href="https://drive.google.com/file/d/1ZSeWDo2g1y0BRLT7KnbmytV2bjWARWba/view?usp=sharing"&gt;here&lt;/a&gt; and unzip. Then run the overall benchmark like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python benchmarks/overall.py --methods marker --scores heuristic,llm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--use_llm&lt;/code&gt; use an llm to improve the marker results.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--max_rows&lt;/code&gt; how many rows to process for the benchmark.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--methods&lt;/code&gt; can be &lt;code&gt;llamaparse&lt;/code&gt;, &lt;code&gt;mathpix&lt;/code&gt;, &lt;code&gt;docling&lt;/code&gt;, &lt;code&gt;marker&lt;/code&gt;. Comma separated.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--scores&lt;/code&gt; which scoring functions to use, can be &lt;code&gt;llm&lt;/code&gt;, &lt;code&gt;heuristic&lt;/code&gt;. Comma separated.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Table Conversion&lt;/h3&gt; 
&lt;p&gt;The processed FinTabNet dataset is hosted &lt;a href="https://huggingface.co/datasets/datalab-to/fintabnet-test"&gt;here&lt;/a&gt; and is automatically downloaded. Run the benchmark with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python benchmarks/table/table.py --max_rows 100
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--use_llm&lt;/code&gt; uses an llm with marker to improve accuracy.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--use_gemini&lt;/code&gt; also benchmarks gemini 2.0 flash.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;How it works&lt;/h1&gt; 
&lt;p&gt;Marker is a pipeline of deep learning models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Extract text, OCR if necessary (heuristics, &lt;a href="https://github.com/VikParuchuri/surya"&gt;surya&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Detect page layout and find reading order (&lt;a href="https://github.com/VikParuchuri/surya"&gt;surya&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Clean and format each block (heuristics, &lt;a href="https://github.com/VikParuchuri/texify"&gt;texify&lt;/a&gt;, &lt;a href="https://github.com/VikParuchuri/surya"&gt;surya&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Optionally use an LLM to improve quality&lt;/li&gt; 
 &lt;li&gt;Combine blocks and postprocess complete text&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;It only uses models where necessary, which improves speed and accuracy.&lt;/p&gt; 
&lt;h1&gt;Limitations&lt;/h1&gt; 
&lt;p&gt;PDF is a tricky format, so marker will not always work perfectly. Here are some known limitations that are on the roadmap to address:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Very complex layouts, with nested tables and forms, may not work&lt;/li&gt; 
 &lt;li&gt;Forms may not be rendered well&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Note: Passing the &lt;code&gt;--use_llm&lt;/code&gt; and &lt;code&gt;--force_ocr&lt;/code&gt; flags will mostly solve these issues.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google/wire</title>
      <link>https://github.com/google/wire</link>
      <description>&lt;p&gt;Compile-time Dependency Injection for Go&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Wire: Automated Initialization in Go&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/google/wire/actions"&gt;&lt;img src="https://github.com/google/wire/actions/workflows/tests.yml/badge.svg?branch=main" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://godoc.org/github.com/google/wire"&gt;&lt;img src="https://godoc.org/github.com/google/wire?status.svg?sanitize=true" alt="godoc" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/google/wire"&gt;&lt;img src="https://codecov.io/gh/google/wire/branch/master/graph/badge.svg?sanitize=true" alt="Coverage" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Wire is a code generation tool that automates connecting components using &lt;a href="https://en.wikipedia.org/wiki/Dependency_injection"&gt;dependency injection&lt;/a&gt;. Dependencies between components are represented in Wire as function parameters, encouraging explicit initialization instead of global variables. Because Wire operates without runtime state or reflection, code written to be used with Wire is useful even for hand-written initialization.&lt;/p&gt; 
&lt;p&gt;For an overview, see the &lt;a href="https://blog.golang.org/wire"&gt;introductory blog post&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installing&lt;/h2&gt; 
&lt;p&gt;Install Wire by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;go install github.com/google/wire/cmd/wire@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and ensuring that &lt;code&gt;$GOPATH/bin&lt;/code&gt; is added to your &lt;code&gt;$PATH&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/wire/main/_tutorial/README.md"&gt;Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/wire/main/docs/guide.md"&gt;User Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/wire/main/docs/best-practices.md"&gt;Best Practices&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/wire/main/docs/faq.md"&gt;FAQ&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Project status&lt;/h2&gt; 
&lt;p&gt;As of version v0.3.0, Wire is &lt;em&gt;beta&lt;/em&gt; and is considered feature complete. It works well for the tasks it was designed to perform, and we prefer to keep it as simple as possible.&lt;/p&gt; 
&lt;p&gt;We'll not be accepting new features at this time, but will gladly accept bug reports and fixes.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;For questions, please use &lt;a href="https://github.com/google/wire/discussions"&gt;GitHub Discussions&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This project is covered by the Go &lt;a href="https://raw.githubusercontent.com/google/wire/main/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>jitsi/jitsi-meet</title>
      <link>https://github.com/jitsi/jitsi-meet</link>
      <description>&lt;p&gt;Jitsi Meet - Secure, Simple and Scalable Video Conferences that you use as a standalone app or embed in your web application.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;p align="center"&gt;Jitsi Meet&lt;/p&gt;&lt;/h1&gt; 
&lt;p&gt;Jitsi Meet is a set of Open Source projects which empower users to use and deploy video conferencing platforms with state-of-the-art video quality and features.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/jitsi/jitsi-meet/master/readme-img1.png" width="900" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Amongst others here are the main features Jitsi Meet offers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Support for all current browsers&lt;/li&gt; 
 &lt;li&gt;Mobile applications&lt;/li&gt; 
 &lt;li&gt;Web and native SDKs for integration&lt;/li&gt; 
 &lt;li&gt;HD audio and video&lt;/li&gt; 
 &lt;li&gt;Content sharing&lt;/li&gt; 
 &lt;li&gt;Raise hand and reactions&lt;/li&gt; 
 &lt;li&gt;Chat with private conversations&lt;/li&gt; 
 &lt;li&gt;Polls&lt;/li&gt; 
 &lt;li&gt;Virtual backgrounds&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;And many more!&lt;/p&gt; 
&lt;h2&gt;Using Jitsi Meet&lt;/h2&gt; 
&lt;p&gt;Using Jitsi Meet is straightforward, as it's browser based. Head over to &lt;a href="https://meet.jit.si"&gt;meet.jit.si&lt;/a&gt; and give it a try. It's scalable and free to use. All you need is a Google, Facebook or GitHub account in order to start a meeting. All browsers are supported!&lt;/p&gt; 
&lt;p&gt;Using mobile? No problem, you can either use your mobile web browser or our fully-featured mobile apps:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Android&lt;/th&gt; 
   &lt;th align="center"&gt;Android (F-Droid)&lt;/th&gt; 
   &lt;th align="center"&gt;iOS&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://play.google.com/store/apps/details?id=org.jitsi.meet"&gt;&lt;img src="https://raw.githubusercontent.com/jitsi/jitsi-meet/master/resources/img/google-play-badge.png" height="50" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://f-droid.org/packages/org.jitsi.meet/"&gt;&lt;img src="https://raw.githubusercontent.com/jitsi/jitsi-meet/master/resources/img/f-droid-badge.png" height="50" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://itunes.apple.com/us/app/jitsi-meet/id1165103905"&gt;&lt;img src="https://raw.githubusercontent.com/jitsi/jitsi-meet/master/resources/img/appstore-badge.png" height="50" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;If you are feeling adventurous and want to get an early scoop of the features as they are being developed you can also sign up for our open beta testing here:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://play.google.com/apps/testing/org.jitsi.meet"&gt;Android&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://testflight.apple.com/join/isy6ja7S"&gt;iOS&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Running your own instance&lt;/h2&gt; 
&lt;p&gt;If you'd like to run your own Jitsi Meet installation head over to the &lt;a href="https://jitsi.github.io/handbook/docs/devops-guide/"&gt;handbook&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;p&gt;We provide Debian packages and a comprehensive Docker setup to make deployments as simple as possible. Advanced users also have the possibility of building all the components from source.&lt;/p&gt; 
&lt;p&gt;You can check the latest releases &lt;a href="https://jitsi.github.io/handbook/docs/releases"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Jitsi as a Service&lt;/h2&gt; 
&lt;p&gt;If you like the branding capabilities of running your own instance but you'd like to avoid dealing with the complexity of monitoring, scaling and updates, JaaS might be for you.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://jaas.8x8.vc"&gt;8x8 Jitsi as a Service (JaaS)&lt;/a&gt; is an enterprise-ready video meeting platform that allows developers, organizations and businesses to easily build and deploy video solutions. With Jitsi as a Service we now give you all the power of Jitsi running on our global platform so you can focus on building secure and branded video experiences.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;All the Jitsi Meet documentation is available in &lt;a href="https://jitsi.github.io/handbook/"&gt;the handbook&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Security&lt;/h2&gt; 
&lt;p&gt;For a comprehensive description of all Jitsi Meet's security aspects, please check &lt;a href="https://jitsi.org/security"&gt;this link&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For a detailed description of Jitsi Meet's End-to-End Encryption (E2EE) implementation, please check &lt;a href="https://jitsi.org/e2ee-whitepaper/"&gt;this link&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For information on reporting security vulnerabilities in Jitsi Meet, see &lt;a href="https://raw.githubusercontent.com/jitsi/jitsi-meet/master/SECURITY.md"&gt;SECURITY.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;If you are looking to contribute to Jitsi Meet, first of all, thank you! Please see our &lt;a href="https://raw.githubusercontent.com/jitsi/jitsi-meet/master/CONTRIBUTING.md"&gt;guidelines for contributing&lt;/a&gt;.&lt;/p&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;footer&gt; 
 &lt;p align="center" style="font-size: smaller;"&gt; Built with ‚ù§Ô∏è by the Jitsi team at &lt;a href="https://8x8.com" target="_blank"&gt;8x8&lt;/a&gt; and our community. &lt;/p&gt; 
&lt;/footer&gt;</description>
    </item>
    
    <item>
      <title>ubicloud/ubicloud</title>
      <link>https://github.com/ubicloud/ubicloud</link>
      <description>&lt;p&gt;Open source alternative to AWS. Elastic compute, block storage (non replicated), firewall and load balancer, managed Postgres, K8s, AI inference, and IAM services.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://github.com/user-attachments/assets/779e73bd-c260-4729-8430-c630628f1b6b" /&gt; &lt;/p&gt; 
&lt;h1&gt;Ubicloud &lt;a href="https://github.com/ubicloud/ubicloud/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/ubicloud/ubicloud/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/ubicloud/ubicloud/actions/workflows/build.yml"&gt;&lt;img src="https://github.com/ubicloud/ubicloud/actions/workflows/build.yml/badge.svg?sanitize=true" alt="Build" /&gt;&lt;/a&gt; &lt;a href="https://app.greptile.com/repo/ubicloud/ubicloud"&gt;&lt;img src="https://img.shields.io/badge/learn_with-greptile-%091B12?color=%091B12" alt="Learn this repo using Greptile" /&gt;&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;Ubicloud is an open source cloud that can run anywhere. Think of it as an open alternative to cloud providers, like what Linux is to proprietary operating systems.&lt;/p&gt; 
&lt;p&gt;Ubicloud provides IaaS cloud features on bare metal providers, such as Hetzner, Leaseweb, and AWS Bare Metal. You can set it up yourself on these providers or you can use our &lt;a href="https://console.ubicloud.com"&gt;managed service&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Quick start&lt;/h2&gt; 
&lt;h3&gt;Managed platform&lt;/h3&gt; 
&lt;p&gt;You can use Ubicloud without installing anything. When you do this, we pass along the underlying provider's benefits to you, such as price or location.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://console.ubicloud.com"&gt;https://console.ubicloud.com&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Build your own cloud&lt;/h3&gt; 
&lt;p&gt;You can also build your own cloud. To do this, start up Ubicloud's control plane and connect to its cloud console.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone git@github.com:ubicloud/ubicloud.git

# Generate secrets for demo
./demo/generate_env

# Run containers: db-migrator, app (web &amp;amp; respirate), postgresql
docker-compose -f demo/docker-compose.yml up

# Visit localhost:3000
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The control plane is responsible for cloudifying bare metal Linux machines. The easiest way to build your own cloud is to lease instances from one of those providers. For example: &lt;a href="https://www.hetzner.com/sb"&gt;https://www.hetzner.com/sb&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Once you lease instance(s), update the &lt;code&gt;.env&lt;/code&gt; file with the following environment variables:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;HETZNER_USER&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;HETZNER_PASSWORD&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;HETZNER_SSH_PUBLIC_KEY&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;HETZNER_SSH_PRIVATE_KEY&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Then, run the following script for each instance to cloudify it. Currently, the script cloudifies bare metal instances leased from Hetzner. After you cloudify your instances, you can provision and manage cloud resources on these machines.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# Enter hostname/IP and provider
docker exec -it ubicloud-app ./demo/cloudify_server
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Later when you create VMs, Ubicloud will assign them IPv6 addresses. If your ISP doesn't support IPv6, please use a VPN or tunnel broker such as Mullvad or Hurricane Electric's &lt;a href="https://tunnelbroker.net/"&gt;https://tunnelbroker.net/&lt;/a&gt; to connect. Alternatively, you could lease IPv4 addresses from your provider and add them to your control plane.&lt;/p&gt; 
&lt;h2&gt;Why use it&lt;/h2&gt; 
&lt;p&gt;Public cloud providers like AWS, Azure, and Google Cloud have made life easier for start-ups and enterprises. But they are closed source, have you rent computers at a huge premium, and lock you in. Ubicloud offers an open source alternative, reduces your costs, and returns control of your infrastructure back to you. All without sacrificing the cloud's convenience.&lt;/p&gt; 
&lt;p&gt;Today, AWS offers about two hundred cloud services. Ultimately, we will implement 10% of the cloud services that make up 80% of that consumption.&lt;/p&gt; 
&lt;p&gt;Example workloads and reasons to use Ubicloud today include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;You have an ephemeral workload like a CI/CD pipeline (we're integrating with GitHub Actions), or you'd like to run compute/memory heavy tests. Our managed cloud is ~3x cheaper than AWS, so you save on costs.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;You want a portable and simple app deployment service like &lt;a href="https://github.com/basecamp/kamal"&gt;Kamal&lt;/a&gt;. We're moving Ubicloud's control plane from Heroku to Kamal; and we want to provide open and portable services for Kamal's dependencies in the process.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;You have bare metal machines sitting somewhere. You'd like to build your own cloud for portability, security, or compliance reasons.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Status&lt;/h2&gt; 
&lt;p&gt;You can provide us your feedback, get help, or ask us questions regarding your Ubicloud installations in the &lt;a href="https://github.com/ubicloud/ubicloud/discussions"&gt;Community Forum&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We follow an established architectural pattern in building public cloud services. A control plane manages a data plane, where the data plane leverages open source software. You can find our current cloud components / services below.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Elastic Compute&lt;/strong&gt;: Our control plane communicates with Linux bare metal servers using SSH. We use &lt;a href="https://github.com/cloud-hypervisor/cloud-hypervisor"&gt;Cloud Hypervisor&lt;/a&gt; as our virtual machine monitor (VMM); and each instance of the VMM is contained within Linux namespaces for further isolation / security.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Networking&lt;/strong&gt;: We use &lt;a href="https://en.wikipedia.org/wiki/IPsec"&gt;IPsec&lt;/a&gt; tunneling to establish an encrypted and private network environment. We support IPv4 and IPv6 in a dual-stack setup and provide both public and private networking. For security, each customer‚Äôs VMs operate in their own networking namespace. For &lt;a href="https://www.ubicloud.com/blog/ubicloud-firewalls-how-linux-nftables-enables-flexible-rules"&gt;firewalls&lt;/a&gt; and &lt;a href="https://www.ubicloud.com/blog/ubicloud-load-balancer-simple-and-cost-free"&gt;load balancers&lt;/a&gt;, we use Linux nftables.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Block Storage, non replicated&lt;/strong&gt;: We use Storage Performance Development Toolkit (&lt;a href="https://spdk.io"&gt;SPDK&lt;/a&gt;) to provide virtualized block storage to VMs. SPDK enables us to add enterprise features such as snapshot and replication in the future. We follow security best practices and encrypt the data encryption key itself.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Attribute-Based Access Control (ABAC)&lt;/strong&gt;: With ABAC, you can define attributes, roles, and permissions for users and give them fine-grained access to resources. You can read more about our &lt;a href="https://raw.githubusercontent.com/ubicloud/ubicloud/main/doc/authorization.md"&gt;ABAC design here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;What's Next?&lt;/strong&gt;: We're planning to work on a managed K8s or metrics/monitoring service next. If you have a workload that would benefit from a specific cloud service, please get in touch with us through our &lt;a href="https://github.com/ubicloud/ubicloud/discussions"&gt;Community Forum&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Control plane: Manages data plane services and resources. This is a Ruby program that stores its data in Postgres. We use the &lt;a href="https://roda.jeremyevans.net/"&gt;Roda&lt;/a&gt; framework to serve HTTP requests and &lt;a href="http://sequel.jeremyevans.net/"&gt;Sequel&lt;/a&gt; to access the database. We manage web authentication with &lt;a href="http://rodauth.jeremyevans.net/"&gt;Rodauth&lt;/a&gt;. We communicate with data plane servers using SSH, via the library &lt;a href="https://github.com/net-ssh/net-ssh"&gt;net-ssh&lt;/a&gt;. For our tests, we use &lt;a href="https://rspec.info/"&gt;RSpec&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Cloud console: Server-side web app served by the Roda framework. For the visual design, we use &lt;a href="https://tailwindcss.com"&gt;Tailwind CSS&lt;/a&gt; with components from &lt;a href="https://tailwindui.com"&gt;Tailwind UI&lt;/a&gt;. We also use jQuery for interactivity.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you‚Äôd like to start hacking with Ubicloud, any method of obtaining Ruby and Postgres versions is acceptable. If you have no opinion on this, our development team uses &lt;code&gt;mise&lt;/code&gt; as &lt;a href="https://raw.githubusercontent.com/ubicloud/ubicloud/main/DEVELOPERS.md"&gt;documented here in detail&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://greptile.com/"&gt;Greptile&lt;/a&gt; provides an AI/LLM that indexes Ubicloud's source code &lt;a href="https://learnthisrepo.com/ubicloud"&gt;can answer questions about it&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;h3&gt;Do you have any experience with building this sort of thing?&lt;/h3&gt; 
&lt;p&gt;Our founding team comes from Azure; and worked at Amazon and Heroku before that. We also have start-up experience. We were co-founders and founding team members at &lt;a href="https://github.com/citusdata/citus"&gt;Citus Data&lt;/a&gt;, &lt;a href="https://news.ycombinator.com/item?id=18990469"&gt;which got acquired by Microsoft&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;How is this different than OpenStack?&lt;/h3&gt; 
&lt;p&gt;We see three differences. First, Ubicloud is available as a managed service (vs boxed software). This way, you can get started in minutes rather than weeks. Since Ubicloud is designed for multi-tenancy, it comes with built-in features such as encryption at rest and in transit, virtual networking, secrets rotation, etc.&lt;/p&gt; 
&lt;p&gt;Second, we're initially targeting developers. This -we hope- will give us fast feedback cycles and enable us to have 6 key services in GA form in the next two years. OpenStack is still primarily used for 3 cloud services.&lt;/p&gt; 
&lt;p&gt;Last, we're designing for simplicity. With OpenStack, you pick between 10 hypervisors, 10 S3 implementations, and 5 block storage implementations. The software needs to work in a way where all of these implementations are compatible with each other. That leads to consultant-ware. We'll take a more opinionated approach with Ubicloud.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/markitdown</title>
      <link>https://github.com/microsoft/markitdown</link>
      <description>&lt;p&gt;Python tool for converting files and office documents to Markdown.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MarkItDown&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/markitdown/"&gt;&lt;img src="https://img.shields.io/pypi/v/markitdown.svg?sanitize=true" alt="PyPI" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/dd/markitdown" alt="PyPI - Downloads" /&gt; &lt;a href="https://github.com/microsoft/autogen"&gt;&lt;img src="https://img.shields.io/badge/Built%20by-AutoGen%20Team-blue" alt="Built by AutoGen Team" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] MarkItDown now offers an MCP (Model Context Protocol) server for integration with LLM applications like Claude Desktop. See &lt;a href="https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp"&gt;markitdown-mcp&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Breaking changes between 0.0.1 to 0.1.0:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Dependencies are now organized into optional feature-groups (further details below). Use &lt;code&gt;pip install 'markitdown[all]'&lt;/code&gt; to have backward-compatible behavior.&lt;/li&gt; 
  &lt;li&gt;convert_stream() now requires a binary file-like object (e.g., a file opened in binary mode, or an io.BytesIO object). This is a breaking change from the previous version, where it previously also accepted text file-like objects, like io.StringIO.&lt;/li&gt; 
  &lt;li&gt;The DocumentConverter class interface has changed to read from file-like streams rather than file paths. &lt;em&gt;No temporary files are created anymore&lt;/em&gt;. If you are the maintainer of a plugin, or custom DocumentConverter, you likely need to update your code. Otherwise, if only using the MarkItDown class or CLI (as in these examples), you should not need to change anything.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;MarkItDown is a lightweight Python utility for converting various files to Markdown for use with LLMs and related text analysis pipelines. To this end, it is most comparable to &lt;a href="https://github.com/deanmalmgren/textract"&gt;textract&lt;/a&gt;, but with a focus on preserving important document structure and content as Markdown (including: headings, lists, tables, links, etc.) While the output is often reasonably presentable and human-friendly, it is meant to be consumed by text analysis tools -- and may not be the best option for high-fidelity document conversions for human consumption.&lt;/p&gt; 
&lt;p&gt;MarkItDown currently supports the conversion from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PDF&lt;/li&gt; 
 &lt;li&gt;PowerPoint&lt;/li&gt; 
 &lt;li&gt;Word&lt;/li&gt; 
 &lt;li&gt;Excel&lt;/li&gt; 
 &lt;li&gt;Images (EXIF metadata and OCR)&lt;/li&gt; 
 &lt;li&gt;Audio (EXIF metadata and speech transcription)&lt;/li&gt; 
 &lt;li&gt;HTML&lt;/li&gt; 
 &lt;li&gt;Text-based formats (CSV, JSON, XML)&lt;/li&gt; 
 &lt;li&gt;ZIP files (iterates over contents)&lt;/li&gt; 
 &lt;li&gt;Youtube URLs&lt;/li&gt; 
 &lt;li&gt;EPubs&lt;/li&gt; 
 &lt;li&gt;... and more!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Why Markdown?&lt;/h2&gt; 
&lt;p&gt;Markdown is extremely close to plain text, with minimal markup or formatting, but still provides a way to represent important document structure. Mainstream LLMs, such as OpenAI's GPT-4o, natively "&lt;em&gt;speak&lt;/em&gt;" Markdown, and often incorporate Markdown into their responses unprompted. This suggests that they have been trained on vast amounts of Markdown-formatted text, and understand it well. As a side benefit, Markdown conventions are also highly token-efficient.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;MarkItDown requires Python 3.10 or higher. It is recommended to use a virtual environment to avoid dependency conflicts.&lt;/p&gt; 
&lt;p&gt;With the standard Python installation, you can create and activate a virtual environment using the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv .venv
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If using &lt;code&gt;uv&lt;/code&gt;, you can create a virtual environment with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv --python=3.12 .venv
source .venv/bin/activate
# NOTE: Be sure to use 'uv pip install' rather than just 'pip install' to install packages in this virtual environment
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are using Anaconda, you can create a virtual environment with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n markitdown python=3.12
conda activate markitdown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install MarkItDown, use pip: &lt;code&gt;pip install 'markitdown[all]'&lt;/code&gt;. Alternatively, you can install it from the source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone git@github.com:microsoft/markitdown.git
cd markitdown
pip install -e 'packages/markitdown[all]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Command-Line&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf &amp;gt; document.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or use &lt;code&gt;-o&lt;/code&gt; to specify the output file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf -o document.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also pipe content:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cat path-to-file.pdf | markitdown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Optional Dependencies&lt;/h3&gt; 
&lt;p&gt;MarkItDown has optional dependencies for activating various file formats. Earlier in this document, we installed all optional dependencies with the &lt;code&gt;[all]&lt;/code&gt; option. However, you can also install them individually for more control. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install 'markitdown[pdf, docx, pptx]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;will install only the dependencies for PDF, DOCX, and PPTX files.&lt;/p&gt; 
&lt;p&gt;At the moment, the following optional dependencies are available:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;[all]&lt;/code&gt; Installs all optional dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[pptx]&lt;/code&gt; Installs dependencies for PowerPoint files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[docx]&lt;/code&gt; Installs dependencies for Word files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[xlsx]&lt;/code&gt; Installs dependencies for Excel files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[xls]&lt;/code&gt; Installs dependencies for older Excel files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[pdf]&lt;/code&gt; Installs dependencies for PDF files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[outlook]&lt;/code&gt; Installs dependencies for Outlook messages&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[az-doc-intel]&lt;/code&gt; Installs dependencies for Azure Document Intelligence&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[audio-transcription]&lt;/code&gt; Installs dependencies for audio transcription of wav and mp3 files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[youtube-transcription]&lt;/code&gt; Installs dependencies for fetching YouTube video transcription&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Plugins&lt;/h3&gt; 
&lt;p&gt;MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown --list-plugins
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To enable plugins use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown --use-plugins path-to-file.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To find available plugins, search GitHub for the hashtag &lt;code&gt;#markitdown-plugin&lt;/code&gt;. To develop a plugin, see &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Azure Document Intelligence&lt;/h3&gt; 
&lt;p&gt;To use Microsoft Document Intelligence for conversion:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf -o document.md -d -e "&amp;lt;document_intelligence_endpoint&amp;gt;"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;More information about how to set up an Azure Document Intelligence Resource can be found &lt;a href="https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/create-document-intelligence-resource?view=doc-intel-4.0.0"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Python API&lt;/h3&gt; 
&lt;p&gt;Basic usage in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown

md = MarkItDown(enable_plugins=False) # Set to True to enable plugins
result = md.convert("test.xlsx")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Document Intelligence conversion in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown

md = MarkItDown(docintel_endpoint="&amp;lt;document_intelligence_endpoint&amp;gt;")
result = md.convert("test.pdf")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use Large Language Models for image descriptions, provide &lt;code&gt;llm_client&lt;/code&gt; and &lt;code&gt;llm_model&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown
from openai import OpenAI

client = OpenAI()
md = MarkItDown(llm_client=client, llm_model="gpt-4o")
result = md.convert("example.jpg")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;docker build -t markitdown:latest .
docker run --rm -i markitdown:latest &amp;lt; ~/your-file.pdf &amp;gt; output.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h3&gt;How to Contribute&lt;/h3&gt; 
&lt;p&gt;You can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as 'open for contribution' and 'open for reviewing' to help facilitate community contributions. These are ofcourse just suggestions and you are welcome to contribute in any way you like.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;/th&gt; 
    &lt;th&gt;All&lt;/th&gt; 
    &lt;th&gt;Especially Needs Help from Community&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Issues&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/issues"&gt;All Issues&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/issues?q=is%3Aissue+is%3Aopen+label%3A%22open+for+contribution%22"&gt;Issues open for contribution&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;PRs&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/pulls"&gt;All PRs&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/pulls?q=is%3Apr+is%3Aopen+label%3A%22open+for+reviewing%22"&gt;PRs open for reviewing&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h3&gt;Running Tests and Checks&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Navigate to the MarkItDown package:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;cd packages/markitdown
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;code&gt;hatch&lt;/code&gt; in your environment and run tests:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/
hatch shell
hatch test
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(Alternative) Use the Devcontainer which has all the dependencies installed:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;# Reopen the project in Devcontainer and run:
hatch test
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run pre-commit checks before submitting a PR: &lt;code&gt;pre-commit run --all-files&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing 3rd-party Plugins&lt;/h3&gt; 
&lt;p&gt;You can also contribute by creating and sharing 3rd party plugins. See &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Trademarks&lt;/h2&gt; 
&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general"&gt;Microsoft's Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>