<rss version="2.0">
  <channel>
    <title>GitHub Python Weekly Trending</title>
    <description>Weekly Trending of Python in GitHub</description>
    <pubDate>Sat, 16 Aug 2025 01:48:02 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>lfnovo/open-notebook</title>
      <link>https://github.com/lfnovo/open-notebook</link>
      <description>&lt;p&gt;An Open Source implementation of Notebook LM with more flexibility and features&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a id="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- [![Contributors][contributors-shield]][contributors-url] --&gt; 
&lt;p&gt;&lt;a href="https://github.com/lfnovo/open-notebook/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/lfnovo/open-notebook.svg?style=for-the-badge" alt="Forks" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lfnovo/open-notebook/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/lfnovo/open-notebook.svg?style=for-the-badge" alt="Stargazers" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;&lt;img src="https://img.shields.io/github/issues/lfnovo/open-notebook.svg?style=for-the-badge" alt="Issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lfnovo/open-notebook/raw/master/LICENSE.txt"&gt;&lt;img src="https://img.shields.io/github/license/lfnovo/open-notebook.svg?style=for-the-badge" alt="MIT License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- [![LinkedIn][linkedin-shield]][linkedin-url] --&gt; 
&lt;!-- PROJECT LOGO --&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/lfnovo/open-notebook"&gt; &lt;img src="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/assets/hero.svg?sanitize=true" alt="Logo" /&gt; &lt;/a&gt; 
 &lt;h3 align="center"&gt;Open Notebook&lt;/h3&gt; 
 &lt;p align="center"&gt; An open source, privacy-focused alternative to Google's Notebook LM! &lt;br /&gt;&lt;strong&gt;Join our &lt;a href="https://discord.gg/37XJPXfz2w"&gt;Discord server&lt;/a&gt; for help, to share workflow ideas, and suggest features!&lt;/strong&gt; &lt;br /&gt; &lt;a href="https://www.open-notebook.ai"&gt;&lt;strong&gt;Checkout our website Â»&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt; &lt;br /&gt; &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/index.md"&gt;ğŸ“š Get Started&lt;/a&gt; Â· &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/index.md"&gt;ğŸ“– User Guide&lt;/a&gt; Â· &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/index.md"&gt;âœ¨ Features&lt;/a&gt; Â· &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/index.md"&gt;ğŸš€ Deploy&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ“¢ Open Notebook is under very active development&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Open Notebook is under active development! We're moving fast and making improvements every week. Your feedback is incredibly valuable to me during this exciting phase and it gives me motivation to keep improving and building this amazing tool. Please feel free to star the project if you find it useful, and don't hesitate to reach out with any questions or suggestions. I'm excited to see how you'll use it and what ideas you'll bring to the project! Let's build something amazing together! ğŸš€&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;About The Project&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/assets/asset_list.png" alt="New Notebook" /&gt;&lt;/p&gt; 
&lt;p&gt;An open source, privacy-focused alternative to Google's Notebook LM. Why give Google more of our data when we can take control of our own research workflows?&lt;/p&gt; 
&lt;p&gt;In a world dominated by Artificial Intelligence, having the ability to think ğŸ§  and acquire new knowledge ğŸ’¡, is a skill that should not be a privilege for a few, nor restricted to a single provider.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Open Notebook empowers you to:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ”’ &lt;strong&gt;Control your data&lt;/strong&gt; - Keep your research private and secure&lt;/li&gt; 
 &lt;li&gt;ğŸ¤– &lt;strong&gt;Choose your AI models&lt;/strong&gt; - Support for 16+ providers including OpenAI, Anthropic, Ollama, LM Studio, and more&lt;/li&gt; 
 &lt;li&gt;ğŸ“š &lt;strong&gt;Organize multi-modal content&lt;/strong&gt; - PDFs, videos, audio, web pages, and more&lt;/li&gt; 
 &lt;li&gt;ğŸ™ï¸ &lt;strong&gt;Generate professional podcasts&lt;/strong&gt; - Advanced multi-speaker podcast generation&lt;/li&gt; 
 &lt;li&gt;ğŸ” &lt;strong&gt;Search intelligently&lt;/strong&gt; - Full-text and vector search across all your content&lt;/li&gt; 
 &lt;li&gt;ğŸ’¬ &lt;strong&gt;Chat with context&lt;/strong&gt; - AI conversations powered by your research&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Learn more about our project at &lt;a href="https://www.open-notebook.ai"&gt;https://www.open-notebook.ai&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ†š Open Notebook vs Google Notebook LM&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;Open Notebook&lt;/th&gt; 
   &lt;th&gt;Google Notebook LM&lt;/th&gt; 
   &lt;th&gt;Advantage&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Privacy &amp;amp; Control&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Self-hosted, your data&lt;/td&gt; 
   &lt;td&gt;Google cloud only&lt;/td&gt; 
   &lt;td&gt;Complete data sovereignty&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;AI Provider Choice&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;16+ providers (OpenAI, Anthropic, Ollama, LM Studio, etc.)&lt;/td&gt; 
   &lt;td&gt;Google models only&lt;/td&gt; 
   &lt;td&gt;Flexibility and cost optimization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Podcast Speakers&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;1-4 speakers with custom profiles&lt;/td&gt; 
   &lt;td&gt;2 speakers only&lt;/td&gt; 
   &lt;td&gt;Extreme flexibility&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Context Control&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;3 granular levels&lt;/td&gt; 
   &lt;td&gt;All-or-nothing&lt;/td&gt; 
   &lt;td&gt;Privacy and performance tuning&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Content Transformations&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Custom and built-in&lt;/td&gt; 
   &lt;td&gt;Limited options&lt;/td&gt; 
   &lt;td&gt;Unlimited processing power&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;API Access&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Full REST API&lt;/td&gt; 
   &lt;td&gt;No API&lt;/td&gt; 
   &lt;td&gt;Complete automation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Deployment&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Docker, cloud, or local&lt;/td&gt; 
   &lt;td&gt;Google hosted only&lt;/td&gt; 
   &lt;td&gt;Deploy anywhere&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Citations&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Comprehensive with sources&lt;/td&gt; 
   &lt;td&gt;Basic references&lt;/td&gt; 
   &lt;td&gt;Research integrity&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Customization&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Open source, fully customizable&lt;/td&gt; 
   &lt;td&gt;Closed system&lt;/td&gt; 
   &lt;td&gt;Unlimited extensibility&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Cost&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Pay only for AI usage&lt;/td&gt; 
   &lt;td&gt;Monthly subscription + usage&lt;/td&gt; 
   &lt;td&gt;Transparent and controllable&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Why Choose Open Notebook?&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ”’ &lt;strong&gt;Privacy First&lt;/strong&gt;: Your sensitive research stays completely private&lt;/li&gt; 
 &lt;li&gt;ğŸ’° &lt;strong&gt;Cost Control&lt;/strong&gt;: Choose cheaper AI providers or run locally with Ollama&lt;/li&gt; 
 &lt;li&gt;ğŸ™ï¸ &lt;strong&gt;Better Podcasts&lt;/strong&gt;: Full script control and multi-speaker flexibility vs limited 2-speaker deep-dive format&lt;/li&gt; 
 &lt;li&gt;ğŸ”§ &lt;strong&gt;Unlimited Customization&lt;/strong&gt;: Modify, extend, and integrate as needed&lt;/li&gt; 
 &lt;li&gt;ğŸŒ &lt;strong&gt;No Vendor Lock-in&lt;/strong&gt;: Switch providers, deploy anywhere, own your data&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Built With&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.python.org/"&gt;&lt;img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white" alt="Python" /&gt;&lt;/a&gt; &lt;a href="https://surrealdb.com/"&gt;&lt;img src="https://img.shields.io/badge/SurrealDB-FF5E00?style=for-the-badge&amp;amp;logo=databricks&amp;amp;logoColor=white" alt="SurrealDB" /&gt;&lt;/a&gt; &lt;a href="https://www.langchain.com/"&gt;&lt;img src="https://img.shields.io/badge/LangChain-3A3A3A?style=for-the-badge&amp;amp;logo=chainlink&amp;amp;logoColor=white" alt="LangChain" /&gt;&lt;/a&gt; &lt;a href="https://streamlit.io/"&gt;&lt;img src="https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&amp;amp;logo=streamlit&amp;amp;logoColor=white" alt="Streamlit" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸš€ Quick Start&lt;/h2&gt; 
&lt;p&gt;Ready to try Open Notebook? Choose your preferred method:&lt;/p&gt; 
&lt;h3&gt;âš¡ Instant Setup (Recommended)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create a new directory for your Open Notebook installation
mkdir open-notebook
cd open-notebook

# Using Docker - Get started in 2 minutes
docker run -d \
  --name open-notebook \
  -p 8502:8502 -p 5055:5055 \
  -v ./notebook_data:/app/data \
  -v ./surreal_data:/mydata \
  -e OPENAI_API_KEY=your_key \
  lfnovo/open_notebook:latest-single
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;What gets created:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;open-notebook/
â”œâ”€â”€ notebook_data/     # Your notebooks and research content
â””â”€â”€ surreal_data/      # Database files
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Access your installation:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ–¥ï¸ Main Interface&lt;/strong&gt;: &lt;a href="http://localhost:8502"&gt;http://localhost:8502&lt;/a&gt; (Streamlit UI)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ”§ API Access&lt;/strong&gt;: &lt;a href="http://localhost:5055"&gt;http://localhost:5055&lt;/a&gt; (REST API)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“š API Documentation&lt;/strong&gt;: &lt;a href="http://localhost:5055/docs"&gt;http://localhost:5055/docs&lt;/a&gt; (Interactive Swagger UI)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;âš ï¸ Important&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Run from a dedicated folder&lt;/strong&gt;: Create and run this from inside a new &lt;code&gt;open-notebook&lt;/code&gt; folder so your data volumes are properly organized&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Volume persistence&lt;/strong&gt;: The volumes (&lt;code&gt;-v ./notebook_data:/app/data&lt;/code&gt; and &lt;code&gt;-v ./surreal_data:/mydata&lt;/code&gt;) are essential to persist your data between container restarts. Without them, you'll lose all your notebooks and research when the container stops.&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;ğŸ› ï¸ Full Installation&lt;/h3&gt; 
&lt;p&gt;For development or customization:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/lfnovo/open-notebook
cd open-notebook
make start-all
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ğŸ“– Need Help?&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ¤– AI Installation Assistant&lt;/strong&gt;: We have a &lt;a href="https://chatgpt.com/g/g-68776e2765b48191bd1bae3f30212631-open-notebook-installation-assistant"&gt;CustomGPT built to help you install Open Notebook&lt;/a&gt; - it will guide you through each step!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;New to Open Notebook?&lt;/strong&gt; Start with our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/index.md"&gt;Getting Started Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Need installation help?&lt;/strong&gt; Check our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/installation.md"&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Want to see it in action?&lt;/strong&gt; Try our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/quick-start.md"&gt;Quick Start Tutorial&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Provider Support Matrix&lt;/h2&gt; 
&lt;p&gt;Thanks to the &lt;a href="https://github.com/lfnovo/esperanto"&gt;Esperanto&lt;/a&gt; library, we support this providers out of the box!&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Provider&lt;/th&gt; 
   &lt;th&gt;LLM Support&lt;/th&gt; 
   &lt;th&gt;Embedding Support&lt;/th&gt; 
   &lt;th&gt;Speech-to-Text&lt;/th&gt; 
   &lt;th&gt;Text-to-Speech&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Anthropic&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Groq&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Google (GenAI)&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vertex AI&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ollama&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Perplexity&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ElevenLabs&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Azure OpenAI&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mistral&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DeepSeek&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Voyage&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;xAI&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenRouter&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI Compatible*&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;*Supports LM Studio and any OpenAI-compatible endpoint&lt;/p&gt; 
&lt;h2&gt;âœ¨ Key Features&lt;/h2&gt; 
&lt;h3&gt;Core Capabilities&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ”’ Privacy-First&lt;/strong&gt;: Your data stays under your control - no cloud dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ¯ Multi-Notebook Organization&lt;/strong&gt;: Manage multiple research projects seamlessly&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“š Universal Content Support&lt;/strong&gt;: PDFs, videos, audio, web pages, Office docs, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ¤– Multi-Model AI Support&lt;/strong&gt;: 16+ providers including OpenAI, Anthropic, Ollama, Google, LM Studio, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ™ï¸ Professional Podcast Generation&lt;/strong&gt;: Advanced multi-speaker podcasts with Episode Profiles&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ” Intelligent Search&lt;/strong&gt;: Full-text and vector search across all your content&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ’¬ Context-Aware Chat&lt;/strong&gt;: AI conversations powered by your research materials&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“ AI-Assisted Notes&lt;/strong&gt;: Generate insights or write notes manually&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;âš¡ Reasoning Model Support&lt;/strong&gt;: Full support for thinking models like DeepSeek-R1 and Qwen3&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ”§ Content Transformations&lt;/strong&gt;: Powerful customizable actions to summarize and extract insights&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸŒ Comprehensive REST API&lt;/strong&gt;: Full programmatic access for custom integrations &lt;a href="http://localhost:5055/docs"&gt;&lt;img src="https://img.shields.io/badge/API-Documentation-blue?style=flat-square" alt="API Docs" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ” Optional Password Protection&lt;/strong&gt;: Secure public deployments with authentication&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“Š Fine-Grained Context Control&lt;/strong&gt;: Choose exactly what to share with AI models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“ Citations&lt;/strong&gt;: Get answers with proper source citations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Three-Column Interface&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Sources&lt;/strong&gt;: Manage all your research materials&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Notes&lt;/strong&gt;: Create manual or AI-generated notes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chat&lt;/strong&gt;: Converse with AI using your content as context&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=D-760MlGwaI"&gt;&lt;img src="https://img.youtube.com/vi/D-760MlGwaI/0.jpg" alt="Check out our podcast sample" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ“š Documentation&lt;/h2&gt; 
&lt;h3&gt;Getting Started&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/introduction.md"&gt;ğŸ“– Introduction&lt;/a&gt;&lt;/strong&gt; - Learn what Open Notebook offers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/quick-start.md"&gt;âš¡ Quick Start&lt;/a&gt;&lt;/strong&gt; - Get up and running in 5 minutes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/installation.md"&gt;ğŸ”§ Installation&lt;/a&gt;&lt;/strong&gt; - Comprehensive setup guide&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/first-notebook.md"&gt;ğŸ¯ Your First Notebook&lt;/a&gt;&lt;/strong&gt; - Step-by-step tutorial&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;User Guide&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/interface-overview.md"&gt;ğŸ“± Interface Overview&lt;/a&gt;&lt;/strong&gt; - Understanding the layout&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/notebooks.md"&gt;ğŸ“š Notebooks&lt;/a&gt;&lt;/strong&gt; - Organizing your research&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/sources.md"&gt;ğŸ“„ Sources&lt;/a&gt;&lt;/strong&gt; - Managing content types&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/notes.md"&gt;ğŸ“ Notes&lt;/a&gt;&lt;/strong&gt; - Creating and managing notes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/chat.md"&gt;ğŸ’¬ Chat&lt;/a&gt;&lt;/strong&gt; - AI conversations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/search.md"&gt;ğŸ” Search&lt;/a&gt;&lt;/strong&gt; - Finding information&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Topics&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/podcasts.md"&gt;ğŸ™ï¸ Podcast Generation&lt;/a&gt;&lt;/strong&gt; - Create professional podcasts&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/transformations.md"&gt;ğŸ”§ Content Transformations&lt;/a&gt;&lt;/strong&gt; - Customize content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/ai-models.md"&gt;ğŸ¤– AI Models&lt;/a&gt;&lt;/strong&gt; - AI model configuration&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/development/api-reference.md"&gt;ğŸ”§ REST API Reference&lt;/a&gt;&lt;/strong&gt; - Complete API documentation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/security.md"&gt;ğŸ” Security&lt;/a&gt;&lt;/strong&gt; - Password protection and privacy&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/index.md"&gt;ğŸš€ Deployment&lt;/a&gt;&lt;/strong&gt; - Complete deployment guides for all scenarios&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;ğŸ—ºï¸ Roadmap&lt;/h2&gt; 
&lt;h3&gt;Upcoming Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;React Frontend&lt;/strong&gt;: Modern React-based frontend to replace Streamlit&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Live Front-End Updates&lt;/strong&gt;: Real-time UI updates for smoother experience&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Async Processing&lt;/strong&gt;: Faster UI through asynchronous content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cross-Notebook Sources&lt;/strong&gt;: Reuse research materials across projects&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bookmark Integration&lt;/strong&gt;: Connect with your favorite bookmarking apps&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Recently Completed âœ…&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Comprehensive REST API&lt;/strong&gt;: Full programmatic access to all functionality&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Model Support&lt;/strong&gt;: 16+ AI providers including OpenAI, Anthropic, Ollama, LM Studio&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced Podcast Generator&lt;/strong&gt;: Professional multi-speaker podcasts with Episode Profiles&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Content Transformations&lt;/strong&gt;: Powerful customizable actions for content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced Citations&lt;/strong&gt;: Improved layout and finer control for source citations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multiple Chat Sessions&lt;/strong&gt;: Manage different conversations within notebooks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;open issues&lt;/a&gt; for a full list of proposed features and known issues.&lt;/p&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;ğŸ¤ Community &amp;amp; Contributing&lt;/h2&gt; 
&lt;h3&gt;Join the Community&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ’¬ &lt;strong&gt;&lt;a href="https://discord.gg/37XJPXfz2w"&gt;Discord Server&lt;/a&gt;&lt;/strong&gt; - Get help, share ideas, and connect with other users&lt;/li&gt; 
 &lt;li&gt;ğŸ› &lt;strong&gt;&lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/strong&gt; - Report bugs and request features&lt;/li&gt; 
 &lt;li&gt;â­ &lt;strong&gt;Star this repo&lt;/strong&gt; - Show your support and help others discover Open Notebook&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing&lt;/h3&gt; 
&lt;p&gt;We welcome contributions! We're especially looking for help with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Frontend Development&lt;/strong&gt;: Help build a modern React-based UI (planned replacement for current Streamlit interface)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Testing &amp;amp; Bug Fixes&lt;/strong&gt;: Make Open Notebook more robust&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Feature Development&lt;/strong&gt;: Build the coolest research tool together&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: Improve guides and tutorials&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Current Tech Stack&lt;/strong&gt;: Python, FastAPI, SurrealDB, Streamlit&lt;br /&gt; &lt;strong&gt;Future Roadmap&lt;/strong&gt;: React frontend, enhanced real-time updates&lt;/p&gt; 
&lt;p&gt;See our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; for detailed information on how to get started.&lt;/p&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; 
&lt;p&gt;Open Notebook is MIT licensed. See the &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;ğŸ“ Contact&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Luis Novo&lt;/strong&gt; - &lt;a href="https://twitter.com/lfnovo"&gt;@lfnovo&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Community Support&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ’¬ &lt;a href="https://discord.gg/37XJPXfz2w"&gt;Discord Server&lt;/a&gt; - Get help, share ideas, and connect with users&lt;/li&gt; 
 &lt;li&gt;ğŸ› &lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;GitHub Issues&lt;/a&gt; - Report bugs and request features&lt;/li&gt; 
 &lt;li&gt;ğŸŒ &lt;a href="https://www.open-notebook.ai"&gt;Website&lt;/a&gt; - Learn more about the project&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ™ Acknowledgments&lt;/h2&gt; 
&lt;p&gt;Open Notebook is built on the shoulders of amazing open-source projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/podcast-creator"&gt;Podcast Creator&lt;/a&gt;&lt;/strong&gt; - Advanced podcast generation capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/surreal-commands"&gt;Surreal Commands&lt;/a&gt;&lt;/strong&gt; - Background job processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/content-core"&gt;Content Core&lt;/a&gt;&lt;/strong&gt; - Content processing and management&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/esperanto"&gt;Esperanto&lt;/a&gt;&lt;/strong&gt; - Multi-provider AI model abstraction&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/docling-project/docling"&gt;Docling&lt;/a&gt;&lt;/strong&gt; - Document processing and parsing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; 
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</description>
    </item>
    
    <item>
      <title>stanfordnlp/dspy</title>
      <link>https://github.com/stanfordnlp/dspy</link>
      <description>&lt;p&gt;DSPy: The framework for programmingâ€”not promptingâ€”language models&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img align="center" src="https://raw.githubusercontent.com/stanfordnlp/dspy/main/docs/docs/static/img/dspy_logo.png" width="460px" /&gt; &lt;/p&gt; 
&lt;p align="left"&gt; &lt;/p&gt;
&lt;h2&gt;DSPy: &lt;em&gt;Programming&lt;/em&gt;â€”not promptingâ€”Foundation Models&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Documentation:&lt;/strong&gt; &lt;a href="https://dspy.ai/"&gt;DSPy Docs&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://pepy.tech/projects/dspy"&gt;&lt;img src="https://static.pepy.tech/badge/dspy/month" alt="PyPI Downloads" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;DSPy is the framework for &lt;em&gt;programmingâ€”rather than promptingâ€”language models&lt;/em&gt;. It allows you to iterate fast on &lt;strong&gt;building modular AI systems&lt;/strong&gt; and offers algorithms for &lt;strong&gt;optimizing their prompts and weights&lt;/strong&gt;, whether you're building simple classifiers, sophisticated RAG pipelines, or Agent loops.&lt;/p&gt; 
&lt;p&gt;DSPy stands for Declarative Self-improving Python. Instead of brittle prompts, you write compositional &lt;em&gt;Python code&lt;/em&gt; and use DSPy to &lt;strong&gt;teach your LM to deliver high-quality outputs&lt;/strong&gt;. Learn more via our &lt;a href="https://dspy.ai/"&gt;official documentation site&lt;/a&gt; or meet the community, seek help, or start contributing via this GitHub repo and our &lt;a href="https://discord.gg/XCGy2WDCQB"&gt;Discord server&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation: &lt;a href="https://dspy.ai"&gt;dspy.ai&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Please go to the &lt;a href="https://dspy.ai"&gt;DSPy Docs at dspy.ai&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install dspy
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To install the very latest from &lt;code&gt;main&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install git+https://github.com/stanfordnlp/dspy.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ“œ Citation &amp;amp; Reading More&lt;/h2&gt; 
&lt;p&gt;If you're looking to understand the framework, please go to the &lt;a href="https://dspy.ai"&gt;DSPy Docs at dspy.ai&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you're looking to understand the underlying research, this is a set of our papers:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;[Jul'25] &lt;a href="https://arxiv.org/abs/2507.19457"&gt;GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;[Jun'24] &lt;a href="https://arxiv.org/abs/2406.11695"&gt;Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;[Oct'23] &lt;a href="https://arxiv.org/abs/2310.03714"&gt;DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; [Jul'24] &lt;a href="https://arxiv.org/abs/2407.10930"&gt;Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together&lt;/a&gt;&lt;br /&gt; [Jun'24] &lt;a href="https://arxiv.org/abs/2406.11706"&gt;Prompts as Auto-Optimized Training Hyperparameters&lt;/a&gt;&lt;br /&gt; [Feb'24] &lt;a href="https://arxiv.org/abs/2402.14207"&gt;Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models&lt;/a&gt;&lt;br /&gt; [Jan'24] &lt;a href="https://arxiv.org/abs/2401.12178"&gt;In-Context Learning for Extreme Multi-Label Classification&lt;/a&gt;&lt;br /&gt; [Dec'23] &lt;a href="https://arxiv.org/abs/2312.13382"&gt;DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines&lt;/a&gt;&lt;br /&gt; [Dec'22] &lt;a href="https://arxiv.org/abs/2212.14024.pdf"&gt;Demonstrate-Search-Predict: Composing Retrieval &amp;amp; Language Models for Knowledge-Intensive NLP&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;To stay up to date or learn more, follow &lt;a href="https://twitter.com/lateinteraction"&gt;@lateinteraction&lt;/a&gt; on Twitter.&lt;/p&gt; 
&lt;p&gt;The &lt;strong&gt;DSPy&lt;/strong&gt; logo is designed by &lt;strong&gt;Chuyi Zhang&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;If you use DSPy or DSP in a research paper, please cite our work as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@inproceedings{khattab2024dspy,
  title={DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines},
  author={Khattab, Omar and Singhvi, Arnav and Maheshwari, Paridhi and Zhang, Zhiyuan and Santhanam, Keshav and Vardhamanan, Sri and Haq, Saiful and Sharma, Ashutosh and Joshi, Thomas T. and Moazam, Hanna and Miller, Heather and Zaharia, Matei and Potts, Christopher},
  journal={The Twelfth International Conference on Learning Representations},
  year={2024}
}
@article{khattab2022demonstrate,
  title={Demonstrate-Search-Predict: Composing Retrieval and Language Models for Knowledge-Intensive {NLP}},
  author={Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},
  journal={arXiv preprint arXiv:2212.14024},
  year={2022}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;!-- You can also read more about the evolution of the framework from Demonstrate-Search-Predict to DSPy:

* [**DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines**](https://arxiv.org/abs/2312.13382)   (Academic Paper, Dec 2023) 
* [**DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines**](https://arxiv.org/abs/2310.03714) (Academic Paper, Oct 2023) 
* [**Releasing DSPy, the latest iteration of the framework**](https://twitter.com/lateinteraction/status/1694748401374490946) (Twitter Thread, Aug 2023)
* [**Releasing the DSP Compiler (v0.1)**](https://twitter.com/lateinteraction/status/1625231662849073160)  (Twitter Thread, Feb 2023)
* [**Introducing DSP**](https://twitter.com/lateinteraction/status/1617953413576425472)  (Twitter Thread, Jan 2023)
* [**Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP**](https://arxiv.org/abs/2212.14024.pdf) (Academic Paper, Dec 2022) --&gt;</description>
    </item>
    
    <item>
      <title>openai/openai-python</title>
      <link>https://github.com/openai/openai-python</link>
      <description>&lt;p&gt;The official Python library for the OpenAI API&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenAI Python API library&lt;/h1&gt; 
&lt;!-- prettier-ignore --&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/openai/"&gt;&lt;img src="https://img.shields.io/pypi/v/openai.svg?label=pypi%20(stable)" alt="PyPI version" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.8+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by &lt;a href="https://github.com/encode/httpx"&gt;httpx&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;It is generated from our &lt;a href="https://github.com/openai/openai-openapi"&gt;OpenAPI specification&lt;/a&gt; with &lt;a href="https://stainlessapi.com/"&gt;Stainless&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;The REST API documentation can be found on &lt;a href="https://platform.openai.com/docs/api-reference"&gt;platform.openai.com&lt;/a&gt;. The full API of this library can be found in &lt;a href="https://raw.githubusercontent.com/openai/openai-python/main/api.md"&gt;api.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# install from PyPI
pip install openai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;The full API of this library can be found in &lt;a href="https://raw.githubusercontent.com/openai/openai-python/main/api.md"&gt;api.md&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The primary API for interacting with OpenAI models is the &lt;a href="https://platform.openai.com/docs/api-reference/responses"&gt;Responses API&lt;/a&gt;. You can generate text from the model with the code below.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
from openai import OpenAI

client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)

response = client.responses.create(
    model="gpt-4o",
    instructions="You are a coding assistant that talks like a pirate.",
    input="How do I check if a Python object is an instance of a class?",
)

print(response.output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The previous standard (supported indefinitely) for generating text is the &lt;a href="https://platform.openai.com/docs/api-reference/chat"&gt;Chat Completions API&lt;/a&gt;. You can use that API to generate text from the model with the code below.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "developer", "content": "Talk like a pirate."},
        {
            "role": "user",
            "content": "How do I check if a Python object is an instance of a class?",
        },
    ],
)

print(completion.choices[0].message.content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;While you can provide an &lt;code&gt;api_key&lt;/code&gt; keyword argument, we recommend using &lt;a href="https://pypi.org/project/python-dotenv/"&gt;python-dotenv&lt;/a&gt; to add &lt;code&gt;OPENAI_API_KEY="My API Key"&lt;/code&gt; to your &lt;code&gt;.env&lt;/code&gt; file so that your API key is not stored in source control. &lt;a href="https://platform.openai.com/settings/organization/api-keys"&gt;Get an API key here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Vision&lt;/h3&gt; 
&lt;p&gt;With an image URL:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;prompt = "What is in this image?"
img_url = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/2023_06_08_Raccoon1.jpg/1599px-2023_06_08_Raccoon1.jpg"

response = client.responses.create(
    model="gpt-4o-mini",
    input=[
        {
            "role": "user",
            "content": [
                {"type": "input_text", "text": prompt},
                {"type": "input_image", "image_url": f"{img_url}"},
            ],
        }
    ],
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With the image as a base64 encoded string:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import base64
from openai import OpenAI

client = OpenAI()

prompt = "What is in this image?"
with open("path/to/image.png", "rb") as image_file:
    b64_image = base64.b64encode(image_file.read()).decode("utf-8")

response = client.responses.create(
    model="gpt-4o-mini",
    input=[
        {
            "role": "user",
            "content": [
                {"type": "input_text", "text": prompt},
                {"type": "input_image", "image_url": f"data:image/png;base64,{b64_image}"},
            ],
        }
    ],
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Async usage&lt;/h2&gt; 
&lt;p&gt;Simply import &lt;code&gt;AsyncOpenAI&lt;/code&gt; instead of &lt;code&gt;OpenAI&lt;/code&gt; and use &lt;code&gt;await&lt;/code&gt; with each API call:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)


async def main() -&amp;gt; None:
    response = await client.responses.create(
        model="gpt-4o", input="Explain disestablishmentarianism to a smart five year old."
    )
    print(response.output_text)


asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Functionality between the synchronous and asynchronous clients is otherwise identical.&lt;/p&gt; 
&lt;h3&gt;With aiohttp&lt;/h3&gt; 
&lt;p&gt;By default, the async client uses &lt;code&gt;httpx&lt;/code&gt; for HTTP requests. However, for improved concurrency performance you may also use &lt;code&gt;aiohttp&lt;/code&gt; as the HTTP backend.&lt;/p&gt; 
&lt;p&gt;You can enable this by installing &lt;code&gt;aiohttp&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# install from PyPI
pip install openai[aiohttp]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you can enable it by instantiating the client with &lt;code&gt;http_client=DefaultAioHttpClient()&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from openai import DefaultAioHttpClient
from openai import AsyncOpenAI


async def main() -&amp;gt; None:
    async with AsyncOpenAI(
        api_key="My API Key",
        http_client=DefaultAioHttpClient(),
    ) as client:
        chat_completion = await client.chat.completions.create(
            messages=[
                {
                    "role": "user",
                    "content": "Say this is a test",
                }
            ],
            model="gpt-4o",
        )


asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Streaming responses&lt;/h2&gt; 
&lt;p&gt;We provide support for streaming responses using Server Side Events (SSE).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI

client = OpenAI()

stream = client.responses.create(
    model="gpt-4o",
    input="Write a one-sentence bedtime story about a unicorn.",
    stream=True,
)

for event in stream:
    print(event)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The async client uses the exact same interface.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI()


async def main():
    stream = await client.responses.create(
        model="gpt-4o",
        input="Write a one-sentence bedtime story about a unicorn.",
        stream=True,
    )

    async for event in stream:
        print(event)


asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Realtime API beta&lt;/h2&gt; 
&lt;p&gt;The Realtime API enables you to build low-latency, multi-modal conversational experiences. It currently supports text and audio as both input and output, as well as &lt;a href="https://platform.openai.com/docs/guides/function-calling"&gt;function calling&lt;/a&gt; through a WebSocket connection.&lt;/p&gt; 
&lt;p&gt;Under the hood the SDK uses the &lt;a href="https://websockets.readthedocs.io/en/stable/"&gt;&lt;code&gt;websockets&lt;/code&gt;&lt;/a&gt; library to manage connections.&lt;/p&gt; 
&lt;p&gt;The Realtime API works through a combination of client-sent events and server-sent events. Clients can send events to do things like update session configuration or send text and audio inputs. Server events confirm when audio responses have completed, or when a text response from the model has been received. A full event reference can be found &lt;a href="https://platform.openai.com/docs/api-reference/realtime-client-events"&gt;here&lt;/a&gt; and a guide can be found &lt;a href="https://platform.openai.com/docs/guides/realtime"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Basic text based example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import asyncio
from openai import AsyncOpenAI

async def main():
    client = AsyncOpenAI()

    async with client.beta.realtime.connect(model="gpt-4o-realtime-preview") as connection:
        await connection.session.update(session={'modalities': ['text']})

        await connection.conversation.item.create(
            item={
                "type": "message",
                "role": "user",
                "content": [{"type": "input_text", "text": "Say hello!"}],
            }
        )
        await connection.response.create()

        async for event in connection:
            if event.type == 'response.text.delta':
                print(event.delta, flush=True, end="")

            elif event.type == 'response.text.done':
                print()

            elif event.type == "response.done":
                break

asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;However the real magic of the Realtime API is handling audio inputs / outputs, see this example &lt;a href="https://github.com/openai/openai-python/raw/main/examples/realtime/push_to_talk_app.py"&gt;TUI script&lt;/a&gt; for a fully fledged example.&lt;/p&gt; 
&lt;h3&gt;Realtime error handling&lt;/h3&gt; 
&lt;p&gt;Whenever an error occurs, the Realtime API will send an &lt;a href="https://platform.openai.com/docs/guides/realtime-model-capabilities#error-handling"&gt;&lt;code&gt;error&lt;/code&gt; event&lt;/a&gt; and the connection will stay open and remain usable. This means you need to handle it yourself, as &lt;em&gt;no errors are raised directly&lt;/em&gt; by the SDK when an &lt;code&gt;error&lt;/code&gt; event comes in.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;client = AsyncOpenAI()

async with client.beta.realtime.connect(model="gpt-4o-realtime-preview") as connection:
    ...
    async for event in connection:
        if event.type == 'error':
            print(event.error.type)
            print(event.error.code)
            print(event.error.event_id)
            print(event.error.message)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Using types&lt;/h2&gt; 
&lt;p&gt;Nested request parameters are &lt;a href="https://docs.python.org/3/library/typing.html#typing.TypedDict"&gt;TypedDicts&lt;/a&gt;. Responses are &lt;a href="https://docs.pydantic.dev"&gt;Pydantic models&lt;/a&gt; which also provide helper methods for things like:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Serializing back into JSON, &lt;code&gt;model.to_json()&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Converting to a dictionary, &lt;code&gt;model.to_dict()&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set &lt;code&gt;python.analysis.typeCheckingMode&lt;/code&gt; to &lt;code&gt;basic&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Pagination&lt;/h2&gt; 
&lt;p&gt;List methods in the OpenAI API are paginated.&lt;/p&gt; 
&lt;p&gt;This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, asynchronously:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI()


async def main() -&amp;gt; None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can use the &lt;code&gt;.has_next_page()&lt;/code&gt;, &lt;code&gt;.next_page_info()&lt;/code&gt;, or &lt;code&gt;.get_next_page()&lt;/code&gt; methods for more granular control working with pages:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or just work directly with the returned data:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # =&amp;gt; "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Nested params&lt;/h2&gt; 
&lt;p&gt;Nested parameters are dictionaries, typed using &lt;code&gt;TypedDict&lt;/code&gt;, for example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI

client = OpenAI()

response = client.chat.responses.create(
    input=[
        {
            "role": "user",
            "content": "How much ?",
        }
    ],
    model="gpt-4o",
    response_format={"type": "json_object"},
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;File uploads&lt;/h2&gt; 
&lt;p&gt;Request parameters that correspond to file uploads can be passed as &lt;code&gt;bytes&lt;/code&gt;, or a &lt;a href="https://docs.python.org/3/library/os.html#os.PathLike"&gt;&lt;code&gt;PathLike&lt;/code&gt;&lt;/a&gt; instance or a tuple of &lt;code&gt;(filename, contents, media type)&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The async client uses the exact same interface. If you pass a &lt;a href="https://docs.python.org/3/library/os.html#os.PathLike"&gt;&lt;code&gt;PathLike&lt;/code&gt;&lt;/a&gt; instance, the file contents will be read asynchronously automatically.&lt;/p&gt; 
&lt;h2&gt;Webhook Verification&lt;/h2&gt; 
&lt;p&gt;Verifying webhook signatures is &lt;em&gt;optional but encouraged&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;For more information about webhooks, see &lt;a href="https://platform.openai.com/docs/guides/webhooks"&gt;the API docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Parsing webhook payloads&lt;/h3&gt; 
&lt;p&gt;For most use cases, you will likely want to verify the webhook and parse the payload at the same time. To achieve this, we provide the method &lt;code&gt;client.webhooks.unwrap()&lt;/code&gt;, which parses a webhook request and verifies that it was sent by OpenAI. This method will raise an error if the signature is invalid.&lt;/p&gt; 
&lt;p&gt;Note that the &lt;code&gt;body&lt;/code&gt; parameter must be the raw JSON string sent from the server (do not parse it first). The &lt;code&gt;.unwrap()&lt;/code&gt; method will parse this JSON for you into an event object after verifying the webhook was sent from OpenAI.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI
from flask import Flask, request

app = Flask(__name__)
client = OpenAI()  # OPENAI_WEBHOOK_SECRET environment variable is used by default


@app.route("/webhook", methods=["POST"])
def webhook():
    request_body = request.get_data(as_text=True)

    try:
        event = client.webhooks.unwrap(request_body, request.headers)

        if event.type == "response.completed":
            print("Response completed:", event.data)
        elif event.type == "response.failed":
            print("Response failed:", event.data)
        else:
            print("Unhandled event type:", event.type)

        return "ok"
    except Exception as e:
        print("Invalid signature:", e)
        return "Invalid signature", 400


if __name__ == "__main__":
    app.run(port=8000)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Verifying webhook payloads directly&lt;/h3&gt; 
&lt;p&gt;In some cases, you may want to verify the webhook separately from parsing the payload. If you prefer to handle these steps separately, we provide the method &lt;code&gt;client.webhooks.verify_signature()&lt;/code&gt; to &lt;em&gt;only verify&lt;/em&gt; the signature of a webhook request. Like &lt;code&gt;.unwrap()&lt;/code&gt;, this method will raise an error if the signature is invalid.&lt;/p&gt; 
&lt;p&gt;Note that the &lt;code&gt;body&lt;/code&gt; parameter must be the raw JSON string sent from the server (do not parse it first). You will then need to parse the body after verifying the signature.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import json
from openai import OpenAI
from flask import Flask, request

app = Flask(__name__)
client = OpenAI()  # OPENAI_WEBHOOK_SECRET environment variable is used by default


@app.route("/webhook", methods=["POST"])
def webhook():
    request_body = request.get_data(as_text=True)

    try:
        client.webhooks.verify_signature(request_body, request.headers)

        # Parse the body after verification
        event = json.loads(request_body)
        print("Verified event:", event)

        return "ok"
    except Exception as e:
        print("Invalid signature:", e)
        return "Invalid signature", 400


if __name__ == "__main__":
    app.run(port=8000)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Handling errors&lt;/h2&gt; 
&lt;p&gt;When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of &lt;code&gt;openai.APIConnectionError&lt;/code&gt; is raised.&lt;/p&gt; 
&lt;p&gt;When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of &lt;code&gt;openai.APIStatusError&lt;/code&gt; is raised, containing &lt;code&gt;status_code&lt;/code&gt; and &lt;code&gt;response&lt;/code&gt; properties.&lt;/p&gt; 
&lt;p&gt;All errors inherit from &lt;code&gt;openai.APIError&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tuning.jobs.create(
        model="gpt-4o",
        training_file="file-abc123",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Error codes are as follows:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Status Code&lt;/th&gt; 
   &lt;th&gt;Error Type&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;400&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;BadRequestError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;401&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;AuthenticationError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;403&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;PermissionDeniedError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;404&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;NotFoundError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;422&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;UnprocessableEntityError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;429&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;RateLimitError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&amp;gt;=500&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;InternalServerError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;APIConnectionError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Request IDs&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For more information on debugging requests, see &lt;a href="https://platform.openai.com/docs/api-reference/debugging-requests"&gt;these docs&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;All object responses in the SDK provide a &lt;code&gt;_request_id&lt;/code&gt; property which is added from the &lt;code&gt;x-request-id&lt;/code&gt; response header so that you can quickly log failing requests and report them back to OpenAI.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;response = await client.responses.create(
    model="gpt-4o-mini",
    input="Say 'this is a test'.",
)
print(response._request_id)  # req_123
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that unlike other properties that use an &lt;code&gt;_&lt;/code&gt; prefix, the &lt;code&gt;_request_id&lt;/code&gt; property &lt;em&gt;is&lt;/em&gt; public. Unless documented otherwise, &lt;em&gt;all&lt;/em&gt; other &lt;code&gt;_&lt;/code&gt; prefix properties, methods and modules are &lt;em&gt;private&lt;/em&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT]&lt;br /&gt; If you need to access request IDs for failed requests you must catch the &lt;code&gt;APIStatusError&lt;/code&gt; exception&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import openai

try:
    completion = await client.chat.completions.create(
        messages=[{"role": "user", "content": "Say this is a test"}], model="gpt-4"
    )
except openai.APIStatusError as exc:
    print(exc.request_id)  # req_123
    raise exc
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Retries&lt;/h2&gt; 
&lt;p&gt;Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and &amp;gt;=500 Internal errors are all retried by default.&lt;/p&gt; 
&lt;p&gt;You can use the &lt;code&gt;max_retries&lt;/code&gt; option to configure or disable retry settings:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in JavaScript?",
        }
    ],
    model="gpt-4o",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Timeouts&lt;/h2&gt; 
&lt;p&gt;By default requests time out after 10 minutes. You can configure this with a &lt;code&gt;timeout&lt;/code&gt; option, which accepts a float or an &lt;a href="https://www.python-httpx.org/advanced/timeouts/#fine-tuning-the-configuration"&gt;&lt;code&gt;httpx.Timeout&lt;/code&gt;&lt;/a&gt; object:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # 20 seconds (default is 10 minutes)
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5.0).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-4o",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;On timeout, an &lt;code&gt;APITimeoutError&lt;/code&gt; is thrown.&lt;/p&gt; 
&lt;p&gt;Note that requests that time out are &lt;a href="https://raw.githubusercontent.com/openai/openai-python/main/#retries"&gt;retried twice by default&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Advanced&lt;/h2&gt; 
&lt;h3&gt;Logging&lt;/h3&gt; 
&lt;p&gt;We use the standard library &lt;a href="https://docs.python.org/3/library/logging.html"&gt;&lt;code&gt;logging&lt;/code&gt;&lt;/a&gt; module.&lt;/p&gt; 
&lt;p&gt;You can enable logging by setting the environment variable &lt;code&gt;OPENAI_LOG&lt;/code&gt; to &lt;code&gt;info&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;$ export OPENAI_LOG=info
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or to &lt;code&gt;debug&lt;/code&gt; for more verbose logging.&lt;/p&gt; 
&lt;h3&gt;How to tell whether &lt;code&gt;None&lt;/code&gt; means &lt;code&gt;null&lt;/code&gt; or missing&lt;/h3&gt; 
&lt;p&gt;In an API response, a field may be explicitly &lt;code&gt;null&lt;/code&gt;, or missing entirely; in either case, its value is &lt;code&gt;None&lt;/code&gt; in this library. You can differentiate the two cases with &lt;code&gt;.model_fields_set&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Accessing raw response data (e.g. headers)&lt;/h3&gt; 
&lt;p&gt;The "raw" Response object can be accessed by prefixing &lt;code&gt;.with_raw_response.&lt;/code&gt; to any HTTP method call, e.g.,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-4o",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These methods return a &lt;a href="https://github.com/openai/openai-python/tree/main/src/openai/_legacy_response.py"&gt;&lt;code&gt;LegacyAPIResponse&lt;/code&gt;&lt;/a&gt; object. This is a legacy class as we're changing it slightly in the next major version.&lt;/p&gt; 
&lt;p&gt;For the sync client this will mostly be the same with the exception of &lt;code&gt;content&lt;/code&gt; &amp;amp; &lt;code&gt;text&lt;/code&gt; will be methods instead of properties. In the async client, all methods will be async.&lt;/p&gt; 
&lt;p&gt;A migration script will be provided &amp;amp; the migration in general should be smooth.&lt;/p&gt; 
&lt;h4&gt;&lt;code&gt;.with_streaming_response&lt;/code&gt;&lt;/h4&gt; 
&lt;p&gt;The above interface eagerly reads the full response body when you make the request, which may not always be what you want.&lt;/p&gt; 
&lt;p&gt;To stream the response body, use &lt;code&gt;.with_streaming_response&lt;/code&gt; instead, which requires a context manager and only reads the response body once you call &lt;code&gt;.read()&lt;/code&gt;, &lt;code&gt;.text()&lt;/code&gt;, &lt;code&gt;.json()&lt;/code&gt;, &lt;code&gt;.iter_bytes()&lt;/code&gt;, &lt;code&gt;.iter_text()&lt;/code&gt;, &lt;code&gt;.iter_lines()&lt;/code&gt; or &lt;code&gt;.parse()&lt;/code&gt;. In the async client, these are async methods.&lt;/p&gt; 
&lt;p&gt;As such, &lt;code&gt;.with_streaming_response&lt;/code&gt; methods return a different &lt;a href="https://github.com/openai/openai-python/tree/main/src/openai/_response.py"&gt;&lt;code&gt;APIResponse&lt;/code&gt;&lt;/a&gt; object, and the async client returns an &lt;a href="https://github.com/openai/openai-python/tree/main/src/openai/_response.py"&gt;&lt;code&gt;AsyncAPIResponse&lt;/code&gt;&lt;/a&gt; object.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;with client.chat.completions.with_streaming_response.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-4o",
) as response:
    print(response.headers.get("X-My-Header"))

    for line in response.iter_lines():
        print(line)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The context manager is required so that the response will reliably be closed.&lt;/p&gt; 
&lt;h3&gt;Making custom/undocumented requests&lt;/h3&gt; 
&lt;p&gt;This library is typed for convenient access to the documented API.&lt;/p&gt; 
&lt;p&gt;If you need to access undocumented endpoints, params, or response properties, the library can still be used.&lt;/p&gt; 
&lt;h4&gt;Undocumented endpoints&lt;/h4&gt; 
&lt;p&gt;To make requests to undocumented endpoints, you can make requests using &lt;code&gt;client.get&lt;/code&gt;, &lt;code&gt;client.post&lt;/code&gt;, and other http verbs. Options on the client will be respected (such as retries) when making this request.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import httpx

response = client.post(
    "/foo",
    cast_to=httpx.Response,
    body={"my_param": True},
)

print(response.headers.get("x-foo"))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Undocumented request params&lt;/h4&gt; 
&lt;p&gt;If you want to explicitly send an extra param, you can do so with the &lt;code&gt;extra_query&lt;/code&gt;, &lt;code&gt;extra_body&lt;/code&gt;, and &lt;code&gt;extra_headers&lt;/code&gt; request options.&lt;/p&gt; 
&lt;h4&gt;Undocumented response properties&lt;/h4&gt; 
&lt;p&gt;To access undocumented response properties, you can access the extra fields like &lt;code&gt;response.unknown_prop&lt;/code&gt;. You can also get all the extra fields on the Pydantic model as a dict with &lt;a href="https://docs.pydantic.dev/latest/api/base_model/#pydantic.BaseModel.model_extra"&gt;&lt;code&gt;response.model_extra&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Configuring the HTTP client&lt;/h3&gt; 
&lt;p&gt;You can directly override the &lt;a href="https://www.python-httpx.org/api/#client"&gt;httpx client&lt;/a&gt; to customize it for your use case, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Support for &lt;a href="https://www.python-httpx.org/advanced/proxies/"&gt;proxies&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Custom &lt;a href="https://www.python-httpx.org/advanced/transports/"&gt;transports&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Additional &lt;a href="https://www.python-httpx.org/advanced/clients/"&gt;advanced&lt;/a&gt; functionality&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import httpx
from openai import OpenAI, DefaultHttpxClient

client = OpenAI(
    # Or use the `OPENAI_BASE_URL` env var
    base_url="http://my.test.server.example.com:8083/v1",
    http_client=DefaultHttpxClient(
        proxy="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also customize the client on a per-request basis by using &lt;code&gt;with_options()&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;client.with_options(http_client=DefaultHttpxClient(...))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Managing HTTP resources&lt;/h3&gt; 
&lt;p&gt;By default the library closes underlying HTTP connections whenever the client is &lt;a href="https://docs.python.org/3/reference/datamodel.html#object.__del__"&gt;garbage collected&lt;/a&gt;. You can manually close the client using the &lt;code&gt;.close()&lt;/code&gt; method if desired, or with a context manager that closes when exiting.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;from openai import OpenAI

with OpenAI() as client:
  # make requests here
  ...

# HTTP client is now closed
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Microsoft Azure OpenAI&lt;/h2&gt; 
&lt;p&gt;To use this library with &lt;a href="https://learn.microsoft.com/azure/ai-services/openai/overview"&gt;Azure OpenAI&lt;/a&gt;, use the &lt;code&gt;AzureOpenAI&lt;/code&gt; class instead of the &lt;code&gt;OpenAI&lt;/code&gt; class.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview",
    # https://learn.microsoft.com/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.to_json())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In addition to the options provided in the base &lt;code&gt;OpenAI&lt;/code&gt; client, the following options are provided:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;azure_endpoint&lt;/code&gt; (or the &lt;code&gt;AZURE_OPENAI_ENDPOINT&lt;/code&gt; environment variable)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;azure_deployment&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;api_version&lt;/code&gt; (or the &lt;code&gt;OPENAI_API_VERSION&lt;/code&gt; environment variable)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;azure_ad_token&lt;/code&gt; (or the &lt;code&gt;AZURE_OPENAI_AD_TOKEN&lt;/code&gt; environment variable)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;azure_ad_token_provider&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;An example of using the client with Microsoft Entra ID (formerly known as Azure Active Directory) can be found &lt;a href="https://github.com/openai/openai-python/raw/main/examples/azure_ad.py"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Versioning&lt;/h2&gt; 
&lt;p&gt;This package generally follows &lt;a href="https://semver.org/spec/v2.0.0.html"&gt;SemVer&lt;/a&gt; conventions, though certain backwards-incompatible changes may be released as minor versions:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Changes that only affect static types, without breaking runtime behavior.&lt;/li&gt; 
 &lt;li&gt;Changes to library internals which are technically public but not intended or documented for external use. &lt;em&gt;(Please open a GitHub issue to let us know if you are relying on such internals.)&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Changes that we do not expect to impact the vast majority of users in practice.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.&lt;/p&gt; 
&lt;p&gt;We are keen for your feedback; please open an &lt;a href="https://www.github.com/openai/openai-python/issues"&gt;issue&lt;/a&gt; with questions, bugs, or suggestions.&lt;/p&gt; 
&lt;h3&gt;Determining the installed version&lt;/h3&gt; 
&lt;p&gt;If you've upgraded to the latest version but aren't seeing any new features you were expecting then your python environment is likely still using an older version.&lt;/p&gt; 
&lt;p&gt;You can determine the version that is being used at runtime with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import openai
print(openai.__version__)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;Python 3.8 or higher.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/openai/openai-python/main/CONTRIBUTING.md"&gt;the contributing documentation&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>tadata-org/fastapi_mcp</title>
      <link>https://github.com/tadata-org/fastapi_mcp</link>
      <description>&lt;p&gt;Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt;&lt;a href="https://github.com/tadata-org/fastapi_mcp"&gt;&lt;img src="https://github.com/user-attachments/assets/7e44e98b-a0ba-4aff-a68a-4ffee3a6189c" alt="fastapi-to-mcp" height="100/" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;span style="font-size: 0.85em; font-weight: normal;"&gt;Built by &lt;a href="https://tadata.com"&gt;Tadata&lt;/a&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;h1 align="center"&gt; FastAPI-MCP &lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/14064" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14064" alt="tadata-org%2Ffastapi_mcp | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt;Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://pypi.org/project/fastapi-mcp/"&gt;&lt;img src="https://img.shields.io/pypi/v/fastapi-mcp?color=%2334D058&amp;amp;label=pypi%20package" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/fastapi-mcp/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/fastapi-mcp.svg?sanitize=true" alt="Python Versions" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/#"&gt;&lt;img src="https://img.shields.io/badge/FastAPI-009485.svg?logo=fastapi&amp;amp;logoColor=white" alt="FastAPI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/tadata-org/fastapi_mcp"&gt;&lt;img src="https://codecov.io/gh/tadata-org/fastapi_mcp/branch/main/graph/badge.svg?sanitize=true" alt="Coverage" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt;&lt;a href="https://github.com/tadata-org/fastapi_mcp"&gt;&lt;img src="https://github.com/user-attachments/assets/b205adc6-28c0-4e3c-a68b-9c1a80eb7d0c" alt="fastapi-mcp-usage" height="400" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Authentication&lt;/strong&gt; built in, using your existing FastAPI dependencies!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;FastAPI-native:&lt;/strong&gt; Not just another OpenAPI -&amp;gt; MCP converter&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zero/Minimal configuration&lt;/strong&gt; required - just point it at your FastAPI app and it works&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Preserving schemas&lt;/strong&gt; of your request models and response models&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Preserve documentation&lt;/strong&gt; of all your endpoints, just as it is in Swagger&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Flexible deployment&lt;/strong&gt; - Mount your MCP server to the same app, or deploy separately&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ASGI transport&lt;/strong&gt; - Uses FastAPI's ASGI interface directly for efficient communication&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Hosted Solution&lt;/h2&gt; 
&lt;p&gt;If you prefer a managed hosted solution check out &lt;a href="https://tadata.com"&gt;tadata.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;We recommend using &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;, a fast Python package installer:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add fastapi-mcp
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can install with pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install fastapi-mcp
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Basic Usage&lt;/h2&gt; 
&lt;p&gt;The simplest way to use FastAPI-MCP is to add an MCP server directly to your FastAPI application:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from fastapi import FastAPI
from fastapi_mcp import FastApiMCP

app = FastAPI()

mcp = FastApiMCP(app)

# Mount the MCP server directly to your FastAPI app
mcp.mount()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That's it! Your auto-generated MCP server is now available at &lt;code&gt;https://app.base.url/mcp&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation, Examples and Advanced Usage&lt;/h2&gt; 
&lt;p&gt;FastAPI-MCP provides &lt;a href="https://fastapi-mcp.tadata.com/"&gt;comprehensive documentation&lt;/a&gt;. Additionaly, check out the &lt;a href="https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/examples"&gt;examples directory&lt;/a&gt; for code samples demonstrating these features in action.&lt;/p&gt; 
&lt;h2&gt;FastAPI-first Approach&lt;/h2&gt; 
&lt;p&gt;FastAPI-MCP is designed as a native extension of FastAPI, not just a converter that generates MCP tools from your API. This approach offers several key advantages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Native dependencies&lt;/strong&gt;: Secure your MCP endpoints using familiar FastAPI &lt;code&gt;Depends()&lt;/code&gt; for authentication and authorization&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ASGI transport&lt;/strong&gt;: Communicates directly with your FastAPI app using its ASGI interface, eliminating the need for HTTP calls from the MCP to your API&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unified infrastructure&lt;/strong&gt;: Your FastAPI app doesn't need to run separately from the MCP server (though &lt;a href="https://fastapi-mcp.tadata.com/advanced/deploy#deploying-separately-from-original-fastapi-app"&gt;separate deployment&lt;/a&gt; is also supported)&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This design philosophy ensures minimum friction when adding MCP capabilities to your existing FastAPI services.&lt;/p&gt; 
&lt;h2&gt;Development and Contributing&lt;/h2&gt; 
&lt;p&gt;Thank you for considering contributing to FastAPI-MCP! We encourage the community to post Issues and create Pull Requests.&lt;/p&gt; 
&lt;p&gt;Before you get started, please see our &lt;a href="https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/CONTRIBUTING.md"&gt;Contribution Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Join &lt;a href="https://join.slack.com/t/themcparty/shared_invite/zt-30yxr1zdi-2FG~XjBA0xIgYSYuKe7~Xg"&gt;MCParty Slack community&lt;/a&gt; to connect with other MCP enthusiasts, ask questions, and share your experiences with FastAPI-MCP.&lt;/p&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+ (Recommended 3.12)&lt;/li&gt; 
 &lt;li&gt;uv&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT License. Copyright (c) 2025 Tadata Inc.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>lipku/LiveTalking</title>
      <link>https://github.com/lipku/LiveTalking</link>
      <description>&lt;p&gt;Real time interactive streaming digital human&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/lipku/LiveTalking/main/README-EN.md"&gt;English&lt;/a&gt; | ä¸­æ–‡ç‰ˆ&lt;br /&gt; å®æ—¶äº¤äº’æµå¼æ•°å­—äººï¼Œå®ç°éŸ³è§†é¢‘åŒæ­¥å¯¹è¯ã€‚åŸºæœ¬å¯ä»¥è¾¾åˆ°å•†ç”¨æ•ˆæœ &lt;a href="https://www.bilibili.com/video/BV1scwBeyELA/"&gt;wav2lipæ•ˆæœ&lt;/a&gt; | &lt;a href="https://www.bilibili.com/video/BV1G1421z73r/"&gt;ernerfæ•ˆæœ&lt;/a&gt; | &lt;a href="https://www.bilibili.com/video/BV1gm421N7vQ/"&gt;musetalkæ•ˆæœ&lt;/a&gt;&lt;br /&gt; å›½å†…é•œåƒåœ°å€:&lt;a href="https://gitee.com/lipku/LiveTalking"&gt;https://gitee.com/lipku/LiveTalking&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ä¸ºé¿å…ä¸3dæ•°å­—äººæ··æ·†ï¼ŒåŸé¡¹ç›®metahuman-streamæ”¹åä¸ºlivetalkingï¼ŒåŸæœ‰é“¾æ¥åœ°å€ç»§ç»­å¯ç”¨&lt;/h2&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;2024.12.8 å®Œå–„å¤šå¹¶å‘ï¼Œæ˜¾å­˜ä¸éšå¹¶å‘æ•°å¢åŠ &lt;/li&gt; 
 &lt;li&gt;2024.12.21 æ·»åŠ wav2lipã€musetalkæ¨¡å‹é¢„çƒ­ï¼Œè§£å†³ç¬¬ä¸€æ¬¡æ¨ç†å¡é¡¿é—®é¢˜ã€‚æ„Ÿè°¢&lt;a href="https://github.com/heimaojinzhangyz"&gt;@heimaojinzhangyz&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;2024.12.28 æ·»åŠ æ•°å­—äººæ¨¡å‹Ultralight-Digital-Humanã€‚ æ„Ÿè°¢&lt;a href="https://github.com/lijihua2017"&gt;@lijihua2017&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;2025.2.7 æ·»åŠ fish-speech tts&lt;/li&gt; 
 &lt;li&gt;2025.2.21 æ·»åŠ wav2lip256å¼€æºæ¨¡å‹ æ„Ÿè°¢@ä¸è ¢ä¸è ¢&lt;/li&gt; 
 &lt;li&gt;2025.3.2 æ·»åŠ è…¾è®¯è¯­éŸ³åˆæˆæœåŠ¡&lt;/li&gt; 
 &lt;li&gt;2025.3.16 æ”¯æŒmac gpuæ¨ç†ï¼Œæ„Ÿè°¢&lt;a href="https://github.com/GcsSloop"&gt;@GcsSloop&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;2025.5.1 ç²¾ç®€è¿è¡Œå‚æ•°ï¼Œernerfæ¨¡å‹ç§»è‡³gitåˆ†æ”¯ernerf-rtmp&lt;/li&gt; 
 &lt;li&gt;2025.6.7 æ·»åŠ è™šæ‹Ÿæ‘„åƒå¤´è¾“å‡º&lt;/li&gt; 
 &lt;li&gt;2025.7.5 æ·»åŠ è±†åŒ…è¯­éŸ³åˆæˆ, æ„Ÿè°¢&lt;a href="https://github.com/ELK-milu"&gt;@ELK-milu&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;2025.7.26 æ”¯æŒmusetalk v1.5ç‰ˆæœ¬&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;æ”¯æŒå¤šç§æ•°å­—äººæ¨¡å‹: ernerfã€musetalkã€wav2lipã€Ultralight-Digital-Human&lt;/li&gt; 
 &lt;li&gt;æ”¯æŒå£°éŸ³å…‹éš†&lt;/li&gt; 
 &lt;li&gt;æ”¯æŒæ•°å­—äººè¯´è¯è¢«æ‰“æ–­&lt;/li&gt; 
 &lt;li&gt;æ”¯æŒå…¨èº«è§†é¢‘æ‹¼æ¥&lt;/li&gt; 
 &lt;li&gt;æ”¯æŒwebrtcã€è™šæ‹Ÿæ‘„åƒå¤´è¾“å‡º&lt;/li&gt; 
 &lt;li&gt;æ”¯æŒåŠ¨ä½œç¼–æ’ï¼šä¸è¯´è¯æ—¶æ’­æ”¾è‡ªå®šä¹‰è§†é¢‘&lt;/li&gt; 
 &lt;li&gt;æ”¯æŒå¤šå¹¶å‘&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;1. Installation&lt;/h2&gt; 
&lt;p&gt;Tested on Ubuntu 24.04, Python3.10, Pytorch 2.5.0 and CUDA 12.4&lt;/p&gt; 
&lt;h3&gt;1.1 Install dependency&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n nerfstream python=3.10
conda activate nerfstream
#å¦‚æœcudaç‰ˆæœ¬ä¸ä¸º12.4(è¿è¡Œnvidia-smiç¡®è®¤ç‰ˆæœ¬)ï¼Œæ ¹æ®&amp;lt;https://pytorch.org/get-started/previous-versions/&amp;gt;å®‰è£…å¯¹åº”ç‰ˆæœ¬çš„pytorch 
conda install pytorch==2.5.0 torchvision==0.20.0 torchaudio==2.5.0 pytorch-cuda=12.4 -c pytorch -c nvidia
pip install -r requirements.txt
#å¦‚æœéœ€è¦è®­ç»ƒernerfæ¨¡å‹ï¼Œå®‰è£…ä¸‹é¢çš„åº“
# pip install "git+https://github.com/facebookresearch/pytorch3d.git"
# pip install tensorflow-gpu==2.8.0
# pip install --upgrade "protobuf&amp;lt;=3.20.1"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;å®‰è£…å¸¸è§é—®é¢˜&lt;a href="https://livetalking-doc.readthedocs.io/zh-cn/latest/faq.html"&gt;FAQ&lt;/a&gt;&lt;br /&gt; linux cudaç¯å¢ƒæ­å»ºå¯ä»¥å‚è€ƒè¿™ç¯‡æ–‡ç«  &lt;a href="https://zhuanlan.zhihu.com/p/674972886"&gt;https://zhuanlan.zhihu.com/p/674972886&lt;/a&gt;&lt;br /&gt; è§†é¢‘è¿ä¸ä¸Šè§£å†³æ–¹æ³• &lt;a href="https://mp.weixin.qq.com/s/MVUkxxhV2cgMMHalphr2cg"&gt;https://mp.weixin.qq.com/s/MVUkxxhV2cgMMHalphr2cg&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;2. Quick Start&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;ä¸‹è½½æ¨¡å‹&lt;br /&gt; å¤¸å…‹äº‘ç›˜&lt;a href="https://pan.quark.cn/s/83a750323ef0"&gt;https://pan.quark.cn/s/83a750323ef0&lt;/a&gt;&lt;br /&gt; GoogleDriver &lt;a href="https://drive.google.com/drive/folders/1FOC_MD6wdogyyX_7V1d4NDIO7P9NlSAJ?usp=sharing"&gt;https://drive.google.com/drive/folders/1FOC_MD6wdogyyX_7V1d4NDIO7P9NlSAJ?usp=sharing&lt;/a&gt;&lt;br /&gt; å°†wav2lip256.pthæ‹·åˆ°æœ¬é¡¹ç›®çš„modelsä¸‹, é‡å‘½åä¸ºwav2lip.pth;&lt;br /&gt; å°†wav2lip256_avatar1.tar.gzè§£å‹åæ•´ä¸ªæ–‡ä»¶å¤¹æ‹·åˆ°æœ¬é¡¹ç›®çš„data/avatarsä¸‹&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;è¿è¡Œ&lt;br /&gt; python app.py --transport webrtc --model wav2lip --avatar_id wav2lip256_avatar1&lt;br /&gt; &lt;font color="red"&gt;æœåŠ¡ç«¯éœ€è¦å¼€æ”¾ç«¯å£ tcp:8010; udp:1-65536 &lt;/font&gt;&lt;br /&gt; å®¢æˆ·ç«¯å¯ä»¥é€‰ç”¨ä»¥ä¸‹ä¸¤ç§æ–¹å¼:&lt;br /&gt; (1)ç”¨æµè§ˆå™¨æ‰“å¼€&lt;a href="http://serverip:8010/webrtcapi.html"&gt;http://serverip:8010/webrtcapi.html&lt;/a&gt; , å…ˆç‚¹â€˜start',æ’­æ”¾æ•°å­—äººè§†é¢‘ï¼›ç„¶ååœ¨æ–‡æœ¬æ¡†è¾“å…¥ä»»æ„æ–‡å­—ï¼Œæäº¤ã€‚æ•°å­—äººæ’­æŠ¥è¯¥æ®µæ–‡å­—&lt;br /&gt; (2)ç”¨å®¢æˆ·ç«¯æ–¹å¼, ä¸‹è½½åœ°å€&lt;a href="https://pan.quark.cn/s/d7192d8ac19b"&gt;https://pan.quark.cn/s/d7192d8ac19b&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;å¿«é€Ÿä½“éªŒ&lt;br /&gt; &lt;a href="https://www.compshare.cn/images/4458094e-a43d-45fe-9b57-de79253befe4?referral_code=3XW3852OBmnD089hMMrtuU&amp;amp;ytag=GPU_GitHub_livetalking"&gt;https://www.compshare.cn/images/4458094e-a43d-45fe-9b57-de79253befe4?referral_code=3XW3852OBmnD089hMMrtuU&amp;amp;ytag=GPU_GitHub_livetalking&lt;/a&gt; ç”¨è¯¥é•œåƒåˆ›å»ºå®ä¾‹å³å¯è¿è¡ŒæˆåŠŸ&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;å¦‚æœè®¿é—®ä¸äº†huggingfaceï¼Œåœ¨è¿è¡Œå‰&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;export HF_ENDPOINT=https://hf-mirror.com
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;3. More Usage&lt;/h2&gt; 
&lt;p&gt;ä½¿ç”¨è¯´æ˜: &lt;a href="https://livetalking-doc.readthedocs.io/"&gt;https://livetalking-doc.readthedocs.io/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;4. Docker Run&lt;/h2&gt; 
&lt;p&gt;ä¸éœ€è¦å‰é¢çš„å®‰è£…ï¼Œç›´æ¥è¿è¡Œã€‚&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;docker run --gpus all -it --network=host --rm registry.cn-zhangjiakou.aliyuncs.com/codewithgpu3/lipku-livetalking:toza2irpHZ
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ä»£ç åœ¨/root/livetalkingï¼Œå…ˆgit pullæ‹‰ä¸€ä¸‹æœ€æ–°ä»£ç ï¼Œç„¶åæ‰§è¡Œå‘½ä»¤åŒç¬¬2ã€3æ­¥&lt;/p&gt; 
&lt;p&gt;æä¾›å¦‚ä¸‹é•œåƒ&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;autodlé•œåƒ: &lt;a href="https://www.codewithgpu.com/i/lipku/livetalking/base"&gt;https://www.codewithgpu.com/i/lipku/livetalking/base&lt;/a&gt;&lt;br /&gt; &lt;a href="https://livetalking-doc.readthedocs.io/en/latest/autodl/README.html"&gt;autodlæ•™ç¨‹&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ucloudé•œåƒ: &lt;a href="https://www.compshare.cn/images/4458094e-a43d-45fe-9b57-de79253befe4?referral_code=3XW3852OBmnD089hMMrtuU&amp;amp;ytag=GPU_GitHub_livetalking"&gt;https://www.compshare.cn/images/4458094e-a43d-45fe-9b57-de79253befe4?referral_code=3XW3852OBmnD089hMMrtuU&amp;amp;ytag=GPU_GitHub_livetalking&lt;/a&gt;&lt;br /&gt; å¯ä»¥å¼€æ”¾ä»»æ„ç«¯å£ï¼Œä¸éœ€è¦å¦å¤–éƒ¨ç½²srsæœåŠ¡.&lt;br /&gt; &lt;a href="https://livetalking-doc.readthedocs.io/en/latest/ucloud/ucloud.html"&gt;ucloudæ•™ç¨‹&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;5. æ€§èƒ½&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;æ€§èƒ½ä¸»è¦è·Ÿcpuå’Œgpuç›¸å…³ï¼Œæ¯è·¯è§†é¢‘å‹ç¼©éœ€è¦æ¶ˆè€—cpuï¼Œcpuæ€§èƒ½ä¸è§†é¢‘åˆ†è¾¨ç‡æ­£ç›¸å…³ï¼›æ¯è·¯å£å‹æ¨ç†è·Ÿgpuæ€§èƒ½ç›¸å…³ã€‚&lt;/li&gt; 
 &lt;li&gt;ä¸è¯´è¯æ—¶çš„å¹¶å‘æ•°è·Ÿcpuç›¸å…³ï¼ŒåŒæ—¶è¯´è¯çš„å¹¶å‘æ•°è·Ÿgpuç›¸å…³ã€‚&lt;/li&gt; 
 &lt;li&gt;åç«¯æ—¥å¿—inferfpsè¡¨ç¤ºæ˜¾å¡æ¨ç†å¸§ç‡ï¼Œfinalfpsè¡¨ç¤ºæœ€ç»ˆæ¨æµå¸§ç‡ã€‚ä¸¤è€…éƒ½è¦åœ¨25ä»¥ä¸Šæ‰èƒ½å®æ—¶ã€‚å¦‚æœinferfpsåœ¨25ä»¥ä¸Šï¼Œfinalfpsè¾¾ä¸åˆ°25è¡¨ç¤ºcpuæ€§èƒ½ä¸è¶³ã€‚&lt;/li&gt; 
 &lt;li&gt;å®æ—¶æ¨ç†æ€§èƒ½&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;æ¨¡å‹&lt;/th&gt; 
   &lt;th align="left"&gt;æ˜¾å¡å‹å·&lt;/th&gt; 
   &lt;th align="left"&gt;fps&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;wav2lip256&lt;/td&gt; 
   &lt;td align="left"&gt;3060&lt;/td&gt; 
   &lt;td align="left"&gt;60&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;wav2lip256&lt;/td&gt; 
   &lt;td align="left"&gt;3080Ti&lt;/td&gt; 
   &lt;td align="left"&gt;120&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;musetalk&lt;/td&gt; 
   &lt;td align="left"&gt;3080Ti&lt;/td&gt; 
   &lt;td align="left"&gt;42&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;musetalk&lt;/td&gt; 
   &lt;td align="left"&gt;3090&lt;/td&gt; 
   &lt;td align="left"&gt;45&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;musetalk&lt;/td&gt; 
   &lt;td align="left"&gt;4090&lt;/td&gt; 
   &lt;td align="left"&gt;72&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;wav2lip256æ˜¾å¡3060ä»¥ä¸Šå³å¯ï¼Œmusetalkéœ€è¦3080Tiä»¥ä¸Šã€‚&lt;/p&gt; 
&lt;h2&gt;6. å•†ä¸šç‰ˆ&lt;/h2&gt; 
&lt;p&gt;æä¾›å¦‚ä¸‹æ‰©å±•åŠŸèƒ½ï¼Œé€‚ç”¨äºå¯¹å¼€æºé¡¹ç›®å·²ç»æ¯”è¾ƒç†Ÿæ‚‰ï¼Œéœ€è¦æ‰©å±•äº§å“åŠŸèƒ½çš„ç”¨æˆ·&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;é«˜æ¸…wav2lipæ¨¡å‹&lt;/li&gt; 
 &lt;li&gt;å®Œå…¨è¯­éŸ³äº¤äº’ï¼Œæ•°å­—äººå›ç­”è¿‡ç¨‹ä¸­æ”¯æŒé€šè¿‡å”¤é†’è¯æˆ–è€…æŒ‰é’®æ‰“æ–­æé—®&lt;/li&gt; 
 &lt;li&gt;å®æ—¶åŒæ­¥å­—å¹•ï¼Œç»™å‰ç«¯æä¾›æ•°å­—äººæ¯å¥è¯æ’­æŠ¥å¼€å§‹ã€ç»“æŸäº‹ä»¶&lt;/li&gt; 
 &lt;li&gt;æ¯ä¸ªè¿æ¥å¯ä»¥æŒ‡å®šå¯¹åº”avatarå’ŒéŸ³è‰²ï¼Œavatarå›¾ç‰‡åŠ è½½åŠ é€Ÿ&lt;/li&gt; 
 &lt;li&gt;åŠ¨ä½œç¼–æ’ï¼šä¸è¯´è¯æ—¶åŠ¨ä½œã€å”¤é†’æ—¶åŠ¨ä½œã€æ€è€ƒæ—¶åŠ¨ä½œ&lt;/li&gt; 
 &lt;li&gt;æ”¯æŒä¸é™æ—¶é•¿çš„æ•°å­—äººå½¢è±¡avatar&lt;/li&gt; 
 &lt;li&gt;æä¾›å®æ—¶éŸ³é¢‘æµè¾“å…¥æ¥å£&lt;/li&gt; 
 &lt;li&gt;æ•°å­—äººé€æ˜èƒŒæ™¯ï¼Œå åŠ åŠ¨æ€èƒŒæ™¯&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;æ›´å¤šè¯¦æƒ…&lt;a href="https://livetalking-doc.readthedocs.io/zh-cn/latest/service.html#wav2lip"&gt;https://livetalking-doc.readthedocs.io/zh-cn/latest/service.html#wav2lip&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;7. å£°æ˜&lt;/h2&gt; 
&lt;p&gt;åŸºäºæœ¬é¡¹ç›®å¼€å‘å¹¶å‘å¸ƒåœ¨Bç«™ã€è§†é¢‘å·ã€æŠ–éŸ³ç­‰ç½‘ç«™ä¸Šçš„è§†é¢‘éœ€å¸¦ä¸ŠLiveTalkingæ°´å°å’Œæ ‡è¯†ï¼Œå¦‚éœ€å»é™¤è¯·è”ç³»ä½œè€…å¤‡æ¡ˆæˆæƒã€‚&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;å¦‚æœæœ¬é¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œå¸®å¿™ç‚¹ä¸ªstarã€‚ä¹Ÿæ¬¢è¿æ„Ÿå…´è¶£çš„æœ‹å‹ä¸€èµ·æ¥å®Œå–„è¯¥é¡¹ç›®.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;çŸ¥è¯†æ˜Ÿçƒ: &lt;a href="https://t.zsxq.com/7NMyO"&gt;https://t.zsxq.com/7NMyO&lt;/a&gt; æ²‰æ·€é«˜è´¨é‡å¸¸è§é—®é¢˜ã€æœ€ä½³å®è·µç»éªŒã€é—®é¢˜è§£ç­”&lt;/li&gt; 
 &lt;li&gt;å¾®ä¿¡å…¬ä¼—å·ï¼šæ•°å­—äººæŠ€æœ¯&lt;br /&gt; &lt;img src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/l3ZibgueFiaeyfaiaLZGuMGQXnhLWxibpJUS2gfs8Dje6JuMY8zu2tVyU9n8Zx1yaNncvKHBMibX0ocehoITy5qQEZg/640?wxfrom=12&amp;amp;tp=wxpic&amp;amp;usePicPrefetch=1&amp;amp;wx_fmt=jpeg&amp;amp;from=appmsg" alt="" /&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>instaloader/instaloader</title>
      <link>https://github.com/instaloader/instaloader</link>
      <description>&lt;p&gt;Download pictures (or videos) along with their captions and other metadata from Instagram.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;.. image:: &lt;a href="https://raw.githubusercontent.com/instaloader/instaloader/master/docs/logo_heading.png"&gt;https://raw.githubusercontent.com/instaloader/instaloader/master/docs/logo_heading.png&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. badges-start&lt;/p&gt; 
&lt;p&gt;|pypi| |pyversion| |license| |aur| |contributors| |downloads|&lt;/p&gt; 
&lt;p&gt;.. |pypi| image:: &lt;a href="https://img.shields.io/pypi/v/instaloader.svg"&gt;https://img.shields.io/pypi/v/instaloader.svg&lt;/a&gt; :alt: Instaloader PyPI Project Page :target: &lt;a href="https://pypi.org/project/instaloader/"&gt;https://pypi.org/project/instaloader/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. |license| image:: &lt;a href="https://img.shields.io/github/license/instaloader/instaloader.svg"&gt;https://img.shields.io/github/license/instaloader/instaloader.svg&lt;/a&gt; :alt: MIT License :target: &lt;a href="https://github.com/instaloader/instaloader/raw/master/LICENSE"&gt;https://github.com/instaloader/instaloader/blob/master/LICENSE&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. |pyversion| image:: &lt;a href="https://img.shields.io/pypi/pyversions/instaloader.svg"&gt;https://img.shields.io/pypi/pyversions/instaloader.svg&lt;/a&gt; :alt: Supported Python Versions&lt;/p&gt; 
&lt;p&gt;.. |contributors| image:: &lt;a href="https://img.shields.io/github/contributors/instaloader/instaloader.svg"&gt;https://img.shields.io/github/contributors/instaloader/instaloader.svg&lt;/a&gt; :alt: Contributor Count :target: &lt;a href="https://github.com/instaloader/instaloader/graphs/contributors"&gt;https://github.com/instaloader/instaloader/graphs/contributors&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. |aur| image:: &lt;a href="https://img.shields.io/aur/version/instaloader.svg"&gt;https://img.shields.io/aur/version/instaloader.svg&lt;/a&gt; :alt: Arch User Repository Package :target: &lt;a href="https://aur.archlinux.org/packages/instaloader/"&gt;https://aur.archlinux.org/packages/instaloader/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. |downloads| image:: &lt;a href="https://pepy.tech/badge/instaloader/month"&gt;https://pepy.tech/badge/instaloader/month&lt;/a&gt; :alt: PyPI Download Count :target: &lt;a href="https://pepy.tech/project/instaloader"&gt;https://pepy.tech/project/instaloader&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. badges-end&lt;/p&gt; 
&lt;p&gt;::&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ pip3 install instaloader

$ instaloader profile [profile ...]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Instaloader&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;downloads &lt;strong&gt;public and private profiles, hashtags, user stories, feeds and saved media&lt;/strong&gt;,&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;downloads &lt;strong&gt;comments, geotags and captions&lt;/strong&gt; of each post,&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;automatically &lt;strong&gt;detects profile name changes&lt;/strong&gt; and renames the target directory accordingly,&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;allows &lt;strong&gt;fine-grained customization&lt;/strong&gt; of filters and where to store downloaded media,&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;automatically &lt;strong&gt;resumes previously-interrupted&lt;/strong&gt; download iterations.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;::&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;instaloader [--comments] [--geotags]
            [--stories] [--highlights] [--tagged] [--reels] [--igtv]
            [--login YOUR-USERNAME] [--fast-update]
            profile | "#hashtag" | :stories | :feed | :saved
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;Instaloader Documentation &amp;lt;https://instaloader.github.io/&amp;gt;&lt;/code&gt;__&lt;/p&gt; 
&lt;h2&gt;How to Automatically Download Pictures from Instagram&lt;/h2&gt; 
&lt;p&gt;To &lt;strong&gt;download all pictures and videos of a profile&lt;/strong&gt;, as well as the &lt;strong&gt;profile picture&lt;/strong&gt;, do&lt;/p&gt; 
&lt;p&gt;::&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;instaloader profile [profile ...]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;where &lt;code&gt;profile&lt;/code&gt; is the name of a profile you want to download. Instead of only one profile, you may also specify a list of profiles.&lt;/p&gt; 
&lt;p&gt;To later &lt;strong&gt;update your local copy&lt;/strong&gt; of that profiles, you may run&lt;/p&gt; 
&lt;p&gt;::&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;instaloader --fast-update profile [profile ...]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If &lt;code&gt;--fast-update&lt;/code&gt; is given, Instaloader stops when arriving at the first already-downloaded picture.&lt;/p&gt; 
&lt;p&gt;Alternatively, you can use &lt;code&gt;--latest-stamps&lt;/code&gt; to have Instaloader store the time each profile was last downloaded and only download newer media:&lt;/p&gt; 
&lt;p&gt;::&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;instaloader --latest-stamps -- profile [profile ...]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With this option it's possible to move or delete downloaded media and still keep the archive updated.&lt;/p&gt; 
&lt;p&gt;When updating profiles, Instaloader automatically &lt;strong&gt;detects profile name changes&lt;/strong&gt; and renames the target directory accordingly.&lt;/p&gt; 
&lt;p&gt;Instaloader can also be used to &lt;strong&gt;download private profiles&lt;/strong&gt;. To do so, invoke it with&lt;/p&gt; 
&lt;p&gt;::&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;instaloader --login=your_username profile [profile ...]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When logging in, Instaloader &lt;strong&gt;stores the session cookies&lt;/strong&gt; in a file in your temporary directory, which will be reused later the next time &lt;code&gt;--login&lt;/code&gt; is given. So you can download private profiles &lt;strong&gt;non-interactively&lt;/strong&gt; when you already have a valid session cookie file.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Instaloader Documentation &amp;lt;https://instaloader.github.io/basic-usage.html&amp;gt;&lt;/code&gt;__&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;As an open source project, Instaloader heavily depends on the contributions from its community. See &lt;code&gt;contributing &amp;lt;https://instaloader.github.io/contributing.html&amp;gt;&lt;/code&gt;__ for how you may help Instaloader to become an even greater tool.&lt;/p&gt; 
&lt;h2&gt;Supporters&lt;/h2&gt; 
&lt;p&gt;.. current-sponsors-start&lt;/p&gt; 
&lt;p&gt;| Instaloader is proudly sponsored by | &lt;code&gt;@rocketapi-io &amp;lt;https://github.com/rocketapi-io&amp;gt;&lt;/code&gt;__&lt;/p&gt; 
&lt;p&gt;See &lt;code&gt;Alex' GitHub Sponsors &amp;lt;https://github.com/sponsors/aandergr&amp;gt;&lt;/code&gt;__ page for how you can sponsor the development of Instaloader!&lt;/p&gt; 
&lt;p&gt;.. current-sponsors-end&lt;/p&gt; 
&lt;p&gt;It is a pleasure for us to share our Instaloader to the world, and we are proud to have attracted such an active and motivating community, with so many users who share their suggestions and ideas with us. Buying a community-sponsored beer or coffee from time to time is very likely to further raise our passion for the development of Instaloader.&lt;/p&gt; 
&lt;p&gt;| For Donations, we provide GitHub Sponsors page, a PayPal.Me link and a Bitcoin address. | GitHub Sponsors: &lt;code&gt;Sponsor @aandergr on GitHub Sponsors &amp;lt;https://github.com/sponsors/aandergr&amp;gt;&lt;/code&gt;__ | PayPal: &lt;code&gt;PayPal.me/aandergr &amp;lt;https://www.paypal.me/aandergr&amp;gt;&lt;/code&gt;__ | BTC: 1Nst4LoadeYzrKjJ1DX9CpbLXBYE9RKLwY&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;.. disclaimer-start&lt;/p&gt; 
&lt;p&gt;Instaloader is in no way affiliated with, authorized, maintained or endorsed by Instagram or any of its affiliates or subsidiaries. This is an independent and unofficial project. Use at your own risk.&lt;/p&gt; 
&lt;p&gt;Instaloader is licensed under an MIT license. Refer to &lt;code&gt;LICENSE&lt;/code&gt; file for more information.&lt;/p&gt; 
&lt;p&gt;.. disclaimer-end&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>coleam00/Archon</title>
      <link>https://github.com/coleam00/Archon</link>
      <description>&lt;p&gt;Beta release of Archon OS - the knowledge and task management backbone for AI coding assistants.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/coleam00/Archon/main/archon-ui-main/public/archon-main-graphic.png" alt="Archon Main Graphic" width="853" height="422" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;em&gt;Power up your AI coding assistants with your own custom knowledge base and task management as an MCP server&lt;/em&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/#quick-start"&gt;Quick Start&lt;/a&gt; â€¢ &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/#whats-included"&gt;What's Included&lt;/a&gt; â€¢ &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/#architecture"&gt;Architecture&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ¯ What is Archon?&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Archon is currently in beta! Expect things to not work 100%, and please feel free to share any feedback and contribute with fixes/new features!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Archon is the &lt;strong&gt;command center&lt;/strong&gt; for AI coding assistants. For you, it's a sleek interface to manage knowledge, context, and tasks for your projects. For the AI coding assistant(s), it's a &lt;strong&gt;Model Context Protocol (MCP) server&lt;/strong&gt; to collaborate on and leverage the same knowledge, context, and tasks. Connect Claude Code, Kiro, Cursor, Windsurf, etc. to give your AI agents access to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Your documentation&lt;/strong&gt; (crawled websites, uploaded PDFs/docs)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart search capabilities&lt;/strong&gt; with advanced RAG strategies&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Task management&lt;/strong&gt; integrated with your knowledge base&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time updates&lt;/strong&gt; as you add new content and collaborate with your coding assistant on tasks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Much more&lt;/strong&gt; coming soon to build Archon into an integrated environment for all context engineering&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This new vision for Archon replaces the old one (the agenteer). Archon used to be the AI agent that builds other agents, and now you can use Archon to do that and more.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;It doesn't matter what you're building or if it's a new/existing codebase - Archon's knowledge and task management capabilities will improve the output of &lt;strong&gt;any&lt;/strong&gt; AI driven coding.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ”— Important Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/coleam00/Archon/discussions"&gt;GitHub Discussions&lt;/a&gt;&lt;/strong&gt; - Join the conversation and share ideas about Archon&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt;&lt;/strong&gt; - How to get involved and contribute to Archon&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://youtu.be/8pRc_s2VQIo"&gt;Introduction Video&lt;/a&gt;&lt;/strong&gt; - Getting Started Guide and Vision for Archon&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://dynamous.ai"&gt;Dynamous AI Mastery&lt;/a&gt;&lt;/strong&gt; - The birthplace of Archon - come join a vibrant community of other early AI adopters all helping each other transform their careers and businesses!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.docker.com/products/docker-desktop/"&gt;Docker Desktop&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://supabase.com/"&gt;Supabase&lt;/a&gt; account (free tier or local Supabase both work)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://platform.openai.com/api-keys"&gt;OpenAI API key&lt;/a&gt; (Gemini and Ollama are supported too!)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Setup Instructions&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone Repository&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/coleam00/archon.git
cd archon
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Environment Configuration&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cp .env.example .env
# Edit .env and add your Supabase credentials:
# SUPABASE_URL=https://your-project.supabase.co
# SUPABASE_SERVICE_KEY=your-service-key-here
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;NOTE: Supabase introduced a new type of service key but use the legacy one (the longer one).&lt;/p&gt; &lt;p&gt;OPTIONAL: If you want to enable the reranking RAG strategy, uncomment lines 20-22 in &lt;code&gt;python\requirements.server.txt&lt;/code&gt;. This will significantly increase the size of the Archon Server container which is why it's off by default.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Database Setup&lt;/strong&gt;: In your &lt;a href="https://supabase.com/dashboard"&gt;Supabase project&lt;/a&gt; SQL Editor, copy, paste, and execute the contents of &lt;code&gt;migration/complete_setup.sql&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start Services&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker-compose up --build -d
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This starts the core microservices:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Server&lt;/strong&gt;: Core API and business logic (Port: 8181)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;MCP Server&lt;/strong&gt;: Protocol interface for AI clients (Port: 8051)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Agents (coming soon!)&lt;/strong&gt;: AI operations and streaming (Port: 8052)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;UI&lt;/strong&gt;: Web interface (Port: 3737)&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Ports are configurable in your .env as well!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configure API Keys&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Open &lt;a href="http://localhost:3737"&gt;http://localhost:3737&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Go to &lt;strong&gt;Settings&lt;/strong&gt; â†’ Select your LLM/embedding provider and set the API key (OpenAI is default)&lt;/li&gt; 
   &lt;li&gt;Test by uploading a document or crawling a website&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ”„ Database Reset (Start Fresh if Needed)&lt;/h2&gt; 
&lt;p&gt;If you need to completely reset your database and start fresh:&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;âš ï¸ &lt;strong&gt;Reset Database - This will delete ALL data for Archon!&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run Reset Script&lt;/strong&gt;: In your Supabase SQL Editor, run the contents of &lt;code&gt;migration/RESET_DB.sql&lt;/code&gt;&lt;/p&gt; &lt;p&gt;âš ï¸ WARNING: This will delete all Archon specific tables and data! Nothing else will be touched in your DB though.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Rebuild Database&lt;/strong&gt;: After reset, run &lt;code&gt;migration/complete_setup.sql&lt;/code&gt; to create all the tables again.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Restart Services&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker-compose up -d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Reconfigure&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Select your LLM/embedding provider and set the API key again&lt;/li&gt; 
    &lt;li&gt;Re-upload any documents or re-crawl websites&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;The reset script safely removes all tables, functions, triggers, and policies with proper dependency handling.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;âš¡ Quick Test&lt;/h2&gt; 
&lt;p&gt;Once everything is running:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Test Web Crawling&lt;/strong&gt;: Go to &lt;a href="http://localhost:3737"&gt;http://localhost:3737&lt;/a&gt; â†’ Knowledge Base â†’ "Crawl Website" â†’ Enter a doc URL (such as &lt;a href="https://ai.pydantic.dev/llms-full.txt"&gt;https://ai.pydantic.dev/llms-full.txt&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test Document Upload&lt;/strong&gt;: Knowledge Base â†’ Upload a PDF&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test Projects&lt;/strong&gt;: Projects â†’ Create a new project and add tasks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Integrate with your AI coding assistant&lt;/strong&gt;: MCP Dashboard â†’ Copy connection config for your AI coding assistant&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ“š Documentation&lt;/h2&gt; 
&lt;h3&gt;Core Services&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Service&lt;/th&gt; 
   &lt;th&gt;Container Name&lt;/th&gt; 
   &lt;th&gt;Default URL&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Web Interface&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;archon-ui&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://localhost:3737"&gt;http://localhost:3737&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Main dashboard and controls&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;API Service&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;archon-server&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://localhost:8181"&gt;http://localhost:8181&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Web crawling, document processing&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MCP Server&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;archon-mcp&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://localhost:8051"&gt;http://localhost:8051&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Model Context Protocol interface&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Agents Service&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;archon-agents&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://localhost:8052"&gt;http://localhost:8052&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;AI/ML operations, reranking&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;What's Included&lt;/h2&gt; 
&lt;h3&gt;ğŸ§  Knowledge Management&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Web Crawling&lt;/strong&gt;: Automatically detects and crawls entire documentation sites, sitemaps, and individual pages&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Document Processing&lt;/strong&gt;: Upload and process PDFs, Word docs, markdown files, and text documents with intelligent chunking&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code Example Extraction&lt;/strong&gt;: Automatically identifies and indexes code examples from documentation for enhanced search&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Vector Search&lt;/strong&gt;: Advanced semantic search with contextual embeddings for precise knowledge retrieval&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Source Management&lt;/strong&gt;: Organize knowledge by source, type, and tags for easy filtering&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ¤– AI Integration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt;: Connect any MCP-compatible client (Claude Code, Cursor, even non-AI coding assistants like Claude Desktop)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;10 MCP Tools&lt;/strong&gt;: Comprehensive yet simple set of tools for RAG queries, task management, and project operations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-LLM Support&lt;/strong&gt;: Works with OpenAI, Ollama, and Google Gemini models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;RAG Strategies&lt;/strong&gt;: Hybrid search, contextual embeddings, and result reranking for optimal AI responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time Streaming&lt;/strong&gt;: Live responses from AI agents with progress tracking&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ“‹ Project &amp;amp; Task Management&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Hierarchical Projects&lt;/strong&gt;: Organize work with projects, features, and tasks in a structured workflow&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AI-Assisted Creation&lt;/strong&gt;: Generate project requirements and tasks using integrated AI agents&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Document Management&lt;/strong&gt;: Version-controlled documents with collaborative editing capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Progress Tracking&lt;/strong&gt;: Real-time updates and status management across all project activities&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ”„ Real-time Collaboration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;WebSocket Updates&lt;/strong&gt;: Live progress tracking for crawling, processing, and AI operations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-user Support&lt;/strong&gt;: Collaborative knowledge building and project management&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Background Processing&lt;/strong&gt;: Asynchronous operations that don't block the user interface&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Health Monitoring&lt;/strong&gt;: Built-in service health checks and automatic reconnection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;h3&gt;Microservices Structure&lt;/h3&gt; 
&lt;p&gt;Archon uses true microservices architecture with clear separation of concerns:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend UI   â”‚    â”‚  Server (API)   â”‚    â”‚   MCP Server    â”‚    â”‚ Agents Service  â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚  React + Vite   â”‚â—„â”€â”€â–ºâ”‚    FastAPI +    â”‚â—„â”€â”€â–ºâ”‚    Lightweight  â”‚â—„â”€â”€â–ºâ”‚   PydanticAI    â”‚
â”‚  Port 3737      â”‚    â”‚    SocketIO     â”‚    â”‚    HTTP Wrapper â”‚    â”‚   Port 8052     â”‚
â”‚                 â”‚    â”‚    Port 8181    â”‚    â”‚    Port 8051    â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                        â”‚                        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚                        â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
                         â”‚    Database     â”‚               â”‚
                         â”‚                 â”‚               â”‚
                         â”‚    Supabase     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚    PostgreSQL   â”‚
                         â”‚    PGVector     â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Service Responsibilities&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Service&lt;/th&gt; 
   &lt;th&gt;Location&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
   &lt;th&gt;Key Features&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Frontend&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;archon-ui-main/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Web interface and dashboard&lt;/td&gt; 
   &lt;td&gt;React, TypeScript, TailwindCSS, Socket.IO client&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Server&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;python/src/server/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Core business logic and APIs&lt;/td&gt; 
   &lt;td&gt;FastAPI, service layer, Socket.IO broadcasts, all ML/AI operations&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MCP Server&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;python/src/mcp/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;MCP protocol interface&lt;/td&gt; 
   &lt;td&gt;Lightweight HTTP wrapper, 10 MCP tools, session management&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Agents&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;python/src/agents/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;PydanticAI agent hosting&lt;/td&gt; 
   &lt;td&gt;Document and RAG agents, streaming responses&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Communication Patterns&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;HTTP-based&lt;/strong&gt;: All inter-service communication uses HTTP APIs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Socket.IO&lt;/strong&gt;: Real-time updates from Server to Frontend&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP Protocol&lt;/strong&gt;: AI clients connect to MCP Server via SSE or stdio&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No Direct Imports&lt;/strong&gt;: Services are truly independent with no shared code dependencies&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Key Architectural Benefits&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Lightweight Containers&lt;/strong&gt;: Each service contains only required dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Independent Scaling&lt;/strong&gt;: Services can be scaled independently based on load&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Development Flexibility&lt;/strong&gt;: Teams can work on different services without conflicts&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Technology Diversity&lt;/strong&gt;: Each service uses the best tools for its specific purpose&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ”§ Configuring Custom Ports &amp;amp; Hostname&lt;/h2&gt; 
&lt;p&gt;By default, Archon services run on the following ports:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Archon-UI&lt;/strong&gt;: 3737&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Archon-Server&lt;/strong&gt;: 8181&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Archon-MCP&lt;/strong&gt;: 8051&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Archon-Agents&lt;/strong&gt;: 8052&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Archon-Docs&lt;/strong&gt;: 3838 (optional)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Changing Ports&lt;/h3&gt; 
&lt;p&gt;To use custom ports, add these variables to your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Service Ports Configuration
ARCHON_UI_PORT=3737
ARCHON_SERVER_PORT=8181
ARCHON_MCP_PORT=8051
ARCHON_AGENTS_PORT=8052
ARCHON_DOCS_PORT=3838
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Example: Running on different ports:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ARCHON_SERVER_PORT=8282
ARCHON_MCP_PORT=8151
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Configuring Hostname&lt;/h3&gt; 
&lt;p&gt;By default, Archon uses &lt;code&gt;localhost&lt;/code&gt; as the hostname. You can configure a custom hostname or IP address by setting the &lt;code&gt;HOST&lt;/code&gt; variable in your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Hostname Configuration
HOST=localhost  # Default

# Examples of custom hostnames:
HOST=192.168.1.100     # Use specific IP address
HOST=archon.local      # Use custom domain
HOST=myserver.com      # Use public domain
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This is useful when:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Running Archon on a different machine and accessing it remotely&lt;/li&gt; 
 &lt;li&gt;Using a custom domain name for your installation&lt;/li&gt; 
 &lt;li&gt;Deploying in a network environment where &lt;code&gt;localhost&lt;/code&gt; isn't accessible&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;After changing hostname or ports:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Restart Docker containers: &lt;code&gt;docker-compose down &amp;amp;&amp;amp; docker-compose up -d&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Access the UI at: &lt;code&gt;http://${HOST}:${ARCHON_UI_PORT}&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Update your AI client configuration with the new hostname and MCP port&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ”§ Development&lt;/h2&gt; 
&lt;p&gt;For development with hot reload:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Backend services (with auto-reload)
docker-compose up archon-server archon-mcp archon-agents --build

# Frontend (with hot reload) 
cd archon-ui-main &amp;amp;&amp;amp; npm run dev

# Documentation (with hot reload)
cd docs &amp;amp;&amp;amp; npm start
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The backend services are configured with &lt;code&gt;--reload&lt;/code&gt; flag in their uvicorn commands and have source code mounted as volumes for automatic hot reloading when you make changes.&lt;/p&gt; 
&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; 
&lt;p&gt;Archon Community License (ACL) v1.2 - see &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Archon is free, open, and hackable. Run it, fork it, share it - just don't sell it as-a-service without permission.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>exo-explore/exo</title>
      <link>https://github.com/exo-explore/exo</link>
      <description>&lt;p&gt;Run your own AI cluster at home with everyday devices ğŸ“±ğŸ’» ğŸ–¥ï¸âŒš&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="/docs/exo-logo-black-bg.jpg" /&gt; 
  &lt;img alt="exo logo" src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/exo-logo-transparent.png" width="50%" height="50%" /&gt; 
 &lt;/picture&gt; 
 &lt;p&gt;exo: Run your own AI cluster at home with everyday devices. Maintained by &lt;a href="https://x.com/exolabs"&gt;exo labs&lt;/a&gt;.&lt;/p&gt; 
 &lt;h3&gt; &lt;p&gt;&lt;a href="https://discord.gg/EUnjGpsmWw"&gt;Discord&lt;/a&gt; | &lt;a href="https://t.me/+Kh-KqHTzFYg3MGNk"&gt;Telegram&lt;/a&gt; | &lt;a href="https://x.com/exolabs"&gt;X&lt;/a&gt;&lt;/p&gt; &lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://github.com/exo-explore/exo/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/exo-explore/exo" alt="GitHub Repo stars" /&gt;&lt;/a&gt; &lt;a href="https://dl.circleci.com/status-badge/redirect/circleci/TrkofJDoGzdQAeL6yVHKsg/4i5hJuafuwZYZQxbRAWS71/tree/main"&gt;&lt;img src="https://dl.circleci.com/status-badge/img/circleci/TrkofJDoGzdQAeL6yVHKsg/4i5hJuafuwZYZQxbRAWS71/tree/main.svg?style=svg" alt="Tests" /&gt;&lt;/a&gt; &lt;a href="https://www.gnu.org/licenses/gpl-3.0"&gt;&lt;img src="https://img.shields.io/badge/License-GPLv3-blue.svg?sanitize=true" alt="License: GPL v3" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/11849" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/11849" alt="exo-explore%2Fexo | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;Unify your existing devices into one powerful GPU: iPhone, iPad, Android, Mac, NVIDIA, Raspberry Pi, pretty much any device!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h2&gt;Update: exo is hiring. See &lt;a href="https://exolabs.net"&gt;here&lt;/a&gt; for more details.&lt;/h2&gt; 
 &lt;h2&gt;Interested in running exo in your business? &lt;a href="mailto:hello@exolabs.net"&gt;Contact us&lt;/a&gt; to discuss.&lt;/h2&gt; 
&lt;/div&gt; 
&lt;h2&gt;Get Involved&lt;/h2&gt; 
&lt;p&gt;exo is &lt;strong&gt;experimental&lt;/strong&gt; software. Expect bugs early on. Create issues so they can be fixed. The &lt;a href="https://x.com/exolabs"&gt;exo labs&lt;/a&gt; team will strive to resolve issues quickly.&lt;/p&gt; 
&lt;p&gt;We also welcome contributions from the community. We have a list of bounties in &lt;a href="https://docs.google.com/spreadsheets/d/1cTCpTIp48UnnIvHeLEUNg1iMy_Q6lRybgECSFCoVJpE/edit?usp=sharing"&gt;this sheet&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;h3&gt;Wide Model Support&lt;/h3&gt; 
&lt;p&gt;exo supports different models including LLaMA (&lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/mlx/models/llama.py"&gt;MLX&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/tinygrad/models/llama.py"&gt;tinygrad&lt;/a&gt;), Mistral, LlaVA, Qwen, and Deepseek.&lt;/p&gt; 
&lt;h3&gt;Dynamic Model Partitioning&lt;/h3&gt; 
&lt;p&gt;exo &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/ring_memory_weighted_partitioning_strategy.py"&gt;optimally splits up models&lt;/a&gt; based on the current network topology and device resources available. This enables you to run larger models than you would be able to on any single device.&lt;/p&gt; 
&lt;h3&gt;Automatic Device Discovery&lt;/h3&gt; 
&lt;p&gt;exo will &lt;a href="https://github.com/exo-explore/exo/raw/945f90f676182a751d2ad7bcf20987ab7fe0181e/exo/orchestration/node.py#L154"&gt;automatically discover&lt;/a&gt; other devices using the best method available. Zero manual configuration.&lt;/p&gt; 
&lt;h3&gt;ChatGPT-compatible API&lt;/h3&gt; 
&lt;p&gt;exo provides a &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/api/chatgpt_api.py"&gt;ChatGPT-compatible API&lt;/a&gt; for running models. It's a &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/examples/chatgpt_api.sh"&gt;one-line change&lt;/a&gt; in your application to run models on your own hardware using exo.&lt;/p&gt; 
&lt;h3&gt;Device Equality&lt;/h3&gt; 
&lt;p&gt;Unlike other distributed inference frameworks, exo does not use a master-worker architecture. Instead, exo devices &lt;a href="https://github.com/exo-explore/exo/raw/945f90f676182a751d2ad7bcf20987ab7fe0181e/exo/orchestration/node.py#L161"&gt;connect p2p&lt;/a&gt;. As long as a device is connected somewhere in the network, it can be used to run models.&lt;/p&gt; 
&lt;p&gt;Exo supports different &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/partitioning_strategy.py"&gt;partitioning strategies&lt;/a&gt; to split up a model across devices. The default partitioning strategy is &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/ring_memory_weighted_partitioning_strategy.py"&gt;ring memory weighted partitioning&lt;/a&gt;. This runs an inference in a ring where each device runs a number of model layers proportional to the memory of the device.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/exo-screenshot.jpg" alt="&amp;quot;A screenshot of exo running 5 nodes" /&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;The current recommended way to install exo is from source.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python&amp;gt;=3.12.0 is required because of &lt;a href="https://github.com/exo-explore/exo/issues/5"&gt;issues with asyncio&lt;/a&gt; in previous versions.&lt;/li&gt; 
 &lt;li&gt;For Linux with NVIDIA GPU support (Linux-only, skip if not using Linux or NVIDIA): 
  &lt;ul&gt; 
   &lt;li&gt;NVIDIA driver - verify with &lt;code&gt;nvidia-smi&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;CUDA toolkit - install from &lt;a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#cuda-cross-platform-installation"&gt;NVIDIA CUDA guide&lt;/a&gt;, verify with &lt;code&gt;nvcc --version&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;cuDNN library - download from &lt;a href="https://developer.nvidia.com/cudnn-downloads"&gt;NVIDIA cuDNN page&lt;/a&gt;, verify installation by following &lt;a href="https://docs.nvidia.com/deeplearning/cudnn/latest/installation/linux.html#verifying-the-install-on-linux:~:text=at%20a%20time.-,Verifying%20the%20Install%20on%20Linux,Test%20passed!,-Upgrading%20From%20Older"&gt;these steps&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Hardware Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;The only requirement to run exo is to have enough memory across all your devices to fit the entire model into memory. For example, if you are running llama 3.1 8B (fp16), you need 16GB of memory across all devices. Any of the following configurations would work since they each have more than 16GB of memory in total: 
  &lt;ul&gt; 
   &lt;li&gt;2 x 8GB M3 MacBook Airs&lt;/li&gt; 
   &lt;li&gt;1 x 16GB NVIDIA RTX 4070 Ti Laptop&lt;/li&gt; 
   &lt;li&gt;2 x Raspberry Pi 400 with 4GB of RAM each (running on CPU) + 1 x 8GB Mac Mini&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;exo is designed to run on devices with heterogeneous capabilities. For example, you can have some devices with powerful GPUs and others with integrated GPUs or even CPUs. Adding less capable devices will slow down individual inference latency but will increase the overall throughput of the cluster.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;From source&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/exo-explore/exo.git
cd exo
pip install -e .
# alternatively, with venv
source install.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Troubleshooting&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;If running on Mac, MLX has an &lt;a href="https://ml-explore.github.io/mlx/build/html/install.html"&gt;install guide&lt;/a&gt; with troubleshooting steps.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Performance&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;There are a number of things users have empirically found to improve performance on Apple Silicon Macs:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol&gt; 
 &lt;li&gt;Upgrade to the latest version of macOS Sequoia.&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;./configure_mlx.sh&lt;/code&gt;. This runs commands to optimize GPU memory allocation on Apple Silicon Macs.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;h3&gt;Example Usage on Multiple macOS Devices&lt;/h3&gt; 
&lt;h4&gt;Device 1:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Device 2:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That's it! No configuration required - exo will automatically discover the other device(s).&lt;/p&gt; 
&lt;p&gt;exo starts a ChatGPT-like WebUI (powered by &lt;a href="https://github.com/tinygrad/tinygrad/tree/master/examples/tinychat"&gt;tinygrad tinychat&lt;/a&gt;) on &lt;a href="http://localhost:52415"&gt;http://localhost:52415&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For developers, exo also starts a ChatGPT-compatible API endpoint on &lt;a href="http://localhost:52415/v1/chat/completions"&gt;http://localhost:52415/v1/chat/completions&lt;/a&gt;. Examples with curl:&lt;/p&gt; 
&lt;h4&gt;Llama 3.2 3B:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "llama-3.2-3b",
     "messages": [{"role": "user", "content": "What is the meaning of exo?"}],
     "temperature": 0.7
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Llama 3.1 405B:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "llama-3.1-405b",
     "messages": [{"role": "user", "content": "What is the meaning of exo?"}],
     "temperature": 0.7
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;DeepSeek R1 (full 671B):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "deepseek-r1",
     "messages": [{"role": "user", "content": "What is the meaning of exo?"}],
     "temperature": 0.7
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Llava 1.5 7B (Vision Language Model):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "llava-1.5-7b-hf",
     "messages": [
      {
        "role": "user",
        "content": [
          {
            "type": "text",
            "text": "What are these?"
          },
          {
            "type": "image_url",
            "image_url": {
              "url": "http://images.cocodataset.org/val2017/000000039769.jpg"
            }
          }
        ]
      }
    ],
     "temperature": 0.0
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Example Usage on Multiple Heterogenous Devices (macOS + Linux)&lt;/h3&gt; 
&lt;h4&gt;Device 1 (macOS):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: We don't need to explicitly tell exo to use the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine. &lt;strong&gt;MLX&lt;/strong&gt; and &lt;strong&gt;tinygrad&lt;/strong&gt; are interoperable!&lt;/p&gt; 
&lt;h4&gt;Device 2 (Linux):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Linux devices will automatically default to using the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine.&lt;/p&gt; 
&lt;p&gt;You can read about tinygrad-specific env vars &lt;a href="https://docs.tinygrad.org/env_vars/"&gt;here&lt;/a&gt;. For example, you can configure tinygrad to use the cpu by specifying &lt;code&gt;CLANG=1&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Example Usage on a single device with "exo run" command&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo run llama-3.2-3b
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With a custom prompt:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo run llama-3.2-3b --prompt "What is the meaning of exo?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Model Storage&lt;/h3&gt; 
&lt;p&gt;Models by default are stored in &lt;code&gt;~/.cache/exo/downloads&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can set a different model storage location by setting the &lt;code&gt;EXO_HOME&lt;/code&gt; env var.&lt;/p&gt; 
&lt;h2&gt;Model Downloading&lt;/h2&gt; 
&lt;p&gt;Models are downloaded from Hugging Face. If you are running exo in a country with strict internet censorship, you may need to download the models manually and put them in the &lt;code&gt;~/.cache/exo/downloads&lt;/code&gt; directory.&lt;/p&gt; 
&lt;p&gt;To download models from a proxy endpoint, set the &lt;code&gt;HF_ENDPOINT&lt;/code&gt; environment variable. For example, to run exo with the huggingface mirror endpoint:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;HF_ENDPOINT=https://hf-mirror.com exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Debugging&lt;/h2&gt; 
&lt;p&gt;Enable debug logs with the DEBUG environment variable (0-9).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;DEBUG=9 exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine specifically, there is a separate DEBUG flag &lt;code&gt;TINYGRAD_DEBUG&lt;/code&gt; that can be used to enable debug logs (1-6).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;TINYGRAD_DEBUG=2 exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Formatting&lt;/h2&gt; 
&lt;p&gt;We use &lt;a href="https://github.com/google/yapf"&gt;yapf&lt;/a&gt; to format the code. To format the code, first install the formatting requirements:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip3 install -e '.[formatting]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then run the formatting script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python3 format.py ./exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Known Issues&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;On certain versions of Python on macOS, certificates may not installed correctly, potentially causing SSL errors (e.g., when accessing huggingface.co). To resolve this, run the &lt;code&gt;Install Certificates&lt;/code&gt; command, typicall as follows:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;/Applications/Python 3.x/Install Certificates.command
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸš§ As the library is evolving so quickly, the iOS implementation has fallen behind Python. We have decided for now not to put out the buggy iOS version and receive a bunch of GitHub issues for outdated code. We are working on solving this properly and will make an announcement when it's ready. If you would like access to the iOS implementation now, please email &lt;a href="mailto:alex@exolabs.net"&gt;alex@exolabs.net&lt;/a&gt; with your GitHub username explaining your use-case and you will be granted access on GitHub.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Inference Engines&lt;/h2&gt; 
&lt;p&gt;exo supports the following inference engines:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;âœ… &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/mlx/sharded_inference_engine.py"&gt;MLX&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/tinygrad/inference.py"&gt;tinygrad&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸš§ &lt;a href="https://github.com/exo-explore/exo/pull/139"&gt;PyTorch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸš§ &lt;a href="https://github.com/exo-explore/exo/issues/167"&gt;llama.cpp&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Discovery Modules&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;âœ… &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/udp"&gt;UDP&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/manual"&gt;Manual&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/tailscale"&gt;Tailscale&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸš§ Radio&lt;/li&gt; 
 &lt;li&gt;ğŸš§ Bluetooth&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Peer Networking Modules&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;âœ… &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/grpc"&gt;GRPC&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸš§ NCCL&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>HKUDS/LightRAG</title>
      <link>https://github.com/HKUDS/LightRAG</link>
      <description>&lt;p&gt;"LightRAG: Simple and Fast Retrieval-Augmented Generation"&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;div style="margin: 20px 0;"&gt; 
  &lt;img src="https://raw.githubusercontent.com/HKUDS/LightRAG/main/assets/logo.png" width="120" height="120" alt="LightRAG Logo" style="border-radius: 20px; box-shadow: 0 8px 32px rgba(0, 217, 255, 0.3);" /&gt; 
 &lt;/div&gt; 
 &lt;h1&gt;ğŸš€ LightRAG: Simple and Fast Retrieval-Augmented Generation&lt;/h1&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://trendshift.io/repositories/13043" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13043" alt="HKUDS%2FLightRAG | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
 &lt;/div&gt; 
 &lt;div align="center"&gt; 
  &lt;div style="width: 100%; height: 2px; margin: 20px 0; background: linear-gradient(90deg, transparent, #00d9ff, transparent);"&gt;&lt;/div&gt; 
 &lt;/div&gt; 
 &lt;div align="center"&gt; 
  &lt;div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; padding: 25px; text-align: center;"&gt; 
   &lt;p&gt; &lt;a href="https://github.com/HKUDS/LightRAG"&gt;&lt;img src="https://img.shields.io/badge/ğŸ”¥Project-Page-00d9ff?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2410.05779"&gt;&lt;img src="https://img.shields.io/badge/ğŸ“„arXiv-2410.05779-ff6b6b?style=for-the-badge&amp;amp;logo=arxiv&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/LightRAG/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/HKUDS/LightRAG?color=00d9ff&amp;amp;style=for-the-badge&amp;amp;logo=star&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
   &lt;p&gt; &lt;img src="https://img.shields.io/badge/ğŸPython-3.10-4ecdc4?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;a href="https://pypi.org/project/lightrag-hku/"&gt;&lt;img src="https://img.shields.io/pypi/v/lightrag-hku.svg?style=for-the-badge&amp;amp;logo=pypi&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e&amp;amp;color=ff6b6b" /&gt;&lt;/a&gt; &lt;/p&gt; 
   &lt;p&gt; &lt;a href="https://discord.gg/yF2MmDJyGJ"&gt;&lt;img src="https://img.shields.io/badge/ğŸ’¬Discord-Community-7289da?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/LightRAG/issues/285"&gt;&lt;img src="https://img.shields.io/badge/ğŸ’¬WeChat-Group-07c160?style=for-the-badge&amp;amp;logo=wechat&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
   &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/README-zh.md"&gt;&lt;img src="https://img.shields.io/badge/ğŸ‡¨ğŸ‡³ä¸­æ–‡ç‰ˆ-1a1a2e?style=for-the-badge" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/README.md"&gt;&lt;img src="https://img.shields.io/badge/ğŸ‡ºğŸ‡¸English-1a1a2e?style=for-the-badge" /&gt;&lt;/a&gt; &lt;/p&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div align="center" style="margin: 30px 0;"&gt; 
 &lt;img src="https://user-images.githubusercontent.com/74038190/212284100-561aa473-3905-4a80-b561-0d28506553ee.gif" width="800" /&gt; 
&lt;/div&gt; 
&lt;div align="center" style="margin: 30px 0;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/HKUDS/LightRAG/main/README.assets/b2aaf634151b4706892693ffb43d9093.png" width="800" alt="LightRAG Diagram" /&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ‰ News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2025.06.16]ğŸ¯ğŸ“¢Our team has released &lt;a href="https://github.com/HKUDS/RAG-Anything"&gt;RAG-Anything&lt;/a&gt; an All-in-One Multimodal RAG System for seamless text, image, table, and equation processing.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2025.06.05]ğŸ¯ğŸ“¢LightRAG now supports comprehensive multimodal data handling through &lt;a href="https://github.com/HKUDS/RAG-Anything"&gt;RAG-Anything&lt;/a&gt; integration, enabling seamless document parsing and RAG capabilities across diverse formats including PDFs, images, Office documents, tables, and formulas. Please refer to the new &lt;a href="https://github.com/HKUDS/LightRAG/?tab=readme-ov-file#multimodal-document-processing-rag-anything-integration"&gt;multimodal section&lt;/a&gt; for details.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2025.03.18]ğŸ¯ğŸ“¢LightRAG now supports citation functionality, enabling proper source attribution.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2025.02.05]ğŸ¯ğŸ“¢Our team has released &lt;a href="https://github.com/HKUDS/VideoRAG"&gt;VideoRAG&lt;/a&gt; understanding extremely long-context videos.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2025.01.13]ğŸ¯ğŸ“¢Our team has released &lt;a href="https://github.com/HKUDS/MiniRAG"&gt;MiniRAG&lt;/a&gt; making RAG simpler with small models.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2025.01.06]ğŸ¯ğŸ“¢You can now &lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/#using-postgresql-for-storage"&gt;use PostgreSQL for Storage&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2024.12.31]ğŸ¯ğŸ“¢LightRAG now supports &lt;a href="https://github.com/HKUDS/LightRAG?tab=readme-ov-file#delete"&gt;deletion by document ID&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2024.11.25]ğŸ¯ğŸ“¢LightRAG now supports seamless integration of &lt;a href="https://github.com/HKUDS/LightRAG?tab=readme-ov-file#insert-custom-kg"&gt;custom knowledge graphs&lt;/a&gt;, empowering users to enhance the system with their own domain expertise.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2024.11.19]ğŸ¯ğŸ“¢A comprehensive guide to LightRAG is now available on &lt;a href="https://learnopencv.com/lightrag"&gt;LearnOpenCV&lt;/a&gt;. Many thanks to the blog author.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2024.11.11]ğŸ¯ğŸ“¢LightRAG now supports &lt;a href="https://github.com/HKUDS/LightRAG?tab=readme-ov-file#delete"&gt;deleting entities by their names&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2024.11.09]ğŸ¯ğŸ“¢Introducing the &lt;a href="https://lightrag-gui.streamlit.app"&gt;LightRAG Gui&lt;/a&gt;, which allows you to insert, query, visualize, and download LightRAG knowledge.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2024.11.04]ğŸ¯ğŸ“¢You can now &lt;a href="https://github.com/HKUDS/LightRAG?tab=readme-ov-file#using-neo4j-for-storage"&gt;use Neo4J for Storage&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2024.10.29]ğŸ¯ğŸ“¢LightRAG now supports multiple file types, including PDF, DOC, PPT, and CSV via &lt;code&gt;textract&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2024.10.20]ğŸ¯ğŸ“¢We've added a new feature to LightRAG: Graph Visualization.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2024.10.18]ğŸ¯ğŸ“¢We've added a link to a &lt;a href="https://youtu.be/oageL-1I0GE"&gt;LightRAG Introduction Video&lt;/a&gt;. Thanks to the author!&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2024.10.17]ğŸ¯ğŸ“¢We have created a &lt;a href="https://discord.gg/yF2MmDJyGJ"&gt;Discord channel&lt;/a&gt;! Welcome to join for sharing and discussions! ğŸ‰ğŸ‰&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2024.10.16]ğŸ¯ğŸ“¢LightRAG now supports &lt;a href="https://github.com/HKUDS/LightRAG?tab=readme-ov-file#quick-start"&gt;Ollama models&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2024.10.15]ğŸ¯ğŸ“¢LightRAG now supports &lt;a href="https://github.com/HKUDS/LightRAG?tab=readme-ov-file#quick-start"&gt;Hugging Face models&lt;/a&gt;!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary style="font-size: 1.4em; font-weight: bold; cursor: pointer; display: list-item;"&gt; Algorithm Flowchart &lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://learnopencv.com/wp-content/uploads/2024/11/LightRAG-VectorDB-Json-KV-Store-Indexing-Flowchart-scaled.jpg" alt="LightRAG Indexing Flowchart" /&gt; &lt;em&gt;Figure 1: LightRAG Indexing Flowchart - Img Caption : &lt;a href="https://learnopencv.com/lightrag/"&gt;Source&lt;/a&gt;&lt;/em&gt; &lt;img src="https://learnopencv.com/wp-content/uploads/2024/11/LightRAG-Querying-Flowchart-Dual-Level-Retrieval-Generation-Knowledge-Graphs-scaled.jpg" alt="LightRAG Retrieval and Querying Flowchart" /&gt; &lt;em&gt;Figure 2: LightRAG Retrieval and Querying Flowchart - Img Caption : &lt;a href="https://learnopencv.com/lightrag/"&gt;Source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Install LightRAG Server&lt;/h3&gt; 
&lt;p&gt;The LightRAG Server is designed to provide Web UI and API support. The Web UI facilitates document indexing, knowledge graph exploration, and a simple RAG query interface. LightRAG Server also provide an Ollama compatible interfaces, aiming to emulate LightRAG as an Ollama chat model. This allows AI chat bot, such as Open WebUI, to access LightRAG easily.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install from PyPI&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "lightrag-hku[api]"
cp env.example .env
lightrag-server
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Installation from Source&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/HKUDS/LightRAG.git
cd LightRAG
# create a Python virtual enviroment if neccesary
# Install in editable mode with API support
pip install -e ".[api]"
cp env.example .env
lightrag-server
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Launching the LightRAG Server with Docker Compose&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/HKUDS/LightRAG.git
cd LightRAG
cp env.example .env
# modify LLM and Embedding settings in .env
docker compose up
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Historical versions of LightRAG docker images can be found here: &lt;a href="https://github.com/HKUDS/LightRAG/pkgs/container/lightrag"&gt;LightRAG Docker Images&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Install LightRAG Core&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install from source (Recommend)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd LightRAG
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install from PyPI&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install lightrag-hku
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;LLM and Technology Stack Requirements for LightRAG&lt;/h3&gt; 
&lt;p&gt;LightRAG's demands on the capabilities of Large Language Models (LLMs) are significantly higher than those of traditional RAG, as it requires the LLM to perform entity-relationship extraction tasks from documents. Configuring appropriate Embedding and Reranker models is also crucial for improving query performance.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;LLM Selection&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;It is recommended to use an LLM with at least 32 billion parameters.&lt;/li&gt; 
   &lt;li&gt;The context length should be at least 32KB, with 64KB being recommended.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Embedding Model&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;A high-performance Embedding model is essential for RAG.&lt;/li&gt; 
   &lt;li&gt;We recommend using mainstream multilingual Embedding models, such as: &lt;code&gt;BAAI/bge-m3&lt;/code&gt; and &lt;code&gt;text-embedding-3-large&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Important Note&lt;/strong&gt;: The Embedding model must be determined before document indexing, and the same model must be used during the document query phase. For certain storage solutions (e.g., PostgreSQL), the vector dimension must be defined upon initial table creation. Therefore, when changing embedding models, it is necessary to delete the existing vector-related tables and allow LightRAG to recreate them with the new dimensions.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reranker Model Configuration&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;Configuring a Reranker model can significantly enhance LightRAG's retrieval performance.&lt;/li&gt; 
   &lt;li&gt;When a Reranker model is enabled, it is recommended to set the "mix mode" as the default query mode.&lt;/li&gt; 
   &lt;li&gt;We recommend using mainstream Reranker models, such as: &lt;code&gt;BAAI/bge-reranker-v2-m3&lt;/code&gt; or models provided by services like Jina.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Start for LightRAG Server&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;For more information about LightRAG Server, please refer to &lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/lightrag/api/README.md"&gt;LightRAG Server&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Start for LightRAG core&lt;/h3&gt; 
&lt;p&gt;To get started with LightRAG core, refer to the sample codes available in the &lt;code&gt;examples&lt;/code&gt; folder. Additionally, a &lt;a href="https://www.youtube.com/watch?v=g21royNJ4fw"&gt;video demo&lt;/a&gt; demonstration is provided to guide you through the local setup process. If you already possess an OpenAI API key, you can run the demo right away:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;### you should run the demo code with project folder
cd LightRAG
### provide your API-KEY for OpenAI
export OPENAI_API_KEY="sk-...your_opeai_key..."
### download the demo document of "A Christmas Carol" by Charles Dickens
curl https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/tests/mock_data.txt &amp;gt; ./book.txt
### run the demo code
python examples/lightrag_openai_demo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For a streaming response implementation example, please see &lt;code&gt;examples/lightrag_openai_compatible_demo.py&lt;/code&gt;. Prior to execution, ensure you modify the sample code's LLM and embedding configurations accordingly.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note 1&lt;/strong&gt;: When running the demo program, please be aware that different test scripts may use different embedding models. If you switch to a different embedding model, you must clear the data directory (&lt;code&gt;./dickens&lt;/code&gt;); otherwise, the program may encounter errors. If you wish to retain the LLM cache, you can preserve the &lt;code&gt;kv_store_llm_response_cache.json&lt;/code&gt; file while clearing the data directory.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note 2&lt;/strong&gt;: Only &lt;code&gt;lightrag_openai_demo.py&lt;/code&gt; and &lt;code&gt;lightrag_openai_compatible_demo.py&lt;/code&gt; are officially supported sample codes. Other sample files are community contributions that haven't undergone full testing and optimization.&lt;/p&gt; 
&lt;h2&gt;Programing with LightRAG Core&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;If you would like to integrate LightRAG into your project, we recommend utilizing the REST API provided by the LightRAG Server. LightRAG Core is typically intended for embedded applications or for researchers who wish to conduct studies and evaluations.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;âš ï¸ Important: Initialization Requirements&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LightRAG requires explicit initialization before use.&lt;/strong&gt; You must call both &lt;code&gt;await rag.initialize_storages()&lt;/code&gt; and &lt;code&gt;await initialize_pipeline_status()&lt;/code&gt; after creating a LightRAG instance, otherwise you will encounter errors like:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;AttributeError: __aenter__&lt;/code&gt; - if storages are not initialized&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;KeyError: 'history_messages'&lt;/code&gt; - if pipeline status is not initialized&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;A Simple Program&lt;/h3&gt; 
&lt;p&gt;Use the below Python snippet to initialize LightRAG, insert text to it, and perform queries:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
import asyncio
from lightrag import LightRAG, QueryParam
from lightrag.llm.openai import gpt_4o_mini_complete, gpt_4o_complete, openai_embed
from lightrag.kg.shared_storage import initialize_pipeline_status
from lightrag.utils import setup_logger

setup_logger("lightrag", level="INFO")

WORKING_DIR = "./rag_storage"
if not os.path.exists(WORKING_DIR):
    os.mkdir(WORKING_DIR)

async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        embedding_func=openai_embed,
        llm_model_func=gpt_4o_mini_complete,
    )
    # IMPORTANT: Both initialization calls are required!
    await rag.initialize_storages()  # Initialize storage backends
    await initialize_pipeline_status()  # Initialize processing pipeline
    return rag

async def main():
    try:
        # Initialize RAG instance
        rag = await initialize_rag()
        await rag.ainsert("Your text")

        # Perform hybrid search
        mode = "hybrid"
        print(
          await rag.aquery(
              "What are the top themes in this story?",
              param=QueryParam(mode=mode)
          )
        )

    except Exception as e:
        print(f"An error occurred: {e}")
    finally:
        if rag:
            await rag.finalize_storages()

if __name__ == "__main__":
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Important notes for the above snippet:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Export your OPENAI_API_KEY environment variable before running the script.&lt;/li&gt; 
 &lt;li&gt;This program uses the default storage settings for LightRAG, so all data will be persisted to WORKING_DIR/rag_storage.&lt;/li&gt; 
 &lt;li&gt;This program demonstrates only the simplest way to initialize a LightRAG object: Injecting the embedding and LLM functions, and initializing storage and pipeline status after creating the LightRAG object.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;LightRAG init parameters&lt;/h3&gt; 
&lt;p&gt;A full list of LightRAG init parameters:&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; Parameters &lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;strong&gt;Parameter&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Type&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Explanation&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Default&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;working_dir&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;str&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Directory where the cache will be stored&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;lightrag_cache+timestamp&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;workspace&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;str&lt;/td&gt; 
    &lt;td&gt;Workspace name for data isolation between different LightRAG Instances&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;kv_storage&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;str&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Storage type for documents and text chunks. Supported types: &lt;code&gt;JsonKVStorage&lt;/code&gt;,&lt;code&gt;PGKVStorage&lt;/code&gt;,&lt;code&gt;RedisKVStorage&lt;/code&gt;,&lt;code&gt;MongoKVStorage&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;JsonKVStorage&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;vector_storage&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;str&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Storage type for embedding vectors. Supported types: &lt;code&gt;NanoVectorDBStorage&lt;/code&gt;,&lt;code&gt;PGVectorStorage&lt;/code&gt;,&lt;code&gt;MilvusVectorDBStorage&lt;/code&gt;,&lt;code&gt;ChromaVectorDBStorage&lt;/code&gt;,&lt;code&gt;FaissVectorDBStorage&lt;/code&gt;,&lt;code&gt;MongoVectorDBStorage&lt;/code&gt;,&lt;code&gt;QdrantVectorDBStorage&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;NanoVectorDBStorage&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;graph_storage&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;str&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Storage type for graph edges and nodes. Supported types: &lt;code&gt;NetworkXStorage&lt;/code&gt;,&lt;code&gt;Neo4JStorage&lt;/code&gt;,&lt;code&gt;PGGraphStorage&lt;/code&gt;,&lt;code&gt;AGEStorage&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;NetworkXStorage&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;doc_status_storage&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;str&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Storage type for documents process status. Supported types: &lt;code&gt;JsonDocStatusStorage&lt;/code&gt;,&lt;code&gt;PGDocStatusStorage&lt;/code&gt;,&lt;code&gt;MongoDocStatusStorage&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;JsonDocStatusStorage&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;chunk_token_size&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Maximum token size per chunk when splitting documents&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;1200&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;chunk_overlap_token_size&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Overlap token size between two chunks when splitting documents&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;100&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;tokenizer&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;Tokenizer&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;The function used to convert text into tokens (numbers) and back using .encode() and .decode() functions following &lt;code&gt;TokenizerInterface&lt;/code&gt; protocol. If you don't specify one, it will use the default Tiktoken tokenizer.&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;TiktokenTokenizer&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;tiktoken_model_name&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;str&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;If you're using the default Tiktoken tokenizer, this is the name of the specific Tiktoken model to use. This setting is ignored if you provide your own tokenizer.&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;gpt-4o-mini&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;entity_extract_max_gleaning&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Number of loops in the entity extraction process, appending history messages&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;node_embedding_algorithm&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;str&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Algorithm for node embedding (currently not used)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;node2vec&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;node2vec_params&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;dict&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Parameters for node embedding&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;{"dimensions": 1536,"num_walks": 10,"walk_length": 40,"window_size": 2,"iterations": 3,"random_seed": 3,}&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;embedding_func&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;EmbeddingFunc&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Function to generate embedding vectors from text&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;openai_embed&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;embedding_batch_num&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Maximum batch size for embedding processes (multiple texts sent per batch)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;32&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;embedding_func_max_async&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Maximum number of concurrent asynchronous embedding processes&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;16&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;llm_model_func&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;callable&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Function for LLM generation&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;gpt_4o_mini_complete&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;llm_model_name&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;str&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;LLM model name for generation&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;meta-llama/Llama-3.2-1B-Instruct&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;summary_max_tokens&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Maximum tokens send to LLM to generate entity relation summaries&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;32000&lt;/code&gt;ï¼ˆdefault value changed by env var MAX_TOKENS)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;llm_model_max_async&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Maximum number of concurrent asynchronous LLM processes&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;4&lt;/code&gt;ï¼ˆdefault value changed by env var MAX_ASYNC)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;llm_model_kwargs&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;dict&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Additional parameters for LLM generation&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;vector_db_storage_cls_kwargs&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;dict&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Additional parameters for vector database, like setting the threshold for nodes and relations retrieval&lt;/td&gt; 
    &lt;td&gt;cosine_better_than_threshold: 0.2ï¼ˆdefault value changed by env var COSINE_THRESHOLD)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;enable_llm_cache&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;bool&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;If &lt;code&gt;TRUE&lt;/code&gt;, stores LLM results in cache; repeated prompts return cached responses&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;TRUE&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;enable_llm_cache_for_entity_extract&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;bool&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;If &lt;code&gt;TRUE&lt;/code&gt;, stores LLM results in cache for entity extraction; Good for beginners to debug your application&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;TRUE&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;addon_params&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;dict&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Additional parameters, e.g., &lt;code&gt;{"example_number": 1, "language": "Simplified Chinese", "entity_types": ["organization", "person", "geo", "event"]}&lt;/code&gt;: sets example limit, entiy/relation extraction output language&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;example_number: all examples, language: English&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;embedding_cache_config&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;dict&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Configuration for question-answer caching. Contains three parameters: &lt;code&gt;enabled&lt;/code&gt;: Boolean value to enable/disable cache lookup functionality. When enabled, the system will check cached responses before generating new answers. &lt;code&gt;similarity_threshold&lt;/code&gt;: Float value (0-1), similarity threshold. When a new question's similarity with a cached question exceeds this threshold, the cached answer will be returned directly without calling the LLM. &lt;code&gt;use_llm_check&lt;/code&gt;: Boolean value to enable/disable LLM similarity verification. When enabled, LLM will be used as a secondary check to verify the similarity between questions before returning cached answers.&lt;/td&gt; 
    &lt;td&gt;Default: &lt;code&gt;{"enabled": False, "similarity_threshold": 0.95, "use_llm_check": False}&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;Query Param&lt;/h3&gt; 
&lt;p&gt;Use QueryParam to control the behavior your query:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;class QueryParam:
    """Configuration parameters for query execution in LightRAG."""

    mode: Literal["local", "global", "hybrid", "naive", "mix", "bypass"] = "global"
    """Specifies the retrieval mode:
    - "local": Focuses on context-dependent information.
    - "global": Utilizes global knowledge.
    - "hybrid": Combines local and global retrieval methods.
    - "naive": Performs a basic search without advanced techniques.
    - "mix": Integrates knowledge graph and vector retrieval.
    """

    only_need_context: bool = False
    """If True, only returns the retrieved context without generating a response."""

    only_need_prompt: bool = False
    """If True, only returns the generated prompt without producing a response."""

    response_type: str = "Multiple Paragraphs"
    """Defines the response format. Examples: 'Multiple Paragraphs', 'Single Paragraph', 'Bullet Points'."""

    stream: bool = False
    """If True, enables streaming output for real-time responses."""

    top_k: int = int(os.getenv("TOP_K", "60"))
    """Number of top items to retrieve. Represents entities in 'local' mode and relationships in 'global' mode."""

    chunk_top_k: int = int(os.getenv("CHUNK_TOP_K", "10"))
    """Number of text chunks to retrieve initially from vector search and keep after reranking.
    If None, defaults to top_k value.
    """

    max_entity_tokens: int = int(os.getenv("MAX_ENTITY_TOKENS", "10000"))
    """Maximum number of tokens allocated for entity context in unified token control system."""

    max_relation_tokens: int = int(os.getenv("MAX_RELATION_TOKENS", "10000"))
    """Maximum number of tokens allocated for relationship context in unified token control system."""

    max_total_tokens: int = int(os.getenv("MAX_TOTAL_TOKENS", "30000"))
    """Maximum total tokens budget for the entire query context (entities + relations + chunks + system prompt)."""

    conversation_history: list[dict[str, str]] = field(default_factory=list)
    """Stores past conversation history to maintain context.
    Format: [{"role": "user/assistant", "content": "message"}].
    """

    # Deprated: history message have negtive effect on query performance
    history_turns: int = 0
    """Number of complete conversation turns (user-assistant pairs) to consider in the response context."""

    ids: list[str] | None = None
    """List of ids to filter the results."""

    model_func: Callable[..., object] | None = None
    """Optional override for the LLM model function to use for this specific query.
    If provided, this will be used instead of the global model function.
    This allows using different models for different query modes.
    """

    user_prompt: str | None = None
    """User-provided prompt for the query.
    If proivded, this will be use instead of the default vaulue from prompt template.
    """

    enable_rerank: bool = True
    """Enable reranking for retrieved text chunks. If True but no rerank model is configured, a warning will be issued.
    Default is True to enable reranking when rerank model is available.
    """
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;default value of Top_k can be change by environment variables TOP_K.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;LLM and Embedding Injection&lt;/h3&gt; 
&lt;p&gt;LightRAG requires the utilization of LLM and Embedding models to accomplish document indexing and querying tasks. During the initialization phase, it is necessary to inject the invocation methods of the relevant models into LightRAGï¼š&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Using Open AI-like APIs&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;LightRAG also supports Open AI-like chat/embeddings APIs:&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;async def llm_model_func(
    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs
) -&amp;gt; str:
    return await openai_complete_if_cache(
        "solar-mini",
        prompt,
        system_prompt=system_prompt,
        history_messages=history_messages,
        api_key=os.getenv("UPSTAGE_API_KEY"),
        base_url="https://api.upstage.ai/v1/solar",
        **kwargs
    )

async def embedding_func(texts: list[str]) -&amp;gt; np.ndarray:
    return await openai_embed(
        texts,
        model="solar-embedding-1-large-query",
        api_key=os.getenv("UPSTAGE_API_KEY"),
        base_url="https://api.upstage.ai/v1/solar"
    )

async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=llm_model_func,
        embedding_func=EmbeddingFunc(
            embedding_dim=4096,
            func=embedding_func
        )
    )

    await rag.initialize_storages()
    await initialize_pipeline_status()

    return rag
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Using Hugging Face Models&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;If you want to use Hugging Face models, you only need to set LightRAG as follows:&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;See &lt;code&gt;lightrag_hf_demo.py&lt;/code&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Initialize LightRAG with Hugging Face model
rag = LightRAG(
    working_dir=WORKING_DIR,
    llm_model_func=hf_model_complete,  # Use Hugging Face model for text generation
    llm_model_name='meta-llama/Llama-3.1-8B-Instruct',  # Model name from Hugging Face
    # Use Hugging Face embedding function
    embedding_func=EmbeddingFunc(
        embedding_dim=384,
        func=lambda texts: hf_embed(
            texts,
            tokenizer=AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2"),
            embed_model=AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
        )
    ),
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Using Ollama Models&lt;/b&gt; &lt;/summary&gt; **Overview** 
 &lt;p&gt;If you want to use Ollama models, you need to pull model you plan to use and embedding model, for example &lt;code&gt;nomic-embed-text&lt;/code&gt;.&lt;/p&gt; 
 &lt;p&gt;Then you only need to set LightRAG as follows:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Initialize LightRAG with Ollama model
rag = LightRAG(
    working_dir=WORKING_DIR,
    llm_model_func=ollama_model_complete,  # Use Ollama model for text generation
    llm_model_name='your_model_name', # Your model name
    # Use Ollama embedding function
    embedding_func=EmbeddingFunc(
        embedding_dim=768,
        func=lambda texts: ollama_embed(
            texts,
            embed_model="nomic-embed-text"
        )
    ),
)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Increasing context size&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;In order for LightRAG to work context should be at least 32k tokens. By default Ollama models have context size of 8k. You can achieve this using one of two ways:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Increasing the &lt;code&gt;num_ctx&lt;/code&gt; parameter in Modelfile&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Pull the model:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ollama pull qwen2
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Display the model file:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ollama show --modelfile qwen2 &amp;gt; Modelfile
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="3"&gt; 
  &lt;li&gt;Edit the Modelfile by adding the following line:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;PARAMETER num_ctx 32768
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="4"&gt; 
  &lt;li&gt;Create the modified model:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ollama create -f Modelfile qwen2m
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Setup &lt;code&gt;num_ctx&lt;/code&gt; via Ollama API&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Tiy can use &lt;code&gt;llm_model_kwargs&lt;/code&gt; param to configure ollama:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;rag = LightRAG(
    working_dir=WORKING_DIR,
    llm_model_func=ollama_model_complete,  # Use Ollama model for text generation
    llm_model_name='your_model_name', # Your model name
    llm_model_kwargs={"options": {"num_ctx": 32768}},
    # Use Ollama embedding function
    embedding_func=EmbeddingFunc(
        embedding_dim=768,
        func=lambda texts: ollama_embed(
            texts,
            embed_model="nomic-embed-text"
        )
    ),
)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Low RAM GPUs&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;In order to run this experiment on low RAM GPU you should select small model and tune context window (increasing context increase memory consumption). For example, running this ollama example on repurposed mining GPU with 6Gb of RAM required to set context size to 26k while using &lt;code&gt;gemma2:2b&lt;/code&gt;. It was able to find 197 entities and 19 relations on &lt;code&gt;book.txt&lt;/code&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;LlamaIndex&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;LightRAG supports integration with LlamaIndex (&lt;code&gt;llm/llama_index_impl.py&lt;/code&gt;):&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Integrates with OpenAI and other providers through LlamaIndex&lt;/li&gt; 
  &lt;li&gt;See &lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/lightrag/llm/Readme.md"&gt;LlamaIndex Documentation&lt;/a&gt; for detailed setup and examples&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Example Usage&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Using LlamaIndex with direct OpenAI access
import asyncio
from lightrag import LightRAG
from lightrag.llm.llama_index_impl import llama_index_complete_if_cache, llama_index_embed
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI
from lightrag.kg.shared_storage import initialize_pipeline_status
from lightrag.utils import setup_logger

# Setup log handler for LightRAG
setup_logger("lightrag", level="INFO")

async def initialize_rag():
    rag = LightRAG(
        working_dir="your/path",
        llm_model_func=llama_index_complete_if_cache,  # LlamaIndex-compatible completion function
        embedding_func=EmbeddingFunc(    # LlamaIndex-compatible embedding function
            embedding_dim=1536,
            func=lambda texts: llama_index_embed(texts, embed_model=embed_model)
        ),
    )

    await rag.initialize_storages()
    await initialize_pipeline_status()

    return rag

def main():
    # Initialize RAG instance
    rag = asyncio.run(initialize_rag())

    with open("./book.txt", "r", encoding="utf-8") as f:
        rag.insert(f.read())

    # Perform naive search
    print(
        rag.query("What are the top themes in this story?", param=QueryParam(mode="naive"))
    )

    # Perform local search
    print(
        rag.query("What are the top themes in this story?", param=QueryParam(mode="local"))
    )

    # Perform global search
    print(
        rag.query("What are the top themes in this story?", param=QueryParam(mode="global"))
    )

    # Perform hybrid search
    print(
        rag.query("What are the top themes in this story?", param=QueryParam(mode="hybrid"))
    )

if __name__ == "__main__":
    main()
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;For detailed documentation and examples, see:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/lightrag/llm/Readme.md"&gt;LlamaIndex Documentation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/examples/lightrag_llamaindex_direct_demo.py"&gt;Direct OpenAI Example&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/examples/lightrag_llamaindex_litellm_demo.py"&gt;LiteLLM Proxy Example&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;Conversation History Support&lt;/h3&gt; 
&lt;p&gt;LightRAG now supports multi-turn dialogue through the conversation history feature. Here's how to use it:&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt; Usage Example &lt;/b&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Create conversation history
conversation_history = [
    {"role": "user", "content": "What is the main character's attitude towards Christmas?"},
    {"role": "assistant", "content": "At the beginning of the story, Ebenezer Scrooge has a very negative attitude towards Christmas..."},
    {"role": "user", "content": "How does his attitude change?"}
]

# Create query parameters with conversation history
query_param = QueryParam(
    mode="mix",  # or any other mode: "local", "global", "hybrid"
    conversation_history=conversation_history,  # Add the conversation history
)

# Make a query that takes into account the conversation history
response = rag.query(
    "What causes this change in his character?",
    param=query_param
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;User Prompt vs. Query&lt;/h3&gt; 
&lt;p&gt;When using LightRAG for content queries, avoid combining the search process with unrelated output processing, as this significantly impacts query effectiveness. The &lt;code&gt;user_prompt&lt;/code&gt; parameter in Query Param is specifically designed to address this issue â€” it does not participate in the RAG retrieval phase, but rather guides the LLM on how to process the retrieved results after the query is completed. Here's how to use it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Create query parameters
query_param = QueryParam(
    mode = "hybrid",  # Other modesï¼šlocal, global, hybrid, mix, naive
    user_prompt = "For diagrams, use mermaid format with English/Pinyin node names and Chinese display labels",
)

# Query and process
response_default = rag.query(
    "Please draw a character relationship diagram for Scrooge",
    param=query_param
)
print(response_default)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Insert&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt; Basic Insert &lt;/b&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Basic Insert
rag.insert("Text")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt; Batch Insert &lt;/b&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Basic Batch Insert: Insert multiple texts at once
rag.insert(["TEXT1", "TEXT2",...])

# Batch Insert with custom batch size configuration
rag = LightRAG(
    ...
    working_dir=WORKING_DIR,
    max_parallel_insert = 4
)

rag.insert(["TEXT1", "TEXT2", "TEXT3", ...])  # Documents will be processed in batches of 4
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;The &lt;code&gt;max_parallel_insert&lt;/code&gt; parameter determines the number of documents processed concurrently in the document indexing pipeline. If unspecified, the default value is &lt;strong&gt;2&lt;/strong&gt;. We recommend keeping this setting &lt;strong&gt;below 10&lt;/strong&gt;, as the performance bottleneck typically lies with the LLM (Large Language Model) processing.The &lt;code&gt;max_parallel_insert&lt;/code&gt; parameter determines the number of documents processed concurrently in the document indexing pipeline. If unspecified, the default value is &lt;strong&gt;2&lt;/strong&gt;. We recommend keeping this setting &lt;strong&gt;below 10&lt;/strong&gt;, as the performance bottleneck typically lies with the LLM (Large Language Model) processing.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt; Insert with ID &lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;If you want to provide your own IDs for your documents, number of documents and number of IDs must be the same.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Insert single text, and provide ID for it
rag.insert("TEXT1", ids=["ID_FOR_TEXT1"])

# Insert multiple texts, and provide IDs for them
rag.insert(["TEXT1", "TEXT2",...], ids=["ID_FOR_TEXT1", "ID_FOR_TEXT2"])
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Insert using Pipeline&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;The &lt;code&gt;apipeline_enqueue_documents&lt;/code&gt; and &lt;code&gt;apipeline_process_enqueue_documents&lt;/code&gt; functions allow you to perform incremental insertion of documents into the graph.&lt;/p&gt; 
 &lt;p&gt;This is useful for scenarios where you want to process documents in the background while still allowing the main thread to continue executing.&lt;/p&gt; 
 &lt;p&gt;And using a routine to process new documents.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;rag = LightRAG(..)

await rag.apipeline_enqueue_documents(input)
# Your routine in loop
await rag.apipeline_process_enqueue_documents(input)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Insert Multi-file Type Support&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;The &lt;code&gt;textract&lt;/code&gt; supports reading file types such as TXT, DOCX, PPTX, CSV, and PDF.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import textract

file_path = 'TEXT.pdf'
text_content = textract.process(file_path)

rag.insert(text_content.decode('utf-8'))
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Citation Functionality&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;By providing file paths, the system ensures that sources can be traced back to their original documents.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Define documents and their file paths
documents = ["Document content 1", "Document content 2"]
file_paths = ["path/to/doc1.txt", "path/to/doc2.txt"]

# Insert documents with file paths
rag.insert(documents, file_paths=file_paths)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Storage&lt;/h3&gt; 
&lt;p&gt;LightRAG uses 4 types of storage for different purposes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;KV_STORAGE: llm response cache, text chunks, document information&lt;/li&gt; 
 &lt;li&gt;VECTOR_STORAGE: entities vectors, relation vectors, chunks vectors&lt;/li&gt; 
 &lt;li&gt;GRAPH_STORAGE: entity relation graph&lt;/li&gt; 
 &lt;li&gt;DOC_STATUS_STORAGE: document indexing status&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Each storage type has several implementations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;KV_STORAGE supported implementations:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;JsonKVStorage    JsonFile (default)
PGKVStorage      Postgres
RedisKVStorage   Redis
MongoKVStorage   MongoDB
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;GRAPH_STORAGE supported implementations:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;NetworkXStorage      NetworkX (default)
Neo4JStorage         Neo4J
PGGraphStorage       PostgreSQL with AGE plugin
MemgraphStorage.     Memgraph
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Testing has shown that Neo4J delivers superior performance in production environments compared to PostgreSQL with AGE plugin.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;VECTOR_STORAGE supported implementations:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;NanoVectorDBStorage         NanoVector (default)
PGVectorStorage             Postgres
MilvusVectorDBStorage       Milvus
FaissVectorDBStorage        Faiss
QdrantVectorDBStorage       Qdrant
MongoVectorDBStorage        MongoDB
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;DOC_STATUS_STORAGE: supported implementations:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;JsonDocStatusStorage        JsonFile (default)
PGDocStatusStorage          Postgres
MongoDocStatusStorage       MongoDB
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Example connection configurations for each storage type can be found in the &lt;code&gt;env.example&lt;/code&gt; file. The database instance in the connection string needs to be created by you on the database server beforehand. LightRAG is only responsible for creating tables within the database instance, not for creating the database instance itself. If using Redis as storage, remember to configure automatic data persistence rules for Redis, otherwise data will be lost after the Redis service restarts. If using PostgreSQL, it is recommended to use version 16.6 or above.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Using Neo4J for Storage&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;For production level scenarios you will most likely want to leverage an enterprise solution&lt;/li&gt; 
  &lt;li&gt;for KG storage. Running Neo4J in Docker is recommended for seamless local testing.&lt;/li&gt; 
  &lt;li&gt;See: &lt;a href="https://hub.docker.com/_/neo4j"&gt;https://hub.docker.com/_/neo4j&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;export NEO4J_URI="neo4j://localhost:7687"
export NEO4J_USERNAME="neo4j"
export NEO4J_PASSWORD="password"

# Setup logger for LightRAG
setup_logger("lightrag", level="INFO")

# When you launch the project be sure to override the default KG: NetworkX
# by specifying kg="Neo4JStorage".

# Note: Default settings use NetworkX
# Initialize LightRAG with Neo4J implementation.
async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=gpt_4o_mini_complete,  # Use gpt_4o_mini_complete LLM model
        graph_storage="Neo4JStorage", #&amp;lt;-----------override KG default
    )

    # Initialize database connections
    await rag.initialize_storages()
    # Initialize pipeline status for document processing
    await initialize_pipeline_status()

    return rag
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;see test_neo4j.py for a working example.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Using PostgreSQL for Storage&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;For production level scenarios you will most likely want to leverage an enterprise solution. PostgreSQL can provide a one-stop solution for you as KV store, VectorDB (pgvector) and GraphDB (apache AGE). PostgreSQL version 16.6 or higher is supported.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;PostgreSQL is lightweight,the whole binary distribution including all necessary plugins can be zipped to 40MB: Ref to &lt;a href="https://github.com/ShanGor/apache-age-windows/releases/tag/PG17%2Fv1.5.0-rc0"&gt;Windows Release&lt;/a&gt; as it is easy to install for Linux/Mac.&lt;/li&gt; 
  &lt;li&gt;If you prefer docker, please start with this image if you are a beginner to avoid hiccups (DO read the overview): &lt;a href="https://hub.docker.com/r/shangor/postgres-for-rag"&gt;https://hub.docker.com/r/shangor/postgres-for-rag&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;How to start? Ref to: &lt;a href="https://github.com/HKUDS/LightRAG/raw/main/examples/lightrag_zhipu_postgres_demo.py"&gt;examples/lightrag_zhipu_postgres_demo.py&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;For high-performance graph database requirements, Neo4j is recommended as Apache AGE's performance is not as competitive.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Using Faiss for Storage&lt;/b&gt; &lt;/summary&gt; Before using Faiss vector database, you must manually install `faiss-cpu` or `faiss-gpu`. 
 &lt;ul&gt; 
  &lt;li&gt;Install the required dependencies:&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code&gt;pip install faiss-cpu
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;You can also install &lt;code&gt;faiss-gpu&lt;/code&gt; if you have GPU support.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Here we are using &lt;code&gt;sentence-transformers&lt;/code&gt; but you can also use &lt;code&gt;OpenAIEmbedding&lt;/code&gt; model with &lt;code&gt;3072&lt;/code&gt; dimensions.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;async def embedding_func(texts: list[str]) -&amp;gt; np.ndarray:
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode(texts, convert_to_numpy=True)
    return embeddings

# Initialize LightRAG with the LLM model function and embedding function
rag = LightRAG(
    working_dir=WORKING_DIR,
    llm_model_func=llm_model_func,
    embedding_func=EmbeddingFunc(
        embedding_dim=384,
        func=embedding_func,
    ),
    vector_storage="FaissVectorDBStorage",
    vector_db_storage_cls_kwargs={
        "cosine_better_than_threshold": 0.3  # Your desired threshold
    }
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Using Memgraph for Storage&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Memgraph is a high-performance, in-memory graph database compatible with the Neo4j Bolt protocol.&lt;/li&gt; 
  &lt;li&gt;You can run Memgraph locally using Docker for easy testing:&lt;/li&gt; 
  &lt;li&gt;See: &lt;a href="https://memgraph.com/download"&gt;https://memgraph.com/download&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;export MEMGRAPH_URI="bolt://localhost:7687"

# Setup logger for LightRAG
setup_logger("lightrag", level="INFO")

# When you launch the project, override the default KG: NetworkX
# by specifying kg="MemgraphStorage".

# Note: Default settings use NetworkX
# Initialize LightRAG with Memgraph implementation.
async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=gpt_4o_mini_complete,  # Use gpt_4o_mini_complete LLM model
        graph_storage="MemgraphStorage", #&amp;lt;-----------override KG default
    )

    # Initialize database connections
    await rag.initialize_storages()
    # Initialize pipeline status for document processing
    await initialize_pipeline_status()

    return rag
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Data Isolation Between LightRAG Instances&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;workspace&lt;/code&gt; parameter ensures data isolation between different LightRAG instances. Once initialized, the &lt;code&gt;workspace&lt;/code&gt; is immutable and cannot be changed.Here is how workspaces are implemented for different types of storage:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;For local file-based databases, data isolation is achieved through workspace subdirectories:&lt;/strong&gt; &lt;code&gt;JsonKVStorage&lt;/code&gt;, &lt;code&gt;JsonDocStatusStorage&lt;/code&gt;, &lt;code&gt;NetworkXStorage&lt;/code&gt;, &lt;code&gt;NanoVectorDBStorage&lt;/code&gt;, &lt;code&gt;FaissVectorDBStorage&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;For databases that store data in collections, it's done by adding a workspace prefix to the collection name:&lt;/strong&gt; &lt;code&gt;RedisKVStorage&lt;/code&gt;, &lt;code&gt;RedisDocStatusStorage&lt;/code&gt;, &lt;code&gt;MilvusVectorDBStorage&lt;/code&gt;, &lt;code&gt;QdrantVectorDBStorage&lt;/code&gt;, &lt;code&gt;MongoKVStorage&lt;/code&gt;, &lt;code&gt;MongoDocStatusStorage&lt;/code&gt;, &lt;code&gt;MongoVectorDBStorage&lt;/code&gt;, &lt;code&gt;MongoGraphStorage&lt;/code&gt;, &lt;code&gt;PGGraphStorage&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;For relational databases, data isolation is achieved by adding a &lt;code&gt;workspace&lt;/code&gt; field to the tables for logical data separation:&lt;/strong&gt; &lt;code&gt;PGKVStorage&lt;/code&gt;, &lt;code&gt;PGVectorStorage&lt;/code&gt;, &lt;code&gt;PGDocStatusStorage&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;For the Neo4j graph database, logical data isolation is achieved through labels:&lt;/strong&gt; &lt;code&gt;Neo4JStorage&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To maintain compatibility with legacy data, the default workspace for PostgreSQL non-graph storage is &lt;code&gt;default&lt;/code&gt; and, for PostgreSQL AGE graph storage is null, for Neo4j graph storage is &lt;code&gt;base&lt;/code&gt; when no workspace is configured. For all external storages, the system provides dedicated workspace environment variables to override the common &lt;code&gt;WORKSPACE&lt;/code&gt; environment variable configuration. These storage-specific workspace environment variables are: &lt;code&gt;REDIS_WORKSPACE&lt;/code&gt;, &lt;code&gt;MILVUS_WORKSPACE&lt;/code&gt;, &lt;code&gt;QDRANT_WORKSPACE&lt;/code&gt;, &lt;code&gt;MONGODB_WORKSPACE&lt;/code&gt;, &lt;code&gt;POSTGRES_WORKSPACE&lt;/code&gt;, &lt;code&gt;NEO4J_WORKSPACE&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Edit Entities and Relations&lt;/h2&gt; 
&lt;p&gt;LightRAG now supports comprehensive knowledge graph management capabilities, allowing you to create, edit, and delete entities and relationships within your knowledge graph.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt; Create Entities and Relations &lt;/b&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Create new entity
entity = rag.create_entity("Google", {
    "description": "Google is a multinational technology company specializing in internet-related services and products.",
    "entity_type": "company"
})

# Create another entity
product = rag.create_entity("Gmail", {
    "description": "Gmail is an email service developed by Google.",
    "entity_type": "product"
})

# Create relation between entities
relation = rag.create_relation("Google", "Gmail", {
    "description": "Google develops and operates Gmail.",
    "keywords": "develops operates service",
    "weight": 2.0
})
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt; Edit Entities and Relations &lt;/b&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Edit an existing entity
updated_entity = rag.edit_entity("Google", {
    "description": "Google is a subsidiary of Alphabet Inc., founded in 1998.",
    "entity_type": "tech_company"
})

# Rename an entity (with all its relationships properly migrated)
renamed_entity = rag.edit_entity("Gmail", {
    "entity_name": "Google Mail",
    "description": "Google Mail (formerly Gmail) is an email service."
})

# Edit a relation between entities
updated_relation = rag.edit_relation("Google", "Google Mail", {
    "description": "Google created and maintains Google Mail service.",
    "keywords": "creates maintains email service",
    "weight": 3.0
})
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;All operations are available in both synchronous and asynchronous versions. The asynchronous versions have the prefix "a" (e.g., &lt;code&gt;acreate_entity&lt;/code&gt;, &lt;code&gt;aedit_relation&lt;/code&gt;).&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt; Insert Custom KG &lt;/b&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;custom_kg = {
        "chunks": [
            {
                "content": "Alice and Bob are collaborating on quantum computing research.",
                "source_id": "doc-1",
                "file_path": "test_file",
            }
        ],
        "entities": [
            {
                "entity_name": "Alice",
                "entity_type": "person",
                "description": "Alice is a researcher specializing in quantum physics.",
                "source_id": "doc-1",
                "file_path": "test_file"
            },
            {
                "entity_name": "Bob",
                "entity_type": "person",
                "description": "Bob is a mathematician.",
                "source_id": "doc-1",
                "file_path": "test_file"
            },
            {
                "entity_name": "Quantum Computing",
                "entity_type": "technology",
                "description": "Quantum computing utilizes quantum mechanical phenomena for computation.",
                "source_id": "doc-1",
                "file_path": "test_file"
            }
        ],
        "relationships": [
            {
                "src_id": "Alice",
                "tgt_id": "Bob",
                "description": "Alice and Bob are research partners.",
                "keywords": "collaboration research",
                "weight": 1.0,
                "source_id": "doc-1",
                "file_path": "test_file"
            },
            {
                "src_id": "Alice",
                "tgt_id": "Quantum Computing",
                "description": "Alice conducts research on quantum computing.",
                "keywords": "research expertise",
                "weight": 1.0,
                "source_id": "doc-1",
                "file_path": "test_file"
            },
            {
                "src_id": "Bob",
                "tgt_id": "Quantum Computing",
                "description": "Bob researches quantum computing.",
                "keywords": "research application",
                "weight": 1.0,
                "source_id": "doc-1",
                "file_path": "test_file"
            }
        ]
    }

rag.insert_custom_kg(custom_kg)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Other Entity and Relation Operations&lt;/b&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;create_entity&lt;/strong&gt;: Creates a new entity with specified attributes&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;edit_entity&lt;/strong&gt;: Updates an existing entity's attributes or renames it&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;create_relation&lt;/strong&gt;: Creates a new relation between existing entities&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;edit_relation&lt;/strong&gt;: Updates an existing relation's attributes&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;These operations maintain data consistency across both the graph database and vector database components, ensuring your knowledge graph remains coherent.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Delete Functions&lt;/h2&gt; 
&lt;p&gt;LightRAG provides comprehensive deletion capabilities, allowing you to delete documents, entities, and relationships.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Delete Entities&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;You can delete entities by their name along with all associated relationships:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Delete entity and all its relationships (synchronous version)
rag.delete_by_entity("Google")

# Asynchronous version
await rag.adelete_by_entity("Google")
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;When deleting an entity:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Removes the entity node from the knowledge graph&lt;/li&gt; 
  &lt;li&gt;Deletes all associated relationships&lt;/li&gt; 
  &lt;li&gt;Removes related embedding vectors from the vector database&lt;/li&gt; 
  &lt;li&gt;Maintains knowledge graph integrity&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Delete Relations&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;You can delete relationships between two specific entities:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Delete relationship between two entities (synchronous version)
rag.delete_by_relation("Google", "Gmail")

# Asynchronous version
await rag.adelete_by_relation("Google", "Gmail")
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;When deleting a relationship:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Removes the specified relationship edge&lt;/li&gt; 
  &lt;li&gt;Deletes the relationship's embedding vector from the vector database&lt;/li&gt; 
  &lt;li&gt;Preserves both entity nodes and their other relationships&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Delete by Document ID&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;You can delete an entire document and all its related knowledge through document ID:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Delete by document ID (asynchronous version)
await rag.adelete_by_doc_id("doc-12345")
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Optimized processing when deleting by document ID:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Smart Cleanup&lt;/strong&gt;: Automatically identifies and removes entities and relationships that belong only to this document&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Preserve Shared Knowledge&lt;/strong&gt;: If entities or relationships exist in other documents, they are preserved and their descriptions are rebuilt&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Cache Optimization&lt;/strong&gt;: Clears related LLM cache to reduce storage overhead&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Incremental Rebuilding&lt;/strong&gt;: Reconstructs affected entity and relationship descriptions from remaining documents&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;The deletion process includes:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Delete all text chunks related to the document&lt;/li&gt; 
  &lt;li&gt;Identify and delete entities and relationships that belong only to this document&lt;/li&gt; 
  &lt;li&gt;Rebuild entities and relationships that still exist in other documents&lt;/li&gt; 
  &lt;li&gt;Update all related vector indexes&lt;/li&gt; 
  &lt;li&gt;Clean up document status records&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;Note: Deletion by document ID is an asynchronous operation as it involves complex knowledge graph reconstruction processes.&lt;/p&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;strong&gt;Important Reminders:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Irreversible Operations&lt;/strong&gt;: All deletion operations are irreversible, please use with caution&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Performance Considerations&lt;/strong&gt;: Deleting large amounts of data may take some time, especially deletion by document ID&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Consistency&lt;/strong&gt;: Deletion operations automatically maintain consistency between the knowledge graph and vector database&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backup Recommendations&lt;/strong&gt;: Consider backing up data before performing important deletion operations&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Batch Deletion Recommendations:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For batch deletion operations, consider using asynchronous methods for better performance&lt;/li&gt; 
 &lt;li&gt;For large-scale deletions, consider processing in batches to avoid excessive system load&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Entity Merging&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Merge Entities and Their Relationships&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;LightRAG now supports merging multiple entities into a single entity, automatically handling all relationships:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Basic entity merging
rag.merge_entities(
    source_entities=["Artificial Intelligence", "AI", "Machine Intelligence"],
    target_entity="AI Technology"
)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;With custom merge strategy:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Define custom merge strategy for different fields
rag.merge_entities(
    source_entities=["John Smith", "Dr. Smith", "J. Smith"],
    target_entity="John Smith",
    merge_strategy={
        "description": "concatenate",  # Combine all descriptions
        "entity_type": "keep_first",   # Keep the entity type from the first entity
        "source_id": "join_unique"     # Combine all unique source IDs
    }
)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;With custom target entity data:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Specify exact values for the merged entity
rag.merge_entities(
    source_entities=["New York", "NYC", "Big Apple"],
    target_entity="New York City",
    target_entity_data={
        "entity_type": "LOCATION",
        "description": "New York City is the most populous city in the United States.",
    }
)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Advanced usage combining both approaches:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Merge company entities with both strategy and custom data
rag.merge_entities(
    source_entities=["Microsoft Corp", "Microsoft Corporation", "MSFT"],
    target_entity="Microsoft",
    merge_strategy={
        "description": "concatenate",  # Combine all descriptions
        "source_id": "join_unique"     # Combine source IDs
    },
    target_entity_data={
        "entity_type": "ORGANIZATION",
    }
)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;When merging entities:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;All relationships from source entities are redirected to the target entity&lt;/li&gt; 
  &lt;li&gt;Duplicate relationships are intelligently merged&lt;/li&gt; 
  &lt;li&gt;Self-relationships (loops) are prevented&lt;/li&gt; 
  &lt;li&gt;Source entities are removed after merging&lt;/li&gt; 
  &lt;li&gt;Relationship weights and attributes are preserved&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Multimodal Document Processing (RAG-Anything Integration)&lt;/h2&gt; 
&lt;p&gt;LightRAG now seamlessly integrates with &lt;a href="https://github.com/HKUDS/RAG-Anything"&gt;RAG-Anything&lt;/a&gt;, a comprehensive &lt;strong&gt;All-in-One Multimodal Document Processing RAG system&lt;/strong&gt; built specifically for LightRAG. RAG-Anything enables advanced parsing and retrieval-augmented generation (RAG) capabilities, allowing you to handle multimodal documents seamlessly and extract structured contentâ€”including text, images, tables, and formulasâ€”from various document formats for integration into your RAG pipeline.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;End-to-End Multimodal Pipeline&lt;/strong&gt;: Complete workflow from document ingestion and parsing to intelligent multimodal query answering&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Universal Document Support&lt;/strong&gt;: Seamless processing of PDFs, Office documents (DOC/DOCX/PPT/PPTX/XLS/XLSX), images, and diverse file formats&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Specialized Content Analysis&lt;/strong&gt;: Dedicated processors for images, tables, mathematical equations, and heterogeneous content types&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multimodal Knowledge Graph&lt;/strong&gt;: Automatic entity extraction and cross-modal relationship discovery for enhanced understanding&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Hybrid Intelligent Retrieval&lt;/strong&gt;: Advanced search capabilities spanning textual and multimodal content with contextual understanding&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Quick Start:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install RAG-Anything:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install raganything
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Process multimodal documents:&lt;/p&gt; 
  &lt;details&gt; 
   &lt;summary&gt; &lt;b&gt; RAGAnything Usage Example &lt;/b&gt;&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class="language-python"&gt;    import asyncio
    from raganything import RAGAnything
    from lightrag import LightRAG
    from lightrag.llm.openai import openai_complete_if_cache, openai_embed
    from lightrag.utils import EmbeddingFunc
    import os

    async def load_existing_lightrag():
        # First, create or load an existing LightRAG instance
        lightrag_working_dir = "./existing_lightrag_storage"

        # Check if previous LightRAG instance exists
        if os.path.exists(lightrag_working_dir) and os.listdir(lightrag_working_dir):
            print("âœ… Found existing LightRAG instance, loading...")
        else:
            print("âŒ No existing LightRAG instance found, will create new one")

        # Create/Load LightRAG instance with your configurations
        lightrag_instance = LightRAG(
            working_dir=lightrag_working_dir,
            llm_model_func=lambda prompt, system_prompt=None, history_messages=[], **kwargs: openai_complete_if_cache(
                "gpt-4o-mini",
                prompt,
                system_prompt=system_prompt,
                history_messages=history_messages,
                api_key="your-api-key",
                **kwargs,
            ),
            embedding_func=EmbeddingFunc(
                embedding_dim=3072,
                func=lambda texts: openai_embed(
                    texts,
                    model="text-embedding-3-large",
                    api_key=api_key,
                    base_url=base_url,
                ),
            )
        )
        # Initialize storage (this will load existing data if available)
        await lightrag_instance.initialize_storages()
        # Now initialize RAGAnything with the existing LightRAG instance
        rag = RAGAnything(
            lightrag=lightrag_instance,  # Pass the existing LightRAG instance
            # Only need vision model for multimodal processing
            vision_model_func=lambda prompt, system_prompt=None, history_messages=[], image_data=None, **kwargs: openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=[
                    {"role": "system", "content": system_prompt} if system_prompt else None,
                    {"role": "user", "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
                    ]} if image_data else {"role": "user", "content": prompt}
                ],
                api_key="your-api-key",
                **kwargs,
            ) if image_data else openai_complete_if_cache(
                "gpt-4o-mini",
                prompt,
                system_prompt=system_prompt,
                history_messages=history_messages,
                api_key="your-api-key",
                **kwargs,
            )
            # Note: working_dir, llm_model_func, embedding_func, etc. are inherited from lightrag_instance
        )
        # Query the existing knowledge base
        result = await rag.query_with_multimodal(
            "What data has been processed in this LightRAG instance?",
            mode="hybrid"
        )
        print("Query result:", result)
        # Add new multimodal documents to the existing LightRAG instance
        await rag.process_document_complete(
            file_path="path/to/new/multimodal_document.pdf",
            output_dir="./output"
        )

    if __name__ == "__main__":
        asyncio.run(load_existing_lightrag())
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;For detailed documentation and advanced usage, please refer to the &lt;a href="https://github.com/HKUDS/RAG-Anything"&gt;RAG-Anything repository&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Token Usage Tracking&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Overview and Usage&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;LightRAG provides a TokenTracker tool to monitor and manage token consumption by large language models. This feature is particularly useful for controlling API costs and optimizing performance.&lt;/p&gt; 
 &lt;h3&gt;Usage&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from lightrag.utils import TokenTracker

# Create TokenTracker instance
token_tracker = TokenTracker()

# Method 1: Using context manager (Recommended)
# Suitable for scenarios requiring automatic token usage tracking
with token_tracker:
    result1 = await llm_model_func("your question 1")
    result2 = await llm_model_func("your question 2")

# Method 2: Manually adding token usage records
# Suitable for scenarios requiring more granular control over token statistics
token_tracker.reset()

rag.insert()

rag.query("your question 1", param=QueryParam(mode="naive"))
rag.query("your question 2", param=QueryParam(mode="mix"))

# Display total token usage (including insert and query operations)
print("Token usage:", token_tracker.get_usage())
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Usage Tips&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Use context managers for long sessions or batch operations to automatically track all token consumption&lt;/li&gt; 
  &lt;li&gt;For scenarios requiring segmented statistics, use manual mode and call reset() when appropriate&lt;/li&gt; 
  &lt;li&gt;Regular checking of token usage helps detect abnormal consumption early&lt;/li&gt; 
  &lt;li&gt;Actively use this feature during development and testing to optimize production costs&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;Practical Examples&lt;/h3&gt; 
 &lt;p&gt;You can refer to these examples for implementing token tracking:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;examples/lightrag_gemini_track_token_demo.py&lt;/code&gt;: Token tracking example using Google Gemini model&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;examples/lightrag_siliconcloud_track_token_demo.py&lt;/code&gt;: Token tracking example using SiliconCloud model&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;These examples demonstrate how to effectively use the TokenTracker feature with different models and scenarios.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Data Export Functions&lt;/h2&gt; 
&lt;h3&gt;Overview&lt;/h3&gt; 
&lt;p&gt;LightRAG allows you to export your knowledge graph data in various formats for analysis, sharing, and backup purposes. The system supports exporting entities, relations, and relationship data.&lt;/p&gt; 
&lt;h3&gt;Export Functions&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt; Basic Usage &lt;/b&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Basic CSV export (default format)
rag.export_data("knowledge_graph.csv")

# Specify any format
rag.export_data("output.xlsx", file_format="excel")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt; Different File Formats supported &lt;/b&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;#Export data in CSV format
rag.export_data("graph_data.csv", file_format="csv")

# Export data in Excel sheet
rag.export_data("graph_data.xlsx", file_format="excel")

# Export data in markdown format
rag.export_data("graph_data.md", file_format="md")

# Export data in Text
rag.export_data("graph_data.txt", file_format="txt")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt; Additional Options &lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Include vector embeddings in the export (optional):&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;rag.export_data("complete_data.csv", include_vector_data=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Data Included in Export&lt;/h3&gt; 
&lt;p&gt;All exports include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Entity information (names, IDs, metadata)&lt;/li&gt; 
 &lt;li&gt;Relation data (connections between entities)&lt;/li&gt; 
 &lt;li&gt;Relationship information from vector database&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Cache&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Clear Cache&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;You can clear the LLM response cache with different modes:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Clear all cache
await rag.aclear_cache()

# Clear local mode cache
await rag.aclear_cache(modes=["local"])

# Clear extraction cache
await rag.aclear_cache(modes=["default"])

# Clear multiple modes
await rag.aclear_cache(modes=["local", "global", "hybrid"])

# Synchronous version
rag.clear_cache(modes=["local"])
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Valid modes are:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;"default"&lt;/code&gt;: Extraction cache&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;"naive"&lt;/code&gt;: Naive search cache&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;"local"&lt;/code&gt;: Local search cache&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;"global"&lt;/code&gt;: Global search cache&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;"hybrid"&lt;/code&gt;: Hybrid search cache&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;"mix"&lt;/code&gt;: Mix search cache&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;h3&gt;Common Initialization Errors&lt;/h3&gt; 
&lt;p&gt;If you encounter these errors when using LightRAG:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;AttributeError: __aenter__&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Cause&lt;/strong&gt;: Storage backends not initialized&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;: Call &lt;code&gt;await rag.initialize_storages()&lt;/code&gt; after creating the LightRAG instance&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;KeyError: 'history_messages'&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Cause&lt;/strong&gt;: Pipeline status not initialized&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;: Call &lt;code&gt;await initialize_pipeline_status()&lt;/code&gt; after initializing storages&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Both errors in sequence&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Cause&lt;/strong&gt;: Neither initialization method was called&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;: Always follow this pattern:&lt;/li&gt; 
  &lt;/ul&gt; &lt;pre&gt;&lt;code class="language-python"&gt;rag = LightRAG(...)
await rag.initialize_storages()
await initialize_pipeline_status()
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Model Switching Issues&lt;/h3&gt; 
&lt;p&gt;When switching between different embedding models, you must clear the data directory to avoid errors. The only file you may want to preserve is &lt;code&gt;kv_store_llm_response_cache.json&lt;/code&gt; if you wish to retain the LLM cache.&lt;/p&gt; 
&lt;h2&gt;LightRAG API&lt;/h2&gt; 
&lt;p&gt;The LightRAG Server is designed to provide Web UI and API support. &lt;strong&gt;For more information about LightRAG Server, please refer to &lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/lightrag/api/README.md"&gt;LightRAG Server&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Graph Visualization&lt;/h2&gt; 
&lt;p&gt;The LightRAG Server offers a comprehensive knowledge graph visualization feature. It supports various gravity layouts, node queries, subgraph filtering, and more. &lt;strong&gt;For more information about LightRAG Server, please refer to &lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/lightrag/api/README.md"&gt;LightRAG Server&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/HKUDS/LightRAG/main/README.assets/iShot_2025-03-23_12.40.08.png" alt="iShot_2025-03-23_12.40.08" /&gt;&lt;/p&gt; 
&lt;h2&gt;Evaluation&lt;/h2&gt; 
&lt;h3&gt;Dataset&lt;/h3&gt; 
&lt;p&gt;The dataset used in LightRAG can be downloaded from &lt;a href="https://huggingface.co/datasets/TommyChien/UltraDomain"&gt;TommyChien/UltraDomain&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Generate Query&lt;/h3&gt; 
&lt;p&gt;LightRAG uses the following prompt to generate high-level queries, with the corresponding code in &lt;code&gt;example/generate_query.py&lt;/code&gt;.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; Prompt &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;Given the following description of a dataset:

{description}

Please identify 5 potential users who would engage with this dataset. For each user, list 5 tasks they would perform with this dataset. Then, for each (user, task) combination, generate 5 questions that require a high-level understanding of the entire dataset.

Output the results in the following structure:
- User 1: [user description]
    - Task 1: [task description]
        - Question 1:
        - Question 2:
        - Question 3:
        - Question 4:
        - Question 5:
    - Task 2: [task description]
        ...
    - Task 5: [task description]
- User 2: [user description]
    ...
- User 5: [user description]
    ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Batch Eval&lt;/h3&gt; 
&lt;p&gt;To evaluate the performance of two RAG systems on high-level queries, LightRAG uses the following prompt, with the specific code available in &lt;code&gt;reproduce/batch_eval.py&lt;/code&gt;.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; Prompt &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;---Role---
You are an expert tasked with evaluating two answers to the same question based on three criteria: **Comprehensiveness**, **Diversity**, and **Empowerment**.
---Goal---
You will evaluate two answers to the same question based on three criteria: **Comprehensiveness**, **Diversity**, and **Empowerment**.

- **Comprehensiveness**: How much detail does the answer provide to cover all aspects and details of the question?
- **Diversity**: How varied and rich is the answer in providing different perspectives and insights on the question?
- **Empowerment**: How well does the answer help the reader understand and make informed judgments about the topic?

For each criterion, choose the better answer (either Answer 1 or Answer 2) and explain why. Then, select an overall winner based on these three categories.

Here is the question:
{query}

Here are the two answers:

**Answer 1:**
{answer1}

**Answer 2:**
{answer2}

Evaluate both answers using the three criteria listed above and provide detailed explanations for each criterion.

Output your evaluation in the following JSON format:

{{
    "Comprehensiveness": {{
        "Winner": "[Answer 1 or Answer 2]",
        "Explanation": "[Provide explanation here]"
    }},
    "Empowerment": {{
        "Winner": "[Answer 1 or Answer 2]",
        "Explanation": "[Provide explanation here]"
    }},
    "Overall Winner": {{
        "Winner": "[Answer 1 or Answer 2]",
        "Explanation": "[Summarize why this answer is the overall winner based on the three criteria]"
    }}
}}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Overall Performance Table&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Agriculture&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;CS&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Legal&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Mix&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;NaiveRAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;NaiveRAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;NaiveRAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;NaiveRAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Comprehensiveness&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;32.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;67.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;38.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;61.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;16.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;83.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;38.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;61.2%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Diversity&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;23.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;76.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;38.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;62.0%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;13.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;86.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;32.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;67.6%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Empowerment&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;32.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;67.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;38.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;61.2%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;16.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;83.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;42.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;57.2%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Overall&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;32.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;67.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;38.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;61.2%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;15.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;84.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;40.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;60.0%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;RQ-RAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;RQ-RAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;RQ-RAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;RQ-RAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Comprehensiveness&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;31.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;68.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;38.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;61.2%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;15.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;84.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;39.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;60.8%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Diversity&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;29.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;70.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;39.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;60.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;11.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;88.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;30.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;69.2%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Empowerment&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;31.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;68.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;36.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;63.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;15.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;84.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;42.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;57.6%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Overall&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;32.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;67.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;38.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;62.0%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;14.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;85.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;40.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;60.0%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;HyDE&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;HyDE&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;HyDE&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;HyDE&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Comprehensiveness&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;26.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;74.0%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;41.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;58.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;26.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;73.2%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;40.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;59.6%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Diversity&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;24.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;76.0%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;38.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;61.2%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;20.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;80.0%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;32.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;67.6%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Empowerment&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;25.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;74.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;40.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;59.2%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;26.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;74.0%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;46.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;54.0%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Overall&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;24.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;75.2%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;41.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;58.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;26.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;73.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;42.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;57.6%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;GraphRAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GraphRAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GraphRAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GraphRAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Comprehensiveness&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;45.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;54.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;48.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;51.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;48.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;51.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;50.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;49.6%&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Diversity&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;22.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;77.2%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;40.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;59.2%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;26.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;73.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;36.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;64.0%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Empowerment&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;41.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;58.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;45.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;54.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;43.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;56.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;50.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;49.2%&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Overall&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;45.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;54.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;48.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;52.0%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;47.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;52.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;50.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;49.6%&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Reproduce&lt;/h2&gt; 
&lt;p&gt;All the code can be found in the &lt;code&gt;./reproduce&lt;/code&gt; directory.&lt;/p&gt; 
&lt;h3&gt;Step-0 Extract Unique Contexts&lt;/h3&gt; 
&lt;p&gt;First, we need to extract unique contexts in the datasets.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; Code &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;def extract_unique_contexts(input_directory, output_directory):

    os.makedirs(output_directory, exist_ok=True)

    jsonl_files = glob.glob(os.path.join(input_directory, '*.jsonl'))
    print(f"Found {len(jsonl_files)} JSONL files.")

    for file_path in jsonl_files:
        filename = os.path.basename(file_path)
        name, ext = os.path.splitext(filename)
        output_filename = f"{name}_unique_contexts.json"
        output_path = os.path.join(output_directory, output_filename)

        unique_contexts_dict = {}

        print(f"Processing file: {filename}")

        try:
            with open(file_path, 'r', encoding='utf-8') as infile:
                for line_number, line in enumerate(infile, start=1):
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        json_obj = json.loads(line)
                        context = json_obj.get('context')
                        if context and context not in unique_contexts_dict:
                            unique_contexts_dict[context] = None
                    except json.JSONDecodeError as e:
                        print(f"JSON decoding error in file {filename} at line {line_number}: {e}")
        except FileNotFoundError:
            print(f"File not found: {filename}")
            continue
        except Exception as e:
            print(f"An error occurred while processing file {filename}: {e}")
            continue

        unique_contexts_list = list(unique_contexts_dict.keys())
        print(f"There are {len(unique_contexts_list)} unique `context` entries in the file {filename}.")

        try:
            with open(output_path, 'w', encoding='utf-8') as outfile:
                json.dump(unique_contexts_list, outfile, ensure_ascii=False, indent=4)
            print(f"Unique `context` entries have been saved to: {output_filename}")
        except Exception as e:
            print(f"An error occurred while saving to the file {output_filename}: {e}")

    print("All files have been processed.")

&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Step-1 Insert Contexts&lt;/h3&gt; 
&lt;p&gt;For the extracted contexts, we insert them into the LightRAG system.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; Code &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;def insert_text(rag, file_path):
    with open(file_path, mode='r') as f:
        unique_contexts = json.load(f)

    retries = 0
    max_retries = 3
    while retries &amp;lt; max_retries:
        try:
            rag.insert(unique_contexts)
            break
        except Exception as e:
            retries += 1
            print(f"Insertion failed, retrying ({retries}/{max_retries}), error: {e}")
            time.sleep(10)
    if retries == max_retries:
        print("Insertion failed after exceeding the maximum number of retries")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Step-2 Generate Queries&lt;/h3&gt; 
&lt;p&gt;We extract tokens from the first and the second half of each context in the dataset, then combine them as dataset descriptions to generate queries.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; Code &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

def get_summary(context, tot_tokens=2000):
    tokens = tokenizer.tokenize(context)
    half_tokens = tot_tokens // 2

    start_tokens = tokens[1000:1000 + half_tokens]
    end_tokens = tokens[-(1000 + half_tokens):1000]

    summary_tokens = start_tokens + end_tokens
    summary = tokenizer.convert_tokens_to_string(summary_tokens)

    return summary
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Step-3 Query&lt;/h3&gt; 
&lt;p&gt;For the queries generated in Step-2, we will extract them and query LightRAG.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; Code &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;def extract_queries(file_path):
    with open(file_path, 'r') as f:
        data = f.read()

    data = data.replace('**', '')

    queries = re.findall(r'- Question \d+: (.+)', data)

    return queries
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸ”— Related Projects&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Ecosystem &amp;amp; Extensions&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center"&gt; &lt;a href="https://github.com/HKUDS/RAG-Anything"&gt; 
      &lt;div style="width: 100px; height: 100px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2); display: flex; align-items: center; justify-content: center; margin-bottom: 10px;"&gt; 
       &lt;span style="font-size: 32px;"&gt;ğŸ“¸&lt;/span&gt; 
      &lt;/div&gt; &lt;b&gt;RAG-Anything&lt;/b&gt;&lt;br /&gt; &lt;sub&gt;Multimodal RAG&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; 
    &lt;td align="center"&gt; &lt;a href="https://github.com/HKUDS/VideoRAG"&gt; 
      &lt;div style="width: 100px; height: 100px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2); display: flex; align-items: center; justify-content: center; margin-bottom: 10px;"&gt; 
       &lt;span style="font-size: 32px;"&gt;ğŸ¥&lt;/span&gt; 
      &lt;/div&gt; &lt;b&gt;VideoRAG&lt;/b&gt;&lt;br /&gt; &lt;sub&gt;Extreme Long-Context Video RAG&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; 
    &lt;td align="center"&gt; &lt;a href="https://github.com/HKUDS/MiniRAG"&gt; 
      &lt;div style="width: 100px; height: 100px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2); display: flex; align-items: center; justify-content: center; margin-bottom: 10px;"&gt; 
       &lt;span style="font-size: 32px;"&gt;âœ¨&lt;/span&gt; 
      &lt;/div&gt; &lt;b&gt;MiniRAG&lt;/b&gt;&lt;br /&gt; &lt;sub&gt;Extremely Simple RAG&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;â­ Star History&lt;/h2&gt; 
&lt;a href="https://star-history.com/#HKUDS/LightRAG&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=HKUDS/LightRAG&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=HKUDS/LightRAG&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=HKUDS/LightRAG&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;h2&gt;ğŸ¤ Contribution&lt;/h2&gt; 
&lt;div align="center"&gt;
  We thank all our contributors for their valuable contributions. 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/HKUDS/LightRAG/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=HKUDS/LightRAG" style="border-radius: 15px; box-shadow: 0 0 20px rgba(0, 217, 255, 0.3);" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ“– Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@article{guo2024lightrag,
title={LightRAG: Simple and Fast Retrieval-Augmented Generation},
author={Zirui Guo and Lianghao Xia and Yanhua Yu and Tu Ao and Chao Huang},
year={2024},
eprint={2410.05779},
archivePrefix={arXiv},
primaryClass={cs.IR}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;div align="center" style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; padding: 30px; margin: 30px 0;"&gt; 
 &lt;div&gt; 
  &lt;img src="https://user-images.githubusercontent.com/74038190/212284100-561aa473-3905-4a80-b561-0d28506553ee.gif" width="500" /&gt; 
 &lt;/div&gt; 
 &lt;div style="margin-top: 20px;"&gt; 
  &lt;a href="https://github.com/HKUDS/LightRAG" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/â­%20Star%20us%20on%20GitHub-1a1a2e?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" /&gt; &lt;/a&gt; 
  &lt;a href="https://github.com/HKUDS/LightRAG/issues" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/ğŸ›%20Report%20Issues-ff6b6b?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" /&gt; &lt;/a&gt; 
  &lt;a href="https://github.com/HKUDS/LightRAG/discussions" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/ğŸ’¬%20Discussions-4ecdc4?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" /&gt; &lt;/a&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;div style="width: 100%; max-width: 600px; margin: 20px auto; padding: 20px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2);"&gt; 
  &lt;div style="display: flex; justify-content: center; align-items: center; gap: 15px;"&gt; 
   &lt;span style="font-size: 24px;"&gt;â­&lt;/span&gt; 
   &lt;span style="color: #00d9ff; font-size: 18px;"&gt;Thank you for visiting LightRAG!&lt;/span&gt; 
   &lt;span style="font-size: 24px;"&gt;â­&lt;/span&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>gpustack/gpustack</title>
      <link>https://github.com/gpustack/gpustack</link>
      <description>&lt;p&gt;Simple, scalable AI model deployment on GPU clusters&lt;/p&gt;&lt;hr&gt;&lt;br /&gt; 
&lt;p align="center"&gt; &lt;img alt="GPUStack" src="https://raw.githubusercontent.com/gpustack/gpustack/main/docs/assets/gpustack-logo.png" width="300px" /&gt; &lt;/p&gt; 
&lt;br /&gt; 
&lt;p align="center"&gt; &lt;a href="https://docs.gpustack.ai" target="_blank"&gt; &lt;img alt="Documentation" src="https://img.shields.io/badge/Docs-GPUStack-blue?logo=readthedocs&amp;amp;logoColor=white" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/gpustack/gpustack/main/LICENSE" target="_blank"&gt; &lt;img alt="License" src="https://img.shields.io/github/license/gpustack/gpustack?logo=github&amp;amp;logoColor=white&amp;amp;label=License&amp;amp;color=blue" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/gpustack/gpustack/main/docs/assets/wechat-group-qrcode.jpg" target="_blank"&gt; &lt;img alt="WeChat" src="https://img.shields.io/badge/å¾®ä¿¡ç¾¤-GPUStack-blue?logo=wechat&amp;amp;logoColor=white" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/VXYJzuaqwD" target="_blank"&gt; &lt;img alt="Discord" src="https://img.shields.io/badge/Discord-GPUStack-blue?logo=discord&amp;amp;logoColor=white" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/intent/follow?screen_name=gpustack_ai" target="_blank"&gt; &lt;img alt="Follow on X(Twitter)" src="https://img.shields.io/twitter/follow/gpustack_ai?logo=X" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;br /&gt; 
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/gpustack/gpustack/main/README.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/gpustack/gpustack/main/README_CN.md"&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/gpustack/gpustack/main/README_JP.md"&gt;æ—¥æœ¬èª&lt;/a&gt; &lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/gpustack/gpustack/main/docs/assets/gpustack-demo.gif" alt="demo" /&gt;&lt;/p&gt; 
&lt;p&gt;GPUStack is an open-source GPU cluster manager for running AI models.&lt;/p&gt; 
&lt;h3&gt;Key Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Broad GPU Compatibility:&lt;/strong&gt; Seamlessly supports GPUs from various vendors across Apple Macs, Windows PCs, and Linux servers.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensive Model Support:&lt;/strong&gt; Supports a wide range of models including LLMs, VLMs, image models, audio models, embedding models, and rerank models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible Inference Backends:&lt;/strong&gt; Flexibly integrates with multiple inference backends including vLLM, Ascend MindIE, llama-box (llama.cpp &amp;amp; stable-diffusion.cpp) and vox-box.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Version Backend Support:&lt;/strong&gt; Run multiple versions of inference backends concurrently to meet the diverse runtime requirements of different models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Distributed Inference:&lt;/strong&gt; Supports single-node and multi-node multi-GPU inference, including heterogeneous GPUs across vendors and runtime environments.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scalable GPU Architecture:&lt;/strong&gt; Easily scale up by adding more GPUs or nodes to your infrastructure.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Robust Model Stability:&lt;/strong&gt; Ensures high availability with automatic failure recovery, multi-instance redundancy, and load balancing for inference requests.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent Deployment Evaluation:&lt;/strong&gt; Automatically assess model resource requirements, backend and architecture compatibility, OS compatibility, and other deployment-related factors.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automated Scheduling:&lt;/strong&gt; Dynamically allocate models based on available resources.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Lightweight Python Package:&lt;/strong&gt; Minimal dependencies and low operational overhead.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;OpenAI-Compatible APIs:&lt;/strong&gt; Fully compatible with OpenAIâ€™s API specifications for seamless integration.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;User &amp;amp; API Key Management:&lt;/strong&gt; Simplified management of users and API keys.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-Time GPU Monitoring:&lt;/strong&gt; Track GPU performance and utilization in real time.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Token and Rate Metrics:&lt;/strong&gt; Monitor token usage and API request rates.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Linux&lt;/h3&gt; 
&lt;p&gt;If you are using NVIDIA GPUs, ensure &lt;a href="https://docs.docker.com/engine/install/"&gt;Docker&lt;/a&gt; and &lt;a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html"&gt;NVIDIA Container Toolkit&lt;/a&gt; are installed on your system. Then, run the following command to start the GPUStack server.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d --name gpustack \
      --restart=unless-stopped \
      --gpus all \
      --network=host \
      --ipc=host \
      -v gpustack-data:/var/lib/gpustack \
      gpustack/gpustack
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more details on the installation or other GPU hardware platforms, please refer to the &lt;a href="https://raw.githubusercontent.com/gpustack/gpustack/main/docs/installation/installation-requirements.md"&gt;Installation Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;After the server starts, run the following command to get the default admin password:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker exec gpustack cat /var/lib/gpustack/initial_admin_password
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open your browser and navigate to &lt;code&gt;http://your_host_ip&lt;/code&gt; to access the GPUStack UI. Use the default username &lt;code&gt;admin&lt;/code&gt; and the password you retrieved above to log in.&lt;/p&gt; 
&lt;h3&gt;macOS &amp;amp; Windows&lt;/h3&gt; 
&lt;p&gt;A desktop installer is available for macOS and Windows â€” see the &lt;a href="https://docs.gpustack.ai/latest/installation/desktop-installer/"&gt;documentation&lt;/a&gt; for installation details.&lt;/p&gt; 
&lt;h2&gt;Deploy a Model&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Navigate to the &lt;code&gt;Catalog&lt;/code&gt; page in the GPUStack UI.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Select the &lt;code&gt;Qwen3&lt;/code&gt; model from the list of available models.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;After the deployment compatibility checks pass, click the &lt;code&gt;Save&lt;/code&gt; button to deploy the model.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/gpustack/gpustack/main/docs/assets/quick-start/quick-start-qwen3.png" alt="deploy qwen3 from catalog" /&gt;&lt;/p&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;GPUStack will start downloading the model files and deploying the model. When the deployment status shows &lt;code&gt;Running&lt;/code&gt;, the model has been deployed successfully.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/gpustack/gpustack/main/docs/assets/quick-start/model-running.png" alt="model is running" /&gt;&lt;/p&gt; 
&lt;ol start="5"&gt; 
 &lt;li&gt;Click &lt;code&gt;Playground - Chat&lt;/code&gt; in the navigation menu, check that the model &lt;code&gt;qwen3&lt;/code&gt; is selected from the top-right &lt;code&gt;Model&lt;/code&gt; dropdown. Now you can chat with the model in the UI playground.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/gpustack/gpustack/main/docs/assets/quick-start/quick-chat.png" alt="quick chat" /&gt;&lt;/p&gt; 
&lt;h2&gt;Use the model via API&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Hover over the user avatar and navigate to the &lt;code&gt;API Keys&lt;/code&gt; page, then click the &lt;code&gt;New API Key&lt;/code&gt; button.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Fill in the &lt;code&gt;Name&lt;/code&gt; and click the &lt;code&gt;Save&lt;/code&gt; button.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Copy the generated API key and save it somewhere safe. Please note that you can only see it once on creation.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;You can now use the API key to access the OpenAI-compatible API endpoints provided by GPUStack. For example, use curl as the following:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Replace `your_api_key` and `your_gpustack_server_url`
# with your actual API key and GPUStack server URL.
export GPUSTACK_API_KEY=your_api_key
curl http://your_gpustack_server_url/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $GPUSTACK_API_KEY" \
  -d '{
    "model": "qwen3",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "Tell me a joke."
      }
    ],
    "stream": true
  }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Supported Platforms&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Linux&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; macOS&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Windows&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Supported Accelerators&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; NVIDIA CUDA (&lt;a href="https://developer.nvidia.com/cuda-gpus"&gt;Compute Capability&lt;/a&gt; 6.0 and above)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Apple Metal (M-series chips)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; AMD ROCm&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Ascend CANN&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Hygon DTK&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Moore Threads MUSA&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Iluvatar Corex&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Cambricon MLU&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Supported Models&lt;/h2&gt; 
&lt;p&gt;GPUStack uses &lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;, &lt;a href="https://www.hiascend.com/en/software/mindie"&gt;Ascend MindIE&lt;/a&gt;, &lt;a href="https://github.com/gpustack/llama-box"&gt;llama-box&lt;/a&gt; (bundled &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;llama.cpp&lt;/a&gt; and &lt;a href="https://github.com/leejet/stable-diffusion.cpp"&gt;stable-diffusion.cpp&lt;/a&gt; server) and &lt;a href="https://github.com/gpustack/vox-box"&gt;vox-box&lt;/a&gt; as the backends and supports a wide range of models. Models from the following sources are supported:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://huggingface.co/"&gt;Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://modelscope.cn/"&gt;ModelScope&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Local File Path&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Example Models&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Category&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Models&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Large Language Models(LLMs)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/models?search=Qwen/Qwen"&gt;Qwen&lt;/a&gt;, &lt;a href="https://huggingface.co/meta-llama"&gt;LLaMA&lt;/a&gt;, &lt;a href="https://huggingface.co/mistralai"&gt;Mistral&lt;/a&gt;, &lt;a href="https://huggingface.co/models?search=deepseek-ai/deepseek"&gt;DeepSeek&lt;/a&gt;, &lt;a href="https://huggingface.co/models?search=microsoft/phi"&gt;Phi&lt;/a&gt;, &lt;a href="https://huggingface.co/models?search=Google/gemma"&gt;Gemma&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Vision Language Models(VLMs)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/models?pipeline_tag=image-text-to-text&amp;amp;search=llama3.2"&gt;Llama3.2-Vision&lt;/a&gt;, &lt;a href="https://huggingface.co/models?search=pixtral"&gt;Pixtral&lt;/a&gt; , &lt;a href="https://huggingface.co/models?search=Qwen/Qwen2.5-VL"&gt;Qwen2.5-VL&lt;/a&gt;, &lt;a href="https://huggingface.co/models?search=llava"&gt;LLaVA&lt;/a&gt;, &lt;a href="https://huggingface.co/models?search=internvl3"&gt;InternVL3&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Diffusion Models&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/models?search=gpustack/stable-diffusion"&gt;Stable Diffusion&lt;/a&gt;, &lt;a href="https://huggingface.co/models?search=gpustack/flux"&gt;FLUX&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Embedding Models&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/gpustack/bge-m3-GGUF"&gt;BGE&lt;/a&gt;, &lt;a href="https://huggingface.co/gpustack/bce-embedding-base_v1-GGUF"&gt;BCE&lt;/a&gt;, &lt;a href="https://huggingface.co/models?search=gpustack/jina-embeddings"&gt;Jina&lt;/a&gt;, &lt;a href="https://huggingface.co/models?search=qwen/qwen3-embedding"&gt;Qwen3-Embedding&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Reranker Models&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/gpustack/bge-reranker-v2-m3-GGUF"&gt;BGE&lt;/a&gt;, &lt;a href="https://huggingface.co/gpustack/bce-reranker-base_v1-GGUF"&gt;BCE&lt;/a&gt;, &lt;a href="https://huggingface.co/models?search=gpustack/jina-reranker"&gt;Jina&lt;/a&gt;, &lt;a href="https://huggingface.co/models?search=qwen/qwen3-reranker"&gt;Qwen3-Reranker&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Audio Models&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/models?search=Systran/faster"&gt;Whisper&lt;/a&gt; (Speech-to-Text), &lt;a href="https://huggingface.co/models?search=FunAudioLLM/CosyVoice"&gt;CosyVoice&lt;/a&gt; (Text-to-Speech)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;For full list of supported models, please refer to the supported models section in the &lt;a href="https://docs.gpustack.ai/latest/user-guide/inference-backends/"&gt;inference backends&lt;/a&gt; documentation.&lt;/p&gt; 
&lt;h2&gt;OpenAI-Compatible APIs&lt;/h2&gt; 
&lt;p&gt;GPUStack serves the following OpenAI compatible APIs under the &lt;code&gt;/v1-openai&lt;/code&gt; path:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://platform.openai.com/docs/api-reference/models/list"&gt;List Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://platform.openai.com/docs/api-reference/completions/create"&gt;Create Completion&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://platform.openai.com/docs/api-reference/chat/create"&gt;Create Chat Completion&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://platform.openai.com/docs/api-reference/embeddings/create"&gt;Create Embeddings&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://platform.openai.com/docs/api-reference/images/create"&gt;Create Image&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://platform.openai.com/docs/api-reference/images/createEdit"&gt;Create Image Edit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://platform.openai.com/docs/api-reference/audio/createSpeech"&gt;Create Speech&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://platform.openai.com/docs/api-reference/audio/createTranscription"&gt;Create Transcription&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For example, you can use the official &lt;a href="https://github.com/openai/openai-python"&gt;OpenAI Python API library&lt;/a&gt; to consume the APIs:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI
client = OpenAI(base_url="http://your_gpustack_server_url/v1-openai", api_key="your_api_key")

completion = client.chat.completions.create(
  model="llama3.2",
  messages=[
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}
  ]
)

print(completion.choices[0].message)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;GPUStack users can generate their own API keys in the UI.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Please see the &lt;a href="https://docs.gpustack.ai"&gt;official docs site&lt;/a&gt; for complete documentation.&lt;/p&gt; 
&lt;h2&gt;Build&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install Python (version 3.10 to 3.12).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run &lt;code&gt;make build&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;You can find the built wheel package in &lt;code&gt;dist&lt;/code&gt; directory.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Please read the &lt;a href="https://raw.githubusercontent.com/gpustack/gpustack/main/docs/contributing.md"&gt;Contributing Guide&lt;/a&gt; if you're interested in contributing to GPUStack.&lt;/p&gt; 
&lt;h2&gt;Join Community&lt;/h2&gt; 
&lt;p&gt;Any issues or have suggestions, feel free to join our &lt;a href="https://discord.gg/VXYJzuaqwD"&gt;Community&lt;/a&gt; for support.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Copyright (c) 2024 The GPUStack authors&lt;/p&gt; 
&lt;p&gt;Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at &lt;a href="https://raw.githubusercontent.com/gpustack/gpustack/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>hao-ai-lab/FastVideo</title>
      <link>https://github.com/hao-ai-lab/FastVideo</link>
      <description>&lt;p&gt;A unified inference and post-training framework for accelerated video generation.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/hao-ai-lab/FastVideo/main/assets/logo.png" width="30%" /&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;FastVideo is a unified post-training and inference framework for accelerated video generation.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;FastVideo features an end-to-end unified pipeline for accelerating diffusion models, starting from data preprocessing to model training, finetuning, distillation, and inference. FastVideo is designed to be modular and extensible, allowing users to easily add new optimizations and techniques. Whether it is training-free optimizations or post-training optimizations, FastVideo has you covered.&lt;/p&gt; 
&lt;p align="center"&gt; | ğŸ•¹ï¸ &lt;a href="https://fastwan.fastvideo.org/" &lt;b&gt;Online Demo&lt;/a&gt; | &lt;a href="https://hao-ai-lab.github.io/FastVideo"&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://hao-ai-lab.github.io/FastVideo/inference/inference_quick_start.html"&gt;&lt;b&gt; Quick Start&lt;/b&gt;&lt;/a&gt; | ğŸ¤— &lt;a href="https://huggingface.co/collections/FastVideo/fastwan-6886a305d9799c8cd1496408" target="_blank"&gt;&lt;b&gt;FastWan&lt;/b&gt;&lt;/a&gt; | ğŸŸ£ğŸ’¬ &lt;a href="https://join.slack.com/t/fastvideo/shared_invite/zt-38u6p1jqe-yDI1QJOCEnbtkLoaI5bjZQ" target="_blank"&gt; &lt;b&gt;Slack&lt;/b&gt; &lt;/a&gt; | ğŸŸ£ğŸ’¬ &lt;a href="https://ibb.co/wZPZTLKg" target="_blank"&gt; &lt;b&gt; WeChat &lt;/b&gt; &lt;/a&gt; | &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/hao-ai-lab/FastVideo/main/assets/fastwan.png" width="90%" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;NEWS&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;2025/08/04&lt;/code&gt;: Release &lt;a href="https://hao-ai-lab.github.io/FastVideo/distillation/dmd.html"&gt;FastWan&lt;/a&gt; models and &lt;a href="https://hao-ai-lab.github.io/blogs/fastvideo_post_training/"&gt;Sparse-Distillation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;2025/06/14&lt;/code&gt;: Release finetuning and inference code for &lt;a href="https://arxiv.org/pdf/2505.13389"&gt;VSA&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;2025/04/24&lt;/code&gt;: &lt;a href="https://hao-ai-lab.github.io/blogs/fastvideo/"&gt;FastVideo V1&lt;/a&gt; is released!&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;2025/02/18&lt;/code&gt;: Release the inference code for &lt;a href="https://hao-ai-lab.github.io/blogs/sta/"&gt;Sliding Tile Attention&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;p&gt;FastVideo has the following features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;End-to-end post-training support: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://hao-ai-lab.github.io/blogs/fastvideo_post_training/"&gt;Sparse distillation&lt;/a&gt; for Wan2.1 and Wan2.2 to achineve &amp;gt;50x denoising speedup&lt;/li&gt; 
   &lt;li&gt;Data preprocessing pipeline for video data&lt;/li&gt; 
   &lt;li&gt;Support full finetuning and LoRA finetuning for state-of-the-art open video DiTs&lt;/li&gt; 
   &lt;li&gt;Scalable training with FSDP2, sequence parallelism, and selective activation checkpointing, with near linear scaling to 64 GPUs&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;State-of-the-art performance optimizations for inference 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://arxiv.org/pdf/2505.13389"&gt;Video Sparse Attention&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://arxiv.org/pdf/2502.04507"&gt;Sliding Tile Attention&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://arxiv.org/pdf/2411.19108"&gt;TeaCache&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://arxiv.org/abs/2410.02367"&gt;Sage Attention&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Diverse hardware and OS support 
  &lt;ul&gt; 
   &lt;li&gt;Support H100, A100, 4090&lt;/li&gt; 
   &lt;li&gt;Support Linux, Windows, MacOS&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;We recommend using an environment manager such as &lt;code&gt;Conda&lt;/code&gt; to create a clean environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create and activate a new conda environment
conda create -n fastvideo python=3.12
conda activate fastvideo

# Install FastVideo
pip install fastvideo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please see our &lt;a href="https://hao-ai-lab.github.io/FastVideo/getting_started/installation.html"&gt;docs&lt;/a&gt; for more detailed installation instructions.&lt;/p&gt; 
&lt;h2&gt;Sparse Distillation&lt;/h2&gt; 
&lt;p&gt;For our sparse distillation techniques, please see our &lt;a href="https://hao-ai-lab.github.io/FastVideo/distillation/dmd.html"&gt;distillation docs&lt;/a&gt; and check out our &lt;a href="https://hao-ai-lab.github.io/blogs/fastvideo_post_training/"&gt;blog&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;See below for recipes and datasets:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Model&lt;/th&gt; 
   &lt;th align="center"&gt;Sparse Distillation&lt;/th&gt; 
   &lt;th align="center"&gt;Dataset&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/FastVideo/FastWan2.1-T2V-1.3B-Diffusers"&gt;FastWan2.1-T2V-1.3B&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/hao-ai-lab/FastVideo/tree/main/examples/distill/Wan2.1-T2V/Wan-Syn-Data-480P"&gt;Recipe&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/datasets/FastVideo/Wan-Syn_77x448x832_600k"&gt;FastVideo Synthetic Wan2.1 480P&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/FastVideo/FastWan2.1-T2V-14B-Diffusers"&gt;FastWan2.1-T2V-14B-Preview&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Coming soon!&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/datasets/FastVideo/Wan-Syn_77x768x1280_250k"&gt;FastVideo Synthetic Wan2.1 720P&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/FastVideo/FastWan2.2-TI2V-5B-Diffusers"&gt;FastWan2.2-TI2V-5B&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/hao-ai-lab/FastVideo/tree/main/examples/distill/Wan2.2-TI2V-5B-Diffusers/Data-free"&gt;Recipe&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/datasets/FastVideo/Wan2.2-Syn-121x704x1280_32k"&gt;FastVideo Synthetic Wan2.2 720P&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Inference&lt;/h2&gt; 
&lt;h3&gt;Generating Your First Video&lt;/h3&gt; 
&lt;p&gt;Here's a minimal example to generate a video using the default settings. Make sure VSA kernels are &lt;a href="https://hao-ai-lab.github.io/FastVideo/video_sparse_attention/installation.html"&gt;installed&lt;/a&gt;. Create a file called &lt;code&gt;example.py&lt;/code&gt; with the following code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
from fastvideo import VideoGenerator

def main():
    os.environ["FASTVIDEO_ATTENTION_BACKEND"] = "VIDEO_SPARSE_ATTN"

    # Create a video generator with a pre-trained model
    generator = VideoGenerator.from_pretrained(
        "FastVideo/FastWan2.1-T2V-1.3B-Diffusers",
        num_gpus=1,  # Adjust based on your hardware
    )

    # Define a prompt for your video
    prompt = "A curious raccoon peers through a vibrant field of yellow sunflowers, its eyes wide with interest."

    # Generate the video
    video = generator.generate_video(
        prompt,
        return_frames=True,  # Also return frames from this call (defaults to False)
        output_path="my_videos/",  # Controls where videos are saved
        save_video=True
    )

if __name__ == '__main__':
    main()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run the script with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python example.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For a more detailed guide, please see our &lt;a href="https://hao-ai-lab.github.io/FastVideo/inference/inference_quick_start.html"&gt;inference quick start&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Other docs:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://hao-ai-lab.github.io/FastVideo/design/overview.html"&gt;Design Overview&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://hao-ai-lab.github.io/FastVideo/getting_started/installation.html"&gt;Contribution Guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Distillation and Finetuning&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://hao-ai-lab.github.io/FastVideo/distillation/dmd.html"&gt;Distillation Guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- - [Finetuning Guide](https://hao-ai-lab.github.io/FastVideo/training/finetune.html) --&gt; 
&lt;h2&gt;ğŸ“‘ Development Plan&lt;/h2&gt; 
&lt;!-- - More distillation methods --&gt; 
&lt;!-- - [ ] Add Distribution Matching Distillation --&gt; 
&lt;p&gt;More FastWan Models Coming Soon!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add FastWan2.1-T2V-14B&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add FastWan2.2-T2V-14B&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add FastWan2.2-I2V-14B&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- - Optimization features
- Code updates --&gt; 
&lt;!-- - [ ] fp8 support --&gt; 
&lt;!-- - [ ] faster load model and save model support --&gt; 
&lt;p&gt;See details in &lt;a href="https://github.com/hao-ai-lab/FastVideo/issues/468"&gt;development roadmap&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ¤ Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome all contributions. Please check out our guide &lt;a href="https://hao-ai-lab.github.io/FastVideo/contributing/overview.html"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Acknowledgement&lt;/h2&gt; 
&lt;p&gt;We learned and reused code from the following projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Wan-Video"&gt;Wan-Video&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/HazyResearch/ThunderKittens"&gt;ThunderKittens&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/triton-lang/triton"&gt;Triton&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tianweiy/DMD2"&gt;DMD2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/diffusers"&gt;diffusers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/xdit-project/xDiT"&gt;xDiT&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sgl-project/sglang"&gt;SGLang&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We thank &lt;a href="https://ifm.mbzuai.ac.ae/"&gt;MBZUAI&lt;/a&gt;, &lt;a href="https://www.anyscale.com/"&gt;Anyscale&lt;/a&gt;, and &lt;a href="https://www.gmicloud.ai/"&gt;GMI Cloud&lt;/a&gt; for their support throughout this project.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find FastVideo useful, please considering citing our work:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@software{fastvideo2024,
  title        = {FastVideo: A Unified Framework for Accelerated Video Generation},
  author       = {The FastVideo Team},
  url          = {https://github.com/hao-ai-lab/FastVideo},
  month        = apr,
  year         = {2024},
}

@article{zhang2025vsa,
  title={VSA: Faster Video Diffusion with Trainable Sparse Attention},
  author={Zhang, Peiyuan and Huang, Haofeng and Chen, Yongqi and Lin, Will and Liu, Zhengzhong and Stoica, Ion and Xing, Eric and Zhang, Hao},
  journal={arXiv preprint arXiv:2505.13389},
  year={2025}
}

@article{zhang2025fast,
  title={Fast video generation with sliding tile attention},
  author={Zhang, Peiyuan and Chen, Yongqi and Su, Runlong and Ding, Hangliang and Stoica, Ion and Liu, Zhengzhong and Zhang, Hao},
  journal={arXiv preprint arXiv:2502.04507},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>denizsafak/abogen</title>
      <link>https://github.com/denizsafak/abogen</link>
      <description>&lt;p&gt;Generate audiobooks from EPUBs, PDFs and text with synchronized captions.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;abogen &lt;img width="40px" title="abogen icon" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/abogen/assets/icon.ico" align="right" style="padding-left: 10px; padding-top:5px;" /&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/denizsafak/abogen/actions"&gt;&lt;img src="https://github.com/denizsafak/abogen/actions/workflows/test_pip.yml/badge.svg?sanitize=true" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/denizsafak/abogen/releases/latest"&gt;&lt;img src="https://img.shields.io/github/v/release/denizsafak/abogen" alt="GitHub Release" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/abogen/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/abogen" alt="Abogen PyPi Python Versions" /&gt;&lt;/a&gt; &lt;a href="https://github.com/denizsafak/abogen/releases/latest"&gt;&lt;img src="https://img.shields.io/badge/os-windows%20%7C%20linux%20%7C%20macos%20-blue" alt="Operating Systems" /&gt;&lt;/a&gt; &lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true" alt="Code style: black" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-maroon.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Abogen is a powerful text-to-speech conversion tool that makes it easy to turn ePub, PDF, or text files into high-quality audio with matching subtitles in seconds. Use it for audiobooks, voiceovers for Instagram, YouTube, TikTok, or any project that needs natural-sounding text-to-speech, using &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;Kokoro-82M&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img title="Abogen Main" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen.png" width="380" /&gt; &lt;img title="Abogen Processing" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen2.png" width="380" /&gt;&lt;/p&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/cb66512d-0a52-48c3-bda4-f1e6a03fb8d6"&gt;https://github.com/user-attachments/assets/cb66512d-0a52-48c3-bda4-f1e6a03fb8d6&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;This demo was generated in just 5&amp;nbsp;seconds, producing âˆ¼1&amp;nbsp;minute of audio with perfectly synced subtitles. To create a similar video, see &lt;a href="https://github.com/denizsafak/abogen/tree/main/demo"&gt;the demo guide&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;code&gt;How to install?&lt;/code&gt; &lt;a href="https://pypi.org/project/abogen/" target="_blank"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/abogen" alt="Abogen Compatible PyPi Python Versions" align="right" style="margin-top:6px;" /&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;p&gt;Go to &lt;a href="https://github.com/espeak-ng/espeak-ng/releases/latest"&gt;espeak-ng latest release&lt;/a&gt; download and run the *.msi file.&lt;/p&gt; 
&lt;h4&gt;OPTION 1: Install using script&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://github.com/denizsafak/abogen/archive/refs/heads/main.zip"&gt;Download&lt;/a&gt; the repository&lt;/li&gt; 
 &lt;li&gt;Extract the ZIP file&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;WINDOWS_INSTALL.bat&lt;/code&gt; by double-clicking it&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This method handles everything automatically - installing all dependencies including CUDA in a self-contained environment without requiring a separate Python installation. (You still need to install &lt;a href="https://github.com/espeak-ng/espeak-ng/releases/latest"&gt;espeak-ng&lt;/a&gt;.)&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] You don't need to install Python separately. The script will install Python automatically.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;OPTION 2: Install using pip&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create a virtual environment (optional)
mkdir abogen &amp;amp;&amp;amp; cd abogen
python -m venv venv
venv\Scripts\activate

# For NVIDIA GPUs:
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128

# For AMD GPUs:
# Not supported yet, because ROCm is not available on Windows. Use Linux if you have AMD GPU.

# Install abogen
pip install abogen
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Mac&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install espeak-ng
brew install espeak-ng

# Create a virtual environment (recommended)
mkdir abogen &amp;amp;&amp;amp; cd abogen
python3 -m venv venv
source venv/bin/activate

# Install abogen
pip3 install abogen
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Linux&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install espeak-ng
sudo apt install espeak-ng # Ubuntu/Debian
sudo pacman -S espeak-ng # Arch Linux
sudo dnf install espeak-ng # Fedora

# Create a virtual environment (recommended)
mkdir abogen &amp;amp;&amp;amp; cd abogen
python3 -m venv venv
source venv/bin/activate

# Install abogen
pip3 install abogen

# For NVIDIA GPUs:
# Already supported, no need to install CUDA separately.

# For AMD GPUs:
# After installing abogen, we need to uninstall the existing torch package
pip3 uninstall torch 
pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.4
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] If you get &lt;code&gt;WARNING: The script abogen-cli is installed in '/home/username/.local/bin' which is not on PATH.&lt;/code&gt; error, run the following command to add it to your PATH:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;echo "export PATH=\"/home/$USER/.local/bin:\$PATH\"" &amp;gt;&amp;gt; ~/.bashrc &amp;amp;&amp;amp; source ~/.bashrc
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] If you get "No matching distribution found" error, try installing it on supported Python (3.10 to 3.12). You can use &lt;a href="https://github.com/pyenv/pyenv"&gt;pyenv&lt;/a&gt; to manage multiple Python versions easily in Linux. Watch this &lt;a href="https://www.youtube.com/watch?v=MVyb-nI4KyI"&gt;video&lt;/a&gt; by NetworkChuck for a quick guide.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Special thanks to &lt;a href="https://github.com/hg000125"&gt;@hg000125&lt;/a&gt; for his contribution in &lt;a href="https://github.com/denizsafak/abogen/issues/23"&gt;#23&lt;/a&gt;. AMD GPU support is possible thanks to his work.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;code&gt;How to run?&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;If you installed using pip, you can simply run the following command to start Abogen:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;abogen
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] If you installed using the Windows installer &lt;code&gt;(WINDOWS_INSTALL.bat)&lt;/code&gt;, It should have created a shortcut in the same folder, or your desktop. You can run it from there. If you lost the shortcut, Abogen is located in &lt;code&gt;python_embedded/Scripts/abogen.exe&lt;/code&gt;. You can run it from there directly.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;code&gt;How to use?&lt;/code&gt;&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Drag and drop any ePub, PDF, or text file (or use the built-in text editor)&lt;/li&gt; 
 &lt;li&gt;Configure the settings: 
  &lt;ul&gt; 
   &lt;li&gt;Set speech speed&lt;/li&gt; 
   &lt;li&gt;Select a voice (or create a custom voice using voice mixer)&lt;/li&gt; 
   &lt;li&gt;Select subtitle generation style (by sentence, word, etc.)&lt;/li&gt; 
   &lt;li&gt;Select output format&lt;/li&gt; 
   &lt;li&gt;Select where to save the output&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Hit Start&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;&lt;code&gt;In action&lt;/code&gt;&lt;/h2&gt; 
&lt;img title="Abogen in action" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen.gif" /&gt; 
&lt;p&gt;Hereâ€™s Abogen in action: in this demo, it processes âˆ¼3,000 characters of text in just 11 seconds and turns it into 3 minutes and 28 seconds of audio, and I have a low-end &lt;strong&gt;RTX&amp;nbsp;2060&amp;nbsp;Mobile laptop GPU&lt;/strong&gt;. Your results may vary depending on your hardware.&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;Configuration&lt;/code&gt;&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Input Box&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Drag and drop &lt;code&gt;ePub&lt;/code&gt;, &lt;code&gt;PDF&lt;/code&gt;, or &lt;code&gt;.TXT&lt;/code&gt; files (or use built-in text editor)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Queue options&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Add multiple files to a queue and process them in batch, with individual settings for each file. See &lt;a href="https://raw.githubusercontent.com/denizsafak/abogen/main/#queue-mode"&gt;Queue mode&lt;/a&gt; for more details.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Speed&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Adjust speech rate from &lt;code&gt;0.1x&lt;/code&gt; to &lt;code&gt;2.0x&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Select Voice&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;First letter of the language code (e.g., &lt;code&gt;a&lt;/code&gt; for American English, &lt;code&gt;b&lt;/code&gt; for British English, etc.), second letter is for &lt;code&gt;m&lt;/code&gt; for male and &lt;code&gt;f&lt;/code&gt; for female.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Voice mixer&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Create custom voices by mixing different voice models with a profile system. See &lt;a href="https://raw.githubusercontent.com/denizsafak/abogen/main/#voice-mixer"&gt;Voice Mixer&lt;/a&gt; for more details.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Voice preview&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Listen to the selected voice before processing.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Generate subtitles&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;Disabled&lt;/code&gt;, &lt;code&gt;Sentence&lt;/code&gt;, &lt;code&gt;Sentence + Comma&lt;/code&gt;, &lt;code&gt;1 word&lt;/code&gt;, &lt;code&gt;2 words&lt;/code&gt;, &lt;code&gt;3 words&lt;/code&gt;, etc. (Represents the number of words in each subtitle entry)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Output voice format&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;.WAV&lt;/code&gt;, &lt;code&gt;.FLAC&lt;/code&gt;, &lt;code&gt;.MP3&lt;/code&gt;, &lt;code&gt;.OPUS (best compression)&lt;/code&gt; and &lt;code&gt;M4B (with chapters)&lt;/code&gt; (Special thanks to &lt;a href="https://github.com/jborza"&gt;@jborza&lt;/a&gt; for chapter support in PR &lt;a href="https://github.com/denizsafak/abogen/pull/10"&gt;#10&lt;/a&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Output subtitle format&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Configures the subtitle format as &lt;code&gt;SRT (standard)&lt;/code&gt;, &lt;code&gt;ASS (wide)&lt;/code&gt;, &lt;code&gt;ASS (narrow)&lt;/code&gt;, &lt;code&gt;ASS (centered wide)&lt;/code&gt;, or &lt;code&gt;ASS (centered narrow)&lt;/code&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Replace single newlines with spaces&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Replaces single newlines with spaces in the text. This is useful for texts that have imaginary line breaks.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Save location&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;Save next to input file&lt;/code&gt;, &lt;code&gt;Save to desktop&lt;/code&gt;, or &lt;code&gt;Choose output folder&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Book handler options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Chapter Control&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Select specific &lt;code&gt;chapters&lt;/code&gt; from ePUBs or &lt;code&gt;chapters + pages&lt;/code&gt; from PDFs.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Save each chapter separately&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Save each chapter in e-books as a separate audio file.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Create a merged version&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Create a single audio file that combines all chapters. (If &lt;code&gt;Save each chapter separately&lt;/code&gt; is disabled, this option will be the default behavior.)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Save in a project folder with metadata&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Save the converted items in a project folder with available metadata files.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Menu options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Theme&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Change the application's theme using &lt;code&gt;System&lt;/code&gt;, &lt;code&gt;Light&lt;/code&gt;, or &lt;code&gt;Dark&lt;/code&gt; options.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Configure max words per subtitle&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Configures the maximum number of words per subtitle entry.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Configure max lines in log window&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Configures the maximum number of lines to display in the log window.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Separate chapters audio format&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Configures the audio format for separate chapters as &lt;code&gt;wav&lt;/code&gt;, &lt;code&gt;flac&lt;/code&gt;, &lt;code&gt;mp3&lt;/code&gt;, or &lt;code&gt;opus&lt;/code&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Create desktop shortcut&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Creates a shortcut on your desktop for easy access.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Open config directory&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Opens the directory where the configuration file is stored.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Open cache directory&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Opens the cache directory where converted text files are stored.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Clear cache files&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Deletes cache files created during the conversion or preview.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Check for updates at startup&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Automatically checks for updates when the program starts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Disable Kokoro's internet access&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Prevents Kokoro from downloading models or voices from HuggingFace Hub, useful for offline use.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Reset to default settings&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Resets all settings to their default values.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;&lt;code&gt;Voice Mixer&lt;/code&gt;&lt;/h2&gt; 
&lt;img title="Abogen Voice Mixer" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/voice_mixer.png" /&gt; 
&lt;p&gt;With voice mixer, you can create custom voices by mixing different voice models. You can adjust the weight of each voice and save your custom voice as a profile for future use. The voice mixer allows you to create unique and personalized voices. (Huge thanks to &lt;a href="https://github.com/jborza"&gt;@jborza&lt;/a&gt; for making this possible through his contributions in &lt;a href="https://github.com/denizsafak/abogen/pull/5"&gt;#5&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;Queue Mode&lt;/code&gt;&lt;/h2&gt; 
&lt;img title="Abogen queue mode" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/queue.png" /&gt; 
&lt;p&gt;Abogen supports &lt;strong&gt;queue mode&lt;/strong&gt;, allowing you to add multiple files to a processing queue. This is useful if you want to convert several files in one batch.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can add text files (&lt;code&gt;.txt&lt;/code&gt;) directly using the &lt;strong&gt;Add files&lt;/strong&gt; button in the Queue Manager. To add PDF or EPUB files, use the input box in the main window and click the &lt;strong&gt;Add to Queue&lt;/strong&gt; button.&lt;/li&gt; 
 &lt;li&gt;Each file in the queue keeps the configuration settings that were active when it was added. Changing the main window configuration afterward does &lt;strong&gt;not&lt;/strong&gt; affect files already in the queue.&lt;/li&gt; 
 &lt;li&gt;You can view each file's configuration by hovering over them.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Abogen will process each item in the queue automatically, saving outputs as configured.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Special thanks to &lt;a href="https://github.com/jborza"&gt;@jborza&lt;/a&gt; for adding queue mode in PR &lt;a href="https://github.com/denizsafak/abogen/pull/35"&gt;#35&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;code&gt;About Chapter Markers&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;When you process ePUB or PDF files, Abogen converts them into text files stored in your cache directory. When you click "Edit," you're actually modifying these converted text files. In these text files, you'll notice tags that look like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;lt;&amp;lt;CHAPTER_MARKER:Chapter Title&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These are chapter markers. They are automatically added when you process ePUB or PDF files, based on the chapters you select. They serve an important purpose:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Allow you to split the text into separate audio files for each chapter&lt;/li&gt; 
 &lt;li&gt;Save time by letting you reprocess only specific chapters if errors occur, rather than the entire file&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can manually add these markers to plain text files for the same benefits. Simply include them in your text like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;lt;&amp;lt;CHAPTER_MARKER:Introduction&amp;gt;&amp;gt;
This is the beginning of my text...  

&amp;lt;&amp;lt;CHAPTER_MARKER:Main Content&amp;gt;&amp;gt; 
Here's another part...  
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When you process the text file, Abogen will detect these markers automatically and ask if you want to save each chapter separately and create a merged version.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/chapter_marker.png" alt="Abogen Chapter Marker" /&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;About Metadata Tags&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;Similar to chapter markers, it is possible to add metadata tags for &lt;code&gt;M4B&lt;/code&gt; files. This is useful for audiobook players that support metadata, allowing you to add information like title, author, year, etc. Abogen automatically adds these tags when you process ePUB or PDF files, but you can also add them manually to your text files. Add metadata tags &lt;strong&gt;at the beginning of your text file&lt;/strong&gt; like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;lt;&amp;lt;METADATA_TITLE:Title&amp;gt;&amp;gt;
&amp;lt;&amp;lt;METADATA_ARTIST:Author&amp;gt;&amp;gt;
&amp;lt;&amp;lt;METADATA_ALBUM:Album Title&amp;gt;&amp;gt;
&amp;lt;&amp;lt;METADATA_YEAR:Year&amp;gt;&amp;gt;
&amp;lt;&amp;lt;METADATA_ALBUM_ARTIST:Album Artist&amp;gt;&amp;gt;
&amp;lt;&amp;lt;METADATA_COMPOSER:Narrator&amp;gt;&amp;gt;
&amp;lt;&amp;lt;METADATA_GENRE:Audiobook&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;code&gt;Supported Languages&lt;/code&gt;&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;# ğŸ‡ºğŸ‡¸ 'a' =&amp;gt; American English, ğŸ‡¬ğŸ‡§ 'b' =&amp;gt; British English
# ğŸ‡ªğŸ‡¸ 'e' =&amp;gt; Spanish es
# ğŸ‡«ğŸ‡· 'f' =&amp;gt; French fr-fr
# ğŸ‡®ğŸ‡³ 'h' =&amp;gt; Hindi hi
# ğŸ‡®ğŸ‡¹ 'i' =&amp;gt; Italian it
# ğŸ‡¯ğŸ‡µ 'j' =&amp;gt; Japanese: pip install misaki[ja]
# ğŸ‡§ğŸ‡· 'p' =&amp;gt; Brazilian Portuguese pt-br
# ğŸ‡¨ğŸ‡³ 'z' =&amp;gt; Mandarin Chinese: pip install misaki[zh]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For a complete list of supported languages and voices, refer to Kokoro's &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md"&gt;VOICES.md&lt;/a&gt;. To listen to sample audio outputs, see &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M/blob/main/SAMPLES.md"&gt;SAMPLES.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;MPV Config&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;I highly recommend using &lt;a href="https://mpv.io/installation/"&gt;MPV&lt;/a&gt; to play your audio files, as it supports displaying subtitles even without a video track. Here's my &lt;code&gt;mpv.conf&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# --- MPV Settings ---
save-position-on-quit
keep-open=yes
# --- Subtitle ---
sub-ass-override=no
sub-margin-y=50
sub-margin-x=50
# --- Audio Quality ---
audio-spdif=ac3,dts,eac3,truehd,dts-hd
audio-channels=auto
audio-samplerate=48000
volume-max=200
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;code&gt;Docker Guide&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;If you want to run Abogen in a Docker container:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://github.com/denizsafak/abogen/archive/refs/heads/main.zip"&gt;Download the repository&lt;/a&gt; and extract, or clone it using git.&lt;/li&gt; 
 &lt;li&gt;Go to &lt;code&gt;abogen&lt;/code&gt; folder. You should see &lt;code&gt;Dockerfile&lt;/code&gt; there.&lt;/li&gt; 
 &lt;li&gt;Open your termminal in that directory and run the following commands:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Build the Docker image:
docker build --progress plain -t abogen .

# Note that building the image may take a while.
# After building is complete, run the Docker container:

# Windows
docker run --name abogen -v %cd%:/shared -p 5800:5800 -p 5900:5900 --gpus all abogen

# Linux
docker run --name abogen -v $(pwd):/shared -p 5800:5800 -p 5900:5900 --gpus all abogen

# MacOS
docker run --name abogen -v $(pwd):/shared -p 5800:5800 -p 5900:5900 abogen

# We expose port 5800 for use by a web browser, 5900 if you want to connect with a VNC client.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Abogen launches automatically inside the container.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can access it via a web browser at &lt;a href="http://localhost:5800"&gt;http://localhost:5800&lt;/a&gt; or connect to it using a VNC client at &lt;code&gt;localhost:5900&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;You can use &lt;code&gt;/shared&lt;/code&gt; directory to share files between your host and the container.&lt;/li&gt; 
 &lt;li&gt;For later use, start it with &lt;code&gt;docker start abogen&lt;/code&gt; and stop it with &lt;code&gt;docker stop abogen&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Known issues:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Audio preview is not working inside container (ALSA error).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Open cache directory&lt;/code&gt; and &lt;code&gt;Open configuration directory&lt;/code&gt; options in settings not working. (Tried pcmanfm, did not work with Abogen).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;(Special thanks to &lt;a href="https://www.reddit.com/user/geo38/"&gt;@geo38&lt;/a&gt; from Reddit, who provided the Dockerfile and instructions in &lt;a href="https://www.reddit.com/r/selfhosted/comments/1k8x1yo/comment/mpe0bz8/"&gt;this comment&lt;/a&gt;.)&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;Similar Projects&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;Abogen is a standalone project, but it is inspired by and shares some similarities with other projects. Here are a few:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/santinic/audiblez"&gt;audiblez&lt;/a&gt;: Generate audiobooks from e-books. &lt;strong&gt;(Has CLI and GUI support)&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/plusuncold/autiobooks"&gt;autiobooks&lt;/a&gt;: Automatically convert epubs to audiobooks&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mateogon/pdf-narrator"&gt;pdf-narrator&lt;/a&gt;: Convert your PDFs and EPUBs into audiobooks effortlessly.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/p0n1/epub_to_audiobook"&gt;epub_to_audiobook&lt;/a&gt;: EPUB to audiobook converter, optimized for Audiobookshelf&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook"&gt;ebook2audiobook&lt;/a&gt;: Convert ebooks to audiobooks with chapters and metadata using dynamic AI models and voice cloning&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;code&gt;Roadmap&lt;/code&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add OCR scan feature for PDF files using docling/teserract.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Add chapter metadata for .m4a files. (Issue &lt;a href="https://github.com/denizsafak/abogen/issues/9"&gt;#9&lt;/a&gt;, PR &lt;a href="https://github.com/denizsafak/abogen/pull/10"&gt;#10&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add support for different languages in GUI.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Add voice formula feature that enables mixing different voice models. (Issue &lt;a href="https://github.com/denizsafak/abogen/issues/1"&gt;#1&lt;/a&gt;, PR &lt;a href="https://github.com/denizsafak/abogen/pull/5"&gt;#5&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add support for kokoro-onnx (If it's necessary).&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Add dark mode.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;code&gt;Troubleshooting&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;If you encounter any issues while running Abogen, try launching it from the command line with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;abogen-cli
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will start Abogen in command-line mode and display detailed error messages. Please open a new issue on the &lt;a href="https://github.com/denizsafak/abogen/issues"&gt;Issues&lt;/a&gt; page with the error message and a description of your problem.&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;Contributing&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;I welcome contributions! If you have ideas for new features, improvements, or bug fixes, please fork the repository and submit a pull request.&lt;/p&gt; 
&lt;h3&gt;For developers and contributors&lt;/h3&gt; 
&lt;p&gt;If you'd like to modify the code and contribute to development, you can &lt;a href="https://github.com/denizsafak/abogen/archive/refs/heads/main.zip"&gt;download the repository&lt;/a&gt;, extract it and run the following commands to build &lt;strong&gt;or&lt;/strong&gt; install the package:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Go to the directory where you extracted the repository and run:
pip install -e .      # Installs the package in editable mode
pip install build     # Install the build package
python -m build       # Builds the package in dist folder (optional)
abogen                # Opens the GUI
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Feel free to explore the code and make any changes you like.&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;Credits&lt;/code&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Abogen uses &lt;a href="https://github.com/hexgrad/kokoro"&gt;Kokoro&lt;/a&gt; for its high-quality, natural-sounding text-to-speech synthesis. Huge thanks to the Kokoro team for making this possible.&lt;/li&gt; 
 &lt;li&gt;Thanks to &lt;a href="https://github.com/wojiushixiaobai"&gt;@wojiushixiaobai&lt;/a&gt; for &lt;a href="https://github.com/wojiushixiaobai/Python-Embed-Win64"&gt;Embedded Python&lt;/a&gt; packages. These modified packages include pip pre-installed, enabling Abogen to function as a standalone application without requiring users to separately install Python in Windows.&lt;/li&gt; 
 &lt;li&gt;Thanks to creators of &lt;a href="https://github.com/aerkalov/ebooklib"&gt;EbookLib&lt;/a&gt;, a Python library for reading and writing ePub files, which is used for extracting text from ePub files.&lt;/li&gt; 
 &lt;li&gt;Special thanks to the &lt;a href="https://www.riverbankcomputing.com/software/pyqt/"&gt;PyQt&lt;/a&gt; team for providing the cross-platform GUI toolkit that powers Abogen's interface.&lt;/li&gt; 
 &lt;li&gt;Icons: &lt;a href="https://icons8.com/icon/aRiu1GGi6Aoe/usa"&gt;US&lt;/a&gt;, &lt;a href="https://icons8.com/icon/t3NE3BsOAQwq/great-britain"&gt;Great Britain&lt;/a&gt;, &lt;a href="https://icons8.com/icon/ly7tzANRt33n/spain"&gt;Spain&lt;/a&gt;, &lt;a href="https://icons8.com/icon/3muzEmi4dpD5/france"&gt;France&lt;/a&gt;, &lt;a href="https://icons8.com/icon/esGVrxg9VCJ1/india"&gt;India&lt;/a&gt;, &lt;a href="https://icons8.com/icon/PW8KZnP7qXzO/italy"&gt;Italy&lt;/a&gt;, &lt;a href="https://icons8.com/icon/McQbrq9qaQye/japan"&gt;Japan&lt;/a&gt;, &lt;a href="https://icons8.com/icon/zHmH8HpOmM90/brazil"&gt;Brazil&lt;/a&gt;, &lt;a href="https://icons8.com/icon/Ej50Oe3crXwF/china"&gt;China&lt;/a&gt;, &lt;a href="https://icons8.com/icon/uI49hxbpxTkp/female"&gt;Female&lt;/a&gt;, &lt;a href="https://icons8.com/icon/12351/male"&gt;Male&lt;/a&gt;, &lt;a href="https://icons8.com/icon/21698/adjust"&gt;Adjust&lt;/a&gt; and &lt;a href="https://icons8.com/icon/GskSeVoroQ7u/voice-id"&gt;Voice Id&lt;/a&gt; icons by &lt;a href="https://icons8.com/"&gt;Icons8&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;code&gt;License&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;This project is available under the MIT License - see the &lt;a href="https://github.com/denizsafak/abogen/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details. &lt;a href="https://github.com/hexgrad/kokoro"&gt;Kokoro&lt;/a&gt; is licensed under &lt;a href="https://github.com/hexgrad/kokoro/raw/main/LICENSE"&gt;Apache-2.0&lt;/a&gt; which allows commercial use, modification, distribution, and private use.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Subtitle generation currently works only for English. This is because Kokoro provides timestamp tokens only for English text. If you want subtitles in other languages, please request this feature in the &lt;a href="https://github.com/hexgrad/kokoro"&gt;Kokoro project&lt;/a&gt;. For more technical details, see &lt;a href="https://github.com/hexgrad/kokoro/raw/6d87f4ae7abc2d14dbc4b3ef2e5f19852e861ac2/kokoro/pipeline.py#L383"&gt;this line&lt;/a&gt; in the Kokoro's code.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Tags: audiobook, kokoro, text-to-speech, TTS, audiobook generator, audiobooks, text to speech, audiobook maker, audiobook creator, audiobook generator, voice-synthesis, text to audio, text to audio converter, text to speech converter, text to speech generator, text to speech software, text to speech app, epub to audio, pdf to audio, content-creation, media-generation&lt;/p&gt; 
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>sinaptik-ai/pandas-ai</title>
      <link>https://github.com/sinaptik-ai/pandas-ai</link>
      <description>&lt;p&gt;Chat with your database or your datalake (SQL, CSV, parquet). PandasAI makes data analysis conversational using LLMs and RAG.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src="https://raw.githubusercontent.com/sinaptik-ai/pandas-ai/main/assets/logo.png" alt="PandasAI" /&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/pandasai/"&gt;&lt;img src="https://img.shields.io/pypi/v/pandasai?label=Release&amp;amp;style=flat-square" alt="Release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sinaptik-ai/pandas-ai/actions/workflows/ci-core.yml/badge.svg"&gt;&lt;img src="https://github.com/sinaptik-ai/pandas-ai/actions/workflows/ci-core.yml/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sinaptik-ai/pandas-ai/actions/workflows/cd.yml/badge.svg"&gt;&lt;img src="https://github.com/sinaptik-ai/pandas-ai/actions/workflows/cd.yml/badge.svg?sanitize=true" alt="CD" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/sinaptik-ai/pandas-ai"&gt;&lt;img src="https://codecov.io/gh/sinaptik-ai/pandas-ai/branch/main/graph/badge.svg?sanitize=true" alt="Coverage" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/KYKj9F2FRH"&gt;&lt;img src="https://dcbadge.vercel.app/api/server/kF7FqH2FwS?style=flat&amp;amp;compact=true" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/pandasai"&gt;&lt;img src="https://static.pepy.tech/badge/pandasai" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/drive/1ZnO-njhL7TBOYPZaqvMvGtsjckZKrv2E?usp=sharing"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open in Colab" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;PandasAI is a Python platform that makes it easy to ask questions to your data in natural language. It helps non-technical users to interact with their data in a more natural way, and it helps technical users to save time, and effort when working with data.&lt;/p&gt; 
&lt;h1&gt;ğŸ”§ Getting started&lt;/h1&gt; 
&lt;p&gt;You can find the full documentation for PandasAI &lt;a href="https://pandas-ai.readthedocs.io/en/latest/"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can either decide to use PandasAI in your Jupyter notebooks, Streamlit apps, or use the client and server architecture from the repo.&lt;/p&gt; 
&lt;h2&gt;ğŸ“š Using the library&lt;/h2&gt; 
&lt;h3&gt;Python Requirements&lt;/h3&gt; 
&lt;p&gt;Python version &lt;code&gt;3.8+ &amp;lt;3.12&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;ğŸ“¦ Installation&lt;/h3&gt; 
&lt;p&gt;You can install the PandasAI library using pip or poetry.&lt;/p&gt; 
&lt;p&gt;With pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "pandasai&amp;gt;=3.0.0b2"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With poetry:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry add "pandasai&amp;gt;=3.0.0b2"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ğŸ’» Usage&lt;/h3&gt; 
&lt;h4&gt;Ask questions&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pandasai as pai
from pandasai_openai.openai import OpenAI

llm = OpenAI("OPEN_AI_API_KEY")

pai.config.set({
    "llm": llm
})

# Sample DataFrame
df = pai.DataFrame({
    "country": ["United States", "United Kingdom", "France", "Germany", "Italy", "Spain", "Canada", "Australia", "Japan", "China"],
    "revenue": [5000, 3200, 2900, 4100, 2300, 2100, 2500, 2600, 4500, 7000]
})

df.chat('Which are the top 5 countries by sales?')
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;China, United States, Japan, Germany, Australia
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;Or you can ask more complex questions:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;df.chat(
    "What is the total sales for the top 3 countries by sales?"
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;The total sales for the top 3 countries by sales is 16500.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Visualize charts&lt;/h4&gt; 
&lt;p&gt;You can also ask PandasAI to generate charts for you:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;df.chat(
    "Plot the histogram of countries showing for each one the gd. Use different colors for each bar",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/sinaptik-ai/pandas-ai/main/assets/histogram-chart.png?raw=true" alt="Chart" /&gt;&lt;/p&gt; 
&lt;h4&gt;Multiple DataFrames&lt;/h4&gt; 
&lt;p&gt;You can also pass in multiple dataframes to PandasAI and ask questions relating them.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pandasai as pai
from pandasai_openai.openai import OpenAI

employees_data = {
    'EmployeeID': [1, 2, 3, 4, 5],
    'Name': ['John', 'Emma', 'Liam', 'Olivia', 'William'],
    'Department': ['HR', 'Sales', 'IT', 'Marketing', 'Finance']
}

salaries_data = {
    'EmployeeID': [1, 2, 3, 4, 5],
    'Salary': [5000, 6000, 4500, 7000, 5500]
}

llm = OpenAI("OPEN_AI_API_KEY")

pai.config.set({
    "llm": llm
})

employees_df = pai.DataFrame(employees_data)
salaries_df = pai.DataFrame(salaries_data)


pai.chat("Who gets paid the most?", employees_df, salaries_df)
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;Olivia gets paid the most.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Docker Sandbox&lt;/h4&gt; 
&lt;p&gt;You can run PandasAI in a Docker sandbox, providing a secure, isolated environment to execute code safely and mitigate the risk of malicious attacks.&lt;/p&gt; 
&lt;h5&gt;Python Requirements&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "pandasai-docker"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Usage&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pandasai as pai
from pandasai_docker import DockerSandbox
from pandasai_openai.openai import OpenAI

# Initialize the sandbox
sandbox = DockerSandbox()
sandbox.start()

employees_data = {
    'EmployeeID': [1, 2, 3, 4, 5],
    'Name': ['John', 'Emma', 'Liam', 'Olivia', 'William'],
    'Department': ['HR', 'Sales', 'IT', 'Marketing', 'Finance']
}

salaries_data = {
    'EmployeeID': [1, 2, 3, 4, 5],
    'Salary': [5000, 6000, 4500, 7000, 5500]
}

llm = OpenAI("OPEN_AI_API_KEY")

pai.config.set({
    "llm": llm
})

employees_df = pai.DataFrame(employees_data)
salaries_df = pai.DataFrame(salaries_data)

pai.chat("Who gets paid the most?", employees_df, salaries_df, sandbox=sandbox)

# Don't forget to stop the sandbox when done
sandbox.stop()
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;Olivia gets paid the most.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can find more examples in the &lt;a href="https://raw.githubusercontent.com/sinaptik-ai/pandas-ai/main/examples"&gt;examples&lt;/a&gt; directory.&lt;/p&gt; 
&lt;h2&gt;ğŸ“œ License&lt;/h2&gt; 
&lt;p&gt;PandasAI is available under the MIT expat license, except for the &lt;code&gt;pandasai/ee&lt;/code&gt; directory of this repository, which has its &lt;a href="https://github.com/sinaptik-ai/pandas-ai/raw/main/ee/LICENSE"&gt;license here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you are interested in managed PandasAI Cloud or self-hosted Enterprise Offering, &lt;a href="https://getpanda.ai/pricing"&gt;contact us&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Resources&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Beta Notice&lt;/strong&gt;&lt;br /&gt; Release v3 is currently in beta. The following documentation and examples reflect the features and functionality in progress and may change before the final release.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pandas-ai.readthedocs.io/en/latest/"&gt;Docs&lt;/a&gt; for comprehensive documentation&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/sinaptik-ai/pandas-ai/main/examples"&gt;Examples&lt;/a&gt; for example notebooks&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/KYKj9F2FRH"&gt;Discord&lt;/a&gt; for discussion with the community and PandasAI team&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ¤ Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! Please check the outstanding issues and feel free to open a pull request. For more information, please check out the &lt;a href="https://raw.githubusercontent.com/sinaptik-ai/pandas-ai/main/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Thank you!&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/sinaptik-ai/pandas-ai/graphs/contributors"&gt;&lt;img src="https://contrib.rocks/image?repo=sinaptik-ai/pandas-ai" alt="Contributors" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>LeCAR-Lab/ASAP</title>
      <link>https://github.com/LeCAR-Lab/ASAP</link>
      <description>&lt;p&gt;Official implementation of [RSS 2025] "ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills"&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; ASAP: Aligning Simulation and Real-World Physics for &lt;p&gt;Learning Agile Humanoid Whole-Body Skills &lt;/p&gt;&lt;/h1&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;Robotics: Science and Systems (RSS) 2025&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://agile.human2humanoid.com/"&gt;[Website]&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2502.01143"&gt;[Arxiv]&lt;/a&gt; &lt;a href="https://www.youtube.com/watch?v=tu7LSNYWDTs&amp;amp;ab_channel=LeCARLabatCMU"&gt;[Video]&lt;/a&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/CMU-NV-logo-crop-png.png" height="50&amp;quot;" /&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://developer.nvidia.com/isaac-gym"&gt;&lt;img src="https://img.shields.io/badge/IsaacGym-Preview4-b.svg?sanitize=true" alt="IsaacGym" /&gt;&lt;/a&gt; &lt;a href="https://docs.isaacsim.omniverse.nvidia.com/4.2.0/index.html"&gt;&lt;img src="https://img.shields.io/badge/IsaacSim-4.2.0-b.svg?sanitize=true" alt="IsaacSim" /&gt;&lt;/a&gt; &lt;a href="https://docs.isaacsim.omniverse.nvidia.com/4.2.0/index.html"&gt;&lt;img src="https://img.shields.io/badge/Genesis-0.2.1-b.svg?sanitize=true" alt="IsaacSim" /&gt;&lt;/a&gt; &lt;a href="https://ubuntu.com/blog/tag/22-04-lts"&gt;&lt;img src="https://img.shields.io/badge/Platform-linux--64-orange.svg?sanitize=true" alt="Linux platform" /&gt;&lt;/a&gt; &lt;a href=""&gt;&lt;img src="https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;img src="https://agile.human2humanoid.com/static/images/asap-preview-gif-480P.gif" width="400px" /&gt; 
&lt;/div&gt; 
&lt;!-- # Table of Contents --&gt; 
&lt;h2&gt;ğŸ“š Table of Contents&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#overview"&gt;Overview&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Links: &lt;a href="https://agile.human2humanoid.com/"&gt;Website&lt;/a&gt; â€¢ &lt;a href="https://arxiv.org/pdf/2502.01143"&gt;Arxiv&lt;/a&gt; â€¢ &lt;a href="https://www.youtube.com/watch?v=tu7LSNYWDTs&amp;amp;ab_channel=LeCARLabatCMU"&gt;Video&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#installation"&gt;Installation &amp;amp; Setup&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; 2.1 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#isaacgym-conda-env"&gt;Base Frameworks&lt;/a&gt;&lt;br /&gt; 2.2 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#install-isaacgym"&gt;IsaacGym Setup&lt;/a&gt;&lt;br /&gt; 2.3 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#install-humanoidverse"&gt;HumanoidVerse Setup&lt;/a&gt;&lt;br /&gt; 2.4 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#isaaclab-environment"&gt;IsaacSim + IsaacLab Setup&lt;/a&gt;&lt;br /&gt; 2.5 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#genesis-environment"&gt;Genesis Environment Setup&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#motion-tracking-training"&gt;Training Pipelines&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; 3.1 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#motion-tracking-training"&gt;Phase-Based Motion Tracking&lt;/a&gt;&lt;br /&gt; 3.2 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#asap-delta-action-model-training"&gt;ASAP Delta Action Model&lt;/a&gt;&lt;br /&gt; - &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#train-delta-action-model"&gt;Train Delta Action Model&lt;/a&gt;&lt;br /&gt; - &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#use-delta-action-model-for-policy-finetuning"&gt;Finetune Policy with Delta Action Model&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#motion-retargeting-to-any-humanoid"&gt;Motion Retargeting to Any Humanoid&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; 4.1 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#1-smpl-shape-preparation"&gt;Step 1: SMPL Shape Preparation&lt;/a&gt;&lt;br /&gt; 4.2 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#2-smpl-motion-preparation-amass"&gt;Step 2: SMPL Motion Preparation (AMASS)&lt;/a&gt;&lt;br /&gt; 4.3 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#3-robot-xml-and-motion-config-preparation"&gt;Step 3: Robot XML &amp;amp; Motion Config&lt;/a&gt;&lt;br /&gt; 4.4 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#4-humanoid-smpl-shape-fitting"&gt;Step 4: Humanoid-SMPL Shape Fitting&lt;/a&gt;&lt;br /&gt; 4.5 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#5-humanoid-smpl-motion-retargeting"&gt;Step 5: Humanoid-SMPL Motion Retargeting&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#sim2simsim2real"&gt;Deployment: Sim2Sim &amp;amp; Sim2Real&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; 5.1 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#environment-setup"&gt;Environment Setup&lt;/a&gt;&lt;br /&gt; 5.2 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#sim2sim"&gt;Sim2Sim Deployment&lt;/a&gt;&lt;br /&gt; 5.3 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#sim2real"&gt;Sim2Real Deployment&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#citation"&gt;Citation&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#license"&gt;License&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;TODO&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release code backbone&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release phase-based motion tracking training pipeline&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release ASAP motion datasets&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release motion retargeting pipeline&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release sim2sim in MuJoCo&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release sim2real with UnitreeSDK&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release ASAP delta action model training pipeline&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;p&gt;ASAP codebase is built on top of &lt;a href="https://github.com/LeCAR-Lab/HumanoidVerse"&gt;HumanoidVerse&lt;/a&gt; (a multi-simulator framework for humanoid learning) and &lt;a href="https://github.com/LeCAR-Lab/human2humanoid"&gt;Human2Humanoid&lt;/a&gt; (our prior work on humanoid whole-body tracking).&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/LeCAR-Lab/HumanoidVerse"&gt;HumanoidVerse&lt;/a&gt; allows you to train humanoid skills in multiple simulators, including IsaacGym, IsaacSim, and Genesis. Its key design logic is the separation and modularization of simulators, tasks, and algorithms, which enables smooth transfers between different simulators and the real world with minimum effort (just one line of code change). We leverage this framework to develop &lt;a href="https://agile.human2humanoid.com/"&gt;ASAP&lt;/a&gt; and study how to best transfer policies across simulators and the real world.&lt;/p&gt; 
&lt;h2&gt;IsaacGym Conda Env&lt;/h2&gt; 
&lt;p&gt;Create mamba/conda environment, in the following we use conda for example, but you can use mamba as well.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n hvgym python=3.8
conda activate hvgym
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install IsaacGym&lt;/h3&gt; 
&lt;p&gt;Download &lt;a href="https://developer.nvidia.com/isaac-gym/download"&gt;IsaacGym&lt;/a&gt; and extract:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;wget https://developer.nvidia.com/isaac-gym-preview-4
tar -xvzf isaac-gym-preview-4
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install IsaacGym Python API:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e isaacgym/python
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Test installation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python 1080_balls_of_solitude.py  # or
python joint_monkey.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For libpython error:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check conda path: &lt;pre&gt;&lt;code class="language-bash"&gt;conda info -e
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Set LD_LIBRARY_PATH: &lt;pre&gt;&lt;code class="language-bash"&gt;export LD_LIBRARY_PATH=&amp;lt;/path/to/conda/envs/your_env/lib&amp;gt;:$LD_LIBRARY_PATH
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Install HumanoidVerse&lt;/h3&gt; 
&lt;p&gt;Install dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .
pip install -e isaac_utils
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Test with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;HYDRA_FULL_ERROR=1 python humanoidverse/train_agent.py \
+simulator=isaacgym \
+exp=locomotion \
+domain_rand=NO_domain_rand \
+rewards=loco/reward_g1_locomotion \
+robot=g1/g1_29dof_anneal_23dof \
+terrain=terrain_locomotion_plane \
+obs=loco/leggedloco_obs_singlestep_withlinvel \
num_envs=1 \
project_name=TestIsaacGymInstallation \
experiment_name=G123dof_loco \
headless=False
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;Note:&lt;/summary&gt; This is ONLY for testing, NOT how we train the locomotion policy in the ASAP paper. But still, you can train a locomotion policy by: 
 &lt;pre&gt;&lt;code class="language-bash"&gt;HYDRA_FULL_ERROR=1 python humanoidverse/train_agent.py \
+simulator=isaacgym \
+exp=locomotion \
+domain_rand=NO_domain_rand \
+rewards=loco/reward_g1_locomotion \
+robot=g1/g1_29dof_anneal_23dof \
+terrain=terrain_locomotion_plane \
+obs=loco/leggedloco_obs_singlestep_withlinvel \
num_envs=4096 \
project_name=TestIsaacGymInstallation \
experiment_name=G123dof_loco \
headless=True \
rewards.reward_penalty_curriculum=True \
rewards.reward_initial_penalty_scale=0.1 \
rewards.reward_penalty_degree=0.00003 
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;IsaacLab Environment&lt;/h2&gt; 
&lt;h3&gt;Install IsaacSim&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Download Omniverse Launcher&lt;/li&gt; 
 &lt;li&gt;Install Isaac Sim through launcher&lt;/li&gt; 
 &lt;li&gt;Set environment variables:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export ISAACSIM_PATH="${HOME}/.local/share/ov/pkg/isaac-sim-4.2.0"
export ISAACSIM_PYTHON_EXE="${ISAACSIM_PATH}/python.sh"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install IsaacLab&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/isaac-sim/IsaacLab.git
cd IsaacLab &amp;amp;&amp;amp; ./isaaclab.sh --conda hvlab
mamba activate hvlab
sudo apt install cmake build-essential
./isaaclab.sh --install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Setup HumanoidVerse&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .
pip install -e isaac_utils
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Genesis Environment&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mamba create -n hvgen python=3.10
mamba activate hvgen
pip install genesis-world torch
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .
pip install -e isaac_utils
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Motion Tracking Training&lt;/h1&gt; 
&lt;p&gt;Train a phase-based motion tracking policy to imitate Cristiano Ronaldo's signature Siuuu move&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python humanoidverse/train_agent.py \
+simulator=isaacgym \
+exp=motion_tracking \
+domain_rand=NO_domain_rand \
+rewards=motion_tracking/reward_motion_tracking_dm_2real \
+robot=g1/g1_29dof_anneal_23dof \
+terrain=terrain_locomotion_plane \
+obs=motion_tracking/deepmimic_a2c_nolinvel_LARGEnoise_history \
num_envs=4096 \
project_name=MotionTracking \
experiment_name=MotionTracking_CR7 \
robot.motion.motion_file="humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles/0-TairanTestbed_TairanTestbed_CR7_video_CR7_level1_filter_amass.pkl" \
rewards.reward_penalty_curriculum=True \
rewards.reward_penalty_degree=0.00001 \
env.config.resample_motion_when_training=False \
env.config.termination.terminate_when_motion_far=True \
env.config.termination_curriculum.terminate_when_motion_far_curriculum=True \
env.config.termination_curriculum.terminate_when_motion_far_threshold_min=0.3 \
env.config.termination_curriculum.terminate_when_motion_far_curriculum_degree=0.000025 \
robot.asset.self_collisions=0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After training, you can visualize the policy by:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python humanoidverse/eval_agent.py \
+checkpoint=logs/MotionTracking/xxxxxxxx_xxxxxxx-MotionTracking_CR7-motion_tracking-g1_29dof_anneal_23dof/model_5800.pt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This is the visualization of the policy after traning 5800 iters. The policy is able to imitate the motion of Cristiano Ronaldo's Siuuu move. With more training, the policy will be more accurate and smooth (see the video in the &lt;a href="https://arxiv.org/pdf/2502.01143"&gt;paper&lt;/a&gt;).&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/motion_tracking_5800.gif" width="400px" /&gt; 
&lt;h1&gt;ASAP delta action model training&lt;/h1&gt; 
&lt;p&gt;Note that the only difference between the delta action model training and naive motion tracking training is that delta action model needs a motion file with extra keyname &lt;code&gt;"action"&lt;/code&gt; in the motion file, so that the resulting RL policy we are training is able to use the delta action model to &lt;code&gt;"control the robot"&lt;/code&gt; to match the real-world/sim2sim motions.&lt;/p&gt; 
&lt;h2&gt;Train delta action model&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;python humanoidverse/train_agent.py \                                                                                   
  +simulator=isaacgym \
  +exp=train_delta_a_open_loop \
  +domain_rand=NO_domain_rand \
  +rewards=motion_tracking/delta_a/reward_delta_a_openloop \
  +robot=g1/g1_29dof_anneal_23dof \
  +terrain=terrain_locomotion_plane \
  +obs=delta_a/open_loop \
  num_envs=5000 \
  project_name=DeltaA_Training \
  experiment_name=openloopDeltaA_training \
  robot.motion.motion_file="&amp;lt;PATH_TO_YOUR_MOTION_FILE_WITH_ACTION_KEYNAME&amp;gt;" \
  env.config.max_episode_length_s=1.0 \
  rewards.reward_scales.penalty_minimal_action_norm=-0.1 \
  +device=cuda:0 \
  env.config.resample_motion_when_training=True \
  env.config.resample_time_interval_s=10000
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Use delta action model for policy finetuning&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;HYDRA_FULL_ERROR=1 \
python humanoidverse/train_agent.py \
+simulator=isaacgym \
+exp=train_delta_a_closed_loop \
algo.config.policy_checkpoint='&amp;lt;PATH_TO_YOUR_DELTA_A_MODEL&amp;gt;' \
+domain_rand=NO_domain_rand_finetune_with_deltaA \
+rewards=motion_tracking/reward_motion_tracking_dm_simfinetuning \
+robot=g1/g1_29dof_anneal_23dof \
+terrain=terrain_locomotion_plane \
+obs=delta_a/train_policy_with_delta_a \
num_envs=4096 \
project_name=DeltaA_Finetune \
experiment_name=finetune_with_deltaA \
robot.motion.motion_file="&amp;lt;PATH_TO_YOUR_MOTION_FILE&amp;gt;" \
+opt=wandb \
env.config.add_extra_action=True \
+checkpoint="&amp;lt;PATH_TO_YOUR_POLICY_TO_BE_FINETUNED&amp;gt;" \
domain_rand.push_robots=False \
env.config.noise_to_initial_level=1 \
rewards.reward_penalty_curriculum=True \
+device=cuda:0 \
algo.config.save_interval=5 \
algo.config.num_learning_iterations=1000 

&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Motion Retargeting to Any Humanoid&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Here we share a generic humanoid motion retargeting pipeline to any humanoid from &lt;a href="https://github.com/ZhengyiLuo/PHC"&gt;PHC&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] We have provided all the SMPL motions (&lt;code&gt;ASAP/humanoidverse/data/motions/raw_tairantestbed_smpl&lt;/code&gt;) and retargtted G1 motions (&lt;code&gt;ASAP/humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles&lt;/code&gt;) used in the ASAP paper in this codebase. If you are interested in using these motions G1, you can ignore this section. If you are interested in retargeting other humanoids or other motions, you can follow the steps below to prepare the SMPL shapes and motions.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;It has three steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;SMPL Shape preparation&lt;/li&gt; 
 &lt;li&gt;SMPL Motion preparation&lt;/li&gt; 
 &lt;li&gt;Robot XML and Motion Config preparation&lt;/li&gt; 
 &lt;li&gt;Humanoid-SMPL shape fitting&lt;/li&gt; 
 &lt;li&gt;Humanoid-SMPL motion retargeting&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;1. SMPL Shape preparation&lt;/h2&gt; 
&lt;p&gt;Download &lt;a href="https://download.is.tue.mpg.de/download.php?domain=smpl&amp;amp;sfile=SMPL_python_v.1.1.0.zip"&gt;v1.1.0 SMPL files with pkl format&lt;/a&gt; and put it under &lt;code&gt;humanoidverse/data/smpl/&lt;/code&gt;, and you should have:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;|-- ASAP
    |-- humanoidverse
        |-- data
            |-- smpl
                |-- SMPL_python_v.1.1.0.zip
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then &lt;code&gt;cd ASAP/humanoidverse/data/smpl/&lt;/code&gt; and &lt;code&gt;unzip SMPL_python_v.1.1.0.zip&lt;/code&gt;, after some copying and moving, you should have:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;|-- ASAP
    |-- humanoidverse
        |-- data
            |-- smpl
                |-- SMPL_python_v.1.1.0
                |-- models
                    |-- basicmodel_f_lbs_10_207_0_v1.1.0.pkl
                    |-- basicmodel_m_lbs_10_207_0_v1.1.0.pkl
                    |-- basicmodel_neutral_lbs_10_207_0_v1.1.0.pkl
                |-- smpl_webuser
                |-- ...

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Rename these three pkl files and move it under smpl like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;|-- ASAP
    |-- humanoidverse
        |-- data
            |-- smpl
                |-- SMPL_FEMALE.pkl
                |-- SMPL_MALE.pkl
                |-- SMPL_NEUTRAL.pkl
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;2. SMPL Motion preparation (AMASS)&lt;/h2&gt; 
&lt;p&gt;Download &lt;a href="https://amass.is.tue.mpg.de/index.html"&gt;AMASS Dataset&lt;/a&gt; with &lt;code&gt;SMPL + H G format&lt;/code&gt; and put it under &lt;code&gt;humanoidverse/data/motions/AMASS/AMASS_Complete/&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;|-- ASAP
    |-- humanoidverse
        |-- data
            |-- AMASS
                |-- AMASS_Complete
                    |-- ACCAD.tar.bz2
                    |-- BMLhandball.tar.bz2
                    |-- BMLmovi.tar.bz2
                    |-- BMLrub.tar
                    |-- CMU.tar.bz2
                    |-- ...
                    |-- Transitions.tar.bz2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And then cd ASAP/humanoidverse/data/motions/AMASS/AMASS_Complete/ and extract all the motion files by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;for file in *.tar.bz2; do
    tar -xvjf "$file"
done
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you should have:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;|-- ASAP
    |-- humanoidverse
        |-- data
            |-- AMASS
                |-- AMASS_Complete
                    |-- ACCAD
                    |-- BioMotionLab_NTroje
                    |-- BMLhandball
                    |-- BMLmovi
                    |-- CMU
                    |-- ...
                    |-- Transitions
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;3. Robot XML and Motion Config preparation&lt;/h2&gt; 
&lt;p&gt;Make sure you have robot xml and meshes ready at (G1 as example) &lt;code&gt;humanoidverse/data/robots/g1/g1_29dof_anneal_23dof_fitmotionONLY.xml&lt;/code&gt; And add your config for the robot motion in &lt;code&gt;humanoidverse/config/robot/g1_29dof_anneal_23dof.yaml&lt;/code&gt; with like the following. Remember to link the xml path in the config.&lt;/p&gt; 
&lt;h2&gt;4. Humanoid-SMPL shape fitting&lt;/h2&gt; 
&lt;p&gt;Run the following command to fit the SMPL shape to the humanoid.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python scripts/data_process/fit_smpl_shape.py +robot=g1/g1_29dof_anneal_23dof
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And you should have you shape file located at &lt;code&gt;humanoidverse/data/shape/g1_29dof_anneal_23dof/shape_optimized_v1.pkl&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;If you want to visualize the shape, you can run with flag &lt;code&gt;+vis=True&lt;/code&gt;, then you can have visualization of the fitted SMPL body shape and the humanoid body keypoints like this shape. The blue is the humanoid body keypoints and the orange is the fitted SMPL body keypoint. You can tune the &lt;code&gt;robot motion&lt;/code&gt; in &lt;code&gt;humanoidverse/config/robot/g1_29dof_anneal_23dof.yaml&lt;/code&gt; to adjust the correspondence, extend links lengths to get better fitted SMPL shape.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/g1_29dof_anneal_23dof_shape.png" width="400px" /&gt; 
&lt;h2&gt;5. Humanoid-SMPL motion retargeting&lt;/h2&gt; 
&lt;p&gt;Run the following command to retarget the motion to the humanoid.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python scripts/data_process/fit_smpl_motion.py +robot=g1/g1_29dof_anneal_23dof
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Visualize motion&lt;/h3&gt; 
&lt;p&gt;Run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python scripts/vis/vis_q_mj.py +robot=g1/g1_29dof_anneal_23dof +visualize_motion_file="humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles/0-motions_raw_tairantestbed_smpl_video_side_jump_level4_filter_amass.pkl"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To test, and you should have you one single motion file located at &lt;code&gt;humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles/0-motions_raw_tairantestbed_smpl_video_side_jump_level4_filter_amass.pkl&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;If you want to visualize the motion, you can run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python scripts/vis/vis_q_mj.py +robot=g1/g1_29dof_anneal_23dof +visualize_motion_file="humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles/0-motions_raw_tairantestbed_smpl_video_side_jump_level4_filter_amass.pkl"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should have&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/g1_29dof_anneal_23dof_motion.gif" width="400px" /&gt; 
&lt;h1&gt;Sim2Sim/Sim2Real&lt;/h1&gt; 
&lt;h2&gt;Environment Setup&lt;/h2&gt; 
&lt;p&gt;Env Installation:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mamba create -n asap_deploy python=3.10
mamba activate asap_deploy
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install ros2-python&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# this adds the conda-forge channel to the new created environment configuration 
conda config --env --add channels conda-forge
# and the robostack channel
conda config --env --add channels robostack-staging
# remove the defaults channel just in case, this might return an error if it is not in the list which is ok
conda config --env --remove channels defaults
# install the ros2-python package
conda install ros-humble-desktop
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Test Ros2Installation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;rviz2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should see the UI like this:&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/rviz.png" width="400px" /&gt; 
&lt;p&gt;Install Unitree SDK&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone git@github.com:unitreerobotics/unitree_sdk2_python.git
cd unitree_sdk2_python
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;minor issue to fix:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade numpy scipy
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Sim2Sim&lt;/h2&gt; 
&lt;p&gt;start the simulation in the sim2real folder:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python sim_env/base_sim.py --config=config/g1_29dof_hist.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;in another terminal, start the policy:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python rl_policy/deepmimic_dec_loco_height.py --config=config/g1_29dof_hist.yaml --loco_model_path=./models/dec_loco/20250109_231507-noDR_rand_history_loco_stand_height_noise-decoupled_locomotion-g1_29dof/model_6600.onnx --mimic_model_paths=./models/mimic
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;click to the policy terminal and press &lt;code&gt;]&lt;/code&gt; to activate the locomotion policy&lt;/li&gt; 
 &lt;li&gt;click to the policy terminal and press &lt;code&gt;[&lt;/code&gt; to activate the asap policy (phase-based motion tracking policy)&lt;/li&gt; 
 &lt;li&gt;click to the policy terminal and press &lt;code&gt;;&lt;/code&gt; to switch to the asap policy&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;i&lt;/code&gt; to make the robot the initial position&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;o&lt;/code&gt; to emergence stop the robot&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;9&lt;/code&gt; in mujoco viewer to release the robostack&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;=&lt;/code&gt; to switch between tapping and walking for the locomotion policy&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;w/a/s/d&lt;/code&gt; to control the linear velocity&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;q/e&lt;/code&gt; to control the angular velocity&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;z&lt;/code&gt; to set all commands to zero&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;And you should be able to play around with some checkpoints from the ASAP paper. Have fun!&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/asap-sim2sim-clip0-ezgif.com-video-to-gif-converter.gif" width="300px" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/asap-sim2sim-clip1-ezgif.com-video-to-gif-converter.gif" width="300px" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/asap-sim2sim-clip3-ezgif.com-video-to-gif-converter.gif" width="300px" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/asap-sim2sim-clip4-ezgif.com-video-to-gif-converter.gif" width="300px" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/asap-sim2sim-clip5-ezgif.com-video-to-gif-converter.gif" width="300px" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/asap-sim2sim-clip6-ezgif.com-video-to-gif-converter.gif" width="300px" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;!-- Replace gif1.gif ... gif6.gif with your actual gif filenames and optionally add captions below each if desired --&gt; 
&lt;h2&gt;Sim2Real&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;Note from Tairan&lt;/code&gt;: make sure to make the G1 robot to 29dof following this &lt;a href="https://support.unitree.com/home/en/G1_developer/waist_fastener"&gt;doc&lt;/a&gt; and restart the robot after waist unlocking. If you don't know how to log into the Unitree Explore APP, contact unitree support.&lt;/p&gt; 
&lt;p&gt;Enter Low-Level for g1&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Open humanoid and wait until the head blue light is constantly on&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;L2+R2&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;L2+A&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;L2+B&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Connect PC to the G1 by ethernet cable and configure the network following &lt;a href="https://support.unitree.com/home/en/G1_developer/quick_development"&gt;this document&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Before starting the policy, modify the &lt;code&gt;config/g1_29dof_hist.yaml&lt;/code&gt; to set &lt;code&gt;INTERFACE&lt;/code&gt; to &lt;code&gt;eth0&lt;/code&gt; (if you are using linux), basically the network interface that you are using to connect to the robot with your PC's IP shown as &lt;code&gt;192.168.123.xxx&lt;/code&gt; in &lt;code&gt;ifconfig&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;start the policy:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python rl_policy/deepmimic_dec_loco_height.py --config=config/g1_29dof_hist.yaml --loco_model_path=./models/dec_loco/20250109_231507-noDR_rand_history_loco_stand_height_noise-decoupled_locomotion-g1_29dof/model_6600.onnx --mimic_model_paths=./models/mimic
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;click to the policy terminal and press &lt;code&gt;]&lt;/code&gt; to activate the locomotion policy&lt;/li&gt; 
 &lt;li&gt;click to the policy terminal and press &lt;code&gt;[&lt;/code&gt; to activate the asap policy (phase-based motion tracking policy)&lt;/li&gt; 
 &lt;li&gt;click to the policy terminal and press &lt;code&gt;;&lt;/code&gt; to switch to the asap policy&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;i&lt;/code&gt; to make the robot the initial position&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;o&lt;/code&gt; to emergence stop the robot&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;9&lt;/code&gt; in mujoco viewer to release the robostack&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;=&lt;/code&gt; to switch between tapping and walking for the locomotion policy&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;w/a/s/d&lt;/code&gt; to control the linear velocity&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;q/e&lt;/code&gt; to control the angular velocity&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;z&lt;/code&gt; to set all commands to zero&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;o&lt;/code&gt; to emergence stop the robot&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;â€¼ï¸Alert &amp;amp; Disclaimer&lt;/h3&gt; 
&lt;p&gt;Deploying these models on physical hardware can be hazardous. Unless you have deep simâ€‘toâ€‘real expertise and robust safety protocols, we strongly advise against running the model on real robots. These models are supplied for research use only, and we disclaim all responsibility for any harm, loss, or malfunction arising from their deployment.&lt;/p&gt; 
&lt;h3&gt;Demo code to collect real-world data&lt;/h3&gt; 
&lt;p&gt;We provide a demo code to collect real-world data in the &lt;code&gt;sim2real/rl_policy/listener_deltaa.py&lt;/code&gt; file. Since MoCap setup is hard to transfer across different robots/labs, we hope this code can help you to collect data for your own experiments. Contact us (&lt;a href="mailto:tairanh@andrew.cmu.edu"&gt;tairanh@andrew.cmu.edu&lt;/a&gt;) if you have any questions.&lt;/p&gt; 
&lt;h1&gt;Citation&lt;/h1&gt; 
&lt;p&gt;If you find our work useful, please consider citing us!&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{he2025asap,
  title={ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills},
  author={He, Tairan and Gao, Jiawei and Xiao, Wenli and Zhang, Yuanhang and Wang, Zi and Wang, Jiashun and Luo, Zhengyi and He, Guanqi and Sobanbabu, Nikhil and Pan, Chaoyi and Yi, Zeji and Qu, Guannan and Kitani, Kris and Hodgins, Jessica and Fan, Linxi "Jim" and Zhu, Yuke and Liu, Changliu and Shi, Guanya},
  journal={arXiv preprint arXiv:2502.01143},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>bytedance/Dolphin</title>
      <link>https://github.com/bytedance/Dolphin</link>
      <description>&lt;p&gt;The official repo for â€œDolphin: Document Image Parsing via Heterogeneous Anchor Promptingâ€, ACL, 2025.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/bytedance/Dolphin/master/assets/dolphin.png" width="300" /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://arxiv.org/abs/2505.14059"&gt; &lt;img src="https://img.shields.io/badge/Paper-arXiv-red" /&gt; &lt;/a&gt; 
 &lt;a href="https://huggingface.co/ByteDance/Dolphin"&gt; &lt;img src="https://img.shields.io/badge/HuggingFace-Dolphin-yellow" /&gt; &lt;/a&gt; 
 &lt;a href="https://modelscope.cn/models/ByteDance/Dolphin"&gt; &lt;img src="https://img.shields.io/badge/ModelScope-Dolphin-purple" /&gt; &lt;/a&gt; 
 &lt;a href="http://115.190.42.15:8888/dolphin/"&gt; &lt;img src="https://img.shields.io/badge/Demo-Dolphin-blue" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/bytedance/Dolphin"&gt; &lt;img src="https://img.shields.io/badge/Code-Github-green" /&gt; &lt;/a&gt; 
 &lt;a href="https://opensource.org/licenses/MIT"&gt; &lt;img src="https://img.shields.io/badge/License-MIT-lightgray" /&gt; &lt;/a&gt; 
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/bytedance/Dolphin/master/assets/demo.gif" width="800" /&gt; 
&lt;/div&gt; 
&lt;h1&gt;Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting&lt;/h1&gt; 
&lt;p&gt;Dolphin (&lt;strong&gt;Do&lt;/strong&gt;cument Image &lt;strong&gt;P&lt;/strong&gt;arsing via &lt;strong&gt;H&lt;/strong&gt;eterogeneous Anchor Prompt&lt;strong&gt;in&lt;/strong&gt;g) is a novel multimodal document image parsing model following an analyze-then-parse paradigm. This repository contains the demo code and pre-trained models for Dolphin.&lt;/p&gt; 
&lt;h2&gt;ğŸ“‘ Overview&lt;/h2&gt; 
&lt;p&gt;Document image parsing is challenging due to its complexly intertwined elements such as text paragraphs, figures, formulas, and tables. Dolphin addresses these challenges through a two-stage approach:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ” Stage 1&lt;/strong&gt;: Comprehensive page-level layout analysis by generating element sequence in natural reading order&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ§© Stage 2&lt;/strong&gt;: Efficient parallel parsing of document elements using heterogeneous anchors and task-specific prompts&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/bytedance/Dolphin/master/assets/framework.png" width="680" /&gt; 
&lt;/div&gt; 
&lt;p&gt;Dolphin achieves promising performance across diverse page-level and element-level parsing tasks while ensuring superior efficiency through its lightweight architecture and parallel parsing mechanism.&lt;/p&gt; 
&lt;h2&gt;ğŸš€ Demo&lt;/h2&gt; 
&lt;p&gt;Try our demo on &lt;a href="http://115.190.42.15:8888/dolphin/"&gt;Demo-Dolphin&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ“… Changelog&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ”¥ &lt;strong&gt;2025.07.10&lt;/strong&gt; Released the &lt;em&gt;Fox-Page Benchmark&lt;/em&gt;, a manually refined subset of the original &lt;a href="https://github.com/ucaslcl/Fox"&gt;Fox dataset&lt;/a&gt;. Download via: &lt;a href="https://pan.baidu.com/share/init?surl=t746ULp6iU5bUraVrPlMSw&amp;amp;pwd=fox1"&gt;Baidu Yun&lt;/a&gt; | &lt;a href="https://drive.google.com/file/d/1yZQZqI34QCqvhB4Tmdl3X_XEvYvQyP0q/view?usp=sharing"&gt;Google Drive&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;ğŸ”¥ &lt;strong&gt;2025.06.30&lt;/strong&gt; Added &lt;a href="https://github.com/bytedance/Dolphin/raw/master/deployment/tensorrt_llm/ReadMe.md"&gt;TensorRT-LLM support&lt;/a&gt; for accelerated inferenceï¼&lt;/li&gt; 
 &lt;li&gt;ğŸ”¥ &lt;strong&gt;2025.06.27&lt;/strong&gt; Added &lt;a href="https://github.com/bytedance/Dolphin/raw/master/deployment/vllm/ReadMe.md"&gt;vLLM support&lt;/a&gt; for accelerated inferenceï¼&lt;/li&gt; 
 &lt;li&gt;ğŸ”¥ &lt;strong&gt;2025.06.13&lt;/strong&gt; Added multi-page PDF document parsing capability.&lt;/li&gt; 
 &lt;li&gt;ğŸ”¥ &lt;strong&gt;2025.05.21&lt;/strong&gt; Our demo is released at &lt;a href="http://115.190.42.15:8888/dolphin/"&gt;link&lt;/a&gt;. Check it out!&lt;/li&gt; 
 &lt;li&gt;ğŸ”¥ &lt;strong&gt;2025.05.20&lt;/strong&gt; The pretrained model and inference code of Dolphin are released.&lt;/li&gt; 
 &lt;li&gt;ğŸ”¥ &lt;strong&gt;2025.05.16&lt;/strong&gt; Our paper has been accepted by ACL 2025. Paper link: &lt;a href="https://arxiv.org/abs/2505.14059"&gt;arXiv&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ› ï¸ Installation&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Clone the repository:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/ByteDance/Dolphin.git
cd Dolphin
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install the dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Download the pre-trained models using one of the following options:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Option A: Original Model Format (config-based)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Download from &lt;a href="https://pan.baidu.com/s/15zcARoX0CTOHKbW8bFZovQ?pwd=9rpx"&gt;Baidu Yun&lt;/a&gt; or &lt;a href="https://drive.google.com/drive/folders/1PQJ3UutepXvunizZEw-uGaQ0BCzf-mie?usp=sharing"&gt;Google Drive&lt;/a&gt; and put them in the &lt;code&gt;./checkpoints&lt;/code&gt; folder.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Option B: Hugging Face Model Format&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Visit our Huggingface &lt;a href="https://huggingface.co/ByteDance/Dolphin"&gt;model card&lt;/a&gt;, or download model by:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Download the model from Hugging Face Hub
git lfs install
git clone https://huggingface.co/ByteDance/Dolphin ./hf_model
# Or use the Hugging Face CLI
pip install huggingface_hub
huggingface-cli download ByteDance/Dolphin --local-dir ./hf_model
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;âš¡ Inference&lt;/h2&gt; 
&lt;p&gt;Dolphin provides two inference frameworks with support for two parsing granularities:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Page-level Parsing&lt;/strong&gt;: Parse the entire document page into a structured JSON and Markdown format&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Element-level Parsing&lt;/strong&gt;: Parse individual document elements (text, table, formula)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ“„ Page-level Parsing&lt;/h3&gt; 
&lt;h4&gt;Using Original Framework (config-based)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Process a single document image
python demo_page.py --config ./config/Dolphin.yaml --input_path ./demo/page_imgs/page_1.jpeg --save_dir ./results

# Process a single document pdf
python demo_page.py --config ./config/Dolphin.yaml --input_path ./demo/page_imgs/page_6.pdf --save_dir ./results

# Process all documents in a directory
python demo_page.py --config ./config/Dolphin.yaml --input_path ./demo/page_imgs --save_dir ./results

# Process with custom batch size for parallel element decoding
python demo_page.py --config ./config/Dolphin.yaml --input_path ./demo/page_imgs --save_dir ./results --max_batch_size 8
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Using Hugging Face Framework&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Process a single document image
python demo_page_hf.py --model_path ./hf_model --input_path ./demo/page_imgs/page_1.jpeg --save_dir ./results

# Process a single document pdf
python demo_page_hf.py --model_path ./hf_model --input_path ./demo/page_imgs/page_6.pdf --save_dir ./results

# Process all documents in a directory
python demo_page_hf.py --model_path ./hf_model --input_path ./demo/page_imgs --save_dir ./results

# Process with custom batch size for parallel element decoding
python demo_page_hf.py --model_path ./hf_model --input_path ./demo/page_imgs --save_dir ./results --max_batch_size 16
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ğŸ§© Element-level Parsing&lt;/h3&gt; 
&lt;h4&gt;Using Original Framework (config-based)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Process a single table image
python demo_element.py --config ./config/Dolphin.yaml --input_path ./demo/element_imgs/table_1.jpeg --element_type table

# Process a single formula image
python demo_element.py --config ./config/Dolphin.yaml --input_path ./demo/element_imgs/line_formula.jpeg --element_type formula

# Process a single text paragraph image
python demo_element.py --config ./config/Dolphin.yaml --input_path ./demo/element_imgs/para_1.jpg --element_type text
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Using Hugging Face Framework&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Process a single table image
python demo_element_hf.py --model_path ./hf_model --input_path ./demo/element_imgs/table_1.jpeg --element_type table

# Process a single formula image
python demo_element_hf.py --model_path ./hf_model --input_path ./demo/element_imgs/line_formula.jpeg --element_type formula

# Process a single text paragraph image
python demo_element_hf.py --model_path ./hf_model --input_path ./demo/element_imgs/para_1.jpg --element_type text
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸŒŸ Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ”„ Two-stage analyze-then-parse approach based on a single VLM&lt;/li&gt; 
 &lt;li&gt;ğŸ“Š Promising performance on document parsing tasks&lt;/li&gt; 
 &lt;li&gt;ğŸ” Natural reading order element sequence generation&lt;/li&gt; 
 &lt;li&gt;ğŸ§© Heterogeneous anchor prompting for different document elements&lt;/li&gt; 
 &lt;li&gt;â±ï¸ Efficient parallel parsing mechanism&lt;/li&gt; 
 &lt;li&gt;ğŸ¤— Support for Hugging Face Transformers for easier integration&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“® Notice&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Call for Bad Cases:&lt;/strong&gt; If you have encountered any cases where the model performs poorly, we would greatly appreciate it if you could share them in the issue. We are continuously working to optimize and improve the model.&lt;/p&gt; 
&lt;h2&gt;ğŸ’– Acknowledgement&lt;/h2&gt; 
&lt;p&gt;We would like to acknowledge the following open-source projects that provided inspiration and reference for this work:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/clovaai/donut/"&gt;Donut&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/facebookresearch/nougat"&gt;Nougat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Ucas-HaoranWei/GOT-OCR2.0"&gt;GOT&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/MinerU/tree/master"&gt;MinerU&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/Swin-Transformer"&gt;Swin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/transformers"&gt;Hugging Face Transformers&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“ Citation&lt;/h2&gt; 
&lt;p&gt;If you find this code useful for your research, please use the following BibTeX entry.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{feng2025dolphin,
  title={Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting},
  author={Feng, Hao and Wei, Shu and Fei, Xiang and Shi, Wei and Han, Yingdong and Liao, Lei and Lu, Jinghui and Wu, Binghong and Liu, Qi and Lin, Chunhui and others},
  journal={arXiv preprint arXiv:2505.14059},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#bytedance/Dolphin&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=bytedance/Dolphin&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>omkarcloud/botasaurus</title>
      <link>https://github.com/omkarcloud/botasaurus</link>
      <description>&lt;p&gt;The All in One Framework to Build Undefeatable Scrapers&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/mascot.png" alt="botasaurus" /&gt; &lt;/p&gt; 
&lt;div align="center" style="margin-top: 0;"&gt; 
 &lt;h1&gt;ğŸ¤– Botasaurus ğŸ¤–&lt;/h1&gt; 
&lt;/div&gt; 
&lt;h3 align="center"&gt; The All in One Framework to Build Undefeatable Scrapers &lt;/h3&gt; 
&lt;p align="center"&gt; &lt;b&gt;The web has evolved. Finally, web scraping has too.&lt;/b&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://views.whatilearened.today/views/github/omkarcloud/botasaurus.svg?sanitize=true" width="80px" height="28px" alt="View" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://gitpod.io/#https://github.com/omkarcloud/botasaurus-starter"&gt; &lt;img alt="Run in Gitpod" src="https://gitpod.io/button/open-in-gitpod.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;ğŸ¿ï¸ Botasaurus In a Nutshell&lt;/h2&gt; 
&lt;p&gt;How wonderful that of all the web scraping tools out there, you chose to learn about Botasaurus. Congratulations!&lt;/p&gt; 
&lt;p&gt;And now that you are here, you are in for an exciting, unusual, and rewarding journey that will make your web scraping life a lot easier.&lt;/p&gt; 
&lt;p&gt;Now, let me tell you about Botasaurus in bullet points. (Because as per marketing gurus, YOU as a member of the Developer Tribe have a VERY short attention span.)&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;So, what is Botasaurus?&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Botasaurus is an all-in-one web scraping framework that enables you to build awesome scrapers in less time, with less code, and with more fun.&lt;/p&gt; 
&lt;p&gt;We have put all our web scraping experience and best practices into Botasaurus to save you hundreds of hours of development time!&lt;/p&gt; 
&lt;p&gt;Now, for the magical powers awaiting you after learning Botasaurus:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;In terms of humaneness, what Superman is to Man, Botasaurus is to Selenium and Playwright. Easily pass every (Yes, E-V-E-R-Y) bot test, and build undetected scrapers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In the video below, watch as we &lt;strong&gt;bypass some of the best bot detection systems&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;âœ… &lt;a href="https://nopecha.com/demo/cloudflare"&gt;Cloudflare Web Application Firewall (WAF)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;a href="https://www.browserscan.net/bot-detection"&gt;BrowserScan Bot Detection&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;a href="https://fingerprint.com/products/bot-detection/"&gt;Fingerprint Bot Detection&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;a href="https://antoinevastel.com/bots/datadome"&gt;Datadome Bot Detection&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;a href="https://turnstile.zeroclover.io/"&gt;Cloudflare Turnstile CAPTCHA&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; 
 &lt;video src="https://github.com/user-attachments/assets/b4f6171f-f2a2-4255-9feb-2973ee9a25ae"&gt;&lt;/video&gt; &lt;/p&gt; 
&lt;p&gt;ğŸ”— Want to try it yourself? See the code behind these tests &lt;a href="https://github.com/omkarcloud/botasaurus/raw/master/bot_detection_tests.py"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Perform realistic, human-like mouse movements and say sayonara to detection &lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/human-mode-demo.gif" alt="human-mode-demo" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Convert your scraper into a desktop app for Mac, Windows, and Linux in 1 day, so not only developers but everyone can use your web scraper.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/desktop-app-photo.png" alt="desktop-app-photo" /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Turn your scraper into a beautiful website, making it easy for your customers to use it from anywhere, anytime.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/google-maps-scraper/master/screenshots/demo.gif" alt="pro-gmaps-demo" /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Save up to 97%, yes 97%, on browser proxy costs by using &lt;a href="https://github.com/omkarcloud/botasaurus#how-to-significantly-reduce-proxy-costs-when-scraping-at-scale"&gt;browser-based fetch requests.&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Easily save hours of development time with easy parallelization, profiles, extensions, and proxy configuration. Botasaurus makes asynchronous, parallel scraping child's play.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Use caching, sitemap, data cleaning, and other utilities to save hours of time spent writing and debugging code.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Easily scale your scraper to multiple machines with Kubernetes, and get your data faster than ever.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;And those are just the highlights. I mean!&lt;/p&gt; 
&lt;p&gt;There is so much more to Botasaurus that you will be amazed at how much time you will save with it.&lt;/p&gt; 
&lt;h2&gt;ğŸš€ Getting Started with Botasaurus&lt;/h2&gt; 
&lt;p&gt;Let's dive right in with a straightforward example to understand Botasaurus.&lt;/p&gt; 
&lt;p&gt;In this example, we will go through the steps to scrape the heading text from &lt;a href="https://www.omkar.cloud/"&gt;https://www.omkar.cloud/&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/starter-bot-running.gif" alt="Botasaurus in action" /&gt;&lt;/p&gt; 
&lt;h3&gt;Step 1: Install Botasaurus&lt;/h3&gt; 
&lt;p&gt;First things first, you need to install Botasaurus. Run the following command in your terminal:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python -m pip install --upgrade botasaurus
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Step 2: Set Up Your Botasaurus Project&lt;/h3&gt; 
&lt;p&gt;Next, let's set up the project:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Create a directory for your Botasaurus project and navigate into it:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;mkdir my-botasaurus-project
cd my-botasaurus-project
code .  # This will open the project in VSCode if you have it installed
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Step 3: Write the Scraping Code&lt;/h3&gt; 
&lt;p&gt;Now, create a Python script named &lt;code&gt;main.py&lt;/code&gt; in your project directory and paste the following code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.browser import browser, Driver

@browser
def scrape_heading_task(driver: Driver, data):
    # Visit the Omkar Cloud website
    driver.get("https://www.omkar.cloud/")
    
    # Retrieve the heading element's text
    heading = driver.get_text("h1")

    # Save the data as a JSON file in output/scrape_heading_task.json
    return {
        "heading": heading
    }
     
# Initiate the web scraping task
scrape_heading_task()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Let's understand this code:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;We define a custom scraping task, &lt;code&gt;scrape_heading_task&lt;/code&gt;, decorated with &lt;code&gt;@browser&lt;/code&gt;:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@browser
def scrape_heading_task(driver: Driver, data):
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Botasaurus automatically provides a Humane Driver to our function:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;def scrape_heading_task(driver: Driver, data):
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Inside the function, we: 
  &lt;ul&gt; 
   &lt;li&gt;Visit Omkar Cloud&lt;/li&gt; 
   &lt;li&gt;Extract the heading text&lt;/li&gt; 
   &lt;li&gt;Return the data to be automatically saved as &lt;code&gt;scrape_heading_task.json&lt;/code&gt; by Botasaurus:&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;    driver.get("https://www.omkar.cloud/")
    heading = driver.get_text("h1")
    return {"heading": heading}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Finally, we initiate the scraping task:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Initiate the web scraping task
scrape_heading_task()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Step 4: Run the Scraping Task&lt;/h3&gt; 
&lt;p&gt;Time to run it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After executing the script, it will:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Launch Google Chrome&lt;/li&gt; 
 &lt;li&gt;Visit &lt;a href="https://www.omkar.cloud/"&gt;omkar.cloud&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Extract the heading text&lt;/li&gt; 
 &lt;li&gt;Save it automatically as &lt;code&gt;output/scrape_heading_task.json&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/starter-bot-running.gif" alt="Botasaurus in action" /&gt;&lt;/p&gt; 
&lt;p&gt;Now, let's explore another way to scrape the heading using the &lt;code&gt;request&lt;/code&gt; module. Replace the previous code in &lt;code&gt;main.py&lt;/code&gt; with the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.request import request, Request
from botasaurus.soupify import soupify

@request
def scrape_heading_task(request: Request, data):
    # Visit the Omkar Cloud website
    response = request.get("https://www.omkar.cloud/")

    # Create a BeautifulSoup object    
    soup = soupify(response)
    
    # Retrieve the heading element's text
    heading = soup.find('h1').get_text()

    # Save the data as a JSON file in output/scrape_heading_task.json
    return {
        "heading": heading
    }     
# Initiate the web scraping task
scrape_heading_task()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In this code:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;We scrape the HTML using &lt;code&gt;request&lt;/code&gt;, which is specifically designed for making browser-like humane requests.&lt;/li&gt; 
 &lt;li&gt;Next, we parse the HTML into a &lt;code&gt;BeautifulSoup&lt;/code&gt; object using &lt;code&gt;soupify()&lt;/code&gt; and extract the heading.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Step 5: Run the Scraping Task (which makes Humane HTTP Requests)&lt;/h3&gt; 
&lt;p&gt;Finally, run it again:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This time, you will observe the exact same result as before, but instead of opening a whole browser, we are making browser-like humane HTTP requests.&lt;/p&gt; 
&lt;h2&gt;ğŸ’¡ Understanding Botasaurus&lt;/h2&gt; 
&lt;h3&gt;What is Botasaurus Driver, and why should I use it over Selenium and Playwright?&lt;/h3&gt; 
&lt;p&gt;Botasaurus Driver is a web automation driver like Selenium, and the single most important reason to use it is because it is truly humane. You will not, and I repeat NOT, have any issues accessing any website.&lt;/p&gt; 
&lt;p&gt;Plus, it is super fast to launch and use, and the API is designed by and for web scrapers, and you will love it.&lt;/p&gt; 
&lt;h3&gt;How do I access Cloudflare-protected pages using Botasaurus?&lt;/h3&gt; 
&lt;p&gt;Cloudflare is the most popular protection system on the web. So, let's see how Botasaurus can help you solve various Cloudflare challenges.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Connection Challenge&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;This is the single most popular challenge and requires making a browser-like connection with appropriate headers. It's commonly used for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Product Pages&lt;/li&gt; 
 &lt;li&gt;Blog Pages&lt;/li&gt; 
 &lt;li&gt;Search Result Pages&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- Example Page: https://www.g2.com/products/github/reviews --&gt; 
&lt;h4&gt;What Works?&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Visiting the website via Google Referrer (which makes it seem as if the user has arrived from a Google search).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.browser import browser, Driver

@browser
def scrape_heading_task(driver: Driver, data):
    # Visit the website via Google Referrer
    driver.google_get("https://www.cloudflare.com/en-in/")
    driver.prompt()
    heading = driver.get_text('h1')
    return heading

scrape_heading_task()
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use the request module. The Request Object is smart and, by default, visits any link with a Google Referrer. Although it works, you will need to use retries.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.request import request, Request

@request(max_retry=10)
def scrape_heading_task(request: Request, data):
    response = request.get("https://www.cloudflare.com/en-in/")
    print(response.status_code)
    response.raise_for_status()
    return response.text

scrape_heading_task()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;JS with Captcha Challenge&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;This challenge requires performing JS computations that differentiate a Chrome controlled by Selenium/Puppeteer/Playwright from a real Chrome. It also involves solving a Captcha. It's used to for pages which are rarely but sometimes visited by people, like:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;5th Review page&lt;/li&gt; 
 &lt;li&gt;Auth pages&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Example Page: &lt;a href="https://nopecha.com/demo/cloudflare"&gt;https://nopecha.com/demo/cloudflare&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;What Does Not Work?&lt;/h4&gt; 
&lt;p&gt;Using &lt;code&gt;@request&lt;/code&gt; does not work because although it can make browser-like HTTP requests, it cannot run JavaScript to solve the challenge.&lt;/p&gt; 
&lt;h4&gt;What Works?&lt;/h4&gt; 
&lt;p&gt;Pass the &lt;code&gt;bypass_cloudflare=True&lt;/code&gt; argument to the &lt;code&gt;google_get&lt;/code&gt; method.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.browser import browser, Driver

@browser
def scrape_heading_task(driver: Driver, data):
    driver.google_get("https://nopecha.com/demo/cloudflare", bypass_cloudflare=True)
    driver.prompt()

scrape_heading_task()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/cloudflare-js-captcha-demo.gif" alt="Cloudflare JS with Captcha Challenge Demo" /&gt;&lt;/p&gt; 
&lt;h3&gt;What are the benefits of a UI scraper?&lt;/h3&gt; 
&lt;p&gt;Here are some benefits of creating a scraper with a user interface:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Simplify your scraper usage for customers, eliminating the need to teach them how to modify and run your code.&lt;/li&gt; 
 &lt;li&gt;Protect your code by hosting the scraper on the web and offering a monthly subscription, rather than providing full access to your code. This approach: 
  &lt;ul&gt; 
   &lt;li&gt;Safeguards your Python code from being copied and reused, increasing your customer's lifetime value.&lt;/li&gt; 
   &lt;li&gt;Generate monthly recurring revenue via subscription from your customers, surpassing a one-time payment.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Enable sorting, filtering, and downloading of data in various formats (JSON, Excel, CSV, etc.).&lt;/li&gt; 
 &lt;li&gt;Provide access via a REST API for seamless integration.&lt;/li&gt; 
 &lt;li&gt;Create a polished frontend, backend, and API integration with minimal code.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;How to run a UI-based scraper?&lt;/h3&gt; 
&lt;p&gt;Let's run the Botasaurus Starter Template (the recommended template for greenfield Botasaurus projects), which scrapes the heading of the provided link by following these steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Clone the Starter Template:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/omkarcloud/botasaurus-starter my-botasaurus-project
cd my-botasaurus-project
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install dependencies (will take a few minutes):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python -m pip install -r requirements.txt
python run.py install
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the scraper:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python run.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Your browser will automatically open up at &lt;a href="http://localhost:3000/"&gt;http://localhost:3000/&lt;/a&gt;. Then, enter the link you want to scrape (e.g., &lt;a href="https://www.omkar.cloud/"&gt;https://www.omkar.cloud/&lt;/a&gt;) and click on the Run Button.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/starter-scraper-demo.gif" alt="starter-scraper-demo" /&gt;&lt;/p&gt; 
&lt;p&gt;After some seconds, the data will be scraped. &lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/starter-scraper-demo-result.png" alt="starter-scraper-demo-result" /&gt;&lt;/p&gt; 
&lt;p&gt;Visit &lt;a href="http://localhost:3000/output"&gt;http://localhost:3000/output&lt;/a&gt; to see all the tasks you have started.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/starter-scraper-demo-tasks.png" alt="starter-scraper-demo-tasks" /&gt;&lt;/p&gt; 
&lt;p&gt;Go to &lt;a href="http://localhost:3000/about"&gt;http://localhost:3000/about&lt;/a&gt; to see the rendered README.md file of the project.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/starter-scraper-demo-readme.png" alt="starter-scraper-demo-readme" /&gt;&lt;/p&gt; 
&lt;p&gt;Finally, visit &lt;a href="http://localhost:3000/api-integration"&gt;http://localhost:3000/api-integration&lt;/a&gt; to see how to access the scraper via API.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/starter-scraper-demo-api.png" alt="starter-scraper-demo-api" /&gt;&lt;/p&gt; 
&lt;p&gt;The API documentation is generated dynamically based on your scraper's inputs, sorts, filters, etc., and is unique to your scraper.&lt;/p&gt; 
&lt;p&gt;So, whenever you need to run the scraper via API, visit this tab and copy the code specific to your scraper.&lt;/p&gt; 
&lt;h3&gt;How to create a UI scraper using Botasaurus?&lt;/h3&gt; 
&lt;p&gt;Creating a UI scraper with Botasaurus is a simple 3-step process:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Create your scraper function&lt;/li&gt; 
 &lt;li&gt;Add the scraper to the server using 1 line of code&lt;/li&gt; 
 &lt;li&gt;Define the input controls for the scraper&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;To understand these steps, let's go through the code of the Botasaurus Starter Template that you just ran.&lt;/p&gt; 
&lt;h4&gt;Step 1: Create the Scraper Function&lt;/h4&gt; 
&lt;p&gt;In &lt;code&gt;src/scrape_heading_task.py&lt;/code&gt;, we define a scraping function that basically does the following:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Receives a &lt;code&gt;data&lt;/code&gt; object and extracts the "link".&lt;/li&gt; 
 &lt;li&gt;Retrieves the HTML content of the webpage using the "link".&lt;/li&gt; 
 &lt;li&gt;Converts the HTML into a BeautifulSoup object.&lt;/li&gt; 
 &lt;li&gt;Locates the heading element, extracts its text content, and returns it.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.request import request, Request
from botasaurus.soupify import soupify

@request
def scrape_heading_task(request: Request, data):
    # Visit the Link
    response = request.get(data["link"])

    # Create a BeautifulSoup object    
    soup = soupify(response)
    
    # Retrieve the heading element's text
    heading = soup.find('h1').get_text()

    # Save the data as a JSON file in output/scrape_heading_task.json
    return {
        "heading": heading
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Step 2: Add the Scraper to the Server&lt;/h4&gt; 
&lt;p&gt;In &lt;code&gt;backend/scrapers.py&lt;/code&gt;, we:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Import our scraping function&lt;/li&gt; 
 &lt;li&gt;Use &lt;code&gt;Server.add_scraper()&lt;/code&gt; to register the scraper&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus_server.server import Server
from src.scrape_heading_task import scrape_heading_task

# Add the scraper to the server
Server.add_scraper(scrape_heading_task)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Step 3: Define the Input Controls&lt;/h4&gt; 
&lt;p&gt;In &lt;code&gt;backend/inputs/scrape_heading_task.js&lt;/code&gt;, we:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Define a &lt;code&gt;getInput&lt;/code&gt; function that takes the controls parameter&lt;/li&gt; 
 &lt;li&gt;Add a link input control to it&lt;/li&gt; 
 &lt;li&gt;Use JSDoc comments to enable IntelliSense Code Completion in VSCode as you won't be able to remember all the controls in botasaurus.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;/**
 * @typedef {import('../../frontend/node_modules/botasaurus-controls/dist/index').Controls} Controls
 */

/**
 * @param {Controls} controls
 */
function getInput(controls) {
    controls
        // Render a Link Input, which is required, defaults to "https://stackoverflow.blog/open-source". 
        .link('link', { isRequired: true, defaultValue: "https://stackoverflow.blog/open-source" })
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Above was a simple example; below is a real-world example with multi-text, number, switch, select, section, and other controls.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;/**
 * @typedef {import('../../frontend/node_modules/botasaurus-controls/dist/index').Controls} Controls
 */


/**
 * @param {Controls} controls
 */
function getInput(controls) {
    controls
        .listOfTexts('queries', {
            defaultValue: ["Web Developers in Bangalore"],
            placeholder: "Web Developers in Bangalore",
            label: 'Search Queries',
            isRequired: true
        })
        .section("Email and Social Links Extraction", (section) =&amp;gt; {
            section.text('api_key', {
                placeholder: "2e5d346ap4db8mce4fj7fc112s9h26s61e1192b6a526af51n9",
                label: 'Email and Social Links Extraction API Key',
                helpText: 'Enter your API key to extract email addresses and social media links.',
            })
        })
        .section("Reviews Extraction", (section) =&amp;gt; {
            section
                .switch('enable_reviews_extraction', {
                    label: "Enable Reviews Extraction"
                })
                .numberGreaterThanOrEqualToZero('max_reviews', {
                    label: 'Max Reviews per Place (Leave empty to extract all reviews)',
                    placeholder: 20,
                    isShown: (data) =&amp;gt; data['enable_reviews_extraction'], defaultValue: 20,
                })
                .choose('reviews_sort', {
                    label: "Sort Reviews By",
                    isRequired: true, isShown: (data) =&amp;gt; data['enable_reviews_extraction'], defaultValue: 'newest', options: [{ value: 'newest', label: 'Newest' }, { value: 'most_relevant', label: 'Most Relevant' }, { value: 'highest_rating', label: 'Highest Rating' }, { value: 'lowest_rating', label: 'Lowest Rating' }]
                })
        })
        .section("Language and Max Results", (section) =&amp;gt; {
            section
                .addLangSelect()
                .numberGreaterThanOrEqualToOne('max_results', {
                    placeholder: 100,
                    label: 'Max Results per Search Query (Leave empty to extract all places)'
                })
        })
        .section("Geo Location", (section) =&amp;gt; {
            section
                .text('coordinates', {
                    placeholder: '12.900490, 77.571466'
                })
                .numberGreaterThanOrEqualToOne('zoom_level', {
                    label: 'Zoom Level (1-21)',
                    defaultValue: 14,
                    placeholder: 14
                })
        })
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;I encourage you to paste the above code into &lt;code&gt;backend/inputs/scrape_heading_task.js&lt;/code&gt; and reload the page, and you will see a complex set of input controls like the image shown.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/complex-input.png" alt="complex-input" /&gt;&lt;/p&gt; 
&lt;p&gt;Now, to use the Botasaurus UI for adding new scrapers, remember these points:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Create a &lt;code&gt;backend/inputs/{your_scraping_function_name}.js&lt;/code&gt; file for each scraping function.&lt;/li&gt; 
 &lt;li&gt;Define the &lt;code&gt;getInput&lt;/code&gt; function in the file with the necessary controls.&lt;/li&gt; 
 &lt;li&gt;Use JSDoc comments to enable IntelliSense code completion in VSCode, as you won't be able to remember all the controls in Botasaurus.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Use this template as a starting point for new scraping function's input controls js file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;/**
 * @typedef {import('../../frontend/node_modules/botasaurus-controls/dist/index').Controls} Controls
 */

/**
 * @param {Controls} controls
 */
function getInput(controls) {
    // Define your controls here.
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That's it! With these simple steps, you can create a fully functional UI scraper using Botasaurus.&lt;/p&gt; 
&lt;p&gt;Later, you will learn how to add sorts and filters to make your UI scraper even more powerful and user-friendly.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/sorts-filters.png" alt="sorts-filters" /&gt;&lt;/p&gt; 
&lt;h3&gt;What is a Desktop Extractor?&lt;/h3&gt; 
&lt;p&gt;A &lt;strong&gt;Desktop Extractor&lt;/strong&gt; is a standalone application that runs on your computer and extracts specific data from websites, PDFs, Excel files, and other documents. Unlike web-based tools, desktop extractors run locally, giving &lt;strong&gt;faster performance&lt;/strong&gt; and &lt;strong&gt;zero cloud costs&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/desktop-app-photo.png" alt="Desktop Extractor showing an application interface with extraction options" /&gt;&lt;/p&gt; 
&lt;h3&gt;What advantages do Desktop Scrapers have over web-based scrapers?&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Desktop Scrapers&lt;/strong&gt; offer key advantages over web-based scraper solutions like Outscraper:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zero Infrastructure Costs&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Runs on the user's machine, eliminating expensive cloud computing fees.&lt;/li&gt; 
   &lt;li&gt;Lower cloud costs allow you to offer lower pricing, attracting more customers and increasing revenue.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Faster Execution&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Instant execution, no delays for cloud resource allocation.&lt;/li&gt; 
   &lt;li&gt;Uses the user's system, which is much faster than shared cloud servers.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Increased Customer Engagement&lt;/strong&gt;:&lt;br /&gt; The app sits right on the user's desktop, encouraging frequent use compared to web tools they must actively visit via browser.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Cross-Platform Deployment in 1 Day&lt;/strong&gt;:&lt;br /&gt; With &lt;strong&gt;Botasaurus&lt;/strong&gt;, you can launch a desktop scraper for &lt;strong&gt;Windows, macOS, and Linux&lt;/strong&gt; within a day. No need to build a website, manage servers, or handle scaling issues. Bota Desktop includes built-in features such as:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Task management&lt;/li&gt; 
   &lt;li&gt;Data Table&lt;/li&gt; 
   &lt;li&gt;Data export (Excel, CSV, etc.)&lt;/li&gt; 
   &lt;li&gt;Sorting &amp;amp; Filtering&lt;/li&gt; 
   &lt;li&gt;Caching and many more&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;With zero usage costs, faster performance, and easier development, Desktop Scrapers outperform web-based alternatives.&lt;/p&gt; 
&lt;h3&gt;How to Build a Desktop Extractor&lt;/h3&gt; 
&lt;p&gt;Creating Desktop Extractors is easier than you think! All you need is a basic understanding of JavaScript. Once you're ready, read the &lt;a href="https://www.omkar.cloud/botasaurus/docs/botasaurus-desktop/quick-start"&gt;Desktop Extraction Tutorial&lt;/a&gt;, where we'll guide you through building two practical extractors:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Yahoo Finance Stock Scraper&lt;/strong&gt; â€“ Extracts real-time stock prices from Yahoo Finance.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/stock-scraper-preview.gif" alt="Stock Scraper Demo showing the application extracting stock prices from Yahoo Finance" /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Amazon Invoice PDF Extractor&lt;/strong&gt; â€“ Automates the extraction of key invoice data like Document Number, Document Date, and Place of Supply from Amazon PDFs.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/pdf-extract-preview.gif" alt="PDF Extraction Demo showing the application extracting data from Amazon PDF invoices" /&gt;&lt;/p&gt; 
&lt;p&gt;As a web scraper, you might naturally want to focus on web scraping. Still, I want you to create the &lt;strong&gt;Amazon Invoice PDF Extractor&lt;/strong&gt; project. Why? Because many developers overlook the immense potential of extracting data from PDFs, Excel files, and other documents.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Document Data Extraction is a large untapped market.&lt;/strong&gt; For example, even in most developed countries, accountants often spend hundreds of hours manually entering invoice data for tax filings. A desktop extractor can transform this tedious, error-prone process into a task that takes just minutes, delivering 100% accurate results.&lt;/p&gt; 
&lt;p&gt;Please read the step-by-step tutorial &lt;a href="https://www.omkar.cloud/botasaurus/docs/botasaurus-desktop/quick-start"&gt;here&lt;/a&gt;. By the end of this short guide, you'll be able to create powerful desktop extractors in very little time.&lt;/p&gt; 
&lt;h3&gt;What is Botasaurus, and what are its main features?&lt;/h3&gt; 
&lt;p&gt;Botasaurus is an all-in-one web scraping framework designed to achieve three main goals:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Provide essential web scraping utilities to streamline the scraping process.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;To accomplish these goals, Botasaurus gives you 3 decorators:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;@browser&lt;/code&gt;: For scraping web pages using a humane browser.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;@request&lt;/code&gt;: For scraping web pages using lightweight and humane HTTP requests.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;@task&lt;/code&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;For scraping web pages using third-party libraries like &lt;code&gt;playwright&lt;/code&gt; or &lt;code&gt;selenium&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;or, For running non-web scraping tasks, such as data processing (e.g., converting video to audio). Botasaurus is not limited to web scraping tasks; any Python function can be made accessible with a stunning UI and user-friendly API.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In practice, while developing with Botasaurus, you will spend most of your time in the following areas:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Configuring your scrapers via decorators with settings like: 
  &lt;ul&gt; 
   &lt;li&gt;Which proxy to use&lt;/li&gt; 
   &lt;li&gt;How many scrapers to run in parallel, etc.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Writing your core web scraping logic using BeautifulSoup (bs4) or the Botasaurus Driver.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Additionally, you will utilize the following Botasaurus utilities for debugging and development:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;bt&lt;/code&gt;: Mainly for writing JSON, EXCEL, and HTML temporary files, and for data cleaning.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Sitemap&lt;/code&gt;: For accessing the website's links and sitemap.&lt;/li&gt; 
 &lt;li&gt;Minor utilities like: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;LocalStorage&lt;/code&gt;: For storing scraper state.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;soupify&lt;/code&gt;: For creating BeautifulSoup objects from Driver, Requests response, Driver Element, or HTML string.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;IPUtils&lt;/code&gt;: For obtaining information (IP, country, etc.) about the current IP address.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;Cache&lt;/code&gt;: For managing the cache.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By simply configuring these three decorators (&lt;code&gt;@browser&lt;/code&gt;, &lt;code&gt;@request&lt;/code&gt;, and &lt;code&gt;@task&lt;/code&gt;) with arguments, you can easily create &lt;code&gt;real-time scrapers&lt;/code&gt; and &lt;code&gt;large-scale datasets&lt;/code&gt;, thus saving you countless hours that would otherwise be spent writing and debugging code from scratch.&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt; &lt;p&gt;Offering a Python-based UI scraper that allows non-technical users to run scrapers online by simply visiting a website link. (As described in the previous FAQ)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Make it easy to create desktop applications for Mac, Windows, and Linux, using JavaScript. More details can be found in the &lt;a href="https://www.omkar.cloud/botasaurus/docs/botasaurus-desktop/introduction"&gt;Botasaurus Desktop Documentation here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;How to use decorators in Botasaurus?&lt;/h3&gt; 
&lt;p&gt;Decorators are the heart of Botasaurus. To use a decorator function, you can call it with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A single item&lt;/li&gt; 
 &lt;li&gt;A list of items&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If a scraping function is given a list of items, it will sequentially call the scraping function for each data item.&lt;/p&gt; 
&lt;p&gt;For example, if you pass a list of three links to the &lt;code&gt;scrape_heading_task&lt;/code&gt; function:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.browser import browser, Driver

@browser
def scrape_heading_task(driver: Driver, link):
    driver.get(link)
    heading = driver.get_text("h1")
    return heading

scrape_heading_task(["https://www.omkar.cloud/", "https://www.omkar.cloud/blog/", "https://stackoverflow.com/"]) # &amp;lt;-- list of items
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, Botasaurus will launch a new browser instance for each item, and the final results will be stored in &lt;code&gt;output/scrape_heading_task.json&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/list-demo.gif" alt="list-demo" /&gt;&lt;/p&gt; 
&lt;h3&gt;How does Botasaurus help me in debugging?&lt;/h3&gt; 
&lt;p&gt;Botasaurus helps you in debugging by:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Easily viewing the result of the scraping function, as it is saved in &lt;code&gt;output/{your_scraping_function_name}.json&lt;/code&gt;. Say goodbye to print statements.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/scraped-data.png" alt="scraped data" /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Bringing your attention to errors in browser mode with a beep sound and pausing the browser, allowing you to debug the error on the spot.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/error-prompt.png" alt="" /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Even if an exception is raised in headless mode, it will still open the website in your default browser, making it easier to debug code in a headless browser. (Isn't it cool?)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/headless-error.png" alt="headless-error" /&gt;&lt;/p&gt; 
&lt;h3&gt;How to configure the Browser Decorator?&lt;/h3&gt; 
&lt;p&gt;The Browser Decorator allows you to easily configure various aspects of the browser, such as:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Blocking images and CSS&lt;/li&gt; 
 &lt;li&gt;Setting up proxies&lt;/li&gt; 
 &lt;li&gt;Specifying profiles&lt;/li&gt; 
 &lt;li&gt;Enabling headless mode&lt;/li&gt; 
 &lt;li&gt;Using Chrome extensions&lt;/li&gt; 
 &lt;li&gt;Captcha Solving&lt;/li&gt; 
 &lt;li&gt;Selecting language&lt;/li&gt; 
 &lt;li&gt;Passing Arguments to Chrome&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Blocking Images and CSS&lt;/h4&gt; 
&lt;p&gt;Blocking images is one of the most important configurations when scraping at scale. Blocking images can significantly:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Speed up your web scraping tasks&lt;/li&gt; 
 &lt;li&gt;Reduce bandwidth usage&lt;/li&gt; 
 &lt;li&gt;And save money on proxies. (Best of All!)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For example, a page that originally takes 4 seconds and 12 MB to load might only take one second and 100 KB after blocking images and CSS.&lt;/p&gt; 
&lt;p&gt;To block images, use the &lt;code&gt;block_images&lt;/code&gt; parameter:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@browser(
    block_images=True,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To block both images and CSS, use &lt;code&gt;block_images_and_css&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@browser(
    block_images_and_css=True,
)    
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Proxies&lt;/h4&gt; 
&lt;p&gt;To use proxies, simply specify the &lt;code&gt;proxy&lt;/code&gt; parameter:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@browser(
    proxy="http://username:password@proxy-provider-domain:port"
)    
def visit_what_is_my_ip(driver: Driver, data):
    driver.get("https://whatismyipaddress.com/")
    driver.prompt()

visit_what_is_my_ip()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also pass a list of proxies, and the proxy will be automatically rotated:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@browser(
    proxy=[
        "http://username:password@proxy-provider-domain:port", 
        "http://username2:password2@proxy-provider-domain:port"
    ]
)
def visit_what_is_my_ip(driver: Driver, data):
    driver.get("https://whatismyipaddress.com/")
    driver.prompt()

visit_what_is_my_ip() 
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Profile&lt;/h4&gt; 
&lt;p&gt;Easily specify the Chrome profile using the &lt;code&gt;profile&lt;/code&gt; option:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@browser(
    profile="pikachu"
)    
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;However, each Chrome profile can become very large (e.g., 100 MB) and can eat up all your computer storage.&lt;/p&gt; 
&lt;p&gt;To solve this problem, use the &lt;code&gt;tiny_profile&lt;/code&gt; option, which is a lightweight alternative to Chrome profiles.&lt;/p&gt; 
&lt;p&gt;When creating hundreds of Chrome profiles, it is highly recommended to use the &lt;code&gt;tiny_profile&lt;/code&gt; option because:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Creating 1000 Chrome profiles will take at least 100 GB, whereas 1000 tiny profiles will take up only 1 MB of storage, making tiny profiles easy to store and back up.&lt;/li&gt; 
 &lt;li&gt;Tiny profiles are cross-platform, meaning you can create profiles on a Linux server, copy the &lt;code&gt;./profiles&lt;/code&gt; folder to a Windows PC, and easily run them.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Under the hood, tiny profiles persist cookies from visited websites, making them extremely lightweight (around 1 KB) while providing the same session persistence.&lt;/p&gt; 
&lt;p&gt;Here's how to use the tiny profile:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@browser(
    tiny_profile=True, 
    profile="pikachu",
)    
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Headless Mode&lt;/h4&gt; 
&lt;p&gt;Enable headless mode with &lt;code&gt;headless=True&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@browser(
    headless=True
)    
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that if you use headless mode, you will surely be identified by services like Cloudflare and Datadome. Therefore, use headless mode only when scraping websites that don't use such services.&lt;/p&gt; 
&lt;h4&gt;Chrome Extensions&lt;/h4&gt; 
&lt;p&gt;Botasaurus allows the use of ANY Chrome Extension with just 1 line of code. The example below shows how to use the Mouse Coordinates Chrome Extension to show current mouse X and Y coordinates on web pages:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.browser import browser, Driver
from chrome_extension_python import Extension

@browser(
    extensions=[
        Extension(
            "https://chromewebstore.google.com/detail/mouse-coordinates/mfohnjojhopfcahiddmeljeholnciakl"
        )
    ],
)
def scrape_while_blocking_ads(driver: Driver, data):
    driver.get("https://example.com/")
    driver.prompt()

scrape_while_blocking_ads()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In some cases, an extension may require additional configuration, such as API keys or credentials. For such scenarios, you can create a custom extension. Learn more about creating and configuring custom extensions &lt;a href="https://github.com/omkarcloud/chrome-extension-python"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Captcha Solving&lt;/h4&gt; 
&lt;p&gt;Encountering captchas is common in web scraping. You can use the &lt;a href="https://github.com/omkarcloud/capsolver-extension-python?tab=readme-ov-file#installation"&gt;capsolver_extension_python&lt;/a&gt; package to automatically solve CAPTCHAs with Capsolver.&lt;/p&gt; 
&lt;p&gt;To use it, first install the package:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m pip install capsolver_extension_python
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, integrate it into your code as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.browser import browser, Driver
from capsolver_extension_python import Capsolver

# Replace "CAP-MY_KEY" with your actual CapSolver API key
@browser(extensions=[Capsolver(api_key="CAP-MY_KEY")])  
def solve_captcha(driver: Driver, data):
    driver.get("https://recaptcha-demo.appspot.com/recaptcha-v2-checkbox.php")
    driver.prompt()

solve_captcha()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Language&lt;/h4&gt; 
&lt;p&gt;Specify the language using the &lt;code&gt;lang&lt;/code&gt; option:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.lang import Lang

@browser(
    lang=Lang.Hindi,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;User Agent and Window Size&lt;/h4&gt; 
&lt;p&gt;To make the browser really humane, Botasaurus does not change browser fingerprints by default, because using fingerprints makes the browser easily identifiable by running CSS tests to find mismatches between the provided user agent and the actual user agent.&lt;/p&gt; 
&lt;p&gt;However, if you need fingerprinting, use the &lt;code&gt;user_agent&lt;/code&gt; and &lt;code&gt;window_size&lt;/code&gt; options:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.browser import browser, Driver
from botasaurus.user_agent import UserAgent
from botasaurus.window_size import WindowSize

@browser(
    user_agent=UserAgent.RANDOM,
    window_size=WindowSize.RANDOM,
)
def visit_whatsmyua(driver: Driver, data):
    driver.get("https://www.whatsmyua.info/")
    driver.prompt()

visit_whatsmyua()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When working with profiles, you want the fingerprints to remain consistent. You don't want the user's user agent to be Chrome 106 on the first visit and then become Chrome 102 on the second visit.&lt;/p&gt; 
&lt;p&gt;So, when using profiles, use the &lt;code&gt;HASHED&lt;/code&gt; option to generate a consistent user agent and window size based on the profile's hash:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.browser import browser, Driver
from botasaurus.user_agent import UserAgent
from botasaurus.window_size import WindowSize

@browser(
    profile="pikachu",
    user_agent=UserAgent.HASHED,
    window_size=WindowSize.HASHED,
)
def visit_whatsmyua(driver: Driver, data):
    driver.get("https://www.whatsmyua.info/")
    driver.prompt()
    
visit_whatsmyua()

# Everytime Same UserAgent and WindowSize
visit_whatsmyua()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Passing Arguments to Chrome&lt;/h4&gt; 
&lt;p&gt;To pass arguments to Chrome, use the &lt;code&gt;add_arguments&lt;/code&gt; option:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@browser(
    add_arguments=['--headless=new'],
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To dynamically generate arguments based on the &lt;code&gt;data&lt;/code&gt; parameter, pass a function:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;def get_arguments(data):
    return ['--headless=new']

@browser(
    add_arguments=get_arguments,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Wait for Complete Page Load&lt;/h4&gt; 
&lt;p&gt;By default, Botasaurus waits for all page resources (DOM, JavaScript, CSS, images, etc.) to load before calling your scraping function with the driver.&lt;/p&gt; 
&lt;p&gt;However, sometimes the DOM is ready, but JavaScript, images, etc., take forever to load.&lt;/p&gt; 
&lt;p&gt;In such cases, you can set &lt;code&gt;wait_for_complete_page_load&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt; to interact with the DOM as soon as the HTML is parsed and the DOM is ready:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@browser(
    wait_for_complete_page_load=False,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Reuse Driver&lt;/h4&gt; 
&lt;p&gt;Consider the following example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.browser import browser, Driver

@browser
def scrape_data(driver: Driver, link):
    driver.get(link)

scrape_data(["https://www.omkar.cloud/", "https://www.omkar.cloud/blog/", "https://stackoverflow.com/"])
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you run this code, the browser will be recreated on each page visit, which is inefficient.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/list-demo.gif" alt="list-demo-omkar" /&gt;&lt;/p&gt; 
&lt;p&gt;To solve this problem, use the &lt;code&gt;reuse_driver&lt;/code&gt; option which is great for cases like:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Scraping a large number of links and reusing the same browser instance for all page visits.&lt;/li&gt; 
 &lt;li&gt;Running your scraper in a cloud server to scrape data on demand, without recreating Chrome on each request.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Here's how to use &lt;code&gt;reuse_driver&lt;/code&gt; which will reuse the same Chrome instance for visiting each link.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.browser import browser, Driver

@browser(
    reuse_driver=True
)
def scrape_data(driver: Driver, link):
    driver.get(link)

scrape_data(["https://www.omkar.cloud/", "https://www.omkar.cloud/blog/", "https://stackoverflow.com/"])
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Result&lt;/strong&gt; &lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/list-demo-reuse-driver.gif" alt="list-demo-reuse-driver.gif" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Also, by default, whenever the program ends or is canceled, Botasaurus smartly closes any open Chrome instances, leaving no instances running in the background.&lt;/p&gt; 
&lt;p&gt;In rare cases, you may want to explicitly close the Chrome instance. For such scenarios, you can use the &lt;code&gt;.close()&lt;/code&gt; method on the scraping function:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;scrape_data.close()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will close any Chrome instances that remain open after the scraping function ends.&lt;/p&gt; 
&lt;h3&gt;How to Significantly Reduce Proxy Costs When Scraping at Scale?&lt;/h3&gt; 
&lt;p&gt;Recently, we had a project requiring access to around 100,000 pages from a well-protected website, necessitating the use of Residential Proxies.&lt;/p&gt; 
&lt;p&gt;Even after blocking images, we still required 250GB of proxy bandwidth, costing approximately $1050 (at $4.2 per GB with IP Royal).&lt;/p&gt; 
&lt;p&gt;This was beyond our budget :(&lt;/p&gt; 
&lt;p&gt;To solve this, we implemented a smart strategy:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;We first visited the website normally.&lt;/li&gt; 
 &lt;li&gt;We then made requests for subsequent pages using the browser's &lt;code&gt;fetch&lt;/code&gt; API.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Since we were only requesting the HTML, which was well compressed by the browser, we reduced our proxy bandwidth needs to just 5GB, costing only $30.&lt;/p&gt; 
&lt;p&gt;This resulted in savings of around $1000!&lt;/p&gt; 
&lt;p&gt;Here's an example of how you can do something similar in Botasaurus:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.browser import browser, Driver
from botasaurus.soupify import soupify

@browser(
    reuse_driver=True,  # Reuse the browser
    max_retry=5,        # Retry up to 5 times on failure
)
def scrape_data(driver: Driver, link):
    # If the browser is newly opened, first visit the link
    if driver.config.is_new:
        driver.google_get(link)
    
    # Make requests using the browser fetch API
    response = driver.requests.get(link)
    response.raise_for_status()  # Ensure the request was successful
    html = response.text

    # Parse the HTML to extract the desired data
    soup = soupify(html)
    stock_name = soup.select_one('[data-testid="quote-hdr"] h1').get_text()
    stock_price = soup.select_one('[data-testid="qsp-price"]').get_text()
    
    return {
        "stock_name": stock_name,
        "stock_price": stock_price,
    }

# List of URLs to scrape
links = [
    "https://finance.yahoo.com/quote/AAPL/",
    "https://finance.yahoo.com/quote/GOOG/",
    "https://finance.yahoo.com/quote/MSFT/",
]

# Execute the scraping function for the list of links
scrape_data(links)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dealing with 429 (Too Many Requests) Errors&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If you encounter a 429 error, add a delay before making another request. Most websites using Nginx, setting a rate limit of 1 request per second. To respect this limit, a delay of 1.13 seconds is recommended.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;driver.sleep(1.13)  # Delay to respect the rate limit
response = driver.requests.get(link)
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Handling 400 Errors Due to Large Cookies&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If you encounter a 400 error with a "cookie too large" message, delete the cookies and retry the request.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;response = driver.requests.get(link)

if response.status_code == 400:
    driver.delete_cookies()  # Delete cookies to resolve the error
    driver.short_random_sleep()  # Short delay before retrying
    response = driver.requests.get(link)
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;You can also use &lt;code&gt;driver.requests.get_mank(links)&lt;/code&gt; to make multiple requests in parallel, which is faster than making them sequentially.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;How to Configure the Browser's Chrome Profile, Language, and Proxy Dynamically Based on Data Parameters?&lt;/h3&gt; 
&lt;p&gt;The decorators in Botasaurus are really flexible, allowing you to pass a function that can derive the browser configuration based on the data item parameter. This is particularly useful when working with multiple Chrome profiles.&lt;/p&gt; 
&lt;p&gt;You can dynamically configure the browser's Chrome profile and proxy using decorators in two ways:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Using functions to extract configuration values from data:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Define functions to extract the desired configuration values from the &lt;code&gt;data&lt;/code&gt; parameter.&lt;/li&gt; 
   &lt;li&gt;Pass these functions as arguments to the &lt;code&gt;@browser&lt;/code&gt; decorator.&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Example:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.browser import browser, Driver

def get_profile(data):
    return data["profile"]

def get_proxy(data):
    return data["proxy"]

@browser(profile=get_profile, proxy=get_proxy)
def scrape_heading_task(driver: Driver, data):
    profile, proxy = driver.config.profile, driver.config.proxy
    print(profile, proxy)
    return profile, proxy

data = [
    {"profile": "pikachu", "proxy": "http://142.250.77.228:8000"},
    {"profile": "greyninja", "proxy": "http://142.250.77.229:8000"},
]

scrape_heading_task(data)
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Directly passing configuration values when calling the decorated function:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Pass the profile and proxy values directly as arguments to the decorated function when calling it.&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Example:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.browser import browser, Driver

@browser
def scrape_heading_task(driver: Driver, data):
    profile, proxy = driver.config.profile, driver.config.proxy
    print(profile, proxy)
    return profile, proxy

scrape_heading_task(
    profile='pikachu',  # Directly pass the profile
    proxy="http://142.250.77.228:8000",  # Directly pass the proxy
)
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;PS: Most Botasaurus decorators allow passing functions to derive configurations from data parameters. Check the decorator's argument type hint to see if it supports this functionality.&lt;/p&gt; 
&lt;h3&gt;What is the best way to manage profile-specific data like name, age across multiple profiles?&lt;/h3&gt; 
&lt;p&gt;To store data related to the active profile, use &lt;code&gt;driver.profile&lt;/code&gt;. Here's an example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.browser import browser, Driver

def get_profile(data):
    return data["profile"]

@browser(profile=get_profile)
def run_profile_task(driver: Driver, data):
    # Set profile data
    driver.profile = {
        'name': 'Amit Sharma',
        'age': 30
    }

    # Update the name in the profile
    driver.profile['name'] = 'Amit Verma'

    # Delete the age from the profile
    del driver.profile['age']

    # Print the updated profile
    print(driver.profile)  # Output: {'name': 'Amit Verma'}

    # Delete the entire profile
    driver.profile = None

run_profile_task([{"profile": "amit"}])
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For managing all profiles, use the &lt;code&gt;Profiles&lt;/code&gt; utility. Here's an example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.profiles import Profiles

# Set profiles
Profiles.set_profile('amit', {'name': 'Amit Sharma', 'age': 30})
Profiles.set_profile('rahul', {'name': 'Rahul Verma', 'age': 30})

# Get a profile
profile = Profiles.get_profile('amit')
print(profile)  # Output: {'name': 'Amit Sharma', 'age': 30}

# Get all profiles
all_profiles = Profiles.get_profiles()
print(all_profiles)  # Output: [{'name': 'Amit Sharma', 'age': 30}, {'name': 'Rahul Verma', 'age': 30}]

# Get all profiles in random order
random_profiles = Profiles.get_profiles(random=True)
print(random_profiles)  # Output: [{'name': 'Rahul Verma', 'age': 30}, {'name': 'Amit Sharma', 'age': 30}] in random order

# Delete a profile
Profiles.delete_profile('amit')
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: All profile data is stored in the &lt;code&gt;profiles.json&lt;/code&gt; file in the current working directory. &lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/profiles.png" alt="profiles" /&gt;&lt;/p&gt; 
&lt;h3&gt;What are some common methods in Botasaurus Driver?&lt;/h3&gt; 
&lt;p&gt;Botasaurus Driver provides several handy methods for web automation tasks, such as:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Visiting URLs:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;driver.get("https://www.example.com")
driver.google_get("https://www.example.com")  # Use Google as the referer [Recommended]
driver.get_via("https://www.example.com", referer="https://duckduckgo.com/")  # Use custom referer
driver.get_via_this_page("https://www.example.com")  # Use current page as referer
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Finding elements:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.browser import Wait
search_results = driver.select(".search-results", wait=Wait.SHORT)  # Wait for up to 4 seconds for the element to be present, return None if not found
all_links = driver.select_all("a")  # Get all elements matching the selector
search_results = driver.wait_for_element(".search-results", wait=Wait.LONG)  # Wait for up to 8 seconds for the element to be present, raise exception if not found
hello_mom = driver.get_element_with_exact_text("Hello Mom", wait=Wait.VERY_LONG)  # Wait for up to 16 seconds for an element having the exact text "Hello Mom"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Interacting with elements:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;driver.type("input[name='username']", "john_doe")  # Type into an input field
driver.click("button.submit")  # Click an element
element = driver.select("button.submit")
element.click()  # Click on an element
element.select_option("select#fruits", index=2)  # Select an option
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Retrieving element properties:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;header_text = driver.get_text("h1")  # Get text content
error_message = driver.get_element_containing_text("Error: Invalid input")
image_url = driver.select("img.logo").get_attribute("src")  # Get attribute value
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Working with parent-child elements:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;parent_element = driver.select(".parent")
child_element = parent_element.select(".child")
child_element.click()  # Click child element
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Executing JavaScript:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;result = driver.run_js("script.js") # Run a JavaScript file located in the current working directory.
result = driver.run_js("return document.title")
pikachu = driver.run_js("return args.pokemon", {"pokemon": 'pikachu'}) # args can be a dictionary, list, string, etc.
text_content = driver.select("body").run_js("(el) =&amp;gt; el.textContent")
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Enable human mode to perform, human-like mouse movements and say sayonara to detection:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;# Navigate to Cloudflare's Turnstile Captcha demo
driver.get(
  "https://nopecha.com/demo/cloudflare",
)

# Wait for page to fully load
driver.long_random_sleep()

# Locate iframe containing the Cloudflare challenge
iframe = driver.get_element_at_point(160, 290)

# Find checkbox element within the iframe
checkbox = iframe.get_element_at_point(30, 30)

# Enable human mode for realistic, human-like mouse movements
driver.enable_human_mode()

# Click the checkbox to solve the challenge
checkbox.click()

# (Optional) Disable human mode if no longer needed  
driver.disable_human_mode()

# Pause execution, for inspection
driver.prompt()
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/human-mode-demo.gif" alt="human-mode-demo" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Drag and Drop:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;# Open React DnD tutorial  
driver.get("https://react-dnd.github.io/react-dnd/examples/tutorial")  

# Select draggable and droppable elements  
draggable = driver.select('[draggable="true"]')  
droppable = driver.select('[data-testid="(3,6)"]')  

# Perform drag-and-drop  
draggable.drag_and_drop_to(droppable)  

# Pause execution, for inspection
driver.prompt()  
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/drag-and-drop-demo.gif" alt="drag-and-drop-demo" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Selecting Shadow Root Elements:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;# Visit the website
driver.get("https://nopecha.com/demo/cloudflare")

# Wait for page to fully load
driver.long_random_sleep()

# Locate the element containing shadow root
shadow_root_element = driver.select('[name="cf-turnstile-response"]').parent

# Access the iframe
iframe = shadow_root_element.get_shadow_root()

# Access the nested shadow DOM inside the iframe 
content = iframe.get_shadow_root()

# print the text content of the "label" element.
print(content.select("label", wait = 8).text)

# Pause execution, for inspection
driver.prompt()
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/selecting-shadow-root-elements.gif" alt="Selecting Shadow Root Elements" /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Monitoring requests:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.browser import browser, Driver, cdp

@browser()
def scrape_responses_task(driver: Driver, data):
    # Define a handler function that will be called after a response is received
    def after_response_handler(
        request_id: str,
        response: cdp.network.Response,
        event: cdp.network.ResponseReceived,
    ):
        # Extract URL, status, and headers from the response
        url = response.url
        status = response.status
        headers = response.headers
        
        # Print the response details 
        print(
            "after_response_handler",
            {
                "request_id": request_id,
                "url": url,
                "status": status,
                "headers": headers,
            },
        )

        # Append the request ID to the driver's responses list
        driver.responses.append(request_id)

    # Register the after_response_handler to be called after each response is received
    driver.after_response_received(after_response_handler)

    # Navigate to the specified URL
    driver.get("https://example.com/")

    # Collect all the responses that were appended during the navigation
    collected_responses = driver.responses.collect()
    
    # Save it in output/scrape_responses_task.json
    return collected_responses

# Execute the scraping task
scrape_responses_task()
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Working with iframes:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;driver.get("https://www.freecodecamp.org/news/using-entity-framework-core-with-mongodb/")
iframe = driver.get_iframe_by_link("www.youtube.com/embed") 
# OR the following works as well
# iframe = driver.select_iframe(".embed-wrapper iframe") 
freecodecamp_youtube_subscribers_count = iframe.select(".ytp-title-expanded-subtitle").text
print(freecodecamp_youtube_subscribers_count)
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Executing CDP Command:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.browser import browser, Driver, cdp
driver.run_cdp_command(cdp.page.navigate(url='https://stackoverflow.blog/open-source'))
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Miscellaneous:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;form.type("input[name='password']", "secret_password")  # Type into a form field
container.is_element_present(".button")  # Check element presence
page_html = driver.page_html  # Current page HTML
driver.select(".footer").scroll_into_view()  # Scroll element into view
driver.close()  # Close the browser
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;How Can I Pause the Browser to Inspect Website when Developing the Scraper?&lt;/h3&gt; 
&lt;p&gt;To pause the scraper and wait for user input before proceeding, use &lt;code&gt;driver.prompt()&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;driver.prompt()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;How do I configure authenticated proxies with SSL in Botasaurus?&lt;/h3&gt; 
&lt;p&gt;Proxy providers like BrightData, IPRoyal, and others typically provide authenticated proxies in the format "&lt;a href="http://username:password@proxy-provider-domain:port"&gt;http://username:password@proxy-provider-domain:port&lt;/a&gt;". For example, "&lt;a href="http://greyninja:awesomepassword@geo.iproyal.com:12321"&gt;http://greyninja:awesomepassword@geo.iproyal.com:12321&lt;/a&gt;".&lt;/p&gt; 
&lt;p&gt;However, if you use an authenticated proxy with a library like seleniumwire to visit a website using Cloudflare, or Datadome, you are GUARANTEED to be identified because you are using a non-SSL connection.&lt;/p&gt; 
&lt;p&gt;To verify this, run the following code:&lt;/p&gt; 
&lt;p&gt;First, install the necessary packages:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m pip install selenium_wire
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, execute this Python script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from seleniumwire import webdriver  # Import from seleniumwire

# Define the proxy
proxy_options = {
    'proxy': {
        'http': 'http://username:password@proxy-provider-domain:port', # TODO: Replace with your own proxy
        'https': 'http://username:password@proxy-provider-domain:port', # TODO: Replace with your own proxy
    }
}

# Install and set up the driver
driver = webdriver.Chrome(seleniumwire_options=proxy_options)

# Visit the desired URL
link = 'https://fingerprint.com/products/bot-detection/'
driver.get("https://www.google.com/")
driver.execute_script(f'window.location.href = "{link}"')

# Prompt for user input
input("Press Enter to exit...")

# Clean up
driver.quit()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You will SURELY be identified:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/seleniumwireblocked.png" alt="identified" /&gt;&lt;/p&gt; 
&lt;p&gt;However, using proxies with Botasaurus solves this issue. See the difference by running the following code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.browser import browser, Driver

@browser(proxy="http://username:password@proxy-provider-domain:port") # TODO: Replace with your own proxy 
def scrape_heading_task(driver: Driver, data):
    driver.google_get("https://fingerprint.com/products/bot-detection/")
    driver.prompt()

scrape_heading_task()    
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Result: &lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/botasaurussuccesspage.png" alt="not identified" /&gt;&lt;/p&gt; 
&lt;p&gt;Important Note: To run the code above, you will need &lt;a href="https://nodejs.org/en"&gt;Node.js&lt;/a&gt; installed.&lt;/p&gt; 
&lt;h3&gt;Why am I getting a socket connection error when using a proxy to access a website?&lt;/h3&gt; 
&lt;p&gt;Certain proxy providers like BrightData will block access to specific websites. To determine if this is the case, run the following code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.browser import browser, Driver

@browser(proxy="http://username:password@proxy-provider-domain:port")  # TODO: Replace with your own proxy
def visit_what_is_my_ip(driver: Driver, data):
    driver.get("https://whatismyipaddress.com/")
    driver.prompt()

visit_what_is_my_ip()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you can successfully access &lt;a href="https://whatismyipaddress.com/"&gt;whatismyipaddress.com&lt;/a&gt; but not the website you're attempting to scrape, it means the proxy provider is blocking access to that particular website.&lt;/p&gt; 
&lt;p&gt;In such situations, the only solution is to switch to a different proxy provider.&lt;/p&gt; 
&lt;p&gt;Some good proxy providers we personally use are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For Rotating Datacenter Proxies: &lt;strong&gt;BrightData Datacenter Proxies&lt;/strong&gt;, which cost around $0.6 per GB on a pay-as-you-go basis. No KYC is required.&lt;/li&gt; 
 &lt;li&gt;For Rotating Residential Proxies: &lt;strong&gt;IPRoyal Royal Residential Proxies&lt;/strong&gt;, which cost around $7 per GB on a pay-as-you-go basis. No KYC is required.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;As always, nothing good in life comes free. Proxies are expensive, and will take up almost all of your scraping costs.&lt;/p&gt; 
&lt;p&gt;So, use proxies only when you need them, and prefer request-based scrapers over browser-based scrapers to save bandwidth.&lt;/p&gt; 
&lt;p&gt;Note: BrightData and IPRoyal have not paid us. We are recommending them based on our personal experience.&lt;/p&gt; 
&lt;h3&gt;Which country should I choose when using proxies for web scraping?&lt;/h3&gt; 
&lt;p&gt;The United States is often the best choice because:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The United States has a highly developed internet infrastructure and is home to numerous data centers, ensuring faster internet speeds.&lt;/li&gt; 
 &lt;li&gt;Most global companies host their websites in the US, so using a US proxy will result in faster scraping speeds.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Should I use a proxy for web scraping?&lt;/h3&gt; 
&lt;p&gt;ONLY IF you encounter IP blocks.&lt;/p&gt; 
&lt;p&gt;Sadly, most scrapers unnecessarily use proxies, even when they are not needed. Everything seems like a nail when you have a hammer.&lt;/p&gt; 
&lt;p&gt;We have seen scrapers which can easily access hundreds of thousands of protected pages using the @browser module on home Wi-Fi without any issues.&lt;/p&gt; 
&lt;p&gt;So, as a best practice scrape using the @browser module on your home Wi-Fi first. Only resort to proxies when you encounter IP blocks.&lt;/p&gt; 
&lt;p&gt;This practice will save you a considerable amount of time (as proxies are really slow) and money (as proxies are expensive as well).&lt;/p&gt; 
&lt;h3&gt;How to configure the Request Decorator?&lt;/h3&gt; 
&lt;p&gt;The Request Decorator is used to make humane requests. Under the hood, it uses botasaurus-requests, a library based on hrequests, which incorporates important features like:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Using browser-like headers in the correct order.&lt;/li&gt; 
 &lt;li&gt;Makes a browser-like connection with correct ciphers.&lt;/li&gt; 
 &lt;li&gt;Uses &lt;code&gt;google.com&lt;/code&gt; referer by default to make it appear as if the user has arrived from google search.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Also, The Request Decorator allows you to configure proxy as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@request(
    proxy="http://username:password@proxy-provider-domain:port"
)    
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;What Options Can I Configure in all 3 Decorators?&lt;/h3&gt; 
&lt;p&gt;All 3 decorators allow you to configure the following options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Parallel Execution:&lt;/li&gt; 
 &lt;li&gt;Caching Results&lt;/li&gt; 
 &lt;li&gt;Passing Common Metadata&lt;/li&gt; 
 &lt;li&gt;Asynchronous Queues&lt;/li&gt; 
 &lt;li&gt;Asynchronous Execution&lt;/li&gt; 
 &lt;li&gt;Handling Crashes&lt;/li&gt; 
 &lt;li&gt;Configuring Output&lt;/li&gt; 
 &lt;li&gt;Exception Handling&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Let's dive into each of these options and in later sections we will see their real-world applications.&lt;/p&gt; 
&lt;h4&gt;&lt;code&gt;parallel&lt;/code&gt;&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;parallel&lt;/code&gt; option allows you to scrape data in parallel by launching multiple browser/request/task instances simultaneously. This can significantly speed up the scraping process.&lt;/p&gt; 
&lt;p&gt;Run the example below to see parallelization in action:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.browser import browser, Driver

@browser(parallel=3, data=["https://stackoverflow.blog/open-source", "https://stackoverflow.blog/ai", "https://stackoverflow.blog/productivity",])
def scrape_heading_task(driver: Driver, link):
    driver.get(link)
    heading = driver.get_text('h1')
    return heading

scrape_heading_task()    
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;&lt;code&gt;cache&lt;/code&gt;&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;cache&lt;/code&gt; option enables caching of web scraping results to avoid re-scraping the same data. This can significantly improve performance and reduce redundant requests.&lt;/p&gt; 
&lt;p&gt;Run the example below to see how caching works:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.browser import browser, Driver

@browser(cache=True, data=["https://stackoverflow.blog/open-source", "https://stackoverflow.blog/ai", "https://stackoverflow.blog/productivity",])
def scrape_heading_task(driver: Driver, link):
    driver.get(link)
    heading = driver.get_text('h1')
    return heading

print(scrape_heading_task())
print(scrape_heading_task())  # Data will be fetched from cache immediately 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: Caching is one of the most important features of Botasaurus.&lt;/p&gt; 
&lt;h4&gt;&lt;code&gt;metadata&lt;/code&gt;&lt;/h4&gt; 
&lt;p&gt;The metadata option allows you to pass common information shared across all data items. This can include things like API keys, browser cookies, or any other data that remains constant throughout the scraping process.&lt;/p&gt; 
&lt;p&gt;It is commonly used with caching to exclude details like API keys and browser cookies from the cache key.&lt;/p&gt; 
&lt;p&gt;Here's an example of how to use the &lt;code&gt;metadata&lt;/code&gt; option:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.task import task

@task()
def scrape_heading_task(driver: Driver, data, metadata):
    print("metadata:", metadata)
    print("data:", data)

data = [
    {"profile": "pikachu", "proxy": "http://142.250.77.228:8000"},
    {"profile": "greyninja", "proxy": "http://142.250.77.229:8000"},
]
scrape_heading_task(
  data, 
  metadata={"api_key": "BDEC26..."}
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;&lt;code&gt;async_queue&lt;/code&gt;&lt;/h4&gt; 
&lt;p&gt;In the world of web scraping, there are only two types of scrapers:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Dataset Scrapers: These extract data from websites and store it as datasets. Companies like Bright Data use them to build datasets for Crunchbase, Indeed, etc.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Real-time Scrapers: These fetch data from sources in real-time, like SERP APIs that provide Google and DuckDuckGo search results.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;When building real-time scrapers, speed is paramount because customers are waiting for requests to complete. The &lt;code&gt;async_queue&lt;/code&gt; feature is incredibly useful in such cases.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;async_queue&lt;/code&gt; allows you to run scraping tasks asynchronously in a queue and gather the results using the &lt;code&gt;.get()&lt;/code&gt; method.&lt;/p&gt; 
&lt;p&gt;A great use case for &lt;code&gt;async_queue&lt;/code&gt; is scraping Google Maps. Instead of scrolling through the list of places and then scraping the details of each place sequentially, you can use &lt;code&gt;async_queue&lt;/code&gt; to:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Scroll through the list of places.&lt;/li&gt; 
 &lt;li&gt;Simultaneously make HTTP requests to scrape the details of each place in the background.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;By executing the scrolling and requesting tasks concurrently, you can significantly speed up the scraper.&lt;/p&gt; 
&lt;p&gt;Run the code below to see browser scrolling and request scraping happening concurrently (really cool, must try!):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.browser import browser, Driver, AsyncQueueResult
from botasaurus.request import request, Request
import json

def extract_title(html):
    return json.loads(
        html.split(";window.APP_INITIALIZATION_STATE=")[1].split(";window.APP_FLAGS")[0]
    )[5][3][2][1]

@request(
    parallel=5,
    async_queue=True,
    max_retry=5,
)
def scrape_place_title(request: Request, link, metadata):
    cookies = metadata["cookies"]
    html = request.get(link, cookies=cookies, timeout=12).text
    title = extract_title(html)
    print("Title:", title)
    return title

def has_reached_end(driver):
    return driver.select('p.fontBodyMedium &amp;gt; span &amp;gt; span') is not None

def extract_links(driver):
    return driver.get_all_links('[role="feed"] &amp;gt; div &amp;gt; div &amp;gt; a')

@browser()
def scrape_google_maps(driver: Driver, link):
    driver.google_get(link, accept_google_cookies=True)  # accepts google cookies popup

    scrape_place_obj: AsyncQueueResult = scrape_place_title()  # initialize the async queue for scraping places
    cookies = driver.get_cookies_dict()  # get the cookies from the driver

    while True:
        links = extract_links(driver)  # get the links to places
        scrape_place_obj.put(links, metadata={"cookies": cookies})  # add the links to the async queue for scraping

        print("scrolling")
        driver.scroll_to_bottom('[role="feed"]')  # scroll to the bottom of the feed

        if has_reached_end(driver):  # we have reached the end, let's break buddy
            break

    results = scrape_place_obj.get()  # get the scraped results from the async queue
    return results

scrape_google_maps("https://www.google.com/maps/search/web+developers+in+bangalore")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;&lt;code&gt;run_async&lt;/code&gt;&lt;/h4&gt; 
&lt;p&gt;Similarly, the &lt;code&gt;run_async&lt;/code&gt; option allows you to execute scraping tasks asynchronously, enabling concurrent execution.&lt;/p&gt; 
&lt;p&gt;Similar to &lt;code&gt;async_queue&lt;/code&gt;, you can use the &lt;code&gt;.get()&lt;/code&gt; method to retrieve the results of an asynchronous task.&lt;/p&gt; 
&lt;p&gt;Code Example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.browser import browser, Driver
from time import sleep

@browser(run_async=True)
def scrape_heading(driver: Driver, data):
    sleep(5)
    return {}

if __name__ == "__main__":
    result1 = scrape_heading()  # Launches asynchronously
    result2 = scrape_heading()  # Launches asynchronously

    result1.get()  # Wait for the first result
    result2.get()  # Wait for the second result
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;&lt;code&gt;close_on_crash&lt;/code&gt;&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;close_on_crash&lt;/code&gt; option determines the behavior of the scraper when an exception occurs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;If set to &lt;code&gt;False&lt;/code&gt; (default): 
  &lt;ul&gt; 
   &lt;li&gt;The scraper will make a beep sound and pause the browser.&lt;/li&gt; 
   &lt;li&gt;This makes debugging easier by keeping the browser open at the point of the crash.&lt;/li&gt; 
   &lt;li&gt;Use this setting during development and testing.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;If set to &lt;code&gt;True&lt;/code&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;The scraper will close the browser and continue with the rest of the data items.&lt;/li&gt; 
   &lt;li&gt;This is suitable for production environments when you are confident that your scraper is robust.&lt;/li&gt; 
   &lt;li&gt;Use this setting to avoid interruptions and ensure the scraper processes all data items.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.browser import browser, Driver

@browser(
    close_on_crash=False  # Determines whether the browser is paused (default: False) or closed when an error occurs
)
def scrape_heading_task(driver: Driver, data):
    raise Exception("An error occurred during scraping.")

scrape_heading_task()  
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;&lt;code&gt;output&lt;/code&gt; and &lt;code&gt;output_formats&lt;/code&gt;&lt;/h4&gt; 
&lt;p&gt;By default, Botasaurus saves the result of scraping in the &lt;code&gt;output/{your_scraping_function_name}.json&lt;/code&gt; file. Let's learn about various ways to configure the output.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Change Output Filename&lt;/strong&gt;: Use the &lt;code&gt;output&lt;/code&gt; parameter in the decorator to specify a custom filename for the output.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.task import task

@task(output="my-output")
def scrape_heading_task(data): 
    return {"heading": "Hello, Mom!"}

scrape_heading_task()
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;&lt;strong&gt;Disable Output&lt;/strong&gt;: If you don't want any output to be saved, set &lt;code&gt;output&lt;/code&gt; to &lt;code&gt;None&lt;/code&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.task import task

@task(output=None)
def scrape_heading_task(data): 
    return {"heading": "Hello, Mom!"}

scrape_heading_task()
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;&lt;strong&gt;Dynamically Write Output&lt;/strong&gt;: To dynamically write output based on data and result, pass a function to the &lt;code&gt;output&lt;/code&gt; parameter:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.task import task
from botasaurus import bt

def write_output(data, result):
    json_filename = bt.write_json(result, 'data')
    excel_filename = bt.write_excel(result, 'data')
    bt.zip_files([json_filename, excel_filename]) # Zip the JSON and Excel files for easy delivery to the customer

@task(output=write_output)  
def scrape_heading_task(data): 
    return {"heading": "Hello, Mom!"}

scrape_heading_task()
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;&lt;strong&gt;Upload File to S3&lt;/strong&gt;: Use &lt;code&gt;bt.upload_to_s3&lt;/code&gt; to upload file to S3 bucket.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.task import task
from botasaurus import bt

def write_output(data, result):
    json_filename = bt.write_json(result, 'data')
    bt.upload_to_s3(json_filename, 'my-magical-bucket', "AWS_ACCESS_KEY", "AWS_SECRET_KEY")

@task(output=write_output)  
def scrape_heading_task(data): 
    return {"heading": "Hello, Mom!"}

scrape_heading_task()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;5.&lt;strong&gt;Save Outputs in Multiple Formats&lt;/strong&gt;: Use the &lt;code&gt;output_formats&lt;/code&gt; parameter to save outputs in different formats like JSON and EXCEL.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.task import task

@task(output_formats=[bt.Formats.JSON, bt.Formats.EXCEL])  
def scrape_heading_task(data): 
    return {"heading": "Hello, Mom!"}

scrape_heading_task()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;PRO TIP: When delivering data to customers, provide the dataset in JSON and Excel formats. Avoid CSV unless the customer asks, because Microsoft Excel has a hard time rendering CSV files with nested JSON.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;CSV vs Excel&lt;/strong&gt; &lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/csv-vs-excel.png" alt="csv-vs-excel" /&gt;&lt;/p&gt; 
&lt;h4&gt;Exception Handling Options&lt;/h4&gt; 
&lt;p&gt;Botasaurus provides various exception handling options to make your scrapers more robust:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;max_retry&lt;/code&gt;: By default, any failed task is not retried. You can specify the maximum number of times to retry scraping when an error occurs using the &lt;code&gt;max_retry&lt;/code&gt; option.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;retry_wait&lt;/code&gt;: Specifies the waiting time between retries.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;raise_exception&lt;/code&gt;: By default, Botasaurus does not raise an exception when an error occurs during scraping, because let's say you are keeping your PC running overnight to scrape 10,000 links. If one link fails, you really don't want to stop the entire scraping process, and ruin your morning by seeing an unfinished dataset.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;must_raise_exceptions&lt;/code&gt;: Specifies exceptions that must be raised, even if &lt;code&gt;raise_exception&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;create_error_logs&lt;/code&gt;: Determines whether error logs should be created when exceptions occur. In production, when scraping hundreds of thousands of links, it's recommended to set &lt;code&gt;create_error_logs&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt; to avoid using computational resources for creating error logs.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@browser(
    raise_exception=True,  # Raise an exception and halt the scraping process when an error occurs
    max_retry=5,  # Retry scraping a failed task a maximum of 5 times
    retry_wait=10,  # Wait for 10 seconds before retrying a failed task
    must_raise_exceptions=[CustomException],  # Definitely raise CustomException, even if raise_exception is set to False
    create_error_logs=False  # Disable the creation of error logs to optimize scraper performance
)
def scrape_heading_task(driver: Driver, data):
  # ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;What are some examples of common web scraping utilities provided by Botasaurus that make scraping easier?&lt;/h3&gt; 
&lt;h4&gt;bt Utility&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;bt&lt;/code&gt; utility provides helper functions for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Writing and reading JSON, EXCEL, and CSV files&lt;/li&gt; 
 &lt;li&gt;Data cleaning&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Some key functions are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;bt.write_json&lt;/code&gt; and &lt;code&gt;bt.read_json&lt;/code&gt;: Easily write and read JSON files.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus import bt

data = {"name": "pikachu", "power": 101}
bt.write_json(data, "output")
loaded_data = bt.read_json("output")
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;bt.write_excel&lt;/code&gt; and &lt;code&gt;bt.read_excel&lt;/code&gt;: Easily write and read EXCEL files.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus import bt

data = {"name": "pikachu", "power": 101}
bt.write_excel(data, "output")
loaded_data = bt.read_excel("output")
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;bt.write_csv&lt;/code&gt; and &lt;code&gt;bt.read_csv&lt;/code&gt;: Easily write and read CSV files.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus import bt

data = {"name": "pikachu", "power": 101}
bt.write_csv(data, "output")
loaded_data = bt.read_csv("output")
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;bt.write_html&lt;/code&gt; and &lt;code&gt;bt.read_html&lt;/code&gt;: Write HTML content to a file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus import bt

html_content = "&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;Hello, Mom!&amp;lt;/h1&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;"
bt.write_html(html_content, "output")
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;bt.write_temp_json&lt;/code&gt;, &lt;code&gt;bt.write_temp_csv&lt;/code&gt;, &lt;code&gt;bt.write_temp_html&lt;/code&gt;: Write temporary JSON, CSV, or HTML files for debugging purposes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus import bt

data = {"name": "pikachu", "power": 101}
bt.write_temp_json(data)
bt.write_temp_csv(data)
bt.write_temp_html("&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;Hello, Mom!&amp;lt;/h1&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;")
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Data cleaning functions like &lt;code&gt;bt.extract_numbers&lt;/code&gt;, &lt;code&gt;bt.extract_links&lt;/code&gt;, &lt;code&gt;bt.remove_html_tags&lt;/code&gt;, and more.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;text = "The price is $19.99 and the website is https://www.example.com"
numbers = bt.extract_numbers(text)  # [19.99]
links = bt.extract_links(text)  # ["https://www.example.com"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Local Storage Utility&lt;/h4&gt; 
&lt;p&gt;The Local Storage utility allows you to store and retrieve key-value pairs, which can be useful for maintaining state between scraper runs.&lt;/p&gt; 
&lt;p&gt;Here's how to use it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.local_storage import LocalStorage

LocalStorage.set_item("credits_used", 100)
print(LocalStorage.get_item("credits_used", 0))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;soupify Utility&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;soupify&lt;/code&gt; utility creates a BeautifulSoup object from a Driver, Requests response, Driver Element, or HTML string.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.soupify import soupify
from botasaurus.request import request, Request
from botasaurus.browser import browser, Driver

@request
def get_heading_from_request(req: Request, data):
   """
   Get the heading of a web page using the request object.
   """
   response = req.get("https://www.example.com")
   soup = soupify(response)
   heading = soup.find("h1").text
   print(f"Page Heading: {heading}")

@browser
def get_heading_from_driver(driver: Driver, data):
   """
   Get the heading of a web page using the driver object.
   """
   driver.get("https://www.example.com")

   # Get the heading from the entire page
   page_soup = soupify(driver)
   page_heading = page_soup.find("h1").text
   print(f"Heading from Driver's Soup: {page_heading}")

   # Get the heading from the body element
   body_soup = soupify(driver.select("body"))
   body_heading = body_soup.find("h1").text
   print(f"Heading from Element's Soup: {body_heading}")

# Call the functions
get_heading_from_request()
get_heading_from_driver()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;IP Utils&lt;/h4&gt; 
&lt;p&gt;IP Utils provide functions to get information about the current IP address, such as the IP itself, country, ISP, and more:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.ip_utils import IPUtils

# Get the current IP address
current_ip = IPUtils.get_ip()
print(current_ip)
# Output: 47.31.226.180

# Get detailed information about the current IP address
ip_info = IPUtils.get_ip_info()
print(ip_info)
# Output: {
#     "ip": "47.31.226.180",
#     "country": "IN",
#     "region": "Delhi",
#     "city": "Delhi",
#     "postal": "110001",
#     "coordinates": "28.6519,77.2315",
#     "latitude": "28.6519",
#     "longitude": "77.2315",
#     "timezone": "Asia/Kolkata",
#     "org": "AS55836 Reliance Jio Infocomm Limited"
# }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Cache Utility&lt;/h4&gt; 
&lt;p&gt;The Cache utility in Botasaurus allows you to manage cached data for your scraper. You can put, get, has, remove, and clear cache data.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Basic Usage&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.task import task
from botasaurus.cache import Cache

# Example scraping function
@task
def scrape_data(data):
    # Your scraping logic here
    return {"processed": data}

# Sample data for scraping
input_data = {"key": "value"}

# Adding data to the cache
Cache.put('scrape_data', input_data, scrape_data(input_data))

# Checking if data is in the cache
if Cache.has('scrape_data', input_data):
    # Retrieving data from the cache
    cached_data = Cache.get('scrape_data', input_data)
    print(f"Cached data: {cached_data}")

# Removing specific data from the cache
Cache.remove('scrape_data', input_data)

# Clearing the complete cache for the scrape_data function
Cache.clear('scrape_data')
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Advanced Usage for large-scale scraping projects&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Count Cached Items&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;You can count the number of items cached for a particular function, which can serve as a scraping progress bar.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.cache import Cache

Cache.print_cached_items_count('scraping_function')
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Filter Cached/Uncached Items&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;You can filter items that have been cached or not cached for a particular function.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.cache import Cache

all_items = ['1', '2', '3', '4', '5']

# Get items that are cached
cached_items = Cache.filter_items_in_cache('scraping_function', all_items)
print(cached_items)

# Get items that are not cached
uncached_items = Cache.filter_items_not_in_cache('scraping_function', all_items)
print(uncached_items)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Delete Cache&lt;/em&gt; The cache for a function is stored in the &lt;code&gt;cache/{your_scraping_function_name}/&lt;/code&gt; folder. To delete the cache, simply delete that folder.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/delete-cache.png" alt="delete-cache" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Delete Specific Items&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;You can delete specific items from the cache for a particular function.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.cache import Cache

all_items = ['1', '2', '3', '4', '5']
deleted_count = Cache.delete_items('scraping_function', all_items)
print(f"Deleted {deleted_count} items from the cache.")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Delete Items by Filter&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;In some cases, you may want to delete specific items from the cache based on a condition. For example, if you encounter honeypots (mock HTML served to dupe web scrapers) while scraping a website, you may want to delete those items from the cache.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;def should_delete_item(item, result):
    if 'Honeypot Item' in result:
        return True  # Delete the item
    return False  # Don't delete the item

all_items = ['1', '2', '3', '4', '5']
# List of items to iterate over, it is fine if the list contains items which have not been cached, as they will be simply ignored.
Cache.delete_items_by_filter('scraping_function', should_delete_item, all_items)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Importantly, be cautious and first use &lt;code&gt;delete_items_by_filter&lt;/code&gt; on a small set of items which you want to be deleted. Here's an example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus import bt
from botasaurus.cache import Cache

def should_delete_item(item, result):
    # TODO: Update the logic
    if 'Honeypot Item' in result:
        return True # Delete the item
    return False # Don't delete the item

test_items = ['1', '2'] # TODO: update with target items
scraping_function_name = 'scraping_function' # TODO:  update with target scraping function name
Cache.delete_items_by_filter(scraping_function_name, test_items, should_delete_item)

for item in test_items:
    if Cache.has(scraping_function_name, item):
        bt.prompt(f"Item {item} was not deleted. Please review the logic of the should_delete_item function.")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;How to Extract Links from a Sitemap?&lt;/h3&gt; 
&lt;p&gt;In web scraping, it is a common use case to scrape product pages, blogs, etc. But before scraping these pages, you need to get the links to these pages.&lt;/p&gt; 
&lt;p&gt;Sadly, many developers unnecessarily increase their work by writing code to visit each page one by one and scrape links, which they could have easily obtained by just looking at the Sitemap.&lt;/p&gt; 
&lt;p&gt;The Botasaurus Sitemap Module makes this process easy as cake by allowing you to get all links or sitemaps using:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The homepage URL (e.g., &lt;code&gt;https://www.omkar.cloud/&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;A direct sitemap link (e.g., &lt;code&gt;https://www.omkar.cloud/sitemap.xml&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;A &lt;code&gt;.gz&lt;/code&gt; compressed sitemap&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For example, if you're an Angel Investor seeking innovative tech startups to invest in, G2 is an ideal platform to find such startups. You can run the following code to fetch over 190K+ product links from G2:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus import bt
from botasaurus.sitemap import Sitemap, Filters, Extractors

links = (
    Sitemap("https://www.g2.com/sitemaps/sitemap_index.xml.gz")
    .filter(Filters.first_segment_equals("products"))
    .extract(Extractors.extract_link_upto_second_segment())
    .write_links('g2-products')
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Output:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/g2-sitemap-links.png" alt="g2-sitemap-links.png" /&gt;&lt;/p&gt; 
&lt;p&gt;Or, let's say you're in the mood for some reading and looking for good stories. The following code will get you over 1000+ stories from &lt;a href="https://moralstories26.com/"&gt;moralstories26.com&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus import bt
from botasaurus.sitemap import Sitemap, Filters

links = (
    Sitemap("https://moralstories26.com/")
    .filter(
        Filters.has_exactly_1_segment(),
        Filters.first_segment_not_equals(
            ["about", "privacy-policy", "akbar-birbal", "animal", "education", "fables", "facts", "family", "famous-personalities", "folktales", "friendship", "funny", "heartbreaking", "inspirational", "life", "love", "management", "motivational", "mythology", "nature", "quotes", "spiritual", "uncategorized", "zen"]
        ),
    )
    .write_links('moral-stories')
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Output:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/moralstories26-sitemap-links.png" alt="moralstories26-sitemap-links.png" /&gt;&lt;/p&gt; 
&lt;p&gt;Also, before scraping a site, it's useful to identify the available sitemaps. This can be easily done with the following code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus import bt
from botasaurus.sitemap import Sitemap

sitemaps = Sitemap("https://www.omkar.cloud/").write_sitemaps('omkar-sitemaps')
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Output:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/omkar-sitemap-links.png" alt="omkar-sitemap-links.png" /&gt;&lt;/p&gt; 
&lt;p&gt;To ensure your scrapers run super fast, we cache the Sitemap, but you may want to periodically refresh the cache. To do so, pass the Cache.REFRESH parameter.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus import bt
from botasaurus.sitemap import Sitemap, Filters, Extractors
from botasaurus.cache import Cache

links = (
    Sitemap("https://moralstories26.com/", cache=Cache.REFRESH)
    .write_links('moral-stories')
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;How can I filter a list of links, similar to working with Sitemaps?&lt;/h3&gt; 
&lt;p&gt;Filtering links from a webpage is a common requirement in web scraping. For example, you might want to filter out all non-product pages.&lt;/p&gt; 
&lt;p&gt;Botasaurus's &lt;code&gt;Links&lt;/code&gt; module simplifies link filtering and extraction:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.links import Links, Filters, Extractors

# Sample list of links
links = [
    "https://finance.yahoo.com/topic/stock-market-news/",
    "https://finance.yahoo.com/topic/morning-brief/", 
    "https://finance.yahoo.com/quote/AAPL/", 
    "https://finance.yahoo.com/quote/GOOG/"
]

# Filter and extract links
filtered_links = (
    Links(links)
    .filter(Filters.first_segment_equals("quote"))
    .extract(Extractors.extract_link_upto_second_segment())
    .write('stocks')
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;What is the best way to use caching in Botasaurus?&lt;/h3&gt; 
&lt;p&gt;Sadly, when using caching, most developers write a scraping function that scrapes the HTML and extracts the data from the HTML in the same function, like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus.request import request, Request
from botasaurus.soupify import soupify

@request
def scrape_data(request: Request, data):
    # Visit the Link
    response = request.get(data)
    
    # Create a BeautifulSoup object
    soup = soupify(response)
    
    # Retrieve the heading element's text
    heading = soup.find('h1').get_text()
    
    # Save the data as a JSON file in output/scrape_data.json
    return {"heading": heading}

data_items = [
    "https://stackoverflow.blog/open-source",
    "https://stackoverflow.blog/ai",
    "https://stackoverflow.blog/productivity",
]

scrape_data(data_items)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now, let's say, after 50% of the dataset has been scraped, what if:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Your customer wants to add another data point (which is very likely), or&lt;/li&gt; 
 &lt;li&gt;One of your BeautifulSoup selectors happens to be flaky and needs to be updated (which is super likely)?&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In such cases, you will have to scrape all the pages again, which is painful as it will take a lot of time and incur high proxy costs.&lt;/p&gt; 
&lt;p&gt;To resolve this issue, you can:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Write a function that only scrapes and caches the HTML.&lt;/li&gt; 
 &lt;li&gt;Write a separate function that calls the HTML scraping function, extracts data using BeautifulSoup, and caches the result.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Here's a practical example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from bs4 import BeautifulSoup
from botasaurus.task import task
from botasaurus.request import request, Request
from botasaurus.soupify import soupify

@request(cache=True)
def scrape_html(request: Request, link):
    # Scrape the HTML and cache it
    html = request.get(link).text
    return html

def extract_data(soup: BeautifulSoup):
    # Extract the heading from the HTML
    heading = soup.find("h1").get_text()
    return {"heading": heading}

# Cache the scrape_data task as well
@task(cache=True)
def scrape_data(link):
    # Call the scrape_html function to get the cached HTML
    html = scrape_html(link)
    # Extract data from the HTML using the extract_data function
    return extract_data(soupify(html))

data_items = [
    "https://stackoverflow.blog/open-source",
    "https://stackoverflow.blog/ai",
    "https://stackoverflow.blog/productivity",
]

scrape_data(data_items)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With this approach:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you need to add data points or fix BeautifulSoup bugs, delete the &lt;code&gt;cache/scrape_data&lt;/code&gt; folder and re-run the scraper. &lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/delete-cache.png" alt="delete-cache" /&gt;&lt;/li&gt; 
 &lt;li&gt;You only need to re-run the BeautifulSoup extraction, not the entire HTML scraping, saving time and proxy costs. Yahoo!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;PRO TIP&lt;/strong&gt;: This approach also makes your &lt;code&gt;extract_data&lt;/code&gt; code easier and faster to test, like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from bs4 import BeautifulSoup
from botasaurus import bt

def extract_data(soup: BeautifulSoup):
    heading = soup.find('h1').get_text()
    return {"heading": heading}

if __name__ == '__main__':
    # Will use the cached HTML and run the extract_data function again.
    bt.write_temp_json(scrape_data("https://stackoverflow.blog/open-source", cache=False))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;What are the recommended settings for each decorator to build a production-ready scraper in Botasaurus?&lt;/h3&gt; 
&lt;p&gt;For websites with minimal protection, use the &lt;code&gt;Request&lt;/code&gt; module.&lt;/p&gt; 
&lt;p&gt;Here's a template for creating production-ready datasets using the &lt;code&gt;Request&lt;/code&gt; module:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from bs4 import BeautifulSoup
from botasaurus.task import task
from botasaurus.request import request, Request,NotFoundException
from botasaurus.soupify import soupify

@request(
    # proxy='http://username:password@datacenter-proxy-domain:proxy-port', # Uncomment to use Proxy ONLY if you face IP blocking
    cache=True,

    max_retry=20, # Retry up to 20 times, which is a good default

    output=None,

    close_on_crash=True,
    raise_exception=True,
    create_error_logs=False,
)
def scrape_html(request: Request, link):
    # Scrape the HTML and cache it
    response = request.get(link)
    if response.status_code == 404:
        # A Special Exception to skip retrying this link
        raise NotFoundException(link)
    response.raise_for_status()
    return response.text

def extract_data(soup: BeautifulSoup):
    # Extract the heading from the HTML
    heading = soup.find("h1").get_text()
    return {"heading": heading}

# Cache the scrape_data task as well
@task(
    cache=True,
    close_on_crash=True,
    create_error_logs=False,
    parallel=40, # Run 40 requests in parallel, which is a good default
)
def scrape_data(link):
    # Call the scrape_html function to get the cached HTML
    html = scrape_html(link)
    # Extract data from the HTML using the extract_data function
    return extract_data(soupify(html))

data_items = [
    "https://stackoverflow.blog/open-source",
    "https://stackoverflow.blog/ai",
    "https://stackoverflow.blog/productivity",
]

scrape_data(data_items)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For visiting well protected websites, use the &lt;code&gt;Browser&lt;/code&gt; module.&lt;/p&gt; 
&lt;p&gt;Here's a template for creating production-ready datasets using the &lt;code&gt;Browser&lt;/code&gt; module:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from bs4 import BeautifulSoup
from botasaurus.task import task
from botasaurus.browser import browser, Driver, NotFoundException
from botasaurus.soupify import soupify

@browser(
    # proxy='http://username:password@datacenter-proxy-domain:proxy-port', # Uncomment to use Proxy ONLY if you face IP blocking

    # block_images_and_css=True, # Uncomment to block images and CSS, which can speed up scraping
    # wait_for_complete_page_load=False, # Uncomment to proceed once the DOM (Document Object Model) is loaded, without waiting for all resources to finish loading. This is recommended for faster scraping of Server Side Rendered (HTML) pages.

    cache=True,
    max_retry=5,  # Retry up to 5 times, which is a good default

    reuse_driver= True, # Reuse the same driver for all tasks
    
    output=None,

    close_on_crash=True,
    raise_exception=True,
    create_error_logs=False,
)
def scrape_html(driver: Driver, link):
    # Scrape the HTML and cache it
    if driver.config.is_new:
        driver.google_get(
            link,
            bypass_cloudflare=True,  # delete this line if the website you're accessing is not protected by Cloudflare
        )
    response = driver.requests.get(link)
    
    if response.status_code == 404:
        # A Special Exception to skip retrying this link
        raise NotFoundException(link)
    response.raise_for_status()
    
    html = response.text        
    return html

def extract_data(soup: BeautifulSoup):
    # Extract the heading from the HTML
    stock_name = soup.select_one('[data-testid="quote-hdr"] h1').get_text()
    stock_price = soup.select_one('[data-testid="qsp-price"]').get_text()
    
    return {
        "stock_name": stock_name,
        "stock_price": stock_price,
    }

# Cache the scrape_data task as well
@task(
    cache=True,
    close_on_crash=True,
    create_error_logs=False,
)
def scrape_data(link):
    # Call the scrape_html function to get the cached HTML
    html = scrape_html(link)
    # Extract data from the HTML using the extract_data function
    return extract_data(soupify(html))

data_items = [
    "https://finance.yahoo.com/quote/AAPL/",
    "https://finance.yahoo.com/quote/GOOG/",
    "https://finance.yahoo.com/quote/MSFT/",
]

scrape_data(data_items)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Why Am I Getting Detected on Some Websites?&lt;/h3&gt; 
&lt;p&gt;If you're getting detected, it's likely due to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Using a non-residential proxy â€” Services like Datadome and Cloudflare often flag datacenter IPs/VPNs.&lt;/li&gt; 
 &lt;li&gt;Clicking without Human Mode enabled â€” Unnatural mouse movements can trigger detection.&lt;/li&gt; 
 &lt;li&gt;Visiting websites too quickly â€” Rapid, bot-like navigation is easy to detect.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To reduce detection, follow these best practices:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Upgrade all Botasaurus packages to the latest version:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python -m pip install --upgrade bota botasaurus botasaurus-api botasaurus-requests botasaurus-driver botasaurus-proxy-authentication botasaurus-server botasaurus-humancursor
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Enable Human Mode for human-like mouse movements:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;driver.enable_human_mode()
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Avoid rapid &lt;code&gt;driver.get&lt;/code&gt; calls. Instead, try:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Clicking within pages with Human Mode enabled, if possible.&lt;/li&gt; 
   &lt;li&gt;Using &lt;code&gt;driver.google_get&lt;/code&gt; or &lt;code&gt;driver.get_via_this_page&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;Using &lt;a href="https://github.com/omkarcloud/botasaurus?tab=readme-ov-file#how-to-significantly-reduce-proxy-costs-when-scraping-at-scale"&gt;&lt;code&gt;driver.requests.get&lt;/code&gt;&lt;/a&gt; to fetch the page HTML content.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Slow down your scraper with random delays:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;driver.short_random_sleep()
driver.long_random_sleep()
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Avoid using &lt;code&gt;headless&lt;/code&gt; mode, as it can be easily detected by Cloudflare, Datadome, and Imperva.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Use a residential proxy, as datacenter IPs are often flagged.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Remove the &lt;code&gt;--no-default-browser-check&lt;/code&gt; argument as it is detectable by systems like Datadome, as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;@browser(remove_default_browser_check_argument=True)
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If your IP has been flagged, you can perform this technique to change it:&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Visit &lt;a href="https://whatismyipaddress.com/"&gt;whatismyipaddress.com&lt;/a&gt; to see your current IP Address.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Connect your PC to a smartphone's hotspot.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;On your smartphone, turn airplane mode on and off.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Turn the hotspot on again.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Now visit &lt;a href="https://whatismyipaddress.com/"&gt;whatismyipaddress.com&lt;/a&gt;. You'll see a new IP address.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;How Do I Close All Running Chrome Instances?&lt;/h3&gt; 
&lt;p&gt;While developing a scraper, multiple browser instances may remain open in the background (because of interrupting it with CTRL + C). This situation can cause your computer to hang.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/chrome-running.png" alt="Many Chrome processes running in Task Manager" /&gt;&lt;/p&gt; 
&lt;p&gt;To prevent your PC from hanging, you can run the following command to close all Chrome instances:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python -m close_chrome
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;How to Run Scraper in Docker?&lt;/h3&gt; 
&lt;p&gt;To run a Scraper in Docker, use the Botasaurus Starter Template, which includes the necessary Dockerfile and Docker Compose configurations.&lt;/p&gt; 
&lt;p&gt;Use the following commands to clone the Botasaurus Starter Template, build a Docker image from it, and execute the scraper within a Docker environment.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/omkarcloud/botasaurus-starter my-botasaurus-project
cd my-botasaurus-project
docker-compose build
docker-compose up
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;How to Run Scraper in Gitpod?&lt;/h3&gt; 
&lt;p&gt;Running a scraper in Gitpod offers several benefits:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Allows your scraper to use a powerful 8-core machine with 1000 Mbps internet speed&lt;/li&gt; 
 &lt;li&gt;Makes it easy to showcase your scraper to customers without them having to install anything, by simply sharing the Gitpod machine link&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In this example, we will run the Botasaurus Starter template in Gitpod:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;First, visit &lt;a href="https://gitpod.io/#https://github.com/omkarcloud/botasaurus-starter"&gt;this link&lt;/a&gt; and sign up using your GitHub account.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/google-maps-scraper/master/screenshots/open-in-gitpod.png" alt="Screenshot (148)" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Once signed up, open the starter project in Gitpod.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/assets/master/images/gitpod-continue.png" alt="gp-continue" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;To use UI Scraper, run the following command in Terminal:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python run.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;You will see a popup notification with the heading "A service is available on port 3000". In the popup notification, click the &lt;strong&gt;"Open Browser"&lt;/strong&gt; button to open the UI Dashboard in your browser&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/open-browser.png" alt="open-browser.png" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Now, you can press the &lt;code&gt;Run&lt;/code&gt; button to get the results.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/starter-photo.png" alt="starter-photo.png" /&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Note: Gitpod is not suitable for long-running tasks, as the environment will automatically shut down after a short period of inactivity. Use your local machine or a cloud VM for long-running scrapers.&lt;/p&gt; 
&lt;h2&gt;Should I Scrape Datasets Locally or in the Cloud?&lt;/h2&gt; 
&lt;p&gt;For most scraping tasks, we recommend running the scraper &lt;strong&gt;locally&lt;/strong&gt; on your system because:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;It requires far fewer setup steps&lt;/li&gt; 
 &lt;li&gt;It saves time and costs&lt;/li&gt; 
 &lt;li&gt;Most importantly, it allows you to quickly fix bugs and continue scraping.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;However, consider cloud scraping when:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Running tasks longer than 5 days.&lt;/li&gt; 
 &lt;li&gt;Scraping large-scale data (terabytes).&lt;/li&gt; 
 &lt;li&gt;Performing recurring monthly scrapes.&lt;/li&gt; 
 &lt;li&gt;Having slow Internet or data caps&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Cloud scraping is also significantly faster due to superior internet speeds (often 10x+ faster than home Wi-Fi).&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;How to Run a Data Scraper in a Virtual Machine?&lt;/h2&gt; 
&lt;p&gt;To run a scraper in a virtual machine (VM), follow these steps:&lt;/p&gt; 
&lt;h3&gt;1. Prepare Your Scraper&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Use the &lt;a href="https://github.com/omkarcloud/botasaurus-starter"&gt;Botasaurus Starter Template&lt;/a&gt; to create your dataset scraper.&lt;/li&gt; 
 &lt;li&gt;For large datasets, ensure memory efficiency (e.g., by using file formats like &lt;code&gt;ndjson&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;Add project dependencies to &lt;code&gt;requirements.txt&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Push your project to GitHub.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;2. Set Up a Virtual Machine&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;If you don't already have one, create a Google Cloud Account. You'll receive a $300 credit to use over 3 months. &lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/Select-your-billing-country.png" alt="Select-your-billing-country" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Go to &lt;a href="https://console.cloud.google.com/marketplace/product/click-to-deploy-images/nodejs"&gt;Google Click to Deploy&lt;/a&gt;, create a deployment and configure it as follows or as appropriate based on your workload:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Zone: us-central1-a # Use us-central1 (Iowa) for the lowest-cost VMs
Series: N1
Machine Type: n1-standard-2 (2 vCPU, 1 core, 7.5 GB memory)
Boot Disk Type: Standard persistent disk	# This is the most cost-effective disk option.
Boot disk size in GB: 20 GB # Adjust based on storage needs  
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/deploy-node-vm.gif" alt="Deployment setup" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Navigate to &lt;a href="https://console.cloud.google.com/compute/instances"&gt;VM Instances&lt;/a&gt; and click the SSH button to SSH into the VM. &lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/ssh-vm.png" alt="ssh-vm" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Now, run the following command in the terminal and wait for it to complete:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -sL https://raw.githubusercontent.com/omkarcloud/botasaurus/master/vm-scripts/install-bota.sh | bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="5"&gt; 
 &lt;li&gt;Install your scraper by running the following command. It may take 5 minutes to install the scraper, so grab a coffee or watch &lt;a href="https://www.youtube.com/watch?v=nwAYpLVyeFU"&gt;this awesome video&lt;/a&gt;:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3 -m bota install-scraper --repo-url https://github.com/omkarcloud/botasaurus-starter
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/install-scraper-vm.gif" alt="install-scraper" /&gt;&lt;/p&gt; 
&lt;p&gt;Note: If you are using a different repo, replace &lt;code&gt;https://github.com/omkarcloud/botasaurus-starter&lt;/code&gt; with your repo URL.&lt;/p&gt; 
&lt;p&gt;That's it! You have successfully installed the scraper in a virtual machine. The scraper will now start running and succeed.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/vm-succeed.png" alt="ls-output" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Notes&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The &lt;code&gt;main.py&lt;/code&gt; file serves as the scraper's entry point.&lt;/li&gt; 
 &lt;li&gt;Update your project's &lt;code&gt;requirements.txt&lt;/code&gt; to ensure it has all the dependencies required to run the scraper.&lt;/li&gt; 
 &lt;li&gt;Ensure your VM has enough memory for your scraper's needs.&lt;/li&gt; 
 &lt;li&gt;If running a headful browser, enable a virtual display by setting &lt;code&gt;enable_xvfb_virtual_display=True&lt;/code&gt;. This creates a virtual display required for running a headful browser in a VM. &lt;pre&gt;&lt;code class="language-python"&gt;@browser(enable_xvfb_virtual_display=True)
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;The scraper will run until it completes successfully or fails three times. You can also configure retries as follows: For example, to allow a maximum of 5 retries: &lt;pre&gt;&lt;code class="language-bash"&gt;python3 -m bota install-scraper --repo-url &amp;lt;your-repo&amp;gt; --max-retry=5
&lt;/code&gt;&lt;/pre&gt; or, to allow unlimited retries: &lt;pre&gt;&lt;code class="language-bash"&gt;python3 -m bota install-scraper --repo-url &amp;lt;your-repo&amp;gt; --max-retry=unlimited
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;If your scraper fails, you can check the logs of the current boot by running: &lt;pre&gt;&lt;code class="language-bash"&gt;journalctl -u botasaurus-starter.service -b # replace botasaurus-starter with your repo name
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;Downloading Data&lt;/em&gt; To download the scraped data, you can either:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Download it directly from the VM. &lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/download-data-vm.png" alt="Download Data from VM" /&gt;&lt;/li&gt; 
 &lt;li&gt;Upload it to Amazon S3: &lt;pre&gt;&lt;code class="language-python"&gt;from botasaurus import bt
bt.upload_to_s3('data.json', 'my-bucket', "AWS_ACCESS_KEY", "AWS_SECRET_KEY")
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;How to Stop the Scraper&lt;/h2&gt; 
&lt;p&gt;If you are performing recurring monthly scrapes, it's best to stop the scraper instead of deleting it. Note that you will only incur storage costs (~$0.4 per month for a 10GB Standard Persistent Disk) but not compute costs.&lt;/p&gt; 
&lt;p&gt;To stop the scraper:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Visit &lt;a href="https://console.cloud.google.com/compute/instances"&gt;VM Instances&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Select your VM and stop it.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/stop-scraper-in-vm.png" alt="stop-scraper-in-vm" /&gt;&lt;/p&gt; 
&lt;p&gt;To restart later:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Start the VM from &lt;a href="https://console.cloud.google.com/compute/instances"&gt;VM Instances&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/run-scraper-in-vm.png" alt="run-scraper-in-vm" /&gt; 2. SSH into it. 3. Delete caches and update sitemaps if needed. 4. Restart with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;shutdown -r now
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;How to Delete the Scraper and Avoid Incurring Further Charges&lt;/h2&gt; 
&lt;p&gt;If you no longer need the scraper, please ensure you have downloaded your data before deleting it.&lt;/p&gt; 
&lt;p&gt;Next, follow these steps to delete the scraper:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Go to &lt;a href="https://console.cloud.google.com/products/solutions/deployments"&gt;Deployments&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Delete your deployment.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/delete-deployment.png" alt="Delete deployment" /&gt;&lt;/p&gt; 
&lt;p&gt;That's it! You have successfully deleted the scraper, and you will not incur any disk or compute charges.&lt;/p&gt; 
&lt;h2&gt;How to Run a UI Scraper in a Virtual Machine&lt;/h2&gt; 
&lt;p&gt;To run your scraper in a virtual machine, we will:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create a static IP&lt;/li&gt; 
 &lt;li&gt;Create a VM with that IP&lt;/li&gt; 
 &lt;li&gt;SSH into the VM&lt;/li&gt; 
 &lt;li&gt;Install the scraper&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Now, follow these steps to run your scraper in a virtual machine:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Create a Google Cloud Account if you don't already have one. &lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/Select-your-billing-country.png" alt="Select-your-billing-country" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Visit the &lt;a href="https://console.cloud.google.com/welcome?cloudshell=true"&gt;Google Cloud Console&lt;/a&gt; and click the Cloud Shell button. A terminal will open up. &lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/click-cloud-shell-btn.png" alt="click-cloud-shell-btn" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the following commands in the terminal:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python -m pip install bota
python -m bota create-ip
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You will be asked for a VM name. Enter any name you like, such as "pikachu".&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;Name: pikachu&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;p&gt;Then, you will be asked for the region for the scraper. Press Enter to go with the default, which is "us-central1", as it has the lowest-cost VMs.&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;Region: Default&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/install-bota.gif" alt="Install bota" /&gt;&lt;/p&gt; &lt;p&gt;With this a static IP address will be created for your VM, which you can use to access your app later.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Go to &lt;a href="https://console.cloud.google.com/marketplace/product/click-to-deploy-images/nodejs"&gt;Google Click to Deploy&lt;/a&gt;, create a deployment and configure it as follows or as appropriate based on your workload:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Zone: us-central1-a # Use the zone from the region you selected in the previous step.
Series: N1
Machine Type: n1-standard-2 (2 vCPU, 1 core, 7.5 GB memory)
Boot Disk Type: Standard persistent disk	# This is the most cost-effective disk option.
Boot disk size in GB: 20 GB # Adjust based on storage needs  
Network Interface [External IP]: pikachu-ip # Use the IP name you created in the previous step.
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/deploy-node.gif" alt="deploy-node" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Navigate to &lt;a href="https://console.cloud.google.com/compute/instances"&gt;VM Instances&lt;/a&gt; and click the SSH button to SSH into the VM. &lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/ssh-vm.png" alt="ssh-vm" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Now, run the following command in the terminal:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -sL https://raw.githubusercontent.com/omkarcloud/botasaurus/master/vm-scripts/install-bota.sh | bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="7"&gt; 
 &lt;li&gt; &lt;p&gt;Finally, install the UI scraper by running the following command, then wait for 5 minutes for it to complete:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python3 -m bota install-ui-scraper --repo-url https://github.com/omkarcloud/botasaurus-starter
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/install-scraper.gif" alt="install-scraper" /&gt; Note: If you are using a different repo, replace &lt;code&gt;https://github.com/omkarcloud/botasaurus-starter&lt;/code&gt; with your repo URL.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;That's it! You have successfully launched the scraper in a virtual machine. When the previous commands are done, you will see a &lt;strong&gt;link&lt;/strong&gt; to your scraper. Visit it to run your scraper.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/vm-success.gif" alt="vm-success" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Notes&lt;/em&gt; - Update your project's &lt;code&gt;requirements.txt&lt;/code&gt; to ensure it has all the dependencies required to run the scraper. - Ensure your VM has enough memory for your scraper's needs. - If running a headful browser, enable a virtual display by setting &lt;code&gt;enable_xvfb_virtual_display=True&lt;/code&gt;. This creates a virtual display required for running a headful browser in a VM.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@browser(enable_xvfb_virtual_display=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;- The UI scraper will run indefinitely and will be available at the printed link.
- If your UI scraper fails, you can check the logs of the current boot by running:
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;journalctl -u backend.service -b 
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;How to Delete the UI Scraper and Avoid Incurring Further Charges&lt;/h2&gt; 
&lt;p&gt;If you no longer need the scraper, please ensure you have downloaded your data before deleting it.&lt;/p&gt; 
&lt;p&gt;Next, follow these steps to delete the scraper:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Delete the static IP by running the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python -m bota delete-ip
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You will be asked for the name of the VM you created in the first step. Enter the name and press Enter.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/delete-ip.png" alt="Delete IP" /&gt;&lt;/p&gt; &lt;p&gt;Note: If you forgot the name of the IP, you can also delete all the IPs by running &lt;code&gt;python -m bota delete-all-ips&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Go to &lt;a href="https://console.cloud.google.com/products/solutions/deployments"&gt;Deployments&lt;/a&gt; and delete your deployment.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/delete-deployment.png" alt="Delete deployment" /&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;That's it! You have successfully deleted the scraper, and you will not incur any further charges.&lt;/p&gt; 
&lt;h3&gt;How to Run Scraper in Kubernetes?&lt;/h3&gt; 
&lt;p&gt;Visit &lt;a href="https://github.com/omkarcloud/botasaurus/raw/master/run-scraper-in-kubernetes.md"&gt;this link&lt;/a&gt; to learn how to run scraper at scale using Kubernetes.&lt;/p&gt; 
&lt;h3&gt;I have a feature request!&lt;/h3&gt; 
&lt;p&gt;We'd love to hear it! Share them on &lt;a href="https://github.com/omkarcloud/botasaurus/discussions"&gt;GitHub Discussions&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.omkar.cloud/l/botasaurus-make-discussions/"&gt;&lt;img src="https://raw.githubusercontent.com/omkarcloud/google-maps-scraper/master/screenshots/ask-on-github.png" alt="Make" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- 
### Do you have a Discord community?

Yes, we have a Discord community where you can connect with other developers, ask questions, and share your experiences. Join our Discord community [here](https://discord.com/invite/rw9VeyuSM8). --&gt; 
&lt;h3&gt;â“ Advanced Questions&lt;/h3&gt; 
&lt;p&gt;Congratulations on completing the Botasaurus Documentation! Now, you have all the knowledge needed to effectively use Botasaurus.&lt;/p&gt; 
&lt;p&gt;You may choose to read the following questions based on your interests:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/omkarcloud/botasaurus/raw/master/advanced.md#how-to-run-botasaurus-in-google-colab"&gt;How to Run Botasaurus in Google Colab?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/omkarcloud/botasaurus/raw/master/advanced.md#how-can-i-allow-users-to-filter-the-scraped-data"&gt;How can I allow users to filter the scraped data?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/omkarcloud/botasaurus/raw/master/advanced.md#how-can-i-allow-the-user-to-sort-the-scraped-data"&gt;How can I allow the user to sort the scraped data?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/omkarcloud/botasaurus/raw/master/advanced.md#how-can-i-present-the-scraped-data-in-different-views"&gt;How can I present the scraped data in different views?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/omkarcloud/botasaurus/raw/master/advanced.md#when-building-a-large-dataset-customers-often-request-data-in-different-formats-like-overview-and-review-how-can-i-do-that"&gt;When building a large dataset, customers often request data in different formats like overview and review. How can I do that?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/omkarcloud/botasaurus/raw/master/advanced.md#what-more-can-i-configure-when-adding-a-scraper"&gt;What more can I configure when adding a scraper?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/omkarcloud/botasaurus/raw/master/advanced.md#how-to-control-the-maximum-number-of-browsers-and-requests-running-at-any-point-of-time"&gt;How to control the maximum number of browsers and requests running at any point of time?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/omkarcloud/botasaurus/raw/master/advanced.md#how-do-i-change-the-title-header-title-and-description-of-the-scraper"&gt;How do I change the title, header title, and description of the scraper?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/omkarcloud/botasaurus/raw/master/advanced.md#how-can-i-use-a-database-like-postgresql-with-ui-scraper"&gt;How can I use a database like PostgreSQL with UI Scraper?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/omkarcloud/botasaurus/raw/master/advanced.md#which-postgresql-provider-should-i-choose-among-supabase-google-cloud-sql-heroku-and-amazon-rds"&gt;Which PostgreSQL provider should I choose among Supabase, Google Cloud SQL, Heroku, and Amazon RDS?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/omkarcloud/botasaurus/raw/master/advanced.md#how-to-create-a-postgresql-database-on-supabase"&gt;How to create a PostgreSQL database on Supabase?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/omkarcloud/botasaurus/raw/master/advanced.md#how-to-create-a-postgresql-database-on-google-cloud"&gt;How to create a PostgreSQL database on Google Cloud?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/omkarcloud/botasaurus/raw/master/advanced.md#i-am-a-youtuber-should-i-create-youtube-videos-about-botasaurus-if-so-how-can-you-help-me"&gt;I am a Youtuber, Should I create YouTube videos about Botasaurus? If so, how can you help me?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Thank You&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;To That, who has given me a sufficiently intelligent mind to create Botasaurus and do a lot of good.&lt;/li&gt; 
 &lt;li&gt;I made Botasaurus because I would be really happy if you could use it to successfully complete your project. So, a Gigantic Thank you for using Botasaurus!&lt;/li&gt; 
 &lt;li&gt;A heartfelt thank you to &lt;a href="https://zcbenz.com/"&gt;Cheng Zhao&lt;/a&gt; from GitHub for creating Electron, which powers Botasaurus Desktop.&lt;/li&gt; 
 &lt;li&gt;Kudos to the Apify Team for creating the &lt;code&gt;proxy-chain&lt;/code&gt; library. The implementation of SSL-based Proxy Authentication wouldn't have been possible without their groundbreaking work on &lt;code&gt;proxy-chain&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Shout out to &lt;a href="https://github.com/ultrafunkamsterdam"&gt;ultrafunkamsterdam&lt;/a&gt; for creating &lt;code&gt;nodriver&lt;/code&gt;, which inspired the creation of Botasaurus Driver.&lt;/li&gt; 
 &lt;li&gt;A big thank you to &lt;a href="https://github.com/daijro"&gt;daijro&lt;/a&gt; for creating &lt;a href="https://github.com/daijro/hrequests"&gt;hrequest&lt;/a&gt;, which inspired the creation of botasaurus-requests.&lt;/li&gt; 
 &lt;li&gt;Deepest gratitude to &lt;a href="https://github.com/riflosnake"&gt;Flori Batusha&lt;/a&gt; and &lt;a href="https://github.com/iLeaf30/"&gt;Ambri&lt;/a&gt; for their contributions in creating &lt;strong&gt;Botasaurus Humancursor&lt;/strong&gt;, which brings human-like mouse movements to Botasaurus.&lt;/li&gt; 
 &lt;li&gt;A humongous thank you to Cloudflare, DataDome, Imperva, and all bot recognition systems. Had you not been there, we wouldn't be either ğŸ˜….&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;Now, what are you waiting for? ğŸ¤” Go and make something mastastic! ğŸš€&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Love It? &lt;a href="https://github.com/omkarcloud/botasaurus"&gt;Star It! â­&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Become one of our amazing stargazers by giving us a star â­ on GitHub!&lt;/p&gt; 
&lt;p&gt;It's just one click, but it means the world to me.&lt;/p&gt; 
&lt;a href="https://github.com/omkarcloud/botasaurus/stargazers"&gt; &lt;img src="https://bytecrank.com/nastyox/reporoster/php/stargazersSVG.php?user=omkarcloud&amp;amp;repo=botasaurus" alt="Stargazers for @omkarcloud/botasaurus" /&gt; &lt;/a&gt; 
&lt;h2&gt;Disclaimer for Botasaurus Project&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;By using Botasaurus, you agree to comply with all applicable local and international laws related to data scraping, copyright, and privacy. The developers of Botasaurus are not responsible for any misuse of this software. It is the sole responsibility of the user to ensure adherence to all relevant laws regarding data scraping, copyright, and privacy, and to use Botasaurus in an ethical and legal manner.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;We take the concerns of the Botasaurus Project very seriously. For any inquiries or issues, please contact Chetan Jain at &lt;a href="mailto:chetan@omkar.cloud"&gt;chetan@omkar.cloud&lt;/a&gt;. We will take prompt and necessary action in response to your emails.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>