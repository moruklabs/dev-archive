<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Fri, 19 Sep 2025 01:35:47 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>TheAlgorithms/Python</title>
      <link>https://github.com/TheAlgorithms/Python</link>
      <description>&lt;p&gt;All Algorithms implemented in Python&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;!-- Title: --&gt; 
 &lt;a href="https://github.com/TheAlgorithms/"&gt; &lt;img src="https://raw.githubusercontent.com/TheAlgorithms/website/1cd824df116b27029f17c2d1b42d81731f28a920/public/logo.svg?sanitize=true" height="100" /&gt; &lt;/a&gt; 
 &lt;h1&gt;&lt;a href="https://github.com/TheAlgorithms/"&gt;The Algorithms&lt;/a&gt; - Python&lt;/h1&gt; 
 &lt;!-- Labels: --&gt; 
 &lt;!-- First row: --&gt; 
 &lt;a href="https://gitpod.io/#https://github.com/TheAlgorithms/Python"&gt; &lt;img src="https://img.shields.io/badge/Gitpod-Ready--to--Code-blue?logo=gitpod&amp;amp;style=flat-square" height="20" alt="Gitpod Ready-to-Code" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/TheAlgorithms/Python/raw/master/CONTRIBUTING.md"&gt; &lt;img src="https://img.shields.io/static/v1.svg?label=Contributions&amp;amp;message=Welcome&amp;amp;color=0059b3&amp;amp;style=flat-square" height="20" alt="Contributions Welcome" /&gt; &lt;/a&gt; 
 &lt;img src="https://img.shields.io/github/repo-size/TheAlgorithms/Python.svg?label=Repo%20size&amp;amp;style=flat-square" height="20" /&gt; 
 &lt;a href="https://the-algorithms.com/discord"&gt; &lt;img src="https://img.shields.io/discord/808045925556682782.svg?logo=discord&amp;amp;colorB=7289DA&amp;amp;style=flat-square" height="20" alt="Discord chat" /&gt; &lt;/a&gt; 
 &lt;a href="https://gitter.im/TheAlgorithms/community"&gt; &lt;img src="https://img.shields.io/badge/Chat-Gitter-ff69b4.svg?label=Chat&amp;amp;logo=gitter&amp;amp;style=flat-square" height="20" alt="Gitter chat" /&gt; &lt;/a&gt; 
 &lt;!-- Second row: --&gt; 
 &lt;br /&gt; 
 &lt;a href="https://github.com/TheAlgorithms/Python/actions"&gt; &lt;img src="https://img.shields.io/github/actions/workflow/status/TheAlgorithms/Python/build.yml?branch=master&amp;amp;label=CI&amp;amp;logo=github&amp;amp;style=flat-square" height="20" alt="GitHub Workflow Status" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/pre-commit/pre-commit"&gt; &lt;img src="https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&amp;amp;logoColor=white&amp;amp;style=flat-square" height="20" alt="pre-commit" /&gt; &lt;/a&gt; 
 &lt;a href="https://docs.astral.sh/ruff/formatter/"&gt; &lt;img src="https://img.shields.io/static/v1?label=code%20style&amp;amp;message=ruff&amp;amp;color=black&amp;amp;style=flat-square" height="20" alt="code style: black" /&gt; &lt;/a&gt; 
 &lt;!-- Short description: --&gt; 
 &lt;h3&gt;All algorithms implemented in Python - for education üìö&lt;/h3&gt; 
&lt;/div&gt; 
&lt;p&gt;Implementations are for learning purposes only. They may be less efficient than the implementations in the Python standard library. Use them at your discretion.&lt;/p&gt; 
&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; 
&lt;p&gt;üìã Read through our &lt;a href="https://raw.githubusercontent.com/TheAlgorithms/Python/master/CONTRIBUTING.md"&gt;Contribution Guidelines&lt;/a&gt; before you contribute.&lt;/p&gt; 
&lt;h2&gt;üåê Community Channels&lt;/h2&gt; 
&lt;p&gt;We are on &lt;a href="https://the-algorithms.com/discord"&gt;Discord&lt;/a&gt; and &lt;a href="https://gitter.im/TheAlgorithms/community"&gt;Gitter&lt;/a&gt;! Community channels are a great way for you to ask questions and get help. Please join us!&lt;/p&gt; 
&lt;h2&gt;üìú List of Algorithms&lt;/h2&gt; 
&lt;p&gt;See our &lt;a href="https://raw.githubusercontent.com/TheAlgorithms/Python/master/DIRECTORY.md"&gt;directory&lt;/a&gt; for easier navigation and a better overview of the project.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PaddlePaddle/PaddleOCR</title>
      <link>https://github.com/PaddlePaddle/PaddleOCR</link>
      <description>&lt;p&gt;Turn any PDF or image document into structured data for your AI. A powerful, lightweight OCR toolkit that bridges the gap between images/PDFs and LLMs. Supports 80+ languages.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/Banner.png" alt="PaddleOCR Banner" /&gt; &lt;/p&gt; 
 &lt;p&gt;English | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_cn.md"&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_tcn.md"&gt;ÁπÅÈ´î‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ja.md"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ko.md"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_fr.md"&gt;Fran√ßais&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ru.md"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_es.md"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ar.md"&gt;ÿßŸÑÿπÿ±ÿ®Ÿäÿ©&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleOCR"&gt;&lt;img src="https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?color=ccf" alt="stars" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2507.05595"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2507.05595-b31b1b.svg?logo=arXiv" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/paddleocr"&gt;&lt;img src="https://static.pepy.tech/badge/paddleocr/month" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/paddleocr"&gt;&lt;img src="https://static.pepy.tech/badge/paddleocr" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/PaddlePaddle/PaddleOCR/network/dependents"&gt;&lt;img src="https://img.shields.io/badge/Used%20by-5.9k%2B%20repositories-blue" alt="Used by" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://img.shields.io/badge/python-3.8~3.12-aff.svg?sanitize=true" alt="python" /&gt; &lt;img src="https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg?sanitize=true" alt="os" /&gt; &lt;img src="https://img.shields.io/badge/hardware-cpu%2C%20gpu%2C%20xpu%2C%20npu-yellow.svg?sanitize=true" alt="hardware" /&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-Apache_2.0-green" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/PaddlePaddle/PaddleOCR"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;PaddleOCR is an industry-leading, production-ready OCR and document AI engine, offering end-to-end solutions from text extraction to intelligent document understanding&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h1&gt;PaddleOCR&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://www.paddlepaddle.org.cn/en"&gt;&lt;img src="https://img.shields.io/badge/PaddlePaddle-3.0-orange" alt="Framework" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Recognition%20Accuracy-%F0%9F%8F%86-green" alt="Accuracy" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Support_Languages-80+-brightgreen" alt="Multi-Language" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Handwriting-%E2%9C%93-success" alt="Handwriting" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Heterogeneous%20Hardware-Kunlunxin%20%7C%20Ascend_NPU-red" alt="Hardware" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] PaddleOCR now provides an MCP server that supports integration with Agent applications like Claude Desktop. For details, please refer to &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html"&gt;PaddleOCR MCP Server&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;The PaddleOCR 3.0 Technical Report is now available. See details at: &lt;a href="https://arxiv.org/abs/2507.05595"&gt;PaddleOCR 3.0 Technical Report&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;PaddleOCR&lt;/strong&gt; converts documents and images into &lt;strong&gt;structured, AI-friendly data&lt;/strong&gt; (like JSON and Markdown) with &lt;strong&gt;industry-leading accuracy&lt;/strong&gt;‚Äîpowering AI applications for everyone from indie developers and startups to large enterprises worldwide. With over &lt;strong&gt;50,000 stars&lt;/strong&gt; and deep integration into leading projects like &lt;strong&gt;MinerU, RAGFlow, and OmniParser&lt;/strong&gt;, PaddleOCR has become the &lt;strong&gt;premier solution&lt;/strong&gt; for developers building intelligent document applications in the &lt;strong&gt;AI era&lt;/strong&gt;.&lt;/p&gt; 
&lt;h3&gt;PaddleOCR 3.0 Core Features&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aistudio.baidu.com/community/app/91660/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_OCRv5-Demo_on_AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518494/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_StructureV3-Demo_on_AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518493/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_ChatOCRv4-Demo_on_AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://www.modelscope.cn/organization/PaddlePaddle"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%96_Demo_on_ModelScope-purple" alt="ModelScope" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/PaddlePaddle"&gt;&lt;img src="https://img.shields.io/badge/Demo_on_HuggingFace-purple.svg?logo=huggingface" alt="HuggingFace" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-OCRv5 ‚Äî Universal Scene Text Recognition&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;Single model supports five text types&lt;/strong&gt; (Simplified Chinese, Traditional Chinese, English, Japanese, and Pinyin) with &lt;strong&gt;13% accuracy improvement&lt;/strong&gt;. Solves multilingual mixed document recognition challenges.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-StructureV3 ‚Äî Complex Document Parsing&lt;/strong&gt;&lt;br /&gt; Intelligently converts complex PDFs and document images into &lt;strong&gt;Markdown and JSON files that preserve original structure&lt;/strong&gt;. &lt;strong&gt;Outperforms&lt;/strong&gt; numerous commercial solutions in public benchmarks. &lt;strong&gt;Perfectly maintains document layout and hierarchical structure&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-ChatOCRv4 ‚Äî Intelligent Information Extraction&lt;/strong&gt;&lt;br /&gt; Natively integrates ERNIE 4.5 to &lt;strong&gt;precisely extract key information&lt;/strong&gt; from massive documents, with 15% accuracy improvement over previous generation. Makes documents "&lt;strong&gt;understand&lt;/strong&gt;" your questions and provide accurate answers.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In addition to providing an outstanding model library, PaddleOCR 3.0 also offers user-friendly tools covering model training, inference, and service deployment, so developers can rapidly bring AI applications to production.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/Arch.jpg" alt="PaddleOCR Architecture" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Special Note&lt;/strong&gt;: PaddleOCR 3.x introduces several significant interface changes. &lt;strong&gt;Old code written based on PaddleOCR 2.x is likely incompatible with PaddleOCR 3.x&lt;/strong&gt;. Please ensure that the documentation you are reading matches the version of PaddleOCR you are using. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/upgrade_notes.html"&gt;This document&lt;/a&gt; explains the reasons for the upgrade and the major changes from PaddleOCR 2.x to 3.x.&lt;/p&gt; 
&lt;h2&gt;üì£ Recent updates&lt;/h2&gt; 
&lt;h3&gt;üî•üî•2025.08.21: Release of PaddleOCR 3.2.0, includes:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Significant Model Additions:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Introduced training, inference, and deployment for PP-OCRv5 recognition models in English, Thai, and Greek. &lt;strong&gt;The PP-OCRv5 English model delivers an 11% improvement in English scenarios compared to the main PP-OCRv5 model, with the Thai and Greek recognition models achieving accuracies of 82.68% and 89.28%, respectively.&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deployment Capability Upgrades:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Full support for PaddlePaddle framework versions 3.1.0 and 3.1.1.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Comprehensive upgrade of the PP-OCRv5 C++ local deployment solution, now supporting both Linux and Windows, with feature parity and identical accuracy to the Python implementation.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;High-performance inference now supports CUDA 12, and inference can be performed using either the Paddle Inference or ONNX Runtime backends.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;The high-stability service-oriented deployment solution is now fully open-sourced, allowing users to customize Docker images and SDKs as required.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;The high-stability service-oriented deployment solution also supports invocation via manually constructed HTTP requests, enabling client-side code development in any programming language.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Benchmark Support:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;All production lines now support fine-grained benchmarking, enabling measurement of end-to-end inference time as well as per-layer and per-module latency data to assist with performance analysis. &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/version3.x/pipeline_usage/instructions/benchmark.en.md"&gt;Here's&lt;/a&gt; how to set up and use the benchmark feature.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Documentation has been updated to include key metrics for commonly used configurations on mainstream hardware, such as inference latency and memory usage, providing deployment references for users.&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Resolved the issue of failed log saving during model training.&lt;/li&gt; 
   &lt;li&gt;Upgraded the data augmentation component for formula models for compatibility with newer versions of the albumentations dependency, and fixed deadlock warnings when using the tokenizers package in multi-process scenarios.&lt;/li&gt; 
   &lt;li&gt;Fixed inconsistencies in switch behaviors (e.g., &lt;code&gt;use_chart_parsing&lt;/code&gt;) in the PP-StructureV3 configuration files compared to other pipelines.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Other Enhancements:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Separated core and optional dependencies. Only minimal core dependencies are required for basic text recognition; additional dependencies for document parsing and information extraction can be installed as needed.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Enabled support for NVIDIA RTX 50 series graphics cards on Windows; users can refer to the &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/version3.x/installation.en.md"&gt;installation guide&lt;/a&gt; for the corresponding PaddlePaddle framework versions.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;PP-OCR series models now support returning single-character coordinates.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;Added AIStudio, ModelScope, and other model download sources, allowing users to specify the source for model downloads.&lt;/li&gt; 
   &lt;li&gt;Added support for chart-to-table conversion via the PP-Chart2Table module.&lt;/li&gt; 
   &lt;li&gt;Optimized documentation descriptions to improve usability.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.08.15: PaddleOCR 3.1.1 Released&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Added the missing methods &lt;code&gt;save_vector&lt;/code&gt;, &lt;code&gt;save_visual_info_list&lt;/code&gt;, &lt;code&gt;load_vector&lt;/code&gt;, and &lt;code&gt;load_visual_info_list&lt;/code&gt; in the &lt;code&gt;PP-ChatOCRv4&lt;/code&gt; class.&lt;/li&gt; 
    &lt;li&gt;Added the missing parameters &lt;code&gt;glossary&lt;/code&gt; and &lt;code&gt;llm_request_interval&lt;/code&gt; to the &lt;code&gt;translate&lt;/code&gt; method in the &lt;code&gt;PPDocTranslation&lt;/code&gt; class.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Improvements:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Added a demo to the MCP documentation.&lt;/li&gt; 
    &lt;li&gt;Added information about the PaddlePaddle and PaddleOCR version used for performance metrics testing in the documentation.&lt;/li&gt; 
    &lt;li&gt;Fixed errors and omissions in the production line document translation.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Others:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Changed the MCP server dependency to use the pure Python library &lt;code&gt;puremagic&lt;/code&gt; instead of &lt;code&gt;python-magic&lt;/code&gt; to reduce installation issues.&lt;/li&gt; 
    &lt;li&gt;Retested PP-OCRv5 performance metrics with PaddleOCR version 3.1.0 and updated the documentation.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.29: PaddleOCR 3.1.0 Released&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Key Models and Pipelines:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Added PP-OCRv5 Multilingual Text Recognition Model&lt;/strong&gt;, which supports the training and inference process for text recognition models in 37 languages, including French, Spanish, Portuguese, Russian, Korean, etc. &lt;strong&gt;Average accuracy improved by over 30%.&lt;/strong&gt; &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/algorithm/PP-OCRv5/PP-OCRv5_multi_languages.html"&gt;Details&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Upgraded the &lt;strong&gt;PP-Chart2Table model&lt;/strong&gt; in PP-StructureV3, further enhancing the capability of converting charts to tables. On internal custom evaluation sets, the metric (RMS-F1) &lt;strong&gt;increased by 9.36 percentage points (71.24% -&amp;gt; 80.60%).&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Newly launched &lt;strong&gt;document translation pipeline, PP-DocTranslation, based on PP-StructureV3 and ERNIE 4.5&lt;/strong&gt;, which supports the translation of Markdown format documents, various complex-layout PDF documents, and document images, with the results saved as Markdown format documents. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/pipeline_usage/PP-DocTranslation.html"&gt;Details&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;New MCP server:&lt;/strong&gt; &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html"&gt;Details&lt;/a&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Supports both OCR and PP-StructureV3 pipelines.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Supports three working modes: local Python library, AIStudio Community Cloud Service, and self-hosted service.&lt;/li&gt; 
    &lt;li&gt;Supports invoking local services via stdio and remote services via Streamable HTTP.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Optimization:&lt;/strong&gt; Improved the descriptions in some user guides for a smoother reading experience.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.26: PaddleOCR 3.0.3 Released&lt;/strong&gt;&lt;/summary&gt; - Bug Fix: Resolved the issue where the `enable_mkldnn` parameter was not effective, restoring the default behavior of using MKL-DNN for CPU inference. 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.19: PaddleOCR 3.0.2 Released&lt;/strong&gt;&lt;/summary&gt; - **New Features:** 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;The default download source has been changed from &lt;code&gt;BOS&lt;/code&gt; to &lt;code&gt;HuggingFace&lt;/code&gt;. Users can also change the environment variable &lt;code&gt;PADDLE_PDX_MODEL_SOURCE&lt;/code&gt; to &lt;code&gt;BOS&lt;/code&gt; to set the model download source back to Baidu Object Storage (BOS).&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Added service invocation examples for six languages‚ÄîC++, Java, Go, C#, Node.js, and PHP‚Äîfor pipelines like PP-OCRv5, PP-StructureV3, and PP-ChatOCRv4.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Improved the layout partition sorting algorithm in the PP-StructureV3 pipeline, enhancing the sorting logic for complex vertical layouts to deliver better results.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Enhanced model selection logic: when a language is specified but a model version is not, the system will automatically select the latest model version supporting that language.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Set a default upper limit for MKL-DNN cache size to prevent unlimited growth, while also allowing users to configure cache capacity.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Updated default configurations for high-performance inference to support Paddle MKL-DNN acceleration and optimized the logic for automatic configuration selection for smarter choices.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Adjusted the logic for obtaining the default device to consider the actual support for computing devices by the installed Paddle framework, making program behavior more intuitive.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Added Android example for PP-OCRv5. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/on_device_deployment.html"&gt;Details&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Fixed an issue with some CLI parameters in PP-StructureV3 not taking effect.&lt;/li&gt; 
    &lt;li&gt;Resolved an issue where &lt;code&gt;export_paddlex_config_to_yaml&lt;/code&gt; would not function correctly in certain cases.&lt;/li&gt; 
    &lt;li&gt;Corrected the discrepancy between the actual behavior of &lt;code&gt;save_path&lt;/code&gt; and its documentation description.&lt;/li&gt; 
    &lt;li&gt;Fixed potential multithreading errors when using MKL-DNN in basic service deployment.&lt;/li&gt; 
    &lt;li&gt;Corrected channel order errors in image preprocessing for the Latex-OCR model.&lt;/li&gt; 
    &lt;li&gt;Fixed channel order errors in saving visualized images within the text recognition module.&lt;/li&gt; 
    &lt;li&gt;Resolved channel order errors in visualized table results within PP-StructureV3 pipeline.&lt;/li&gt; 
    &lt;li&gt;Fixed an overflow issue in the calculation of &lt;code&gt;overlap_ratio&lt;/code&gt; under extremely special circumstances in the PP-StructureV3 pipeline.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Improvements:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Updated the description of the &lt;code&gt;enable_mkldnn&lt;/code&gt; parameter in the documentation to accurately reflect the program's actual behavior.&lt;/li&gt; 
    &lt;li&gt;Fixed errors in the documentation regarding the &lt;code&gt;lang&lt;/code&gt; and &lt;code&gt;ocr_version&lt;/code&gt; parameters.&lt;/li&gt; 
    &lt;li&gt;Added instructions for exporting pipeline configuration files via CLI.&lt;/li&gt; 
    &lt;li&gt;Fixed missing columns in the performance data table for PP-OCRv5.&lt;/li&gt; 
    &lt;li&gt;Refined benchmark metrics for PP-StructureV3 across different configurations.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Others:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Relaxed version restrictions on dependencies like numpy and pandas, restoring support for Python 3.12.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;History Log&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;2025.06.05: &lt;strong&gt;PaddleOCR 3.0.1 Released&lt;/strong&gt;, includes:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Optimisation of certain models and model configurations:&lt;/strong&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Updated the default model configuration for PP-OCRv5, changing both detection and recognition from mobile to server models. To improve default performance in most scenarios, the parameter &lt;code&gt;limit_side_len&lt;/code&gt; in the configuration has been changed from 736 to 64.&lt;/li&gt; 
    &lt;li&gt;Added a new text line orientation classification model &lt;code&gt;PP-LCNet_x1_0_textline_ori&lt;/code&gt; with an accuracy of 99.42%. The default text line orientation classifier for OCR, PP-StructureV3, and PP-ChatOCRv4 pipelines has been updated to this model.&lt;/li&gt; 
    &lt;li&gt;Optimized the text line orientation classification model &lt;code&gt;PP-LCNet_x0_25_textline_ori&lt;/code&gt;, improving accuracy by 3.3 percentage points to a current accuracy of 98.85%.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Optimizations and fixes for some issues in version 3.0.0, &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/update.html"&gt;details&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;üî•üî•2025.05.20: Official Release of &lt;strong&gt;PaddleOCR v3.0&lt;/strong&gt;, including:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-OCRv5&lt;/strong&gt;: High-Accuracy Text Recognition Model for All Scenarios - Instant Text from Images/PDFs.&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;üåê Single-model support for &lt;strong&gt;five&lt;/strong&gt; text types - Seamlessly process &lt;strong&gt;Simplified Chinese, Traditional Chinese, Simplified Chinese Pinyin, English&lt;/strong&gt; and &lt;strong&gt;Japanese&lt;/strong&gt; within a single model.&lt;/li&gt; 
    &lt;li&gt;‚úçÔ∏è Improved &lt;strong&gt;handwriting recognition&lt;/strong&gt;: Significantly better at complex cursive scripts and non-standard handwriting.&lt;/li&gt; 
    &lt;li&gt;üéØ &lt;strong&gt;13-point accuracy gain&lt;/strong&gt; over PP-OCRv4, achieving state-of-the-art performance across a variety of real-world scenarios.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-StructureV3&lt;/strong&gt;: General-Purpose Document Parsing ‚Äì Unleash SOTA Images/PDFs Parsing for Real-World Scenarios!&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;üßÆ &lt;strong&gt;High-Accuracy multi-scene PDF parsing&lt;/strong&gt;, leading both open- and closed-source solutions on the OmniDocBench benchmark.&lt;/li&gt; 
    &lt;li&gt;üß† Specialized capabilities include &lt;strong&gt;seal recognition&lt;/strong&gt;, &lt;strong&gt;chart-to-table conversion&lt;/strong&gt;, &lt;strong&gt;table recognition with nested formulas/images&lt;/strong&gt;, &lt;strong&gt;vertical text document parsing&lt;/strong&gt;, and &lt;strong&gt;complex table structure analysis&lt;/strong&gt;.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-ChatOCRv4&lt;/strong&gt;: Intelligent Document Understanding ‚Äì Extract Key Information, not just text from Images/PDFs.&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;üî• &lt;strong&gt;15-point accuracy gain&lt;/strong&gt; in key-information extraction on PDF/PNG/JPG files over the previous generation.&lt;/li&gt; 
    &lt;li&gt;üíª Native support for &lt;strong&gt;ERNIE 4.5&lt;/strong&gt;, with compatibility for large-model deployments via PaddleNLP, Ollama, vLLM, and more.&lt;/li&gt; 
    &lt;li&gt;ü§ù Integrated &lt;a href="https://github.com/PaddlePaddle/PaddleMIX/tree/develop/paddlemix/examples/ppdocbee2"&gt;PP-DocBee2&lt;/a&gt;, enabling extraction and understanding of printed text, handwriting, seals, tables, charts, and other common elements in complex documents.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/update.html"&gt;History Log&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚ö° Quick Start&lt;/h2&gt; 
&lt;h3&gt;1. Run online demo&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aistudio.baidu.com/community/app/91660/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_OCRv5-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518494/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_StructureV3-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518493/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_ChatOCRv4-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;2. Installation&lt;/h3&gt; 
&lt;p&gt;Install PaddlePaddle refer to &lt;a href="https://www.paddlepaddle.org.cn/en/install/quick?docurl=/documentation/docs/en/develop/install/pip/linux-pip_en.html"&gt;Installation Guide&lt;/a&gt;, after then, install the PaddleOCR toolkit.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# If you only want to use the basic text recognition feature (returns text position coordinates and content), including the PP-OCR series
python -m pip install paddleocr
# If you want to use all features such as document parsing, document understanding, document translation, key information extraction, etc.
# python -m pip install "paddleocr[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Starting from version 3.2.0, in addition to the &lt;code&gt;all&lt;/code&gt; dependency group demonstrated above, PaddleOCR also supports installing partial optional features by specifying other dependency groups. All dependency groups provided by PaddleOCR are as follows:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Dependency Group Name&lt;/th&gt; 
   &lt;th&gt;Corresponding Functionality&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;doc-parser&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Document parsing: can be used to extract layout elements such as tables, formulas, stamps, images, etc. from documents; includes models like PP-StructureV3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ie&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Information extraction: can be used to extract key information from documents, such as names, dates, addresses, amounts, etc.; includes models like PP-ChatOCRv4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;trans&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Document translation: can be used to translate documents from one language to another; includes models like PP-DocTranslation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;all&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Complete functionality&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;3. Run inference by CLI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run PP-OCRv5 inference
paddleocr ocr -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png --use_doc_orientation_classify False --use_doc_unwarping False --use_textline_orientation False  

# Run PP-StructureV3 inference
paddleocr pp_structurev3 -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png --use_doc_orientation_classify False --use_doc_unwarping False

# Get the Qianfan API Key at first, and then run PP-ChatOCRv4 inference
paddleocr pp_chatocrv4_doc -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png -k È©æÈ©∂ÂÆ§ÂáÜ‰πò‰∫∫Êï∞ --qianfan_api_key your_api_key --use_doc_orientation_classify False --use_doc_unwarping False 

# Get more information about "paddleocr ocr"
paddleocr ocr --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4. Run inference by API&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;4.1 PP-OCRv5 Example&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Initialize PaddleOCR instance
from paddleocr import PaddleOCR
ocr = PaddleOCR(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False,
    use_textline_orientation=False)

# Run OCR inference on a sample image 
result = ocr.predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png")

# Visualize the results and save the JSON results
for res in result:
    res.print()
    res.save_to_img("output")
    res.save_to_json("output")
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;4.2 PP-StructureV3 Example&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from pathlib import Path
from paddleocr import PPStructureV3

pipeline = PPStructureV3(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False
)

# For Image
output = pipeline.predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png",
)

# Visualize the results and save the JSON results
for res in output:
    res.print() 
    res.save_to_json(save_path="output") 
    res.save_to_markdown(save_path="output")           
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;4.3 PP-ChatOCRv4 Example&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from paddleocr import PPChatOCRv4Doc

chat_bot_config = {
    "module_name": "chat_bot",
    "model_name": "ernie-3.5-8k",
    "base_url": "https://qianfan.baidubce.com/v2",
    "api_type": "openai",
    "api_key": "api_key",  # your api_key
}

retriever_config = {
    "module_name": "retriever",
    "model_name": "embedding-v1",
    "base_url": "https://qianfan.baidubce.com/v2",
    "api_type": "qianfan",
    "api_key": "api_key",  # your api_key
}

pipeline = PPChatOCRv4Doc(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False
)

visual_predict_res = pipeline.visual_predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png",
    use_common_ocr=True,
    use_seal_recognition=True,
    use_table_recognition=True,
)

mllm_predict_info = None
use_mllm = False
# If a multimodal large model is used, the local mllm service needs to be started. You can refer to the documentation: https://github.com/PaddlePaddle/PaddleX/blob/release/3.0/docs/pipeline_usage/tutorials/vlm_pipelines/doc_understanding.en.md performs deployment and updates the mllm_chat_bot_config configuration.
if use_mllm:
    mllm_chat_bot_config = {
        "module_name": "chat_bot",
        "model_name": "PP-DocBee",
        "base_url": "http://127.0.0.1:8080/",  # your local mllm service url
        "api_type": "openai",
        "api_key": "api_key",  # your api_key
    }

    mllm_predict_res = pipeline.mllm_pred(
        input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png",
        key_list=["È©æÈ©∂ÂÆ§ÂáÜ‰πò‰∫∫Êï∞"],
        mllm_chat_bot_config=mllm_chat_bot_config,
    )
    mllm_predict_info = mllm_predict_res["mllm_res"]

visual_info_list = []
for res in visual_predict_res:
    visual_info_list.append(res["visual_info"])
    layout_parsing_result = res["layout_parsing_result"]

vector_info = pipeline.build_vector(
    visual_info_list, flag_save_bytes_vector=True, retriever_config=retriever_config
)
chat_result = pipeline.chat(
    key_list=["È©æÈ©∂ÂÆ§ÂáÜ‰πò‰∫∫Êï∞"],
    visual_info=visual_info_list,
    vector_info=vector_info,
    mllm_predict_info=mllm_predict_info,
    chat_bot_config=chat_bot_config,
    retriever_config=retriever_config,
)
print(chat_result)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;5. Chinese Heterogeneous AI Accelerators&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/other_devices_support/paddlepaddle_install_NPU.html"&gt;Huawei Ascend&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/other_devices_support/paddlepaddle_install_XPU.html"&gt;KUNLUNXIN&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üß© More Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Convert models to ONNX format: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/obtaining_onnx_models.html"&gt;Obtaining ONNX Models&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Accelerate inference using engines like OpenVINO, ONNX Runtime, TensorRT, or perform inference using ONNX format models: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/high_performance_inference.html"&gt;High-Performance Inference&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Accelerate inference using multi-GPU and multi-process: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/pipeline_usage/instructions/parallel_inference.html"&gt;Parallel Inference for Pipelines&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Integrate PaddleOCR into applications written in C++, C#, Java, etc.: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/serving.html"&gt;Serving&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚õ∞Ô∏è Advanced Tutorials&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/OCR.html"&gt;PP-OCRv5 Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PP-StructureV3.html"&gt;PP-StructureV3 Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PP-ChatOCRv4.html"&gt;PP-ChatOCRv4 Tutorial&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üîÑ Quick Overview of Execution Results&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/demo.gif" alt="PP-OCRv5 Demo" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/blue_v3.gif" alt="PP-StructureV3 Demo" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;‚ú® Stay Tuned&lt;/h2&gt; 
&lt;p&gt;‚≠ê &lt;strong&gt;Star this repository to keep up with exciting updates and new releases, including powerful OCR and document parsing capabilities!&lt;/strong&gt; ‚≠ê&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="1200" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/star_paddleocr.en.gif" alt="Star-Project" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üë©‚Äçüë©‚Äçüëß‚Äçüë¶ Community&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;PaddlePaddle WeChat official account&lt;/th&gt; 
    &lt;th align="center"&gt;Join the tech discussion group&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/refs/heads/main/images/paddleocr/README/qrcode_for_paddlepaddle_official_account.jpg" width="150" /&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/refs/heads/main/images/paddleocr/README/qr_code_for_the_questionnaire.jpg" width="150" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;üòÉ Awesome Projects Leveraging PaddleOCR&lt;/h2&gt; 
&lt;p&gt;PaddleOCR wouldn't be where it is today without its incredible community! üíó A massive thank you to all our longtime partners, new collaborators, and everyone who's poured their passion into PaddleOCR ‚Äî whether we've named you or not. Your support fuels our fire!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Project Name&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/infiniflow/ragflow"&gt;RAGFlow&lt;/a&gt; &lt;a href="https://github.com/infiniflow/ragflow"&gt;&lt;img src="https://img.shields.io/github/stars/infiniflow/ragflow" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;RAG engine based on deep document understanding.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/opendatalab/MinerU"&gt;MinerU&lt;/a&gt; &lt;a href="https://github.com/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/github/stars/opendatalab/MinerU" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Multi-type Document to Markdown Conversion Tool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hiroi-sora/Umi-OCR"&gt;Umi-OCR&lt;/a&gt; &lt;a href="https://github.com/hiroi-sora/Umi-OCR"&gt;&lt;img src="https://img.shields.io/github/stars/hiroi-sora/Umi-OCR" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Free, Open-source, Batch Offline OCR Software.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/OmniParser"&gt;OmniParser&lt;/a&gt;&lt;a href="https://github.com/microsoft/OmniParser"&gt;&lt;img src="https://img.shields.io/github/stars/microsoft/OmniParser" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;OmniParser: Screen Parsing tool for Pure Vision Based GUI Agent.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/netease-youdao/QAnything"&gt;QAnything&lt;/a&gt;&lt;a href="https://github.com/netease-youdao/QAnything"&gt;&lt;img src="https://img.shields.io/github/stars/netease-youdao/QAnything" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Question and Answer based on Anything.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;PDF-Extract-Kit&lt;/a&gt; &lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;&lt;img src="https://img.shields.io/github/stars/opendatalab/PDF-Extract-Kit" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;A powerful open-source toolkit designed to efficiently extract high-quality content from complex and diverse PDF documents.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/PantsuDango/Dango-Translator"&gt;Dango-Translator&lt;/a&gt;&lt;a href="https://github.com/PantsuDango/Dango-Translator"&gt;&lt;img src="https://img.shields.io/github/stars/PantsuDango/Dango-Translator" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Recognize text on the screen, translate it and show the translation results in real time.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/awesome_projects.md"&gt;Learn more projects&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/awesome_projects.md"&gt;More projects based on PaddleOCR&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;üë©‚Äçüë©‚Äçüëß‚Äçüë¶ Contributors&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/PaddlePaddle/PaddleOCR/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=PaddlePaddle/PaddleOCR&amp;amp;max=400&amp;amp;columns=20" width="800" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;üåü Star&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="800" src="https://api.star-history.com/svg?repos=PaddlePaddle/PaddleOCR&amp;amp;type=Date" alt="Star-history" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;This project is released under the &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/LICENSE"&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üéì Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@misc{cui2025paddleocr30technicalreport,
      title={PaddleOCR 3.0 Technical Report}, 
      author={Cheng Cui and Ting Sun and Manhui Lin and Tingquan Gao and Yubo Zhang and Jiaxuan Liu and Xueqing Wang and Zelun Zhang and Changda Zhou and Hongen Liu and Yue Zhang and Wenyu Lv and Kui Huang and Yichao Zhang and Jing Zhang and Jun Zhang and Yi Liu and Dianhai Yu and Yanjun Ma},
      year={2025},
      eprint={2507.05595},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2507.05595}, 
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Alibaba-NLP/DeepResearch</title>
      <link>https://github.com/Alibaba-NLP/DeepResearch</link>
      <description>&lt;p&gt;Tongyi DeepResearch, the Leading Open-source DeepResearch Agent&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/logo.png" width="100%" /&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center" style="line-height: 1;"&gt; 
 &lt;p&gt;&lt;a href="https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B"&gt;&lt;img src="https://img.shields.io/badge/Models-5EDDD2?style=for-the-badge&amp;amp;logo=huggingface&amp;amp;logoColor=ffffff&amp;amp;labelColor" alt="MODELS" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Alibaba-NLP/DeepResearch"&gt;&lt;img src="https://img.shields.io/badge/Github-24292F?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" alt="GITHUB" /&gt;&lt;/a&gt; &lt;a href="https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/"&gt;&lt;img src="https://img.shields.io/badge/Blog-4285F4?style=for-the-badge&amp;amp;logo=google-chrome&amp;amp;logoColor=white" alt="Blog" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; ü§ó &lt;a href="https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B" target="_blank"&gt;HuggingFace&lt;/a&gt; ÔΩú &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/tongyi.png" width="14px" style="display:inline;" /&gt; &lt;a href="https://modelscope.cn/models/iic/Tongyi-DeepResearch-30B-A3B" target="_blank"&gt;ModelScope&lt;/a&gt; &lt;/p&gt;
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/14217" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14217" alt="Alibaba-NLP%2FWebAgent | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt;
&lt;h1&gt;Introduction&lt;/h1&gt; 
&lt;p&gt;We present &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/tongyi.png" width="14px" style="display:inline;" /&gt; &lt;strong&gt;Tongyi DeepResearch&lt;/strong&gt;, an agentic large language model featuring 30.5 billion total parameters, with only 3.3 billion activated per token. Developed by Tongyi Lab, the model is specifically designed for &lt;strong&gt;long-horizon, deep information-seeking&lt;/strong&gt; tasks. Tongyi DeepResearch demonstrates state-of-the-art performance across a range of agentic search benchmarks, including Humanity's Last Exam, BrowserComp, BrowserComp-ZH, WebWalkerQA,xbench-DeepSearch, FRAMES and SimpleQA.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Tongyi DeepResearch builds upon our previous work on the &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/tongyi.png" width="14px" style="display:inline;" /&gt; &lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/WebAgent/"&gt;WebAgent&lt;/a&gt; project.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;More details can be found in our üì∞&amp;nbsp;&lt;a href="https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/"&gt;Tech Blog&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/performance.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚öôÔ∏è &lt;strong&gt;Fully automated synthetic data generation pipeline&lt;/strong&gt;: We design a highly scalable data synthesis pipeline, which is fully automatic and empowers agentic pre-training, supervised fine-tuning, and reinforcement learning.&lt;/li&gt; 
 &lt;li&gt;üîÑ &lt;strong&gt;Large-scale continual pre-training on agentic data&lt;/strong&gt;: Leveraging diverse, high-quality agentic interaction data to extend model capabilities, maintain freshness, and strengthen reasoning performance.&lt;/li&gt; 
 &lt;li&gt;üîÅ &lt;strong&gt;End-to-end reinforcement learning&lt;/strong&gt;: We employ a strictly on-policy RL approach based on a customized Group Relative Policy Optimization framework, with token-level policy gradients, leave-one-out advantage estimation, and selective filtering of negative samples to stabilize training in a non‚Äëstationary environment.&lt;/li&gt; 
 &lt;li&gt;ü§ñ &lt;strong&gt;Agent Inference Paradigm Compatibility&lt;/strong&gt;: At inference, Tongyi DeepResearch is compatible with two inference paradigms: ReAct, for rigorously evaluating the model's core intrinsic abilities, and an IterResearch-based 'Heavy' mode, which uses a test-time scaling strategy to unlock the model's maximum performance ceiling.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Model Download&lt;/h1&gt; 
&lt;p&gt;You can directly download the model by following the links below.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Model&lt;/th&gt; 
   &lt;th align="center"&gt;Download Links&lt;/th&gt; 
   &lt;th align="center"&gt;Model Size&lt;/th&gt; 
   &lt;th align="center"&gt;Context Length&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Tongyi-DeepResearch-30B-A3B&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B"&gt;ü§ó HuggingFace&lt;/a&gt;&lt;br /&gt; &lt;a href="https://modelscope.cn/models/iic/Tongyi-DeepResearch-30B-A3B"&gt;ü§ñ ModelScope&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;30B-A3B&lt;/td&gt; 
   &lt;td align="center"&gt;128K&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;News&lt;/h1&gt; 
&lt;p&gt;[2025/09/17]üî• We have released &lt;strong&gt;Tongyi-DeepResearch-30B-A3B&lt;/strong&gt;.&lt;/p&gt; 
&lt;h1&gt;Deep Research Benchmark Results&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/benchmark.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;This guide provides instructions for setting up the environment and running inference scripts located in the &lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/inference/"&gt;inference&lt;/a&gt; folder.&lt;/p&gt; 
&lt;h3&gt;1. Environment Setup&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Recommended Python version: &lt;strong&gt;3.10.0&lt;/strong&gt; (using other versions may cause dependency issues).&lt;/li&gt; 
 &lt;li&gt;It is strongly advised to create an isolated environment using &lt;code&gt;conda&lt;/code&gt; or &lt;code&gt;virtualenv&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Example with Conda
conda create -n react_infer_env python=3.10.0 
conda activate react_infer_env
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Installation&lt;/h3&gt; 
&lt;p&gt;Install the required dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Prepare Evaluation Data&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create a folder named &lt;code&gt;eval_data/&lt;/code&gt; in the project root.&lt;/li&gt; 
 &lt;li&gt;Place your QA file in &lt;strong&gt;JSONL&lt;/strong&gt; format inside this directory, e.g. &lt;code&gt;eval_data/example.jsonl&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Each line must be a JSON object that includes &lt;strong&gt;both&lt;/strong&gt; of the following keys: &lt;pre&gt;&lt;code class="language-json"&gt;{"question": "...","answer": "..."}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;A sample file is provided in the &lt;code&gt;eval_data&lt;/code&gt; folder for reference.&lt;/li&gt; 
 &lt;li&gt;If you plan to use the &lt;em&gt;file parser&lt;/em&gt; tool, &lt;strong&gt;prepend the file name to the &lt;code&gt;question&lt;/code&gt; field&lt;/strong&gt; and place the referenced file inside the &lt;code&gt;eval_data/file_corpus/&lt;/code&gt; directory.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;4. Configure the Inference Script&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Open &lt;code&gt;run_react_infer.sh&lt;/code&gt; and modify the following variables as instructed in the comments: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;MODEL_PATH&lt;/code&gt; - path to the local or remote model weights.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;DATASET&lt;/code&gt; - path to the evaluation set, e.g. &lt;code&gt;example&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;OUTPUT_PATH&lt;/code&gt; - path for saving the prediction results, e.g. &lt;code&gt;./outputs&lt;/code&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Depending on the tools you enable (retrieval, calculator, web search, etc.), provide the required &lt;code&gt;API_KEY&lt;/code&gt;, &lt;code&gt;BASE_URL&lt;/code&gt;, or other credentials. Each key is explained inline in the bash script.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;5. Run the Inference Script&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;bash run_react_infer.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;With these steps, you can fully prepare the environment, configure the dataset, and run the model. For more details, consult the inline comments in each script or open an issue.&lt;/p&gt; 
&lt;h2&gt;Benchmark Evaluation&lt;/h2&gt; 
&lt;p&gt;We provide benchmark evaluation scripts for various datasets. Please refer to the &lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/evaluation/"&gt;evaluation scripts&lt;/a&gt; directory for more details.&lt;/p&gt; 
&lt;h2&gt;Deep Research Agent Family&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/family.png" /&gt; &lt;/p&gt; 
&lt;p&gt;Tongyi DeepResearch also has an extensive deep research agent family. You can find more information in the following paper:&lt;/p&gt; 
&lt;p&gt;[1] &lt;a href="https://arxiv.org/pdf/2501.07572"&gt;WebWalker: Benchmarking LLMs in Web Traversal&lt;/a&gt;&lt;br /&gt; [2] &lt;a href="https://arxiv.org/pdf/2505.22648"&gt;WebDancer: Towards Autonomous Information Seeking Agency&lt;/a&gt;&lt;br /&gt; [3] &lt;a href="https://arxiv.org/pdf/2507.02592"&gt;WebSailor: Navigating Super-human Reasoning for Web Agent&lt;/a&gt;&lt;br /&gt; [4] &lt;a href="https://arxiv.org/pdf/2507.15061"&gt;WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization&lt;/a&gt;&lt;br /&gt; [5] &lt;a href="https://arxiv.org/pdf/2508.05748"&gt;WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent&lt;/a&gt;&lt;br /&gt; [6] &lt;a href="https://arxiv.org/pdf/2509.13309"&gt;WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents&lt;/a&gt;&lt;br /&gt; [7] &lt;a href="https://arxiv.org/pdf/2509.13313"&gt;ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization&lt;/a&gt;&lt;br /&gt; [8] &lt;a href="https://arxiv.org/pdf/2509.13312"&gt;WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research&lt;/a&gt;&lt;br /&gt; [9] &lt;a href="https://arxiv.org/pdf/2509.13305"&gt;WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning&lt;/a&gt;&lt;br /&gt; [10] &lt;a href="https://arxiv.org/pdf/2509.13310"&gt;Scaling Agents via Continual Pre-training&lt;/a&gt;&lt;br /&gt; [11] &lt;a href="https://arxiv.org/pdf/2509.13311"&gt;Towards General Agentic Intelligence via Environment Scaling&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üåü Misc&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://www.star-history.com/#Alibaba-NLP/DeepResearch&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Alibaba-NLP/DeepResearch&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üö© Talent Recruitment&lt;/h2&gt; 
&lt;p&gt;üî•üî•üî• We are hiring! Research intern positions are open (based in Hangzhou„ÄÅBeijing„ÄÅShanghai)&lt;/p&gt; 
&lt;p&gt;üìö &lt;strong&gt;Research Area&lt;/strong&gt;ÔºöWeb Agent, Search Agent, Agent RL, MultiAgent RL, Agentic RAG&lt;/p&gt; 
&lt;p&gt;‚òéÔ∏è &lt;strong&gt;Contact&lt;/strong&gt;Ôºö&lt;a href=""&gt;yongjiang.jy@alibaba-inc.com&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contact Information&lt;/h2&gt; 
&lt;p&gt;For communications, please contact Yong Jiang (&lt;a href="mailto:yongjiang.jy@alibaba-inc.com"&gt;yongjiang.jy@alibaba-inc.com&lt;/a&gt;).&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{tongyidr,
  author={Tongyi DeepResearch Team},
  title={Tongyi-DeepResearch},
  year={2025},
  howpublished={\url{https://github.com/Alibaba-NLP/DeepResearch}}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>dagster-io/dagster</title>
      <link>https://github.com/dagster-io/dagster</link>
      <description>&lt;p&gt;An orchestration platform for the development, production, and observation of data assets.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;!-- Note: Do not try adding the dark mode version here with the `picture` element, it will break formatting in PyPI --&gt; 
 &lt;a target="_blank" href="https://dagster.io" style="background:none"&gt; &lt;img alt="dagster logo" src="https://raw.githubusercontent.com/dagster-io/dagster/master/.github/dagster-readme-header.svg?sanitize=true" width="auto" height="100%" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://github.com/dagster-io/dagster" style="background:none"&gt; &lt;img src="https://img.shields.io/github/stars/dagster-io/dagster?labelColor=4F43DD&amp;amp;color=163B36&amp;amp;logo=github" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://github.com/dagster-io/dagster/raw/master/LICENSE" style="background:none"&gt; &lt;img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg?label=license&amp;amp;labelColor=4F43DD&amp;amp;color=163B36" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://pypi.org/project/dagster/" style="background:none"&gt; &lt;img src="https://img.shields.io/pypi/v/dagster?labelColor=4F43DD&amp;amp;color=163B36" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://pypi.org/project/dagster/" style="background:none"&gt; &lt;img src="https://img.shields.io/pypi/pyversions/dagster?labelColor=4F43DD&amp;amp;color=163B36" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://twitter.com/dagster" style="background:none"&gt; &lt;img src="https://img.shields.io/badge/twitter-dagster-blue.svg?labelColor=4F43DD&amp;amp;color=163B36&amp;amp;logo=twitter" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://dagster.io/slack" style="background:none"&gt; &lt;img src="https://img.shields.io/badge/slack-dagster-blue.svg?labelColor=4F43DD&amp;amp;color=163B36&amp;amp;logo=slack" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://linkedin.com/showcase/dagster" style="background:none"&gt; &lt;img src="https://img.shields.io/badge/linkedin-dagster-blue.svg?labelColor=4F43DD&amp;amp;color=163B36&amp;amp;logo=linkedin" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Dagster is a cloud-native data pipeline orchestrator for the whole development lifecycle, with integrated lineage and observability, a declarative programming model, and best-in-class testability.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;It is designed for &lt;strong&gt;developing and maintaining data assets&lt;/strong&gt;, such as tables, data sets, machine learning models, and reports.&lt;/p&gt; 
&lt;p&gt;With Dagster, you declare‚Äîas Python functions‚Äîthe data assets that you want to build. Dagster then helps you run your functions at the right time and keep your assets up-to-date.&lt;/p&gt; 
&lt;p&gt;Here is an example of a graph of three assets defined in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from dagster import asset
from pandas import DataFrame, read_html, get_dummies
from sklearn.linear_model import LinearRegression

@asset
def country_populations() -&amp;gt; DataFrame:
    df = read_html("https://tinyurl.com/mry64ebh")[0]
    df.columns = ["country", "pop2022", "pop2023", "change", "continent", "region"]
    df["change"] = df["change"].str.rstrip("%").str.replace("‚àí", "-").astype("float")
    return df

@asset
def continent_change_model(country_populations: DataFrame) -&amp;gt; LinearRegression:
    data = country_populations.dropna(subset=["change"])
    return LinearRegression().fit(get_dummies(data[["continent"]]), data["change"])

@asset
def continent_stats(country_populations: DataFrame, continent_change_model: LinearRegression) -&amp;gt; DataFrame:
    result = country_populations.groupby("continent").sum()
    result["pop_change_factor"] = continent_change_model.coef_
    return result
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The graph loaded into Dagster's web UI:&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img width="100%" alt="An example asset graph as rendered in the Dagster UI" src="https://raw.githubusercontent.com/dagster-io/dagster/master/.github/example-lineage.png" /&gt; &lt;/p&gt; 
&lt;p&gt;Dagster is built to be used at every stage of the data development lifecycle - local development, unit tests, integration tests, staging environments, all the way up to production.&lt;/p&gt; 
&lt;h2&gt;Quick Start:&lt;/h2&gt; 
&lt;p&gt;If you're new to Dagster, we recommend checking out the &lt;a href="https://docs.dagster.io"&gt;docs&lt;/a&gt; or following the hands-on &lt;a href="https://docs.dagster.io/etl-pipeline-tutorial/"&gt;tutorial&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Dagster is available on PyPI and officially supports Python 3.9 through Python 3.13.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install dagster dagster-webserver
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This installs two packages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;dagster&lt;/code&gt;: The core programming model.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;dagster-webserver&lt;/code&gt;: The server that hosts Dagster's web UI for developing and operating Dagster jobs and assets.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;You can find the full Dagster documentation &lt;a href="https://docs.dagster.io"&gt;here&lt;/a&gt;, including the &lt;a href="https://docs.dagster.io/getting-started/quickstart"&gt;Quickstart guide&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Key Features:&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img width="100%" alt="image" src="https://raw.githubusercontent.com/dagster-io/dagster/master/.github/key-features-cards.svg?sanitize=true" /&gt; &lt;/p&gt; 
&lt;h3&gt;Dagster as a productivity platform&lt;/h3&gt; 
&lt;p&gt;Identify the key assets you need to create using a declarative approach, or you can focus on running basic tasks. Embrace CI/CD best practices from the get-go: build reusable components, spot data quality issues, and flag bugs early.&lt;/p&gt; 
&lt;h3&gt;Dagster as a robust orchestration engine&lt;/h3&gt; 
&lt;p&gt;Put your pipelines into production with a robust multi-tenant, multi-tool engine that scales technically and organizationally.&lt;/p&gt; 
&lt;h3&gt;Dagster as a unified control plane&lt;/h3&gt; 
&lt;p&gt;Maintain control over your data as the complexity scales. Centralize your metadata in one tool with built-in observability, diagnostics, cataloging, and lineage. Spot any issues and identify performance improvement opportunities.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Master the Modern Data Stack with integrations&lt;/h2&gt; 
&lt;p&gt;Dagster provides a growing library of integrations for today‚Äôs most popular data tools. Integrate with the tools you already use, and deploy to your infrastructure.&lt;/p&gt; 
&lt;br /&gt; 
&lt;p align="center"&gt; &lt;a target="_blank" href="https://dagster.io/integrations" style="background:none"&gt; &lt;img width="100%" alt="image" src="https://raw.githubusercontent.com/dagster-io/dagster/master/.github/integrations-bar-for-readme.png" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Connect with thousands of other data practitioners building with Dagster. Share knowledge, get help, and contribute to the open-source project. To see featured material and upcoming events, check out our &lt;a href="https://dagster.io/community"&gt;Dagster Community&lt;/a&gt; page.&lt;/p&gt; 
&lt;p&gt;Join our community here:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üåü &lt;a href="https://github.com/dagster-io/dagster"&gt;Star us on GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üì• &lt;a href="https://dagster.io/newsletter-signup"&gt;Subscribe to our Newsletter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üê¶ &lt;a href="https://twitter.com/dagster"&gt;Follow us on Twitter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üï¥Ô∏è &lt;a href="https://linkedin.com/showcase/dagster"&gt;Follow us on LinkedIn&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üì∫ &lt;a href="https://www.youtube.com/@dagsterio"&gt;Subscribe to our YouTube channel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìö &lt;a href="https://dagster.io/blog"&gt;Read our blog posts&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üëã &lt;a href="https://dagster.io/slack"&gt;Join us on Slack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üóÉ &lt;a href="https://discuss.dagster.io"&gt;Browse Slack archives&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;‚úèÔ∏è &lt;a href="https://github.com/dagster-io/dagster/discussions"&gt;Start a GitHub Discussion&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;For details on contributing or running the project for development, check out our &lt;a href="https://docs.dagster.io/about/contributing"&gt;contributing guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Dagster is &lt;a href="https://github.com/dagster-io/dagster/raw/master/LICENSE"&gt;Apache 2.0 licensed&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>sansan0/TrendRadar</title>
      <link>https://github.com/sansan0/TrendRadar</link>
      <description>&lt;p&gt;üéØ ÂëäÂà´‰ø°ÊÅØËøáËΩΩÔºåÂè™ÁúãÁúüÊ≠£ÂÖ≥ÂøÉÁöÑÊñ∞Èóª - Â§öÂπ≥Âè∞ÁÉ≠ÁÇπËÅöÂêàÂ∑•ÂÖ∑ÔºåË∂ãÂäøÂàÜÊûêÂ∑•ÂÖ∑Ôºå‰∏ÄÈîÆÁõëÊéßÊäñÈü≥„ÄÅÁü•‰πé„ÄÅÂìîÂì©ÂìîÂì©„ÄÅ‰ªäÊó•Â§¥Êù°„ÄÅÁôæÂ∫¶ÁÉ≠Êêú„ÄÅË¥¥Âêß„ÄÅÂæÆÂçö„ÄÅÂçéÂ∞îË°óËßÅÈóª„ÄÅË¥¢ËÅîÁ§æÁ≠â35‰∏™Âπ≥Âè∞ÔºåÊô∫ËÉΩÂÖ≥ÈîÆËØçÁ≠õÈÄâÔºåËá™Âä®ÁîüÊàêÁÉ≠ÁÇπÂàÜÊûêÊä•Âëä„ÄÇÊîØÊåÅ‰ºÅ‰∏öÂæÆ‰ø°„ÄÅÈ£û‰π¶„ÄÅÈíâÈíâ„ÄÅTelegramÊé®ÈÄÅÔºå30ÁßíÁΩëÈ°µÈÉ®ÁΩ≤Ôºå1ÂàÜÈíüÊâãÊú∫ÈÄöÁü•ÔºåÊó†ÈúÄÁºñÁ®ãÂü∫Á°Ä„ÄÇ‰πüÊîØÊåÅdockerÁßÅ‰∫∫ÈÉ®ÁΩ≤‚≠ê ËÆ©ÁÆóÊ≥ï‰∏∫‰Ω†ÊúçÂä°ÔºåËÄåÈùûË¢´ÁÆóÊ≥ïÁªëÊû∂&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;üéØTrendRadar&lt;/h1&gt; 
 &lt;p&gt;üöÄ ÊúÄÂø´&lt;strong&gt;30Áßí&lt;/strong&gt;ÈÉ®ÁΩ≤ÁöÑÁÉ≠ÁÇπÂä©Êâã ‚Äî‚Äî ÂëäÂà´Êó†ÊïàÂà∑Â±èÔºåÂè™ÁúãÁúüÊ≠£ÂÖ≥ÂøÉÁöÑÊñ∞ÈóªËµÑËÆØ&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/sansan0/TrendRadar/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/sansan0/TrendRadar?style=flat-square&amp;amp;logo=github&amp;amp;color=yellow" alt="GitHub Stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sansan0/TrendRadar/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/sansan0/TrendRadar?style=flat-square&amp;amp;logo=github&amp;amp;color=blue" alt="GitHub Forks" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-GPL--3.0-blue.svg?style=flat-square" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sansan0/TrendRadar"&gt;&lt;img src="https://img.shields.io/badge/version-v2.2.0-green.svg?style=flat-square" alt="Version" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://work.weixin.qq.com/"&gt;&lt;img src="https://img.shields.io/badge/%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1-%E9%80%9A%E7%9F%A5%E6%94%AF%E6%8C%81-00D4AA?style=flat-square" alt="‰ºÅ‰∏öÂæÆ‰ø°ÈÄöÁü•" /&gt;&lt;/a&gt; &lt;a href="https://telegram.org/"&gt;&lt;img src="https://img.shields.io/badge/Telegram-%E9%80%9A%E7%9F%A5%E6%94%AF%E6%8C%81-00D4AA?style=flat-square" alt="TelegramÈÄöÁü•" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#"&gt;&lt;img src="https://img.shields.io/badge/%E9%92%89%E9%92%89-%E9%80%9A%E7%9F%A5%E6%94%AF%E6%8C%81-00D4AA?style=flat-square" alt="dingtalkÈÄöÁü•" /&gt;&lt;/a&gt; &lt;a href="https://www.feishu.cn/"&gt;&lt;img src="https://img.shields.io/badge/%E9%A3%9E%E4%B9%A6-%E9%80%9A%E7%9F%A5%E6%94%AF%E6%8C%81-00D4AA?style=flat-square" alt="È£û‰π¶ÈÄöÁü•" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sansan0/TrendRadar"&gt;&lt;img src="https://img.shields.io/badge/GitHub_Actions-%E8%87%AA%E5%8A%A8%E5%8C%96-2088FF?style=flat-square&amp;amp;logo=github-actions&amp;amp;logoColor=white" alt="GitHub Actions" /&gt;&lt;/a&gt; &lt;a href="https://sansan0.github.io/TrendRadar"&gt;&lt;img src="https://img.shields.io/badge/GitHub_Pages-%E9%83%A8%E7%BD%B2-4285F4?style=flat-square&amp;amp;logo=github&amp;amp;logoColor=white" alt="GitHub Pages" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/"&gt;&lt;img src="https://img.shields.io/badge/Docker-%E9%83%A8%E7%BD%B2-2496ED?style=flat-square&amp;amp;logo=docker&amp;amp;logoColor=white" alt="Docker" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Êú¨È°πÁõÆ‰ª•ËΩªÈáèÔºåÊòìÈÉ®ÁΩ≤‰∏∫ÁõÆÊ†áÔºå‰∏ªË¶ÅÂ§ÑÁêÜ issues&lt;/p&gt; 
 &lt;p&gt;ÈÅáÂà∞ÈóÆÈ¢òÊèê issuesÔºåÊàñ„ÄêÁ°ÖÂü∫Ëå∂Ê∞¥Èó¥„ÄëÂÖ¨‰ºóÂè∑ÁïôË®Ä&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details&gt; 
 &lt;summary&gt;üëâ ÁÇπÂáªÊü•Áúã&lt;strong&gt;Ëá¥Ë∞¢ÂêçÂçï&lt;/strong&gt; (ÂΩìÂâç &lt;strong&gt;üî•19üî•&lt;/strong&gt; ‰Ωç)&lt;/summary&gt; 
 &lt;h3&gt;Êï∞ÊçÆÊîØÊåÅ&lt;/h3&gt; 
 &lt;p&gt;Êú¨È°πÁõÆ‰ΩøÁî®‰∫Ü &lt;a href="https://github.com/ourongxing/newsnow"&gt;newsnow&lt;/a&gt; È°πÁõÆÊèê‰æõÁöÑ API Êé•Âè£Ëé∑ÂèñÂ§öÂπ≥Âè∞Êï∞ÊçÆ&lt;/p&gt; 
 &lt;h3&gt;Êé®ÂπøÂä©Âäõ&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;ÊÑüË∞¢‰ª•‰∏ãÂπ≥Âè∞Âíå‰∏™‰∫∫ÁöÑÊé®Ëçê(ÊåâÊó∂Èó¥ÊéíÂàó)Ôºå‰ª•ÂèäÂêÑÂæÆ‰ø°Áæ§ÔºåqqÁæ§Á≠âÁªôÂà∞Ëøô‰∏™È°πÁõÆÂ∏ÆÂä©ÁöÑ‰∫∫&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/fvutkJ_NPUelSW9OGK39aA"&gt;Â∞è‰ºóËΩØ‰ª∂&lt;/a&gt; - ÂºÄÊ∫êËΩØ‰ª∂Êé®ËçêÂπ≥Âè∞&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://linux.do/"&gt;LinuxDo Á§æÂå∫&lt;/a&gt; - ÊäÄÊúØÁà±Â•ΩËÄÖÁöÑËÅöÈõÜÂú∞&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/ruanyf/weekly"&gt;ÈòÆ‰∏ÄÂ≥∞Âë®Âàä&lt;/a&gt; - ÊäÄÊúØÂúàÊúâÂΩ±ÂìçÂäõÁöÑÂë®Âàä&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;ËßÇ‰ºóÊîØÊåÅ&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;ÊÑüË∞¢‰ª•‰∏ãÁÉ≠ÂøÉËßÇ‰ºóÁöÑ‰ø°‰ªª‰∏éÊîØÊåÅ&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;ÁÇπËµû‰∫∫&lt;/th&gt; 
    &lt;th align="center"&gt;ÈáëÈ¢ù&lt;/th&gt; 
    &lt;th align="center"&gt;Êó•Êúü&lt;/th&gt; 
    &lt;th align="center"&gt;Â§áÊ≥®&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;**Êòä&lt;/td&gt; 
    &lt;td align="center"&gt;5&lt;/td&gt; 
    &lt;td align="center"&gt;2025.9.17&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*Âè∑&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.9.15&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;T*T&lt;/td&gt; 
    &lt;td align="center"&gt;2&lt;/td&gt; 
    &lt;td align="center"&gt;2025.9.15&lt;/td&gt; 
    &lt;td align="center"&gt;ÁÇπËµû&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*ÂÆ∂&lt;/td&gt; 
    &lt;td align="center"&gt;10&lt;/td&gt; 
    &lt;td align="center"&gt;2025.9.10&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*X&lt;/td&gt; 
    &lt;td align="center"&gt;1.11&lt;/td&gt; 
    &lt;td align="center"&gt;2025.9.3&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*È£ô&lt;/td&gt; 
    &lt;td align="center"&gt;20&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.31&lt;/td&gt; 
    &lt;td align="center"&gt;Êù•Ëá™ËÄÅÁ´•Ë∞¢Ë∞¢&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*‰∏ã&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.30&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;2*D&lt;/td&gt; 
    &lt;td align="center"&gt;88&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.13 ‰∏ãÂçà&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;2*D&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.13 ‰∏äÂçà&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;S*o&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.05&lt;/td&gt; 
    &lt;td align="center"&gt;ÊîØÊåÅ‰∏Ä‰∏ã&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*‰æ†&lt;/td&gt; 
    &lt;td align="center"&gt;10&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.04&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;x*x&lt;/td&gt; 
    &lt;td align="center"&gt;2&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.03&lt;/td&gt; 
    &lt;td align="center"&gt;trendRadar Â•ΩÈ°πÁõÆ ÁÇπËµû&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*Ëøú&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.01&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*ÈÇ™&lt;/td&gt; 
    &lt;td align="center"&gt;5&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.01&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*Ê¢¶&lt;/td&gt; 
    &lt;td align="center"&gt;0.1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.7.30&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;**Èæô&lt;/td&gt; 
    &lt;td align="center"&gt;10&lt;/td&gt; 
    &lt;td align="center"&gt;2025.7.29&lt;/td&gt; 
    &lt;td align="center"&gt;ÊîØÊåÅ‰∏Ä‰∏ã&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚ú® Ê†∏ÂøÉÂäüËÉΩ&lt;/h2&gt; 
&lt;h3&gt;&lt;strong&gt;ÂÖ®ÁΩëÁÉ≠ÁÇπËÅöÂêà&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;‰ªäÊó•Â§¥Êù°&lt;/li&gt; 
 &lt;li&gt;ÁôæÂ∫¶ÁÉ≠Êêú&lt;/li&gt; 
 &lt;li&gt;ÂçéÂ∞îË°óËßÅÈóª&lt;/li&gt; 
 &lt;li&gt;ÊæéÊπÉÊñ∞Èóª&lt;/li&gt; 
 &lt;li&gt;bilibili ÁÉ≠Êêú&lt;/li&gt; 
 &lt;li&gt;Ë¥¢ËÅîÁ§æÁÉ≠Èó®&lt;/li&gt; 
 &lt;li&gt;Âá§Âá∞ÁΩë&lt;/li&gt; 
 &lt;li&gt;Ë¥¥Âêß&lt;/li&gt; 
 &lt;li&gt;ÂæÆÂçö&lt;/li&gt; 
 &lt;li&gt;ÊäñÈü≥&lt;/li&gt; 
 &lt;li&gt;Áü•‰πé&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ÈªòËÆ§ÁõëÊéß 11 ‰∏™‰∏ªÊµÅÂπ≥Âè∞ÔºåÂ¶ÇÊÉ≥È¢ùÂ§ñÂ¢ûÂä†ÔºåÂèØÁúãÊúÄ‰∏ãÊñπÁöÑ&lt;strong&gt;Ëá™ÂÆö‰πâÁõëÊéßÂπ≥Âè∞&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;Êô∫ËÉΩÊé®ÈÄÅÁ≠ñÁï•&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;‰∏âÁßçÊé®ÈÄÅÊ®°Âºè&lt;/strong&gt;Ôºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üìà ÊäïËµÑËÄÖ/‰∫§ÊòìÂëò&lt;/strong&gt; ‚Üí ÈÄâÊã© &lt;code&gt;incremental&lt;/code&gt;ÔºåÂèäÊó∂Ëé∑ÂèñÊñ∞Â¢ûËµÑËÆØ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üì∞ Ëá™Â™í‰Ωì‰∫∫/ÂÜÖÂÆπÂàõ‰ΩúËÄÖ&lt;/strong&gt; ‚Üí ÈÄâÊã© &lt;code&gt;current&lt;/code&gt;ÔºåÊéåÊè°ÂÆûÊó∂ÁÉ≠ÁÇπË∂ãÂäø&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìã ‰ºÅ‰∏öÁÆ°ÁêÜËÄÖ/ÊôÆÈÄöÁî®Êà∑&lt;/strong&gt; ‚Üí ÈÄâÊã© &lt;code&gt;daily&lt;/code&gt;ÔºåÂÆöÊó∂Ëé∑ÂèñÂÆåÊï¥Êó•Êä•&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;ÈùôÈªòÊé®ÈÄÅÊ®°Âºè&lt;/strong&gt;Ôºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Êó∂Èó¥ËåÉÂõ¥ÊéßÂà∂&lt;/strong&gt;ÔºöËÆæÂÆöÊé®ÈÄÅÊó∂Èó¥Á™óÂè£ÔºàÂ¶Ç 9:00-18:00ÔºâÔºå‰ªÖÂú®ÊåáÂÆöÊó∂Èó¥ÂÜÖÊé®ÈÄÅ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÈÄÇÁî®Âú∫ÊôØ&lt;/strong&gt;Ôºö 
  &lt;ul&gt; 
   &lt;li&gt;Êó∂Èó¥ÂÜÖÊØèÊ¨°ÊâßË°åÈÉΩÊé®ÈÄÅ&lt;/li&gt; 
   &lt;li&gt;Êó∂Èó¥ËåÉÂõ¥ÂÜÖÂè™Êé®ÈÄÅ‰∏ÄÊ¨°&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;strong&gt;Á≤æÂáÜÂÜÖÂÆπÁ≠õÈÄâ&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;ËÆæÁΩÆ‰∏™‰∫∫ÂÖ≥ÈîÆËØçÔºàÂ¶ÇÔºöAI„ÄÅÊØî‰∫öËø™„ÄÅÊïôËÇ≤ÊîøÁ≠ñÔºâÔºåÂè™Êé®ÈÄÅÁõ∏ÂÖ≥ÁÉ≠ÁÇπÔºåËøáÊª§Êó†ÂÖ≥‰ø°ÊÅØ&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ÊîØÊåÅÊôÆÈÄöËØç„ÄÅÂøÖÈ°ªËØç(+)„ÄÅËøáÊª§ËØç(!)‰∏âÁßçËØ≠Ê≥ïÔºåÂÖ∑‰ΩìËßÅ„Äêfrequency_words.txt ÈÖçÁΩÆÊïôÁ®ã„Äë&lt;/li&gt; 
 &lt;li&gt;ËØçÁªÑÂåñÁÆ°ÁêÜÔºåÁã¨Á´ãÁªüËÆ°‰∏çÂêå‰∏ªÈ¢òÁÉ≠ÁÇπ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‰πüÂèØ‰ª•‰∏çÂÅöÁ≠õÈÄâÔºåÂÆåÊï¥ÁöÑÊé®ÈÄÅÊâÄÊúâÁÉ≠ÁÇπÔºåÂÖ∑‰ΩìËßÅ„ÄêÂéÜÂè≤Êõ¥Êñ∞„Äë‰∏≠ÁöÑ v2.0.1&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;‰∏™ÊÄßÂåñÁÉ≠ÁÇπÁÆóÊ≥ï&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;‰∏çÂÜçË¢´ÂêÑ‰∏™Âπ≥Âè∞ÁöÑÁÆóÊ≥ïÁâµÁùÄËµ∞ÔºåTrendRadar ‰ºöÈáçÊñ∞Êï¥ÁêÜÂÖ®ÁΩëÁÉ≠ÊêúÔºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ÁúãÈáçÊéíÂêçÈ´òÁöÑÊñ∞Èóª&lt;/strong&gt;ÔºàÂç†60%ÔºâÔºöÂêÑÂπ≥Âè∞ÂâçÂá†ÂêçÁöÑÊñ∞Èóª‰ºòÂÖàÊòæÁ§∫&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÂÖ≥Ê≥®ÊåÅÁª≠Âá∫Áé∞ÁöÑËØùÈ¢ò&lt;/strong&gt;ÔºàÂç†30%ÔºâÔºöÂèçÂ§çÂá∫Áé∞ÁöÑÊñ∞ÈóªÊõ¥ÈáçË¶Å&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ËÄÉËôëÊéíÂêçË¥®Èáè&lt;/strong&gt;ÔºàÂç†10%ÔºâÔºö‰∏ç‰ªÖÂ§öÊ¨°Âá∫Áé∞ÔºåËøòÁªèÂ∏∏ÊéíÂú®ÂâçÂàó&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;ÂÆûÈôÖÊïàÊûú&lt;/strong&gt;ÔºöÊääÂàÜÊï£Âú®ÂêÑ‰∏™Âπ≥Âè∞ÁöÑÁÉ≠ÊêúÂêàÂπ∂Ëµ∑Êù•ÔºåÊåâÁÖß‰Ω†ÂÖ≥ÂøÉÁöÑÁÉ≠Â∫¶ÈáçÊñ∞ÊéíÂ∫è&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Ëøô‰∏â‰∏™ÊØî‰æãÂèØ‰ª•ÈÄâÊã©ÈÄÇÂêàËá™Â∑±ÁöÑÂú∫ÊôØËøõË°åË∞ÉÊï¥ÔºåÂÖ∑‰ΩìËßÅ„ÄêÁÉ≠ÁÇπÊùÉÈáçË∞ÉÊï¥„Äë&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;Â§öÊ∏†ÈÅìÂÆûÊó∂Êé®ÈÄÅ&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;ÊîØÊåÅ&lt;strong&gt;‰ºÅ‰∏öÂæÆ‰ø°&lt;/strong&gt;„ÄÅ&lt;strong&gt;È£û‰π¶&lt;/strong&gt;„ÄÅ&lt;strong&gt;ÈíâÈíâ&lt;/strong&gt;„ÄÅ&lt;strong&gt;Telegram&lt;/strong&gt;ÔºåÊ∂àÊÅØÁõ¥ËææÊâãÊú∫&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;Â§öÁ´ØÈÄÇÈÖç&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Pages&lt;/strong&gt;ÔºöËá™Âä®ÁîüÊàêÁ≤æÁæéÁΩëÈ°µÊä•ÂëäÔºåPC/ÁßªÂä®Á´ØÈÄÇÈÖç&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DockerÈÉ®ÁΩ≤&lt;/strong&gt;ÔºöÊîØÊåÅÂ§öÊû∂ÊûÑÂÆπÂô®ÂåñËøêË°å&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Êï∞ÊçÆÊåÅ‰πÖÂåñ&lt;/strong&gt;ÔºöHTML/TXTÂ§öÊ†ºÂºèÂéÜÂè≤ËÆ∞ÂΩï‰øùÂ≠ò&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;strong&gt;Èõ∂ÊäÄÊúØÈó®ÊßõÈÉ®ÁΩ≤&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;GitHub ‰∏ÄÈîÆ Fork Âç≥ÂèØ‰ΩøÁî®ÔºåÊó†ÈúÄÁºñÁ®ãÂü∫Á°Ä„ÄÇ&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;30ÁßíÈÉ®ÁΩ≤Ôºö GitHub PagesÔºàÁΩëÈ°µÊµèËßàÔºâÊîØÊåÅ‰∏ÄÈîÆ‰øùÂ≠òÊàêÂõæÁâáÔºåÈöèÊó∂ÂàÜ‰∫´Áªô‰ªñ‰∫∫&lt;/p&gt; 
 &lt;p&gt;1ÂàÜÈíüÈÉ®ÁΩ≤Ôºö ‰ºÅ‰∏öÂæÆ‰ø°ÔºàÊâãÊú∫ÈÄöÁü•Ôºâ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;üí° ÊèêÁ§∫Ôºö&lt;/strong&gt; ÊÉ≥Ë¶Å&lt;strong&gt;ÂÆûÊó∂Êõ¥Êñ∞&lt;/strong&gt;ÁöÑÁΩëÈ°µÁâàÔºüfork ÂêéÔºåËøõÂÖ•‰Ω†ÁöÑ‰ªìÂ∫ì Settings ‚Üí PagesÔºåÂêØÁî® GitHub Pages„ÄÇ&lt;a href="https://sansan0.github.io/TrendRadar/"&gt;ÊïàÊûúÈ¢ÑËßà&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;ÂáèÂ∞ë APP ‰æùËµñ&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;‰ªé"Ë¢´ÁÆóÊ≥ïÊé®ËçêÁªëÊû∂"ÂèòÊàê"‰∏ªÂä®Ëé∑ÂèñËá™Â∑±ÊÉ≥Ë¶ÅÁöÑ‰ø°ÊÅØ"&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ÈÄÇÂêà‰∫∫Áæ§Ôºö&lt;/strong&gt; ÊäïËµÑËÄÖ„ÄÅËá™Â™í‰Ωì‰∫∫„ÄÅ‰ºÅ‰∏öÂÖ¨ÂÖ≥„ÄÅÂÖ≥ÂøÉÊó∂‰∫ãÁöÑÊôÆÈÄöÁî®Êà∑&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ÂÖ∏ÂûãÂú∫ÊôØÔºö&lt;/strong&gt; ËÇ°Â∏ÇÊäïËµÑÁõëÊéß„ÄÅÂìÅÁâåËàÜÊÉÖËøΩË∏™„ÄÅË°å‰∏öÂä®ÊÄÅÂÖ≥Ê≥®„ÄÅÁîüÊ¥ªËµÑËÆØËé∑Âèñ&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Github Pages ÁΩëÈ°µÊïàÊûú(ÊâãÊú∫Á´Ø‰πüÈÄÇÈÖç)&lt;/th&gt; 
   &lt;th align="center"&gt;È£û‰π¶Êé®ÈÄÅÊïàÊûú&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/github-pages.png" alt="Github PagesÊïàÊûú" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/feishu.jpg" alt="È£û‰π¶Êé®ÈÄÅÊïàÊûú" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üëâ Êé®ÈÄÅÊ†ºÂºèËØ¥Êòé&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h2&gt;&lt;strong&gt;ÈÄöÁü•Á§∫‰æãÔºö&lt;/strong&gt;&lt;/h2&gt; 
 &lt;p&gt;üìä ÁÉ≠ÁÇπËØçÊ±áÁªüËÆ°&lt;/p&gt; 
 &lt;p&gt;üî• [1/3] AI ChatGPT : 2 Êù°&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;[ÁôæÂ∫¶ÁÉ≠Êêú] üÜï ChatGPT-5Ê≠£ÂºèÂèëÂ∏É [&lt;strong&gt;1&lt;/strong&gt;] - 09Êó∂15ÂàÜ (1Ê¨°)&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[‰ªäÊó•Â§¥Êù°] AIËäØÁâáÊ¶ÇÂøµËÇ°Êö¥Ê∂® [&lt;strong&gt;3&lt;/strong&gt;] - [08Êó∂30ÂàÜ ~ 10Êó∂45ÂàÜ] (3Ê¨°)&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ&lt;/p&gt; 
 &lt;p&gt;üìà [2/3] ÊØî‰∫öËø™ ÁâπÊñØÊãâ : 2 Êù°&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;[ÂæÆÂçö] üÜï ÊØî‰∫öËø™ÊúàÈîÄÈáèÁ†¥Á∫™ÂΩï [&lt;strong&gt;2&lt;/strong&gt;] - 10Êó∂20ÂàÜ (1Ê¨°)&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[ÊäñÈü≥] ÁâπÊñØÊãâÈôç‰ª∑‰øÉÈîÄ [&lt;strong&gt;4&lt;/strong&gt;] - [07Êó∂45ÂàÜ ~ 09Êó∂15ÂàÜ] (2Ê¨°)&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ&lt;/p&gt; 
 &lt;p&gt;üìå [3/3] AËÇ° ËÇ°Â∏Ç : 1 Êù°&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;[ÂçéÂ∞îË°óËßÅÈóª] AËÇ°ÂçàÁõòÁÇπËØÑÂàÜÊûê [&lt;strong&gt;5&lt;/strong&gt;] - [11Êó∂30ÂàÜ ~ 12Êó∂00ÂàÜ] (2Ê¨°)&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;üÜï Êú¨Ê¨°Êñ∞Â¢ûÁÉ≠ÁÇπÊñ∞Èóª (ÂÖ± 2 Êù°)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;ÁôæÂ∫¶ÁÉ≠Êêú&lt;/strong&gt; (1 Êù°):&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;ChatGPT-5Ê≠£ÂºèÂèëÂ∏É [&lt;strong&gt;1&lt;/strong&gt;]&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;ÂæÆÂçö&lt;/strong&gt; (1 Êù°):&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;ÊØî‰∫öËø™ÊúàÈîÄÈáèÁ†¥Á∫™ÂΩï [&lt;strong&gt;2&lt;/strong&gt;]&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;Êõ¥Êñ∞Êó∂Èó¥Ôºö2025-01-15 12:30:15&lt;/p&gt; 
 &lt;h2&gt;&lt;strong&gt;Ê∂àÊÅØÊ†ºÂºèËØ¥Êòé&lt;/strong&gt;&lt;/h2&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Ê†ºÂºèÂÖÉÁ¥†&lt;/th&gt; 
    &lt;th&gt;Á§∫‰æã&lt;/th&gt; 
    &lt;th&gt;Âê´‰πâ&lt;/th&gt; 
    &lt;th&gt;ËØ¥Êòé&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;üî•üìàüìå&lt;/td&gt; 
    &lt;td&gt;üî• [1/3] AI ChatGPT&lt;/td&gt; 
    &lt;td&gt;ÁÉ≠Â∫¶Á≠âÁ∫ß&lt;/td&gt; 
    &lt;td&gt;üî•È´òÁÉ≠Â∫¶(‚â•10Êù°) üìà‰∏≠ÁÉ≠Â∫¶(5-9Êù°) üìåÊôÆÈÄöÁÉ≠Â∫¶(&amp;lt;5Êù°)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;[Â∫èÂè∑/ÊÄªÊï∞]&lt;/td&gt; 
    &lt;td&gt;[1/3]&lt;/td&gt; 
    &lt;td&gt;ÊéíÂ∫è‰ΩçÁΩÆ&lt;/td&gt; 
    &lt;td&gt;ÂΩìÂâçËØçÁªÑÂú®ÊâÄÊúâÂåπÈÖçËØçÁªÑ‰∏≠ÁöÑÊéíÂêç&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;È¢ëÁéáËØçÁªÑ&lt;/td&gt; 
    &lt;td&gt;AI ChatGPT&lt;/td&gt; 
    &lt;td&gt;ÂÖ≥ÈîÆËØçÁªÑ&lt;/td&gt; 
    &lt;td&gt;ÈÖçÁΩÆÊñá‰ª∂‰∏≠ÁöÑËØçÁªÑÔºåÊ†áÈ¢òÂøÖÈ°ªÂåÖÂê´ÂÖ∂‰∏≠ËØçÊ±á&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;: N Êù°&lt;/td&gt; 
    &lt;td&gt;: 2 Êù°&lt;/td&gt; 
    &lt;td&gt;ÂåπÈÖçÊï∞Èáè&lt;/td&gt; 
    &lt;td&gt;ËØ•ËØçÁªÑÂåπÈÖçÁöÑÊñ∞ÈóªÊÄªÊï∞&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;[Âπ≥Âè∞Âêç]&lt;/td&gt; 
    &lt;td&gt;[ÁôæÂ∫¶ÁÉ≠Êêú]&lt;/td&gt; 
    &lt;td&gt;Êù•Ê∫êÂπ≥Âè∞&lt;/td&gt; 
    &lt;td&gt;Êñ∞ÈóªÊâÄÂ±ûÁöÑÂπ≥Âè∞ÂêçÁß∞&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;üÜï&lt;/td&gt; 
    &lt;td&gt;üÜï ChatGPT-5Ê≠£ÂºèÂèëÂ∏É&lt;/td&gt; 
    &lt;td&gt;Êñ∞Â¢ûÊ†áËÆ∞&lt;/td&gt; 
    &lt;td&gt;Êú¨ËΩÆÊäìÂèñ‰∏≠È¶ñÊ¨°Âá∫Áé∞ÁöÑÁÉ≠ÁÇπ&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;[&lt;strong&gt;Êï∞Â≠ó&lt;/strong&gt;]&lt;/td&gt; 
    &lt;td&gt;[&lt;strong&gt;1&lt;/strong&gt;]&lt;/td&gt; 
    &lt;td&gt;È´òÊéíÂêç&lt;/td&gt; 
    &lt;td&gt;ÊéíÂêç‚â§ÈòàÂÄºÁöÑÁÉ≠ÊêúÔºåÁ∫¢Ëâ≤Âä†Á≤óÊòæÁ§∫&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;[Êï∞Â≠ó]&lt;/td&gt; 
    &lt;td&gt;[7]&lt;/td&gt; 
    &lt;td&gt;ÊôÆÈÄöÊéíÂêç&lt;/td&gt; 
    &lt;td&gt;ÊéíÂêç&amp;gt;ÈòàÂÄºÁöÑÁÉ≠ÊêúÔºåÊôÆÈÄöÊòæÁ§∫&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;- Êó∂Èó¥&lt;/td&gt; 
    &lt;td&gt;- 09Êó∂15ÂàÜ&lt;/td&gt; 
    &lt;td&gt;È¶ñÊ¨°Êó∂Èó¥&lt;/td&gt; 
    &lt;td&gt;ËØ•Êñ∞ÈóªÈ¶ñÊ¨°Ë¢´ÂèëÁé∞ÁöÑÊó∂Èó¥&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;[Êó∂Èó¥~Êó∂Èó¥]&lt;/td&gt; 
    &lt;td&gt;[08Êó∂30ÂàÜ ~ 10Êó∂45ÂàÜ]&lt;/td&gt; 
    &lt;td&gt;ÊåÅÁª≠Êó∂Èó¥&lt;/td&gt; 
    &lt;td&gt;‰ªéÈ¶ñÊ¨°Âá∫Áé∞Âà∞ÊúÄÂêéÂá∫Áé∞ÁöÑÊó∂Èó¥ËåÉÂõ¥&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;(NÊ¨°)&lt;/td&gt; 
    &lt;td&gt;(3Ê¨°)&lt;/td&gt; 
    &lt;td&gt;Âá∫Áé∞È¢ëÁéá&lt;/td&gt; 
    &lt;td&gt;Âú®ÁõëÊéßÊúüÈó¥Âá∫Áé∞ÁöÑÊÄªÊ¨°Êï∞&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Êñ∞Â¢ûÂå∫Âüü&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;üÜï &lt;strong&gt;Êú¨Ê¨°Êñ∞Â¢ûÁÉ≠ÁÇπÊñ∞Èóª&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Êñ∞ËØùÈ¢òÊ±áÊÄª&lt;/td&gt; 
    &lt;td&gt;ÂçïÁã¨Â±ïÁ§∫Êú¨ËΩÆÊñ∞Âá∫Áé∞ÁöÑÁÉ≠ÁÇπËØùÈ¢ò&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h2&gt;üìù Êõ¥Êñ∞Êó•Âøó&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;ÂçáÁ∫ßËØ¥ÊòéÔºö&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ÊèêÁ§∫1&lt;/strong&gt;ÔºöËØ∑ÈÄöËøá‰ª•‰∏ãÊñπÂºèÊõ¥Êñ∞È°πÁõÆ(ÊàñÊ†πÊçÆ&lt;strong&gt;Êõ¥Êñ∞ÊèêÁ§∫&lt;/strong&gt;ÂçáÁ∫ß)Ôºå‰∏çË¶ÅÈÄöËøá &lt;strong&gt;Sync fork&lt;/strong&gt; Êõ¥Êñ∞&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÊèêÁ§∫2&lt;/strong&gt;ÔºöÊØîÂ¶Ç‰Ω†ÂΩìÂâçÊòØv2.0.1ÔºåÊÉ≥ÂçáÁ∫ßÔºåÂª∫ËÆÆÊü•Áúã„ÄêÂéÜÂè≤Êõ¥Êñ∞„ÄëÔºåÊòéÁ°ÆÂçáÁ∫ßÁöÑÊñπÂºèÂíåÊõ¥Êñ∞ÁöÑÂäüËÉΩ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Â∞èÁâàÊú¨Êõ¥Êñ∞&lt;/strong&gt;Ôºö‰∏ÄËà¨ÊÉÖÂÜµÔºåÁõ¥Êé•Âú® GitHub ÁΩëÈ°µÁºñËæëÂô®‰∏≠ÔºåÁî®Êú¨È°πÁõÆÁöÑ &lt;code&gt;main.py&lt;/code&gt; ‰ª£Á†ÅÊõøÊç¢‰Ω† fork ‰ªìÂ∫ì‰∏≠ÁöÑÂØπÂ∫îÊñá‰ª∂&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Â§ßÁâàÊú¨ÂçáÁ∫ß&lt;/strong&gt;Ôºö‰ªé v1.x ÂçáÁ∫ßÂà∞ v2.0 Âª∫ËÆÆÂà†Èô§Áé∞Êúâ fork ÂêéÈáçÊñ∞ forkÔºåËøôÊ†∑Êõ¥ÁúÅÂäõ‰∏îÈÅøÂÖçÈÖçÁΩÆÂÜ≤Á™Å&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ÊÑüË∞¢ÂêÑ‰ΩçÊúãÂèãÁöÑÊîØÊåÅ‰∏éÂéöÁà±ÔºåÁâπÂà´ÊÑüË∞¢Ôºö&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;fork Âπ∂‰∏∫È°πÁõÆÁÇπ star&lt;/strong&gt; ÁöÑËßÇ‰ºó‰ª¨Ôºå‰Ω†‰ª¨ÁöÑËÆ§ÂèØÊòØÊàëÂâçËøõÁöÑÂä®Âäõ&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;ÂÖ≥Ê≥®ÂÖ¨‰ºóÂè∑Âπ∂ÁßØÊûÅ‰∫íÂä®&lt;/strong&gt; ÁöÑËØªËÄÖ‰ª¨Ôºå‰Ω†‰ª¨ÁöÑÁïôË®ÄÂíåÁÇπËµûËÆ©ÂÜÖÂÆπÊõ¥ÊúâÊ∏©Â∫¶&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Áªô‰∫àËµÑÈáëÁÇπËµûÊîØÊåÅ&lt;/strong&gt; ÁöÑÊúãÂèã‰ª¨Ôºå‰Ω†‰ª¨ÁöÑÊÖ∑ÊÖ®ËÆ©È°πÁõÆÂæó‰ª•ÊåÅÁª≠ÂèëÂ±ï&lt;/p&gt; 
 &lt;p&gt;‰∏ã‰∏ÄÊ¨°&lt;strong&gt;Êñ∞ÂäüËÉΩ&lt;/strong&gt;ÔºåÂ§ßÊ¶Ç‰ºöÊòØ ai ÂàÜÊûêÂäüËÉΩ(Â§ßÊ¶Ç(‚óè'‚ó°'‚óè)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;2025/09/17 - v2.2.0&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Êñ∞Â¢û‰∏ÄÈîÆ‰øùÂ≠òÊñ∞ÈóªÂõæÁâáÂäüËÉΩÔºåËÆ©‰Ω†ËΩªÊùæÂàÜ‰∫´ÂÖ≥Ê≥®ÁöÑÁÉ≠ÁÇπ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;‰ΩøÁî®ËØ¥Êòé&lt;/strong&gt;Ôºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ÈÄÇÁî®Âú∫ÊôØÔºöÂΩì‰Ω†ÊåâÁÖßÊïôÁ®ãÂºÄÂêØ‰∫ÜÁΩëÈ°µÁâàÂäüËÉΩÂêé(GitHub Pages)&lt;/li&gt; 
 &lt;li&gt;‰ΩøÁî®ÊñπÊ≥ïÔºöÁî®ÊâãÊú∫ÊàñÁîµËÑëÊâìÂºÄËØ•ÁΩëÈ°µÈìæÊé•ÔºåÁÇπÂáªÈ°µÈù¢È°∂ÈÉ®ÁöÑ"‰øùÂ≠ò‰∏∫ÂõæÁâá"ÊåâÈíÆ&lt;/li&gt; 
 &lt;li&gt;ÂÆûÈôÖÊïàÊûúÔºöÁ≥ªÁªü‰ºöËá™Âä®Â∞ÜÂΩìÂâçÁöÑÊñ∞ÈóªÊä•ÂëäÂà∂‰ΩúÊàê‰∏ÄÂº†Á≤æÁæéÂõæÁâáÔºå‰øùÂ≠òÂà∞‰Ω†ÁöÑÊâãÊú∫Áõ∏ÂÜåÊàñÁîµËÑëÊ°åÈù¢&lt;/li&gt; 
 &lt;li&gt;ÂàÜ‰∫´‰æøÂà©Ôºö‰Ω†ÂèØ‰ª•Áõ¥Êé•ÊääËøôÂº†ÂõæÁâáÂèëÁªôÊúãÂèã„ÄÅÂèëÂà∞ÊúãÂèãÂúàÔºåÊàñÂàÜ‰∫´Âà∞Â∑•‰ΩúÁæ§ÔºåËÆ©Âà´‰∫∫‰πüËÉΩÁúãÂà∞‰Ω†ÂèëÁé∞ÁöÑÈáçË¶ÅËµÑËÆØ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üëâ ÂéÜÂè≤Êõ¥Êñ∞&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h3&gt;2025/09/13 - v2.1.2&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Ëß£ÂÜ≥ÈíâÈíâÁöÑÊé®ÈÄÅÂÆπÈáèÈôêÂà∂ÂØºËá¥ÁöÑÊñ∞ÈóªÊé®ÈÄÅÂ§±Ë¥•ÈóÆÈ¢ò(ÈááÁî®ÂàÜÊâπÊé®ÈÄÅ)&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/09/04 - v2.1.1&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;‰øÆÂ§çdockerÂú®Êüê‰∫õÊû∂ÊûÑ‰∏≠Êó†Ê≥ïÊ≠£Â∏∏ËøêË°åÁöÑÈóÆÈ¢ò&lt;/li&gt; 
  &lt;li&gt;Ê≠£ÂºèÂèëÂ∏ÉÂÆòÊñπ Docker ÈïúÂÉè wantcat/trendradarÔºåÊîØÊåÅÂ§öÊû∂ÊûÑ&lt;/li&gt; 
  &lt;li&gt;‰ºòÂåñ Docker ÈÉ®ÁΩ≤ÊµÅÁ®ãÔºåÊó†ÈúÄÊú¨Âú∞ÊûÑÂª∫Âç≥ÂèØÂø´ÈÄü‰ΩøÁî®&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/08/30 - v2.1.0&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;Ê†∏ÂøÉÊîπËøõ&lt;/strong&gt;Ôºö&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Êé®ÈÄÅÈÄªËæë‰ºòÂåñ&lt;/strong&gt;Ôºö‰ªé"ÊØèÊ¨°ÊâßË°åÈÉΩÊé®ÈÄÅ"Êîπ‰∏∫"Êó∂Èó¥Á™óÂè£ÂÜÖÂèØÊéßÊé®ÈÄÅ"&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Êó∂Èó¥Á™óÂè£ÊéßÂà∂&lt;/strong&gt;ÔºöÂèØËÆæÂÆöÊé®ÈÄÅÊó∂Èó¥ËåÉÂõ¥ÔºåÈÅøÂÖçÈùûÂ∑•‰ΩúÊó∂Èó¥ÊâìÊâ∞&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Êé®ÈÄÅÈ¢ëÁéáÂèØÈÄâ&lt;/strong&gt;ÔºöÊó∂Èó¥ÊÆµÂÜÖÊîØÊåÅÂçïÊ¨°Êé®ÈÄÅÊàñÂ§öÊ¨°Êé®ÈÄÅ&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Êõ¥Êñ∞ÊèêÁ§∫&lt;/strong&gt;Ôºö&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Êú¨ÂäüËÉΩÈªòËÆ§ÂÖ≥Èó≠ÔºåÈúÄÊâãÂä®Âú® config.yaml ‰∏≠ÂºÄÂêØÈùôÈªòÊé®ÈÄÅÊ®°Âºè&lt;/li&gt; 
  &lt;li&gt;ÂçáÁ∫ßÈúÄÂêåÊó∂Êõ¥Êñ∞ main.py Âíå config.yaml ‰∏§‰∏™Êñá‰ª∂&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/08/27 - v2.0.4&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Êú¨Ê¨°ÁâàÊú¨‰∏çÊòØÂäüËÉΩ‰øÆÂ§çÔºåËÄåÊòØÈáçË¶ÅÊèêÈÜí&lt;/li&gt; 
  &lt;li&gt;ËØ∑Âä°ÂøÖÂ¶•ÂñÑ‰øùÁÆ°Â•Ω webhooksÔºå‰∏çË¶ÅÂÖ¨ÂºÄÔºå‰∏çË¶ÅÂÖ¨ÂºÄÔºå‰∏çË¶ÅÂÖ¨ÂºÄ&lt;/li&gt; 
  &lt;li&gt;Â¶ÇÊûú‰Ω†‰ª• fork ÁöÑÊñπÂºèÂ∞ÜÊú¨È°πÁõÆÈÉ®ÁΩ≤Âú® GitHub ‰∏äÔºåËØ∑Â∞Ü webhooks Â°´ÂÖ• GitHub SecretÔºåËÄåÈùû config.yaml&lt;/li&gt; 
  &lt;li&gt;Â¶ÇÊûú‰Ω†Â∑≤ÁªèÊö¥Èú≤‰∫Ü webhooks ÊàñÂ∞ÜÂÖ∂Â°´ÂÖ•‰∫Ü config.yamlÔºåÂª∫ËÆÆÂà†Èô§ÂêéÈáçÊñ∞ÁîüÊàê&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/08/06 - v2.0.3&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;‰ºòÂåñ github page ÁöÑÁΩëÈ°µÁâàÊïàÊûúÔºåÊñπ‰æøÁßªÂä®Á´Ø‰ΩøÁî®&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/07/28 - v2.0.2&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ÈáçÊûÑ‰ª£Á†Å&lt;/li&gt; 
  &lt;li&gt;Ëß£ÂÜ≥ÁâàÊú¨Âè∑ÂÆπÊòìË¢´ÈÅóÊºè‰øÆÊîπÁöÑÈóÆÈ¢ò&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/07/27 - v2.0.1&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;‰øÆÂ§çÈóÆÈ¢ò&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;docker ÁöÑ shell ËÑöÊú¨ÁöÑÊç¢Ë°åÁ¨¶‰∏∫ CRLF ÂØºËá¥ÁöÑÊâßË°åÂºÇÂ∏∏ÈóÆÈ¢ò&lt;/li&gt; 
  &lt;li&gt;frequency_words.txt ‰∏∫Á©∫Êó∂ÔºåÂØºËá¥Êñ∞ÈóªÂèëÈÄÅ‰πü‰∏∫Á©∫ÁöÑÈÄªËæëÈóÆÈ¢ò&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;ul&gt; 
  &lt;li&gt;‰øÆÂ§çÂêéÔºåÂΩì‰Ω†ÈÄâÊã© frequency_words.txt ‰∏∫Á©∫Êó∂ÔºåÂ∞Ü&lt;strong&gt;Êé®ÈÄÅÊâÄÊúâÊñ∞Èóª&lt;/strong&gt;Ôºå‰ΩÜÂèóÈôê‰∫éÊ∂àÊÅØÊé®ÈÄÅÂ§ßÂ∞èÈôêÂà∂ÔºåËØ∑ÂÅöÂ¶Ç‰∏ãË∞ÉÊï¥ 
   &lt;ul&gt; 
    &lt;li&gt;ÊñπÊ°à‰∏ÄÔºöÂÖ≥Èó≠ÊâãÊú∫Êé®ÈÄÅÔºåÂè™ÈÄâÊã© Github Pages Â∏ÉÁΩÆ(ËøôÊòØËÉΩËé∑ÂæóÊúÄÂÆåÊï¥‰ø°ÊÅØÁöÑÊñπÊ°àÔºåÂ∞ÜÊääÊâÄÊúâÂπ≥Âè∞ÁöÑÁÉ≠ÁÇπÊåâÁÖß‰Ω†&lt;strong&gt;Ëá™ÂÆö‰πâÁöÑÁÉ≠ÊêúÁÆóÊ≥ï&lt;/strong&gt;ËøõË°åÈáçÊñ∞ÊéíÂ∫è)&lt;/li&gt; 
    &lt;li&gt;ÊñπÊ°à‰∫åÔºöÂáèÂ∞ëÊé®ÈÄÅÂπ≥Âè∞Ôºå‰ºòÂÖàÈÄâÊã©&lt;strong&gt;‰ºÅ‰∏öÂæÆ‰ø°&lt;/strong&gt;Êàñ&lt;strong&gt;Telegram&lt;/strong&gt;ÔºåËøô‰∏§‰∏™Êé®ÈÄÅÊàëÂÅö‰∫ÜÂàÜÊâπÊé®ÈÄÅÂäüËÉΩ(Âõ†‰∏∫ÂàÜÊâπÊé®ÈÄÅÂΩ±ÂìçÊé®ÈÄÅ‰ΩìÈ™åÔºå‰∏îÂè™ÊúâËøô‰∏§‰∏™Âπ≥Âè∞Âè™Áªô‰∏ÄÁÇπÁÇπÊé®ÈÄÅÂÆπÈáèÔºåÊâÄ‰ª•Êâç‰∏çÂæóÂ∑≤ÂÅö‰∫ÜÂàÜÊâπÊé®ÈÄÅÂäüËÉΩÔºå‰ΩÜËá≥Â∞ëËÉΩ‰øùËØÅËé∑ÂæóÁöÑ‰ø°ÊÅØÂÆåÊï¥)&lt;/li&gt; 
    &lt;li&gt;ÊñπÊ°à‰∏âÔºöÂèØ‰∏éÊñπÊ°à‰∫åÁªìÂêàÔºåÊ®°ÂºèÈÄâÊã© current Êàñ incremental ÂèØÊúâÊïàÂáèÂ∞ë‰∏ÄÊ¨°ÊÄßÊé®ÈÄÅÁöÑÂÜÖÂÆπ&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/07/17 - v2.0.0&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;ÈáçÂ§ßÈáçÊûÑ&lt;/strong&gt;Ôºö&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ÈÖçÁΩÆÁÆ°ÁêÜÈáçÊûÑÔºöÊâÄÊúâÈÖçÁΩÆÁé∞Âú®ÈÄöËøá &lt;code&gt;config/config.yaml&lt;/code&gt; Êñá‰ª∂ÁÆ°ÁêÜÔºàmain.py Êàë‰æùÊóßÊ≤°ÊãÜÂàÜÔºåÊñπ‰æø‰Ω†‰ª¨Â§çÂà∂ÂçáÁ∫ßÔºâ&lt;/li&gt; 
  &lt;li&gt;ËøêË°åÊ®°ÂºèÂçáÁ∫ßÔºöÊîØÊåÅ‰∏âÁßçÊ®°Âºè - &lt;code&gt;daily&lt;/code&gt;ÔºàÂΩìÊó•Ê±áÊÄªÔºâ„ÄÅ&lt;code&gt;current&lt;/code&gt;ÔºàÂΩìÂâçÊ¶úÂçïÔºâ„ÄÅ&lt;code&gt;incremental&lt;/code&gt;ÔºàÂ¢ûÈáèÁõëÊéßÔºâ&lt;/li&gt; 
  &lt;li&gt;Docker ÊîØÊåÅÔºöÂÆåÊï¥ÁöÑ Docker ÈÉ®ÁΩ≤ÊñπÊ°àÔºåÊîØÊåÅÂÆπÂô®ÂåñËøêË°å&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;ÈÖçÁΩÆÊñá‰ª∂ËØ¥Êòé&lt;/strong&gt;Ôºö&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;config/config.yaml&lt;/code&gt; - ‰∏ªÈÖçÁΩÆÊñá‰ª∂ÔºàÂ∫îÁî®ËÆæÁΩÆ„ÄÅÁà¨Ëô´ÈÖçÁΩÆ„ÄÅÈÄöÁü•ÈÖçÁΩÆ„ÄÅÂπ≥Âè∞ÈÖçÁΩÆÁ≠âÔºâ&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;config/frequency_words.txt&lt;/code&gt; - ÂÖ≥ÈîÆËØçÈÖçÁΩÆÔºàÁõëÊéßËØçÊ±áËÆæÁΩÆÔºâ&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/07/09 - v1.4.1&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;ÂäüËÉΩÊñ∞Â¢û&lt;/strong&gt;ÔºöÂ¢ûÂä†Â¢ûÈáèÊé®ÈÄÅ(Âú® main.py Â§¥ÈÉ®ÈÖçÁΩÆ FOCUS_NEW_ONLY)ÔºåËØ•ÂºÄÂÖ≥Âè™ÂÖ≥ÂøÉÊñ∞ËØùÈ¢òËÄåÈùûÊåÅÁª≠ÁÉ≠Â∫¶ÔºåÂè™Âú®ÊúâÊñ∞ÂÜÖÂÆπÊó∂ÊâçÂèëÈÄöÁü•„ÄÇ&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;‰øÆÂ§çÈóÆÈ¢ò&lt;/strong&gt;: Êüê‰∫õÊÉÖÂÜµ‰∏ãÔºåÁî±‰∫éÊñ∞ÈóªÊú¨Ë∫´Âê´ÊúâÁâπÊÆäÁ¨¶Âè∑ÂØºËá¥ÁöÑÂÅ∂ÂèëÊÄßÊéíÁâàÂºÇÂ∏∏„ÄÇ&lt;/p&gt; 
 &lt;h3&gt;2025/06/23 - v1.3.0&lt;/h3&gt; 
 &lt;p&gt;‰ºÅ‰∏öÂæÆ‰ø° Âíå Telegram ÁöÑÊé®ÈÄÅÊ∂àÊÅØÊúâÈïøÂ∫¶ÈôêÂà∂ÔºåÂØπÊ≠§ÊàëÈááÁî®Â∞ÜÊ∂àÊÅØÊãÜÂàÜÊé®ÈÄÅÁöÑÊñπÂºè„ÄÇÂºÄÂèëÊñáÊ°£ËØ¶ËßÅ&lt;a href="https://developer.work.weixin.qq.com/document/path/91770"&gt;‰ºÅ‰∏öÂæÆ‰ø°&lt;/a&gt; Âíå &lt;a href="https://core.telegram.org/bots/api"&gt;Telegram&lt;/a&gt;&lt;/p&gt; 
 &lt;h3&gt;2025/06/21 - v1.2.1&lt;/h3&gt; 
 &lt;p&gt;Âú®Êú¨ÁâàÊú¨‰πãÂâçÁöÑÊóßÁâàÊú¨Ôºå‰∏ç‰ªÖ main.py ÈúÄË¶ÅÂ§çÂà∂ÊõøÊç¢Ôºå crawler.yml ‰πüÈúÄË¶Å‰Ω†Â§çÂà∂ÊõøÊç¢ &lt;a href="https://github.com/sansan0/TrendRadar/raw/master/.github/workflows/crawler.yml"&gt;https://github.com/sansan0/TrendRadar/blob/master/.github/workflows/crawler.yml&lt;/a&gt;&lt;/p&gt; 
 &lt;h3&gt;2025/06/19 - v1.2.0&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;ÊÑüË∞¢ claude research Êï¥ÁêÜÁöÑÂêÑÂπ≥Âè∞ api ,ËÆ©ÊàëÂø´ÈÄüÂÆåÊàêÂêÑÂπ≥Âè∞ÈÄÇÈÖçÔºàËôΩÁÑ∂‰ª£Á†ÅÊõ¥Â§öÂÜó‰Ωô‰∫Ü~&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ol&gt; 
  &lt;li&gt;ÊîØÊåÅ telegram Ôºå‰ºÅ‰∏öÂæÆ‰ø°ÔºåÈíâÈíâÊé®ÈÄÅÊ∏†ÈÅì, ÊîØÊåÅÂ§öÊ∏†ÈÅìÈÖçÁΩÆÂíåÂêåÊó∂Êé®ÈÄÅ&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;2025/06/18 - v1.1.0&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;200 star‚≠ê&lt;/strong&gt; ‰∫Ü, ÁªßÁª≠ÁªôÂ§ß‰ºôÂÑøÂä©ÂÖ¥~ËøëÊúüÔºåÂú®ÊàëÁöÑ"ÊÄÇÊÅø"‰∏ãÔºåÊå∫Â§ö‰∫∫Âú®ÊàëÂÖ¨‰ºóÂè∑ÁÇπËµûÂàÜ‰∫´Êé®ËçêÂä©Âäõ‰∫ÜÊàëÔºåÊàëÈÉΩÂú®ÂêéÂè∞ÁúãËßÅ‰∫ÜÂÖ∑‰ΩìË¥¶Âè∑ÁöÑÈºìÂä±Êï∞ÊçÆÔºåÂæàÂ§öÈÉΩÊàê‰∫ÜÂ§©‰ΩøËΩÆËÄÅÁ≤âÔºàÊàëÁé©ÂÖ¨‰ºóÂè∑Êâç‰∏Ä‰∏™Â§öÊúàÔºåËôΩÁÑ∂Ê≥®ÂÜåÊòØ‰∏ÉÂÖ´Âπ¥ÂâçÁöÑ‰∫ã‰∫ÜÂìàÂìàÔºåÂ±û‰∫é‰∏äËΩ¶Êó©ÔºåÂèëËΩ¶ÊôöÔºâÔºå‰ΩÜÂõ†‰∏∫‰Ω†‰ª¨Ê≤°ÊúâÁïôË®ÄÊàñÁßÅ‰ø°ÊàëÔºåÊâÄ‰ª•Êàë‰πüÊó†Ê≥ï‰∏Ä‰∏ÄÂõûÂ∫îÂπ∂ÊÑüË∞¢ÊîØÊåÅÔºåÂú®Ê≠§‰∏ÄÂπ∂Ë∞¢Ë∞¢ÔºÅ&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ol&gt; 
  &lt;li&gt;ÈáçË¶ÅÁöÑÊõ¥Êñ∞ÔºåÂä†‰∫ÜÊùÉÈáçÔºå‰Ω†Áé∞Âú®ÁúãÂà∞ÁöÑÊñ∞ÈóªÈÉΩÊòØÊúÄÁÉ≠ÁÇπÊúÄÊúâÂÖ≥Ê≥®Â∫¶ÁöÑÂá∫Áé∞Âú®ÊúÄ‰∏äÈù¢&lt;/li&gt; 
  &lt;li&gt;Êõ¥Êñ∞ÊñáÊ°£‰ΩøÁî®ÔºåÂõ†‰∏∫ËøëÊúüÊõ¥Êñ∞‰∫ÜÂæàÂ§öÂäüËÉΩÔºåËÄå‰∏î‰πãÂâçÁöÑ‰ΩøÁî®ÊñáÊ°£ÊàëÂÅ∑ÊáíÂÜôÁöÑÁÆÄÂçïÔºàËßÅ‰∏ãÈù¢ÁöÑ ‚öôÔ∏è frequency_words.txt ÈÖçÁΩÆÂÆåÊï¥ÊïôÁ®ãÔºâ&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;2025/06/16 - v1.0.0&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Â¢ûÂä†‰∫Ü‰∏Ä‰∏™È°πÁõÆÊñ∞ÁâàÊú¨Êõ¥Êñ∞ÊèêÁ§∫ÔºåÈªòËÆ§ÊâìÂºÄÔºåÂ¶ÇË¶ÅÂÖ≥ÊéâÔºåÂèØ‰ª•Âú® main.py ‰∏≠Êää "FEISHU_SHOW_VERSION_UPDATE": True ‰∏≠ÁöÑ True ÊîπÊàê False Âç≥ÂèØ&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;2025/06/13+14&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;ÂéªÊéâ‰∫ÜÂÖºÂÆπ‰ª£Á†ÅÔºå‰πãÂâç fork ÁöÑÂêåÂ≠¶ÔºåÁõ¥Êé•Â§çÂà∂‰ª£Á†Å‰ºöÂú®ÂΩìÂ§©ÊòæÁ§∫ÂºÇÂ∏∏ÔºàÁ¨¨‰∫åÂ§©‰ºöÊÅ¢Â§çÊ≠£Â∏∏Ôºâ&lt;/li&gt; 
  &lt;li&gt;feishu Âíå html Â∫ïÈÉ®Â¢ûÂä†‰∏Ä‰∏™Êñ∞Â¢ûÊñ∞ÈóªÊòæÁ§∫&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;2025/06/09&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;100 star‚≠ê&lt;/strong&gt; ‰∫ÜÔºåÂÜô‰∏™Â∞èÂäüËÉΩÁªôÂ§ß‰ºôÂÑøÂä©Âä©ÂÖ¥ frequency_words.txt Êñá‰ª∂Â¢ûÂä†‰∫Ü‰∏Ä‰∏™„ÄêÂøÖÈ°ªËØç„ÄëÂäüËÉΩÔºå‰ΩøÁî® + Âè∑&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;ÂøÖÈ°ªËØçËØ≠Ê≥ïÂ¶Ç‰∏ãÔºö&lt;br /&gt; ÂîêÂÉßÊàñËÄÖÁå™ÂÖ´ÊàíÂøÖÈ°ªÂú®Ê†áÈ¢òÈáåÂêåÊó∂Âá∫Áé∞ÔºåÊâç‰ºöÊî∂ÂΩïÂà∞Êé®ÈÄÅÊñ∞Èóª‰∏≠&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code&gt;+ÂîêÂÉß
+Áå™ÂÖ´Êàí
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;ËøáÊª§ËØçÁöÑ‰ºòÂÖàÁ∫ßÊõ¥È´òÔºö&lt;br /&gt; Â¶ÇÊûúÊ†áÈ¢ò‰∏≠ËøáÊª§ËØçÂåπÈÖçÂà∞ÂîêÂÉßÂøµÁªèÔºåÈÇ£‰πàÂç≥‰ΩøÂøÖÈ°ªËØçÈáåÊúâÂîêÂÉßÔºå‰πü‰∏çÊòæÁ§∫&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code&gt;+ÂîêÂÉß
!ÂîêÂÉßÂøµÁªè
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;2025/06/02&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;ÁΩëÈ°µ&lt;/strong&gt;Âíå&lt;strong&gt;È£û‰π¶Ê∂àÊÅØ&lt;/strong&gt;ÊîØÊåÅÊâãÊú∫Áõ¥Êé•Ë∑≥ËΩ¨ËØ¶ÊÉÖÊñ∞Èóª&lt;/li&gt; 
  &lt;li&gt;‰ºòÂåñÊòæÁ§∫ÊïàÊûú + 1&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;2025/05/26&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;È£û‰π¶Ê∂àÊÅØÊòæÁ§∫ÊïàÊûú‰ºòÂåñ&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center"&gt; ‰ºòÂåñÂâç&lt;br /&gt; &lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/before.jpg" alt="È£û‰π¶Ê∂àÊÅØÁïåÈù¢ - ‰ºòÂåñÂâç" width="400" /&gt; &lt;/td&gt; 
    &lt;td align="center"&gt; ‰ºòÂåñÂêé&lt;br /&gt; &lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/after.jpg" alt="È£û‰π¶Ê∂àÊÅØÁïåÈù¢ - ‰ºòÂåñÂêé" width="400" /&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h2&gt;üöÄ ‰ΩøÁî®ÊñπÂºè&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Fork Êú¨È°πÁõÆ&lt;/strong&gt;Âà∞‰Ω†ÁöÑ GitHub Ë¥¶Êà∑&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ÁÇπÂáªÊú¨È°µÈù¢Âè≥‰∏äËßíÁöÑ"Fork"ÊåâÈíÆ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ËÆæÁΩÆ GitHub SecretsÔºàÈÄâÊã©‰Ω†ÈúÄË¶ÅÁöÑÂπ≥Âè∞Ôºâ&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;Âú®‰Ω† Fork ÂêéÁöÑ‰ªìÂ∫ì‰∏≠ÔºåËøõÂÖ• &lt;code&gt;Settings&lt;/code&gt; &amp;gt; &lt;code&gt;Secrets and variables&lt;/code&gt; &amp;gt; &lt;code&gt;Actions&lt;/code&gt; &amp;gt; &lt;code&gt;New repository secret&lt;/code&gt;ÔºåÁÑ∂ÂêéÊ†πÊçÆÈúÄË¶ÅÈÖçÁΩÆ‰ª•‰∏ã‰ªª‰∏ÄÊàñÂ§ö‰∏™ÈÄöÁü•Âπ≥Âè∞Ôºö&lt;/p&gt; &lt;p&gt;ÂèØ‰ª•ÂêåÊó∂ÈÖçÁΩÆÂ§ö‰∏™Âπ≥Âè∞ÔºåÁ≥ªÁªü‰ºöÂêëÊâÄÊúâÈÖçÁΩÆÁöÑÂπ≥Âè∞ÂèëÈÄÅÈÄöÁü•„ÄÇ&lt;/p&gt; 
  &lt;details&gt; 
   &lt;summary&gt; &lt;strong&gt;üëâ ‰ºÅ‰∏öÂæÆ‰ø°Êú∫Âô®‰∫∫&lt;/strong&gt;ÔºàÈÖçÁΩÆÊúÄÁÆÄÂçïÊúÄËøÖÈÄüÔºâ&lt;/summary&gt; 
   &lt;br /&gt; 
   &lt;p&gt;&lt;strong&gt;GitHub Secret ÈÖçÁΩÆÔºö&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;ÂêçÁß∞Ôºö&lt;code&gt;WEWORK_WEBHOOK_URL&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;ÂÄºÔºö‰Ω†ÁöÑ‰ºÅ‰∏öÂæÆ‰ø°Êú∫Âô®‰∫∫ Webhook Âú∞ÂùÄ&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;Êú∫Âô®‰∫∫ËÆæÁΩÆÊ≠•È™§Ôºö&lt;/strong&gt;&lt;/p&gt; 
   &lt;h4&gt;ÊâãÊú∫Á´ØËÆæÁΩÆÔºö&lt;/h4&gt; 
   &lt;ol&gt; 
    &lt;li&gt;ÊâìÂºÄ‰ºÅ‰∏öÂæÆ‰ø° App ‚Üí ËøõÂÖ•ÁõÆÊ†áÂÜÖÈÉ®Áæ§ËÅä&lt;/li&gt; 
    &lt;li&gt;ÁÇπÂáªÂè≥‰∏äËßí"‚Ä¶"ÊåâÈíÆ ‚Üí ÈÄâÊã©"Áæ§Êú∫Âô®‰∫∫"&lt;/li&gt; 
    &lt;li&gt;ÁÇπÂáª"Ê∑ªÂä†" ‚Üí ÁÇπÂáª"Êñ∞Âª∫" ‚Üí ËÆæÁΩÆÊú∫Âô®‰∫∫ÊòµÁß∞&lt;/li&gt; 
    &lt;li&gt;Â§çÂà∂ Webhook Âú∞ÂùÄÔºåÈÖçÁΩÆÂà∞‰∏äÊñπÁöÑ GitHub Secret ‰∏≠&lt;/li&gt; 
   &lt;/ol&gt; 
   &lt;h4&gt;PC Á´ØËÆæÁΩÆÊµÅÁ®ãÁ±ª‰ºº&lt;/h4&gt; 
  &lt;/details&gt; 
  &lt;details&gt; 
   &lt;summary&gt; &lt;strong&gt;üëâ È£û‰π¶Êú∫Âô®‰∫∫&lt;/strong&gt;ÔºàÊ∂àÊÅØÊòæÁ§∫ÊúÄÂèãÂ•ΩÔºâ&lt;/summary&gt; 
   &lt;br /&gt; 
   &lt;p&gt;&lt;strong&gt;GitHub Secret ÈÖçÁΩÆÔºö&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;ÂêçÁß∞Ôºö&lt;code&gt;FEISHU_WEBHOOK_URL&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;ÂÄºÔºö‰Ω†ÁöÑÈ£û‰π¶Êú∫Âô®‰∫∫ Webhook Âú∞ÂùÄ&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;Êú∫Âô®‰∫∫ËÆæÁΩÆÊ≠•È™§Ôºö&lt;/strong&gt;&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt; &lt;p&gt;ÁîµËÑëÊµèËßàÂô®ÊâìÂºÄ &lt;a href="https://botbuilder.feishu.cn/home/my-app"&gt;https://botbuilder.feishu.cn/home/my-app&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;ÁÇπÂáª"Êñ∞Âª∫Êú∫Âô®‰∫∫Â∫îÁî®"&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;ËøõÂÖ•ÂàõÂª∫ÁöÑÂ∫îÁî®ÂêéÔºåÁÇπÂáª"ÊµÅÁ®ãÊ∂âÂèä" &amp;gt; "ÂàõÂª∫ÊµÅÁ®ã" &amp;gt; "ÈÄâÊã©Ëß¶ÂèëÂô®"&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;ÂæÄ‰∏ãÊªëÂä®ÔºåÁÇπÂáª"Webhook Ëß¶Âèë"&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;Ê≠§Êó∂‰Ω†‰ºöÁúãÂà∞"Webhook Âú∞ÂùÄ"ÔºåÊääËøô‰∏™ÈìæÊé•ÂÖàÂ§çÂà∂Âà∞Êú¨Âú∞ËÆ∞‰∫ãÊú¨ÊöÇÂ≠òÔºåÁªßÁª≠Êé•‰∏ãÊù•ÁöÑÊìç‰Ωú&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;"ÂèÇÊï∞"ÈáåÈù¢Êîæ‰∏ä‰∏ãÈù¢ÁöÑÂÜÖÂÆπÔºåÁÑ∂ÂêéÁÇπÂáª"ÂÆåÊàê"&lt;/p&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
   &lt;pre&gt;&lt;code class="language-json"&gt;{
  "message_type": "text",
  "content": {
    "total_titles": "{{ÂÜÖÂÆπ}}",
    "timestamp": "{{ÂÜÖÂÆπ}}",
    "report_type": "{{ÂÜÖÂÆπ}}",
    "text": "{{ÂÜÖÂÆπ}}"
  }
}
&lt;/code&gt;&lt;/pre&gt; 
   &lt;ol start="7"&gt; 
    &lt;li&gt; &lt;p&gt;ÁÇπÂáª"ÈÄâÊã©Êìç‰Ωú" &amp;gt; "ÂèëÈÄÅÈ£û‰π¶Ê∂àÊÅØ"ÔºåÂãæÈÄâ "Áæ§Ê∂àÊÅØ"ÔºåÁÑ∂ÂêéÁÇπÂáª‰∏ãÈù¢ÁöÑËæìÂÖ•Ê°ÜÔºåÁÇπÂáª"ÊàëÁÆ°ÁêÜÁöÑÁæ§ÁªÑ"ÔºàÂ¶ÇÊûúÊ≤°ÊúâÁæ§ÁªÑÔºå‰Ω†ÂèØ‰ª•Âú®È£û‰π¶ app ‰∏äÂàõÂª∫Áæ§ÁªÑÔºâ&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;Ê∂àÊÅØÊ†áÈ¢òÂ°´ÂÜô"TrendRadar ÁÉ≠ÁÇπÁõëÊéß"&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;ÊúÄÂÖ≥ÈîÆÁöÑÈÉ®ÂàÜÊù•‰∫ÜÔºåÁÇπÂáª + ÊåâÈíÆÔºåÈÄâÊã©"Webhook Ëß¶Âèë"ÔºåÁÑ∂ÂêéÊåâÁÖß‰∏ãÈù¢ÁöÑÂõæÁâáÊëÜÊîæ&lt;/p&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
   &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/image.png" alt="È£û‰π¶Êú∫Âô®‰∫∫ÈÖçÁΩÆÁ§∫‰æã" /&gt;&lt;/p&gt; 
   &lt;ol start="10"&gt; 
    &lt;li&gt;ÈÖçÁΩÆÂÆåÊàêÂêéÔºåÂ∞ÜÁ¨¨ 5 Ê≠•Â§çÂà∂ÁöÑ Webhook Âú∞ÂùÄÈÖçÁΩÆÂà∞ GitHub Secrets ‰∏≠ÁöÑ &lt;code&gt;FEISHU_WEBHOOK_URL&lt;/code&gt;&lt;/li&gt; 
   &lt;/ol&gt; 
  &lt;/details&gt; 
  &lt;details&gt; 
   &lt;summary&gt; &lt;strong&gt;üëâ ÈíâÈíâÊú∫Âô®‰∫∫&lt;/strong&gt;&lt;/summary&gt; 
   &lt;br /&gt; 
   &lt;p&gt;&lt;strong&gt;GitHub Secret ÈÖçÁΩÆÔºö&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;ÂêçÁß∞Ôºö&lt;code&gt;DINGTALK_WEBHOOK_URL&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;ÂÄºÔºö‰Ω†ÁöÑÈíâÈíâÊú∫Âô®‰∫∫ Webhook Âú∞ÂùÄ&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;Êú∫Âô®‰∫∫ËÆæÁΩÆÊ≠•È™§Ôºö&lt;/strong&gt;&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂàõÂª∫Êú∫Âô®‰∫∫Ôºà‰ªÖ PC Á´ØÊîØÊåÅÔºâ&lt;/strong&gt;Ôºö&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;ÊâìÂºÄÈíâÈíâ PC ÂÆ¢Êà∑Á´ØÔºåËøõÂÖ•ÁõÆÊ†áÁæ§ËÅä&lt;/li&gt; 
      &lt;li&gt;ÁÇπÂáªÁæ§ËÆæÁΩÆÂõæÊ†áÔºà‚öôÔ∏èÔºâ‚Üí ÂæÄ‰∏ãÁøªÊâæÂà∞"Êú∫Âô®‰∫∫"ÁÇπÂºÄ&lt;/li&gt; 
      &lt;li&gt;ÈÄâÊã©"Ê∑ªÂä†Êú∫Âô®‰∫∫" ‚Üí "Ëá™ÂÆö‰πâ"&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÈÖçÁΩÆÊú∫Âô®‰∫∫&lt;/strong&gt;Ôºö&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;ËÆæÁΩÆÊú∫Âô®‰∫∫ÂêçÁß∞&lt;/li&gt; 
      &lt;li&gt;&lt;strong&gt;ÂÆâÂÖ®ËÆæÁΩÆ&lt;/strong&gt;Ôºö 
       &lt;ul&gt; 
        &lt;li&gt;&lt;strong&gt;Ëá™ÂÆö‰πâÂÖ≥ÈîÆËØç&lt;/strong&gt;ÔºöËÆæÁΩÆ "ÁÉ≠ÁÇπ"&lt;/li&gt; 
       &lt;/ul&gt; &lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂÆåÊàêËÆæÁΩÆ&lt;/strong&gt;Ôºö&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;ÂãæÈÄâÊúçÂä°Êù°Ê¨æÂçèËÆÆ ‚Üí ÁÇπÂáª"ÂÆåÊàê"&lt;/li&gt; 
      &lt;li&gt;Â§çÂà∂Ëé∑ÂæóÁöÑ Webhook URL&lt;/li&gt; 
      &lt;li&gt;Â∞Ü URL ÈÖçÁΩÆÂà∞ GitHub Secrets ‰∏≠ÁöÑ &lt;code&gt;DINGTALK_WEBHOOK_URL&lt;/code&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
   &lt;p&gt;&lt;strong&gt;Ê≥®ÊÑè&lt;/strong&gt;ÔºöÁßªÂä®Á´ØÂè™ËÉΩÊé•Êî∂Ê∂àÊÅØÔºåÊó†Ê≥ïÂàõÂª∫Êñ∞Êú∫Âô®‰∫∫„ÄÇ&lt;/p&gt; 
  &lt;/details&gt; 
  &lt;details&gt; 
   &lt;summary&gt; &lt;strong&gt;üëâ Telegram Bot&lt;/strong&gt;&lt;/summary&gt; 
   &lt;br /&gt; 
   &lt;p&gt;&lt;strong&gt;GitHub Secret ÈÖçÁΩÆÔºö&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;ÂêçÁß∞Ôºö&lt;code&gt;TELEGRAM_BOT_TOKEN&lt;/code&gt; - ‰Ω†ÁöÑ Telegram Bot Token&lt;/li&gt; 
    &lt;li&gt;ÂêçÁß∞Ôºö&lt;code&gt;TELEGRAM_CHAT_ID&lt;/code&gt; - ‰Ω†ÁöÑ Telegram Chat ID&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;Êú∫Âô®‰∫∫ËÆæÁΩÆÊ≠•È™§Ôºö&lt;/strong&gt;&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂàõÂª∫Êú∫Âô®‰∫∫&lt;/strong&gt;Ôºö&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;Âú® Telegram ‰∏≠ÊêúÁ¥¢ &lt;code&gt;@BotFather&lt;/code&gt;ÔºàÂ§ßÂ∞èÂÜôÊ≥®ÊÑèÔºåÊúâËìùËâ≤ÂæΩÁ´†ÂãæÂãæÔºåÊúâÁ±ª‰ºº 37849827 monthly usersÔºåËøô‰∏™ÊâçÊòØÂÆòÊñπÁöÑÔºåÊúâ‰∏Ä‰∫õ‰ªøÂÆòÊñπÁöÑË¥¶Âè∑Ê≥®ÊÑèËæ®Âà´Ôºâ&lt;/li&gt; 
      &lt;li&gt;ÂèëÈÄÅ &lt;code&gt;/newbot&lt;/code&gt; ÂëΩ‰ª§ÂàõÂª∫Êñ∞Êú∫Âô®‰∫∫&lt;/li&gt; 
      &lt;li&gt;ËÆæÁΩÆÊú∫Âô®‰∫∫ÂêçÁß∞ÔºàÂøÖÈ°ª‰ª•"bot"ÁªìÂ∞æÔºåÂæàÂÆπÊòìÈÅáÂà∞ÈáçÂ§çÂêçÂ≠óÔºåÊâÄ‰ª•‰Ω†Ë¶ÅÁªûÂ∞ΩËÑëÊ±ÅÊÉ≥‰∏çÂêåÁöÑÂêçÂ≠óÔºâ&lt;/li&gt; 
      &lt;li&gt;Ëé∑Âèñ Bot TokenÔºàÊ†ºÂºèÂ¶ÇÔºö&lt;code&gt;123456789:AAHfiqksKZ8WmR2zSjiQ7_v4TMAKdiHm9T0&lt;/code&gt;Ôºâ&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Ëé∑Âèñ Chat ID&lt;/strong&gt;Ôºö&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ÊñπÊ≥ï‰∏ÄÔºöÈÄöËøáÂÆòÊñπ API Ëé∑Âèñ&lt;/strong&gt;&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;ÂÖàÂêë‰Ω†ÁöÑÊú∫Âô®‰∫∫ÂèëÈÄÅ‰∏ÄÊù°Ê∂àÊÅØ&lt;/li&gt; 
      &lt;li&gt;ËÆøÈóÆÔºö&lt;code&gt;https://api.telegram.org/bot&amp;lt;‰Ω†ÁöÑBot Token&amp;gt;/getUpdates&lt;/code&gt;&lt;/li&gt; 
      &lt;li&gt;Âú®ËøîÂõûÁöÑ JSON ‰∏≠ÊâæÂà∞ &lt;code&gt;"chat":{"id":Êï∞Â≠ó}&lt;/code&gt; ‰∏≠ÁöÑÊï∞Â≠ó&lt;/li&gt; 
     &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;ÊñπÊ≥ï‰∫åÔºö‰ΩøÁî®Á¨¨‰∏âÊñπÂ∑•ÂÖ∑&lt;/strong&gt;&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;ÊêúÁ¥¢ &lt;code&gt;@userinfobot&lt;/code&gt; Âπ∂ÂèëÈÄÅ &lt;code&gt;/start&lt;/code&gt;&lt;/li&gt; 
      &lt;li&gt;Ëé∑Âèñ‰Ω†ÁöÑÁî®Êà∑ ID ‰Ωú‰∏∫ Chat ID&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÈÖçÁΩÆÂà∞ GitHub&lt;/strong&gt;Ôºö&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;code&gt;TELEGRAM_BOT_TOKEN&lt;/code&gt;ÔºöÂ°´ÂÖ•Á¨¨ 1 Ê≠•Ëé∑ÂæóÁöÑ Bot Token&lt;/li&gt; 
      &lt;li&gt;&lt;code&gt;TELEGRAM_CHAT_ID&lt;/code&gt;ÔºöÂ°´ÂÖ•Á¨¨ 2 Ê≠•Ëé∑ÂæóÁöÑ Chat ID&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;‰∏ªË¶ÅÈÖçÁΩÆ&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Êé®ÈÄÅËÆæÁΩÆÔºö&lt;/strong&gt; : Âú® &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/config.yaml"&gt;config/config.yaml&lt;/a&gt; ‰∏≠ËøõË°åÔºåÂèØÊ†πÊçÆÈáåÈù¢ÁöÑÊèèËø∞ÊñáÂ≠óÊìç‰ΩúÔºåËøôÈáå‰∏çÈáçÂ§ç‰∫Ü&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;ÊØîÂ¶Ç: Âú® &lt;code&gt;config/config.yaml&lt;/code&gt; ‰∏≠‰øÆÊîπ &lt;code&gt;report.mode&lt;/code&gt; ËÆæÁΩÆÔºö&lt;/p&gt; 
    &lt;table&gt; 
     &lt;thead&gt; 
      &lt;tr&gt; 
       &lt;th&gt;Ê®°Âºè&lt;/th&gt; 
       &lt;th&gt;Êé®ÈÄÅÊó∂Êú∫&lt;/th&gt; 
       &lt;th&gt;ÊòæÁ§∫ÂÜÖÂÆπ&lt;/th&gt; 
       &lt;th&gt;ÈÄÇÁî®Âú∫ÊôØ&lt;/th&gt; 
      &lt;/tr&gt; 
     &lt;/thead&gt; 
     &lt;tbody&gt; 
      &lt;tr&gt; 
       &lt;td&gt;&lt;strong&gt;ÂΩìÊó•Ê±áÊÄªÊ®°Âºè&lt;/strong&gt;&lt;br /&gt;&lt;code&gt;daily&lt;/code&gt;&lt;/td&gt; 
       &lt;td&gt;ÊåâÊó∂Êé®ÈÄÅ&lt;/td&gt; 
       &lt;td&gt;ÂΩìÊó•ÊâÄÊúâÂåπÈÖçÊñ∞Èóª&lt;br /&gt;+ Êñ∞Â¢ûÊñ∞ÈóªÂå∫Âüü&lt;/td&gt; 
       &lt;td&gt;Êó•Êä•ÊÄªÁªì&lt;br /&gt;ÂÖ®Èù¢‰∫ÜËß£ÂΩìÊó•ÁÉ≠ÁÇπË∂ãÂäø&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td&gt;&lt;strong&gt;ÂΩìÂâçÊ¶úÂçïÊ®°Âºè&lt;/strong&gt;&lt;br /&gt;&lt;code&gt;current&lt;/code&gt;&lt;/td&gt; 
       &lt;td&gt;ÊåâÊó∂Êé®ÈÄÅ&lt;/td&gt; 
       &lt;td&gt;ÂΩìÂâçÊ¶úÂçïÂåπÈÖçÊñ∞Èóª&lt;br /&gt;+ Êñ∞Â¢ûÊñ∞ÈóªÂå∫Âüü&lt;/td&gt; 
       &lt;td&gt;ÂÆûÊó∂ÁÉ≠ÁÇπËøΩË∏™&lt;br /&gt;‰∫ÜËß£ÂΩìÂâçÊúÄÁÅ´ÁöÑÂÜÖÂÆπ&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td&gt;&lt;strong&gt;Â¢ûÈáèÁõëÊéßÊ®°Âºè&lt;/strong&gt;&lt;br /&gt;&lt;code&gt;incremental&lt;/code&gt;&lt;/td&gt; 
       &lt;td&gt;ÊúâÊñ∞Â¢ûÊâçÊé®ÈÄÅ&lt;/td&gt; 
       &lt;td&gt;Êñ∞Âá∫Áé∞ÁöÑÂåπÈÖçÈ¢ëÁéáËØçÊñ∞Èóª&lt;/td&gt; 
       &lt;td&gt;ÈÅøÂÖçÈáçÂ§ç‰ø°ÊÅØÂπ≤Êâ∞&lt;br /&gt;È´òÈ¢ëÁõëÊéßÂú∫ÊôØ&lt;/td&gt; 
      &lt;/tr&gt; 
     &lt;/tbody&gt; 
    &lt;/table&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂÖ≥ÈîÆËØçÈÖçÁΩÆ&lt;/strong&gt;: ‰øÆÊîπ &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/frequency_words.txt"&gt;config/frequency_words.txt&lt;/a&gt; Êñá‰ª∂ÔºåÊ∑ªÂä†‰Ω†ÂÖ≥ÂøÉÁöÑÂÖ≥ÈîÆËØç&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;details&gt; 
   &lt;summary&gt;&lt;strong&gt;üëâ frequency_words.txt ÈÖçÁΩÆÊïôÁ®ã&lt;/strong&gt;&lt;/summary&gt; 
   &lt;br /&gt; 
   &lt;p&gt;Âú® &lt;code&gt;frequency_words.txt&lt;/code&gt; Êñá‰ª∂‰∏≠ÈÖçÁΩÆÁõëÊéßÁöÑÂÖ≥ÈîÆËØçÔºåÊîØÊåÅ‰∏âÁßçËØ≠Ê≥ïÂíåËØçÁªÑÂäüËÉΩ„ÄÇ&lt;/p&gt; 
   &lt;p&gt;ÂÖ≥ÈîÆËØçË∂äÈù†ÂâçÔºåÊñ∞ÈóªÁöÑ‰ºòÂÖàÁ∫ßË∂äÈ´òÔºå‰Ω†ÂèØ‰ª•Ê†πÊçÆËá™Â∑±ÁöÑÂÖ≥Ê≥®Â∫¶Ë∞ÉÊï¥ÂÖ≥ÈîÆËØçÈ°∫Â∫è&lt;/p&gt; 
   &lt;h3&gt;üìã Âü∫Á°ÄËØ≠Ê≥ïËØ¥Êòé&lt;/h3&gt; 
   &lt;h4&gt;1. &lt;strong&gt;ÊôÆÈÄöÂÖ≥ÈîÆËØç&lt;/strong&gt; - Âü∫Á°ÄÂåπÈÖç&lt;/h4&gt; 
   &lt;pre&gt;&lt;code class="language-txt"&gt;Âçé‰∏∫
OPPO
ËãπÊûú
&lt;/code&gt;&lt;/pre&gt; 
   &lt;p&gt;&lt;strong&gt;‰ΩúÁî®Ôºö&lt;/strong&gt; Êñ∞ÈóªÊ†áÈ¢òÂåÖÂê´ÂÖ∂‰∏≠&lt;strong&gt;‰ªªÊÑè‰∏Ä‰∏™ËØç&lt;/strong&gt;Â∞±‰ºöË¢´ÊçïËé∑&lt;/p&gt; 
   &lt;h4&gt;2. &lt;strong&gt;ÂøÖÈ°ªËØç&lt;/strong&gt; &lt;code&gt;+ËØçÊ±á&lt;/code&gt; - ÈôêÂÆöËåÉÂõ¥&lt;/h4&gt; 
   &lt;pre&gt;&lt;code class="language-txt"&gt;Âçé‰∏∫
OPPO
+ÊâãÊú∫
&lt;/code&gt;&lt;/pre&gt; 
   &lt;p&gt;&lt;strong&gt;‰ΩúÁî®Ôºö&lt;/strong&gt; ÂøÖÈ°ªÂêåÊó∂ÂåÖÂê´ÊôÆÈÄöËØç&lt;strong&gt;Âíå&lt;/strong&gt;ÂøÖÈ°ªËØçÊâç‰ºöË¢´ÊçïËé∑&lt;/p&gt; 
   &lt;h4&gt;3. &lt;strong&gt;ËøáÊª§ËØç&lt;/strong&gt; &lt;code&gt;!ËØçÊ±á&lt;/code&gt; - ÊéíÈô§Âπ≤Êâ∞&lt;/h4&gt; 
   &lt;pre&gt;&lt;code class="language-txt"&gt;ËãπÊûú
Âçé‰∏∫
!Ê∞¥Êûú
!‰ª∑Ê†º
&lt;/code&gt;&lt;/pre&gt; 
   &lt;p&gt;&lt;strong&gt;‰ΩúÁî®Ôºö&lt;/strong&gt; ÂåÖÂê´ËøáÊª§ËØçÁöÑÊñ∞Èóª‰ºöË¢´&lt;strong&gt;Áõ¥Êé•ÊéíÈô§&lt;/strong&gt;ÔºåÂç≥‰ΩøÂåÖÂê´ÂÖ≥ÈîÆËØç&lt;/p&gt; 
   &lt;h3&gt;üîó ËØçÁªÑÂäüËÉΩ - Á©∫Ë°åÂàÜÈöîÁöÑÈáçË¶Å‰ΩúÁî®&lt;/h3&gt; 
   &lt;p&gt;&lt;strong&gt;Ê†∏ÂøÉËßÑÂàôÔºö&lt;/strong&gt; Áî®&lt;strong&gt;Á©∫Ë°å&lt;/strong&gt;ÂàÜÈöî‰∏çÂêåÁöÑËØçÁªÑÔºåÊØè‰∏™ËØçÁªÑÁã¨Á´ãÁªüËÆ°&lt;/p&gt; 
   &lt;h4&gt;Á§∫‰æãÈÖçÁΩÆÔºö&lt;/h4&gt; 
   &lt;pre&gt;&lt;code class="language-txt"&gt;iPhone
Âçé‰∏∫
OPPO
+ÂèëÂ∏É

AËÇ°
‰∏äËØÅ
Ê∑±ËØÅ
+Ê∂®Ë∑å
!È¢ÑÊµã

‰∏ñÁïåÊùØ
Ê¨ßÊ¥≤ÊùØ
‰∫öÊ¥≤ÊùØ
+ÊØîËµõ
&lt;/code&gt;&lt;/pre&gt; 
   &lt;h4&gt;ËØçÁªÑËß£ÈáäÂèäÂåπÈÖçÊïàÊûúÔºö&lt;/h4&gt; 
   &lt;p&gt;&lt;strong&gt;Á¨¨1ÁªÑ - ÊâãÊú∫Êñ∞ÂìÅÁ±ªÔºö&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;ÂÖ≥ÈîÆËØçÔºöiPhone„ÄÅÂçé‰∏∫„ÄÅOPPO&lt;/li&gt; 
    &lt;li&gt;ÂøÖÈ°ªËØçÔºöÂèëÂ∏É&lt;/li&gt; 
    &lt;li&gt;ÊïàÊûúÔºöÂøÖÈ°ªÂåÖÂê´ÊâãÊú∫ÂìÅÁâåÂêçÔºåÂêåÊó∂ÂåÖÂê´"ÂèëÂ∏É"&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;ÂåπÈÖçÁ§∫‰æãÔºö&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;‚úÖ "iPhone 15Ê≠£ÂºèÂèëÂ∏ÉÂîÆ‰ª∑ÂÖ¨Â∏É" ‚Üê Êúâ"iPhone"+"ÂèëÂ∏É"&lt;/li&gt; 
    &lt;li&gt;‚úÖ "Âçé‰∏∫Mate60Á≥ªÂàóÂèëÂ∏É‰ºöÁõ¥Êí≠" ‚Üê Êúâ"Âçé‰∏∫"+"ÂèëÂ∏É"&lt;/li&gt; 
    &lt;li&gt;‚úÖ "OPPO Find X7ÂèëÂ∏ÉÊó∂Èó¥Á°ÆÂÆö" ‚Üê Êúâ"OPPO"+"ÂèëÂ∏É"&lt;/li&gt; 
    &lt;li&gt;‚ùå "iPhoneÈîÄÈáèÂàõÊñ∞È´ò" ‚Üê Êúâ"iPhone"‰ΩÜÁº∫Â∞ë"ÂèëÂ∏É"&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;Á¨¨2ÁªÑ - ËÇ°Â∏ÇË°åÊÉÖÁ±ªÔºö&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;ÂÖ≥ÈîÆËØçÔºöAËÇ°„ÄÅ‰∏äËØÅ„ÄÅÊ∑±ËØÅ&lt;/li&gt; 
    &lt;li&gt;ÂøÖÈ°ªËØçÔºöÊ∂®Ë∑å&lt;/li&gt; 
    &lt;li&gt;ËøáÊª§ËØçÔºöÈ¢ÑÊµã&lt;/li&gt; 
    &lt;li&gt;ÊïàÊûúÔºöÂåÖÂê´ËÇ°Â∏ÇÁõ∏ÂÖ≥ËØçÔºåÂêåÊó∂ÂåÖÂê´"Ê∂®Ë∑å"Ôºå‰ΩÜÊéíÈô§ÂåÖÂê´"È¢ÑÊµã"ÁöÑÂÜÖÂÆπ&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;ÂåπÈÖçÁ§∫‰æãÔºö&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;‚úÖ "AËÇ°‰ªäÊó•Â§ßÂπÖÊ∂®Ë∑åÂàÜÊûê" ‚Üê Êúâ"AËÇ°"+"Ê∂®Ë∑å"&lt;/li&gt; 
    &lt;li&gt;‚úÖ "‰∏äËØÅÊåáÊï∞Ê∂®Ë∑åÂéüÂõ†Ëß£ËØª" ‚Üê Êúâ"‰∏äËØÅ"+"Ê∂®Ë∑å"&lt;/li&gt; 
    &lt;li&gt;‚ùå "‰∏ìÂÆ∂È¢ÑÊµãAËÇ°Ê∂®Ë∑åË∂ãÂäø" ‚Üê Êúâ"AËÇ°"+"Ê∂®Ë∑å"‰ΩÜÂåÖÂê´"È¢ÑÊµã"&lt;/li&gt; 
    &lt;li&gt;‚ùå "AËÇ°Êàê‰∫§ÈáèÂàõÊñ∞È´ò" ‚Üê Êúâ"AËÇ°"‰ΩÜÁº∫Â∞ë"Ê∂®Ë∑å"&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;Á¨¨3ÁªÑ - Ë∂≥ÁêÉËµõ‰∫ãÁ±ªÔºö&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;ÂÖ≥ÈîÆËØçÔºö‰∏ñÁïåÊùØ„ÄÅÊ¨ßÊ¥≤ÊùØ„ÄÅ‰∫öÊ¥≤ÊùØ&lt;/li&gt; 
    &lt;li&gt;ÂøÖÈ°ªËØçÔºöÊØîËµõ&lt;/li&gt; 
    &lt;li&gt;ÊïàÊûúÔºöÂøÖÈ°ªÂåÖÂê´ÊùØËµõÂêçÁß∞ÔºåÂêåÊó∂ÂåÖÂê´"ÊØîËµõ"&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;ÂåπÈÖçÁ§∫‰æãÔºö&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;‚úÖ "‰∏ñÁïåÊùØÂ∞èÁªÑËµõÊØîËµõÁªìÊûú" ‚Üê Êúâ"‰∏ñÁïåÊùØ"+"ÊØîËµõ"&lt;/li&gt; 
    &lt;li&gt;‚úÖ "Ê¨ßÊ¥≤ÊùØÂÜ≥ËµõÊØîËµõÊó∂Èó¥" ‚Üê Êúâ"Ê¨ßÊ¥≤ÊùØ"+"ÊØîËµõ"&lt;/li&gt; 
    &lt;li&gt;‚ùå "‰∏ñÁïåÊùØÈó®Á•®ÂºÄÂîÆ" ‚Üê Êúâ"‰∏ñÁïåÊùØ"‰ΩÜÁº∫Â∞ë"ÊØîËµõ"&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;h3&gt;üéØ ÈÖçÁΩÆÊäÄÂ∑ß&lt;/h3&gt; 
   &lt;h4&gt;1. &lt;strong&gt;‰ªéÂÆΩÂà∞‰∏•ÁöÑÈÖçÁΩÆÁ≠ñÁï•&lt;/strong&gt;&lt;/h4&gt; 
   &lt;pre&gt;&lt;code class="language-txt"&gt;# Á¨¨‰∏ÄÊ≠•ÔºöÂÖàÁî®ÂÆΩÊ≥õÂÖ≥ÈîÆËØçÊµãËØï
‰∫∫Â∑•Êô∫ËÉΩ
AI
ChatGPT

# Á¨¨‰∫åÊ≠•ÔºöÂèëÁé∞ËØØÂåπÈÖçÂêéÔºåÂä†ÂÖ•ÂøÖÈ°ªËØçÈôêÂÆö
‰∫∫Â∑•Êô∫ËÉΩ  
AI
ChatGPT
+ÊäÄÊúØ

# Á¨¨‰∏âÊ≠•ÔºöÂèëÁé∞Âπ≤Êâ∞ÂÜÖÂÆπÂêéÔºåÂä†ÂÖ•ËøáÊª§ËØç
‰∫∫Â∑•Êô∫ËÉΩ
AI  
ChatGPT
+ÊäÄÊúØ
!ÂπøÂëä
!ÂüπËÆ≠
&lt;/code&gt;&lt;/pre&gt; 
   &lt;h4&gt;2. &lt;strong&gt;ÈÅøÂÖçËøáÂ∫¶Â§çÊùÇ&lt;/strong&gt;&lt;/h4&gt; 
   &lt;p&gt;‚ùå &lt;strong&gt;‰∏çÊé®ËçêÔºö&lt;/strong&gt; ‰∏Ä‰∏™ËØçÁªÑÂåÖÂê´Â§™Â§öËØçÊ±á&lt;/p&gt; 
   &lt;pre&gt;&lt;code class="language-txt"&gt;Âçé‰∏∫
OPPO
ËãπÊûú
‰∏âÊòü
vivo
‰∏ÄÂä†
È≠ÖÊóè
+ÊâãÊú∫
+ÂèëÂ∏É
+ÈîÄÈáè
!ÂÅáË¥ß
!Áª¥‰øÆ
!‰∫åÊâã
&lt;/code&gt;&lt;/pre&gt; 
   &lt;p&gt;‚úÖ &lt;strong&gt;Êé®ËçêÔºö&lt;/strong&gt; ÊãÜÂàÜÊàêÂ§ö‰∏™Á≤æÁ°ÆÁöÑËØçÁªÑ&lt;/p&gt; 
   &lt;pre&gt;&lt;code class="language-txt"&gt;Âçé‰∏∫
OPPO
+Êñ∞ÂìÅ

ËãπÊûú
‰∏âÊòü  
+ÂèëÂ∏É

ÊâãÊú∫
ÈîÄÈáè
+Â∏ÇÂú∫
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üëâ Ëá™ÂÆö‰πâÁõëÊéßÂπ≥Âè∞&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h3&gt;üîß Ëá™ÂÆö‰πâÁõëÊéßÂπ≥Âè∞&lt;/h3&gt; 
 &lt;p&gt;Êú¨È°πÁõÆÁöÑËµÑËÆØÊï∞ÊçÆÊù•Ê∫ê‰∫é &lt;a href="https://github.com/ourongxing/newsnow"&gt;newsnow&lt;/a&gt; Ôºå‰Ω†ÂèØ‰ª•ÁÇπÂáª&lt;a href="https://newsnow.busiyi.world/"&gt;ÁΩëÁ´ô&lt;/a&gt;ÔºåÁÇπÂáª[Êõ¥Â§ö]ÔºåÊü•ÁúãÊòØÂê¶Êúâ‰Ω†ÊÉ≥Ë¶ÅÁöÑÂπ≥Âè∞„ÄÇ&lt;/p&gt; 
 &lt;p&gt;ÂÖ∑‰ΩìÊ∑ªÂä†ÂèØËÆøÈóÆ &lt;a href="https://github.com/ourongxing/newsnow/tree/main/server/sources"&gt;È°πÁõÆÊ∫ê‰ª£Á†Å&lt;/a&gt;ÔºåÊ†πÊçÆÈáåÈù¢ÁöÑÊñá‰ª∂ÂêçÔºåÂú® &lt;code&gt;config/config.yaml&lt;/code&gt; Êñá‰ª∂‰∏≠‰øÆÊîπ &lt;code&gt;platforms&lt;/code&gt; ÈÖçÁΩÆÔºö&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;platforms:
  - id: "toutiao"
    name: "‰ªäÊó•Â§¥Êù°"
  - id: "baidu"  
    name: "ÁôæÂ∫¶ÁÉ≠Êêú"
  - id: "wallstreetcn-hot"
    name: "ÂçéÂ∞îË°óËßÅÈóª"
  # Ê∑ªÂä†Êõ¥Â§öÂπ≥Âè∞...
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üëâ Docker ÈÉ®ÁΩ≤&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h3&gt;üê≥ Docker ÈÉ®ÁΩ≤&lt;/h3&gt; 
 &lt;h4&gt;ÊñπÂºè‰∏ÄÔºöÂø´ÈÄü‰ΩìÈ™åÔºà‰∏ÄË°åÂëΩ‰ª§Ôºâ&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Áõ¥Êé•ËøêË°åÔºå‰ΩøÁî®ÈªòËÆ§ÈÖçÁΩÆÔºà‰ªÖ‰ΩìÈ™åÂäüËÉΩÔºåÊó†Êé®ÈÄÅÈÄöÁü•Ôºâ
docker run -d --name trend-radar \
  -v ./config:/app/config:ro \
  -v ./output:/app/output \
  -e CRON_SCHEDULE="*/30 * * * *" \
  -e RUN_MODE="cron" \
  -e IMMEDIATE_RUN="true" \
  wantcat/trendradar:latest

# ÊàñËÄÖÂêØÁî®ÊâãÊú∫Â∫îÁî®Êé®ÈÄÅÈÄöÁü•
docker run -d --name trend-radar \
  -v ./config:/app/config:ro \
  -v ./output:/app/output \
  -e FEISHU_WEBHOOK_URL="‰Ω†ÁöÑÈ£û‰π¶webhook" \
  -e DINGTALK_WEBHOOK_URL="‰Ω†ÁöÑÈíâÈíâwebhook" \
  -e WEWORK_WEBHOOK_URL="‰Ω†ÁöÑ‰ºÅ‰∏öÂæÆ‰ø°webhook" \
  -e TELEGRAM_BOT_TOKEN="‰Ω†ÁöÑtelegram_bot_token" \
  -e TELEGRAM_CHAT_ID="‰Ω†ÁöÑtelegram_chat_id" \
  -e CRON_SCHEDULE="*/30 * * * *" \
  -e RUN_MODE="cron" \
  -e IMMEDIATE_RUN="true" \
  wantcat/trendradar:latest
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Ê≥®ÊÑè&lt;/strong&gt;ÔºöÂø´ÈÄü‰ΩìÈ™åÊ®°ÂºèÈúÄË¶ÅÂÖàÂáÜÂ§áÈÖçÁΩÆÊñá‰ª∂Ôºö&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Linux/macOS Á≥ªÁªüÔºö&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# ÂàõÂª∫ÈÖçÁΩÆÁõÆÂΩïÂπ∂‰∏ãËΩΩÈÖçÁΩÆÊñá‰ª∂
mkdir -p config output
wget https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/config.yaml -P config/
wget https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/frequency_words.txt -P config/
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;ÊàñËÄÖ&lt;strong&gt;ÊâãÂä®ÂàõÂª∫&lt;/strong&gt;Ôºö&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Âú®ÂΩìÂâçÁõÆÂΩï‰∏ãÂàõÂª∫‰∏§‰∏™Êñá‰ª∂Â§πÔºö&lt;code&gt;config&lt;/code&gt; Âíå &lt;code&gt;output&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;‰∏ãËΩΩÈÖçÁΩÆÊñá‰ª∂Âà∞ÂØπÂ∫î‰ΩçÁΩÆÔºö 
   &lt;ul&gt; 
    &lt;li&gt;ËÆøÈóÆ &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/config.yaml"&gt;https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/config.yaml&lt;/a&gt; ‚Üí Âè≥ÈîÆ"Âè¶Â≠ò‰∏∫" ‚Üí ‰øùÂ≠òÂà∞ &lt;code&gt;config\config.yaml&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;ËÆøÈóÆ &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/frequency_words.txt"&gt;https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/frequency_words.txt&lt;/a&gt; ‚Üí Âè≥ÈîÆ"Âè¶Â≠ò‰∏∫" ‚Üí ‰øùÂ≠òÂà∞ &lt;code&gt;config\frequency_words.txt&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;ÂÆåÊàêÂêéÁöÑÁõÆÂΩïÁªìÊûÑÂ∫îËØ•ÊòØÔºö&lt;/p&gt; 
 &lt;pre&gt;&lt;code&gt;ÂΩìÂâçÁõÆÂΩï/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ config.yaml
‚îÇ   ‚îî‚îÄ‚îÄ frequency_words.txt
‚îî‚îÄ‚îÄ output/
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;ÊñπÂºè‰∫åÔºö‰ΩøÁî® docker-composeÔºàÊé®ËçêÔºâ&lt;/h4&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;ÂàõÂª∫È°πÁõÆÁõÆÂΩïÂíåÈÖçÁΩÆ&lt;/strong&gt;: &lt;pre&gt;&lt;code class="language-bash"&gt;# ÂàõÂª∫ÁõÆÂΩïÁªìÊûÑ
mkdir -p trendradar/{config,docker}
cd trendradar

# ‰∏ãËΩΩÈÖçÁΩÆÊñá‰ª∂Ê®°Êùø
wget https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/config.yaml -P config/
wget https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/frequency_words.txt -P config/

# ‰∏ãËΩΩ docker-compose ÈÖçÁΩÆ
wget https://raw.githubusercontent.com/sansan0/TrendRadar/master/docker/.env
wget https://raw.githubusercontent.com/sansan0/TrendRadar/master/docker/docker-compose.yml
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;ÂÆåÊàêÂêéÁöÑÁõÆÂΩïÁªìÊûÑÂ∫îËØ•ÊòØÔºö&lt;/p&gt; 
 &lt;pre&gt;&lt;code&gt;ÂΩìÂâçÁõÆÂΩï/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ config.yaml
‚îÇ   ‚îî‚îÄ‚îÄ frequency_words.txt
‚îî‚îÄ‚îÄ docker/
    ‚îú‚îÄ‚îÄ .env
    ‚îî‚îÄ‚îÄ docker-compose.yml
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÈÖçÁΩÆÊñá‰ª∂ËØ¥Êòé&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;code&gt;config/config.yaml&lt;/code&gt; - Â∫îÁî®‰∏ªÈÖçÁΩÆÔºàÊä•ÂëäÊ®°Âºè„ÄÅÊé®ÈÄÅËÆæÁΩÆÁ≠âÔºâ&lt;/li&gt; 
    &lt;li&gt;&lt;code&gt;config/frequency_words.txt&lt;/code&gt; - ÂÖ≥ÈîÆËØçÈÖçÁΩÆÔºàËÆæÁΩÆ‰Ω†ÂÖ≥ÂøÉÁöÑÁÉ≠ÁÇπËØçÊ±áÔºâ&lt;/li&gt; 
    &lt;li&gt;&lt;code&gt;.env&lt;/code&gt; - ÁéØÂ¢ÉÂèòÈáèÈÖçÁΩÆÔºàwebhook URLs ÂíåÂÆöÊó∂‰ªªÂä°Ôºâ&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂêØÂä®ÊúçÂä°&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# ÊãâÂèñÊúÄÊñ∞ÈïúÂÉèÂπ∂ÂêØÂä®
docker-compose pull
docker-compose up -d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Êü•ÁúãËøêË°åÁä∂ÊÄÅ&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Êü•ÁúãÊó•Âøó
docker logs -f trend-radar

# Êü•ÁúãÂÆπÂô®Áä∂ÊÄÅ
docker ps | grep trend-radar
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h4&gt;ÊñπÂºè‰∏âÔºöÊú¨Âú∞ÊûÑÂª∫ÔºàÂºÄÂèëËÄÖÈÄâÈ°πÔºâ&lt;/h4&gt; 
 &lt;p&gt;Â¶ÇÊûúÈúÄË¶ÅËá™ÂÆö‰πâ‰øÆÊîπ‰ª£Á†ÅÊàñÊûÑÂª∫Ëá™Â∑±ÁöÑÈïúÂÉèÔºö&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# ÂÖãÈöÜÈ°πÁõÆ
git clone https://github.com/sansan0/TrendRadar.git
cd TrendRadar

# ‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂
vim config/config.yaml
vim config/frequency_words.txt

# ‰ΩøÁî®ÊûÑÂª∫ÁâàÊú¨ÁöÑ docker-compose
cd docker
cp docker-compose-build.yml docker-compose.yml

# ÊûÑÂª∫Âπ∂ÂêØÂä®
docker-compose build
docker-compose up -d
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;ÈïúÂÉèÊõ¥Êñ∞&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# ÊñπÂºè‰∏ÄÔºöÊâãÂä®Êõ¥Êñ∞
docker pull wantcat/trendradar:latest
docker-compose down
docker-compose up -d

# ÊñπÂºè‰∫åÔºö‰ΩøÁî® docker-compose Êõ¥Êñ∞
docker-compose pull
docker-compose up -d
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;ÊúçÂä°ÁÆ°ÁêÜÂëΩ‰ª§&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Êü•ÁúãËøêË°åÁä∂ÊÄÅ
docker exec -it trend-radar python manage.py status

# ÊâãÂä®ÊâßË°å‰∏ÄÊ¨°Áà¨Ëô´
docker exec -it trend-radar python manage.py run

# Êü•ÁúãÂÆûÊó∂Êó•Âøó
docker exec -it trend-radar python manage.py logs

# ÊòæÁ§∫ÂΩìÂâçÈÖçÁΩÆ
docker exec -it trend-radar python manage.py config

# ÊòæÁ§∫ËæìÂá∫Êñá‰ª∂
docker exec -it trend-radar python manage.py files

# Êü•ÁúãÂ∏ÆÂä©‰ø°ÊÅØ
docker exec -it trend-radar python manage.py help

# ÈáçÂêØÂÆπÂô®
docker restart trend-radar

# ÂÅúÊ≠¢ÂÆπÂô®
docker stop trend-radar

# Âà†Èô§ÂÆπÂô®Ôºà‰øùÁïôÊï∞ÊçÆÔºâ
docker rm trend-radar
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Êï∞ÊçÆÊåÅ‰πÖÂåñ&lt;/h4&gt; 
 &lt;p&gt;ÁîüÊàêÁöÑÊä•ÂëäÂíåÊï∞ÊçÆÈªòËÆ§‰øùÂ≠òÂú® &lt;code&gt;./output&lt;/code&gt; ÁõÆÂΩï‰∏ãÔºåÂç≥‰ΩøÂÆπÂô®ÈáçÂêØÊàñÂà†Èô§ÔºåÊï∞ÊçÆ‰πü‰ºö‰øùÁïô„ÄÇ&lt;/p&gt; 
 &lt;h4&gt;ÊïÖÈöúÊéíÊü•&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Ê£ÄÊü•ÂÆπÂô®Áä∂ÊÄÅ
docker inspect trend-radar

# Êü•ÁúãÂÆπÂô®Êó•Âøó
docker logs --tail 100 trend-radar

# ËøõÂÖ•ÂÆπÂô®Ë∞ÉËØï
docker exec -it trend-radar /bin/bash

# È™åËØÅÈÖçÁΩÆÊñá‰ª∂
docker exec -it trend-radar ls -la /app/config/
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üëâ ÁÉ≠ÁÇπÊùÉÈáçË∞ÉÊï¥&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;ÂΩìÂâçÈªòËÆ§ÁöÑÈÖçÁΩÆÊòØÂπ≥Ë°°ÊÄßÈÖçÁΩÆ&lt;/p&gt; 
 &lt;h3&gt;‰∏§‰∏™Ê†∏ÂøÉÂú∫ÊôØ&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;ËøΩÂÆûÊó∂ÁÉ≠ÁÇπÂûã&lt;/strong&gt;Ôºö&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;weight:
  rank_weight: 0.8    # ‰∏ªË¶ÅÁúãÊéíÂêç
  frequency_weight: 0.1  # ‰∏çÂ§™Âú®‰πéÊåÅÁª≠ÊÄß
  hotness_weight: 0.1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;ÈÄÇÁî®‰∫∫Áæ§&lt;/strong&gt;ÔºöËá™Â™í‰ΩìÂçö‰∏ª„ÄÅËê•ÈîÄ‰∫∫Âëò„ÄÅÊÉ≥Âø´ÈÄü‰∫ÜËß£ÂΩì‰∏ãÊúÄÁÅ´ËØùÈ¢òÁöÑÁî®Êà∑&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;ËøΩÊ∑±Â∫¶ËØùÈ¢òÂûã&lt;/strong&gt;Ôºö&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;weight:
  rank_weight: 0.4    # ÈÄÇÂ∫¶ÁúãÊéíÂêç
  frequency_weight: 0.5  # ÈáçËßÜÂΩìÂ§©ÂÜÖÁöÑÊåÅÁª≠ÁÉ≠Â∫¶
  hotness_weight: 0.1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;ÈÄÇÁî®‰∫∫Áæ§&lt;/strong&gt;ÔºöÊäïËµÑËÄÖ„ÄÅÁ†îÁ©∂‰∫∫Âëò„ÄÅÊñ∞ÈóªÂ∑•‰ΩúËÄÖ„ÄÅÈúÄË¶ÅÊ∑±Â∫¶ÂàÜÊûêË∂ãÂäøÁöÑÁî®Êà∑&lt;/p&gt; 
 &lt;h3&gt;Ë∞ÉÊï¥ÁöÑÊñπÊ≥ï&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;‰∏â‰∏™Êï∞Â≠óÂä†Ëµ∑Êù•ÂøÖÈ°ªÁ≠â‰∫é 1.0&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Âì™‰∏™ÈáçË¶ÅÂ∞±Ë∞ÉÂ§ßÂì™‰∏™&lt;/strong&gt;ÔºöÂú®‰πéÊéíÂêçÂ∞±Ë∞ÉÂ§ß rank_weightÔºåÂú®‰πéÊåÅÁª≠ÊÄßÂ∞±Ë∞ÉÂ§ß frequency_weight&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Âª∫ËÆÆÊØèÊ¨°Âè™Ë∞É 0.1-0.2&lt;/strong&gt;ÔºåËßÇÂØüÊïàÊûú&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;Ê†∏ÂøÉÊÄùË∑ØÔºöËøΩÊ±ÇÈÄüÂ∫¶ÂíåÊó∂ÊïàÊÄßÁöÑÁî®Êà∑ÊèêÈ´òÊéíÂêçÊùÉÈáçÔºåËøΩÊ±ÇÊ∑±Â∫¶ÂíåÁ®≥ÂÆöÊÄßÁöÑÁî®Êà∑ÊèêÈ´òÈ¢ëÊ¨°ÊùÉÈáç„ÄÇ&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚òï Â≠¶‰π†‰∫§ÊµÅ‰∏é1ÂÖÉÁÇπËµû&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ÂøÉÊÑèÂà∞Â∞±Ë°åÔºåÊî∂Âà∞ÁöÑÁÇπËµûÁî®‰∫éÊèêÈ´òÂºÄÂèëËÄÖÂºÄÊ∫êÁöÑÁßØÊûÅÊÄß&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;ÂÖ¨‰ºóÂè∑ÂÖ≥Ê≥®&lt;/th&gt; 
    &lt;th align="center"&gt;ÂæÆ‰ø°ÁÇπËµû&lt;/th&gt; 
    &lt;th align="center"&gt;ÊîØ‰ªòÂÆùÁÇπËµû&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/weixin.png" width="300" title="Á°ÖÂü∫Ëå∂Ê∞¥Èó¥" /&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;img src="https://cdn-1258574687.cos.ap-shanghai.myqcloud.com/img/%2F2025%2F07%2F17%2F2ae0a88d98079f7e876c2b4dc85233c6-9e8025.JPG" width="300" title="ÂæÆ‰ø°ÊîØ‰ªò" /&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;img src="https://cdn-1258574687.cos.ap-shanghai.myqcloud.com/img/%2F2025%2F07%2F17%2Fed4f20ab8e35be51f8e84c94e6e239b4-fe4947.JPG" width="300" title="ÊîØ‰ªòÂÆùÊîØ‰ªò" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h3&gt;È°πÁõÆÁõ∏ÂÖ≥Êé®Ëçê&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ÈôÑÈ°πÁõÆÁõ∏ÂÖ≥ÁöÑ‰∏§ÁØáÊñáÁ´†ÔºåÊ¨¢ËøéÁïôË®Ä‰∫§ÊµÅ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/jzn0vLiQFX408opcfpPPxQ"&gt;2‰∏™ÊúàÁ†¥ 1000 starÔºåÊàëÁöÑGitHubÈ°πÁõÆÊé®ÂπøÂÆûÊàòÁªèÈ™å&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/8ghyfDAtQZjLrnWTQabYOQ"&gt;Âü∫‰∫éÊú¨È°πÁõÆÔºåÂ¶Ç‰ΩïÂºÄÂ±ïÂÖ¨‰ºóÂè∑ÊàñËÄÖÊñ∞ÈóªËµÑËÆØÁ±ªÊñáÁ´†ÂÜô‰Ωú&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;AI ÂºÄÂèëÔºö&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;Â¶ÇÊûú‰Ω†ÊúâÂ∞è‰ºóÈúÄÊ±ÇÔºåÂÆåÂÖ®ÂèØ‰ª•Âü∫‰∫éÊàëÁöÑÈ°πÁõÆËá™Ë°åÂºÄÂèëÔºåÈõ∂ÁºñÁ®ãÂü∫Á°ÄÁöÑ‰πüÂèØ‰ª•ËØïËØï&lt;/li&gt; 
 &lt;li&gt;ÊàëÊâÄÊúâÁöÑÂºÄÊ∫êÈ°πÁõÆÊàñÂ§öÊàñÂ∞ëÈÉΩ‰ΩøÁî®‰∫ÜËá™Â∑±ÂÜôÁöÑ&lt;strong&gt;AIËæÖÂä©ËΩØ‰ª∂&lt;/strong&gt;Êù•ÊèêÂçáÂºÄÂèëÊïàÁéáÔºåËøôÊ¨æÂ∑•ÂÖ∑Â∑≤ÂºÄÊ∫ê&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ê†∏ÂøÉÂäüËÉΩ&lt;/strong&gt;ÔºöËøÖÈÄüÁ≠õÈÄâÈ°πÁõÆ‰ª£Á†ÅÂñÇÁªôAIÔºå‰Ω†Âè™ÈúÄË¶ÅË°•ÂÖÖ‰∏™‰∫∫ÈúÄÊ±ÇÂç≥ÂèØ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;È°πÁõÆÂú∞ÂùÄ&lt;/strong&gt;Ôºö&lt;a href="https://github.com/sansan0/ai-code-context-helper"&gt;https://github.com/sansan0/ai-code-context-helper&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ÂÖ∂‰ΩôÈ°πÁõÆ&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üìç ÊØõ‰∏ªÂ∏≠Ë∂≥ËøπÂú∞Âõæ - ‰∫§‰∫íÂºèÂä®ÊÄÅÂ±ïÁ§∫1893-1976Âπ¥ÂÆåÊï¥ËΩ®Ëøπ„ÄÇÊ¨¢ËøéËØ∏‰ΩçÂêåÂøóË¥°ÁåÆÊï∞ÊçÆ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sansan0/mao-map"&gt;https://github.com/sansan0/mao-map&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ÂìîÂì©ÂìîÂì©(bilibili)ËØÑËÆ∫Âå∫Êï∞ÊçÆÂèØËßÜÂåñÂàÜÊûêËΩØ‰ª∂&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sansan0/bilibili-comment-analyzer"&gt;https://github.com/sansan0/bilibili-comment-analyzer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üëâ ÂæÆ‰ø°Êé®ÈÄÅÈÄöÁü•ÊñπÊ°à&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;Áî±‰∫éËØ•ÊñπÊ°àÊòØÂü∫‰∫é‰ºÅ‰∏öÂæÆ‰ø°ÁöÑÊèí‰ª∂Êú∫Âà∂ÔºåÊé®ÈÄÅÊ†∑Âºè‰πüÂçÅÂàÜ‰∏çÂêåÔºåÊâÄ‰ª•Áõ∏ÂÖ≥ÂÆûÁé∞ÊàëÊöÇÊó∂‰∏çÂáÜÂ§áÁ∫≥ÂÖ•ÂΩìÂâçÈ°πÁõÆ&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;fork Ëøô‰ΩçÂÖÑÂè∞ÁöÑÈ°πÁõÆ &lt;a href="https://github.com/jayzqj/TrendRadar"&gt;https://github.com/jayzqj/TrendRadar&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ÂÆåÊàê‰∏äÊñπÁöÑ‰ºÅ‰∏öÂæÆ‰ø°Êé®ÈÄÅËÆæÁΩÆ&lt;/li&gt; 
  &lt;li&gt;ÊåâÁÖß‰∏ãÈù¢ÂõæÁâáÊìç‰Ωú&lt;/li&gt; 
  &lt;li&gt;ÈÖçÁΩÆÂ•ΩÂêéÔºåÊâãÊú∫‰∏äÁöÑ‰ºÅ‰∏öÂæÆ‰ø° app Âà†Èô§Êéâ‰πüÊ≤°‰∫ã&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/wework.png" title="github" /&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üëâ Êú¨È°πÁõÆÊµÅÁ®ãÂõæ&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-mermaid"&gt;flowchart TD
    A[üë§ Áî®Êà∑ÂºÄÂßã] --&amp;gt; B[üç¥ Fork È°πÁõÆ]
    B --&amp;gt; C[‚öôÔ∏è ÈÄâÊã©ÈÄöÁü•ÊñπÂºè]
    
    C --&amp;gt; D1[üì± ‰ºÅ‰∏öÂæÆ‰ø°Áæ§Êú∫Âô®‰∫∫&amp;lt;br/&amp;gt;ÊúÄÁÆÄÂçïÂø´ÈÄü]
    C --&amp;gt; D2[üí¨ È£û‰π¶Êú∫Âô®‰∫∫&amp;lt;br/&amp;gt;ÊòæÁ§∫ÊïàÊûúÊúÄ‰Ω≥]
    C --&amp;gt; D3[üîî ÈíâÈíâÊú∫Âô®‰∫∫&amp;lt;br/&amp;gt;]
    C --&amp;gt; D4[üìü Telegram Bot&amp;lt;br/&amp;gt;]
    
    D1 --&amp;gt; E[üîë ÈÖçÁΩÆ GitHub Secrets&amp;lt;br/&amp;gt;Â°´ÂÖ•Êú∫Âô®‰∫∫ Webhook Âú∞ÂùÄ]
    D2 --&amp;gt; E
    D3 --&amp;gt; E  
    D4 --&amp;gt; E
    
    E --&amp;gt; F[üìù ÁºñËæëÂÖ≥ÈîÆËØçÈÖçÁΩÆ&amp;lt;br/&amp;gt;config/frequency_words.txt&amp;lt;br/&amp;gt;Ê∑ªÂä†‰Ω†ÂÖ≥ÂøÉÁöÑËØçÊ±á]
    F --&amp;gt; G[üéØ ÈÄâÊã©ËøêË°åÊ®°Âºè&amp;lt;br/&amp;gt;config/config.yaml&amp;lt;br/&amp;gt;daily/current/incremental]
    
    G --&amp;gt; H[‚úÖ ÈÖçÁΩÆÂÆåÊàê]
    H --&amp;gt; I[ü§ñ Á≥ªÁªüÊ†πÊçÆËÆæÂÆöÊó∂Èó¥Ëá™Âä®ËøêË°å]
    
    I --&amp;gt; J[üìä Áà¨ÂèñÂêÑÂ§ßÂπ≥Âè∞ÁÉ≠ÁÇπ]
    J --&amp;gt; K[üîç Ê†πÊçÆÂÖ≥ÈîÆËØçÁ≠õÈÄâ]
    K --&amp;gt; L[üì± Êé®ÈÄÅÂà∞‰Ω†ÁöÑÊâãÊú∫]
    
    L --&amp;gt; M[üìà Êü•ÁúãÊé®ÈÄÅÁªìÊûú]
    M --&amp;gt; N{Êª°ÊÑèÊïàÊûú?}
    N --&amp;gt;|‰∏çÊª°ÊÑè| F
    N --&amp;gt;|Êª°ÊÑè| O[üéâ ÊåÅÁª≠Êé•Êî∂Á≤æÂáÜÊé®ÈÄÅ]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style E fill:#fff3e0
    style F fill:#e8f5e8
    style G fill:#e8f5e8
    style L fill:#ffebee
    style O fill:#e8f5e8
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#sansan0/TrendRadar&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=sansan0/TrendRadar&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üìÑ ËÆ∏ÂèØËØÅ&lt;/h2&gt; 
&lt;p&gt;GPL-3.0 License&lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;‚≠ê Â¶ÇÊûúËøô‰∏™Â∑•ÂÖ∑ÂØπ‰Ω†ÊúâÂ∏ÆÂä©ÔºåËØ∑ÁªôÈ°πÁõÆÁÇπ‰∏™ Star ÊîØÊåÅÂºÄÂèëÔºÅ&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#trendradar"&gt;üîù ÂõûÂà∞È°∂ÈÉ®&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>HKUDS/DeepCode</title>
      <link>https://github.com/HKUDS/DeepCode</link>
      <description>&lt;p&gt;"DeepCode: Open Agentic Coding (Paper2Code &amp; Text2Web &amp; Text2Backend)"&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;table style="border: none; margin: 0 auto; padding: 0; border-collapse: collapse;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center" style="vertical-align: middle; padding: 10px; border: none; width: 250px;"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/DeepCode/main/assets/logo.png" alt="DeepCode Logo" width="200" style="margin: 0; padding: 0; display: block;" /&gt; &lt;/td&gt; 
    &lt;td align="left" style="vertical-align: middle; padding: 10px 0 10px 30px; border: none;"&gt; &lt;pre style="font-family: 'Courier New', monospace; font-size: 16px; color: #0EA5E9; margin: 0; padding: 0; text-shadow: 0 0 10px #0EA5E9, 0 0 20px rgba(14,165,233,0.5); line-height: 1.2; transform: skew(-1deg, 0deg); display: block;"&gt;    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
    ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù
    ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
    ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïù ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù
    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë     ‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù      ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù&lt;/pre&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://trendshift.io/repositories/14665" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14665" alt="HKUDS%2FDeepCode | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
 &lt;/div&gt; 
 &lt;!-- &lt;img src="https://readme-typing-svg.herokuapp.com?font=Russo+One&amp;size=28&amp;duration=2000&amp;pause=800&amp;color=06B6D4&amp;background=00000000&amp;center=true&amp;vCenter=true&amp;width=800&amp;height=50&amp;lines=%E2%9A%A1+OPEN+AGENTIC+CODING+%E2%9A%A1" alt="DeepCode Tech Subtitle" style="margin-top: 5px; filter: drop-shadow(0 0 12px #06B6D4) drop-shadow(0 0 24px rgba(6,182,212,0.4));"/&gt; --&gt; 
 &lt;h1&gt;&lt;img src="https://github.com/Zongwei9888/Experiment_Images/raw/43c585dca3d21b8e4b6390d835cdd34dc4b4b23d/DeepCode_images/title_logo.svg?sanitize=true" alt="DeepCode Logo" width="32" height="32" style="vertical-align: middle; margin-right: 8px;" /&gt; DeepCode: Open Agentic Coding&lt;/h1&gt; 
 &lt;h3&gt;&lt;em&gt;Advancing Code Generation with Multi-Agent Systems&lt;/em&gt;&lt;/h3&gt; 
 &lt;!-- &lt;p align="center"&gt;
  &lt;img src="https://img.shields.io/badge/Version-1.0.0-00d4ff?style=for-the-badge&amp;logo=rocket&amp;logoColor=white" alt="Version"&gt;

  &lt;img src="https://img.shields.io/badge/License-MIT-4ecdc4?style=for-the-badge&amp;logo=opensourceinitiative&amp;logoColor=white" alt="License"&gt;
  &lt;img src="https://img.shields.io/badge/AI-Multi--Agent-9b59b6?style=for-the-badge&amp;logo=brain&amp;logoColor=white" alt="AI"&gt;
  &lt;img src="https://img.shields.io/badge/HKU-Data_Intelligence_Lab-f39c12?style=for-the-badge&amp;logo=university&amp;logoColor=white" alt="HKU"&gt;
&lt;/p&gt; --&gt; 
 &lt;p&gt; &lt;a href="https://github.com/HKUDS/DeepCode/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/HKUDS/DeepCode?color=00d9ff&amp;amp;style=for-the-badge&amp;amp;logo=star&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/üêçPython-3.13-4ecdc4?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;a href="https://pypi.org/project/deepcode-hku/"&gt;&lt;img src="https://img.shields.io/pypi/v/deepcode-hku.svg?style=for-the-badge&amp;amp;logo=pypi&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e&amp;amp;color=ff6b6b" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;a href="https://discord.gg/yF2MmDJyGJ"&gt;&lt;img src="https://img.shields.io/badge/üí¨Discord-Community-7289da?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/DeepCode/issues/11"&gt;&lt;img src="https://img.shields.io/badge/üí¨WeChat-Group-07c160?style=for-the-badge&amp;amp;logo=wechat&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;div style="width: 100%; height: 2px; margin: 20px 0; background: linear-gradient(90deg, transparent, #00d9ff, transparent);"&gt;&lt;/div&gt; 
 &lt;/div&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-quick-start" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/Quick%20Start-Get%20Started%20Now-00d9ff?style=for-the-badge&amp;amp;logo=rocket&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;/a&gt; 
 &lt;/div&gt; 
 &lt;h3&gt;üñ•Ô∏è &lt;strong&gt;Interface Showcase&lt;/strong&gt;&lt;/h3&gt; 
 &lt;table align="center" width="100%" style="border: none; border-collapse: collapse; margin: 30px 0;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td width="50%" align="center" style="vertical-align: top; padding: 20px;"&gt; &lt;h4&gt;üñ•Ô∏è &lt;strong&gt;CLI Interface&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Terminal-Based Development&lt;/strong&gt;&lt;/p&gt; 
     &lt;div align="center"&gt; 
      &lt;img src="https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/CLI.gif" alt="CLI Interface Demo" width="100%" style="border-radius: 10px; box-shadow: 0 8px 20px rgba(45,55,72,0.3); margin: 15px 0;" /&gt; 
      &lt;div style="background: linear-gradient(135deg, #2D3748 0%, #4A5568 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;"&gt; 
       &lt;strong&gt;üöÄ Advanced Terminal Experience&lt;/strong&gt;
       &lt;br /&gt; 
       &lt;small&gt;‚ö° Fast command-line workflow&lt;br /&gt;üîß Developer-friendly interface&lt;br /&gt;üìä Real-time progress tracking&lt;/small&gt; 
      &lt;/div&gt; 
      &lt;p&gt;&lt;em&gt;Professional terminal interface for advanced users and CI/CD integration&lt;/em&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td width="50%" align="center" style="vertical-align: top; padding: 20px;"&gt; &lt;h4&gt;üåê &lt;strong&gt;Web Interface&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Visual Interactive Experience&lt;/strong&gt;&lt;/p&gt; 
     &lt;div align="center"&gt; 
      &lt;img src="https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/UI.gif" alt="Web Interface Demo" width="100%" style="border-radius: 10px; box-shadow: 0 8px 20px rgba(14,165,233,0.3); margin: 15px 0;" /&gt; 
      &lt;div style="background: linear-gradient(135deg, #0EA5E9 0%, #00D4FF 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;"&gt; 
       &lt;strong&gt;üé® Modern Web Dashboard&lt;/strong&gt;
       &lt;br /&gt; 
       &lt;small&gt;üñ±Ô∏è Intuitive drag-and-drop&lt;br /&gt;üì± Responsive design&lt;br /&gt;üéØ Visual progress tracking&lt;/small&gt; 
      &lt;/div&gt; 
      &lt;p&gt;&lt;em&gt;Beautiful web interface with streamlined workflow for all skill levels&lt;/em&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;hr /&gt; 
 &lt;div align="center"&gt; 
  &lt;h3&gt;üé¨ &lt;strong&gt;Introduction Video&lt;/strong&gt;&lt;/h3&gt; 
  &lt;div style="margin: 20px 0;"&gt; 
   &lt;a href="https://youtu.be/PRgmP8pOI08" target="_blank"&gt; &lt;img src="https://img.youtube.com/vi/PRgmP8pOI08/maxresdefault.jpg" alt="DeepCode Introduction Video" width="75%" style="border-radius: 12px; box-shadow: 0 8px 25px rgba(0,0,0,0.15); transition: transform 0.3s ease;" /&gt; &lt;/a&gt; 
  &lt;/div&gt; 
  &lt;p&gt;&lt;em&gt;üéØ &lt;strong&gt;Watch our complete introduction&lt;/strong&gt; - See how DeepCode transforms research papers and natural language into production-ready code&lt;/em&gt;&lt;/p&gt; 
  &lt;p&gt; &lt;a href="https://youtu.be/PRgmP8pOI08" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/‚ñ∂Ô∏è_Watch_Video-FF0000?style=for-the-badge&amp;amp;logo=youtube&amp;amp;logoColor=white" alt="Watch Video" /&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;/div&gt; 
 &lt;hr /&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;em&gt;"Where AI Agents Transform Ideas into Production-Ready Code"&lt;/em&gt;&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìë Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-key-features"&gt;üöÄ Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#%EF%B8%8F-architecture"&gt;üèóÔ∏è Architecture&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-quick-start"&gt;üöÄ Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-examples"&gt;üí° Examples&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-live-demonstrations"&gt;üé¨ Live Demonstrations&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-star-history"&gt;‚≠ê Star History&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-license"&gt;üìÑ License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ Key Features&lt;/h2&gt; 
&lt;br /&gt; 
&lt;table align="center" width="100%" style="border: none; table-layout: fixed;"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="30%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;üöÄ &lt;strong&gt;Paper2Code&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/ALGORITHM-IMPLEMENTATION-ff6b6b?style=for-the-badge&amp;amp;logo=algorithm&amp;amp;logoColor=white" alt="Algorithm Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Automated Implementation of Complex Algorithms&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Effortlessly converts complex algorithms from research papers into &lt;strong&gt;high-quality&lt;/strong&gt;, &lt;strong&gt;production-ready&lt;/strong&gt; code, accelerating algorithm reproduction.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="30%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;üé® &lt;strong&gt;Text2Web&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/FRONTEND-DEVELOPMENT-4ecdc4?style=for-the-badge&amp;amp;logo=react&amp;amp;logoColor=white" alt="Frontend Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Automated Front-End Web Development&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Translates plain textual descriptions into &lt;strong&gt;fully functional&lt;/strong&gt;, &lt;strong&gt;visually appealing&lt;/strong&gt; front-end web code for rapid interface creation.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="30%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;‚öôÔ∏è &lt;strong&gt;Text2Backend&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/BACKEND-DEVELOPMENT-9b59b6?style=for-the-badge&amp;amp;logo=server&amp;amp;logoColor=white" alt="Backend Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Automated Back-End Development&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Generates &lt;strong&gt;efficient&lt;/strong&gt;, &lt;strong&gt;scalable&lt;/strong&gt;, and &lt;strong&gt;feature-rich&lt;/strong&gt; back-end code from simple text inputs, streamlining server-side development.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;br /&gt; 
&lt;h3&gt;üéØ &lt;strong&gt;Autonomous Multi-Agent Workflow&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;The Challenges&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;üìÑ &lt;strong&gt;Implementation Complexity&lt;/strong&gt;: Converting academic papers and complex algorithms into working code requires significant technical effort and domain expertise&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üî¨ &lt;strong&gt;Research Bottleneck&lt;/strong&gt;: Researchers spend valuable time implementing algorithms instead of focusing on their core research and discovery work&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚è±Ô∏è &lt;strong&gt;Development Delays&lt;/strong&gt;: Product teams experience long wait times between concept and testable prototypes, slowing down innovation cycles&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üîÑ &lt;strong&gt;Repetitive Coding&lt;/strong&gt;: Developers repeatedly implement similar patterns and functionality instead of building on existing solutions&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;DeepCode&lt;/strong&gt; addresses these workflow inefficiencies by providing reliable automation for common development tasks, streamlining your development workflow from concept to code.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;pre&gt;&lt;code class="language-mermaid"&gt;flowchart LR
    A["üìÑ Research Papers&amp;lt;br/&amp;gt;üí¨ Text Prompts&amp;lt;br/&amp;gt;üåê URLs &amp;amp; Document&amp;lt;br/&amp;gt;üìé Files: PDF, DOC, PPTX, TXT, HTML"] --&amp;gt; B["üß† DeepCode&amp;lt;br/&amp;gt;Multi-Agent Engine"]
    B --&amp;gt; C["üöÄ Algorithm Implementation &amp;lt;br/&amp;gt;üé® Frontend Development &amp;lt;br/&amp;gt;‚öôÔ∏è Backend Development"]

    style A fill:#ff6b6b,stroke:#c0392b,stroke-width:2px,color:#000
    style B fill:#00d4ff,stroke:#0984e3,stroke-width:3px,color:#000
    style C fill:#00b894,stroke:#00a085,stroke-width:2px,color:#000
&lt;/code&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üèóÔ∏è Architecture&lt;/h2&gt; 
&lt;h3&gt;üìä &lt;strong&gt;System Overview&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;DeepCode&lt;/strong&gt; is an AI-powered development platform that automates code generation and implementation tasks. Our multi-agent system handles the complexity of translating requirements into functional, well-structured code, allowing you to focus on innovation rather than implementation details.&lt;/p&gt; 
&lt;p&gt;üéØ &lt;strong&gt;Technical Capabilities&lt;/strong&gt;:&lt;/p&gt; 
&lt;p&gt;üß¨ &lt;strong&gt;Research-to-Production Pipeline&lt;/strong&gt;&lt;br /&gt; Multi-modal document analysis engine that extracts algorithmic logic and mathematical models from academic papers. Generates optimized implementations with proper data structures while preserving computational complexity characteristics.&lt;/p&gt; 
&lt;p&gt;ü™Ñ &lt;strong&gt;Natural Language Code Synthesis&lt;/strong&gt;&lt;br /&gt; Context-aware code generation using fine-tuned language models trained on curated code repositories. Maintains architectural consistency across modules while supporting multiple programming languages and frameworks.&lt;/p&gt; 
&lt;p&gt;‚ö° &lt;strong&gt;Automated Prototyping Engine&lt;/strong&gt;&lt;br /&gt; Intelligent scaffolding system generating complete application structures including database schemas, API endpoints, and frontend components. Uses dependency analysis to ensure scalable architecture from initial generation.&lt;/p&gt; 
&lt;p&gt;üíé &lt;strong&gt;Quality Assurance Automation&lt;/strong&gt;&lt;br /&gt; Integrated static analysis with automated unit test generation and documentation synthesis. Employs AST analysis for code correctness and property-based testing for comprehensive coverage.&lt;/p&gt; 
&lt;p&gt;üîÆ &lt;strong&gt;CodeRAG Integration System&lt;/strong&gt;&lt;br /&gt; Advanced retrieval-augmented generation combining semantic vector embeddings with graph-based dependency analysis. Automatically discovers optimal libraries and implementation patterns from large-scale code corpus.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üîß &lt;strong&gt;Core Techniques&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;üß† &lt;strong&gt;Intelligent Orchestration Agent&lt;/strong&gt;: Central decision-making system that coordinates workflow phases and analyzes requirements. Employs dynamic planning algorithms to adapt execution strategies in real-time based on evolving project complexity. Dynamically selects optimal processing strategies for each implementation step. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üíæ &lt;strong&gt;Efficient Memory Mechanism&lt;/strong&gt;: Advanced context engineering system that manages large-scale code contexts efficiently. Implements hierarchical memory structures with intelligent compression for handling complex codebases. This component enables instant retrieval of implementation patterns and maintains semantic coherence across extended development sessions. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üîç &lt;strong&gt;Advanced CodeRAG System&lt;/strong&gt;: Global code comprehension engine that analyzes complex inter-dependencies across repositories. Performs cross-codebase relationship mapping to understand architectural patterns from a holistic perspective. This module leverages dependency graphs and semantic analysis to provide globally-aware code recommendations during implementation.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;ü§ñ &lt;strong&gt;Multi-Agent Architecture of DeepCode&lt;/strong&gt;:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üéØ Central Orchestrating Agent&lt;/strong&gt;: Orchestrates entire workflow execution and makes strategic decisions. Coordinates specialized agents based on input complexity analysis. Implements dynamic task planning and resource allocation algorithms. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìù Intent Understanding Agent&lt;/strong&gt;: Performs deep semantic analysis of user requirements to decode complex intentions. Extracts functional specifications and technical constraints through advanced NLP processing. Transforms ambiguous human descriptions into precise, actionable development specifications with structured task decomposition. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìÑ Document Parsing Agent&lt;/strong&gt;: Processes complex technical documents and research papers with advanced parsing capabilities. Extracts algorithms and methodologies using document understanding models. Converts academic concepts into practical implementation specifications through intelligent content analysis. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üèóÔ∏è Code Planning Agent&lt;/strong&gt;: Performs architectural design and technology stack optimization. Dynamic planning for adaptive development roadmaps. Enforces coding standards and generates modular structures through automated design pattern selection.&lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üîç Code Reference Mining Agent&lt;/strong&gt;: Discovers relevant repositories and frameworks through intelligent search algorithms. Analyzes codebases for compatibility and integration potential. Provides recommendations based on similarity metrics and automated dependency analysis. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìö Code Indexing Agent&lt;/strong&gt;: Builds comprehensive knowledge graphs of discovered codebases. Maintains semantic relationships between code components. Enables intelligent retrieval and cross-reference capabilities. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üß¨ Code Generation Agent&lt;/strong&gt;: Synthesizes gathered information into executable code implementations. Creates functional interfaces and integrates discovered components. Generates comprehensive test suites and documentation for reproducibility.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h4&gt;üõ†Ô∏è &lt;strong&gt;Implementation Tools Matrix&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;üîß Powered by MCP (Model Context Protocol)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;DeepCode leverages the &lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt; standard to seamlessly integrate with various tools and services. This standardized approach ensures reliable communication between AI agents and external systems, enabling powerful automation capabilities.&lt;/p&gt; 
&lt;h5&gt;üì° &lt;strong&gt;MCP Servers &amp;amp; Tools&lt;/strong&gt;&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;üõ†Ô∏è &lt;strong&gt;MCP Server&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;üîß &lt;strong&gt;Primary Function&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;üí° &lt;strong&gt;Purpose &amp;amp; Capabilities&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üîç brave&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Web Search Engine&lt;/td&gt; 
   &lt;td&gt;Real-time information retrieval via Brave Search API&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üåê bocha-mcp&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alternative Search&lt;/td&gt; 
   &lt;td&gt;Secondary search option with independent API access&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìÇ filesystem&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;File System Operations&lt;/td&gt; 
   &lt;td&gt;Local file and directory management, read/write operations&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üåê fetch&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Web Content Retrieval&lt;/td&gt; 
   &lt;td&gt;Fetch and extract content from URLs and web resources&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üì• github-downloader&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Repository Management&lt;/td&gt; 
   &lt;td&gt;Clone and download GitHub repositories for analysis&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìã file-downloader&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Document Processing&lt;/td&gt; 
   &lt;td&gt;Download and convert files (PDF, DOCX, etc.) to Markdown&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;‚ö° command-executor&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;System Commands&lt;/td&gt; 
   &lt;td&gt;Execute bash/shell commands for environment management&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üß¨ code-implementation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Code Generation Hub&lt;/td&gt; 
   &lt;td&gt;Comprehensive code reproduction with execution and testing&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìö code-reference-indexer&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Smart Code Search&lt;/td&gt; 
   &lt;td&gt;Intelligent indexing and search of code repositories&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìÑ document-segmentation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Smart Document Analysis&lt;/td&gt; 
   &lt;td&gt;Intelligent document segmentation for large papers and technical documents&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h5&gt;üîß &lt;strong&gt;Legacy Tool Functions&lt;/strong&gt; &lt;em&gt;(for reference)&lt;/em&gt;&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;üõ†Ô∏è &lt;strong&gt;Function&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;üéØ &lt;strong&gt;Usage Context&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìÑ read_code_mem&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Efficient code context retrieval from memory&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;‚úçÔ∏è write_file&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Direct file content generation and modification&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üêç execute_python&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Python code testing and validation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìÅ get_file_structure&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Project structure analysis and organization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;‚öôÔ∏è set_workspace&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Dynamic workspace and environment configuration&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìä get_operation_history&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Process monitoring and operation tracking&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;p&gt;üéõÔ∏è &lt;strong&gt;Multi-Interface Framework&lt;/strong&gt;&lt;br /&gt; RESTful API with CLI and web frontends featuring real-time code streaming, interactive debugging, and extensible plugin architecture for CI/CD integration.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;üöÄ Multi-Agent Intelligent Pipeline:&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;üåü &lt;strong&gt;Intelligence Processing Flow&lt;/strong&gt;&lt;/h3&gt; 
 &lt;table align="center" width="100%" style="border: none; border-collapse: collapse;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; color: white; font-weight: bold;"&gt; üí° &lt;strong&gt;INPUT LAYER&lt;/strong&gt;&lt;br /&gt; üìÑ Research Papers ‚Ä¢ üí¨ Natural Language ‚Ä¢ üåê URLs ‚Ä¢ üìã Requirements &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="20"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; üéØ &lt;strong&gt;CENTRAL ORCHESTRATION&lt;/strong&gt;&lt;br /&gt; Strategic Decision Making ‚Ä¢ Workflow Coordination ‚Ä¢ Agent Management &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #3742fa 0%, #2f3542 100%); border-radius: 10px; color: white; width: 50%;"&gt; üìù &lt;strong&gt;TEXT ANALYSIS&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Requirement Processing&lt;/small&gt; &lt;/td&gt; 
    &lt;td width="10"&gt;&lt;/td&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #8c7ae6 0%, #9c88ff 100%); border-radius: 10px; color: white; width: 50%;"&gt; üìÑ &lt;strong&gt;DOCUMENT ANALYSIS&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Paper &amp;amp; Spec Processing&lt;/small&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #00d2d3 0%, #54a0ff 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; üìã &lt;strong&gt;REPRODUCTION PLANNING&lt;/strong&gt;&lt;br /&gt; Deep Paper Analysis ‚Ä¢ Code Requirements Parsing ‚Ä¢ Reproduction Strategy Development &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #ffa726 0%, #ff7043 100%); border-radius: 10px; color: white; width: 50%;"&gt; üîç &lt;strong&gt;REFERENCE ANALYSIS&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Repository Discovery&lt;/small&gt; &lt;/td&gt; 
    &lt;td width="10"&gt;&lt;/td&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #e056fd 0%, #f368e0 100%); border-radius: 10px; color: white; width: 50%;"&gt; üìö &lt;strong&gt;CODE INDEXING&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Knowledge Graph Building&lt;/small&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #26de81 0%, #20bf6b 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; üß¨ &lt;strong&gt;CODE IMPLEMENTATION&lt;/strong&gt;&lt;br /&gt; Implementation Generation ‚Ä¢ Testing ‚Ä¢ Documentation &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 20px; background: linear-gradient(135deg, #045de9 0%, #09c6f9 100%); border-radius: 15px; color: white; font-weight: bold;"&gt; ‚ö° &lt;strong&gt;OUTPUT DELIVERY&lt;/strong&gt;&lt;br /&gt; üì¶ Complete Codebase ‚Ä¢ üß™ Test Suite ‚Ä¢ üìö Documentation ‚Ä¢ üöÄ Deployment Ready &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;br /&gt; 
 &lt;h3&gt;üîÑ &lt;strong&gt;Process Intelligence Features&lt;/strong&gt;&lt;/h3&gt; 
 &lt;table align="center" style="border: none;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #ff6b6b;"&gt; 
      &lt;h4&gt;üéØ Adaptive Flow&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Dynamic agent selection based on input complexity&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #4ecdc4;"&gt; 
      &lt;h4&gt;üß† Smart Coordination&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Intelligent task distribution and parallel processing&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #45b7d1;"&gt; 
      &lt;h4&gt;üîç Context Awareness&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Deep understanding through CodeRAG integration&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #96ceb4;"&gt; 
      &lt;h4&gt;‚ö° Quality Assurance&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Automated testing and validation throughout&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;h3&gt;üì¶ &lt;strong&gt;Step 1: Installation&lt;/strong&gt;&lt;/h3&gt; 
&lt;h4&gt;‚ö° &lt;strong&gt;Direct Installation (Recommended)&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# üöÄ Install DeepCode package directly
pip install deepcode-hku

# üîë Download configuration files
curl -O https://raw.githubusercontent.com/HKUDS/DeepCode/main/mcp_agent.config.yaml
curl -O https://raw.githubusercontent.com/HKUDS/DeepCode/main/mcp_agent.secrets.yaml

# üîë Configure API keys (required)
# Edit mcp_agent.secrets.yaml with your API keys and base_url:
# - openai: api_key, base_url (for OpenAI/custom endpoints)
# - anthropic: api_key (for Claude models)

# üîë Configure search API keys for web search (optional)
# Edit mcp_agent.config.yaml to set your API keys:
# - For Brave Search: Set BRAVE_API_KEY: "your_key_here" in brave.env section (line ~28)
# - For Bocha-MCP: Set BOCHA_API_KEY: "your_key_here" in bocha-mcp.env section (line ~74)

# üìÑ Configure document segmentation (optional)
# Edit mcp_agent.config.yaml to control document processing:
# - enabled: true/false (whether to use intelligent document segmentation)
# - size_threshold_chars: 50000 (document size threshold to trigger segmentation)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;üîß &lt;strong&gt;Development Installation (From Source)&lt;/strong&gt;&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìÇ Click to expand development installation options&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h5&gt;üî• &lt;strong&gt;Using UV (Recommended for Development)&lt;/strong&gt;&lt;/h5&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# üîΩ Clone the repository
git clone https://github.com/HKUDS/DeepCode.git
cd DeepCode/

# üì¶ Install UV package manager
curl -LsSf https://astral.sh/uv/install.sh | sh

# üîß Install dependencies with UV
uv venv --python=3.13
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
uv pip install -r requirements.txt

# üîë Configure API keys (required)
# Edit mcp_agent.secrets.yaml with your API keys and base_url:
# - openai: api_key, base_url (for OpenAI/custom endpoints)
# - anthropic: api_key (for Claude models)

# üîë Configure search API keys for web search (optional)
# Edit mcp_agent.config.yaml to set your API keys:
# - For Brave Search: Set BRAVE_API_KEY: "your_key_here" in brave.env section (line ~28)
# - For Bocha-MCP: Set BOCHA_API_KEY: "your_key_here" in bocha-mcp.env section (line ~74)

# üìÑ Configure document segmentation (optional)
# Edit mcp_agent.config.yaml to control document processing:
# - enabled: true/false (whether to use intelligent document segmentation)
# - size_threshold_chars: 50000 (document size threshold to trigger segmentation)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h5&gt;üêç &lt;strong&gt;Using Traditional pip&lt;/strong&gt;&lt;/h5&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# üîΩ Clone the repository
git clone https://github.com/HKUDS/DeepCode.git
cd DeepCode/

# üì¶ Install dependencies
pip install -r requirements.txt

# üîë Configure API keys (required)
# Edit mcp_agent.secrets.yaml with your API keys and base_url:
# - openai: api_key, base_url (for OpenAI/custom endpoints)
# - anthropic: api_key (for Claude models)

# üîë Configure search API keys for web search (optional)
# Edit mcp_agent.config.yaml to set your API keys:
# - For Brave Search: Set BRAVE_API_KEY: "your_key_here" in brave.env section (line ~28)
# - For Bocha-MCP: Set BOCHA_API_KEY: "your_key_here" in bocha-mcp.env section (line ~74)

# üìÑ Configure document segmentation (optional)
# Edit mcp_agent.config.yaml to control document processing:
# - enabled: true/false (whether to use intelligent document segmentation)
# - size_threshold_chars: 50000 (document size threshold to trigger segmentation)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;ü™ü &lt;strong&gt;Windows Users: Additional MCP Server Configuration&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;If you're using Windows, you may need to configure MCP servers manually in &lt;code&gt;mcp_agent.config.yaml&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Install MCP servers globally
npm i -g @modelcontextprotocol/server-brave-search
npm i -g @modelcontextprotocol/server-filesystem

# 2. Find your global node_modules path
npm -g root
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then update your &lt;code&gt;mcp_agent.config.yaml&lt;/code&gt; to use absolute paths:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;mcp:
  servers:
    brave:
      command: "node"
      args: ["C:/Program Files/nodejs/node_modules/@modelcontextprotocol/server-brave-search/dist/index.js"]
    filesystem:
      command: "node"
      args: ["C:/Program Files/nodejs/node_modules/@modelcontextprotocol/server-filesystem/dist/index.js", "."]
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Replace the path with your actual global node_modules path from step 2.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;üîç &lt;strong&gt;Search Server Configuration (Optional)&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;DeepCode supports multiple search servers for web search functionality. You can configure your preferred option in &lt;code&gt;mcp_agent.config.yaml&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# Default search server configuration
# Options: "brave" or "bocha-mcp"
default_search_server: "brave"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Available Options:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üîç Brave Search&lt;/strong&gt; (&lt;code&gt;"brave"&lt;/code&gt;):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Default option with high-quality search results&lt;/li&gt; 
   &lt;li&gt;Requires BRAVE_API_KEY configuration&lt;/li&gt; 
   &lt;li&gt;Recommended for most users&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üåê Bocha-MCP&lt;/strong&gt; (&lt;code&gt;"bocha-mcp"&lt;/code&gt;):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Alternative search server option&lt;/li&gt; 
   &lt;li&gt;Requires BOCHA_API_KEY configuration&lt;/li&gt; 
   &lt;li&gt;Uses local Python server implementation&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;API Key Configuration in mcp_agent.config.yaml:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# For Brave Search (default) - around line 28
brave:
  command: "npx"
  args: ["-y", "@modelcontextprotocol/server-brave-search"]
  env:
    BRAVE_API_KEY: "your_brave_api_key_here"

# For Bocha-MCP (alternative) - around line 74
bocha-mcp:
  command: "python"
  args: ["tools/bocha_search_server.py"]
  env:
    PYTHONPATH: "."
    BOCHA_API_KEY: "your_bocha_api_key_here"
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üí° Tip&lt;/strong&gt;: Both search servers require API key configuration. Choose the one that best fits your API access and requirements.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;‚ö° &lt;strong&gt;Step 2: Launch Application&lt;/strong&gt;&lt;/h3&gt; 
&lt;h4&gt;üöÄ &lt;strong&gt;Using Installed Package (Recommended)&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# üåê Launch web interface directly
deepcode

# The application will automatically start at http://localhost:8501
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;üõ†Ô∏è &lt;strong&gt;Using Source Code&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;Choose your preferred interface:&lt;/p&gt; 
&lt;h5&gt;üåê &lt;strong&gt;Web Interface&lt;/strong&gt; (Recommended)&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Using UV
uv run streamlit run ui/streamlit_app.py
# Or using traditional Python
streamlit run ui/streamlit_app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://img.shields.io/badge/Access-localhost:8501-00d4ff?style=flat-square&amp;amp;logo=streamlit&amp;amp;logoColor=white" alt="Web Access" /&gt; 
&lt;/div&gt; 
&lt;h5&gt;üñ•Ô∏è &lt;strong&gt;CLI Interface&lt;/strong&gt; (Advanced Users)&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Using UV
uv run python cli/main_cli.py
# Or using traditional Python
python cli/main_cli.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://img.shields.io/badge/Mode-Interactive_Terminal-9b59b6?style=flat-square&amp;amp;logo=terminal&amp;amp;logoColor=white" alt="CLI Mode" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;üéØ &lt;strong&gt;Step 3: Generate Code&lt;/strong&gt;&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;üìÑ Input&lt;/strong&gt;: Upload your research paper, provide requirements, or paste a URL&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ü§ñ Processing&lt;/strong&gt;: Watch the multi-agent system analyze and plan&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;‚ö° Output&lt;/strong&gt;: Receive production-ready code with tests and documentation&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üí° Examples&lt;/h2&gt; 
&lt;h3&gt;üé¨ &lt;strong&gt;Live Demonstrations&lt;/strong&gt;&lt;/h3&gt; 
&lt;table align="center"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="33%" align="center"&gt; &lt;h4&gt;üìÑ &lt;strong&gt;Paper2Code Demo&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Research to Implementation&lt;/strong&gt;&lt;/p&gt; 
    &lt;div align="center"&gt; 
     &lt;a href="https://www.youtube.com/watch?v=MQZYpLkzsbw"&gt; &lt;img src="https://img.youtube.com/vi/MQZYpLkzsbw/maxresdefault.jpg" alt="Paper2Code Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);" /&gt; &lt;/a&gt; 
     &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=MQZYpLkzsbw"&gt;‚ñ∂Ô∏è Watch Demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
     &lt;p&gt;&lt;em&gt;Transform academic papers into production-ready code automatically&lt;/em&gt;&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="33%" align="center"&gt; &lt;h4&gt;üñºÔ∏è &lt;strong&gt;Image Processing Demo&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;AI-Powered Image Tools&lt;/strong&gt;&lt;/p&gt; 
    &lt;div align="center"&gt; 
     &lt;a href="https://www.youtube.com/watch?v=nFt5mLaMEac"&gt; &lt;img src="https://img.youtube.com/vi/nFt5mLaMEac/maxresdefault.jpg" alt="Image Processing Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);" /&gt; &lt;/a&gt; 
     &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=nFt5mLaMEac"&gt;‚ñ∂Ô∏è Watch Demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
     &lt;p&gt;&lt;em&gt;Intelligent image processing with background removal and enhancement&lt;/em&gt;&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="33%" align="center"&gt; &lt;h4&gt;üåê &lt;strong&gt;Frontend Implementation&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Complete Web Application&lt;/strong&gt;&lt;/p&gt; 
    &lt;div align="center"&gt; 
     &lt;a href="https://www.youtube.com/watch?v=78wx3dkTaAU"&gt; &lt;img src="https://img.youtube.com/vi/78wx3dkTaAU/maxresdefault.jpg" alt="Frontend Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);" /&gt; &lt;/a&gt; 
     &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=78wx3dkTaAU"&gt;‚ñ∂Ô∏è Watch Demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
     &lt;p&gt;&lt;em&gt;Full-stack web development from concept to deployment&lt;/em&gt;&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;üÜï &lt;strong&gt;Recent Updates&lt;/strong&gt;&lt;/h3&gt; 
&lt;h4&gt;üìÑ &lt;strong&gt;Smart Document Segmentation (v1.2.0)&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent Processing&lt;/strong&gt;: Automatically handles large research papers and technical documents that exceed LLM token limits&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configurable Control&lt;/strong&gt;: Toggle segmentation via configuration with size-based thresholds&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Semantic Analysis&lt;/strong&gt;: Advanced content understanding with algorithm, concept, and formula preservation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backward Compatibility&lt;/strong&gt;: Seamlessly falls back to traditional processing for smaller documents&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üöÄ &lt;strong&gt;Coming Soon&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;We're continuously enhancing DeepCode with exciting new features:&lt;/p&gt; 
&lt;h4&gt;üîß &lt;strong&gt;Enhanced Code Reliability &amp;amp; Validation&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Automated Testing&lt;/strong&gt;: Comprehensive functionality testing with execution verification and error detection.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code Quality Assurance&lt;/strong&gt;: Multi-level validation through static analysis, dynamic testing, and performance benchmarking.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Debugging&lt;/strong&gt;: AI-powered error detection with automatic correction suggestions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;üìä &lt;strong&gt;PaperBench Performance Showcase&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Benchmark Dashboard&lt;/strong&gt;: Comprehensive performance metrics on the PaperBench evaluation suite.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Accuracy Metrics&lt;/strong&gt;: Detailed comparison with state-of-the-art paper reproduction systems.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Success Analytics&lt;/strong&gt;: Statistical analysis across paper categories and complexity levels.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;‚ö° &lt;strong&gt;System-wide Optimizations&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Performance Boost&lt;/strong&gt;: Multi-threaded processing and optimized agent coordination for faster generation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced Reasoning&lt;/strong&gt;: Advanced reasoning capabilities with improved context understanding.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Expanded Support&lt;/strong&gt;: Extended compatibility with additional programming languages and frameworks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚≠ê Star History&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;em&gt;Community Growth Trajectory&lt;/em&gt;&lt;/p&gt; 
 &lt;a href="https://star-history.com/#HKUDS/DeepCode&amp;amp;Date"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=HKUDS/DeepCode&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=HKUDS/DeepCode&amp;amp;type=Date" /&gt; 
   &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=HKUDS/DeepCode&amp;amp;type=Date" style="border-radius: 15px; box-shadow: 0 0 30px rgba(0, 217, 255, 0.3);" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üöÄ &lt;strong&gt;Ready to Transform Development?&lt;/strong&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-quick-start"&gt;&lt;img src="https://img.shields.io/badge/üöÄ_Get_Started-00d4ff?style=for-the-badge&amp;amp;logo=rocket&amp;amp;logoColor=white" alt="Get Started" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS"&gt;&lt;img src="https://img.shields.io/badge/üèõÔ∏è_View_on_GitHub-00d4ff?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" alt="View on GitHub" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/deepcode-agent"&gt;&lt;img src="https://img.shields.io/badge/‚≠ê_Star_Project-00d4ff?style=for-the-badge&amp;amp;logo=star&amp;amp;logoColor=white" alt="Star Project" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;üìÑ &lt;strong&gt;License&lt;/strong&gt;&lt;/h3&gt; 
 &lt;img src="https://img.shields.io/badge/License-MIT-4ecdc4?style=for-the-badge&amp;amp;logo=opensourceinitiative&amp;amp;logoColor=white" alt="MIT License" /&gt; 
 &lt;p&gt;&lt;strong&gt;MIT License&lt;/strong&gt; - Copyright (c) 2025 Data Intelligence Lab, The University of Hong Kong&lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;img src="https://visitor-badge.laobi.icu/badge?page_id=deepcode.readme&amp;amp;style=for-the-badge&amp;amp;color=00d4ff" alt="Visitors" /&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>virattt/ai-hedge-fund</title>
      <link>https://github.com/virattt/ai-hedge-fund</link>
      <description>&lt;p&gt;An AI Hedge Fund Team&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Hedge Fund&lt;/h1&gt; 
&lt;p&gt;This is a proof of concept for an AI-powered hedge fund. The goal of this project is to explore the use of AI to make trading decisions. This project is for &lt;strong&gt;educational&lt;/strong&gt; purposes only and is not intended for real trading or investment.&lt;/p&gt; 
&lt;p&gt;This system employs several agents working together:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Aswath Damodaran Agent - The Dean of Valuation, focuses on story, numbers, and disciplined valuation&lt;/li&gt; 
 &lt;li&gt;Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety&lt;/li&gt; 
 &lt;li&gt;Bill Ackman Agent - An activist investor, takes bold positions and pushes for change&lt;/li&gt; 
 &lt;li&gt;Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption&lt;/li&gt; 
 &lt;li&gt;Charlie Munger Agent - Warren Buffett's partner, only buys wonderful businesses at fair prices&lt;/li&gt; 
 &lt;li&gt;Michael Burry Agent - The Big Short contrarian who hunts for deep value&lt;/li&gt; 
 &lt;li&gt;Mohnish Pabrai Agent - The Dhandho investor, who looks for doubles at low risk&lt;/li&gt; 
 &lt;li&gt;Peter Lynch Agent - Practical investor who seeks "ten-baggers" in everyday businesses&lt;/li&gt; 
 &lt;li&gt;Phil Fisher Agent - Meticulous growth investor who uses deep "scuttlebutt" research&lt;/li&gt; 
 &lt;li&gt;Rakesh Jhunjhunwala Agent - The Big Bull of India&lt;/li&gt; 
 &lt;li&gt;Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential&lt;/li&gt; 
 &lt;li&gt;Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price&lt;/li&gt; 
 &lt;li&gt;Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Sentiment Agent - Analyzes market sentiment and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Fundamentals Agent - Analyzes fundamental data and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Technicals Agent - Analyzes technical indicators and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Risk Manager - Calculates risk metrics and sets position limits&lt;/li&gt; 
 &lt;li&gt;Portfolio Manager - Makes final trading decisions and generates orders&lt;/li&gt; 
&lt;/ol&gt; 
&lt;img width="1042" alt="Screenshot 2025-03-22 at 6 19 07 PM" src="https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4" /&gt; 
&lt;p&gt;Note: the system does not actually make any trades.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://twitter.com/virattt"&gt;&lt;img src="https://img.shields.io/twitter/follow/virattt?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This project is for &lt;strong&gt;educational and research purposes only&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Not intended for real trading or investment&lt;/li&gt; 
 &lt;li&gt;No investment advice or guarantees provided&lt;/li&gt; 
 &lt;li&gt;Creator assumes no liability for financial losses&lt;/li&gt; 
 &lt;li&gt;Consult a financial advisor for investment decisions&lt;/li&gt; 
 &lt;li&gt;Past performance does not indicate future results&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By using this software, you agree to use it solely for learning purposes.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-install"&gt;How to Install&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-run"&gt;How to Run&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#%EF%B8%8F-command-line-interface"&gt;‚å®Ô∏è Command Line Interface&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#%EF%B8%8F-web-application"&gt;üñ•Ô∏è Web Application&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-contribute"&gt;How to Contribute&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#feature-requests"&gt;Feature Requests&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How to Install&lt;/h2&gt; 
&lt;p&gt;Before you can run the AI Hedge Fund, you'll need to install it and set up your API keys. These steps are common to both the full-stack web application and command line interface.&lt;/p&gt; 
&lt;h3&gt;1. Clone the Repository&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Set up API keys&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file for your API keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create .env file for your API keys (in the root directory)
cp .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open and edit the &lt;code&gt;.env&lt;/code&gt; file to add your API keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
OPENAI_API_KEY=your-openai-api-key

# For getting financial data to power the hedge fund
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: You must set at least one LLM API key (e.g. &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;, &lt;code&gt;GROQ_API_KEY&lt;/code&gt;, &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;, or &lt;code&gt;DEEPSEEK_API_KEY&lt;/code&gt;) for the hedge fund to work.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Financial Data&lt;/strong&gt;: Data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key. For any other ticker, you will need to set the &lt;code&gt;FINANCIAL_DATASETS_API_KEY&lt;/code&gt; in the .env file.&lt;/p&gt; 
&lt;h2&gt;How to Run&lt;/h2&gt; 
&lt;h3&gt;‚å®Ô∏è Command Line Interface&lt;/h3&gt; 
&lt;p&gt;You can run the AI Hedge Fund directly via terminal. This approach offers more granular control and is useful for automation, scripting, and integration purposes.&lt;/p&gt; 
&lt;img width="992" alt="Screenshot 2025-01-06 at 5 50 17 PM" src="https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b" /&gt; 
&lt;h4&gt;Quick Start&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install Poetry (if not already installed):&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -sSL https://install.python-poetry.org | python3 -
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install dependencies:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run the AI Hedge Fund&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also specify a &lt;code&gt;--ollama&lt;/code&gt; flag to run the AI hedge fund using local LLMs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA --ollama
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can optionally specify the start and end dates to make decisions over a specific time period.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run the Backtester&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Example Output:&lt;/strong&gt; &lt;img width="941" alt="Screenshot 2025-01-06 at 5 47 52 PM" src="https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47" /&gt;&lt;/p&gt; 
&lt;p&gt;Note: The &lt;code&gt;--ollama&lt;/code&gt;, &lt;code&gt;--start-date&lt;/code&gt;, and &lt;code&gt;--end-date&lt;/code&gt; flags work for the backtester, as well!&lt;/p&gt; 
&lt;h3&gt;üñ•Ô∏è Web Application&lt;/h3&gt; 
&lt;p&gt;The new way to run the AI Hedge Fund is through our web application that provides a user-friendly interface. This is recommended for users who prefer visual interfaces over command line tools.&lt;/p&gt; 
&lt;p&gt;Please see detailed instructions on how to install and run the web application &lt;a href="https://github.com/virattt/ai-hedge-fund/tree/main/app"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;img width="1721" alt="Screenshot 2025-06-28 at 6 41 03‚ÄØPM" src="https://github.com/user-attachments/assets/b95ab696-c9f4-416c-9ad1-51feb1f5374b" /&gt; 
&lt;h2&gt;How to Contribute&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repository&lt;/li&gt; 
 &lt;li&gt;Create a feature branch&lt;/li&gt; 
 &lt;li&gt;Commit your changes&lt;/li&gt; 
 &lt;li&gt;Push to the branch&lt;/li&gt; 
 &lt;li&gt;Create a Pull Request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Please keep your pull requests small and focused. This will make it easier to review and merge.&lt;/p&gt; 
&lt;h2&gt;Feature Requests&lt;/h2&gt; 
&lt;p&gt;If you have a feature request, please open an &lt;a href="https://github.com/virattt/ai-hedge-fund/issues"&gt;issue&lt;/a&gt; and make sure it is tagged with &lt;code&gt;enhancement&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>localstack/localstack</title>
      <link>https://github.com/localstack/localstack</link>
      <description>&lt;p&gt;üíª A fully functional local AWS cloud stack. Develop and test your cloud &amp; Serverless apps offline&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;span&gt;‚ö°&lt;/span&gt; We are thrilled to announce the release of &lt;a href="https://blog.localstack.cloud/localstack-for-aws-release-v-4-8-0/"&gt;LocalStack 4.8&lt;/a&gt; &lt;span&gt;‚ö°&lt;/span&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/localstack/localstack/main/docs/localstack-readme-banner.svg?sanitize=true" alt="LocalStack - The Leading Platform for Local Cloud Development" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/localstack/localstack/actions/workflows/aws-main.yml?query=branch%3Amain"&gt;&lt;img alt="GitHub Actions" src="https://github.com/localstack/localstack/actions/workflows/aws-main.yml/badge.svg?branch=main" /&gt;&lt;/a&gt; &lt;a href="https://coveralls.io/github/localstack/localstack?branch=main"&gt;&lt;img alt="Coverage Status" src="https://coveralls.io/repos/github/localstack/localstack/badge.svg?branch=main" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/localstack/"&gt;&lt;img alt="PyPI Version" src="https://img.shields.io/pypi/v/localstack?color=blue" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/localstack/localstack"&gt;&lt;img alt="Docker Pulls" src="https://img.shields.io/docker/pulls/localstack/localstack" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/localstack"&gt;&lt;img alt="PyPi downloads" src="https://static.pepy.tech/badge/localstack" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/localstack/localstack/main/#backers"&gt;&lt;img alt="Backers on Open Collective" src="https://opencollective.com/localstack/backers/badge.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/localstack/localstack/main/#sponsors"&gt;&lt;img alt="Sponsors on Open Collective" src="https://opencollective.com/localstack/sponsors/badge.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://img.shields.io/pypi/l/localstack.svg"&gt;&lt;img alt="PyPI License" src="https://img.shields.io/pypi/l/localstack.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://github.com/psf/black"&gt;&lt;img alt="Code style: black" src="https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://github.com/astral-sh/ruff"&gt;&lt;img alt="Ruff" src="https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json" /&gt;&lt;/a&gt; &lt;a href="https://bsky.app/profile/localstack.cloud"&gt;&lt;img alt="Bluesky" src="https://img.shields.io/badge/bluesky-Follow-blue?logo=bluesky" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; LocalStack is a cloud software development framework to develop and test your AWS applications locally. &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/localstack/localstack/main/#overview"&gt;Overview&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/localstack/localstack/main/#install"&gt;Install&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/localstack/localstack/main/#quickstart"&gt;Quickstart&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/localstack/localstack/main/#running"&gt;Run&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/localstack/localstack/main/#usage"&gt;Usage&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/localstack/localstack/main/#releases"&gt;Releases&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/localstack/localstack/main/#contributing"&gt;Contributing&lt;/a&gt; &lt;br /&gt; &lt;a href="https://docs.localstack.cloud" target="_blank"&gt;üìñ Docs&lt;/a&gt; ‚Ä¢ &lt;a href="https://app.localstack.cloud" target="_blank"&gt;üíª Pro version&lt;/a&gt; ‚Ä¢ &lt;a href="https://docs.localstack.cloud/references/coverage/" target="_blank"&gt;‚òëÔ∏è LocalStack coverage&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;Overview&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://localstack.cloud"&gt;LocalStack&lt;/a&gt; is a cloud service emulator that runs in a single container on your laptop or in your CI environment. With LocalStack, you can run your AWS applications or Lambdas entirely on your local machine without connecting to a remote cloud provider! Whether you are testing complex CDK applications or Terraform configurations, or just beginning to learn about AWS services, LocalStack helps speed up and simplify your testing and development workflow.&lt;/p&gt; 
&lt;p&gt;LocalStack supports a growing number of AWS services, like AWS Lambda, S3, DynamoDB, Kinesis, SQS, SNS, and many more! The &lt;a href="https://localstack.cloud/pricing"&gt;Pro version of LocalStack&lt;/a&gt; supports additional APIs and advanced features. You can find a comprehensive list of supported APIs on our &lt;a href="https://docs.localstack.cloud/user-guide/aws/feature-coverage/"&gt;‚òëÔ∏è Feature Coverage&lt;/a&gt; page.&lt;/p&gt; 
&lt;p&gt;LocalStack also provides additional features to make your life as a cloud developer easier! Check out LocalStack's &lt;a href="https://docs.localstack.cloud/user-guide/"&gt;User Guides&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Install&lt;/h2&gt; 
&lt;p&gt;The quickest way to get started with LocalStack is by using the LocalStack CLI. It enables you to start and manage the LocalStack Docker container directly through your command line. Ensure that your machine has a functional &lt;a href="https://docs.docker.com/get-docker/"&gt;&lt;code&gt;docker&lt;/code&gt; environment&lt;/a&gt; installed before proceeding.&lt;/p&gt; 
&lt;h3&gt;Brew (macOS or Linux with Homebrew)&lt;/h3&gt; 
&lt;p&gt;Install the LocalStack CLI through our &lt;a href="https://github.com/localstack/homebrew-tap"&gt;official LocalStack Brew Tap&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew install localstack/tap/localstack-cli
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Binary download (macOS, Linux, Windows)&lt;/h3&gt; 
&lt;p&gt;If Brew is not installed on your machine, you can download the pre-built LocalStack CLI binary directly:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Visit &lt;a href="https://github.com/localstack/localstack-cli/releases/latest"&gt;localstack/localstack-cli&lt;/a&gt; and download the latest release for your platform.&lt;/li&gt; 
 &lt;li&gt;Extract the downloaded archive to a directory included in your &lt;code&gt;PATH&lt;/code&gt; variable: 
  &lt;ul&gt; 
   &lt;li&gt;For macOS/Linux, use the command: &lt;code&gt;sudo tar xvzf ~/Downloads/localstack-cli-*-darwin-*-onefile.tar.gz -C /usr/local/bin&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;PyPI (macOS, Linux, Windows)&lt;/h3&gt; 
&lt;p&gt;LocalStack is developed using Python. To install the LocalStack CLI using &lt;code&gt;pip&lt;/code&gt;, run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3 -m pip install localstack
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;localstack-cli&lt;/code&gt; installation enables you to run the Docker image containing the LocalStack runtime. To interact with the local AWS services, you need to install the &lt;code&gt;awslocal&lt;/code&gt; CLI separately. For installation guidelines, refer to the &lt;a href="https://docs.localstack.cloud/user-guide/integrations/aws-cli/#localstack-aws-cli-awslocal"&gt;&lt;code&gt;awslocal&lt;/code&gt; documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Do not use &lt;code&gt;sudo&lt;/code&gt; or run as &lt;code&gt;root&lt;/code&gt; user. LocalStack must be installed and started entirely under a local non-root user. If you have problems with permissions in macOS High Sierra, install with &lt;code&gt;pip install --user localstack&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;Start LocalStack inside a Docker container by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt; % localstack start -d

     __                     _______ __             __
    / /   ____  _________ _/ / ___// /_____ ______/ /__
   / /   / __ \/ ___/ __ `/ /\__ \/ __/ __ `/ ___/ //_/
  / /___/ /_/ / /__/ /_/ / /___/ / /_/ /_/ / /__/ ,&amp;lt;
 /_____/\____/\___/\__,_/_//____/\__/\__,_/\___/_/|_|

- LocalStack CLI: 4.8.0
- Profile: default
- App: https://app.localstack.cloud

[17:00:15] starting LocalStack in Docker mode üê≥               localstack.py:512
           preparing environment                               bootstrap.py:1322
           configuring container                               bootstrap.py:1330
           starting container                                  bootstrap.py:1340
[17:00:16] detaching                                           bootstrap.py:1344
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can query the status of respective services on LocalStack by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;% localstack status services
‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ Service                  ‚îÉ Status      ‚îÉ
‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©
‚îÇ acm                      ‚îÇ ‚úî available ‚îÇ
‚îÇ apigateway               ‚îÇ ‚úî available ‚îÇ
‚îÇ cloudformation           ‚îÇ ‚úî available ‚îÇ
‚îÇ cloudwatch               ‚îÇ ‚úî available ‚îÇ
‚îÇ config                   ‚îÇ ‚úî available ‚îÇ
‚îÇ dynamodb                 ‚îÇ ‚úî available ‚îÇ
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use SQS, a fully managed distributed message queuing service, on LocalStack, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;% awslocal sqs create-queue --queue-name sample-queue
{
    "QueueUrl": "http://sqs.us-east-1.localhost.localstack.cloud:4566/000000000000/sample-queue"
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Learn more about &lt;a href="https://docs.localstack.cloud/references/coverage/"&gt;LocalStack AWS services&lt;/a&gt; and using them with LocalStack's &lt;code&gt;awslocal&lt;/code&gt; CLI.&lt;/p&gt; 
&lt;h2&gt;Running&lt;/h2&gt; 
&lt;p&gt;You can run LocalStack through the following options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.localstack.cloud/getting-started/installation/#localstack-cli"&gt;LocalStack CLI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.localstack.cloud/getting-started/installation/#docker"&gt;Docker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.localstack.cloud/getting-started/installation/#docker-compose"&gt;Docker Compose&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.localstack.cloud/getting-started/installation/#helm"&gt;Helm&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;To start using LocalStack, check out our &lt;a href="https://docs.localstack.cloud"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.localstack.cloud/references/configuration/"&gt;LocalStack Configuration&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.localstack.cloud/user-guide/ci/"&gt;LocalStack in CI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.localstack.cloud/user-guide/integrations/"&gt;LocalStack Integrations&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.localstack.cloud/user-guide/tools/"&gt;LocalStack Tools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.localstack.cloud/references/"&gt;Understanding LocalStack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.localstack.cloud/getting-started/faq/"&gt;Frequently Asked Questions&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To use LocalStack with a graphical user interface, you can use the following UI clients:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://app.localstack.cloud"&gt;LocalStack Web Application&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.localstack.cloud/user-guide/tools/localstack-desktop/"&gt;LocalStack Desktop&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.localstack.cloud/user-guide/tools/localstack-docker-extension/"&gt;LocalStack Docker Extension&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Releases&lt;/h2&gt; 
&lt;p&gt;Please refer to &lt;a href="https://github.com/localstack/localstack/releases"&gt;GitHub releases&lt;/a&gt; to see the complete list of changes for each release. For extended release notes, please refer to the &lt;a href="https://docs.localstack.cloud/references/changelog/"&gt;changelog&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;If you are interested in contributing to LocalStack:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Start by reading our &lt;a href="https://raw.githubusercontent.com/localstack/localstack/main/docs/CONTRIBUTING.md"&gt;contributing guide&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Check out our &lt;a href="https://raw.githubusercontent.com/localstack/localstack/main/docs/development-environment-setup/README.md"&gt;development environment setup guide&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Navigate our codebase and &lt;a href="https://github.com/localstack/localstack/issues"&gt;open issues&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We are thankful for all the contributions and feedback we receive.&lt;/p&gt; 
&lt;h2&gt;Get in touch&lt;/h2&gt; 
&lt;p&gt;Get in touch with the LocalStack Team to report üêû &lt;a href="https://github.com/localstack/localstack/issues/new/choose"&gt;issues&lt;/a&gt;, upvote üëç &lt;a href="https://github.com/localstack/localstack/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc+"&gt;feature requests&lt;/a&gt;, üôãüèΩ ask &lt;a href="https://docs.localstack.cloud/getting-started/help-and-support/"&gt;support questions&lt;/a&gt;, or üó£Ô∏è discuss local cloud development:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://localstack.cloud/slack/"&gt;LocalStack Slack Community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/localstack/localstack/issues"&gt;LocalStack GitHub Issue tracker&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributors&lt;/h3&gt; 
&lt;p&gt;We are thankful to all the people who have contributed to this project.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/localstack/localstack/graphs/contributors"&gt;&lt;img src="https://opencollective.com/localstack/contributors.svg?width=890" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Backers&lt;/h3&gt; 
&lt;p&gt;We are also grateful to all our backers who have donated to the project. You can become a backer on &lt;a href="https://opencollective.com/localstack#backer"&gt;Open Collective&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://opencollective.com/localstack#backers" target="_blank"&gt;&lt;img src="https://opencollective.com/localstack/backers.svg?width=890" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Sponsors&lt;/h3&gt; 
&lt;p&gt;You can also support this project by becoming a sponsor on &lt;a href="https://opencollective.com/localstack#sponsor"&gt;Open Collective&lt;/a&gt;. Your logo will show up here along with a link to your website.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://opencollective.com/localstack/sponsor/0/website" target="_blank"&gt;&lt;img src="https://opencollective.com/localstack/sponsor/0/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/localstack/sponsor/1/website" target="_blank"&gt;&lt;img src="https://opencollective.com/localstack/sponsor/1/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/localstack/sponsor/2/website" target="_blank"&gt;&lt;img src="https://opencollective.com/localstack/sponsor/2/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/localstack/sponsor/3/website" target="_blank"&gt;&lt;img src="https://opencollective.com/localstack/sponsor/3/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/localstack/sponsor/4/website" target="_blank"&gt;&lt;img src="https://opencollective.com/localstack/sponsor/4/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/localstack/sponsor/5/website" target="_blank"&gt;&lt;img src="https://opencollective.com/localstack/sponsor/5/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/localstack/sponsor/6/website" target="_blank"&gt;&lt;img src="https://opencollective.com/localstack/sponsor/6/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/localstack/sponsor/7/website" target="_blank"&gt;&lt;img src="https://opencollective.com/localstack/sponsor/7/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/localstack/sponsor/8/website" target="_blank"&gt;&lt;img src="https://opencollective.com/localstack/sponsor/8/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/localstack/sponsor/9/website" target="_blank"&gt;&lt;img src="https://opencollective.com/localstack/sponsor/9/avatar.svg?sanitize=true" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Copyright (c) 2017-2025 LocalStack maintainers and contributors.&lt;/p&gt; 
&lt;p&gt;Copyright (c) 2016 Atlassian and others.&lt;/p&gt; 
&lt;p&gt;This version of LocalStack is released under the Apache License, Version 2.0 (see &lt;a href="https://raw.githubusercontent.com/localstack/localstack/main/LICENSE.txt"&gt;LICENSE&lt;/a&gt;). By downloading and using this software you agree to the &lt;a href="https://raw.githubusercontent.com/localstack/localstack/main/docs/end_user_license_agreement"&gt;End-User License Agreement (EULA)&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>pytorch/torchtitan</title>
      <link>https://github.com/pytorch/torchtitan</link>
      <description>&lt;p&gt;A PyTorch native platform for training generative AI models&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;torchtitan&lt;/h1&gt; 
 &lt;h4&gt;A PyTorch native platform for training generative AI models&lt;/h4&gt; 
 &lt;p&gt;&lt;a href="https://github.com/pytorch/torchtitan/actions/workflows/integration_test_8gpu.yaml?query=branch%3Amain"&gt;&lt;img src="https://github.com/pytorch/torchtitan/actions/workflows/integration_test_8gpu_features.yaml/badge.svg?branch=main" alt="8 GPU Feature Tests" /&gt;&lt;/a&gt; &lt;a href="https://github.com/pytorch/torchtitan/actions/workflows/integration_test_8gpu.yaml?query=branch%3Amain"&gt;&lt;img src="https://github.com/pytorch/torchtitan/actions/workflows/integration_test_8gpu_models.yaml/badge.svg?branch=main" alt="8 GPU Model Tests" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2410.06511"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2410.06511-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://iclr.cc/virtual/2025/poster/29620"&gt;&lt;img src="https://img.shields.io/badge/ICLR-2025-violet.svg?sanitize=true" alt="ICLR" /&gt;&lt;/a&gt; &lt;a href="https://discuss.pytorch.org/c/distributed/torchtitan/44"&gt;&lt;img src="https://img.shields.io/badge/pytorch-forum-DE3412.svg?sanitize=true" alt="forum" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/pytorch/torchtitan/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-BSD_3--Clause-lightgrey.svg?sanitize=true" alt="license" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/torchtitan/"&gt;&lt;img src="https://img.shields.io/pypi/v/torchtitan?color=blue" alt="pip" /&gt;&lt;/a&gt; &lt;a href="https://anaconda.org/conda-forge/torchtitan"&gt;&lt;img src="https://img.shields.io/conda/vn/conda-forge/torchtitan?color=green" alt="conda" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;code&gt;torchtitan&lt;/code&gt; is currently in a pre-release state and under extensive development. We showcase training Llama 3.1 LLMs at scale, and are working on other types of generative AI models, including LLMs with MoE architectures, multimodal LLMs, and diffusion models, in the &lt;a href="https://raw.githubusercontent.com/pytorch/torchtitan/main/torchtitan/experiments"&gt;&lt;code&gt;experiments&lt;/code&gt;&lt;/a&gt; folder. To use the latest features of &lt;code&gt;torchtitan&lt;/code&gt;, we recommend using the most recent PyTorch nightly.&lt;/p&gt; 
&lt;h2&gt;Latest News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2025/07] We published &lt;a href="https://raw.githubusercontent.com/pytorch/torchtitan/main/torchtitan/models/README.md"&gt;instructions&lt;/a&gt; on how to add a model to &lt;code&gt;torchtitan&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/07] We released &lt;code&gt;torchtitan&lt;/code&gt; &lt;a href="https://github.com/pytorch/torchtitan/releases"&gt;v0.1.0&lt;/a&gt;, and also set up nightly builds.&lt;/li&gt; 
 &lt;li&gt;[2025/04] Our paper was accepted by &lt;a href="https://iclr.cc/virtual/2025/poster/29620"&gt;ICLR 2025&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/04] &lt;a href="https://raw.githubusercontent.com/pytorch/torchtitan/main/torchtitan/experiments/llama4/"&gt;Llama 4&lt;/a&gt; initial support is available as an experiment.&lt;/li&gt; 
 &lt;li&gt;[2025/04] Training the diffusion model &lt;a href="https://raw.githubusercontent.com/pytorch/torchtitan/main/torchtitan/experiments/flux/"&gt;FLUX&lt;/a&gt; with FSDP/HSDP is available as an experiment.&lt;/li&gt; 
 &lt;li&gt;[2025/04] The frontend implementation of &lt;a href="https://raw.githubusercontent.com/pytorch/torchtitan/main/torchtitan/experiments/simple_fsdp/"&gt;SimpleFSDP&lt;/a&gt;, a compiler-based FSDP framework, is available as an experiment.&lt;/li&gt; 
 &lt;li&gt;[2024/12] GPU MODE &lt;a href="https://www.youtube.com/watch?v=VYWRjcUqW6w"&gt;lecture&lt;/a&gt; on torchtitan.&lt;/li&gt; 
 &lt;li&gt;[2024/11] &lt;a href="https://www.alluxio.io/videos/ai-ml-infra-meetup-torchtitan-one-stop-pytorch-native-solution-for-production-ready-llm-pre-training"&gt;Presentation&lt;/a&gt; at an AI/ML Infra Meetup.&lt;/li&gt; 
 &lt;li&gt;[2024/07] &lt;a href="https://pytorch2024.sched.com/event/1fHn3"&gt;Presentation&lt;/a&gt; at PyTorch Conference 2024.&lt;/li&gt; 
 &lt;li&gt;[2024/04] &lt;a href="https://youtu.be/ee5DOEqD35I?si=_B94PbVv0V5ZnNKE"&gt;Intro video&lt;/a&gt; - learn more about &lt;code&gt;torchtitan&lt;/code&gt; in under 4 minutes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;torchtitan&lt;/code&gt; is a PyTorch native platform designed for &lt;strong&gt;rapid experimentation and large-scale training&lt;/strong&gt; of generative AI models. As a minimal clean-room implementation of PyTorch native scaling techniques, &lt;code&gt;torchtitan&lt;/code&gt; provides a flexible foundation for developers to build upon. With &lt;code&gt;torchtitan&lt;/code&gt; &lt;a href="https://raw.githubusercontent.com/pytorch/torchtitan/main/docs/extension.md"&gt;extension points&lt;/a&gt;, one can easily create custom extensions tailored to specific needs.&lt;/p&gt; 
&lt;p&gt;Our mission is to accelerate innovation in the field of generative AI by empowering researchers and developers to explore new modeling architectures and infrastructure techniques.&lt;/p&gt; 
&lt;p&gt;The Guiding Principles when building &lt;code&gt;torchtitan&lt;/code&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Designed to be easy to understand, use and extend for different training purposes.&lt;/li&gt; 
 &lt;li&gt;Minimal changes to the model code when applying multi-dimensional parallelism.&lt;/li&gt; 
 &lt;li&gt;Bias towards a clean, minimal codebase while providing basic reusable / swappable components.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;code&gt;torchtitan&lt;/code&gt; has been showcasing PyTorch's latest distributed training features, via pretraining Llama 3.1 LLMs of various sizes. To accelerate contributions to and innovations around torchtitan, we are hosting a new &lt;a href="https://raw.githubusercontent.com/pytorch/torchtitan/main/torchtitan/experiments"&gt;&lt;code&gt;experiments&lt;/code&gt;&lt;/a&gt; folder. We look forward to your contributions!&lt;/p&gt; 
&lt;h2&gt;Llama 3.1 pretraining&lt;/h2&gt; 
&lt;h3&gt;Key features available&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Multi-dimensional composable parallelisms 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pytorch/torchtitan/main/docs/fsdp.md"&gt;FSDP2&lt;/a&gt; with per-parameter sharding&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://pytorch.org/docs/stable/distributed.tensor.parallel.html"&gt;Tensor Parallel&lt;/a&gt; (including &lt;a href="https://discuss.pytorch.org/t/distributed-w-torchtitan-introducing-async-tensor-parallelism-in-pytorch/209487"&gt;async TP&lt;/a&gt;)&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://discuss.pytorch.org/t/distributed-w-torchtitan-training-with-zero-bubble-pipeline-parallelism/214420"&gt;Pipeline Parallel&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://discuss.pytorch.org/t/distributed-w-torchtitan-breaking-barriers-training-long-context-llms-with-1m-sequence-length-in-pytorch-using-context-parallel/215082"&gt;Context Parallel&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pytorch.org/docs/stable/meta.html"&gt;Meta device&lt;/a&gt; initialization&lt;/li&gt; 
 &lt;li&gt;Selective (layer or operator) and full activation checkpointing&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discuss.pytorch.org/t/distributed-w-torchtitan-optimizing-checkpointing-efficiency-with-pytorch-dcp/211250"&gt;Distributed checkpointing&lt;/a&gt; (including async checkpointing) 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pytorch/torchtitan/main/docs/checkpoint.md"&gt;Interoperable checkpoints&lt;/a&gt; which can be loaded directly into &lt;a href="https://github.com/pytorch/torchtune"&gt;&lt;code&gt;torchtune&lt;/code&gt;&lt;/a&gt; for fine-tuning&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;torch.compile&lt;/code&gt; support&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discuss.pytorch.org/t/distributed-w-torchtitan-enabling-float8-all-gather-in-fsdp2/209323"&gt;Float8&lt;/a&gt; support (&lt;a href="https://raw.githubusercontent.com/pytorch/torchtitan/main/docs/float8.md"&gt;how-to&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;DDP and HSDP&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pytorch/torchft"&gt;TorchFT&lt;/a&gt; integration&lt;/li&gt; 
 &lt;li&gt;Checkpointable data-loading, with the C4 dataset pre-configured (144M entries) and support for &lt;a href="https://raw.githubusercontent.com/pytorch/torchtitan/main/docs/datasets.md"&gt;custom datasets&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Gradient accumulation, enabled by giving an additional &lt;code&gt;--training.global_batch_size&lt;/code&gt; argument in configuration&lt;/li&gt; 
 &lt;li&gt;Flexible learning rate scheduler (warmup-stable-decay)&lt;/li&gt; 
 &lt;li&gt;Loss, GPU memory, throughput (tokens/sec), TFLOPs, and MFU displayed and logged via &lt;a href="https://raw.githubusercontent.com/pytorch/torchtitan/main/docs/metrics.md"&gt;Tensorboard or Weights &amp;amp; Biases&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pytorch/torchtitan/main/docs/debugging.md"&gt;Debugging tools&lt;/a&gt; including CPU/GPU profiling, memory profiling, Flight Recorder, etc.&lt;/li&gt; 
 &lt;li&gt;All options easily configured via &lt;a href="https://raw.githubusercontent.com/pytorch/torchtitan/main/torchtitan/models/llama3/train_configs/"&gt;toml files&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pytorch/torchtitan/main/scripts/"&gt;Helper scripts&lt;/a&gt; to 
  &lt;ul&gt; 
   &lt;li&gt;download tokenizers from Hugging Face&lt;/li&gt; 
   &lt;li&gt;convert original Llama 3 checkpoints into the expected DCP format&lt;/li&gt; 
   &lt;li&gt;estimate FSDP/HSDP memory usage without materializing the model&lt;/li&gt; 
   &lt;li&gt;run distributed inference with Tensor Parallel&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;We report &lt;a href="https://raw.githubusercontent.com/pytorch/torchtitan/main/benchmarks/llama3_h100_202412_torchtitan.md"&gt;performance&lt;/a&gt; on up to 512 GPUs, and verify &lt;a href="https://raw.githubusercontent.com/pytorch/torchtitan/main/docs/converging.md"&gt;loss converging&lt;/a&gt; correctness of various techniques.&lt;/p&gt; 
&lt;h3&gt;Dive into the code&lt;/h3&gt; 
&lt;p&gt;You may want to see how the model is defined or how parallelism techniques are applied. For a guided tour, see these files first:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pytorch/torchtitan/main/torchtitan/train.py"&gt;torchtitan/train.py&lt;/a&gt; - the main training loop and high-level setup code&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pytorch/torchtitan/main/torchtitan/models/llama3/model/model.py"&gt;torchtitan/models/llama3/model/model.py&lt;/a&gt; - the Llama 3.1 model definition&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pytorch/torchtitan/main/torchtitan/models/llama3/infra/parallelize.py"&gt;torchtitan/models/llama3/infra/parallelize.py&lt;/a&gt; - helpers for applying Data Parallel, Tensor Parallel, activation checkpointing, and &lt;code&gt;torch.compile&lt;/code&gt; to the model&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pytorch/torchtitan/main/torchtitan/models/llama3/infra/pipeline.py"&gt;torchtitan/models/llama3/infra/pipeline.py&lt;/a&gt; - helpers for applying Pipeline Parallel to the model&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pytorch/torchtitan/main/torchtitan/components/checkpoint.py"&gt;torchtitan/components/checkpoint.py&lt;/a&gt; - utils for saving/loading distributed checkpoints&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pytorch/torchtitan/main/torchtitan/components/quantization/float8.py"&gt;torchtitan/components/quantization/float8.py&lt;/a&gt; - utils for applying Float8 techniques&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;One can choose to install &lt;code&gt;torchtitan&lt;/code&gt; from a stable release, a nightly build, or directly run the source code. Please &lt;a href="https://pytorch.org/get-started/locally/"&gt;install PyTorch&lt;/a&gt; before proceeding.&lt;/p&gt; 
&lt;h3&gt;Stable releases&lt;/h3&gt; 
&lt;p&gt;One can install the latest &lt;a href="https://github.com/pytorch/torchtitan/releases"&gt;stable release&lt;/a&gt; of &lt;code&gt;torchtitan&lt;/code&gt; via &lt;code&gt;pip&lt;/code&gt; or &lt;code&gt;conda&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install torchtitan
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;conda install conda-forge::torchtitan
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that each stable release pins the nightly versions of &lt;code&gt;torch&lt;/code&gt; and &lt;code&gt;torchao&lt;/code&gt;. Please see &lt;a href="https://raw.githubusercontent.com/pytorch/torchtitan/main/docs/release.md"&gt;release.md&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;Nightly builds&lt;/h3&gt; 
&lt;p&gt;This method requires the nightly build of PyTorch. You can replace &lt;code&gt;cu126&lt;/code&gt; with another version of cuda (e.g. &lt;code&gt;cu128&lt;/code&gt;) or an AMD GPU (e.g. &lt;code&gt;rocm6.3&lt;/code&gt;).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu126 --force-reinstall
pip install --pre torchtitan --index-url https://download.pytorch.org/whl/nightly/cu126
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;From source&lt;/h3&gt; 
&lt;p&gt;This method requires the nightly build of PyTorch or the latest PyTorch built &lt;a href="https://github.com/pytorch/pytorch?tab=readme-ov-file#from-source"&gt;from source&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/pytorch/torchtitan
cd torchtitan
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Downloading a tokenizer&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;torchtitan&lt;/code&gt; currently supports training Llama 3.1 (8B, 70B, 405B) out of the box. To get started training these models, we need to download the tokenizer. Follow the instructions on the official &lt;a href="https://huggingface.co/meta-llama/Llama-3.1-8B"&gt;meta-llama&lt;/a&gt; repository to ensure you have access to the Llama model weights.&lt;/p&gt; 
&lt;p&gt;Once you have confirmed access, you can run the following command to download the Llama 3.1 tokenizer to your local machine.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Get your HF token from https://huggingface.co/settings/tokens

# Llama 3.1 tokenizer
python scripts/download_hf_assets.py --repo_id meta-llama/Llama-3.1-8B --assets tokenizer --hf_token=...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Start a training run&lt;/h3&gt; 
&lt;p&gt;Llama 3 8B model locally on 8 GPUs&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CONFIG_FILE="./torchtitan/models/llama3/train_configs/llama3_8b.toml" ./run_train.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Multi-Node Training&lt;/h3&gt; 
&lt;p&gt;For training on ParallelCluster/Slurm type configurations, you can use the &lt;code&gt;multinode_trainer.slurm&lt;/code&gt; file to submit your sbatch job.&lt;/p&gt; 
&lt;p&gt;To get started adjust the number of nodes and GPUs&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;#SBATCH --ntasks=2
#SBATCH --nodes=2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then start a run where &lt;code&gt;nnodes&lt;/code&gt; is your total node count, matching the sbatch node count above.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;srun torchrun --nnodes 2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If your gpu count per node is not 8, adjust &lt;code&gt;--nproc_per_node&lt;/code&gt; in the torchrun command and &lt;code&gt;#SBATCH --gpus-per-task&lt;/code&gt; in the SBATCH command section.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;We provide a detailed look into the parallelisms and optimizations available in &lt;code&gt;torchtitan&lt;/code&gt;, along with summary advice on when to use various techniques.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://openreview.net/forum?id=SFN6Wm7YBI"&gt;TorchTitan: One-stop PyTorch native solution for production ready LLM pre-training&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@inproceedings{
   liang2025torchtitan,
   title={TorchTitan: One-stop PyTorch native solution for production ready {LLM} pretraining},
   author={Wanchao Liang and Tianyu Liu and Less Wright and Will Constable and Andrew Gu and Chien-Chin Huang and Iris Zhang and Wei Feng and Howard Huang and Junjie Wang and Sanket Purandare and Gokul Nadathur and Stratos Idreos},
   booktitle={The Thirteenth International Conference on Learning Representations},
   year={2025},
   url={https://openreview.net/forum?id=SFN6Wm7YBI}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Source code is made available under a &lt;a href="https://raw.githubusercontent.com/pytorch/torchtitan/main/LICENSE"&gt;BSD 3 license&lt;/a&gt;, however you may have other legal obligations that govern your use of other content linked in this repository, such as the license or terms of service for third-party data and models.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>facebookresearch/detectron2</title>
      <link>https://github.com/facebookresearch/detectron2</link>
      <description>&lt;p&gt;Detectron2 is a platform for object detection, segmentation and other visual recognition tasks.&lt;/p&gt;&lt;hr&gt;&lt;img src="https://raw.githubusercontent.com/facebookresearch/detectron2/main/.github/Detectron2-Logo-Horz.svg?sanitize=true" width="300" /&gt; 
&lt;p&gt;Detectron2 is Facebook AI Research's next generation library that provides state-of-the-art detection and segmentation algorithms. It is the successor of &lt;a href="https://github.com/facebookresearch/Detectron/"&gt;Detectron&lt;/a&gt; and &lt;a href="https://github.com/facebookresearch/maskrcnn-benchmark/"&gt;maskrcnn-benchmark&lt;/a&gt;. It supports a number of computer vision research projects and production applications in Facebook.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://user-images.githubusercontent.com/1381301/66535560-d3422200-eace-11e9-9123-5535d469db19.png" /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2&gt;Learn More about Detectron2&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Includes new capabilities such as panoptic segmentation, Densepose, Cascade R-CNN, rotated bounding boxes, PointRend, DeepLab, ViTDet, MViTv2 etc.&lt;/li&gt; 
 &lt;li&gt;Used as a library to support building &lt;a href="https://raw.githubusercontent.com/facebookresearch/detectron2/main/projects/"&gt;research projects&lt;/a&gt; on top of it.&lt;/li&gt; 
 &lt;li&gt;Models can be exported to TorchScript format or Caffe2 format for deployment.&lt;/li&gt; 
 &lt;li&gt;It &lt;a href="https://detectron2.readthedocs.io/notes/benchmarks.html"&gt;trains much faster&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See our &lt;a href="https://ai.meta.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-/"&gt;blog post&lt;/a&gt; to see more demos. See this &lt;a href="https://ai.meta.com/blog/detectron-everingham-prize/"&gt;interview&lt;/a&gt; to learn more about the stories behind detectron2.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://detectron2.readthedocs.io/tutorials/install.html"&gt;installation instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://detectron2.readthedocs.io/tutorials/getting_started.html"&gt;Getting Started with Detectron2&lt;/a&gt;, and the &lt;a href="https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5"&gt;Colab Notebook&lt;/a&gt; to learn about basic usage.&lt;/p&gt; 
&lt;p&gt;Learn more at our &lt;a href="https://detectron2.readthedocs.org"&gt;documentation&lt;/a&gt;. And see &lt;a href="https://raw.githubusercontent.com/facebookresearch/detectron2/main/projects/"&gt;projects/&lt;/a&gt; for some projects that are built on top of detectron2.&lt;/p&gt; 
&lt;h2&gt;Model Zoo and Baselines&lt;/h2&gt; 
&lt;p&gt;We provide a large set of baseline results and trained models available for download in the &lt;a href="https://raw.githubusercontent.com/facebookresearch/detectron2/main/MODEL_ZOO.md"&gt;Detectron2 Model Zoo&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Detectron2 is released under the &lt;a href="https://raw.githubusercontent.com/facebookresearch/detectron2/main/LICENSE"&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Citing Detectron2&lt;/h2&gt; 
&lt;p&gt;If you use Detectron2 in your research or wish to refer to the baseline results published in the &lt;a href="https://raw.githubusercontent.com/facebookresearch/detectron2/main/MODEL_ZOO.md"&gt;Model Zoo&lt;/a&gt;, please use the following BibTeX entry.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;@misc{wu2019detectron2,
  author =       {Yuxin Wu and Alexander Kirillov and Francisco Massa and
                  Wan-Yen Lo and Ross Girshick},
  title =        {Detectron2},
  howpublished = {\url{https://github.com/facebookresearch/detectron2}},
  year =         {2019}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>dreammis/social-auto-upload</title>
      <link>https://github.com/dreammis/social-auto-upload</link>
      <description>&lt;p&gt;Ëá™Âä®Âåñ‰∏ä‰º†ËßÜÈ¢ëÂà∞Á§æ‰∫§Â™í‰ΩìÔºöÊäñÈü≥„ÄÅÂ∞èÁ∫¢‰π¶„ÄÅËßÜÈ¢ëÂè∑„ÄÅtiktok„ÄÅyoutube„ÄÅbilibili&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;social-auto-upload&lt;/h1&gt; 
&lt;p&gt;&lt;code&gt;social-auto-upload&lt;/code&gt; ÊòØ‰∏Ä‰∏™Âº∫Â§ßÁöÑËá™Âä®ÂåñÂ∑•ÂÖ∑ÔºåÊó®Âú®Â∏ÆÂä©ÂÜÖÂÆπÂàõ‰ΩúËÄÖÂíåËøêËê•ËÄÖÈ´òÊïàÂú∞Â∞ÜËßÜÈ¢ëÂÜÖÂÆπ‰∏ÄÈîÆÂèëÂ∏ÉÂà∞Â§ö‰∏™ÂõΩÂÜÖÂ§ñ‰∏ªÊµÅÁ§æ‰∫§Â™í‰ΩìÂπ≥Âè∞„ÄÇ È°πÁõÆÂÆûÁé∞‰∫ÜÂØπ &lt;code&gt;ÊäñÈü≥&lt;/code&gt;„ÄÅ&lt;code&gt;Bilibili&lt;/code&gt;„ÄÅ&lt;code&gt;Â∞èÁ∫¢‰π¶&lt;/code&gt;„ÄÅ&lt;code&gt;Âø´Êâã&lt;/code&gt;„ÄÅ&lt;code&gt;ËßÜÈ¢ëÂè∑&lt;/code&gt;„ÄÅ&lt;code&gt;ÁôæÂÆ∂Âè∑&lt;/code&gt; ‰ª•Âèä &lt;code&gt;TikTok&lt;/code&gt; Á≠âÂπ≥Âè∞ÁöÑËßÜÈ¢ë‰∏ä‰º†„ÄÅÂÆöÊó∂ÂèëÂ∏ÉÁ≠âÂäüËÉΩ„ÄÇ ÁªìÂêàÂêÑÂπ≥Âè∞ &lt;code&gt;uploader&lt;/code&gt; Ê®°ÂùóÔºåÊÇ®ÂèØ‰ª•ËΩªÊùæÈÖçÁΩÆÂíåÊâ©Â±ïÊîØÊåÅÁöÑÂπ≥Âè∞ÔºåÂπ∂ÈÄöËøáÁ§∫‰æãËÑöÊú¨Âø´ÈÄü‰∏äÊâã„ÄÇ&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/media/show/tkupload.gif" alt="tiktok show" width="800" /&gt; 
&lt;h2&gt;ÁõÆÂΩï&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/#%F0%9F%92%A1%E5%8A%9F%E8%83%BD%E7%89%B9%E6%80%A7"&gt;üí° ÂäüËÉΩÁâπÊÄß&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/#%F0%9F%9A%80%E6%94%AF%E6%8C%81%E7%9A%84%E5%B9%B3%E5%8F%B0"&gt;üöÄ ÊîØÊåÅÁöÑÂπ≥Âè∞&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/#%F0%9F%92%BE%E5%AE%89%E8%A3%85%E6%8C%87%E5%8D%97"&gt;üíæ ÂÆâË£ÖÊåáÂçó&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/#%F0%9F%8F%81%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B"&gt;üèÅ Âø´ÈÄüÂºÄÂßã&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/#%F0%9F%90%87%E9%A1%B9%E7%9B%AE%E8%83%8C%E6%99%AF"&gt;üêá È°πÁõÆËÉåÊôØ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/#%F0%9F%93%83%E8%AF%A6%E7%BB%86%E6%96%87%E6%A1%A3"&gt;üìÉ ËØ¶ÁªÜÊñáÊ°£&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/#%F0%9F%90%BE%E4%BA%A4%E6%B5%81%E4%B8%8E%E6%94%AF%E6%8C%81"&gt;üêæ ‰∫§ÊµÅ‰∏éÊîØÊåÅ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/#%F0%9F%A4%9D%E8%B4%A1%E7%8C%AE%E6%8C%87%E5%8D%97"&gt;ü§ù Ë¥°ÁåÆÊåáÂçó&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/#%F0%9F%93%9C%E8%AE%B8%E5%8F%AF%E8%AF%81"&gt;üìú ËÆ∏ÂèØËØÅ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/#%E2%AD%90Star-History"&gt;‚≠ê Star History&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üí°ÂäüËÉΩÁâπÊÄß&lt;/h2&gt; 
&lt;h3&gt;Â∑≤ÊîØÊåÅÂπ≥Âè∞&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ÂõΩÂÜÖÂπ≥Âè∞&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; ÊäñÈü≥&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; ËßÜÈ¢ëÂè∑&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Bilibili&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Â∞èÁ∫¢‰π¶&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Âø´Êâã&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; ÁôæÂÆ∂Âè∑&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÂõΩÂ§ñÂπ≥Âè∞&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; TikTok&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Ê†∏ÂøÉÂäüËÉΩ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; ÂÆöÊó∂‰∏ä‰º† (Cron Job / Scheduled Upload)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Cookie ÁÆ°ÁêÜ (ÈÉ®ÂàÜÂÆûÁé∞ÔºåÊåÅÁª≠‰ºòÂåñ‰∏≠)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; ÂõΩÂ§ñÂπ≥Âè∞ Proxy ËÆæÁΩÆ (ÈÉ®ÂàÜÂÆûÁé∞)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ËÆ°ÂàíÊîØÊåÅ‰∏éÂºÄÂèë‰∏≠&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Âπ≥Âè∞Êâ©Â±ï&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled /&gt; YouTube&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÂäüËÉΩÂ¢ûÂº∫&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Êõ¥ÊòìÁî®ÁöÑÁâàÊú¨ (GUI / CLI ‰∫§‰∫í‰ºòÂåñ)&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; API Â∞ÅË£Ö&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Docker ÈÉ®ÁΩ≤&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Ëá™Âä®Âåñ‰∏ä‰º† (Êõ¥Êô∫ËÉΩÁöÑË∞ÉÂ∫¶Á≠ñÁï•)&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Â§öÁ∫øÁ®ã/ÂºÇÊ≠•‰∏ä‰º†‰ºòÂåñ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Slack/Ê∂àÊÅØÊé®ÈÄÅÈÄöÁü•&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄÊîØÊåÅÁöÑÂπ≥Âè∞&lt;/h2&gt; 
&lt;p&gt;Êú¨È°πÁõÆÈÄöËøáÂêÑÂπ≥Âè∞ÂØπÂ∫îÁöÑ &lt;code&gt;uploader&lt;/code&gt; Ê®°ÂùóÂÆûÁé∞ËßÜÈ¢ë‰∏ä‰º†ÂäüËÉΩ„ÄÇÊÇ®ÂèØ‰ª•Âú® &lt;code&gt;examples&lt;/code&gt; ÁõÆÂΩï‰∏ãÊâæÂà∞ÂêÑ‰∏™Âπ≥Âè∞ÁöÑ‰ΩøÁî®Á§∫‰æãËÑöÊú¨„ÄÇ&lt;/p&gt; 
&lt;p&gt;ÊØè‰∏™Á§∫‰æãËÑöÊú¨Â±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÈÖçÁΩÆÂíåË∞ÉÁî®Áõ∏Â∫îÁöÑ uploader„ÄÇ&lt;/p&gt; 
&lt;h2&gt;üíæÂÆâË£ÖÊåáÂçó&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂÖãÈöÜÈ°πÁõÆ&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/dreammis/social-auto-upload.git
cd social-auto-upload
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂÆâË£Ö‰æùËµñ&lt;/strong&gt;: Âª∫ËÆÆÂú®ËôöÊãüÁéØÂ¢É‰∏≠ÂÆâË£Ö‰æùËµñ„ÄÇ&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n social-auto-upload python=3.10
conda activate social-auto-upload
# ÊåÇËΩΩÊ∏ÖÂçéÈïúÂÉè or ÂëΩ‰ª§Ë°å‰ª£ÁêÜ
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂÆâË£Ö Playwright ÊµèËßàÂô®È©±Âä®&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;playwright install chromium firefox
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Ê†πÊçÆÊÇ®ÁöÑÈúÄÊ±ÇÔºåËá≥Â∞ëÈúÄË¶ÅÂÆâË£Ö &lt;code&gt;chromium&lt;/code&gt;„ÄÇ&lt;code&gt;firefox&lt;/code&gt; ‰∏ªË¶ÅÁî®‰∫é TikTok ‰∏ä‰º†ÔºàÊóßÁâàÔºâ„ÄÇ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂&lt;/strong&gt;: Â§çÂà∂ &lt;code&gt;conf.example.py&lt;/code&gt; Âπ∂ÈáçÂëΩÂêç‰∏∫ &lt;code&gt;conf.py&lt;/code&gt;„ÄÇ Âú® &lt;code&gt;conf.py&lt;/code&gt; ‰∏≠ÔºåÊÇ®ÈúÄË¶ÅÈÖçÁΩÆ‰ª•‰∏ãÂÜÖÂÆπÔºö&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;LOCAL_CHROME_PATH&lt;/code&gt;: Êú¨Âú∞ Chrome ÊµèËßàÂô®ÁöÑË∑ØÂæÑÔºåÊØîÂ¶Ç &lt;code&gt;C:\Program Files\Google\Chrome\Application\chrome.exe&lt;/code&gt; ‰øùÂ≠ò„ÄÇ&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;‰∏¥Êó∂Ëß£ÂÜ≥ÊñπÊ°à&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;ÈúÄË¶ÅÂú®Ê†πÁõÆÂΩïÂàõÂª∫ &lt;code&gt;cookiesFile&lt;/code&gt; Âíå &lt;code&gt;videoFile&lt;/code&gt; ‰∏§‰∏™Êñá‰ª∂Â§πÔºåÂàÜÂà´ÊòØ Â≠òÂÇ®cookieÊñá‰ª∂ Âíå Â≠òÂÇ®‰∏ä‰º†Êñá‰ª∂ ÁöÑÊñá‰ª∂Â§π&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÈÖçÁΩÆÊï∞ÊçÆÂ∫ì&lt;/strong&gt;: Â¶ÇÊûú db/database.db Êñá‰ª∂‰∏çÂ≠òÂú®ÔºåÊÇ®ÂèØ‰ª•ËøêË°å‰ª•‰∏ãÂëΩ‰ª§Êù•ÂàùÂßãÂåñÊï∞ÊçÆÂ∫ìÔºö&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd db
python createTable.py
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Ê≠§ÂëΩ‰ª§Â∞ÜÂàùÂßãÂåñ SQLite Êï∞ÊçÆÂ∫ì„ÄÇ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂêØÂä®ÂêéÁ´ØÈ°πÁõÆ&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python sau_backend.py
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;ÂêéÁ´ØÈ°πÁõÆÂ∞ÜÂú® &lt;code&gt;http://localhost:5409&lt;/code&gt; ÂêØÂä®„ÄÇ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂêØÂä®ÂâçÁ´ØÈ°πÁõÆ&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd sau_frontend
npm install
npm run dev
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;ÂâçÁ´ØÈ°πÁõÆÂ∞ÜÂú® &lt;code&gt;http://localhost:5173&lt;/code&gt; ÂêØÂä®ÔºåÂú®ÊµèËßàÂô®‰∏≠ÊâìÂºÄÊ≠§ÈìæÊé•Âç≥ÂèØËÆøÈóÆ„ÄÇ&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ÈùûÁ®ãÂ∫èÂëòÁî®Êà∑ÂèØ‰ª•ÂèÇËÄÉÔºö&lt;a href="https://juejin.cn/post/7372114027840208911"&gt;Êñ∞ÊâãÁ∫ßÊïôÁ®ã&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;üèÅÂø´ÈÄüÂºÄÂßã&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂáÜÂ§á Cookie&lt;/strong&gt;: Â§ßÂ§öÊï∞Âπ≥Âè∞ÈúÄË¶ÅÁôªÂΩïÂêéÁöÑ Cookie ‰ø°ÊÅØÊâçËÉΩËøõË°åÊìç‰Ωú„ÄÇËØ∑ÂèÇÁÖß examples ÁõÆÂΩï‰∏ãÂêÑ &lt;code&gt;get_xxx_cookie.py&lt;/code&gt; ËÑöÊú¨Ôºà‰æãÂ¶Ç get_douyin_cookie.py, get_ks_cookie.pyÔºâÁöÑËØ¥ÊòéÔºåËøêË°åËÑöÊú¨‰ª•ÁîüÊàêÂπ∂‰øùÂ≠ò Cookie Êñá‰ª∂ÔºàÈÄöÂ∏∏Âú® &lt;code&gt;cookies/[PLATFORM]_uploader/account.json&lt;/code&gt;Ôºâ„ÄÇ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂáÜÂ§áËßÜÈ¢ëÊñá‰ª∂&lt;/strong&gt;: Â∞ÜÈúÄË¶Å‰∏ä‰º†ÁöÑËßÜÈ¢ëÊñá‰ª∂ÔºàÈÄöÂ∏∏‰∏∫ &lt;code&gt;.mp4&lt;/code&gt; Ê†ºÂºèÔºâÊîæÁΩÆÂú® videos ÁõÆÂΩï‰∏ã„ÄÇ ÈÉ®ÂàÜÂπ≥Âè∞ÊîØÊåÅËßÜÈ¢ëÂ∞ÅÈù¢ÔºåÂèØ‰ª•Â∞ÜÂ∞ÅÈù¢ÂõæÁâáÔºà‰æãÂ¶Ç &lt;code&gt;.png&lt;/code&gt; Ê†ºÂºèÔºå‰∏éËßÜÈ¢ëÂêåÂêçÔºâ‰πüÊîæÂú®Ê≠§ÁõÆÂΩï„ÄÇ Â¶ÇÊûúÈúÄË¶Å‰∏ä‰º†Ê†áÈ¢òÂèäÊ†áÁ≠æÔºåËØ∑Âú®ËßÜÈ¢ëÊñá‰ª∂ÊóÅËæπÂàõÂª∫‰∏Ä‰∏™ÂêåÂêçÁöÑ &lt;code&gt;.txt&lt;/code&gt; Êñá‰ª∂ÔºåÂÜÖÂÆπ‰∏∫Ê†áÈ¢òÂíåÊ†áÁ≠æÔºå‰ª•Êç¢Ë°åÂàÜÈöî„ÄÇ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;‰øÆÊîπÂπ∂ËøêË°åÁ§∫‰æãËÑöÊú¨&lt;/strong&gt;: ÊâìÂºÄ examples ÁõÆÂΩï‰∏≠ÊÇ®ÊÉ≥‰ΩøÁî®ÁöÑÂπ≥Âè∞ÁöÑ‰∏ä‰º†ËÑöÊú¨Ôºà‰æãÂ¶Ç upload_video_to_douyin.pyÔºâ„ÄÇ&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Ê†πÊçÆËÑöÊú¨ÂÜÖÁöÑÊ≥®ÈáäÂíåËØ¥ÊòéÔºåÁ°ÆËÆ§ Cookie Êñá‰ª∂Ë∑ØÂæÑ„ÄÅËßÜÈ¢ëÊñá‰ª∂Ë∑ØÂæÑÁ≠âÈÖçÁΩÆÊòØÂê¶Ê≠£Á°Æ„ÄÇ&lt;/li&gt; 
   &lt;li&gt;ÊÇ®ÂèØ‰ª•‰øÆÊîπËÑöÊú¨‰ª•ÈÄÇÂ∫îÊÇ®ÁöÑÂÖ∑‰ΩìÈúÄÊ±ÇÔºå‰æãÂ¶ÇÊâπÈáè‰∏ä‰º†„ÄÅËá™ÂÆö‰πâÊ†áÈ¢ò„ÄÅÊ†áÁ≠æÁ≠â„ÄÇ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÊâßË°å‰∏ä‰º†&lt;/strong&gt;: ËøêË°å‰øÆÊîπÂêéÁöÑÁ§∫‰æãËÑöÊú¨Ôºå‰æãÂ¶ÇÔºö&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python examples/upload_video_to_douyin.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;üêáÈ°πÁõÆËÉåÊôØ&lt;/h2&gt; 
&lt;p&gt;ËØ•È°πÁõÆÊúÄÂàùÊòØÊàë‰∏™‰∫∫Áî®‰∫éËá™Âä®ÂåñÁÆ°ÁêÜÁ§æ‰∫§Â™í‰ΩìËßÜÈ¢ëÂèëÂ∏ÉÁöÑÂ∑•ÂÖ∑„ÄÇÊàëÁöÑ‰∏ªË¶ÅÂèëÂ∏ÉÁ≠ñÁï•ÊòØÊèêÂâç‰∏ÄÂ§©ËÆæÁΩÆÂÆöÊó∂ÂèëÂ∏ÉÔºåÂõ†Ê≠§È°πÁõÆ‰∏≠ÂæàÂ§öÂÆöÊó∂ÂèëÂ∏ÉÁõ∏ÂÖ≥ÁöÑÈÄªËæëÊòØÂü∫‰∫é‚ÄúÁ¨¨‰∫åÂ§©‚ÄùÁöÑÊó∂Èó¥ËøõË°åËÆ°ÁÆóÁöÑ„ÄÇ&lt;/p&gt; 
&lt;p&gt;Â¶ÇÊûúÊÇ®ÈúÄË¶ÅÁ´ãÂç≥ÂèëÂ∏ÉÊàñÂÖ∂‰ªñÂÆöÂà∂ÂåñÁöÑÂèëÂ∏ÉÁ≠ñÁï•ÔºåÊ¨¢ËøéÁ†îÁ©∂Ê∫êÁ†ÅÊàñÂú®Á§æÂå∫ÊèêÈóÆ„ÄÇ&lt;/p&gt; 
&lt;h2&gt;üìÉËØ¶ÁªÜÊñáÊ°£&lt;/h2&gt; 
&lt;p&gt;Êõ¥ËØ¶ÁªÜÁöÑÊñáÊ°£ÂíåËØ¥ÊòéÔºåËØ∑Êü•ÁúãÔºö&lt;a href="https://sap-doc.nasdaddy.com/"&gt;social-auto-upload ÂÆòÊñπÊñáÊ°£&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üêæ‰∫§ÊµÅ‰∏éÊîØÊåÅ&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.buymeacoffee.com/hysn2001m"&gt;‚òï Donate as u like&lt;/a&gt; - Â¶ÇÊûúÊÇ®ËßâÂæóËøô‰∏™È°πÁõÆÂØπÊÇ®ÊúâÂ∏ÆÂä©ÔºåÂèØ‰ª•ËÄÉËôëËµûÂä©„ÄÇ&lt;/p&gt; 
&lt;p&gt;Â¶ÇÊûúÊÇ®‰πüÊòØÁã¨Á´ãÂºÄÂèëËÄÖ„ÄÅÊäÄÊúØÁà±Â•ΩËÄÖÔºåÂØπ #ÊäÄÊúØÂèòÁé∞ #AIÂàõ‰∏ö #Ë∑®Â¢ÉÁîµÂïÜ #Ëá™Âä®ÂåñÂ∑•ÂÖ∑ #ËßÜÈ¢ëÂàõ‰Ωú Á≠âËØùÈ¢òÊÑüÂÖ¥Ë∂£ÔºåÊ¨¢ËøéÂä†ÂÖ•Á§æÁæ§‰∫§ÊµÅ„ÄÇ&lt;/p&gt; 
&lt;h3&gt;Creator&lt;/h3&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt;
   &lt;td align="center"&gt; &lt;a href="https://sap-doc.nasdaddy.com/"&gt; &lt;img src="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/media/mp.jpg" width="200px" alt="NasDaddyÂÖ¨‰ºóÂè∑" /&gt; &lt;br /&gt; &lt;sub&gt;&lt;b&gt;ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/dreammis/social-auto-upload/commits?author=dreammis" title="Code"&gt;üíª&lt;/a&gt; &lt;br /&gt; ÂÖ≥Ê≥®ÂÖ¨‰ºóÂè∑ÔºåÂêéÂè∞ÂõûÂ§ç `‰∏ä‰º†` Ëé∑ÂèñÂä†Áæ§ÊñπÂºè &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://sap-doc.nasdaddy.com/"&gt; &lt;img src="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/media/QR.png" width="200px" alt="ËµûËµèÁ†Å/ÂÖ•Áæ§ÂºïÂØº" /&gt; &lt;br /&gt; &lt;sub&gt;&lt;b&gt;‰∫§ÊµÅÁæ§ (ÈÄöËøáÂÖ¨‰ºóÂè∑Ëé∑Âèñ)&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://sap-doc.nasdaddy.com/" title="Documentation"&gt;üìñ&lt;/a&gt; &lt;br /&gt; Â¶ÇÊûúÊÇ®ËßâÂæóÈ°πÁõÆÊúâÁî®ÔºåÂèØ‰ª•ËÄÉËôëÊâìËµèÊîØÊåÅ‰∏Ä‰∏ã &lt;/td&gt; 
  &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Active Core Team&lt;/h3&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt;
   &lt;td align="center"&gt; &lt;a href="https://leedebug.github.io/"&gt; &lt;img src="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/media/edan-qrcode.png" width="200px" alt="Edan Lee" /&gt; &lt;br /&gt; &lt;sub&gt;&lt;b&gt;Edan Lee&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/dreammis/social-auto-upload/commits?author=LeeDebug" title="Code"&gt;üíª&lt;/a&gt; &lt;a href="https://leedebug.github.io/" title="Documentation"&gt;üìñ&lt;/a&gt; &lt;br /&gt; Â∞ÅË£Ö‰∫Ü api Êé•Âè£Âíå web ÂâçÁ´ØÁÆ°ÁêÜÁïåÈù¢ &lt;br /&gt; ÔºàËØ∑Ê≥®ÊòéÊù•ÊÑèÔºöËøõÁæ§„ÄÅÂ≠¶‰π†„ÄÅ‰ºÅ‰∏öÂí®ËØ¢Á≠âÔºâ &lt;/td&gt; 
  &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;ü§ùË¥°ÁåÆÊåáÂçó&lt;/h2&gt; 
&lt;p&gt;Ê¨¢ËøéÂêÑÁßçÂΩ¢ÂºèÁöÑË¥°ÁåÆÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éÔºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Êèê‰∫§ BugÊä•Âëä Âíå FeatureËØ∑Ê±Ç„ÄÇ&lt;/li&gt; 
 &lt;li&gt;ÊîπËøõ‰ª£Á†Å„ÄÅÊñáÊ°£„ÄÇ&lt;/li&gt; 
 &lt;li&gt;ÂàÜ‰∫´‰ΩøÁî®ÁªèÈ™åÂíåÊïôÁ®ã„ÄÇ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Â¶ÇÊûúÊÇ®Â∏åÊúõË¥°ÁåÆ‰ª£Á†ÅÔºåËØ∑ÈÅµÂæ™‰ª•‰∏ãÊ≠•È™§Ôºö&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork Êú¨‰ªìÂ∫ì„ÄÇ&lt;/li&gt; 
 &lt;li&gt;ÂàõÂª∫‰∏Ä‰∏™Êñ∞ÁöÑÂàÜÊîØ (&lt;code&gt;git checkout -b feature/YourFeature&lt;/code&gt; Êàñ &lt;code&gt;bugfix/YourBugfix&lt;/code&gt;)„ÄÇ&lt;/li&gt; 
 &lt;li&gt;Êèê‰∫§ÊÇ®ÁöÑÊõ¥Êîπ (&lt;code&gt;git commit -m 'Add some feature'&lt;/code&gt;)„ÄÇ&lt;/li&gt; 
 &lt;li&gt;PushÂà∞ÊÇ®ÁöÑÂàÜÊîØ (&lt;code&gt;git push origin feature/YourFeature&lt;/code&gt;)„ÄÇ&lt;/li&gt; 
 &lt;li&gt;ÂàõÂª∫‰∏Ä‰∏™ Pull Request„ÄÇ&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;üìúËÆ∏ÂèØËØÅ&lt;/h2&gt; 
&lt;p&gt;Êú¨È°πÁõÆÊöÇÊó∂ÈááÁî® &lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/LICENSE"&gt;MIT License&lt;/a&gt; ÂºÄÊ∫êËÆ∏ÂèØËØÅ„ÄÇ&lt;/p&gt; 
&lt;h2&gt;‚≠êStar-History&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Â¶ÇÊûúËøô‰∏™È°πÁõÆÂØπÊÇ®ÊúâÂ∏ÆÂä©ÔºåËØ∑Áªô‰∏Ä‰∏™ ‚≠ê Star ‰ª•Ë°®Á§∫ÊîØÊåÅÔºÅ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#dreammis/social-auto-upload&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=dreammis/social-auto-upload&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/markitdown</title>
      <link>https://github.com/microsoft/markitdown</link>
      <description>&lt;p&gt;Python tool for converting files and office documents to Markdown.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MarkItDown&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/markitdown/"&gt;&lt;img src="https://img.shields.io/pypi/v/markitdown.svg?sanitize=true" alt="PyPI" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/dd/markitdown" alt="PyPI - Downloads" /&gt; &lt;a href="https://github.com/microsoft/autogen"&gt;&lt;img src="https://img.shields.io/badge/Built%20by-AutoGen%20Team-blue" alt="Built by AutoGen Team" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] MarkItDown now offers an MCP (Model Context Protocol) server for integration with LLM applications like Claude Desktop. See &lt;a href="https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp"&gt;markitdown-mcp&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Breaking changes between 0.0.1 to 0.1.0:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Dependencies are now organized into optional feature-groups (further details below). Use &lt;code&gt;pip install 'markitdown[all]'&lt;/code&gt; to have backward-compatible behavior.&lt;/li&gt; 
  &lt;li&gt;convert_stream() now requires a binary file-like object (e.g., a file opened in binary mode, or an io.BytesIO object). This is a breaking change from the previous version, where it previously also accepted text file-like objects, like io.StringIO.&lt;/li&gt; 
  &lt;li&gt;The DocumentConverter class interface has changed to read from file-like streams rather than file paths. &lt;em&gt;No temporary files are created anymore&lt;/em&gt;. If you are the maintainer of a plugin, or custom DocumentConverter, you likely need to update your code. Otherwise, if only using the MarkItDown class or CLI (as in these examples), you should not need to change anything.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;MarkItDown is a lightweight Python utility for converting various files to Markdown for use with LLMs and related text analysis pipelines. To this end, it is most comparable to &lt;a href="https://github.com/deanmalmgren/textract"&gt;textract&lt;/a&gt;, but with a focus on preserving important document structure and content as Markdown (including: headings, lists, tables, links, etc.) While the output is often reasonably presentable and human-friendly, it is meant to be consumed by text analysis tools -- and may not be the best option for high-fidelity document conversions for human consumption.&lt;/p&gt; 
&lt;p&gt;MarkItDown currently supports the conversion from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PDF&lt;/li&gt; 
 &lt;li&gt;PowerPoint&lt;/li&gt; 
 &lt;li&gt;Word&lt;/li&gt; 
 &lt;li&gt;Excel&lt;/li&gt; 
 &lt;li&gt;Images (EXIF metadata and OCR)&lt;/li&gt; 
 &lt;li&gt;Audio (EXIF metadata and speech transcription)&lt;/li&gt; 
 &lt;li&gt;HTML&lt;/li&gt; 
 &lt;li&gt;Text-based formats (CSV, JSON, XML)&lt;/li&gt; 
 &lt;li&gt;ZIP files (iterates over contents)&lt;/li&gt; 
 &lt;li&gt;Youtube URLs&lt;/li&gt; 
 &lt;li&gt;EPubs&lt;/li&gt; 
 &lt;li&gt;... and more!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Why Markdown?&lt;/h2&gt; 
&lt;p&gt;Markdown is extremely close to plain text, with minimal markup or formatting, but still provides a way to represent important document structure. Mainstream LLMs, such as OpenAI's GPT-4o, natively "&lt;em&gt;speak&lt;/em&gt;" Markdown, and often incorporate Markdown into their responses unprompted. This suggests that they have been trained on vast amounts of Markdown-formatted text, and understand it well. As a side benefit, Markdown conventions are also highly token-efficient.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;MarkItDown requires Python 3.10 or higher. It is recommended to use a virtual environment to avoid dependency conflicts.&lt;/p&gt; 
&lt;p&gt;With the standard Python installation, you can create and activate a virtual environment using the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv .venv
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If using &lt;code&gt;uv&lt;/code&gt;, you can create a virtual environment with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv --python=3.12 .venv
source .venv/bin/activate
# NOTE: Be sure to use 'uv pip install' rather than just 'pip install' to install packages in this virtual environment
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are using Anaconda, you can create a virtual environment with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n markitdown python=3.12
conda activate markitdown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install MarkItDown, use pip: &lt;code&gt;pip install 'markitdown[all]'&lt;/code&gt;. Alternatively, you can install it from the source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone git@github.com:microsoft/markitdown.git
cd markitdown
pip install -e 'packages/markitdown[all]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Command-Line&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf &amp;gt; document.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or use &lt;code&gt;-o&lt;/code&gt; to specify the output file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf -o document.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also pipe content:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cat path-to-file.pdf | markitdown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Optional Dependencies&lt;/h3&gt; 
&lt;p&gt;MarkItDown has optional dependencies for activating various file formats. Earlier in this document, we installed all optional dependencies with the &lt;code&gt;[all]&lt;/code&gt; option. However, you can also install them individually for more control. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install 'markitdown[pdf, docx, pptx]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;will install only the dependencies for PDF, DOCX, and PPTX files.&lt;/p&gt; 
&lt;p&gt;At the moment, the following optional dependencies are available:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;[all]&lt;/code&gt; Installs all optional dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[pptx]&lt;/code&gt; Installs dependencies for PowerPoint files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[docx]&lt;/code&gt; Installs dependencies for Word files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[xlsx]&lt;/code&gt; Installs dependencies for Excel files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[xls]&lt;/code&gt; Installs dependencies for older Excel files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[pdf]&lt;/code&gt; Installs dependencies for PDF files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[outlook]&lt;/code&gt; Installs dependencies for Outlook messages&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[az-doc-intel]&lt;/code&gt; Installs dependencies for Azure Document Intelligence&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[audio-transcription]&lt;/code&gt; Installs dependencies for audio transcription of wav and mp3 files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[youtube-transcription]&lt;/code&gt; Installs dependencies for fetching YouTube video transcription&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Plugins&lt;/h3&gt; 
&lt;p&gt;MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown --list-plugins
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To enable plugins use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown --use-plugins path-to-file.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To find available plugins, search GitHub for the hashtag &lt;code&gt;#markitdown-plugin&lt;/code&gt;. To develop a plugin, see &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Azure Document Intelligence&lt;/h3&gt; 
&lt;p&gt;To use Microsoft Document Intelligence for conversion:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf -o document.md -d -e "&amp;lt;document_intelligence_endpoint&amp;gt;"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;More information about how to set up an Azure Document Intelligence Resource can be found &lt;a href="https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/create-document-intelligence-resource?view=doc-intel-4.0.0"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Python API&lt;/h3&gt; 
&lt;p&gt;Basic usage in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown

md = MarkItDown(enable_plugins=False) # Set to True to enable plugins
result = md.convert("test.xlsx")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Document Intelligence conversion in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown

md = MarkItDown(docintel_endpoint="&amp;lt;document_intelligence_endpoint&amp;gt;")
result = md.convert("test.pdf")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use Large Language Models for image descriptions (currently only for pptx and image files), provide &lt;code&gt;llm_client&lt;/code&gt; and &lt;code&gt;llm_model&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown
from openai import OpenAI

client = OpenAI()
md = MarkItDown(llm_client=client, llm_model="gpt-4o", llm_prompt="optional custom prompt")
result = md.convert("example.jpg")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;docker build -t markitdown:latest .
docker run --rm -i markitdown:latest &amp;lt; ~/your-file.pdf &amp;gt; output.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h3&gt;How to Contribute&lt;/h3&gt; 
&lt;p&gt;You can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as 'open for contribution' and 'open for reviewing' to help facilitate community contributions. These are of course just suggestions and you are welcome to contribute in any way you like.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;/th&gt; 
    &lt;th&gt;All&lt;/th&gt; 
    &lt;th&gt;Especially Needs Help from Community&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Issues&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/issues"&gt;All Issues&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/issues?q=is%3Aissue+is%3Aopen+label%3A%22open+for+contribution%22"&gt;Issues open for contribution&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;PRs&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/pulls"&gt;All PRs&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/pulls?q=is%3Apr+is%3Aopen+label%3A%22open+for+reviewing%22"&gt;PRs open for reviewing&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h3&gt;Running Tests and Checks&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Navigate to the MarkItDown package:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;cd packages/markitdown
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;code&gt;hatch&lt;/code&gt; in your environment and run tests:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/
hatch shell
hatch test
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(Alternative) Use the Devcontainer which has all the dependencies installed:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;# Reopen the project in Devcontainer and run:
hatch test
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run pre-commit checks before submitting a PR: &lt;code&gt;pre-commit run --all-files&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing 3rd-party Plugins&lt;/h3&gt; 
&lt;p&gt;You can also contribute by creating and sharing 3rd party plugins. See &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Trademarks&lt;/h2&gt; 
&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general"&gt;Microsoft's Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google-research/timesfm</title>
      <link>https://github.com/google-research/timesfm</link>
      <description>&lt;p&gt;TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TimesFM&lt;/h1&gt; 
&lt;p&gt;TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2310.10688"&gt;A decoder-only foundation model for time-series forecasting&lt;/a&gt;, ICML 2024.&lt;/li&gt; 
 &lt;li&gt;All checkpoints: &lt;a href="https://huggingface.co/collections/google/timesfm-release-66e4be5fdb56e960c1e482a6"&gt;TimesFM Hugging Face Collection&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/"&gt;Google Research blog&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cloud.google.com/bigquery/docs/timesfm-model"&gt;TimesFM in BigQuery&lt;/a&gt;: an official Google product.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This open version is not an officially supported Google product.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Latest Model Version:&lt;/strong&gt; TimesFM 2.5&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Archived Model Versions:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;1.0 and 2.0: relevant code archived in the sub directory &lt;code&gt;v1&lt;/code&gt;. You can &lt;code&gt;pip install timesfm==1.3.0&lt;/code&gt; to install an older version of this package to load them.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Update - Sept. 15, 2025&lt;/h2&gt; 
&lt;p&gt;TimesFM 2.5 is out!&lt;/p&gt; 
&lt;p&gt;Comparing to TimesFM 2.0, this new 2.5 model:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;uses 200M parameters, down from 500M.&lt;/li&gt; 
 &lt;li&gt;supports up to 16k context length, up from 2048.&lt;/li&gt; 
 &lt;li&gt;supports continuous quantile forecast up to 1k horizon via an optional 30M quantile head.&lt;/li&gt; 
 &lt;li&gt;gets rid of the &lt;code&gt;frequency&lt;/code&gt; indicator.&lt;/li&gt; 
 &lt;li&gt;has a couple of new forecasting flags.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Along with the model upgrade we have also upgraded the inference API. This repo will be under construction over the next few weeks to&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;add support for an upcoming Flax version of the model (faster inference).&lt;/li&gt; 
 &lt;li&gt;add back covariate support.&lt;/li&gt; 
 &lt;li&gt;populate more docstrings, docs and notebook.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Install&lt;/h3&gt; 
&lt;p&gt;TODO(siriuz42): Package timesfm==2.0.0 and upload to PyPI .&lt;/p&gt; 
&lt;p&gt;Run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/google-research/timesfm.git
cd timesfm
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Code Example&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np
import timesfm
model = timesfm.TimesFM_2p5_200M_torch()
model.load_checkpoint()
model.compile(
    timesfm.ForecastConfig(
        max_context=1024,
        max_horizon=256,
        normalize_inputs=True,
        use_continuous_quantile_head=True,
        force_flip_invariance=True,
        infer_is_positive=True,
        fix_quantile_crossing=True,
    )
)
point_forecast, quantile_forecast = model.forecast(
    horizon=12,
    inputs=[
        np.linspace(0, 1, 100),
        np.sin(np.linspace(0, 20, 67)),
    ],  # Two dummy inputs
)
point_forecast.shape  # (2, 12)
quantile_forecast.shape  # (2, 12, 10): mean, then 10th to 90th quantiles.
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>dataease/SQLBot</title>
      <link>https://github.com/dataease/SQLBot</link>
      <description>&lt;p&gt;Âü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü„ÄÇText-to-SQL Generation via LLMs using RAG.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt;&lt;img src="https://resource-fit2cloud-com.oss-cn-hangzhou.aliyuncs.com/sqlbot/sqlbot.png" alt="SQLBot" width="300" /&gt;&lt;/p&gt; 
&lt;h3 align="center"&gt;Âü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü&lt;/h3&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/dataease/SQLBot/releases/latest"&gt;&lt;img src="https://img.shields.io/github/v/release/dataease/SQLBot" alt="Latest release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/dataease/SQLBot"&gt;&lt;img src="https://img.shields.io/github/stars/dataease/SQLBot?color=%231890FF&amp;amp;style=flat-square" alt="Stars" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/dataease/SQLbot"&gt;&lt;img src="https://img.shields.io/docker/pulls/dataease/sqlbot?label=downloads" alt="Download" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;SQLBot ÊòØ‰∏ÄÊ¨æÂü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü„ÄÇSQLBot ÁöÑ‰ºòÂäøÂåÖÊã¨Ôºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ÂºÄÁÆ±Âç≥Áî®&lt;/strong&gt;: Âè™ÈúÄÈÖçÁΩÆÂ§ßÊ®°ÂûãÂíåÊï∞ÊçÆÊ∫êÂç≥ÂèØÂºÄÂêØÈóÆÊï∞‰πãÊóÖÔºåÈÄöËøáÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÁªìÂêàÊù•ÂÆûÁé∞È´òË¥®ÈáèÁöÑ text2sqlÔºõ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Êòì‰∫éÈõÜÊàê&lt;/strong&gt;: ÊîØÊåÅÂø´ÈÄüÂµåÂÖ•Âà∞Á¨¨‰∏âÊñπ‰∏öÂä°Á≥ªÁªüÔºå‰πüÊîØÊåÅË¢´ n8n„ÄÅMaxKB„ÄÅDify„ÄÅCoze Á≠â AI Â∫îÁî®ÂºÄÂèëÂπ≥Âè∞ÈõÜÊàêË∞ÉÁî®ÔºåËÆ©ÂêÑÁ±ªÂ∫îÁî®Âø´ÈÄüÊã•ÊúâÊô∫ËÉΩÈóÆÊï∞ËÉΩÂäõÔºõ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÂÆâÂÖ®ÂèØÊéß&lt;/strong&gt;: Êèê‰æõÂü∫‰∫éÂ∑•‰ΩúÁ©∫Èó¥ÁöÑËµÑÊ∫êÈöîÁ¶ªÊú∫Âà∂ÔºåËÉΩÂ§üÂÆûÁé∞ÁªÜÁ≤íÂ∫¶ÁöÑÊï∞ÊçÆÊùÉÈôêÊéßÂà∂„ÄÇ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Â∑•‰ΩúÂéüÁêÜ&lt;/h2&gt; 
&lt;img width="1189" height="624" alt="system-arch" src="https://github.com/user-attachments/assets/cde40783-369e-493e-bb59-44ce43c2e7c5" /&gt; 
&lt;h2&gt;Âø´ÈÄüÂºÄÂßã&lt;/h2&gt; 
&lt;h3&gt;ÂÆâË£ÖÈÉ®ÁΩ≤&lt;/h3&gt; 
&lt;p&gt;ÂáÜÂ§á‰∏ÄÂè∞ Linux ÊúçÂä°Âô®ÔºåÂÆâË£ÖÂ•Ω &lt;a href="https://docs.docker.com/get-docker/"&gt;Docker&lt;/a&gt;ÔºåÊâßË°å‰ª•‰∏ã‰∏ÄÈîÆÂÆâË£ÖËÑöÊú¨Ôºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d \
  --name sqlbot \
  --restart unless-stopped \
  -p 8000:8000 \
  -p 8001:8001 \
  -v ./data/sqlbot/excel:/opt/sqlbot/data/excel \
  -v ./data/sqlbot/images:/opt/sqlbot/images \
  -v ./data/sqlbot/logs:/opt/sqlbot/app/logs \
  -v ./data/postgresql:/var/lib/postgresql/data \
  --privileged=true \
  dataease/sqlbot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;‰Ω†‰πüÂèØ‰ª•ÈÄöËøá &lt;a href="https://apps.fit2cloud.com/1panel"&gt;1Panel Â∫îÁî®ÂïÜÂ∫ó&lt;/a&gt; Âø´ÈÄüÈÉ®ÁΩ≤ SQLBot„ÄÇ&lt;/p&gt; 
&lt;p&gt;Â¶ÇÊûúÊòØÂÜÖÁΩëÁéØÂ¢ÉÔºå‰Ω†ÂèØ‰ª•ÈÄöËøá &lt;a href="https://community.fit2cloud.com/#/products/sqlbot/downloads"&gt;Á¶ªÁ∫øÂÆâË£ÖÂåÖÊñπÂºè&lt;/a&gt; ÈÉ®ÁΩ≤ SQLBot„ÄÇ&lt;/p&gt; 
&lt;h3&gt;ËÆøÈóÆÊñπÂºè&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Âú®ÊµèËßàÂô®‰∏≠ÊâìÂºÄ: http://&amp;lt;‰Ω†ÁöÑÊúçÂä°Âô®IP&amp;gt;:8000/&lt;/li&gt; 
 &lt;li&gt;Áî®Êà∑Âêç: admin&lt;/li&gt; 
 &lt;li&gt;ÂØÜÁ†Å: SQLBot@123456&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ËÅîÁ≥ªÊàë‰ª¨&lt;/h3&gt; 
&lt;p&gt;Â¶Ç‰Ω†ÊúâÊõ¥Â§öÈóÆÈ¢òÔºåÂèØ‰ª•Âä†ÂÖ•Êàë‰ª¨ÁöÑÊäÄÊúØ‰∫§ÊµÅÁæ§‰∏éÊàë‰ª¨‰∫§ÊµÅ„ÄÇ&lt;/p&gt; 
&lt;img width="180" height="180" alt="contact_me_qr" src="https://github.com/user-attachments/assets/2594ff29-5426-4457-b051-279855610030" /&gt; 
&lt;h2&gt;UI Â±ïÁ§∫&lt;/h2&gt;  
&lt;img alt="q&amp;amp;a" src="https://github.com/user-attachments/assets/55526514-52f3-4cfe-98ec-08a986259280" /&gt;  
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#dataease/sqlbot&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=dataease/sqlbot&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;È£ûËá¥‰∫ëÊóó‰∏ãÁöÑÂÖ∂‰ªñÊòéÊòüÈ°πÁõÆ&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dataease/dataease/"&gt;DataEase&lt;/a&gt; - ‰∫∫‰∫∫ÂèØÁî®ÁöÑÂºÄÊ∫ê BI Â∑•ÂÖ∑&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1panel-dev/1panel/"&gt;1Panel&lt;/a&gt; - Áé∞‰ª£Âåñ„ÄÅÂºÄÊ∫êÁöÑ Linux ÊúçÂä°Âô®ËøêÁª¥ÁÆ°ÁêÜÈù¢Êùø&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1panel-dev/MaxKB/"&gt;MaxKB&lt;/a&gt; - Âº∫Â§ßÊòìÁî®ÁöÑ‰ºÅ‰∏öÁ∫ßÊô∫ËÉΩ‰ΩìÂπ≥Âè∞&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jumpserver/jumpserver/"&gt;JumpServer&lt;/a&gt; - ÂπøÂèóÊ¨¢ËøéÁöÑÂºÄÊ∫êÂ†°ÂûíÊú∫&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/halo-dev/halo/"&gt;Halo&lt;/a&gt; - Âº∫Â§ßÊòìÁî®ÁöÑÂºÄÊ∫êÂª∫Á´ôÂ∑•ÂÖ∑&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/metersphere/metersphere/"&gt;MeterSphere&lt;/a&gt; - Êñ∞‰∏Ä‰ª£ÁöÑÂºÄÊ∫êÊåÅÁª≠ÊµãËØïÂ∑•ÂÖ∑&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Êú¨‰ªìÂ∫ìÈÅµÂæ™ &lt;a href="https://raw.githubusercontent.com/dataease/SQLBot/main/LICENSE"&gt;FIT2CLOUD Open Source License&lt;/a&gt; ÂºÄÊ∫êÂçèËÆÆÔºåËØ•ËÆ∏ÂèØËØÅÊú¨Ë¥®‰∏äÊòØ GPLv3Ôºå‰ΩÜÊúâ‰∏Ä‰∫õÈ¢ùÂ§ñÁöÑÈôêÂà∂„ÄÇ&lt;/p&gt; 
&lt;p&gt;‰Ω†ÂèØ‰ª•Âü∫‰∫é SQLBot ÁöÑÊ∫ê‰ª£Á†ÅËøõË°å‰∫åÊ¨°ÂºÄÂèëÔºå‰ΩÜÊòØÈúÄË¶ÅÈÅµÂÆà‰ª•‰∏ãËßÑÂÆöÔºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‰∏çËÉΩÊõøÊç¢Âíå‰øÆÊîπ SQLBot ÁöÑ Logo ÂíåÁâàÊùÉ‰ø°ÊÅØÔºõ&lt;/li&gt; 
 &lt;li&gt;‰∫åÊ¨°ÂºÄÂèëÂêéÁöÑË°çÁîü‰ΩúÂìÅÂøÖÈ°ªÈÅµÂÆà GPL V3 ÁöÑÂºÄÊ∫ê‰πâÂä°„ÄÇ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Â¶ÇÈúÄÂïÜ‰∏öÊéàÊùÉÔºåËØ∑ËÅîÁ≥ª &lt;a href="mailto:support@fit2cloud.com"&gt;support@fit2cloud.com&lt;/a&gt; „ÄÇ&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>apache/airflow</title>
      <link>https://github.com/apache/airflow</link>
      <description>&lt;p&gt;Apache Airflow - A platform to programmatically author, schedule, and monitor workflows&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Apache Airflow&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Badges&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;License&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.apache.org/licenses/LICENSE-2.0.txt"&gt;&lt;img src="https://img.shields.io/:license-Apache%202-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PyPI&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://badge.fury.io/py/apache-airflow"&gt;&lt;img src="https://badge.fury.io/py/apache-airflow.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/apache-airflow/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/apache-airflow.svg?sanitize=true" alt="PyPI - Python Version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/apache-airflow/"&gt;&lt;img src="https://img.shields.io/pypi/dm/apache-airflow" alt="PyPI - Downloads" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Containers&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://hub.docker.com/r/apache/airflow"&gt;&lt;img src="https://img.shields.io/docker/pulls/apache/airflow.svg?sanitize=true" alt="Docker Pulls" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/apache/airflow"&gt;&lt;img src="https://img.shields.io/docker/stars/apache/airflow.svg?sanitize=true" alt="Docker Stars" /&gt;&lt;/a&gt; &lt;a href="https://artifacthub.io/packages/search?repo=apache-airflow"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/apache-airflow" alt="Artifact HUB" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Community&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/apache/airflow/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/apache/airflow" alt="Contributors" /&gt;&lt;/a&gt; &lt;a href="https://s.apache.org/airflow-slack"&gt;&lt;img src="https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&amp;amp;style=social" alt="Slack Status" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/commit-activity/m/apache/airflow" alt="Commit Activity" /&gt; &lt;a href="https://ossrank.com/p/6"&gt;&lt;img src="https://shields.io/endpoint?url=https://ossrank.com/shield/6" alt="OSSRank" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Version&lt;/th&gt; 
   &lt;th&gt;Build Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Main&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/apache/airflow/actions"&gt;&lt;img src="https://github.com/apache/airflow/actions/workflows/ci-amd.yml/badge.svg?sanitize=true" alt="GitHub Build main" /&gt;&lt;/a&gt; &lt;a href="https://github.com/apache/airflow/actions"&gt;&lt;img src="https://github.com/apache/airflow/actions/workflows/ci-arm.yml/badge.svg?sanitize=true" alt="GitHub Build main" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3.x&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/apache/airflow/actions"&gt;&lt;img src="https://github.com/apache/airflow/actions/workflows/ci-amd.yml/badge.svg?branch=v3-1-test" alt="GitHub Build 3.1" /&gt;&lt;/a&gt; &lt;a href="https://github.com/apache/airflow/actions"&gt;&lt;img src="https://github.com/apache/airflow/actions/workflows/ci-arm.yml/badge.svg?branch=v3-1-test" alt="GitHub Build 3.1" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2.x&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/apache/airflow/actions"&gt;&lt;img src="https://github.com/apache/airflow/actions/workflows/ci.yml/badge.svg?branch=v2-11-test" alt="GitHub Build 2.11" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;picture width="500"&gt; 
 &lt;img src="https://github.com/apache/airflow/raw/19ebcac2395ef9a6b6ded3a2faa29dc960c1e635/docs/apache-airflow/img/logos/wordmark_1.png?raw=true" alt="Apache Airflow logo" /&gt; 
&lt;/picture&gt; 
&lt;p&gt;&lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/"&gt;Apache Airflow&lt;/a&gt; (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows.&lt;/p&gt; 
&lt;p&gt;When workflows are defined as code, they become more maintainable, versionable, testable, and collaborative.&lt;/p&gt; 
&lt;p&gt;Use Airflow to author workflows (Dags) that orchestrate tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.&lt;/p&gt; 
&lt;!-- END Apache Airflow, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;!-- START doctoc generated TOC please keep comment here to allow auto update --&gt; 
&lt;!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE --&gt; 
&lt;p&gt;&lt;strong&gt;Table of contents&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#project-focus"&gt;Project Focus&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#principles"&gt;Principles&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#requirements"&gt;Requirements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#getting-started"&gt;Getting started&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#installing-from-pypi"&gt;Installing from PyPI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#official-source-code"&gt;Official source code&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#convenience-packages"&gt;Convenience packages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#user-interface"&gt;User Interface&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#semantic-versioning"&gt;Semantic versioning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#version-life-cycle"&gt;Version Life Cycle&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#support-for-python-and-kubernetes-versions"&gt;Support for Python and Kubernetes versions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#base-os-support-for-reference-airflow-images"&gt;Base OS support for reference Airflow images&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#approach-to-dependencies-of-airflow"&gt;Approach to dependencies of Airflow&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#voting-policy"&gt;Voting Policy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#who-uses-apache-airflow"&gt;Who uses Apache Airflow?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#who-maintains-apache-airflow"&gt;Who maintains Apache Airflow?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#what-goes-into-the-next-release"&gt;What goes into the next release?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#can-i-use-the-apache-airflow-logo-in-my-presentation"&gt;Can I use the Apache Airflow logo in my presentation?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#links"&gt;Links&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#sponsors"&gt;Sponsors&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- END doctoc generated TOC please keep comment here to allow auto update --&gt; 
&lt;h2&gt;Project Focus&lt;/h2&gt; 
&lt;p&gt;Airflow works best with workflows that are mostly static and slowly changing. When the DAG structure is similar from one run to the next, it clarifies the unit of work and continuity. Other similar projects include &lt;a href="https://github.com/spotify/luigi"&gt;Luigi&lt;/a&gt;, &lt;a href="https://oozie.apache.org/"&gt;Oozie&lt;/a&gt; and &lt;a href="https://azkaban.github.io/"&gt;Azkaban&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Airflow is commonly used to process data, but has the opinion that tasks should ideally be idempotent (i.e., results of the task will be the same, and will not create duplicated data in a destination system), and should not pass large quantities of data from one task to the next (though tasks can pass metadata using Airflow's &lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/concepts/xcoms.html"&gt;XCom feature&lt;/a&gt;). For high-volume, data-intensive tasks, a best practice is to delegate to external services specializing in that type of work.&lt;/p&gt; 
&lt;p&gt;Airflow is not a streaming solution, but it is often used to process real-time data, pulling data off streams in batches.&lt;/p&gt; 
&lt;h2&gt;Principles&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Dynamic&lt;/strong&gt;: Pipelines are defined in code, enabling dynamic dag generation and parameterization.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensible&lt;/strong&gt;: The Airflow framework includes a wide range of built-in operators and can be extended to fit your needs.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible&lt;/strong&gt;: Airflow leverages the &lt;a href="https://jinja.palletsprojects.com"&gt;&lt;strong&gt;Jinja&lt;/strong&gt;&lt;/a&gt; templating engine, allowing rich customizations.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- START Requirements, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;Apache Airflow is tested with:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;Main version (dev)&lt;/th&gt; 
   &lt;th&gt;Stable version (3.0.6)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python&lt;/td&gt; 
   &lt;td&gt;3.10, 3.11, 3.12, 3.13&lt;/td&gt; 
   &lt;td&gt;3.9, 3.10, 3.11, 3.12&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Platform&lt;/td&gt; 
   &lt;td&gt;AMD64/ARM64(*)&lt;/td&gt; 
   &lt;td&gt;AMD64/ARM64(*)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Kubernetes&lt;/td&gt; 
   &lt;td&gt;1.30, 1.31, 1.32, 1.33&lt;/td&gt; 
   &lt;td&gt;1.30, 1.31, 1.32, 1.33&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PostgreSQL&lt;/td&gt; 
   &lt;td&gt;13, 14, 15, 16, 17&lt;/td&gt; 
   &lt;td&gt;13, 14, 15, 16, 17&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MySQL&lt;/td&gt; 
   &lt;td&gt;8.0, 8.4, Innovation&lt;/td&gt; 
   &lt;td&gt;8.0, 8.4, Innovation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SQLite&lt;/td&gt; 
   &lt;td&gt;3.15.0+&lt;/td&gt; 
   &lt;td&gt;3.15.0+&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;* Experimental&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: MariaDB is not tested/recommended.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: SQLite is used in Airflow tests. Do not use it in production. We recommend using the latest stable version of SQLite for local development.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Airflow currently can be run on POSIX-compliant Operating Systems. For development, it is regularly tested on fairly modern Linux Distros and recent versions of macOS. On Windows you can run it via WSL2 (Windows Subsystem for Linux 2) or via Linux Containers. The work to add Windows support is tracked via &lt;a href="https://github.com/apache/airflow/issues/10388"&gt;#10388&lt;/a&gt;, but it is not a high priority. You should only use Linux-based distros as "Production" execution environment as this is the only environment that is supported. The only distro that is used in our CI tests and that is used in the &lt;a href="https://hub.docker.com/p/apache/airflow"&gt;Community managed DockerHub image&lt;/a&gt; is &lt;code&gt;Debian Bookworm&lt;/code&gt;.&lt;/p&gt; 
&lt;!-- END Requirements, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;!-- START Getting started, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;Visit the official Airflow website documentation (latest &lt;strong&gt;stable&lt;/strong&gt; release) for help with &lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/installation/"&gt;installing Airflow&lt;/a&gt;, &lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/start.html"&gt;getting started&lt;/a&gt;, or walking through a more complete &lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/tutorial/"&gt;tutorial&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: If you're looking for documentation for the main branch (latest development branch): you can find it on &lt;a href="https://s.apache.org/airflow-docs/"&gt;s.apache.org/airflow-docs&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;For more information on Airflow Improvement Proposals (AIPs), visit the &lt;a href="https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals"&gt;Airflow Wiki&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Documentation for dependent projects like provider distributions, Docker image, Helm Chart, you'll find it in &lt;a href="https://airflow.apache.org/docs/"&gt;the documentation index&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- END Getting started, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;!-- START Installing from PyPI, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Installing from PyPI&lt;/h2&gt; 
&lt;p&gt;We publish Apache Airflow as &lt;code&gt;apache-airflow&lt;/code&gt; package in PyPI. Installing it however might be sometimes tricky because Airflow is a bit of both a library and application. Libraries usually keep their dependencies open, and applications usually pin them, but we should do neither and both simultaneously. We decided to keep our dependencies as open as possible (in &lt;code&gt;pyproject.toml&lt;/code&gt;) so users can install different versions of libraries if needed. This means that &lt;code&gt;pip install apache-airflow&lt;/code&gt; will not work from time to time or will produce unusable Airflow installation.&lt;/p&gt; 
&lt;p&gt;To have repeatable installation, however, we keep a set of "known-to-be-working" constraint files in the orphan &lt;code&gt;constraints-main&lt;/code&gt; and &lt;code&gt;constraints-2-0&lt;/code&gt; branches. We keep those "known-to-be-working" constraints files separately per major/minor Python version. You can use them as constraint files when installing Airflow from PyPI. Note that you have to specify correct Airflow tag/version/branch and Python versions in the URL.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Installing just Airflow:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: Only &lt;code&gt;pip&lt;/code&gt; installation is currently officially supported.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;While it is possible to install Airflow with tools like &lt;a href="https://python-poetry.org"&gt;Poetry&lt;/a&gt; or &lt;a href="https://pypi.org/project/pip-tools"&gt;pip-tools&lt;/a&gt;, they do not share the same workflow as &lt;code&gt;pip&lt;/code&gt; - especially when it comes to constraint vs. requirements management. Installing via &lt;code&gt;Poetry&lt;/code&gt; or &lt;code&gt;pip-tools&lt;/code&gt; is not currently supported.&lt;/p&gt; 
&lt;p&gt;There are known issues with &lt;code&gt;bazel&lt;/code&gt; that might lead to circular dependencies when using it to install Airflow. Please switch to &lt;code&gt;pip&lt;/code&gt; if you encounter such problems. &lt;code&gt;Bazel&lt;/code&gt; community works on fixing the problem in &lt;code&gt;this PR &amp;lt;https://github.com/bazelbuild/rules_python/pull/1166&amp;gt;&lt;/code&gt;_ so it might be that newer versions of &lt;code&gt;bazel&lt;/code&gt; will handle it.&lt;/p&gt; 
&lt;p&gt;If you wish to install Airflow using those tools, you should use the constraint files and convert them to the appropriate format and workflow that your tool requires.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install 'apache-airflow==3.0.6' \
 --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-3.0.6/constraints-3.10.txt"
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Installing with extras (i.e., postgres, google)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install 'apache-airflow[postgres,google]==3.0.6' \
 --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-3.0.6/constraints-3.10.txt"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For information on installing provider distributions, check &lt;a href="http://airflow.apache.org/docs/apache-airflow-providers/index.html"&gt;providers&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- END Installing from PyPI, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;For comprehensive instructions on setting up your local development environment and installing Apache Airflow, please refer to the &lt;a href="https://raw.githubusercontent.com/apache/airflow/main/INSTALLING.md"&gt;INSTALLING.md&lt;/a&gt; file.&lt;/p&gt; 
&lt;!-- START Official source code, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Official source code&lt;/h2&gt; 
&lt;p&gt;Apache Airflow is an &lt;a href="https://www.apache.org"&gt;Apache Software Foundation&lt;/a&gt; (ASF) project, and our official source code releases:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Follow the &lt;a href="https://www.apache.org/legal/release-policy.html"&gt;ASF Release Policy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Can be downloaded from &lt;a href="https://downloads.apache.org/airflow"&gt;the ASF Distribution Directory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Are cryptographically signed by the release manager&lt;/li&gt; 
 &lt;li&gt;Are officially voted on by the PMC members during the &lt;a href="https://www.apache.org/legal/release-policy.html#release-approval"&gt;Release Approval Process&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Following the ASF rules, the source packages released must be sufficient for a user to build and test the release provided they have access to the appropriate platform and tools.&lt;/p&gt; 
&lt;!-- END Official source code, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Convenience packages&lt;/h2&gt; 
&lt;p&gt;There are other ways of installing and using Airflow. Those are "convenience" methods - they are not "official releases" as stated by the &lt;code&gt;ASF Release Policy&lt;/code&gt;, but they can be used by the users who do not want to build the software themselves.&lt;/p&gt; 
&lt;p&gt;Those are - in the order of most common ways people install Airflow:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/apache-airflow/"&gt;PyPI releases&lt;/a&gt; to install Airflow using standard &lt;code&gt;pip&lt;/code&gt; tool&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://hub.docker.com/r/apache/airflow"&gt;Docker Images&lt;/a&gt; to install airflow via &lt;code&gt;docker&lt;/code&gt; tool, use them in Kubernetes, Helm Charts, &lt;code&gt;docker-compose&lt;/code&gt;, &lt;code&gt;docker swarm&lt;/code&gt;, etc. You can read more about using, customizing, and extending the images in the &lt;a href="https://airflow.apache.org/docs/docker-stack/index.html"&gt;Latest docs&lt;/a&gt;, and learn details on the internals in the &lt;a href="https://airflow.apache.org/docs/docker-stack/index.html"&gt;images&lt;/a&gt; document.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/apache/airflow/tags"&gt;Tags in GitHub&lt;/a&gt; to retrieve the git project sources that were used to generate official source packages via git&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;All those artifacts are not official releases, but they are prepared using officially released sources. Some of those artifacts are "development" or "pre-release" ones, and they are clearly marked as such following the ASF Policy.&lt;/p&gt; 
&lt;h2&gt;User Interface&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;DAGs&lt;/strong&gt;: Overview of all DAGs in your environment.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/dags.png" alt="DAGs" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Assets&lt;/strong&gt;: Overview of Assets with dependencies.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/assets_graph.png" alt="Asset Dependencies" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Grid&lt;/strong&gt;: Grid representation of a DAG that spans across time.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/grid.png" alt="Grid" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Graph&lt;/strong&gt;: Visualization of a DAG's dependencies and their current status for a specific run.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/graph.png" alt="Graph" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Home&lt;/strong&gt;: Summary statistics of your Airflow environment.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/home.png" alt="Home" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Backfill&lt;/strong&gt;: Backfilling a DAG for a specific date range.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/backfill.png" alt="Backfill" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;: Quick way to view source code of a DAG.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/code.png" alt="Code" /&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Semantic versioning&lt;/h2&gt; 
&lt;p&gt;As of Airflow 2.0.0, we support a strict &lt;a href="https://semver.org/"&gt;SemVer&lt;/a&gt; approach for all packages released.&lt;/p&gt; 
&lt;p&gt;There are few specific rules that we agreed to that define details of versioning of the different packages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Airflow&lt;/strong&gt;: SemVer rules apply to core airflow only (excludes any changes to providers). Changing limits for versions of Airflow dependencies is not a breaking change on its own.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Airflow Providers&lt;/strong&gt;: SemVer rules apply to changes in the particular provider's code only. SemVer MAJOR and MINOR versions for the packages are independent of the Airflow version. For example, &lt;code&gt;google 4.1.0&lt;/code&gt; and &lt;code&gt;amazon 3.0.6&lt;/code&gt; providers can happily be installed with &lt;code&gt;Airflow 2.1.2&lt;/code&gt;. If there are limits of cross-dependencies between providers and Airflow packages, they are present in providers as &lt;code&gt;install_requires&lt;/code&gt; limitations. We aim to keep backwards compatibility of providers with all previously released Airflow 2 versions but there will sometimes be breaking changes that might make some, or all providers, have minimum Airflow version specified.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Airflow Helm Chart&lt;/strong&gt;: SemVer rules apply to changes in the chart only. SemVer MAJOR and MINOR versions for the chart are independent of the Airflow version. We aim to keep backwards compatibility of the Helm Chart with all released Airflow 2 versions, but some new features might only work starting from specific Airflow releases. We might however limit the Helm Chart to depend on minimal Airflow version.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Airflow API clients&lt;/strong&gt;: Their versioning is independent from Airflow versions. They follow their own SemVer rules for breaking changes and new features - which for example allows to change the way we generate the clients.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Version Life Cycle&lt;/h2&gt; 
&lt;p&gt;Apache Airflow version life cycle:&lt;/p&gt; 
&lt;!-- This table is automatically updated by prek scripts/ci/prek/supported_versions.py --&gt; 
&lt;!-- Beginning of auto-generated table --&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Version&lt;/th&gt; 
   &lt;th&gt;Current Patch/Minor&lt;/th&gt; 
   &lt;th&gt;State&lt;/th&gt; 
   &lt;th&gt;First Release&lt;/th&gt; 
   &lt;th&gt;Limited Maintenance&lt;/th&gt; 
   &lt;th&gt;EOL/Terminated&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;3.0.6&lt;/td&gt; 
   &lt;td&gt;Supported&lt;/td&gt; 
   &lt;td&gt;Apr 22, 2025&lt;/td&gt; 
   &lt;td&gt;TBD&lt;/td&gt; 
   &lt;td&gt;TBD&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;2.11.0&lt;/td&gt; 
   &lt;td&gt;Supported&lt;/td&gt; 
   &lt;td&gt;Dec 17, 2020&lt;/td&gt; 
   &lt;td&gt;Oct 22, 2025&lt;/td&gt; 
   &lt;td&gt;Apr 22, 2026&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1.10&lt;/td&gt; 
   &lt;td&gt;1.10.15&lt;/td&gt; 
   &lt;td&gt;EOL&lt;/td&gt; 
   &lt;td&gt;Aug 27, 2018&lt;/td&gt; 
   &lt;td&gt;Dec 17, 2020&lt;/td&gt; 
   &lt;td&gt;June 17, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1.9&lt;/td&gt; 
   &lt;td&gt;1.9.0&lt;/td&gt; 
   &lt;td&gt;EOL&lt;/td&gt; 
   &lt;td&gt;Jan 03, 2018&lt;/td&gt; 
   &lt;td&gt;Aug 27, 2018&lt;/td&gt; 
   &lt;td&gt;Aug 27, 2018&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1.8&lt;/td&gt; 
   &lt;td&gt;1.8.2&lt;/td&gt; 
   &lt;td&gt;EOL&lt;/td&gt; 
   &lt;td&gt;Mar 19, 2017&lt;/td&gt; 
   &lt;td&gt;Jan 03, 2018&lt;/td&gt; 
   &lt;td&gt;Jan 03, 2018&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1.7&lt;/td&gt; 
   &lt;td&gt;1.7.1.2&lt;/td&gt; 
   &lt;td&gt;EOL&lt;/td&gt; 
   &lt;td&gt;Mar 28, 2016&lt;/td&gt; 
   &lt;td&gt;Mar 19, 2017&lt;/td&gt; 
   &lt;td&gt;Mar 19, 2017&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!-- End of auto-generated table --&gt; 
&lt;p&gt;Limited support versions will be supported with security and critical bug fix only. EOL versions will not get any fixes nor support. We always recommend that all users run the latest available minor release for whatever major version is in use. We &lt;strong&gt;highly&lt;/strong&gt; recommend upgrading to the latest Airflow major release at the earliest convenient time and before the EOL date.&lt;/p&gt; 
&lt;h2&gt;Support for Python and Kubernetes versions&lt;/h2&gt; 
&lt;p&gt;As of Airflow 2.0, we agreed to certain rules we follow for Python and Kubernetes support. They are based on the official release schedule of Python and Kubernetes, nicely summarized in the &lt;a href="https://devguide.python.org/#status-of-python-branches"&gt;Python Developer's Guide&lt;/a&gt; and &lt;a href="https://kubernetes.io/docs/setup/release/version-skew-policy/"&gt;Kubernetes version skew policy&lt;/a&gt;.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;We drop support for Python and Kubernetes versions when they reach EOL. Except for Kubernetes, a version stays supported by Airflow if two major cloud providers still provide support for it. We drop support for those EOL versions in main right after EOL date, and it is effectively removed when we release the first new MINOR (Or MAJOR if there is no new MINOR version) of Airflow. For example, for Python 3.10 it means that we will drop support in main right after 27.06.2023, and the first MAJOR or MINOR version of Airflow released after will not have it.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;We support a new version of Python/Kubernetes in main after they are officially released, as soon as we make them work in our CI pipeline (which might not be immediate due to dependencies catching up with new versions of Python mostly) we release new images/support in Airflow based on the working CI setup.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;This policy is best-effort which means there may be situations where we might terminate support earlier if circumstances require it.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Base OS support for reference Airflow images&lt;/h2&gt; 
&lt;p&gt;The Airflow Community provides conveniently packaged container images that are published whenever we publish an Apache Airflow release. Those images contain:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Base OS with necessary packages to install Airflow (stable Debian OS)&lt;/li&gt; 
 &lt;li&gt;Base Python installation in versions supported at the time of release for the MINOR version of Airflow released (so there could be different versions for 2.3 and 2.2 line for example)&lt;/li&gt; 
 &lt;li&gt;Libraries required to connect to supported Databases (again the set of databases supported depends on the MINOR version of Airflow)&lt;/li&gt; 
 &lt;li&gt;Predefined set of popular providers (for details see the &lt;a href="https://raw.githubusercontent.com/apache/airflow/main/Dockerfile"&gt;Dockerfile&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;Possibility of building your own, custom image where the user can choose their own set of providers and libraries (see &lt;a href="https://airflow.apache.org/docs/docker-stack/build.html"&gt;Building the image&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;In the future Airflow might also support a "slim" version without providers nor database clients installed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The version of the base OS image is the stable version of Debian. Airflow supports using all currently active stable versions - as soon as all Airflow dependencies support building, and we set up the CI pipeline for building and testing the OS version. Approximately 6 months before the end-of-regular support of a previous stable version of the OS, Airflow switches the images released to use the latest supported version of the OS.&lt;/p&gt; 
&lt;p&gt;For example switch from &lt;code&gt;Debian Bullseye&lt;/code&gt; to &lt;code&gt;Debian Bookworm&lt;/code&gt; has been implemented before 2.8.0 release in October 2023 and &lt;code&gt;Debian Bookworm&lt;/code&gt; will be the only option supported as of Airflow 2.10.0.&lt;/p&gt; 
&lt;p&gt;Users will continue to be able to build their images using stable Debian releases until the end of regular support and building and verifying of the images happens in our CI but no unit tests were executed using this image in the &lt;code&gt;main&lt;/code&gt; branch.&lt;/p&gt; 
&lt;h2&gt;Approach to dependencies of Airflow&lt;/h2&gt; 
&lt;p&gt;Airflow has a lot of dependencies - direct and transitive, also Airflow is both - library and application, therefore our policies to dependencies has to include both - stability of installation of application, but also ability to install newer version of dependencies for those users who develop DAGs. We developed the approach where &lt;code&gt;constraints&lt;/code&gt; are used to make sure airflow can be installed in a repeatable way, while we do not limit our users to upgrade most of the dependencies. As a result we decided not to upper-bound version of Airflow dependencies by default, unless we have good reasons to believe upper-bounding them is needed because of importance of the dependency as well as risk it involves to upgrade specific dependency. We also upper-bound the dependencies that we know cause problems.&lt;/p&gt; 
&lt;p&gt;The constraint mechanism of ours takes care about finding and upgrading all the non-upper bound dependencies automatically (providing that all the tests pass). Our &lt;code&gt;main&lt;/code&gt; build failures will indicate in case there are versions of dependencies that break our tests - indicating that we should either upper-bind them or that we should fix our code/tests to account for the upstream changes from those dependencies.&lt;/p&gt; 
&lt;p&gt;Whenever we upper-bound such a dependency, we should always comment why we are doing it - i.e. we should have a good reason why dependency is upper-bound. And we should also mention what is the condition to remove the binding.&lt;/p&gt; 
&lt;h3&gt;Approach for dependencies for Airflow Core&lt;/h3&gt; 
&lt;p&gt;Those dependencies are maintained in &lt;code&gt;pyproject.toml&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;There are few dependencies that we decided are important enough to upper-bound them by default, as they are known to follow predictable versioning scheme, and we know that new versions of those are very likely to bring breaking changes. We commit to regularly review and attempt to upgrade to the newer versions of the dependencies as they are released, but this is manual process.&lt;/p&gt; 
&lt;p&gt;The important dependencies are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;SQLAlchemy&lt;/code&gt;: upper-bound to specific MINOR version (SQLAlchemy is known to remove deprecations and introduce breaking changes especially that support for different Databases varies and changes at various speed)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Alembic&lt;/code&gt;: it is important to handle our migrations in predictable and performant way. It is developed together with SQLAlchemy. Our experience with Alembic is that it very stable in MINOR version&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Flask&lt;/code&gt;: We are using Flask as the back-bone of our web UI and API. We know major version of Flask are very likely to introduce breaking changes across those so limiting it to MAJOR version makes sense&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;werkzeug&lt;/code&gt;: the library is known to cause problems in new versions. It is tightly coupled with Flask libraries, and we should update them together&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;celery&lt;/code&gt;: Celery is a crucial component of Airflow as it used for CeleryExecutor (and similar). Celery &lt;a href="https://docs.celeryq.dev/en/stable/contributing.html?highlight=semver#versions"&gt;follows SemVer&lt;/a&gt;, so we should upper-bound it to the next MAJOR version. Also, when we bump the upper version of the library, we should make sure Celery Provider minimum Airflow version is updated.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;kubernetes&lt;/code&gt;: Kubernetes is a crucial component of Airflow as it is used for the KubernetesExecutor (and similar). Kubernetes Python library &lt;a href="https://github.com/kubernetes-client/python#compatibility"&gt;follows SemVer&lt;/a&gt;, so we should upper-bound it to the next MAJOR version. Also, when we bump the upper version of the library, we should make sure Kubernetes Provider minimum Airflow version is updated.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Approach for dependencies in Airflow Providers and extras&lt;/h3&gt; 
&lt;p&gt;The main part of the Airflow is the Airflow Core, but the power of Airflow also comes from a number of providers that extend the core functionality and are released separately, even if we keep them (for now) in the same monorepo for convenience. You can read more about the providers in the &lt;a href="https://airflow.apache.org/docs/apache-airflow-providers/index.html"&gt;Providers documentation&lt;/a&gt;. We also have set of policies implemented for maintaining and releasing community-managed providers as well as the approach for community vs. 3rd party providers in the &lt;a href="https://github.com/apache/airflow/raw/main/PROVIDERS.rst"&gt;providers&lt;/a&gt; document.&lt;/p&gt; 
&lt;p&gt;Those &lt;code&gt;extras&lt;/code&gt; and &lt;code&gt;providers&lt;/code&gt; dependencies are maintained in &lt;code&gt;provider.yaml&lt;/code&gt; of each provider.&lt;/p&gt; 
&lt;p&gt;By default, we should not upper-bound dependencies for providers, however each provider's maintainer might decide to add additional limits (and justify them with comment).&lt;/p&gt; 
&lt;!-- START Contributing, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Want to help build Apache Airflow? Check out our &lt;a href="https://github.com/apache/airflow/raw/main/contributing-docs/README.rst"&gt;contributors' guide&lt;/a&gt; for a comprehensive overview of how to contribute, including setup instructions, coding standards, and pull request guidelines.&lt;/p&gt; 
&lt;p&gt;If you can't wait to contribute, and want to get started asap, check out the &lt;a href="https://github.com/apache/airflow/raw/main/contributing-docs/03_contributors_quick_start.rst"&gt;contribution quickstart&lt;/a&gt; here!&lt;/p&gt; 
&lt;p&gt;Official Docker (container) images for Apache Airflow are described in &lt;a href="https://github.com/apache/airflow/raw/main/dev/breeze/doc/ci/02_images.md"&gt;images&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- END Contributing, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;!-- START Who uses Apache Airflow, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Voting Policy&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Commits need a +1 vote from a committer who is not the author&lt;/li&gt; 
 &lt;li&gt;When we do AIP voting, both PMC member's and committer's &lt;code&gt;+1s&lt;/code&gt; are considered a binding vote.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Who uses Apache Airflow?&lt;/h2&gt; 
&lt;p&gt;We know about around 500 organizations that are using Apache Airflow (but there are likely many more) &lt;a href="https://github.com/apache/airflow/raw/main/INTHEWILD.md"&gt;in the wild&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you use Airflow - feel free to make a PR to add your organisation to the list.&lt;/p&gt; 
&lt;!-- END Who uses Apache Airflow, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;!-- START Who maintains Apache Airflow, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Who maintains Apache Airflow?&lt;/h2&gt; 
&lt;p&gt;Airflow is the work of the &lt;a href="https://github.com/apache/airflow/graphs/contributors"&gt;community&lt;/a&gt;, but the &lt;a href="https://people.apache.org/committers-by-project.html#airflow"&gt;core committers/maintainers&lt;/a&gt; are responsible for reviewing and merging PRs as well as steering conversations around new feature requests. If you would like to become a maintainer, please review the Apache Airflow &lt;a href="https://github.com/apache/airflow/raw/main/COMMITTERS.rst#guidelines-to-become-an-airflow-committer"&gt;committer requirements&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- END Who maintains Apache Airflow, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;What goes into the next release?&lt;/h2&gt; 
&lt;p&gt;Often you will see an issue that is assigned to specific milestone with Airflow version, or a PR that gets merged to the main branch and you might wonder which release the merged PR(s) will be released in or which release the fixed issues will be in. The answer to this is as usual - it depends on various scenarios. The answer is different for PRs and Issues.&lt;/p&gt; 
&lt;p&gt;To add a bit of context, we are following the &lt;a href="https://semver.org/"&gt;Semver&lt;/a&gt; versioning scheme as described in &lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/release-process.html"&gt;Airflow release process&lt;/a&gt;. More details are explained in detail in this README under the &lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#semantic-versioning"&gt;Semantic versioning&lt;/a&gt; chapter, but in short, we have &lt;code&gt;MAJOR.MINOR.PATCH&lt;/code&gt; versions of Airflow.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;MAJOR&lt;/code&gt; version is incremented in case of breaking changes&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;MINOR&lt;/code&gt; version is incremented when there are new features added&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;PATCH&lt;/code&gt; version is incremented when there are only bug-fixes and doc-only changes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Generally we release &lt;code&gt;MINOR&lt;/code&gt; versions of Airflow from a branch that is named after the MINOR version. For example &lt;code&gt;2.7.*&lt;/code&gt; releases are released from &lt;code&gt;v2-7-stable&lt;/code&gt; branch, &lt;code&gt;2.8.*&lt;/code&gt; releases are released from &lt;code&gt;v2-8-stable&lt;/code&gt; branch, etc.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Most of the time in our release cycle, when the branch for next &lt;code&gt;MINOR&lt;/code&gt; branch is not yet created, all PRs merged to &lt;code&gt;main&lt;/code&gt; (unless they get reverted), will find their way to the next &lt;code&gt;MINOR&lt;/code&gt; release. For example if the last release is &lt;code&gt;2.7.3&lt;/code&gt; and &lt;code&gt;v2-8-stable&lt;/code&gt; branch is not created yet, the next &lt;code&gt;MINOR&lt;/code&gt; release is &lt;code&gt;2.8.0&lt;/code&gt; and all PRs merged to main will be released in &lt;code&gt;2.8.0&lt;/code&gt;. However, some PRs (bug-fixes and doc-only changes) when merged, can be cherry-picked to current &lt;code&gt;MINOR&lt;/code&gt; branch and released in the next &lt;code&gt;PATCHLEVEL&lt;/code&gt; release. For example, if &lt;code&gt;2.8.1&lt;/code&gt; is already released and we are working on &lt;code&gt;2.9.0dev&lt;/code&gt;, then marking a PR with &lt;code&gt;2.8.2&lt;/code&gt; milestone means that it will be cherry-picked to &lt;code&gt;v2-8-test&lt;/code&gt; branch and released in &lt;code&gt;2.8.2rc1&lt;/code&gt;, and eventually in &lt;code&gt;2.8.2&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;When we prepare for the next &lt;code&gt;MINOR&lt;/code&gt; release, we cut new &lt;code&gt;v2-*-test&lt;/code&gt; and &lt;code&gt;v2-*-stable&lt;/code&gt; branch and prepare &lt;code&gt;alpha&lt;/code&gt;, &lt;code&gt;beta&lt;/code&gt; releases for the next &lt;code&gt;MINOR&lt;/code&gt; version, the PRs merged to main will still be released in the next &lt;code&gt;MINOR&lt;/code&gt; release until &lt;code&gt;rc&lt;/code&gt; version is cut. This is happening because the &lt;code&gt;v2-*-test&lt;/code&gt; and &lt;code&gt;v2-*-stable&lt;/code&gt; branches are rebased on top of main when next &lt;code&gt;beta&lt;/code&gt; and &lt;code&gt;rc&lt;/code&gt; releases are prepared. For example, when we cut &lt;code&gt;2.10.0beta1&lt;/code&gt; version, anything merged to main before &lt;code&gt;2.10.0rc1&lt;/code&gt; is released, will find its way to 2.10.0rc1.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Then, once we prepare the first RC candidate for the MINOR release, we stop moving the &lt;code&gt;v2-*-test&lt;/code&gt; and &lt;code&gt;v2-*-stable&lt;/code&gt; branches and the PRs merged to main will be released in the next &lt;code&gt;MINOR&lt;/code&gt; release. However, some PRs (bug-fixes and doc-only changes) when merged, can be cherry-picked to current &lt;code&gt;MINOR&lt;/code&gt; branch and released in the next &lt;code&gt;PATCHLEVEL&lt;/code&gt; release - for example when the last released version from &lt;code&gt;v2-10-stable&lt;/code&gt; branch is &lt;code&gt;2.10.0rc1&lt;/code&gt;, some of the PRs from main can be marked as &lt;code&gt;2.10.0&lt;/code&gt; milestone by committers, the release manager will try to cherry-pick them into the release branch. If successful, they will be released in &lt;code&gt;2.10.0rc2&lt;/code&gt; and subsequently in &lt;code&gt;2.10.0&lt;/code&gt;. This also applies to subsequent &lt;code&gt;PATCHLEVEL&lt;/code&gt; versions. When for example &lt;code&gt;2.10.1&lt;/code&gt; is already released, marking a PR with &lt;code&gt;2.10.2&lt;/code&gt; milestone will mean that it will be cherry-picked to &lt;code&gt;v2-10-stable&lt;/code&gt; branch and released in &lt;code&gt;2.10.2rc1&lt;/code&gt; and eventually in &lt;code&gt;2.10.2&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The final decision about cherry-picking is made by the release manager.&lt;/p&gt; 
&lt;p&gt;Marking issues with a milestone is a bit different. Maintainers do not mark issues with a milestone usually, normally they are only marked in PRs. If PR linked to the issue (and "fixing it") gets merged and released in a specific version following the process described above, the issue will be automatically closed, no milestone will be set for the issue, you need to check the PR that fixed the issue to see which version it was released in.&lt;/p&gt; 
&lt;p&gt;However, sometimes maintainers mark issues with specific milestone, which means that the issue is important to become a candidate to take a look when the release is being prepared. Since this is an Open-Source project, where basically all contributors volunteer their time, there is no guarantee that specific issue will be fixed in specific version. We do not want to hold the release because some issue is not fixed, so in such case release manager will reassign such unfixed issues to the next milestone in case they are not fixed in time for the current release. Therefore, the milestone for issue is more of an intent that it should be looked at, than promise it will be fixed in the version.&lt;/p&gt; 
&lt;p&gt;More context and &lt;strong&gt;FAQ&lt;/strong&gt; about the patchlevel release can be found in the &lt;a href="https://raw.githubusercontent.com/apache/airflow/main/dev/WHAT_GOES_INTO_THE_NEXT_RELEASE.md"&gt;What goes into the next release&lt;/a&gt; document in the &lt;code&gt;dev&lt;/code&gt; folder of the repository.&lt;/p&gt; 
&lt;h2&gt;Can I use the Apache Airflow logo in my presentation?&lt;/h2&gt; 
&lt;p&gt;Yes! Be sure to abide by the Apache Foundation &lt;a href="https://www.apache.org/foundation/marks/#books"&gt;trademark policies&lt;/a&gt; and the Apache Airflow &lt;a href="https://cwiki.apache.org/confluence/display/AIRFLOW/Brandbook"&gt;Brandbook&lt;/a&gt;. The most up-to-date logos are found in &lt;a href="https://github.com/apache/airflow/tree/main/airflow-core/docs/img/logos/"&gt;this repo&lt;/a&gt; and on the Apache Software Foundation &lt;a href="https://www.apache.org/logos/about.html"&gt;website&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://s.apache.org/airflow-slack"&gt;Chat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://airflow.apache.org/community/"&gt;Community Information&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;p&gt;The CI infrastructure for Apache Airflow has been sponsored by:&lt;/p&gt; 
&lt;!-- Ordered by most recently "funded" --&gt; 
&lt;p&gt;&lt;a href="https://astronomer.io"&gt;&lt;img src="https://assets2.astronomer.io/logos/logoForLIGHTbackground.png" alt="astronomer.io" width="250px" /&gt;&lt;/a&gt; &lt;a href="https://aws.amazon.com/opensource/"&gt;&lt;img src="https://github.com/apache/airflow/raw/main/providers/amazon/docs/integration-logos/AWS-Cloud-alt_light-bg@4x.png?raw=true" alt="AWS OpenSource" width="130px" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>unslothai/unsloth</title>
      <link>https://github.com/unslothai/unsloth</link>
      <description>&lt;p&gt;Fine-tuning &amp; Reinforcement Learning for LLMs. ü¶• Train OpenAI gpt-oss, Qwen3, Llama 4, DeepSeek-R1, Gemma 3, TTS 2x faster with 70% less VRAM.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://unsloth.ai"&gt;
   &lt;picture&gt; 
    &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png" /&gt; 
    &lt;source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png" /&gt; 
    &lt;img alt="unsloth logo" src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png" height="110" style="max-width: 100%;" /&gt; 
   &lt;/picture&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb"&gt;&lt;img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png" width="154" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/unsloth"&gt;&lt;img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png" width="165" /&gt;&lt;/a&gt; &lt;a href="https://docs.unsloth.ai"&gt;&lt;img src="https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/Documentation%20Button.png" width="137" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;h3&gt;Finetune gpt-oss, Gemma 3n, Qwen3, Llama 4, &amp;amp; Mistral 2x faster with 80% less VRAM!&lt;/h3&gt; 
 &lt;p&gt;&lt;img src="https://i.ibb.co/sJ7RhGG/image-41.png" alt="" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;‚ú® Finetune for Free&lt;/h2&gt; 
&lt;p&gt;Notebooks are beginner friendly. Read our &lt;a href="https://docs.unsloth.ai/get-started/fine-tuning-guide"&gt;guide&lt;/a&gt;. Add your dataset, click "Run All", and export your finetuned model to GGUF, Ollama, vLLM or Hugging Face.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Unsloth supports&lt;/th&gt; 
   &lt;th&gt;Free Notebooks&lt;/th&gt; 
   &lt;th&gt;Performance&lt;/th&gt; 
   &lt;th&gt;Memory use&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;gpt-oss (20B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.5x faster&lt;/td&gt; 
   &lt;td&gt;70% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemma 3n (4B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Conversational.ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.5x faster&lt;/td&gt; 
   &lt;td&gt;50% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen3 (14B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(14B)-Reasoning-Conversational.ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;70% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen3 (4B): GRPO&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;80% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen2.5-VL (7B): GSPO&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_5_7B_VL_GRPO.ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.5x faster&lt;/td&gt; 
   &lt;td&gt;80% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Phi-4 (14B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;70% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama 3.2 Vision (11B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;50% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama 3.1 (8B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;70% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Mistral v0.3 (7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2.2x faster&lt;/td&gt; 
   &lt;td&gt;75% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Orpheus-TTS (3B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Orpheus_(3B)-TTS.ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.5x faster&lt;/td&gt; 
   &lt;td&gt;50% less&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;See all our notebooks for: &lt;a href="https://github.com/unslothai/notebooks?tab=readme-ov-file#-kaggle-notebooks"&gt;Kaggle&lt;/a&gt;, &lt;a href="https://docs.unsloth.ai/get-started/unsloth-notebooks#grpo-reasoning-rl-notebooks"&gt;GRPO&lt;/a&gt;, &lt;strong&gt;&lt;a href="https://docs.unsloth.ai/get-started/unsloth-notebooks#text-to-speech-tts-notebooks"&gt;TTS&lt;/a&gt;&lt;/strong&gt; &amp;amp; &lt;a href="https://docs.unsloth.ai/get-started/unsloth-notebooks#vision-multimodal-notebooks"&gt;Vision&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;See &lt;a href="https://docs.unsloth.ai/get-started/all-our-models"&gt;all our models&lt;/a&gt; and &lt;a href="https://github.com/unslothai/notebooks"&gt;all our notebooks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;See detailed documentation for Unsloth &lt;a href="https://docs.unsloth.ai/"&gt;here&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚ö° Quickstart&lt;/h2&gt; 
&lt;h3&gt;Linux or WSL&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;p&gt;For Windows, &lt;code&gt;pip install unsloth&lt;/code&gt; works only if you have Pytorch installed. For more info, read our &lt;a href="https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation"&gt;Windows Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;p&gt;Use our official &lt;a href="https://hub.docker.com/r/unsloth/unsloth"&gt;Unsloth Docker image&lt;/a&gt; &lt;code&gt;unsloth/unsloth&lt;/code&gt; container. Read our &lt;a href="https://docs.unsloth.ai/get-started/install-and-update/docker"&gt;Docker Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Blackwell&lt;/h3&gt; 
&lt;p&gt;For RTX 50x, B200, 6000 GPUs, simply do &lt;code&gt;pip install unsloth&lt;/code&gt;. Read our &lt;a href="https://docs.unsloth.ai/basics/training-llms-with-blackwell-rtx-50-series-and-unsloth"&gt;Blackwell Guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;ü¶• Unsloth.ai News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üì£ &lt;strong&gt;Vision RL&lt;/strong&gt; You can now train VLMs with GRPO or GSPO in Unsloth! &lt;a href="https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl"&gt;Read guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üì£ &lt;strong&gt;Memory-efficient RL&lt;/strong&gt; We're introducing even better RL. Our new kernels &amp;amp; algos allows faster RL with 50% less VRAM &amp;amp; 10√ó more context. &lt;a href="https://docs.unsloth.ai/new/memory-efficient-rl"&gt;Read blog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üì£ &lt;strong&gt;gpt-oss&lt;/strong&gt; by OpenAI: For details on &lt;a href="https://docs.unsloth.ai/new/long-context-gpt-oss-training"&gt;Unsloth Flex Attention&lt;/a&gt;, long-context training, bug fixes, &lt;a href="https://docs.unsloth.ai/basics/gpt-oss"&gt;Read our Guide&lt;/a&gt;. 20B works on a 14GB GPU and 120B on 65GB VRAM. &lt;a href="https://huggingface.co/collections/unsloth/gpt-oss-6892433695ce0dee42f31681"&gt;gpt-oss uploads&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;üì£ &lt;strong&gt;Gemma 3n&lt;/strong&gt; by Google: &lt;a href="https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune"&gt;Read Blog&lt;/a&gt;. We &lt;a href="https://huggingface.co/collections/unsloth/gemma-3n-685d3874830e49e1c93f9339"&gt;uploaded GGUFs, 4-bit models&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;üì£ &lt;strong&gt;&lt;a href="https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning"&gt;Text-to-Speech (TTS)&lt;/a&gt;&lt;/strong&gt; is now supported, including &lt;code&gt;sesame/csm-1b&lt;/code&gt; and STT &lt;code&gt;openai/whisper-large-v3&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;üì£ &lt;strong&gt;&lt;a href="https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune"&gt;Qwen3&lt;/a&gt;&lt;/strong&gt; is now supported. Qwen3-30B-A3B fits on 17.5GB VRAM.&lt;/li&gt; 
 &lt;li&gt;üì£ Introducing &lt;strong&gt;&lt;a href="https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs"&gt;Dynamic 2.0&lt;/a&gt;&lt;/strong&gt; quants that set new benchmarks on 5-shot MMLU &amp;amp; KL Divergence.&lt;/li&gt; 
 &lt;li&gt;üì£ &lt;a href="https://unsloth.ai/blog/gemma3#everything"&gt;&lt;strong&gt;EVERYTHING&lt;/strong&gt; is now supported&lt;/a&gt; - all models (BERT, diffusion, Cohere, Mamba), FFT, etc. MultiGPU coming soon. Enable FFT with &lt;code&gt;full_finetuning = True&lt;/code&gt;, 8-bit with &lt;code&gt;load_in_8bit = True&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click for more news&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;üì£ &lt;a href="https://unsloth.ai/blog/deepseek-r1"&gt;DeepSeek-R1&lt;/a&gt; - run or fine-tune them &lt;a href="https://unsloth.ai/blog/deepseek-r1"&gt;with our guide&lt;/a&gt;. All model uploads: &lt;a href="https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üì£ Introducing Long-context &lt;a href="https://unsloth.ai/blog/grpo"&gt;Reasoning (GRPO)&lt;/a&gt; in Unsloth. Train your own reasoning model with just 5GB VRAM. Transform Llama, Phi, Mistral etc. into reasoning LLMs!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üì£ Introducing Unsloth &lt;a href="https://unsloth.ai/blog/dynamic-4bit"&gt;Dynamic 4-bit Quantization&lt;/a&gt;! We dynamically opt not to quantize certain parameters and this greatly increases accuracy while only using &amp;lt;10% more VRAM than BnB 4-bit. See our collection on &lt;a href="https://huggingface.co/collections/unsloth/unsloth-4-bit-dynamic-quants-67503bb873f89e15276c44e7"&gt;Hugging Face here.&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üì£ &lt;strong&gt;&lt;a href="https://unsloth.ai/blog/llama4"&gt;Llama 4&lt;/a&gt;&lt;/strong&gt; by Meta, including Scout &amp;amp; Maverick are now supported.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üì£ &lt;a href="https://unsloth.ai/blog/phi4"&gt;Phi-4&lt;/a&gt; by Microsoft: We also &lt;a href="https://unsloth.ai/blog/phi4"&gt;fixed bugs&lt;/a&gt; in Phi-4 and &lt;a href="https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa"&gt;uploaded GGUFs, 4-bit&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üì£ &lt;a href="https://unsloth.ai/blog/vision"&gt;Vision models&lt;/a&gt; now supported! &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb"&gt;Llama 3.2 Vision (11B)&lt;/a&gt;, &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb"&gt;Qwen 2.5 VL (7B)&lt;/a&gt; and &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Pixtral_(12B)-Vision.ipynb"&gt;Pixtral (12B) 2409&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üì£ &lt;a href="https://huggingface.co/collections/unsloth/llama-33-all-versions-67535d7d994794b9d7cf5e9f"&gt;Llama 3.3 (70B)&lt;/a&gt;, Meta's latest model is supported.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üì£ We worked with Apple to add &lt;a href="https://arxiv.org/abs/2411.09009"&gt;Cut Cross Entropy&lt;/a&gt;. Unsloth now supports 89K context for Meta's Llama 3.3 (70B) on a 80GB GPU - 13x longer than HF+FA2. For Llama 3.1 (8B), Unsloth enables 342K context, surpassing its native 128K support.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üì£ We found and helped fix a &lt;a href="https://unsloth.ai/blog/gradient"&gt;gradient accumulation bug&lt;/a&gt;! Please update Unsloth and transformers.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üì£ We cut memory usage by a &lt;a href="https://unsloth.ai/blog/long-context"&gt;further 30%&lt;/a&gt; and now support &lt;a href="https://unsloth.ai/blog/long-context"&gt;4x longer context windows&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;üîó Links and Resources&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Links&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üìö &lt;strong&gt;Documentation &amp;amp; Wiki&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.unsloth.ai"&gt;Read Our Docs&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img width="16" src="https://upload.wikimedia.org/wikipedia/commons/6/6f/Logo_of_Twitter.svg?sanitize=true" /&gt;&amp;nbsp; &lt;strong&gt;Twitter (aka X)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://twitter.com/unslothai"&gt;Follow us on X&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üíæ &lt;strong&gt;Installation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.unsloth.ai/get-started/installing-+-updating"&gt;Pip install&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üîÆ &lt;strong&gt;Our Models&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.unsloth.ai/get-started/all-our-models"&gt;Unsloth Releases&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;‚úçÔ∏è &lt;strong&gt;Blog&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://unsloth.ai/blog"&gt;Read our Blogs&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img width="15" src="https://redditinc.com/hs-fs/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png" /&gt;&amp;nbsp; &lt;strong&gt;Reddit&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://reddit.com/r/unsloth"&gt;Join our Reddit&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;‚≠ê Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports &lt;strong&gt;full-finetuning&lt;/strong&gt;, pretraining, 4b-bit, 16-bit and &lt;strong&gt;8-bit&lt;/strong&gt; training&lt;/li&gt; 
 &lt;li&gt;Supports &lt;strong&gt;all transformer-style models&lt;/strong&gt; including &lt;a href="https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning"&gt;TTS, STT&lt;/a&gt;, multimodal, diffusion, &lt;a href="https://docs.unsloth.ai/get-started/unsloth-notebooks#other-important-notebooks"&gt;BERT&lt;/a&gt; and more!&lt;/li&gt; 
 &lt;li&gt;All kernels written in &lt;a href="https://openai.com/index/triton/"&gt;OpenAI's Triton&lt;/a&gt; language. &lt;strong&gt;Manual backprop engine&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;0% loss in accuracy&lt;/strong&gt; - no approximation methods - all exact.&lt;/li&gt; 
 &lt;li&gt;No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) &lt;a href="https://developer.nvidia.com/cuda-gpus"&gt;Check your GPU!&lt;/a&gt; GTX 1070, 1080 works, but is slow.&lt;/li&gt; 
 &lt;li&gt;Works on &lt;strong&gt;Linux&lt;/strong&gt; and &lt;strong&gt;Windows&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;If you trained a model with ü¶•Unsloth, you can use this cool sticker! &amp;nbsp; &lt;img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png" width="200" align="center" /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üíæ Install Unsloth&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!warning] Python 3.14 does not support Unsloth. Use 3.13 or lower.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;You can also see our documentation for more detailed installation and updating instructions &lt;a href="https://docs.unsloth.ai/get-started/installing-+-updating"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Pip Installation&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Install with pip (recommended) for Linux devices:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;To update Unsloth:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install --upgrade --force-reinstall --no-cache-dir unsloth unsloth_zoo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://github.com/unslothai/unsloth/edit/main/README.md#advanced-pip-installation"&gt;here&lt;/a&gt; for advanced pip install instructions.&lt;/p&gt; 
&lt;h3&gt;Windows Installation&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install NVIDIA Video Driver:&lt;/strong&gt; You should install the latest version of your GPUs driver. Download drivers here: &lt;a href="https://www.nvidia.com/Download/index.aspx"&gt;NVIDIA GPU Drive&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Visual Studio C++:&lt;/strong&gt; You will need Visual Studio, with C++ installed. By default, C++ is not installed with &lt;a href="https://visualstudio.microsoft.com/vs/community/"&gt;Visual Studio&lt;/a&gt;, so make sure you select all of the C++ options. Also select options for Windows 10/11 SDK. For detailed instructions with options, see &lt;a href="https://docs.unsloth.ai/get-started/installing-+-updating"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install CUDA Toolkit:&lt;/strong&gt; Follow the instructions to install &lt;a href="https://developer.nvidia.com/cuda-toolkit-archive"&gt;CUDA Toolkit&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install PyTorch:&lt;/strong&gt; You will need the correct version of PyTorch that is compatible with your CUDA drivers, so make sure to select them carefully. &lt;a href="https://pytorch.org/get-started/locally/"&gt;Install PyTorch&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Unsloth:&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;pip install unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Notes&lt;/h4&gt; 
&lt;p&gt;To run Unsloth directly on Windows:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install Triton from this Windows fork and follow the instructions &lt;a href="https://github.com/woct0rdho/triton-windows"&gt;here&lt;/a&gt; (be aware that the Windows fork requires PyTorch &amp;gt;= 2.4 and CUDA 12)&lt;/li&gt; 
 &lt;li&gt;In the &lt;code&gt;SFTConfig&lt;/code&gt;, set &lt;code&gt;dataset_num_proc=1&lt;/code&gt; to avoid a crashing issue:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;SFTConfig(
    dataset_num_proc=1,
    ...
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Advanced/Troubleshooting&lt;/h4&gt; 
&lt;p&gt;For &lt;strong&gt;advanced installation instructions&lt;/strong&gt; or if you see weird errors during installations:&lt;/p&gt; 
&lt;p&gt;First try using an isolated environment via then &lt;code&gt;pip install unsloth&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv unsloth
source unsloth/bin/activate
pip install unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install &lt;code&gt;torch&lt;/code&gt; and &lt;code&gt;triton&lt;/code&gt;. Go to &lt;a href="https://pytorch.org"&gt;https://pytorch.org&lt;/a&gt; to install it. For example &lt;code&gt;pip install torch torchvision torchaudio triton&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Confirm if CUDA is installed correctly. Try &lt;code&gt;nvcc&lt;/code&gt;. If that fails, you need to install &lt;code&gt;cudatoolkit&lt;/code&gt; or CUDA drivers.&lt;/li&gt; 
 &lt;li&gt;Install &lt;code&gt;xformers&lt;/code&gt; manually via:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install ninja
pip install -v --no-build-isolation -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;Check if `xformers` succeeded with `python -m xformers.info` Go to https://github.com/facebookresearch/xformers. Another option is to install `flash-attn` for Ampere GPUs and ignore `xformers`
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="5"&gt; 
 &lt;li&gt;For GRPO runs, you can try installing &lt;code&gt;vllm&lt;/code&gt; and seeing if &lt;code&gt;pip install vllm&lt;/code&gt; succeeds.&lt;/li&gt; 
 &lt;li&gt;Double check that your versions of Python, CUDA, CUDNN, &lt;code&gt;torch&lt;/code&gt;, &lt;code&gt;triton&lt;/code&gt;, and &lt;code&gt;xformers&lt;/code&gt; are compatible with one another. The &lt;a href="https://github.com/pytorch/pytorch/raw/main/RELEASE.md#release-compatibility-matrix"&gt;PyTorch Compatibility Matrix&lt;/a&gt; may be useful.&lt;/li&gt; 
 &lt;li&gt;Finally, install &lt;code&gt;bitsandbytes&lt;/code&gt; and check it with &lt;code&gt;python -m bitsandbytes&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Conda Installation (Optional)&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;‚ö†Ô∏èOnly use Conda if you have it. If not, use Pip&lt;/code&gt;. Select either &lt;code&gt;pytorch-cuda=11.8,12.1&lt;/code&gt; for CUDA 11.8 or CUDA 12.1. We support &lt;code&gt;python=3.10,3.11,3.12&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create --name unsloth_env \
    python=3.11 \
    pytorch-cuda=12.1 \
    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \
    -y
conda activate unsloth_env

pip install unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;If you're looking to install Conda in a Linux environment, &lt;a href="https://docs.anaconda.com/miniconda/"&gt;read here&lt;/a&gt;, or run the below üîΩ&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;mkdir -p ~/miniconda3
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
rm -rf ~/miniconda3/miniconda.sh
~/miniconda3/bin/conda init bash
~/miniconda3/bin/conda init zsh
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Advanced Pip Installation&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;‚ö†Ô∏èDo **NOT** use this if you have Conda.&lt;/code&gt; Pip is a bit more complex since there are dependency issues. The pip command is different for &lt;code&gt;torch 2.2,2.3,2.4,2.5&lt;/code&gt; and CUDA versions.&lt;/p&gt; 
&lt;p&gt;For other torch versions, we support &lt;code&gt;torch211&lt;/code&gt;, &lt;code&gt;torch212&lt;/code&gt;, &lt;code&gt;torch220&lt;/code&gt;, &lt;code&gt;torch230&lt;/code&gt;, &lt;code&gt;torch240&lt;/code&gt; and for CUDA versions, we support &lt;code&gt;cu118&lt;/code&gt; and &lt;code&gt;cu121&lt;/code&gt; and &lt;code&gt;cu124&lt;/code&gt;. For Ampere devices (A100, H100, RTX3090) and above, use &lt;code&gt;cu118-ampere&lt;/code&gt; or &lt;code&gt;cu121-ampere&lt;/code&gt; or &lt;code&gt;cu124-ampere&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For example, if you have &lt;code&gt;torch 2.4&lt;/code&gt; and &lt;code&gt;CUDA 12.1&lt;/code&gt;, use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade pip
pip install "unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Another example, if you have &lt;code&gt;torch 2.5&lt;/code&gt; and &lt;code&gt;CUDA 12.4&lt;/code&gt;, use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade pip
pip install "unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And other examples:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "unsloth[cu121-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu118-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu118-torch240] @ git+https://github.com/unslothai/unsloth.git"

pip install "unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git"

pip install "unsloth[cu121-torch250] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu124-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, run the below in a terminal to get the &lt;strong&gt;optimal&lt;/strong&gt; pip installation command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, run the below manually in a Python REPL:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;try: import torch
except: raise ImportError('Install torch via `pip install torch`')
from packaging.version import Version as V
import re
v = V(re.match(r"[0-9\.]{3,}", torch.__version__).group(0))
cuda = str(torch.version.cuda)
is_ampere = torch.cuda.get_device_capability()[0] &amp;gt;= 8
USE_ABI = torch._C._GLIBCXX_USE_CXX11_ABI
if cuda not in ("11.8", "12.1", "12.4", "12.6", "12.8"): raise RuntimeError(f"CUDA = {cuda} not supported!")
if   v &amp;lt;= V('2.1.0'): raise RuntimeError(f"Torch = {v} too old!")
elif v &amp;lt;= V('2.1.1'): x = 'cu{}{}-torch211'
elif v &amp;lt;= V('2.1.2'): x = 'cu{}{}-torch212'
elif v  &amp;lt; V('2.3.0'): x = 'cu{}{}-torch220'
elif v  &amp;lt; V('2.4.0'): x = 'cu{}{}-torch230'
elif v  &amp;lt; V('2.5.0'): x = 'cu{}{}-torch240'
elif v  &amp;lt; V('2.5.1'): x = 'cu{}{}-torch250'
elif v &amp;lt;= V('2.5.1'): x = 'cu{}{}-torch251'
elif v  &amp;lt; V('2.7.0'): x = 'cu{}{}-torch260'
elif v  &amp;lt; V('2.7.9'): x = 'cu{}{}-torch270'
elif v  &amp;lt; V('2.8.0'): x = 'cu{}{}-torch271'
elif v  &amp;lt; V('2.8.9'): x = 'cu{}{}-torch280'
else: raise RuntimeError(f"Torch = {v} too new!")
if v &amp;gt; V('2.6.9') and cuda not in ("11.8", "12.6", "12.8"): raise RuntimeError(f"CUDA = {cuda} not supported!")
x = x.format(cuda.replace(".", ""), "-ampere" if is_ampere else "")
print(f'pip install --upgrade pip &amp;amp;&amp;amp; pip install "unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git"')
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker Installation&lt;/h3&gt; 
&lt;p&gt;You can use our pre-built Docker container with all dependencies to use Unsloth instantly with no setup required. &lt;a href="https://docs.unsloth.ai/get-started/install-and-update/docker"&gt;Read our guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;This container requires installing &lt;a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html"&gt;NVIDIA's Container Toolkit&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d -e JUPYTER_PASSWORD="mypassword" \
  -p 8888:8888 -p 2222:22 \
  -v $(pwd)/work:/workspace/work \
  --gpus all \
  unsloth/unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Access Jupyter Lab at &lt;code&gt;http://localhost:8888&lt;/code&gt; and start fine-tuning!&lt;/p&gt; 
&lt;h2&gt;üìú Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Go to our official &lt;a href="https://docs.unsloth.ai"&gt;Documentation&lt;/a&gt; for saving to GGUF, checkpointing, evaluation and more!&lt;/li&gt; 
 &lt;li&gt;We support Huggingface's TRL, Trainer, Seq2SeqTrainer or even Pytorch code!&lt;/li&gt; 
 &lt;li&gt;We're in ü§óHugging Face's official docs! Check out the &lt;a href="https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth"&gt;SFT docs&lt;/a&gt; and &lt;a href="https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth"&gt;DPO docs&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;If you want to download models from the ModelScope community, please use an environment variable: &lt;code&gt;UNSLOTH_USE_MODELSCOPE=1&lt;/code&gt;, and install the modelscope library by: &lt;code&gt;pip install modelscope -U&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;unsloth_cli.py also supports &lt;code&gt;UNSLOTH_USE_MODELSCOPE=1&lt;/code&gt; to download models and datasets. please remember to use the model and dataset id in the ModelScope community.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from unsloth import FastLanguageModel, FastModel
import torch
from trl import SFTTrainer, SFTConfig
from datasets import load_dataset
max_seq_length = 2048 # Supports RoPE Scaling internally, so choose any!
# Get LAION dataset
url = "https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl"
dataset = load_dataset("json", data_files = {"train" : url}, split = "train")

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/Meta-Llama-3.1-8B-bnb-4bit",      # Llama-3.1 2x faster
    "unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit",
    "unsloth/Meta-Llama-3.1-70B-bnb-4bit",
    "unsloth/Meta-Llama-3.1-405B-bnb-4bit",    # 4bit for 405b!
    "unsloth/Mistral-Small-Instruct-2409",     # Mistral 22b 2x faster!
    "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "unsloth/Phi-3.5-mini-instruct",           # Phi-3.5 2x faster!
    "unsloth/Phi-3-medium-4k-instruct",
    "unsloth/gemma-2-9b-bnb-4bit",
    "unsloth/gemma-2-27b-bnb-4bit",            # Gemma 2x faster!

    "unsloth/Llama-3.2-1B-bnb-4bit",           # NEW! Llama 3.2 models
    "unsloth/Llama-3.2-1B-Instruct-bnb-4bit",
    "unsloth/Llama-3.2-3B-bnb-4bit",
    "unsloth/Llama-3.2-3B-Instruct-bnb-4bit",

    "unsloth/Llama-3.3-70B-Instruct-bnb-4bit" # NEW! Llama 3.3 70B!
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastModel.from_pretrained(
    model_name = "unsloth/gemma-3-4B-it",
    max_seq_length = 2048, # Choose any for long context!
    load_in_4bit = True,  # 4 bit quantization to reduce memory
    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory
    full_finetuning = False, # [NEW!] We have full finetuning now!
    # token = "hf_...", # use one if using gated models
)

# Do model patching and add fast LoRA weights
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    max_seq_length = max_seq_length,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

trainer = SFTTrainer(
    model = model,
    train_dataset = dataset,
    tokenizer = tokenizer,
    args = SFTConfig(
        max_seq_length = max_seq_length,
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 10,
        max_steps = 60,
        logging_steps = 1,
        output_dir = "outputs",
        optim = "adamw_8bit",
        seed = 3407,
    ),
)
trainer.train()

# Go to https://github.com/unslothai/unsloth/wiki for advanced tips like
# (1) Saving to GGUF / merging to 16bit for vLLM
# (2) Continued training from a saved LoRA adapter
# (3) Adding an evaluation loop / OOMs
# (4) Customized chat templates
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a name="RL"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üí° Reinforcement Learning&lt;/h2&gt; 
&lt;p&gt;RL including DPO, GRPO, PPO, Reward Modelling, Online DPO all work with Unsloth. We're in ü§óHugging Face's official docs! We're on the &lt;a href="https://huggingface.co/learn/nlp-course/en/chapter12/6"&gt;GRPO docs&lt;/a&gt; and the &lt;a href="https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth"&gt;DPO docs&lt;/a&gt;! List of RL notebooks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Advanced Qwen3 GRPO notebook: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb"&gt;Link&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ORPO notebook: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-ORPO.ipynb"&gt;Link&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;DPO Zephyr notebook: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Zephyr_(7B)-DPO.ipynb"&gt;Link&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;KTO notebook: &lt;a href="https://colab.research.google.com/drive/1MRgGtLWuZX4ypSfGguFgC-IblTvO2ivM?usp=sharing"&gt;Link&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;SimPO notebook: &lt;a href="https://colab.research.google.com/drive/1Hs5oQDovOay4mFA6Y9lQhVJ8TnbFLFh2?usp=sharing"&gt;Link&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click for DPO code&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0" # Optional set GPU device ID

from unsloth import FastLanguageModel
import torch
from trl import DPOTrainer, DPOConfig
max_seq_length = 2048

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/zephyr-sft-bnb-4bit",
    max_seq_length = max_seq_length,
    load_in_4bit = True,
)

# Do model patching and add fast LoRA weights
model = FastLanguageModel.get_peft_model(
    model,
    r = 64,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 64,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    max_seq_length = max_seq_length,
)

dpo_trainer = DPOTrainer(
    model = model,
    ref_model = None,
    train_dataset = YOUR_DATASET_HERE,
    # eval_dataset = YOUR_DATASET_HERE,
    tokenizer = tokenizer,
    args = DPOConfig(
        per_device_train_batch_size = 4,
        gradient_accumulation_steps = 8,
        warmup_ratio = 0.1,
        num_train_epochs = 3,
        logging_steps = 1,
        optim = "adamw_8bit",
        seed = 42,
        output_dir = "outputs",
        max_length = 1024,
        max_prompt_length = 512,
        beta = 0.1,
    ),
)
dpo_trainer.train()
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;ü•á Performance Benchmarking&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;For our most detailed benchmarks, read our &lt;a href="https://unsloth.ai/blog/llama3-3"&gt;Llama 3.3 Blog&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Benchmarking of Unsloth was also conducted by &lt;a href="https://huggingface.co/blog/unsloth-trl"&gt;ü§óHugging Face&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We tested using the Alpaca Dataset, a batch size of 2, gradient accumulation steps of 4, rank = 32, and applied QLoRA on all linear layers (q, k, v, o, gate, up, down):&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;VRAM&lt;/th&gt; 
   &lt;th&gt;ü¶• Unsloth speed&lt;/th&gt; 
   &lt;th&gt;ü¶• VRAM reduction&lt;/th&gt; 
   &lt;th&gt;ü¶• Longer context&lt;/th&gt; 
   &lt;th&gt;üòä Hugging Face + FA2&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 3.3 (70B)&lt;/td&gt; 
   &lt;td&gt;80GB&lt;/td&gt; 
   &lt;td&gt;2x&lt;/td&gt; 
   &lt;td&gt;&amp;gt;75%&lt;/td&gt; 
   &lt;td&gt;13x longer&lt;/td&gt; 
   &lt;td&gt;1x&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 3.1 (8B)&lt;/td&gt; 
   &lt;td&gt;80GB&lt;/td&gt; 
   &lt;td&gt;2x&lt;/td&gt; 
   &lt;td&gt;&amp;gt;70%&lt;/td&gt; 
   &lt;td&gt;12x longer&lt;/td&gt; 
   &lt;td&gt;1x&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Context length benchmarks&lt;/h3&gt; 
&lt;h4&gt;Llama 3.1 (8B) max. context length&lt;/h4&gt; 
&lt;p&gt;We tested Llama 3.1 (8B) Instruct and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;GPU VRAM&lt;/th&gt; 
   &lt;th&gt;ü¶•Unsloth context length&lt;/th&gt; 
   &lt;th&gt;Hugging Face + FA2&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;8 GB&lt;/td&gt; 
   &lt;td&gt;2,972&lt;/td&gt; 
   &lt;td&gt;OOM&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;12 GB&lt;/td&gt; 
   &lt;td&gt;21,848&lt;/td&gt; 
   &lt;td&gt;932&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;16 GB&lt;/td&gt; 
   &lt;td&gt;40,724&lt;/td&gt; 
   &lt;td&gt;2,551&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;24 GB&lt;/td&gt; 
   &lt;td&gt;78,475&lt;/td&gt; 
   &lt;td&gt;5,789&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;40 GB&lt;/td&gt; 
   &lt;td&gt;153,977&lt;/td&gt; 
   &lt;td&gt;12,264&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;48 GB&lt;/td&gt; 
   &lt;td&gt;191,728&lt;/td&gt; 
   &lt;td&gt;15,502&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;80 GB&lt;/td&gt; 
   &lt;td&gt;342,733&lt;/td&gt; 
   &lt;td&gt;28,454&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Llama 3.3 (70B) max. context length&lt;/h4&gt; 
&lt;p&gt;We tested Llama 3.3 (70B) Instruct on a 80GB A100 and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;GPU VRAM&lt;/th&gt; 
   &lt;th&gt;ü¶•Unsloth context length&lt;/th&gt; 
   &lt;th&gt;Hugging Face + FA2&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;48 GB&lt;/td&gt; 
   &lt;td&gt;12,106&lt;/td&gt; 
   &lt;td&gt;OOM&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;80 GB&lt;/td&gt; 
   &lt;td&gt;89,389&lt;/td&gt; 
   &lt;td&gt;6,916&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://i.ibb.co/sJ7RhGG/image-41.png" alt="" /&gt; &lt;br /&gt;&lt;/p&gt; 
&lt;h3&gt;Citation&lt;/h3&gt; 
&lt;p&gt;You can cite the Unsloth repo as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@software{unsloth,
  author = {Daniel Han, Michael Han and Unsloth team},
  title = {Unsloth},
  url = {http://github.com/unslothai/unsloth},
  year = {2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Thank You to&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;The &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;llama.cpp library&lt;/a&gt; that lets users save models with Unsloth&lt;/li&gt; 
 &lt;li&gt;The Hugging Face team and their &lt;a href="https://github.com/huggingface/trl"&gt;TRL library&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/erikwijmans"&gt;Erik&lt;/a&gt; for his help adding &lt;a href="https://github.com/apple/ml-cross-entropy"&gt;Apple's ML Cross Entropy&lt;/a&gt; in Unsloth&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Etherll"&gt;Etherl&lt;/a&gt; for adding support for &lt;a href="https://github.com/unslothai/notebooks/pull/34"&gt;TTS, diffusion and BERT models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;And of course for every single person who has contributed or has used Unsloth!&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>FunAudioLLM/CosyVoice</title>
      <link>https://github.com/FunAudioLLM/CosyVoice</link>
      <description>&lt;p&gt;Multi-lingual large voice generation model, providing inference, training and deployment full-stack ability.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://github.com/Akshay090/svg-banners"&gt;&lt;img src="https://svg-banners.vercel.app/api?type=origin&amp;amp;text1=CosyVoice%F0%9F%A4%A0&amp;amp;text2=Text-to-Speech%20%F0%9F%92%96%20Large%20Language%20Model&amp;amp;width=800&amp;amp;height=210" alt="SVG Banners" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üëâüèª CosyVoice üëàüèª&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;CosyVoice 3.0&lt;/strong&gt;: &lt;a href="https://funaudiollm.github.io/cosyvoice3/"&gt;Demos&lt;/a&gt;; &lt;a href="https://arxiv.org/abs/2505.17589"&gt;Paper&lt;/a&gt;; &lt;a href="https://github.com/FunAudioLLM/CV3-Eval"&gt;CV3-Eval&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;CosyVoice 2.0&lt;/strong&gt;: &lt;a href="https://funaudiollm.github.io/cosyvoice2/"&gt;Demos&lt;/a&gt;; &lt;a href="https://arxiv.org/abs/2412.10117"&gt;Paper&lt;/a&gt;; &lt;a href="https://www.modelscope.cn/studios/iic/CosyVoice2-0.5B"&gt;Modelscope&lt;/a&gt;; &lt;a href="https://huggingface.co/spaces/FunAudioLLM/CosyVoice2-0.5B"&gt;HuggingFace&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;CosyVoice 1.0&lt;/strong&gt;: &lt;a href="https://fun-audio-llm.github.io"&gt;Demos&lt;/a&gt;; &lt;a href="https://funaudiollm.github.io/pdf/CosyVoice_v1.pdf"&gt;Paper&lt;/a&gt;; &lt;a href="https://www.modelscope.cn/studios/iic/CosyVoice-300M"&gt;Modelscope&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Highlightüî•&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;CosyVoice 2.0&lt;/strong&gt; has been released! Compared to version 1.0, the new version offers more accurate, more stable, faster, and better speech generation capabilities.&lt;/p&gt; 
&lt;h3&gt;Multilingual&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Supported Language&lt;/strong&gt;: Chinese, English, Japanese, Korean, Chinese dialects (Cantonese, Sichuanese, Shanghainese, Tianjinese, Wuhanese, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Crosslingual &amp;amp; Mixlingual&lt;/strong&gt;ÔºöSupport zero-shot voice cloning for cross-lingual and code-switching scenarios.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Ultra-Low Latency&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Bidirectional Streaming Support&lt;/strong&gt;: CosyVoice 2.0 integrates offline and streaming modeling technologies.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Rapid First Packet Synthesis&lt;/strong&gt;: Achieves latency as low as 150ms while maintaining high-quality audio output.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;High Accuracy&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Improved Pronunciation&lt;/strong&gt;: Reduces pronunciation errors by 30% to 50% compared to CosyVoice 1.0.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Benchmark Achievements&lt;/strong&gt;: Attains the lowest character error rate on the hard test set of the Seed-TTS evaluation set.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Strong Stability&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Consistency in Timbre&lt;/strong&gt;: Ensures reliable voice consistency for zero-shot and cross-language speech synthesis.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cross-language Synthesis&lt;/strong&gt;: Marked improvements compared to version 1.0.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Natural Experience&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced Prosody and Sound Quality&lt;/strong&gt;: Improved alignment of synthesized audio, raising MOS evaluation scores from 5.4 to 5.53.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Emotional and Dialectal Flexibility&lt;/strong&gt;: Now supports more granular emotional controls and accent adjustments.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;2025/08&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Thanks to the contribution from NVIDIA Yuekai Zhang, add triton trtllm runtime support and cosyvoice2 grpo training support&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;2025/07&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; release cosyvoice 3.0 eval set&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;2025/05&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; add cosyvoice 2.0 vllm support&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;2024/12&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 25hz cosyvoice 2.0 released&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;2024/09&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 25hz cosyvoice base model&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 25hz cosyvoice voice conversion model&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;2024/08&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Repetition Aware Sampling(RAS) inference for llm stability&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Streaming inference mode support, including kv cache and sdpa for rtf optimization&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;2024/07&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Flow matching training support&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; WeTextProcessing support when ttsfrd is not available&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Fastapi server and client&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Install&lt;/h2&gt; 
&lt;h3&gt;Clone and install&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Clone the repo&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;git clone --recursive https://github.com/FunAudioLLM/CosyVoice.git
# If you failed to clone the submodule due to network failures, please run the following command until success
cd CosyVoice
git submodule update --init --recursive
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install Conda: please see &lt;a href="https://docs.conda.io/en/latest/miniconda.html"&gt;https://docs.conda.io/en/latest/miniconda.html&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Create Conda env:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;conda create -n cosyvoice -y python=3.10
conda activate cosyvoice
pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/ --trusted-host=mirrors.aliyun.com

# If you encounter sox compatibility issues
# ubuntu
sudo apt-get install sox libsox-dev
# centos
sudo yum install sox sox-devel
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Model download&lt;/h3&gt; 
&lt;p&gt;We strongly recommend that you download our pretrained &lt;code&gt;CosyVoice2-0.5B&lt;/code&gt; &lt;code&gt;CosyVoice-300M&lt;/code&gt; &lt;code&gt;CosyVoice-300M-SFT&lt;/code&gt; &lt;code&gt;CosyVoice-300M-Instruct&lt;/code&gt; model and &lt;code&gt;CosyVoice-ttsfrd&lt;/code&gt; resource.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# SDKÊ®°Âûã‰∏ãËΩΩ
from modelscope import snapshot_download
snapshot_download('iic/CosyVoice2-0.5B', local_dir='pretrained_models/CosyVoice2-0.5B')
snapshot_download('iic/CosyVoice-300M', local_dir='pretrained_models/CosyVoice-300M')
snapshot_download('iic/CosyVoice-300M-SFT', local_dir='pretrained_models/CosyVoice-300M-SFT')
snapshot_download('iic/CosyVoice-300M-Instruct', local_dir='pretrained_models/CosyVoice-300M-Instruct')
snapshot_download('iic/CosyVoice-ttsfrd', local_dir='pretrained_models/CosyVoice-ttsfrd')
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# gitÊ®°Âûã‰∏ãËΩΩÔºåËØ∑Á°Æ‰øùÂ∑≤ÂÆâË£Ögit lfs
mkdir -p pretrained_models
git clone https://www.modelscope.cn/iic/CosyVoice2-0.5B.git pretrained_models/CosyVoice2-0.5B
git clone https://www.modelscope.cn/iic/CosyVoice-300M.git pretrained_models/CosyVoice-300M
git clone https://www.modelscope.cn/iic/CosyVoice-300M-SFT.git pretrained_models/CosyVoice-300M-SFT
git clone https://www.modelscope.cn/iic/CosyVoice-300M-Instruct.git pretrained_models/CosyVoice-300M-Instruct
git clone https://www.modelscope.cn/iic/CosyVoice-ttsfrd.git pretrained_models/CosyVoice-ttsfrd
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Optionally, you can unzip &lt;code&gt;ttsfrd&lt;/code&gt; resource and install &lt;code&gt;ttsfrd&lt;/code&gt; package for better text normalization performance.&lt;/p&gt; 
&lt;p&gt;Notice that this step is not necessary. If you do not install &lt;code&gt;ttsfrd&lt;/code&gt; package, we will use wetext by default.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cd pretrained_models/CosyVoice-ttsfrd/
unzip resource.zip -d .
pip install ttsfrd_dependency-0.1-py3-none-any.whl
pip install ttsfrd-0.4.2-cp310-cp310-linux_x86_64.whl
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Basic Usage&lt;/h3&gt; 
&lt;p&gt;We strongly recommend using &lt;code&gt;CosyVoice2-0.5B&lt;/code&gt; for better performance. Follow the code below for detailed usage of each model.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import sys
sys.path.append('third_party/Matcha-TTS')
from cosyvoice.cli.cosyvoice import CosyVoice, CosyVoice2
from cosyvoice.utils.file_utils import load_wav
import torchaudio
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;CosyVoice2 Usage&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;cosyvoice = CosyVoice2('pretrained_models/CosyVoice2-0.5B', load_jit=False, load_trt=False, load_vllm=False, fp16=False)

# NOTE if you want to reproduce the results on https://funaudiollm.github.io/cosyvoice2, please add text_frontend=False during inference
# zero_shot usage
prompt_speech_16k = load_wav('./asset/zero_shot_prompt.wav', 16000)
for i, j in enumerate(cosyvoice.inference_zero_shot('Êî∂Âà∞Â•ΩÂèã‰ªéËøúÊñπÂØÑÊù•ÁöÑÁîüÊó•Á§ºÁâ©ÔºåÈÇ£‰ªΩÊÑèÂ§ñÁöÑÊÉäÂñú‰∏éÊ∑±Ê∑±ÁöÑÁ•ùÁ¶èËÆ©ÊàëÂøÉ‰∏≠ÂÖÖÊª°‰∫ÜÁîúËúúÁöÑÂø´‰πêÔºåÁ¨ëÂÆπÂ¶ÇËä±ÂÑøËà¨ÁªΩÊîæ„ÄÇ', 'Â∏åÊúõ‰Ω†‰ª•ÂêéËÉΩÂ§üÂÅöÁöÑÊØîÊàëËøòÂ•ΩÂë¶„ÄÇ', prompt_speech_16k, stream=False)):
    torchaudio.save('zero_shot_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)

# save zero_shot spk for future usage
assert cosyvoice.add_zero_shot_spk('Â∏åÊúõ‰Ω†‰ª•ÂêéËÉΩÂ§üÂÅöÁöÑÊØîÊàëËøòÂ•ΩÂë¶„ÄÇ', prompt_speech_16k, 'my_zero_shot_spk') is True
for i, j in enumerate(cosyvoice.inference_zero_shot('Êî∂Âà∞Â•ΩÂèã‰ªéËøúÊñπÂØÑÊù•ÁöÑÁîüÊó•Á§ºÁâ©ÔºåÈÇ£‰ªΩÊÑèÂ§ñÁöÑÊÉäÂñú‰∏éÊ∑±Ê∑±ÁöÑÁ•ùÁ¶èËÆ©ÊàëÂøÉ‰∏≠ÂÖÖÊª°‰∫ÜÁîúËúúÁöÑÂø´‰πêÔºåÁ¨ëÂÆπÂ¶ÇËä±ÂÑøËà¨ÁªΩÊîæ„ÄÇ', '', '', zero_shot_spk_id='my_zero_shot_spk', stream=False)):
    torchaudio.save('zero_shot_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)
cosyvoice.save_spkinfo()

# fine grained control, for supported control, check cosyvoice/tokenizer/tokenizer.py#L248
for i, j in enumerate(cosyvoice.inference_cross_lingual('Âú®‰ªñËÆ≤Ëø∞ÈÇ£‰∏™ËçíËØûÊïÖ‰∫ãÁöÑËøáÁ®ã‰∏≠Ôºå‰ªñÁ™ÅÁÑ∂[laughter]ÂÅú‰∏ãÊù•ÔºåÂõ†‰∏∫‰ªñËá™Â∑±‰πüË¢´ÈÄóÁ¨ë‰∫Ü[laughter]„ÄÇ', prompt_speech_16k, stream=False)):
    torchaudio.save('fine_grained_control_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)

# instruct usage
for i, j in enumerate(cosyvoice.inference_instruct2('Êî∂Âà∞Â•ΩÂèã‰ªéËøúÊñπÂØÑÊù•ÁöÑÁîüÊó•Á§ºÁâ©ÔºåÈÇ£‰ªΩÊÑèÂ§ñÁöÑÊÉäÂñú‰∏éÊ∑±Ê∑±ÁöÑÁ•ùÁ¶èËÆ©ÊàëÂøÉ‰∏≠ÂÖÖÊª°‰∫ÜÁîúËúúÁöÑÂø´‰πêÔºåÁ¨ëÂÆπÂ¶ÇËä±ÂÑøËà¨ÁªΩÊîæ„ÄÇ', 'Áî®ÂõõÂ∑ùËØùËØ¥ËøôÂè•ËØù', prompt_speech_16k, stream=False)):
    torchaudio.save('instruct_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)

# bistream usage, you can use generator as input, this is useful when using text llm model as input
# NOTE you should still have some basic sentence split logic because llm can not handle arbitrary sentence length
def text_generator():
    yield 'Êî∂Âà∞Â•ΩÂèã‰ªéËøúÊñπÂØÑÊù•ÁöÑÁîüÊó•Á§ºÁâ©Ôºå'
    yield 'ÈÇ£‰ªΩÊÑèÂ§ñÁöÑÊÉäÂñú‰∏éÊ∑±Ê∑±ÁöÑÁ•ùÁ¶è'
    yield 'ËÆ©ÊàëÂøÉ‰∏≠ÂÖÖÊª°‰∫ÜÁîúËúúÁöÑÂø´‰πêÔºå'
    yield 'Á¨ëÂÆπÂ¶ÇËä±ÂÑøËà¨ÁªΩÊîæ„ÄÇ'
for i, j in enumerate(cosyvoice.inference_zero_shot(text_generator(), 'Â∏åÊúõ‰Ω†‰ª•ÂêéËÉΩÂ§üÂÅöÁöÑÊØîÊàëËøòÂ•ΩÂë¶„ÄÇ', prompt_speech_16k, stream=False)):
    torchaudio.save('zero_shot_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;CosyVoice2 vllm Usage&lt;/h4&gt; 
&lt;p&gt;If you want to use vllm for inference, please install &lt;code&gt;vllm==v0.9.0&lt;/code&gt;. Older vllm version do not support CosyVoice2 inference.&lt;/p&gt; 
&lt;p&gt;Notice that &lt;code&gt;vllm==v0.9.0&lt;/code&gt; has a lot of specific requirements, for example &lt;code&gt;torch==2.7.0&lt;/code&gt;. You can create a new env to in case your hardward do not support vllm and old env is corrupted.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;conda create -n cosyvoice_vllm --clone cosyvoice
conda activate cosyvoice_vllm
pip install vllm==v0.9.0 -i https://mirrors.aliyun.com/pypi/simple/ --trusted-host=mirrors.aliyun.com
python vllm_example.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;CosyVoice Usage&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;cosyvoice = CosyVoice('pretrained_models/CosyVoice-300M-SFT', load_jit=False, load_trt=False, fp16=False)
# sft usage
print(cosyvoice.list_available_spks())
# change stream=True for chunk stream inference
for i, j in enumerate(cosyvoice.inference_sft('‰Ω†Â•ΩÔºåÊàëÊòØÈÄö‰πâÁîüÊàêÂºèËØ≠Èü≥Â§ßÊ®°ÂûãÔºåËØ∑ÈóÆÊúâ‰ªÄ‰πàÂèØ‰ª•Â∏ÆÊÇ®ÁöÑÂêóÔºü', '‰∏≠ÊñáÂ•≥', stream=False)):
    torchaudio.save('sft_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)

cosyvoice = CosyVoice('pretrained_models/CosyVoice-300M')
# zero_shot usage, &amp;lt;|zh|&amp;gt;&amp;lt;|en|&amp;gt;&amp;lt;|jp|&amp;gt;&amp;lt;|yue|&amp;gt;&amp;lt;|ko|&amp;gt; for Chinese/English/Japanese/Cantonese/Korean
prompt_speech_16k = load_wav('./asset/zero_shot_prompt.wav', 16000)
for i, j in enumerate(cosyvoice.inference_zero_shot('Êî∂Âà∞Â•ΩÂèã‰ªéËøúÊñπÂØÑÊù•ÁöÑÁîüÊó•Á§ºÁâ©ÔºåÈÇ£‰ªΩÊÑèÂ§ñÁöÑÊÉäÂñú‰∏éÊ∑±Ê∑±ÁöÑÁ•ùÁ¶èËÆ©ÊàëÂøÉ‰∏≠ÂÖÖÊª°‰∫ÜÁîúËúúÁöÑÂø´‰πêÔºåÁ¨ëÂÆπÂ¶ÇËä±ÂÑøËà¨ÁªΩÊîæ„ÄÇ', 'Â∏åÊúõ‰Ω†‰ª•ÂêéËÉΩÂ§üÂÅöÁöÑÊØîÊàëËøòÂ•ΩÂë¶„ÄÇ', prompt_speech_16k, stream=False)):
    torchaudio.save('zero_shot_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)
# cross_lingual usage
prompt_speech_16k = load_wav('./asset/cross_lingual_prompt.wav', 16000)
for i, j in enumerate(cosyvoice.inference_cross_lingual('&amp;lt;|en|&amp;gt;And then later on, fully acquiring that company. So keeping management in line, interest in line with the asset that\'s coming into the family is a reason why sometimes we don\'t buy the whole thing.', prompt_speech_16k, stream=False)):
    torchaudio.save('cross_lingual_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)
# vc usage
prompt_speech_16k = load_wav('./asset/zero_shot_prompt.wav', 16000)
source_speech_16k = load_wav('./asset/cross_lingual_prompt.wav', 16000)
for i, j in enumerate(cosyvoice.inference_vc(source_speech_16k, prompt_speech_16k, stream=False)):
    torchaudio.save('vc_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)

cosyvoice = CosyVoice('pretrained_models/CosyVoice-300M-Instruct')
# instruct usage, support &amp;lt;laughter&amp;gt;&amp;lt;/laughter&amp;gt;&amp;lt;strong&amp;gt;&amp;lt;/strong&amp;gt;[laughter][breath]
for i, j in enumerate(cosyvoice.inference_instruct('Âú®Èù¢ÂØπÊåëÊàòÊó∂Ôºå‰ªñÂ±ïÁé∞‰∫ÜÈùûÂá°ÁöÑ&amp;lt;strong&amp;gt;ÂãáÊ∞î&amp;lt;/strong&amp;gt;‰∏é&amp;lt;strong&amp;gt;Êô∫ÊÖß&amp;lt;/strong&amp;gt;„ÄÇ', '‰∏≠ÊñáÁî∑', 'Theo \'Crimson\', is a fiery, passionate rebel leader. Fights with fervor for justice, but struggles with impulsiveness.', stream=False)):
    torchaudio.save('instruct_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Start web demo&lt;/h4&gt; 
&lt;p&gt;You can use our web demo page to get familiar with CosyVoice quickly.&lt;/p&gt; 
&lt;p&gt;Please see the demo website for details.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# change iic/CosyVoice-300M-SFT for sft inference, or iic/CosyVoice-300M-Instruct for instruct inference
python3 webui.py --port 50000 --model_dir pretrained_models/CosyVoice-300M
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Advanced Usage&lt;/h4&gt; 
&lt;p&gt;For advanced users, we have provided training and inference scripts in &lt;code&gt;examples/libritts/cosyvoice/run.sh&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;Build for deployment&lt;/h4&gt; 
&lt;p&gt;Optionally, if you want service deployment, You can run the following steps.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cd runtime/python
docker build -t cosyvoice:v1.0 .
# change iic/CosyVoice-300M to iic/CosyVoice-300M-Instruct if you want to use instruct inference
# for grpc usage
docker run -d --runtime=nvidia -p 50000:50000 cosyvoice:v1.0 /bin/bash -c "cd /opt/CosyVoice/CosyVoice/runtime/python/grpc &amp;amp;&amp;amp; python3 server.py --port 50000 --max_conc 4 --model_dir iic/CosyVoice-300M &amp;amp;&amp;amp; sleep infinity"
cd grpc &amp;amp;&amp;amp; python3 client.py --port 50000 --mode &amp;lt;sft|zero_shot|cross_lingual|instruct&amp;gt;
# for fastapi usage
docker run -d --runtime=nvidia -p 50000:50000 cosyvoice:v1.0 /bin/bash -c "cd /opt/CosyVoice/CosyVoice/runtime/python/fastapi &amp;amp;&amp;amp; python3 server.py --port 50000 --model_dir iic/CosyVoice-300M &amp;amp;&amp;amp; sleep infinity"
cd fastapi &amp;amp;&amp;amp; python3 client.py --port 50000 --mode &amp;lt;sft|zero_shot|cross_lingual|instruct&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Using Nvidia TensorRT-LLM for deployment&lt;/h4&gt; 
&lt;p&gt;Using TensorRT-LLM to accelerate cosyvoice2 llm could give 4x acceleration comparing with huggingface transformers implementation. To quick start:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cd runtime/triton_trtllm
docker compose up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more details, you could check &lt;a href="https://github.com/FunAudioLLM/CosyVoice/tree/main/runtime/triton_trtllm"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Discussion &amp;amp; Communication&lt;/h2&gt; 
&lt;p&gt;You can directly discuss on &lt;a href="https://github.com/FunAudioLLM/CosyVoice/issues"&gt;Github Issues&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can also scan the QR code to join our official Dingding chat group.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/FunAudioLLM/CosyVoice/main/asset/dingding.png" width="250px" /&gt; 
&lt;h2&gt;Acknowledge&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;We borrowed a lot of code from &lt;a href="https://github.com/modelscope/FunASR"&gt;FunASR&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;We borrowed a lot of code from &lt;a href="https://github.com/modelscope/FunCodec"&gt;FunCodec&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;We borrowed a lot of code from &lt;a href="https://github.com/shivammehta25/Matcha-TTS"&gt;Matcha-TTS&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;We borrowed a lot of code from &lt;a href="https://github.com/yangdongchao/AcademiCodec"&gt;AcademiCodec&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;We borrowed a lot of code from &lt;a href="https://github.com/wenet-e2e/wenet"&gt;WeNet&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Citations&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{du2024cosyvoice,
  title={Cosyvoice: A scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens},
  author={Du, Zhihao and Chen, Qian and Zhang, Shiliang and Hu, Kai and Lu, Heng and Yang, Yexin and Hu, Hangrui and Zheng, Siqi and Gu, Yue and Ma, Ziyang and others},
  journal={arXiv preprint arXiv:2407.05407},
  year={2024}
}

@article{du2024cosyvoice,
  title={Cosyvoice 2: Scalable streaming speech synthesis with large language models},
  author={Du, Zhihao and Wang, Yuxuan and Chen, Qian and Shi, Xian and Lv, Xiang and Zhao, Tianyu and Gao, Zhifu and Yang, Yexin and Gao, Changfeng and Wang, Hui and others},
  journal={arXiv preprint arXiv:2412.10117},
  year={2024}
}

@article{du2025cosyvoice,
  title={CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and Post-training},
  author={Du, Zhihao and Gao, Changfeng and Wang, Yuxuan and Yu, Fan and Zhao, Tianyu and Wang, Hao and Lv, Xiang and Wang, Hui and Shi, Xian and An, Keyu and others},
  journal={arXiv preprint arXiv:2505.17589},
  year={2025}
}

@inproceedings{lyu2025build,
  title={Build LLM-Based Zero-Shot Streaming TTS System with Cosyvoice},
  author={Lyu, Xiang and Wang, Yuxuan and Zhao, Tianyu and Wang, Hao and Liu, Huadai and Du, Zhihao},
  booktitle={ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--2},
  year={2025},
  organization={IEEE}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;The content provided above is for academic purposes only and is intended to demonstrate technical capabilities. Some examples are sourced from the internet. If any content infringes on your rights, please contact us to request its removal.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Lightricks/LTX-Video</title>
      <link>https://github.com/Lightricks/LTX-Video</link>
      <description>&lt;p&gt;Official repository for LTX-Video&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;LTX-Video&lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://www.lightricks.com/ltxv"&gt;&lt;img src="https://img.shields.io/badge/Website-LTXV-181717?logo=google-chrome" alt="Website" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/Lightricks/LTX-Video"&gt;&lt;img src="https://img.shields.io/badge/HuggingFace-Model-orange?logo=huggingface" alt="Model" /&gt;&lt;/a&gt; &lt;a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b"&gt;&lt;img src="https://img.shields.io/badge/Demo-Try%20Now-brightgreen?logo=vercel" alt="Demo" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2501.00103"&gt;&lt;img src="https://img.shields.io/badge/Paper-arXiv-B31B1B?logo=arxiv" alt="Paper" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Lightricks/LTX-Video-Trainer"&gt;&lt;img src="https://img.shields.io/badge/LTXV-Trainer-9146FF?logo=github" alt="Trainer" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/Mn8BRgUKKy"&gt;&lt;img src="https://img.shields.io/badge/Join-Discord-5865F2?logo=discord" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;This is the official repository for LTX-Video.&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#news"&gt;What's new&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#models"&gt;Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#quick-start-guide"&gt;Quick Start Guide&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#online-inference"&gt;Online demo&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#run-locally"&gt;Run locally&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#inference"&gt;Inference&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#comfyui-integration"&gt;ComfyUI Integration&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#diffusers-integration"&gt;Diffusers Integration&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#model-user-guide"&gt;Model User Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#community-contribution"&gt;Community Contribution&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#training"&gt;Training&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#control-models"&gt;Control Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#join-us-"&gt;Join Us!&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#acknowledgement"&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Introduction&lt;/h1&gt; 
&lt;p&gt;LTX-Video is the first DiT-based video generation model that can generate high-quality videos in &lt;em&gt;real-time&lt;/em&gt;. It can generate 30 FPS videos at 1216√ó704 resolution, faster than it takes to watch them. The model is trained on a large-scale dataset of diverse videos and can generate high-resolution videos with realistic and diverse content.&lt;/p&gt; 
&lt;p&gt;The model supports image-to-video, keyframe-based animation, video extension (both forward and backward), video-to-video transformations, and any combination of these features.&lt;/p&gt; 
&lt;h3&gt;Image-to-video examples&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00001.gif" alt="example1" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00002.gif" alt="example2" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00003.gif" alt="example3" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00004.gif" alt="example4" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00005.gif" alt="example5" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00006.gif" alt="example6" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00007.gif" alt="example7" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00008.gif" alt="example8" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00009.gif" alt="example9" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Controlled video examples&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_ic_2v_example_00000.gif" alt="control0" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_ic_2v_example_00001.gif" alt="control1" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_ic_2v_example_00002.gif" alt="control2" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_ic_2v_example_00003.gif" alt="control3" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_ic_2v_example_00004.gif" alt="control4" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;News&lt;/h1&gt; 
&lt;h2&gt;July, 16th, 2025: New Distilled models v0.9.8 with up to 60 seconds of video:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Long shot generation in LTXV-13B! 
  &lt;ul&gt; 
   &lt;li&gt;LTX-Video now supports up to 60 seconds of video.&lt;/li&gt; 
   &lt;li&gt;Compatible also with the official IC-LoRAs.&lt;/li&gt; 
   &lt;li&gt;Try now in &lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/tree/master/example_workflows/ltxv-13b-i2v-long-multi-prompt.json"&gt;ComfyUI&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Release a new distilled models: 
  &lt;ul&gt; 
   &lt;li&gt;13B distilled model &lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-distilled.yaml"&gt;ltxv-13b-0.9.8-distilled&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;2B distilled model &lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.8-distilled.yaml"&gt;ltxv-2b-0.9.8-distilled&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Both models are distilled from the same base model &lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-dev.yaml"&gt;ltxv-13b-0.9.8-dev&lt;/a&gt; and are compatible for use together in the same multiscale pipeline.&lt;/li&gt; 
   &lt;li&gt;Improved prompt understanding and detail generation&lt;/li&gt; 
   &lt;li&gt;Includes corresponding FP8 weights and workflows.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Release a new detailer model &lt;a href="https://huggingface.co/Lightricks/LTX-Video-ICLoRA-detailer-13b-0.9.8"&gt;LTX-Video-ICLoRA-detailer-13B-0.9.8&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Available in &lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/tree/master/example_workflows/ltxv-13b-upscale.json"&gt;ComfyUI&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;July, 8th, 2025: New Control Models Released!&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Released three new control models for LTX-Video on HuggingFace: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Depth Control&lt;/strong&gt;: &lt;a href="https://huggingface.co/Lightricks/LTX-Video-ICLoRA-depth-13b-0.9.7"&gt;LTX-Video-ICLoRA-depth-13b-0.9.7&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Pose Control&lt;/strong&gt;: &lt;a href="https://huggingface.co/Lightricks/LTX-Video-ICLoRA-pose-13b-0.9.7"&gt;LTX-Video-ICLoRA-pose-13b-0.9.7&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Canny Control&lt;/strong&gt;: &lt;a href="https://huggingface.co/Lightricks/LTX-Video-ICLoRA-canny-13b-0.9.7"&gt;LTX-Video-ICLoRA-canny-13b-0.9.7&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;May, 14th, 2025: New distilled model 13B v0.9.7:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release a new 13B distilled model &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled.safetensors"&gt;ltxv-13b-0.9.7-distilled&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Amazing for iterative work - generates HD videos in 10 seconds, with low-res preview after just 3 seconds (on H100)!&lt;/li&gt; 
   &lt;li&gt;Does not require classifier-free guidance and spatio-temporal guidance.&lt;/li&gt; 
   &lt;li&gt;Supports sampling with 8 (recommended), or less diffusion steps.&lt;/li&gt; 
   &lt;li&gt;Also released a LoRA version of the distilled model, &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled-lora128.safetensors"&gt;ltxv-13b-0.9.7-distilled-lora128&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Requires only 1GB of VRAM&lt;/li&gt; 
     &lt;li&gt;Can be used with the full 13B model for fast inference&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Release a new quantized distilled model &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled-fp8.safetensors"&gt;ltxv-13b-0.9.7-distilled-fp8&lt;/a&gt; for &lt;em&gt;real-time&lt;/em&gt; generation (on H100) with even less VRAM&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;May, 5th, 2025: New model 13B v0.9.7:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release a new 13B model &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev.safetensors"&gt;ltxv-13b-0.9.7-dev&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Release a new quantized model &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev-fp8.safetensors"&gt;ltxv-13b-0.9.7-dev-fp8&lt;/a&gt; for faster inference with less VRam&lt;/li&gt; 
 &lt;li&gt;Release a new upscalers 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-temporal-upscaler-0.9.7.safetensors"&gt;ltxv-temporal-upscaler-0.9.7&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-spatial-upscaler-0.9.7.safetensors"&gt;ltxv-spatial-upscaler-0.9.7&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Breakthrough prompt adherence and physical understanding.&lt;/li&gt; 
 &lt;li&gt;New Pipeline for multi-scale video rendering for fast and high quality results&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;April, 15th, 2025: New checkpoints v0.9.6:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release a new checkpoint &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-dev-04-25.safetensors"&gt;ltxv-2b-0.9.6-dev-04-25&lt;/a&gt; with improved quality&lt;/li&gt; 
 &lt;li&gt;Release a new distilled model &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-distilled-04-25.safetensors"&gt;ltxv-2b-0.9.6-distilled-04-25&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;15x faster inference than non-distilled model.&lt;/li&gt; 
   &lt;li&gt;Does not require classifier-free guidance and spatio-temporal guidance.&lt;/li&gt; 
   &lt;li&gt;Supports sampling with 8 (recommended), or less diffusion steps.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Improved prompt adherence, motion quality and fine details.&lt;/li&gt; 
 &lt;li&gt;New default resolution and FPS: 1216 √ó 704 pixels at 30 FPS 
  &lt;ul&gt; 
   &lt;li&gt;Still real time on H100 with the distilled model.&lt;/li&gt; 
   &lt;li&gt;Other resolutions and FPS are still supported.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Support stochastic inference (can improve visual quality when using the distilled model)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;March, 5th, 2025: New checkpoint v0.9.5&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;New license for commercial use (&lt;a href="https://huggingface.co/Lightricks/LTX-Video/ltx-video-2b-v0.9.5.license.txt"&gt;OpenRail-M&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Release a new checkpoint v0.9.5 with improved quality&lt;/li&gt; 
 &lt;li&gt;Support keyframes and video extension&lt;/li&gt; 
 &lt;li&gt;Support higher resolutions&lt;/li&gt; 
 &lt;li&gt;Improved prompt understanding&lt;/li&gt; 
 &lt;li&gt;Improved VAE&lt;/li&gt; 
 &lt;li&gt;New online web app in &lt;a href="https://app.ltx.studio/ltx-video"&gt;LTX-Studio&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Automatic prompt enhancement&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;February, 20th, 2025: More inference options&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Improve STG (Spatiotemporal Guidance) for LTX-Video&lt;/li&gt; 
 &lt;li&gt;Support MPS on macOS with PyTorch 2.3.0&lt;/li&gt; 
 &lt;li&gt;Add support for 8-bit model, LTX-VideoQ8&lt;/li&gt; 
 &lt;li&gt;Add TeaCache for LTX-Video&lt;/li&gt; 
 &lt;li&gt;Add &lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#comfyui-integration"&gt;ComfyUI-LTXTricks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Add Diffusion-Pipe&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;December 31st, 2024: Research paper&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release the &lt;a href="https://arxiv.org/abs/2501.00103"&gt;research paper&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;December 20th, 2024: New checkpoint v0.9.1&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release a new checkpoint v0.9.1 with improved quality&lt;/li&gt; 
 &lt;li&gt;Support for STG / PAG&lt;/li&gt; 
 &lt;li&gt;Support loading checkpoints of LTX-Video in Diffusers format (conversion is done on-the-fly)&lt;/li&gt; 
 &lt;li&gt;Support offloading unused parts to CPU&lt;/li&gt; 
 &lt;li&gt;Support the new timestep-conditioned VAE decoder&lt;/li&gt; 
 &lt;li&gt;Reference contributions from the community in the readme file&lt;/li&gt; 
 &lt;li&gt;Relax transformers dependency&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;November 21th, 2024: Initial release v0.9.0&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Initial release of LTX-Video&lt;/li&gt; 
 &lt;li&gt;Support text-to-video and image-to-video generation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Models &amp;amp; Workflows&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Name&lt;/th&gt; 
   &lt;th&gt;Notes&lt;/th&gt; 
   &lt;th&gt;inference.py config&lt;/th&gt; 
   &lt;th&gt;ComfyUI workflow (Recommended)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-13b-0.9.8-dev&lt;/td&gt; 
   &lt;td&gt;Highest quality, requires more VRAM&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-dev.yaml"&gt;ltxv-13b-0.9.8-dev.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ltxv-13b-i2v-base.json"&gt;ltxv-13b-i2v-base.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b"&gt;ltxv-13b-0.9.8-mix&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Mix ltxv-13b-dev and ltxv-13b-distilled in the same multi-scale rendering workflow for balanced speed-quality&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ltxv-13b-i2v-mixed-multiscale.json"&gt;ltxv-13b-i2v-mixed-multiscale.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv"&gt;ltxv-13b-0.9.8-distilled&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Faster, less VRAM usage, slight quality reduction compared to 13b. Ideal for rapid iterations&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-distilled.yaml"&gt;ltxv-13b-0.9.8-distilled.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/13b-distilled/ltxv-13b-dist-i2v-base.json"&gt;ltxv-13b-dist-i2v-base.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-2b-0.9.8-distilled&lt;/td&gt; 
   &lt;td&gt;Smaller model, slight quality reduction compared to 13b distilled. Ideal for fast generation with light VRAM usage&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.8-distilled.yaml"&gt;ltxv-2b-0.9.8-distilled.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-13b-0.9.8-dev-fp8&lt;/td&gt; 
   &lt;td&gt;Quantized version of ltxv-13b&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-dev-fp8.yaml"&gt;ltxv-13b-0.9.8-dev-fp8.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ltxv-13b-i2v-base-fp8.json"&gt;ltxv-13b-i2v-base-fp8.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-13b-0.9.8-distilled-fp8&lt;/td&gt; 
   &lt;td&gt;Quantized version of ltxv-13b-distilled&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-distilled-fp8.yaml"&gt;ltxv-13b-0.9.8-distilled-fp8.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/13b-distilled/ltxv-13b-dist-i2v-base-fp8.json"&gt;ltxv-13b-dist-i2v-base-fp8.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-2b-0.9.8-distilled-fp8&lt;/td&gt; 
   &lt;td&gt;Quantized version of ltxv-2b-distilled&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.8-distilled-fp8.yaml"&gt;ltxv-2b-0.9.8-distilled-fp8.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-2b-0.9.6&lt;/td&gt; 
   &lt;td&gt;Good quality, lower VRAM requirement than ltxv-13b&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.6-dev.yaml"&gt;ltxv-2b-0.9.6-dev.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/low_level/ltxvideo-i2v.json"&gt;ltxvideo-i2v.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-2b-0.9.6-distilled&lt;/td&gt; 
   &lt;td&gt;15√ó faster, real-time capable, fewer steps needed, no STG/CFG required&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.6-distilled.yaml"&gt;ltxv-2b-0.9.6-distilled.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/low_level/ltxvideo-i2v-distilled.json"&gt;ltxvideo-i2v-distilled.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Quick Start Guide&lt;/h1&gt; 
&lt;h2&gt;Online inference&lt;/h2&gt; 
&lt;p&gt;The model is accessible right away via the following links:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b"&gt;LTX-Studio image-to-video (13B-mix)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv"&gt;LTX-Studio image-to-video (13B distilled)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://fal.ai/models/fal-ai/ltx-video-13b-dev/image-to-video"&gt;Fal.ai image-to-video (13B full)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://fal.ai/models/fal-ai/ltx-video-13b-distilled/image-to-video"&gt;Fal.ai image-to-video (13B distilled)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://replicate.com/lightricks/ltx-video"&gt;Replicate image-to-video&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Run locally&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;The codebase was tested with Python 3.10.5, CUDA version 12.2, and supports PyTorch &amp;gt;= 2.1.2. On macOS, MPS was tested with PyTorch 2.3.0, and should support PyTorch == 2.3 or &amp;gt;= 2.6.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Lightricks/LTX-Video.git
cd LTX-Video

# create env
python -m venv env
source env/bin/activate
python -m pip install -e .\[inference\]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;FP8 Kernels (optional)&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://github.com/Lightricks/LTXVideo-Q8-Kernels"&gt;FP8 kernels&lt;/a&gt; developed for LTX-Video provide performance boost on supported graphics cards (Ada architecture and later). To install FP8 kernels, follow the instructions in that repository.&lt;/p&gt; 
&lt;h3&gt;Inference&lt;/h3&gt; 
&lt;p&gt;üìù &lt;strong&gt;Note:&lt;/strong&gt; For best results, we recommend using our &lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#comfyui-integration"&gt;ComfyUI&lt;/a&gt; workflow. We're working on updating the inference.py script to match the high quality and output fidelity of ComfyUI.&lt;/p&gt; 
&lt;p&gt;To use our model, please follow the inference code in &lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/inference.py"&gt;inference.py&lt;/a&gt;:&lt;/p&gt; 
&lt;h4&gt;For image-to-video generation:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python inference.py --prompt "PROMPT" --conditioning_media_paths IMAGE_PATH --conditioning_start_frames 0 --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Extending a video:&lt;/h4&gt; 
&lt;p&gt;üìù &lt;strong&gt;Note:&lt;/strong&gt; Input video segments must contain a multiple of 8 frames plus 1 (e.g., 9, 17, 25, etc.), and the target frame number should be a multiple of 8.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python inference.py --prompt "PROMPT" --conditioning_media_paths VIDEO_PATH --conditioning_start_frames START_FRAME --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;For video generation with multiple conditions:&lt;/h4&gt; 
&lt;p&gt;You can now generate a video conditioned on a set of images and/or short video segments. Simply provide a list of paths to the images or video segments you want to condition on, along with their target frame numbers in the generated video. You can also specify the conditioning strength for each item (default: 1.0).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python inference.py --prompt "PROMPT" --conditioning_media_paths IMAGE_OR_VIDEO_PATH_1 IMAGE_OR_VIDEO_PATH_2 --conditioning_start_frames TARGET_FRAME_1 TARGET_FRAME_2 --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using as a library&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from ltx_video.inference import infer, InferenceConfig

infer(
    InferenceConfig(
        pipeline_config="configs/ltxv-13b-0.9.8-distilled.yaml",
        prompt=PROMPT,
        height=HEIGHT,
        width=WIDTH,
        num_frames=NUM_FRAMES,
        output_path="output.mp4",
    )
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ComfyUI Integration&lt;/h2&gt; 
&lt;p&gt;To use our model with ComfyUI, please follow the instructions at &lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/"&gt;https://github.com/Lightricks/ComfyUI-LTXVideo/&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Diffusers Integration&lt;/h2&gt; 
&lt;p&gt;To use our model with the Diffusers Python library, check out the &lt;a href="https://huggingface.co/docs/diffusers/main/en/api/pipelines/ltx_video"&gt;official documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Diffusers also support an 8-bit version of LTX-Video, &lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#ltx-videoq8"&gt;see details below&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Model User Guide&lt;/h1&gt; 
&lt;h2&gt;üìù Prompt Engineering&lt;/h2&gt; 
&lt;p&gt;When writing prompts, focus on detailed, chronological descriptions of actions and scenes. Include specific movements, appearances, camera angles, and environmental details - all in a single flowing paragraph. Start directly with the action, and keep descriptions literal and precise. Think like a cinematographer describing a shot list. Keep within 200 words. For best results, build your prompts using this structure:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Start with main action in a single sentence&lt;/li&gt; 
 &lt;li&gt;Add specific details about movements and gestures&lt;/li&gt; 
 &lt;li&gt;Describe character/object appearances precisely&lt;/li&gt; 
 &lt;li&gt;Include background and environment details&lt;/li&gt; 
 &lt;li&gt;Specify camera angles and movements&lt;/li&gt; 
 &lt;li&gt;Describe lighting and colors&lt;/li&gt; 
 &lt;li&gt;Note any changes or sudden events&lt;/li&gt; 
 &lt;li&gt;See &lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#introduction"&gt;examples&lt;/a&gt; for more inspiration.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Automatic Prompt Enhancement&lt;/h3&gt; 
&lt;p&gt;When using &lt;code&gt;LTXVideoPipeline&lt;/code&gt; directly, you can enable prompt enhancement by setting &lt;code&gt;enhance_prompt=True&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;üéÆ Parameter Guide&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Resolution Preset: Higher resolutions for detailed scenes, lower for faster generation and simpler scenes. The model works on resolutions that are divisible by 32 and number of frames that are divisible by 8 + 1 (e.g. 257). In case the resolution or number of frames are not divisible by 32 or 8 + 1, the input will be padded with -1 and then cropped to the desired resolution and number of frames. The model works best on resolutions under 720 x 1280 and number of frames below 257&lt;/li&gt; 
 &lt;li&gt;Seed: Save seed values to recreate specific styles or compositions you like&lt;/li&gt; 
 &lt;li&gt;Guidance Scale: 3-3.5 are the recommended values&lt;/li&gt; 
 &lt;li&gt;Inference Steps: More steps (40+) for quality, fewer steps (20-30) for speed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üìù For advanced parameters usage, please see &lt;code&gt;python inference.py --help&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Community Contribution&lt;/h2&gt; 
&lt;h3&gt;ComfyUI-LTXTricks üõ†Ô∏è&lt;/h3&gt; 
&lt;p&gt;A community project providing additional nodes for enhanced control over the LTX Video model. It includes implementations of advanced techniques like RF-Inversion, RF-Edit, FlowEdit, and more. These nodes enable workflows such as Image and Video to Video (I+V2V), enhanced sampling via Spatiotemporal Skip Guidance (STG), and interpolation with precise frame settings.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks"&gt;ComfyUI-LTXTricks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;üîÑ &lt;strong&gt;RF-Inversion:&lt;/strong&gt; Implements &lt;a href="https://rf-inversion.github.io/"&gt;RF-Inversion&lt;/a&gt; with an &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_inversion.json"&gt;example workflow here&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;‚úÇÔ∏è &lt;strong&gt;RF-Edit:&lt;/strong&gt; Implements &lt;a href="https://github.com/wangjiangshan0725/RF-Solver-Edit"&gt;RF-Solver-Edit&lt;/a&gt; with an &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_rf_edit.json"&gt;example workflow here&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;üåä &lt;strong&gt;FlowEdit:&lt;/strong&gt; Implements &lt;a href="https://github.com/fallenshock/FlowEdit"&gt;FlowEdit&lt;/a&gt; with an &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_flow_edit.json"&gt;example workflow here&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;üé• &lt;strong&gt;I+V2V:&lt;/strong&gt; Enables Video to Video with a reference image. &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_iv2v.json"&gt;Example workflow&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;‚ú® &lt;strong&gt;Enhance:&lt;/strong&gt; Partial implementation of &lt;a href="https://junhahyung.github.io/STGuidance/"&gt;STGuidance&lt;/a&gt;. &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltxv_stg.json"&gt;Example workflow&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;üñºÔ∏è &lt;strong&gt;Interpolation and Frame Setting:&lt;/strong&gt; Nodes for precise control of latents per frame. &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_interpolation.json"&gt;Example workflow&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;LTX-VideoQ8 üé± &lt;a id="ltx-videoq8"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LTX-VideoQ8&lt;/strong&gt; is an 8-bit optimized version of &lt;a href="https://github.com/Lightricks/LTX-Video"&gt;LTX-Video&lt;/a&gt;, designed for faster performance on NVIDIA ADA GPUs.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href="https://github.com/KONAKONA666/LTX-Video"&gt;LTX-VideoQ8&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;üöÄ Up to 3X speed-up with no accuracy loss&lt;/li&gt; 
   &lt;li&gt;üé• Generate 720x480x121 videos in under a minute on RTX 4060 (8GB VRAM)&lt;/li&gt; 
   &lt;li&gt;üõ†Ô∏è Fine-tune 2B transformer models with precalculated latents&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Community Discussion:&lt;/strong&gt; &lt;a href="https://www.reddit.com/r/StableDiffusion/comments/1h79ks2/fast_ltx_video_on_rtx_4060_and_other_ada_gpus/"&gt;Reddit Thread&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Diffusers integration:&lt;/strong&gt; A diffusers integration for the 8-bit model is already out! &lt;a href="https://github.com/sayakpaul/q8-ltx-video"&gt;Details here&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;TeaCache for LTX-Video üçµ &lt;a id="TeaCache"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;TeaCache&lt;/strong&gt; is a training-free caching approach that leverages timestep differences across model outputs to accelerate LTX-Video inference by up to 2x without significant visual quality degradation.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href="https://github.com/ali-vilab/TeaCache/tree/main/TeaCache4LTX-Video"&gt;TeaCache4LTX-Video&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;üöÄ Speeds up LTX-Video inference.&lt;/li&gt; 
   &lt;li&gt;üìä Adjustable trade-offs between speed (up to 2x) and visual quality using configurable parameters.&lt;/li&gt; 
   &lt;li&gt;üõ†Ô∏è No retraining required: Works directly with existing models.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Your Contribution&lt;/h3&gt; 
&lt;p&gt;...is welcome! If you have a project or tool that integrates with LTX-Video, please let us know by opening an issue or pull request.&lt;/p&gt; 
&lt;h1&gt;‚ö°Ô∏è Training&lt;/h1&gt; 
&lt;p&gt;We provide an open-source repository for fine-tuning the LTX-Video model: &lt;a href="https://github.com/Lightricks/LTX-Video-Trainer"&gt;LTX-Video-Trainer&lt;/a&gt;. This repository supports both the 2B and 13B model variants, enabling full fine-tuning as well as LoRA (Low-Rank Adaptation) fine-tuning for more efficient training. This includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Control LoRAs&lt;/strong&gt;: Train custom control models like depth, pose, and canny control&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Effect LoRAs&lt;/strong&gt;: Create specialized effects and transformations for video generation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Explore the repository to customize the model for your specific use cases! More information and training instructions can be found in the &lt;a href="https://github.com/Lightricks/LTX-Video-Trainer/raw/main/README.md"&gt;README&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;üé¨ Control Models&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo"&gt;ComfyUI-LTXVideo&lt;/a&gt; repository now contains workflows and models for 3 specialized models that enable precise control over LTX-Video generation:&lt;/p&gt; 
&lt;p&gt;Pose Control, Depth Control and Canny Control&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Example ComfyUI Workflow (for all control types):&lt;/strong&gt; &lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ic_lora/ic-lora.json"&gt;ic-lora.json&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;üöÄ Join Us&lt;/h1&gt; 
&lt;p&gt;Want to work on cutting-edge AI research and make a real impact on millions of users worldwide?&lt;/p&gt; 
&lt;p&gt;At &lt;strong&gt;Lightricks&lt;/strong&gt;, an AI-first company, we're revolutionizing how visual content is created.&lt;/p&gt; 
&lt;p&gt;If you are passionate about AI, computer vision, and video generation, we would love to hear from you!&lt;/p&gt; 
&lt;p&gt;Please visit our &lt;a href="https://careers.lightricks.com/careers?query=&amp;amp;office=all&amp;amp;department=R%26D"&gt;careers page&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h1&gt;Acknowledgement&lt;/h1&gt; 
&lt;p&gt;We are grateful for the following awesome projects when implementing LTX-Video:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/facebookresearch/DiT"&gt;DiT&lt;/a&gt; and &lt;a href="https://github.com/PixArt-alpha/PixArt-alpha"&gt;PixArt-alpha&lt;/a&gt;: vision transformers for image generation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;üìÑ Our tech report is out! If you find our work helpful, please ‚≠êÔ∏è star the repository and cite our paper.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{HaCohen2024LTXVideo,
  title={LTX-Video: Realtime Video Latent Diffusion},
  author={HaCohen, Yoav and Chiprut, Nisan and Brazowski, Benny and Shalem, Daniel and Moshe, Dudu and Richardson, Eitan and Levin, Eran and Shiran, Guy and Zabari, Nir and Gordon, Ori and Panet, Poriya and Weissbuch, Sapir and Kulikov, Victor and Bitterman, Yaki and Melumian, Zeev and Bibi, Ofir},
  journal={arXiv preprint arXiv:2501.00103},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
  </channel>
</rss>