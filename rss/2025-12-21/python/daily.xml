<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Sat, 20 Dec 2025 01:37:22 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>GreyDGL/PentestGPT</title>
      <link>https://github.com/GreyDGL/PentestGPT</link>
      <description>&lt;p&gt;A GPT-empowered penetration testing tool&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- PROJECT SHIELDS --&gt; 
&lt;p&gt;&lt;a href="https://github.com/GreyDGL/PentestGPT/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/GreyDGL/PentestGPT.svg?style=for-the-badge" alt="Contributors" /&gt;&lt;/a&gt; &lt;a href="https://github.com/GreyDGL/PentestGPT/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/GreyDGL/PentestGPT.svg?style=for-the-badge" alt="Forks" /&gt;&lt;/a&gt; &lt;a href="https://github.com/GreyDGL/PentestGPT/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/GreyDGL/PentestGPT.svg?style=for-the-badge" alt="Stargazers" /&gt;&lt;/a&gt; &lt;a href="https://github.com/GreyDGL/PentestGPT/issues"&gt;&lt;img src="https://img.shields.io/github/issues/GreyDGL/PentestGPT.svg?style=for-the-badge" alt="Issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/GreyDGL/PentestGPT/raw/master/LICENSE.md"&gt;&lt;img src="https://img.shields.io/github/license/GreyDGL/PentestGPT.svg?style=for-the-badge" alt="MIT License" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/eC34CEfEkK"&gt;&lt;img src="https://dcbadge.vercel.app/api/server/eC34CEfEkK" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- PROJECT LOGO --&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;h3 align="center"&gt;PentestGPT&lt;/h3&gt; 
 &lt;p align="center"&gt; AI-Powered Autonomous Penetration Testing Agent &lt;br /&gt; &lt;strong&gt;Published at USENIX Security 2024&lt;/strong&gt; &lt;br /&gt; &lt;br /&gt; &lt;a href="https://www.usenix.org/conference/usenixsecurity24/presentation/deng"&gt;Research Paper&lt;/a&gt; ¬∑ &lt;a href="https://github.com/GreyDGL/PentestGPT/issues"&gt;Report Bug&lt;/a&gt; ¬∑ &lt;a href="https://github.com/GreyDGL/PentestGPT/issues"&gt;Request Feature&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;!-- ABOUT THE PROJECT --&gt; 
&lt;p&gt;&lt;a href="https://trendshift.io/repositories/3770" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/3770" alt="GreyDGL%2FPentestGPT | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] &lt;strong&gt;PentestGPT is a research prototype only&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;PentestGPT is a research prototype that pioneered the use of GenAI in cybersecurity. Please be aware of third-party services claiming to offer paid PentestGPT products - the original project is free and open-source.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://asciinema.org/a/761661"&gt;&lt;img src="https://asciinema.org/a/761661.svg?sanitize=true" alt="Installation Demo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=RUNmoXqBwVg"&gt;Watch on YouTube&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;PentestGPT in Action&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://asciinema.org/a/761663"&gt;&lt;img src="https://asciinema.org/a/761663.svg?sanitize=true" alt="PentestGPT Demo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=cWi3Yb7RmZA"&gt;Watch on YouTube&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;What's New in v1.0 (Agentic Upgrade)&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Autonomous Agent&lt;/strong&gt; - Agentic pipeline for intelligent, autonomous penetration testing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Session Persistence&lt;/strong&gt; - Save and resume penetration testing sessions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Docker-First&lt;/strong&gt; - Isolated, reproducible environment with security tools pre-installed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;In Progress&lt;/strong&gt;: Multi-model support for OpenAI, Gemini, and other LLM providers&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;AI-Powered Challenge Solver&lt;/strong&gt; - Leverages LLM advanced reasoning to perform penetration testing and CTFs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Live Walkthrough&lt;/strong&gt; - Tracks steps in real-time as the agent works through challenges&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Category Support&lt;/strong&gt; - Web, Crypto, Reversing, Forensics, PWN, Privilege Escalation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-Time Feedback&lt;/strong&gt; - Watch the AI work with live activity updates&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensible Architecture&lt;/strong&gt; - Clean, modular design ready for future enhancements&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Docker&lt;/strong&gt; (required) - &lt;a href="https://docs.docker.com/get-docker/"&gt;Install Docker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLM Provider&lt;/strong&gt; (choose one): 
  &lt;ul&gt; 
   &lt;li&gt;Anthropic API Key from &lt;a href="https://console.anthropic.com/"&gt;console.anthropic.com&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Claude OAuth Login (requires Claude subscription)&lt;/li&gt; 
   &lt;li&gt;OpenRouter for alternative models at &lt;a href="https://openrouter.ai/keys"&gt;openrouter.ai&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://docs.google.com/document/d/1ixK7x-wlr5t5TYZJdfm75UME5KnPCpS46boLkUXKg1w/edit?usp=sharing"&gt;Tutorial: Using Local Models with Claude Code&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone and build
git clone --recurse-submodules https://github.com/GreyDGL/PentestGPT.git
cd PentestGPT
make install

# Configure authentication (first time only)
make config

# Connect to container
make connect
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The &lt;code&gt;--recurse-submodules&lt;/code&gt; flag downloads the benchmark suite. If you already cloned without it, run: &lt;code&gt;git submodule update --init --recursive&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Try a Benchmark&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run pentestgpt-benchmark start XBEN-037-24 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then connect into the container and run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pentestgpt --target http://host.docker.internal:8000
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Commands Reference&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Command&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;make install&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Build the Docker image&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;make config&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Configure API key (first-time setup)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;make connect&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Connect to container (main entry point)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;make stop&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Stop container (config persists)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;make clean-docker&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Remove everything including config&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Interactive TUI mode (default)
pentestgpt --target 10.10.11.234

# Non-interactive mode
pentestgpt --target 10.10.11.100 --non-interactive

# With challenge context
pentestgpt --target 10.10.11.50 --instruction "WordPress site, focus on plugin vulnerabilities"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Keyboard Shortcuts:&lt;/strong&gt; &lt;code&gt;F1&lt;/code&gt; Help | &lt;code&gt;Ctrl+P&lt;/code&gt; Pause/Resume | &lt;code&gt;Ctrl+Q&lt;/code&gt; Quit&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Using Local LLMs&lt;/h2&gt; 
&lt;p&gt;PentestGPT supports routing requests to local LLM servers (LM Studio, Ollama, text-generation-webui, etc.) running on your host machine.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Local LLM server with an OpenAI-compatible API endpoint 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LM Studio&lt;/strong&gt;: Enable server mode (default port 1234)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt;: Run &lt;code&gt;ollama serve&lt;/code&gt; (default port 11434)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Setup&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Configure PentestGPT for local LLM
make config
# Select option 4: Local LLM

# Start your local LLM server on the host machine
# Then connect to the container
make connect
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Customizing Models&lt;/h3&gt; 
&lt;p&gt;Edit &lt;code&gt;scripts/ccr-config-template.json&lt;/code&gt; to customize:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;localLLM.api_base_url&lt;/code&gt;&lt;/strong&gt;: Your LLM server URL (default: &lt;code&gt;host.docker.internal:1234&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;localLLM.models&lt;/code&gt;&lt;/strong&gt;: Available model names on your server&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Router section&lt;/strong&gt;: Which models handle which operations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Route&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
   &lt;th&gt;Default Model&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;default&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;General tasks&lt;/td&gt; 
   &lt;td&gt;openai/gpt-oss-20b&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;background&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Background operations&lt;/td&gt; 
   &lt;td&gt;openai/gpt-oss-20b&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;think&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Reasoning-heavy tasks&lt;/td&gt; 
   &lt;td&gt;qwen/qwen3-coder-30b&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;longContext&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Large context handling&lt;/td&gt; 
   &lt;td&gt;qwen/qwen3-coder-30b&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;webSearch&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Web search operations&lt;/td&gt; 
   &lt;td&gt;openai/gpt-oss-20b&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Troubleshooting&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Connection refused&lt;/strong&gt;: Ensure your LLM server is running and listening on the configured port&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Docker networking&lt;/strong&gt;: Use &lt;code&gt;host.docker.internal&lt;/code&gt; (not &lt;code&gt;localhost&lt;/code&gt;) to access host services from Docker&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Check CCR logs&lt;/strong&gt;: Inside the container, run &lt;code&gt;cat /tmp/ccr.log&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Telemetry&lt;/h2&gt; 
&lt;p&gt;PentestGPT collects anonymous usage data to help improve the tool. This data is sent to our &lt;a href="https://langfuse.com"&gt;Langfuse&lt;/a&gt; project and includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Session metadata (target type, duration, completion status)&lt;/li&gt; 
 &lt;li&gt;Tool execution patterns (which tools are used, not the actual commands)&lt;/li&gt; 
 &lt;li&gt;Flag detection events (that a flag was found, not the flag content)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;No sensitive data is collected&lt;/strong&gt; - command outputs, credentials, or actual flag values are never transmitted.&lt;/p&gt; 
&lt;h3&gt;Opting Out&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Via command line flag
pentestgpt --target 10.10.11.234 --no-telemetry

# Via environment variable
export LANGFUSE_ENABLED=false
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Benchmarks&lt;/h2&gt; 
&lt;p&gt;PentestGPT includes 100+ vulnerability challenges for testing and development.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pentestgpt-benchmark list                    # List all benchmarks
pentestgpt-benchmark list --levels 1         # Filter by difficulty
pentestgpt-benchmark list --tags sqli        # Filter by vulnerability type
pentestgpt-benchmark start XBEN-037-24       # Start a benchmark
pentestgpt-benchmark status                  # Check running benchmarks
pentestgpt-benchmark stop XBEN-037-24        # Stop a benchmark
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Available Tags:&lt;/strong&gt; &lt;code&gt;sqli&lt;/code&gt;, &lt;code&gt;xss&lt;/code&gt;, &lt;code&gt;idor&lt;/code&gt;, &lt;code&gt;ssti&lt;/code&gt;, &lt;code&gt;ssrf&lt;/code&gt;, &lt;code&gt;lfi&lt;/code&gt;, &lt;code&gt;rce&lt;/code&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;uv&lt;/strong&gt; (required) - Python package manager: &lt;code&gt;curl -LsSf https://astral.sh/uv/install.sh | sh&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Claude Code CLI&lt;/strong&gt; - Configure with &lt;code&gt;claude login&lt;/code&gt; or &lt;code&gt;export ANTHROPIC_API_KEY='your-key'&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://docs.google.com/document/d/1ixK7x-wlr5t5TYZJdfm75UME5KnPCpS46boLkUXKg1w/edit?usp=sharing"&gt;Tutorial: Using Local Models with Claude Code&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Local Development&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv sync                                      # Install dependencies
uv run pentestgpt --target 10.10.11.234      # Run locally
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Project Commands&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make test          # Run pytest
make lint          # Run ruff linter
make typecheck     # Run mypy
make ci            # Run full CI simulation (lint, format, typecheck, test, build)
make ci-quick      # Quick CI without build step
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Legacy Version&lt;/h2&gt; 
&lt;p&gt;The previous multi-LLM version (v0.15) supporting OpenAI, Gemini, Deepseek, and Ollama is archived in &lt;a href="https://raw.githubusercontent.com/GreyDGL/PentestGPT/main/legacy/"&gt;&lt;code&gt;legacy/&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd legacy &amp;amp;&amp;amp; pip install -e . &amp;amp;&amp;amp; pentestgpt --reasoning gpt-4o
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use PentestGPT in your research, please cite our paper:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@inproceedings{299699,
  author = {Gelei Deng and Yi Liu and V√≠ctor Mayoral-Vilches and Peng Liu and Yuekang Li and Yuan Xu and Tianwei Zhang and Yang Liu and Martin Pinzger and Stefan Rass},
  title = {{PentestGPT}: Evaluating and Harnessing Large Language Models for Automated Penetration Testing},
  booktitle = {33rd USENIX Security Symposium (USENIX Security 24)},
  year = {2024},
  isbn = {978-1-939133-44-1},
  address = {Philadelphia, PA},
  pages = {847--864},
  url = {https://www.usenix.org/conference/usenixsecurity24/presentation/deng},
  publisher = {USENIX Association},
  month = aug
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Distributed under the MIT License. See &lt;code&gt;LICENSE.md&lt;/code&gt; for more information.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: This tool is for educational purposes and authorized security testing only. The authors do not condone any illegal use. Use at your own risk.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Contact&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Gelei Deng&lt;/strong&gt; - &lt;a href="https://www.linkedin.com/in/gelei-deng-225a10112/"&gt;&lt;img src="https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&amp;amp;logo=linkedin&amp;amp;colorB=555" alt="LinkedIn" /&gt;&lt;/a&gt; - &lt;a href="mailto:gelei.deng@ntu.edu.sg"&gt;gelei.deng@ntu.edu.sg&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Yi Liu&lt;/strong&gt; - &lt;a href="mailto:yi009@e.ntu.edu.sg"&gt;yi009@e.ntu.edu.sg&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Yuekang Li&lt;/strong&gt; - &lt;a href="mailto:yuekang.li@unsw.edu.au"&gt;yuekang.li@unsw.edu.au&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;V√≠ctor Mayoral Vilches&lt;/strong&gt; - &lt;a href="https://www.linkedin.com/in/vmayoral/"&gt;&lt;img src="https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&amp;amp;logo=linkedin&amp;amp;colorB=555" alt="LinkedIn" /&gt;&lt;/a&gt; - &lt;a href="mailto:v.mayoralv@gmail.com"&gt;v.mayoralv@gmail.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Peng Liu&lt;/strong&gt; - &lt;a href="mailto:liu_peng@i2r.a-star.edu.sg"&gt;liu_peng@i2r.a-star.edu.sg&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Acknowledgments&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Research supported by &lt;a href="https://www.quantstamp.com/"&gt;Quantstamp&lt;/a&gt; and &lt;a href="https://www.ntu.edu.sg/"&gt;NTU Singapore&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/GreyDGL/PentestGPT/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt;</description>
    </item>
    
    <item>
      <title>exo-explore/exo</title>
      <link>https://github.com/exo-explore/exo</link>
      <description>&lt;p&gt;Run your own AI cluster at home with everyday devices üì±üíª üñ•Ô∏è‚åö&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="/docs/exo-logo-black-bg.jpg" /&gt; 
  &lt;img alt="exo logo" src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/exo-logo-transparent.png" width="50%" height="50%" /&gt; 
 &lt;/picture&gt; 
 &lt;p&gt;exo: Run your own AI cluster at home with everyday devices. Maintained by &lt;a href="https://x.com/exolabs"&gt;exo labs&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/exo-explore/exo/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/exo-explore/exo" alt="GitHub Repo stars" /&gt;&lt;/a&gt; &lt;a href="https://www.apache.org/licenses/LICENSE-2.0.html"&gt;&lt;img src="https://img.shields.io/badge/License-Apache2.0-blue.svg?sanitize=true" alt="License: Apache-2.0" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/11849" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/11849" alt="exo-explore%2Fexo | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;EXO connects all your devices into an AI cluster. It pools together the resources of all your devices in order to run large models. Not only does EXO enable running models larger than would fit on a single device, but with &lt;a href="https://x.com/exolabs/status/2001817749744476256?s=20"&gt;day-0 support for RDMA over Thunderbolt&lt;/a&gt;, makes models run faster as you add more devices.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic Device Discovery&lt;/strong&gt;: Devices running EXO automatically discover each other - no manual configuration.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;RDMA over Thunderbolt&lt;/strong&gt;: EXO ships with &lt;a href="https://x.com/exolabs/status/2001817749744476256?s=20"&gt;day-0 support for RDMA over Thunderbolt 5&lt;/a&gt;, enabling 99% reduction in latency between devices.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Topology-Aware Auto Parallel&lt;/strong&gt;: EXO figures out the best way to split your model across all available devices based on a realtime view of your device topology. It takes into account device resources and network latency/bandwidth between each link.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tensor Parallelism&lt;/strong&gt;: EXO supports sharding models, for up to 1.8x speedup on 2 devices and 3.2x speedup on 4 devices.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MLX Support&lt;/strong&gt;: EXO uses &lt;a href="https://github.com/ml-explore/mlx"&gt;MLX&lt;/a&gt; as an inference backend and &lt;a href="https://ml-explore.github.io/mlx/build/html/usage/distributed.html"&gt;MLX distributed&lt;/a&gt; for distributed communication.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Benchmarks&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;Qwen3-235B (8-bit) on 4 √ó M3 Ultra Mac Studio with Tensor Parallel RDMA&lt;/summary&gt; 
 &lt;img src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/benchmarks/jeffgeerling/mac-studio-cluster-ai-full-1-qwen3-235b.jpeg" alt="Benchmark - Qwen3-235B (8-bit) on 4 √ó M3 Ultra Mac Studio with Tensor Parallel RDMA" width="80%" /&gt; 
 &lt;p&gt; &lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5"&gt;Jeff Geerling: 15 TB VRAM on Mac Studio ‚Äì RDMA over Thunderbolt‚ÄØ5&lt;/a&gt; &lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;DeepSeek v3.1 671B (8-bit) on 4 √ó M3 Ultra Mac Studio with Tensor Parallel RDMA&lt;/summary&gt; 
 &lt;img src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/benchmarks/jeffgeerling/mac-studio-cluster-ai-full-2-deepseek-3.1-671b.jpeg" alt="Benchmark - DeepSeek v3.1 671B (8-bit) on 4 √ó M3 Ultra Mac Studio with Tensor Parallel RDMA" width="80%" /&gt; 
 &lt;p&gt; &lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5"&gt;Jeff Geerling: 15 TB VRAM on Mac Studio ‚Äì RDMA over Thunderbolt‚ÄØ5&lt;/a&gt; &lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Kimi K2 Thinking (native 4-bit) on 4 √ó M3 Ultra Mac Studio with Tensor Parallel RDMA&lt;/summary&gt; 
 &lt;img src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/benchmarks/jeffgeerling/mac-studio-cluster-ai-full-3-kimi-k2-thinking.jpeg" alt="Benchmark - Kimi K2 Thinking (native 4-bit) on 4 √ó M3 Ultra Mac Studio with Tensor Parallel RDMA" width="80%" /&gt; 
 &lt;p&gt; &lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5"&gt;Jeff Geerling: 15 TB VRAM on Mac Studio ‚Äì RDMA over Thunderbolt‚ÄØ5&lt;/a&gt; &lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;Devices running EXO automatically discover each other, without needing any manual configuration. Each device provides an API and a dashboard for interacting with your cluster (runs at &lt;code&gt;http://localhost:52415&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;There are two ways to run EXO:&lt;/p&gt; 
&lt;h3&gt;Run from Source (Mac &amp;amp; Linux)&lt;/h3&gt; 
&lt;p&gt;Clone the repo, build the dashboard, and run EXO:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd dashboard &amp;amp;&amp;amp; npm install &amp;amp;&amp;amp; npm run build
uv run exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;One-liner:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/exo-explore/exo &amp;amp;&amp;amp; cd exo/dashboard &amp;amp;&amp;amp; npm i &amp;amp;&amp;amp; npm run build &amp;amp;&amp;amp; cd .. &amp;amp;&amp;amp; uv run exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h3&gt;macOS App&lt;/h3&gt; 
&lt;p&gt;EXO ships a macOS app that runs in the background on your Mac.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/macos-app-one-macbook.png" alt="EXO macOS App - running on a MacBook" width="35%" /&gt; 
&lt;p&gt;The macOS app requires macOS Tahoe 26.2 or later.&lt;/p&gt; 
&lt;p&gt;Download the latest build here: &lt;a href="https://assets.exolabs.net/EXO-latest.dmg"&gt;EXO-latest.dmg&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The app will ask for permission to modify system settings and install a new Network profile. Improvements to this are being worked on.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Hardware Accelerator Support&lt;/h2&gt; 
&lt;p&gt;On macOS, EXO uses the GPU. On Linux, EXO currently runs on CPU. We are working on extending hardware accelerator support. If you'd like support for a new hardware platform, please search for an existing feature request and add a thumbs up so we know what hardware is important to the community.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for guidelines on how to contribute to EXO.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>astral-sh/ty</title>
      <link>https://github.com/astral-sh/ty</link>
      <description>&lt;p&gt;An extremely fast Python type checker and language server, written in Rust.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ty&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/astral-sh/ty"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ty/main/assets/badge/v0.json" alt="ty" /&gt;&lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/ty"&gt;&lt;img src="https://img.shields.io/pypi/v/ty.svg?sanitize=true" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/astral-sh"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;An extremely fast Python type checker and language server, written in Rust.&lt;/p&gt; 
&lt;br /&gt; 
&lt;p align="center"&gt; &lt;img alt="Shows a bar chart with benchmark results." width="500px" src="https://raw.githubusercontent.com/astral-sh/ty/main/docs/assets/ty-benchmark-cli.svg?sanitize=true" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;i&gt;Type checking the &lt;a href="https://github.com/home-assistant/core"&gt;home-assistant&lt;/a&gt; project without caching.&lt;/i&gt; &lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;ty is backed by &lt;a href="https://astral.sh"&gt;Astral&lt;/a&gt;, the creators of &lt;a href="https://github.com/astral-sh/uv"&gt;uv&lt;/a&gt; and &lt;a href="https://github.com/astral-sh/ruff"&gt;Ruff&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Highlights&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;10x - 100x faster than mypy and Pyright&lt;/li&gt; 
 &lt;li&gt;Comprehensive &lt;a href="https://docs.astral.sh/ty/features/diagnostics/"&gt;diagnostics&lt;/a&gt; with rich contextual information&lt;/li&gt; 
 &lt;li&gt;Configurable &lt;a href="https://docs.astral.sh/ty/rules/"&gt;rule levels&lt;/a&gt;, &lt;a href="https://docs.astral.sh/ty/reference/configuration/#overrides"&gt;per-file overrides&lt;/a&gt;, &lt;a href="https://docs.astral.sh/ty/suppression/"&gt;suppression comments&lt;/a&gt;, and first-class project support&lt;/li&gt; 
 &lt;li&gt;Designed for adoption, with support for &lt;a href="https://docs.astral.sh/ty/features/type-system/#redeclarations"&gt;redeclarations&lt;/a&gt; and &lt;a href="https://docs.astral.sh/ty/features/type-system/#gradual-guarantee"&gt;partially typed code&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.astral.sh/ty/features/language-server/"&gt;Language server&lt;/a&gt; with code navigation, completions, code actions, auto-import, inlay hints, on-hover help, etc.&lt;/li&gt; 
 &lt;li&gt;Fine-grained &lt;a href="https://docs.astral.sh/ty/features/language-server/#fine-grained-incrementality"&gt;incremental analysis&lt;/a&gt; designed for fast updates when editing files in an IDE&lt;/li&gt; 
 &lt;li&gt;Editor integrations for &lt;a href="https://docs.astral.sh/ty/editors/#vs-code"&gt;VS Code&lt;/a&gt;, &lt;a href="https://docs.astral.sh/ty/editors/#pycharm"&gt;PyCharm&lt;/a&gt;, &lt;a href="https://docs.astral.sh/ty/editors/#neovim"&gt;Neovim&lt;/a&gt; and more&lt;/li&gt; 
 &lt;li&gt;Advanced typing features like first-class &lt;a href="https://docs.astral.sh/ty/features/type-system/#intersection-types"&gt;intersection types&lt;/a&gt;, advanced &lt;a href="https://docs.astral.sh/ty/features/type-system/#top-and-bottom-materializations"&gt;type narrowing&lt;/a&gt;, and &lt;a href="https://docs.astral.sh/ty/features/type-system/#reachability-based-on-types"&gt;sophisticated reachability analysis&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;Run ty with &lt;a href="https://docs.astral.sh/uv/guides/tools/#running-tools"&gt;uvx&lt;/a&gt; to get started quickly:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;uvx ty check
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, check out the &lt;a href="https://play.ty.dev"&gt;ty playground&lt;/a&gt; to try it out in your browser.&lt;/p&gt; 
&lt;p&gt;To learn more about using ty, see the &lt;a href="https://docs.astral.sh/ty/"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install ty, see the &lt;a href="https://docs.astral.sh/ty/installation/"&gt;installation&lt;/a&gt; documentation.&lt;/p&gt; 
&lt;p&gt;To add the ty language server to your editor, see the &lt;a href="https://docs.astral.sh/ty/editors/"&gt;editor integration&lt;/a&gt; guide.&lt;/p&gt; 
&lt;h2&gt;Getting help&lt;/h2&gt; 
&lt;p&gt;If you have questions or want to report a bug, please open an &lt;a href="https://github.com/astral-sh/ty/issues"&gt;issue&lt;/a&gt; in this repository.&lt;/p&gt; 
&lt;p&gt;You may also join our &lt;a href="https://discord.com/invite/astral-sh"&gt;Discord server&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Development of this project takes place in the &lt;a href="https://github.com/astral-sh/ruff"&gt;Ruff&lt;/a&gt; repository at this time. Please &lt;a href="https://github.com/astral-sh/ruff/pulls"&gt;open pull requests&lt;/a&gt; there for changes to anything in the &lt;code&gt;ruff&lt;/code&gt; submodule (which includes all of the Rust source code).&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/astral-sh/ty/main/CONTRIBUTING.md"&gt;contributing guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;!-- We intentionally use smaller headings for the FAQ items --&gt; 
&lt;!-- markdownlint-disable MD001 --&gt; 
&lt;h4&gt;Why is ty doing _____?&lt;/h4&gt; 
&lt;p&gt;See our &lt;a href="https://docs.astral.sh/ty/reference/typing-faq"&gt;typing FAQ&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;How do you pronounce ty?&lt;/h4&gt; 
&lt;p&gt;It's pronounced as "tee - why" (&lt;a href="https://en.wikipedia.org/wiki/Help:IPA/English#Key"&gt;&lt;code&gt;/tiÀê wa…™/&lt;/code&gt;&lt;/a&gt;)&lt;/p&gt; 
&lt;h4&gt;How should I stylize ty?&lt;/h4&gt; 
&lt;p&gt;Just "ty", please.&lt;/p&gt; 
&lt;!-- markdownlint-enable MD001 --&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;ty is licensed under the MIT license (&lt;a href="https://raw.githubusercontent.com/astral-sh/ty/main/LICENSE"&gt;LICENSE&lt;/a&gt; or &lt;a href="https://opensource.org/licenses/MIT"&gt;https://opensource.org/licenses/MIT&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in ty by you, as defined in the MIT license, shall be licensed as above, without any additional terms or conditions.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a target="_blank" href="https://astral.sh" style="background:none"&gt; &lt;img src="https://raw.githubusercontent.com/astral-sh/uv/main/assets/svg/Astral.svg?sanitize=true" alt="Made by Astral" /&gt; &lt;/a&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>letta-ai/letta</title>
      <link>https://github.com/letta-ai/letta</link>
      <description>&lt;p&gt;Letta is the platform for building stateful agents: open AI with advanced memory that can learn and self-improve over time.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/letta-ai/letta/refs/heads/main/assets/Letta-logo-RGB_GreyonTransparent_cropped_small.png" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/letta-ai/letta/refs/heads/main/assets/Letta-logo-RGB_OffBlackonTransparent_cropped_small.png" /&gt; 
  &lt;img alt="Letta logo" src="https://raw.githubusercontent.com/letta-ai/letta/refs/heads/main/assets/Letta-logo-RGB_GreyonOffBlack_cropped_small.png" width="500" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;h1&gt;Letta (formerly MemGPT)&lt;/h1&gt; 
&lt;p&gt;Letta is the platform for building stateful agents: open AI with advanced memory that can learn and self-improve over time.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.letta.com/quickstart"&gt;&lt;strong&gt;Quickstart&lt;/strong&gt;&lt;/a&gt;: Build your first stateful agent in 5 minutes using Python or TypeScript&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.letta.com/core-concepts"&gt;&lt;strong&gt;Understanding agent memory&lt;/strong&gt;&lt;/a&gt;: Learn about memory blocks, tools, and how Letta agents maintain state&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.letta.com/tutorials/"&gt;&lt;strong&gt;Examples and tutorials&lt;/strong&gt;&lt;/a&gt;: Working code examples for common use cases and agent patterns&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.letta.com/api"&gt;&lt;strong&gt;API reference&lt;/strong&gt;&lt;/a&gt;: Complete REST API and SDK documentation for Python and TypeScript&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] &lt;strong&gt;Letta Code&lt;/strong&gt; is a memory-first coding harness, built on top of the Letta API. Instead of working in independent sessions, you work with a persisted agent that learns over time and is portable across models. You can use Letta Code to interact with any Letta agent via the CLI.&lt;/p&gt; 
 &lt;p&gt;Read more about how to use Letta Code on the &lt;a href="https://docs.letta.com/letta-code"&gt;official docs page&lt;/a&gt;, or on the &lt;a href="https://github.com/letta-ai/letta-code"&gt;GitHub repo&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Get started with the Letta API&lt;/h2&gt; 
&lt;p&gt;Use the Letta API to build stateful agents that remember, learn, and improve over time. Letta allows you to build agents on any model provider, including OpenAI, Anthropic, Google Gemini, and more.&lt;/p&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;TypeScript / Node.js:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm install @letta-ai/letta-client
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install letta-client
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Hello World example&lt;/h3&gt; 
&lt;p&gt;Below is a quick example of creating a stateful agent and sending it a message. See the full &lt;a href="https://docs.letta.com/quickstart"&gt;quickstart guide&lt;/a&gt; for complete documentation.&lt;/p&gt; 
&lt;p&gt;Running the examples require a &lt;a href="https://app.letta.com"&gt;Letta Developer Platform&lt;/a&gt; account, or a &lt;a href="https://docs.letta.com/guides/selfhosting/"&gt;self-hosted Letta server&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;TypeScript:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-typescript"&gt;import Letta from "@letta-ai/letta-client";

const client = new Letta({ apiKey: process.env.LETTA_API_KEY });

// Create your agent
const agentState = await client.agents.create({
  model: "openai/gpt-4.1",
  embedding: "openai/text-embedding-3-small",
  memory_blocks: [
    {
      label: "human",
      value:
        "Name: Timber. Status: dog. Occupation: building Letta, infrastructure to democratize self-improving superintelligence",
    },
    {
      label: "persona",
      value:
        "I am a self-improving superintelligence. Timber is my best friend and collaborator.",
    },
  ],
  tools: ["web_search", "run_code"],
});

console.log("Agent created with ID:", agentState.id);

// Send your agent a message
const response = await client.agents.messages.create(agentState.id, {
  input: "What do you know about me?",
});

for (const message of response.messages) {
  console.log(message);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from letta_client import Letta
import os

client = Letta(api_key=os.getenv("LETTA_API_KEY"))

# Create your agent
agent_state = client.agents.create(
    model="openai/gpt-4.1",
    embedding="openai/text-embedding-3-small",
    memory_blocks=[
        {
          "label": "human",
          "value": "Name: Timber. Status: dog. Occupation: building Letta, infrastructure to democratize self-improving superintelligence"
        },
        {
          "label": "persona",
          "value": "I am a self-improving superintelligence. Timber is my best friend and collaborator."
        }
    ],
    tools=["web_search", "run_code"]
)

print(f"Agent created with ID: {agent_state.id}")

# Send your agent a message
response = client.agents.messages.create(
    agent_id=agent_state.id,
    input="What do you know about me?"
)

for message in response.messages:
    print(message)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Letta is an open source project built by over a hundred contributors from around the world. There are many ways to get involved in the Letta OSS project!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/letta"&gt;&lt;strong&gt;Join the Discord&lt;/strong&gt;&lt;/a&gt;: Chat with the Letta devs and other AI developers.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://forum.letta.com/"&gt;&lt;strong&gt;Chat on our forum&lt;/strong&gt;&lt;/a&gt;: If you're not into Discord, check out our developer forum.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Follow our socials&lt;/strong&gt;: &lt;a href="https://twitter.com/Letta_AI"&gt;Twitter/X&lt;/a&gt;, &lt;a href="https://www.linkedin.com/in/letta"&gt;LinkedIn&lt;/a&gt;, &lt;a href="https://www.youtube.com/@letta-ai"&gt;YouTube&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Legal notices&lt;/strong&gt;: By using Letta and related Letta services (such as the Letta endpoint or hosted service), you are agreeing to our &lt;a href="https://www.letta.com/privacy-policy"&gt;privacy policy&lt;/a&gt; and &lt;a href="https://www.letta.com/terms-of-service"&gt;terms of service&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>sgl-project/mini-sglang</title>
      <link>https://github.com/sgl-project/mini-sglang</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img width="400" src="https://raw.githubusercontent.com/sgl-project/mini-sglang/main/assets/logo.png" /&gt; &lt;/p&gt; 
&lt;h1&gt;Mini-SGLang&lt;/h1&gt; 
&lt;p&gt;A &lt;strong&gt;lightweight yet high-performance&lt;/strong&gt; inference framework for Large Language Models.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Mini-SGLang is a compact implementation of &lt;a href="https://github.com/sgl-project/sglang"&gt;SGLang&lt;/a&gt;, designed to demystify the complexities of modern LLM serving systems. With a compact codebase of &lt;strong&gt;~5,000 lines of Python&lt;/strong&gt;, it serves as both a capable inference engine and a transparent reference for researchers and developers.&lt;/p&gt; 
&lt;h2&gt;‚ú® Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;High Performance&lt;/strong&gt;: Achieves state-of-the-art throughput and latency with advanced optimizations.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Lightweight &amp;amp; Readable&lt;/strong&gt;: A clean, modular, and fully type-annotated codebase that is easy to understand and modify.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced Optimizations&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Radix Cache&lt;/strong&gt;: Reuses KV cache for shared prefixes across requests.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Chunked Prefill&lt;/strong&gt;: Reduces peak memory usage for long-context serving.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Overlap Scheduling&lt;/strong&gt;: Hides CPU scheduling overhead with GPU computation.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Tensor Parallelism&lt;/strong&gt;: Scales inference across multiple GPUs.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Optimized Kernels&lt;/strong&gt;: Integrates &lt;strong&gt;FlashAttention&lt;/strong&gt; and &lt;strong&gt;FlashInfer&lt;/strong&gt; for maximum efficiency.&lt;/li&gt; 
   &lt;li&gt;...&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;h3&gt;1. Environment Setup&lt;/h3&gt; 
&lt;p&gt;We recommend using &lt;code&gt;uv&lt;/code&gt; for a fast and reliable installation (note that &lt;code&gt;uv&lt;/code&gt; does not conflict with &lt;code&gt;conda&lt;/code&gt;).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create a virtual environment (Python 3.10+ recommended)
uv venv --python=3.12
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;: Mini-SGLang relies on CUDA kernels that are JIT-compiled. Ensure you have the &lt;strong&gt;NVIDIA CUDA Toolkit&lt;/strong&gt; installed and that its version matches your driver's version. You can check your driver's CUDA capability with &lt;code&gt;nvidia-smi&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;2. Installation&lt;/h3&gt; 
&lt;p&gt;Install Mini-SGLang directly from the source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/sgl-project/mini-sglang.git
cd mini-sglang
uv pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Online Serving&lt;/h3&gt; 
&lt;p&gt;Launch an OpenAI-compatible API server with a single command.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Deploy Qwen/Qwen3-0.6B on a single GPU
python -m minisgl --model "Qwen/Qwen3-0.6B"

# Deploy meta-llama/Llama-3.1-70B-Instruct on 4 GPUs with Tensor Parallelism, on port 30000
python -m minisgl --model "meta-llama/Llama-3.1-70B-Instruct" --tp 4 --port 30000
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once the server is running, you can send requests using standard tools like &lt;code&gt;curl&lt;/code&gt; or any OpenAI-compatible client.&lt;/p&gt; 
&lt;h3&gt;4. Interactive Shell&lt;/h3&gt; 
&lt;p&gt;Chat with your model directly in the terminal by adding the &lt;code&gt;--shell&lt;/code&gt; flag.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m minisgl --model "Qwen/Qwen3-0.6B" --shell
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://lmsys.org/images/blog/minisgl/shell.png" alt="shell-example" /&gt;&lt;/p&gt; 
&lt;p&gt;You can also use &lt;code&gt;/reset&lt;/code&gt; to clear the chat history.&lt;/p&gt; 
&lt;h2&gt;Benchmark&lt;/h2&gt; 
&lt;h3&gt;Offline inference&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/sgl-project/mini-sglang/main/benchmark/offline/bench.py"&gt;bench.py&lt;/a&gt; for more details. Set &lt;code&gt;MINISGL_DISABLE_OVERLAP_SCHEDULING=1&lt;/code&gt; for ablation study on overlap scheduling.&lt;/p&gt; 
&lt;p&gt;Test Configuration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Hardware: 1xH200 GPU.&lt;/li&gt; 
 &lt;li&gt;Model: Qwen3-0.6B, Qwen3-14B&lt;/li&gt; 
 &lt;li&gt;Total Requests: 256 sequences&lt;/li&gt; 
 &lt;li&gt;Input Length: Randomly sampled between 100-1024 tokens&lt;/li&gt; 
 &lt;li&gt;Output Length: Randomly sampled between 100-1024 tokens&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://lmsys.org/images/blog/minisgl/offline.png" alt="offline" /&gt;&lt;/p&gt; 
&lt;h3&gt;Online inference&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/sgl-project/mini-sglang/main/benchmark/online/bench_qwen.py"&gt;benchmark_qwen.py&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;Test Configuration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Hardware: 4xH200 GPU, connected by NVLink.&lt;/li&gt; 
 &lt;li&gt;Model: Qwen3-32B&lt;/li&gt; 
 &lt;li&gt;Dataset: &lt;a href="https://github.com/alibaba-edu/qwen-bailian-usagetraces-anon/raw/main/qwen_traceA_blksz_16.jsonl"&gt;Qwen trace&lt;/a&gt;, replaying first 1000 requests.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Launch command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Mini-SGLang
python -m minisgl --model "Qwen/Qwen3-32B" --tp 4 --cache naive

# SGLang
python3 -m sglang.launch_server --model "Qwen/Qwen3-32B" --tp 4 \
    --disable-radix --port 1919 --decode-attention flashinfer
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://lmsys.org/images/blog/minisgl/online.png" alt="online" /&gt;&lt;/p&gt; 
&lt;h2&gt;üìö Learn More&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sgl-project/mini-sglang/main/docs/features.md"&gt;Detailed Features&lt;/a&gt;&lt;/strong&gt;: Explore all available features and command-line arguments.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sgl-project/mini-sglang/main/docs/structures.md"&gt;System Architecture&lt;/a&gt;&lt;/strong&gt;: Dive deep into the design and data flow of Mini-SGLang.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>virattt/ai-hedge-fund</title>
      <link>https://github.com/virattt/ai-hedge-fund</link>
      <description>&lt;p&gt;An AI Hedge Fund Team&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Hedge Fund&lt;/h1&gt; 
&lt;p&gt;This is a proof of concept for an AI-powered hedge fund. The goal of this project is to explore the use of AI to make trading decisions. This project is for &lt;strong&gt;educational&lt;/strong&gt; purposes only and is not intended for real trading or investment.&lt;/p&gt; 
&lt;p&gt;This system employs several agents working together:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Aswath Damodaran Agent - The Dean of Valuation, focuses on story, numbers, and disciplined valuation&lt;/li&gt; 
 &lt;li&gt;Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety&lt;/li&gt; 
 &lt;li&gt;Bill Ackman Agent - An activist investor, takes bold positions and pushes for change&lt;/li&gt; 
 &lt;li&gt;Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption&lt;/li&gt; 
 &lt;li&gt;Charlie Munger Agent - Warren Buffett's partner, only buys wonderful businesses at fair prices&lt;/li&gt; 
 &lt;li&gt;Michael Burry Agent - The Big Short contrarian who hunts for deep value&lt;/li&gt; 
 &lt;li&gt;Mohnish Pabrai Agent - The Dhandho investor, who looks for doubles at low risk&lt;/li&gt; 
 &lt;li&gt;Peter Lynch Agent - Practical investor who seeks "ten-baggers" in everyday businesses&lt;/li&gt; 
 &lt;li&gt;Phil Fisher Agent - Meticulous growth investor who uses deep "scuttlebutt" research&lt;/li&gt; 
 &lt;li&gt;Rakesh Jhunjhunwala Agent - The Big Bull of India&lt;/li&gt; 
 &lt;li&gt;Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential&lt;/li&gt; 
 &lt;li&gt;Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price&lt;/li&gt; 
 &lt;li&gt;Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Sentiment Agent - Analyzes market sentiment and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Fundamentals Agent - Analyzes fundamental data and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Technicals Agent - Analyzes technical indicators and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Risk Manager - Calculates risk metrics and sets position limits&lt;/li&gt; 
 &lt;li&gt;Portfolio Manager - Makes final trading decisions and generates orders&lt;/li&gt; 
&lt;/ol&gt; 
&lt;img width="1042" alt="Screenshot 2025-03-22 at 6 19 07 PM" src="https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4" /&gt; 
&lt;p&gt;Note: the system does not actually make any trades.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://twitter.com/virattt"&gt;&lt;img src="https://img.shields.io/twitter/follow/virattt?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This project is for &lt;strong&gt;educational and research purposes only&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Not intended for real trading or investment&lt;/li&gt; 
 &lt;li&gt;No investment advice or guarantees provided&lt;/li&gt; 
 &lt;li&gt;Creator assumes no liability for financial losses&lt;/li&gt; 
 &lt;li&gt;Consult a financial advisor for investment decisions&lt;/li&gt; 
 &lt;li&gt;Past performance does not indicate future results&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By using this software, you agree to use it solely for learning purposes.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-install"&gt;How to Install&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-run"&gt;How to Run&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#%EF%B8%8F-command-line-interface"&gt;‚å®Ô∏è Command Line Interface&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#%EF%B8%8F-web-application"&gt;üñ•Ô∏è Web Application&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-contribute"&gt;How to Contribute&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#feature-requests"&gt;Feature Requests&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How to Install&lt;/h2&gt; 
&lt;p&gt;Before you can run the AI Hedge Fund, you'll need to install it and set up your API keys. These steps are common to both the full-stack web application and command line interface.&lt;/p&gt; 
&lt;h3&gt;1. Clone the Repository&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Set up API keys&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file for your API keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create .env file for your API keys (in the root directory)
cp .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open and edit the &lt;code&gt;.env&lt;/code&gt; file to add your API keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
OPENAI_API_KEY=your-openai-api-key

# For getting financial data to power the hedge fund
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: You must set at least one LLM API key (e.g. &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;, &lt;code&gt;GROQ_API_KEY&lt;/code&gt;, &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;, or &lt;code&gt;DEEPSEEK_API_KEY&lt;/code&gt;) for the hedge fund to work.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Financial Data&lt;/strong&gt;: Data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key. For any other ticker, you will need to set the &lt;code&gt;FINANCIAL_DATASETS_API_KEY&lt;/code&gt; in the .env file.&lt;/p&gt; 
&lt;h2&gt;How to Run&lt;/h2&gt; 
&lt;h3&gt;‚å®Ô∏è Command Line Interface&lt;/h3&gt; 
&lt;p&gt;You can run the AI Hedge Fund directly via terminal. This approach offers more granular control and is useful for automation, scripting, and integration purposes.&lt;/p&gt; 
&lt;img width="992" alt="Screenshot 2025-01-06 at 5 50 17 PM" src="https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b" /&gt; 
&lt;h4&gt;Quick Start&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install Poetry (if not already installed):&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -sSL https://install.python-poetry.org | python3 -
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install dependencies:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run the AI Hedge Fund&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also specify a &lt;code&gt;--ollama&lt;/code&gt; flag to run the AI hedge fund using local LLMs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA --ollama
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can optionally specify the start and end dates to make decisions over a specific time period.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run the Backtester&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Example Output:&lt;/strong&gt; &lt;img width="941" alt="Screenshot 2025-01-06 at 5 47 52 PM" src="https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47" /&gt;&lt;/p&gt; 
&lt;p&gt;Note: The &lt;code&gt;--ollama&lt;/code&gt;, &lt;code&gt;--start-date&lt;/code&gt;, and &lt;code&gt;--end-date&lt;/code&gt; flags work for the backtester, as well!&lt;/p&gt; 
&lt;h3&gt;üñ•Ô∏è Web Application&lt;/h3&gt; 
&lt;p&gt;The new way to run the AI Hedge Fund is through our web application that provides a user-friendly interface. This is recommended for users who prefer visual interfaces over command line tools.&lt;/p&gt; 
&lt;p&gt;Please see detailed instructions on how to install and run the web application &lt;a href="https://github.com/virattt/ai-hedge-fund/tree/main/app"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;img width="1721" alt="Screenshot 2025-06-28 at 6 41 03‚ÄØPM" src="https://github.com/user-attachments/assets/b95ab696-c9f4-416c-9ad1-51feb1f5374b" /&gt; 
&lt;h2&gt;How to Contribute&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repository&lt;/li&gt; 
 &lt;li&gt;Create a feature branch&lt;/li&gt; 
 &lt;li&gt;Commit your changes&lt;/li&gt; 
 &lt;li&gt;Push to the branch&lt;/li&gt; 
 &lt;li&gt;Create a Pull Request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Please keep your pull requests small and focused. This will make it easier to review and merge.&lt;/p&gt; 
&lt;h2&gt;Feature Requests&lt;/h2&gt; 
&lt;p&gt;If you have a feature request, please open an &lt;a href="https://github.com/virattt/ai-hedge-fund/issues"&gt;issue&lt;/a&gt; and make sure it is tagged with &lt;code&gt;enhancement&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>GeeeekExplorer/nano-vllm</title>
      <link>https://github.com/GeeeekExplorer/nano-vllm</link>
      <description>&lt;p&gt;Nano vLLM&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img width="300" src="https://raw.githubusercontent.com/GeeeekExplorer/nano-vllm/main/assets/logo.png" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/15323" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/15323" alt="GeeeekExplorer%2Fnano-vllm | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;Nano-vLLM&lt;/h1&gt; 
&lt;p&gt;A lightweight vLLM implementation built from scratch.&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üöÄ &lt;strong&gt;Fast offline inference&lt;/strong&gt; - Comparable inference speeds to vLLM&lt;/li&gt; 
 &lt;li&gt;üìñ &lt;strong&gt;Readable codebase&lt;/strong&gt; - Clean implementation in ~ 1,200 lines of Python code&lt;/li&gt; 
 &lt;li&gt;‚ö° &lt;strong&gt;Optimization Suite&lt;/strong&gt; - Prefix caching, Tensor Parallelism, Torch compilation, CUDA graph, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install git+https://github.com/GeeeekExplorer/nano-vllm.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Model Download&lt;/h2&gt; 
&lt;p&gt;To download the model weights manually, use the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;huggingface-cli download --resume-download Qwen/Qwen3-0.6B \
  --local-dir ~/huggingface/Qwen3-0.6B/ \
  --local-dir-use-symlinks False
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;See &lt;code&gt;example.py&lt;/code&gt; for usage. The API mirrors vLLM's interface with minor differences in the &lt;code&gt;LLM.generate&lt;/code&gt; method:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from nanovllm import LLM, SamplingParams
llm = LLM("/YOUR/MODEL/PATH", enforce_eager=True, tensor_parallel_size=1)
sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = ["Hello, Nano-vLLM."]
outputs = llm.generate(prompts, sampling_params)
outputs[0]["text"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Benchmark&lt;/h2&gt; 
&lt;p&gt;See &lt;code&gt;bench.py&lt;/code&gt; for benchmark.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Test Configuration:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Hardware: RTX 4070 Laptop (8GB)&lt;/li&gt; 
 &lt;li&gt;Model: Qwen3-0.6B&lt;/li&gt; 
 &lt;li&gt;Total Requests: 256 sequences&lt;/li&gt; 
 &lt;li&gt;Input Length: Randomly sampled between 100‚Äì1024 tokens&lt;/li&gt; 
 &lt;li&gt;Output Length: Randomly sampled between 100‚Äì1024 tokens&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Performance Results:&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Inference Engine&lt;/th&gt; 
   &lt;th&gt;Output Tokens&lt;/th&gt; 
   &lt;th&gt;Time (s)&lt;/th&gt; 
   &lt;th&gt;Throughput (tokens/s)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;vLLM&lt;/td&gt; 
   &lt;td&gt;133,966&lt;/td&gt; 
   &lt;td&gt;98.37&lt;/td&gt; 
   &lt;td&gt;1361.84&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Nano-vLLM&lt;/td&gt; 
   &lt;td&gt;133,966&lt;/td&gt; 
   &lt;td&gt;93.41&lt;/td&gt; 
   &lt;td&gt;1434.13&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#GeeeekExplorer/nano-vllm&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=GeeeekExplorer/nano-vllm&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>sooperset/mcp-atlassian</title>
      <link>https://github.com/sooperset/mcp-atlassian</link>
      <description>&lt;p&gt;MCP server for Atlassian tools (Confluence, Jira)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MCP Atlassian&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/pypi/v/mcp-atlassian" alt="PyPI Version" /&gt; &lt;img src="https://img.shields.io/pypi/dm/mcp-atlassian" alt="PyPI - Downloads" /&gt; &lt;img src="https://static.pepy.tech/personalized-badge/mcp-atlassian?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=blue&amp;amp;left_text=Total%20Downloads" alt="PePy - Total Downloads" /&gt; &lt;a href="https://github.com/sooperset/mcp-atlassian/actions/workflows/tests.yml"&gt;&lt;img src="https://github.com/sooperset/mcp-atlassian/actions/workflows/tests.yml/badge.svg?sanitize=true" alt="Run Tests" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/license/sooperset/mcp-atlassian" alt="License" /&gt;&lt;/p&gt; 
&lt;p&gt;Model Context Protocol (MCP) server for Atlassian products (Confluence and Jira). This integration supports both Confluence &amp;amp; Jira Cloud and Server/Data Center deployments.&lt;/p&gt; 
&lt;h2&gt;Example Usage&lt;/h2&gt; 
&lt;p&gt;Ask your AI assistant to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üìù Automatic Jira Updates&lt;/strong&gt; - "Update Jira from our meeting notes"&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîç AI-Powered Confluence Search&lt;/strong&gt; - "Find our OKR guide in Confluence and summarize it"&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üêõ Smart Jira Issue Filtering&lt;/strong&gt; - "Show me urgent bugs in PROJ project from last week"&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìÑ Content Creation &amp;amp; Management&lt;/strong&gt; - "Create a tech design doc for XYZ feature"&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Feature Demo&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/35303504-14c6-4ae4-913b-7c25ea511c3e"&gt;https://github.com/user-attachments/assets/35303504-14c6-4ae4-913b-7c25ea511c3e&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Confluence Demo&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/7fe9c488-ad0c-4876-9b54-120b666bb785"&gt;https://github.com/user-attachments/assets/7fe9c488-ad0c-4876-9b54-120b666bb785&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;Compatibility&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Product&lt;/th&gt; 
   &lt;th&gt;Deployment Type&lt;/th&gt; 
   &lt;th&gt;Support Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Confluence&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Cloud&lt;/td&gt; 
   &lt;td&gt;‚úÖ Fully supported&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Confluence&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Server/Data Center&lt;/td&gt; 
   &lt;td&gt;‚úÖ Supported (version 6.0+)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Jira&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Cloud&lt;/td&gt; 
   &lt;td&gt;‚úÖ Fully supported&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Jira&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Server/Data Center&lt;/td&gt; 
   &lt;td&gt;‚úÖ Supported (version 8.14+)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Quick Start Guide&lt;/h2&gt; 
&lt;h3&gt;üîê 1. Authentication Setup&lt;/h3&gt; 
&lt;p&gt;MCP Atlassian supports three authentication methods:&lt;/p&gt; 
&lt;h4&gt;A. API Token Authentication (Cloud) - &lt;strong&gt;Recommended&lt;/strong&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Go to &lt;a href="https://id.atlassian.com/manage-profile/security/api-tokens"&gt;https://id.atlassian.com/manage-profile/security/api-tokens&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Click &lt;strong&gt;Create API token&lt;/strong&gt;, name it&lt;/li&gt; 
 &lt;li&gt;Copy the token immediately&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;B. Personal Access Token (Server/Data Center)&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Go to your profile (avatar) ‚Üí &lt;strong&gt;Profile&lt;/strong&gt; ‚Üí &lt;strong&gt;Personal Access Tokens&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Click &lt;strong&gt;Create token&lt;/strong&gt;, name it, set expiry&lt;/li&gt; 
 &lt;li&gt;Copy the token immediately&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;C. OAuth 2.0 Authentication (Cloud) - &lt;strong&gt;Advanced&lt;/strong&gt;&lt;/h4&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] OAuth 2.0 is more complex to set up but provides enhanced security features. For most users, API Token authentication (Method A) is simpler and sufficient.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt;Go to &lt;a href="https://developer.atlassian.com/console/myapps/"&gt;Atlassian Developer Console&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Create an "OAuth 2.0 (3LO) integration" app&lt;/li&gt; 
 &lt;li&gt;Configure &lt;strong&gt;Permissions&lt;/strong&gt; (scopes) for Jira/Confluence&lt;/li&gt; 
 &lt;li&gt;Set &lt;strong&gt;Callback URL&lt;/strong&gt; (e.g., &lt;code&gt;http://localhost:8080/callback&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Run setup wizard: &lt;pre&gt;&lt;code class="language-bash"&gt;docker run --rm -i \
  -p 8080:8080 \
  -v "${HOME}/.mcp-atlassian:/home/app/.mcp-atlassian" \
  ghcr.io/sooperset/mcp-atlassian:latest --oauth-setup -v
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Follow prompts for &lt;code&gt;Client ID&lt;/code&gt;, &lt;code&gt;Secret&lt;/code&gt;, &lt;code&gt;URI&lt;/code&gt;, and &lt;code&gt;Scope&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Complete browser authorization&lt;/li&gt; 
 &lt;li&gt;Add obtained credentials to &lt;code&gt;.env&lt;/code&gt; or IDE config: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;ATLASSIAN_OAUTH_CLOUD_ID&lt;/code&gt; (from wizard)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;ATLASSIAN_OAUTH_CLIENT_ID&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;ATLASSIAN_OAUTH_CLIENT_SECRET&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;ATLASSIAN_OAUTH_REDIRECT_URI&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;ATLASSIAN_OAUTH_SCOPE&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] For the standard OAuth flow described above, include &lt;code&gt;offline_access&lt;/code&gt; in your scope (e.g., &lt;code&gt;read:jira-work write:jira-work offline_access&lt;/code&gt;). This allows the server to refresh the access token automatically.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details&gt; 
 &lt;summary&gt;Alternative: Using a Pre-existing OAuth Access Token (BYOT)&lt;/summary&gt; 
 &lt;p&gt;If you are running mcp-atlassian part of a larger system that manages Atlassian OAuth 2.0 access tokens externally (e.g., through a central identity provider or another application), you can provide an access token directly to this MCP server. This method bypasses the interactive setup wizard and the server's internal token management (including refresh capabilities).&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;A valid Atlassian OAuth 2.0 Access Token with the necessary scopes for the intended operations.&lt;/li&gt; 
  &lt;li&gt;The corresponding &lt;code&gt;ATLASSIAN_OAUTH_CLOUD_ID&lt;/code&gt; for your Atlassian instance.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Configuration:&lt;/strong&gt; To use this method, set the following environment variables (or use the corresponding command-line flags when starting the server):&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;ATLASSIAN_OAUTH_CLOUD_ID&lt;/code&gt;: Your Atlassian Cloud ID. (CLI: &lt;code&gt;--oauth-cloud-id&lt;/code&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;ATLASSIAN_OAUTH_ACCESS_TOKEN&lt;/code&gt;: Your pre-existing OAuth 2.0 access token. (CLI: &lt;code&gt;--oauth-access-token&lt;/code&gt;)&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Important Considerations for BYOT:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Token Lifecycle Management:&lt;/strong&gt; When using BYOT, the MCP server &lt;strong&gt;does not&lt;/strong&gt; handle token refresh. The responsibility for obtaining, refreshing (before expiry), and revoking the access token lies entirely with you or the external system providing the token.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Unused Variables:&lt;/strong&gt; The standard OAuth client variables (&lt;code&gt;ATLASSIAN_OAUTH_CLIENT_ID&lt;/code&gt;, &lt;code&gt;ATLASSIAN_OAUTH_CLIENT_SECRET&lt;/code&gt;, &lt;code&gt;ATLASSIAN_OAUTH_REDIRECT_URI&lt;/code&gt;, &lt;code&gt;ATLASSIAN_OAUTH_SCOPE&lt;/code&gt;) are &lt;strong&gt;not&lt;/strong&gt; used and can be omitted when configuring for BYOT.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;No Setup Wizard:&lt;/strong&gt; The &lt;code&gt;--oauth-setup&lt;/code&gt; wizard is not applicable and should not be used for this approach.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;No Token Cache Volume:&lt;/strong&gt; The Docker volume mount for token storage (e.g., &lt;code&gt;-v "${HOME}/.mcp-atlassian:/home/app/.mcp-atlassian"&lt;/code&gt;) is also not necessary if you are exclusively using the BYOT method, as no tokens are stored or managed by this server.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Scope:&lt;/strong&gt; The provided access token must already have the necessary permissions (scopes) for the Jira/Confluence operations you intend to perform.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;This option is useful in scenarios where OAuth credential management is centralized or handled by other infrastructure components.&lt;/p&gt; 
&lt;/details&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] &lt;strong&gt;Multi-Cloud OAuth Support&lt;/strong&gt;: If you're building a multi-tenant application where users provide their own OAuth tokens, see the &lt;a href="https://raw.githubusercontent.com/sooperset/mcp-atlassian/main/#multi-cloud-oauth-support"&gt;Multi-Cloud OAuth Support&lt;/a&gt; section for minimal configuration setup.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;üì¶ 2. Installation&lt;/h3&gt; 
&lt;p&gt;MCP Atlassian is distributed as a Docker image. This is the recommended way to run the server, especially for IDE integration. Ensure you have Docker installed.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Pull Pre-built Image
docker pull ghcr.io/sooperset/mcp-atlassian:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üõ†Ô∏è IDE Integration&lt;/h2&gt; 
&lt;p&gt;MCP Atlassian is designed to be used with AI assistants through IDE integration.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] &lt;strong&gt;For Claude Desktop&lt;/strong&gt;: Locate and edit the configuration file directly:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: &lt;code&gt;%APPDATA%\Claude\claude_desktop_config.json&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: &lt;code&gt;~/Library/Application Support/Claude/claude_desktop_config.json&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt;: &lt;code&gt;~/.config/Claude/claude_desktop_config.json&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;For Cursor&lt;/strong&gt;: Open Settings ‚Üí MCP ‚Üí + Add new global MCP server&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;‚öôÔ∏è Configuration Methods&lt;/h3&gt; 
&lt;p&gt;There are two main approaches to configure the Docker container:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Passing Variables Directly&lt;/strong&gt; (shown in examples below)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Using an Environment File&lt;/strong&gt; with &lt;code&gt;--env-file&lt;/code&gt; flag (shown in collapsible sections)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Common environment variables include:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;CONFLUENCE_SPACES_FILTER&lt;/code&gt;: Filter by space keys (e.g., "DEV,TEAM,DOC")&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;JIRA_PROJECTS_FILTER&lt;/code&gt;: Filter by project keys (e.g., "PROJ,DEV,SUPPORT")&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;READ_ONLY_MODE&lt;/code&gt;: Set to "true" to disable write operations&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;MCP_VERBOSE&lt;/code&gt;: Set to "true" for more detailed logging&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;MCP_LOGGING_STDOUT&lt;/code&gt;: Set to "true" to log to stdout instead of stderr&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;ENABLED_TOOLS&lt;/code&gt;: Comma-separated list of tool names to enable (e.g., "confluence_search,jira_get_issue")&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;See the &lt;a href="https://github.com/sooperset/mcp-atlassian/raw/main/.env.example"&gt;.env.example&lt;/a&gt; file for all available options.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;üìù Configuration Examples&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Method 1 (Passing Variables Directly):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcp-atlassian": {
      "command": "docker",
      "args": [
        "run",
        "-i",
        "--rm",
        "-e", "CONFLUENCE_URL",
        "-e", "CONFLUENCE_USERNAME",
        "-e", "CONFLUENCE_API_TOKEN",
        "-e", "JIRA_URL",
        "-e", "JIRA_USERNAME",
        "-e", "JIRA_API_TOKEN",
        "ghcr.io/sooperset/mcp-atlassian:latest"
      ],
      "env": {
        "CONFLUENCE_URL": "https://your-company.atlassian.net/wiki",
        "CONFLUENCE_USERNAME": "your.email@company.com",
        "CONFLUENCE_API_TOKEN": "your_confluence_api_token",
        "JIRA_URL": "https://your-company.atlassian.net",
        "JIRA_USERNAME": "your.email@company.com",
        "JIRA_API_TOKEN": "your_jira_api_token"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;Alternative: Using Environment File&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcp-atlassian": {
      "command": "docker",
      "args": [
        "run",
        "--rm",
        "-i",
        "--env-file",
        "/path/to/your/mcp-atlassian.env",
        "ghcr.io/sooperset/mcp-atlassian:latest"
      ]
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Server/Data Center Configuration&lt;/summary&gt; 
 &lt;p&gt;For Server/Data Center deployments, use direct variable passing:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcp-atlassian": {
      "command": "docker",
      "args": [
        "run",
        "--rm",
        "-i",
        "-e", "CONFLUENCE_URL",
        "-e", "CONFLUENCE_PERSONAL_TOKEN",
        "-e", "CONFLUENCE_SSL_VERIFY",
        "-e", "JIRA_URL",
        "-e", "JIRA_PERSONAL_TOKEN",
        "-e", "JIRA_SSL_VERIFY",
        "ghcr.io/sooperset/mcp-atlassian:latest"
      ],
      "env": {
        "CONFLUENCE_URL": "https://confluence.your-company.com",
        "CONFLUENCE_PERSONAL_TOKEN": "your_confluence_pat",
        "CONFLUENCE_SSL_VERIFY": "false",
        "JIRA_URL": "https://jira.your-company.com",
        "JIRA_PERSONAL_TOKEN": "your_jira_pat",
        "JIRA_SSL_VERIFY": "false"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;[!NOTE] Set &lt;code&gt;CONFLUENCE_SSL_VERIFY&lt;/code&gt; and &lt;code&gt;JIRA_SSL_VERIFY&lt;/code&gt; to "false" only if you have self-signed certificates.&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;OAuth 2.0 Configuration (Cloud Only)&lt;/summary&gt; 
 &lt;a name="oauth-20-configuration-example-cloud-only"&gt;&lt;/a&gt; 
 &lt;p&gt;These examples show how to configure &lt;code&gt;mcp-atlassian&lt;/code&gt; in your IDE (like Cursor or Claude Desktop) when using OAuth 2.0 for Atlassian Cloud.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example for Standard OAuth 2.0 Flow (using Setup Wizard):&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;This configuration is for when you use the server's built-in OAuth client and have completed the &lt;a href="https://raw.githubusercontent.com/sooperset/mcp-atlassian/main/#c-oauth-20-authentication-cloud---advanced"&gt;OAuth setup wizard&lt;/a&gt;.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcp-atlassian": {
      "command": "docker",
      "args": [
        "run",
        "--rm",
        "-i",
        "-v", "&amp;lt;path_to_your_home&amp;gt;/.mcp-atlassian:/home/app/.mcp-atlassian",
        "-e", "JIRA_URL",
        "-e", "CONFLUENCE_URL",
        "-e", "ATLASSIAN_OAUTH_CLIENT_ID",
        "-e", "ATLASSIAN_OAUTH_CLIENT_SECRET",
        "-e", "ATLASSIAN_OAUTH_REDIRECT_URI",
        "-e", "ATLASSIAN_OAUTH_SCOPE",
        "-e", "ATLASSIAN_OAUTH_CLOUD_ID",
        "ghcr.io/sooperset/mcp-atlassian:latest"
      ],
      "env": {
        "JIRA_URL": "https://your-company.atlassian.net",
        "CONFLUENCE_URL": "https://your-company.atlassian.net/wiki",
        "ATLASSIAN_OAUTH_CLIENT_ID": "YOUR_OAUTH_APP_CLIENT_ID",
        "ATLASSIAN_OAUTH_CLIENT_SECRET": "YOUR_OAUTH_APP_CLIENT_SECRET",
        "ATLASSIAN_OAUTH_REDIRECT_URI": "http://localhost:8080/callback",
        "ATLASSIAN_OAUTH_SCOPE": "read:jira-work write:jira-work read:confluence-content.all write:confluence-content offline_access",
        "ATLASSIAN_OAUTH_CLOUD_ID": "YOUR_CLOUD_ID_FROM_SETUP_WIZARD"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;[!NOTE]&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;For the Standard Flow: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;ATLASSIAN_OAUTH_CLOUD_ID&lt;/code&gt; is obtained from the &lt;code&gt;--oauth-setup&lt;/code&gt; wizard output or is known for your instance.&lt;/li&gt; 
     &lt;li&gt;Other &lt;code&gt;ATLASSIAN_OAUTH_*&lt;/code&gt; client variables are from your OAuth app in the Atlassian Developer Console.&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;JIRA_URL&lt;/code&gt; and &lt;code&gt;CONFLUENCE_URL&lt;/code&gt; for your Cloud instances are always required.&lt;/li&gt; 
     &lt;li&gt;The volume mount (&lt;code&gt;-v .../.mcp-atlassian:/home/app/.mcp-atlassian&lt;/code&gt;) is crucial for persisting the OAuth tokens obtained by the wizard, enabling automatic refresh.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Example for Pre-existing Access Token (BYOT - Bring Your Own Token):&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;This configuration is for when you are providing your own externally managed OAuth 2.0 access token.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcp-atlassian": {
      "command": "docker",
      "args": [
        "run",
        "--rm",
        "-i",
        "-e", "JIRA_URL",
        "-e", "CONFLUENCE_URL",
        "-e", "ATLASSIAN_OAUTH_CLOUD_ID",
        "-e", "ATLASSIAN_OAUTH_ACCESS_TOKEN",
        "ghcr.io/sooperset/mcp-atlassian:latest"
      ],
      "env": {
        "JIRA_URL": "https://your-company.atlassian.net",
        "CONFLUENCE_URL": "https://your-company.atlassian.net/wiki",
        "ATLASSIAN_OAUTH_CLOUD_ID": "YOUR_KNOWN_CLOUD_ID",
        "ATLASSIAN_OAUTH_ACCESS_TOKEN": "YOUR_PRE_EXISTING_OAUTH_ACCESS_TOKEN"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;[!NOTE]&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;For the BYOT Method: 
    &lt;ul&gt; 
     &lt;li&gt;You primarily need &lt;code&gt;JIRA_URL&lt;/code&gt;, &lt;code&gt;CONFLUENCE_URL&lt;/code&gt;, &lt;code&gt;ATLASSIAN_OAUTH_CLOUD_ID&lt;/code&gt;, and &lt;code&gt;ATLASSIAN_OAUTH_ACCESS_TOKEN&lt;/code&gt;.&lt;/li&gt; 
     &lt;li&gt;Standard OAuth client variables (&lt;code&gt;ATLASSIAN_OAUTH_CLIENT_ID&lt;/code&gt;, &lt;code&gt;CLIENT_SECRET&lt;/code&gt;, &lt;code&gt;REDIRECT_URI&lt;/code&gt;, &lt;code&gt;SCOPE&lt;/code&gt;) are &lt;strong&gt;not&lt;/strong&gt; used.&lt;/li&gt; 
     &lt;li&gt;Token lifecycle (e.g., refreshing the token before it expires and restarting mcp-atlassian) is your responsibility, as the server will not refresh BYOT tokens.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Proxy Configuration&lt;/summary&gt; 
 &lt;p&gt;MCP Atlassian supports routing API requests through standard HTTP/HTTPS/SOCKS proxies. Configure using environment variables:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Supports standard &lt;code&gt;HTTP_PROXY&lt;/code&gt;, &lt;code&gt;HTTPS_PROXY&lt;/code&gt;, &lt;code&gt;NO_PROXY&lt;/code&gt;, &lt;code&gt;SOCKS_PROXY&lt;/code&gt;.&lt;/li&gt; 
  &lt;li&gt;Service-specific overrides are available (e.g., &lt;code&gt;JIRA_HTTPS_PROXY&lt;/code&gt;, &lt;code&gt;CONFLUENCE_NO_PROXY&lt;/code&gt;).&lt;/li&gt; 
  &lt;li&gt;Service-specific variables override global ones for that service.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Add the relevant proxy variables to the &lt;code&gt;args&lt;/code&gt; (using &lt;code&gt;-e&lt;/code&gt;) and &lt;code&gt;env&lt;/code&gt; sections of your MCP configuration:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcp-atlassian": {
      "command": "docker",
      "args": [
        "run",
        "-i",
        "--rm",
        "-e", "... existing Confluence/Jira vars",
        "-e", "HTTP_PROXY",
        "-e", "HTTPS_PROXY",
        "-e", "NO_PROXY",
        "ghcr.io/sooperset/mcp-atlassian:latest"
      ],
      "env": {
        "... existing Confluence/Jira vars": "...",
        "HTTP_PROXY": "http://proxy.internal:8080",
        "HTTPS_PROXY": "http://proxy.internal:8080",
        "NO_PROXY": "localhost,.your-company.com"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Credentials in proxy URLs are masked in logs. If you set &lt;code&gt;NO_PROXY&lt;/code&gt;, it will be respected for requests to matching hosts.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Custom HTTP Headers Configuration&lt;/summary&gt; 
 &lt;p&gt;MCP Atlassian supports adding custom HTTP headers to all API requests. This feature is particularly useful in corporate environments where additional headers are required for security, authentication, or routing purposes.&lt;/p&gt; 
 &lt;p&gt;Custom headers are configured using environment variables with comma-separated key=value pairs:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcp-atlassian": {
      "command": "docker",
      "args": [
        "run",
        "-i",
        "--rm",
        "-e", "CONFLUENCE_URL",
        "-e", "CONFLUENCE_USERNAME",
        "-e", "CONFLUENCE_API_TOKEN",
        "-e", "CONFLUENCE_CUSTOM_HEADERS",
        "-e", "JIRA_URL",
        "-e", "JIRA_USERNAME",
        "-e", "JIRA_API_TOKEN",
        "-e", "JIRA_CUSTOM_HEADERS",
        "ghcr.io/sooperset/mcp-atlassian:latest"
      ],
      "env": {
        "CONFLUENCE_URL": "https://your-company.atlassian.net/wiki",
        "CONFLUENCE_USERNAME": "your.email@company.com",
        "CONFLUENCE_API_TOKEN": "your_confluence_api_token",
        "CONFLUENCE_CUSTOM_HEADERS": "X-Confluence-Service=mcp-integration,X-Custom-Auth=confluence-token,X-ALB-Token=secret-token",
        "JIRA_URL": "https://your-company.atlassian.net",
        "JIRA_USERNAME": "your.email@company.com",
        "JIRA_API_TOKEN": "your_jira_api_token",
        "JIRA_CUSTOM_HEADERS": "X-Forwarded-User=service-account,X-Company-Service=mcp-atlassian,X-Jira-Client=mcp-integration"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Security Considerations:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Custom header values are masked in debug logs to protect sensitive information&lt;/li&gt; 
  &lt;li&gt;Ensure custom headers don't conflict with standard HTTP or Atlassian API headers&lt;/li&gt; 
  &lt;li&gt;Avoid including sensitive authentication tokens in custom headers if already using basic auth or OAuth&lt;/li&gt; 
  &lt;li&gt;Headers are sent with every API request - verify they don't interfere with API functionality&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Multi-Cloud OAuth Support&lt;/summary&gt; 
 &lt;p&gt;MCP Atlassian supports multi-cloud OAuth scenarios where each user connects to their own Atlassian cloud instance. This is useful for multi-tenant applications, chatbots, or services where users provide their own OAuth tokens.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Minimal OAuth Configuration:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;Enable minimal OAuth mode (no client credentials required):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker run -e ATLASSIAN_OAUTH_ENABLE=true -p 9000:9000 \
  ghcr.io/sooperset/mcp-atlassian:latest \
  --transport streamable-http --port 9000
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Users provide authentication via HTTP headers:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;code&gt;Authorization: Bearer &amp;lt;user_oauth_token&amp;gt;&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;code&gt;X-Atlassian-Cloud-Id: &amp;lt;user_cloud_id&amp;gt;&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;Example Integration (Python):&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from mcp.client.streamable_http import streamablehttp_client
from mcp import ClientSession

user_token = "user-specific-oauth-token"
user_cloud_id = "user-specific-cloud-id"

async def main():
    # Connect to streamable HTTP server with custom headers
    async with streamablehttp_client(
        "http://localhost:9000/mcp",
        headers={
            "Authorization": f"Bearer {user_token}",
            "X-Atlassian-Cloud-Id": user_cloud_id
        }
    ) as (read_stream, write_stream, _):
        # Create a session using the client streams
        async with ClientSession(read_stream, write_stream) as session:
            # Initialize the connection
            await session.initialize()

            # Example: Get a Jira issue
            result = await session.call_tool(
                "jira_get_issue",
                {"issue_key": "PROJ-123"}
            )
            print(result)

asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Configuration Notes:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Each request can use a different cloud instance via the &lt;code&gt;X-Atlassian-Cloud-Id&lt;/code&gt; header&lt;/li&gt; 
  &lt;li&gt;User tokens are isolated per request - no cross-tenant data leakage&lt;/li&gt; 
  &lt;li&gt;Falls back to global &lt;code&gt;ATLASSIAN_OAUTH_CLOUD_ID&lt;/code&gt; if header not provided&lt;/li&gt; 
  &lt;li&gt;Compatible with standard OAuth 2.0 bearer token authentication&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Single Service Configurations&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;For Confluence Cloud only:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcp-atlassian": {
      "command": "docker",
      "args": [
        "run",
        "--rm",
        "-i",
        "-e", "CONFLUENCE_URL",
        "-e", "CONFLUENCE_USERNAME",
        "-e", "CONFLUENCE_API_TOKEN",
        "ghcr.io/sooperset/mcp-atlassian:latest"
      ],
      "env": {
        "CONFLUENCE_URL": "https://your-company.atlassian.net/wiki",
        "CONFLUENCE_USERNAME": "your.email@company.com",
        "CONFLUENCE_API_TOKEN": "your_api_token"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;For Confluence Server/DC, use:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcp-atlassian": {
      "command": "docker",
      "args": [
        "run",
        "--rm",
        "-i",
        "-e", "CONFLUENCE_URL",
        "-e", "CONFLUENCE_PERSONAL_TOKEN",
        "ghcr.io/sooperset/mcp-atlassian:latest"
      ],
      "env": {
        "CONFLUENCE_URL": "https://confluence.your-company.com",
        "CONFLUENCE_PERSONAL_TOKEN": "your_personal_token"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;For Jira Cloud only:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcp-atlassian": {
      "command": "docker",
      "args": [
        "run",
        "--rm",
        "-i",
        "-e", "JIRA_URL",
        "-e", "JIRA_USERNAME",
        "-e", "JIRA_API_TOKEN",
        "ghcr.io/sooperset/mcp-atlassian:latest"
      ],
      "env": {
        "JIRA_URL": "https://your-company.atlassian.net",
        "JIRA_USERNAME": "your.email@company.com",
        "JIRA_API_TOKEN": "your_api_token"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;For Jira Server/DC, use:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcp-atlassian": {
      "command": "docker",
      "args": [
        "run",
        "--rm",
        "-i",
        "-e", "JIRA_URL",
        "-e", "JIRA_PERSONAL_TOKEN",
        "ghcr.io/sooperset/mcp-atlassian:latest"
      ],
      "env": {
        "JIRA_URL": "https://jira.your-company.com",
        "JIRA_PERSONAL_TOKEN": "your_personal_token"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;üë• HTTP Transport Configuration&lt;/h3&gt; 
&lt;p&gt;Instead of using &lt;code&gt;stdio&lt;/code&gt;, you can run the server as a persistent HTTP service using either:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;sse&lt;/code&gt; (Server-Sent Events) transport at &lt;code&gt;/sse&lt;/code&gt; endpoint&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;streamable-http&lt;/code&gt; transport at &lt;code&gt;/mcp&lt;/code&gt; endpoint&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Both transport types support single-user and multi-user authentication:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Authentication Options:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Single-User&lt;/strong&gt;: Use server-level authentication configured via environment variables&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-User&lt;/strong&gt;: Each user provides their own authentication: 
  &lt;ul&gt; 
   &lt;li&gt;Cloud: OAuth 2.0 Bearer tokens&lt;/li&gt; 
   &lt;li&gt;Server/Data Center: Personal Access Tokens (PATs)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Basic HTTP Transport Setup&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;Start the server with your chosen transport:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# For SSE transport
docker run --rm -p 9000:9000 \
  --env-file /path/to/your/.env \
  ghcr.io/sooperset/mcp-atlassian:latest \
  --transport sse --port 9000 -vv

# OR for streamable-http transport
docker run --rm -p 9000:9000 \
  --env-file /path/to/your/.env \
  ghcr.io/sooperset/mcp-atlassian:latest \
  --transport streamable-http --port 9000 -vv
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Configure your IDE (single-user example):&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SSE Transport Example:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcp-atlassian-http": {
      "url": "http://localhost:9000/sse"
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Streamable-HTTP Transport Example:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcp-atlassian-service": {
      "url": "http://localhost:9000/mcp"
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Multi-User Authentication Setup&lt;/summary&gt; 
 &lt;p&gt;Here's a complete example of setting up multi-user authentication with streamable-HTTP transport:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;First, run the OAuth setup wizard to configure the server's OAuth credentials:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker run --rm -i \
  -p 8080:8080 \
  -v "${HOME}/.mcp-atlassian:/home/app/.mcp-atlassian" \
  ghcr.io/sooperset/mcp-atlassian:latest --oauth-setup -v
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Start the server with streamable-HTTP transport:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker run --rm -p 9000:9000 \
  --env-file /path/to/your/.env \
  ghcr.io/sooperset/mcp-atlassian:latest \
  --transport streamable-http --port 9000 -vv
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Configure your IDE's MCP settings:&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;Choose the appropriate Authorization method for your Atlassian deployment:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Cloud (OAuth 2.0):&lt;/strong&gt; Use this if your organization is on Atlassian Cloud and you have an OAuth access token for each user.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Server/Data Center (PAT):&lt;/strong&gt; Use this if you are on Atlassian Server or Data Center and each user has a Personal Access Token (PAT).&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Cloud (OAuth 2.0) Example:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcp-atlassian-service": {
      "url": "http://localhost:9000/mcp",
      "headers": {
        "Authorization": "Bearer &amp;lt;USER_OAUTH_ACCESS_TOKEN&amp;gt;"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Server/Data Center (PAT) Example:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcp-atlassian-service": {
      "url": "http://localhost:9000/mcp",
      "headers": {
        "Authorization": "Token &amp;lt;USER_PERSONAL_ACCESS_TOKEN&amp;gt;"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="4"&gt; 
  &lt;li&gt;Required environment variables in &lt;code&gt;.env&lt;/code&gt;: &lt;pre&gt;&lt;code class="language-bash"&gt;JIRA_URL=https://your-company.atlassian.net
CONFLUENCE_URL=https://your-company.atlassian.net/wiki
ATLASSIAN_OAUTH_CLIENT_ID=your_oauth_app_client_id
ATLASSIAN_OAUTH_CLIENT_SECRET=your_oauth_app_client_secret
ATLASSIAN_OAUTH_REDIRECT_URI=http://localhost:8080/callback
ATLASSIAN_OAUTH_SCOPE=read:jira-work write:jira-work read:confluence-content.all write:confluence-content offline_access
ATLASSIAN_OAUTH_CLOUD_ID=your_cloud_id_from_setup_wizard
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;[!NOTE]&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;The server should have its own fallback authentication configured (e.g., via environment variables for API token, PAT, or its own OAuth setup using --oauth-setup). This is used if a request doesn't include user-specific authentication.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;OAuth&lt;/strong&gt;: Each user needs their own OAuth access token from your Atlassian OAuth app.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;PAT&lt;/strong&gt;: Each user provides their own Personal Access Token.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Multi-Cloud&lt;/strong&gt;: For OAuth users, optionally include &lt;code&gt;X-Atlassian-Cloud-Id&lt;/code&gt; header to specify which Atlassian cloud instance to use&lt;/li&gt; 
   &lt;li&gt;The server will use the user's token for API calls when provided, falling back to server auth if not&lt;/li&gt; 
   &lt;li&gt;User tokens should have appropriate scopes for their needed operations&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;h2&gt;Tools&lt;/h2&gt; 
&lt;h3&gt;Key Tools&lt;/h3&gt; 
&lt;h4&gt;Jira Tools&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;jira_get_issue&lt;/code&gt;: Get details of a specific issue&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jira_search&lt;/code&gt;: Search issues using JQL&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jira_create_issue&lt;/code&gt;: Create a new issue&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jira_update_issue&lt;/code&gt;: Update an existing issue&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jira_transition_issue&lt;/code&gt;: Transition an issue to a new status&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jira_add_comment&lt;/code&gt;: Add a comment to an issue&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Confluence Tools&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;confluence_search&lt;/code&gt;: Search Confluence content using CQL&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;confluence_get_page&lt;/code&gt;: Get content of a specific page&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;confluence_create_page&lt;/code&gt;: Create a new page&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;confluence_update_page&lt;/code&gt;: Update an existing page&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;View All Tools&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Operation&lt;/th&gt; 
    &lt;th&gt;Jira Tools&lt;/th&gt; 
    &lt;th&gt;Confluence Tools&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Read&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_search&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;confluence_search&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_get_issue&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;confluence_get_page&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_get_all_projects&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;confluence_get_page_children&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_get_project_issues&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;confluence_get_comments&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_get_worklog&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;confluence_get_labels&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_get_transitions&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;confluence_search_user&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_search_fields&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_get_agile_boards&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_get_board_issues&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_get_sprints_from_board&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_get_sprint_issues&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_get_issue_link_types&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_batch_get_changelogs&lt;/code&gt;*&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_get_user_profile&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_download_attachments&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_get_project_versions&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Write&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_create_issue&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;confluence_create_page&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_update_issue&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;confluence_update_page&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_delete_issue&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;confluence_delete_page&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_batch_create_issues&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;confluence_add_label&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_add_comment&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;confluence_add_comment&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_transition_issue&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_add_worklog&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_link_to_epic&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_create_sprint&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_update_sprint&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_create_issue_link&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_remove_issue_link&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_create_version&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_batch_create_versions&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;p&gt;*Tool only available on Jira Cloud&lt;/p&gt;  
&lt;h3&gt;Tool Filtering and Access Control&lt;/h3&gt; 
&lt;p&gt;The server provides two ways to control tool access:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tool Filtering&lt;/strong&gt;: Use &lt;code&gt;--enabled-tools&lt;/code&gt; flag or &lt;code&gt;ENABLED_TOOLS&lt;/code&gt; environment variable to specify which tools should be available:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Via environment variable
ENABLED_TOOLS="confluence_search,jira_get_issue,jira_search"

# Or via command line flag
docker run ... --enabled-tools "confluence_search,jira_get_issue,jira_search" ...
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Read/Write Control&lt;/strong&gt;: Tools are categorized as read or write operations. When &lt;code&gt;READ_ONLY_MODE&lt;/code&gt; is enabled, only read operations are available regardless of &lt;code&gt;ENABLED_TOOLS&lt;/code&gt; setting.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Troubleshooting &amp;amp; Debugging&lt;/h2&gt; 
&lt;h3&gt;Common Issues&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Authentication Failures&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;For Cloud: Check your API tokens (not your account password)&lt;/li&gt; 
   &lt;li&gt;For Server/Data Center: Verify your personal access token is valid and not expired&lt;/li&gt; 
   &lt;li&gt;For older Confluence servers: Some older versions require basic authentication with &lt;code&gt;CONFLUENCE_USERNAME&lt;/code&gt; and &lt;code&gt;CONFLUENCE_API_TOKEN&lt;/code&gt; (where token is your password)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SSL Certificate Issues&lt;/strong&gt;: If using Server/Data Center and encounter SSL errors, set &lt;code&gt;CONFLUENCE_SSL_VERIFY=false&lt;/code&gt; or &lt;code&gt;JIRA_SSL_VERIFY=false&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Permission Errors&lt;/strong&gt;: Ensure your Atlassian account has sufficient permissions to access the spaces/projects&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Custom Headers Issues&lt;/strong&gt;: See the &lt;a href="https://raw.githubusercontent.com/sooperset/mcp-atlassian/main/#debugging-custom-headers"&gt;"Debugging Custom Headers"&lt;/a&gt; section below to analyze and resolve issues with custom headers&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Debugging Custom Headers&lt;/h3&gt; 
&lt;p&gt;To verify custom headers are being applied correctly:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Enable Debug Logging&lt;/strong&gt;: Set &lt;code&gt;MCP_VERY_VERBOSE=true&lt;/code&gt; to see detailed request logs&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# In your .env file or environment
MCP_VERY_VERBOSE=true
MCP_LOGGING_STDOUT=true
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Check Header Parsing&lt;/strong&gt;: Custom headers appear in logs with masked values for security:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;DEBUG Custom headers applied: {'X-Forwarded-User': '***', 'X-ALB-Token': '***'}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Verify Service-Specific Headers&lt;/strong&gt;: Check logs to confirm the right headers are being used:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;DEBUG Jira request headers: service-specific headers applied
DEBUG Confluence request headers: service-specific headers applied
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Test Header Format&lt;/strong&gt;: Ensure your header string format is correct:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Correct format
JIRA_CUSTOM_HEADERS=X-Custom=value1,X-Other=value2
CONFLUENCE_CUSTOM_HEADERS=X-Custom=value1,X-Other=value2

# Incorrect formats (will be ignored)
JIRA_CUSTOM_HEADERS="X-Custom=value1,X-Other=value2"  # Extra quotes
JIRA_CUSTOM_HEADERS=X-Custom: value1,X-Other: value2  # Colon instead of equals
JIRA_CUSTOM_HEADERS=X-Custom = value1               # Spaces around equals
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Security Note&lt;/strong&gt;: Header values containing sensitive information (tokens, passwords) are automatically masked in logs to prevent accidental exposure.&lt;/p&gt; 
&lt;h3&gt;Debugging Tools&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Using MCP Inspector for testing
npx @modelcontextprotocol/inspector uvx mcp-atlassian ...

# For local development version
npx @modelcontextprotocol/inspector uv --directory /path/to/your/mcp-atlassian run mcp-atlassian ...

# View logs
# macOS
tail -n 20 -f ~/Library/Logs/Claude/mcp*.log
# Windows
type %APPDATA%\Claude\logs\mcp*.log | more
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Security&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Never share API tokens&lt;/li&gt; 
 &lt;li&gt;Keep .env files secure and private&lt;/li&gt; 
 &lt;li&gt;See &lt;a href="https://raw.githubusercontent.com/sooperset/mcp-atlassian/main/SECURITY.md"&gt;SECURITY.md&lt;/a&gt; for best practices&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions to MCP Atlassian! If you'd like to contribute:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Check out our &lt;a href="https://raw.githubusercontent.com/sooperset/mcp-atlassian/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; guide for detailed development setup instructions.&lt;/li&gt; 
 &lt;li&gt;Make changes and submit a pull request.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;We use pre-commit hooks for code quality and follow semantic versioning for releases.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Licensed under MIT - see &lt;a href="https://raw.githubusercontent.com/sooperset/mcp-atlassian/main/LICENSE"&gt;LICENSE&lt;/a&gt; file. This is not an official Atlassian product.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>droidrun/droidrun</title>
      <link>https://github.com/droidrun/droidrun</link>
      <description>&lt;p&gt;Automate your mobile devices with natural language commands - an LLM agnostic mobile Agent ü§ñ&lt;/p&gt;&lt;hr&gt;&lt;picture align="center"&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="./static/droidrun-dark.png" /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="./static/droidrun.png" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/droidrun/droidrun/main/static/droidrun.png" width="full" /&gt; 
&lt;/picture&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://docs.droidrun.ai"&gt;&lt;img src="https://img.shields.io/badge/Docs-%F0%9F%93%95-0D9373?style=for-the-badge" alt="Docs" /&gt;&lt;/a&gt; &lt;a href="https://cloud.droidrun.ai/sign-in?waitlist=true"&gt;&lt;img src="https://img.shields.io/badge/Cloud-%E2%98%81%EF%B8%8F-0D9373?style=for-the-badge" alt="Cloud" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/droidrun/droidrun/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/droidrun/droidrun?style=social" alt="GitHub stars" /&gt;&lt;/a&gt; &lt;a href="https://droidrun.ai"&gt;&lt;img src="https://img.shields.io/badge/droidrun.ai-white" alt="droidrun.ai" /&gt;&lt;/a&gt; &lt;a href="https://x.com/droid_run"&gt;&lt;img src="https://img.shields.io/twitter/follow/droid_run?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/ZZbKEZZkwK"&gt;&lt;img src="https://img.shields.io/discord/1360219330318696488?color=white&amp;amp;label=Discord&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://droidrun.ai/benchmark"&gt;&lt;img src="https://img.shields.io/badge/Benchmark-91.4%EF%B9%AA-white" alt="Benchmark" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=983810&amp;amp;theme=dark&amp;amp;period=daily&amp;amp;t=1753948032207" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=983810&amp;amp;theme=neutral&amp;amp;period=daily&amp;amp;t=1753948125523" /&gt; 
  &lt;a href="https://www.producthunt.com/products/droidrun-framework-for-mobile-agent?embed=true&amp;amp;utm_source=badge-top-post-badge&amp;amp;utm_medium=badge&amp;amp;utm_source=badge-droidrun" target="_blank"&gt;&lt;img src="https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=983810&amp;amp;theme=neutral&amp;amp;period=daily&amp;amp;t=1753948125523" alt="Droidrun - Give AI native control of physical &amp;amp; virtual phones. | Product Hunt" style="width: 200px; height: 54px;" width="200" height="54" /&gt;&lt;/a&gt; 
 &lt;/picture&gt; 
 &lt;p&gt;&lt;a href="https://zdoc.app/de/droidrun/droidrun"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://zdoc.app/es/droidrun/droidrun"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://zdoc.app/fr/droidrun/droidrun"&gt;fran√ßais&lt;/a&gt; | &lt;a href="https://zdoc.app/ja/droidrun/droidrun"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://zdoc.app/ko/droidrun/droidrun"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://zdoc.app/pt/droidrun/droidrun"&gt;Portugu√™s&lt;/a&gt; | &lt;a href="https://zdoc.app/ru/droidrun/droidrun"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://zdoc.app/zh/droidrun/droidrun"&gt;‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;DroidRun is a powerful framework for controlling Android and iOS devices through LLM agents. It allows you to automate device interactions using natural language commands. &lt;a href="https://droidrun.ai/benchmark"&gt;Checkout our benchmark results&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Why Droidrun?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ü§ñ Control Android and iOS devices with natural language commands&lt;/li&gt; 
 &lt;li&gt;üîÄ Supports multiple LLM providers (OpenAI, Anthropic, Gemini, Ollama, DeepSeek)&lt;/li&gt; 
 &lt;li&gt;üß† Planning capabilities for complex multi-step tasks&lt;/li&gt; 
 &lt;li&gt;üíª Easy to use CLI with enhanced debugging features&lt;/li&gt; 
 &lt;li&gt;üêç Extendable Python API for custom automations&lt;/li&gt; 
 &lt;li&gt;üì∏ Screenshot analysis for visual understanding of the device&lt;/li&gt; 
 &lt;li&gt;ü´Ü Execution tracing with Arize Phoenix&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üì¶ Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install 'droidrun[google,anthropic,openai,deepseek,ollama,dev]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üöÄ Quickstart&lt;/h2&gt; 
&lt;p&gt;Read on how to get droidrun up and running within seconds in &lt;a href="https://docs.droidrun.ai/v3/quickstart"&gt;our docs&lt;/a&gt;!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=4WT7FXJah2I"&gt;&lt;img src="https://img.youtube.com/vi/4WT7FXJah2I/0.jpg" alt="Quickstart Video" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üé¨ Demo Videos&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Accommodation booking&lt;/strong&gt;: Let Droidrun search for an apartment for you&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/VUpCyq1PSXw"&gt;&lt;img src="https://img.youtube.com/vi/VUpCyq1PSXw/0.jpg" alt="Droidrun Accommodation Booking Demo" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;br /&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Trend Hunter&lt;/strong&gt;: Let Droidrun hunt down trending posts&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/7V8S2f8PnkQ"&gt;&lt;img src="https://img.youtube.com/vi/7V8S2f8PnkQ/0.jpg" alt="Droidrun Trend Hunter Demo" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;br /&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Streak Saver&lt;/strong&gt;: Let Droidrun save your streak on your favorite language learning app&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/B5q2B467HKw"&gt;&lt;img src="https://img.youtube.com/vi/B5q2B467HKw/0.jpg" alt="Droidrun Streak Saver Demo" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;üí° Example Use Cases&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Automated UI testing of mobile applications&lt;/li&gt; 
 &lt;li&gt;Creating guided workflows for non-technical users&lt;/li&gt; 
 &lt;li&gt;Automating repetitive tasks on mobile devices&lt;/li&gt; 
 &lt;li&gt;Remote assistance for less technical users&lt;/li&gt; 
 &lt;li&gt;Exploring mobile UI with natural language commands&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üë• Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! Please feel free to submit a Pull Request.&lt;/p&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt; 
&lt;h2&gt;Security Checks&lt;/h2&gt; 
&lt;p&gt;To ensure the security of the codebase, we have integrated security checks using &lt;code&gt;bandit&lt;/code&gt; and &lt;code&gt;safety&lt;/code&gt;. These tools help identify potential security issues in the code and dependencies.&lt;/p&gt; 
&lt;h3&gt;Running Security Checks&lt;/h3&gt; 
&lt;p&gt;Before submitting any code, please run the following security checks:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bandit&lt;/strong&gt;: A tool to find common security issues in Python code.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;bandit -r droidrun
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Safety&lt;/strong&gt;: A tool to check your installed dependencies for known security vulnerabilities.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;safety scan
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt;</description>
    </item>
    
    <item>
      <title>swisskyrepo/PayloadsAllTheThings</title>
      <link>https://github.com/swisskyrepo/PayloadsAllTheThings</link>
      <description>&lt;p&gt;A list of useful payloads and bypass for Web Application Security and Pentest/CTF&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Payloads All The Things&lt;/h1&gt; 
&lt;p&gt;A list of useful payloads and bypasses for Web Application Security. Feel free to improve with your payloads and techniques!&lt;/p&gt; 
&lt;p&gt;You can also contribute with a &lt;span&gt;üçª&lt;/span&gt; IRL, or using the sponsor button.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/sponsors/swisskyrepo"&gt;&lt;img src="https://img.shields.io/static/v1?label=Sponsor&amp;amp;message=%E2%9D%A4&amp;amp;logo=GitHub&amp;amp;link=https://github.com/sponsors/swisskyrepo" alt="Sponsor" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/intent/tweet?text=Payloads%20All%20The%20Things,%20a%20list%20of%20useful%20payloads%20and%20bypasses%20for%20Web%20Application%20Security%20-%20by%20@pentest_swissky&amp;amp;url=https://github.com/swisskyrepo/PayloadsAllTheThings/"&gt;&lt;img src="https://img.shields.io/twitter/url/http/shields.io.svg?style=social" alt="Tweet" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;An alternative display version is available at &lt;a href="https://swisskyrepo.github.io/PayloadsAllTheThings/"&gt;PayloadsAllTheThingsWeb&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/swisskyrepo/PayloadsAllTheThings/master/.github/banner.png" alt="banner" /&gt; &lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;üìñ&lt;/span&gt; Documentation&lt;/h2&gt; 
&lt;p&gt;Every section contains the following files, you can use the &lt;code&gt;_template_vuln&lt;/code&gt; folder to create a new chapter:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;README.md - vulnerability description and how to exploit it, including several payloads&lt;/li&gt; 
 &lt;li&gt;Intruder - a set of files to give to Burp Intruder&lt;/li&gt; 
 &lt;li&gt;Images - pictures for the README.md&lt;/li&gt; 
 &lt;li&gt;Files - some files referenced in the README.md&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You might also like the other projects from the AllTheThings family :&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://swisskyrepo.github.io/InternalAllTheThings/"&gt;InternalAllTheThings&lt;/a&gt; - Active Directory and Internal Pentest Cheatsheets&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://swisskyrepo.github.io/HardwareAllTheThings/"&gt;HardwareAllTheThings&lt;/a&gt; - Hardware/IOT Pentesting Wiki&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You want more? Check the &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/_LEARNING_AND_SOCIALS/BOOKS.md"&gt;Books&lt;/a&gt; and &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/_LEARNING_AND_SOCIALS/YOUTUBE.md"&gt;YouTube channel&lt;/a&gt; selections.&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;üßëüíª&lt;/span&gt; Contributions&lt;/h2&gt; 
&lt;p&gt;Be sure to read &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=swisskyrepo/PayloadsAllTheThings&amp;amp;max=36" alt="sponsors-list" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;Thanks again for your contribution! &lt;span&gt;‚ù§Ô∏è&lt;/span&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;üçª&lt;/span&gt; Sponsors&lt;/h2&gt; 
&lt;p&gt;This project is proudly sponsored by these companies.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Logo&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://serpapi.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/34724717?s=40&amp;amp;v=4" alt="sponsor-serpapi" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;SerpApi&lt;/strong&gt; is a real time API to access Google search results. It solves the issues of having to rent proxies, solving captchas, and JSON parsing.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://projectdiscovery.io/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/50994705?s=40&amp;amp;v=4" alt="sponsor-projectdiscovery" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;ProjectDiscovery&lt;/strong&gt; - Detect real, exploitable vulnerabilities. Harness the power of Nuclei for fast and accurate findings without false positives.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.vaadata.com/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/48131541?s=40&amp;amp;v=4" alt="sponsor-vaadata" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;VAADATA&lt;/strong&gt; - Ethical Hacking Services&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>NVIDIA-NeMo/Gym</title>
      <link>https://github.com/NVIDIA-NeMo/Gym</link>
      <description>&lt;p&gt;Build RL environments for LLM training&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;NeMo Gym&lt;/h1&gt; 
&lt;p&gt;NeMo Gym is a library for building reinforcement learning (RL) training environments for large language models (LLMs). It provides infrastructure to develop environments, scale rollout collection, and integrate seamlessly with your preferred training framework.&lt;/p&gt; 
&lt;p&gt;NeMo Gym is a component of the &lt;a href="https://docs.nvidia.com/nemo-framework/"&gt;NVIDIA NeMo Framework&lt;/a&gt;, NVIDIA‚Äôs GPU-accelerated platform for building and training generative AI models.&lt;/p&gt; 
&lt;h2&gt;üèÜ Why NeMo Gym?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Scaffolding and patterns to accelerate environment development: multi-step, multi-turn, and user modeling scenarios&lt;/li&gt; 
 &lt;li&gt;Contribute environments without expert knowledge of the entire RL training loop&lt;/li&gt; 
 &lt;li&gt;Test environments and throughput end-to-end, independent of the RL training loop&lt;/li&gt; 
 &lt;li&gt;Interoperable with existing environments, systems, and RL training frameworks&lt;/li&gt; 
 &lt;li&gt;Growing collection of training environments and datasets for Reinforcement Learning from Verifiable Reward (RLVR)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] NeMo Gym is currently in early development. You should expect evolving APIs, incomplete documentation, and occasional bugs. We welcome contributions and feedback - for any changes, please open an issue first to kick off discussion!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;üìã Requirements&lt;/h2&gt; 
&lt;h3&gt;Hardware Requirements&lt;/h3&gt; 
&lt;p&gt;NeMo Gym is designed to run on standard development machines:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;GPU&lt;/strong&gt;: Not required for NeMo Gym library operation 
  &lt;ul&gt; 
   &lt;li&gt;GPU may be needed for specific resource servers or model inference (see individual server documentation)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: Any modern x86_64 or ARM64 processor (e.g., Intel, AMD, Apple Silicon)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;RAM&lt;/strong&gt;: Minimum 8 GB (16 GB+ recommended for larger environments)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: Minimum 5 GB free disk space for installation and basic usage&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Software Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Operating System&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;Linux (Ubuntu 20.04+, or equivalent)&lt;/li&gt; 
   &lt;li&gt;macOS (11.0+ for x86_64, 12.0+ for Apple Silicon)&lt;/li&gt; 
   &lt;li&gt;Windows (via WSL2)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt;: 3.12 or higher&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Git&lt;/strong&gt;: For cloning the repository&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Internet Connection&lt;/strong&gt;: Required for downloading dependencies and API access&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Additional Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;API Keys&lt;/strong&gt;: OpenAI API key with available credits (for the quickstart examples) 
  &lt;ul&gt; 
   &lt;li&gt;Other model providers supported (Azure OpenAI, self-hosted models via vLLM)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ray&lt;/strong&gt;: Automatically installed as a dependency (no separate setup required)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;h3&gt;Setup&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the repository
git clone git@github.com:NVIDIA-NeMo/Gym.git
cd Gym

# Install UV (Python package manager)
curl -LsSf https://astral.sh/uv/install.sh | sh
source $HOME/.local/bin/env

# Create virtual environment
uv venv --python 3.12
source .venv/bin/activate

# Install NeMo Gym
uv sync --extra dev --group docs
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Configure Your API Key&lt;/h3&gt; 
&lt;p&gt;Create an &lt;code&gt;env.yaml&lt;/code&gt; file that contains your OpenAI API key and the &lt;a href="https://docs.nvidia.com/nemo/gym/latest/about/concepts/key-terminology.html#term-Policy-Model"&gt;policy model&lt;/a&gt; you want to use. Replace &lt;code&gt;your-openai-api-key&lt;/code&gt; with your actual key. This file helps keep your secrets out of version control while still making them available to NeMo Gym.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;echo "policy_base_url: https://api.openai.com/v1
policy_api_key: your-openai-api-key
policy_model_name: gpt-4.1-2025-04-14" &amp;gt; env.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] We use GPT-4.1 in this quickstart because it provides low latency (no reasoning step) and works reliably out-of-the-box. NeMo Gym is &lt;strong&gt;not limited to OpenAI models&lt;/strong&gt;‚Äîyou can use self-hosted models via vLLM or any OpenAI-compatible inference server. See the &lt;a href="https://docs.nvidia.com/nemo/gym/latest/get-started/detailed-setup.html"&gt;documentation&lt;/a&gt; for details.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Start Servers&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Terminal 1 (start servers)&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start servers (this will keep running)
config_paths="resources_servers/example_single_tool_call/configs/example_single_tool_call.yaml,\
responses_api_models/openai_model/configs/openai_model.yaml"
ng_run "+config_paths=[${config_paths}]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Terminal 2 (interact with agent)&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# In a NEW terminal, activate environment
source .venv/bin/activate

# Interact with your agent
python responses_api_agents/simple_agent/client.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Collect Rollouts&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Terminal 2&lt;/strong&gt; (keep servers running in Terminal 1):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create a simple dataset with one query
echo '{"responses_create_params":{"input":[{"role":"developer","content":"You are a helpful assistant."},{"role":"user","content":"What is the weather in Seattle?"}]}}' &amp;gt; weather_query.jsonl

# Collect verified rollouts
ng_collect_rollouts \
    +agent_name=example_single_tool_call_simple_agent \
    +input_jsonl_fpath=weather_query.jsonl \
    +output_jsonl_fpath=weather_rollouts.jsonl

# View the result
cat weather_rollouts.jsonl | python -m json.tool
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This generates training data with verification scores!&lt;/p&gt; 
&lt;h3&gt;Clean Up Servers&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Terminal 1&lt;/strong&gt; with the running servers: Ctrl+C to stop the ng_run process.&lt;/p&gt; 
&lt;h3&gt;What's Next?&lt;/h3&gt; 
&lt;p&gt;Now that you can generate rollouts, choose your path:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Use an existing training environment&lt;/strong&gt; ‚Äî Browse the &lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/Gym/main/#-available-resource-servers"&gt;Available Resource Servers&lt;/a&gt; below to find a training-ready environment that matches your goals.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Build a custom training environment&lt;/strong&gt; ‚Äî Implement or integrate existing tools and define task verification logic. Get started with the &lt;a href="https://docs.nvidia.com/nemo/gym/latest/tutorials/creating-resource-server.html"&gt;Creating a Resource Server&lt;/a&gt; tutorial.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üì¶ Available Resource Servers&lt;/h2&gt; 
&lt;p&gt;NeMo Gym includes a curated collection of resource servers for training and evaluation across multiple domains:&lt;/p&gt; 
&lt;h3&gt;Table 1: Example Resource Servers&lt;/h3&gt; 
&lt;p&gt;Purpose: Demonstrate NeMo Gym patterns and concepts.&lt;/p&gt; 
&lt;!-- START_EXAMPLE_ONLY_SERVERS_TABLE --&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Name&lt;/th&gt; 
   &lt;th&gt;Demonstrates&lt;/th&gt; 
   &lt;th&gt;Config&lt;/th&gt; 
   &lt;th&gt;README&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Multi Step&lt;/td&gt; 
   &lt;td&gt;Multi-step tool calling&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/Gym/main/resources_servers/example_multi_step/configs/example_multi_step.yaml"&gt;example_multi_step.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/Gym/main/resources_servers/example_multi_step/README.md"&gt;README&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Session State Mgmt&lt;/td&gt; 
   &lt;td&gt;Session state management (in-memory)&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/Gym/main/resources_servers/example_session_state_mgmt/configs/example_session_state_mgmt.yaml"&gt;example_session_state_mgmt.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/Gym/main/resources_servers/example_session_state_mgmt/README.md"&gt;README&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Single Tool Call&lt;/td&gt; 
   &lt;td&gt;Basic single-step tool calling&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/Gym/main/resources_servers/example_single_tool_call/configs/example_single_tool_call.yaml"&gt;example_single_tool_call.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/Gym/main/resources_servers/example_single_tool_call/README.md"&gt;README&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!-- END_EXAMPLE_ONLY_SERVERS_TABLE --&gt; 
&lt;h3&gt;Table 2: Resource Servers for Training&lt;/h3&gt; 
&lt;p&gt;Purpose: Training-ready environments with curated datasets.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Each resource server includes example data, configuration files, and tests. See each server's README for details.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;!-- START_TRAINING_SERVERS_TABLE --&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Resource Server&lt;/th&gt; 
   &lt;th&gt;Domain&lt;/th&gt; 
   &lt;th&gt;Dataset&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Value&lt;/th&gt; 
   &lt;th&gt;Config&lt;/th&gt; 
   &lt;th&gt;Train&lt;/th&gt; 
   &lt;th&gt;Validation&lt;/th&gt; 
   &lt;th&gt;License&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Calendar&lt;/td&gt; 
   &lt;td&gt;agent&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/datasets/nvidia/Nemotron-RL-agent-calendar_scheduling"&gt;Nemotron-RL-agent-calendar_scheduling&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/Gym/main/resources_servers/calendar/configs/calendar.yaml"&gt;config&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;Apache 2.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Google Search&lt;/td&gt; 
   &lt;td&gt;agent&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/datasets/nvidia/Nemotron-RL-knowledge-web_search-mcqa"&gt;Nemotron-RL-knowledge-web_search-mcqa&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Multi-choice question answering problems with search tools integrated&lt;/td&gt; 
   &lt;td&gt;Improve knowledge-related benchmarks with search tools&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/Gym/main/resources_servers/google_search/configs/google_search.yaml"&gt;config&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;Apache 2.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Math Advanced Calculations&lt;/td&gt; 
   &lt;td&gt;agent&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/datasets/nvidia/Nemotron-RL-math-advanced_calculations"&gt;Nemotron-RL-math-advanced_calculations&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;An instruction following math environment with counter-intuitive calculators&lt;/td&gt; 
   &lt;td&gt;Improve instruction following capabilities in specific math environments&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/Gym/main/resources_servers/math_advanced_calculations/configs/math_advanced_calculations.yaml"&gt;config&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;Apache 2.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Workplace Assistant&lt;/td&gt; 
   &lt;td&gt;agent&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/datasets/nvidia/Nemotron-RL-agent-workplace_assistant"&gt;Nemotron-RL-agent-workplace_assistant&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Workplace assistant multi-step tool-using environment&lt;/td&gt; 
   &lt;td&gt;Improve multi-step tool use capability&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/Gym/main/resources_servers/workplace_assistant/configs/workplace_assistant.yaml"&gt;config&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;Apache 2.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Code Gen&lt;/td&gt; 
   &lt;td&gt;coding&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/datasets/nvidia/nemotron-RL-coding-competitive_coding"&gt;nemotron-RL-coding-competitive_coding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/Gym/main/resources_servers/code_gen/configs/code_gen.yaml"&gt;config&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;Apache 2.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mini Swe Agent&lt;/td&gt; 
   &lt;td&gt;coding&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/datasets/SWE-Gym/SWE-Gym"&gt;SWE-Gym&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A software development with mini-swe-agent orchestration&lt;/td&gt; 
   &lt;td&gt;Improve software development capabilities, like SWE-bench&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/Gym/main/resources_servers/mini_swe_agent/configs/mini_swe_agent.yaml"&gt;config&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;MIT&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Instruction Following&lt;/td&gt; 
   &lt;td&gt;instruction_following&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/datasets/nvidia/Nemotron-RL-instruction_following"&gt;Nemotron-RL-instruction_following&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Instruction following datasets targeting IFEval and IFBench style instruction following capabilities&lt;/td&gt; 
   &lt;td&gt;Improve IFEval and IFBench&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/Gym/main/resources_servers/instruction_following/configs/instruction_following.yaml"&gt;config&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;Apache 2.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Structured Outputs&lt;/td&gt; 
   &lt;td&gt;instruction_following&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/datasets/nvidia/Nemotron-RL-instruction_following-structured_outputs"&gt;Nemotron-RL-instruction_following-structured_outputs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Check if responses are following structured output requirements in prompts&lt;/td&gt; 
   &lt;td&gt;Improve instruction following capabilities&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/Gym/main/resources_servers/structured_outputs/configs/structured_outputs_json.yaml"&gt;config&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;Apache 2.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Equivalence Llm Judge&lt;/td&gt; 
   &lt;td&gt;knowledge&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/datasets/nvidia/Nemotron-RL-knowledge-openQA"&gt;Nemotron-RL-knowledge-openQA&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Short answer questions with LLM-as-a-judge&lt;/td&gt; 
   &lt;td&gt;Improve knowledge-related benchmarks like GPQA / HLE&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/Gym/main/resources_servers/equivalence_llm_judge/configs/equivalence_llm_judge.yaml"&gt;config&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;Apache 2.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mcqa&lt;/td&gt; 
   &lt;td&gt;knowledge&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/datasets/nvidia/Nemotron-RL-knowledge-mcqa"&gt;Nemotron-RL-knowledge-mcqa&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Multi-choice question answering problems&lt;/td&gt; 
   &lt;td&gt;Improve benchmarks like MMLU / GPQA / HLE&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/Gym/main/resources_servers/mcqa/configs/mcqa.yaml"&gt;config&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;Apache 2.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Math With Judge&lt;/td&gt; 
   &lt;td&gt;math&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/datasets/nvidia/Nemotron-RL-math-OpenMathReasoning"&gt;Nemotron-RL-math-OpenMathReasoning&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Math dataset with math-verify and LLM-as-a-judge&lt;/td&gt; 
   &lt;td&gt;Improve math capabilities including AIME 24 / 25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/Gym/main/resources_servers/math_with_judge/configs/math_with_judge.yaml"&gt;config&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;Creative Commons Attribution 4.0 International&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Math With Judge&lt;/td&gt; 
   &lt;td&gt;math&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/datasets/nvidia/Nemotron-RL-math-stack_overflow"&gt;Nemotron-RL-math-stack_overflow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/Gym/main/resources_servers/math_with_judge/configs/math_stack_overflow.yaml"&gt;config&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;Creative Commons Attribution-ShareAlike 4.0 International&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!-- END_TRAINING_SERVERS_TABLE --&gt; 
&lt;h2&gt;üìñ Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.nvidia.com/nemo/gym/latest/index.html"&gt;Documentation&lt;/a&gt;&lt;/strong&gt; - Technical reference docs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.nvidia.com/nemo/gym/latest/tutorials/index.html"&gt;Tutorials&lt;/a&gt;&lt;/strong&gt; - Hands-on tutorials and practical examples&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ü§ù Community &amp;amp; Support&lt;/h2&gt; 
&lt;p&gt;We'd love your contributions! Here's how to get involved:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/NVIDIA-NeMo/Gym/issues"&gt;Report Issues&lt;/a&gt;&lt;/strong&gt; - Bug reports and feature requests&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.nvidia.com/nemo/gym/latest/contribute/index.html"&gt;Contributing Guide&lt;/a&gt;&lt;/strong&gt; - How to contribute code, docs, new environments, or training framework integrations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìö Citations&lt;/h2&gt; 
&lt;p&gt;If you use NeMo Gym in your research, please cite it using the following BibTeX entry:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{nemo-gym,
  title = {NeMo Gym: An Open Source Library for Scaling Reinforcement Learning Environments for LLM},
  howpublished = {\url{https://github.com/NVIDIA-NeMo/Gym}},
  author={NVIDIA},
  year = {2025},
  note = {GitHub repository},
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>pytorch/executorch</title>
      <link>https://github.com/pytorch/executorch</link>
      <description>&lt;p&gt;On-device AI across mobile, embedded and edge for PyTorch&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/pytorch/executorch/main/docs/source/_static/img/et-logo.png" alt="ExecuTorch logo mark" width="200" /&gt; 
 &lt;h1&gt;ExecuTorch&lt;/h1&gt; 
 &lt;p&gt;&lt;strong&gt;On-device AI inference powered by PyTorch&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://pypi.org/project/executorch/"&gt;&lt;img src="https://img.shields.io/pypi/v/executorch?style=for-the-badge&amp;amp;color=blue" alt="PyPI - Version" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/pytorch/executorch/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/pytorch/executorch?style=for-the-badge&amp;amp;color=blue" alt="GitHub - Contributors" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/pytorch/executorch/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/pytorch/executorch?style=for-the-badge&amp;amp;color=blue" alt="GitHub - Stars" /&gt;&lt;/a&gt; 
 &lt;a href="https://discord.gg/Dh43CKSAdc"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20Us-blue?logo=discord&amp;amp;logoColor=white&amp;amp;style=for-the-badge" alt="Discord - Chat with Us" /&gt;&lt;/a&gt; 
 &lt;a href="https://docs.pytorch.org/executorch/main/index.html"&gt;&lt;img src="https://img.shields.io/badge/Documentation-blue?logo=googledocs&amp;amp;logoColor=white&amp;amp;style=for-the-badge" alt="Documentation" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;ExecuTorch&lt;/strong&gt; is PyTorch's unified solution for deploying AI models on-device‚Äîfrom smartphones to microcontrollers‚Äîbuilt for privacy, performance, and portability. It powers Meta's on-device AI across &lt;strong&gt;Instagram, WhatsApp, Quest 3, Ray-Ban Meta Smart Glasses&lt;/strong&gt;, and &lt;a href="https://docs.pytorch.org/executorch/main/success-stories.html"&gt;more&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Deploy &lt;strong&gt;LLMs, vision, speech, and multimodal models&lt;/strong&gt; with the same PyTorch APIs you already know‚Äîaccelerating research to production with seamless model export, optimization, and deployment. No manual C++ rewrites. No format conversions. No vendor lock-in.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìò Table of Contents&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pytorch/executorch/main/#why-executorch"&gt;Why ExecuTorch?&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pytorch/executorch/main/#how-it-works"&gt;How It Works&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pytorch/executorch/main/#quick-start"&gt;Quick Start&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pytorch/executorch/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pytorch/executorch/main/#export-and-deploy-in-3-steps"&gt;Export and Deploy in 3 Steps&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pytorch/executorch/main/#run-on-device"&gt;Run on Device&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pytorch/executorch/main/#llm-example-llama"&gt;LLM Example: Llama&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pytorch/executorch/main/#platform--hardware-support"&gt;Platform &amp;amp; Hardware Support&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pytorch/executorch/main/#production-deployments"&gt;Production Deployments&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pytorch/executorch/main/#examples--models"&gt;Examples &amp;amp; Models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pytorch/executorch/main/#key-features"&gt;Key Features&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pytorch/executorch/main/#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pytorch/executorch/main/#community--contributing"&gt;Community &amp;amp; Contributing&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pytorch/executorch/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Why ExecuTorch?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üîí Native PyTorch Export&lt;/strong&gt; ‚Äî Direct export from PyTorch. No .onnx, .tflite, or intermediate format conversions. Preserve model semantics.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;‚ö° Production-Proven&lt;/strong&gt; ‚Äî Powers billions of users at &lt;a href="https://engineering.fb.com/2025/07/28/android/executorch-on-device-ml-meta-family-of-apps/"&gt;Meta with real-time on-device inference&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üíæ Tiny Runtime&lt;/strong&gt; ‚Äî 50KB base footprint. Runs on microcontrollers to high-end smartphones.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üöÄ &lt;a href="https://docs.pytorch.org/executorch/main/backends-overview.html"&gt;12+ Hardware Backends&lt;/a&gt;&lt;/strong&gt; ‚Äî Open-source acceleration for Apple, Qualcomm, ARM, MediaTek, Vulkan, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üéØ One Export, Multiple Backends&lt;/strong&gt; ‚Äî Switch hardware targets with a single line change. Deploy the same model everywhere.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How It Works&lt;/h2&gt; 
&lt;p&gt;ExecuTorch uses &lt;strong&gt;ahead-of-time (AOT) compilation&lt;/strong&gt; to prepare PyTorch models for edge deployment:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;üß© Export&lt;/strong&gt; ‚Äî Capture your PyTorch model graph with &lt;code&gt;torch.export()&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;‚öôÔ∏è Compile&lt;/strong&gt; ‚Äî Quantize, optimize, and partition to hardware backends ‚Üí &lt;code&gt;.pte&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üöÄ Execute&lt;/strong&gt; ‚Äî Load &lt;code&gt;.pte&lt;/code&gt; on-device via lightweight C++ runtime&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Models use a standardized &lt;a href="https://docs.pytorch.org/executorch/main/compiler-ir-advanced.html#intermediate-representation"&gt;Core ATen operator set&lt;/a&gt;. &lt;a href="https://docs.pytorch.org/executorch/main/compiler-delegate-and-partitioner.html"&gt;Partitioners&lt;/a&gt; delegate subgraphs to specialized hardware (NPU/GPU) with CPU fallback.&lt;/p&gt; 
&lt;p&gt;Learn more: &lt;a href="https://docs.pytorch.org/executorch/main/intro-how-it-works.html"&gt;How ExecuTorch Works&lt;/a&gt; ‚Ä¢ &lt;a href="https://docs.pytorch.org/executorch/main/getting-started-architecture.html"&gt;Architecture Guide&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install executorch
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For platform-specific setup (Android, iOS, embedded systems), see the &lt;a href="https://docs.pytorch.org/executorch/main/quick-start-section.html"&gt;Quick Start&lt;/a&gt; documentation for additional info.&lt;/p&gt; 
&lt;h3&gt;Export and Deploy in 3 Steps&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import torch
from executorch.exir import to_edge_transform_and_lower
from executorch.backends.xnnpack.partition.xnnpack_partitioner import XnnpackPartitioner

# 1. Export your PyTorch model
model = MyModel().eval()
example_inputs = (torch.randn(1, 3, 224, 224),)
exported_program = torch.export.export(model, example_inputs)

# 2. Optimize for target hardware (switch backends with one line)
program = to_edge_transform_and_lower(
    exported_program,
    partitioner=[XnnpackPartitioner()]  # CPU | CoreMLPartitioner() for iOS | QnnPartitioner() for Qualcomm
).to_executorch()

# 3. Save for deployment
with open("model.pte", "wb") as f:
    f.write(program.buffer)

# Test locally via ExecuTorch runtime's pybind API (optional)
from executorch.runtime import Runtime
runtime = Runtime.get()
method = runtime.load_program("model.pte").load_method("forward")
outputs = method.execute([torch.randn(1, 3, 224, 224)])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Run on Device&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.pytorch.org/executorch/main/using-executorch-cpp.html"&gt;C++&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-cpp"&gt;#include &amp;lt;executorch/extension/module/module.h&amp;gt;
#include &amp;lt;executorch/extension/tensor/tensor.h&amp;gt;

Module module("model.pte");
auto tensor = make_tensor_ptr({2, 2}, {1.0f, 2.0f, 3.0f, 4.0f});
auto outputs = module.forward(tensor);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.pytorch.org/executorch/main/ios-section.html"&gt;Swift (iOS)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-swift"&gt;import ExecuTorch

let module = Module(filePath: "model.pte")
let input = Tensor&amp;lt;Float&amp;gt;([1.0, 2.0, 3.0, 4.0], shape: [2, 2])
let outputs = try module.forward(input)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.pytorch.org/executorch/main/android-section.html"&gt;Kotlin (Android)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-kotlin"&gt;val module = Module.load("model.pte")
val inputTensor = Tensor.fromBlob(floatArrayOf(1.0f, 2.0f, 3.0f, 4.0f), longArrayOf(2, 2))
val outputs = module.forward(EValue.from(inputTensor))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;LLM Example: Llama&lt;/h3&gt; 
&lt;p&gt;Export Llama models using the &lt;a href="https://docs.pytorch.org/executorch/main/llm/export-llm.html"&gt;&lt;code&gt;export_llm&lt;/code&gt;&lt;/a&gt; script or &lt;a href="https://github.com/huggingface/optimum-executorch"&gt;Optimum-ExecuTorch&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Using export_llm
python -m executorch.extension.llm.export.export_llm --model llama3_2 --output llama.pte

# Using Optimum-ExecuTorch
optimum-cli export executorch \
  --model meta-llama/Llama-3.2-1B \
  --task text-generation \
  --recipe xnnpack \
  --output_dir llama_model
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run on-device with the LLM runner API:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.pytorch.org/executorch/main/llm/run-with-c-plus-plus.html"&gt;C++&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-cpp"&gt;#include &amp;lt;executorch/extension/llm/runner/text_llm_runner.h&amp;gt;

auto runner = create_llama_runner("llama.pte", "tiktoken.bin");
executorch::extension::llm::GenerationConfig config{
    .seq_len = 128, .temperature = 0.8f};
runner-&amp;gt;generate("Hello, how are you?", config);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.pytorch.org/executorch/main/llm/run-on-ios.html"&gt;Swift (iOS)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-swift"&gt;import ExecuTorchLLM

let runner = TextRunner(modelPath: "llama.pte", tokenizerPath: "tiktoken.bin")
try runner.generate("Hello, how are you?", Config {
    $0.sequenceLength = 128
}) { token in
    print(token, terminator: "")
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Kotlin (Android)&lt;/strong&gt; ‚Äî &lt;a href="https://docs.pytorch.org/executorch/main/javadoc/org/pytorch/executorch/extension/llm/package-summary.html"&gt;API Docs&lt;/a&gt; ‚Ä¢ &lt;a href="https://github.com/meta-pytorch/executorch-examples/tree/main/llm/android/LlamaDemo"&gt;Demo App&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-kotlin"&gt;val llmModule = LlmModule("llama.pte", "tiktoken.bin", 0.8f)
llmModule.load()
llmModule.generate("Hello, how are you?", 128, object : LlmCallback {
    override fun onResult(result: String) { print(result) }
    override fun onStats(stats: String) { }
})
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For multimodal models (vision, audio), use the &lt;a href="https://raw.githubusercontent.com/pytorch/executorch/main/extension/llm/runner"&gt;MultiModal runner API&lt;/a&gt; which extends the LLM runner to handle image and audio inputs alongside text. See &lt;a href="https://raw.githubusercontent.com/pytorch/executorch/main/examples/models/llava/README.md"&gt;Llava&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/pytorch/executorch/main/examples/models/voxtral/README.md"&gt;Voxtral&lt;/a&gt; examples.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/pytorch/executorch/main/examples/models/llama/README.md"&gt;examples/models/llama&lt;/a&gt; for complete workflow including quantization, mobile deployment, and advanced options.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Next Steps:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìñ &lt;a href="https://docs.pytorch.org/executorch/main/getting-started.html"&gt;Step-by-step tutorial&lt;/a&gt; ‚Äî Complete walkthrough for your first model&lt;/li&gt; 
 &lt;li&gt;‚ö° &lt;a href="https://colab.research.google.com/drive/1qpxrXC3YdJQzly3mRg-4ayYiOjC6rue3?usp=sharing"&gt;Colab notebook&lt;/a&gt; ‚Äî Try ExecuTorch instantly in your browser&lt;/li&gt; 
 &lt;li&gt;ü§ñ &lt;a href="https://raw.githubusercontent.com/pytorch/executorch/main/examples/models/llama/README.md"&gt;Deploy Llama models&lt;/a&gt; ‚Äî LLM workflow with quantization and mobile demos&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Platform &amp;amp; Hardware Support&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Platform&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Supported Backends&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Android&lt;/td&gt; 
   &lt;td&gt;XNNPACK, Vulkan, Qualcomm, MediaTek, Samsung Exynos&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;iOS&lt;/td&gt; 
   &lt;td&gt;XNNPACK, MPS, CoreML (Neural Engine)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Linux / Windows&lt;/td&gt; 
   &lt;td&gt;XNNPACK, OpenVINO, CUDA &lt;em&gt;(experimental)&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;macOS&lt;/td&gt; 
   &lt;td&gt;XNNPACK, MPS, Metal &lt;em&gt;(experimental)&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Embedded / MCU&lt;/td&gt; 
   &lt;td&gt;XNNPACK, ARM Ethos-U, NXP, Cadence DSP&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;See &lt;a href="https://docs.pytorch.org/executorch/main/backends-overview.html"&gt;Backend Documentation&lt;/a&gt; for detailed hardware requirements and optimization guides.&lt;/p&gt; 
&lt;h2&gt;Production Deployments&lt;/h2&gt; 
&lt;p&gt;ExecuTorch powers on-device AI at scale across Meta's family of apps, VR/AR devices, and partner deployments. &lt;a href="https://docs.pytorch.org/executorch/main/success-stories.html"&gt;View success stories ‚Üí&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Examples &amp;amp; Models&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;LLMs:&lt;/strong&gt; &lt;a href="https://raw.githubusercontent.com/pytorch/executorch/main/examples/models/llama/README.md"&gt;Llama 3.2/3.1/3&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/pytorch/executorch/main/examples/models/qwen3/README.md"&gt;Qwen 3&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/pytorch/executorch/main/examples/models/phi_4_mini/README.md"&gt;Phi-4-mini&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/pytorch/executorch/main/examples/models/lfm2/README.md"&gt;LiquidAI LFM2&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Multimodal:&lt;/strong&gt; &lt;a href="https://raw.githubusercontent.com/pytorch/executorch/main/examples/models/llava/README.md"&gt;Llava&lt;/a&gt; (vision-language), &lt;a href="https://raw.githubusercontent.com/pytorch/executorch/main/examples/models/voxtral/README.md"&gt;Voxtral&lt;/a&gt; (audio-language), &lt;a href="https://raw.githubusercontent.com/pytorch/executorch/main/examples/models/gemma3"&gt;Gemma&lt;/a&gt; (vision-language)&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Vision/Speech:&lt;/strong&gt; &lt;a href="https://github.com/meta-pytorch/executorch-examples/tree/main/mv2"&gt;MobileNetV2&lt;/a&gt;, &lt;a href="https://github.com/meta-pytorch/executorch-examples/tree/main/dl3"&gt;DeepLabV3&lt;/a&gt;, &lt;a href="https://github.com/meta-pytorch/executorch-examples/tree/main/whisper/android/WhisperApp"&gt;Whisper&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Resources:&lt;/strong&gt; &lt;a href="https://raw.githubusercontent.com/pytorch/executorch/main/examples/"&gt;&lt;code&gt;examples/&lt;/code&gt;&lt;/a&gt; directory ‚Ä¢ &lt;a href="https://github.com/meta-pytorch/executorch-examples"&gt;executorch-examples&lt;/a&gt; out-of-tree demos ‚Ä¢ &lt;a href="https://github.com/huggingface/optimum-executorch"&gt;Optimum-ExecuTorch&lt;/a&gt; for HuggingFace models&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;p&gt;ExecuTorch provides advanced capabilities for production deployment:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Quantization&lt;/strong&gt; ‚Äî Built-in support via &lt;a href="https://docs.pytorch.org/ao"&gt;torchao&lt;/a&gt; for 8-bit, 4-bit, and dynamic quantization&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Memory Planning&lt;/strong&gt; ‚Äî Optimize memory usage with ahead-of-time allocation strategies&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Developer Tools&lt;/strong&gt; ‚Äî ETDump profiler, ETRecord inspector, and model debugger&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Selective Build&lt;/strong&gt; ‚Äî Strip unused operators to minimize binary size&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Custom Operators&lt;/strong&gt; ‚Äî Extend with domain-specific kernels&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dynamic Shapes&lt;/strong&gt; ‚Äî Support variable input sizes with bounded ranges&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See &lt;a href="https://docs.pytorch.org/executorch/main/advanced-topics-section.html"&gt;Advanced Topics&lt;/a&gt; for quantization techniques, custom backends, and compiler passes.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.pytorch.org/executorch/main/index.html"&gt;&lt;strong&gt;Documentation Home&lt;/strong&gt;&lt;/a&gt; ‚Äî Complete guides and tutorials&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.pytorch.org/executorch/main/api-section.html"&gt;&lt;strong&gt;API Reference&lt;/strong&gt;&lt;/a&gt; ‚Äî Python, C++, Java/Kotlin APIs&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.pytorch.org/executorch/main/backend-delegates-integration.html"&gt;&lt;strong&gt;Backend Integration&lt;/strong&gt;&lt;/a&gt; ‚Äî Build custom hardware backends&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.pytorch.org/executorch/main/support-section.html"&gt;&lt;strong&gt;Troubleshooting&lt;/strong&gt;&lt;/a&gt; ‚Äî Common issues and solutions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Community &amp;amp; Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üí¨ &lt;a href="https://github.com/pytorch/executorch/discussions"&gt;&lt;strong&gt;GitHub Discussions&lt;/strong&gt;&lt;/a&gt; ‚Äî Ask questions and share ideas&lt;/li&gt; 
 &lt;li&gt;üéÆ &lt;a href="https://discord.gg/Dh43CKSAdc"&gt;&lt;strong&gt;Discord&lt;/strong&gt;&lt;/a&gt; ‚Äî Chat with the team and community&lt;/li&gt; 
 &lt;li&gt;üêõ &lt;a href="https://github.com/pytorch/executorch/issues"&gt;&lt;strong&gt;Issues&lt;/strong&gt;&lt;/a&gt; ‚Äî Report bugs or request features&lt;/li&gt; 
 &lt;li&gt;ü§ù &lt;a href="https://raw.githubusercontent.com/pytorch/executorch/main/CONTRIBUTING.md"&gt;&lt;strong&gt;Contributing Guide&lt;/strong&gt;&lt;/a&gt; ‚Äî Guidelines and codebase structure&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;ExecuTorch is BSD licensed, as found in the &lt;a href="https://raw.githubusercontent.com/pytorch/executorch/main/LICENSE"&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt; 
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;Part of the PyTorch ecosystem&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt; &lt;a href="https://github.com/pytorch/executorch"&gt;GitHub&lt;/a&gt; ‚Ä¢ &lt;a href="https://docs.pytorch.org/executorch"&gt;Documentation&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>pollen-robotics/reachy_mini</title>
      <link>https://github.com/pollen-robotics/reachy_mini</link>
      <description>&lt;p&gt;Reachy Mini's SDK&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Reachy Mini ü§ñ&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/chat/?attachments=https%3A%2F%2Fgist.githubusercontent.com%2FFabienDanieau%2F919e1d7468fb16e70dbe984bdc277bba%2Fraw%2Fdoc_reachy_mini_full.md&amp;amp;prompt=Read%20this%20documentation%20about%20Reachy%20Mini%20so%20I%20can%20ask%20questions%20about%20it."&gt;&lt;img src="https://img.shields.io/badge/Ask_on-HuggingChat-yellow?logo=huggingface&amp;amp;logoColor=yellow&amp;amp;style=for-the-badge" alt="Ask on HuggingChat" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/Y7FgMqHsub"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join_the_Community-7289DA?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Reachy Mini is an open-source, expressive robot made for hackers and AI builders.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;üõí &lt;a href="https://www.hf.co/reachy-mini/"&gt;&lt;strong&gt;Buy Reachy Mini&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.pollen-robotics.com/reachy-mini/"&gt;&lt;img src="https://raw.githubusercontent.com/pollen-robotics/reachy_mini/develop/docs/assets/reachy_mini_hello.gif" alt="Reachy Mini Hello" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;‚ö°Ô∏è Build and start your own robot&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Choose your platform to access the specific guide:&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;ü§ñ Reachy Mini (Wireless)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;üîå Reachy Mini Lite&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;üíª Simulation&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;The full autonomous experience.&lt;br /&gt;Raspberry Pi 4 + Battery + WiFi.&lt;/td&gt; 
   &lt;td align="center"&gt;The developer version.&lt;br /&gt;USB connection to your computer.&lt;/td&gt; 
   &lt;td align="center"&gt;No hardware required.&lt;br /&gt;Prototype in MuJoCo.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;üëâ &lt;a href="https://raw.githubusercontent.com/pollen-robotics/reachy_mini/develop/docs/platforms/reachy_mini/get_started.md"&gt;&lt;strong&gt;Go to Wireless Guide&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;üëâ &lt;a href="https://raw.githubusercontent.com/pollen-robotics/reachy_mini/develop/docs/platforms/reachy_mini_lite/get_started.md"&gt;&lt;strong&gt;Go to Lite Guide&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;üëâ &lt;a href="https://raw.githubusercontent.com/pollen-robotics/reachy_mini/develop/docs/platforms/simulation/get_started.md"&gt;&lt;strong&gt;Go to Simulation&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ö° &lt;strong&gt;Pro tip:&lt;/strong&gt; Install &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;uv&lt;/a&gt; for 10-100x faster app installations (auto-detected, falls back to &lt;code&gt;pip&lt;/code&gt;).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;br /&gt; 
&lt;h2&gt;üì± Apps &amp;amp; Ecosystem&lt;/h2&gt; 
&lt;p&gt;Reachy Mini comes with an app store powered by Hugging Face Spaces. You can install these apps directly from your robot's dashboard with one click!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üó£Ô∏è &lt;a href="https://huggingface.co/spaces/pollen-robotics/reachy_mini_conversation_app"&gt;Conversation App&lt;/a&gt;:&lt;/strong&gt; Talk naturally with Reachy Mini (powered by LLMs).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìª &lt;a href="https://huggingface.co/spaces/pollen-robotics/reachy_mini_radio"&gt;Radio&lt;/a&gt;:&lt;/strong&gt; Listen to the radio with Reachy Mini !&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üëã &lt;a href="https://huggingface.co/spaces/pollen-robotics/hand_tracker_v2"&gt;Hand Tracker&lt;/a&gt;:&lt;/strong&gt; The robot follows your hand movements in real-time.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üëâ &lt;a href="https://hf.co/reachy-mini/#/apps"&gt;&lt;strong&gt;Browse all apps on Hugging Face&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;üöÄ Getting Started with Reachy Mini SDK&lt;/h2&gt; 
&lt;h3&gt;Quick Look&lt;/h3&gt; 
&lt;p&gt;Control your robot in just &lt;strong&gt;a few lines of code&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from reachy_mini import ReachyMini
from reachy_mini.utils import create_head_pose

with ReachyMini() as mini:
    # Look up and tilt head
    mini.goto_target(
        head=create_head_pose(z=10, roll=15, degrees=True, mm=True),
        duration=1.0
    )
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;User guides&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/pollen-robotics/reachy_mini/develop/docs/SDK/installation.md"&gt;Installation&lt;/a&gt;&lt;/strong&gt;: 5 minutes to set up your computer&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/pollen-robotics/reachy_mini/develop/docs/SDK/quickstart.md"&gt;Quickstart Guide&lt;/a&gt;&lt;/strong&gt;: Run your first behavior on Reachy Mini&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/pollen-robotics/reachy_mini/develop/docs/SDK/python-sdk.md"&gt;Python SDK&lt;/a&gt;&lt;/strong&gt;: Learn to move, see, speak, and hear.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/pollen-robotics/reachy_mini/develop/docs/SDK/integration.md"&gt;AI Integrations&lt;/a&gt;&lt;/strong&gt;: Connect LLMs, build Apps, and publish to Hugging Face.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/pollen-robotics/reachy_mini/develop/docs/SDK/core-concept.md"&gt;Core Concepts&lt;/a&gt;&lt;/strong&gt;: Architecture, coordinate systems, and safety limits.&lt;/li&gt; 
 &lt;li&gt;ü§ó&lt;a href="https://huggingface.co/blog/pollen-robotics/make-and-publish-your-reachy-mini-apps"&gt;&lt;strong&gt;Share your app with the community&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìÇ &lt;a href="https://raw.githubusercontent.com/pollen-robotics/reachy_mini/develop/examples"&gt;&lt;strong&gt;Browse the Examples Folder&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h2&gt;üõ† Hardware Overview&lt;/h2&gt; 
&lt;p&gt;Reachy Mini robots are sold as kits and generally take &lt;strong&gt;2 to 3 hours&lt;/strong&gt; to assemble. Detailed step-by-step guides are available in the platform-specific folders linked above.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Reachy Mini (Wireless):&lt;/strong&gt; Runs onboard (RPi 4), autonomous, includes IMU. &lt;a href="https://raw.githubusercontent.com/pollen-robotics/reachy_mini/develop/docs/platforms/reachy_mini/hardware.md"&gt;See specs&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reachy Mini Lite:&lt;/strong&gt; Runs on your PC, powered via wall outlet. &lt;a href="https://raw.githubusercontent.com/pollen-robotics/reachy_mini/develop/docs/platforms/reachy_mini_lite/hardware.md"&gt;See specs&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h2&gt;‚ùì Troubleshooting&lt;/h2&gt; 
&lt;p&gt;Encountering an issue? üëâ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/pollen-robotics/reachy_mini/develop/docs/troubleshooting.md"&gt;Check the Troubleshooting &amp;amp; FAQ Guide&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;ü§ù Community &amp;amp; Contributing&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Join the Community:&lt;/strong&gt; Join &lt;a href="https://discord.gg/2bAhWfXme9"&gt;Discord&lt;/a&gt; to share your moments with Reachy, build apps together, and get help.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Found a bug?&lt;/strong&gt; Open an issue on this repository.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the Apache 2.0 License. See the &lt;a href="https://raw.githubusercontent.com/pollen-robotics/reachy_mini/develop/LICENSE"&gt;LICENSE&lt;/a&gt; file for details. Hardware design files are licensed under Creative Commons BY-SA-NC.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ansible/ansible</title>
      <link>https://github.com/ansible/ansible</link>
      <description>&lt;p&gt;Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy and maintain. Automate everything from code deployment to network configuration to cloud management, in a language that approaches plain English, using SSH, with no agents to install on remote systems. https://docs.ansible.com.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://pypi.org/project/ansible-core"&gt;&lt;img src="https://img.shields.io/pypi/v/ansible-core.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/latest/"&gt;&lt;img src="https://img.shields.io/badge/docs-latest-brightgreen.svg?sanitize=true" alt="Docs badge" /&gt;&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/devel/community/communication.html"&gt;&lt;img src="https://img.shields.io/badge/chat-IRC-brightgreen.svg?sanitize=true" alt="Chat badge" /&gt;&lt;/a&gt; &lt;a href="https://dev.azure.com/ansible/ansible/_build/latest?definitionId=20&amp;amp;branchName=devel"&gt;&lt;img src="https://dev.azure.com/ansible/ansible/_apis/build/status/CI?branchName=devel" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/devel/community/code_of_conduct.html"&gt;&lt;img src="https://img.shields.io/badge/code%20of%20conduct-Ansible-silver.svg?sanitize=true" alt="Ansible Code of Conduct" /&gt;&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/devel/community/communication.html#mailing-list-information"&gt;&lt;img src="https://img.shields.io/badge/mailing%20lists-Ansible-orange.svg?sanitize=true" alt="Ansible mailing lists" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/ansible/ansible/devel/COPYING"&gt;&lt;img src="https://img.shields.io/badge/license-GPL%20v3.0-brightgreen.svg?sanitize=true" alt="Repository License" /&gt;&lt;/a&gt; &lt;a href="https://bestpractices.coreinfrastructure.org/projects/2372"&gt;&lt;img src="https://bestpractices.coreinfrastructure.org/projects/2372/badge" alt="Ansible CII Best Practices certification" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Ansible&lt;/h1&gt; 
&lt;p&gt;Ansible is a radically simple IT automation system. It handles configuration management, application deployment, cloud provisioning, ad-hoc task execution, network automation, and multi-node orchestration. Ansible makes complex changes like zero-downtime rolling updates with load balancers easy. More information on the Ansible &lt;a href="https://ansible.com/"&gt;website&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Design Principles&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Have an extremely simple setup process with a minimal learning curve.&lt;/li&gt; 
 &lt;li&gt;Manage machines quickly and in parallel.&lt;/li&gt; 
 &lt;li&gt;Avoid custom-agents and additional open ports, be agentless by leveraging the existing SSH daemon.&lt;/li&gt; 
 &lt;li&gt;Describe infrastructure in a language that is both machine and human friendly.&lt;/li&gt; 
 &lt;li&gt;Focus on security and easy auditability/review/rewriting of content.&lt;/li&gt; 
 &lt;li&gt;Manage new remote machines instantly, without bootstrapping any software.&lt;/li&gt; 
 &lt;li&gt;Allow module development in any dynamic language, not just Python.&lt;/li&gt; 
 &lt;li&gt;Be usable as non-root.&lt;/li&gt; 
 &lt;li&gt;Be the easiest IT automation system to use, ever.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Use Ansible&lt;/h2&gt; 
&lt;p&gt;You can install a released version of Ansible with &lt;code&gt;pip&lt;/code&gt; or a package manager. See our &lt;a href="https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html"&gt;installation guide&lt;/a&gt; for details on installing Ansible on a variety of platforms.&lt;/p&gt; 
&lt;p&gt;Power users and developers can run the &lt;code&gt;devel&lt;/code&gt; branch, which has the latest features and fixes, directly. Although it is reasonably stable, you are more likely to encounter breaking changes when running the &lt;code&gt;devel&lt;/code&gt; branch. We recommend getting involved in the Ansible community if you want to run the &lt;code&gt;devel&lt;/code&gt; branch.&lt;/p&gt; 
&lt;h2&gt;Communication&lt;/h2&gt; 
&lt;p&gt;Join the Ansible forum to ask questions, get help, and interact with the community.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://forum.ansible.com/c/help/6"&gt;Get Help&lt;/a&gt;: Find help or share your Ansible knowledge to help others. Use tags to filter and subscribe to posts, such as the following: 
  &lt;ul&gt; 
   &lt;li&gt;Posts tagged with &lt;a href="https://forum.ansible.com/tag/ansible"&gt;ansible&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Posts tagged with &lt;a href="https://forum.ansible.com/tag/ansible-core"&gt;ansible-core&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Posts tagged with &lt;a href="https://forum.ansible.com/tag/playbook"&gt;playbook&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://forum.ansible.com/c/chat/4"&gt;Social Spaces&lt;/a&gt;: Meet and interact with fellow enthusiasts.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://forum.ansible.com/c/news/5"&gt;News &amp;amp; Announcements&lt;/a&gt;: Track project-wide announcements including social events.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.ansible.com/ansible/devel/community/communication.html#the-bullhorn"&gt;Bullhorn newsletter&lt;/a&gt;: Get release announcements and important changes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For more ways to get in touch, see &lt;a href="https://docs.ansible.com/ansible/devel/community/communication.html"&gt;Communicating with the Ansible community&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contribute to Ansible&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check out the &lt;a href="https://raw.githubusercontent.com/ansible/ansible/devel/.github/CONTRIBUTING.md"&gt;Contributor's Guide&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Read &lt;a href="https://docs.ansible.com/ansible/devel/community"&gt;Community Information&lt;/a&gt; for all kinds of ways to contribute to and interact with the project, including how to submit bug reports and code to Ansible.&lt;/li&gt; 
 &lt;li&gt;Submit a proposed code update through a pull request to the &lt;code&gt;devel&lt;/code&gt; branch.&lt;/li&gt; 
 &lt;li&gt;Talk to us before making larger changes to avoid duplicate efforts. This not only helps everyone know what is going on, but it also helps save time and effort if we decide some changes are needed.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Coding Guidelines&lt;/h2&gt; 
&lt;p&gt;We document our Coding Guidelines in the &lt;a href="https://docs.ansible.com/ansible/devel/dev_guide/"&gt;Developer Guide&lt;/a&gt;. We particularly suggest you review:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_checklist.html"&gt;Contributing your module to Ansible&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_best_practices.html"&gt;Conventions, tips, and pitfalls&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Branch Info&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;The &lt;code&gt;devel&lt;/code&gt; branch corresponds to the release actively under development.&lt;/li&gt; 
 &lt;li&gt;The &lt;code&gt;stable-2.X&lt;/code&gt; branches correspond to stable releases.&lt;/li&gt; 
 &lt;li&gt;Create a branch based on &lt;code&gt;devel&lt;/code&gt; and set up a &lt;a href="https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_general.html#common-environment-setup"&gt;dev environment&lt;/a&gt; if you want to open a PR.&lt;/li&gt; 
 &lt;li&gt;See the &lt;a href="https://docs.ansible.com/ansible/devel/reference_appendices/release_and_maintenance.html"&gt;Ansible release and maintenance&lt;/a&gt; page for information about active branches.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;p&gt;Based on team and community feedback, an initial roadmap will be published for a major or minor version (ex: 2.7, 2.8). The &lt;a href="https://docs.ansible.com/ansible/devel/roadmap/"&gt;Ansible Roadmap page&lt;/a&gt; details what is planned and how to influence the roadmap.&lt;/p&gt; 
&lt;h2&gt;Authors&lt;/h2&gt; 
&lt;p&gt;Ansible was created by &lt;a href="https://github.com/mpdehaan"&gt;Michael DeHaan&lt;/a&gt; and has contributions from over 5000 users (and growing). Thanks everyone!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.ansible.com"&gt;Ansible&lt;/a&gt; is sponsored by &lt;a href="https://www.redhat.com"&gt;Red Hat, Inc.&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;GNU General Public License v3.0 or later&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/ansible/ansible/devel/COPYING"&gt;COPYING&lt;/a&gt; to see the full text.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>